---
ver: rpa2
title: 'Simplicial SMOTE: Oversampling Solution to the Imbalanced Learning Problem'
arxiv_id: '2503.03418'
source_url: https://arxiv.org/abs/2503.03418
tags:
- smote
- data
- simplicial
- points
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Simplicial SMOTE, a novel geometric oversampling
  method for imbalanced learning. It improves upon SMOTE by modeling data using simplicial
  complexes instead of graphs, allowing sampling from higher-dimensional simplices
  formed by points in p-ary neighborhood relations.
---

# Simplicial SMOTE: Oversampling Solution to the Imbalanced Learning Problem

## Quick Facts
- arXiv ID: 2503.03418
- Source URL: https://arxiv.org/abs/2503.03418
- Reference count: 40
- Primary result: Simplicial SMOTE achieves 4.5% average F1 improvement over SMOTE for k-NN classifiers and 5% average improvement for gradient boosting classifiers on benchmark datasets.

## Executive Summary
This paper proposes Simplicial SMOTE, a geometric oversampling method that generalizes SMOTE by replacing neighborhood graphs with simplicial complexes. Instead of generating synthetic points only along edges between minority samples, Simplicial SMOTE samples from higher-dimensional simplices (triangles, tetrahedra, etc.) formed by points in p-ary neighborhood relations. This approach provides better coverage of the minority class distribution and generates synthetic points closer to the decision boundary. The method achieves significant improvements over SMOTE and several other geometric sampling methods on both synthetic and real datasets, with particularly strong results for k-NN classifiers.

## Method Summary
Simplicial SMOTE operates by first constructing a k-NN graph on minority class points, then computing the p-skeleton of the corresponding clique complex to extract maximal simplices. Synthetic points are generated by uniformly sampling simplices from this complex and then sampling barycentric coordinates from a symmetric Dirichlet distribution to compute convex combinations of simplex vertices. The method extends to simplicial versions of Borderline SMOTE, Safe-level SMOTE, and ADASYN by modifying the simplex selection criteria and Dirichlet parameters. Hyperparameter tuning is performed via nested cross-validation, with k and p grid-searched independently.

## Key Results
- Simplicial SMOTE outperforms SMOTE and other geometric sampling methods on 21 benchmark datasets
- For k-NN classifiers, achieves 4.5% average improvement in F1 score and up to 29.3% individual gains
- For gradient boosting classifiers, shows 5% average improvement and up to 25.7% individual gains
- All simplicial extensions (Borderline, Safe-level, ADASYN) outperform their graph-based counterparts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simplicial complexes provide better coverage of the minority class distribution than graph-based models.
- Mechanism: By modeling data as a union of higher-dimensional simplices rather than 1-dimensional edges, the method can sample synthetic points from the interior of convex regions spanned by multiple nearby minority points.
- Core assumption: The true minority class distribution occupies regions that can be approximated by local convex hulls of nearby points.
- Evidence anchors:
  - [abstract] "Such a replacement of the geometric data model results in better coverage of the underlying data distribution compared to existing geometric sampling methods"
  - [page 2] "Such graphs model the data as the union of one-dimensional segments, which is insufficient to sample from high-dimensional spaces"
- Break condition: If minority class data has highly non-convex local structure, simplicial interpolation may generate points in unoccupied regions.

### Mechanism 2
- Claim: Higher-dimensional simplices produce synthetic points closer to the decision boundary.
- Mechanism: For a simplex spanned by minority points equidistant to a majority point, the orthogonal projection of the majority point onto the simplex is closer than its projection onto any edge.
- Core assumption: The decision boundary lies near minority class points that are close to majority class points.
- Evidence anchors:
  - [page 2] "Hence, generating points by considering higher-dimensional simplices would result in synthetic points of the minor class being closer to the points of the majority class"
  - [page 2, Figure 2] Explicit geometric proof: projection distance to 2-simplex d₂=0.577 vs. edge d₁=0.707
- Break condition: If minority points near the boundary are noise or outliers, this mechanism may corrupt the decision boundary.

### Mechanism 3
- Claim: Barycentric coordinates sampled from a symmetric Dirichlet distribution provide uniform coverage within each simplex.
- Mechanism: For a p-simplex, synthetic points are generated as weighted sums where weights are drawn from Dir(α) with α=(1,...,1), ensuring uniform sampling over the simplex volume.
- Core assumption: Uniform sampling within simplices is appropriate.
- Evidence anchors:
  - [page 4, Algorithm 1] "Sample barycentric coordinates λᵢ ~ Dir(α), where α = (1,...,1)"
  - [page 4] "To sample uniformly from a p-simplex, we sample barycentric coordinates λ ∈ R^(p+1) according to the symmetric Dirichlet distribution"
- Break condition: If certain regions of a simplex are more likely to contain valid minority points, uniform sampling may be suboptimal.

## Foundational Learning

- Concept: **SMOTE (Synthetic Minority Oversampling Technique)**
  - Why needed here: Simplicial SMOTE is a direct generalization; understanding SMOTE's edge-based interpolation is prerequisite to grasping the simplex-based extension.
  - Quick check question: Can you explain how SMOTE generates a synthetic point from a minority sample and one of its k-nearest neighbors?

- Concept: **Simplicial Complexes and Cliques**
  - Why needed here: The core innovation replaces neighborhood graphs with their clique complexes; understanding that a p-simplex corresponds to a (p+1)-clique is essential.
  - Quick check question: Given a k-NN graph, what does a 3-clique in this graph correspond to in the simplicial complex?

- Concept: **Barycentric Coordinates**
  - Why needed here: Synthetic points are defined by barycentric coordinates relative to simplex vertices; these weights must sum to 1 and define convex combinations.
  - Quick check question: For a triangle with vertices at (0,0), (1,0), and (0.5,1), what point has barycentric coordinates (0.2, 0.3, 0.5)?

## Architecture Onboarding

- Component map: k-NN graph construction -> Clique complex computation -> Simplex sampling -> Barycentric sampling -> Variant integration
- Critical path: The clique enumeration step is the computational bottleneck. For sparse neighborhood graphs (small k), this remains tractable; for dense graphs, complexity grows rapidly.
- Design tradeoffs:
  - **k (neighborhood size)**: Larger k increases graph density, creating more/larger simplices but risks oversmoothing and higher clique computation cost.
  - **p (max simplex dimension)**: Higher p improves coverage but may generate overly smoothed central points.
  - **Simplex vs. edge sampling**: Simplicial sampling trades local precision for broader coverage.
- Failure signatures:
  - Noise amplification: If minority class contains outliers, simplices spanning them will generate synthetic points in empty regions.
  - Class overlap: On datasets with substantial class overlap, pushing synthetic points toward the boundary may increase overlap rather than clarify it.
  - High-dimensional sparsity: In very high dimensions with sparse data, simplices may be degenerate or span vast empty regions.
- First 3 experiments:
  1. Reproduce synthetic data comparison (page 5, Table 1): Implement Simplicial SMOTE on moons and circles datasets with k∈{3,...,8}, p=2, compare F1 vs. SMOTE using k-NN classifier.
  2. Hyperparameter sensitivity sweep (Appendix B): On a single benchmark dataset (e.g., ecoli), grid search k∈{3,5,7} and p∈{2,3,4,k} to identify optimal region.
  3. Ablation on simplex dimension: Fix k=5, compare p=1 (degenerates to SMOTE), p=2 (triangles), p=3 (tetrahedra) on 3 datasets with varying imbalance ratios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance advantage of Simplicial SMOTE persist in multi-class imbalanced classification scenarios?
- Basis in paper: [explicit] Section 4.2 explicitly states that while the method can theoretically handle multi-class scenarios, the experimental evaluation was restricted to binary classification tasks.
- Why unresolved: The paper provides no empirical validation or specific analysis regarding the interactions between multiple minority classes in a simplicial complex setting.
- What evidence would resolve it: Benchmark results on multi-class datasets comparing Simplicial SMOTE against standard multi-class SMOTE extensions.

### Open Question 2
- Question: Can the maximal relation arity $p$ be determined adaptively based on intrinsic data properties to avoid manual tuning?
- Basis in paper: [inferred] The Conclusion notes that the optimal $p$ follows a trade-off and currently recommends a grid search, implying a lack of theoretical guidance for automatic selection.
- Why unresolved: The paper demonstrates sensitivity to $p$ but does not provide a heuristic or theoretical bound to set this hyperparameter without cross-validation.
- What evidence would resolve it: A derived theoretical relationship or a proposed heuristic algorithm that correlates the optimal $p$ with local data density or dimensionality.

### Open Question 3
- Question: How does the computational complexity of Simplicial SMOTE scale with high-dimensional data where neighborhood graphs become dense?
- Basis in paper: [inferred] Section 3.3 acknowledges that the clique finding step is NP-complete in the worst case and depends on graph density, which is typically affected by the "curse of dimensionality."
- Why unresolved: The experiments were conducted on moderate-dimensional datasets; it is unclear if the assumption of graph sparsity holds for significantly higher-dimensional feature spaces.
- What evidence would resolve it: Scalability analysis on high-dimensional datasets (e.g., >1000 features) measuring the growth of clique enumeration time relative to feature count.

## Limitations
- Theoretical advantage of simplicial sampling over graph-based methods is demonstrated geometrically for 2-simplices but lacks formal proofs for general p
- Computational complexity of clique enumeration is not rigorously analyzed beyond reporting 1.4× slowdown on benchmarks
- Performance on highly non-convex or overlapping class distributions remains unexplored

## Confidence
- **High confidence**: Simplicial SMOTE improves F1 scores on tested datasets (4.5% average gain for k-NN, 5% for gradient boosting)
- **Medium confidence**: The mechanism of improved coverage through higher-dimensional simplices (geometric intuition supported but not formally proven)
- **Medium confidence**: Extension to simplicial Borderline/Safe-level/ADASYN variants outperform graph-based versions (limited to 4 variants, small margin improvements)

## Next Checks
1. **Theoretical analysis**: Prove that simplicial sampling achieves strictly better coverage than edge sampling for convex minority class distributions, or identify conditions where this fails.
2. **Scaling experiment**: Benchmark clique enumeration time vs. dataset size (n) and dimension (d) on synthetic graphs to establish computational limits and identify when simplicial sampling becomes impractical.
3. **Robustness test**: Evaluate performance on datasets with known non-convex minority class structure (e.g., concentric rings, disconnected components) to measure degradation and identify failure modes.