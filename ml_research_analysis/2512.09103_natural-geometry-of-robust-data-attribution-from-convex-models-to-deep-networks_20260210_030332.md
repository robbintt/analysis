---
ver: rpa2
title: 'Natural Geometry of Robust Data Attribution: From Convex Models to Deep Networks'
arxiv_id: '2512.09103'
source_url: https://arxiv.org/abs/2512.09103
tags:
- training
- natural
- influence
- data
- lipschitz
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the fragility of data attribution methods in
  deep learning, where small perturbations to training data can significantly alter
  influence rankings. The authors develop a unified framework for certified robust
  attribution that extends from convex models to deep networks.
---

# Natural Geometry of Robust Data Attribution: From Convex Models to Deep Networks

## Quick Facts
- arXiv ID: 2512.09103
- Source URL: https://arxiv.org/abs/2512.09103
- Authors: Shihao Li; Jiachen Li; Dongmei Chen
- Reference count: 15
- The paper develops a unified framework for certified robust attribution that extends from convex models to deep networks, achieving non-vacuous certification for neural network attribution.

## Executive Summary
This paper addresses a critical vulnerability in data attribution methods for deep learning: small perturbations to training data can dramatically alter influence rankings, undermining trust in attribution results. The authors introduce the Natural Wasserstein metric, which measures perturbations in the geometry induced by the model's own feature covariance, effectively eliminating spectral amplification that renders traditional Euclidean certification vacuous. This framework provides the first non-vacuous certified bounds for neural network attribution, certifying 68.7% of ranking pairs on CIFAR-10 with ResNet-18 compared to 0% for baseline approaches.

## Method Summary
The paper develops a unified framework for certified robust attribution that bridges convex models and deep networks. The core innovation is the Natural Wasserstein metric, which measures perturbations in the geometry induced by the model's feature covariance matrix, eliminating spectral amplification that plagues Euclidean certification. For convex models, the authors derive Wasserstein-Robust Influence Functions (W-RIF) with provable coverage guarantees. For deep networks, they prove that Self-Influence (leverage score) equals the Lipschitz constant governing attribution stability under the Natural metric. The primary method, Natural W-TRAK, extends TRAK (a quadratic form influence method) to certified attribution by bounding the change in influence scores under Wasserstein-perturbed training data.

## Key Results
- On CIFAR-10 with ResNet-18, Natural W-TRAK certifies 68.7% of ranking pairs compared to 0% for Euclidean baselines
- Self-Influence achieves 0.970 AUROC for label noise detection, identifying 94.1% of corrupted labels by examining just the top 20% of training data
- The Natural Wasserstein metric eliminates spectral amplification, preventing Lipschitz bounds from inflating by over 10,000x in deep representations

## Why This Works (Mechanism)
The Natural Wasserstein metric works by measuring perturbations in the geometry induced by the model's own feature covariance matrix rather than using Euclidean distance. This approach eliminates spectral amplification - a phenomenon where ill-conditioned deep representations cause Lipschitz bounds to inflate dramatically in Euclidean space. By using the model's internal geometry, the framework naturally adapts to the representation space where attribution is computed, providing tighter and more meaningful certification bounds.

## Foundational Learning
- **Influence Functions**: A classical technique for estimating how training data affects model predictions by considering infinitesimal upweighting of training points. Why needed: Forms the theoretical foundation for attribution methods. Quick check: Verify the influence calculation matches the expected quadratic form for quadratic losses.
- **Mahalanobis Distance**: A distance metric that accounts for the covariance structure of the data, measuring distance in units of standard deviation. Why needed: The Natural Wasserstein metric is essentially a Mahalanobis distance in feature space. Quick check: Confirm the metric reduces to Euclidean distance when covariance is identity.
- **Leverage Scores**: A measure of how much a data point influences the model, equivalent to the diagonal of the hat matrix in linear regression. Why needed: Self-Influence is shown to be equivalent to leverage scores in deep networks. Quick check: Verify Self-Influence values are bounded and interpretable as leverage.
- **Lipschitz Continuity**: A property ensuring that small changes in input lead to proportionally small changes in output. Why needed: Certification bounds rely on Lipschitz constants to guarantee robustness. Quick check: Compute Lipschitz bounds under both metrics to observe the dramatic difference.
- **Spectral Amplification**: The phenomenon where ill-conditioned matrices cause numerical instability and inflated bounds. Why needed: The paper's core contribution is eliminating this issue. Quick check: Measure condition numbers of feature covariance matrices across layers.

## Architecture Onboarding
**Component Map**: Data -> Feature Extraction -> Covariance Estimation -> Metric Computation -> Attribution -> Certification -> Robustness Analysis

**Critical Path**: Training data → Feature extraction → Covariance matrix Q → Natural Wasserstein metric → Influence computation → Certification bounds

**Design Tradeoffs**: The framework trades computational complexity (O(n²) for all-pair influences) for provable robustness guarantees. The Natural metric provides tighter bounds but requires careful handling of OOD points through heuristic capping.

**Failure Signatures**: Vacuous certification bounds (intervals containing zero), inflated Self-Influence values for OOD points, computational bottlenecks with large training sets.

**First Experiments**: 1) Verify certification rates on CIFAR-10 with ResNet-18, 2) Test label noise detection on corrupted CIFAR-10, 3) Compare Lipschitz bounds under Euclidean vs Natural metrics on a simple convex model.

## Open Questions the Paper Calls Out
### Open Question 1
Can the Natural Wasserstein framework be extended to attribution methods beyond TRAK, such as TracIn or Datamodels?
- Basis in paper: Section 7 states future work could "extend the framework to other attribution methods beyond TRAK."
- Why unresolved: The current derivation relies on TRAK's specific quadratic form. It is unclear if the spectral amplification elimination applies to methods with different functional structures or implicit Hessians.
- What evidence would resolve it: Deriving Lipschitz bounds for other influence variants under the Natural metric and empirically testing if they yield non-vacuous certification.

### Open Question 2
How can the Natural metric be adapted to maintain certification guarantees under test-time distribution shift?
- Basis in paper: Section 7 suggests exploring "adaptive metrics that account for test-time distribution shift."
- Why unresolved: The metric is currently static, defined by the training covariance. If test inputs drift, the fixed geometry may not correctly counterbalance amplification in the shifted feature space.
- What evidence would resolve it: An adaptive theoretical extension where the metric updates based on test-time statistics, validated on datasets with controlled covariate shift.

### Open Question 3
Can the framework scale to Large Language Models (LLMs) given the prohibitive cost of gradient feature computations?
- Basis in paper: Section 7 notes the "framework assumes access to gradient features, which may be expensive for very large models."
- Why unresolved: Computing and inverting the covariance is intractable for LLMs with billions of parameters. Standard random projections might distort the specific geometry required for certification.
- What evidence would resolve it: Demonstrating certified attribution on models with >1B parameters using efficient approximations that preserve Mahalanobis geometry.

### Open Question 4
Is there a theoretically grounded alternative to heuristic capping for handling Self-Influence inflation in Out-of-Distribution (OOD) test points?
- Basis in paper: Section 5.5 acknowledges OOD points inflate Self-Influence, requiring "Practical Mitigation" (capping) to prevent vacuous certification.
- Why unresolved: Capping is a heuristic trade-off. A formal bound that naturally integrates the distance from the training manifold without hard thresholds is lacking.
- What evidence would resolve it: A modified certification bound that incorporates manifold distance into the radius, allowing valid intervals for OOD points without artificial capping.

## Limitations
- The computational complexity of Natural W-TRAK is O(n²) for all-pair influence computations, making it prohibitive for large datasets without optimization
- Certification guarantees rely on assumptions about loss landscape smoothness that may not hold in highly non-convex deep networks
- The framework requires access to gradient features, which is computationally expensive for very large models

## Confidence
- Natural Wasserstein metric effectiveness: High - Supported by rigorous mathematical analysis and empirical validation showing 68.7% certification rate versus 0% for Euclidean baselines
- Self-Influence as Lipschitz constant: Medium - Theoretically sound but requires more extensive empirical validation across diverse architectures
- Label noise detection capability: High - Strong AUROC score (0.970) and practical identification rate (94.1% of corrupted labels in top 20%)

## Next Checks
1. Evaluate Natural W-TRAK on larger-scale vision datasets (ImageNet) and different model architectures (Vision Transformers) to assess generalizability
2. Perform ablation studies on the impact of different perturbation magnitudes in the Natural Wasserstein space to establish practical bounds
3. Test the robustness of certified attributions against adversarial training methods and compare with alternative attribution approaches under distribution shift scenarios