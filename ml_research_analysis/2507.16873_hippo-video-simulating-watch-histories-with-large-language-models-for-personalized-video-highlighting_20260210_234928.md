---
ver: rpa2
title: 'HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized
  Video Highlighting'
arxiv_id: '2507.16873'
source_url: https://arxiv.org/abs/2507.16873
tags:
- video
- user
- preferences
- videos
- watch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HIPPO-VIDEO, a novel dataset for personalized
  video highlighting that addresses the limitations of existing video datasets by
  incorporating user watch histories to capture complex preferences. The dataset,
  generated using an LLM-based user simulator, contains 2,040 (watch history, saliency
  score) pairs covering 20,400 videos across 170 semantic categories.
---

# HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting

## Quick Facts
- arXiv ID: 2507.16873
- Source URL: https://arxiv.org/abs/2507.16873
- Authors: Jeongeun Lee; Youngjae Yu; Dongha Lee
- Reference count: 28
- Introduces HIPPO-VIDEO dataset for personalized video highlighting using LLM-generated watch histories

## Executive Summary
This paper addresses the challenge of personalized video highlighting by introducing HIPPO-VIDEO, a novel dataset that incorporates user watch histories to capture complex viewing preferences. Unlike existing video datasets that lack personalization capabilities, HIPPO-VIDEO uses LLM-based user simulation to generate 2,040 (watch history, saliency score) pairs across 20,400 videos spanning 170 semantic categories. The authors propose HiPHer, a Transformer-based method that leverages personalized watch histories to predict preference-conditioned segment-wise saliency scores, achieving superior performance compared to generic and query-based approaches.

## Method Summary
The authors developed an LLM-based user simulator to generate synthetic watch histories that reflect diverse viewing preferences across 170 semantic categories. Using this simulator, they created the HIPPO-VIDEO dataset containing 2,040 paired examples of watch histories and corresponding saliency scores for video segments. The HiPHer method employs a Transformer architecture to process watch history sequences and predict personalized saliency scores for video segments. The approach conditions saliency prediction on individual user preferences derived from their viewing history, enabling more relevant and personalized video highlighting compared to one-size-fits-all approaches.

## Key Results
- HIPPO-VIDEO dataset contains 2,040 (watch history, saliency score) pairs covering 20,400 videos across 170 semantic categories
- HiPHer achieves RMSE of 0.301 and mAP of 0.766 in personalized video highlighting
- Outperforms existing generic and query-based approaches in capturing user-specific preferences for video content

## Why This Works (Mechanism)
The approach works by leveraging the rich contextual information embedded in user watch histories to model individual preferences. By conditioning saliency prediction on personalized viewing patterns, the system can identify content segments that are more likely to be relevant and engaging to specific users. The LLM-based simulation captures complex preference patterns that emerge from diverse viewing behaviors across semantic categories, enabling more nuanced personalization than traditional content-based or collaborative filtering approaches.

## Foundational Learning
- **User preference modeling**: Essential for understanding individual viewing patterns and predicting content relevance
  - *Why needed*: Enables personalized highlighting rather than generic content selection
  - *Quick check*: Verify that watch history features capture meaningful preference signals

- **Saliency prediction in video**: Critical for identifying visually and contextually important segments
  - *Why needed*: Determines which portions of video content merit highlighting
  - *Quick check*: Confirm that saliency scores align with human attention patterns

- **Transformer architectures for sequence modeling**: Enables effective processing of variable-length watch history sequences
  - *Why needed*: Handles temporal dependencies in viewing behavior
  - *Quick check*: Validate that attention mechanisms capture relevant historical patterns

## Architecture Onboarding
**Component Map**: Watch History Input -> Transformer Encoder -> Saliency Prediction Head -> Personalized Highlights

**Critical Path**: User watch history sequences are encoded through the Transformer, which learns to map viewing patterns to preference-conditioned saliency scores for video segments.

**Design Tradeoffs**: The system prioritizes personalization accuracy over computational efficiency, using complex Transformer architectures rather than simpler linear models. The synthetic dataset generation enables controlled experimentation but may not fully capture real-world viewing complexity.

**Failure Signatures**: Poor personalization may occur when watch histories lack sufficient diversity, when semantic categories don't match user interests, or when the Transformer fails to capture long-range dependencies in viewing patterns.

**First Experiments**: 
1. Test HiPHer's performance across different semantic category distributions
2. Evaluate cold-start performance with minimal watch history
3. Compare synthetic vs. real user watch history performance

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic watch histories generated by LLMs may not fully capture real human viewing behaviors and preferences
- The dataset's 170 semantic categories may not represent the full diversity of real-world content consumption
- Limited qualitative validation of whether predicted highlights align with actual user preferences and perceived relevance

## Confidence
**High confidence**: The technical implementation of HiPHer as a Transformer-based architecture is sound and well-executed, with clearly demonstrated performance improvements over baseline methods.

**Medium confidence**: The claim that watch history provides superior personalization is supported by experimental results, though real-world applicability depends on factors like data privacy and cold-start scenarios not fully explored in the paper.

**Medium confidence**: The representativeness of "complex user preferences" in the synthetic dataset is plausible but uncertain due to potential LLM simulation artifacts and limited ecological validity.

## Next Checks
1. Conduct user studies with real viewers to validate whether HiPHer's predicted highlights align with actual user preferences across diverse demographic groups.

2. Test the system's performance in cold-start scenarios where minimal watch history is available, and evaluate how it transitions from generic to personalized highlighting.

3. Analyze the sensitivity of results to different LLM configurations and training data used for user simulation to quantify the impact of simulation artifacts on highlighting performance.