---
ver: rpa2
title: 'From Curiosity to Competence: How World Models Interact with the Dynamics
  of Exploration'
arxiv_id: '2507.08210'
source_url: https://arxiv.org/abs/2507.08210
tags:
- agent
- exploration
- gain
- empowerment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how intrinsic motivations\u2014curiosity\
  \ and competence\u2014guide exploration in model-based reinforcement learning agents.\
  \ We compared two architectures: a tabular agent with fixed state representations\
  \ and a Dreamer agent with learned latent states."
---

# From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration

## Quick Facts
- arXiv ID: 2507.08210
- Source URL: https://arxiv.org/abs/2507.08210
- Reference count: 6
- This study investigates how intrinsic motivations—curiosity and competence—guide exploration in model-based reinforcement learning agents.

## Executive Summary
This study investigates how intrinsic motivations—curiosity and competence—guide exploration in model-based reinforcement learning agents. We compared two architectures: a tabular agent with fixed state representations and a Dreamer agent with learned latent states. Our results show distinct trade-offs among intrinsic motivation strategies. Curiosity-driven exploration (novelty and information gain) can become stuck in local optima or fixate on irreducible uncertainty, while competence-driven exploration (empowerment) sometimes hinders exploration by staying in a controllable "comfort zone" but can also avoid harmful stochasticity to preserve agency. Hybrid strategies combining information gain and empowerment achieved better exploration-safety balances in the tabular agent and more robust generalization in the Dreamer agent. These findings illustrate the complementarity of curiosity and competence in adaptive exploration.

## Method Summary
The study employed two reinforcement learning architectures: a tabular Dyna-Q agent with fixed state representations and a Dreamer agent with learned latent states. Both were trained on grid-world environments with varying levels of stochasticity and risk (including lava hazards). Intrinsic motivation strategies were implemented through novelty, information gain, and empowerment rewards. The agents were evaluated on exploration efficiency, safety (avoiding hazards), and robustness to stochastic dynamics across multiple experimental conditions.

## Key Results
- Curiosity-driven exploration can become trapped in local optima or fixate on irreducible uncertainty
- Competence-driven exploration sometimes stays in controllable "comfort zones" but can avoid harmful stochasticity
- Hybrid strategies combining information gain and empowerment showed better exploration-safety balances in the tabular agent and more robust generalization in the Dreamer agent

## Why This Works (Mechanism)
The study demonstrates that different intrinsic motivations create distinct exploration dynamics based on their fundamental objectives. Curiosity (novelty and information gain) drives agents to seek novel states and reduce uncertainty, while competence (empowerment) drives agents to maintain and expand their control over the environment. These motivations interact with the world model's ability to predict and plan, creating trade-offs between exploration breadth and safety. The hybrid approaches work by balancing the predictive uncertainty reduction of curiosity with the control-oriented optimization of competence.

## Foundational Learning
- **Intrinsic Motivation**: Why needed - Drives exploration beyond extrinsic rewards; Quick check - Agent explores unseen states without reward signals
- **World Models**: Why needed - Enable planning and uncertainty estimation; Quick check - Agent can predict next states given actions
- **Empowerment**: Why needed - Measures agent's control over environment; Quick check - Higher empowerment correlates with more available actions
- **Information Gain**: Why needed - Quantifies reduction in predictive uncertainty; Quick check - Decreases as model becomes more certain
- **Stochastic Dynamics**: Why needed - Creates irreducible uncertainty; Quick check - Same action yields different outcomes

## Architecture Onboarding

**Component Map:** Dreamer (latent dynamics -> world model -> actor-critic) <-(intrinsic rewards)-> Tabular (state-action table -> planning)

**Critical Path:** Observation -> World Model Prediction -> Intrinsic Reward Calculation -> Policy Update -> Action

**Design Tradeoffs:** Learned latent states vs fixed tabular states; pure curiosity vs pure competence vs hybrid strategies; exploration efficiency vs safety

**Failure Signatures:** Getting stuck in local optima (curiosity), avoiding exploration (competence), poor generalization (wrong architecture)

**First Experiments:** 1) Test pure novelty in deterministic environment, 2) Test empowerment in stochastic environment, 3) Compare hybrid strategies in mixed environment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can agents achieve more adaptive exploration by dynamically adjusting the balance between curiosity and competence based on environmental context?
- Basis in paper: [explicit] The Discussion explicitly proposes integrating "adaptive motivation strategies that dynamically adapt the weighting of curiosity and competence based on environment type" as future work.
- Why unresolved: The current study relied on fixed combinations (sum or product) of intrinsic rewards rather than context-dependent modulation.
- What evidence would resolve it: A meta-learning agent that successfully modulates its intrinsic reward weights in response to changes in environmental stochasticity or risk.

### Open Question 2
- Question: Do the observed "safety first" behaviors in empowerment-driven agents accurately predict human exploration under threat?
- Basis in paper: [explicit] The authors suggest testing whether "empowerment-like 'safety first' models of behavior could be used to describe human subjects under threat-of-shock paradigms."
- Why unresolved: The current results are purely computational and have not been validated against human behavioral data in high-risk scenarios.
- What evidence would resolve it: Behavioral experiments demonstrating that humans, like the simulated agents, prioritize controllable states when exposed to penalties analogous to the "lava" in the study.

### Open Question 3
- Question: Do the benefits of combining information gain and empowerment generalize to open-ended environments or real-world robotics?
- Basis in paper: [inferred] The authors note that simplified grid-worlds "exclude challenges posed by open-ended worlds" and suggest scaling to robotics as a necessary extension.
- Why unresolved: It remains unclear if the synergy between curiosity and competence holds when the state space is high-dimensional and the "world model" must handle raw, noisy sensory data.
- What evidence would resolve it: The successful application of the hybrid motivation framework in complex robotic tasks requiring robustness to stochastic dynamics.

## Limitations
- Results may not generalize to high-dimensional environments with partial observability
- Only specific intrinsic motivation strategies were explored, missing other potential mechanisms
- Performance metrics focus on exploration efficiency and safety, potentially overlooking sample efficiency

## Confidence
- Distinct trade-offs between curiosity and competence: **High**
- Hybrid strategies showing better performance: **Medium**
- Avoiding harmful stochasticity through competence: **Medium**

## Next Checks
1. Test hybrid strategies across broader range of environments, particularly high-dimensional and partially observable ones
2. Conduct ablation studies to isolate individual contributions of each intrinsic motivation component
3. Implement additional metrics to evaluate sample efficiency and transfer learning capabilities