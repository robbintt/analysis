---
ver: rpa2
title: Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N
  Sampling
arxiv_id: '2601.22636'
source_url: https://arxiv.org/abs/2601.22636
tags:
- risk
- budget
- adversarial
- beta
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are evaluated for safety under single-shot
  or low-budget adversarial prompting, which underestimates real-world risk. Attackers
  can exploit large-scale parallel sampling to repeatedly probe a model until a harmful
  response is produced, making risk increase sharply with sampling budget.
---

# Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling

## Quick Facts
- **arXiv ID**: 2601.22636
- **Source URL**: https://arxiv.org/abs/2601.22636
- **Reference count**: 40
- **Primary result**: SABER reduces Mean Absolute Error in predicting large-scale adversarial risk from 12.04 to 1.66 (86.2% improvement) using only 100 samples per query

## Executive Summary
Large language models are typically evaluated for safety under single-shot or low-budget adversarial prompting, which significantly underestimates real-world risk. Attackers can exploit large-scale parallel sampling (Best-of-N) to repeatedly probe models until harmful responses are produced, causing risk to increase sharply with sampling budget. This paper proposes SABER, a scaling-aware statistical framework that models sample-level success probabilities using a Beta distribution and derives analytic scaling laws to reliably extrapolate large-N attack success rates from small-budget measurements.

The key innovation is anchoring the estimator at small known budgets, which reduces sensitivity to parameter estimation errors and enables accurate risk prediction. Using only n=100 samples, SABER predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for baseline methods. This represents a significant advance in realistic LLM safety assessment, revealing that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure.

## Method Summary
The method involves collecting n=100 attack attempts per query to generate binary success/failure outcomes, then fitting a Beta-Binomial Maximum Likelihood Estimator (MLE) to estimate the Beta distribution parameters (α, β) governing the sample-level success probabilities. The SABER-Anchored estimator then extrapolates to target N=1000 using the formula: ASR@N = 1 - (1-ASR@n)(n/N)^α̂. This approach leverages the analytic scaling law derived from Beta-Bernoulli conjugacy, where the failure rate decays as a power law N^(-α). The anchored variant specifically uses the observed small-N ASR to solve for the leading constant, avoiding sensitivity to β estimation errors.

## Key Results
- SABER-Anchored reduces MAE from 12.04 to 1.66 when predicting ASR@1000 from n=100 samples (86.2% error reduction)
- Risk amplification profiles are heterogeneous across different (attacker, victim, judge) triplets
- Models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure
- The scaling behavior is governed by the α parameter of the Beta distribution, which controls the rate of risk amplification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The scaling of adversarial risk (ASR@N) is governed by the distribution of sample-level success probabilities across the query dataset, not just the single-shot success rate.
- **Mechanism:** The paper models the per-query attack success probability θ as a random variable drawn from a Beta distribution, θ ~ Beta(α, β). The probability of failing N independent attempts follows a Bernoulli hierarchy. The parameter α specifically controls the "heaviness" of the left tail (low-probability queries), which dictates how quickly risk accumulates as N increases.
- **Core assumption:** The sample-level success probabilities {θi} for a given (attacker, victim, judge) triplet are i.i.d. draws from a Beta distribution (Assumption 3.2).
- **Evidence anchors:** [Section 3.2] "Theorem 3.3 implies that large-N risk amplification is governed by the left-tail behavior of the sample-level vulnerability distribution: α controls the scaling speed in N." [Abstract] "We model sample-level success probabilities using a Beta distribution... and derive an analytic scaling law."
- **Break condition:** If the per-query success probabilities are bimodal (e.g., queries are either perfectly safe or always broken) rather than Beta-distributed, the scaling law may lose accuracy.

### Mechanism 2
- **Claim:** Large-scale risk can be predicted from small-budget measurements via an analytic scaling law derived from Gamma function asymptotics.
- **Mechanism:** By treating Best-of-N sampling as an OR-aggregation of Bernoulli trials under a Beta prior, the paper derives a closed-form survival function (Theorem 3.3). The failure rate 1 - ASR@N decays as a power law C · N^(-α). This allows extrapolation from observable data at n=100 to predict risk at N=1000.
- **Core assumption:** The asymptotic approximation (Stirling-type expansion of Gamma functions) is accurate at the moderate N values used in practice (e.g., N ≥ 20).
- **Evidence anchors:** [Section 3.2] "ASR@N ≃ 1 - Γ(α+β)/Γ(β) · N^(-α)... governing the rate of risk amplification." [Section 3.4] "Theorem 3.3 is an asymptotic statement... empirically, this source of error is typically negligible once N leaves the very small regime."
- **Break condition:** If the attacker's success probability is not stationary (i.e., the model updates or the attacker adapts strategy during the N attempts), the i.i.d. Bernoulli assumption fails.

### Mechanism 3
- **Claim:** Anchoring the estimator at a small known budget reduces sensitivity to estimation errors in the Beta distribution parameters.
- **Mechanism:** The "SABER-Anchored" estimator leverages the observed ASR@n to solve for the leading constant in the scaling law, bypassing the need to precisely estimate the β parameter. It predicts ASR@N using the formula 1 - (1 - ASR@n)(n/N)^α̂. This relies on the observation that α alone determines the slope of the risk curve in log-log space.
- **Core assumption:** The observed small-budget ASR@n is a reliable, low-noise estimate of the true risk at that scale.
- **Evidence anchors:** [Section 3.4] "SABER-Anchored... depends only on ASR@n, N, and α̂, and therefore avoids sensitivity to β̂." [Section 4.2.1] "Our anchored estimator reduces the Mean Absolute Error (MAE) from 12.04 to 1.66... an 86.2% relative error reduction."
- **Break condition:** If the small-budget measurement n is too small (e.g., n < 5), the observed ASR@n may be statistically unstable (high variance), leading to unreliable anchoring.

## Foundational Learning

- **Concept: Beta Distribution as Conjugate Prior**
  - **Why needed here:** The paper models binary attack outcomes (Success/Fail) as Bernoulli trials. The "success rate" is unknown and varies by query. The Beta distribution is the conjugate prior for the Bernoulli likelihood, meaning the posterior distribution is also a Beta, allowing closed-form updates and analysis.
  - **Quick check question:** If you observe 3 successes in 10 attempts for a specific query, how does the Beta prior update? (Answer: Add 3 to α, add 7 to β).

- **Concept: Power Law Scaling**
  - **Why needed here:** The paper derives that the survival probability (1 - ASR) decays as a power law N^(-α). Understanding that a small change in α (the exponent) leads to massive differences in long-term risk is critical for interpreting the results.
  - **Quick check question:** If α=0.5, does doubling the attempts N halve the failure rate? (Answer: No, it reduces it by a factor of √2 ≈ 1.41).

- **Concept: Best-of-N (BoN) Sampling**
  - **Why needed here:** This is the threat model. It assumes an attacker is successful if any one of N independent attempts succeeds. The math relies on aggregating these independent events.
  - **Quick check question:** Does BoN risk increase linearly with N? (Answer: No, it follows a diminishing returns curve dictated by the underlying distribution).

## Architecture Onboarding

- **Component map:** Data Collector -> Distribution Fitter (Beta-Binomial MLE) -> Risk Predictor
- **Critical path:** The accuracy of the MLE fit for α. The estimator is highly sensitive to α̂ because it is the exponent of the scaling law.
- **Design tradeoffs:**
  - SABER-Anchored vs. Plugin: Anchored is preferred for large N extrapolation (more robust to β errors). Plugin is required if you don't have a reliable small-n datapoint.
  - Dataset Size (K) vs. Per-Query Budget (n): Variance scales inversely with K. It is often better to test more queries (K) with fewer attempts (n) than fewer queries with many attempts, provided n is sufficient to characterize the distribution tail.
- **Failure signatures:**
  - Goodness-of-Fit Failure: If the data is bimodal (many θ ≈ 0 and θ ≈ 1), the Beta assumption fails. Check Section C.2.1 for chi-squared test results.
  - Early Saturation: If ASR@1 is already near 100%, the scaling law is numerically unstable (log of near-zero).
- **First 3 experiments:**
  1. Validation on Synthetic Data: Generate synthetic success counts from a known Beta(0.4, 4) distribution. Run the MLE fitter to verify it recovers the ground truth α and β with varying K and n.
  2. Budget Allocation Sweep: Fix a total budget (e.g., 10,000 queries). Sweep the ratio of n (attempts per query) vs K (number of queries) to find the "sweet spot" for minimizing estimation variance.
  3. Target Extrapolation: Using only n=50 samples per query from the provided HarmBench data, predict ASR@1000. Compare against the ground truth (calculated from full n=10000 data) to validate the 86% error reduction claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the SABER framework be extended to handle non-binary judge outputs, such as continuous severity scores or multi-class assessments?
- **Basis in paper:** [explicit] Section D states the formulation assumes a binary judge output and suggests replacing the Bernoulli-Beta hierarchy with a categorical likelihood and Dirichlet prior.
- **Why unresolved:** The current mathematical derivation relies on the Bernoulli-Beta conjugacy, which does not directly support continuous or ordinal safety metrics without theoretical modification.
- **What evidence would resolve it:** A derivation of the scaling law using a Dirichlet prior and experiments showing accurate extrapolation on datasets with graded severity labels.

### Open Question 2
- **Question:** What causes the systematic underestimation bias in SABER, and can the estimator be recalibrated to correct it?
- **Basis in paper:** [explicit] Section D notes that empirical errors tend to come from underestimating ASR@N and calls for investigating the sources of this bias.
- **Why unresolved:** The paper demonstrates the bias empirically but does not analyze whether it stems from the Beta distribution assumption, the asymptotic approximation, or data sparsity.
- **What evidence would resolve it:** A theoretical error decomposition identifying the specific term causing the bias, followed by a modified estimator that reduces the MAE specifically for high-risk queries.

### Open Question 3
- **Question:** Does SABER maintain predictive accuracy when applied to multimodal jailbreak tasks and state-of-the-art frontier models?
- **Basis in paper:** [explicit] Section D lists "Broader task coverage" as a limitation, noting the current evaluation is restricted to textual jailbreaks and excludes frontier models.
- **Why unresolved:** The scaling behavior (governed by the α parameter) may differ significantly in multimodal settings or against stronger safety alignments.
- **What evidence would resolve it:** Evaluation results on multimodal benchmarks (e.g., VLSafe) and models like GPT-5 showing comparable Mean Absolute Error (MAE) reduction to the text-only results.

## Limitations
- Beta Distribution Assumption: 3 out of 12 (attacker, victim, judge) triplets fail goodness-of-fit tests, questioning the universal validity of the core statistical model
- Sample Budget Sensitivity: Performance critically depends on having reliable small-budget measurements, with no systematic analysis for minimum viable measurement budgets
- Evaluation Scope: Results validated only on HarmBench with specific configurations, limiting generalizability to other safety domains and adaptive adversaries

## Confidence
- **Beta Distribution Assumption** (Confidence: Medium): Core theoretical framework shows promise but empirical violations in 25% of test cases suggest limitations
- **Sample Budget Sensitivity** (Confidence: High): Strong empirical evidence supports n=100 as effective, but sensitivity analysis to smaller budgets remains incomplete
- **Evaluation Scope** (Confidence: Low): Limited to one benchmark and specific configurations, with untested generalizability to other domains and stronger models

## Next Checks
1. **Distribution Assumption Validation**: Apply chi-squared goodness-of-fit tests across all (attacker, victim, judge) triplets in the full HarmBench dataset to quantify the prevalence and impact of Beta assumption violations on prediction accuracy.

2. **Budget Sensitivity Analysis**: Systematically evaluate SABER's prediction accuracy across different small-budget settings (n=20, 50, 100, 200) to identify the minimum viable measurement budget that maintains the claimed 86% MAE reduction over baseline.

3. **Generalization Study**: Test SABER on at least two additional safety domains (e.g., misinformation generation, biased content) with different model architectures to assess whether the observed risk scaling patterns and prediction accuracy transfer beyond the original experimental setup.