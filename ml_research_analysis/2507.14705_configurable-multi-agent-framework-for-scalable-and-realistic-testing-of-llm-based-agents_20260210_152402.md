---
ver: rpa2
title: Configurable multi-agent framework for scalable and realistic testing of llm-based
  agents
arxiv_id: '2507.14705'
source_url: https://arxiv.org/abs/2507.14705
tags:
- testing
- test
- human
- agent
- tone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neo is a configurable multi-agent framework for scalable and realistic
  testing of LLM-based agents. It uses a probabilistic state model to simulate human-like,
  multi-turn conversations with dynamic variation in dialogue flow, tone, and intent.
---

# Configurable multi-agent framework for scalable and realistic testing of llm-based agents

## Quick Facts
- arXiv ID: 2507.14705
- Source URL: https://arxiv.org/abs/2507.14705
- Reference count: 4
- Primary result: 10-12× higher throughput than human testers with 3.3% break rate in adversarial testing

## Executive Summary
Neo is a configurable multi-agent framework that simulates human-like, multi-turn conversations to test LLM-based agents at scale. Using a probabilistic state model with configurable dialogue flow, intent, and tone, Neo generates diverse test inputs that adapt dynamically based on evaluation feedback. When applied to a production Seller Financial Assistant chatbot, Neo achieved comparable break rates to human testers (3.3% vs 5.8%) while delivering 10-12× higher throughput and broader behavioral exploration than manually crafted scripts.

## Method Summary
Neo employs a three-agent architecture: a Question Generation Agent that creates test inputs using context from the Context Hub, an Evaluation Agent that provides binary success/failure feedback, and a Context Hub that stores static configuration and dynamic conversation state. The system uses a probabilistic state vector (flow, intent, tone, feedback) to sample test scenarios, enabling both security testing across five attack categories and realism testing with human-like dialogue variation. GPT-4o serves as the backend with temperature=0.7, and the framework operates through closed-loop feedback where evaluation results inform subsequent test generation.

## Key Results
- Break rate of 3.3% vs human testers' 5.8% in adversarial security testing
- 10-12× throughput improvement: 180 questions in 45 minutes vs 16 hours for humans
- 100% topic accuracy and 68% natural conversation continuity in realism testing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic state modeling enables diverse, human-like multi-turn dialogue generation that static benchmarks cannot achieve.
- Mechanism: A structured state vector S = ⟨F, I, T, FB⟩ encodes flow type (follow-up, switch, repeat), intent category (baseline, edge-case, adversarial, malicious), tone index (−10 to +10), and prior feedback. Each turn samples from configurable probability distributions over these dimensions, producing combinatorially large test spaces (Formula 2: N_total = n! × (|I| × |T|)^n).
- Core assumption: LLM-based agents exhibit context-sensitive failures that only emerge through varied multi-turn trajectories, not single-turn probes.
- Evidence anchors:
  - [abstract] "Test inputs are sampled from a probabilistic state model spanning dialogue flow, user intent and emotional tone, enabling diverse, human-like conversations that adapt after every turn."
  - [Section 3.2] Defines the State Vector and demonstrates that with n=3 turns, |I|=4 intents, |T|=3 tones, the upper bound reaches 10,368 unique session variants.
  - [corpus] MATRIX (arXiv:2508.19163) uses similar multi-agent simulation for clinical dialogue safety, suggesting cross-domain validity of state-driven simulation, though direct replication is limited.
- Break condition: If target agent failures are primarily single-turn (e.g., toxicity detection), the combinatorial advantage diminishes; static test cases may suffice.

### Mechanism 2
- Claim: Decoupling generation and evaluation into specialized agents with shared context enables modular, extensible testing pipelines.
- Mechanism: The Question Agent retrieves static (domain metadata, templates) and dynamic (conversation history, tone) context from the Context Hub to generate test inputs. The Evaluation Agent receives the full interaction and emits a binary success/failure signal, which updates the Context Hub for subsequent turns. This creates a closed-loop feedback system.
- Core assumption: Separation of concerns improves test quality compared to monolithic testers; specialized agents can be independently improved.
- Evidence anchors:
  - [abstract] "Neo couples a Question Generation Agent and an Evaluation Agent through a shared context-hub, allowing domain prompts, scenario controls and dynamic feedback to be composed modularly."
  - [Section 3.1] "The Context Hub serves as Neo's centralized memory and orchestration layer... enabling scalable, modular, and feedback-driven simulation across multiple testing cycles."
  - [corpus] Related multi-agent frameworks (AdaCoder, CREW-WILDFIRE) similarly decompose complex tasks, but none specifically target LLM testing—the modular testing application appears novel.
- Break condition: If evaluation requires domain expertise encoded in the generation phase (e.g., knowing what question was intended to probe), strict separation may introduce misalignment.

### Mechanism 3
- Claim: Feedback-driven state transitions approximate adaptive human tester behavior, increasing failure detection in subsequent turns.
- Mechanism: The Feedback signal (FB) from the Evaluation Agent modulates transition probabilities—e.g., increasing follow-up likelihood after success, triggering retries or topic shifts after failure. This mirrors how human testers probe harder when they sense vulnerability.
- Core assumption: Binary success/failure contains sufficient signal to guide productive exploration; failures cluster in certain conversational trajectories.
- Evidence anchors:
  - [Section 3.2] "A failure may increase the likelihood of repeating or rephrasing a question, whereas a success may result in a coherent follow-up or a shift in intent focus."
  - [Section 4, Experiment 1] Both human and Neo testers triggered failures exclusively in the "Mixed Attack" category (7/120 human, 4/120 Neo), suggesting certain intent combinations are systematically more failure-prone.
  - [corpus] No direct corpus comparison for feedback-driven LLM testing; this appears underexplored in prior work.
- Break condition: If failures are uniformly distributed across states, adaptive feedback provides no advantage over random exploration; coarse binary feedback may miss nuanced degradation signals.

## Foundational Learning

- Concept: Probabilistic User Simulation
  - Why needed here: Neo's core innovation is replacing static test scripts with sampled dialogue trajectories. Understanding how statistical user models work (e.g., agenda-based, schema-based) clarifies why configurable probabilities matter.
  - Quick check question: Can you explain why increasing follow-up probability from 0.2 to 0.7 changes tree depth but not necessarily failure rate?

- Concept: LLM Agent Evaluation Paradigms
  - Why needed here: The Evaluation Agent currently uses binary classification. Understanding the spectrum (task completion, factual grounding, safety, coherence) helps contextualize the paper's "coarse-grained" limitation.
  - Quick check question: What evaluation dimensions would you add beyond success/failure for a financial chatbot?

- Concept: Combinatorial Testing Theory
  - Why needed here: Formula (2) claims combinatorial coverage advantages. Understanding t-way interaction testing helps assess whether Neo truly explores interaction space or just surface variation.
  - Quick check question: With 4 intents and 3 tones over 5 turns, are all intent-tone pairs guaranteed to co-occur? Why or why not?

## Architecture Onboarding

- Component map:
  - Question Agent -> Context Hub -> Evaluation Agent -> Context Hub -> Target Agent
  - State Controller updates Context Hub after each evaluation

- Critical path: Configure test goal → Initialize state vector → Question Agent generates input → Target Agent responds → Evaluation Agent classifies → Update Context Hub → Sample next state → Repeat

- Design tradeoffs:
  - **Binary vs. multi-dimensional evaluation**: Current implementation is coarse; extensible design anticipates richer feedback at cost of complexity
  - **Configurability vs. reproducibility**: Probabilistic sampling enables diversity but reduces exact reproducibility; seed control is essential
  - **Single-threaded vs. parallel**: Current experiments are single-threaded; batch testing requires orchestration layer not yet described

- Failure signatures:
  - **Tone misalignment**: 55% accuracy in mid-range tones (−5 to +5); extreme tones work better
  - **Abrupt transitions**: 32% of multi-turn sessions lack contextual continuity in follow-ups
  - **Coverage vs. depth**: Randomized follow-up produces shallow trees; depth requires explicit configuration

- First 3 experiments:
  1. **Baseline parity test**: Run Neo with randomized config on your target agent; compare break rate and topic coverage against 2–3 human testers over 50 questions each. Measure time-to-completion.
  2. **Flow probability sweep**: Vary follow-up probability (0.2, 0.5, 0.7) while holding intent/tone constant; analyze tree depth distribution and failure clustering per depth level.
  3. **Feedback ablation**: Disable feedback-driven transitions (use fixed probabilities); compare failure detection rate and session diversity against full feedback loop over 100+ sessions.

## Open Questions the Paper Calls Out

- Question: Does incorporating multi-dimensional feedback (e.g., factual grounding, tone alignment) into the Evaluation Agent improve the detection of subtle policy violations compared to the current binary success/failure classification?
  - Basis in paper: [explicit] The conclusion states the intent to "extend the Evaluation Agent beyond binary success/failure classification by incorporating multi-dimensional assessments."
  - Why unresolved: The current framework utilizes a coarse-grained evaluation mechanism, limiting the granularity of failure analysis.
  - What evidence would resolve it: A comparative study measuring the break-rate and failure categorization accuracy of the enhanced Evaluation Agent against the binary baseline.

- Question: To what extent can fine-tuning the state model on anonymized production logs narrow the behavioral gap between Neo and real human users?
  - Basis in paper: [explicit] The authors propose to "incorporate anonymized real-world interaction logs to fine-tune Neo’s generation strategies... thereby narrowing the gap between simulated and actual user behavior."
  - Why unresolved: The current probabilistic state model relies on manual configuration and randomization rather than data-driven calibration.
  - What evidence would resolve it: A/B testing showing statistically significant improvements in interaction realism scores when using log-finetuned state transitions versus the default probabilistic configuration.

- Question: Can specific prompt engineering or state-tracking mechanisms resolve the observed difficulty in simulating mid-range emotional tones (-5 to +5), which currently lack clarity 45% of the time?
  - Basis in paper: [inferred] The results section notes that while extreme tones were generated successfully, "questions in the mid-range tone spectrum... often lacked emotional clarity," suggesting a need for improved modeling.
  - Why unresolved: The paper identifies the limitation but does not propose or test a specific architectural change to fix mid-range tone expression.
  - What evidence would resolve it: Empirical results demonstrating >90% accuracy in human evaluations of mid-range tone prompts after implementing the proposed fixes.

## Limitations
- Binary evaluation signal is coarse-grained and may miss nuanced degradation in agent performance
- No human baseline comparison for realism-focused testing scenario
- Missing exact probabilistic transition matrices needed for exact reproduction

## Confidence
- **High**: Multi-agent framework architecture with clear separation of concerns
- **Medium**: Comparative performance claims (3.3% vs 5.8% break rate, 10-12× throughput)
- **Low**: Claims about uncovering edge-case failures and broader behavioral exploration without direct empirical comparison

## Next Checks
1. Implement the missing probabilistic transition matrix and conduct sensitivity analysis on transition probabilities
2. Design and run human baseline experiment for realism-focused testing scenario
3. Extend Evaluation Agent to use multi-dimensional scoring and measure impact on failure detection