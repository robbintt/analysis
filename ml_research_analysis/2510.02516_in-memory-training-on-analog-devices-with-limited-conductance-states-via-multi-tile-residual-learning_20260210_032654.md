---
ver: rpa2
title: In-memory Training on Analog Devices with Limited Conductance States via Multi-tile
  Residual Learning
arxiv_id: '2510.02516'
source_url: https://arxiv.org/abs/2510.02516
tags:
- analog
- training
- tile
- equation
- wmin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training deep neural networks
  on analog in-memory computing hardware with limited conductance states (typically
  4-5 bits), which causes convergence failure due to quantization noise. The authors
  propose a multi-tile residual learning framework that represents high-precision
  weights using multiple low-precision analog tiles with geometric scaling factors,
  where each tile learns to compensate the residual error left by preceding tiles.
---

# In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning

## Quick Facts
- arXiv ID: 2510.02516
- Source URL: https://arxiv.org/abs/2510.02516
- Reference count: 40
- This paper proposes a multi-tile residual learning framework for analog in-memory computing that enables training of deep neural networks with limited conductance states, achieving accuracy comparable to mixed-precision digital approaches

## Executive Summary
This paper addresses the critical challenge of training deep neural networks on analog in-memory computing hardware with limited conductance states (typically 4-5 bits), which causes convergence failure due to quantization noise. The authors propose a multi-tile residual learning framework that represents high-precision weights using multiple low-precision analog tiles with geometric scaling factors. Each tile learns to compensate the residual error left by preceding tiles, with theoretical analysis showing that the optimality gap shrinks exponentially with the number of tiles. The approach achieves consistent accuracy improvements over state-of-the-art in-memory analog training methods across multiple datasets including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100.

## Method Summary
The proposed multi-tile residual learning framework represents high-precision weights using multiple low-precision analog tiles, where each tile compensates the residual error from previous tiles. The approach employs geometric scaling factors between tiles, enabling the representation of high-precision weights while maintaining the benefits of analog in-memory computing. Theoretical analysis demonstrates that the optimality gap between the multi-tile approach and ideal high-precision training shrinks exponentially with the number of tiles. The method is particularly effective for training on analog devices with limited conductance states (4-5 bits), where traditional approaches fail to converge due to quantization noise. Experiments show the framework achieves accuracy comparable to mixed-precision digital approaches while incurring only moderate hardware overhead.

## Key Results
- Multi-tile residual learning framework achieves consistent accuracy improvements over state-of-the-art in-memory analog training methods
- Theoretical analysis shows optimality gap shrinks exponentially with the number of tiles
- Method achieves comparable accuracy to mixed-precision digital approaches with moderate hardware overhead
- Particularly effective for larger models where other methods fail to maintain accuracy

## Why This Works (Mechanism)
The multi-tile residual learning framework works by decomposing high-precision weights into multiple low-precision components, where each tile compensates for the quantization error of the previous tiles. The geometric scaling between tiles enables efficient representation of high-precision values using limited conductance states. The residual learning approach ensures that each subsequent tile learns to correct the approximation errors from earlier tiles, leading to exponential convergence toward the optimal solution. This mechanism effectively mitigates the quantization noise that typically causes convergence failure in analog in-memory training with limited conductance states.

## Foundational Learning
- **Analog in-memory computing**: Understanding of how computations are performed directly in memory arrays using conductance states, which provides energy efficiency but suffers from limited precision
  - Why needed: Fundamental to understanding the problem space and why quantization noise occurs
  - Quick check: Can you explain the difference between digital and analog in-memory computing?

- **Quantization noise in training**: Knowledge of how weight quantization affects gradient updates and convergence during training
  - Why needed: Essential for understanding why limited conductance states cause training failure
  - Quick check: What happens to training convergence when weights are quantized to very few bits?

- **Geometric scaling in multi-tile systems**: Understanding how different tiles can represent different orders of magnitude in weight values
  - Why needed: Key to understanding how high-precision weights can be represented using multiple low-precision tiles
  - Quick check: How does geometric scaling between tiles enable representation of high-precision values?

## Architecture Onboarding
- **Component map**: Input data -> Multiple analog tiles (with geometric scaling) -> Residual error compensation -> Output prediction
- **Critical path**: Data flows through each tile sequentially, with each tile computing based on the residual from the previous tile
- **Design tradeoffs**: Higher accuracy vs. increased hardware overhead and complexity; more tiles provide better accuracy but require more area and power
- **Failure signatures**: Training divergence when quantization noise exceeds compensation capability; performance degradation with insufficient number of tiles
- **3 first experiments**: 1) Train on MNIST with varying numbers of tiles to observe accuracy vs. tile count relationship, 2) Compare convergence speed with single-tile vs. multi-tile approaches, 3) Test robustness to conductance state variations across different hardware instances

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on relatively small-scale image classification tasks (MNIST, Fashion-MNIST, CIFAR-10/100), leaving uncertainty about performance on larger, more complex models and datasets
- Hardware overhead characterization is qualitative rather than quantitative, lacking detailed analysis of area, power, and latency implications
- Assumes geometric scaling factors between tiles may not always hold in practice due to device-to-device variations in analog hardware

## Confidence
- Theoretical framework and exponential convergence analysis: High
- Experimental results on tested datasets: Medium (limited to specific tasks)
- Hardware overhead characterization: Low (lacks quantitative analysis)
- Scalability to larger models: Medium (extrapolated from small-scale experiments)

## Next Checks
1. Evaluate the approach on larger-scale vision models (e.g., ResNet-50, ViT) and more complex datasets (e.g., ImageNet) to assess scalability limits
2. Conduct detailed hardware characterization including area, power, and latency measurements for the multi-tile implementation compared to single-tile alternatives
3. Test the framework's robustness to device-to-device variations and non-idealities in analog conductance states through extensive Monte Carlo simulations