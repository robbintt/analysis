---
ver: rpa2
title: 'From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement
  Learning'
arxiv_id: '2507.12815'
source_url: https://arxiv.org/abs/2507.12815
tags:
- learning
- reward
- reload
- offline
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReLOAD addresses the challenge of offline RL without reward annotations
  by distilling intrinsic rewards from expert transitions using Random Network Distillation
  (RND). The method trains a predictor network to match embeddings from a fixed target
  network using expert data, with prediction error serving as a reward signal for
  the broader dataset.
---

# From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.12815
- Source URL: https://arxiv.org/abs/2507.12815
- Reference count: 17
- Achieves competitive performance with traditional reward-based offline RL methods, matching or exceeding state-of-the-art baselines on 7 of 9 D4RL locomotion tasks

## Executive Summary
ReLOAD addresses the challenge of offline RL without reward annotations by distilling intrinsic rewards from expert transitions using Random Network Distillation (RND). The method trains a predictor network to match embeddings from a fixed target network using expert data, with prediction error serving as a reward signal for the broader dataset. This approach is conceptually simpler and more scalable than prior methods relying on optimal transport or latent space modeling. ReLOAD achieves performance competitive with traditional reward-based offline RL methods, matching or exceeding state-of-the-art baselines on 7 of 9 D4RL locomotion tasks. It also demonstrates strong results on sparse-reward tasks like AntMaze and Adroit, showing robustness to limited and noisy expert data while avoiding the computational overhead of complex alignment procedures.

## Method Summary
ReLOAD uses Random Network Distillation to create self-supervised rewards for offline RL. A fixed target network and trainable predictor network both map (state, next state) pairs to embeddings. The predictor is trained on expert transitions to minimize prediction error, then frozen. Prediction error on the full dataset serves as a reward signal - low error indicates expert-like transitions, high error indicates novelty. This reward is exponentially squashed to stabilize learning. The relabeled dataset is then used with an offline RL algorithm (IQL) to train policies. The method requires only expert demonstrations, not reward annotations, making it suitable for reward-free offline RL settings.

## Key Results
- Achieves 93.1 ± 1.9 normalized score on halfcheetah-medium-expert, matching ground-truth reward IQL baseline
- Outperforms prior state-of-the-art on 7 of 9 D4RL locomotion tasks
- Shows robust performance on sparse-reward tasks (AntMaze, Adroit) with limited expert data
- Requires only K=1 expert trajectory to achieve competitive results

## Why This Works (Mechanism)

### Mechanism 1: Embedding Discrepancy as Expert-Quality Proxy
Prediction error between a fixed target network and predictor network trained on expert transitions correlates inversely with transition quality. The predictor learns to approximate the target's embeddings for expert behavior, producing low error on expert-like transitions and high error on novel or suboptimal ones. This inverts RND's traditional exploration use-case from novelty-seeking to expert-alignment.

### Mechanism 2: Support Containment Enables Distributional Separation
Theoretical guarantee that expected prediction error over the broader dataset exceeds error over expert distribution when expert support is a proper subset. This formalizes the intuition that expert-like transitions are "familiar" to the predictor trained only on expert data.

### Mechanism 3: Exponential Reward Squashing Stabilizes Policy Learning
Transforming raw negative MSE rewards via exponential squashing bounds reward variance and improves convergence. This prevents large prediction errors from dominating the reward landscape, which is critical for mixed-quality datasets.

## Foundational Learning

- **Random Network Distillation (RND)**: Needed to understand how prediction error serves as reward signal. Quick check: Can you explain why fixing the target network's weights prevents the predictor from simply memorizing all transitions?

- **Offline RL Constraints**: Needed to understand why policy improvement relies entirely on inferred reward signal. Quick check: What failure mode does IQL's expectile regression address, and why does this matter for reward-distilled datasets?

- **Imitation Learning vs. Inverse RL**: Needed to understand how ReLOAD bypasses explicit reward function optimization. Quick check: How does ReLOAD's approach differ from adversarial imitation learning in terms of training stability and computational overhead?

## Architecture Onboarding

- **Component map**: (s, s') -> Target network f_ψ (fixed) -> Predictor network g_θ (trainable on expert) -> MSE reward -> Exponential squashing -> Offline RL (IQL)

- **Critical path**: 1) Extract expert trajectory, 2) Train predictor on expert transitions, 3) Freeze networks, 4) Annotate full dataset with reward, 5) Apply squashing, 6) Train IQL on relabeled dataset

- **Design tradeoffs**: Predictor architecture depth (2-layer MLP with 256 hidden), expert data quantity (K=1 often sufficient), action-label independence (operates on state transitions only)

- **Failure signatures**: Near-zero rewards across dataset (over-capacity or over-training), high variance in policy performance (check squashing parameters), expert trajectory not representative (misaligned reward signal)

- **First 3 experiments**: 1) Sanity check on embedding separation using t-SNE visualization, 2) Ablation on predictor training epochs to find convergence point, 3) Cross-task reward transfer from one task to another dataset variant

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Relies on Assumption 2 in Theorem 1 requiring expert support to be proper subset of broader dataset
- Method requires access to at least one high-quality expert trajectory for predictor training
- Depends on task-specific reward squashing parameters requiring hyperparameter tuning

## Confidence
- **High**: Empirical performance claims on D4RL benchmarks; predictor training procedure and reward squashing mechanics
- **Medium**: Theoretical guarantee in Theorem 1 (depends on strict inequality assumption); ablation study conclusions
- **Low**: Claims about computational efficiency compared to optimal transport methods (no runtime measurements provided)

## Next Checks
1. Test ReLOAD's performance when expert data contains multiple distinct behavioral modes to assess whether the predictor can effectively discriminate between them
2. Evaluate whether the predictor's random initialization consistently produces meaningful embedding separation across different random seeds and network architectures
3. Compare ReLOAD's sample efficiency and final performance against IRL methods like GAIL when ground-truth rewards are available during evaluation