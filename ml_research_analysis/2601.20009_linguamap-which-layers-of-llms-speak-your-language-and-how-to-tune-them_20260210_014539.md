---
ver: rpa2
title: 'LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?'
arxiv_id: '2601.20009'
source_url: https://arxiv.org/abs/2601.20009
tags:
- language
- task
- layers
- consistency
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LinguaMap identifies and characterizes two failure modes in multilingual
  large language models: the multilingual transfer bottleneck (correct language, incorrect
  task response) and the language consistency bottleneck (correct task response, wrong
  language). The paper introduces a four-scenario evaluation protocol spanning MMLU,
  MGSM, and XQuAD benchmarks and uses logit lens analysis and semantic similarity
  evaluation to probe internal representations.'
---

# LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?

## Quick Facts
- **arXiv ID**: 2601.20009
- **Source URL**: https://arxiv.org/abs/2601.20009
- **Reference count**: 34
- **Primary result**: Selective fine-tuning of only the final layers responsible for language control achieves >98% language consistency across six languages while fine-tuning only 3–5% of parameters, without sacrificing task accuracy.

## Executive Summary
LinguaMap identifies two key failure modes in multilingual LLMs: the multilingual transfer bottleneck (correct language, incorrect task response) and the language consistency bottleneck (correct task response, wrong language). The paper introduces a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks and uses logit lens analysis and semantic similarity evaluation to probe internal representations. Results reveal a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, selective fine-tuning of only the final layers responsible for language control is introduced. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98% language consistency across six languages while fine-tuning only 3–5% of parameters, without sacrificing task accuracy, and is nearly identical to full-scope fine-tuning but uses a fraction of the computational resources.

## Method Summary
LinguaMap employs a four-scenario evaluation protocol to characterize multilingual LLM failures, including monolingual, code-switched, English-distractor, and bilingual-answer prompting. Using logit lens analysis and semantic similarity evaluation across layers, the paper identifies a three-phase internal structure: early layers align inputs into a shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Selective fine-tuning targets only the final layers responsible for language control, with optimal layer selection determined through ablation studies. The method uses masked SFT loss, AdamW optimizer with OneCycleLR, and trains on CoT-augmented MMLU Business subset data. Evaluation measures language consistency (via LangDetect) and task accuracy (F1 for XQuAD), targeting >98% consistency with preserved accuracy.

## Key Results
- Selective fine-tuning of last 1-2 layers achieves >98% language consistency across six languages while fine-tuning only 3–5% of parameters
- Task accuracy is preserved post-fine-tuning, matching full-scope fine-tuning performance
- Three-phase internal structure identified: semantic alignment (early), reasoning (middle), language output (late)
- Random layer selection causes severe task performance collapse (66.6%→0.13% on Qwen MGSM)

## Why This Works (Mechanism)
The method works by isolating language control to the final layers of the model, where language-specific token generation occurs. Early layers perform semantic alignment across languages, creating a shared representation space. Middle layers handle task reasoning independently of language. Late layers then map these shared representations to language-specific outputs. By fine-tuning only these late layers, the model learns to maintain task reasoning capabilities while improving language consistency. The approach exploits the modular structure of multilingual LLMs, where language control is localized rather than distributed throughout the network.

## Foundational Learning
**Multilingual Transfer Bottleneck** - When models produce correct language but wrong answers due to interference between language and task reasoning. *Why needed*: Understanding this failure mode is crucial for targeted intervention. *Quick check*: Compare model outputs under monolingual vs code-switched prompts to identify when language switching causes task errors.

**Language Consistency Bottleneck** - When models produce correct answers but in the wrong language. *Why needed*: This reveals that language generation is separable from task reasoning. *Quick check*: Evaluate whether models maintain task accuracy when language is controlled externally.

**Logit Lens Analysis** - Technique for examining layer-wise token probabilities to understand internal representation changes. *Why needed*: Provides empirical evidence for the three-phase structure without requiring full forward passes. *Quick check*: Plot language probability trajectories across layers to verify semantic alignment → reasoning → language output phases.

**Cross-lingual Semantic Similarity** - Measures how similar hidden states are across languages at different layers. *Why needed*: Quantifies the degree of semantic alignment and reasoning preservation. *Why needed*: Values above 0.95-0.99 indicate reasoning is preserved during fine-tuning. *Quick check*: Compute cosine similarity of middle-layer hidden states across languages before and after fine-tuning.

## Architecture Onboarding

**Component Map**: Input tokens → Early layers (semantic alignment) -> Middle layers (reasoning) -> Late layers (language output) -> Output tokens

**Critical Path**: The reasoning path through middle layers must remain intact; only the language output path in late layers should be modified.

**Design Tradeoffs**: Selective fine-tuning trades parameter efficiency (3-5% vs full model) against potential reduced flexibility in handling complex multilingual scenarios.

**Failure Signatures**: Random layer selection causes task collapse; over-tuning middle layers degrades reasoning; under-tuning fails to improve consistency.

**First Experiments**:
1. Baseline evaluation on pretrained models using four-prompt protocol to establish failure modes
2. Layer-wise logit lens analysis to identify language control boundaries
3. Cross-lingual similarity analysis in middle layers to verify reasoning preservation

## Open Questions the Paper Calls Out

**Open Question 1**: How can explicit reasoning-level disambiguation strategies be integrated with late-layer selective fine-tuning to improve task accuracy under high-interference conditions (e.g., English Distractor prompting)? The current method isolates language control but fails to resolve semantic interference when the model must reject misleading English alternatives in the input. A combined fine-tuning approach that targets both reasoning layers and output layers, demonstrating restored task accuracy in distractor-heavy scenarios, would resolve this.

**Open Question 2**: Does the observed three-phase structure (semantic alignment, reasoning, language output) and the efficacy of last-layer tuning hold for open-ended generative tasks? The analysis is conducted on MMLU (multiple-choice), MGSM (math), and XQuAD (extractive QA), which have constrained output formats. It is unclear if language control is strictly localized to final layers in tasks requiring long, unconstrained generation where content planning might be more intertwined with language selection. Applying the logit lens and selective fine-tuning protocol to long-form generation benchmarks would resolve this.

**Open Question 3**: Is the optimal layer selection for selective fine-tuning static (last 1-2 layers) or does it vary significantly based on model depth and architecture type? The ablation study finds optimal settings via grid search for specific models but does not provide a theoretical basis for predicting this across architectures. The paper validates the method on Qwen-3-32B and Bloom-7.1B but does not establish if the "3–5%" parameter rule or specific layer indices generalize to other model families. A systematic study correlating model depth/architecture with the optimal layer window would resolve this.

## Limitations
- Methodical generality is limited to tested model families (Qwen, Bloom, Llama) without theoretical proof of universal applicability
- Language coverage gaps exist, with performance on low-resource or structurally distant languages unverified
- CoT generation process opacity creates uncertainty about trace quality and consistency across target languages

## Confidence
**High confidence**: Four-scenario evaluation protocol is well-specified and reproducible; Layer-wise logit lens analysis methodology is clearly described; Selective fine-tuning approach with parameter efficiency claims is verifiable; Hardware and training configuration details are complete.

**Medium confidence**: Three-phase internal structure hypothesis is observationally supported but not theoretically proven; Optimal layer selection is empirically determined but selection criteria remain informal; Task accuracy preservation claims are supported but dependent on quality of CoT generation.

**Low confidence**: Cross-lingual generalizability beyond tested models and languages; Long-term stability of selective fine-tuning effects across epochs; Performance on languages with different typological features.

## Next Checks
1. **Layer selection sensitivity analysis** - Systematically vary the number of tuned layers (k=1,2,3,4) on held-out validation data to confirm the claimed optimal points and establish formal selection criteria based on language probability thresholds and similarity metrics.

2. **Cross-lingual transfer evaluation** - Test the selective fine-tuning method on language pairs from different families (e.g., Korean, Swahili, Hindi) to verify the 98% consistency claim generalizes beyond Indo-European and East Asian languages in the original evaluation.

3. **Middle-layer reasoning preservation** - After fine-tuning, measure cross-lingual cosine similarity of hidden states in middle layers across the six languages. Confirm similarity remains above 0.95-0.99 to verify reasoning capabilities are preserved, not just task outputs.