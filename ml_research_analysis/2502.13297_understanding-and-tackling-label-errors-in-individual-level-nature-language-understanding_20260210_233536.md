---
ver: rpa2
title: Understanding and Tackling Label Errors in Individual-Level Nature Language
  Understanding
arxiv_id: '2502.13297'
source_url: https://arxiv.org/abs/2502.13297
tags:
- stance
- tweets
- dataset
- label
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies systematic label errors in individual-level
  natural language understanding (NLU) tasks due to reliance on text-level annotation
  guidelines. The authors propose incorporating multiple posts from the same user
  to capture consistent individual perspectives, re-annotate stance detection and
  topic-based sentiment analysis datasets, and find error rates as high as 31.7% and
  23.3%, respectively.
---

# Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding

## Quick Facts
- arXiv ID: 2502.13297
- Source URL: https://arxiv.org/abs/2502.13297
- Authors: Yunpeng Xiao; Youpeng Zhao; Kai Shu
- Reference count: 14
- Primary result: Individual-level NLU annotation using multiple user posts reduces label errors from 31.7% to <3% in stance detection

## Executive Summary
This paper identifies a critical flaw in current natural language understanding datasets: text-level annotation guidelines systematically introduce label errors when applied to individual-level tasks like stance detection and topic-based sentiment analysis. The authors demonstrate that these errors arise because annotators lack context about individual users' perspectives when judging a single post in isolation. By incorporating multiple posts from the same user, the study shows that LLMs can achieve over 87% accuracy on corrected datasets, significantly outperforming their performance on original labels. The work establishes that individual factors must be considered in NLU dataset creation to ensure label quality and model performance.

## Method Summary
The authors propose a novel approach to individual-level NLU annotation that leverages multiple posts from the same user rather than relying on single-post text-level guidelines. They first use LLMs (GPT-4o, Llama3-70B, PHI-4) to identify potential label errors in existing stance detection and topic-based sentiment analysis datasets by comparing single-post predictions against multi-post context. The LLM-identified errors are then presented to human annotators who make final judgments using expanded context from multiple user posts. The re-annotation process corrects datasets where initial error rates reached 31.7% for stance detection and 23.3% for sentiment analysis. Performance evaluation shows that LLMs trained and tested on these corrected labels achieve superior accuracy compared to their performance on original datasets.

## Key Results
- Label error rates as high as 31.7% (stance detection) and 23.3% (sentiment analysis) identified in original datasets
- LLMs achieve >87% accuracy on corrected datasets, outperforming performance on original labels
- Ablation studies confirm multi-post context significantly improves inference accuracy
- Text-level annotation guidelines systematically fail to capture individual perspectives

## Why This Works (Mechanism)
The mechanism works because individual users exhibit consistent perspectives across their posts, which single-post annotations cannot capture. When annotators judge a post in isolation using text-level guidelines, they miss the broader context of the user's stance or sentiment patterns. By providing multiple posts from the same user, the system enables annotators (both human and LLM) to identify the user's typical perspective, reducing errors caused by ambiguous or sarcastic language that might be misinterpreted without context. This approach aligns the annotation process with how individuals actually express opinions online - through multiple, contextually related posts rather than isolated statements.

## Foundational Learning
- Individual vs. Text-Level Annotation: Individual-level tasks require understanding user context beyond single posts. Needed because users express consistent perspectives across multiple posts. Quick check: Compare annotation consistency when using single vs. multiple posts from same user.
- Context Expansion Methods: Keyword-based retrieval to find related user posts. Needed to build comprehensive user perspective profiles. Quick check: Measure annotation accuracy improvement with varying numbers of retrieved posts.
- LLM-Assisted Error Detection: Using LLMs to identify potential label errors before human re-annotation. Needed to scale error detection across large datasets. Quick check: Compare LLM error detection accuracy against random sampling.

## Architecture Onboarding

Component Map: Raw Data -> LLM Error Detection -> Human Re-annotation -> Corrected Dataset -> Model Training -> Evaluation

Critical Path: Raw Data → LLM Error Detection → Human Re-annotation → Corrected Dataset
- Data Collection: Social media posts with original labels
- Error Detection: LLM analysis using multi-post context
- Human Validation: Annotators review LLM-flagged errors
- Dataset Correction: Integration of validated corrections

Design Tradeoffs: Using LLMs for initial error detection trades computational cost for scalability, while human validation ensures quality but limits throughput. The multi-post approach requires more data per annotation but significantly improves accuracy.

Failure Signatures: High disagreement between LLM predictions and original labels indicates potential systematic annotation errors. Low improvement after multi-post context suggests the task may not benefit from individual-level annotation.

First Experiments:
1. Apply multi-post context to a small subset of original data and measure annotation agreement rates
2. Test different numbers of posts per user (3, 5, 10) to find optimal context size
3. Compare LLM error detection accuracy across different model sizes (7B vs 70B parameters)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can heterogeneous user metadata (retweets, likes, follows, profiles) be effectively modeled to enhance individual-level NLU, given the variance in user social media habits?
- Basis in paper: The authors identify utilizing non-text user information as a future direction, noting that current methods only leverage posted text.
- Why unresolved: User metadata is sparse and inconsistent (e.g., some users lack profiles) compared to the relative homogeneity of text posts.
- What evidence would resolve it: A model architecture that successfully integrates sparse behavioral data with text to improve stance detection accuracy.

### Open Question 2
- Question: What information retrieval methods can enable individual-level annotation for tasks lacking specific targets, such as sarcasm detection?
- Basis in paper: The authors state their keyword-based expansion is limited to tasks with targets (stance/sentiment) and cannot retrieve context for target-less tasks like sarcasm.
- Why unresolved: The proposed data expansion relies heavily on keywords associated with a specific topic, which do not exist for general sarcasm.
- What evidence would resolve it: A retrieval mechanism capable of gathering relevant contextual posts for sarcasm without relying on explicit topic keywords.

### Open Question 3
- Question: Does the performance of LLMs on corrected labels hold consistently when scaling the dataset to include a larger volume of annotation samples?
- Basis in paper: The authors acknowledge their annotations are "relatively small" and list building a larger dataset to verify robustness as a future step.
- Why unresolved: It is unclear if the >87% accuracy achieved by LLMs on the small re-annotated set is robust against the noise likely present in a much larger corpus.
- What evidence would resolve it: Evaluation results from a scaled-up re-annotation effort showing similar LLM performance metrics.

## Limitations
- Error rates and approach effectiveness validated only on stance detection and sentiment analysis tasks
- Reliance on LLMs for both error detection and validation may introduce model-specific biases
- Small re-annotation sample size raises questions about scalability and generalizability

## Confidence
- High confidence in core finding that individual context is crucial for accurate NLU annotation
- Medium confidence in generalizability across different NLU tasks and domains
- Medium confidence in effectiveness of multi-post approach across all real-world scenarios

## Next Checks
1. Conduct a cross-domain validation study using the multi-post approach on at least three additional NLU tasks (e.g., emotion detection, intent classification, and semantic textual similarity) to assess generalizability.

2. Implement a human-only validation study where trained annotators, provided with multi-post context, independently re-annotate a subset of the original datasets to verify if the LLM-detected errors align with human judgment.

3. Test the approach with smaller language models (7B-13B parameters) to evaluate whether the benefits of multi-post context persist across different model sizes and resource constraints.