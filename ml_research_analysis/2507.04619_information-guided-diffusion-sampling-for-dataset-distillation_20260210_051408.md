---
ver: rpa2
title: Information-Guided Diffusion Sampling for Dataset Distillation
arxiv_id: '2507.04619'
source_url: https://arxiv.org/abs/2507.04619
tags:
- dataset
- information
- distillation
- diffusion
- inproc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of dataset distillation in low
  images-per-class (IPC) settings, where diffusion models struggle to generate diverse
  samples. The authors propose an information-theoretic approach, identifying prototype
  information (I(X;Y)) and contextual information (H(X|Y)) as crucial components for
  effective dataset distillation.
---

# Information-Guided Diffusion Sampling for Dataset Distillation

## Quick Facts
- arXiv ID: 2507.04619
- Source URL: https://arxiv.org/abs/2507.04619
- Reference count: 27
- Achieves 41.9% accuracy on ImageWoof with IPC-10, outperforming existing methods by 4.9%

## Executive Summary
This paper addresses the challenge of dataset distillation in low Images-Per-Class (IPC) settings, where diffusion models struggle to generate diverse samples. The authors propose an information-theoretic approach, identifying prototype information (I(X;Y)) and contextual information (H(X|Y)) as crucial components for effective dataset distillation. They introduce a variational estimator (VE) to tightly lower-bound these quantities and propose maximizing I(X;Y) + βH(X|Y) during the diffusion model sampling process, where β is IPC-dependent. Their method, information-guided diffusion sampling (IGDS), significantly outperforms existing methods, particularly in low-IPC regimes, on Tiny ImageNet and ImageNet subsets.

## Method Summary
The approach combines a pre-trained DDPM with a variational estimator (VE) trained on the target dataset. The VE, built on MoCo-style contrastive learning, estimates I(X;Y) and H(X|Y) via an encoder-classifier architecture. During diffusion sampling, gradients from the VE guide the generation process to maximize the weighted sum of prototype and contextual information. The IPC-dependent parameter β controls the balance between these information components, with lower values prioritizing discriminative features in low-IPC settings.

## Key Results
- Achieves 41.9% accuracy on ImageWoof with IPC-10, compared to 37.0% for the next best method
- Outperforms existing methods by 2-4% absolute accuracy on Tiny ImageNet and ImageNet subsets
- Shows monotonic relationship between β and IPC, validating the information-theoretic approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing dataset information into prototype information I(X;Y) and contextual information H(X|Y) enables targeted preservation of discriminative features versus intra-class diversity.
- **Mechanism:** The entropy decomposition H(X) = I(X;Y) + H(X|Y) separates label-predictive information (prototype) from class-conditional variability (contextual). By maximizing I(X;Y) + βH(X|Y) during sampling with IPC-dependent β, low-IPC settings prioritize prototype information (smaller β), while high-IPC settings allow more contextual richness (larger β).
- **Core assumption:** The optimal balance between prototype and contextual information scales monotonically with IPC—this relationship is empirically observed but not theoretically proven.
- **Evidence anchors:**
  - [abstract] "Observing that the required contextual information scales with IPC, we propose maximizing I(X;Y) + βH(X|Y)"
  - [section 5.2, Figure 2] Shows H(X|Y) vs. model accuracy curves for different IPC settings; optimal H(X|Y) increases with IPC.
  - [corpus] Weak direct support; related work "Taming Diffusion for Dataset Distillation" addresses representativeness but uses different formulation.
- **Break condition:** If β selection does not correlate with IPC in new datasets, or if prototype/contextual decomposition fails to capture task-relevant information (e.g., fine-grained recognition), the mechanism may not generalize.

### Mechanism 2
- **Claim:** A variational estimator (VE) trained to maximize I(X;X̂) provides tractable, tight lower bounds on intractable I(X;Y) and H(X|Y).
- **Mechanism:** The encoder fθ maps X→X̂; classifier gψ maps X̂→Ŷ. Per Propositions 2 and 4, maximizing I(X;X̂) simultaneously minimizes I(Y;X|X̂) and H(X|X̂,Y), yielding tight bounds. Training objective JVE combines instance discrimination (MoCo-style contrastive loss) with KL divergence: JVE = -E log[exp(⟨x̂ᵢ,x̂ᵢ⟩/τ)/Σₖ exp(⟨x̂ᵢ,x̂ₖ⟩/τ)] + λE KL(σ(X̂)||Q_Y).
- **Core assumption:** The classifier gψ is Bayes-optimal for equality in Eq. 6; assumption: encoder features have zero mean for Proposition 3 injectivity.
- **Evidence anchors:**
  - [section 4.1.1, Eq. 9] Î(X;Y) = H(Ŷ) + E_Y log P_Ŷ|Y
  - [section 4.1.2, Eq. 15] Ĥ(X|Y) = E_{X,Y} KL(σ(X̂)||Q_Y)
  - [corpus] No direct corpus evidence for this specific variational approach to I(X;Y) and H(X|Y).
- **Break condition:** If encoder collapses to trivial representations (mode collapse), or if Q_Y estimates are inaccurate for small/long-tailed classes, bounds become loose.

### Mechanism 3
- **Claim:** Gradient-based guidance using the VE during diffusion sampling steers generated samples toward higher I(X;Y) + βH(X|Y).
- **Mechanism:** During DDPM reverse sampling (Algorithm 2), at each step t, the intermediate sample x'_{t-1} is passed through frozen VE to compute L_IGDS = E log P_{ŷ|y} + H(ŷ) + β KL(Ĥ_{x_{t-1}}||Q_{t-1}). The gradient ∇_{x_t} L_IGDS updates x_{t-1} = x'_{t-1} + η∇_{x_t} L_IGDS, iteratively refining samples.
- **Core assumption:** VE gradients transfer meaningfully to diffusion latent space; assumption: 250 steps with η tuning suffices for convergence.
- **Evidence anchors:**
  - [section 4.2, Algorithm 2] Pseudo-code showing L_IGDS computation and gradient update.
  - [section 5.1] "We set the temperature to τ = 0.07 and run the diffusion process for 250 steps in all experiments."
  - [corpus] "GVD: Guiding Video Diffusion Model" uses similar guidance concept but for video; "D³-Predictor" addresses noise misalignment in diffusion for prediction tasks.
- **Break condition:** If VE and diffusion model are misaligned (e.g., different training distributions), gradients may be uninformative or harmful.

## Foundational Learning

- **Concept: Mutual information I(X;Y)**
  - **Why needed here:** Core theoretical quantity for prototype information; understanding MI decomposition is essential to grasp why maximizing I(X;Y) preserves discriminative features.
  - **Quick check question:** Given two random variables X and Y, what does I(X;Y) = 0 imply about their relationship?

- **Concept: Variational lower bounds**
  - **Why needed here:** Direct computation of I(X;Y) and H(X|Y) is intractable; VE provides tractable bounds via encoder-classifier optimization.
  - **Quick check question:** What property must a lower bound satisfy to be "tight," and why does maximizing I(X;X̂) help achieve tightness?

- **Concept: Diffusion model reverse sampling (DDPM)**
  - **Why needed here:** IGDS operates by modifying the reverse diffusion process; understanding x_t → x_{t-1} transitions is prerequisite to integrating guidance.
  - **Quick check question:** In DDPM, what role does the noise prediction network s_θ(x_t, t) play in computing x_{t-1} from x_t?

## Architecture Onboarding

- **Component map:** Pre-trained DDPM (frozen) -> DDPM noise prediction network -> Intermediate sample x_t -> Variational Estimator (frozen) -> Information quantities -> Gradient update -> x_{t-1} -> Next diffusion step

- **Critical path:**
  1. Train VE: Run Algorithm 1 on target dataset D to obtain fθ and Q_y statistics.
  2. Train classifier: Freeze fθ, train gψ with CE loss on encoder features.
  3. Run IGDS: For each class y and desired IPC n, run Algorithm 2 with β calibrated per Figure 5.
  4. Evaluate: Train downstream model (ConvNet/ResNet) on distilled dataset, report validation accuracy.

- **Design tradeoffs:**
  - β selection: Low β (≈0) for IPC-1 maximizes prototype, sacrifices diversity; high β (≈0.5) for IPC-50+ increases contextual richness. Empirical calibration required (Figure 5).
  - Temperature τ: Set to 0.07; lower τ sharpens contrastive distribution but risks gradient instability.
  - Diffusion steps: 250 steps balances quality and compute; fewer steps may under-utilize guidance.
  - Compute overhead: IGDS requires ~30% more time than MiniMax (Table 6) due to VE forward/backward passes.

- **Failure signatures:**
  - Low diversity in generated samples (similar backgrounds, poses): Indicates β too low or VE encoder collapsed.
  - Misclassified samples in distilled set: Prototype information not maximized; check classifier accuracy on encoder features.
  - Accuracy comparable to random subset (baseline): VE not trained properly; verify Q_y statistics and contrastive loss convergence.
  - Cross-architecture performance drop: Distilled set overfits to VE architecture; validate with multiple test architectures.

- **First 3 experiments:**
  1. **Reproduce β-IPC calibration on Tiny ImageNet:** Train ConvNet-4 on subsets with controlled H(X|Y) (Appendix A weighted sampling); plot accuracy vs. β for IPC∈{1,10,50}; verify monotonic trend matches Figure 2.
  2. **Ablate VE components:** Train VE with λ=0 (MoCo only) vs. λ>0 (full VE); measure Î(X;Y) and Ĥ(X|Y) tightness; confirm λ>0 improves downstream accuracy by ≥2% on IPC-10 ImageWoof.
  3. **Cross-architecture validation:** Generate distilled ImageWoof IPC-10 with IGDS; train ResNet-18, ResNetAP-10, ConvNet-6, and MobileNet-V2 on distilled set; report accuracy variance across architectures per Table 4 protocol.

## Open Questions the Paper Calls Out

- **Open Question 1:** Is the reliance on a pretrained diffusion model as the prior distribution optimal for dataset distillation?
  - **Basis in paper:** [explicit] The conclusion states that while using a pretrained diffusion model is intuitive, "its optimality for dataset distillation remains unverified."
  - **Why unresolved:** The authors utilize existing priors without proving if they are the theoretical best fit for the specific distillation objective.
  - **What evidence would resolve it:** A theoretical analysis or comparative study showing whether training a prior specifically for distillation yields better information retention than using standard pretrained generative models.

- **Open Question 2:** Can the computational cost of the Information-Guided Diffusion Sampling (IGDS) process be reduced?
  - **Basis in paper:** [explicit] The conclusion identifies the increased computational cost resulting from backpropagating gradients through both the classifier and encoder during sampling as a limitation.
  - **Why unresolved:** The requirement to guide the diffusion process using gradients from the variational estimator adds significant overhead compared to standard sampling.
  - **What evidence would resolve it:** Development of an efficient gradient approximation method or a "feedback-free" guidance mechanism that maintains high performance without iterative backpropagation.

- **Open Question 3:** Can the optimal weighting parameter β be theoretically derived rather than empirically selected?
  - **Basis in paper:** [inferred] Section 5.2 and Figure 5 show β is currently determined through empirical observation of validation accuracy for different IPC settings.
  - **Why unresolved:** The paper demonstrates a correlation between IPC and optimal β, but does not provide a closed-form theoretical relationship.
  - **What evidence would resolve it:** Deriving a theoretical function for β based on IPC and dataset entropy that matches or improves upon the empirical performance curves.

## Limitations

- The IPC-dependent β calibration requires extensive empirical tuning per dataset, and the monotonic relationship between β and contextual information may not hold for datasets with unusual class distributions or fine-grained tasks.
- The variational estimator's tightness depends critically on the classifier's Bayes-optimality assumption, which is difficult to verify in practice.
- Cross-architecture performance consistency shows some degradation, suggesting potential overfitting to the VE encoder architecture.

## Confidence

- **High confidence:** The information-theoretic decomposition H(X) = I(X;Y) + H(X|Y) and its empirical validation on IPC-IPC relationships (Figure 2).
- **Medium confidence:** The variational estimator provides tight bounds for I(X;Y) and H(X|Y) under the stated assumptions, supported by Proposition 2-4 but lacking extensive empirical bound verification.
- **Medium confidence:** The 2-4% absolute accuracy improvements over state-of-the-art methods, though impressive, come from relatively controlled experimental conditions on specific datasets.

## Next Checks

1. Test β-IPC calibration on CIFAR-100 with its long-tailed class distribution to verify the monotonic relationship holds beyond the studied datasets.
2. Measure VE bound tightness by computing empirical I(X;Y) and H(X|Y) on validation subsets and comparing against variational estimates.
3. Evaluate cross-architecture consistency by testing distilled ImageWoof IPC-10 with ViT-B/16 and Swin-Tiny, comparing performance variance to ConvNet-4 baseline.