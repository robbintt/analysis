---
ver: rpa2
title: 'Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study'
arxiv_id: '2510.12835'
source_url: https://arxiv.org/abs/2510.12835
tags:
- annotation
- guidelines
- disease
- category
- annotations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using annotation guidelines to instruct LLM
  annotators, addressing the challenge of adapting guidelines designed for humans
  to work with LLMs. The authors propose a "moderation-oriented guideline repurposing"
  method that involves incorporating guidelines into prompts and iteratively refining
  them through an LLM moderation process.
---

# Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study

## Quick Facts
- **arXiv ID**: 2510.12835
- **Source URL**: https://arxiv.org/abs/2510.12835
- **Reference count**: 28
- **Primary result**: Guideline repurposing improved F1-scores from 0.36 to 0.78 in biomedical entity annotation

## Executive Summary
This paper explores using annotation guidelines to instruct LLM annotators, addressing the challenge of adapting guidelines designed for humans to work with LLMs. The authors propose a "moderation-oriented guideline repurposing" method that involves incorporating guidelines into prompts and iteratively refining them through an LLM moderation process. Using the NCBI Disease Corpus, they show that incorporating guidelines improves annotation accuracy, with F1-scores increasing from 0.36 to 0.58. The human-in-the-loop approach further improves performance to 0.78 F1-score. The study demonstrates the potential of this workflow for scalable and cost-effective annotation guideline refinement and automated annotation, while highlighting challenges in scope definition and category matching.

## Method Summary
The authors developed a workflow that repurposes human annotation guidelines for LLM annotators through iterative refinement. The process involves (1) extracting guidelines from structured text, (2) incorporating them into LLM prompts with specific annotation instructions, (3) having an LLM perform the annotation task, (4) using another LLM as a moderator to analyze annotation errors and identify guideline deficiencies, and (5) iteratively updating the guidelines based on moderator feedback. The method was tested on the NCBI Disease Corpus, where disease mentions needed to be identified in biomedical text. The authors compared different approaches including zero-shot annotation, guideline-incorporated annotation, and human-in-the-loop moderation, measuring performance using strict and soft matching metrics.

## Key Results
- F1-score improved from 0.36 (zero-shot) to 0.58 (guidelines incorporated) to 0.78 (human-in-the-loop)
- Guideline-incorporated approach reduced false positives by 40% compared to zero-shot
- Human-in-the-loop approach significantly improved precision and recall over automated moderation alone
- Soft match F1 (0.62) was consistently higher than strict match F1 (0.58) across conditions, indicating boundary selection challenges

## Why This Works (Mechanism)
The method works by bridging the gap between human-designed annotation guidelines and LLM capabilities through iterative refinement. When LLMs receive textual guidelines without modification, they struggle with ambiguous terminology and scope definitions. By having an LLM moderator analyze errors and suggest guideline improvements, the system creates a feedback loop that progressively aligns guidelines with LLM interpretation patterns. This approach leverages LLMs' ability to process natural language instructions while addressing their tendency to misinterpret nuanced human guidelines through systematic error analysis and refinement.

## Foundational Learning
- **Annotation guideline structure** - Understanding how human guidelines are organized (rules, examples, exceptions) is critical because LLMs need this information converted into actionable instructions. Quick check: Can you identify the three main components of typical annotation guidelines?
- **LLM prompt engineering** - The way guidelines are incorporated into prompts significantly affects performance. Quick check: What prompt format yielded the highest F1-score in the experiments?
- **Error analysis methodology** - Systematic identification of error patterns (scope mismatches, boundary issues, category confusion) drives guideline refinement. Quick check: Which error type showed the most improvement after human-in-the-loop moderation?
- **Evaluation metrics for annotation** - Understanding strict vs. soft matching reveals different aspects of annotation quality. Quick check: What does the gap between strict and soft F1-scores indicate about LLM performance?
- **Iterative workflow design** - The cyclical nature of annotation, moderation, and guideline updating enables progressive improvement. Quick check: How many iterations were typically needed to achieve stable performance?
- **Domain-specific terminology** - Biomedical annotation requires understanding specialized ontologies (MeSH, OMIM) that affect scope definition. Quick check: Which ambiguous term caused the most confusion in the experiments?

## Architecture Onboarding

**Component Map**: Human Guidelines -> Guideline Extraction -> LLM Annotator -> LLM Moderator -> Error Analysis -> Updated Guidelines -> (Repeat)

**Critical Path**: Guideline Extraction → LLM Annotator → LLM Moderator → Updated Guidelines (forms the iterative improvement loop)

**Design Tradeoffs**: 
- Fully automated workflow trades speed for accuracy (F1: 0.58 vs 0.78)
- Human-in-the-loop improves quality but reduces scalability
- Guideline complexity must balance comprehensiveness with LLM processing limitations

**Failure Signatures**:
- Persistent scope mismatches indicate inadequate ontology reference
- Boundary selection errors suggest guidelines lack explicit boundary rules
- Category confusion reveals terminology ambiguity not addressed in guidelines

**First Experiments**:
1. Test guideline incorporation on a simple named entity recognition task with clear scope boundaries
2. Compare performance with and without ontology/terminology references during annotation
3. Measure impact of explicit boundary rule examples in guidelines on span selection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can enabling LLM annotators to directly reference original ontology/terminology sources (e.g., MeSH, OMIM) alongside annotation guidelines improve annotation accuracy and reduce scope mismatches?
- Basis in paper: [explicit] The authors state as future work: "allow the LLM to specifically use the original ontology/terminology as a reference for annotation while following the annotation guidelines."
- Why unresolved: The current approach only provides textual guidelines without direct access to the underlying terminologies that define scope, leading to errors with ambiguous terms like "adenomatous polyposis coli" (gene vs. disease).
- What evidence would resolve it: Experiments comparing LLM annotation performance with and without ontology/terminology access, measuring reductions in scope mismatch errors.

### Open Question 2
- Question: How can the LLM moderation process be improved to reduce or eliminate human involvement while achieving higher annotation quality?
- Basis in paper: [explicit] Future work includes: "improve the moderation process so that we can improve the accuracy of categorization and reduce human involvement as we gradually move towards fully automating the workflow."
- Why unresolved: The human-in-the-loop approach showed noticeable improvements over fully automated moderation, indicating the LLM moderator's analysis and guideline updates are not yet sufficiently accurate.
- What evidence would resolve it: Iterative improvements to the moderator prompting strategy that achieve comparable or better F1-scores to human-in-the-loop without human intervention.

### Open Question 3
- Question: Does the moderation-oriented guideline repurposing approach generalize effectively to other annotation tasks and domains beyond biomedical disease mention recognition?
- Basis in paper: [explicit] The authors note they plan to "expand this study to other datasets and domains to validate its practicality."
- Why unresolved: This study is a case study on a single corpus with relatively structured guidelines; the method's effectiveness with more complex guidelines or different annotation paradigms is unknown.
- What evidence would resolve it: Application of the same workflow to diverse annotation tasks (e.g., relation extraction, event detection, sentiment annotation) across multiple domains showing consistent improvements.

### Open Question 4
- Question: What modifications to the guideline representation or prompting strategy can improve LLM annotators' precision in text span boundary selection?
- Basis in paper: [inferred] The large gap between Strict Match (0.58) and Soft Match (0.62) F1-scores in the human-in-the-loop condition indicates that "precisely regulating the selection of the text spans to be annotated as disease names is still challenging."
- Why unresolved: Even with repurposed guidelines, LLM annotators struggle with exact boundary matching, suggesting guidelines do not adequately convey boundary rules to LLMs.
- What evidence would resolve it: Studies testing alternative guideline formulations (e.g., explicit boundary rules, negative examples, structural formats) that show reduced strict/soft match gaps.

## Limitations
- Single dataset limitation restricts generalizability to other annotation domains
- Human-in-the-loop component introduces potential subjectivity in moderation decisions
- Performance improvements may not scale linearly with more complex guideline structures

## Confidence
- **High**: The observed improvement in annotation accuracy through guideline incorporation (F1: 0.36 → 0.58) and the effectiveness of the human-in-the-loop moderation approach (F1: 0.58 → 0.78)
- **Medium**: The claim that this workflow is broadly applicable to "any annotation guidelines" given the single case study context
- **Medium**: The assertion that this method is "cost-effective" without detailed cost analysis compared to traditional annotation methods

## Next Checks
1. Test the moderation-oriented guideline repurposing workflow across multiple annotation domains (e.g., sentiment analysis, named entity recognition for different entity types) to assess generalizability
2. Conduct systematic error analysis to identify specific types of guideline misinterpretations by LLMs and develop targeted prompt engineering strategies
3. Perform controlled experiments comparing the proposed method against alternative LLM annotation approaches (zero-shot, few-shot, chain-of-thought) on the same tasks to establish relative effectiveness