---
ver: rpa2
title: 'The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of
  Self-Reflection in RL-Trained LLMs'
arxiv_id: '2601.01580'
source_url: https://arxiv.org/abs/2601.01580
tags:
- sample
- gradient
- policy
- reward
- decision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Two-Stage Decision-Sampling Hypothesis provides a mechanistic
  framework explaining why reinforcement learning induces self-reflection capabilities
  in large language models while supervised fine-tuning fails. By decomposing the
  model's policy into sampling and decision components, the analysis reveals that
  surrogate rewards exhibit balanced gradient attribution while KL penalties and SFT
  objectives exhibit unbalanced gradient attribution.
---

# The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs

## Quick Facts
- **arXiv ID:** 2601.01580
- **Source URL:** https://arxiv.org/abs/2601.01580
- **Reference count:** 40
- **Primary result:** RL induces self-reflection by balancing gradient attribution across sampling and decision components, while SFT's token-level KL penalties create unbalanced gradients that suppress decision-making.

## Executive Summary
This paper presents a mechanistic framework explaining why reinforcement learning induces self-reflection capabilities in large language models while supervised fine-tuning fails. The authors decompose the model's policy into sampling (answer generation) and decision (error detection/resolution) components, showing that surrogate reward objectives in RL enable balanced gradient attribution that learns both tasks effectively. In contrast, SFT objectives with KL penalties exhibit unbalanced gradient attribution that heavily regularizes sampling while leaving decision-making under-optimized. Empirical validation on arithmetic reasoning tasks confirms RL's superior generalization stems primarily from improved decision quality (ability to reject incorrect answers) rather than better initial sampling, with RL models maintaining discriminative ability to reject wrong answers where SFT models collapse to near-zero.

## Method Summary
The study trains Qwen2.5-7B-Instruct on multi-digit multiplication tasks, comparing base, SFT (with and without reflection trajectories), and RL (GRPO) conditions. SFT models are trained with 20,000 examples of 4×5 and 5×4 multiplication, while RL uses the same data with surrogate rewards. Evaluation tests generalization on out-of-distribution problems (3×3 through 3×9). The analysis calibrates three parameters: sampling accuracy (p_s), probability of stopping given correct (p_{d|C}), and probability of resampling given incorrect (p_{d|W}). The two-stage hypothesis is validated by comparing these calibrated parameters across training methods and their correlation with final accuracy on OOD tasks.

## Key Results
- RL models maintain p_{d|W} at 40-60% on OOD tasks, while SFT models' p_{d|W} collapses to near zero
- SFT models exhibit the "echoing effect" - high first-try accuracy but inability to correct errors
- Generalization improvements in RL stem primarily from enhanced decision quality (π_d) rather than sampling capabilities
- Surrogate rewards enable balanced gradient attribution across sampling and decision components

## Why This Works (Mechanism)

### Mechanism 1: Balanced Gradient Attribution via Surrogate Rewards
The surrogate reward objective (e.g., GRPO) distributes learning gradients symmetrically across generation and verification capabilities. The trajectory-level scalar advantage (A_i) weights the log-probabilities of both sampling (π_sample) and decision (π_d) actions equally, allowing a unified network to coherently learn both tasks. The advantage being a single scalar reflecting total outcome means both sub-policies rely on the same sufficient statistic, enabling balanced gradient flow.

### Mechanism 2: Unbalanced Gradient Attribution via Length-Weighted Penalties
Standard KL penalties and SFT objectives induce asymmetry that suppresses decision-making (π_d). KL penalties are calculated token-level, so sampling actions sum penalties over sequence length L_k (magnitude O(L_k)) while decision actions contribute single-token penalties (O(1)). This scale separation creates "gradient drag" that heavily regularizes the generator while leaving the verifier under-optimized.

### Mechanism 3: Decision-Policy Driven Generalization
RL generalizes better than SFT on OOD tasks primarily because it learns to reject incorrect answers. RL training maintains p_{d|W} at 40-60% even on difficult OOD tasks, while SFT models' p_{d|W} collapses to near zero, causing them to stick with wrong answers even when the sampling policy (π_sample) could generate correct ones in a retry.

## Foundational Learning

- **Concept: Policy Gradient Theorem**
  - Why needed here: The paper relies on decomposing the policy gradient to prove that gradients flow differently to sampling vs. decision logits
  - Quick check question: How does the gradient of the expected return relate to the Q-function and the log-policy?

- **Concept: Advantage Functions (A)**
  - Why needed here: The "Balanced Attribution" hinges on the advantage A_i being a trajectory-level scalar that does not decompose into action-specific components
  - Quick check question: Why is the advantage defined as Q(s,a) - V(s), and how does centering the reward help balance gradients?

- **Concept: KL Divergence Regularization**
  - Why needed here: Understanding why "Unbalanced Attribution" arises requires knowing that KL is a sum of log-ratios over tokens, making it inherently length-dependent
  - Quick check question: How does the token-level calculation of KL divergence differ from a sequence-level constraint?

## Architecture Onboarding

- **Component map:** Unified LLM Backbone -> Reward Calculator -> Advantage Estimator -> Reference Model
- **Critical path:**
  1. Model generates trajectory τ (interleaved reasoning + decision tokens)
  2. Calculate scalar advantage A_i based on final answer correctness
  3. Compute token-level KL divergence against π_ref
  4. Backpropagate combined loss: Verify that A_i scales gradients for decision tokens while KL dominates sampling token gradients

- **Design tradeoffs:**
  - GRPO vs. PPO: The paper abstracts away clipping/trust regions for analytical tractability; real implementations must handle variance
  - Calibration: Operationalizing π_d requires proxy tokens (e.g., "Wait," "Alternatively"), which are noisy estimators of internal decision states

- **Failure signatures:**
  - SFT Echo Chamber: High first-try accuracy but p_{d|W} ≈ 0. The model never corrects itself
  - Gradient Mismatch: If KL weight is too high, π_sample freezes (stays near π_ref) and π_d receives no signal because the model cannot explore new answers to reward

- **First 3 experiments:**
  1. Calibration Check: Estimate parameters (p_s, p_{d|C}, p_{d|W}) for an SFT model vs. an RL model on a held-out set. Verify that SFT has p_{d|W} ≈ 0
  2. Gradient Attribution Ablation: Train two small models: one with pure surrogate reward (no KL) and one with heavy KL. Compare the ratio of gradient norms |∇θ_sample|/|∇θ_decision|
  3. Intervention Study: Force an SFT model to resample (override decision). If accuracy improves significantly, it confirms the decision policy (π_d) was the bottleneck, not the sampling policy

## Open Questions the Paper Calls Out
None

## Limitations
- The hypothesis relies on idealized gradient attribution analysis that assumes clean decomposition of sampling and decision components despite shared weights
- Empirical validation focuses on narrow arithmetic reasoning tasks, potentially limiting generalizability
- Reflection marker detection depends on manually defined heuristics that may not capture all self-correction behaviors
- Analysis assumes two-stage decomposition captures essential mechanics, potentially overlooking other RL-induced mechanisms

## Confidence

- **High Confidence:** The gradient attribution asymmetry mechanism (Mechanism 2) - mathematical derivation is sound and directly supported by token-level vs. trajectory-level calculation differences
- **Medium Confidence:** The balanced attribution via surrogate rewards (Mechanism 1) - theoretical framework is clear but empirical validation focuses primarily on outcomes rather than directly measuring gradient distributions
- **Medium Confidence:** The decision-policy driven generalization claim (Mechanism 3) - supported by calibration results but relies on specific experimental conditions and marker-based detection of decision behavior

## Next Checks

1. **Gradient Attribution Measurement:** Directly measure and compare gradient norms flowing to sampling vs. decision parameters during training for SFT with KL penalties versus RL with surrogate rewards, using ablation studies with varying KL weights.

2. **Cross-Domain Generalization:** Test the two-stage hypothesis on non-arithmetic reasoning tasks (e.g., commonsense reasoning or code generation) to verify that the decision-policy collapse pattern (p_{d|W} → 0) persists across domains and marker types.

3. **Decision Boundary Robustness:** Evaluate how sensitive the calibration of p_{d|C} and p_{d|W} is to different definitions of reflection markers, and test whether learned decision boundaries align with human-annotated error detection rather than superficial token patterns.