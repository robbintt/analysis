---
ver: rpa2
title: Synthetic Function Demonstrations Improve Generation in Low-Resource Programming
  Languages
arxiv_id: '2503.18760'
source_url: https://arxiv.org/abs/2503.18760
tags:
- function
- data
- excel
- table
- match
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of low-resource programming language
  (LRPL) generation, exemplified by Excel formulas, where limited training data results
  in poor LLM performance. The authors propose a synthetic data finetuning approach
  that leverages public documentation of language specifications and library functions
  to efficiently generate textbook-quality training examples.
---

# Synthetic Function Demonstrations Improve Generation in Low-Resource Programming Languages

## Quick Facts
- arXiv ID: 2503.18760
- Source URL: https://arxiv.org/abs/2503.18760
- Reference count: 23
- Base models achieve 15-20% execution match on Excel formula generation; synthetic data finetuning improves performance to 28-30% execution match

## Executive Summary
This paper addresses the challenge of generating Excel formulas in low-resource settings where training data is scarce due to privacy concerns. The authors propose a synthetic data generation approach that leverages function documentation and sampled data tables to create textbook-quality training examples. A teacher model (GPT-4o) generates step-by-step demonstrations for each Excel function, which are validated against Python solutions before finetuning student models. The method significantly improves formula generation accuracy, particularly for code-specialized models, with execution match improvements of over 10 percentage points compared to base models and RAG approaches.

## Method Summary
The approach involves four stages: (1) curating a function library from real usage data and documentation, (2) sampling relevant data tables from existing QA datasets, (3) generating synthetic problems with step-by-step explanations using a teacher model, and (4) validating generations by executing both Excel and Python solutions. Student models are finetuned on validated synthetic data using LoRA with early stopping. The method focuses on demonstrating individual function arguments systematically to create comprehensive coverage of the function library while maintaining execution correctness through cross-language validation.

## Key Results
- Synthetic data finetuning improves execution match from 15.34% to 28.64% for Qwen2.5-coder 3B (vs 20.81% for non-coder Qwen2.5 3B)
- Validated synthetic data achieves 54.93% execution match vs 52.11% with unvalidated data (equal sample sizes)
- Models show improved table reading and formula syntax accuracy while preserving planning and function selection skills
- The method reduces table indexing errors from 35 to 13 but increases single-function solutions from 44.4% to 64.3%

## Why This Works (Mechanism)

### Mechanism 1: Documentation-Grounded Synthetic Curriculum
Generating textbook-quality demonstrations from public function documentation can substitute for scarce real-world training pairs in LRPLs. A teacher model receives function documentation plus sampled data tables, then produces natural language queries with step-by-step explanations and executable formulas, systematically demonstrating each argument of a function.

### Mechanism 2: HRPL Pretraining Enables Faster LRPL Adaptation
Code-specialized models pretrained on high-resource languages like Python gain larger improvements from synthetic LRPL finetuning than non-code-specialized models because abstract programming concepts (control flow, variable binding, function composition) learned from HRPLs transfer to the new syntax.

### Mechanism 3: Cross-Language Execution Validation Filters Noise
Validating synthetic Excel solutions against parallel Python solutions improves downstream performance by filtering incorrect generations. For each synthetic tuple, generate a Python/Pandas solution to the same query and accept only samples where both executions produce matching outputs.

## Foundational Learning

- **Concept: Low-Resource Programming Language (LRPL)**
  - Why needed: The paper's core problem definition; Excel is LRPL not because it's unpopular, but because public training pairs are scarce due to enterprise privacy
  - Quick check: Why might a widely-used language still be "low-resource" for LLM training?

- **Concept: Execution Match (EM) vs Exact String Match**
  - Why needed: The evaluation metric accepts any formula that executes to the correct answer, rewarding functional correctness over syntactic imitation
  - Quick check: If `=SUM(A1:A5)` and `=A1+A2+A3+A4+A5` both return 15, which should be preferred and why?

- **Concept: Knowledge Transfer / Negative Transfer**
  - Why needed: Explains why code-pretrained models adapt faster—they leverage transferred reasoning skills
  - Quick check: What happens if pretrained knowledge conflicts with target LRPL syntax (e.g., different operator precedence)?

## Architecture Onboarding

- **Component map:**
Documentation (100 functions) -> Real Usage Analysis -> Function Library Curation -> Table Sampling (WikiTQ train split, 10 tables/function) -> Teacher Model (GPT-4o): generates (query, explanation, formula) tuples -> Validator: (1) Excel execution check, (2) Python parallel generation + match -> Student Model Finetuning (LoRA, 1-6 epochs, early stopping)

- **Critical path:** Table-function matching quality -> Teacher generation correctness -> Validation pass rate -> Finetuning data volume. The validation step accepts ~50% of generations.

- **Design tradeoffs:**
  - Single-function focus ensures deep coverage but biases models toward simple formulas (64.3% single-function after FT vs. 44.4% baseline)
  - Python validation improves quality but adds computational cost and may reject valid Excel-specific idioms
  - Synthetic-only training avoids distribution shift from toy documentation examples but cannot teach complex multi-function composition patterns

- **Failure signatures:**
  - Over-simplification bias: Models generate single-function solutions even when multi-function solutions are needed
  - Table indexing errors: Reduced dramatically (35→13) but not eliminated
  - RAG degradation: Providing all 100 function signatures hurts performance—context dilution effect

- **First 3 experiments:**
  1. Validation ablation: Replicate the validated vs. unvalidated comparison on your target LRPL
  2. Error category shift analysis: Manually annotate ~100 generations pre/post finetuning using the error taxonomy
  3. Function complexity scaling: Generate multi-function synthetic examples and measure whether the single-function bias decreases

## Open Questions the Paper Calls Out

### Open Question 1
How can synthetic data pipelines be designed to mitigate the bias toward generating simple, single-function programs while still efficiently covering a function library? The current synthetic data strategy focuses on demonstrating individual function arguments one by one, which results in students generating single-function solutions significantly more often than base models (64.3% vs 44.4%), potentially limiting expressivity.

### Open Question 2
Does the effectiveness of documentation-based synthetic curricula generalize to other Low-Resource Programming Languages (LRPLs) with different structural constraints than Excel? The authors state they "demonstrated our methods on a single LRPL" and assume findings "should apply to any low-resource programming domain," but they do not verify this.

### Open Question 3
Would methods for correcting or "fixing" synthetic generations yield better downstream performance than the current approach of pruning unverified samples? The authors note they "only investigated methods for pruning bad synthetic generations, and not any methods for fixing those bad generations" due to computational costs.

## Limitations

- Documentation coverage gap: The method assumes comprehensive function documentation exists for the target LRPL, which may not hold for proprietary languages or DSLs
- Validation domain shift: Cross-language validation may fail for Excel-specific behaviors (1-indexed references, date handling) that differ from pandas implementations
- Single-function bias: The generation strategy produces predominantly single-function solutions, limiting complex formula composition

## Confidence

- **High Confidence**: Claims about execution match improvements (54.93% validated data, +13.3 points for coder models vs +5.9 for non-coders) are well-supported by ablation studies
- **Medium Confidence**: The single-function bias explanation is plausible but could reflect artifacts of the synthetic data generation process
- **Low Confidence**: Claims about documentation-grounded synthetic curriculum substituting for real-world training data assume the teacher model can accurately translate documentation to executable examples

## Next Checks

1. **Multi-Function Generation Test**: Generate synthetic examples containing 2-3 nested function calls and measure whether the single-function bias persists or whether syntax accuracy degrades with complexity

2. **Domain Shift Validation**: Test whether Excel validation against pandas solutions introduces false rejections by identifying Excel-specific behaviors (1-indexing, cell reference syntax) and measuring rejection rates for these cases

3. **Documentation Completeness Analysis**: Systematically test the impact of incomplete function documentation by removing 20-30% of function signatures and measuring performance degradation to quantify the documentation coverage requirement