---
ver: rpa2
title: 'Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes:
  A Proof-of-Concept Study'
arxiv_id: '2511.08600'
source_url: https://arxiv.org/abs/2511.08600
tags:
- clinical
- language
- prompt
- system
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This proof-of-concept study demonstrates the technical feasibility
  of using retrieval-augmented generation (RAG) with engineered prompts to generate
  school-based speech-language pathology (SLP) vignettes. A multi-model system was
  developed integrating curated domain knowledge with prompt templates, supporting
  five commercial and open-source LLMs.
---

# Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study

## Quick Facts
- **arXiv ID:** 2511.08600
- **Source URL:** https://arxiv.org/abs/2511.08600
- **Reference count:** 40
- **Primary result:** RAG with engineered prompts generates valid JSON SLP vignettes with 100% structural completeness, commercial models achieving quality scores of 4.39-4.50 and open-source models 4.18-4.25.

## Executive Summary
This proof-of-concept study demonstrates the technical feasibility of using retrieval-augmented generation (RAG) with engineered prompts to generate school-based speech-language pathology (SLP) vignettes. A multi-model system was developed integrating curated domain knowledge with prompt templates, supporting five commercial and open-source LLMs. Across 35 validation cases spanning seven disorder types and grade levels, the system achieved 100% structural completeness with marginal quality differences between commercial (4.39-4.50) and open-source models (4.18-4.25). Integration of authoritative clinical guidelines enabled generation of content aligned with professional standards. The study establishes proof-of-concept for scalable, privacy-preserving generation of SLP simulation materials, though extensive expert validation remains necessary before educational or research implementation.

## Method Summary
The system retrieves the top k=10 relevant chunks from a curated knowledge base of 44 ASHA guidelines and developmental milestone documents, then injects them into structured prompts. A dual-prompt architecture uses comprehensive 493-line templates for commercial models and simplified 281-line templates for open-source models to ensure JSON structural compliance. The system supports OpenAI/Anthropic APIs and local Ollama instances (Llama 3.2, Qwen 2.5) with ChromaDB vector storage. Quality is assessed through automated evaluation using Claude Sonnet 4.5 on clinical appropriateness, consistency, and documentation standards.

## Key Results
- 100% structural completeness across all 35 validation cases spanning seven disorder types
- Commercial models achieved quality scores of 4.39-4.50 versus open-source models at 4.18-4.25
- Open-source models showed internal consistency gaps (3.0-3.3) for complex multi-domain disorders
- Local deployment enabled privacy-preserving generation while maintaining acceptable clinical quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** RAG grounds general-purpose LLMs in curated domain knowledge, reducing hallucinations and aligning outputs with professional standards.
- **Mechanism:** The system retrieves the top k=10 relevant chunks from a curated knowledge base (ASHA guidelines, developmental milestones) and injects them into the prompt context. This provides specific evidence (e.g., assessment criteria, goal formats) that the model likely lacks in its parametric memory.
- **Core assumption:** The retrieved context fits within the model's effective context window and overrides its pre-trained "general knowledge" biases.
- **Evidence anchors:** [abstract] "Integration of authoritative clinical guidelines enabled generation of content aligned with professional standards." [section 1.2] "RAG reduces hallucinations by anchoring generation in verified documents rather than parametric knowledge alone."

### Mechanism 2
- **Claim:** A dual-prompt architecture enforces JSON structural compliance across models with varying instruction-following capacities.
- **Mechanism:** The system uses a complex 493-line prompt for premium models and a simplified 281-line prompt for smaller open-source models (Llama 3.2, Qwen 2.5-7B). The simplified prompt reduces "cognitive load" to prevent smaller models from failing to close JSON structures.
- **Core assumption:** Premium models have better attention mechanisms for long instructions, while smaller models fail on complex, multi-step constraints.
- **Evidence anchors:** [abstract] "...achieved 100% structural completeness with marginal quality differences..." [section 2.4] "...smaller open-source models... struggled with extended multi-step instructions, producing incomplete JSON structures..."

### Mechanism 3
- **Claim:** Local deployment of open-source models preserves data privacy while maintaining "acceptable" clinical quality via RAG augmentation.
- **Mechanism:** By running models locally via Ollama and compensating for their smaller size with external knowledge retrieval (RAG), the system achieves quality scores (4.18-4.25) within range of commercial APIs (4.39-4.50).
- **Core assumption:** The drop in "internal consistency" observed in open-source models is an acceptable trade-off for keeping student/patient data on-premise.
- **Evidence anchors:** [abstract] "...open-source alternatives achieved acceptable performance, suggesting potential for privacy-preserving institutional deployment." [section 2.6.2] "Open-source models... deployed locally using Ollama... ensuring data privacy and offline operation."

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - **Why needed here:** General LLMs lack specific SLP clinical guidelines (ASHA) and IDEA regulations. RAG injects this specific context at inference time.
  - **Quick check question:** If the vector database is empty, will the system produce better or worse clinical advice?

- **Concept:** Structured Output / JSON Schema Enforcement
  - **Why needed here:** Educational vignettes must be machine-readable for downstream processing (LMS integration). The prompts explicitly demand valid JSON with specific fields (demographics, IEP goals).
  - **Quick check question:** Which model type (premium vs. open-source) requires a simplified prompt to reliably produce valid JSON?

- **Concept:** SMART IEP Goals
  - **Why needed here:** This is the domain-specific quality standard. The prompt engineering encodes the requirement for goals to be "Specific, Measurable, Achievable, Relevant, Time-bound."
  - **Quick check question:** Does the system rely on the LLM's pre-trained knowledge of SMART goals, or does it provide examples in the prompt?

## Architecture Onboarding

- **Component map:** User Input -> Query Embedding -> ChromaDB Retrieval (k=10) -> Prompt Engine (Dual-Template) -> LLM Interface -> JSON Output

- **Critical path:**
  1. User inputs disorder and grade
  2. System creates query embedding
  3. ChromaDB retrieves top 10 chunks
  4. System constructs prompt (Template + Retrieved Context + Constraints)
  5. LLM generates JSON case file
  6. UI displays parsed JSON and Excel download

- **Design tradeoffs:**
  - **Prompt Complexity:** Premium prompts are verbose to maximize quality; Free prompts are concise to ensure structural validity (JSON closure)
  - **Latency vs. Privacy:** Commercial APIs are faster and higher quality but send data off-premise. Local models ensure privacy but have lower "internal consistency" scores

- **Failure signatures:**
  - **Incomplete JSON:** Usually indicates the prompt was too complex for the selected model (specifically open-source)
  - **Internal Inconsistency (Score < 3.5):** Often seen in complex, multi-domain disorders (e.g., mixed receptive-expressive) with open-source models
  - **Missing Goal Numbers:** Session notes referencing "Goal 1" without the corresponding goal definition (observed in 66% of cases)

- **First 3 experiments:**
  1. **Prompt A/B Test:** Run the 493-line prompt against Llama 3.2. Verify if JSON parsing fails compared to the 281-line version
  2. **Retrieval Validation:** Generate a case for a rare disorder (e.g., "Voice Disorders") and manually inspect the retrieved chunks in {context} to ensure they are relevant and not generic
  3. **Consistency Check:** Generate 5 cases for "Mixed Receptive-Expressive Language" using a commercial model. Evaluate if the "Internal Consistency" score improves over the open-source baseline (3.0 -> 3.3+)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do AI-generated SLP vignettes achieve clinical accuracy and psychometric properties comparable to human-authored vignettes when evaluated by expert speech-language pathologists?
- **Basis in paper:** [explicit] "Comparative studies investigating whether expert-reviewed AI-generated cases achieve psychometric properties comparable to human-authored cases would establish their utility for assessment purposes."
- **Why unresolved:** The study used only automated quality assessment, not expert clinical review; authors explicitly state "the system has not undergone expert clinical validation by experienced school-based SLPs."
- **What evidence would resolve it:** Multi-reviewer expert evaluation with inter-rater reliability protocols comparing AI-generated vs. human-authored cases on clinical realism, developmental appropriateness, and diagnostic validity.

### Open Question 2
- **Question:** Does practice with RAG-generated clinical vignettes improve student clinical reasoning, diagnostic accuracy, or goal-writing skills compared to traditional instructional approaches?
- **Basis in paper:** [explicit] "Student pilot studies must investigate whether practice with AI-generated cases improves clinical reasoning skills, diagnostic accuracy, or goal-writing quality compared to traditional instructional approaches."
- **Why unresolved:** "Educational effectiveness cannot be assumed based on technical performance alone" â€” no learner outcomes have been measured.
- **What evidence would resolve it:** Randomized controlled trials measuring student performance on diagnostic reasoning and documentation tasks after training with AI-generated vs. traditional cases.

### Open Question 3
- **Question:** Can prompt engineering or knowledge base modifications improve internal consistency for complex multi-domain disorder presentations (e.g., mixed receptive-expressive language)?
- **Basis in paper:** [inferred] Results show language disorders with multiple domains consistently yielded the lowest internal consistency scores (commercial: 3.3, open-source: 3.0), attributed to "inherent complexity of coordinating syntax, morphology, semantics, and phonology within cohesive goal structures."
- **Why unresolved:** The paper documents the pattern but does not test interventions to address it; the authors note this as an area requiring "targeted prompt refinement, knowledge base augmentation."
- **What evidence would resolve it:** Ablation studies systematically varying prompt structure and retrieved knowledge for multi-domain cases, with expert-rated consistency outcomes.

## Limitations
- Expert clinical validation was limited to 35 cases across seven disorder types without longitudinal assessment of vignette utility
- Automated quality assessment using Claude Sonnet 4.5 represents AI evaluation of AI-generated content without direct clinician oversight
- Significant internal consistency gaps (3.0 vs 4.0+) between open-source and commercial models for complex disorders raises reliability concerns for local deployments

## Confidence

- **High Confidence:** The technical architecture (RAG + dual-prompt system) successfully generates valid JSON vignettes with consistent structural compliance across all tested models
- **Medium Confidence:** The quality assessment scores (4.39-4.50 for commercial models, 4.18-4.25 for open-source) reflect meaningful clinical relevance, though these are AI-evaluated rather than clinician-validated
- **Low Confidence:** The study's assertion that the system can scale to "broader clinical applications" lacks empirical support and does not address potential liability issues

## Next Checks
1. **Clinical Expert Validation:** Have licensed SLPs independently review 50 generated vignettes (random sample across all disorder types) and rate them for clinical accuracy, appropriateness for student training, and potential safety concerns
2. **Longitudinal Utility Assessment:** Implement the system in an actual SLP training program for one semester, tracking student performance on tasks using generated vignettes versus traditional case studies
3. **Knowledge Base Coverage Analysis:** Systematically audit the 44-document knowledge base for coverage gaps by identifying rare or emerging SLP conditions not well-represented in ASHA guidelines, then test system performance on these edge cases and measure hallucination rates