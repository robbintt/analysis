---
ver: rpa2
title: 'LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models'
arxiv_id: '2505.22208'
source_url: https://arxiv.org/abs/2505.22208
tags:
- dataset
- pre-training
- energy
- atoms
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LaMM addresses the computational bottleneck in pre-training neural
  network potentials (NNPs) for materials science by enabling semi-supervised pre-training
  on a large-scale dataset (~300M samples) that includes both labeled and unlabeled
  data. The key innovation is a novel loss function that supports joint training across
  heterogeneous datasets with varying label availability, along with a denoising self-supervised
  learning approach that uses a unique noise labeling scheme for unlabeled structural
  data.
---

# LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models

## Quick Facts
- **arXiv ID**: 2505.22208
- **Source URL**: https://arxiv.org/abs/2505.22208
- **Reference count**: 34
- **Key outcome**: LaMM enables semi-supervised pre-training on ~300M samples, reducing fine-tuning steps by up to 2.61x while improving final accuracy

## Executive Summary
LaMM addresses the computational bottleneck in pre-training neural network potentials (NNPs) for materials science by enabling semi-supervised pre-training on a large-scale dataset (~300M samples) that includes both labeled and unlabeled data. The key innovation is a novel loss function that supports joint training across heterogeneous datasets with varying label availability, along with a denoising self-supervised learning approach that uses a unique noise labeling scheme for unlabeled structural data. To handle the load imbalance caused by varying system sizes during multi-GPU training, LaMM introduces a load-balancing algorithm that improves throughput by up to 3.38x. The pre-trained models (LaMM-S and LaMM-L) accelerate fine-tuning on unseen datasets, reducing the number of steps required to reach target accuracy by up to 2.61x while also improving final energy and force prediction accuracy.

## Method Summary
LaMM employs a semi-supervised pre-training framework using a joint dataset of ~282M samples (432M with temperature sampling) combining 10 sources including OC20/OC22, ODAC23, Transition1x, QM7-X, QMugs, and PubChem3D. The method uses a universal loss function with binary masks to handle missing labels across datasets, and a denoising self-supervised learning approach with relative noise labels for unlabeled structural data. To address computational load imbalance from variable system sizes, LaMM implements a sorting and rearrangement algorithm that groups samples by atom count before GPU assignment. The models use PaiNN (LaMM-S) or EquiformerV2 (LaMM-L) encoders with multi-head decoders, trained using temperature sampling to balance subset contributions.

## Key Results
- Pre-trained models reduce fine-tuning steps to reach target accuracy by up to 2.61x
- Load balancing algorithm improves throughput by 2.44x-3.38x during multi-GPU training
- Denoising with relative noise labels accelerates convergence by 2.00x compared to standard denoising
- LaMM-S achieves Energy MAE < 25 meV/atom and Force MAE < 0.1 eV/Ã… on downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semi-supervised pre-training on heterogeneous data improves fine-tuning efficiency compared to training from scratch.
- **Mechanism:** The system employs a universal loss function using binary masks ($m_E, m_F$) to handle missing labels across disparate datasets. For unlabeled structural data (e.g., PubChem3D), it generates "pseudo-force" labels via a denoising task, allowing the encoder to learn structural constraints without expensive DFT calculations.
- **Core assumption:** The structural knowledge gained from denoising generic molecular geometries transfers beneficially to energy/force prediction tasks on specific materials.
- **Evidence anchors:** [abstract] mentions "semi-supervised pre-training" and "denoising self-supervised learning"; [section 3.2] defines the masked loss function $L$ that conditionally applies energy or force losses based on data availability; [corpus] supports the general validity of denoising objectives for learning force fields.

### Mechanism 2
- **Claim:** The proposed denoising labeling scheme accelerates convergence in self-supervised pre-training.
- **Mechanism:** Unlike standard denoising which predicts raw noise $\Delta x$, LaMM predicts noise relative to the system's center of mass ($\Delta x_i - \Delta x$). This removes the translational arbitrariness of the label, reducing ambiguity in the gradient signal.
- **Core assumption:** Standard denoising introduces "arbitrary offsets" that confuse the model because NNPs typically rely on relative coordinates.
- **Evidence anchors:** [section 3.3] explicitly contrasts the existing method with the proposed "unique labels" approach; [section 4.2] reports a 2.00x speedup in reaching a validation error threshold using the proposed method.

### Mechanism 3
- **Claim:** Load balancing via sorting and rearranging samples maximizes GPU throughput for variable-sized atomic systems.
- **Mechanism:** The algorithm sorts data by atom count (descending) and performs an all-to-all rearrangement to create mini-batches. This ensures the total atom count per GPU is consistent across the batch, minimizing idle time (stragglers) and memory fragmentation overhead.
- **Core assumption:** The variance in computation time is primarily dictated by the number of atoms, and the overhead of pre-processing/sorting is amortized over the training run.
- **Evidence anchors:** [section 3.4] describes the 3-step rearrangement method to solve the "small per-GPU batch size" variance issue; [section 4.1] shows throughput improvements of 2.44x-3.38x and reduced step-time variance.

## Foundational Learning

- **Concept: Neural Network Potentials (NNPs)**
  - **Why needed here:** Understanding that the goal is to surrogate Density Functional Theory (DFT) by mapping atomic coordinates to energy/forces is the core problem statement.
  - **Quick check question:** How does the model handle the rotational invariance of energy vs. the equivariance of forces?

- **Concept: Semi-Supervised Learning (Masked Loss)**
  - **Why needed here:** LaMM's primary contribution is training on a mixture of labeled (DFT) and unlabeled (structure-only) data.
  - **Quick check question:** How does the loss function prevent backpropagation on missing force labels in a mixed batch?

- **Concept: Graph Load Balancing**
  - **Why needed here:** The paper highlights a systems bottleneck (load imbalance) rather than just a modeling one.
  - **Quick check question:** Why does a variable number of atoms per sample cause GPU synchronization delays in multi-node training?

## Architecture Onboarding

- **Component map:** Input Data -> Load Balancer -> PaiNN/EquiformerV2 Encoder -> Multi-head Decoder (Energy/Force) -> Masked Loss
- **Critical path:**
  1. **Input Processing:** Load balanced batch $\rightarrow$ Coordinate noise injection (for unlabeled data).
  2. **Forward Pass:** Encoder generates atomic representations $\rightarrow$ Multi-head prediction.
  3. **Loss Calculation:** Apply masks ($m_E, m_F$) $\rightarrow$ Compute MAE/L2 loss against DFT labels or noise labels.
- **Design tradeoffs:**
  - **LaMM-S vs. LaMM-L:** PaiNN offers faster inference for MD simulations; EquiformerV2 offers higher accuracy for screening but requires aggressive load balancing to fit in memory.
  - **Dataset Scaling:** The paper uses Temperature Sampling ($T=2$) to upsample smaller datasets, effectively biasing the model toward rarer data domains at the cost of pure data efficiency.
- **Failure signatures:**
  - **OOM Errors:** Occurring stochastically if the load balancer is disabled, as large atoms cluster in specific GPU buffers.
  - **Slow Convergence:** If using standard denoising (not the proposed relative noise), validation loss plateaus higher.
- **First 3 experiments:**
  1. **Throughput Baseline:** Run `n` steps with and without the load-balancing shuffler on 4+ GPUs to verify the reported speedup.
  2. **Ablation on Noise:** Train on the PubChem subset using standard vs. relative noise labeling to reproduce the curve in Figure 7.
  3. **Fine-tuning Transfer:** Fine-tune the pre-trained LaMM-S on a small downstream dataset (e.g., HME21) vs. training from scratch to verify the 2.5x step reduction.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important research directions that are not fully explored in the current work.

## Limitations
- The denoising noise magnitude and exact sampling parameters remain unspecified, making exact reproduction challenging
- The temperature sampling hyperparameter (T=2) is described but its optimal value is not justified
- Load balancing benefits are measured primarily on the joint pre-training task, not on typical downstream fine-tuning scenarios

## Confidence
- **High Confidence**: The semi-supervised loss function (Mechanism 1) and its implementation via binary masks - well-specified and independently verifiable through the loss equation and ablation results.
- **Medium Confidence**: The denoising noise labeling scheme (Mechanism 2) - the core idea is clear, but the specific noise magnitude and its impact on convergence require external validation.
- **Medium Confidence**: The load balancing algorithm (Mechanism 3) - the method is described in detail, but the 3.38x improvement depends heavily on the specific multi-GPU configuration and dataset characteristics.

## Next Checks
1. **External Fine-tuning Validation**: Fine-tune LaMM-S on a benchmark dataset (e.g., Materials Project subset) not used in pre-training and compare convergence curves to training from scratch, verifying the 2.61x speedup claim.
2. **Noise Labeling Ablation**: Implement and compare standard vs. relative noise labeling on a controlled subset of PubChem3D, measuring convergence speed and final accuracy to validate the 2.00x improvement.
3. **Load Balancing Robustness**: Test the load balancing algorithm across different multi-GPU configurations (varying GPU counts and memory capacities) to verify the claimed throughput improvements are consistent and not specific to the 16-node V100 setup.