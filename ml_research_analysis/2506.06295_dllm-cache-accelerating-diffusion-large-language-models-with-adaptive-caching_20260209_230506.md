---
ver: rpa2
title: 'dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching'
arxiv_id: '2506.06295'
source_url: https://arxiv.org/abs/2506.06295
tags:
- response
- cache
- prompt
- caching
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: dLLM-Cache addresses the high inference latency of diffusion-based
  large language models (dLLMs) by introducing a training-free adaptive caching framework.
  The core idea leverages the observation that dLLM inference involves a static prompt
  and a partially dynamic response, where most tokens remain stable across adjacent
  denoising steps.
---

# dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching

## Quick Facts
- **arXiv ID:** 2506.06295
- **Source URL:** https://arxiv.org/abs/2506.06295
- **Reference count:** 40
- **Primary result:** 9.1× speedup over standard dLLM inference while maintaining output quality

## Executive Summary
dLLM-Cache introduces a training-free adaptive caching framework that significantly accelerates diffusion-based large language models (dLLMs) by exploiting the observation that dLLM inference involves a static prompt and a partially dynamic response. The method combines long-interval prompt caching with adaptive short-interval response caching guided by feature similarity, using a V-verify mechanism that leverages cosine similarity of Value vectors to identify tokens requiring updates. Experiments demonstrate up to 9.1× speedup over standard inference while maintaining output quality, bringing dLLM inference latency close to that of autoregressive models under many settings.

## Method Summary
The method addresses dLLM inference latency by implementing a dual caching strategy: long-interval caching for static prompt features (recomputed every Kp steps) and adaptive caching for response features using V-verify. The V-verify mechanism computes cosine similarity between current and cached Value vectors, selecting the bottom ρ fraction of tokens for full recomputation while reusing cached features for others. This approach exploits the quasi-static nature of prompts and the sparse dynamics of response token evolution during denoising steps.

## Key Results
- Up to 9.1× speedup on LongBench-HotpotQA while improving F1 score from 34.56 to 36.10
- 78.54% accuracy on GSM8K with Kp=50, Kr=7, ρ=0.25 versus 81.34% baseline
- FLOPs reduction of up to 87.5% compared to standard dLLM inference

## Why This Works (Mechanism)

### Mechanism 1: Long-Interval Prompt Caching
Static prompt features can be reused across many denoising steps without recomputation. At initialization, compute and cache all prompt-related features per layer, then recompute only when `k ≡ 0 (mod Kp)`. The core assumption is that input prompt c remains constant throughout inference and its representations stay sufficiently stable.

### Mechanism 2: V-verify for Adaptive Token Selection
Cosine similarity of Value vectors effectively identifies response tokens whose downstream features have changed significantly. For each response token j, compute similarity `s_j = cosine(V_new[j], V_cached[j])` and select bottom ⌊ρL⌋ tokens for recomputation. The core assumption is that Value vector changes correlate strongly with changes in Attention Output and FFN Output.

### Mechanism 3: Differentiated Prompt/Response Refresh Intervals
Separate treatment of prompt and response caching with different refresh cadences yields better speed-quality tradeoffs. Long interval Kp for prompt cache (e.g., 100) combined with short interval Kr for response refresh (e.g., 5-8) exploits quasi-static prompt + sparse response dynamics.

## Foundational Learning

- **Diffusion-based LLMs (Masked Diffusion Models):** dLLMs generate text by iteratively denoising a fully masked sequence over K steps, unlike ARMs which generate sequentially. This multi-step process causes high latency. Quick check: Can you explain why bidirectional attention in dLLMs prevents direct reuse of standard ARM KV-cache?

- **Key-Value Caching in Transformers:** Standard KV caching works for causal attention because past tokens' K/V don't change. dLLMs use bidirectional attention where all tokens attend to all tokens, so K/V for response tokens can change each step. Quick check: What property of causal attention enables KV-cache reuse in ARMs?

- **Cosine Similarity as a Feature Drift Metric:** V-verify relies on cosine similarity to quantify how much a token's representation has drifted. L2 distance performed worse (55.95% vs 78.54% accuracy on GSM8K), suggesting directional change matters more than magnitude. Quick check: Why might cosine similarity outperform L2 distance for detecting semantic drift in high-dimensional embeddings?

## Architecture Onboarding

- **Component map:** Input [prompt c | response y(k)] → For each layer l: Prompt Cache Cp[l] → {K, V, AttnOut, FFNOut}, Response Cache Cr[l] → {K, V, AttnOut, FFNOut}, Refresh logic (Kp, Kr intervals), V-verify (cosine similarity selection)

- **Critical path:** 1. Initialization (k=K): Full computation, populate both caches. 2. Each step k: Check refresh conditions → full refresh, prompt-only refresh, response-only refresh, or adaptive partial update. 3. Adaptive update path: Compute V projections → V-verify → selective K/Q/V/AttnOut/FFNOut for low-similarity tokens → scatter-update cache.

- **Design tradeoffs:** Larger Kp → more speedup but risk if prompt-response interactions matter; larger Kr → more reliance on adaptive updates; ρ ≈ 0.25 is empirically optimal; storage overhead O(T × d × 4 × L) ~1GB extra for 8B model.

- **Failure signatures:** Quality drop without caching (likely ρ too low or Kr too large); TPS not improving despite low ρ (fixed overhead dominates); inconsistency across layers (check scatter-update indexing); long prompt tasks underperforming (ensure Kp is appropriate).

- **First 3 experiments:** 1. Baseline validation on GSM8K with LLaDA 8B (Kp=50, Kr=7, ρ=0.25) to verify TPS, FLOPs reduction, and accuracy. 2. Ablation on ρ (sweep 0.1, 0.25, 0.5, 0.75, 1.0) with Kp=50, Kr=5 to confirm ρ≈0.25 sweet spot. 3. LongBench-HotpotQA test to validate 9.1× speedup with F1 improvement (34.56 → 36.10).

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical basis of V-verify effectiveness may not generalize beyond tested datasets and model sizes
- Fixed parameter selection (Kp, Kr, ρ) requires extensive tuning for different scenarios
- Storage overhead scales linearly with sequence length, potentially prohibitive for very long sequences

## Confidence
**High Confidence:** Basic observation about static prompts/dynamic responses is sound; FLOPs reduction measurements are reliable; architectural implementation details are clearly described.

**Medium Confidence:** Quality preservation claims on tested datasets; 9.1× speedup figure is dataset-dependent; optimal parameter settings require tuning.

**Low Confidence:** Universal applicability of V-similarity correlation across all dLLM variants; claim about bringing latency "close to ARMs" is highly dependent; assertion about equal effectiveness for autoregressive and non-autoregressive tasks.

## Next Checks
1. **Cross-Domain Correlation Validation:** Measure V-verify correlation coefficients on three additional diverse datasets (code generation, conversational dialogue, multilingual text) to test generalizability.

2. **Parameter Sensitivity on Long Sequences:** Systematically vary Kp, Kr, and ρ on sequences of increasing length (1K, 4K, 8K, 16K tokens) to understand scaling behavior and identify length-dependent parameter regimes.

3. **Memory vs Compute Tradeoff Analysis:** For fixed sequence length and quality target, measure wall-clock speedup while varying cache storage budget (by disabling cached features or reducing precision) to quantify practical impact of storage overhead.