---
ver: rpa2
title: 'MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large
  Language Models?'
arxiv_id: '2506.13065'
source_url: https://arxiv.org/abs/2506.13065
tags:
- question
- reasoning
- behavior
- motivation
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MotiveBench is a new benchmark for evaluating LLMs\u2019 human-like\
  \ motivational reasoning. It includes 200 rich scenarios and 600 questions covering\
  \ Maslow\u2019s hierarchy and 16 basic desires."
---

# MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?

## Quick Facts
- arXiv ID: 2506.13065
- Source URL: https://arxiv.org/abs/2506.13065
- Reference count: 40
- Primary result: New benchmark reveals LLMs achieve only 80.89% accuracy on human-like motivational reasoning, significantly below human performance

## Executive Summary
MotiveBench is a comprehensive benchmark designed to evaluate how well large language models can reason about human motivations. The benchmark includes 200 rich scenarios and 600 questions that cover Maslow's hierarchy of needs and 16 basic desires. Testing 29 models across seven model families revealed that even the best-performing model (GPT-4o) achieved only 80.89% accuracy, falling short of human-level reasoning capabilities. The study found that smaller models performed significantly worse with an average accuracy of 57.16%, and counterintuitively, Chain-of-Thought prompting often reduced model accuracy. LLMs particularly struggled with "love & belonging" needs and tended toward excessive rationality and idealism in their responses.

## Method Summary
The researchers developed MotiveBench as a benchmark for evaluating large language models' human-like motivational reasoning capabilities. The benchmark consists of 200 rich scenarios and 600 questions designed to assess understanding across Maslow's hierarchy of needs and 16 basic desires. The evaluation included 29 models spanning seven different model families, testing their ability to reason about human motivations in various contexts. The scenarios were carefully crafted to represent diverse motivational situations, and the questions were designed to probe different aspects of motivational reasoning. Human expert validation was employed to ensure the quality and relevance of the benchmark content.

## Key Results
- GPT-4o achieved only 80.89% accuracy, falling short of human-level motivational reasoning
- Smaller models averaged 57.16% accuracy, showing significant performance gaps
- Chain-of-Thought prompting often reduced accuracy, contrary to typical expectations
- LLMs struggled particularly with "love & belonging" needs and exhibited excessive rationality and idealism

## Why This Works (Mechanism)
The benchmark works by providing rich, context-specific scenarios that require understanding of human motivations rather than simple pattern matching. By covering both Maslow's hierarchy and 16 basic desires, the benchmark tests models' ability to reason about motivations across multiple psychological frameworks. The scenarios are designed to capture the complexity and nuance of real human motivational situations, forcing models to demonstrate genuine understanding rather than surface-level correlations. The multi-question structure per scenario allows for testing different aspects of motivational reasoning within the same context.

## Foundational Learning
- Maslow's hierarchy of needs: Understanding the five levels of human needs (physiological, safety, love/belonging, esteem, self-actualization) is essential for interpreting motivational scenarios
- Basic desires theory: Knowledge of the 16 fundamental human desires (power, independence, curiosity, acceptance, order, saving, honor, idealism, social contact, family, status, vengeance, romance, eating, physical exercise, tranquility) provides the psychological framework for evaluation
- Motivational reasoning: The cognitive process of inferring why people act based on their needs, desires, and circumstances
- Chain-of-Thought prompting: A technique where models generate intermediate reasoning steps before final answers
- Human motivation assessment: Methods for evaluating how well models understand and predict human behavior based on underlying motivations
- Cross-model benchmarking: Comparative evaluation across different model families and sizes to identify performance patterns

Why needed: Understanding human motivation requires knowledge of established psychological theories and reasoning frameworks that go beyond simple language understanding.
Quick check: Can the reader identify which of Maslow's five needs are being tested in a given scenario?

## Architecture Onboarding

Component map: Scenario generation -> Question formulation -> Model evaluation -> Human validation -> Performance analysis

Critical path: The most important sequence is Scenario generation -> Question formulation -> Model evaluation, as this determines the quality and validity of the benchmark results.

Design tradeoffs: The authors balanced scenario complexity against interpretability, choosing rich scenarios that capture motivational nuance while remaining evaluable by both humans and models. They also traded breadth (200 scenarios across multiple frameworks) against depth (more questions per scenario type).

Failure signatures: Models showing excessive rationality in responses, inability to capture emotional aspects of motivation, or consistent misclassification of love/belonging needs indicate fundamental gaps in motivational reasoning capabilities.

3 first experiments:
1. Test baseline model performance on simple motivational scenarios before introducing complex scenarios
2. Evaluate human performance on the same benchmark to establish a gold standard
3. Test model performance with and without Chain-of-Thought prompting on identical scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- The subjective nature of motivational reasoning makes objective evaluation challenging even with human expert validation
- The 200 scenarios may not capture the full complexity of human motivational reasoning despite broad coverage
- The finding that CoT prompting reduces accuracy raises questions about LLM reasoning processes but lacks explanation

## Confidence
High: Benchmark design and methodology are sound
Medium: Findings about model performance gaps and CoT limitations
Low: Generalizability of results to all motivational reasoning contexts

## Next Checks
1. Conduct cross-cultural validation with diverse human evaluators to ensure benchmark scenarios are universally interpretable
2. Test model performance on systematically varied scenario complexity levels to identify specific reasoning bottlenecks
3. Compare LLM responses against longitudinal human reasoning data to assess whether observed differences represent fundamental limitations or training artifacts