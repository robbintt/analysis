---
ver: rpa2
title: Adapting Biomedical Abstracts into Plain language using Large Language Models
arxiv_id: '2501.15700'
source_url: https://arxiv.org/abs/2501.15700
tags:
- language
- evaluation
- available
- plain
- adaptations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to adapt biomedical abstracts into
  plain language using Large Language Models (LLMs). The work addresses the problem
  of low health literacy by automatically simplifying complex biomedical text for
  general public consumption.
---

# Adapting Biomedical Abstracts into Plain language using Large Language Models

## Quick Facts
- arXiv ID: 2501.15700
- Source URL: https://arxiv.org/abs/2501.15700
- Reference count: 40
- Primary result: GPT-4-based model achieved first place in external manual evaluation for simplicity and third place for accuracy in biomedical abstract simplification task

## Executive Summary
This paper presents an approach to adapt biomedical abstracts into plain language using Large Language Models (LLMs). The work addresses the problem of low health literacy by automatically simplifying complex biomedical text for general public consumption. The authors fine-tuned and applied in-context learning with various LLMs including T5, LLaMa2, GPT-3.5, and GPT-4. Their top-performing GPT-4-based model achieved first place in the external manual evaluation for simplicity and third place for accuracy, demonstrating that LLMs can effectively transform technical biomedical language into accessible plain language while preserving meaning and readability.

## Method Summary
The authors employed multiple approaches: fine-tuning T5-Large, fine-tuning GPT-3.5 via OpenAI API, applying Parameter-Efficient Fine-Tuning (PEFT) with LoRA to LLaMa2-13B and LLaMa2-70B models, and using in-context learning (ICL) with GPT-4. The LoRA fine-tuning used 4-bit precision with rank=16, training only 0.05% of parameters. For GPT-4, they used a distilled version of PLABA annotation guidelines with one exemplar in the prompt. The models were evaluated using automatic metrics (BLEU, ROUGE, SARI) and human evaluation along two axes: simplicity (sentence/term simplicity, term accuracy, fluency) and accuracy (completeness, faithfulness).

## Key Results
- GPT-4-based model achieved first place in external manual evaluation for simplicity
- Fine-tuned models (T5, LLaMa2) scored higher on automatic metrics but simply copied input sentences with minimal simplification
- LLaMa2-70B with LoRA achieved competitive automatic scores but required human evaluation for proper assessment
- The approach demonstrated effective transformation of technical biomedical language into accessible plain language

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning with Guideline Distillation
Providing distilled annotation guidelines within the prompt enables LLMs to perform domain-specific text adaptation without weight updates. The GPT-4 model received a condensed version of the PLABA annotation guidelines plus one exemplar illustrating key instructions. This primed the model to follow task-specific adaptation rules rather than generic summarization behavior. Core assumption: The model's pre-training includes sufficient biomedical knowledge and instruction-following capability to apply guidelines from context alone.

### Mechanism 2: Parameter-Efficient Fine-Tuning with Low-Rank Adaptation
LoRA-based PEFT enables effective task adaptation for open-source LLMs while preserving pre-trained knowledge and reducing computational requirements. LoRA adds low-rank decomposition matrices to transformer layers, training only 0.05% of parameters while keeping base weights frozen. This allows LLaMa2-70B to learn PLABA-specific adaptation patterns without catastrophic forgetting. Core assumption: Task-relevant adaptations can be captured in low-rank subspaces without modifying the full parameter matrix.

### Mechanism 3: Multi-Axis Human Evaluation Alignment
Automatic metrics (BLEU, ROUGE) fail to capture simplicity-accuracy tradeoffs; human evaluation along explicit axes (sentence simplicity, term accuracy, faithfulness) is necessary for valid assessment. The PLABA evaluation framework decomposes quality into Simplicity axis and Accuracy axis. Models optimizing for n-gram overlap may copy complex text verbatim, scoring high on automatic metrics but failing human evaluation. Core assumption: Human annotators can reliably distinguish and score these axes independently.

## Foundational Learning

- **In-Context Learning (ICL)**: Why needed here: Core technique for GPT-4 deployment; enables task adaptation without fine-tuning by conditioning on prompt examples and instructions. Quick check: Can you explain why adding a single exemplar to a prompt might change model behavior more than adding generic instructions?

- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**: Why needed here: Required for adapting LLaMa2 models with limited compute; understanding low-rank decomposition helps diagnose underfitting vs. rank insufficiency. Quick check: If LoRA rank is set too low, what failure mode would you expect during fine-tuning on a specialized domain task?

- **Text Simplification Evaluation (SARI, BLEU, ROUGE)**: Why needed here: The paper demonstrates that automatic metrics can be misleading; understanding what each metric measures helps interpret results and design better evaluation protocols. Quick check: Why might a model that copies source text verbatim score highly on BLEU but poorly on human simplicity ratings?

## Architecture Onboarding

- **Component map**: Input Layer (PubMed abstract + MedlinePlus consumer question) -> Model Layer (GPT-4 with ICL, LLaMa2 with LoRA, GPT-3.5 fine-tuned) -> Evaluation Layer (Automatic metrics + Human evaluation)

- **Critical path**: 1. Parse PLABA annotation guidelines → distill into prompt template 2. Select exemplar(s) for ICL or prepare training pairs for fine-tuning 3. Run inference/fine-tuning on target model 4. Evaluate with SARI first; if competitive, proceed to human evaluation sampling

- **Design tradeoffs**: ICL (GPT-4) achieved better human ratings but requires API access; fine-tuning (LLaMa2) offers deployment control but showed verbosity and copying behaviors. Model scale: LLaMa2-70B with LoRA vs. LLaMa2-13B—paper tested both, but additional context experiments showed no significant improvement from inter-sentence context. Evaluation cost: Automatic metrics are cheap but misaligned with human judgment; human evaluation is expensive but necessary for final validation.

- **Failure signatures**: High BLEU/ROUGE, low human simplicity: Model copying source text verbatim (observed with T5, LLaMa2). Verbose but inaccurate output: Model generating fluent text that misrepresents source claims (risk with GPT-3.5). Guideline ignoring: Model failing to apply simplification rules (increase exemplar count or guideline specificity).

- **First 3 experiments**: 1. Replicate GPT-4 ICL setup: Distill annotation guidelines, add one exemplar, evaluate on 10 held-out abstracts with human ranking against baseline 2. Ablate guideline components: Test prompt with full guidelines vs. short instruction only to measure contribution of guideline distillation 3. Cross-validate automatic vs. human metrics: Compute correlation between SARI scores and human simplicity ratings on internal test set to confirm evaluation alignment before external submission

## Open Questions the Paper Calls Out

### Open Question 1
Can automatic evaluation metrics be developed or refined to better correlate with human judgments of simplicity and faithfulness in biomedical text adaptation? Basis in paper: The authors note that high BLEU and ROUGE scores for fine-tuned models often corresponded to repetitive or insufficient simplification, failing to reflect the semantic quality captured by human evaluation. Why unresolved: Current n-gram overlap metrics fail to capture the semantic transformations required for plain language adaptation. What evidence would resolve it: A study demonstrating a strong statistical correlation between a novel/modified automatic metric and the human evaluation criteria defined in the paper.

### Open Question 2
Why does the inclusion of abstract-specific inter-sentence context fail to improve performance for large models like LLaMa2-70B in this specific task? Basis in paper: The authors state that additional experiments including abstract-specific inter-sentence context for a target sentence did not significantly improve performance. Why unresolved: It is unclear if the model fails to leverage the context window effectively, or if sentence-level simplification is predominantly a local phenomenon. What evidence would resolve it: Ablation studies analyzing attention mechanisms or varying context window sizes to determine if the model attends to inter-sentence information during simplification.

### Open Question 3
What are the trade-offs between fine-tuning open-source models (like LLaMa2) and using in-context learning with proprietary models (like GPT-4) regarding factual accuracy in medical text simplification? Basis in paper: The GPT-4 model ranked 1st in simplicity but 3rd in accuracy, while fine-tuned models struggled with repetition; the optimal balance between accessibility and medical precision remains undetermined. Why unresolved: The paper compares rankings but does not deeply analyze the specific types of factual errors or hallucinations introduced by different training approaches. What evidence would resolve it: A qualitative error analysis comparing the faithfulness and completeness scores of fine-tuned vs. in-context learning models across a standardized test set.

## Limitations
- Limited generalizability: Results focus specifically on biomedical abstracts and may not extend to other domains or longer-form text simplification tasks
- Evaluation methodology constraints: Human evaluation relied on sampling and pairwise comparisons rather than comprehensive annotation of all outputs
- Automatic metric misalignment: BLEU and ROUGE can be misleading for simplification tasks, creating discrepancy between automatic benchmarks and utility

## Confidence
- High confidence: GPT-4 with ICL approach achieving top human evaluation rankings is well-supported by both internal evaluation (9/10 abstracts preferred) and external competition results
- Medium confidence: The superiority of LoRA-based PEFT for open-source models is supported by automatic metrics but lacks human evaluation validation
- Medium confidence: The claim that guideline distillation enables effective ICL is supported by competition results but the specific distillation process remains unclear

## Next Checks
1. Ablation study of guideline components: Systematically test prompt variations with full guidelines vs. short instructions vs. no guidelines to quantify the contribution of guideline distillation to GPT-4 performance
2. Extended human evaluation correlation: Compute correlation between SARI scores and human simplicity ratings on a larger internal test set to validate automatic metric alignment before future external submissions
3. Cross-domain generalization test: Apply the top-performing GPT-4 ICL approach to a different biomedical text type (e.g., clinical trial results) to assess whether the simplification capabilities transfer beyond PubMed abstracts