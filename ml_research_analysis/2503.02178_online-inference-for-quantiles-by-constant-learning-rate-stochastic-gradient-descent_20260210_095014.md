---
ver: rpa2
title: Online Inference for Quantiles by Constant Learning-Rate Stochastic Gradient
  Descent
arxiv_id: '2503.02178'
source_url: https://arxiv.org/abs/2503.02178
tags:
- quantile
- distribution
- stochastic
- stationary
- markov
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel online inference method for quantile
  estimation using constant learning-rate stochastic gradient descent (SGD), addressing
  the challenge of non-smooth and non-strongly-convex quantile loss functions. The
  authors interpret the SGD iterates as an irreducible and positive recurrent Markov
  chain, proving the existence of a unique asymptotic stationary distribution regardless
  of initialization.
---

# Online Inference for Quantiles by Constant Learning-Rate Stochastic Gradient Descent

## Quick Facts
- arXiv ID: 2503.02178
- Source URL: https://arxiv.org/abs/2503.02178
- Authors: Ziyang Wei; Jiaqi Li; Likai Chen; Wei Biao Wu
- Reference count: 40
- Primary result: First CLT-type guarantees for constant learning-rate SGD in quantile estimation with online inference method

## Executive Summary
This paper introduces a novel approach for online inference of quantiles using constant learning-rate stochastic gradient descent (SGD). The authors address the challenge of non-smooth and non-strongly-convex quantile loss functions by interpreting SGD iterates as an irreducible and positive recurrent Markov chain. They prove the existence of a unique asymptotic stationary distribution and demonstrate its convergence to a Gaussian distribution as the learning rate approaches zero. The paper also develops a recursive algorithm for constructing confidence intervals in an online manner, with numerical studies showing strong finite-sample performance.

## Method Summary
The authors propose using constant learning-rate SGD for quantile estimation, interpreting the iterates as a Markov chain to analyze their asymptotic behavior. They establish theoretical guarantees by bounding the moment generating function and tail probabilities, showing convergence to a Gaussian distribution. A recursive algorithm is developed for online confidence interval construction. The method combines stochastic optimization with Markov chain theory to handle the non-smooth quantile loss function.

## Key Results
- First central limit theorem-type theoretical guarantees for constant learning-rate SGD in quantile estimation
- Proves unique asymptotic stationary distribution exists regardless of initialization
- Demonstrates stationary distribution converges to Gaussian as learning rate approaches zero
- Numerical studies validate strong finite-sample performance of both estimator and inference method

## Why This Works (Mechanism)
The method works by treating constant learning-rate SGD iterates as a Markov chain, allowing analysis of their long-term behavior. By proving the chain is irreducible and positive recurrent, the authors establish a unique stationary distribution. The Gaussian limit arises from carefully bounding the moment generating function and tail probabilities, which controls the fluctuations around the stationary distribution.

## Foundational Learning
- Markov chain theory: Needed to analyze SGD iterates as a stochastic process
  - Quick check: Verify irreducibility and positive recurrence conditions
- Quantile regression loss: Non-smooth objective requiring special treatment
  - Quick check: Confirm loss function satisfies required assumptions
- Central limit theorems for Markov chains: Framework for asymptotic analysis
  - Quick check: Validate mixing conditions for the Markov chain

## Architecture Onboarding

**Component map**: Data stream -> Quantile loss function -> Constant learning-rate SGD -> Markov chain analysis -> Stationary distribution -> Confidence intervals

**Critical path**: The key computational path involves updating the quantile estimate using SGD, tracking the Markov chain state, and recursively updating confidence intervals based on the stationary distribution.

**Design tradeoffs**: Constant vs. decreasing learning rates (constant enables online inference but may sacrifice convergence speed), computational complexity of recursive confidence intervals vs. batch methods, theoretical guarantees vs. practical performance.

**Failure signatures**: Poor mixing of the Markov chain leading to inaccurate stationary distribution estimates, numerical instability in recursive confidence interval updates, breakdown of Gaussian approximation for finite samples.

**First experiments**:
1. Validate Markov chain convergence under different data distributions
2. Test confidence interval coverage rates across sample sizes
3. Compare computational complexity of recursive vs. batch confidence interval methods

## Open Questions the Paper Calls Out
None

## Limitations
- Practical implementation challenges of recursive confidence interval algorithm, particularly computational complexity and numerical stability in high dimensions
- Theoretical assumptions about constant learning rates and specific data distributions may not hold in real applications
- Lack of analysis for heavy-tailed noise or non-stationary data streams

## Confidence
- High: Theoretical framework for constant learning-rate SGD as Markov chain and asymptotic Gaussianity
- Medium: Practical utility of online inference method given limited complexity analysis
- Low: Performance under model misspecification, non-i.i.d. data, or non-convex landscapes

## Next Checks
1. Empirical evaluation of recursive confidence interval algorithm's computational complexity and numerical stability across varying dimensionalities and sample sizes
2. Stress testing under non-stationary data streams and heavy-tailed noise distributions to assess robustness
3. Comparison against existing offline inference techniques in terms of statistical efficiency and computational overhead