---
ver: rpa2
title: 'Adversarial Attacks Against Automated Fact-Checking: A Survey'
arxiv_id: '2509.08463'
source_url: https://arxiv.org/abs/2509.08463
tags:
- attacks
- adversarial
- claims
- evidence
- fever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically reviews adversarial attacks targeting
  automated fact-checking (AFC) systems, which remain vulnerable despite significant
  advances in verification accuracy. We introduce a novel taxonomy categorizing attacks
  based on target components (claim, evidence, or claim-evidence pairs) and edit granularity
  (character to article level), revealing 53 distinct attack strategies.
---

# Adversarial Attacks Against Automated Fact-Checking: A Survey

## Quick Facts
- arXiv ID: 2509.08463
- Source URL: https://arxiv.org/abs/2509.08463
- Reference count: 40
- Primary result: Survey systematically reviews 53 adversarial attack strategies against AFC systems, finding only 13 have defenses while attacks can degrade accuracy by up to 40%

## Executive Summary
This survey provides the first comprehensive taxonomy of adversarial attacks targeting automated fact-checking systems, categorizing 53 distinct attack strategies across three target components (claims, evidence, claim-evidence pairs) and multiple edit granularities. The analysis reveals that while generation-based attacks dominate claim manipulation, evidence attacks show particularly strong resilience with only 13 attacks currently addressed by defenses. Performance evaluation across 15 datasets demonstrates that adversarial claims can degrade model accuracy by up to 40% and retrieval recall by over 20%, with NEI verdicts being especially susceptible. The findings highlight the urgent need for robust AFC frameworks that can withstand adversarial manipulations while preserving verification accuracy in real-world misinformation scenarios.

## Method Summary
The survey employs a systematic methodology to identify and categorize adversarial attacks against AFC systems, analyzing attacks targeting different pipeline stages (claim detection, evidence retrieval, verdict prediction). Individual attacks use rule-based transformations (SEARs, paraphrasing) or LM-based generation (GPT-2, BART, T5, PEGASUS) with black-box access assumptions. The evaluation draws from 15 benchmark datasets, primarily FEVER 1.0/2.0, using metrics including FEVER Score, Potency, Attack Success Rate, and Document/Evidence Recall. The survey also reviews 14 defense strategies, finding they address only 13 of 53 identified attacks.

## Key Results
- Generation-based claim attacks degrade AFC accuracy by up to 40% through surface-level heuristic exploitation
- Evidence poisoning attacks succeed due to retrieval systems' lack of authenticity verification, with NEI verdicts particularly vulnerable
- Only 13 of 53 identified attacks have corresponding defenses, covering less than a quarter of attack vectors
- Dataset-level biases persist in large-scale benchmarks, causing over 20-point accuracy drops on unbiased test pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generation-based claim attacks degrade AFC accuracy by up to 40% by exploiting models' reliance on surface-level heuristics rather than deep reasoning.
- Mechanism: Adversaries manipulate or generate claims using rule-based transformations (SEARs, paraphrasing) or language models (GPT-2, BART) to preserve semantic plausibility while inducing misclassification. Attacks like Multi-hop and Fact Mixing blend facts across sources, forcing models to perform compositional reasoning they haven't learned.
- Core assumption: FC models trained on benchmark datasets (e.g., FEVER) learn spurious correlations between claim patterns and verdicts rather than true semantic verification.
- Evidence anchors:
  - [abstract] "adversarial claims degrade model accuracy by up to 40%"
  - [section 5.1] "SubsetNum results in an average of over 80% incorrect verdict predictions"; "attacks such as Multi-hop Temp., Colloquial, and Fact Mixing reveal that many models rely on shallow heuristics rather than deep reasoning"
  - [corpus] DECEIVE-AFC paper confirms LLM-based fact-checking systems remain vulnerable to adversarial claim attacks

### Mechanism 2
- Claim: Evidence poisoning attacks succeed because retrieval systems lack robustness to corpus-level contamination and verdict predictors over-rely on retrieved evidence quality.
- Mechanism: Attackers inject fabricated or paraphrased evidence (via GPT-2, PEGASUS, Grover) that either (a) is retrieved preferentially over gold evidence, or (b) flips verdict predictions when retrieved. The Imperceptible attack uses homoglyphs/invisible characters to evade human detection while corrupting tokenization.
- Core assumption: Retrieval systems rank evidence by lexical/semantic similarity without verifying authenticity, and downstream verdict models lack mechanisms to detect evidence tampering.
- Evidence anchors:
  - [abstract] "retrieval recall by over 20%, with NEI verdicts being particularly susceptible"
  - [section 5.2] "NEI claims are especially vulnerable" to AdvAdd attacks; "Omitting Paraphrase... KGAT (BERT BASE) mistakenly retrieves 54.4% of adversarial evidence"
  - [corpus] "Attacks by Content" paper notes manipulation of retrieved documents can subvert agent behavior without instruction injection

### Mechanism 3
- Claim: Claim-evidence pair attacks expose dataset-level idiosyncratic biases that persist even in large-scale benchmarks.
- Mechanism: Symmetric attacks construct claim-evidence pairs that retain original relationship labels (SUP/REF) while encoding contradictory factual content. Models trained on biased datasets (FEVER 1.0) learn to predict based on surface cues (e.g., "did not" → REF) rather than factual content, causing >20-point accuracy drops on unbiased test pairs.
- Core assumption: Dataset construction processes introduce systematic correlations between linguistic patterns and labels that models exploit as shortcuts.
- Evidence anchors:
  - [abstract] "13 of 53 attacks currently addressed by defenses"
  - [section 5.3] "adversarial pairs... result in performance falling below 60%—a decline of over 20 points"; "models' reliance on surface-level cues and expose biases inherent in dataset construction"
  - [corpus] VeriTaS paper highlights that static benchmarks are subject to temporal drift and don't capture real-world misinformation diversity

## Foundational Learning

- Concept: **AFC Pipeline Stages** (Claim Detection → Evidence Retrieval → Verdict Prediction → Justification Production)
  - Why needed here: Adversarial attacks target specific stages; understanding the pipeline identifies where defenses should be placed.
  - Quick check question: Can you identify which pipeline stage each attack (claim, evidence, pair) primarily disrupts?

- Concept: **Attack Taxonomy Dimensions** (Target × Granularity)
  - Why needed here: The paper's taxonomy (Fig. 2) organizes 53 attacks across targets (verdict/retrieval) and granularity (character→dataset), enabling systematic vulnerability assessment.
  - Quick check question: For a character-level homoglyph attack, what is the target and what stage does it affect?

- Concept: **Debiasing and Contrastive Learning Defenses**
  - Why needed here: Only 13/53 attacks have defenses; understanding PoE, CLEVER, CrossAug, and Causal Walk is essential for extending coverage.
  - Quick check question: Why does reweighting training examples help against Symmetric attacks but not against Imperceptible evidence attacks?

## Architecture Onboarding

- Component map:
  ```
  [Claim Input] → [Claim Detection] → [Evidence Retrieval] ← [Evidence Corpus]
                                              ↓
                                     [Verdict Prediction (SUP/REF/NEI)]
                                              ↓
                                     [Justification Production]
  ```

- Critical path: Evidence retrieval quality directly determines verdict accuracy; adversarial evidence that passes retrieval has outsized impact on downstream predictions (NEI claims most vulnerable per section 5.2).

- Design tradeoffs:
  - Dense vs. sparse retrieval: Dense retrievers (DPR) more vulnerable to semantic manipulation; sparse (BM25) more robust but lower recall
  - Rule-based vs. LM-based attacks: Rule-based more interpretable/controllable; LM-based more fluent/realistic
  - White-box vs. black-box: Most surveyed attacks are black-box; white-box verification attacks remain underexplored

- Failure signatures:
  - Sudden accuracy drops on specific claim types (e.g., colloquial claims: 90%→72% retrieval recall)
  - High "→ NEI" shift ratios (>50%) indicate evidence sufficiency attacks
  - Retrieval systems returning adversarial evidence with high confidence

- First 3 experiments:
  1. **Baseline vulnerability assessment**: Run FEVER benchmark against 3 representative attacks (SubsetNum for claims, AdvAdd for evidence, Symmetric for pairs) using KGAT + BERT baseline; measure FEVER score degradation and retrieval recall changes.
  2. **Defense coverage mapping**: Implement CLEVER (counterfactual debiasing) and evaluate against Model-targeting, Dataset Bias, and Lexically-informed attacks; measure accuracy recovery on FEVER-adv.
  3. **Cross-category robustness test**: Apply Imperceptible character-level evidence attack to current system; assess whether existing retrieval (BM25 vs. dense) shows different vulnerability profiles, then prototype homoglyph detection preprocessing.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a unified evaluation benchmark be developed that standardizes models, metrics, and perturbation budgets for comparing adversarial attack effectiveness across AFC systems?
- Basis in paper: [explicit] Section 7 states "A universal benchmark for evaluating AFC model robustness across datasets and metrics is still lacking (see Appendix C), limiting comparability across attacks and defenses."
- Why unresolved: The paper identifies that evaluations across 15 datasets use inconsistent settings, models, and metrics, making direct comparison impossible. Current benchmarks like FEVER focus on benign conditions, not adversarial robustness.
- What evidence would resolve it: Creation of a benchmark with standardized test models, unified attack success metrics (e.g., potency, adjusted potency), fixed perturbation budgets, and adversarial examples that evade both system-level detection and human detection.

### Open Question 2
- Question: How can defense coverage be expanded to address the 40 of 53 identified attack vectors that currently lack any mitigation strategies?
- Basis in paper: [explicit] Section 7 states "current defenses address only 13 of the 53 attacks across all categories, covering less than a quarter. More disruptive attacks exploiting inductive reasoning and knowledge compositional weaknesses (e.g., SubsetNum and Multi-hop Temp.) remain unsolved."
- Why unresolved: Existing defenses like debiasing (CLEVER, PoE) and contrastive learning target specific attack types but do not generalize. Attacks exploiting multi-hop reasoning and temporal shifts have no known defenses.
- What evidence would resolve it: Development of defense methods that demonstrate robustness across multiple attack categories simultaneously, measured by maintained accuracy under combined attack scenarios.

### Open Question 3
- Question: How do cross-modal adversarial attacks (e.g., text-image inconsistencies) affect multimodal AFC system robustness?
- Basis in paper: [explicit] Section 7 states "Most attacks target text, overlooking real-world FC tasks that span text, images, and videos." Appendix C.1.1 adds "current datasets... are exclusively focused on single-modal data, specifically text data."
- Why unresolved: No existing work has systematically evaluated multimodal adversarial attacks on AFC systems, despite real-world misinformation increasingly using multimodal content.
- What evidence would resolve it: Construction of multimodal adversarial datasets and empirical evaluation showing attack success rates against multimodal AFC models compared to text-only baselines.

## Limitations
- Most attacks assume black-box access, leaving white-box verification attacks underexplored
- Evaluation primarily relies on FEVER benchmarks, which may not capture real-world misinformation dynamics and temporal drift
- Defense coverage claims may be overestimated since many referenced defenses lack detailed implementation specifications

## Confidence
- High confidence: Mechanism 1 (claim attack degradation up to 40%) and Mechanism 2 (evidence poisoning effectiveness) - supported by multiple evaluation tables and corroborated by recent DECEIVE-AFC findings
- Medium confidence: Mechanism 3 (dataset bias exposure) - based on FEVER-specific results that may not generalize to newer benchmarks
- Low confidence: Defense coverage claims - many referenced defenses lack detailed implementation specifications in surveyed papers

## Next Checks
1. **Cross-dataset validation**: Test the most potent attacks (SubsetNum, AdvAdd, Symmetric) on FEVER 2.0, FEVEROUS, and VeriTaS to verify if performance degradation patterns hold across benchmarks
2. **Defense implementation verification**: Replicate CLEVER and Causal Walk defenses on FEVER-adv dataset with exact hyperparameters to measure actual coverage against the 40+ unaddressed attack vectors
3. **Real-world robustness test**: Deploy a subset of adversarial claim and evidence attacks against an operational LLM-based fact-checking system (e.g., GPT-4 with retrieval) to assess whether benchmark vulnerabilities translate to production environments