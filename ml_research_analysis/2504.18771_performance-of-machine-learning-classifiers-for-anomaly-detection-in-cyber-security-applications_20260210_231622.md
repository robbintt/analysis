---
ver: rpa2
title: Performance of Machine Learning Classifiers for Anomaly Detection in Cyber
  Security Applications
arxiv_id: '2504.18771'
source_url: https://arxiv.org/abs/2504.18771
tags:
- learning
- data
- dataset
- datasets
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work empirically evaluates machine learning models for anomaly
  detection in cyber security, focusing on two imbalanced datasets (KDDCUP99 and Credit
  Card Fraud 2013). The study compares supervised (XGB, MLP) and unsupervised (GAN,
  VAE, MO-GAAL) learning approaches using an 80/20 train/test split with 5-fold cross-validation.
---

# Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications

## Quick Facts
- **arXiv ID:** 2504.18771
- **Source URL:** https://arxiv.org/abs/2504.18771
- **Reference count:** 25
- **Primary result:** XGBoost outperforms MLP and generative models (VAE, GAN, MO-GAAL) for anomaly detection on imbalanced tabular cybersecurity datasets, with Random Over-Sampling improving MLP performance

## Executive Summary
This work empirically evaluates machine learning models for anomaly detection in cyber security applications using two highly imbalanced datasets (KDDCUP99 and Credit Card Fraud 2013). The study compares supervised learning approaches (XGBoost and MLP) with unsupervised generative models (GAN, VAE, MO-GAAL) using an 80/20 train/test split with 5-fold cross-validation. XGBoost achieved the best overall performance across both datasets, confirming its effectiveness for tabular data classification tasks. Random Over-Sampling improved MLP performance significantly, particularly on the credit card dataset, while generative models showed poorer generalization performance due to optimization complexity. The findings highlight the importance of selecting appropriate sampling techniques and demonstrate the limitations of generative models for anomaly detection in imbalanced cyber security applications.

## Method Summary
The study compares supervised (XGB, MLP) and unsupervised (GAN, VAE, MO-GAAL) learning approaches on two imbalanced datasets using an 80/20 stratified train/test split with 5-fold cross-validation. XGB and MLP were trained with standard supervised learning, while generative models were trained on the majority class to learn normal behavior and detect anomalies via reconstruction error or discriminator output. The authors tested Random Over-Sampling and Self-Paced Ensemble for imbalance handling, and evaluated mean, median, and IterativeImputer techniques for missing value imputation. XGB achieved the best performance, with ROS improving MLP results and IterativeImputer showing high computational cost without accuracy gains.

## Key Results
- XGB achieved the best overall performance across both datasets, confirming its effectiveness for tabular data classification tasks
- Random Over-Sampling improved MLP performance, particularly on the credit card dataset with large improvements
- Generative models (VAE, GAN, MO-GAAL) showed poorer generalization performance due to optimization complexity and hyperparameter sensitivity
- IterativeImputer showed no significant performance improvements but had high computational complexity, making it unsuitable for large datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** XGB outperforms other models on tabular cybersecurity data with class imbalance when combined with appropriate sampling strategies.
- **Mechanism:** Gradient boosting sequentially corrects residual errors from prior trees, focusing learning capacity on difficult examples. Tree-based splits naturally handle mixed feature types and non-linear boundaries without requiring feature scaling. scale_pos_weight or sampling adjustments counteract majority-class gradient dominance.
- **Core assumption:** The anomaly patterns are learnable from available features and maintain some consistency between training and deployment distributions.
- **Evidence anchors:**
  - [abstract]: "XGB achieved the best overall performance, confirming its effectiveness for tabular data classification tasks"
  - [section 3, Results]: "XGB outperforms all models and its performance is similar on both datasets"
  - [corpus]: Limited direct corpus support for XGB superiority; neighboring papers focus on broader ML approaches for cybersecurity without specific model comparisons.
- **Break condition:** Feature distributions shift significantly post-deployment; anomaly patterns are conceptually novel and lack representation in training data.

### Mechanism 2
- **Claim:** Random Over-Sampling (ROS) improves MLP performance on highly imbalanced fraud detection tasks by rebalancing the gradient signal.
- **Mechanism:** MLPs optimize via gradient descent; with severe imbalance, majority-class gradients dominate weight updates, causing the network to prioritize majority-class accuracy. ROS increases minority-class sample frequency, ensuring minority-class loss contributes proportionally to gradient updates. This trades recall for precision improvements in some cases.
- **Core assumption:** The minority-class samples are representative of the true anomaly distribution; synthetic or replicated samples do not introduce harmful overfitting.
- **Evidence anchors:**
  - [abstract]: "Random Over-Sampling improved MLP performance"
  - [section 3, Results]: "the combination of MLP + ROS led to an improvement in anomaly detection for both datasets. Particularly in the case of the credit card dataset, large improvements were achieved"
  - [corpus]: No direct corpus evidence on ROS+MLP interactions for fraud detection.
- **Break condition:** Oversampling causes severe overfitting; minority samples are outliers rather than representative patterns; test distribution differs substantially from oversampled training distribution.

### Mechanism 3
- **Claim:** Generative unsupervised models (GAN, VAE, MO-GAAL) underperform supervised approaches for binary anomaly detection on imbalanced tabular data due to optimization complexity and training instability.
- **Mechanism:** VAE and GAN are trained on majority-class (negative) examples to learn the "normal" distribution; anomalies are detected via reconstruction error or discriminator output. This requires learning an accurate latent representation of normality. Optimization involves multiple interacting networks (generator/discriminator or encoder/decoder), increasing hyperparameter sensitivity and convergence difficulty compared to single-objective supervised learning.
- **Core assumption:** The "normal" class distribution is learnable and sufficiently distinct from anomalies in the learned latent space; reconstruction error correlates with anomaly status.
- **Evidence anchors:**
  - [section 3, Results]: "VAE and GAN show poorer generalization performance for both datasets"
  - [section 4.2, Discussion]: "This could be due to their complexity. In particular, the optimization of the hyperparameters for generative models proved to be more difficult"
  - [section 3, Results]: "MO-GAAL is able to learn and detect anomalies on KDDCUP but not on Credit Card Dataset"
  - [corpus]: No direct corpus validation; corpus papers discuss anomaly detection broadly without comparing generative vs. supervised approaches.
- **Break condition:** Normal behavior is multimodal or temporally shifting; anomalies are close to normal samples in feature space; reconstruction error is not a reliable anomaly proxy.

## Foundational Learning

- **Concept: Class Imbalance and Imbalance Ratio (IR)**
  - **Why needed here:** Both datasets are highly imbalanced (Credit Card IR=577.88, KDDCUP99 IR=4.13), which biases standard classifiers toward majority-class predictions and invalidates accuracy as a metric.
  - **Quick check question:** If a classifier predicts "not fraud" for every transaction on the Credit Card dataset, what accuracy would it achieve? (Answer: ~99.8%, but it detects zero fraud.)

- **Concept: Stratified Sampling in Train/Test Splits**
  - **Why needed here:** The paper uses stratified 80/20 splits to preserve imbalance ratios across partitions; random splits could produce test sets with insufficient minority samples for reliable evaluation.
  - **Quick check question:** Why might a random split fail to preserve the minority-class proportion in small datasets? (Answer: Sampling variance; with few minority samples, random splits may place disproportionate numbers in train or test.)

- **Concept: F1-Score, Precision, and Recall for Imbalanced Evaluation**
  - **Why needed here:** Accuracy is misleading under imbalance; the paper reports Precision, Recall, and F1 to capture the trade-off between detecting anomalies (Recall) and avoiding false alarms (Precision).
  - **Quick check question:** A fraud detector has Recall=0.95 and Precision=0.10. What does this mean operationally? (Answer: It catches 95% of fraud but 90% of its fraud alerts are false positives.)

## Architecture Onboarding

- **Component map:** Raw Data → Preprocessing (encode categorical, StandardScaler) → Stratified 80/20 Split → [Optional: ROS/SPE sampling] → Model Training (XGB/MLP/GAN/VAE/MO-GAAL) → 5-Fold CV Evaluation → Model Selection → [Optional: Imputation robustness test with XGB] → Final Evaluation

- **Critical path:**
  1. Preprocessing consistency (all models receive identically scaled data)
  2. Stratified partitioning (preserves IR for valid cross-dataset comparison)
  3. Baseline XGB training (fastest, strongest performer—establish benchmark early)
  4. Sampling ablation (test ROS/SPE on MLP and XGB separately per dataset)

- **Design tradeoffs:**
  - **XGB vs. MLP:** XGB is faster to train and tune (page 5–6 results); MLP may benefit more from oversampling but requires more hyperparameter effort.
  - **Supervised vs. Unsupervised:** Supervised (XGB/MLP) requires labels but outperforms; unsupervised (VAE/GAN/MO-GAAL) avoids labeling but underperforms and is harder to tune.
  - **IterativeImputer vs. Mean/Median:** IterativeImputer showed no significant F1 improvement but orders-of-magnitude longer runtime on larger datasets (O(k·n·p³·min(n,p)) complexity).
  - **ROS vs. SPE:** ROS improved MLP but had mixed effects on XGB; SPE showed dataset-dependent results and was not extensively tuned.

- **Failure signatures:**
  - **MLP without sampling on Credit Card:** Low precision/recall on minority class due to gradient imbalance.
  - **MO-GAAL on Credit Card:** Near-random detection (figure 2) despite success on KDDCUP99—suggests sensitivity to feature dimensionality or IR.
  - **IterativeImputer on KDDCUP99:** "Enormously high execution times" with no accuracy gain.
  - **SPE on MLP + Credit Card:** Negative performance effect reported—sampling strategy may be mismatched to data characteristics.

- **First 3 experiments:**
  1. **Establish XGB baseline:** Train vanilla XGB (default params) on both datasets with stratified 80/20 split; record Precision, Recall, F1 via 5-fold CV. This confirms reproducibility and provides a reference point.
  2. **MLP + ROS ablation:** Train MLP with and without Random Over-Sampling on the Credit Card dataset only; compare F1 and training time. Expect significant precision improvement per paper findings.
  3. **Imputation stress test:** Inject 30% missing values into KDDCUP99; compare Mean vs. Median vs. IterativeImputer runtime and XGB F1. Confirm IterativeImputer overhead is unacceptable for this data scale.

## Open Questions the Paper Calls Out

- **Question:** How does classifier performance vary across systematically different class imbalance ratios?
  - **Basis in paper:** [explicit] The authors state that "future work could investigate classifier performance on different imbalance ratios."
  - **Why unresolved:** This study only evaluated two specific datasets with fixed imbalance ratios (577.88 and 4.13), limiting generalizability to other imbalance levels.
  - **What evidence would resolve it:** Empirical results from the tested classifiers (XGB, MLP, etc.) applied to datasets with artificially varied imbalance ratios.

- **Question:** To what extent does in-depth hyperparameter tuning improve the effectiveness of Self-Paced Ensemble (SPE) methods?
  - **Basis in paper:** [explicit] The authors note that "no in-depth experiments with different hyperparameters for SPE were conducted" due to time constraints.
  - **Why unresolved:** SPE showed mixed results (negative effects on the credit card dataset), but it is unclear if this was due to the method itself or suboptimal configuration.
  - **What evidence would resolve it:** Performance comparisons showing SPE results after extensive hyperparameter search versus the baseline configurations used in this study.

- **Question:** Can IterativeImputer parameters be optimized to achieve practical execution times on large datasets without sacrificing accuracy?
  - **Basis in paper:** [explicit] The paper notes that IterativeImputer parameters "were not optimized" despite the method showing "enormous processing time" on the KDDCUP99 dataset.
  - **Why unresolved:** While IterativeImputer showed competitive accuracy, it was dismissed for large datasets based on complexity, though it is unknown if tuning could mitigate this cost.
  - **What evidence would resolve it:** Benchmarks comparing computational complexity and imputation quality between default IterativeImputer settings and tuned configurations on large-scale data.

## Limitations

- The study's findings are limited to tabular datasets with specific imbalance ratios (IR=4.13 and IR=577.88); results may not generalize to continuous streams, image-based, or naturally balanced datasets
- Generative model hyperparameters were difficult to optimize, suggesting the performance gap may narrow with extensive tuning
- Missing value imputation analysis was limited to one classifier (XGB) and one small dataset (KDDCUP99)
- The Self-Paced Ensemble results showed mixed effects without clear explanation of when it helps versus harms performance

## Confidence

- **High confidence:** XGB superiority on tabular imbalanced data (supported by multiple result tables and discussion)
- **Medium confidence:** Random Over-Sampling consistently improves MLP performance (supported by Credit Card results but less consistent on KDDCUP99)
- **Medium confidence:** Generative models underperform supervised approaches (consistent across datasets but sensitive to hyperparameter tuning)

## Next Checks

1. Test XGB performance on datasets with different imbalance ratios (IR < 2 and IR > 1000) to establish generalization boundaries
2. Conduct systematic hyperparameter optimization for VAE/GAN models to determine if performance gap is fundamental or tuning-related
3. Evaluate the same model pipeline on a third, independently sourced imbalanced cybersecurity dataset to test external validity