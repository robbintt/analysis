---
ver: rpa2
title: 'PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically
  Grounded Text-to-Audio-Video Generation'
arxiv_id: '2512.23994'
source_url: https://arxiv.org/abs/2512.23994
tags:
- audio
- generation
- physical
- arxiv
- sound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PhyAVBench introduces a physics-sensitivity benchmark for evaluating
  physically grounded text-to-audio-video (T2AV) generation models. The benchmark
  comprises 1,000 groups of paired text prompts with controlled physical variables
  that induce sound variations, enabling fine-grained assessment of models' understanding
  of audio-related physical laws across 6 major audio physics dimensions and 50 test
  points.
---

# PhyAVBench: A Challenging Audio Physics-Sensitivity Benchmark for Physically Grounded Text-to-Audio-Video Generation

## Quick Facts
- **arXiv ID**: 2512.23994
- **Source URL**: https://arxiv.org/abs/2512.23994
- **Reference count**: 40
- **Primary result**: Introduces PhyAVBench, a physics-sensitivity benchmark for T2AV models with 1,000 paired prompts across 6 physics dimensions and 50 test points

## Executive Summary
PhyAVBench introduces a novel benchmark for evaluating physically grounded text-to-audio-video (T2AV) generation models. The benchmark comprises 1,000 groups of paired text prompts with controlled physical variables that induce sound variations, enabling fine-grained assessment of models' understanding of audio-related physical laws. Using Contrastive Physical Response Score (CPRS) and Fine-Grained Alignment Score (FGAS), the benchmark measures both physics sensitivity and audio-video synchronization across 6 major audio physics dimensions and 4 acoustic scenarios.

## Method Summary
The benchmark uses paired prompts differing in exactly one physical variable, with each prompt group containing at least 20 real-world ground-truth videos. CPRS measures directional alignment between generated audio changes and ground-truth physical trends using CAV-MAE Sync embeddings, while FGAS evaluates temporal synchronization through diagonal alignment of video and audio token sequences. The evaluation covers 6 audio physics dimensions with 50 fine-grained test points across music, sound effects, speech, and mixed acoustic scenarios.

## Key Results
- Introduces the Audio-Physics Sensitivity Test (APST) paradigm for evaluating T2AV models
- Establishes CPRS and FGAS metrics for physics direction alignment and temporal synchronization
- Creates benchmark with 1,000 prompt groups covering 6 physics dimensions and 50 test points
- Includes 4 acoustic scenarios (music, SFX, speech, mixed) with ≥20 GT videos per prompt

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Physical Response Score (CPRS)
CPRS measures directional alignment between generated audio changes and ground-truth physical trends by computing mean embeddings of N ground-truth audio samples per prompt using CAV-MAE Sync encoder. The displacement vector between means defines the ground-truth physical direction, compared to generated direction via normalized cosine similarity. This isolates physics understanding from general audio quality.

### Mechanism 2: Controlled-Variable Prompt Pairing
Each test point uses prompt groups where non-target conditions remain invariant, isolating the model's response to the target physical factor. Human experts manually verify and revise prompts to guarantee each paired prompt differs only in a single physical factor, enabling causal attribution of model sensitivity.

### Mechanism 3: Fine-Grained Alignment Score (FGAS)
FGAS measures frame-level cosine similarity between aligned video and audio token sequences, isolating temporal synchronization from semantic relevance. The metric constructs T×T similarity matrix where FGAS is the mean of diagonal elements, representing average per-frame alignment between temporally corresponding tokens.

## Foundational Learning

- **Concept: Contrastive embedding spaces** - Why needed: CPRS depends on embedding directions reflecting physical differences. Quick check: Given two audio samples differing in material, would their embeddings differ primarily along a single direction?
- **Concept: Controlled-variable experimental design** - Why needed: The APST paradigm assumes varying one factor while holding others constant isolates causal relationships. Quick check: What non-material factors might confound comparing "hammer hitting metal" vs "hammer hitting wood"?
- **Concept: Temporal cross-modal alignment** - Why needed: FGAS assumes frame-level alignment is correct granularity for audio-video synchronization. Quick check: Would FGAS detect a 100ms audio latency in a drum strike video?

## Architecture Onboarding

- **Component map**: Benchmark dataset (1,000 prompt groups × 20+ GT videos) -> CAV-MAE Sync encoder (feature extraction) -> Metrics suite (CPRS, FGAS, CLAP/CLIPSIM, FAD/FVD/KVD, WER, PR-MOS)
- **Critical path**: 1) Load prompt group pairs from benchmark 2) Generate outputs from target model 3) Extract audio embeddings via CAV-MAE Sync 4) Compute v_GT from N ground-truth samples 5) Compute v_gen from generated samples 6) Calculate CPRS via cosine similarity
- **Design tradeoffs**: CPRS prioritizes directional correctness over absolute fidelity; 20+ GT videos reduces noise but increases collection cost; paired comparison design prevents evaluating single-prompt physics consistency
- **Failure signatures**: CPRS ≈ 0.5 indicates orthogonal changes (random or non-physical cues); CPRS ≈ 0.0 indicates inverted physics; high FGAS with low CPRS indicates good synchronization but physics-agnostic generation
- **First 3 experiments**: 1) Baseline CPRS on open-source T2AV models to identify weakest physics dimensions 2) Ablation on embedding encoder to test metric sensitivity 3) Confounded prompt stress test to test text statistics vs physics exploitation

## Open Questions the Paper Calls Out

### Open Question 1
Does CPRS correlate strongly with human judgments of physical plausibility (PR-MOS), and can it serve as a reliable automated proxy for human evaluation? The paper introduces both CPRS and PR-MOS but provides no validation of their correlation since comprehensive model evaluation will be provided in a future release.

### Open Question 2
How do current commercial and academic T2AV models perform across the six audio-physics dimensions, and are certain physical principles systematically harder than others? The benchmark hierarchy is designed for fine-grained diagnosis, but without model results, it's unknown which physical principles current architectures fundamentally struggle with.

### Open Question 3
Can models that perform well on controlled variable comparisons in PhyAVBench generalize to more complex, multi-variable physical scenarios where multiple acoustic conditions change simultaneously? The paired-prompt design isolates single physical variables, but real-world scenarios typically involve multiple interacting physical factors.

## Limitations
- Metric validity depends on CAV-MAE Sync embeddings capturing physically meaningful audio feature variations
- Text prompts may not consistently specify all relevant physical variables, creating implicit confounders
- Benchmark evaluates physics understanding without testing generalization across different T2AV architectures

## Confidence

**High Confidence**: Controlled-variable design and use of real-world ground-truth videos for physics direction establishment; CPRS calculation methodology is mathematically sound

**Medium Confidence**: Claim that CPRS isolates physics understanding from general audio quality; metric design supports this but validation requires additional empirical verification

**Low Confidence**: Assertions about current state-of-the-art T2AV models having poor physics understanding based on preliminary results; comprehensive evaluation across multiple models is pending

## Next Checks

1. **Embedding Space Validation**: Test CPRS robustness by swapping CAV-MAE Sync encoder with alternative audio encoders (CLAP audio encoder, AudioMAE) to determine if rankings change significantly

2. **Controlled Confounder Introduction**: Systematically introduce linguistic confounders into prompt pairs (material-associated adjectives) and measure whether CPRS scores correlate with textual cues rather than physical properties

3. **Cross-Architecture Physics Transfer**: Evaluate CPRS across multiple T2AV architectures (diffusion-based, autoregressive, hybrid) on identical prompt pairs to compare architecture-specific weaknesses