---
ver: rpa2
title: 'Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language
  Models'
arxiv_id: '2507.15868'
source_url: https://arxiv.org/abs/2507.15868
tags:
- prompt
- jargon
- masked
- each
- anchor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates whether large language models (LLMs) can\
  \ distinguish between harmless prompt noise and meaning-altering edits when solving\
  \ coding tasks. The authors compiled 50 LeetCode problems and applied three perturbation\
  \ regimes\u2014progressive underspecification (deleting 10% of tokens per step),\
  \ lexical flip (swapping pivotal quantifiers like \"max\" to \"min\"), and jargon\
  \ inflation (replacing common nouns with obscure technical synonyms)\u2014to probe\
  \ LLM sensitivity."
---

# Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models

## Quick Facts
- arXiv ID: 2507.15868
- Source URL: https://arxiv.org/abs/2507.15868
- Reference count: 13
- Models show over-robustness to prompt noise but fail to detect meaning-altering edits in coding tasks

## Executive Summary
This study investigates whether large language models can distinguish between harmless prompt noise and meaning-altering edits when solving coding tasks. The authors compiled 50 LeetCode problems and applied three perturbation regimes—progressive underspecification, lexical flip, and jargon inflation—to probe LLM sensitivity. Across 11,853 generations, models showed extreme over-robustness to noise but only 54% adapted to a single quantifier flip that changed task semantics. The study reveals that current LLMs blur the line between benign and meaning-changing edits, often treating both as ignorable.

## Method Summary
The authors compiled 50 LeetCode problems from a dataset of 2,869 problems, stripping examples and constraints to create canonical specifications. They applied three perturbation families (progressive deletion, lexical flip, jargon inflation) across 10 cumulative steps in two identifier modes (masked/unmasked function names). Six frontier models including reasoning variants generated Python solutions that were executed against original LeetCode test suites. Pass rates, semantic drift in chain-of-thought, and inter-model agreement were analyzed to assess differential sensitivity to perturbations.

## Key Results
- Models showed extreme over-robustness to noise, maintaining 85% accuracy even after losing 90% of the prompt
- Only 54% of models adapted to a single quantifier flip (max→min) that changed task semantics
- Jargon inflation yielded a 56% pass-through rate, revealing difficulty in judging contextual importance
- Reasoning variants were less sensitive than baselines, producing longer rationales that drifted semantically while code remained unchanged

## Why This Works (Mechanism)

### Mechanism 1: Identifier Anchoring Dominates Task Inference
A single descriptive function name acts as a primary "compass" for LLMs, overriding other prompt signals and determining solution generation. The LLM's attention mechanism privileges the function name as a salient anchor, facilitating template matching to known coding patterns. Masking it with a neutral token (solved) cuts pass-rates by 15–21 pp.

### Mechanism 2: Template Regression Under Semantic Drift
LLMs regress to canonical solutions when prompts are obscured or inflated with jargon, ignoring subtle semantic changes in favor of high-probability patterns. When faced with jargon inflation or massive token deletion, models map unfamiliar terms back to the nearest high-probability interpretation in their training distribution, maintaining functionally identical code to the baseline solution.

### Mechanism 3: Reasoning Variants Add Rhetorical Flexibility, Not Algorithmic Adaptation
Reasoning-optimized endpoints produce longer, more varied chain-of-thought explanations but do not improve sensitivity to semantic drift. The chain-of-thought may drift semantically (cosine distance ~0.35) as jargon accumulates, but the underlying code remains unchanged. The rhetorical flexibility masks a lack of genuine comprehension.

## Foundational Learning

- **Template Matching vs. Semantic Parsing**: Why needed - The paper's central finding is that LLMs often fail to distinguish between benign noise and meaning-altering edits because they rely on template matching rather than deep semantic understanding. Quick check - When an LLM solves a coding problem, is it parsing the full specification or matching keywords to known solution patterns?

- **Perturbation Types in NLP**: Why needed - The study uses three distinct perturbation families (deletion, lexical flip, jargon) to probe different aspects of robustness; understanding these categories is essential for interpreting the results. Quick check - Which perturbation type is most likely to reveal a model's inability to detect critical semantic changes?

- **Evaluation Oracle Limitations**: Why needed - The paper acknowledges that passing the original test suite demonstrates compatibility with the original task, not correctness for the mutated one, which complicates causal attribution. Quick check - If a model passes the original tests after a quantifier flip, does it mean it adapted correctly or regressed to the baseline?

## Architecture Onboarding

- **Component map**: LeetCodeDataset -> Pre-processor (clean/ canonicalize) -> Perturbation Engine (deletion/flip/jargon) -> Model Interface (6 LLMs) -> Execution Engine (run against test suites) -> Analysis Layer (pass-rates/semantic drift)

- **Critical path**: 1) Clean and canonicalize 50 LeetCode problems (remove examples, constraints) 2) Apply perturbations across 10 cumulative steps per family 3) Generate solutions with masked/unmasked identifiers 4) Execute code against original tests; log pass/fail and reasoning traces 5) Analyze pass-rates, consensus, and semantic drift

- **Design tradeoffs**: Using original tests is efficient but conflates regression with adaptation; creating mutated test suites would be more precise but labor-intensive. Code-only enforcement may underestimate pragmatic robustness but ensures consistent evaluation. Limited reasoning variant sampling potentially misses unmasked behaviors.

- **Failure signatures**: Over-robustness - high pass-rates (>80%) under progressive deletion indicate template reliance. Insensitivity - low adaptation rates (<55%) to lexical flips reveal semantic blindness. Rhetorical drift - high cosine distance in reasoning traces without code changes signals pseudo-adaptation.

- **First 3 experiments**: 1) Baseline Anchor Test - Run all models on unmodified prompts with masked/unmasked identifiers 2) Perturbation Sweep - Apply all three perturbation families at step 1 only 3) Reasoning Trace Audit - Manually compare chain-of-thought content to code changes on lexical-flip cases

## Open Questions the Paper Calls Out

- Do the observed "robustness regimes" persist in non-coding domains like mathematical reasoning or multimodal tasks? The study is restricted to coding problems; domain-diverse corpora might expose different failure modes.

- How does evaluation against a mutated ground truth change the assessment of model "insensitivity"? The current method conflates template regression with valid adaptation; fresh ground-truth outputs would tighten causal attributions.

- To what extent does removing the "code-only" enforcement constraint allow models to recover "pragmatic robustness" via clarification or refusal? The protocol suppresses natural behavior where models might ask clarifying questions or refuse, potentially underestimating their ability to handle ambiguity safely.

- Can "anchor randomization" or "token-aligned rewards" in training successfully mitigate reliance on descriptive function names? The paper advocates these as solutions but does not validate them experimentally.

## Limitations
- Reliance on original test suites cannot distinguish template regression from valid adaptation
- Reasoning variant analysis is incomplete (only masked conditions tested)
- Perturbations were generated via meta-prompt rather than systematic vocabulary mapping
- Limited sample size (50 problems) from a specific domain (coding)

## Confidence

- **High confidence**: Models show extreme over-robustness to token deletion (85% pass at 90% prompt loss), and single quantifier flips yield only 54% adaptation
- **Medium confidence**: Identifier anchoring dominates task inference, though not fully isolated from other confounds
- **Low confidence**: Reasoning variants are less sensitive than baselines, based on partial data (masked tracks only)

## Next Checks

1. **Oracle Expansion**: Generate mutated test suites for a subset of lexical-flip cases and rerun executions to distinguish template regression from valid adaptation

2. **Reasoning Trace Audit**: For reasoning variants, manually annotate chain-of-thought vs. code changes on a stratified sample of semantic flips to quantify the rhetorical-algorithmic mismatch

3. **Anchor Ablation**: Systematically mask other high-salience tokens (class names, problem signatures) in addition to function names to isolate the true impact of identifier anchoring on template matching