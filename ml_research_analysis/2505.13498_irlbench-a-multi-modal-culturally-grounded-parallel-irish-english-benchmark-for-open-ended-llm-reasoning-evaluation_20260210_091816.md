---
ver: rpa2
title: 'IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark
  for Open-Ended LLM Reasoning Evaluation'
arxiv_id: '2505.13498'
source_url: https://arxiv.org/abs/2505.13498
tags:
- irish
- language
- irlbench
- multilingual
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IRLBench is a novel multilingual, multimodal benchmark for open-ended
  reasoning evaluation, presented in parallel English and Irish, the latter classified
  as definitely endangered by UNESCO. Inspired by educational exam data, IRLBench
  contains 1,700 question-marking scheme pairs from 12 subjects across Science, Applied
  Science, Business Studies, and Social Studies.
---

# IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation

## Quick Facts
- arXiv ID: 2505.13498
- Source URL: https://arxiv.org/abs/2505.13498
- Authors: Khanh-Tung Tran; Barry O'Sullivan; Hoang D. Nguyen
- Reference count: 38
- Best model achieves 76.2% accuracy in English but only 55.8% in Irish

## Executive Summary
IRLBench is a novel multilingual, multimodal benchmark for open-ended reasoning evaluation, presented in parallel English and Irish, the latter classified as definitely endangered by UNESCO. Inspired by educational exam data, IRLBench contains 1,700 question-marking scheme pairs from 12 subjects across Science, Applied Science, Business Studies, and Social Studies. Unlike multiple-choice benchmarks, it assesses generative reasoning through long-form responses, using LLM-as-a-judge for evaluation against official marking schemes. Extensive experiments on leading closed- and open-source models reveal a significant performance gap: the best model achieves 76.2% accuracy in English but only 55.8% in Irish, and produces valid Irish responses less than 80% of the time. These results underscore the challenges of open-ended reasoning in extremely low-resource languages and highlight the importance of culturally grounded, multimodal evaluation. IRLBench is released to enable future research on robust, multilingual AI development.

## Method Summary
IRLBench extracts 1,700 question-marking scheme pairs from 2024 Irish Leaving Certificate Higher Level exam papers across 12 subjects, using a vision-language model for PDF processing followed by automated verification and human annotation. The benchmark presents identical exam content in parallel English-Irish versions to enable fine-grained analysis of model capabilities across domains. Evaluation employs an LLM-as-a-judge paradigm, where gemini-2.5-flash assesses long-form responses against official marking schemes using binary correctness labels. Language fidelity is measured through sentence-level detection with FastText, classifying responses as valid Irish only if >50% of sentences are detected as Irish.

## Key Results
- Best model (o4-mini) achieves 76.2% accuracy in English but only 55.8% in Irish
- Most evaluated models produce valid Irish responses less than 80% of the time
- 18.11% of questions include multimodal content (images/figures)
- Significant performance degradation observed in low-resource language setting

## Why This Works (Mechanism)

### Mechanism 1: Parallel Bilingual Evaluation for Isolating Language Transfer Effects
- **Claim:** Presenting identical exam content in parallel English-Irish versions enables direct measurement of multilingual transfer degradation.
- **Mechanism:** By holding task content constant and varying only the language, performance differences can be attributed to language-handling capability rather than domain knowledge gaps.
- **Core assumption:** Performance gaps reflect language transfer failures rather than content difficulty differences.
- **Evidence anchors:** Parallel exam versions with identical content from Irish Leaving Certificate; comparison with multilingual benchmarks emphasizing cultural grounding.
- **Break condition:** If translated content introduces systematic difficulty differences, observed gaps would conflate language transfer with translation quality.

### Mechanism 2: LLM-as-Judge with Official Marking Schemes for Open-Ended Evaluation
- **Claim:** Binary correctness judgments from instruction-tuned LLMs, when grounded in official marking schemes, provide reliable evaluation of long-form responses where traditional metrics fail.
- **Mechanism:** Traditional metrics like BLEU/ROUGE cannot handle semantic equivalence across varied phrasings. By providing the judge model with question, marking criteria, and candidate response, the LLM's instruction-following and reasoning capabilities are leveraged to assess semantic correctness.
- **Core assumption:** The judge LLM's binary classification aligns with human evaluator judgments and captures the essence of marking scheme requirements.
- **Evidence anchors:** LLM-as-judge approaches referenced in neighbor papers; binary labeling to reduce variability.
- **Break condition:** If the judge model exhibits systematic bias or fails on edge cases in the marking scheme, evaluation reliability degrades.

### Mechanism 3: Sentence-Level Language Identification for Fidelity Measurement
- **Claim:** Threshold-based language detection at the sentence level reveals models' generative language fidelity that multiple-choice benchmarks cannot capture.
- **Mechanism:** FastText language identification classifies each sentence. If >50% of sentences are detected as English when Irish was requested, the response is classified as non-Irish.
- **Core assumption:** The 50% threshold meaningfully distinguishes language fidelity, and FastText detection is sufficiently accurate for Irish.
- **Evidence anchors:** FastText used for sentence-level detection with 50% threshold; models producing <80% valid Irish responses.
- **Break condition:** If FastText misclassifies valid Irish sentences, fidelity estimates become unreliable.

## Foundational Learning

- **Concept: Low-Resource Language Challenges in LLMs**
  - **Why needed here:** Irish is classified as "definitely endangered" by UNESCO with limited training data. Models exhibit both accuracy degradation (20%+ gap) and language fidelity failures (<80% valid Irish generation).
  - **Quick check question:** Can you explain why a model might achieve high accuracy on English questions but fail to even respond in Irish when the same question is presented in Irish?

- **Concept: Open-Ended vs. Multiple-Choice Benchmark Trade-offs**
  - **Why needed here:** IRLBench deliberately avoids multiple-choice to eliminate memorization shortcuts and test genuine generative reasoning. This introduces evaluation complexity requiring LLM-as-judge.
  - **Quick check question:** Why would a model that performs well on multiple-choice Irish benchmarks struggle with open-ended generation in the same language?

- **Concept: LLM-as-Judge Evaluation Paradigm**
  - **Why needed here:** The benchmark relies entirely on gemini-2.5-flash for correctness evaluation. Understanding the strengths and failure modes of this approach is critical for interpreting results.
  - **Quick check question:** What are two potential failure modes when using an LLM to evaluate another LLM's open-ended responses?

## Architecture Onboarding

- **Component map:** PDF exam papers -> VLM (gemini-2.0-flash) extraction -> Automated verification -> Human annotation -> Question-Marking Scheme pairs -> LLM-as-judge evaluation (gemini-2.5-flash) + FastText language detection

- **Critical path:** PDF processing via VLM extraction (primary bottleneck for data quality) -> Human verification for pairing accuracy (catches VLM extraction errors) -> Judge model evaluation (determines benchmark scores) -> Language detection (filters invalid responses before accuracy computation)

- **Design tradeoffs:** Binary correctness vs. granular scoring (simplifies evaluation but loses partial credit information) -> Single judge model (reduces complexity but introduces model-specific biases) -> 50% sentence threshold for language fidelity (pragmatic but arbitrary) -> Higher-level exams only (maximizes challenge but limits accessibility)

- **Failure signatures:** Low language fidelity (<80%) with moderate accuracy indicates models are answering in English despite Irish prompts -> High confidence (>80%) with low accuracy (<60%) on Irish split indicates miscalibration from training on high-resource languages -> Subject-specific gaps may indicate domain-specific multilingual transfer weaknesses

- **First 3 experiments:** Baseline establishment: Run target model on both English and Irish splits; compute accuracy gap and language fidelity. Judge model sensitivity analysis: Evaluate subset with alternative judge models to assess evaluation variance. Language fidelity threshold analysis: Vary FastText threshold (25%, 50%, 75%) to understand sensitivity of fidelity measurements.

## Open Questions the Paper Calls Out

- **Question 1:** Does the binary LLM-as-a-judge evaluation methodology robustly capture nuanced errors in low-resource languages like Irish?
  - **Basis in paper:** Listed as a limitation that "may present challenges in scalability and robustness"
  - **Why unresolved:** Simplifies complex marking schemes into binary labels, potentially obscuring partial understanding or specific linguistic failures unique to endangered languages
  - **What evidence would resolve it:** Comparative study measuring agreement rate between automated binary judge and human expert graders on Irish response subset

- **Question 2:** Do the significant performance gaps found in Irish generalize to other extremely low-resource languages?
  - **Basis in paper:** Limitations section notes benchmark "currently addresses a single extremely low-resource language scenario (Irish)"
  - **Why unresolved:** Irish has specific Celtic syntactic structures; unverified whether ~20% performance drop is universal or specific to English-Irish transfer
  - **What evidence would resolve it:** Applying IRLBench framework to parallel exam data in other endangered languages (e.g., Welsh or Maltese)

- **Question 3:** What is the causal mechanism driving the correlation between reasoning correctness and language fidelity?
  - **Basis in paper:** Error analysis notes "clear correlation between correctness and capability to generate outputs in Irish" but doesn't explain underlying cause
  - **Why unresolved:** Unclear if models fail because they cannot reason in Irish, or if they default to English reasoning and subsequently fail to translate
  - **What evidence would resolve it:** Ablation study analyzing intermediate reasoning traces to determine if model "thinks" in English before generating in Irish

## Limitations

- Evaluation reliability depends on single LLM-as-judge model without comparative validation against human graders
- 50% sentence-level threshold for Irish language detection is arbitrary and FastText may be unreliable for endangered language
- Parallel evaluation assumes identical difficulty between English and Irish exam versions, but translation differences could introduce systematic performance gaps

## Confidence

- **High Confidence:** Benchmark construction methodology is well-specified and reproducible; reported performance gaps are internally consistent
- **Medium Confidence:** Open-ended generation challenges beyond multiple-choice formats are supported by language fidelity failures but need human validation
- **Low Confidence:** Interpretation that performance gaps solely reflect language transfer limitations lacks sufficient validation against translation quality issues

## Next Checks

1. **Human Validation Subset:** Select 50 randomly sampled responses from both English and Irish splits and have bilingual subject matter experts grade them independently. Compare human accuracy scores with gemini-2.5-flash judgments to quantify evaluation bias.

2. **Judge Model Sensitivity Analysis:** Evaluate the same response set using alternative LLM-as-judge models (e.g., gpt-4.1, Claude-3.5-Sonnet) and compute inter-judge agreement rates. Identify cases where judges disagree to understand evaluation variance.

3. **Language Fidelity Threshold Sensitivity:** Re-run the fidelity analysis with multiple thresholds (25%, 50%, 75%) and examine how accuracy calculations change. Identify specific responses that flip between valid/invalid classifications to understand threshold sensitivity.