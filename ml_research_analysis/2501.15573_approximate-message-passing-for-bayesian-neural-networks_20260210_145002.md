---
ver: rpa2
title: Approximate Message Passing for Bayesian Neural Networks
arxiv_id: '2501.15573'
source_url: https://arxiv.org/abs/2501.15573
tags:
- message
- messages
- training
- factor
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a message-passing framework for Bayesian neural
  networks that avoids double-counting training data and handles convolutional architectures.
  By modeling the predictive posterior as a factor graph with Gaussian message approximations,
  the method achieves competitive performance with state-of-the-art baselines on CIFAR-10,
  showing an edge in calibration (ECE of 0.029 versus 0.041-0.046 for AdamW and IVON).
---

# Approximate Message Passing for Bayesian Neural Networks

## Quick Facts
- arXiv ID: 2501.15573
- Source URL: https://arxiv.org/abs/2501.15573
- Reference count: 40
- Primary result: Competitive ECE of 0.029 vs 0.041-0.046 for baselines on CIFAR-10

## Executive Summary
This paper introduces a message-passing framework for Bayesian neural networks that operates on factor graphs with Gaussian message approximations. The method avoids double-counting training data by carefully managing aggregated messages across batches, using a periodic marginal recomputation strategy. The framework achieves competitive accuracy and significantly better calibration than standard training methods, with an ECE of 0.029 on CIFAR-10 compared to 0.041-0.046 for AdamW and IVON. The approach scales to large models (5.6M parameters) but remains slower than variational inference during training.

## Method Summary
The method constructs a factor graph where each neuron and layer represents a factor node connected to incoming and outgoing message edges. Messages are represented in natural parameters (precision-weighted mean and precision) and updated through loopy belief propagation. Key innovations include a batching strategy that prevents double-counting by dividing out old batch messages and multiplying in new ones, GPU-optimized factor graph implementation using Julia/Tullio, and careful prior initialization with a specific variance formula. The framework handles convolutional architectures and uses an Argmax factor for classification. During training, the model iterates on individual examples within batches while maintaining aggregated messages outside the active batch.

## Key Results
- Achieves ECE of 0.029 on CIFAR-10 versus 0.041-0.046 for AdamW and IVON
- Shows strong correlation (0.9) between posterior credible intervals and coverage rates on synthetic data
- Maintains competitive accuracy while improving calibration on CIFAR-10
- Scales to MLPs with 5.6M parameters but is 100x slower than VI during training

## Why This Works (Mechanism)
The framework works by decomposing the Bayesian inference problem into a factor graph where messages flow between nodes representing neurons and layers. Gaussian message approximations enable tractable computation while maintaining uncertainty quantification. The key innovation is the batching strategy that prevents double-counting of training data by maintaining separate aggregated messages for each batch. This allows the model to incorporate uncertainty about each training example exactly once per epoch while still benefiting from batch processing efficiency. The Argmax factor for classification ensures proper handling of discrete label distributions.

## Foundational Learning
- **Factor Graphs**: Bipartite graphs with variable nodes and factor nodes; needed to decompose complex probability distributions into tractable computations; quick check: verify factor graph construction for a simple MLP
- **Gaussian Message Passing**: Natural parameter representation (τ, ρ) for efficient computation; needed to maintain tractable Gaussian approximations; quick check: verify message multiplication and division operations
- **LeakyReLU Moment Matching**: Approximating non-linearities through moment matching; needed to handle activation functions in message passing; quick check: validate LeakyReLU message computation with edge cases
- **Batching with Aggregated Messages**: Dividing out old batch messages before multiplying new ones; needed to prevent double-counting training data; quick check: verify message accumulation prevents overcounting
- **Natural Parameters**: τ = μ/σ², ρ = 1/σ² representation; needed for efficient Gaussian message operations; quick check: confirm conversion between moment and natural parameters
- **Prior Initialization Formula**: σ²p = (1.5 - 0.8041·min(1,d₂/d₁)^0.8041 + 0.4496·d₁); needed for numerical stability across different layer sizes; quick check: apply formula to various layer dimensions

## Architecture Onboarding
**Component Map**: Input Data -> Convolutional Layers -> Linear Layers -> Argmax Factor -> Output Predictions
**Critical Path**: Forward message propagation through Conv/Linear layers → Argmax factor computation → Backward message propagation → Message aggregation and damping
**Design Tradeoffs**: Batch processing for efficiency vs. individual example iteration for accuracy; Gaussian approximations for tractability vs. potential loss of expressiveness; message damping for stability vs. slower convergence
**Failure Signatures**: Variance explosion in forward messages; LeakyReLU messages diverging to NaN/Inf; overconfident predictions indicating double-counting
**First Experiments**: 1) Verify message passing works on a simple 2-layer MLP with synthetic data; 2) Test batching logic on a single convolutional layer; 3) Validate LeakyReLU message computations with edge-case inputs

## Open Questions the Paper Calls Out
1. Can the message-passing framework be extended to efficiently model residual connections and transformer attention mechanisms?
2. Can a CUDA C++ implementation with batched computations close the training speed gap with variational inference methods?
3. Does iterating on individual examples rather than batches reduce GPU memory footprints without reintroducing double-counting errors?

## Limitations
- Training is two orders of magnitude slower than VI methods despite GPU optimization
- Current framework cannot handle normalization layers, residual connections, or transformer architectures
- Missing precise specifications for iteration-per-batch schedule and message damping coefficients

## Confidence
- **High**: Factor graph formulation and message operations are well-defined
- **Medium**: Achieving CIFAR-10 results depends on unspecified batching schedules and damping parameters
- **High**: Synthetic data coverage claims rely on provided prior variance formula

## Next Checks
1. Verify batching logic prevents double-counting by logging message accumulation per batch
2. Test prior variance formula across layer sizes to ensure numerical stability
3. Validate LeakyReLU message computation with edge-case inputs (small/large values) to confirm guardrail behavior