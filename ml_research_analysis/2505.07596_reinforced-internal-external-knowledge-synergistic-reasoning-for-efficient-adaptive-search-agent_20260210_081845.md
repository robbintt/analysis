---
ver: rpa2
title: Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient
  Adaptive Search Agent
arxiv_id: '2505.07596'
source_url: https://arxiv.org/abs/2505.07596
tags:
- knowledge
- answer
- search
- should
- internal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of current search agents that
  over-rely on external retrieval and underutilize their intrinsic parametric knowledge,
  leading to redundant retrievals and potential knowledge conflicts. The proposed
  Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA) enables
  LLMs to delineate their knowledge boundaries and prioritize internal parametric
  knowledge before resorting to external retrieval.
---

# Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent

## Quick Facts
- **arXiv ID**: 2505.07596
- **Source URL**: https://arxiv.org/abs/2505.07596
- **Reference count**: 40
- **Primary result**: IKEA reduces retrieval frequency while improving performance on knowledge-intensive reasoning tasks by teaching LLMs to leverage internal parametric knowledge before external retrieval

## Executive Summary
This paper addresses the limitations of current search agents that over-rely on external retrieval and underutilize their intrinsic parametric knowledge, leading to redundant retrievals and potential knowledge conflicts. The proposed Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA) enables LLMs to delineate their knowledge boundaries and prioritize internal parametric knowledge before resorting to external retrieval. IKEA achieves this through a novel knowledge-boundary aware reward function and training dataset construction that incentivizes correct answers while minimizing unnecessary retrievals. Evaluations across multiple knowledge-intensive reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reducing retrieval frequency while improving performance, and exhibits strong generalization capabilities on out-of-distribution datasets.

## Method Summary
IKEA is a reinforcement learning-based agent that learns to balance internal parametric knowledge with external retrieval. The method uses Group Relative Policy Optimization (GRPO) with a custom reward function that penalizes unnecessary retrievals while encouraging correct answers. The training dataset is constructed by probing the base model's knowledge boundaries to create a 1:1 ratio of internally answerable ("easy") and externally required ("hard") questions. During training, the agent receives rewards for correct answers and additional bonuses/penalties based on retrieval efficiency, encouraging it to first attempt answers using internal knowledge before resorting to external search.

## Key Results
- IKEA achieves 37.10% EM on NQ and 31.44% EM on HotpotQA hard subsets, outperforming baselines by 4.52% and 4.88% respectively
- Retrieval frequency is significantly reduced to 1.07 on NQ (vs 1.86 for w/o rkb), demonstrating efficient knowledge utilization
- Strong generalization demonstrated on CIFAR-QA with 27.56% EM, showing effectiveness on out-of-distribution datasets

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Boundary Aware Reward Shaping
The reward function combines answer correctness (rans) with knowledge-boundary bonuses (rkb+) that scale inversely with retrieval count when correct, plus small penalties (rkb−) for retrieving when incorrect. This creates a gradient where correct-without-retrieval > correct-with-retrieval > incorrect-with-retrieval > incorrect-without-retrieval. Evidence shows removing rkb causes retrieval to spike to 1.86 (vs 1.07), while removing rkb− drops EM on hard subsets from 31.44% to 28.91%.

### Mechanism 2: Balanced Difficulty Dataset for Synergistic Learning
Training on a 1:1 mix of internally-answerable and retrieval-required questions prevents model collapse to a single strategy. Dataset construction uses N samples per question—if any sample is correct, it's labeled "easy" (internal); otherwise "hard" (external). Training on both equally forces the model to learn the decision boundary. Evidence shows easy-only training collapses retrieval to 0.49 and drops hard EM to 21.88%; hard-only training spikes retrieval to 1.44 with degraded easy performance.

### Mechanism 3: GRPO for Memory-Efficient Multi-Turn RL
Group Relative Policy Optimization enables training without a separate value model by using group-relative rewards as advantage estimates. Instead of PPO's learned value function, GRPO samples G trajectories per task, computes mean (μr) and std (σr) of rewards, and uses normalized deviation as advantage: Âτi = (ri - μr) / σr. This eliminates ~50% memory overhead while maintaining policy improvement signal.

## Foundational Learning

- **Parametric vs. Retrieved Knowledge Distinction**
  - Why needed here: The entire method hinges on the model learning when its internal parameters contain sufficient knowledge vs. when external retrieval is needed
  - Quick check question: Given a question about a 2024 event, can you explain why a model trained only on pre-2024 data would need retrieval?

- **Reward Shaping in RL**
  - Why needed here: The rkb+/rkb− design is a classic reward shaping problem—without careful balancing, the model exploits the reward in unintended ways
  - Quick check question: If you set rkb− = 0.5 (same magnitude as rkb+), what behavior would you expect during training?

- **Multi-Turn Agent RL with Tool Use**
  - Why needed here: IKEA operates over N rounds where each action can be reasoning, search, or answer—understanding the trajectory structure is essential
  - Quick check question: In the trajectory τ = (t, a0, o1, ..., aN-1, oN, r), which tokens have loss masked and why?

## Architecture Onboarding

- **Component map**: Agent Prompt Template -> Environment (Search Engine + Retriever) -> Reward Model -> Training Loop (GRPO)
- **Critical path**: 1) Dataset probing → Qeasy/Qhard labels → balanced training set; 2) Rollout generation → trajectory collection → group reward normalization; 3) Policy update via clipped surrogate loss with KL penalty
- **Design tradeoffs**: rkb+ = 0.6, rkb− = 0.05 (asymmetric penalties); RTmax = 3 (limits latency but may fail on complex multi-hop questions); G = 16 rollouts (more improve advantage estimation but increase compute linearly)
- **Failure signatures**: Retrieval count converging to 0 (rkb− too high or training on easy-only data); Retrieval count staying high with low EM (rkb not properly weighted, or probing failed to identify hard questions); Format violations (initial model lacks instruction-following)
- **First 3 experiments**: 1) Reproduce Table 2 ablation: train IKEA w/o rkb and w/o rkb− to verify reward component contributions on a small dataset slice; 2) Probe knowledge boundary with different sampling temperatures: test if N=1 vs N=10 samples changes Qeasy/Qhard classification quality; 3) Vary easy:hard ratio from 1:3 to 3:1 to find the dataset balance point for your target domain

## Open Questions the Paper Calls Out

- **Open Question 1**: Can dynamic knowledge boundary learning methods be developed to replace the current reliance on static, pre-probed datasets? The current method depends on a fixed pre-training dataset classification (Easy vs. Hard), which cannot adapt to changes in the model's parametric knowledge or context drift over time.

- **Open Question 2**: Does the reliance on specific dataset construction limit the model's universal applicability across diverse task types? The paper evaluates primarily on standard knowledge reasoning tasks; it is unclear if the "Easy/Hard" binary classification works for tasks requiring complex reasoning where knowledge boundaries are ambiguous.

- **Open Question 3**: How robust is the training process against noise in the initial knowledge boundary probing phase? If ICL fails to elicit existing internal knowledge (false negative), the model is trained on mislabeled data, potentially reinforcing unnecessary retrieval or confusing reward signals.

## Limitations
- Knowledge boundary probing methodology may be unreliable if probing model's capabilities don't match training model's
- Asymmetric reward weights (rkb+ = 0.6 vs rkb− = 0.05) may not generalize across domains with different knowledge distribution characteristics
- GRPO implementation details remain partially unspecified, potentially affecting reproducibility

## Confidence

**High Confidence**: Core mechanism of knowledge-boundary aware reward shaping (rkb+ and rkb−) is well-supported by ablation studies showing clear degradation when removing either component.

**Medium Confidence**: Dataset construction methodology and its impact on learning synergistic reasoning is reasonably supported but has gaps regarding probing methodology reliability.

**Low Confidence**: Generalization claims to out-of-distribution datasets and specific advantages of GRPO over PPO require more validation, with CIFAR-QA results showing modest performance rather than robust transfer.

## Next Checks

1. **Probe Methodology Sensitivity Analysis**: Systematically vary the number of ICL samples (N=1, 5, 10) during dataset construction and measure how Qeasy/Qhard classification accuracy changes to validate the 1:1 ratio claim.

2. **Reward Weight Robustness Test**: Run ablation experiments with rkb+ values ranging from 0.3 to 0.9 (keeping rkb− = 0.05) to identify the optimal balance point and test asymmetric weighting claims.

3. **Direct GRPO vs PPO Comparison**: Implement the same IKEA training pipeline using standard PPO with value function estimation instead of GRPO to quantify claimed memory efficiency and convergence benefits.