---
ver: rpa2
title: Brain-language fusion enables interactive neural readout and in-silico experimentation
arxiv_id: '2509.23941'
source_url: https://arxiv.org/abs/2509.23941
tags:
- image
- white
- sitting
- cortext
- describe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CorText introduces a brain-language fusion framework that maps
  neural activity into the latent space of large language models, enabling open-ended
  interaction with brain data. The method uses region-wise brain tokenizers to translate
  fMRI responses into language-compatible embeddings, which are then processed by
  a frozen LLM with additional fine-tuning for cross-modal alignment.
---

# Brain-language fusion enables interactive neural readout and in-silico experimentation

## Quick Facts
- arXiv ID: 2509.23941
- Source URL: https://arxiv.org/abs/2509.23941
- Reference count: 40
- Primary result: CorText maps brain activity into LLM space, achieving CLIPScore of 0.647 for fMRI-based image captioning

## Executive Summary
CorText introduces a brain-language fusion framework that maps neural activity into the latent space of large language models, enabling open-ended interaction with brain data. The method uses region-wise brain tokenizers to translate fMRI responses into language-compatible embeddings, which are then processed by a frozen LLM with additional fine-tuning for cross-modal alignment. Applied to a 7T fMRI dataset of natural scene viewing, CorText generates accurate image captions from brain activity alone, achieving CLIPScore of 0.647, significantly outperforming shuffled-control baselines (0.376). It also answers detailed visual questions beyond the training scope and generalizes to novel semantic categories in zero-shot settings. In-silico microstimulation experiments show systematic, graded effects on semantic content in generated captions, demonstrating the causal mapping between neural patterns and language outputs. This approach marks a shift from passive decoding to interactive, generative interfaces between brain data and language.

## Method Summary
CorText uses a two-phase training approach to align brain activity with language models. First, region-specific brain tokenizers (single-hidden-layer MLPs) learn to project distributed fMRI responses into language embedding space. These brain tokens are concatenated with text prompts and fed into a frozen Llama 3.1-8B decoder. In Phase 1, only tokenizers and layer norms are trained for 20 epochs using cross-entropy loss. Phase 2 applies QLoRA (rank=16) to attention query/value projections for 2 epochs. The system generates captions and answers questions from brain data alone, with in-silico perturbations demonstrating causal mapping between neural patterns and semantic content.

## Key Results
- Achieved CLIPScore of 0.647 for caption generation, significantly outperforming shuffled-control baseline (0.376, p<0.01)
- Successfully answered visual questions beyond training scope using QwenScore metric
- Demonstrated zero-shot generalization to novel semantic categories (zebras, surfers, airplanes)
- Showed systematic, graded semantic effects from in-silico microstimulation of functionally-localized brain regions

## Why This Works (Mechanism)

### Mechanism 1: Region-Specific Brain Tokenization
Mapping parcellated fMRI activity to language embedding space preserves spatial-functional structure while enabling LLM integration. Each brain parcel passes through a dedicated MLP tokenizer that projects neural activity patterns into a 921-dimensional space, allowing the frozen LLM to condition generation on neural context via its existing attention mechanism.

### Mechanism 2: Two-Phase Alignment Training
Separating tokenizer pretraining from LLM adaptation prevents catastrophic forgetting while achieving brain-language alignment. Phase 1 trains tokenizers + layer norms with frozen Llama, while Phase 2 applies QLoRA to query/value projections only, preserving LLM capabilities while learning to attend to brain tokens.

### Mechanism 3: In-Silico Perturbation via Selective Vertex Modulation
Additive perturbation of functionally-localized brain vertices produces graded, semantically coherent changes in language output. Using independent localizer data, identified vertices are selectively modulated during inference, showing systematic semantic shifts in generated captions that are location-specific and graded.

## Foundational Learning

- **Cross-Modal Embedding Alignment**: Understanding why brain-to-language projection is learnable requires recognizing shared latent geometries across modalities. Quick check: Given Llama-3.1-8B was never trained on brain data, what property of its embedding space makes brain-to-language projection learnable?

- **Functional Brain Parcellation**: The tokenizer operates on parcellated regions, not raw voxels. Understanding why parcels matter (preserving functional segregation, reducing dimensionality while retaining semantic structure) is critical for debugging poor performance. Quick check: If you switch from Schaefer-100 to Schaefer-400, should performance increase or decrease—and why might the paper use 100?

- **Parameter-Efficient Fine-Tuning (PEFT/QLoRA)**: Phase 2 uses QLoRA to adapt attention without full finetuning. Without understanding PEFT, you cannot diagnose whether adaptation failures stem from insufficient rank, wrong target modules, or training dynamics. Quick check: Why adapt query/value projections specifically rather than all attention weights or MLP layers?

## Architecture Onboarding

- **Component map**: fMRI betas → Schaefer parcellation → 100 parcels × ~300 vertices → region tokenizers → concatenated embeddings → Llama attention → generated text
- **Critical path**: Brain betas → parcellation → region tokenizers → concatenated embeddings → Llama attention → generated text. Failure anywhere breaks the chain; tokenizer quality determines upper bound on alignment.
- **Design tradeoffs**: Parcellation granularity (100 vs 200-500 vs HCP-MMP1) affects tokenizer parameters vs fine-grained selectivity; single-hidden-layer MLP tokenizers balance simplicity vs capturing nonlinear mappings; QLoRA rank=16 chosen empirically.
- **Failure signatures**: Shuffled-brain control matching true-brain performance indicates tokenizers not learning neural structure; zero-shot failures suggest overfitting to training semantics; inconsistent microstimulation effects indicate brittle alignment.
- **First 3 experiments**:
  1. Train with shuffled-brain data; if performance drops to control levels (0.376 CLIPScore), confirm tokenizers are learning real neural-language mappings.
  2. Retrain with Schaefer-200, 400, and HCP-MPP1 on held-out test set; plot CLIPScore vs parcel count to identify optimal granularity.
  3. Train tokenizers on Subject 1, test on Subject 2's brain data without retraining; quantify transfer gap to assess subject-specificity vs shared neural structure.

## Open Questions the Paper Calls Out

- Can the CorText framework be adapted to generalize to new participants without requiring extensive subject-specific retraining? The current study trained separate model instances for each participant, and individual variability in neural architecture typically hinders universal decoding.

- Can this brain-language fusion architecture be successfully applied to time-resolved neuroimaging modalities like M/EEG? The current implementation relies on spatially rich but temporally slow fMRI data; high-temporal-resolution data presents different noise and dimensionality challenges.

- To what extent does integrating neural data into the latent space enhance the general reasoning or background knowledge capabilities of the LLM itself? The current work focuses on the brain-to-language direction (decoding); it is unknown if the neural data acts as a beneficial training signal for the model's general intelligence.

## Limitations

- Spatial resolution and parcellation choice may smooth over fine-grained neural patterns crucial for detailed visual semantics
- Subject-specificity and generalizability concerns as all results come from single-subject models
- Causal inference from microstimulation relies on in-silico simulation rather than true neural manipulation

## Confidence

- **High Confidence**: Core technical implementation (two-phase training, region tokenization, QLoRA adaptation) is well-specified and reproducible
- **Medium Confidence**: Zero-shot generalization claims supported by qualitative examples but lack quantitative validation across multiple held-out categories
- **Low Confidence**: Causal interpretation of microstimulation effects requires independent validation through actual neural perturbation experiments

## Next Checks

1. Train brain tokenizers on Subject 1's data, then evaluate zero-shot performance on Subject 2's held-out trials without any retraining to quantify subject-specificity of learned mappings.

2. Systematically retrain models using Schaefer-200, Schaefer-400, and HCP-MMP1 (360 regions) while keeping all other parameters constant to identify optimal granularity.

3. Partner with an electrophysiology or optogenetics lab to test whether targeted neural perturbations produce predicted semantic shifts in language output, validating causal mapping between neural activity and language generation.