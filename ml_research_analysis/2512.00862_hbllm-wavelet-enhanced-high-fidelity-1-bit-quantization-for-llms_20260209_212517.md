---
ver: rpa2
title: 'HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs'
arxiv_id: '2512.00862'
source_url: https://arxiv.org/abs/2512.00862
tags:
- quantization
- should
- hbllm
- answer
- does
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes HBLLM, a 1-bit post-training quantization\
  \ framework for LLMs that integrates Haar wavelet transforms with frequency-domain\
  \ grouping strategies. The method addresses the limited expressiveness and poor\
  \ adaptability of existing 1-bit quantization approaches by decomposing weight matrices\
  \ into high- and low-frequency components, then applying structure-aware grouping\
  \ that considers both column saliency (via \u21132-norm) and intra-row frequency\
  \ patterns."
---

# HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs

## Quick Facts
- arXiv ID: 2512.00862
- Source URL: https://arxiv.org/abs/2512.00862
- Authors: Ningning Chen; Weicai Ye; Ying Jiang
- Reference count: 40
- One-line primary result: Achieves state-of-the-art 1-bit LLM quantization with perplexity ratios of 1.2–2.2 and 73.8%–88.8% QA accuracy retention

## Executive Summary
HBLLM introduces a novel 1-bit post-training quantization framework for large language models that leverages Haar wavelet transforms and frequency-domain grouping strategies. The method addresses limitations in existing approaches by decomposing weight matrices into high- and low-frequency components, then applying structure-aware grouping that considers both column saliency (via ℓ2-norm) and intra-row frequency patterns. Experimental results demonstrate superior performance compared to existing methods, maintaining high fidelity while achieving efficient memory compression.

## Method Summary
HBLLM operates by first computing Hessian-based importance scores for weight parameters, then selecting salient columns using ℓ2-norm metrics. The method fills missing salient columns with local averages (FillAvg) before applying Haar wavelet transforms to decompose weights into frequency components. These components are then binarized using frequency-aware grouping strategies, with shared mean values to reduce storage overhead. The approach supports both row-wise and column-wise variants, achieving approximately 1.08 bits per weight while maintaining high model performance across multiple benchmarks.

## Key Results
- Achieves perplexity ratios of 1.2–2.2 compared to full-precision models
- Retains 73.8%–88.8% of original QA accuracy on 9 benchmarks
- Demonstrates robustness on LLaMA3-8B where other methods fail
- Maintains low memory overhead with average 1.08 bits per weight

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Haar wavelet transforms increase representational capacity of binary quantization
- **Mechanism:** Decomposes weight rows into high- and low-frequency components, creating larger inverse quantization sets
- **Core assumption:** Weight matrices contain compressible frequency-domain structures
- **Evidence anchors:** CIQ of 1024 vs 8 for BiLLM; mentions leveraging Haar transforms in abstract
- **Break condition:** Fails if weight matrices are high-entropy random noise

### Mechanism 2
- **Claim:** ℓ2-norm identifies critical weight columns more effectively than ℓ1-norm
- **Mechanism:** ℓ2-norm penalizes large outliers more heavily, isolating weights contributing most to activation magnitude
- **Core assumption:** Activation importance correlates with ℓ2-norm of weight columns
- **Evidence anchors:** Table 2a shows 10.52 vs 10.78 perplexity; explicit mention in abstract
- **Break condition:** Fails if performance relies on small, distributed weights rather than sparse outliers

### Mechanism 3
- **Claim:** Frequency-aware intra-row grouping captures structural patterns better than global grouping
- **Mechanism:** Applies localized quantization boundaries per row based on frequency bands
- **Core assumption:** Rows have heterogeneous variance distributions requiring localized boundaries
- **Evidence anchors:** Table 2b shows 11.08 vs 16.32 perplexity; section 3.4 describes frequency-aware grouping
- **Break condition:** Fails if calibration dataset is too small to capture row variance

## Foundational Learning

- **Concept: Haar Wavelet Transform**
  - **Why needed here:** Core modification over standard PTQ; decomposes signals into averages and differences
  - **Quick check question:** How does computational cost of Haar transform compare to standard matrix multiplication?

- **Concept: Post-Training Quantization (PTQ)**
  - **Why needed here:** HBLLM operates without re-training using small calibration set
  - **Quick check question:** Why is Hessian matrix used to determine parameter importance in PTQ?

- **Concept: Salient vs. Non-Salient Weights**
  - **Why needed here:** Method bifurcates weight matrix into high-priority exceptions and aggressively binarized weights
  - **Quick check question:** What operation is performed on non-salient columns before Haar transform to handle "holes"?

## Architecture Onboarding

- **Component map:** Input (FP16 Weights + Calibration Data) -> Pre-processing (Hessian importance scores) -> Saliency Selection (ℓ2-norm top-K) -> Transformation (FillAvg -> Haar Transform) -> Quantization (Frequency-aware grouping) -> Output (Binary weights + Scaling factors)

- **Critical path:** FillAvg step is critical - cannot perform valid row-wise wavelet transform if salient columns are zeroed out; must fill with average of neighbors

- **Design tradeoffs:**
  - HBLLM-row vs HBLLM-col: Row-wise preserves more fidelity (lower perplexity) but requires 1.08 bits vs 1.00 bits for column-wise
  - Shared Mean: Reduces storage (~0.25 bits) but theoretically reduces precision; occasionally improves performance

- **Failure signatures:**
  - Performance Collapse: LLaMA3 with perplexity >50 suggests calibration mismatch or aggressive saliency threshold
  - Memory Overflow: OOM during Hessian computation suggests reducing block size or calibration samples

- **First 3 experiments:**
  1. Baseline Validation: Run HBLLM-row on LLaMA2-7B with C4 dataset (128 samples) and verify Wiki2 perplexity ≈ 10.52
  2. Saliency Ablation: Compare ℓ1 vs ℓ2 selection on OPT-1.3B to confirm robustness of saliency metric
  3. Architecture Robustness: Apply HBLLM to LLaMA3-8B to confirm survival where BiLLM/FrameQuant fail (target perplexity <25)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can HBLLM be adapted for Mixture-of-Experts (MoE) architectures without degrading routing accuracy?
- **Basis in paper:** Conclusion states current HBLLM supports only dense models, with focus on MoE PTQ next
- **Why unresolved:** Current implementation restricted to dense models; interaction between frequency-domain quantization and sparse MoE routing unexplored
- **What evidence would resolve it:** Successful application to MoE models (Mixtral, DeepSeek) with benchmarked perplexity and routing fidelity

### Open Question 2
- **Question:** What are actual wall-clock inference latency and energy overheads compared to theoretical gains?
- **Basis in paper:** Section 4.5 notes latency is "estimated" using proxy GEMV test due to lack of supporting inference framework
- **Why unresolved:** No specialized inference kernel prevents validation of real-world speedups vs theoretical O(d) complexity
- **What evidence would resolve it:** Deployment on edge devices with measured throughput and power consumption

### Open Question 3
- **Question:** Would replacing Haar with higher-order wavelets (e.g., Daubechies) yield better fidelity without compromising O(d) complexity?
- **Basis in paper:** Section 3.6 justifies Haar primarily by simplicity and efficiency, not evaluation of other local orthogonal transforms
- **Why unresolved:** While efficient, Haar may lack smoothness of other wavelets, limiting expressiveness for certain weight distributions
- **What evidence would resolve it:** Ablation study comparing reconstruction error and task performance using various wavelet bases

## Limitations

- Method currently supports only dense models, not Mixture-of-Experts architectures
- Lack of specialized inference framework prevents validation of real-world latency and energy benefits
- Specific algorithmic details for FillAvg step remain underspecified in provided text

## Confidence

- **High Confidence:** ℓ2-norm saliency selection mechanism well-supported by ablation studies (Table 2a)
- **Medium Confidence:** Haar wavelet transform contribution to representational capacity logically sound but not directly isolated
- **Low Confidence:** LLaMA3-8B robustness claim based on single data point without comparative validation

## Next Checks

1. Apply HBLLM to BERT-base model on GLUE benchmarks to test generalization across transformer architectures

2. Design synthetic weight matrix with small, distributed weights and compare ℓ1 vs ℓ2 saliency metrics to test assumption breakdown

3. Systematically vary salient column ratio (K) and plot perplexity/memory Pareto frontier on LLaMA-2-7B to validate 1.08 bits/weight claim across sparsity levels