---
ver: rpa2
title: 'Less is More: The Effectiveness of Compact Typological Language Representations'
arxiv_id: '2509.20129'
source_url: https://arxiv.org/abs/2509.20129
tags:
- feature
- features
- language
- selection
- typological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of high dimensionality and sparsity
  in typological feature datasets like URIEL+, which limit the effectiveness of distance
  metrics for cross-lingual NLP. The authors propose a pipeline combining feature
  selection and imputation to create compact, interpretable language representations.
---

# Less is More: The Effectiveness of Compact Typological Language Representations

## Quick Facts
- **arXiv ID**: 2509.20129
- **Source URL**: https://arxiv.org/abs/2509.20129
- **Reference count**: 25
- **Primary result**: Compact typological feature subsets outperform full URIEL+ space in linguistic distance alignment and downstream cross-lingual NLP tasks

## Executive Summary
This paper addresses the challenge of high dimensionality and sparsity in typological feature datasets like URIEL+, which limit the effectiveness of distance metrics for cross-lingual NLP. The authors propose a pipeline combining feature selection and imputation to create compact, interpretable language representations. Using four feature selection methods—Variance, PCA Loadings, Laplacian Score, and Correlation-based Feature Selection—alongside SoftImpute for missing value imputation, they produce reduced-size typological feature subsets. These subsets outperform the full URIEL+ feature space in linguistic distance alignment with a known similarity measure (Spearman correlation ρ up to 0.358 for Laplacian Score at 300 features). Downstream task results show consistent improvements: up to 7.8% gain in cross-lingual transfer ranking (LangRank NDCG@3), 1.4% reduction in language model performance prediction error (ProxyLM RMSE), and comparable accuracy in linguistic regularization tasks (LinguAlchemy), demonstrating that smaller, well-chosen feature sets yield more informative language distances and improve multilingual NLP applications.

## Method Summary
The authors combine feature selection and imputation to optimize URIEL+ typological feature space. They apply four selection methods—Variance, PCA Loadings (95% variance), Laplacian Score (5-NN graph), and Correlation-based FS (hill-climbing with mutual information and phi correlation)—at subset sizes k ∈ {100,200,...,700}. After selection, they use SoftImpute with mean initial fill to impute missing values. The pipeline produces compact vectors used to compute angular distances, which are evaluated against linguistic distance alignment with Gower scores and three downstream tasks: cross-lingual transfer ranking (LangRank), language model performance prediction (ProxyLM), and linguistic regularization (LinguAlchemy).

## Key Results
- Laplacian Score achieved strongest linguistic distance alignment (ρ=0.358 at k=300) versus baseline (ρ=0.260)
- Cross-lingual transfer ranking improved by up to 7.8% NDCG@3
- Language model performance prediction error reduced by 1.4% RMSE
- Feature subsets of 300-500 features consistently outperformed full 800-feature space

## Why This Works (Mechanism)

### Mechanism 1: Manifold Preservation via Laplacian Scoring
Filtering features based on their ability to preserve local similarity graphs yields distance metrics with higher alignment to linguistic reality than variance-based selection. The Laplacian Score prioritizes features where similar languages share similar values, filtering out high-variance features that might be random or noisy, retaining only those that respect the underlying "data manifold" of language typology. Lower-scoring features are more stable across similar languages and thus more likely to encode intrinsic typological patterns. Laplacian Score achieved the strongest alignment (ρ=0.358) compared to Variance (ρ=0.295) and Baseline (ρ=0.260). If the k-NN graph is constructed on extremely sparse data, the "neighbors" may be artifacts of missingness rather than similarity, causing the score to optimize for data availability instead of linguistic structure.

### Mechanism 2: Redundancy Elimination via Genetic Class Information
Supervised selection using language family labels reduces feature redundancy more effectively than unsupervised statistics, improving performance on transfer prediction tasks. Correlation-based Feature Selection uses Mutual Information to measure how well a feature distinguishes language families, while penalizing features that correlate with those already selected. This enforces diversity in the feature subset. CFS showed peak gains of +6.0% in LANGRANK at subset size 100, excelling in smaller subsets. If the downstream task relies on areal features rather than genetic features, penalizing correlation might discard a cluster of relevant syntactic features that happen to co-occur in a specific language family.

### Mechanism 3: Low-Rank Matrix Completion for Sparsity
Imputing missing values after feature selection allows matrix completion algorithms to recover global structure with less noise than imputing the full high-dimensional space. SoftImpute approximates the language-feature matrix with a low-rank representation. By first removing redundant/noisy features, the imputation algorithm solves a better-conditioned problem, inferring missing values based on the "core" latent structure. The true underlying typological data matrix is assumed to be low-rank (i.e., language features are highly interdependent). Consistent performance gains further indicate that the combined pipeline can effectively trim noisy and redundant features. If the remaining features have too little overlap, the low-rank assumption fails, and imputation becomes an unstable guess.

## Foundational Learning

- **Concept: Curse of Dimensionality & Distance Concentration**
  - Why needed: The paper motivates its work by noting that high dimensionality (800 features) and sparsity (87% missing) "limit the effectiveness of distance metrics." You must understand that in high-dimensional sparse spaces, distances between points converge, making "similarity" meaningless.
  - Quick check: Why does increasing the number of random features tend to make all pairwise distances appear roughly equal?

- **Concept: Manifold Learning / Graph Laplacian**
  - Why needed: The best-performing method (Laplacian Score) relies on graph theory. You need to understand that the "Laplacian" captures the connectivity of a graph, and "Locality Preservation" means ensuring points that are close on a graph remain close in the reduced feature space.
  - Quick check: In the context of the paper, what does it physically mean for a feature to have a "low Laplacian Score"?

- **Concept: Mutual Information (MI) vs. Correlation**
  - Why needed: The CFS mechanism distinguishes between MI (non-linear relevance to a class) and Phi correlation (linear redundancy between features). Understanding this distinction is key to why CFS selects "diverse" features.
  - Quick check: Why is Mutual Information used to score relevance against the "class" (language family) while Phi correlation is used to score redundancy between features?

## Architecture Onboarding

- **Component map**: URIEL+ (4555×800) -> Union aggregation -> Binary Matrix -> 4 parallel Selection (Variance, PCA, Laplacian, CFS) -> SoftImpute -> Compact Vectors -> Angular Distance Calculation -> LANGRANK/PROXYLM/LINGUALCHEMY

- **Critical path**: The determination of the subset size k. The paper explicitly shows performance is non-monotonic with size; e.g., Laplacian peaks at k=300, while CFS peaks at k=100. Tuning k is the primary control lever.

- **Design tradeoffs**:
  - Interpretability vs. Density: Selection methods like PCA maximize variance explained but may mix features, whereas CFS selects discrete features that are easier to map to specific linguistic properties
  - Sensitivity vs. Stability: The Alignment task (G_d) is sensitive to method choice (LS wins), whereas LM Regularization (LinguAlchemy) is robust to reduction (all methods work fine)

- **Failure signatures**:
  - Distance Skew: If Variance selection is used without sufficient imputation, distances may reflect "data availability" rather than linguistic type
  - Over-pruning: Setting k < 100 caused performance drops in LANGRANK for unsupervised methods, suggesting a minimum feature threshold is needed to distinguish syntax/morphology

- **First 3 experiments**:
  1. Baseline Reproduction: Compute angular distances on raw URIEL+ (k=800) and correlate with the G_d similarity scores (target: ρ ≈ 0.260)
  2. Ablation on k: Run Laplacian Score selection for k ∈ {100, 300, 500}, impute with SoftImpute, and re-measure ρ. Verify peak at 300
  3. Downstream Sanity Check: Plug the k=300 LS vectors into LANGRANK for a single subtask (e.g., Dependency Parsing) to verify the reported +7.8% NDCG improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a hybrid or ensemble-based feature selection strategy combining multiple methods (variance, PCA, Laplacian Score, CFS) yield more balanced and robust typological representations than any single method alone?
- Basis: Limitations section states "A hybrid or ensemble-based selection strategy may yield more balanced representations" to address how "each feature selection method inherently favors different properties."
- Why unresolved: The paper evaluates each method independently and does not experiment with combining them.
- What evidence would resolve it: Experiments combining selection methods (e.g., voting, weighted aggregation) showing improved alignment and downstream task performance across diverse language families.

### Open Question 2
- Question: Can alternative imputation techniques such as random forest models or autoencoders outperform SoftImpute for highly sparse languages or specific feature types?
- Basis: Limitations section notes "alternative techniques (e.g., random forest models or autoencoders) could better capture linguistic relationships and offer complementary benefits, particularly for highly sparse languages or feature types."
- Why unresolved: The pipeline relies solely on SoftImpute without comparing against other imputation methods.
- What evidence would resolve it: Comparative imputation experiments on sparse subsets measuring alignment correlation and downstream task gains.

### Open Question 3
- Question: How do the optimized feature subsets generalize across a broader range of typologically diverse, low-resource languages beyond the current evaluation sets?
- Basis: Limitations section acknowledges evaluation "focuses on a relatively small set of languages (19 in the alignment study, 105 in LANGRANK, 51 in PROXYLM and 63 in LINGUALCHEMY), with an emphasis on high-resource languages" and calls for "broader benchmarks."
- Why unresolved: Current experiments cover limited language samples and may not represent global typological diversity.
- What evidence would resolve it: Evaluation on expanded benchmarks with more low-resource languages and under-represented families.

## Limitations
- Linguistic distance alignment relies on Gower scores for only 28 language pairs (low-resource Isthmo-Colombian languages), raising questions about generalizability
- SoftImpute assumes low-rank structure which may not hold for heterogeneous typological features with 87% missingness
- Supervised feature selection assumes language family labels capture relevant structure, which may not hold for contact languages or mixed families

## Confidence

**High Confidence** (Robust empirical support, well-established mechanisms):
- Compact representations improve cross-lingual transfer ranking (LANGRANK NDCG@3 gains up to 7.8%)
- Downstream task improvements are consistent across multiple benchmarks (PROXYLM RMSE reduction 1.4%, LinguAlchemy accuracy)
- Variance-based selection underperforms compared to manifold-preserving methods

**Medium Confidence** (Supported by evidence but with methodological constraints):
- Laplacian Score's superiority in linguistic distance alignment (ρ=0.358 vs baseline 0.260)
- SoftImpute imputation effectiveness after feature selection
- CFS performance peak at k=100 for transfer prediction

**Low Confidence** (Limited validation, theoretical assumptions):
- Gower distance alignment results generalize beyond Isthmo-Colombian languages
- Low-rank matrix completion assumption holds for heterogeneous typological features
- Language family labels are optimal supervised signals for feature selection

## Next Checks
1. **Cross-Domain Alignment Validation**: Test linguistic distance alignment using Gower scores computed for language pairs from diverse typological families (e.g., Indo-European vs. Sino-Tibetan) to verify Laplacian Score's superiority isn't domain-specific.

2. **Imputation Quality Assessment**: Compare imputed typological values against expert annotations for a subset of languages to evaluate whether SoftImpute recovers linguistically plausible values versus statistical artifacts.

3. **Family Label Sensitivity Analysis**: Run CFS selection using alternative supervised signals (e.g., geographic proximity, typological similarity) and measure impact on downstream task performance to test the robustness of family-based feature selection.