---
ver: rpa2
title: 'AudioMAE++: learning better masked audio representations with SwiGLU FFNs'
arxiv_id: '2507.10464'
source_url: https://arxiv.org/abs/2507.10464
tags:
- audio
- masked
- audiomae
- learning
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AudioMAE++, a self-supervised masked autoencoder
  for learning general-purpose audio representations. The model enhances the standard
  transformer architecture by incorporating macaron-style feedforward networks and
  gated linear units (SwiGLU), which have shown promise in speech recognition.
---

# AudioMAE++: learning better masked audio representations with SwiGLU FFNs

## Quick Facts
- **arXiv ID:** 2507.10464
- **Source URL:** https://arxiv.org/abs/2507.10464
- **Reference count:** 0
- **Primary result:** Achieves state-of-the-art performance among SSL models on HEAR benchmark with 91.8±0.2 (Base) and 93.7±0.2 (Large) scores

## Executive Summary
This paper introduces AudioMAE++, a self-supervised masked autoencoder for learning general-purpose audio representations. The model enhances the standard transformer architecture by incorporating macaron-style feedforward networks and gated linear units (SwiGLU), which have shown promise in speech recognition. The proposed approach is pretrained on the AudioSet dataset and evaluated on 10 diverse downstream audio tasks spanning music, speech, and audio classification. AudioMAE++ outperforms existing MAE-based approaches and achieves state-of-the-art results among SSL models, with an overall score of 91.8±0.2 for the Base model and 93.7±0.2 for the Large model. Notably, AudioMAE++ Base outperforms larger MAE baselines with up to 4× more parameters, demonstrating better scaling characteristics. The paper also explores the impact of rotary positional embeddings and decoder complexity, finding that decoder size can be a bottleneck when scaling up encoders.

## Method Summary
AudioMAE++ is a masked autoencoder that uses macaron-style transformer blocks with SwiGLU feedforward networks. The model processes log-mel spectrograms from AudioSet, applying 80% random masking before encoding. The encoder processes only visible patches using macaron-style blocks (sandwiching attention between two FFNs, with the second being SwiGLU). A decoder with learnable mask tokens reconstructs the masked patches using MSE loss. Pretraining uses AdamW with linear warmup and cosine decay schedules. Downstream evaluation uses frozen features with a linear MLP classifier across 10 HEAR benchmark tasks.

## Key Results
- AudioMAE++ achieves state-of-the-art performance with 91.8±0.2 for Base and 93.7±0.2 for Large models
- AudioMAE++ Base outperforms MAE-Huge (629.8M params) with only 141.9M parameters
- SwiGLU+macaron architecture provides better scaling than parameter increase alone
- Decoder dimension scaling matters: 512 dims needed for Large model (vs 384 for Base)
- RoPE positional embeddings hurt performance on utterance-level tasks compared to fixed embeddings

## Why This Works (Mechanism)

### Mechanism 1
SwiGLU-based feedforward networks improve audio representation quality over standard pointwise FFNs in masked autoencoders. SwiGLU computes FSwiGLU(x; W, V, O) = (Swish(xW) ⊗ xV)O, an element-wise product of two linear projections where one branch uses Swish activation. This gating mechanism provides learnable control over information flow, potentially enabling richer gradient paths and representational capacity compared to standard FFN layers. The gating mechanism captures useful inductive biases for audio spectrogram modeling that standard FFNs lack.

### Mechanism 2
Macaron-style transformer blocks (sandwiching attention between two FFNs) improve learning over single-FFN standard blocks. Each block applies half-FFN → LayerNorm → MHA → LayerNorm → half-SwiGLU FFN with residual connections. Processing features before and after attention enables complementary transformations at two points in the block. The dual-FFN structure provides better feature mixing than single-FFN blocks, beyond the benefit of additional parameters.

### Mechanism 3
High masking rate (80%) forces the encoder to learn robust semantic representations rather than relying on local interpolation. With only 20% of patches visible, the model must capture higher-level dependencies across time-frequency regions. The encoder processes only visible patches (no mask tokens), reducing computation and preventing shortcut learning from neighboring patches. Audio spectrograms exhibit high temporal-frequency redundancy similar to spatial correlation in images.

## Foundational Learning

- **Masked Autoencoders (MAE)**: Core architecture—encoder processes only visible patches, decoder reconstructs masked patches. Understanding the asymmetric encoder-decoder design and mask-token insertion is essential. Quick check: Why does MAE encode only visible patches rather than inserting mask tokens before encoding (like BERT)?

- **Vision Transformer Patch Embeddings**: Input representation uses 4×16 rectangular patches on 80-bin mel spectrograms. Non-overlapping patches are linearly projected to dm-dimensional tokens. Quick check: Why might rectangular patches (4 time × 16 frequency) be preferred over square patches for audio spectrograms?

- **Self-Supervised Transfer Evaluation**: Pretraining on AudioSet (2M clips), evaluation via frozen features + linear MLP classifier on 10 downstream tasks using HEAR benchmark protocol. Quick check: Why use frozen features + MLP rather than end-to-end fine-tuning for downstream evaluation?

## Architecture Onboarding

- **Component map**: Input: Log-mel spectrogram [200×80] (2-sec, 16kHz) → Patch embedding: 4×16 patches → N tokens, dm=768 (Base) → Add [CLS] token + positional embeddings (fixed sinusoidal, NOT RoPE) → Random mask 80% → Keep ~20% patches only → Encoder (12 blocks): Each = LN→Half-MLP→LN→MHA→LN→Half-SwiGLU → Decoder: Insert mask tokens → 4 blocks (ddec=512) → Linear → MSE loss

- **Critical path**: SwiGLU implementation: Two parallel linear projections (xW, xV), Swish on first branch, element-wise multiply, then output projection O. Proper masking: Drop masked patches entirely in encoder; insert learnable mask tokens only in decoder. Decoder dimension scaling: Match decoder capacity to encoder scale (512-dim decoder for Large encoder)

- **Design tradeoffs**: RoPE vs. fixed embeddings: RoPE hurts performance (91.4→92.0 without RoPE) on 2-sec utterance-level tasks—contrary to ASR benefits. Parameter efficiency: AudioMAE++-Base (141.9M params) outperforms MAE-Huge (629.8M params), suggesting architectural improvements > brute-force scaling. Decoder capacity: 384→512 dims improves Large model (91.7→93.7); 512→768 shows no further gain

- **Failure signatures**: RoPE in encoder+decoder: Performance drop on utterance-level classification tasks. Undersized decoder with large encoder: Bottlenecks representation quality. Direct transfer to longer sequences: 2-sec pretraining crops may not generalize to tasks requiring longer temporal context

- **First 3 experiments**: RoPE ablation replication: Verify RoPE hurts your target downstream task; if it helps, investigate whether longer input contexts explain the difference. Decoder dimension sweep: Test 384/512/768 with your encoder scale to find the capacity sweet spot before diminishing returns. Parameter-controlled comparison: Compare AudioMAE++-Base against a wider standard-FFN MAE with matched parameter count to isolate SwiGLU+macaron benefit from capacity increase

## Open Questions the Paper Calls Out
- How do gated linear units, local-global information processing, and sequence length interact to influence performance in masked audio autoencoders?
- Does the utility of Rotary Positional Embeddings (RoPE) in audio MAEs depend on input duration or the specific nature of the downstream task?
- Can macaron-style SwiGLU feedforward networks functionally replace explicit local-global attention mechanisms for all audio domains?

## Limitations
- Missing architectural details: SwiGLU intermediate dimension and peak learning rate not specified
- Decoder scaling sensitivity: Performance depends critically on matching decoder capacity to encoder scale
- Task-specific generalization: RoPE embedding performance may vary with input duration and task type

## Confidence
- **High confidence**: Overall performance improvements and parameter efficiency claims
- **Medium confidence**: Specific mechanisms (SwiGLU, macaron blocks, 80% masking) driving improvements
- **Medium confidence**: Generalizability of decoder scaling findings across configurations

## Next Checks
1. Replicate the RoPE ablation study on your target downstream task to verify whether fixed embeddings consistently outperform RoPE for your specific audio domain and input lengths
2. Perform a decoder dimension sweep (384→512→768) with your chosen encoder scale to identify the capacity sweet spot before diminishing returns
3. Conduct a parameter-matched ablation study comparing AudioMAE++-Base against a wider standard-FFN MAE with matched parameter count to isolate architectural benefits versus capacity increase