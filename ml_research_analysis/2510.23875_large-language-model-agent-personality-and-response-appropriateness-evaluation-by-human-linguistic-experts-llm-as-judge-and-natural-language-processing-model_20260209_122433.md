---
ver: rpa2
title: 'Large Language Model Agent Personality and Response Appropriateness: Evaluation
  by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model'
arxiv_id: '2510.23875'
source_url: https://arxiv.org/abs/2510.23875
tags:
- personality
- agent
- agents
- linguistic
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This interdisciplinary study evaluates LLM-based agent personality
  and response appropriateness using three complementary methods: NLP models, a separate
  LLM-as-judge, and human linguistic experts. The authors developed two poetry expert
  agents (introvert and extrovert) and created a novel question bank based on Bloom''s
  taxonomy and linguistic criteria.'
---

# Large Language Model Agent Personality and Response Appropriateness: Evaluation by Human Linguistic Experts, LLM-as-Judge, and Natural Language Processing Model

## Quick Facts
- arXiv ID: 2510.23875
- Source URL: https://arxiv.org/abs/2510.23875
- Reference count: 40
- Primary result: Automated personality assessment methods show significant biases, while human linguistic expert evaluation remains the most reliable approach for validating LLM agent personality.

## Executive Summary
This interdisciplinary study evaluates LLM-based agent personality and response appropriateness using three complementary methods: NLP models, a separate LLM-as-judge, and human linguistic experts. The authors developed two poetry expert agents (introvert and extrovert) and created a novel question bank based on Bloom's taxonomy and linguistic criteria. Evaluation revealed significant biases in both the personality transformer model (trained on an imbalanced Reddit dataset) and the Judge LLM, which consistently identified both agents as introverted despite design differences. The Judge LLM's reasoning showed anomalies, such as assigning "friendly" more frequently to the introvert agent. Only human linguistic expert evaluation provided reliable assessment, highlighting the limitations of purely deep learning approaches and emphasizing the critical need for interdisciplinary design in agent development.

## Method Summary
The study built two poetry expert agents using GPT-4o mini with Langchain's RAG framework, where contextual documents (poems) were chunked, embedded using OpenAI's text-embedding-ada-002, and stored in ChromaDB. Agents were created with distinct personality prompts (introvert and extrovert) and evaluated using a novel question bank of 94 questions categorized by Bloom's taxonomy complexity levels. Three evaluation methods were employed: a RoBERTa-based personality transformer trained on the PANDORA Reddit dataset, Google Gemini as a Judge LLM, and human linguistic experts using four assessment mechanisms (linguistic competence, structure/content, discourse/pragmatics, and contextualization).

## Key Results
- The personality transformer model, trained on a 3.7:1 imbalanced dataset, consistently classified both agents as introverted despite their design differences
- The Judge LLM showed reasoning anomalies, assigning personality traits that contradicted agent design (e.g., "friendly" more frequently to the introvert agent)
- Human linguistic expert evaluation was the only method that reliably distinguished between the introvert and extrovert agents
- Both agents performed better on complex questions requiring analysis and synthesis compared to simple retrieval questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval Augmented Generation (RAG) with personality prompting can create domain-expert agents with distinct behavioral profiles, but personality expression depends heavily on prompt specificity.
- Mechanism: The Langchain framework chunks contextual documents (poems), generates embeddings via OpenAI's text-embedding-ada-002, stores them in ChromaDB, and retrieves relevant chunks based on semantic similarity to user queries. Personality prompts shape the tone and style of response generation while RAG ensures factual grounding.
- Core assumption: Personality prompts will consistently influence response style across varied query complexity levels.
- Evidence anchors:
  - [abstract] "LLM-based agents can be used to create highly engaging interactive applications through prompting personality traits and contextual data"
  - [section 3.1.1] "Langchain's retrieval mechanism is powered by the Retrieval Augmented Generation (RAG) technique... provides the LLM with external context, allowing it to generate accurate, domain-specific responses"
  - [corpus] CAPE paper notes context-free personality testing may be artificial; contextual responses reveal different traits
- Break condition: If personality prompts are underspecified, agents may converge toward similar output styles regardless of design intent.

### Mechanism 2
- Claim: Triangulated evaluation using NLP models, LLM-as-Judge, and human experts exposes systematic biases in automated assessment methods.
- Mechanism: Each evaluation method has different failure modes—transformer models inherit training data biases (PANDORA dataset has 7,134 introverted vs. 1,920 extroverted users), Judge LLMs show opaque reasoning patterns, while human linguistic experts apply pragmatic knowledge through four assessment mechanisms.
- Core assumption: Human linguistic experts provide ground truth for personality assessment in conversational agents.
- Evidence anchors:
  - [abstract] "Evaluation revealed significant biases in both the personality transformer model (trained on an imbalanced Reddit dataset) and the Judge LLM, which consistently identified both agents as introverted despite design differences"
  - [section 5.1] "This observation that both agents are indicated as introverted is strongly explained by the fact that the transformer model used is trained on the PANDORA dataset... The dataset is unbalanced"
  - [corpus] Multiple related papers (CAPE, Deterministic AI Agent Personality) address personality evaluation but none systematically compare all three methods with human experts
- Break condition: When NLP model and Judge LLM agree but contradict human expert assessment, automated methods are unreliable for personality validation.

### Mechanism 3
- Claim: Bloom's taxonomy-based question banks provide a structured framework for stress-testing agent capabilities across cognitive complexity levels.
- Mechanism: Questions progress from simple retrieval through complex analysis to critical synthesis requiring external knowledge, with a fourth category designed to trigger hallucination. This reveals where agent performance degrades and personality consistency breaks down.
- Core assumption: Higher cognitive complexity questions will reveal more personality markers in agent responses.
- Evidence anchors:
  - [section 3.2] "we crafted a novel question bank taking inspiration from Bloom's taxonomy of levels of cognitive learning... merging them with linguistic principles"
  - [section 5.2] "Both agents perform much better when questions are meant for complex and more complex questions: answers are more organized, systematic, to-the-point and explicit"
  - [corpus] Weak corpus evidence—related papers do not systematically apply Bloom's taxonomy to agent evaluation
- Break condition: Complex questions may elicit more detailed but not necessarily more personality-distinctive responses; simple questions with emotional content might better reveal personality.

## Foundational Learning

- Concept: Big Five Personality Model (OCEAN framework)
  - Why needed here: The study operationalizes personality along five dimensions (Extraversion, Agreeableness, Conscientiousness, Neuroticism, Openness). Understanding that extraversion vs. introversion is the primary axis being tested is essential for interpreting results.
  - Quick check question: Can you explain why the transformer model outputs five continuous scores that sum to 1, and what "conscientiousness" being highest for IA might indicate about expert behavior?

- Concept: Training Data Bias and Class Imbalance
  - Why needed here: The PANDORA dataset's 3.7:1 introvert-to-extrovert ratio explains why both agents were classified as introverted. Understanding how class imbalance affects model predictions is critical for interpreting automated evaluation failures.
  - Quick check question: If you were to retrain the personality transformer, what sampling strategy would you apply to the PANDORA dataset?

- Concept: LLM-as-Judge Evaluation Paradigm
  - Why needed here: Using one LLM (Gemini) to evaluate another LLM's outputs (GPT-4o mini) is a common but problematic practice. The study reveals reasoning anomalies ("friendly" assigned more frequently to introvert) that challenge this paradigm's reliability.
  - Quick check question: What are two potential sources of bias when using Gemini to evaluate GPT-4o mini outputs, and how does using a different model family address (or not address) these?

## Architecture Onboarding

- Component map: User Query → Gradio Interface → Langchain Retriever → ChromaDB (vector store) → OpenAI text-embedding-ada-002 → GPT-4o mini (with personality prompt) → Response → Three evaluators: 1. Personality Transformer (RoBERTA-based) 2. Judge LLM (Gemini) 3. Human Linguistic Expert

- Critical path: The personality prompt template is the highest-leverage component. The current prompts for IA and EA differ only by "without a lot of repetition" (IA) and personality label. This underspecification likely caused personality convergence. The evaluation triangulation pipeline is the critical validation path.

- Design tradeoffs:
  - Using GPT-4o mini vs. larger model: Cost-efficient but may have weaker personality adherence
  - RAG vs. fine-tuning: RAG allows rapid domain adaptation but personality is prompt-dependent only
  - Gemini as Judge vs. same-model evaluation: Avoids self-agreement bias but introduces Gemini's unknown training biases
  - Human expert evaluation: Most reliable but least scalable

- Failure signatures:
  - Both agents classified as introvert → check personality transformer training data balance
  - Judge LLM assigns contradictory reasoning (e.g., "friendly" to introvert) → Judge LLM bias or prompt evaluation issues
  - Agents perform worse on simple questions → insufficient context in vector store or retrieval failure
  - Extraversion score lower for EA than IA → prompt specificity problem or model defaulting to expert persona over personality

- First 3 experiments:
  1. **Prompt engineering iteration**: Rewrite EA prompt with explicit behavioral markers (e.g., "Use emojis frequently," "Ask follow-up questions about the user's day," "Share personal opinions"). Measure whether Judge LLM classification shifts.
  2. **Training data audit**: Sample 100 entries from PANDORA dataset classified as extrovert and introvert. Compare linguistic features to your agent outputs. Determine if transformer model's introvert classification reflects genuine similarity or dataset artifact.
  3. **Judge calibration study**: Have human experts rate 20 agent responses on extraversion (1-10 scale). Compare with Judge LLM binary classifications. Calculate agreement rate and identify systematic divergence patterns in reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the introversion bias observed in transformer-based personality models be effectively mitigated, given their training on imbalanced datasets like PANDORA?
- Basis in paper: [explicit] The authors identify that the NLP model's consistent identification of both agents as introverted stems from the imbalanced PANDORA dataset and explicitly state they will "mitigate the bias of the NLP model" in future work.
- Why unresolved: The current study only identified and demonstrated the existence of the bias (skewed introversion scores) but did not implement or test correction methods.
- What evidence would resolve it: A study comparing baseline model scores against those from a model re-trained on a balanced dataset or adjusted via algorithmic fairness techniques, validated against human expert baselines.

### Open Question 2
- Question: To what extent do human participant psychometric evaluations align with the assessments of linguistic experts versus automated Judge LLMs?
- Basis in paper: [explicit] The conclusion outlines a plan to "undertake psychometric testing of agent responses by participants" to address the limitations of the current evaluation methods.
- Why unresolved: The present study relied on a small number of linguistic experts and found automated methods unreliable, but did not conduct broader human participant studies to validate the agents' perceived personalities.
- What evidence would resolve it: Correlation scores between human participant ratings, linguistic expert evaluations, and Judge LLM outputs across a statistically significant sample of interactions.

### Open Question 3
- Question: Is the superior reliability of human linguistic experts consistent across non-literary domains, such as technical or transactional tasks?
- Basis in paper: [inferred] The study uses a poetry explanation task requiring "deep linguistic analysis," but the introduction suggests applications for "medical text" and "sightseeing tours," implying a need to verify if this evaluation framework generalizes.
- Why unresolved: It is unclear if the linguistic criteria used (e.g., phonetic properties, complex syntax) are as discriminative for agents performing straightforward, non-creative tasks.
- What evidence would resolve it: A replication of the study using agents designed for technical support or factual retrieval, comparing the alignment of NLP models and human experts in those contexts.

## Limitations

- The personality transformer model's performance is severely compromised by the PANDORA dataset's 3.7:1 introvert-to-extrovert ratio, making automated personality assessment unreliable for this use case.
- The Judge LLM (Gemini) exhibits reasoning anomalies, assigning personality traits that contradict the agents' design intent (e.g., "friendly" more frequently to the introvert agent).
- The question bank's Bloom's taxonomy categorization may not align with how personality manifests in responses, potentially conflating cognitive complexity with personality expression.

## Confidence

- **High confidence**: The necessity of human linguistic expert evaluation for reliable personality assessment is well-supported by the systematic failures of automated methods.
- **Medium confidence**: The finding that both agents are classified as introverted is explained by training data bias, but the extent to which prompt engineering could overcome this bias remains uncertain.
- **Low confidence**: The claim that Bloom's taxonomy-based question banks are optimal for stress-testing personality expression lacks strong empirical support.

## Next Checks

1. **Prompt Engineering Impact Test**: Systematically vary personality prompt specificity (from minimal to highly detailed behavioral markers) across 10 iterations and measure changes in Judge LLM classification accuracy. This would determine if the current prompts are insufficiently specific rather than the evaluation method being fundamentally flawed.

2. **Dataset Balance Experiment**: Retrain the personality transformer on a balanced subset of the PANDORA dataset (1:1 introvert-to-extrovert ratio) using stratified sampling. Compare classification accuracy against the original imbalanced model to quantify the impact of class imbalance on personality detection.

3. **Cross-Evaluator Consistency Analysis**: Have 10 additional linguistic experts evaluate a random sample of 50 agent responses using the same four assessment mechanisms. Calculate inter-rater reliability (Cohen's kappa) and compare against Judge LLM consistency to determine if human expert agreement varies significantly across different evaluators.