---
ver: rpa2
title: 'HERA: Hybrid Edge-cloud Resource Allocation for Cost-Efficient AI Agents'
arxiv_id: '2504.00434'
source_url: https://arxiv.org/abs/2504.00434
tags:
- subtask
- hera
- accuracy
- request
- subtasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HERA, a hybrid edge-cloud resource allocation
  framework for cost-efficient AI agents. HERA addresses the challenge of high operational
  costs from cloud-based LLM usage by dynamically partitioning subtasks between local
  SLMs and cloud LLMs.
---

# HERA: Hybrid Edge-cloud Resource Allocation for Cost-Efficient AI Agents

## Quick Facts
- arXiv ID: 2504.00434
- Source URL: https://arxiv.org/abs/2504.00434
- Authors: Shiyi Liu; Haiying Shen; Shuai Che; Mahdi Ghandi; Mingqin Li
- Reference count: 40
- Key outcome: HERA achieves up to 9.1% accuracy improvement and 10.8% higher SLM usage compared to HybridLLM, while offloading 45.67% of subtasks to local devices and reducing operational costs by up to 30%.

## Executive Summary
This paper introduces HERA, a hybrid edge-cloud resource allocation framework for cost-efficient AI agents. HERA addresses the challenge of high operational costs from cloud-based LLM usage by dynamically partitioning subtasks between local SLMs and cloud LLMs. It employs a hierarchical decision-making process using offline-trained estimators to evaluate subtask similarity, predict convergence points, and decompose complex tasks. HERA achieves significant cost reductions while maintaining accuracy, making it particularly valuable for organizations seeking to scale AI agent deployments efficiently.

## Method Summary
HERA is a hybrid edge-cloud resource allocation framework that uses four offline-trained estimators (URC, SP, DP, SD) to dynamically route subtasks between local SLMs and cloud LLMs. The framework employs position-aware thresholding, convergence prediction, and subtask decomposition to maximize local processing while maintaining accuracy. The method involves extensive offline profiling of model pairs, fine-tuning of estimators with LoRA, and online routing using adaptive similarity thresholds based on subtask sequence position.

## Key Results
- Achieves 9.1% accuracy improvement and 10.8% higher SLM usage compared to HybridLLM baseline
- Offloads 45.67% of subtasks to local devices while maintaining comparable accuracy to cloud-only approaches
- Reduces operational costs by up to 30% through intelligent resource allocation
- Validated across six diverse datasets including DROP, HumanEval, and Webshop

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Position-aware thresholding enables safe SLM assignment for early-stage subtasks while protecting accuracy-critical late-stage reasoning.
- Mechanism: HERA applies an adaptive similarity threshold κ = threshold_base + min(ID, 5) × 0.02 that increases as subtask sequence position advances. Early subtasks permit looser SLM-LLM output similarity (κ ≈ 0.6), while later subtasks require stricter matching (κ ≈ 0.7). This reflects the observed pattern: switching LLM→SLM causes 3.25% accuracy drop in early stages vs. 9.53% in late stages.
- Core assumption: Subtask sequence position correlates with accuracy sensitivity, which holds for multi-hop reasoning chains but may not generalize to parallel or non-linear agent workflows.
- Evidence anchors: [abstract] "HERA considers the varying subtask features and strategically decides the location for each subtask"; [Section 2.3] Figure 3 shows accuracy impact by stage; Section 3.3.3 details adaptive threshold formula; [corpus] Division-of-Thoughts (arXiv:2502.04392) similarly exploits hybrid LLM-SLM synergy but without explicit position-aware routing.

### Mechanism 2
- Claim: S-L distance prediction enables speculative SLM execution by forecasting when SLM outputs will converge with LLM outputs.
- Mechanism: The distance predictor estimates how many additional SLM subtasks are needed before output aligns with LLM output. If predicted S-L distance is d, HERA compares the (d+1)th predicted SLM subtask with the current predicted LLM subtask. High similarity permits SLM assignment with confidence of eventual convergence.
- Core assumption: SLM and LLM reasoning trajectories, while initially divergent, tend toward semantic alignment on tractable subtasks—observed in matched final result scenarios but fails in unmatched cases where distances reach infinity.
- Evidence anchors: [abstract] "predict convergence points"; [Section 3.2] Distance Predictor training details; Section 2.3 Figure 7 shows S-L distance patterns in matched vs. unmatched scenarios; [corpus] Conformal Sparsification (arXiv:2510.09942) addresses edge-cloud bandwidth for speculative decoding but uses different convergence mechanism.

### Mechanism 3
- Claim: Subtask decomposition increases SLM utilization by creating finer-grained units that fall within SLM capability boundaries.
- Mechanism: When direct SLM assignment and convergence detection both fail, the Subtask Decomposer (fine-tuned Llama 3.2 1B) breaks the subtask into smaller sub-subtasks. Each sub-subtask undergoes SSE evaluation; only if ALL sub-subtasks pass SLM suitability does HERA assign them to SLM. Otherwise, the original subtask routes to LLM.
- Core assumption: Complex subtasks contain tractable sub-components that individually match SLM capabilities—validated by SLM's tendency to generate more granular subtasks (6.9 vs. 5.8 average for LLM).
- Evidence anchors: [abstract] "decomposes complex tasks"; [Section 3.3.3] Subtask Decomposer logic; Section 2.3 Observation 4 notes SLM generates more granular subtasks; [corpus] Conditional Diffusion Model (arXiv:2511.13137) addresses multi-agent task decomposition but through reinforcement learning, not capability matching.

## Foundational Learning

- **Concept: Agentic task decomposition chains**
  - Why needed here: HERA operates on the observation that AI agents process requests as sequential subtask chains where each subtask's output influences subsequent subtask generation. Understanding this interconnectedness is prerequisite to grasping why independent subtask allocation fails.
  - Quick check question: Can you explain why assigning subtask 1 to SLM might affect the accuracy of subtask 3's output, even if subtask 3 runs on LLM?

- **Concept: Semantic similarity metrics for output comparison**
  - Why needed here: HERA relies on cosine similarity of SBERT embeddings (threshold 0.7) to determine if SLM and LLM outputs are "similar enough." This is the core comparison primitive for all routing decisions.
  - Quick check question: Why might exact string matching fail where semantic similarity succeeds for comparing SLM vs. LLM subtask outputs?

- **Concept: Offline profiling for online decision-making**
  - Why needed here: HERA's estimators (URC, SP, DP, SD) are fine-tuned on profiled data from running requests through both SLM and LLM. The quality of offline profiling directly determines online routing accuracy.
  - Quick check question: What happens to HERA's routing decisions if the profiling dataset doesn't cover the distribution of production queries?

## Architecture Onboarding

- **Component map:** User Request Classifier (URC) -> Subtask Similarity Evaluator (SSE) -> S-L Similarity Evaluator (SLE) -> Convergence Detector (CD) -> Subtask Decomposer (SD) -> recursive SSE on sub-subtasks

- **Critical path:** URC → (if complex) SSE → (if dissimilar) SLE → (if no high similarity) CD → (if no convergence) SD → recursive SSE on sub-subtasks

- **Design tradeoffs:**
  - Higher similarity threshold → higher accuracy, lower SLM usage, higher cost (Figure 13 sensitivity analysis)
  - All-or-nothing subtask decomposition → fewer LLM calls but may miss partial SLM opportunities
  - One-time offline profiling → lower runtime overhead but requires retraining for new domains

- **Failure signatures:**
  - Accuracy drops >5%: Check if profiling data covers production distribution; consider lowering threshold_base
  - SLM usage <30%: Check URC false negative rate; may need more diverse profiling datasets
  - High latency (>50s): Check convergence detector iterations; may be stuck in infinite prediction loops
  - Distance Predictor accuracy <75%: Needs continual fine-tuning with recent queries (Table 5 shows 75.8%→79.1% improvement with fine-tuning)

- **First 3 experiments:**
  1. **Baseline comparison on held-out datasets**: Run HERA on DROP, HumanEval, Webshop (not in training) and compare accuracy/SLM-usage vs. HybridLLM and All-LLM. Expected: 4-6% accuracy gap from All-LLM with 19-30% cost reduction.
  2. **Threshold sensitivity sweep**: Vary similarity threshold from 0.5 to 0.9 on GSM8K. Plot accuracy vs. SLM-usage curve to find pareto-optimal point for your cost-accuracy requirements.
  3. **Ablation of SD component**: Run HERA without Subtask Decomposer on HotpotQA. Expect 0.08-2.14% accuracy increase but 3.65-8.46% SLM usage decrease—validates decomposition's contribution to cost savings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can profiling efficiency be improved to reduce the time-consuming offline data collection required for each new SLM-LLM pair?
- Basis in paper: [explicit] The authors state in Section 5: "The current system requires extensive profiling of each SLM and LLM pair, which can be time-consuming and hard to retrain. Future work could explore more efficient profiling techniques to leverage information from previously profiled models."
- Why unresolved: Each model pair requires generating binary trees of subtasks to depth 15, training multiple estimators (URC, SP, DP, SD), with ~2 hours of fine-tuning per pair.
- What evidence would resolve it: Demonstrating transfer learning between model pairs, or few-shot/zero-shot profiling methods that achieve comparable accuracy without full retraining.

### Open Question 2
- Question: How can HERA be extended to incorporate comprehensive real-world cost factors beyond LLM API usage?
- Basis in paper: [explicit] Section 5 notes: "Our current implementation uses a simplified cost model focusing primarily on LLM API costs" and lists missing factors including "edge device power consumption and hardware costs, data transfer expenses, SLA requirements, hardware utilization, and request queuing patterns."
- Why unresolved: The current framework optimizes only for API costs, ignoring edge operational costs and network transfer fees that affect real deployments.
- What evidence would resolve it: A modified HERA with multi-objective optimization demonstrating Pareto-optimal tradeoffs across cost dimensions in production environments.

### Open Question 3
- Question: Can HERA effectively route subtasks across multiple SLMs and LLMs simultaneously rather than a single SLM-LLM pair?
- Basis in paper: [explicit] Section 5 states: "it could be extended to leverage multiple models simultaneously. This enhancement would enable dynamic model selection based on subtask characteristics and resource constraints."
- Why unresolved: Current architecture assumes one SLM and one LLM; multi-model routing would require model-specific performance profiles and enhanced resource management.
- What evidence would resolve it: Implementation and evaluation of multi-model HERA showing improved accuracy or cost reduction by matching subtasks to specialized models.

## Limitations

- Reliance on sequential subtask chains may not generalize to parallel or non-linear agent workflows where position-based thresholding assumptions break down.
- All-or-nothing subtask decomposition approach may miss opportunities for partial SLM utilization on complex subtasks.
- Effectiveness depends heavily on the quality of offline profiling data; performance may degrade significantly with out-of-distribution production queries.

## Confidence

**High confidence**: The core claim that HERA reduces operational costs by 19-30% while maintaining accuracy within 2-5% of cloud-only approaches is well-supported by the evaluation results across six diverse datasets.

**Medium confidence**: The claim that S-L distance prediction enables reliable speculative SLM execution depends on the assumption that SLM and LLM reasoning trajectories converge on tractable subtasks.

**Low confidence**: The effectiveness of the all-or-nothing subtask decomposition approach for maximizing SLM utilization is less certain, as it may miss partial SLM opportunities.

## Next Checks

1. **Distribution shift validation**: Test HERA on production query logs that weren't part of the profiling dataset to quantify performance degradation when encountering out-of-distribution requests. This validates the assumption that HotPotQA and GSM8K profiling data generalizes to real-world usage.

2. **Partial decomposition evaluation**: Modify the Subtask Decomposer to allow individual sub-subtasks to be assigned independently (rather than all-or-nothing) and measure the impact on SLM utilization and accuracy. This tests whether the current approach leaves money on the table.

3. **Parallel workflow adaptation**: Adapt HERA's routing logic to handle non-sequential agent workflows where subtasks can execute in parallel or have complex dependency graphs. Evaluate whether the position-aware thresholding mechanism needs modification or replacement for these scenarios.