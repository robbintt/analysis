---
ver: rpa2
title: 'AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated
  Scenarios in Autonomous Systems'
arxiv_id: '2601.16964'
source_url: https://arxiv.org/abs/2601.16964
tags:
- scenario
- reasoning
- driving
- autonomous
- open
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AgentDrive introduces a comprehensive benchmark for training and\
  \ evaluating autonomous driving agents using LLM-generated scenarios. It employs\
  \ a factorized scenario space across seven axes\u2014scenario type, driver behavior,\
  \ environment, road layout, objective, difficulty, and traffic density\u2014to generate\
  \ 300K structured driving scenarios via an LLM-driven prompt-to-JSON pipeline."
---

# AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems

## Quick Facts
- arXiv ID: 2601.16964
- Source URL: https://arxiv.org/abs/2601.16964
- Authors: Mohamed Amine Ferrag; Abderrahmane Lakas; Merouane Debbah
- Reference count: 25
- AgentDrive introduces a comprehensive benchmark for training and evaluating autonomous driving agents using LLM-generated scenarios

## Executive Summary
AgentDrive introduces a comprehensive benchmark for training and evaluating autonomous driving agents using LLM-generated scenarios. It employs a factorized scenario space across seven axes—scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density—to generate 300K structured driving scenarios via an LLM-driven prompt-to-JSON pipeline. Each scenario is simulated to produce safety metrics and outcome labels. AgentDrive-MCQ extends this with 100K reasoning questions across five dimensions: physics, policy, hybrid, scenario, and comparative. Evaluations of 50 leading LLMs, including GPT-5, ChatGPT 4o, and Qwen3 235B, show that while proprietary models excel in contextual and policy reasoning, advanced open models are rapidly approaching parity in structured and physics-grounded reasoning tasks.

## Method Summary
AgentDrive generates 300K autonomous driving scenarios by sampling from a factorized scenario space across seven orthogonal axes, then uses an LLM-driven prompt-to-JSON pipeline to create simulation-ready specifications that are validated against physical and schema constraints. Each scenario is executed in the highway-env simulator to produce safety metrics (TTC, headway) and outcome labels. AgentDrive-MCQ adds 100K multiple-choice questions spanning five reasoning dimensions (physics, policy, hybrid, scenario, comparative) to evaluate LLMs' driving reasoning capabilities. The pipeline uses entropy-maximized sampling to ensure diverse coverage, with difficulty-specific constraints applied during prompt construction.

## Key Results
- Proprietary models like ChatGPT 4o and GPT-5 achieve 72.5% and 70.0% accuracy on hybrid reasoning tasks, while advanced open models like Qwen3 235B reach 68.8%
- Models show strong performance on policy (up to 90.0%) and comparative reasoning (up to 88.7%) but struggle with hybrid tasks requiring both physics calculations and policy understanding
- The benchmark reveals a clear accuracy ordering: physics < hybrid < policy < scenario < comparative across evaluated models

## Why This Works (Mechanism)

### Mechanism 1: Factorized Scenario Space Enables Systematic Coverage
Decomposing driving scenarios into seven orthogonal axes (scenario type, driver behavior, environment, road layout, objective, difficulty, traffic density) provides controlled, reproducible coverage of both routine and safety-critical conditions. Each scenario is a tuple s = (t, b, e, r, o, d, q). Entropy-maximized sampling across the Cartesian product S = T × B × E × R × O × D × Q prevents collapse into trivial cases while enabling targeted stress-testing along individual dimensions.

### Mechanism 2: LLM-to-JSON Pipeline Bridges Semantic Richness with Simulation Readiness
An LLM-driven prompt-to-JSON transformation produces physically plausible, simulation-ready specifications from natural language scenario descriptions. Prompts P(s, H(d)) = f(s) ∪ H(d) combine semantic descriptions with difficulty-specific numerical constraints (e.g., min_ttc thresholds). Schema validation and post-processing module Π enforce structural and physical compliance with up to R retries.

### Mechanism 3: Five-Dimensional MCQ Benchmark Probes Distinct Reasoning Modalities
Structured multiple-choice questions across physics, policy, hybrid, scenario, and comparative dimensions reveal differentiated strengths and weaknesses in LLM driving reasoning. Each MCQ style targets specific cognitive demands: physics requires numerical kinematic calculations; policy tests rule interpretation; hybrid demands integration of both. Correct answers are aligned with safe driving strategies.

## Foundational Learning

- **Factorized Scenario Representation**
  - Why needed here: Understanding how scenarios decompose into orthogonal axes is prerequisite to using or extending the dataset.
  - Quick check question: Given a scenario tuple (highway_merging, aggressive, rain, curved, overtake, hard, high), which axis controls the minimum TTC constraint?

- **Surrogate Safety Metrics (TTC, Headway)**
  - Why needed here: Labels are derived from metrics like minimum time-to-collision; interpreting results requires understanding these thresholds.
  - Quick check question: What does TTC_min < 0.5s signify about an episode, and how does it map to the "unsafe" label?

- **Schema-Validated LLM Generation**
  - Why needed here: The pipeline relies on JSON schema enforcement; debugging failed generations requires understanding validation logic.
  - Quick check question: If an LLM generates duration_steps=30 with policy_frequency=10 Hz, why might this fail validation?

## Architecture Onboarding

- **Component map:**
  1. Scenario Sampler → draws tuples from factorized space S
  2. Prompt Builder → combines f(s) with H(d) constraints
  3. LLM Generator Pool → produces candidate JSON (GPT-4o, DeepSeek, Qwen used in generation)
  4. Schema Validator + Repair Module Π → enforces C, retries up to R
  5. highway-env Simulator M → produces rollouts τ
  6. Safety Metric Computer → TTC_min, headway
  7. Rule-Based Labeler → assigns {unsafe, safe_goal, safe_stop, inefficient}
  8. MCQ Generator → produces 5 questions per validated scenario

- **Critical path:** Scenario sampling → Prompt construction → LLM JSON generation → Validation → Simulation → Metric computation → Labeling. MCQ generation runs in parallel post-validation.

- **Design tradeoffs:**
  - highway-env offers efficiency but limited visual fidelity vs. CARLA
  - Text-based MCQs scale well but lack multimodal grounding (paper acknowledges this limitation)
  - Rule-based labeling is interpretable but may miss nuanced safety judgments

- **Failure signatures:**
  - High Π retry rates → LLM struggling with schema/physics constraints
  - Imbalanced label distribution → entropy sampling not functioning correctly
  - MCQs with all-numeric options in "scenario" style → style constraint violation

- **First 3 experiments:**
  1. Run 100 scenario generations with each LLM in the pool; log validation pass rates and retry counts to identify which models best respect schema/physics constraints.
  2. Execute 1,000 validated scenarios in simulation; verify label distribution matches Figure 4 and entropy across axes is maximized.
  3. Evaluate 3 models (one proprietary, one large open, one small open) on 500 MCQs; confirm physics < hybrid < policy accuracy ordering holds as reported in Table V.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multimodal (vision-language) LLMs maintain their reasoning accuracy when visual contexts are represented directly as images rather than textual descriptions?
- Basis in paper: [explicit] The paper states in Table I that AgentDrive is "currently limited to text-based reasoning where visual contexts are represented as textual descriptions; multimodal (visual–language) LLMs to be integrated in future work."
- Why unresolved: The benchmark was designed for text-only inputs; no experiments were conducted with actual visual inputs to test whether reasoning degrades or improves with direct perception.
- What evidence would resolve it: Run the same 100K MCQ benchmark on vision-language models (e.g., GPT-4V, Gemini Pro Vision) with rendered scenario images alongside text, comparing accuracy to text-only baselines.

### Open Question 2
- Question: What architectural or training modifications would enable LLMs to achieve consistent performance on hybrid reasoning tasks that require integrating physics calculations with policy constraints?
- Basis in paper: [explicit] The paper concludes that "hybrid reasoning—requiring the fusion of conceptual policy understanding and numerical grounding—remains an unresolved challenge in current LLM architectures," with even top models (ChatGPT 4o: 72.5%, GPT-5: 70.0%) showing significant gaps.
- Why unresolved: Current LLMs trained on text corpora lack explicit mechanisms for composing symbolic policy rules with numerical physics computations under uncertainty.
- What evidence would resolve it: Train models with structured hybrid-reasoning datasets (physics+policy co-labeled) and evaluate whether accuracy approaches the 90%+ levels seen in standalone policy or comparative tasks.

### Open Question 3
- Question: How well do LLM reasoning capabilities on AgentDrive-MCQ transfer to real-world driving scenarios with actual sensor data and physical vehicle dynamics?
- Basis in paper: [inferred] The paper uses highway-env simulation and acknowledges future plans to "integrate real-world sensor data," suggesting the sim-to-real transfer gap remains unquantified.
- Why unresolved: No experiments validated whether high accuracy on the benchmark correlates with safe or reliable decision-making in real autonomous vehicle deployments.
- What evidence would resolve it: Deploy top-performing models (e.g., ChatGPT 4o, Qwen3 235B) on real vehicle platforms or high-fidelity simulators with real sensor inputs, measuring correlation between MCQ accuracy and driving safety metrics.

## Limitations
- Multimodal integration remains future work; current benchmark relies solely on text-based representations of visual scenarios
- Performance gaps in hybrid reasoning tasks suggest current LLMs struggle with integrating physics calculations and policy constraints
- Sim-to-real transfer validity is untested; benchmark uses highway-env simulation without validation on real-world sensor data

## Confidence
- **High Confidence**: Factorized scenario space design and its orthogonal axis decomposition are clearly articulated and methodologically sound. The safety metric definitions (TTC thresholds, label rules) are explicitly specified.
- **Medium Confidence**: The LLM-to-JSON pipeline mechanism is plausible based on validation retries and schema enforcement, but exact prompt structures and failure handling are underspecified. MCQ generation process is outlined but lacks full implementation details.
- **Low Confidence**: Claims about model performance rankings depend heavily on which specific LLMs were used for generation versus evaluation, information not fully disclosed. The correlation between MCQ accuracy and real-world driving competence remains assumed rather than empirically validated.

## Next Checks
1. Implement the full prompt template with all difficulty constraints (H(easy), H(medium), H(hard)) and measure validation pass rates across 1,000 generations to assess LLM reliability.
2. Execute the complete pipeline end-to-end for 100 scenarios, verifying that TTC_min calculations, event detection, and label assignments match the published distributions.
3. Run a small-scale LLM evaluation (3 models) on 50 MCQs to confirm the physics < hybrid < policy accuracy ordering holds as reported in Table V.