---
ver: rpa2
title: 'ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World
  APIs'
arxiv_id: '2512.16149'
source_url: https://arxiv.org/abs/2512.16149
tags:
- tool
- data
- arxiv
- reasoning
- tool-calling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ToolForge introduces an automated data synthesis pipeline that
  generates multi-hop search data without real API calls. It uses a small set of virtual
  tools and leverages a (question, golden context, answer) triple to synthesize complex
  tool-calling scenarios with reasoning and self-reflection.
---

# ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs

## Quick Facts
- arXiv ID: 2512.16149
- Source URL: https://arxiv.org/abs/2512.16149
- Authors: Hao Chen; Zhexin Hu; Jiajun Chai; Haocheng Yang; Hang He; Xiaohan Wang; Wei Lin; Luhang Wang; Guojun Yin; Zhuofeng zhao
- Reference count: 40
- Primary result: ToolForge-8B achieves state-of-the-art on 8/10 real-world benchmarks using only synthetic training data

## Executive Summary
ToolForge introduces an automated pipeline that generates multi-hop search data without real API calls by leveraging virtual tools and synthetic reasoning chains. It transforms (question, golden context, answer) triples into complex tool-calling scenarios through Generative Interaction Modeling and validates them with a Multi-Layer Validation Framework. When fine-tuned on just 4,250 synthetic samples, an 8B model achieves state-of-the-art performance on real-world benchmarks while maintaining strong zero-shot generalization to unseen tool tasks.

## Method Summary
ToolForge uses a three-stage pipeline: Knowledge Space Planning defines 19 virtual tools with 20 variants each, Generative Interaction Modeling synthesizes multi-turn dialogues with reasoning chains and error perturbations, and Multi-Layer Validation ensures data quality through rule-based and model-based checks. The pipeline transforms (question, golden context, answer) triples into tool-calling training data without real API execution, using diversity filtering and error simulation to create robust synthetic examples.

## Key Results
- ToolForge-8B outperforms GPT-4o on 8 out of 10 real-world benchmarks
- Achieves state-of-the-art performance with only 4,250 synthetic training samples
- Maintains strong zero-shot generalization to unseen tool tasks
- Optimal 9:1 single-hop to multi-hop data ratio critical for performance

## Why This Works (Mechanism)

### Mechanism 1: Virtual Tool Abstraction as API Surrogate
A small set of virtual tools substitutes for thousands of real API calls while preserving generalization through dual-gating diversity filtering that ensures semantic relevance while penalizing redundancy.

### Mechanism 2: Error Perturbation for Self-Reflection Training
Synthesizing error scenarios (tool misselection, argument errors, tool switching) teaches models to self-correct by exposing them to 29 interaction patterns combining four paradigms with three perturbation classes.

### Mechanism 3: Multi-Layer Validation Prevents Synthetic Data Poisoning
Rule-based and model-based validation is necessary to prevent low-quality synthetic data from degrading SFT, with validation accuracy of 98.47% but 1.53% error rate that compounds during training.

## Foundational Learning

- **Multi-hop reasoning chains**: Needed to decompose questions into sub-problems where each tool output feeds the next call; Quick check: Can you trace the dependency chain in "Who was the director of The Band?"
- **Tool-calling paradigms**: The four paradigms (SRST, SRMT, MRST, MRMT) define complexity axes for data synthesis; Quick check: Which paradigm applies to "Compare release years of two films" and why?
- **Monte Carlo Tree Search**: Used for hard negative mining in MLV to explore error combinations; Quick check: Why is random sampling insufficient compared to MCTS-guided exploration?

## Architecture Onboarding

- **Component map**: Input (question, golden_context, answer) triple → KSP Virtual Tool Library + Paradigm Selection → GIM Planning → Augmentation → Generation → Assembly → MLV Rule Checker → Model Checker → Output Validated multi-turn dialogue
- **Critical path**: The GIM → MLV pipeline; if validation fails, sample is discarded; 83.8% MRMT success rate is the bottleneck for complex patterns
- **Design tradeoffs**: 9:1 single-hop/multi-hop ratio optimal; zero API cost but may not capture real API quirks; rules are cheap, model checks are expensive but catch semantic errors
- **Failure signatures**: Low MRMT success rate (<85%) suggests GIM planning issues; high validation rejection indicates golden context quality problems; model fails on unseen tools suggests insufficient virtual tool diversity
- **First 3 experiments**: 1) Replicate 9:1 ratio ablation on domain data; 2) Test ToolForge-8B on target API schemas; 3) Ablate MLV layers to quantify validation ROI

## Open Questions the Paper Calls Out

### Open Question 1
Does the optimal 9:1 single-hop to multi-hop data ratio persist when scaling dataset size beyond 4,250 samples or when fine-tuning larger models (70B+)? The ablation study varies ratio only within fixed 4,250 samples and 8B model constraint.

### Open Question 2
How can the model's robustness be improved to bridge performance gap between controlled retrieval environments and noisy, web-scale corpora? The paper identifies sensitivity to retrieval quality but doesn't explore synthesis modifications to mitigate degradation.

### Open Question 3
To what extent does virtual tool simulation transfer to complex, stateful, or non-textual real-world APIs? Benchmarks focus on QA and function calling that retrieves information, leaving efficacy for action-oriented or multimodal tool use unverified.

### Open Question 4
What are performance trade-offs when scaling virtual tool library from 19 tools to larger ecosystem (100+ tools)? The method claims extensibility but experimental validation limited to 19 manually designed base tools.

## Limitations
- Virtual tool transferability to real API schemas remains unverified beyond Wikipedia-based tasks
- Validation completeness concerns with 1.53% error rate compounding during training
- Tool diversity sufficiency of 19 base tools × 20 variants may not capture full real-world API ecosystem

## Confidence
- **High**: MLV architecture is well-specified and empirically validated; 9:1 ratio finding is robust; training pipeline is reproducible
- **Medium**: ToolForge-8B outperforming GPT-4o claim supported but evaluation methodology may not reflect production conditions; synthetic data quality acceptable but room for improvement
- **Low**: Virtual tool abstraction transferability to unseen API schemas is asserted but untested; self-reflection mechanism effectiveness beyond three perturbation classes is speculative

## Next Checks
1. **Cross-Domain Transfer Test**: Evaluate ToolForge-8B on non-retrieval APIs (calendar scheduling, database queries, payment processing) to verify virtual tool generalization
2. **Validation Cost-Benefit Analysis**: Systematically vary validation strictness and measure tradeoff between data quality and compute cost across different training budgets
3. **Tool Diversity Stress Test**: Remove 25% of virtual tool types and retrain to identify critical versus redundant tools for cross-domain performance