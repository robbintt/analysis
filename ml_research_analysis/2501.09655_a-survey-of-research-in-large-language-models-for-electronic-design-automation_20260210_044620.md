---
ver: rpa2
title: A Survey of Research in Large Language Models for Electronic Design Automation
arxiv_id: '2501.09655'
source_url: https://arxiv.org/abs/2501.09655
tags:
- design
- llms
- generation
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively examines the applications of Large
  Language Models (LLMs) in Electronic Design Automation (EDA), focusing on three
  main design stages: system-level design, RTL design, and logic synthesis/physical
  design. The study reveals that while LLMs have shown significant promise in automating
  HDL generation, script writing, and design specification tasks, their performance
  varies significantly based on model architecture, size, and customization techniques.'
---

# A Survey of Large Language Models for Electronic Design Automation

## Quick Facts
- **arXiv ID**: 2501.09655
- **Source URL**: https://arxiv.org/abs/2501.09655
- **Reference count**: 40
- **Primary result**: Current LLM approaches achieve up to 94.2% functional correctness in Verilog code generation, with promising future directions in multi-modal circuit representations.

## Executive Summary
This comprehensive survey examines the rapidly evolving intersection of Large Language Models and Electronic Design Automation (EDA), revealing significant advancements in automating design tasks across three critical stages: system-level design, RTL design, and logic synthesis/physical design. The study demonstrates that while LLMs show remarkable promise in generating hardware description code, writing automation scripts, and processing design specifications, their performance is highly dependent on model architecture, size, and customization techniques. Current state-of-the-art approaches combining advanced prompt engineering with domain-specific fine-tuning achieve impressive results, but face substantial challenges in scaling to industrial complexity and maintaining reliability in verification processes. The survey identifies critical gaps in evaluation frameworks and highlights the need for standardized benchmarks that better reflect real-world design constraints.

## Method Summary
The survey systematically reviews existing research on LLM applications in EDA, focusing on three main design stages and analyzing performance metrics across different model architectures and customization approaches. The methodology involves comprehensive literature analysis, benchmarking studies using VerilogEval-Human v2, and identification of key technical challenges and future research directions. The study particularly emphasizes the importance of autonomous agent frameworks for functional debugging and the potential of multi-modal representations for capturing circuit characteristics beyond text-based descriptions.

## Key Results
- State-of-the-art LLM approaches achieve 94.2% functional correctness in Verilog code generation using autonomous agent frameworks
- Performance varies significantly based on model architecture, with GPT-4-turbo showing superior results compared to smaller models
- Current approaches face scalability challenges when transitioning from academic benchmarks to industrial-scale designs

## Why This Works (Mechanism)
The effectiveness of LLMs in EDA stems from their ability to process natural language specifications and translate them into formal hardware descriptions, combined with their capacity for pattern recognition in design patterns and code structures. The autonomous agent framework enhances this capability by providing systematic debugging through AST-based waveform tracing, allowing the model to iteratively refine code based on simulation feedback. The integration of task-based planning enables decomposition of complex specifications into manageable design components, while domain-specific fine-tuning adapts general language understanding to the specialized vocabulary and constraints of hardware design.

## Foundational Learning
**Hardware Description Languages (HDL)**: Verilog and VHDL are specialized programming languages for describing digital circuits; needed for LLM to generate synthesizable hardware designs. Quick check: Can the model correctly identify and use standard HDL constructs like always blocks and modules.

**Static Timing Analysis (STA)**: Traditional mathematical methods for verifying timing constraints in digital circuits; needed as a performance benchmark for LLM timing estimation capabilities. Quick check: Compare LLM-predicted timing slack against STA tool results on identical netlists.

**Abstract Syntax Trees (AST)**: Tree representation of code structure used for program analysis; needed for the waveform tracing tool to map simulation results back to code logic. Quick check: Can the AST parser correctly identify signal dependencies in a given Verilog module.

**Electronic Design Automation (EDA) Toolchain**: The integrated suite of software tools for designing integrated circuits; needed for context on where LLMs can augment existing workflows. Quick check: Identify specific points in the design flow where LLM assistance would provide the most value.

**Multi-modal Circuit Representation**: Combining text, graph, and image features to capture circuit characteristics; needed for handling spatial and hierarchical aspects missed by text-only approaches. Quick check: Demonstrate improved PPA prediction using combined feature representations versus text alone.

## Architecture Onboarding

**Component Map**: Natural Language Spec -> Task Planner -> Code Generator -> Simulator -> AST Waveform Analyzer -> LLM Refiner -> Verilog Output

**Critical Path**: The feedback loop from simulator output through AST waveform analysis to LLM refiner represents the most critical path, as this iterative debugging process directly determines functional correctness rates.

**Design Tradeoffs**: Larger models (70B+) offer better understanding but require more computational resources and may overfit on small domain datasets; smaller models are more efficient but may miss complex design patterns. The tradeoff between autonomous agents (slower but more accurate) versus direct code generation (faster but less reliable) significantly impacts practical deployment.

**Failure Signatures**: 
- Syntax errors in generated code indicate prompt engineering issues
- Logical mismatches despite correct syntax suggest insufficient domain understanding
- Infinite correction loops reveal limitations in the autonomous agent's rollback mechanism
- Simulator incompatibility errors point to library or primitive mismatches

**First Experiments**:
1. Generate simple combinational logic (2-3 gates) from natural language and verify functional correctness
2. Test the autonomous agent's debugging capability on a deliberately injected syntax error
3. Evaluate the scaling behavior by generating increasingly complex modules (100 gates → 1K gates) while measuring execution time and correctness

## Open Questions the Paper Calls Out

**Open Question 1**: Can multi-modal representations combining graph-based and image-based encoders with LLMs outperform text-only approaches for physical design tasks?
Based on explicit identification of Graph-Based and Image-Based Circuit Features as future directions, this remains unresolved due to the field's predominant reliance on textual representations. Resolution would require benchmark demonstrations showing improved PPA predictions or floorplan evaluations using GNN or vision encoders compared to text-only baselines.

**Open Question 2**: Can LLMs achieve the mathematical precision required for static timing estimation in backend design flows?
The paper explicitly identifies timing estimation as particularly challenging because LLMs must capture intricate relationships between physical layout and parasitic effects while maintaining mathematical precision. This fidelity gap between probabilistic LLM outputs and exact STA requirements remains unresolved without demonstrated convergence on complex netlists or successful acceleration of timing convergence via LLM optimizations.

**Open Question 3**: How does increasing LLM parameter scale (e.g., to 70B+) impact performance in EDA tasks relative to dataset size constraints?
The survey infers this gap from the lack of comprehensive research on optimal pretrained LLM selection and the observation that few studies scale beyond small datasets (e.g., 8k samples). This question remains unresolved without scaling law analysis correlating model size and dataset volume for HDL generation correctness and logic synthesis quality.

## Limitations
- Heavy reliance on academic benchmarks that may not represent industrial-scale design complexity
- Reported functional correctness rates may not scale to multi-million gate designs typical in semiconductor manufacturing
- Current evaluation metrics focus primarily on functional correctness without adequately addressing timing closure, power optimization, or physical implementation quality

## Confidence
- **High Confidence**: Identification of current LLM capabilities in HDL generation and script automation, supported by multiple independent studies and benchmark results
- **Medium Confidence**: Assessment of scalability challenges and industrial adoption barriers, based on expert opinions and limited industrial case studies
- **Medium Confidence**: Proposed future directions involving multi-modal representations, though specific technical implementations remain largely theoretical

## Next Checks
1. Reproduce the 94.2% functional correctness rate on VerilogEval-Human v2 using the described autonomous agent framework with a different Verilog simulator to verify robustness
2. Scale test with increasing design complexity by applying the best-performing LLM approach to progressively larger Verilog modules (100 gates → 1K gates → 10K gates) while measuring functional correctness and execution time
3. Cross-validate multi-modal feature representation by implementing a small-scale prototype that combines graph-based circuit features with text-based specifications to evaluate improvement over pure text-based methods on existing benchmarks