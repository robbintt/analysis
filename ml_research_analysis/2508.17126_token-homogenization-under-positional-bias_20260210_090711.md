---
ver: rpa2
title: Token Homogenization under Positional Bias
arxiv_id: '2508.17126'
source_url: https://arxiv.org/abs/2508.17126
tags:
- original
- bias
- positional
- homogenization
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates token homogenization\u2014the convergence\
  \ of token representations toward uniformity across transformer layers\u2014and\
  \ its relationship to positional bias in large language models. The authors introduce\
  \ a suite of metrics including effective rank, maximum explainable variance, Schatten\
  \ norms, resultant length, and MAUVE scores to quantify homogenization."
---

# Token Homogenization under Positional Bias

## Quick Facts
- arXiv ID: 2508.17126
- Source URL: https://arxiv.org/abs/2508.17126
- Reference count: 40
- One-line primary result: Positional bias amplifies token homogenization in transformers, measured through spectral metrics and attention analysis.

## Executive Summary
This paper investigates token homogenization—the convergence of token representations toward uniformity across transformer layers—and its relationship to positional bias in large language models. The authors introduce a suite of metrics including effective rank, maximum explainable variance, Schatten norms, resultant length, and MAUVE scores to quantify homogenization. Through controlled experiments on modified movie review datasets with key words positioned at the beginning or end, they demonstrate that positional bias amplifies homogenization, with synthetic datasets showing lower effective rank and higher anisotropy compared to the original dataset. The study establishes a clear connection between positional bias and token homogenization across three models (LLaMA-3, Gemma, Qwen), with homogenization metrics consistently revealing that positional bias increases token similarity through layer-wise propagation.

## Method Summary
The authors construct synthetic datasets by extracting key phrases from 1000 IMDB reviews and rephrasing them with key phrases positioned at the beginning (Front) or end (End) of sentences, validated via BERTScore (>0.89). They run forward passes through LLaMA-3-8B, Gemma-7B, and Qwen-2.5-7B, extracting token representation matrices at each layer. Five metrics are computed: effective rank (spectral entropy), maximum explainable variance (MEV), Schatten p-norms, resultant length of normalized vectors, and MAUVE scores comparing consecutive layer distributions. Positional bias is detected by analyzing column-wise attention sums. The study compares these metrics across original and synthetic datasets to quantify homogenization differences.

## Key Results
- Positional bias increases token homogenization, with Front and End synthetic datasets showing significantly lower effective rank and higher maximum explainable variance than the original dataset.
- Homogenization occurs progressively across layers, with the most rapid change in middle layers for most models.
- Attention analysis reveals positional bias concentrates on first tokens in synthetic datasets, with lower average attention than original dataset.
- The proposed spectral metrics consistently separate original from positionally-biased datasets across all three tested models.

## Why This Works (Mechanism)

### Mechanism 1: Attention-Induced Information Mixing Toward Rank Collapse
Repeated self-attention operations progressively reduce representational diversity by averaging token information across positions. Self-attention computes weighted averages over token representations, and when applied repeatedly across layers, this mixing converges toward uniform representations—formally, toward a rank-1 subspace where all tokens become nearly identical. This tendency is partially mitigated by residual connections but not fully prevented.

### Mechanism 2: Positional Bias Amplifies Homogenization via Additive Attention Alignment
When positional attention (biased toward extremal tokens) aligns with contextual attention, total attention weight on those tokens increases, accelerating their dominance in the mixing process. The paper models total attention as A = λ₁A_cont + λ₂A_pos (λ₁ + λ₂ = 1). When positional bias concentrates on the first token and λ₂ is sufficiently large, recursive application yields x_i^(l) ≈ x_1^(l) for all i—explicit homogenization toward the positionally-favored token.

### Mechanism 3: Layer-wise Spectral Decay as a Diagnostic Signal
Homogenization manifests as measurable changes in the spectral properties of token representation matrices across layers—specifically, reduced effective rank, increased maximum explainable variance (MEV), and altered Schatten norm ratios. As tokens converge toward similar vectors, the representation matrix X^(l) loses rank. The effective rank (spectral entropy-based), MEV (ratio of largest singular value squared to sum), and Schatten norms capture this decay continuously, avoiding brittleness of discrete rank computation.

## Foundational Learning

- **Concept: Self-attention and information mixing**
  - Why needed here: Homogenization is fundamentally driven by how attention matrices aggregate token information across positions.
  - Quick check question: Given an attention matrix A and token embeddings X, what is the output of one attention layer, and why does repeated application tend to reduce diversity?

- **Concept: Singular value decomposition (SVD) and spectral analysis**
  - Why needed here: All primary metrics (effective rank, MEV, Schatten norms) derive from the singular value spectrum of representation matrices.
  - Quick check question: For a matrix X ∈ R^(n×d), what does a rapidly decaying singular value spectrum imply about the geometric distribution of its row vectors?

- **Concept: Positional bias in transformers**
  - Why needed here: The paper hypothesizes that positional bias (systematic attention to extremal tokens) is a key amplifier of homogenization.
  - Quick check question: What is the "U-shaped attention pattern" in LLMs, and which architectural or training factors contribute to it?

## Architecture Onboarding

- **Component map**: Input token embeddings X^(0) ∈ R^(n×d) → Per-layer self-attention A^(l) → weighted aggregation → output X^(l+1) → Positional attention A_pos concentrates on first token → Diagnostics: Extract X^(l) per layer, compute SVD, derive effective rank / MEV / Schatten norms / resultant length / MAUVE

- **Critical path**: 1) Run inference on text dataset (original IMDB reviews and synthetic variants). 2) Capture hidden states X^(l) at each transformer layer. 3) Compute spectral and distributional metrics per layer; compare across datasets (original vs. Front vs. End).

- **Design tradeoffs**: Effective rank is continuous and robust to noise but does not distinguish useful compression from harmful collapse. MEV intuitively captures anisotropy but can be confounded by antonymous vectors with opposite directions. MAUVE is novel for activation analysis but interpretation depends on embedding quantization and distributional assumptions.

- **Failure signatures**: No separation between original and positionally-biased datasets → metrics may not capture homogenization or manipulation did not induce bias. Metric values plateau early → suggests most representational change occurs in early layers; later layers may be redundant. High variance across samples → homogenization may be context-dependent rather than a universal architectural property.

- **First 3 experiments**:
  1. Replicate the paper's setup: Run LLaMA-3 8B on original IMDB reviews and the Front/End synthetic variants. Plot effective rank and MEV across layers to verify separation.
  2. Attention analysis: Visualize column-wise attention sums per token position to confirm positional bias is stronger in synthetic datasets.
  3. Ablation by layer depth: Compute metrics for subsets of layers (first 25%, middle 50%, last 25%) to localize where homogenization accelerates and assess whether certain layers are critical intervention points.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does token homogenization causally degrade model performance, increase vulnerability to adversarial attacks, or exacerbate hallucination tendencies? The paper establishes correlation between positional bias and homogenization metrics but does not measure downstream task performance or adversarial robustness.

- **Open Question 2**: Do the proposed metrics capture positional bias specifically, or do they respond to artifacts of text modification independent of bias? Synthetic datasets differ from originals in both word positioning and paraphrasing; no control condition isolates positional changes from stylistic changes.

- **Open Question 3**: What explains the layer-wise peaks and troughs observed in effective rank and maximum explainable variance metrics across transformer layers? The paper documents non-monotonic behavior but does not analyze what semantic or functional properties correlate with metric peaks at specific layers.

## Limitations

- The synthetic dataset construction relies on LLM-based keyword extraction and rephrasing using models that are not publicly accessible, necessitating substitutions that may alter the bias induction mechanism.
- The exact layer-wise activation extraction point is unspecified, which could significantly affect metric values since normalization and non-linearities alter spectral properties.
- While consistent metric separation is demonstrated across three models, causation is not established through ablation studies or architectural interventions that would directly modify positional bias.

## Confidence

- **High confidence**: The relationship between positional bias and token homogenization as demonstrated through consistent metric separation across multiple models (LLaMA-3, Gemma, Qwen).
- **Medium confidence**: The mechanistic claim that positional bias amplifies homogenization through additive attention decomposition.
- **Low confidence**: The interpretation of MAUVE scores as detecting homogenization, since this metric is applied to token representations in a novel way without establishing its sensitivity to representational collapse.

## Next Checks

1. **Ablation study of positional bias**: Modify the attention mechanism to reduce or eliminate positional bias (e.g., remove rotary positional embeddings or apply attention bias correction) and verify whether homogenization metrics correspondingly decrease, establishing direct causation.

2. **Metric sensitivity analysis**: Test the spectral metrics on controlled synthetic data where tokens are known to homogenize (e.g., progressively averaged vectors) versus data where representations remain diverse but low-dimensional, to validate that metrics distinguish harmful collapse from useful compression.

3. **Layer-wise intervention experiment**: Identify the layer range where homogenization accelerates most rapidly, then apply targeted regularization (e.g., diversity-promoting loss) to only those layers and measure whether it effectively mitigates homogenization without degrading task performance.