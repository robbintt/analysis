---
ver: rpa2
title: 'PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis'
arxiv_id: '2510.04291'
source_url: https://arxiv.org/abs/2510.04291
tags:
- sentiment
- persian
- such
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid framework for Persian aspect-based
  sentiment analysis (ABSA) that combines multilingual BERT-derived polarity scores
  with a decision tree classifier, achieving state-of-the-art accuracy of 93.34% on
  the Pars-ABSA dataset. The method addresses key challenges in Persian sentiment
  analysis such as limited labeled datasets, morphological complexity, and lack of
  high-quality embeddings.
---

# PABSA: Hybrid Framework for Persian Aspect-Based Sentiment Analysis

## Quick Facts
- arXiv ID: 2510.04291
- Source URL: https://arxiv.org/abs/2510.04291
- Reference count: 30
- Accuracy: 93.34% on Pars-ABSA dataset

## Executive Summary
This paper presents a hybrid framework for Persian aspect-based sentiment analysis (ABSA) that combines deep learning contextual embeddings with traditional machine learning. The method addresses key challenges in Persian sentiment analysis including limited labeled datasets, morphological complexity, and lack of high-quality embeddings. By integrating multilingual BERT-derived polarity scores with a decision tree classifier, the approach achieves state-of-the-art accuracy of 93.34% on the Pars-ABSA dataset. The authors also introduce a Persian synonym and named entity dictionary to support text augmentation, enhancing model robustness and generalization.

## Method Summary
The hybrid framework consists of two main stages: feature extraction and classification. First, a fine-tuned multilingual BERT model generates polarity scores (sentiment probabilities) for each input text-aspect pair. These scores serve as features for a decision tree classifier, which makes the final sentiment prediction. The approach leverages the contextual understanding of deep learning models while utilizing the interpretability and efficiency of traditional machine learning. Text augmentation is performed using a novel Persian synonym and named entity dictionary to expand the training data. The methodology specifically targets the challenges of Persian language processing, including its morphological complexity and limited available resources.

## Key Results
- Achieved 93.34% accuracy and 92.00% F1-score on the Pars-ABSA dataset
- Outperformed baseline models including ParsBERT and LCF-BERT
- Demonstrated effectiveness of combining ML and DL techniques for low-resource language sentiment analysis

## Why This Works (Mechanism)
The hybrid approach works by combining the contextual understanding of multilingual BERT with the decision-making capabilities of a decision tree classifier. BERT provides rich, context-aware polarity scores that capture nuanced sentiment relationships, while the decision tree can learn interpretable decision boundaries based on these features. This combination addresses the limitations of both approaches when used in isolation for Persian ABSA. The text augmentation using Persian-specific dictionaries helps overcome data scarcity issues and improves model generalization across diverse linguistic expressions.

## Foundational Learning
- **Persian Text Preprocessing**: Essential for handling Persian-specific normalization and tokenization; quick check: verify tokenization consistency across preprocessing tools
- **BERT Fine-tuning for Classification**: Required to adapt multilingual BERT to the ABSA task; quick check: monitor loss convergence during fine-tuning
- **Decision Tree Classification**: Needed to understand how polarity scores translate to final predictions; quick check: examine feature importance scores
- **Text Augmentation**: Critical for low-resource languages; quick check: measure vocabulary expansion after augmentation
- **Persian Morphology**: Important for understanding language-specific challenges; quick check: analyze token-level complexity metrics

## Architecture Onboarding

**Component Map**: Text → BERT Fine-tuning → Polarity Scores → Decision Tree → Sentiment Prediction

**Critical Path**: The most important sequence is text preprocessing → BERT fine-tuning → feature extraction → decision tree training. Each step must complete successfully for the final model to work.

**Design Tradeoffs**: The hybrid approach trades pure end-to-end deep learning for interpretability and potentially better performance with limited data. The decision tree offers transparency but may be limited by the quality of BERT features.

**Failure Signatures**: 
- Low BERT accuracy indicates issues with fine-tuning or data quality
- High BERT but low Decision Tree accuracy suggests feature extraction problems or classifier misconfiguration
- Poor performance on neutral classes may indicate class imbalance issues

**Three First Experiments**:
1. Fine-tune M-BERT on the ABSA task and evaluate standalone performance
2. Train Decision Tree using only BERT embeddings as features
3. Compare results with and without text augmentation to measure its impact

## Open Questions the Paper Calls Out
1. Can the hybrid framework effectively resolve complex linguistic phenomena such as sarcasm, ambiguity, and irony in Persian text?
2. How robust is the proposed model when applied to Persian domains other than e-commerce, such as social media or news?
3. Does integrating more advanced transformer architectures (e.g., RoBERTa, DeBERTa, or Persian-specific LLMs) into the hybrid framework yield significant performance gains over Multilingual BERT?

## Limitations
- Critical implementation details missing, particularly regarding feature extraction specifications
- Persian synonym and named entity dictionary not publicly available, limiting reproducibility
- Decision tree hyperparameters not specified, making exact replication challenging

## Confidence
- **High Confidence**: Reported state-of-the-art results and overall problem formulation
- **Medium Confidence**: General methodology combining M-BERT with Decision Tree, but implementation specifics unclear
- **Low Confidence**: Benefits of Persian dictionary and exact feature extraction process remain uncertain

## Next Checks
1. Clarify feature input to Decision Tree by testing different configurations (softmax probabilities vs. embeddings)
2. Assess impact of text augmentation by comparing performance with and without dictionary-based augmentation
3. Perform hyperparameter sensitivity analysis for Decision Tree to determine robustness to parameter choices