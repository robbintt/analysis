---
ver: rpa2
title: 'The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference'
arxiv_id: '2511.23455'
source_url: https://arxiv.org/abs/2511.23455
tags:
- price
- benchmark
- data
- prices
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors measured how much it costs to run frontier language
  models on common AI benchmarks, using historical price data from Artificial Analysis
  and benchmark performance data from Epoch AI. They regressed benchmark cost against
  model performance and time, controlling for quality, to estimate the rate at which
  price-performance improves.
---

# The Price of Progress: Algorithmic Efficiency and the Falling Cost of AI Inference

## Quick Facts
- arXiv ID: 2511.23455
- Source URL: https://arxiv.org/abs/2511.23455
- Authors: Hans Gundlach; Jayson Lynch; Matthias Mertens; Neil Thompson
- Reference count: 7
- Primary result: Price for fixed benchmark performance dropped 5–10× per year overall, with algorithmic efficiency gains contributing ~3× per year after adjusting for hardware price declines.

## Executive Summary
This paper measures how the cost of running frontier language models on common AI benchmarks has changed over time, controlling for performance quality. Using historical price data from Artificial Analysis and benchmark performance data from Epoch AI, the authors estimate that price-performance improves at a remarkably fast rate—5–10× per year—across tasks like knowledge, reasoning, math, and software engineering. They isolate algorithmic efficiency gains (about 3× per year) by adjusting for hardware price declines and competition effects from open-weight models. The study reveals a paradox: while price-performance improves, actual benchmark costs have stayed flat or increased due to larger models and longer reasoning traces, suggesting that current evaluations may not fully capture AI progress.

## Method Summary
The authors regress log(benchmark price) against time while controlling for logit-transformed performance to isolate the exponential price trend conditional on quality. They collect historical token prices from Artificial Analysis (via Internet Archive snapshots) and benchmark performance scores with token usage counts from Epoch AI. The analysis filters to Pareto-optimal models (cheapest for given performance) and runs separate regressions for all models, Pareto-restricted, and open-weight only. To estimate algorithmic efficiency, they divide the open-weight price decline rate by an independent hardware price-performance improvement factor (~1.43×/year from Rahman 2024), attributing the residual to algorithmic progress.

## Key Results
- Price for a given benchmark performance level decreased by 5–10× per year across knowledge, reasoning, math, and software engineering tasks.
- Open-weight model trends better isolate technical algorithmic progress from economic/competitive effects, yielding ~3× annual improvement after hardware adjustment.
- Despite algorithmic efficiency gains, actual benchmark costs remained flat or increased due to larger models and longer reasoning traces, indicating that evaluations may not fully reflect real-world AI progress.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Price for a fixed benchmark performance level decreases ~5–10× per year for frontier models.
- Mechanism: Regression of log(benchmark price) on time while controlling for logit-transformed performance isolates the exponential price trend conditional on quality. The logit transform accounts for bounded [0,1] benchmark scores and assumes logistic scaling with inference compute.
- Core assumption: Benchmark performance scales logistically with log inference compute/price; users select Pareto-optimal models (cheapest for given performance).
- Evidence anchors:
  - [abstract] "price for a given level of benchmark performance has decreased remarkably fast, around 5× to 10× per year"
  - [section 2.2] "The coefficient of interest in Eq. (1) is β2, which measures the rate of log price changes, conditional on benchmark performance"
  - [corpus] Cost-of-Pass framework (Erol et al.) reports 3.23–24.5× annual reductions on math benchmarks—same direction, variable magnitude.
- Break condition: If benchmark performance saturates near 1.0 (ceiling effects), the logit transform becomes unstable and the quality-controlled regression may misestimate trends.

### Mechanism 2
- Claim: Open-weight model price trends better isolate technical algorithmic progress from economic/competitive effects.
- Mechanism: Open-weight models face stronger competition (anyone can run them), so prices more closely reflect marginal GPU costs rather than monopoly markups. Comparing open vs. closed trends reveals non-technical price drivers.
- Core assumption: Open-weight models lag the frontier by only a few months and follow the same technical trajectory; market competition prevents sustained markups on open models.
- Evidence anchors:
  - [abstract] "Isolating out open models to control for competition effects and dividing by hardware price declines, we estimate that algorithmic efficiency progress is around 3× per year"
  - [section 2.2] "the trend in open-weight models more accurately measures technical progress rather than economic effects such as increased market pressure"
  - [corpus] Weak direct corroboration; no neighbor papers explicitly validate open-weight competition assumptions. Corpus is sparse on this specific mechanism.
- Break condition: If open-weight models diverge technically from frontier (e.g., different architectures, distillation access), their trends no longer proxy algorithmic progress.

### Mechanism 3
- Claim: After adjusting for hardware price-performance, algorithmic efficiency contributes ~3× annual improvement.
- Mechanism: Divide open-weight price decline rate by independent hardware price-performance gains (~1.43×/year, per Rahman 2024). The residual is attributed to algorithmic efficiency—fewer FLOPs per task at fixed quality.
- Core assumption: Hardware price-performance improves ~30%/year (1.43×); this hardware trend is independent and correctly measured; residual price decline is purely algorithmic.
- Evidence anchors:
  - [section 2.3] "After these adjustments, the remaining price reduction factor is around 3× per year, which can be interpreted as the contribution of algorithmic progress"
  - [appendix C.1] Hardware adjustment uses Rahman (2024) finding 30% annual cost reduction for fixed performance.
  - [corpus] Saad-Falcon et al. (2025) report 3.1× energy efficiency gains 2023–2025 controlling for hardware—consistent magnitude.
- Break condition: If hardware progress accelerates or stalls, or if hardware measurement conflates price vs. performance, the residual attribution to algorithms becomes incorrect.

## Foundational Learning

- **Concept: Logit transformation for bounded metrics**
  - Why needed here: Benchmark scores are bounded [0,1]; raw linear regression would violate bounds and misestimate returns to compute.
  - Quick check question: What happens to the logit transform as performance approaches 0 or 1?

- **Concept: Pareto frontier analysis**
  - Why needed here: Economically rational users select models on the cost-performance frontier; dominated models obscure true progress rates.
  - Quick check question: If all models improved but the frontier stayed fixed, what would the Pareto regression show?

- **Concept: Quality-adjusted price indices**
  - Why needed here: Raw price trends conflate cheaper models with worse models; need to hold quality constant to measure real efficiency.
  - Quick check question: Why does a falling average price per token not guarantee improved price-performance?

## Architecture Onboarding

- **Component map:**
  Internet Archive -> Artificial Analysis price snapshots + Epoch AI benchmark performance/token counts -> merged benchmark-cost dataset -> regression pipeline -> hardware-adjusted algorithmic efficiency estimates

- **Critical path:**
  1. Collect historical price snapshots (Internet Archive) with stable timestamp spacing
  2. Match model names across Artificial Analysis and Epoch AI (handle version variants)
  3. Compute benchmark cost = input_tokens × input_price + output_tokens × output_price (+ cache tokens for SWE-V)
  4. Run quality-controlled regression on Pareto frontier models
  5. Subset to open-weight models; divide trend by hardware factor

- **Design tradeoffs:**
  - Pareto frontier only vs. all models: Frontier better reflects user choice but excludes models optimized for non-cost objectives (latency, other tasks)
  - Benchmark-level vs. token-level: Benchmark-level captures reasoning-token inflation; token-level may show faster apparent decline that doesn't reflect actual task cost
  - Time window (2024–2025 vs. longer): Shorter window misses early-stage drops but has higher-quality price data

- **Failure signatures:**
  - Large confidence intervals (e.g., SWE-V: 4.675× [0.680, 32.156]) indicate insufficient data
  - Non-monotonic price increases from platform deprecation can introduce noise if not filtered
  - Cached token price unavailability for historical data may bias SWE-V estimates

- **First 3 experiments:**
  1. Replicate GPQA-Diamond regression on full model set vs. Pareto-restricted; compare β2 coefficients to validate frontier selection.
  2. Run open-weight regression on AIME; compute hardware-adjusted algorithmic efficiency; check if result falls within reported 3.2–3.3× range.
  3. Extend dataset with one additional benchmark (e.g., MMLU) to test whether 5–10× range holds across more tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the rate of price-performance improvement for agentic tool-using agents compared to static benchmarks?
- Basis in paper: [explicit] The authors explicitly ask, "Have agentic tool-using agents become more efficient?" and note their analysis does not account for agential benchmarks.
- Why unresolved: Current datasets lack standardized resource usage data for agentic tasks, which are becoming prohibitively expensive to evaluate independently.
- What evidence would resolve it: A dataset tracking token counts and inference costs for agents performing tool-use tasks over time.

### Open Question 2
- Question: Does price-performance for software engineering tasks (SWE-bench Verified) actually improve at a rate comparable to knowledge or reasoning tasks?
- Basis in paper: [explicit] The authors state that for SWE-bench Verified, "confidence bounds are large enough to be consistent with there having been no progress at all" due to limited data.
- Why unresolved: The sample size for SWE-bench Verified was small (n=13 on the Pareto frontier), resulting in statistical uncertainty that could not confirm a clear trend.
- What evidence would resolve it: Aggregating a larger dataset of historical benchmark runs and costs specifically for software engineering models.

### Open Question 3
- Question: To what extent are rising benchmarking costs (driven by larger models and reasoning traces) offsetting efficiency gains in the context of human labor substitution?
- Basis in paper: [explicit] The conclusion asks, "Are we closer to an AI software developer that can efficiently replace humans?" given that benchmarking costs have remained flat or increased despite efficiency gains.
- Why unresolved: The paper finds that while price-performance is improving, the absolute cost to run state-of-the-art evaluations is rising, clouding the economic picture of AI substitution.
- What evidence would resolve it: A comparative analysis of total resource costs for specific AI tasks versus human labor costs for the same outputs over time.

## Limitations

- **Small sample size for SWE-bench Verified**: Only 13 models on the Pareto frontier led to large confidence intervals (4.675× [0.680, 32.156]), making it statistically consistent with no progress.
- **Missing agentic benchmark data**: The analysis does not account for resource usage in agentic tool-using tasks, which are becoming increasingly important and expensive.
- **Hardware adjustment uncertainty**: The residual attribution to algorithmic efficiency depends on accurate measurement of hardware price-performance trends, which may conflate price vs. performance improvements.

## Confidence

| Claim | Confidence | Basis |
|-------|------------|--------|
| 5–10× annual price-performance improvement | High | Consistent across multiple benchmarks; robust to Pareto filtering |
| ~3× annual algorithmic efficiency after hardware adjustment | Medium | Relies on external hardware trend estimates; consistent with independent energy efficiency studies |
| SWE-bench Verified trend uncertain | Low | Large confidence intervals from small sample size (n=13) |

## Next Checks

1. Replicate the Pareto frontier filtering logic and compare regression coefficients between full model set and frontier-restricted to validate the selection approach.
2. Verify the hardware price-performance adjustment factor (~1.43×/year) by cross-checking with independent sources or alternative estimation methods.
3. Test the robustness of the logit transformation by running regressions with alternative bounded performance transformations (e.g., arcsine) and comparing results.