---
ver: rpa2
title: Evaluating Large language models on Understanding Korean indirect Speech acts
arxiv_id: '2502.10995'
source_url: https://arxiv.org/abs/2502.10995
tags:
- speech
- llms
- acts
- indirect
- utterance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the pragmatic competence of large language\
  \ models (LLMs) in understanding Korean indirect speech acts, focusing on three\
  \ types of indirect speech acts: directive, commissive, and expressive. A dataset\
  \ of 240 items was constructed, each containing a context and utterance, with two\
  \ scenarios per utterance\u2014one performing a direct speech act and the other\
  \ an indirect speech act."
---

# Evaluating Large language models on Understanding Korean indirect Speech acts

## Quick Facts
- arXiv ID: 2502.10995
- Source URL: https://arxiv.org/abs/2502.10995
- Reference count: 7
- One-line primary result: No LLM reaches human-level performance in understanding Korean indirect speech acts, with Claude3-Opus achieving the highest accuracy (71.94% MCQ, 65% OEQ).

## Executive Summary
This study evaluates the pragmatic competence of large language models (LLMs) in understanding Korean indirect speech acts. Using a dataset of 240 items structured around Searle's speech act taxonomy, the research tests whether models can distinguish between direct and indirect interpretations of the same utterance in different contexts. The results show that while Claude3-Opus outperforms other models, no LLM reaches human-level performance, and most models struggle significantly more with indirect than direct speech acts.

## Method Summary
The evaluation uses a 240-item Korean dataset categorized into directive, commissive, and expressive indirect speech acts. Each item contains context and utterance pairs, with two scenarios per utterance showing direct and indirect interpretations. Two experimental setups are employed: multiple-choice questions (MCQ) for automatic evaluation and open-ended questions (OEQ) for human expert assessment. Twelve LLMs are tested through API or local weights, with no fine-tuning involved.

## Key Results
- Claude3-Opus achieved the highest accuracy at 71.94% in MCQ and 65% in OEQ
- No LLM reached human-level performance (77.64% accuracy)
- All models except Claude3-Opus performed significantly worse on indirect speech acts compared to direct ones
- Most models made errors by confusing direct and indirect speech acts rather than selecting random options

## Why This Works (Mechanism)

### Mechanism 1: Context-Grounded Pragmatic Inference
- Claim: Evaluating indirect speech acts tests a model's ability to infer a speaker's intention that is not explicitly stated, relying on conversational context.
- Mechanism: The evaluation setup provides an utterance whose literal meaning serves one function (e.g., providing information) and a context that signals a different, indirect intention (e.g., making a request). Success requires the model to integrate the context to override a default, literal interpretation.
- Core assumption: The model's ability to process and weigh contextual cues is the primary driver of success.
- Evidence anchors:
  - [abstract] "To accurately understand the intention of an utterance is crucial... particularly in cases where the actual intention differs from the surface-leveled, literal intention of the sentence, i.e. indirect speech acts."
  - [section] "The context... where the same utterance conveys direct speech act and indirect speech act respectively, depending on the conversational context." (Abstract/Intro)
  - [corpus] Weak. Corpus mentions indirect speech in robotics but lacks direct evidence on the internal LLM mechanism.
- Break condition: If the model relies predominantly on surface-level linguistic patterns and fails to strongly condition its output on the provided context, its performance on indirect items will collapse.

### Mechanism 2: Assessment via Controlled Contrast
- Claim: The dataset's design, using paired items where the *same* utterance has different intents in different contexts, isolates the ability to perform context-dependent inference.
- Mechanism: By holding the utterance constant and varying only the context, the evaluation controls for the complexity of the sentence itself. This isolates the variable of interest: the model's use of context to disambiguate intent.
- Core assumption: The model's performance difference on paired items is primarily attributable to its context-processing capability, not other linguistic factors.
- Evidence anchors:
  - [section] "...we organized every 80 items for each type of indirect speech act into 40 pairs, where the same utterance performs direct and indirect speech act." (Methods: Dataset for Evaluation)
  - [section] "We seek to evaluate whether LLMs can distinguish that the utterance can perform either indirect speech act or direct speech act depending on the given context." (Methods: Experimental Setup...)
  - [corpus] Not explicitly detailed in corpus summaries.
- Break condition: If the constructed contexts are not sufficiently clear or if the utterances have a strong inherent bias towards one interpretation, the contrastive signal becomes noisy, weakening the evaluation's diagnostic power.

### Mechanism 3: Diagnosis via Error Type Distribution
- Claim: Analyzing the distribution of incorrect answers—specifically whether models confuse direct/indirect interpretations or choose unrelated options—diagnoses the nature of their failure.
- Mechanism: The multiple-choice format includes options for the literal interpretation, the indirect interpretation, and two unrelated choices. If a model errors by consistently selecting the literal intent, it indicates a failure in pragmatic inference, not a general failure of language understanding.
- Core assumption: The "opposite type" option (literal vs. indirect) is a more semantically plausible distractor than random options, and its selection reflects a specific error mode.
- Evidence anchors:
  - [section] "‘(In)direct’ is when an utterance that is actually a direct speech act is misunderstood as an indirect speech act or vice versa, and ‘Random’ is when one of the two random options is misunderstood." (Results: Error Analysis)
  - [section] "As can be seen in Figure 2, most models made errors by confusing direct and indirect speech acts rather than selecting random options." (Results: Error Analysis)
  - [corpus] Weak. Corpus does not provide details on this specific error analysis methodology.
- Break condition: If a model's errors are driven by a different systematic bias not captured by the two defined error types, the analysis may misattribute the cause of failure.

## Foundational Learning

- Concept: **Indirect Speech Acts (Pragmatics)**
  - Why needed here: This is the core capability being evaluated. You must understand that an utterance like "The elevator is out of order" can, depending on context, perform the act of *informing* or the act of *suggesting*.
  - Quick check question: For the utterance "Can you reach the salt?", is the speaker's primary intent to question your physical ability, or to make a request? What linguistic term describes this?

- Concept: **Searle's Taxonomy of Speech Acts**
  - Why needed here: The paper's dataset and analysis are structured around Searle's five categories (representatives, directives, commissives, expressives, declaratives). Understanding the differences between a *directive* (trying to get someone to do something) and a *commissive* (committing to a future action) is essential to interpret the results.
  - Quick check question: Into which of Searle's categories would you classify the utterance "I promise to call you later"?

- Concept: **High-Context Language (Korean)**
  - Why needed here: The authors chose Korean because it is considered a "high-context" language, where meaning is often implied. This knowledge explains why this evaluation is a particularly challenging and relevant test for an LLM's pragmatic competence.
  - Quick check question: In a high-context communication culture, is a listener expected to rely more on the explicit words spoken or on the shared background and situational context?

## Architecture Onboarding

- Component map:
  - **Evaluation Dataset (`Koo-et-al-Korean-Indirect-Speech`)**: 240 Korean items, structured into 120 pairs. Each item contains `Context`, `Utterance`, and an `Intended Speech Act` label (from Searle's taxonomy). Items are categorized into ReDi, ReCo, ReEx types.
  - **Evaluation Harness**: Two modes:
    1.  **MCQ Task**: Prompts model with (Context, Utterance) + 4 answer choices. Measures accuracy (Agreement Rate) over 3 trials.
    2.  **OEQ Task**: Prompts model with (Context, Utterance) to generate a free-text explanation. Human evaluators score this on a 1-5 scale for pragmatic correctness.
  - **Human Benchmark**: Data from 52 native Korean speakers (20-30 y/o) who took the MCQ test. Serves as the gold standard.

- Critical path: Load the dataset -> Run the target LLM on all 240 items in MCQ mode (3 trials) -> Calculate overall accuracy and compare to the human benchmark (77.64%) -> Disaggregate scores by Direct/Indirect and by Speech Act Type (ReDi/ReCo/ReEx) -> Analyze the distribution of error types (confusing direct/indirect vs. random) -> (Optional) Run OEQ task for qualitative analysis.

- Design tradeoffs:
  - **MCQ vs. OEQ**: The paper uses both. MCQ is scalable and objective but may miss nuances. OEQ provides deep, qualitative insights but is slow and requires human annotators, making it expensive.
  - **Paired Design**: Using the same utterance in different contexts controls for utterance complexity but may create artifacts if the contexts are artificially clear-cut.

- Failure signatures:
  - **Literal Bias**: Accuracy on indirect items is dramatically lower than on direct items. The model's errors predominantly consist of selecting the "opposite type" (i.e., the literal interpretation).
  - **Context Insensitivity**: Performance gap between direct and indirect conditions is very large, and the model shows high variance across trials.
  - **Categorical Weakness**: Systematically lower performance on `ReDi` (directive) items compared to `ReEx` (expressive) items, which often use more learnable idiomatic expressions.

- First 3 experiments:
  1.  **Establish Baseline & Gap**: Evaluate your model on the MCQ task. Report overall accuracy and the performance gap between it and the human benchmark (77.64%).
  2.  **Quantify Pragmatic Deficit**: Disaggregate accuracy into "Direct" vs. "Indirect" conditions. A large gap confirms a specific weakness in pragmatic inference over literal interpretation.
  3.  **Profile Error Modes**: For all incorrect answers on the indirect condition, calculate the percentage of times the model chose the literal interpretation vs. a random option. A high "literal error" rate is the key diagnostic signature of a failure in indirect speech act understanding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the ability to understand indirect speech acts in LLMs generalize robustly across typologically distinct languages like English, Chinese, and German?
- Basis in paper: [explicit] The authors state their future research aims to "develop data sets in other languages... to validate the robustness of the data sets."
- Why unresolved: The current study is limited to Korean, a high-context language, leaving cross-linguistic performance unknown.
- What evidence would resolve it: Evaluation results using translated datasets in the specified languages showing comparable performance metrics.

### Open Question 2
- Question: Can LLMs maintain high pragmatic competence when generating responses in dynamic, multi-turn dialogue settings?
- Basis in paper: [explicit] The authors propose future work exploring "alternative experimental settings... in natural, conversational settings" such as multi-turn prompting.
- Why unresolved: Current evaluation relies on static multiple-choice or open-ended questions based on single contexts, not interactive dialogue.
- What evidence would resolve it: Performance metrics from simulated multi-turn conversations where the model must respond appropriately to indirect cues.

### Open Question 3
- Question: Does optimizing open-source LLMs for standard benchmarks (e.g., Open Ko-LLM Leaderboard) degrade their capability for context-dependent pragmatic inference?
- Basis in paper: [inferred] The authors note that T3Q-ko-solar, a model fine-tuned for leaderboard performance, showed a nearly 5-point performance drop compared to the base Solar model.
- Why unresolved: It is unclear if this drop is a specific anomaly or a general trade-off between benchmark optimization and nuanced contextual understanding.
- What evidence would resolve it: A comparative analysis of base vs. fine-tuned models across multiple pragmatic evaluation datasets.

## Limitations

- The official dataset repository is inaccessible, preventing exact replication of evaluation conditions
- Critical implementation details like exact prompt templates and distractor selection methodology are not specified
- Human evaluation component introduces potential subjectivity in scoring that is not fully characterized
- Performance gap between models and humans (77.64%) suggests room for improvement but specific results may not be fully reproducible

## Confidence

- **High Confidence**: The general finding that LLMs struggle with indirect speech acts compared to direct ones is well-supported by the contrastive experimental design and error analysis
- **Medium Confidence**: The specific performance numbers and relative ranking of models are less certain without exact replication capabilities
- **Low Confidence**: The detailed error type distribution analysis and specific claims about category-specific weaknesses cannot be fully validated without access to raw data

## Next Checks

1. **Dataset Accessibility and Reconstruction**: Attempt to access the dataset via the provided link or contact the authors directly. If unavailable, reconstruct a minimal validation set of 10-20 items per category using Searle's taxonomy and the examples provided in the paper to verify the evaluation methodology works as described.

2. **Prompt Template and Evaluation Protocol**: Request or reconstruct the exact prompt templates used for both MCQ and OEQ tasks, including any system instructions or formatting. Verify the procedure for generating the four answer choices (1 correct, 1 opposite, 2 random) to ensure faithful reproduction of the evaluation conditions.

3. **Human Benchmark Replication**: Attempt to replicate the human benchmark score by having a small group of native Korean speakers complete the MCQ task on a subset of the data. Compare this independent score to the reported 77.64% to validate the relative model performance figures.