---
ver: rpa2
title: 'From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the
  Era of Post-training in NLP'
arxiv_id: '2510.12817'
source_url: https://arxiv.org/abs/2510.12817
tags:
- human
- alignment
- computational
- systems
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This position paper argues that human label variation (HLV)\u2014\
  legitimate disagreement among annotators\u2014should be preserved as a Selbstzweck\
  \ (intrinsic value) in preference dataset construction for LLM alignment. The authors\
  \ trace HLV's evolution from noise to signal, then advocate elevating it as essential\
  \ for pluralistic alignment."
---

# From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP

## Quick Facts
- **arXiv ID**: 2510.12817
- **Source URL**: https://arxiv.org/abs/2510.12817
- **Reference count**: 32
- **Key outcome**: Advocates preserving human label variation as intrinsic value (Selbstzweck) in preference dataset construction for pluralistic LLM alignment

## Executive Summary
This position paper argues that human label variation (HLV)—legitimate disagreement among annotators—should be preserved as a Selbstzweck (intrinsic value) in preference dataset construction for LLM alignment. The authors trace HLV's evolution from noise to signal, then advocate elevating it as essential for pluralistic alignment. They identify current datasets' limitations in flattening diverse perspectives through aggregation and propose actionable strategies: releasing annotator-level preferences, leveraging diverse feedback types, carefully selecting annotator pools to reflect target populations, and documenting pluralistic reasoning alongside final decisions. The paper emphasizes that preserving HLV is crucial for capturing pluralistic human values and enhancing sociotechnical safety evaluation.

## Method Summary
The paper presents a theoretical framework for reframing human label variation in preference dataset construction. It traces the historical evolution of HLV from being considered noise to becoming recognized as signal, and now advocates for elevating it to Selbstzweck (intrinsic value). The methodology involves analyzing current practices in preference dataset construction, identifying their limitations in handling diverse perspectives, and proposing alternative approaches that preserve annotator-level variation. The proposed strategies include releasing individual annotator preferences rather than aggregated labels, leveraging multiple types of feedback beyond simple preference judgments, carefully selecting annotator pools to reflect target population diversity, and documenting the reasoning behind pluralistic preferences.

## Key Results
- Current preference datasets flatten diverse perspectives through aggregation, losing valuable pluralistic information
- Preserving annotator-level preferences and diverse feedback types can enhance pluralistic alignment
- Careful annotator pool selection and documentation of pluralistic reasoning are essential for capturing human values

## Why This Works (Mechanism)
The paper's approach works by recognizing that human disagreement in labeling is not noise but rather reflects legitimate diversity in human values and perspectives. By preserving this variation rather than aggregating it away, alignment training can capture a broader, more representative spectrum of human preferences. This pluralistic approach to alignment training enables models to better handle the complexity and diversity of real-world human values, rather than learning from an oversimplified, averaged representation that may not reflect any actual human perspective.

## Foundational Learning
- **Human Label Variation (HLV)**: Legitimate disagreement among annotators reflecting diverse perspectives and values. Why needed: Forms the foundation for understanding that disagreement is valuable rather than problematic. Quick check: Can be measured through inter-annotator agreement metrics like Cohen's kappa or Krippendorff's alpha.
- **Selbstzweck**: German philosophical concept meaning "purpose in itself" or intrinsic value. Why needed: Provides philosophical grounding for elevating HLV from mere signal to something with inherent worth. Quick check: Can be validated through ethical frameworks that recognize diversity as intrinsically valuable.
- **Preference Dataset Construction**: Methods for creating datasets used in alignment training where annotators indicate preferences between model outputs. Why needed: Current methods typically aggregate labels, losing pluralistic information. Quick check: Can be evaluated by comparing aggregated vs. annotator-level preference distributions.
- **Plurastic Alignment**: Approach to alignment that seeks to capture and preserve diverse human values rather than finding a single "correct" preference. Why needed: Addresses the limitation of current alignment approaches that assume consensus exists. Quick check: Can be measured by diversity metrics in aligned model outputs across different cultural contexts.

## Architecture Onboarding

**Component Map**: Preference dataset construction -> Label aggregation -> Alignment training -> Model evaluation
                    ↓
            Annotator-level preferences
                    ↓
            Diverse feedback types
                    ↓
            Pluralistic reasoning documentation

**Critical Path**: Annotator selection and pool diversity → Individual preference collection → Preservation of variation → Training with diverse signals → Evaluation across pluralistic metrics

**Design Tradeoffs**: Aggregation provides cleaner signals but loses diversity; preserving variation maintains pluralism but increases complexity and potential contradictions; resource requirements increase significantly when maintaining annotator-level data

**Failure Signatures**: Models trained on aggregated preferences show reduced performance on edge cases representing minority perspectives; lack of annotator diversity leads to biased alignment toward majority views; failure to document reasoning makes it difficult to understand or correct alignment failures

**First Experiments**: 1) Compare alignment outcomes using aggregated vs. annotator-level preference datasets on the same model architecture; 2) Test model robustness across diverse cultural contexts when trained with pluralistic vs. aggregated preferences; 3) Evaluate resource and privacy implications of implementing annotator-level preference preservation at scale

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the practical implementation of pluralistic alignment strategies. These include how to handle conflicting pluralistic preferences when they cannot be reconciled, how to define appropriate target populations for annotator pool selection in global applications, and what metrics should be used to evaluate the success of pluralistic alignment approaches. The paper also questions how to balance the resource and privacy implications of maintaining annotator-level preferences against the benefits of preserving pluralistic information.

## Limitations
- The philosophical argument for elevating HLV to Selbstzweck lacks empirical validation demonstrating improved alignment outcomes
- Significant resource and privacy implications of maintaining annotator-level preferences are not fully addressed
- Potential amplification of existing biases through careful annotator pool selection raises concerns
- No framework provided for handling irreconcilable pluralistic preferences in practice

## Confidence

**High confidence**: The historical analysis of HLV's evolution from noise to signal is well-supported by existing literature. The critique of current preference dataset limitations and the importance of capturing diverse perspectives has strong theoretical grounding.

**Medium confidence**: The proposed strategies for preserving HLV have reasonable merit but lack empirical validation. The connection between pluralistic alignment and sociotechnical safety evaluation is conceptually sound but needs more concrete evidence.

**Low confidence**: The philosophical argument for elevating HLV to Selbstzweck as an intrinsic value is provocative but unproven. The practical implementation challenges of the proposed methods are not adequately addressed.

## Next Checks
1. Conduct empirical studies comparing alignment outcomes between aggregated preference datasets versus those preserving annotator-level variation across diverse task types and model architectures.

2. Design and implement a pilot study testing the feasibility of documenting pluralistic reasoning alongside preferences, measuring both resource requirements and downstream utility for model training and evaluation.

3. Develop a framework for evaluating when and how to handle irreconcilable pluralistic preferences, including metrics for determining when aggregation might be appropriate versus when preserving variation is essential.