---
ver: rpa2
title: Improving Document Retrieval Coherence for Semantically Equivalent Queries
arxiv_id: '2508.07975'
source_url: https://arxiv.org/abs/2508.07975
tags:
- query
- queries
- retrieval
- coherence
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a coherence ranking loss for dense retrieval
  models that improves both retrieval accuracy and ranking consistency across semantically
  equivalent queries. The proposed loss extends the Multiple Negative Ranking objective
  by penalizing embedding misalignment between lexical variations of the same query
  and enforcing consistent similarity margins between queries and documents.
---

# Improving Document Retrieval Coherence for Semantically Equivalent Queries

## Quick Facts
- arXiv ID: 2508.07975
- Source URL: https://arxiv.org/abs/2508.07975
- Reference count: 35
- Primary result: Coherence ranking loss improves both retrieval accuracy (+0.6% to +1.8% NDCG) and ranking consistency (+15% to +29% RBO) for semantically equivalent queries

## Executive Summary
This paper addresses the problem of inconsistent document retrieval for semantically equivalent queries in dense retrieval systems. The authors propose a Coherence Ranking (CR) loss that extends the Multiple Negative Ranking objective by adding two penalties: Query Embedding Alignment (QEA) to align embeddings of query variations, and Similarity Margin Consistency (SMC) to ensure consistent scoring margins. Experiments on MS-MARCO, Natural Questions, BEIR, and TREC DL show that models trained with CR loss achieve significant improvements in both retrieval accuracy and ranking coherence while generalizing across different PLMs including MPNet, MiniLM, and ModernBERT.

## Method Summary
The approach trains dense retrieval models using a loss function that combines standard MNR with two coherence-promoting penalties. For each batch containing queries and their semantically equivalent variations, the loss penalizes (1) embedding misalignment between variations via MSE, and (2) inconsistent similarity margins between positive/negative documents via MSE on margin vectors. The method requires generating query variations (10 per query using Phi-3) and mining hard negatives. Training uses a dual encoder architecture with hyperparameters tuned for optimal balance between accuracy and coherence.

## Key Results
- NDCG@10 improvements of +0.6% to +1.8% on MS-MARCO and Natural Questions
- RBO@5 improvements of +15% absolute on MS-MARCO and +29% on Natural Questions
- Models trained with CR loss show better stability on complex queries where top-1 and top-50 scores are similar
- Coherence improvements translate to +4.1% re-ranking opportunity (top document available from equivalent queries)
- Performance generalizes across different PLMs including MPNet, MiniLM, and ModernBERT

## Why This Works (Mechanism)

### Mechanism 1: Query Embedding Alignment (QEA)
- **Claim**: Aligning embeddings of lexically different but semantically equivalent queries reduces model sensitivity to query wording.
- **Mechanism**: The QEA component penalizes embedding distance between query variations using MSE loss, forcing the encoder to map equivalent queries to proximate points in embedding space regardless of lexical form.
- **Core assumption**: Semantic equivalence implies embeddings should converge; embedding proximity correlates with retrieval consistency.
- **Evidence anchors**: [abstract] "penalizing embedding misalignment between lexical variations of the same query"; [section 3.2] "QEA component simply tries to aligning the embeddings of lexically different queries by penalizing their differences measured through Mean Squared Error (MSE)"

### Mechanism 2: Similarity Margin Consistency (SMC)
- **Claim**: Enforcing consistent similarity margins between query variations and documents stabilizes ranking behavior across paraphrases.
- **Mechanism**: SMC explicitly requires that m(q, d+, d) = m(qi, d+, d) for equivalent queries qi, ensuring the relative scoring gap between positive and negative documents remains invariant to query reformulation.
- **Core assumption**: Ranking stability requires not just embedding proximity but consistent scoring behavior at the margin level.
- **Evidence anchors**: [abstract] "enforcing consistent similarity margins between queries and documents"; [section 3.2] "SMC targets alignment in similarity scores... enforces equivalent queries to have the same similarities when compared to the same positive and negative documents"

### Mechanism 3: Joint Optimization Creates Synergy
- **Claim**: Combining QEA and SMC with MNR produces greater coherence and accuracy improvements than any component alone.
- **Mechanism**: MNR handles query-document relevance; QEA handles query-query alignment; SMC handles score-level consistency. Together they create orthogonal constraints that reinforce coherent ranking behavior.
- **Core assumption**: The three objectives are not in conflict and gradients align toward a coherent solution.
- **Evidence anchors**: [Table 3] LCR achieves 53.92 NDCG and 0.70 RBO on NQ, outperforming LQEA (51.63 NDCG, 0.66 RBO) and L_SMC (53.22 NDCG, 0.57 RBO) individually.

## Foundational Learning

- **Concept: Dense Retrieval (Dual Encoder) Architecture**
  - **Why needed here**: CR loss modifies dense retriever training; understanding how query/document encoders produce embeddings and how cosine similarity scores ranking documents is prerequisite.
  - **Quick check question**: Can you explain why a dual encoder produces query and document embeddings separately, and how top-k retrieval works via approximate nearest neighbor search?

- **Concept: Multiple Negative Ranking (MNR) Loss**
  - **Why needed here**: CR loss extends MNR; understanding the standard contrastive objective (positive pair vs. in-batch negatives) is necessary to see what QEA/SMC add.
  - **Quick check question**: Given a batch of (query, positive_doc) pairs, how does MNR treat other documents in the batch as negatives, and what does the loss optimize?

- **Concept: Rank-Biased Overlap (RBO)**
  - **Why needed here**: RBO is the primary coherence metric; understanding it measures similarity between two ranked lists with top-weighting explains why it captures retrieval consistency.
  - **Quick check question**: Why does RBO give more weight to agreement at the top of ranked lists, and what does RBO=0.70 vs. RBO=0.40 mean practically?

## Architecture Onboarding

- **Component map**: Query variations and hard negatives -> Dual encoder (MPNet/MiniLM/ModernBERT) -> Embeddings -> Coherence Ranking Loss (MNR + λ1·QEA + λ2·SMC) -> Updated model parameters

- **Critical path**:
  1. Generate/query equivalent query clusters (10 variations per query using Phi-3 pipeline in Appendix A)
  2. Mine hard negatives (5-10 per query)
  3. Batch sampling must include multiple queries from same cluster to enable QEA/SMC terms
  4. Tune λ1, λ2 ∈ {0.2, 0.5, 0.8, 1.0} on validation set

- **Design tradeoffs**:
  - **λ1 vs. λ2**: Paper found optimal values empirically; higher λ1 emphasizes embedding proximity, higher λ2 emphasizes margin consistency. Ablation (Table 3) suggests SMC contributes more to accuracy on NQ.
  - **Data augmentation vs. loss-based coherence**: Table 2 shows Query Augmentation alone improves RBO but can hurt NDCG on MS-MARCO (-1.46); LCR improves both. Prefer loss-based approach if accuracy is critical.
  - **Generated query quality**: Appendix A reports 100% accuracy on manual validation of 100 samples, but assumption: generated queries truly preserve intent.

- **Failure signatures**:
  - **RBO improves but NDCG drops**: Likely λ1 too high, forcing embeddings too close and losing discriminative power.
  - **No coherence gain on large datasets**: Check batch construction—clusters must have multiple queries per batch for QEA/SMC to activate.
  - **ModernBERT without STS pretraining shows poor coherence (Table 8, RBO=0.08)**: STS pretraining is prerequisite for this architecture; baseline encoder needs semantic similarity knowledge.

- **First 3 experiments**:
  1. **Reproduce baseline comparison**: Train MPNet with standard MNR on MS-MARCO dev split, measure NDCG@10 and RBO@5 against Table 2 values (FT: 41.51 NDCG, 0.46 RBO) to validate pipeline.
  2. **Ablate loss components**: Train three models—MNR+QEA only, MNR+SMC only, MNR+both—and verify that full LCR achieves highest coherence (target RBO 0.60+ on MS-MARCO).
  3. **Stress test on complex queries**: Filter test set for queries where top-1 and top-50 score gap <0.1 (Section 4.5), verify LCR shows larger coherence improvement on this subset (target RBO improvement from ~0.17 to ~0.34).

## Open Questions the Paper Calls Out

- **Question**: Does the proposed coherence loss maintain its effectiveness when scaling to real-world web indexes containing billions of documents?
  - **Basis in paper**: [explicit] The authors conclude by asking, "how does coherence impact real-world applications, based on web indexes with Billions of documents?" following their analysis of retrieval complexity.
  - **Why unresolved**: Experiments were limited to datasets like MS-MARCO (8.8M passages) and NQ, which are smaller than industrial web-scale indexes where score distributions and noise levels differ significantly.
  - **What evidence would resolve it**: Evaluation of the Coherence Ranking (CR) loss on a billion-scale corpus, analyzing if the correlation between low score margins and low coherence persists in high-noise environments.

- **Question**: Why do significant improvements in ranking coherence not translate proportionally into larger gains in standard relevance metrics?
  - **Basis in paper**: [explicit] The Limitations section notes that while ranking overlap improved significantly (e.g., +15% RBO), "the same improvement is not directly translated into relevance," leaving the disconnect unexplained.
  - **Why unresolved**: The paper validates the existence of this gap but does not investigate whether coherence improvements merely stabilize already-correct rankings or if they systematically fail to recover difficult relevant documents.
  - **What evidence would resolve it**: An error analysis categorizing whether the stabilized rankings from the CR loss result in the retrieval of new relevant documents or just consistent retrieval of the same easy documents.

- **Question**: How does optimizing dense retriever coherence interact with the inherent incoherence of downstream re-rankers or Large Language Models?
  - **Basis in paper**: [explicit] The authors state that "How DR coherence affects the entire pipeline is not deeply explored" and explicitly note that "coherence of the re-ranker itself is outside the scope of this work."
  - **Why unresolved**: While "re-ranking opportunity" (the availability of the top document) was measured, the actual behavior of the re-ranker or generator when fed coherent inputs remains unstudied.
  - **What evidence would resolve it**: End-to-end experiments measuring the consistency of final outputs in RAG or retrieve-and-rerank pipelines where the retriever is coherent but the subsequent model is not.

## Limitations

- **Generated query quality dependency**: The entire approach assumes that the 10 generated query variations preserve semantic intent. While Appendix A reports 100% accuracy on 100 manual samples, scaling to the full MS-MARCO dataset (~495K queries) means ~4.95M generated variations. Systematic quality drift or subtle intent shifts could undermine QEA effectiveness.
- **Pre-training assumption**: The coherence gains for ModernBERT (Table 8) vanish without STS pre-training (RBO drops from 0.32 to 0.08). This suggests the CR loss is not a standalone solution—it requires a semantically-aware encoder as foundation. Users without access to STS-pretrained models may see minimal coherence gains.
- **Ambiguous intent ambiguity**: The paper assumes semantic equivalence implies identical optimal documents. In reality, equivalent queries can have different user intents (e.g., "best running shoes" vs. "affordable running shoes"). Forcing coherence in such cases could reduce precision.

## Confidence

- **High confidence**: NDCG improvements (+0.6% to +1.8% on standard benchmarks) and RBO gains (+15% to +29% absolute) are reproducible and directly measurable. Ablation results (Table 3) are internally consistent.
- **Medium confidence**: The mechanism explanations (QEA and SMC) are logically sound and supported by ablations, but the interaction effects between components are not fully disentangled. The claim that "joint optimization creates synergy" is plausible but not rigorously proven.
- **Low confidence**: The practical impact on real-world applications (e.g., reduce-and-rerank pipelines) is demonstrated anecdotally (Figure 3) but not quantitatively validated across multiple application scenarios.

## Next Checks

1. **Intent ambiguity stress test**: Manually curate a set of query pairs where equivalent phrasing maps to different document sets (e.g., "cheap laptops" vs. "budget laptops" in a commercial corpus). Measure whether CR loss forces harmful coherence or preserves intent distinction.
2. **Robustness to query generation noise**: Systematically degrade the quality of generated query variations (e.g., truncate, paraphrase incorrectly) and measure the degradation in coherence gains. This quantifies the approach's sensitivity to preprocessing quality.
3. **Zero-shot transfer coherence**: Evaluate whether models trained with CR loss on MS-MARCO maintain coherence gains on out-of-domain datasets (e.g., BEIR) where query distributions differ significantly. This tests whether coherence is learned or dataset-specific.