---
ver: rpa2
title: 'When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection
  with PsiloQA'
arxiv_id: '2510.04849'
source_url: https://arxiv.org/abs/2510.04849
tags:
- psiloqa
- answer
- hallucination
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PsiloQA, a large-scale, multilingual dataset
  for span-level hallucination detection across 14 languages. The dataset is constructed
  using an automated pipeline that generates question-answer pairs from Wikipedia,
  elicits potentially hallucinated answers from diverse LLMs without context, and
  uses GPT-4o to automatically annotate hallucinated spans by comparing against golden
  answers and retrieved context.
---

# When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA

## Quick Facts
- arXiv ID: 2510.04849
- Source URL: https://arxiv.org/abs/2510.04849
- Reference count: 32
- This paper introduces PsiloQA, a large-scale, multilingual dataset for span-level hallucination detection across 14 languages.

## Executive Summary
This paper presents PsiloQA, a large-scale, multilingual dataset for span-level hallucination detection across 14 languages. The dataset is constructed using an automated pipeline that generates question-answer pairs from Wikipedia, elicits potentially hallucinated answers from diverse LLMs without context, and uses GPT-4o to automatically annotate hallucinated spans by comparing against golden answers and retrieved context. PsiloQA contains 63,792 training samples, 3,355 validation samples, and 2,897 test samples. The paper evaluates uncertainty quantification methods, LLM-based tagging, and fine-tuned encoder models, finding that encoder-based models (particularly mmBERT and ModernBERT) achieve the strongest performance across languages. The dataset demonstrates effective cross-lingual generalization and supports robust knowledge transfer to other benchmarks while being significantly more cost-efficient than human-annotated datasets.

## Method Summary
PsiloQA is generated through a four-stage pipeline: (1) GPT-4o generates question-answer pairs from Wikipedia passages across 14 languages, (2) diverse LLMs generate answers without access to reference context to elicit hallucinations, (3) GPT-4o annotates hallucinated spans by comparing LLM answers against golden answers and retrieved context using [HAL] tags, and (4) rule-based and prompt-based filtering removes low-quality samples. The dataset contains 63,792 training, 3,355 validation, and 2,897 test samples. Encoder models (mmBERT-base and ModernBERT-base) are fine-tuned using learning_rate=1e-5, epochs=6, weight_decay=0.01, and batch_size=8, with evaluation using character-level IoU and Average Precision.

## Key Results
- Multilingual encoder fine-tuning (mmBERT) outperforms language-specific models across 12 of 14 languages, with substantial gains for Arabic (12.4 IoU), Catalan (7.13 IoU), and Farsi (10.5 IoU)
- PsiloQA-trained detectors achieve 45% IoU improvement over RAGTruthQA when transferred to Mu-SHROOM benchmark
- Automated GPT-4o annotation achieves 71.0% IoU against human labels at ~17x cost efficiency ($535 vs $3,000)

## Why This Works (Mechanism)

### Mechanism 1: Zero-Context Hallucination Elicitation
Removing access to reference context during answer generation reliably produces natural hallucinations for training detection models. LLMs rely on parametric knowledge when denied external context; factual gaps and conflations manifest as inconsistent spans. The paper generates hypotheses using diverse LLMs in a "no-context setting" to produce both hallucinated and accurate answers, which are then annotated by comparing against golden answers and retrieved context.

### Mechanism 2: Automated Span-Level Annotation via LLM Comparison
A capable LLM (GPT-4o) can annotate hallucinated spans with quality comparable to human annotators when given golden answers and retrieved context. GPT-4o compares the LLM hypothesis against both the golden answer and the source passage, marking discrepancies with `[HAL]` tags. The paper followed the RAGTruth pipeline, asking GPT-4o to annotate "at the word level, encouraging precise labeling and discouraging overgeneralization."

### Mechanism 3: Multilingual Encoder Fine-Tuning with Cross-Lingual Transfer
Training a single multilingual encoder on all languages simultaneously outperforms language-specific models, even for low-resource and distinct-script languages. mmBERT-base is fine-tuned on the complete PsiloQA dataset. The model learns shared representations across languages, enabling positive transfer. Table 3 shows the multilingual model consistently outperforms per-language models across 12 of 14 languages.

## Foundational Learning

- **Concept: Span-Level vs. Sequence-Level Detection**
  - Why needed here: PsiloQA targets fine-grained span-level detection, which requires understanding that a single response can contain both correct and hallucinated content. The paper explicitly contrasts this with sequence-level benchmarks like TruthfulQA.
  - Quick check question: Can you explain why IoU and Average Precision (AP) are both needed to evaluate span-level detection, and what each metric captures that the other misses?

- **Concept: Uncertainty Quantification (UQ) Methods**
  - Why needed here: The paper evaluates UQ baselines (MaxProb, CCP, Focus) that operate at token/span level. Understanding these is essential to interpret why encoder models outperform them.
  - Quick check question: Why does the paper note that sampling-based UQ methods "require substantial computational overhead and operate only on the sequence level," making them unsuitable for span-level tasks?

- **Concept: Cross-Lingual Transfer in Encoder Models**
  - Why needed here: The core finding is that multilingual training beats per-language training. Understanding how encoders like mmBERT share representations across languages is critical.
  - Quick check question: What might explain why Arabic and Hindi—languages with distinct scripts—still benefit from multilingual training with European languages?

## Architecture Onboarding

- **Component map**: Wikipedia passages → GPT-4o QA generation → Zero-context LLM inference → GPT-4o span annotation → Rule-based + prompt-based filtering → Encoder fine-tuning (mmBERT/ModernBERT) → Evaluation (IoU/AP)

- **Critical path**: Annotation quality is the bottleneck. Section 3.4 shows 71.0% IoU between GPT-4o and human labels—acceptable but with a ~9.8% margin of error at 95% confidence. If annotation quality degrades for specific languages or domains, all downstream results inherit this noise.

- **Design tradeoffs**:
  1. **Synthetic vs. Human Annotation**: ~17x cheaper but introduces GPT-4o-specific biases. The paper acknowledges "annotation source bias" as a limitation.
  2. **QA-Only Task Scope**: PsiloQA focuses on question-answering; the paper notes that summarization, dialogue, and data-to-text generation "also suffer from hallucinations and warrant similar treatment."
  3. **Wikipedia Dependency**: Limits topical and cultural diversity; real-world applications "often involve noisier or domain-specific data."

- **Failure signatures**:
  1. **Over-generalized spans**: If models tag entire answers rather than precise words, check annotation prompt adherence (Figure 8 explicitly discourages this).
  2. **Language-specific degradation**: If performance drops for Farsi, German, or lower-resource languages, check sample distribution (Figure 4 shows imbalance).
  3. **IoU-AP gap widening**: Large gaps indicate boundary detection issues; the paper notes this remains a challenge across all methods.

- **First 3 experiments**:
  1. **Reproduce baseline comparison**: Fine-tune mmBERT-base on PsiloQA train split, evaluate on test split using both IoU and AP across all 14 languages. Confirm results align with Table 2.
  2. **Cross-dataset transfer**: Train on PsiloQA-en only, evaluate on Mu-SHROOM-en. Reproduce the 45% IoU improvement over RAGTruthQA training (Table 4).
  3. **Annotation quality audit**: Sample 50 examples each from 2 high-performing and 2 low-performing languages. Manually verify span annotations to identify systematic GPT-4o biases.

## Open Questions the Paper Calls Out

### Open Question 1
Can the PsiloQA automated pipeline be effectively adapted to non-QA generative tasks such as summarization and data-to-text generation? The authors state in the Conclusion that "Future work will explore extending the PsiloQA pipeline to other generation tasks, such as summarization and data-to-text generation." The current pipeline relies on generating question-answer pairs from Wikipedia passages and "no-context" answers to induce hallucinations. Summarization involves condensing provided text rather than recalling internal knowledge, potentially requiring a different mechanism for inducing and detecting inconsistencies.

### Open Question 2
Does using an ensemble of diverse LLMs for span-level annotation significantly reduce the single-model bias observed with GPT-4o? In the Limitations section, the authors note the "Annotation Source Bias" inherent in using GPT-4o alone and explicitly state: "This bias could be substantially mitigated by using an ensemble of annotators... We consider this a promising direction for future work." While GPT-4o is cost-efficient, it may enforce specific stylistic preferences or consistent blind spots as an annotator. It remains untested whether averaging annotations from multiple models (e.g., Claude, Gemini, Llama) yields higher alignment with human judgment or merely averages out the errors.

### Open Question 3
Does training hallucination detectors on Wikipedia-based synthetic data generalize to noisy, domain-specific applications like medicine or law? The Limitations section notes the "Dependency on Wikipedia," stating that "real-world applications often involve noisier or domain-specific data" and that the dataset may inherit cultural/regional biases. The paper demonstrates transfer to benchmarks like FAVA and HalluEntity, but these often share the general domain or clean structure of the training data. It is unclear if the "clean" hallucination patterns learned from Wikipedia transfer to high-stakes domains where the cost of error is highest and terminology is specialized.

## Limitations

- Dataset construction relies heavily on GPT-4o's judgment for hallucination annotation, with manual verification showing 71.0% IoU against human annotations
- Focus on Wikipedia-derived QA pairs may limit generalizability to other generation tasks like summarization or dialogue
- Heavy class imbalance (Hallucination Ratio of 0.76) may affect model calibration and real-world performance on more balanced data distributions

## Confidence

**High Confidence**: The core finding that multilingual encoder fine-tuning outperforms per-language models is well-supported by Table 3, showing consistent improvements across 12 of 14 languages with substantial gains for Arabic (12.4 IoU), Catalan (7.13 IoU), and Farsi (10.5 IoU). The cost-efficiency advantage over human-annotated datasets (~17x) is also robustly demonstrated through the $535 vs $3,000 comparison.

**Medium Confidence**: The claim about effective cross-lingual generalization assumes hallucination patterns are sufficiently similar across languages. While the results support this, the underlying mechanism isn't fully explored, and certain languages (Farsi, German) still show lower performance despite multilingual training.

**Low Confidence**: The assumption that zero-context hallucinations are representative of real-world hallucination patterns remains largely untested. The paper acknowledges this limitation but doesn't validate whether models trained on PsiloQA transfer effectively to RAG or grounded generation scenarios where context is available.

## Next Checks

1. **Real-world Transfer Validation**: Evaluate PsiloQA-trained models on LLM outputs generated in RAG settings with retrieved context, comparing performance against zero-context scenarios to quantify the impact of context availability on hallucination patterns.

2. **Annotation Quality Audit**: Conduct manual verification of GPT-4o annotations across all 14 languages, focusing on languages with lowest IoU scores (Farsi, German) to identify systematic annotation biases or linguistic factors affecting span detection quality.

3. **Task Generalization Study**: Extend evaluation beyond QA pairs to other generation tasks (summarization, dialogue) using the same detection models, measuring performance degradation and identifying task-specific hallucination patterns not captured in the Wikipedia-based dataset.