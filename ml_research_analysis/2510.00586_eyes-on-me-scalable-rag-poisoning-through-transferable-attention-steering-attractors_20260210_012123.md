---
ver: rpa2
title: 'Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering
  Attractors'
arxiv_id: '2510.00586'
source_url: https://arxiv.org/abs/2510.00586
tags:
- attention
- malicious
- attack
- attractor
- retriever
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Eyes-on-Me, a modular poisoning attack against
  retrieval-augmented generation (RAG) systems. The core idea is to decompose an adversarial
  document into reusable Attention Attractors and swappable Focus Regions, enabling
  scalable and cost-effective attacks without repeated optimization.
---

# Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors

## Quick Facts
- **arXiv ID**: 2510.00586
- **Source URL**: https://arxiv.org/abs/2510.00586
- **Reference count**: 40
- **Primary result**: Modular RAG poisoning method with 57.8% average attack success rate, 2.6× improvement over prior methods

## Executive Summary
This paper introduces Eyes-on-Me, a modular approach to poisoning retrieval-augmented generation (RAG) systems. The core innovation is decomposing adversarial documents into reusable Attention Attractors and swappable Focus Regions, enabling scalable and cost-effective attacks across different RAG configurations without repeated optimization. By steering attention to concentrate on designated regions, the method amplifies the influence of any payload placed there. Experiments across 18 RAG configurations demonstrate an average attack success rate of 57.8%, representing a 2.6× improvement over prior methods.

## Method Summary
Eyes-on-Me addresses the scalability challenge in RAG poisoning by introducing a modular architecture. The method separates adversarial documents into Attention Attractors, which are optimized to attract the model's attention, and Focus Regions, which are swappable payload containers. This separation allows attackers to reuse optimized attractors across different scenarios while only modifying the focus regions for specific targets. The optimization process involves fine-tuning document embeddings to create attractors that draw attention to designated regions where malicious content is placed. This modular design enables the attack to transfer across different retrievers, generators, and triggers without requiring retraining for each configuration.

## Key Results
- Achieves 57.8% average attack success rate across 18 RAG configurations
- Demonstrates 2.6× improvement over prior poisoning methods
- Successfully transfers across different retrievers, generators, and triggers without retraining
- Shows modular design enables efficient adaptation across RAG configurations

## Why This Works (Mechanism)
The effectiveness of Eyes-on-Me stems from its ability to manipulate the attention mechanisms of RAG systems. By creating specialized Attention Attractors, the method ensures that the model's focus is deliberately steered toward specific regions of the document where malicious content is embedded. This targeted attention manipulation amplifies the influence of the payload, making the attack more effective. The modular separation between attractors and focus regions allows for efficient optimization and transfer, as the computationally expensive attention-attracting components can be reused while only the payload regions need modification for different attack scenarios.

## Foundational Learning

**Attention mechanisms in RAG systems** - Understanding how retrievers and generators distribute attention across documents is essential for creating effective attractors. Quick check: Verify that attention scores correlate with the success of malicious payload delivery.

**Transfer learning in adversarial attacks** - The ability to transfer optimized components across different model configurations without retraining is crucial for scalability. Quick check: Test if attractors optimized for one retriever transfer to another with similar architecture.

**Document embedding optimization** - Fine-tuning document representations to create attention attractors requires understanding of how embeddings influence retrieval and generation. Quick check: Confirm that embedding changes correspond to increased attention scores in target regions.

## Architecture Onboarding

**Component map**: Attention Attractors -> Focus Regions -> RAG Pipeline (Retriever -> Generator -> Output)

**Critical path**: Optimized attention attractors steer model attention to focus regions containing malicious payload, which is then retrieved and incorporated into the final generated output.

**Design tradeoffs**: Modularity vs. specificity - while the modular design enables transfer and scalability, it may sacrifice some attack precision compared to fully optimized, non-modular approaches.

**Failure signatures**: When attention attractors fail to attract sufficient attention, the payload in focus regions is not incorporated into the final output, resulting in attack failure.

**First experiments**:
1. Test basic attention steering with simple document modifications
2. Evaluate transfer capability across different retriever architectures
3. Measure attack success rate with varying focus region positions and content

## Open Questions the Paper Calls Out

None

## Limitations
- The 57.8% average success rate, while improved, still leaves significant room for enhancement
- The method's generalizability to real-world RAG systems beyond the 18 tested configurations remains uncertain
- No explicit evaluation of robustness against adaptive defenses like adversarial training or anomaly detection
- Limited testing on document variations in structure, length, and semantic content

## Confidence

| Claim | Confidence |
|-------|------------|
| Modular design enables efficient transfer across RAG configurations | High |
| 2.6× improvement over prior methods | High |
| Effectiveness across 18 tested configurations | High |
| Generalizability to real-world systems | Medium |
| Robustness against adaptive defenses | Medium |

## Next Checks
1. Test transferability across a broader set of retrievers and LLMs not included in the original study
2. Evaluate performance under adaptive defenses like adversarial training or anomaly detection
3. Assess robustness to variations in document structure, length, and semantic content beyond the current test corpus