---
ver: rpa2
title: The Role of Vocabularies in Learning Sparse Representations for Ranking
arxiv_id: '2509.16621'
source_url: https://arxiv.org/abs/2509.16621
tags:
- retrieval
- vocab
- splade
- flops
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how vocabulary size and initialization affect
  sparse retrieval models like SPLADE. It trains BERT-based models with either 32K
  or 100K output vocabularies, comparing randomly initialized vocabularies to ones
  pretrained via ESPLADE.
---

# The Role of Vocabularies in Learning Sparse Representations for Ranking

## Quick Facts
- **arXiv ID:** 2509.16621
- **Source URL:** https://arxiv.org/abs/2509.16621
- **Reference count:** 11
- **Primary result:** Larger vocabularies (100K) with pruning maintain effectiveness while reducing computational cost compared to smaller vocabularies (32K).

## Executive Summary
This paper investigates how vocabulary size and initialization affect sparse retrieval models like SPLADE. The authors train BERT-based models with either 32K or 100K output vocabularies, comparing randomly initialized vocabularies to ones pretrained via ESPLADE. After finetuning on real search click logs, static pruning based on MLM logit scores is applied to balance efficiency and effectiveness. Results show that 100K vocab models with pruning maintain competitive effectiveness while reducing computational cost compared to 32K models. ESPLADE-pretrained vocabularies outperform random ones at similar efficiency, highlighting the role of pretrained weights. Larger vocabularies improve retrieval by increasing representational capacity and reducing posting list lengths. This demonstrates that vocabulary size and initialization are crucial for efficient, effective sparse retrieval, and that pretrained vocabularies better support learning sparse representations optimized for ranking tasks.

## Method Summary
The paper compares SPLADE models with different vocabulary sizes (32K vs 100K) and initialization strategies (random vs ESPLADE-pretrained). Vocabulary expansion is performed by extracting top unigrams from the corpus and initializing new MLM head weights via mean-pooling of constituent WordPiece embeddings. Models are pretrained with EMLM on web titles, then finetuned with in-batch negative loss and joint FLOPS regularization on click logs. Static pruning is applied during inference by keeping only top-k MLM logits for queries and documents. The study evaluates MRR@10, Recall@10, and FLOPS efficiency across different vocabulary configurations.

## Key Results
- 100K vocabulary models with static pruning maintain competitive effectiveness while reducing computational cost compared to 32K models
- ESPLADE-pretrained vocabularies consistently outperform randomly initialized ones at similar efficiency levels
- Larger vocabularies improve retrieval by increasing representational capacity and reducing posting list lengths
- The combination of ranking and FLOPS losses transforms vocabulary terms from lexical units into latent ranking features

## Why This Works (Mechanism)

### Mechanism 1: Vocabulary Size Increases Representational Capacity Under Pruning
Larger vocabularies (100K vs 32K) improve retrieval effectiveness when computational budgets are constrained by static pruning, even with random initialization. Larger vocabularies distribute representations across more term indices, reducing individual posting list lengths. When pruning removes low-scoring terms, models with larger vocabularies retain more discriminative capacity because they can represent documents using a more diverse term set rather than relying on score magnitude alone.

### Mechanism 2: ESPLADE Pretraining Provides Semantically-Grounded Initialization
Pretrained vocabulary weights from ESPLADE's expanded MLM phase yield better effectiveness than random initialization at comparable retrieval costs. ESPLADE pretraining initializes vocabulary weights by mean-pooling WordPiece subword embeddings, transferring semantic knowledge from the pretrained BERT backbone. This provides a better starting point for finetuning with ranking loss, allowing the model to learn sparse representations that more accurately reflect semantic similarity.

### Mechanism 3: Ranking and FLOPS Losses Transform Vocabulary Semantics
The combination of in-batch ranking loss and FLOPS regularization converts vocabulary terms from lexical units into latent ranking features. Ranking loss pulls Q-D pairs together regardless of lexical overlap; FLOPS loss enforces uniform sparsity. Together, they reshape the embedding space so that vocabulary dimensions serve as learned features optimized for Q-D matching rather than semantic expansion.

## Foundational Learning

- **SPLADE Architecture (Sparse Lexical and Expansion Model)**
  - Why needed here: The paper builds directly on SPLADE's bi-encoder structure where BERT MLM logits produce sparse query/document representations scored via inverted index.
  - Quick check question: Can you explain why SPLADE uses MLM logits rather than pooled hidden states for sparse representations?

- **FLOPS Regularization**
  - Why needed here: Understanding how FLOPS loss enforces sparsity and uniform term activation is essential for interpreting the efficiency-effectiveness tradeoffs in pruning experiments.
  - Quick check question: What does FLOPS loss penalize, and how does joint FLOPS (used in ESPLADE) differ from the original formulation?

- **Static Pruning in Inverted Indices**
  - Why needed here: The paper applies post-hoc pruning to learned sparse representations; understanding traditional static pruning clarifies why this works and its limitations.
  - Quick check question: How does static pruning differ from dynamic pruning, and what is the tradeoff in terms of index size versus retrieval completeness?

## Architecture Onboarding

- **Component map:** BERT backbone (6-layer, 768-dim) → expanded vocabulary construction (100K unigrams) → MLM head initialization via mean-pooled WordPiece weights → EMLM pretraining on corpus → finetuning with in-batch negatives and joint FLOPS regularization → ESPLADE model → static pruning (top-k terms by logit score) → sparse representations → inverted index retrieval

- **Critical path:**
  1. Construct 100K vocabulary from target corpus unigrams
  2. Initialize MLM head by mean-pooling WordPiece embeddings for each expanded term
  3. Pretrain EMLM with masked language modeling on large corpus (600K steps)
  4. Finetune with in-batch negatives and joint FLOPS loss on click logs
  5. Apply query/document pruning to meet FLOPS budget before deployment

- **Design tradeoffs:**
  - Larger vocabulary (100K) improves pruning robustness but increases GPU memory during training (requires smaller batch sizes, which may hurt effectiveness per footnote 3)
  - ESPLADE pretraining adds compute overhead but provides consistent effectiveness gains
  - Joint FLOPS loss optimizes Q-D intersection sparsity directly but may converge differently than separate Q/D regularization

- **Failure signatures:**
  - Overfitting: 32K models at 400K steps show degraded test performance (Section 3.3.2)
  - Excessive pruning: qk=5, dk=10 with strict training-time masking causes large effectiveness drops (Table 6, splade-32K-ts-0.1m-e15)
  - Insufficient pretraining: Random initialization underperforms ESPLADE across all FLOPS levels (Figure 1)

- **First 3 experiments:**
  1. **Vocabulary size ablation**: Train identical models with 32K, 50K, 100K, and 150K vocabularies (randomly initialized), apply identical pruning, and plot MRR@10 vs. FLOPS to isolate the size effect.
  2. **Pretraining depth study**: Compare ESPLADE models at different pretraining checkpoints (e.g., 100K, 300K, 600K steps) to identify when vocabulary weights stabilize for effective finetuning.
  3. **Pruning sensitivity analysis**: For a fixed model, sweep qk ∈ {3, 5, 10, 20} and dk ∈ {5, 10, 20, 50} to map the efficiency-effectiveness frontier and identify the Pareto-optimal operating point for your latency budget.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the training efficiency of SPLADE models be improved for vocabulary dimensions significantly larger than 100K?
- Basis in paper: [explicit] The conclusion explicitly identifies "efficiency of training for larger dimensions" as a potential future topic, noting that current large vocabularies create GPU memory bottlenecks.
- Why unresolved: Larger vocabularies increase the size of the MLM logit vector per token, restricting batch sizes (e.g., 3072 vs 7168 for 32K) which negatively impacts model effectiveness.
- What evidence would resolve it: Novel training strategies, such as skipping the storage of lower-ranked MLM logits during the backward pass, that allow for larger batch sizes without increasing memory overhead.

### Open Question 2
- Question: To what extent do the combination of ranking and FLOPS losses force the convergence of query and document term counts ($L_0$) across different vocabulary sizes?
- Basis in paper: [explicit] Section 5.2 observes that $L_0$ values converge regardless of vocabulary size and states, "Although further investigation is needed, we expect the combination of ranking and FLOPS loss affects this phenomenon."
- Why unresolved: It is unclear if the convergence is an inherent property of the sparse representation learning or a specific artifact of the FLOPS regularization interacting with the ranking objective.
- What evidence would resolve it: Ablation studies varying the weights of ranking and FLOPS losses independently to observe their effect on $L_0$ convergence and representational distribution.

### Open Question 3
- Question: Why are "added-latent" vocabularies (randomly initialized dimensions) utilized less effectively for retrieval than pretrained semantic vocabulary extensions?
- Basis in paper: [inferred] Footnote 4 notes that added-latent vocabularies exhibit smaller posting list lengths and scores, suggesting they are "less aligned" with pretrained weights, and explicitly states "more experimentation and analyses are needed to clarify this clearly."
- Why unresolved: The paper demonstrates that pretrained vocabularies outperform random ones, but the specific mechanism preventing the model from fully utilizing the added latent dimensions during finetuning remains unidentified.
- What evidence would resolve it: Comparative analysis of gradient updates and activation patterns for pretrained vs. randomly initialized vocabulary tokens during the fine-tuning process.

## Limitations
- Results depend heavily on proprietary in-house click logs and a Korean-centric corpus, limiting generalizability to other domains or languages
- The paper does not conduct cross-lingual or cross-domain validation to verify if vocabulary size effects persist outside the specific retrieval context studied
- While the study demonstrates efficiency gains through static pruning, it does not address dynamic pruning strategies or the impact on real-time query latency

## Confidence
- **High confidence**: Vocabulary size improves effectiveness under pruning constraints; ESPLADE pretraining consistently outperforms random initialization; ranking + FLOPS losses transform vocabulary semantics from lexical to latent features.
- **Medium confidence**: Larger vocabularies reduce posting list lengths and improve pruning robustness; ESPLADE pretraining transfers semantic knowledge from MLM to ranking tasks.
- **Low confidence**: Claims about semantic transformation mechanisms without extensive ablation studies; effectiveness of vocabulary size effects in non-Korean or non-click-log domains.

## Next Checks
1. **Cross-lingual validation**: Replicate the vocabulary size and initialization experiments using English Wikipedia or MS MARCO to verify if findings generalize beyond Korean search logs.
2. **Dynamic pruning comparison**: Implement and compare static vs. dynamic pruning strategies on the same models to quantify real-world latency improvements and effectiveness trade-offs.
3. **Vocabulary initialization ablation**: Conduct a controlled study varying initialization methods (random, WordPiece pooling, random orthogonal) to isolate the specific contribution of ESPLADE's mean-pooling approach to effectiveness gains.