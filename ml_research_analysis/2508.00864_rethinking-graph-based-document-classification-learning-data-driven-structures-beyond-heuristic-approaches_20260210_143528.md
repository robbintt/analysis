---
ver: rpa2
title: 'Rethinking Graph-Based Document Classification: Learning Data-Driven Structures
  Beyond Heuristic Approaches'
arxiv_id: '2508.00864'
source_url: https://arxiv.org/abs/2508.00864
tags:
- graph
- document
- classification
- graphs
- learned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel self-attention-based framework for
  learning data-driven graph structures in document classification. Unlike traditional
  heuristic-based graph constructions, the method automatically identifies task-relevant
  dependencies between sentences via learned attention weights, followed by statistical
  filtering to retain only the most salient connections.
---

# Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches

## Quick Facts
- arXiv ID: 2508.00864
- Source URL: https://arxiv.org/abs/2508.00864
- Reference count: 19
- Self-attention-based framework learns data-driven graph structures for document classification

## Executive Summary
This paper introduces a novel self-attention-based framework for document classification that learns data-driven graph structures instead of relying on heuristic approaches. The method automatically identifies task-relevant dependencies between sentences through learned attention weights, followed by statistical filtering to retain only the most salient connections. Experiments on BBC News, HND, and arXiv datasets demonstrate consistent performance improvements over traditional heuristic-based methods, with gains up to 4.3 F1 points.

The approach eliminates the need for manual design and domain-specific rules, showing particular effectiveness for longer documents. The paper also identifies optimal filtering strategies for different document lengths, with max-bound filtering performing best for long texts and mean-bound filtering excelling for medium-length documents.

## Method Summary
The framework employs a self-attention mechanism to learn inter-sentence relationships within documents, creating a graph structure where nodes represent sentences and edges represent learned dependencies. Attention weights are computed between all sentence pairs, capturing semantic relationships that may not be apparent through surface-level heuristics. These weights are then filtered using statistical methods (max-bound or mean-bound) to remove noise and retain only the most significant connections. The resulting graph is used for document classification through standard graph neural network or attention-based processing.

## Key Results
- Learned graph structures consistently outperform heuristic baselines across all tested datasets
- Performance improvements reach up to 4.3 F1 points compared to traditional methods
- Max-bound filtering shows superior performance for longer documents
- Mean-bound filtering is optimal for medium-length documents

## Why This Works (Mechanism)
The approach succeeds by capturing semantic dependencies between sentences that traditional heuristics miss. Self-attention naturally learns which sentence pairs are most relevant for the classification task based on the actual content and context, rather than relying on predefined rules about sentence proximity or topic similarity. This data-driven approach adapts to the specific characteristics of each dataset and document type, automatically discovering the most informative relationships.

## Foundational Learning
1. **Self-attention mechanisms** - Needed for capturing semantic relationships between sentences; Quick check: Verify attention weights correlate with semantic similarity
2. **Graph neural networks** - Required for processing the learned document structures; Quick check: Ensure message passing preserves important features
3. **Statistical filtering methods** - Essential for removing noisy connections from attention matrices; Quick check: Confirm filtered graphs maintain task-relevant information
4. **Document representation learning** - Critical for initial sentence embeddings; Quick check: Validate embeddings capture document semantics
5. **Graph-based classification** - Fundamental framework for leveraging learned structures; Quick check: Test classification accuracy with different graph architectures

## Architecture Onboarding
**Component Map**: Sentence embeddings -> Self-attention computation -> Attention matrix -> Statistical filtering -> Graph structure -> Classification model

**Critical Path**: The attention computation and filtering steps are most critical, as they directly determine the quality of the learned graph structure that drives classification performance.

**Design Tradeoffs**: 
- Self-attention provides rich semantic relationships but increases computational cost
- Statistical filtering reduces noise but may remove useful connections if thresholds are too aggressive
- Learned structures adapt to data but require sufficient training examples

**Failure Signatures**:
- Poor classification performance indicates insufficient attention learning or overly aggressive filtering
- Inconsistent results across document lengths suggest inappropriate filtering thresholds
- High computational cost may indicate scalability issues with attention computation

**First 3 Experiments**:
1. Compare learned graph performance against multiple heuristic baselines on each dataset
2. Ablate filtering methods to identify optimal strategies for different document lengths
3. Analyze attention weight distributions to understand learned dependencies

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications emerge from the work. The approach's effectiveness across diverse document types remains unexplored, and the optimal selection of filtering thresholds lacks theoretical justification. The computational efficiency for very long documents and large-scale applications is not thoroughly analyzed.

## Limitations
- Limited validation to three specific datasets (BBC News, HND, and arXiv)
- Empirical rather than theoretical selection of filtering thresholds
- Unclear computational scalability for very long documents
- No comparison with state-of-the-art transformer-based document classification methods

## Confidence
- Graph structure learning effectiveness: High - consistent improvements across multiple datasets
- Filtering method superiority: Medium - optimal method varies by document length
- Domain transferability: Low - limited to three specific datasets
- Computational efficiency claims: Low - insufficient empirical evidence

## Next Checks
1. Test the approach on diverse document types including legal documents, technical manuals, and social media posts to assess domain transferability
2. Conduct ablation studies varying filtering thresholds systematically to identify optimal parameter ranges
3. Measure and compare computational complexity and memory requirements against traditional heuristic methods for varying document lengths