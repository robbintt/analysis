---
ver: rpa2
title: 'Unforgotten Safety: Preserving Safety Alignment of Large Language Models with
  Continual Learning'
arxiv_id: '2512.10150'
source_url: https://arxiv.org/abs/2512.10150
tags:
- safety
- fine-tuning
- data
- arxiv
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper frames safety degradation in fine-tuned language models
  as a continual learning problem, attributing it to catastrophic forgetting of safety
  alignment. The authors adapt several CL approaches (regularization-based, memory-based,
  and model merging) to preserve safety when fine-tuning on both benign and poisoned
  data.
---

# Unforgotten Safety: Preserving Safety Alignment of Large Language Models with Continual Learning

## Quick Facts
- **arXiv ID:** 2512.10150
- **Source URL:** https://arxiv.org/abs/2512.10150
- **Reference count:** 12
- **Primary result:** Continual learning methods reduce attack success rates when fine-tuning LLMs while preserving safety alignment

## Executive Summary
This paper addresses the critical problem of safety degradation in fine-tuned language models, which often forget their safety alignment due to catastrophic forgetting. The authors frame this as a continual learning problem and adapt multiple CL approaches (regularization-based, memory-based, and model merging) to preserve safety during fine-tuning on both benign and poisoned data. They evaluate across three tasks (GSM8K, SST2, Code) and three model families (LLaMA2-7B, Mistral-7B, Gemma-2B), demonstrating that CL methods consistently outperform standard fine-tuning in maintaining safety while preserving task performance.

## Method Summary
The paper adapts several continual learning approaches to preserve safety alignment during fine-tuning: regularization-based methods (EWC, MAS, L2), memory-based approaches (DER), and model merging techniques (EM, SLERP). The framework treats safety degradation as catastrophic forgetting and applies CL methods to mitigate this effect. The evaluation includes both benign fine-tuning scenarios and poisoned data scenarios where safety violations are intentionally injected. The authors measure performance using attack success rates for safety metrics and task-specific performance metrics, comparing CL methods against standard fine-tuning baselines.

## Key Results
- CL methods consistently reduce attack success rates compared to standard fine-tuning across all tested scenarios
- DER (Dark Experience Replay) achieves the best safety-utility balance across all scenarios, including high poison ratios
- Safety preservation generalizes across different model families (LLaMA2-7B, Mistral-7B, Gemma-2B) and task types (GSM8K, SST2, Code)
- The safety benefits persist even when fine-tuning on poisoned data with high contamination ratios

## Why This Works (Mechanism)
The paper's mechanism centers on catastrophic forgetting in safety alignment during fine-tuning. Standard fine-tuning overwrites the safety parameters learned during pre-training, causing models to forget safety constraints. Continual learning methods prevent this by maintaining gradients from safety-relevant parameters or by preserving safety-relevant examples in memory. Regularization methods penalize changes to safety-aligned weights, memory-based approaches replay safety examples during fine-tuning, and model merging combines safety-preserved and task-optimized parameters. This prevents the overwriting of safety knowledge while still allowing task-specific improvements.

## Foundational Learning
- **Catastrophic forgetting**: When neural networks learn new tasks, they overwrite previous knowledge - critical for understanding why safety alignment degrades during fine-tuning
- **Safety alignment in LLMs**: The process of training models to avoid harmful outputs and follow ethical guidelines - why needed: forms the baseline knowledge that degrades
- **Poisoning attacks**: Intentional contamination of training data with harmful examples - quick check: verify attack success rate metrics
- **Regularization-based CL**: Methods that penalize changes to important parameters - quick check: compare EWC vs MAS performance
- **Experience replay**: Storing and replaying previous examples during training - quick check: verify memory requirements scale with safety examples
- **Model merging**: Combining parameters from different models - quick check: validate interpolation weights preserve safety

## Architecture Onboarding

**Component Map:** Pre-training -> Safety Alignment -> Fine-tuning (Task + Safety Preservation) -> Evaluation

**Critical Path:** Safety alignment preservation requires balancing task performance optimization with safety constraint maintenance through CL methods

**Design Tradeoffs:** CL methods add computational overhead and memory requirements but provide significant safety benefits. Regularization methods are lightweight but may be less effective than memory-based approaches. Memory-based methods require storing safety examples but provide stronger preservation.

**Failure Signatures:** High attack success rates indicate catastrophic forgetting of safety alignment. Poor task performance suggests over-regularization. Inconsistent safety across different attack types may indicate incomplete safety preservation.

**First Experiments:**
1. Compare attack success rates of standard fine-tuning vs CL methods on a single model-task pair
2. Measure computational overhead of DER vs regularization methods
3. Test safety preservation across different poison ratios (10%, 50%, 90%)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on attack success rates and task performance but lacks extensive long-term stability analysis across multiple fine-tuning iterations
- Results based on relatively small models (7B parameters) may not generalize to larger 70B+ parameter models
- Poisoning methodology tests specific attack patterns that may not capture full complexity of real-world safety violations

## Confidence

**High confidence:** The core finding that CL methods reduce attack success rates compared to standard fine-tuning is well-supported by experimental results across multiple models and tasks.

**Medium confidence:** The claim about DER achieving the best safety-utility balance, while supported by data, could benefit from additional ablation studies on different poison ratios and task types.

**Medium confidence:** The generalizability claim across different models and tasks is supported but based on a limited sample of three model families and three tasks.

## Next Checks
1. Test CL methods across wider range of model sizes (including 70B+ parameter models) to assess scalability of safety preservation benefits
2. Conduct long-term stability experiments tracking safety performance across multiple sequential fine-tuning iterations
3. Implement broader range of safety violation types beyond specific poisoning scenarios tested to validate robustness against diverse attack patterns