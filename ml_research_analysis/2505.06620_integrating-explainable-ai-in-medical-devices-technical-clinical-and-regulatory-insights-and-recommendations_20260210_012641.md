---
ver: rpa2
title: 'Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory
  Insights and Recommendations'
arxiv_id: '2505.06620'
source_url: https://arxiv.org/abs/2505.06620
tags:
- clinicians
- clinical
- explanations
- these
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined the integration of explainable AI (XAI) in
  medical devices, focusing on enhancing trust and safety in clinical decision support
  systems (CDSS). An expert working group, including clinicians, regulators, and data
  scientists, evaluated AI models and XAI methods for predicting heart attack risk.
---

# Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations

## Quick Facts
- arXiv ID: 2505.06620
- Source URL: https://arxiv.org/abs/2505.06620
- Reference count: 40
- Primary result: XAI methods improve clinician trust and diagnostic accuracy but require careful implementation to avoid over-reliance and automation bias.

## Executive Summary
This study investigates the integration of explainable AI (XAI) in medical devices, specifically for cardiovascular risk prediction. Through an expert working group including clinicians, regulators, and data scientists, the research evaluates both AI models and XAI methods to enhance trust and safety in clinical decision support systems. The findings reveal that while XAI improves clinician confidence and diagnostic accuracy, over-reliance on AI outputs poses significant safety concerns. Regulators emphasize interpretability and model simplicity, while clinicians prefer explanations that align with their clinical knowledge. The study recommends using simpler models with effective XAI, comprehensive stakeholder training, and rigorous validation against clinical standards to ensure safe AI integration in healthcare settings.

## Method Summary
The study employed a mixed-methods approach involving an expert working group of clinicians, regulators, and data scientists. They evaluated multiple AI models (logistic regression, random forest, ANN) using a synthetic cardiovascular dataset (10,000 records, 22 features) derived from CPRD. Global explanations (odds ratios, Gini importance, permutation importance) and local explanations (LIME, counterfactuals via ExMatrix) were generated for each model. A pilot study with clinicians tested how explanations affected diagnostic decisions and confidence levels. The working group iteratively refined XAI methods based on stakeholder feedback, focusing on interpretability, usability, and alignment with clinical workflows.

## Key Results
- XAI methods improved clinician trust and diagnostic accuracy when explanations aligned with clinical knowledge.
- Over-reliance on AI outputs emerged as a safety concern, particularly with high-confidence misclassifications.
- Regulators prioritized model interpretability and simplicity, favoring intrinsically interpretable models when performance was comparable.
- Clinicians found simplified counterfactual explanations more actionable than complex feature combinations.
- The study recommends comprehensive stakeholder training and validation against clinical standards for safe AI integration.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Local explanations (e.g., LIME) increase clinician trust and improve diagnostic accuracy when aligned with clinical knowledge.
- **Mechanism:** XAI methods like LIME provide feature-level attributions for individual predictions, making model reasoning partially transparent. When clinicians see explanations matching their mental models (e.g., "angina history" driving high-risk prediction), trust calibration improves.
- **Core assumption:** Clinicians have sufficient time and cognitive bandwidth to interpret visual explanations, and their prior clinical knowledge aligns with the model's learned feature importances.
- **Evidence anchors:**
  - "Key findings revealed that while XAI methods improved clinician trust and diagnostic accuracy..."
  - "After revealing AI diagnoses, explanations, and decision confidence, all clinicians agreed that these explanations boosted their confidence in their diagnoses..." (Pilot Study Results)
  - Neighbors explore similar XAI trust mechanisms but lack stronger causal evidence.
- **Break condition:** If clinicians encounter high-confidence misclassifications where explanations appear plausible but are wrong (false positives with high model confidence), trust may be miscalibrated, leading to automation bias.

### Mechanism 2
- **Claim:** Global explanations help regulators and clinicians assess model alignment with clinical knowledge and detect potential biases.
- **Mechanism:** Global XAI methods (odds ratios for logistic regression, mean decrease Gini for random forests, permutation importance for ANNs) provide aggregate feature rankings. These allow stakeholders to verify whether the model's primary features are clinically plausible and to compare multiple models.
- **Core assumption:** Agreement between model-derived feature rankings and known clinical risk factors indicates a more reliable and safe model.
- **Evidence anchors:**
  - "Regulators emphasized the importance of interpretability and model simplicity..."
  - "All the global explanation methods presented in the workshops utilised feature ranking... Regulators also highlighted the importance of evaluating how these features related to the specific use case and aligned with clinical knowledge."
  - "Before the Clinic: Transparent and Operable Design Principles for Healthcare AI" discusses aligning AI design with clinician expectations but lacks causal validation.
- **Break condition:** Different models may rank features differently; inconsistent global explanations across models may reduce confidence or indicate that the model is learning spurious correlations.

### Mechanism 3
- **Claim:** Counterfactual explanations can guide clinical action by showing feature changes needed to alter risk predictions, but require feasible, actionable presentation.
- **Mechanism:** Counterfactual methods like ExMatrix identify minimal feature changes to flip a prediction (e.g., from high-risk to low-risk). When presented in simplified form, these may help clinicians understand modifiable risk factors.
- **Core assumption:** Features identified in counterfactuals are modifiable (e.g., treatable conditions) and actionable within clinical workflows.
- **Evidence anchors:**
  - Not mentioned directly in abstract.
  - "Counterfactual explanations are highly useful if they were introduced correctly... In its simplified form... the counterfactual cases were deemed easier for clinicians to interpret."
  - "Integrating Knowledge Graphs and Bayesian Networks" paper discusses hybrid explainable risk prediction but does not validate counterfactual utility.
- **Break condition:** If counterfactuals suggest modifying immutable features (age, gender) or complex combinations, they may be dismissed or cause confusion.

## Foundational Learning

- **Concept: Trust Calibration**
  - **Why needed here:** The paper highlights over-reliance (automation bias) and under-reliance (algorithm aversion) as key risks. Engineers must understand that providing explanations does not automatically fix trust issues.
  - **Quick check question:** Can you explain why a high-confidence model prediction with a plausible local explanation might still be wrong?

- **Concept: Global vs. Local Explanations**
  - **Why needed here:** Different stakeholders need different explanation types. Regulators focus on global behavior; clinicians often need case-specific reasoning.
  - **Quick check question:** For a heart attack risk model, would you use odds ratios (global) or LIME (local) to help a clinician understand why a specific patient was flagged as high-risk?

- **Concept: Model-Agnostic vs. Intrinsic Interpretability**
  - **Why needed here:** The paper compares intrinsically interpretable models (logistic regression) with post-hoc methods (LIME for neural networks). Trade-offs exist between performance and transparency.
  - **Quick check question:** If a random forest and a neural network achieve similar AUC on a tabular clinical dataset, which should you prefer from a regulatory standpoint, and why?

## Architecture Onboarding

- **Component map:**
  - Data Layer (CPRD cardiovascular dataset) -> Model Layer (LR, RF, ANN) -> Explanation Layer (Global: odds ratios, Gini, permutation; Local: LIME, counterfactuals) -> User Interface (visualization outputs)

- **Critical path:**
  1. Train and evaluate multiple models on the same dataset.
  2. Generate global explanations for each; compare top features against clinical knowledge.
  3. For black-box models, generate local explanations (LIME) and counterfactuals for selected cases.
  4. Present outputs to clinicians in a pilot study; measure changes in diagnostic decisions and confidence.

- **Design tradeoffs:**
  - **Model Complexity vs. Interpretability:** Simpler models (LR) may be preferred if performance is comparable; complex models (ANN) require additional XAI tooling.
  - **Explanation Fidelity vs. Usability:** Detailed counterfactual matrices provide more information but were deemed overwhelming; simplified visualizations improved usability.
  - **Automation vs. Human Oversight:** High model confidence does not guarantee correctness; final decision authority must remain with clinicians.

- **Failure signatures:**
  - **High-confidence misclassification:** Model shows 97% confidence on a false positive; clinicians may override correct judgment.
  - **Explanation-clinical mismatch:** Local explanations highlight features clinicians consider irrelevant, reducing trust.
  - **Over-complex counterfactuals:** Suggesting impossible changes (e.g., reduce age) undermines utility.

- **First 3 experiments:**
  1. **Model Benchmarking:** Train LR, RF, and ANN on the same folds. Compare AUC, sensitivity, specificity. Check if a simpler model achieves comparable performance.
  2. **Global Explanation Alignment:** Extract top 5 features from each model's global explainer. Have clinicians rate alignment with known risk factors. Identify discrepancies.
  3. **Local Explanation Usability:** Run LIME on false positive and true positive cases. Present simplified vs. detailed outputs to clinicians. Measure time to interpret and changes in diagnostic confidence.

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on a synthetic cardiovascular dataset rather than real-world clinical data, limiting generalizability to actual healthcare settings.
- The clinician pilot study involved a small, unspecified number of participants without reported statistical power calculations or confidence intervals.
- The paper does not document the decision-making process when conflicts arose between different stakeholder perspectives in the working group.
- While over-reliance on AI is identified as a safety concern, the paper lacks empirical evidence of how this manifests in clinical practice or test mitigation strategies.

## Confidence
- **High Confidence:** The core finding that XAI methods improve clinician trust when explanations align with clinical knowledge is supported by direct quotes from pilot study participants and consistent with established XAI literature.
- **Medium Confidence:** The recommendation to use simpler models with effective XAI when performance is comparable is reasonable but not empirically validated against real-world clinical outcomes or regulatory approval scenarios.
- **Low Confidence:** The assertion that counterfactual explanations are "highly useful if introduced correctly" lacks systematic validation of different presentation formats or comparison with alternative explanation methods.

## Next Checks
1. **Real-World Clinical Validation:** Test the XAI-enhanced models on actual patient data from multiple healthcare institutions, measuring not just diagnostic accuracy but also downstream clinical outcomes and decision-making time.
2. **Longitudinal Trust Calibration Study:** Conduct a controlled study tracking how clinicians' trust in AI predictions evolves over time with repeated exposure, measuring automation bias and algorithm aversion in realistic clinical scenarios.
3. **Regulatory Approval Simulation:** Create a mock regulatory submission using the XAI-enhanced models and have actual regulatory reviewers assess whether the explanations meet approval criteria for clinical use, identifying gaps between current practices and regulatory expectations.