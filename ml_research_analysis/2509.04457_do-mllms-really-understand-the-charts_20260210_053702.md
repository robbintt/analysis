---
ver: rpa2
title: Do MLLMs Really Understand the Charts?
arxiv_id: '2509.04457'
source_url: https://arxiv.org/abs/2509.04457
tags:
- chart
- reasoning
- answer
- visual
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ChartVRBench, a benchmark designed to isolate
  and evaluate genuine visual reasoning in chart understanding by focusing on numerical
  estimation tasks without explicit annotations. The authors identify that current
  multimodal large language models (MLLMs) rely heavily on text recognition rather
  than visual reasoning when interpreting charts, leading to significant performance
  degradation on non-annotated data.
---

# Do MLLMs Really Understand the Charts?
## Quick Facts
- arXiv ID: 2509.04457
- Source URL: https://arxiv.org/abs/2509.04457
- Reference count: 40
- Primary result: ChartVR-7B achieves 72.20% accuracy on ChartVRBench, outperforming open-source and proprietary baselines

## Executive Summary
This paper investigates whether current multimodal large language models (MLLMs) genuinely understand charts or merely rely on text recognition. The authors introduce ChartVRBench, a benchmark focused on numerical estimation from non-annotated charts, revealing that MLLMs fail significantly when explicit annotations are absent. To address this, they propose ChartVR, a model trained via a two-stage Visual Reasoning Reinforcement Finetuning (VR-RFT) strategy that combines supervised fine-tuning with reinforcement learning to enhance visual reasoning capabilities. ChartVR-7B achieves state-of-the-art results on both the new benchmark and public chart reasoning datasets.

## Method Summary
The authors propose a two-stage training strategy to improve visual reasoning in MLLMs for chart understanding. First, CoT-SFT (Chain-of-Thought Supervised Fine-Tuning) uses 43k CoT-distilled samples to teach structured reasoning. Second, GRPO (Group Relative Policy Optimization) reinforcement learning fine-tunes the model using 3.4k high-signal samples, optimizing for both format compliance and numerical accuracy via a composite reward. The approach is evaluated on ChartVRBench, a new benchmark with 2,453 QA pairs across 7 chart types and 38 topics, using relaxed accuracy (τ=0.02).

## Key Results
- ChartVR-7B achieves 72.20% accuracy on ChartVRBench, outperforming Qwen2.5-VL-7B (61.39%) and Gemini-2.5-Flash (55.77%)
- ChartVR shows strong generalization to public chart reasoning benchmarks
- Current MLLMs exhibit significant performance gaps between annotated and non-annotated chart settings

## Why This Works (Mechanism)
The method works by explicitly training the model to perform visual reasoning rather than relying on OCR-based shortcuts. The two-stage VR-RFT strategy first instills structured reasoning chains through supervised learning, then refines these skills via reinforcement learning with a reward that penalizes both format errors and numerical inaccuracies. This approach encourages the model to extract and reason about visual information directly, rather than defaulting to text-based interpretation.

## Foundational Learning
- Visual reasoning vs OCR: Understanding charts requires interpreting visual elements, not just recognizing text
  - Why needed: MLLMs default to text recognition, missing visual reasoning
  - Quick check: Compare performance on annotated vs non-annotated charts
- Chain-of-thought fine-tuning: Teaches structured reasoning before RL
  - Why needed: Provides a foundation for RL to build upon
  - Quick check: Validate CoT-SFT output format and reasoning quality
- Reinforcement learning for numerical accuracy: Optimizes reward based on relative error
  - Why needed: Directly targets the core task metric
  - Quick check: Verify reward calculation with known inputs

## Architecture Onboarding
**Component map**: Qwen2.5-VL-7B -> ViT encoder -> LLM -> Aligner -> Output
**Critical path**: ViT extracts visual features → Aligner conditions LLM → LLM generates reasoning chain → Output formatted answer
**Design tradeoffs**: Freezing vision tower during SFT preserves feature extraction; freezing during RL preserves reasoning patterns; composite reward balances format and accuracy
**Failure signatures**: 
- Poor reasoning but good format → Reward shaping issue
- Good format but poor accuracy → Reward weighting issue
- Unstable training → Learning rate or KL penalty misconfiguration

**First experiments**:
1. Validate ChartVRBench relaxed accuracy metric (τ=0.02) on held-out subset
2. Run CoT-SFT stage with specified hyperparameters to ensure base model is properly initialized
3. Execute small-scale GRPO with composite reward to verify reward shaping and policy updates

## Open Questions the Paper Calls Out
None

## Limitations
- The "true understanding" claim is behavioral, not probing internal reasoning
- GRPO implementation details (advantage normalization, KL penalty weight) are not fully specified
- Proprietary synthetic data and vague filtering criteria for "stochastic correctness" may limit external validation

## Confidence
- ChartVR outperforms baselines on ChartVRBench: High
- MLLMs don't truly understand charts: Medium (behavioral observation)
- Visual reasoning is foundational and transferable: Medium (inferred from generalization)

## Next Checks
1. Reproduce the relaxed accuracy metric (τ=0.02) on a held-out subset of ChartVRBench
2. Implement and validate the CoT-SFT stage with specified hyperparameters
3. Run a small-scale GRPO experiment with the composite reward to verify reward shaping and policy updates behave as expected