---
ver: rpa2
title: 'PerPO: Perceptual Preference Optimization via Discriminative Rewarding'
arxiv_id: '2502.04371'
source_url: https://arxiv.org/abs/2502.04371
tags:
- perpo
- discriminative
- optimization
- visual
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of visual discrimination tasks
  in generative pre-trained multimodal large language models (MLLMs). It proposes
  Perceptual Preference Optimization (PerPO), a method that aligns MLLMs with human
  visual perception by using discriminative rewards to gather negative samples and
  employing listwise preference optimization for ranking.
---

# PerPO: Perceptual Preference Optimization via Discriminative Rewarding

## Quick Facts
- **arXiv ID:** 2502.04371
- **Source URL:** https://arxiv.org/abs/2502.04371
- **Reference count:** 37
- **Primary result:** Improves visual discrimination in MLLMs via discriminative rewards and listwise optimization

## Executive Summary
This paper introduces Perceptual Preference Optimization (PerPO), a method to align Multimodal Large Language Models (MLLMs) with human visual perception by using discriminative rewards as optimization margins. PerPO addresses the challenge of visual discrimination tasks by gathering diverse negative samples through a discriminative reward engine and employing listwise preference optimization for ranking. The method effectively bridges generative preference optimization and discriminative empirical risk minimization, enhancing MLLMs' visual discrimination capabilities while maintaining their generative strengths. Experimental results demonstrate significant improvements on object grounding and dense OCR tasks across multiple datasets.

## Method Summary
PerPO employs discriminative rewarding to gather diverse negative samples from the model itself, using deterministic error metrics (IoU for grounding, Edit Distance for OCR) as perceptual rewards. It then applies listwise preference optimization with reward-margin weighting to optimize the ranking of model responses. The method trains on filtered datasets where the reward margin between best and worst responses exceeds a threshold, using LoRA for efficient fine-tuning. This approach mitigates image-unconditional reward hacking while improving visual discrimination performance.

## Key Results
- Improves object grounding by 3.42%, 8.18%, and 5.58% on RefCOCO, RefCOCO+, and RefCOCOg datasets respectively
- Reduces edit distance by 13.4% and 14.3% in dense OCR tasks on two baselines
- Significantly outperforms standard DPO on visual discrimination tasks while maintaining generative capabilities

## Why This Works (Mechanism)

### Mechanism 1: Discriminative Reward as a Scalable Proxy for Perception
The method uses deterministic error metrics (IoU/Edit Distance) as scalable rewards without human annotation. This exploits the "coarse-to-fine" capability of Best-of-N sampling, where high-reward samples correlate with visual accuracy. The assumption is that these metrics validly proxy human visual perception.

### Mechanism 2: Listwise Optimization to Force Image Reliance
Listwise ranking (comparing N>2 samples) forces the model to distinguish subtle visual differences rather than learning linguistic preferences independent of the image. This mitigates "image-unconditional reward hacking" where models optimize text patterns without visual grounding.

### Mechanism 3: Reward-Margin Weighting as Listwise ERM
The raw discriminative reward difference is used as a margin weight in the loss function, converting the problem into a form of Empirical Risk Minimization. The model is penalized more heavily for confusing very wrong answers with mostly right ones, harmonizing generative preference optimization with discriminative supervision.

## Foundational Learning

**Concept: Direct Preference Optimization (DPO)**
- **Why needed here:** PerPO modifies the standard DPO loss. Understanding the baseline pairwise comparison is essential to grasp why listwise approaches add value.
- **Quick check question:** Can you explain how DPO avoids training an explicit reward model?

**Concept: Empirical Risk Minimization (ERM)**
- **Why needed here:** The paper frames PerPO as bridging generative preference and discriminative ERM. Understanding ERM clarifies why ground truth error is used as a weight.
- **Quick check question:** What is the risk function being minimized in standard supervised classification?

**Concept: Listwise Ranking Loss (LiPO)**
- **Why needed here:** PerPO extends LiPO. Distinguishing pairwise (A vs B) from listwise (ranking A, B, C, D...) optimization strategies is crucial.
- **Quick check question:** Why might a listwise loss capture more information than a pairwise loss for the same number of samples?

## Architecture Onboarding

**Component map:** Base Model -> Sampler -> Reward Engine -> PerPO Loss

**Critical path:**
1. **Data Curation:** Sample N responses → Calculate Rewards → Filter by margin threshold to ensure diversity
2. **Optimization:** Feed sorted list into PerPO loss → Backprop

**Design tradeoffs:**
- **Sampling N:** Higher N increases data quality but drastically increases generation time
- **Data Margin Filter:** High threshold ensures distinct examples but discards more data
- **LoRA vs. Full Finetuning:** Uses LoRA for efficiency, limiting perceptual adaptation depth

**Failure signatures:**
- **Stagnant Performance:** Diminishing gains if base model is already capability-saturated
- **Reward Hacking:** High performance without image inputs indicates failed visual grounding

**First 3 experiments:**
1. **Sanity Check (Best-of-N):** Run base model with N=20; high Best-of-N performance indicates capability but alignment issues
2. **Ablation on List Size:** Compare N=2 (DPO) vs N=8 vs N=20 to verify listwise ranking's causal impact
3. **Image-Conditioning Test:** Evaluate with/without image inputs; large performance drop indicates successful visual grounding

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on deterministic ground truth metrics may not generalize to subjective visual judgments
- Optimal hyperparameters (sampling size N, margin thresholds) appear dataset-specific
- Limited validation beyond grounding and OCR tasks, raising questions about generalizability

## Confidence
- **High Confidence:** Technical implementation and loss formulation are sound and reproducible
- **Medium Confidence:** Empirical improvements on tested datasets are valid, but real-world perceptual alignment is harder to verify
- **Low Confidence:** Theoretical claim about bridging preference optimization and ERM lacks rigorous proof beyond empirical success

## Next Checks
1. **Generalization Test:** Apply PerPO to visual question answering with subjective answers to test method beyond deterministic metrics
2. **Ablation on Margin Threshold:** Systematically vary the margin filtering threshold to measure impact on performance
3. **Cross-Model Validation:** Train PerPO on stronger base models (GPT-4V/Gemini Pro) to test consistency of relative improvements