---
ver: rpa2
title: Large Language Models Acing Chartered Accountancy
arxiv_id: '2506.21031'
source_url: https://arxiv.org/abs/2506.21031
tags:
- financial
- language
- large
- llama
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CA-Ben, a comprehensive benchmark for evaluating
  large language models (LLMs) on financial, legal, and quantitative reasoning tasks
  specific to the Chartered Accountancy curriculum in India. CA-Ben consists of structured
  question-answer datasets derived from ICAI examinations across three difficulty
  levels.
---

# Large Language Models Acing Chartered Accountancy

## Quick Facts
- arXiv ID: 2506.21031
- Source URL: https://arxiv.org/abs/2506.21031
- Authors: Jatin Gupta; Akhil Sharma; Saransh Singhania; Mohammad Adnan; Sakshi Deo; Ali Imam Abidi; Keshav Gupta
- Reference count: 22
- Primary result: Claude 3.5 Sonnet achieved highest overall accuracy (56.31%) on CA-Ben benchmark

## Executive Summary
This paper introduces CA-Ben, a comprehensive benchmark for evaluating large language models (LLMs) on financial, legal, and quantitative reasoning tasks specific to the Chartered Accountancy curriculum in India. CA-Ben consists of structured question-answer datasets derived from ICAI examinations across three difficulty levels. Six prominent LLMs—GPT-4o, Claude 3.5 Sonnet, and four others—were evaluated using standardized prompts and protocols. Claude 3.5 Sonnet and GPT-4o achieved the highest overall accuracy, with Claude leading at 56.31% and GPT-4o at 54.29%. The results highlight strong performance in conceptual subjects but reveal notable challenges in numerical computations and legal interpretation, suggesting opportunities for improvement through hybrid reasoning and retrieval-augmented generation techniques.

## Method Summary
The study evaluated six LLMs (GPT-4o, Claude 3.5 Sonnet, LLAMA 3.3/3.1, Mistral, Phi-4) on CA-Ben, a benchmark derived from ICAI CA examination papers across Foundation, Intermediate, and Final levels. Models were prompted with standardized system prompts at temperature 0.75, and answers were extracted via regex pattern `(?:Result|Answer):\s*([A-D])?`. Accuracy was computed as the percentage of correct answers, with performance analyzed across subjects and difficulty levels. The methodology emphasized fair cross-model comparison through controlled prompting and parsing protocols.

## Key Results
- Claude 3.5 Sonnet achieved highest overall accuracy at 56.31%, followed by GPT-4o at 54.29%
- All models scored 20-40% on Taxation, highlighting numerical computation weaknesses
- Auditing & Ethics showed highest performance (93.33% for Claude), while Taxation and Corporate Laws posed significant challenges
- Performance decreased with difficulty level for most models, particularly in numerical and legal subjects

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standardized prompting with controlled temperature enables consistent cross-model comparison on structured multiple-choice financial questions.
- **Mechanism:** A fixed system prompt is combined with each question and passed to LLMs at temperature T=0.75. Responses are parsed via regex pattern to extract answer choices (A-D) for accuracy scoring.
- **Core assumption:** Temperature 0.75 balances creative flexibility with reliable answer selection without causing models to avoid provided choices.
- **Evidence anchors:**
  - Section 4.2: "A temperature setting of 0.75 was selected to introduce a balanced level of randomness..."
  - Section 4.3, Equation 2-3: Formalizes pipeline with regex extraction
  - Related work (FinBen, CFLUE) uses similar standardized evaluation protocols
- **Break condition:** If models frequently output non-parseable responses, the regex extraction fails and accuracy metrics become unreliable.

### Mechanism 2
- **Claim:** Hierarchical difficulty progression across Foundation → Intermediate → Final levels exposes differentiated model capabilities and failure modes.
- **Mechanism:** The benchmark structure organizes 14 papers across 3 levels with ascending complexity. Foundation tests analytical basics; Intermediate covers accounting, law, taxation; Final requires integrated strategic reasoning.
- **Core assumption:** ICAI exam difficulty progression maps to genuine reasoning complexity that models process differently.
- **Evidence anchors:**
  - "CA-Ben comprises structured question-answer datasets... spanning foundational, intermediate, and advanced CA curriculum stages."
  - Table 1: Claude drops from 60.00 (Foundation) to 54.23 (Final); GPT-4o rises from 54.00 to 55.28; LLAMA 3.3 70B collapses from 57.5 to 41.65.
  - CFA Level III evaluation shows similar hierarchical assessment patterns in financial domain.
- **Break condition:** If level progression doesn't correlate with genuine difficulty, cross-level comparisons become misleading.

### Mechanism 3
- **Claim:** Performance heterogeneity across subject categories reveals separable reasoning capabilities—conceptual/legal strength vs. numerical computation weakness.
- **Mechanism:** Different subjects demand different cognitive operations: Auditing & Ethics tests conceptual understanding (Claude: 93.33%); Taxation requires multi-step numerical calculation (all models: 20-40%); Corporate Laws involves legal text interpretation (GPT-4o: 73.33%).
- **Core assumption:** Subject-specific accuracy differences reflect underlying capability gaps rather than training data coverage variations.
- **Evidence anchors:**
  - "Results indicate variations in performance... especially in conceptual and legal reasoning. Notable challenges emerged in numerical computations and legal interpretations."
  - Section 5.4: "multi-step computations often led to errors... complex legal texts posed difficulties, with models sometimes misinterpreting jargon."
  - KFinEval-Pilot and TermGPT confirm terminology and domain adaptation challenges in financial/legal contexts.
- **Break condition:** If low taxation scores stem primarily from outdated tax law training data, the mechanism confounds computation failure with knowledge staleness.

## Foundational Learning

- **Concept:** Multi-step numerical reasoning in financial contexts (tax calculations, amortization, financial ratios)
  - **Why needed here:** All models scored 20-40% on Taxation; paper explicitly identifies "multi-step computations often led to errors" as a core limitation.
  - **Quick check question:** Given tax rate 30% and income ₹500,000 with deduction §80C of ₹150,000, compute final tax liability step-by-step.

- **Concept:** Legal terminology interpretation and statute-based reasoning
  - **Why needed here:** Corporate Laws performance varied (46-73%); "models sometimes misinterpreting jargon, resulting in incorrect answers."
  - **Quick check question:** Distinguish between "void ab initio" vs. "voidable" contract in Indian Contract Act context.

- **Concept:** Retrieval-Augmented Generation (RAG) for domain knowledge grounding
  - **Why needed here:** Paper recommends "augmenting LLMs with legal terminology, via RAG" as future improvement; addresses hallucination and knowledge staleness.
  - **Quick check question:** How would RAG help when a model encounters Indirect Tax Laws questions referencing GST amendments from 2024?

## Architecture Onboarding

- **Component map:**
  CA-Ben Dataset (14 papers × N questions in markdown) → System Prompt Template (Label + Surrogate Feature + Regulatory Mechanism + User Query) → LLM Layer (6 models) → Response Parser (regex: Result/Answer → [A-D]) → Accuracy Evaluator (Eq. 4: 1/N × Σ matches × 100)

- **Critical path:** Dataset preparation (markdown formatting, ground-truth labeling) → Prompt construction → Model inference at T=0.75 → Regex extraction → Accuracy computation against two thresholds (40% individual, 50% group).

- **Design tradeoffs:**
  - Temperature: 0.75 balances creativity vs. answer selection reliability; lower risks non-selection, higher risks incoherence.
  - Standardized vs. model-specific prompts: Paper uses identical prompts for fair comparison but may underutilize model-specific strengths.
  - Regex vs. LLM-based parsing: Regex is deterministic but brittle; LLM parsing could handle edge cases but introduces another failure layer.

- **Failure signatures:**
  - Null/void responses → temperature too low or prompt ambiguity
  - Consistent ~25% accuracy (random guessing) → model not engaging with domain
  - High conceptual scores, low numerical scores → computation pathway failure (not knowledge gap)
  - Legal interpretation errors on jargon → terminology embedding weakness

- **First 3 experiments:**
  1. **Baseline replication:** Run all 6 models on CA-Ben Foundation subset with T=0.75, verify accuracy ranges match paper (48-60%); identify any parsing failures.
  2. **Temperature sweep:** Test Claude 3.5 Sonnet and GPT-4o at T=0.5, 0.75, 1.0 on Taxation subset to quantify tradeoff between answer selection rate and accuracy.
  3. **RAG augmentation pilot:** Index ICAI study materials for one subject (e.g., Corporate Laws), implement retrieval-augmented prompting, compare accuracy delta vs. baseline for GPT-4o.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does a hybrid approach combining Large Language Models with symbolic reasoning and chain-of-thought prompting improve accuracy on multi-step numerical tasks within the CA-Ben benchmark?
- **Basis in paper:** Section 7 (Future Scope) suggests future improvements should explore "A hybrid approach combining LLMs with symbolic reasoning... and chain-of-thought prompting" to enhance step-by-step problem-solving.
- **Why unresolved:** The current study evaluated models using standardized prompts without these specific augmentations, revealing specific weaknesses in numerical computations where multi-step logic is required.
- **What evidence would resolve it:** A comparative study on CA-Ben showing performance differences between baseline LLMs and those augmented with symbolic calculators or CoT frameworks.

### Open Question 2
- **Question:** Does Retrieval-Augmented Generation (RAG) significantly enhance performance in legal interpretation subjects, such as Corporate Laws, by reducing misinterpretations of specific jargon?
- **Basis in paper:** Section 7 states that "Augmenting LLMs with legal terminology, via RAG... can significantly boost their capacity for accurate legal analysis."
- **Why unresolved:** The current results indicate that models struggled with complex legal texts and jargon, limiting their efficacy in legal domains despite high performance in other areas.
- **What evidence would resolve it:** Benchmark results comparing standard model outputs against RAG-enhanced outputs on the legal-specific subsets of the CA-Ben dataset.

### Open Question 3
- **Question:** Can self-correcting neural-symbolic architectures effectively eliminate the "null or void" hallucinations currently observed in models attempting complex taxation questions?
- **Basis in paper:** Section 7 proposes "Developing self-correcting AI architectures and neural-symbolic models" to ensure precision, while Section 5.4 notes that "Hallucinations remains a challenge in giving null or void answers."
- **Why unresolved:** The paper identifies hallucination as a persistent limitation in the current generation of models when faced with strict financial constraints.
- **What evidence would resolve it:** Error analysis of a self-correcting model showing a statistically significant reduction in void responses compared to the standard LLMs tested in this paper.

## Limitations
- CA-Ben dataset is not publicly available, preventing independent verification of question quality and distribution
- Exact system prompt wording is unspecified beyond structural components, introducing potential variability in replication
- Accuracy metric assumes clean regex extraction from LLM outputs; parsing failures could systematically bias results downward

## Confidence
- **High confidence:** Claude 3.5 Sonnet and GPT-4o achieved the highest overall accuracy (56.31% and 54.29% respectively) under standardized evaluation conditions.
- **Medium confidence:** Performance heterogeneity across subjects (conceptual vs. numerical) reflects genuine capability differences, though training data recency effects cannot be ruled out.
- **Low confidence:** Specific accuracy values for individual subjects and difficulty levels without access to raw data and replication results.

## Next Checks
1. **Dataset acquisition and verification:** Obtain the CA-Ben dataset or equivalent ICAI exam questions; verify question distribution, ground-truth labeling accuracy, and markdown formatting specifications.
2. **Temperature optimization study:** Systematically test GPT-4o and Claude 3.5 Sonnet across temperatures (0.5, 0.75, 1.0) on the Taxation subset to quantify the tradeoff between answer selection reliability and accuracy, validating the paper's choice of T=0.75.
3. **RAG augmentation pilot:** Implement retrieval-augmented generation using ICAI study materials for one subject (e.g., Corporate Laws); compare accuracy improvements against baseline to evaluate the proposed solution for legal terminology and knowledge staleness issues.