---
ver: rpa2
title: 'GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data'
arxiv_id: '2505.17082'
source_url: https://arxiv.org/abs/2505.17082
tags:
- darija
- data
- language
- reasoning
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "We show that a rigorously quality-over-quantity alignment strategy\
  \ can surface fluent Darija while safeguarding the backbone's cross-lingual reasoning\
  \ at a sliver of the usual compute. We translate three compact instruction suites\u2014\
  LIMA 1 K, DEITA 6 K and TULU 50 K\u2014into Darija, preserve 20 % of the English\
  \ originals, and add mathematics, coding and scientific prompts."
---

# GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data

## Quick Facts
- arXiv ID: 2505.17082
- Source URL: https://arxiv.org/abs/2505.17082
- Reference count: 40
- Trained GemMaroc-27B in 48 GPU·h to match Atlas-Chat on DarijaMMLU (61.6 %) and surpass it on HellaSwag (60.5 % vs. 48.4 %)

## Executive Summary
GemMaroc is a low-resource language technology project that fine-tunes Gemma 3 models on Moroccan Darija using a minimal yet carefully curated instruction set. By blending translated and retained English instructions with domain-specific Darija tasks, the method boosts Darija proficiency while preserving English reasoning and mathematics abilities. The approach demonstrates that high-impact multilingual performance can be achieved with a fraction of the usual compute, supporting sustainable AI development. Experiments show state-of-the-art results on Darija-centric benchmarks and highlight the potential for expanding inclusive NLP across North Africa.

## Method Summary
The method translates three compact instruction suites (LIMA 1K, DEITA 6K, TULU 50K) into Darija, retaining 20 % of the original English instructions, and supplements them with mathematics, coding, and scientific prompts. LoRA fine-tuning is applied to Gemma 3 models (4B and 27B parameters) using this mixed-language dataset. Training is completed in 48 GPU·h, with careful balance to maintain English capabilities while lifting Darija performance. Benchmarks include DarijaMMLU, HellaSwag, and general reasoning tasks, with comparisons to the Atlas-Chat baseline.

## Key Results
- LoRA-tuned Gemma 3-4B raises DarijaMMLU from 32.8 % to 42.7 %; adding TULU reasoning boosts it to 47.5 % with no English regression.
- GemMaroc-27B matches Atlas-Chat on DarijaMMLU (61.6 %) and outperforms it on HellaSwag (60.5 % vs. 48.4 %).
- The entire model is trained in just 48 GPU·h, exemplifying a Green AI approach.

## Why This Works (Mechanism)
The method's success hinges on quality over quantity: a small, carefully curated instruction set ensures strong Darija coverage without overwhelming the model. Retaining English instructions preserves the model's cross-lingual reasoning, while adding domain-specific tasks (math, coding, science) improves robustness. The LoRA fine-tuning approach allows efficient adaptation without retraining the entire model. Translation quality and prompt refinement further enhance Darija fluency, enabling the model to reason and respond effectively in a low-resource language.

## Foundational Learning
- **Darija language structure**: Understanding Darija's Arabic-based but uniquely Moroccan dialect is crucial for effective translation and prompt design.
  - Why needed: Ensures accurate instruction translation and culturally relevant responses.
  - Quick check: Validate Darija prompts with native speakers.
- **Instruction tuning**: The process of fine-tuning models on curated instruction-response pairs to improve task-specific performance.
  - Why needed: Enables the model to handle Darija queries with precision.
  - Quick check: Compare performance on held-out Darija benchmarks.
- **LoRA fine-tuning**: A parameter-efficient method that adds low-rank adapters for task adaptation.
  - Why needed: Allows efficient, targeted adaptation without full retraining.
  - Quick check: Measure GPU·h and parameter updates.
- **Cross-lingual reasoning**: Maintaining model capabilities across multiple languages after fine-tuning.
  - Why needed: Ensures English reasoning and math skills are preserved.
  - Quick check: Monitor English benchmark scores during training.
- **Dataset balancing**: Mixing translated Darija with retained English to optimize language representation.
  - Why needed: Prevents Darija-only overfitting and maintains multilingual fluency.
  - Quick check: Analyze language distribution in training data.
- **Benchmark selection**: Choosing appropriate evaluations for low-resource language proficiency.
  - Why needed: Accurately measures Darija gains relative to baselines.
  - Quick check: Confirm benchmark relevance to target language tasks.

## Architecture Onboarding

### Component Map
Dataset translation pipeline -> LoRA adapter injection -> Mixed-language instruction set -> Fine-tuning loop -> Evaluation on Darija/English benchmarks

### Critical Path
1. Translate instruction suites into Darija
2. Retain 20 % English originals
3. Inject LoRA adapters to Gemma base model
4. Fine-tune on mixed-language instruction set
5. Evaluate on Darija and English benchmarks

### Design Tradeoffs
- **Translation vs. native instruction creation**: Automated translation is faster and cheaper but may introduce errors; native Darija prompts could improve accuracy but require more resources.
- **Instruction set size**: A small set minimizes compute but risks missing edge cases; a larger set could improve robustness but increase training cost.
- **Language balance**: Retaining 20 % English maintains reasoning ability but may dilute Darija immersion; increasing Darija-only content could boost fluency but risks English regression.

### Failure Signatures
- Darija task performance plateaus despite more training data (overfitting to translation artifacts).
- English benchmark scores drop sharply after fine-tuning (imbalanced language representation).
- Model generates Darija text with English syntax or vocabulary (translation quality issues).

### First 3 Experiments to Run
1. Compare LoRA-tuned GemMaroc-27B against a fully fine-tuned baseline on DarijaMMLU to quantify efficiency gains.
2. A/B test performance with 10 %, 20 %, and 30 % English instruction retention to find optimal balance.
3. Evaluate sentiment analysis performance with and without dedicated Darija sentiment instructions to diagnose regression.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Would a Darija-specific or script-aware tokenizer yield significant performance gains over the retained SentencePiece tokenizer?
- Basis in paper: Limitations section states the model "retains the original SentencePiece tokenizer, which is not optimized for Darija's script and linguistic nuances."
- Why unresolved: No tokenizer ablation was conducted; all experiments used the default Gemma tokenizer.
- What evidence would resolve it: Comparing GemMaroc fine-tuned with a custom Darija tokenizer vs. the baseline on identical benchmarks.

### Open Question 2
- Question: Can the minimal-data, reasoning-dense alignment recipe transfer effectively to other under-resourced Maghrebi or North African dialects?
- Basis in paper: Conclusion states intent to "explore speech, retrieval-augmented generation, and other North-African dialects to extend the benefits of low-carbon language technology across the region."
- Why unresolved: No experiments were conducted on other dialects; only Moroccan Darija was tested.
- What evidence would resolve it: Applying the identical translation + LoRA pipeline to Algerian, Tunisian, or Libyan Arabic and reporting benchmark scores.

### Open Question 3
- Question: What causes the pronounced regression in sentiment analysis (−13.8 pp vs. Atlas-Chat) despite gains in other Darija tasks?
- Basis in paper: Table 2 shows GemMaroc-27B scores 59.25 % sentiment accuracy vs. Atlas-Chat-27B's 73 %; the discussion notes this as a domain lacking targeted data.
- Why unresolved: The paper does not analyze whether this is due to dataset composition, translation artifacts, or task-specific data scarcity.
- What evidence would resolve it: An ablation adding sentiment-focused instructions to the training mix, or error analysis comparing sentiment predictions across models.

### Open Question 4
- Question: How robust is the approach to translation quality—would human-verified or native-curated Darija instructions outperform machine-translated ones?
- Basis in paper: Limitations note "limited verification" of machine-translated samples; Section 3.1 mentions only iterative prompt refinement, not post-hoc correction.
- Why unresolved: All Darija training data was automatically translated; no comparison to natively authored or human-verified data was conducted.
- What evidence would resolve it: A controlled experiment comparing models trained on machine-translated vs. human-curated Darija instruction sets of equivalent size.

## Limitations
- Relies on machine-translated Darija instructions with limited verification, risking translation artifacts.
- Uses the original SentencePiece tokenizer, not optimized for Darija script and linguistic nuances.
- Does not conduct ablations for instruction set size, language balance, or tokenizer choice.

## Confidence

### Major Uncertainties and Limitations
- **Generalizability beyond Gemma family** (High confidence): The evaluation demonstrates success only with Gemma-3-4B and Gemma-3-27B. It is unclear whether the same alignment recipe would transfer to other architectures (e.g., Llama, Mistral) without retraining hyperparameters.
- **Test-set contamination risk** (Medium confidence): The instruction mix includes general reasoning and coding data that may overlap with held-out benchmarks such as GSM8K and HumanEval. Without full dataset release, inadvertent leakage cannot be excluded.
- **Static benchmark scope** (Medium confidence): All evaluations use English or Arabic-centric datasets. The absence of native Darija-only benchmarks makes it difficult to quantify absolute Darija fluency versus English-anchored proxy gains.
- **Long-term stability and scaling** (Low confidence): The 48 GPU·h training window is exceptionally brief. There is no data on performance drift over extended inference runs, nor on how further scaling (e.g., >27B parameters) would behave under this alignment regime.
- **Cultural and linguistic nuance capture** (Medium confidence): While HellaSwag shows improved commonsense, the datasets used may not fully represent the breadth of Darija idiomatic and regional variation across Morocco.

### Confidence Labels for Major Claims
- **Darija proficiency gains without English regression** (High confidence): Benchmark numbers are internally consistent and show monotonic improvement on Darija tasks with negligible English score changes.
- **Green AI advantage via minimal compute** (High confidence): The reported GPU·h figure is explicit and corroborated by the small instruction set size.
- **Matching or exceeding competitor models** (Medium confidence): Comparisons are limited to a single competitor (Atlas-Chat) on a narrow set of metrics, leaving open questions about relative performance on broader tasks.

## Next Checks
1. Replicate the alignment recipe on a non-Gemma base model (e.g., Llama-3-8B) to confirm architectural portability and calibrate hyperparameter stability.
2. Conduct a controlled ablation study using a Darija-only instruction set to isolate true Darija fluency from cross-lingual transfer effects.
3. Perform long-horizon inference tests (e.g., continuous chat sessions over multiple days) to verify that performance remains stable and that no catastrophic forgetting of either Darija or English abilities occurs.