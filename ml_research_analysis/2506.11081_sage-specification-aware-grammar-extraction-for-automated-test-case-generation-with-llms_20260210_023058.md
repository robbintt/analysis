---
ver: rpa2
title: SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation
  with LLMs
arxiv_id: '2506.11081'
source_url: https://arxiv.org/abs/2506.11081
tags:
- test
- generation
- grammar
- cases
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of automated test case generation
  for competitive programming, where generating valid and general grammars from natural
  language specifications remains challenging, especially under limited supervision.
  The proposed SAGE framework addresses this by first fine-tuning an open-source LLM
  (DeepSeek-R1-Distill-Qwen-14B) with supervised learning to perform specification-to-grammar
  translation, then applying Group Relative Policy Optimization (GRPO) to enhance
  grammar validity and generality using verifiable reward signals.
---

# SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs

## Quick Facts
- **arXiv ID:** 2506.11081
- **Source URL:** https://arxiv.org/abs/2506.11081
- **Reference count:** 40
- **Primary result:** Outperforms 17 open and closed-source LLMs with 96.66% set-based validity, 95.92% set-based generality, and 80.67% set-based effectiveness

## Executive Summary
This paper tackles the problem of automated test case generation for competitive programming, where generating valid and general grammars from natural language specifications remains challenging, especially under limited supervision. The proposed SAGE framework addresses this by first fine-tuning an open-source LLM (DeepSeek-R1-Distill-Qwen-14B) with supervised learning to perform specification-to-grammar translation, then applying Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality using verifiable reward signals. Additionally, the paper explores iterative feedback for refining grammar outputs in multi-turn generation. Experimental results demonstrate that SAGE achieves state-of-the-art performance, outperforming 17 open and closed-source LLMs with 96.66% set-based validity, 95.92% set-based generality, and 80.67% set-based effectiveness, improving over prior work by 15.92 percentage points in grammar validity and 12.34 percentage points in test effectiveness.

## Method Summary
The SAGE framework uses a two-phase approach: first, supervised fine-tuning (SFT) on DeepSeek-R1-Distill-Qwen-14B with 960 specification-grammar pairs to learn CCFG production rules and constraint formulations; second, reinforcement learning with GRPO using a reward function combining validity (ratio of valid test cases to total generated) and generality (ratio of ground-truth test cases the grammar can parse). The method optionally incorporates iterative feedback for error correction, though this shows minimal benefit for fine-tuned models. The framework operates on the CodeContests dataset with 1,200 problems, using 5-shot prompts with chain-of-thought reasoning and cross-entropy loss during SFT, followed by GRPO optimization with temperature=0.9 and top-p=0.9.

## Key Results
- Achieves 96.66% set-based validity, 95.92% set-based generality, and 80.67% set-based effectiveness
- Improves grammar validity by 15.92 percentage points and test effectiveness by 12.34 percentage points over prior work
- Demonstrates that SFT alone achieves 93.33% set-based validity, with GRPO adding 2.29 percentage points
- Shows iterative feedback benefits models without fine-tuning but provides marginal improvement for fine-tuned models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised fine-tuning with <1000 specification-grammar pairs enables an open-source 14B model to outperform closed-source models in grammar induction.
- **Mechanism:** Fine-tuning aligns the LLM's generation distribution toward valid CCFG production rules and constraint formulations by learning from structured prompt-response pairs that include role instructions, 5-shot examples, and chain-of-thought reasoning. Cross-entropy loss trains the model to produce grammars that satisfy both syntactic well-formedness and semantic constraint logic.
- **Core assumption:** The model has sufficient capacity to internalize the CCFG formalism and transfer patterns from seen to unseen specifications.
- **Evidence anchors:**
  - [abstract]: "Our approach first fine-tunes an open-source LLM to perform specification-to-grammar translation"
  - [Section V.A]: "the performance of an open-source LLM, DeepSeek-R1-Distill-Qwen-14B, with 14B parameters dramatically improves by supervised fine-tuning with less than 1,000 specification-grammar pairs"
  - [Table V]: SFT alone achieves 93.33% set-based validity vs 57.03% for base model
- **Break condition:** If training data lacks diversity in constraint types or specification structures, the model may overfit to patterns and fail to generalize to novel constraint formulations.

### Mechanism 2
- **Claim:** Reinforcement learning with GRPO and verifiable reward signals improves grammar validity and generality beyond supervised fine-tuning alone.
- **Mechanism:** GRPO computes group-relative advantages that encourage the policy to generate grammars optimizing validity (ratio of valid test cases to total generated) and generality (ratio of ground-truth test cases the grammar can parse). The reward R = R_V · R_G provides dense, task-specific feedback that shapes generation toward complete constraint coverage.
- **Core assumption:** Validity and generality metrics computed from k=5 sampled test cases provide reliable gradient signals for optimization.
- **Evidence anchors:**
  - [abstract]: "further applies Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality"
  - [Section III.C]: "These reward functions not only encourage the model to generate CCFGs closely matching the ground truth, but also emphasize its capacity to self-evaluate"
  - [Table V]: GRPO improves set-based generality from 93.33% to 95.92%
- **Break condition:** If sampled test cases fail to cover edge cases, the reward signal may reinforce incomplete grammars that appear valid on the sample but miss critical constraints.

### Mechanism 3
- **Claim:** Iterative feedback with error-specific corrections improves grammar quality for models without fine-tuning, but provides marginal benefit for already fine-tuned models.
- **Mechanism:** A validator parses generated grammars and produces structured error feedback (e.g., "Invalid regex pattern with '+' found"). The LLM regenerates with this feedback, iteratively fixing well-formedness violations across up to 5 turns. This targets error categories: null grammar, unbracketed counters, missing variable references, node overflow, and invalid non-terminals.
- **Core assumption:** The model can interpret error messages and map them to correct grammar modifications.
- **Evidence anchors:**
  - [abstract]: "We also examine the effectiveness of iterative feedback for open and closed-source LLMs in correcting syntactic and semantic errors"
  - [Section IX.E]: "iterative feedback shows clear benefits for models that are not fine-tuned... However, iterative feedback does not improve our fine-tuned models"
  - [corpus]: Related work "LogiCase" (FMR=0.537) addresses similar test case generation but without iterative refinement
- **Break condition:** When errors involve symbolic counter expressions (e.g., "n-1") that the CCFG formalism cannot represent, iterative refinement stalls without progress.

## Foundational Learning

- **Context-Free Grammars with Counters (CCFGs)**
  - Why needed here: CCFGs extend CFGs by storing and reusing counter values during derivation, enabling representation of interdependent constraints like "the second line contains n integers where n was defined on the first line."
  - Quick check question: Given a specification "The first line contains t. The next t lines each contain two integers," can you identify which values must be stored as counters?

- **Validity vs. Generality Trade-off**
  - Why needed here: These metrics define the reward landscape for RL. Validity measures precision (no invalid test cases generated); generality measures recall (all valid test cases can be generated). A grammar scoring high on one but low on the other is incomplete.
  - Quick check question: If a grammar generates only the test case "1 2 3" for a specification allowing any three integers, what are its validity and generality scores?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO adapts PPO for grouped task structures, computing advantages relative to group means rather than globally. This enables the model to learn patterns that generalize across structurally similar specifications.
  - Quick check question: How does the group-relative advantage Â_t^(g) = Â_t - E[Â_t'] differ from standard PPO advantage estimation?

## Architecture Onboarding

- **Component map:** Raw dataset → Prompt generation (rules + few-shot + CoT) → Fine-tuned LLM (SFT) → Grammar candidates → Validator (Algorithm 1) → Reward computation → GRPO update → RL-tuned LLM (optional) → Validator → Error message → Feedback prompt → Regeneration (up to 5 iterations)

- **Critical path:** SFT provides the foundation (78% effectiveness alone). RL adds 2.29 percentage points. The paper shows that removing SFT causes the largest degradation, making it the non-negotiable starting point.

- **Design tradeoffs:**
  - 14B model vs. larger models: Trades performance for accessibility/reproducibility
  - k=5 test case samples for reward: Trades reward accuracy for computational efficiency
  - 5-shot prompting: Trades context window for example coverage
  - Maximum 5 feedback iterations: Trades potential convergence for bounded inference cost

- **Failure signatures:**
  - Node overflow error: Production rules allow values beyond constraint bounds (e.g., `[0-9][0-9][0-9]` when constraint is `1 ≤ n ≤ 10^9`)
  - Symbolic counter expressions: Grammar attempts unsupported constructs like `[0-9]{n-1}`
  - Misinterpreted notation: `10^5` read as `105` instead of `100000`

- **First 3 experiments:**
  1. **SFT data ablation:** Train with 100, 500, and 960 specification-grammar pairs to establish the minimal viable training set size for your target domain.
  2. **Reward component analysis:** Compare R_V only, R_G only, and R_V · R_G to determine if both metrics are necessary or if one dominates learning.
  3. **Cross-domain transfer:** Evaluate grammars trained on CodeContests against specifications from a different competitive programming platform to test generalization bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can buggy reference code guide the generation of targeted test cases?
- **Basis in paper:** [explicit] The authors propose "incorporat[ing] incorrect or buggy reference code to guide the generation of targeted test cases."
- **Why unresolved:** The current framework relies on validity and generality rewards based on specifications, lacking a mechanism to target specific implementation faults.
- **What evidence would resolve it:** A modified reward function that leverages execution differences against buggy programs to guide grammar refinement.

### Open Question 2
- **Question:** Can effectiveness-guided reinforcement learning work with unlabeled specifications?
- **Basis in paper:** [explicit] The authors plan to "explore effectiveness-guided RL even with unlabeled specifications."
- **Why unresolved:** The current GRPO training depends on ground-truth grammars to calculate validity and generality rewards (Algorithm 1).
- **What evidence would resolve it:** An unsupervised learning setup that converges on high-quality grammars using only intrinsic validity metrics.

### Open Question 3
- **Question:** How can the grammar formalism be extended to handle more expressive constraints?
- **Basis in paper:** [explicit] The authors intend to "extend SAGE to more expressive grammar representations."
- **Why unresolved:** The case study highlights failure modes where the model struggles with dynamic counter expressions (e.g., `n-1`) and node overflow errors.
- **What evidence would resolve it:** A new grammar representation capable of encoding context-sensitive constraints without manual simplification.

## Limitations

- The framework's reliance on 960 training specifications may limit generalization to specifications with constraint types not well-represented in the training data.
- The CCFG formalism cannot handle symbolic counter expressions like "n-1", restricting the grammar's ability to represent certain valid constraints.
- The validator implementation is not fully specified, creating uncertainty about how edge cases and error feedback are handled in practice.

## Confidence

- **High confidence:** The empirical results showing 96.66% set-based validity and 80.67% set-based effectiveness, given the controlled evaluation setup with 10 correct and 10 incorrect solutions per problem.
- **Medium confidence:** The claim that SFT with <1000 pairs dramatically improves performance, as this assumes the training data adequately represents the diversity of constraint types in competitive programming.
- **Medium confidence:** The GRPO mechanism's effectiveness, as the reward computation relies on sampling only 5 test cases, which may not capture edge cases.

## Next Checks

1. **Data diversity analysis:** Analyze the distribution of constraint types in the 960 training specifications to verify coverage of arithmetic, regex, and symbolic constraints before claiming generalizability.
2. **Reward signal robustness:** Test GRPO with varying k values (k=1, 3, 10) for test case sampling to determine if the 5-sample reward computation is sufficient for stable policy gradients.
3. **Cross-domain evaluation:** Apply the fine-tuned model to specifications from a different competitive programming platform (e.g., AtCoder or HackerRank) to assess whether the learned grammar patterns transfer beyond the CodeContests domain.