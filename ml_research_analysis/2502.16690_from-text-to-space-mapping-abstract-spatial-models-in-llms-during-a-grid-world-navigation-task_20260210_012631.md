---
ver: rpa2
title: 'From Text to Space: Mapping Abstract Spatial Models in LLMs during a Grid-World
  Navigation Task'
arxiv_id: '2502.16690'
source_url: https://arxiv.org/abs/2502.16690
tags:
- spatial
- grid
- units
- agent
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates how LLMs represent spatial information\
  \ in text-based navigation tasks. It evaluates six LLaMA-3 models (1B to 90B parameters)\
  \ on a 5\xD75 grid-world task using six different spatial information representations\
  \ (SIRs)."
---

# From Text to Space: Mapping Abstract Spatial Models in LLMs during a Grid-World Navigation Task

## Quick Facts
- arXiv ID: 2502.16690
- Source URL: https://arxiv.org/abs/2502.16690
- Reference count: 40
- Primary result: Cartesian text representations yield highest spatial navigation performance in LLMs

## Executive Summary
This paper investigates how large language models (LLMs) represent spatial information through a grid-world navigation task. Using six LLaMA-3 models (1B-90B parameters) and six spatial information representations (SIRs), the study finds that Cartesian formats (JSON and chess notation) outperform topographic and textual descriptions across all model sizes. Probing the LLaMA-3.1-8B model reveals that spatial features like agent position and action correctness are encoded in intermediate layers, with some units showing invariance across SIR types. Interestingly, ablating these abstract spatial units does not significantly impact performance, suggesting distributed encoding.

## Method Summary
The study employs a 5×5 Grid-World Spatial Orientation Task (GWSOT) where agents navigate toward goals using four actions. Six models (LLaMA-3 family) are tested across six SIR variants (Cartesian, Topographic, Textual) with 100 trials each. Performance is measured by success rate, path efficiency, and final distance ratio. For probing, hidden states from the LLaMA-3.1-8B model are analyzed using linear regression to predict grid configurations, identifying units correlating with spatial features. Ablation experiments test the functional importance of identified units.

## Key Results
- Cartesian SIRs consistently outperform topographic and textual formats across all model sizes
- Spatial encoding units are concentrated in intermediate layers (8-22) of LLaMA-3.1-8B
- 286 cross-SIR invariant units predict action correctness but ablation shows minimal performance impact
- Performance scales with model size, with smaller models showing near-random performance on non-Cartesian SIRs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cartesian text representations (explicit x,y coordinates) yield superior spatial navigation performance compared to topographic or prose descriptions.
- Mechanism: Explicit coordinate systems enable LLMs to process spatial dimensions independently through numerical operations rather than requiring interpretive parsing of spatial structure from text layout.
- Core assumption: LLMs trained on code/mathematical notation develop stronger associations with structured coordinate formats than with spatial metaphors.
- Evidence anchors:
  - [abstract] "Cartesian representations consistently yield higher success rates and path efficiency across all model sizes"
  - [section 4] "Cartesian SIRs consistently outperformed Topographic and Textual SIRs across model sizes (Binomial GLM: βtopographic = 1.385, p < .001)"
  - [corpus] No direct corpus support for this specific mechanism
- Break condition: If task requires continuous spatial interpolation or 3D reasoning, Cartesian advantages may diminish.

### Mechanism 2
- Claim: Intermediate layers (approximately layers 8-22 in LLaMA-3.1-8B) concentrate units that encode abstract spatial features invariant to prompt format.
- Mechanism: Deeper processing consolidates task-relevant abstractions beyond surface token patterns, concentrating higher-level conceptual representations in middle layers.
- Core assumption: Layer specialization follows a processing hierarchy where early layers capture surface form and intermediate layers consolidate semantic content.
- Evidence anchors:
  - [abstract] "probing LLaMA-3.1-8B revealed subsets of internal units—primarily located in intermediate layers—that robustly correlate with spatial features"
  - [section 5] "These invariant units were primarily concentrated in intermediate layers (approximately layers 8–18)"
  - [corpus] Related work on cognitive maps (arxiv:2510.03286) supports hierarchical spatial abstraction in cognitive architectures
- Break condition: Larger models may distribute spatial encoding differently; mechanism not validated beyond 8B parameters.

### Mechanism 3
- Claim: A small subset of abstract spatial units (286 neurons) predicts action correctness across all representation types, yet ablation of these units does not significantly degrade performance.
- Mechanism: Spatial encoding is distributed across polysemantic neurons; identified abstract units represent a non-essential subset of a redundant, distributed spatial processing system.
- Core assumption: The ablation experiment fully captures the functional role of these units; compensation by other neurons is possible.
- Evidence anchors:
  - [abstract] "ablating these units does not significantly impact navigation performance, suggesting they represent only a minor component"
  - [section 6] "55% success rate during ablation trials compared to 59% for the intact model"
  - [corpus] No direct corpus validation of this specific redundancy claim
- Break condition: If spatial tasks increase in complexity (obstacles, 3D navigation), abstract units may become more critical.

## Foundational Learning

- Concept: Linear probing of hidden states
  - Why needed here: Core methodology for identifying units that predict spatial features from activation patterns
  - Quick check question: Can you explain why a high R² from a linear probe indicates spatial information is linearly decodable from activations?

- Concept: Bonferroni correction for multiple comparisons
  - Why needed here: Controls false positives when testing thousands of neurons for spatial correlations
  - Quick check question: Why is correction necessary when testing each of 4096 units per layer across 32 layers?

- Concept: Polysemantic neurons
  - Why needed here: Explains why ablating identified spatial units doesn't break the model—these neurons encode multiple concepts simultaneously
  - Quick check question: How does polysemanticity complicate mechanistic interpretation of individual neurons?

## Architecture Onboarding

- Component map: The spatial encoding system includes: (1) prompt-specific units concentrated in early layers encoding textual format; (2) abstract spatial units in intermediate layers (8-22) encoding x-coordinate (448 units), y-coordinate (258 units), boundary detection (373 units), and action correctness prediction (286 cross-SIR units); (3) widely distributed polysemantic neurons whose full spatial role remains unmapped.

- Critical path: When implementing spatial agents, prioritize Cartesian SIR formats → verify model size ≥8B for above-chance performance → probe intermediate layers for task-relevant spatial features.

- Design tradeoffs: Cartesian representations maximize performance but require structured state observations; smaller models (<8B) show near-random performance on non-Cartesian SIRs; ablation-based intervention is impractical due to polysemanticity and distributed encoding.

- Failure signatures: Success rates <20% with textual/topographic SIRs in models <11B parameters; invalid JSON responses from small models; inability to output coherent actions when ablating >50% of neurons in any layer group.

- First 3 experiments:
  1. Replicate the 5×5 GWSOT across SIR types on your target model to establish baseline performance and confirm Cartesian advantage.
  2. Extract and probe intermediate layer activations to identify units correlating with agent position; validate cross-SIR generalization.
  3. Test ablation of identified abstract spatial units (position-encoding or action-correctness units) to confirm redundancy before relying on them for interpretation or intervention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified abstract spatial units become necessary for navigation performance in more complex environments (obstacles, multiple goals, 3D spaces)?
- Basis in paper: [explicit] Discussion states: "it would be interesting to investigate whether the abstract components of the spatial model are necessary when solving harder problems or trying to navigate different kinds of environments."
- Why unresolved: Ablation of 286 core units showed minimal performance impact in the simple 5×5 grid, but this may not generalize to harder tasks where abstract representations could be critical.
- What evidence would resolve it: Ablating the core spatial units during navigation with obstacles, larger grids, or 3D environments and measuring whether performance degrades significantly.

### Open Question 2
- Question: Do larger LLMs (70B, 90B) exhibit similar abstract spatial unit structures, or does spatial encoding architecture change fundamentally with scale?
- Basis in paper: [explicit] Discussion notes: "We were not able to probe larger models due to constraints in memory and compute, but future work should look for equivalent structures in these larger LLMs to study how these internal representations of space change or stay invariant with scale."
- Why unresolved: Only the 8B model was probed; larger models may develop different or additional mechanisms for spatial representation.
- What evidence would resolve it: Applying the same linear probing methodology to 70B/90B models to identify cross-SIR invariant units and compare layer distributions.

### Open Question 3
- Question: How does multimodal (vision + text) spatial input affect the formation of abstract spatial units compared to text-only representations?
- Basis in paper: [explicit] Discussion states: "Future work should further explore how integrating visual cues with specific textual descriptions of space might provide complementary advantages in agentic frameworks."
- Why unresolved: All experiments used text-only representations; whether visual input creates more robust abstract spatial representations is unknown.
- What evidence would resolve it: Probing vision-language models on equivalent grid navigation tasks to identify spatial units and test cross-modal transfer between visual and textual SIRs.

### Open Question 4
- Question: Why do Cartesian representations yield better performance, and does this reflect training data prevalence or fundamental computational advantages?
- Basis in paper: [inferred] Discussion speculates: "It is also interesting that the JSON variant usually outperforms the Chess Notation SIR, which could be due to the prevalence of such representations in the training data."
- Why unresolved: The study demonstrates performance differences but cannot disentangle whether JSON's advantage stems from training distribution or cognitive/representational properties.
- What evidence would resolve it: Fine-tuning models on balanced SIR distributions, or analyzing pre-training corpora for representation frequency to correlate with performance differences.

## Limitations
- Spatial encoding analysis limited to simple 5×5 grid, limiting generalizability to complex spatial reasoning tasks
- Ablation experiments suggest redundancy but don't fully map the complete spatial processing architecture
- Performance gains from Cartesian representations may not extend to tasks requiring 3D reasoning or continuous spatial interpolation

## Confidence
- **High confidence:** Cartesian SIR superiority (direct experimental comparison across all model sizes with clear statistical significance)
- **Medium confidence:** Intermediate layer spatial encoding (probing results robust within tested architecture but not validated across model families)
- **Low confidence:** Functional necessity of identified abstract spatial units (ablation results suggest redundancy but distributed encoding complicates interpretation)

## Next Checks
1. Test the 6 SIR types on non-LLaMA models (e.g., GPT-4, Claude) to determine if Cartesian advantage generalizes across architectures.
2. Evaluate performance on extended spatial tasks with obstacles, variable step costs, or 3D navigation to test limits of identified spatial encoding mechanisms.
3. Conduct targeted ablation of non-abstract units identified in intermediate layers to better characterize the complete spatial processing architecture and identify truly essential components.