---
ver: rpa2
title: 'Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and
  Control in Spaces'
arxiv_id: '2506.00123'
source_url: https://arxiv.org/abs/2506.00123
tags:
- arxiv
- vebrain
- robot
- multimodal
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VeBrain, a unified framework that integrates
  multimodal understanding, visual-spatial reasoning, and robotic control into a single
  model. The key innovation is reformulating robotic control as text-based tasks in
  2D visual space, specifically keypoint detection and embodied skill recognition,
  rather than direct action policy learning.
---

# Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces

## Quick Facts
- arXiv ID: 2506.00123
- Source URL: https://arxiv.org/abs/2506.00123
- Authors: Gen Luo; Ganlin Yang; Ziyang Gong; Guanzhou Chen; Haonan Duan; Erfei Cui; Ronglei Tong; Zhi Hou; Tianyi Zhang; Zhe Chen; Shenglong Ye; Lewei Lu; Jingbo Wang; Wenhai Wang; Jifeng Dai; Yu Qiao; Rongrong Ji; Xizhou Zhu
- Reference count: 40
- One-line primary result: Unified multimodal model achieving +5.6% on MMVet and +50% on legged robot tasks through 2D keypoint-based control reformulation

## Executive Summary
VeBrain introduces a unified framework that integrates multimodal understanding, visual-spatial reasoning, and robotic control into a single model. The key innovation is reformulating robotic control as text-based tasks in 2D visual space, specifically keypoint detection and embodied skill recognition, rather than direct action policy learning. This approach enables VeBrain to maintain strong multimodal capabilities while adding control abilities. The framework includes a novel robotic adapter that converts textual control signals into executable motion policies.

## Method Summary
VeBrain fine-tunes Qwen2.5-VL-7B with frozen vision tower and projector, training only the LLM on VeBrain-600k dataset (200k multimodal, 312k spatial, 88k control samples). The model reformulates robot control as 2D keypoint detection and skill recognition, using a four-module robotic adapter (LocoTrack tracking, depth-to-3D conversion, skill executor, dynamic takeover) to bridge MLLM predictions to real robot execution. Training uses LR=5e-6, 1 epoch, 4865 steps on 32 A100 GPUs (~2 days).

## Key Results
- Achieves +5.6% improvement on MMVet compared to Qwen2.5-VL baseline
- Demonstrates +50% average gains on legged robot tasks
- Excels at both 2D multimodal tasks and 3D spatial reasoning benchmarks
- Maintains superior performance across diverse robotic control tasks

## Why This Works (Mechanism)

### Mechanism 1
Reformulating robot control as 2D keypoint detection and skill recognition preserves multimodal understanding while enabling control. By mapping control to the same text generation objective used for understanding tasks, VeBrain avoids task conflicts that plague VLA models.

### Mechanism 2
The robotic adapter closes the gap between discrete MLLM predictions and continuous robot execution through real-time point tracking and skill execution. The four-module pipeline maintains keypoint consistency, converts 2D→3D coordinates, maps text commands to pre-trained policies, and triggers re-planning on failure.

### Mechanism 3
Chain-of-thought integration across modalities enables compositional task handling that pure control data cannot achieve. CoT data generation forces the model to describe observations, decompose tasks, and decide on control in one conversation, creating shared representations between perception, reasoning, and action.

## Foundational Learning

- **Concept: Vision-Language-Action (VLA) Models**
  - Why needed here: VeBrain positions itself as an alternative to VLA approaches that sacrifice multimodal understanding for control. Understanding this tradeoff is essential.
  - Quick check question: Can you explain why VLA models trained on large robotic datasets typically lose performance on standard VQA benchmarks?

- **Concept: Keypoint-based Robot Control**
  - Why needed here: The entire control mechanism hinges on predicting 2D keypoints that get converted to 3D. You need to understand coordinate transformations and calibration.
  - Quick check question: Given a 2D pixel coordinate (u,v) from an RGBD camera, what additional information do you need to compute the corresponding 3D world coordinate?

- **Concept: Chain-of-Thought Prompting**
  - Why needed here: VeBrain's training data embeds reasoning processes. Understanding how CoT affects model behavior helps predict where the approach will generalize or fail.
  - Quick check question: How does embedding a thinking process before the answer differ from direct answer prediction in terms of training signal and inference behavior?

## Architecture Onboarding

- **Component map:**
Input Image/Video → ViT Encoder (frozen) → Projector (frozen) → LLM (fine-tuned) → Text Output: keypoints + skill → Robotic Adapter: Point Tracker → Depth Camera → 2D→3D Transform → Skill Executor → Dynamic Takeover

- **Critical path:**
1. Verify calibration between RGBD camera and robot base frame (rotation matrix + translation vector)
2. Confirm policy pool contains skills matching your task vocabulary (17 policies listed in Table 9)
3. Validate point tracker latency <67ms for 15Hz operation on your edge hardware
4. Test MLLM inference achieves ~0.5Hz on your deployment GPU (A100 used in paper)

- **Design tradeoffs:**
- Frozen vs. full fine-tuning: LoRA achieves 76.9% avg vs. 78.0% for full fine-tuning (1% drop acceptable if compute limited)
- Data proportion: Default [200k multimodal, 312k spatial, 88k control] balances capabilities; increasing control data to 200k improves robot tasks (+15% Complex Find) but costs ~3% on MMVet
- Learning rate: 5e-6 is optimal; 1e-5 causes catastrophic forgetting on multimodal benchmarks (MMVet drops to 43.5%)

- **Failure signatures:**
- Keypoint jitter in egocentric view during locomotion → LocoTrack losing temporal consistency
- MLLM outputs invalid skill names → Vocabulary mismatch between training and policy pool
- Robot executes correct motion but misses target → Calibration drift or depth sensor noise
- Task abandonment mid-execution → Dynamic takeover triggering too aggressively

- **First 3 experiments:**
1. **Ablation validation:** Replicate Table 1 on your hardware. Start with baseline Qwen2.5-VL + adapter only (no fine-tuning), then add control data, then spatial data, then multimodal data. Confirm each adds the claimed ~10-20% improvement on respective task categories.
2. **Calibration stress test:** Place calibration target at 5 distances (0.5m to 3m) and 8 angles. Measure keypoint prediction accuracy in world coordinates. Target: <5cm error at all positions.
3. **Single-task transfer:** Train on VeBrain-600k minus one locomotion task (e.g., exclude "Complex Find"). Test generalization to held-out task. If success rate <40%, the dataset lacks compositional diversity for your specific task type.

## Open Questions the Paper Calls Out

### Open Question 1
Can VeBrain generalize to novel robotic skills beyond its fixed policy pool of 17 predefined actions? The framework depends on pre-existing low-level policies, limiting deployment to platforms or tasks without prior skill training.

### Open Question 2
How does reformulating 3D robotic control into 2D keypoint detection affect performance on tasks requiring precise depth or metric spatial reasoning? The 2D formulation may lose information compared to native 3D representations.

### Open Question 3
What are the failure modes of VeBrain under dynamic environmental changes or visual occlusion during long-horizon tasks? Evaluation is conducted in controlled scenes; real-world deployment robustness remains unquantified.

## Limitations
- VeBrain-600k dataset enabling the claimed capabilities is not publicly available, creating a fundamental reproducibility barrier
- Robotic adapter's performance depends heavily on hardware-specific factors like camera calibration precision and policy pool quality that are underspecified
- The approach excels at tasks matching the 17 pre-trained skills, but generalization to novel tasks outside this vocabulary remains untested

## Confidence
- **High Confidence**: Reformulating control as 2D keypoint detection + skill recognition (supported by ablation studies showing degradation when removing control data)
- **Medium Confidence**: Robotic adapter's effectiveness in bridging MLLM predictions to real-world execution (hardware-specific calibration and latency requirements not fully specified)
- **Low Confidence**: Chain-of-thought integration enabling compositional task handling (limited evidence shows how well CoT-generated data transfers to the 7B model or handles long-horizon tasks)

## Next Checks
1. **Dataset Replication Challenge**: Attempt to recreate VeBrain-600k by collecting 600k instruction samples across the three capability categories using the described semi-automated approach. Train a baseline model and measure whether the claimed +5.6% MMVet improvement and +50% robot task gains are reproducible with approximated data.

2. **Calibration Sensitivity Analysis**: Systematically vary camera calibration parameters (±5% in rotation/translation) and measure keypoint-to-3D conversion accuracy across different distances and angles. Determine the tolerance threshold where robot control performance degrades by >10% on any task.

3. **Task Composition Generalization Test**: Hold out entire skill categories during training (e.g., exclude all navigation-related skills). After training, test on composite tasks requiring chained skills from the held-out category. Measure success rate and identify whether the model can compose known skills into novel sequences or fails completely.