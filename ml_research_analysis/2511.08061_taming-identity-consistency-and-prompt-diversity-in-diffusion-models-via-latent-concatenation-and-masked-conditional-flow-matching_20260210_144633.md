---
ver: rpa2
title: Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent
  Concatenation and Masked Conditional Flow Matching
arxiv_id: '2511.08061'
source_url: https://arxiv.org/abs/2511.08061
tags:
- identity
- image
- generation
- prompt
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a diffusion model framework for subject-driven
  image generation that preserves identity consistency while enabling diverse prompt-driven
  variations. The core innovation is a latent concatenation strategy that combines
  noisy target and noise-free reference latents, coupled with a masked Conditional
  Flow Matching (CFM) objective and LoRA fine-tuning.
---

# Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching

## Quick Facts
- arXiv ID: 2511.08061
- Source URL: https://arxiv.org/abs/2511.08061
- Reference count: 3
- This paper introduces a diffusion model framework that preserves identity consistency while enabling diverse prompt-driven variations through latent concatenation and masked Conditional Flow Matching

## Executive Summary
This paper addresses the challenge of subject-driven image generation by introducing a novel framework that maintains identity consistency while enabling diverse prompt-driven variations. The core innovation combines latent concatenation of noisy target and noise-free reference latents with a masked Conditional Flow Matching objective, all implemented through parameter-efficient LoRA fine-tuning. A two-stage Distilled Data Curation Framework generates high-quality training data at scale, and the CHARIS metric comprehensively evaluates identity preservation across five dimensions.

## Method Summary
The method concatenates noisy target latents with noise-free reference latents along the width dimension, processing them through a DiT transformer with RoPE positional encodings. A spatial mask restricts gradient propagation to target positions only, preventing copy-paste artifacts. The model is fine-tuned using LoRA adapters on attention and FFN layers (r=512) with a masked Conditional Flow Matching objective. Training uses a two-stage approach: initial refinement on 10K consistent image grids, followed by main training on 220K identity-consistent image pairs generated through regional prompting and VLM filtering.

## Key Results
- Achieves CHARIS score of 0.750 on zero-shot benchmarks, outperforming recent baselines (UNO: 0.683, DSD: 0.670)
- Maintains superior identity preservation across complex poses and transformations
- Ablation studies confirm spatial masking and optimal LoRA configuration are critical for performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Concatenating noisy target latents with noise-free reference latents enables identity transfer without architectural modifications.
- **Mechanism**: By stacking z_ref and z_tgt along the width dimension in the same token sequence, the transformer's self-attention naturally learns to extract identity features from the reference region while denoising the target. RoPE positional encodings preserve spatial relationships across both regions, allowing the model to distinguish target positions (j < W) from reference positions (j ≥ W).
- **Core assumption**: The pretrained diffusion transformer has sufficient capacity in its attention mechanisms to learn cross-region identity transfer without explicit cross-attention modules.
- **Evidence anchors**:
  - [abstract] "latent concatenation strategy, which jointly processes reference and target images... enables robust identity preservation without architectural modifications"
  - [Method] "concatenating target and reference latents produces z ∈ R^{4×8×d}, where indices j=0 to 3 correspond to the target region, and indices j=4 to 7 correspond to the reference region"
  - [corpus] CatV2TON uses similar temporal concatenation for video try-on, suggesting concatenation strategies transfer across conditioning tasks
- **Break condition**: If reference and target require different positional encoding schemes (e.g., when aspect ratios differ significantly), RoPE may fail to establish useful spatial correspondences.

### Mechanism 2
- **Claim**: Spatial loss masking prevents copy-paste artifacts by forcing the reference region to serve purely as conditioning rather than reconstruction target.
- **Mechanism**: The binary mask μ activates gradients only on target positions (j < W). During CFM training, the model minimizes ‖μ·(z_0 - z_1 - v_θ(z_t))‖², learning to predict the velocity field for the target while the reference remains a fixed context. This asymmetric objective prevents the model from learning to reconstruct the reference.
- **Core assumption**: Preventing gradient flow through the reference region encourages the model to treat it as a conditioning signal rather than a second generation target.
- **Evidence anchors**:
  - [Method] "This masking strategy confines gradient propagation to parameters directly influencing the transformation of the target region, while the reference region serves as a fixed identity anchor"
  - [Ablation Studies] "masking significantly improves identity preservation, prompt adherence, and generation diversity... full loss introduces identity leakage, reduces diversity, and often results in a copy-paste effect"
  - [corpus] Noise Consistency Regularization paper addresses similar under/overfitting trade-offs in subject-driven synthesis
- **Break condition**: If the mask boundary falls within semantically important regions (e.g., splitting a face), edge artifacts may emerge.

### Mechanism 3
- **Claim**: LoRA fine-tuning on attention + FFN layers (but not modulation layers) optimally balances identity expressivity and generalization.
- **Mechanism**: LoRA adapters on attention layers enable identity-aware cross-region attention patterns, while FFN adapters provide compositional reasoning for attribute control. Excluding modulation layers prevents overfitting to specific reference statistics that would reduce prompt diversity.
- **Core assumption**: FFN layers in diffusion transformers encode compositional/semantic transformations that benefit from adaptation, while modulation layers encode normalization statistics that should remain frozen.
- **Evidence anchors**:
  - [Ablation Studies] "attention+FFN configuration yields the best overall performance... Adding FFNs improves compositional reasoning and attribute control over attention-only tuning, while further including modulation layers leads to overfitting"
  - [Table 2] Config B (attention+FFN) achieves CHARIS 0.743 vs 0.725 (attention-only) and 0.735 (attention+FFN+modulation)
  - [corpus] Causal-Adapter demonstrates modular adaptation of frozen backbones, supporting parameter-efficient fine-tuning paradigms
- **Break condition**: If the reference subject type is out-of-distribution from the curated training data (e.g., unusual art styles), LoRA capacity (r=512) may be insufficient.

## Foundational Learning

- **Concept: Conditional Flow Matching (CFM)**
  - **Why needed here**: The paper uses CFM as its training objective instead of standard diffusion loss. Understanding flow matching helps explain why the velocity prediction v_θ works for identity transfer.
  - **Quick check question**: Can you explain why CFM learns a velocity field from noise to data, and how this differs from predicting noise directly?

- **Concept: Rotary Position Embeddings (RoPE)**
  - **Why needed here**: The latent concatenation relies on RoPE to encode 2D spatial positions across the combined reference-target grid. Without understanding RoPE's relative positioning properties, the spatial mask and cross-region attention mechanism are opaque.
  - **Quick check question**: Given two tokens at positions (2,3) and (2,7) in a concatenated grid, how does RoPE encode their relative positional relationship?

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here**: The paper uses LoRA with r=512 for fine-tuning, and the ablation shows that layer selection matters critically. Understanding LoRA's rank-constrained updates explains why certain layers benefit from adaptation.
  - **Quick check question**: Why would adapting FFN layers improve "compositional reasoning" while adapting modulation layers causes overfitting?

## Architecture Onboarding

- **Component map**: Input: z_noisy [H×W×d] | z_ref [H×W×d] → concat → [H×2W×d] → RoPE on flattened tokens + text embeddings → DiT Transformer with LoRA (attn + FFN layers) → Velocity prediction v_θ [H×2W×d] → Spatial mask μ applied to loss

- **Critical path**: The latent concatenation at input and spatial mask at loss computation are tightly coupled. The mask must align with the concatenation axis (width dimension) or the model will receive contradictory supervision.

- **Design tradeoffs**:
  - **Masked vs. full loss**: Masked loss sacrifices reconstruction fidelity for better identity transfer and diversity (0.750 vs 0.652 CHARIS in ablation)
  - **LoRA rank (r=512)**: Higher than typical (r=16-64), suggesting identity preservation requires more capacity than style/content adaptation
  - **Training data scale (220K pairs)**: Larger than many personalization datasets, enabled by automated curation pipeline

- **Failure signatures**:
  - Copy-paste artifacts (reference reproduced in target region) → indicates full loss used instead of masked
  - Identity drift under pose changes → suggests LoRA not applied to FFN layers
  - Overfitting to training subjects → may indicate modulation layers included or LoRA rank too high for data diversity

- **First 3 experiments**:
  1. **Validate masking effect**: Train identical configs with masked vs. full loss on a held-out subject. Measure CHARIS-CP (identity) and CHARIS-PF (prompt adherence) gap. Expected: masked loss shows 10-15% higher CP·PF product.
  2. **LoRA layer ablation**: Compare attention-only vs. attention+FFN on compositional prompts (e.g., "the character riding a bicycle in rain"). Expected: FFN-included version better preserves subject while adapting to complex scene elements.
  3. **Concatenation axis sanity check**: Verify RoPE encodes positions correctly by visualizing attention patterns from reference region to target region. Expected: strong cross-region attention at semantically aligned positions (e.g., face-to-face).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the latent concatenation strategy be effectively extended to multi-subject conditioning, specifically resolving identity ambiguity when subjects belong to the same semantic category? The current framework focuses on single-subject preservation; it is unclear if the concatenation mechanism can isolate multiple distinct identity features within a single latent grid without cross-contamination. Evidence would be successful generation of images containing multiple distinct subjects from separate reference inputs, validated by high identity consistency scores for each subject via the CHARIS metric.

- **Open Question 2**: How can the framework's robustness be improved to handle partial or occluded reference images while maintaining identity fidelity? The current masked CFM objective relies on a reference latent which presumably assumes visible identity features; partial inputs may provide insufficient anchors for the flow matching process. Evidence would be quantitative benchmarks (CHARIS scores) demonstrating stable performance when reference images are progressively masked or occluded, compared against the current full-reference baseline.

- **Open Question 3**: Can the latent concatenation and masked CFM approach generalize to alternative control modalities such as structural sketches, edge maps, or pose keypoints? The current architecture is tuned for pixel-space identity transfer; it is uncertain if the transformer attention mechanisms can interpret structural guides (e.g., canny edges) through the concatenation channel without conflicting with identity features. Evidence would be implementation results showing the model successfully adhering to structural constraints from a concatenated control map while preserving identity from the reference latent.

## Limitations
- CHARIS evaluation thresholds and VLM filtering criteria are unspecified, requiring access to supplementary material
- Regional prompting templates for data curation are not provided, leaving ambiguity in character sheet generation
- LoRA layer targeting in FLUX.1-Dev transformer is unclear (exact module names/indices for attention/FFN layers)

## Confidence
- **High confidence**: Masked CFM objective prevents copy-paste artifacts (supported by ablation showing 0.750 vs 0.652 CHARIS)
- **Medium confidence**: LoRA on attention+FFN layers is optimal (Ablation Table 2 shows Config B superiority, but exact layer mappings uncertain)
- **Medium confidence**: Latent concatenation enables identity transfer without architectural changes (mechanism plausible, but cross-attention alternatives not directly compared)

## Next Checks
1. **Validate spatial mask alignment** - Verify that the binary mask μ correctly aligns with the concatenation axis by visualizing gradient flow during training. Misalignment would cause contradictory supervision signals.
2. **Test LoRA layer coverage** - Compare performance of attention-only vs. attention+FFN configurations on compositional prompts to confirm FFN adaptation improves attribute control as claimed.
3. **Confirm RoPE encoding** - Visualize attention patterns from reference to target regions to ensure RoPE positional encodings establish correct spatial relationships across concatenated latents.