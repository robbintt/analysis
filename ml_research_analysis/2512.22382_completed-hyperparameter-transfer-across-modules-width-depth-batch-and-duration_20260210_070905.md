---
ver: rpa2
title: Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration
arxiv_id: '2512.22382'
source_url: https://arxiv.org/abs/2512.22382
tags:
- learning
- transfer
- scaling
- per-module
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates that per-module hyperparameters can be optimised
  at a small model scale and then transferred to larger models across multiple scaling
  dimensions. The authors introduce Complete(d)P, an extension of CompleteP that supports
  transfer across width, depth, batch size, and training duration, and apply it to
  optimise hyperparameters such as learning rates, AdamW parameters, weight decay,
  and initialisation scales on a per-module basis.
---

# Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration

## Quick Facts
- arXiv ID: 2512.22382
- Source URL: https://arxiv.org/abs/2512.22382
- Reference count: 40
- Primary result: Demonstrates successful transfer of per-module hyperparameters from small to 600× larger models across width, depth, batch size, and training duration

## Executive Summary
This paper introduces Complete(d)P, an extension of the CompleteP parameterization that enables successful hyperparameter transfer across four scaling dimensions: width, depth, batch size, and training duration. The authors show that optimizing hyperparameters on a per-module basis at a small scale (50M parameters) and transferring them to much larger models yields significant compute savings while maintaining or improving performance. The key innovation is a trust-region random search method that can effectively navigate the high-dimensional, cliff-structured loss landscape of per-module hyperparameters. Applied to decoder-only transformers trained on RedPajama, this approach achieves a 1.32× speedup when transferred to models 600× larger in compute.

## Method Summary
The method extends CompleteP parameterization to support transfer across width, depth, batch size, and training duration. It uses depth-Kronecker factorization to reduce the search space dimensionality and employs trust-region random search to optimize per-module hyperparameters (learning rates, AdamW parameters, weight decay, initialization scales) at a small scale proxy model. The optimized base hyperparameters are then scaled using SDE-derived rules for batch size and training duration before being transferred to the target model. The approach requires implementing Complete(d)P scaling rules, applying SDE batch-size and token-horizon scaling, using depth-Kronecker factorization for per-module HP search, and validating transfer across scaling dimensions.

## Key Results
- Achieved 1.32× speedup when transferring hyperparameters from a 50M parameter model to a model 600× larger in compute
- Demonstrated successful transfer across width, depth, batch size, and training duration using Complete(d)P parameterization
- Showed that per-module hyperparameter optimization yields consistent performance improvements over global tuning
- Established that trust-region random search effectively navigates the high-dimensional, cliff-structured loss landscape of per-module hyperparameters

## Why This Works (Mechanism)
The method works by parameterizing hyperparameters to align with the infinite-width/depth limit, allowing consistent behavior across scales. Per-module optimization captures local variations in the loss landscape that global tuning misses, while the trust-region approach avoids the sharp cliffs that cause divergence. The SDE scaling rules maintain the signal-to-noise ratio in gradient estimates when changing batch size or duration. Depth-Kronecker factorization makes high-dimensional search tractable without significant performance loss. The combination of these elements enables effective transfer from small, cheap-to-search models to large target models without further tuning.

## Foundational Learning
- **Infinite-Width/Depth Limits (Tensor Programs/μP):**
  - Why needed here: The paper's core contribution is a parameterization that aligns models of different sizes to the same infinite limit, which is the theoretical basis for hyperparameter transfer
  - Quick check question: If I double the width of a model, how should I adjust the output logit multiplier to keep its magnitude constant?

- **Training as an SDE:**
  - Why needed here: The derivation of batch-size and token-horizon scaling rules relies on framing optimizer dynamics as a discretization of a continuous stochastic process
  - Quick check question: If I increase the batch size by a factor of 4, what should happen to the learning rate to maintain the same signal-to-noise ratio in the gradient estimate?

- **Per-Module Optimization in High Dimensions:**
  - Why needed here: The paper tackles the high-dimensional search space of per-module hyperparameters. Understanding why standard methods fail (e.g., random search) and why the proposed trust-region method works is key
  - Quick check question: Why is the "cliff" structure of the loss landscape problematic for a standard Gaussian Process-based Bayesian Optimizer?

## Architecture Onboarding

- **Component map:**
  Complete(d)P Parameterization -> Per-Module Hyperparameter Manager -> SDE Scaling Rules -> Transfer to Target

- **Critical path:**
  1. Implement Parameterization: First, implement the Complete(d)P scaling rules from Table 1 for width and depth. This must be done correctly, as errors (like in embedding Adam ε) break transfer. Verify with "coordinate checks" (Yang et al., 2022)
  2. Tune at Small Scale: Use the trust-region random search (or similar local method) to find optimal base multipliers (ζ^type, ζ^depth) for all module types on a cheap proxy model (e.g., 50M parameters)
  3. Apply SDE Rules: For any change in batch size or token count at the target scale, calculate and apply the SDE-derived scaling factors (Eq. 2) to the base hyperparameters
  4. Transfer to Target: Combine the base multipliers, Complete(d)P scaling rules, and SDE adjustments to get the final hyperparameters for the large model. No further tuning should be required

- **Design tradeoffs:**
  - Kronecker Factorization: Reduces the number of search parameters from O(|M|L) to O(|M|+L), making the search feasible. The tradeoff is a potential loss of optimality, though the paper finds this loss to be minimal (Section 3.2)
  - Base Model Size: A smaller base model is cheaper to search but may yield slightly suboptimal hyperparameters at the target scale due to being further from the infinite limit. The paper finds a 50M model is a good compromise (Figure 12)
  - Search Algorithm: Trust-region random search is more robust to the "cliff" landscape but may require more trials than a well-tuned Bayesian Optimizer to converge. The authors chose it for its simplicity and robustness

- **Failure signatures:**
  - Training Divergence (NaNs): Often caused by incorrect per-module learning rates. The loss landscape has sharp instability cliffs. This is mitigated by the trust-region search
  - Suboptimal Transfer: If hyperparameters optimized at a very small scale perform poorly at a larger scale, it indicates a breakdown of the infinite-width/depth approximation or an error in the parameterization
  - Poor Search Convergence: If random search or standard Bayesian optimization fails to improve beyond a baseline, it's likely struggling with the high-dimensional, "cliff"-filled landscape

- **First 3 experiments:**
  1. Sanity Check: Re-run the width/depth transfer experiments from Figure 2 on a small model to verify your Complete(d)P implementation is correct. The optimal learning rate should be consistent across scales
  2. Ablation on Factorization: Implement the per-module search with and without the depth-Kronecker factorization (as in Section 3.2) on a small model to confirm that the factorization doesn't significantly hurt performance while reducing search dimensions
  3. Transfer Validation: Take the optimal hyperparameters found in experiment 2 and transfer them to a moderately larger model (e.g., 4x width/depth) to verify the performance gain persists, as shown in Figure 1 (Right)

## Open Questions the Paper Calls Out
- **Open Question 1:** At what model scale should hyperparameters be optimised before transfer to achieve maximum compute savings?
- **Open Question 2:** Does the per-module hyperparameter advantage persist asymptotically at infinite scale, or does it diminish due to fundamental properties of the limit?
- **Open Question 3:** How should learning rate schedules adapt to different token horizons, and can optimal schedule shapes be predicted without exhaustive search?
- **Open Question 4:** Do the Complete(d)P transfer rules generalise to other architectures and training paradigms beyond decoder-only transformers on language modelling?

## Limitations
- The theoretical foundation relies on the infinite-width/depth limit, which may not hold perfectly for practical model sizes
- Trust-region random search may not find globally optimal hyperparameters and could require more trials than adaptive methods
- Per-module approach adds significant complexity compared to global hyperparameter tuning
- SDE scaling rules introduce additional approximations that may degrade with extreme scale changes

## Confidence
**High Confidence**: The demonstration that Complete(d)P parameters can be successfully transferred across width and depth scales, showing consistent performance improvements when transferred to larger models

**Medium Confidence**: The extension to batch size and training duration transfer via SDE scaling rules, as these introduce additional approximations that may degrade with extreme scale changes

**Medium Confidence**: The claim that per-module optimization yields significant improvements over global tuning, as this depends heavily on the specific architecture and may vary across different model families

## Next Checks
1. **Scale Gap Validation**: Systematically test transfer performance across multiple scale gaps (e.g., 10×, 100×, 1000×) to quantify the degradation of Complete(d)P transfer as the base model moves further from the infinite limit

2. **Alternative Search Algorithm Comparison**: Compare the trust-region random search against well-tuned Bayesian optimization methods (e.g., TuRBO) to quantify the trade-off between robustness and convergence speed in the high-dimensional per-module landscape

3. **Architecture Generalization Test**: Apply the Complete(d)P framework to a different architecture family (e.g., decoder-encoder transformers or architectures with different normalization schemes) to assess the generalizability of the per-module optimization approach