---
ver: rpa2
title: 'FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction'
arxiv_id: '2510.20926'
source_url: https://arxiv.org/abs/2510.20926
tags:
- similarity
- tags
- story
- text
- literary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FICSIM, a dataset designed to evaluate semantic
  textual similarity (STS) in long-form fiction. The dataset contains 90 stories with
  over 33,790 pairwise comparisons across 12 axes of similarity, including plot, theme,
  character states, and fanfiction-specific tags.
---

# FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction

## Quick Facts
- arXiv ID: 2510.20926
- Source URL: https://arxiv.org/abs/2510.20926
- Reference count: 21
- Dataset for evaluating semantic similarity in long-form fiction with 90 stories and 33,790 pairwise comparisons across 12 axes

## Executive Summary
This paper introduces FICSIM, a dataset designed to evaluate semantic textual similarity (STS) in long-form fiction. The dataset contains 90 stories with over 33,790 pairwise comparisons across 12 axes of similarity, including plot, theme, character states, and fanfiction-specific tags. The authors collected these stories from Archive of Our Own (AO3) with author consent, focusing on recently published works to avoid data contamination issues common in public-domain literature. Similarity scores are derived from author-generated tags and validated by digital humanities scholars. When evaluating various embedding models on FICSIM, the authors found that all models struggled to capture fine-grained semantic similarity, instead over-indexing on surface-level features like authorial style and fandom. This highlights a significant gap between current embedding capabilities and the needs of computational literary studies. The dataset and evaluation methodology aim to encourage more focus on narrowing this gap.

## Method Summary
The authors created FICSIM by collecting 90 stories from Archive of Our Own (AO3) with author consent, focusing on recently published works to avoid contamination from public-domain literature. They established 12 axes of similarity including plot, theme, character states, and fanfiction-specific tags. Similarity scores were generated from author-provided tags and validated by digital humanities scholars. The dataset comprises over 33,790 pairwise comparisons across these stories. The authors evaluated multiple embedding models including sentence transformers, BERT variants, and semantic search tools on this dataset to assess their ability to capture fine-grained semantic similarity in long-form fiction.

## Key Results
- Current embedding models struggle to capture fine-grained semantic similarity in long-form fiction
- Models over-index on surface-level features like authorial style and fandom rather than semantic content
- All evaluated models showed poor performance on the FICSIM dataset across multiple similarity axes

## Why This Works (Mechanism)
The FICSIM dataset works by providing a structured evaluation framework that captures multiple dimensions of semantic similarity in long-form fiction. By using author-generated tags as ground truth and validating them through expert review, the dataset establishes meaningful similarity judgments that go beyond simple lexical overlap. The focus on recently published works avoids the contamination issues that plague datasets built on public-domain literature, ensuring that the semantic relationships tested reflect contemporary writing styles and themes.

## Foundational Learning
1. Semantic Textual Similarity (STS) in long-form fiction requires understanding complex narrative elements beyond surface features
   - Why needed: Fiction contains layered meanings that simple embeddings struggle to capture
   - Quick check: Compare model performance on plot vs. style similarity tasks

2. Author-generated tags as ground truth for semantic similarity
   - Why needed: Authors provide intentional semantic signals about their work
   - Quick check: Validate tag-based similarity against expert human judgments

3. Multiple similarity axes for comprehensive evaluation
   - Why needed: Different aspects of similarity (plot, theme, characters) require different modeling approaches
   - Quick check: Analyze model performance breakdown across individual similarity axes

## Architecture Onboarding
**Component Map:** AO3 stories -> Author tags -> Pairwise comparisons (33,790) -> 12 similarity axes -> Model evaluation

**Critical Path:** Story collection and consent -> Tag extraction and validation -> Pairwise similarity computation -> Model testing and analysis

**Design Tradeoffs:** 
- Using author tags as ground truth provides authentic semantic signals but may introduce bias
- Focusing on recently published works avoids contamination but limits dataset size
- Including fanfiction-specific tags makes the dataset specialized but less generalizable

**Failure Signatures:** 
- Models performing well on lexical similarity but poorly on thematic similarity
- Over-indexing on authorial style features rather than semantic content
- Poor performance on character state similarity despite good performance on plot similarity

**3 First Experiments:**
1. Evaluate a simple TF-IDF baseline on FICSIM to establish a lexical baseline
2. Test model performance on individual similarity axes to identify which are most challenging
3. Compare model performance on fanfiction vs. general fiction samples if available

## Open Questions the Paper Calls Out
None

## Limitations
- Relatively small dataset size (90 stories) limits statistical power and generalizability
- Focus on single fanfiction platform (AO3) may not represent broader literary domains
- Reliance on author-generated tags introduces potential bias in similarity judgments

## Confidence
- High Confidence: Current embedding models struggle with fine-grained semantic similarity in long-form fiction
- Medium Confidence: Dataset design and construction methodology is robust but may have limited generalizability
- Medium Confidence: Models over-index on surface features is supported but needs more granular analysis

## Next Checks
1. Test the dataset with additional embedding architectures, including newer models not evaluated in the paper, to establish whether the observed performance limitations are model-agnostic.
2. Conduct a systematic analysis of model failures on individual similarity axes to identify which types of semantic relationships are most challenging for current approaches.
3. Expand the dataset to include stories from additional literary domains and platforms to assess the robustness of the findings across different contexts.