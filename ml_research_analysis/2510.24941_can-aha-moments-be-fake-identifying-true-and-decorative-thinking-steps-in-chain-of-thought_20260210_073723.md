---
ver: rpa2
title: Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in
  Chain-of-Thought
arxiv_id: '2510.24941'
source_url: https://arxiv.org/abs/2510.24941
tags:
- steps
- reasoning
- step
- arxiv
- steering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the True Thinking Score (TTS) to evaluate
  the causal contribution of individual steps in large language models' chain-of-thought
  (CoT) reasoning. The authors find that reasoning steps in CoT can be classified
  as either true-thinking steps that genuinely drive the model's prediction or decorative-thinking
  steps that merely give the appearance of reasoning without causal impact.
---

# Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought

## Quick Facts
- arXiv ID: 2510.24941
- Source URL: https://arxiv.org/abs/2510.24941
- Reference count: 40
- Primary result: Introduces True Thinking Score to distinguish genuine reasoning steps from decorative ones in CoT

## Executive Summary
This paper challenges the assumption that chain-of-thought reasoning in large language models reflects genuine internal reasoning processes. The authors introduce the True Thinking Score (TTS) framework to evaluate whether individual reasoning steps actually contribute causally to model predictions or merely appear to reason without impact. Through systematic perturbations of CoT steps and mathematical reasoning tasks, they reveal that only a small fraction of steps are truly causal, while many are "decorative" - giving the appearance of reasoning without driving the final answer. They further develop a latent steering direction that can control whether models genuinely engage with or disregard specific reasoning steps, demonstrating this by forcing models to truly follow self-verification steps that would otherwise be decorative.

## Method Summary
The authors develop a True Thinking Score framework that evaluates the causal contribution of individual CoT steps through perturbation-based analysis. For each step, they create counterfactual inputs where the step is modified or removed, then measure the change in model output using KL divergence. This provides a quantitative score indicating how much each step actually influences the final prediction. They complement this with a steering direction approach that learns latent directions in the model's activation space to control whether the model follows or ignores specific reasoning steps. The method is evaluated across multiple mathematical reasoning tasks and model architectures (GPT-3.5, LLaMA, DeepSeek), revealing systematic patterns of decorative thinking steps that appear in generated reasoning but lack causal impact on outputs.

## Key Results
- Only 2.3% of CoT steps achieve TTS≥0.7, indicating most steps have minimal causal contribution
- Mathematical reasoning steps show high rates of decorative thinking (e.g., 64.4% for calculation steps in DeepSeek)
- The steering direction method successfully forces models to genuinely engage with previously decorative self-verification steps
- Decorative steps are prevalent across different model architectures and mathematical reasoning tasks

## Why This Works (Mechanism)

## Foundational Learning
1. **Causal attribution in LLMs** - Why needed: To determine which reasoning steps actually drive predictions rather than just correlate with them; Quick check: Perturbation experiments showing output changes when steps are modified
2. **Latent steering directions** - Why needed: To control model behavior by manipulating activation space rather than prompt engineering; Quick check: Consistent behavioral changes when applying learned directions
3. **Chain-of-thought interpretability** - Why needed: To understand whether CoT reflects genuine reasoning or superficial pattern matching; Quick check: Comparison of TTS scores across different step types

## Architecture Onboarding
- **Component map**: Input → CoT generation → Step-by-step reasoning → Output prediction; TTS evaluation ← Step perturbations → Causal attribution
- **Critical path**: Reasoning step generation → TTS scoring → Steering direction learning → Behavioral control
- **Design tradeoffs**: Computational cost of perturbation analysis vs. interpretability gains; choice between discrete step modification vs. continuous activation steering
- **Failure signatures**: High TTS variance across similar problems suggests inconsistent reasoning; uniform low TTS across all steps indicates decorative reasoning throughout
- **First experiments**: 1) Apply TTS to a simple arithmetic CoT; 2) Test steering direction on a single decorative step; 3) Compare TTS distributions across different reasoning domains

## Open Questions the Paper Calls Out
None

## Limitations
- TTS framework assumes perturbation-based counterfactuals provide valid causal measurement
- Threshold-based classification of decorative vs. true-thinking steps needs more validation
- Steering direction mechanism's opacity raises questions about whether it enables genuine reasoning or learned associations
- Evaluation focuses on mathematical reasoning, leaving domain generalization uncertain

## Confidence
- High confidence: CoT steps vary in causal contribution to outputs
- Medium confidence: Only small fraction of steps are "true-thinking" (2.3% with TTS≥0.7)
- Medium confidence: Steering direction reliably controls engagement with specific steps

## Next Checks
1. Apply TTS scoring to non-mathematical reasoning tasks (commonsense QA, multi-hop reasoning) to test domain generalization
2. Compare TTS results using different perturbation strategies (deletion, modification, context masking) to assess robustness
3. Conduct human evaluation validation to determine if steps identified as decorative by TTS are actually non-essential for human problem-solving