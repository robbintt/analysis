---
ver: rpa2
title: Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection
arxiv_id: '2503.09153'
source_url: https://arxiv.org/abs/2503.09153
tags:
- news
- reasoning
- fake
- negative
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explores whether LLM hallucinations can be leveraged\
  \ for fake news detection by generating both positive and negative reasoning for\
  \ news articles. A Self-Reinforced Reasoning Rectification approach (SR\xB3) is\
  \ proposed to iteratively refine reasoning using LLM reflection for semantic consistency."
---

# Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection

## Quick Facts
- arXiv ID: 2503.09153
- Source URL: https://arxiv.org/abs/2503.09153
- Reference count: 12
- Primary result: NRFE-D achieves 0.971 accuracy and 0.970 MacF1 on Twitter-16, outperforming nine baselines

## Executive Summary
This paper investigates whether LLM hallucinations can be leveraged for fake news detection by generating both positive and negative reasoning for news articles. The authors propose a Self-Reinforced Reasoning Rectification approach (SR³) that iteratively refines reasoning using LLM reflection for semantic consistency. This is combined with a Negative Reasoning-based Fake News Detection model (NRFE) that learns semantic consistency between news and reasoning pairs. A student model (NRFE-D) distills knowledge from NRFE while avoiding label-implicated reasoning. Experiments on three datasets show NRFE-D achieves state-of-the-art performance.

## Method Summary
The method consists of three main components: (1) SR³ generates positive and negative reasoning through iterative LLM prompting with polarity and confidence constraints, (2) NRFE learns semantic consistency between news and reasoning using cross-attention and contrastive losses, and (3) NRFE-D distills the knowledge into a news-only student model via reverse KL divergence. The approach transforms hallucination from a liability into an asset by creating structured contrast signals for fake news detection.

## Key Results
- NRFE-D achieves 0.971 accuracy and 0.970 MacF1 on Twitter-16, outperforming nine baselines
- Ablation studies show semantic consistency losses are critical for performance
- Knowledge distillation successfully transfers reasoning-aware patterns to news-only inference
- SR³ successfully generates reasoning that impacts credibility scores within 5-10 iterations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM hallucinations can be systematically redirected to generate "negative reasoning"—incorrect or misleading interpretations—that serve as adversarial training signals for fake news detection.
- **Mechanism:** Supervised prompts instruct the LLM to generate reasoning that either reinforces (positive) or contradicts/degrades confidence (negative) in news credibility. The negative reasoning exploits hallucination tendencies to create semantically related but polarity-reversed interpretations.
- **Core assumption:** Hallucination-induced negative reasoning maintains sufficient semantic connection to news content to provide a meaningful contrast signal, rather than being random noise.
- **Evidence anchors:**
  - [abstract]: "it has never been investigated whether the LLMs' hallucination is possibly usable to generate negative reasoning for facilitating the detection of fake news"
  - [section]: "negative reasoning is expected to decrease the confidence level and even alter the polarity of the news"
  - [corpus]: Related papers (FactGuard, ZoFia) use LLMs for verification but aim to reduce hallucinations rather than exploit them; no direct corpus support for this specific mechanism.
- **Break condition:** If hallucination-generated negative reasoning lacks semantic grounding to the original news, the contrastive learning signal degrades to noise.

### Mechanism 2
- **Claim:** Iterative LLM reflection under explicit constraints (polarity threshold M, confidence increment I) can converge on high-quality positive and negative reasoning.
- **Mechanism:** The SR³ algorithm (Algorithm 1) repeatedly prompts the LLM with previous outputs in a while-loop until: (1) credibility score crosses polarity threshold M, and (2) confidence change exceeds I. The prompt includes the previous reasoning and score, enabling self-correction.
- **Core assumption:** LLMs can reliably evaluate the credibility impact of their own generated reasoning when given explicit numerical constraints and prior outputs.
- **Evidence anchors:**
  - [section]: "while (V^p > M) & (V^p - V_initial > I)" with max iterations as safeguard
  - [section]: "SR³ acts as a reasoning reviser by assessing the polarity... and confidence level... to verify if and how the generated reasoning can impact the original news"
  - [corpus]: No direct corpus evidence; related work (e.g., RAG, knowledge graphs) focuses on eliminating hallucinations rather than iteratively steering them.
- **Break condition:** If LLM self-evaluation is inconsistent across iterations, the rectification loop may not converge or may amplify errors before MaxIter terminates it.

### Mechanism 3
- **Claim:** Aligning news-reasoning representations in a shared semantic space via cross-attention and contrastive losses enables the model to discriminate coherent (positive) from incoherent (negative) reasoning.
- **Mechanism:** Cross-attention produces reasoning-aware news (F_{p→x}, F_{n→x}) and news-aware reasoning (F_{x→p}, F_{x→n}). Three cosine embedding losses (L_rc, L_rxc, L_xrc) maximize similarity for positive pairs and minimize for negative pairs with margin d.
- **Core assumption:** Positive reasoning exhibits high semantic similarity to news content, while negative reasoning (from controlled hallucination) exhibits low but structured dissimilarity.
- **Evidence anchors:**
  - [section]: "L_rc can maximize the correlation between f_x and f_p, and minimize the correlation between f_x and f_n"
  - [section]: Ablation shows "NRFE-D w/o RC yields the relatively lowest performance on PolitiFact and Twitter-16"
  - [corpus]: SEER uses "Semantic Enhancement" for multimodal fake news detection but via image-text alignment, not positive/negative reasoning contrast.
- **Break condition:** If negative reasoning is semantically too similar to positive reasoning (weak hallucination contrast), gradient signals from contrastive losses become weak.

## Foundational Learning

- **Concept: Knowledge Distillation (Teacher-Student)**
  - Why needed here: NRFE requires both news and reasoning at training time (labels implicated), but inference should use news only. Distillation transfers learned patterns without the reasoning dependency.
  - Quick check question: Why does the paper use reverse KL divergence (q||p) rather than forward KL (p||q)—what mode-seeking vs. mass-covering behavior does this encourage?

- **Concept: Contrastive Learning with Margin**
  - Why needed here: The semantic consistency losses must pull positive pairs together and push negative pairs apart. The margin d prevents negative pairs from being pushed infinitely far.
  - Quick check question: If margin d is set too low, what happens to the model's ability to distinguish coherent from incoherent reasoning?

- **Concept: Cross-Attention for Cross-Modal Representation**
  - Why needed here: News and reasoning are different sequences that need bidirectional interaction—reasoning should inform news representation and vice versa.
  - Quick check question: In CrossAttention(F_x, F_p), which sequence provides queries and which provides keys/values? How does this affect what F_{p→x} represents?

## Architecture Onboarding

- **Component map:**
  SR³ (external LLM, OLlama 3 70B) -> NRFE Encoder 1 (BERT) -> Cross-Attention -> Semantic Consistency Module -> Fusion + Classification
  SR³ (external LLM, OLlama 3 70B) -> NRFE Encoder 2 (BERT) -> Cross-Attention -> Semantic Consistency Module -> Fusion + Classification
  NRFE Encoder 1 (BERT) -> NRFE-D Attention Layer -> MLP -> f'_final

- **Critical path:**
  SR³ reasoning quality (polarity/confidence constraints satisfied) -> semantic consistency loss convergence -> discriminative representations in NRFE -> successful distillation to NRFE-D. If SR³ produces low-contrast reasoning, downstream losses provide weak gradients.

- **Design tradeoffs:**
  - Supervised vs. unsupervised reasoning generation: Supervised (using labels y in prompts) yields higher-quality reasoning but requires labeled training data; unsupervised is mentioned as future work.
  - MaxIter in SR³: Higher values improve reasoning quality but increase LLM API costs and latency.
  - Margin d in cosine loss: Larger margins enforce stronger separation but may push useful negative examples too far, reducing fine-grained discrimination.

- **Failure signatures:**
  - SR³ loop hits MaxIter without satisfying constraints -> LLM not following instructions; inspect prompt template Ψ and model capability.
  - Large accuracy gap between NRFE and NRFE-D -> Distillation failing; check if L_dis is decreasing and if encoder weights transferred correctly.
  - w/o RC ablation performs similarly to full model -> Negative reasoning not providing useful contrast; verify R_n differs meaningfully from R_p.
  - T-SNE shows overlapping clusters -> Semantic consistency losses not creating discriminative features; increase training epochs or adjust margin d.

- **First 3 experiments:**
  1. **Sanity-check SR³ on 20 samples:** Manually verify that R_p increases credibility and R_n decreases it for both real and fake news. Log iterations until convergence.
  2. **Ablate semantic consistency losses individually:** Train NRFE-D variants (w/o RC, w/o RXC, w/o XRC, only RC) on one dataset; compare accuracy drops to identify the most critical component.
  3. **Measure NRFE vs. NRFE-D gap:** If gap > 5% accuracy, investigate whether L_dis is converging and whether the news encoder initialization from NRFE was correctly loaded.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Self-Reinforced Reasoning Rectification (SR³) approach be effectively adapted to generate negative reasoning in a completely unsupervised manner without ground truth labels?
- Basis in paper: [explicit] The authors state in the conclusion: "In the future, we will further investigate the potentiality of other source-opened LLMs... in generating negative reasoning in an unsupervised manner."
- Why unresolved: The current SR³ method (Algorithm 1) and prompt template (Eq. 1) explicitly require the ground truth label ($y$) and the type of reasoning ($T$) as inputs to generate and refine the reasoning.
- What evidence would resolve it: A modification of the SR³ algorithm that successfully generates polarity-altering reasoning without $y$ as an input, achieving comparable performance to the supervised NRFE-D model.

### Open Question 2
- Question: How can LLM-based multi-agent architectures be integrated into the fake news detection pipeline to enhance the generation or verification of negative reasoning?
- Basis in paper: [explicit] The conclusion explicitly lists this as a future direction: "On the other hand, LLM-based multi-agents can also be transferred in the scenes of fake news detection, which will be one of our future study directions."
- Why unresolved: The current study utilizes a single LLM (OLlama 3 70B) for reasoning generation and reflection; the potential for collaborative or adversarial multi-agent interactions remains unexplored in this context.
- What evidence would resolve it: An implementation of a multi-agent framework (e.g., using agents for generation vs. verification) that demonstrates improved semantic consistency or detection accuracy over the single-agent SR³ baseline.

### Open Question 3
- Question: Is the SR³ framework dependent on the specific capabilities of the OLlama 3 70B model, or does it generalize effectively to smaller or alternative source-opened LLMs?
- Basis in paper: [inferred] While the conclusion mentions testing other LLMs, the methodology relies heavily on OLlama 3 70B. The experimental results show that smaller LLMs (Gemma 2, Mistral) performed poorly as zero-shot baselines (e.g., 0.594 Acc vs 0.958), raising questions about whether the SR³ rectification process requires the capacity of a 70B model to successfully generate and self-correct reasoning.
- Why unresolved: The paper does not provide ablation studies using different LLMs as the backbone for the SR³ reasoning generation process.
- What evidence would resolve it: Experimental results showing successful SR³ execution and subsequent high detection accuracy when using smaller or different LLM architectures (e.g., Mistral, Gemma) as the reasoning generator.

## Limitations

- The core assumption that hallucination-generated negative reasoning provides meaningful semantic contrast signals requires stronger empirical validation through manual annotation of reasoning quality.
- The iterative SR³ process relies heavily on LLM self-evaluation capabilities that may vary across model versions or prompt variations, making the approach potentially brittle.
- The approach requires labeled training data for supervised reasoning generation, limiting its applicability to unsupervised or few-shot scenarios.

## Confidence

- **High confidence:** NRFE-D achieves state-of-the-art accuracy on Twitter-16 (0.971 Acc, 0.970 MacF1) and outperforms baselines
- **Medium confidence:** The semantic consistency learning mechanism effectively discriminates coherent from incoherent reasoning
- **Medium confidence:** Knowledge distillation successfully transfers reasoning-aware patterns to news-only inference
- **Low confidence:** Hallucination-based negative reasoning consistently provides structured semantic contrast across all datasets

## Next Checks

1. **Reasoning Quality Validation**: Manually annotate 100 randomly selected reasoning pairs (Rp, Rn) to verify that negative reasoning actually contains misinformation related to the original news content rather than generic noise or random statements.

2. **Cross-Dataset Generalization**: Test NRFE-D on a held-out domain (e.g., COVID-19 misinformation dataset) to verify that hallucination-generated reasoning patterns transfer beyond the original Twitter and PolitiFact domains.

3. **Ablation of Reasoning Generation Method**: Compare SR³-generated reasoning against simpler heuristics (e.g., prompt-based inversion without iterative refinement) to quantify the actual contribution of the self-reinforced rectification approach.