---
ver: rpa2
title: 'Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX'
arxiv_id: '2508.12485'
source_url: https://arxiv.org/abs/2508.12485
tags:
- eviction
- cache
- cold-rl
- learning
- nginx
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cold-RL replaces NGINX's LRU eviction policy with a learned approach
  using a dueling Deep Q-Network (DQN) served by an ONNX sidecar. The system samples
  the K coldest objects from the LRU tail, extracts six lightweight features (age,
  size, hit count, inter-arrival time, TTL remaining, and last origin RTT), and requests
  an eviction bitmask from the policy within a strict 500-microsecond timeout.
---

# Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX

## Quick Facts
- arXiv ID: 2508.12485
- Source URL: https://arxiv.org/abs/2508.12485
- Authors: Aayush Gupta; Arpit Bhayani
- Reference count: 12
- Primary result: 146% hit ratio improvement over LRU under high cache pressure (25 MB)

## Executive Summary
Cold-RL replaces NGINX's LRU eviction policy with a learned approach using a dueling Deep Q-Network (DQN) served by an ONNX sidecar. The system samples the K coldest objects from the LRU tail, extracts six lightweight features, and requests an eviction bitmask from the policy within a strict 500-microsecond timeout. If the timeout is exceeded, the system falls back to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a reward signal of +1 for retained objects that are hit again before TTL expiry. The evaluation shows Cold-RL achieves significant improvements in hit ratio over classical policies under high and medium cache pressure while maintaining microsecond-scale eviction latency.

## Method Summary
Cold-RL implements a dueling DQN that operates on a bounded set of K candidate objects sampled from the LRU tail. For each candidate, six features are extracted: age, size, hit count, inter-arrival time, TTL remaining, and last origin RTT. These features form the input to a 10K-parameter neural network that outputs Q-values for eviction decisions. The policy is trained offline via cache simulator replay of access logs, using a sparse reward signal (+1 if a retained object is hit again before TTL expiry). The trained model is exported to an int8-quantized ONNX format and served by a C++ sidecar via lock-free IPC with 500µs timeout. The NGINX module applies the eviction bitmask if within timeout, otherwise falls back to LRU.

## Key Results
- 146% improvement in hit ratio over LRU under high pressure (25 MB cache)
- 15% improvement at medium pressure (100 MB cache)
- 2% CPU overhead and 95th percentile eviction latency within 500µs SLO

## Why This Works (Mechanism)

### Mechanism 1
Sampling only the K coldest objects from the LRU tail captures nearly all viable eviction candidates while bounding inference complexity. By examining only the coldest K objects (typically 8-32), Cold-RL achieves bounded O(K) complexity instead of O(n) for full cache evaluation. The core assumption is that future victims predominantly reside in the LRU tail under realistic cache pressure patterns.

### Mechanism 2
The dueling DQN architecture stabilizes learning when many candidate actions have similarly low value—a common scenario under cache pressure. The network decomposes Q(s,a) into V(s) (state value) and A(s,a) (advantage per action), then combines as Q = V + (A - mean(A)). This separates "how good is this cache state?" from "how much better is evicting object i vs. others?"

### Mechanism 3
A sparse reward signal (+1 if retained object is hit again before TTL expiry) provides sufficient learning signal for eviction policy optimization when trained on sufficient log volume. During offline training, the simulator replays access logs and rewards the policy when retained objects are hit again before TTL expiration. This directly optimizes hit ratio without dense per-step supervision.

## Foundational Learning

- **Deep Q-Networks (DQN)**: Cold-RL uses a dueling DQN to approximate Q-values for eviction decisions. Understanding Q-learning, experience replay, and target networks is prerequisite to modifying the training pipeline. Quick check: Can you explain why DQN uses a replay buffer instead of learning directly from consecutive transitions?

- **Cache Eviction Policies (LRU, LFU, ARC)**: Cold-RL replaces LRU's forced-expire path. Understanding why LRU fails under size-mixed workloads and periodic bursts motivates the learned approach and informs fallback design. Quick check: What specific workload pattern causes LRU to "thrash" that ARC attempts to address?

- **ONNX Runtime and Model Quantization**: The inference sidecar uses int8-quantized ONNX models for microsecond-scale latency. Understanding quantization tradeoffs (accuracy vs. speed) is critical for model updates. Quick check: What is the expected latency impact of int8 quantization vs. float32 for a 10K parameter model?

## Architecture Onboarding

- **Component map**: NGINX Module (C) -> K-tail feature extraction (192 bytes for K=8) -> IPC via Unix domain socket -> ONNX Sidecar (C++) -> Lock-free ring buffer -> 10K-parameter dueling DQN -> Eviction bitmask or LRU fallback

- **Critical path**: Cache full → eviction triggered → Module extracts K-tail features → IPC to sidecar with 500µs deadline → Sidecar inference (p50: 127µs, p95: 342µs) → Bitmask returned → victims evicted, or timeout → LRU fallback

- **Design tradeoffs**: K=16 vs K=32: K=16 gives 86.8% hit ratio at 234µs; K=32 gives 87.0% at 456µs. Higher K risks SLO violation. Feature minimalism: 6 features chosen for interpretability and compute cost; ablation shows size (-31%), inter-arrival (-18%), TTL (-12%) are critical. Offline vs. online learning: Offline eliminates production risk but requires 24–48h log collection; workload shifts need retraining cycles.

- **Failure signatures**: Timeout (0.02% rate): Sidecar overloaded or IPC blocked → automatic LRU fallback. Circuit breaker: N consecutive failures → disable Cold-RL, revert to LRU. Cold-start: New deployment lacks logs → train on 24–48h of historical data first.

- **First 3 experiments**: 1) Shadow mode deployment: Run Cold-RL alongside LRU, log counterfactual decisions without applying them. Validate feature extraction and IPC latency. 2) K-sensitivity analysis: Benchmark K=8, 16, 32 on your workload traces. Measure p95 latency vs. hit ratio tradeoff. 3) Feature ablation: Retain all 6 features as baseline, then remove one at a time. Confirm size, inter-arrival, and TTL contribute positively; if not, your workload may differ from paper's adversarial benchmarks.

## Open Questions the Paper Calls Out

### Open Question 1
Can gradient boosting models (LightGBM/XGBoost) achieve sub-50µs inference latency while maintaining comparable hit ratios to the dueling DQN? The authors describe this as a compelling alternative that could unlock different tradeoffs, including sub-50µs inference through optimized tree traversal and improved interpretability, but have not implemented or evaluated it.

### Open Question 2
How can the policy adapt to sudden workload shifts without requiring full offline retraining cycles? The paper deliberately avoids online learning for safety and reproducibility, but this creates a lag between workload changes and policy effectiveness.

### Open Question 3
Can Bayesian methods (BART) provide posterior uncertainty estimates that enable confidence-aware fallback to LRU and automatic workload shift detection? The primary challenge identified is inference speed—MCMC sampling typically requires milliseconds, not microseconds.

### Open Question 4
What proportion of optimal eviction decisions fall outside the K-tail candidates, and how does this constraint affect achievable hit ratios? The paper relies on an unstated assumption that restricting decisions to the K coldest objects is sufficient, without measuring how often the globally optimal eviction candidate lies outside this set.

## Limitations
- Training hyperparameters (learning rate, epsilon decay, discount factor γ, replay buffer size) are not specified
- Cache simulator fidelity to real NGINX behavior is underspecified
- Action representation ambiguity: how bitmask maps to multi-eviction decisions and reward assignment

## Confidence
- **High Confidence**: Architectural approach (K-tail sampling + dueling DQN + offline training) is clearly specified and technically sound
- **Medium Confidence**: Feature importance claims are supported by ablation studies but may not generalize to all workload patterns
- **Low Confidence**: Exact conditions under which the 0.02% fallback rate was achieved are unclear

## Next Checks
1. **Simulator Fidelity Verification**: Implement the cache simulator with exact TTL handling, admission/expiry logic, and origin RTT simulation as described. Validate that it reproduces the same cache states and trajectories as the paper's training data.

2. **Hyperparameter Sensitivity Analysis**: Train multiple models with varying learning rates (1e-4 to 1e-2), batch sizes (32 to 128), and replay buffer sizes (50K to 200K). Measure hit ratio sensitivity to identify optimal configurations for your workload.

3. **Workload Shift Detection**: Deploy Cold-RL in shadow mode alongside LRU, then inject controlled workload shifts (e.g., sudden popularity skew, TTL changes). Measure how quickly hit ratio degrades and whether retraining cycles are practical in your operational context.