---
ver: rpa2
title: Automated Proof of Polynomial Inequalities via Reinforcement Learning
arxiv_id: '2503.06592'
source_url: https://arxiv.org/abs/2503.06592
tags:
- polynomial
- problem
- proof
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of proving polynomial inequalities,
  which is fundamental to many mathematical disciplines and has wide applications
  in diverse fields. Traditional algebraic methods are limited by truncation degree,
  making them infeasible for complex problems.
---

# Automated Proof of Polynomial Inequalities via Reinforcement Learning

## Quick Facts
- arXiv ID: 2503.06592
- Source URL: https://arxiv.org/abs/2503.06592
- Reference count: 37
- One-line primary result: APPIRL automates polynomial inequality proofs via RL, outperforming random search by ~6x average steps and matching state-of-the-art on maximum stable set problems.

## Executive Summary
This paper presents APPIRL, a reinforcement learning approach for automated proof of polynomial inequalities. The method transforms inequality proving into a linear programming problem using Krivine-basis representations, then employs a Deep Q-Network to incrementally select polynomial bases that establish non-negativity. A key innovation is the use of FFT-based multivariate polynomial multiplication to accelerate the dynamic action space construction. Experimental results demonstrate significant efficiency gains over random search across multiple benchmarks.

## Method Summary
APPIRL automates polynomial inequality proofs by formulating the problem as a linear programming task where a Krivine-basis representation with non-negative coefficients must be found. A DQN agent incrementally selects polynomial bases by multiplying existing bases with variables or their complements, with the action space dynamically expanding at each step. FFT-based polynomial multiplication accelerates this process. The agent receives rewards based on improvements to the LP's lower bound γ, and training proceeds until γ ≥ 0 is achieved or a maximum step limit is reached. The method has been successfully applied to both benchmark polynomial systems and the maximum stable set problem in graphs.

## Key Results
- APPIRL requires 75.4 average steps versus 445.1 for random search across 10 benchmarks
- For C8, APPIRL needs only 3 steps compared to 51 for random search
- APPIRL matches state-of-the-art performance on maximum stable set problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transforming polynomial inequality proving into a linear programming problem with Krivine-basis representation enables efficient verification of non-negativity.
- Mechanism: The paper reformulates proving f(x) ≥ 0 on a unit hypercube as an LP problem (Eq. 6): maximize γ subject to A·[λ,γ]ᵀ = c and λ ⪰ 0. If the optimal solution γ* ≥ 0, the inequality is proven (Theorem 2). This avoids the degree-bound explosion problem in traditional methods like SOS and Sherali-Adams by treating basis selection dynamically rather than exhaustively.
- Core assumption: The Positivstellensatz theorem (Theorem 1) guarantees that a strictly positive polynomial on [0,1]ⁿ admits a Krivine-basis representation with non-negative coefficients.
- Evidence anchors:
  - [abstract]: "we formulate the inequality proving problem as a linear programming (LP) problem"
  - [Section 3]: Detailed LP transformation in problem (5)-(6), Theorem 2 proof
  - [corpus]: Weak direct evidence; corpus focuses on LLM-based theorem proving, not LP-based approaches
- Break condition: If the LP problem (6) is infeasible or γ* remains negative after extensive basis expansion, the method cannot prove the inequality—potentially indicating f(x) is not non-negative or requires higher degrees than tractable.

### Mechanism 2
- Claim: Deep Q-Networks can learn effective policies for incrementally selecting polynomial bases that lead to non-negative Krivine representations.
- Mechanism: The DQN agent maintains a memory M of selected bases x^α(1-x)^β and learns to choose actions that multiply existing bases by x_i or (1-x_i) (Section 4.1). The reward function (Eq. 8) provides positive feedback proportional to improvement in γ_t, guiding the agent toward bases that increase expressiveness. The Q-network Q_θ(s,a) is trained via replay buffer sampling with loss minimization (Eq. 9).
- Core assumption: The optimal basis selection strategy can be learned incrementally through exploration, and improvements in γ correlate with eventual proof success.
- Evidence anchors:
  - [Section 4.1-4.2]: Full MDP formulation with state s=[γ_t, κ], action space A_t, reward r_t
  - [Section 5.1, Table 1]: APPIRL achieves 75.4 average steps vs. 445.1 for random search across 10 benchmarks
  - [corpus]: Moderate support; Fawzi et al. [14] (corpus-adjacent) shows neural-guided dynamic proofs are viable
- Break condition: If the Q-network fails to converge or exploration yields no γ improvement over many episodes, the learned policy may not generalize to the target inequality class.

### Mechanism 3
- Claim: FFT-based multivariate polynomial multiplication accelerates action-space construction without sacrificing correctness.
- Mechanism: Multivariate polynomials are transformed to univariate form via Eq. (10) using degree bound D, multiplied via FFT (computing convolutions in O(n log n)), then mapped back via Euclidean algorithm (Eq. 11). This replaces naive O(n²) multiplication critical when action space grows as 2n|M_t| per step.
- Core assumption: The multivariate-to-univariate affine transformation preserves polynomial structure and the degree bound D > d_p·d_q ensures no information loss.
- Evidence anchors:
  - [Section 4.3]: Complete three-step FFT procedure with transformation equations
  - [abstract]: "a fast multivariate polynomial multiplication method based on Fast Fourier Transform (FFT) is employed to enhance the efficiency"
  - [corpus]: No direct corpus evidence on FFT for polynomial inequalities; mechanism is internally claimed
- Break condition: If D is set too low, high-degree terms are truncated incorrectly; if transformation introduces numerical error, coefficient equality in LP may be violated.

## Foundational Learning

- Concept: **Markov Decision Processes (MDP) and Deep Q-Learning**
  - Why needed here: The entire APPIRL framework models basis selection as an MDP with state, action, reward. Understanding Q-learning, replay buffers, and ε-greedy exploration is essential to debug training convergence.
  - Quick check question: Can you explain why DQN uses a separate target network Q_θ' updated every c steps rather than updating the main network continuously?

- Concept: **Linear Programming Duality and Feasibility**
  - Why needed here: The core proof mechanism relies on solving LP problem (6) repeatedly. Understanding primal/dual formulations, feasibility conditions, and what γ* < 0 implies about the original inequality is critical.
  - Quick check question: In LP problem (6), what does it mean if the constraint A·[λ,γ]ᵀ = c has no solution?

- Concept: **Positivstellensatz and Polynomial Positivity Certificates**
  - Why needed here: The mathematical foundation that guarantees Krivine-basis representations exist for positive polynomials on semialgebraic sets. Without this, the LP approach has no theoretical justification.
  - Quick check question: Why does the Positivstellensatz theorem require f(x) > 0 strictly rather than f(x) ≥ 0?

## Architecture Onboarding

- Component map:
Environment Construction -> Memory M_t -> LP Solver (Gurobi) -> Optimal γ* -> Fast Poly Mult. (FFT-based) -> Action space A_t
Q-Network Q_θ -> Target Network -> Replay Buffer R -> Loss Minimization

- Critical path:
  1. State encoding: s_t = [γ_t, κ] where γ_t is LP solution, κ is consecutive no-reward rounds
  2. Action selection: Agent picks basis a ∈ A_t via ε-greedy over Q(s,a)
  3. LP evaluation: Update M_t ← M_t ∪ {a}, solve LP (7), get γ*
  4. Reward computation: r_t = (γ_t - γ_{t-1})/|γ_0| if improved, else ε
  5. Network update: Sample from R, minimize loss (9), update θ

- Design tradeoffs:
  - Initial memory size M_0 (k): Larger k ensures LP feasibility but increases initial computation; paper sets k = deg(f(x))
  - Network architecture: Table 1 shows varying depths (4 hidden layers) and widths (64-160 neurons) tuned per problem complexity
  - Max steps vs. completeness: Limiting to Maxstep trades completeness for tractability; some inequalities may require unbounded steps

- Failure signatures:
  - γ* consistently negative after many steps → inequality may be false or require higher degree than tractable
  - Reward r_t = ε repeatedly → agent stuck in local plateau, consider increasing exploration ε
  - LP solver returns infeasible → M_0 too small or coefficient matrix A has rank issues
  - Q-loss not converging → check learning rate, replay buffer size, or state representation adequacy

- First 3 experiments:
  1. Reproduce C1 (2D, degree 2): Set up the simplest benchmark with k=14 initial memory, verify APPIRL completes in ~7 steps as reported
  2. Ablate FFT: Replace FFT multiplication with naive multiplication on C3/C4, measure wall-clock time difference to quantify acceleration benefit
  3. Compare initialization strategies: Test different k values (deg(f)/2, deg(f), 2·deg(f)) on C5-C7 to understand sensitivity to initial memory size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the APPIRL framework be generalized to prove inequalities over arbitrary semi-algebraic sets rather than restricting the domain to hyperrectangles?
- Basis in paper: [inferred] Section 2 explicitly states the focus is on the unit hypercube S = {x ∈ [0,1]ⁿ} and utilizes inference rules specifically derived for x_i(1-x_i) structures.
- Why unresolved: The method relies on the Krivine-basis representation which is theoretically adapted to the hypercube geometry; extending this to general polynomial constraints defined by g_i(x) ≥ 0 would require a fundamental change in the basis construction and action space definition.
- What evidence would resolve it: A theoretical derivation of the inference rules for general semi-algebraic sets or experimental results showing the agent successfully learning proofs on non-hyperrectangular domains.

### Open Question 2
- Question: How does the method's performance scale with dimensionality compared to traditional Sum-of-Squares (SOS) approaches in high-variable settings?
- Basis in paper: [inferred] The experimental evaluation in Table 1 is limited to low-dimensional examples (n=2 to n=6), and Section 3 notes that increasing the degree bound D leads to a "dimension explosion" making problems intractable.
- Why unresolved: While the paper claims scalability, the action space size (2ⁿ|M_t|) grows exponentially with variables n, potentially limiting the DQN agent's ability to navigate the search space efficiently in high dimensions.
- What evidence would resolve it: Benchmark results on polynomial systems with significantly more variables (e.g., n > 20) comparing convergence time against standard SDP solvers.

### Open Question 3
- Question: Is the Deep Q-Network (DQN) architecture the optimal choice for handling the dynamic, expanding action space required by the proof search?
- Basis in paper: [inferred] Section 4.1 highlights that the action space is variable and incrementally updates as A_t = A_{t-1} ∪ {a_{t-1}x_i, ...}, which complicates the standard DQN setup.
- Why unresolved: DQN is typically suited for fixed action spaces; the paper does not analyze if the "expanding" nature of the basis selection hinders the stability of the Q-value estimation or if policy gradient methods would be more robust.
- What evidence would resolve it: An ablation study comparing DQN against other RL algorithms (e.g., PPO or Actor-Critic) specifically regarding training stability and sample efficiency in this dynamic environment.

## Limitations

- LP solver performance and action space exponential growth may limit scalability to high-dimensional problems
- Method requires strict positivity (f(x) > 0) rather than non-negativity, excluding certain valid inequalities
- Random search comparison may not represent state-of-the-art algebraic methods with different search strategies

## Confidence

- **High confidence:** The LP formulation for proving polynomial inequalities via Krivine bases (Mechanism 1) is mathematically sound and well-established in the literature. The experimental results showing APPIRL outperforms random search are reproducible and clearly presented.
- **Medium confidence:** The DQN learning mechanism (Mechanism 2) shows effectiveness on the tested benchmarks, but the sensitivity to hyperparameters (learning rate, architecture selection, exploration schedule) remains unclear. The paper provides insufficient detail for parameter tuning across problem classes.
- **Medium confidence:** The FFT-based polynomial multiplication (Mechanism 3) is implemented efficiently, but the paper lacks rigorous numerical stability analysis or comparison with alternative acceleration methods.

## Next Checks

1. **Architecture sensitivity test:** Systematically vary the DQN architecture (neurons/layers) and learning parameters on benchmark C1-C3 to establish robust hyperparameter selection rules rather than problem-specific tuning.

2. **Scalability boundary:** Evaluate APPIRL on polynomial inequalities with 10+ variables to empirically determine when action space explosion becomes prohibitive, despite FFT acceleration.

3. **Numerical robustness:** Test the LP solver's behavior on ill-conditioned coefficient matrices from high-degree polynomial multiplication, and assess whether coefficient rounding errors affect γ* calculation accuracy.