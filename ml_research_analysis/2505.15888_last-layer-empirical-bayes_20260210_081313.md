---
ver: rpa2
title: Last Layer Empirical Bayes
arxiv_id: '2505.15888'
source_url: https://arxiv.org/abs/2505.15888
tags:
- learning
- lleb
- ensembles
- neural
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses uncertainty quantification in neural networks,
  comparing Bayesian neural networks (BNNs) and deep ensembles. It proposes Last Layer
  Empirical Bayes (LLEB), which learns a prior over the last layer weights using a
  normalizing flow to maximize the evidence lower bound, aiming to combine the strengths
  of BNNs and ensembles while maintaining tractability.
---

# Last Layer Empirical Bayes

## Quick Facts
- arXiv ID: 2505.15888
- Source URL: https://arxiv.org/abs/2505.15888
- Reference count: 9
- Primary result: LLEB performs on par with last-layer Laplace and Monte Carlo dropout but does not significantly outperform them

## Executive Summary
This paper proposes Last Layer Empirical Bayes (LLEB), a method for uncertainty quantification in neural networks that learns a prior over the last layer weights using a normalizing flow to maximize the evidence lower bound. The approach aims to combine the strengths of Bayesian neural networks and deep ensembles while maintaining computational tractability by restricting Bayesian treatment to the last layer. LLEB is evaluated on MNIST, Fashion-MNIST, CIFAR-10, and SVHN, showing accuracies around 97-98% on MNIST variants and 92-95% on CIFAR/SVHN with low calibration errors comparable to ensemble methods.

## Method Summary
LLEB learns a data-dependent prior over the last layer weights by parameterizing it as a normalizing flow and maximizing the ELBO. The method partitions the network weights into quasi-uncertain (last layer) and non-quasi-uncertain components, training the backbone via maximum likelihood first, then learning the prior only over the last layer. This two-step procedure (or end-to-end for small models) enables tractable training while avoiding the high-dimensional optimization challenges of full-network Bayesian approaches. At test time, predictions are made by averaging over multiple samples from the learned prior.

## Key Results
- On MNIST and Fashion-MNIST, LLEB achieves accuracies around 97-98% with calibration errors near zero
- On CIFAR-10 and SVHN, LLEB achieves accuracies around 92-95% with low calibration errors
- LLEB performs on par with existing approaches like last-layer Laplace approximation and Monte Carlo dropout
- The results suggest LLEB does not significantly outperform existing methods, likely due to tractability constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normalizing flows provide implicit regularization that prevents the learned prior from collapsing to a point mass while maintaining sufficient flexibility to concentrate mass around high-performing weights.
- Mechanism: The invertibility constraint inherent to normalizing flows acts as an implicit regularizer. When maximizing $\mathbb{E}_{\theta \sim q_\eta}[\log p(\mathcal{D}|\theta)]$ over flow parameters $\eta$, the bijection requirement prevents degenerate solutions while still allowing $q^*$ to place most mass near $\Theta^*$ (the set of maximum-likelihood weights).
- Core assumption: The invertibility of NFs provides sufficient regularization; explicitly verified when the authors note that replacing NFs with fully-connected architectures resulted in collapse to point masses.
- Evidence anchors:
  - [Section 3]: "NFs are very flexible, yet their invertibility acts as an implicit regularizer which prevents collapse onto a point mass and promotes some diversity."
  - [Appendix B]: "We found that implementing this change [fully-connected architectures] resulted in q* collapsing onto a point mass, highlighting that the NFs used in LLEB indeed provide implicit regularization against this collapse."
  - [corpus]: Weak direct evidence; corpus focuses on ensemble-Bayesian connections rather than NF-specific regularization properties.

### Mechanism 2
- Claim: Deep ensembles implicitly perform empirical Bayes by learning a data-dependent prior that equals their posterior, concentrated on maximum-likelihood solutions.
- Mechanism: When maximizing ELBO over both variational distribution $q$ and prior $\pi$ with sufficient flexibility, the optimal solution satisfies $q^* = \pi^* = \pi^*(\cdot|\mathcal{D})$ where all three assign probability 1 to $\Theta^*$. Ensemble weights $\{\theta_m^*\}_{m=1}^M$ are elements of $\Theta^*$, making the ensemble distribution interpretable as both learned prior and posterior.
- Core assumption: Stochastic optimization randomness produces diverse points within $\Theta^*$ rather than converging to a single optimum.
- Evidence anchors:
  - [Section 2]: "Although ensembles are not typically thought of as Bayesian, Loaiza-Ganem et al. (2025) recently argued they can be understood as performing empirical Bayes."
  - [Appendix A]: "π appears only in the KL term, so that if the learnable prior π is flexible enough, it must be the case that π = q holds at optimality."
  - [corpus]: Strong support from "Deep Ensembles Secretly Perform Empirical Bayes" (arXiv:2501.17917) which provides the theoretical foundation.

### Mechanism 3
- Claim: Restricting Bayesian treatment to the last layer provides a tractable approximation that preserves meaningful uncertainty quantification while avoiding the curse of dimensionality in weight space.
- Mechanism: By partitioning $\theta = (\theta_{QU}, \theta_{NU})$ and only applying the normalizing flow to $\theta_{QU}$ (last layer), the optimization problem becomes low-dimensional enough to train stably. The two-step training procedure (first train full network via MLE, then freeze backbone and train flow) further stabilizes optimization.
- Core assumption: Most epistemic uncertainty relevant for prediction is captured by the last layer; earlier layers' uncertainty contribution is secondary.
- Evidence anchors:
  - [Section 3]: "The root cause of this intractability is the high dimensionality of the NF, and we thus propose to quantify uncertainty only over a subset of parameters."
  - [Section 3]: "We found this [end-to-end objective] performed well with small classifiers, but that it resulted in unstable and slow optimization when using larger classifiers."
  - [corpus]: "From Deep Additive Kernel Learning to Last-Layer Bayesian Neural Networks" (arXiv:2502.10540) supports last-layer approximations for computational tractability.

## Foundational Learning

- Concept: **Variational Inference and the ELBO**
  - Why needed here: LLEB is fundamentally an ELBO maximization approach; understanding the trade-off between likelihood term and KL divergence is essential to interpret why learned priors help.
  - Quick check question: Can you explain why maximizing ELBO with flexible $q$ and $\pi$ leads to $q^* = \pi^*$ rather than $q^*$ matching the true posterior?

- Concept: **Normalizing Flows**
  - Why needed here: The prior is parameterized as a normalizing flow; understanding bijections, density computation via change of variables, and flow capacity is critical for implementation.
  - Quick check question: Given a base distribution $p_Z(z)$ and invertible transformation $f$, how do you compute the density of $x = f(z)$?

- Concept: **Empirical Bayes**
  - Why needed here: The core theoretical motivation is interpreting ensembles through empirical Bayes; this frames why learning the prior (rather than fixing it) is well-motivated.
  - Quick check question: How does empirical Bayes differ from hierarchical Bayes in terms of how the prior is determined?

## Architecture Onboarding

- Component map:
  - Input image → Backbone CNN/ResNet18 → Two linear layers (50 hidden) → Last layer weights + flow output → Predictions

- Critical path:
  1. Train backbone to convergence with standard cross-entropy loss
  2. Freeze all parameters except last layer
  3. Initialize normalizing flow; base distribution is isotropic Gaussian
  4. Maximize $\mathbb{E}_{\theta_{QU} \sim q_\eta}[\log p(\mathcal{D}|\theta_{QU}, \theta_{NU}^*)]$ using 10 samples per gradient step
  5. At test time, sample 10 weight configurations and average predictions

- Design tradeoffs:
  - End-to-end vs two-step training: End-to-end is theoretically cleaner but unstable for large models; two-step is stable but may yield suboptimal joint configuration
  - Flow capacity vs computational cost: More coupling layers increase flexibility but slow training; 2 layers worked adequately
  - Last-layer vs full-network Bayesian: Full BNN captures more uncertainty but is computationally intractable with current flow scalability

- Failure signatures:
  - Point mass collapse: Predictive variance goes to zero; check if flow output has near-zero variance → indicates need for explicit regularization or flow capacity adjustment
  - Unstable training on large models: Loss oscillates or diverges during end-to-end training → switch to two-step procedure
  - No improvement over baselines despite convergence: Calibration and AUC metrics match default network → flow may not be learning meaningful uncertainty; inspect weight distribution diversity

- First 3 experiments:
  1. Validation on small-scale (MNIST): Implement end-to-end LLEB with a small CNN; verify accuracy ~97-98% and ECE near 0. Confirm flow samples show diversity (visualize weight distributions).
  2. Two-step procedure validation (CIFAR-10): Train ResNet18 to convergence, freeze backbone, train flow with learning rate 1e-5 for 100 epochs. Compare single-model LLEB against LLL and MCD baselines on accuracy, ECE, and OOD AUC.
  3. Ensemble comparison: Train 5 independent LLEB models; compare against standard 5-model ensemble. Verify computational cost difference (LLEB should be ~5x cheaper at inference if using single model, comparable if ensembling LLEB).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending Bayesian treatment beyond the last layer improve LLEB's performance while maintaining tractability?
- Basis in paper: [explicit] The authors state: "to retain tractability we use the flow only on the last layer" and hypothesize that limited performance "is due to the concessions we made in LLEB for tractability."
- Why unresolved: The dimensionality of full-network normalizing flows remains computationally prohibitive, and the paper does not explore intermediate solutions (e.g., multiple layers, low-rank approximations).
- What evidence would resolve it: Demonstrating that multi-layer or full-network variants of LLEB outperform last-layer-only baselines on standard UQ metrics without incurring prohibitive computational costs.

### Open Question 2
- Question: Is there an alternative to normalizing flows that better concentrates mass around Θ* while preventing collapse to point masses?
- Basis in paper: [inferred] The authors tried fully-connected architectures which "collapsed onto a point mass" and entropy regularization which "did not improve upon LLEB," suggesting the NF choice trades off flexibility versus implicit regularization suboptimally.
- Why unresolved: The paper does not explore other flexible distribution families (e.g., mixture models, diffusion-based densities) that might achieve a better trade-off.
- What evidence would resolve it: Identifying a distribution family that achieves higher diversity within Θ* while maintaining concentration, leading to improved AUC and ECE over NF-based LLEB.

### Open Question 3
- Question: What is the theoretically optimal prior strength between standard BNNs (weak, fixed priors) and ensembles (extremely strong, data-dependent priors) for uncertainty quantification?
- Basis in paper: [explicit] The authors note "the empirical Bayes view of ensembles suggests that very strong priors are better than very weak ones, it does not guarantee that stronger is always better" and position LLEB as exploring this interpolation.
- Why unresolved: LLEB does not explicitly control or measure prior strength, and results show it performs comparably to but not better than existing methods.
- What evidence would resolve it: A systematic study varying prior strength (e.g., via temperature scaling on the learned prior) and measuring UQ performance to identify an optimal intermediate regime.

## Limitations
- Performance is comparable to but not better than existing methods like last-layer Laplace and Monte Carlo dropout
- Normalizing flows are restricted to the last layer for tractability, potentially limiting uncertainty capture in earlier layers
- The two-step training procedure introduces additional hyperparameters and may yield suboptimal joint configurations

## Confidence
- Empirical performance claims: High (extensive experiments with clear metrics)
- Theoretical interpretation of ensembles as empirical Bayes: High (supported by external work)
- NF regularization mechanism: Medium (partially verified empirically, relies on invertability assumption)

## Next Checks
1. Test LLEB sensitivity to flow architecture changes (e.g., number of coupling layers, hidden dimensions) to quantify robustness
2. Evaluate LLEB under domain shift conditions where early-layer uncertainty would be critical (e.g., texture-vs-shape bias datasets)
3. Compare LLEB calibration under different ensemble sizes to determine if learned priors capture similar diversity benefits