---
ver: rpa2
title: 'Queueing, Predictions, and LLMs: Challenges and Open Problems'
arxiv_id: '2503.07545'
source_url: https://arxiv.org/abs/2503.07545
tags:
- scheduling
- predictions
- systems
- time
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the use of machine learning predictions in queueing
  systems, focusing on large language model (LLM) inference scheduling. It reviews
  recent work on using predicted service times to improve queue performance, showing
  that even 1-bit predictions can significantly reduce response times compared to
  FIFO.
---

# Queueing, Predictions, and LLMs: Challenges and Open Problems

## Quick Facts
- **arXiv ID:** 2503.07545
- **Source URL:** https://arxiv.org/abs/2503.07545
- **Reference count:** 40
- **Primary result:** Machine learning predictions can significantly improve queue performance in LLM inference scheduling

## Executive Summary
This paper surveys the intersection of queueing theory and machine learning predictions, with a focus on scheduling large language model (LLM) inference requests. The authors review recent work showing that even simple 1-bit predictions about service times can dramatically reduce response times compared to first-come-first-served (FCFS) scheduling. The paper then addresses the unique challenges posed by LLM systems, including dynamic memory footprints from key-value caches, variable inference times, and complex preemption overhead. It explores scheduling strategies for single LLM instances, compound AI systems with multiple components, and reasoning-based LLM systems, highlighting opportunities to apply queueing theory insights to improve LLM scheduling performance.

## Method Summary
The paper provides a comprehensive survey of existing literature on using machine learning predictions in queueing systems, with particular emphasis on LLM inference scheduling. It synthesizes theoretical results and practical considerations from multiple domains, including classical queueing theory, computer architecture, and systems design. The authors analyze the applicability of prediction-based scheduling techniques to LLM workloads, considering factors such as variable batch sizes, dynamic KV cache management, and heterogeneous hardware resources. The survey methodology involves reviewing recent publications, identifying key challenges in LLM scheduling, and proposing directions for future research based on established queueing theory principles.

## Key Results
- Simple 1-bit predictions about service times can significantly reduce response times compared to FIFO scheduling
- LLM inference presents unique challenges including dynamic KV cache memory footprints and variable inference times
- Scheduling strategies must balance latency, cost, and resource utilization across heterogeneous hardware
- Compound AI systems with multiple interacting components require coordinated scheduling approaches
- Reasoning-based LLM systems introduce additional scheduling complexity due to iterative computation patterns

## Why This Works (Mechanism)
Prediction-based scheduling improves queue performance by making informed decisions about job ordering and resource allocation. By using even simple predictions about service times, schedulers can prioritize shorter jobs and reduce overall response times through the shortest remaining processing time (SRPT) principle. This approach is particularly effective for LLM inference because it can adapt to the variable nature of inference times and the dynamic memory requirements of KV caches. The mechanism works by reducing the time shorter jobs spend waiting behind longer ones, which has a multiplicative effect on system-wide performance metrics.

## Foundational Learning

**Queueing Theory Basics**: Understanding arrival rates, service times, and queue disciplines is essential for analyzing system performance. Quick check: Verify that Little's Law holds for your system by measuring average queue length, arrival rate, and response time.

**Prediction Accuracy Impact**: The effectiveness of prediction-based scheduling depends critically on prediction accuracy. Quick check: Measure the correlation between predicted and actual service times across different workload patterns.

**KV Cache Management**: Large language models maintain key-value caches during inference that consume significant memory and affect scheduling decisions. Quick check: Monitor memory usage patterns and cache hit rates during inference to understand memory pressure.

**Preemption Overhead**: Context switching and state restoration have non-trivial costs that must be considered in scheduling decisions. Quick check: Measure the time and memory overhead of preempting and resuming inference tasks.

**Heterogeneous Resource Scheduling**: Different hardware configurations (GPU, CPU, memory bandwidth) create complex tradeoffs in scheduling decisions. Quick check: Compare performance across different hardware configurations with varying KV cache sizes and memory bandwidth.

## Architecture Onboarding

**Component Map**: Request arrival -> Prediction module -> Scheduler -> Resource allocator -> LLM inference engine -> Response delivery

**Critical Path**: Request arrival → Prediction → Scheduler decision → Resource allocation → KV cache management → Inference execution → Response

**Design Tradeoffs**: 
- Latency vs. throughput: Prioritizing short jobs reduces average response time but may underutilize resources
- Memory vs. performance: Larger KV caches improve inference speed but increase memory pressure
- Cost vs. quality: More accurate predictions improve scheduling but require additional computation
- Preemption vs. simplicity: Allowing preemption improves response times but adds complexity and overhead

**Failure Signatures**: 
- High variance in response times despite predictions
- Memory pressure leading to cache misses and degraded performance
- Prediction errors causing suboptimal scheduling decisions
- Preemption overhead exceeding benefits in certain workload patterns

**3 First Experiments**:
1. Compare FIFO vs. SRPT scheduling with perfect vs. imperfect predictions across different workload patterns
2. Measure the impact of KV cache size on inference latency and memory utilization
3. Evaluate preemption overhead by measuring context switching costs at different interruption points

## Open Questions the Paper Calls Out
None

## Limitations
- Prediction accuracy varies significantly with workload characteristics, affecting scheduling performance
- KV cache management overhead may be more complex in real systems than theoretical models suggest
- The interaction between scheduling decisions and hardware heterogeneity needs more empirical validation
- Cost-latency tradeoffs may differ substantially across deployment scenarios and business requirements

## Confidence

**High**: General applicability of queueing theory to LLM scheduling across different workload patterns
**Medium**: Specific claims about KV cache management challenges and memory pressure effects
**Medium**: Discussion of preemption overhead implications and context switching costs
**Low**: Exact performance improvements from different scheduling strategies in production environments

## Next Checks

1. Measure actual preemption overhead and context switching costs in production LLM serving systems under realistic load conditions
2. Evaluate the impact of prediction accuracy on queue performance across different workload patterns and arrival distributions
3. Test scheduling strategies on heterogeneous hardware configurations with varying KV cache sizes, memory bandwidth, and GPU capabilities