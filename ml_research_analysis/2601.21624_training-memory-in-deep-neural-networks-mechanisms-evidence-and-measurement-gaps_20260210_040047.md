---
ver: rpa2
title: 'Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement
  Gaps'
arxiv_id: '2601.21624'
source_url: https://arxiv.org/abs/2601.21624
tags:
- memory
- learning
- training
- state
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Modern deep-learning training is not memoryless. Updates depend
  on optimizer moments and averaging, data-order policies (random reshuffling vs with-replacement,
  staged augmentations and replay), the nonconvex path, and auxiliary state (teacher
  EMA/SWA, contrastive queues, BatchNorm statistics).
---

# Training Memory in Deep Neural Networks: Mechanisms, Evidence, and Measurement Gaps

## Quick Facts
- arXiv ID: 2601.21624
- Source URL: https://arxiv.org/abs/2601.21624
- Reference count: 40
- Primary result: Modern deep-learning training is not memoryless; updates depend on optimizer moments, averaging, data-order policies, the nonconvex path, and auxiliary state.

## Executive Summary
This paper presents a comprehensive survey and framework for understanding and measuring training memory in deep neural networks. It argues that modern training is inherently stateful, with optimizer buffers, data order, and auxiliary mechanisms like EMA and BatchNorm statistics creating measurable memory effects. The authors introduce a causal attribution framework using seed-paired Average Treatment Effects (ATEs) to isolate and quantify the impact of different memory sources. They propose portable perturbation primitives and a reporting checklist to enable reproducible, auditable measurement of training history effects across models, data, and training regimes.

## Method Summary
The method introduces a branch-and-hold causal attribution framework. At a chosen step, the full training state (weights, optimizer buffers, data order, auxiliary state) is snapshotted. Two branches are created: one control and one treatment. For a window of steps, both branches process identical minibatches and augmentations. The treatment branch applies a single-source perturbation (e.g., momentum reset, order swap). After the window, normal training resumes. The effect is measured via paired Average Treatment Effects (ATEs) computed over multiple seed-matched runs, with bootstrap confidence intervals. Portable perturbation primitives are defined for each memory source, and a reporting checklist with audit artifacts (order hashes, buffer checksums, RNG contracts) is provided.

## Key Results
- Optimizer state (momentum, Adam moments, EMA) carries measurable training history that affects final performance even when weights and data are fixed.
- Data order policy (random reshuffling vs. with-replacement) acts as a sampler-level memory source with epoch-scale lifetime, affecting convergence and final solutions.
- Causal attribution requires single-source perturbation primitives with seed-paired designs to isolate effects via controlled branch-and-hold interventions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizer state (momentum buffers, Adam moments, EMA/SWA snapshots) carries training history that measurably affects downstream behavior even when weights and data are fixed.
- **Mechanism:** Exponential smoothing via β decay creates an effective half-life h₁/₂ = log(0.5)/log(β). Past gradients influence current updates with geometrically decaying weights. Resetting buffers removes this accumulated directional information.
- **Core assumption:** The optimizer's internal state is not redundant with current weights—if it were, resets would have no effect.
- **Evidence anchors:**
  - [abstract]: "Updates depend on optimizer moments and averaging..."
  - [section 3.1 (S1)]: "Momentum/Nesterov keep an exponential moving average (EMA) of gradients; the decay β induces a practical 'half-life'..."
  - [corpus]: "Understanding SGD with Exponential Moving Average" (FMR=0.54) provides theoretical analysis of EMA in linear regression, supporting the half-life framework.
- **Break condition:** If β→0 (no momentum) or gradients are identical across steps, optimizer-state memory becomes negligible.

### Mechanism 2
- **Claim:** Data-order policy (random reshuffling vs. with-replacement sampling) acts as a sampler-level memory source with epoch-scale lifetime, affecting convergence and final solutions.
- **Mechanism:** Without-replacement sampling changes the gradient noise structure across epochs. In nonconvex settings, update order is non-commutative: applying update A then B can yield different endpoints than B then A due to curvature differences encountered mid-path.
- **Core assumption:** Example ordering affects the loss landscape trajectory; IID sampling assumptions do not hold in practice for reshuffled data.
- **Evidence anchors:**
  - [section 4.2]: "In nonconvex objectives, small stochastic updates generally do not commute... theory detects this even in convex baselines through the comparison of with-replacement sampling and random reshuffling (RR)..."
  - [section 5.2]: "Comparisons of with-replacement vs. random reshuffling (RR) show different noise structures; RR often converges faster..."
  - [corpus]: Limited direct corpus evidence on order dependence; related work focuses on optimization rather than memory attribution.
- **Break condition:** If loss surface is convex and smooth, or if batch size → dataset size, order effects diminish.

### Mechanism 3
- **Claim:** Causal attribution of training memory requires single-source perturbation primitives (e.g., momentum reset, order-window swaps) with seed-paired designs, isolating effects via controlled branch-and-hold interventions.
- **Mechanism:** At branch point t₀, snapshot full state (weights, optimizer buffers, sampler order, external queues). Create paired branches differing only in targeted source. Evaluate function-space metrics on fixed probe. Compute paired ATE with bootstrap CIs.
- **Core assumption:** The intervention window W matches the source's characteristic lifetime (e.g., 1–2 half-lives for momentum, one epoch for order).
- **Evidence anchors:**
  - [section 6.1]: "ATE_opt = E_s[M(f_T^(CARRY,s)) - M(f_T^(RESET,s))], ATE_order = E_s[M(f_T^(RR,s)) - M(f_T^(WR,s))]"
  - [section 6.2, Table 4]: Explicit perturbation primitives mapped to sources S1–S5 with window guidelines.
  - [corpus]: Weak corpus evidence; no directly comparable causal-estimand frameworks found in neighbors.
- **Break condition:** If multiple sources are perturbed simultaneously, or if residual nondeterminism corrupts paired branches, attribution fails.

## Foundational Learning

- **Concept: Exponential Moving Average (EMA) half-life**
  - Why needed here: Interpreting optimizer-state memory requires quantifying how long past gradients persist. The half-life formula h₁/₂ = log(0.5)/log(β) lets you choose intervention window W.
  - Quick check question: If β=0.99, how many steps until a gradient's influence halves? (Answer: ~69 steps)

- **Concept: Non-commutativity in nonconvex optimization**
  - Why needed here: Path dependence arises because update order changes outcomes. Understanding this explains why same data in different order yields different solutions.
  - Quick check question: In a convex quadratic loss, does update order matter? (Answer: No—gradients commute in expectation. Path effects require nonconvexity.)

- **Concept: Average Treatment Effect (ATE) with paired seeds**
  - Why needed here: The paper formalizes memory attribution as causal inference. ATE over seed-matched runs quantifies the expected effect of an intervention.
  - Quick check question: Why pair seeds rather than run independent groups? (Answer: Pairing reduces variance by controlling for seed-specific noise.)

## Architecture Onboarding

- **Component map:**
  - Root run -> State snapshot (weights, optimizer buffers, sampler order, BN stats) -> Branch into Control and Treatment -> Hold window (identical minibatches/augmentations) -> Intervention (single-source perturbation) -> Resume training -> Evaluate on fixed probe -> Compute paired ATE with bootstrap CI

- **Critical path:**
  1. Choose target source (S1–S5) and set intervention window W based on half-life or epoch scale.
  2. At t₀, snapshot state and derive named RNG streams from root seed.
  3. Branch into CONTROL and TREAT; apply single-source perturbation to TREAT.
  4. Replay identical minibatch IDs and augmentation RNGs for W steps.
  5. Resume normal training to horizon T; evaluate on probe.
  6. Compute paired ATE and 95% CI; log order hashes, buffer norms, queue fingerprints.

- **Design tradeoffs:**
  - **Smaller W → cleaner attribution but smaller effect size** (may underresolve signal).
  - **Larger W → more visible effect but risk of confounding** (other factors drift in).
  - **More seeds → tighter CIs but higher compute cost.** Paper suggests ≥5 seeds for small/medium benchmarks, ≥3 for costly regimes.

- **Failure signatures:**
  - Branches diverge despite identical perturbation → residual nondeterminism (kernel-level, parallel loading).
  - ATE confidence interval includes zero with wide width → insufficient seeds or underpowered probe.
  - Function-space metrics agree but representation similarity diverges → descriptive not causal; check if change is in layers not affecting output.

- **First 3 experiments:**
  1. **Optimizer-state reset on CIFAR-10/ResNet-18:** At epoch 10, zero momentum buffers (β=0.9) for W≈14 steps (2 half-lives). Report early and final ATE_opt on accuracy and ECE with 5 seeds.
  2. **Order-window swap on random reshuffling vs. with-replacement:** Record one epoch of minibatch IDs under RR, replay under fresh permutation. Hold optimizer state fixed. Report ATE_order with order hashes.
  3. **Phase-boundary carry/reset at pretrain→finetune:** From a pretrained checkpoint, branch into CARRY (load all state) vs. RESET (zero optimizer buffers, reinit BN stats). Report first-k-epoch effects and final calibration delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can standardized, portable protocols be established to enable causal attribution of training memory sources (optimizer state vs. sampler order) across different hardware, datasets, and architectures?
- Basis in paper: [explicit] Section 10 (OP1) states the community lacks portable protocols for auditable attribution and asks for the definition of "protocol cards" and checksums to verify claims across labs.
- Why unresolved: Current reporting often omits specific artifacts like order hashes or buffer-state logs, making it difficult to replicate or verify single-source perturbations on different stacks.
- What evidence would resolve it: Adoption of reproducibility artifacts that successfully allow separate labs to replay and verify specific memory attributions on distinct infrastructure.

### Open Question 2
- Question: Can early-window readouts be defined that reliably predict final generalization across varied memory policies (e.g., momentum schedules, augmentation order) rather than just within a single policy?
- Basis in paper: [explicit] Section 10 (OP2) asks for early diagnostics that retain rank consistency and calibration when optimizer half-life or order policies are varied.
- Why unresolved: Existing learning-curve extrapolation tools usually forecast within a fixed policy, failing to account for how early geometry interacts with varying memory sources.
- What evidence would resolve it: Identification of early metrics that maintain predictive rank correlation with final outcomes even when momentum half-lives or sampler order policies are changed.

### Open Question 3
- Question: How do specific memory mechanisms, such as momentum half-lives or EMA decay, quantitatively bound non-commutativity (AB ≠ BA) in the function space of overparameterized networks?
- Basis in paper: [explicit] Section 10 (OP4) notes the lack of theory quantifying AB ≠ BA effects under realistic training ingredients like heavy augmentation or stateful sampling.
- Why unresolved: Current theory relies largely on convex assumptions or IID sampling, failing to explain non-commutativity in nonconvex settings with realistic hyperparameters.
- What evidence would resolve it: Theoretical bounds linking measurable optimizer half-lives to path divergence, or characterizations of when different orderings diverge into isolated basins versus remaining mode-connected.

### Open Question 4
- Question: How can explicit memory mechanisms (replay buffers, server accumulators) be designed for attribution without compromising training-data privacy or enabling membership inference?
- Basis in paper: [explicit] Section 10 (OP5) highlights the open question of designing "privacy-aware by construction" memory interventions.
- Why unresolved: Making memory visible for audit (e.g., via replay buffers) inherently increases the risk of data leakage or gradient inversion, especially in federated settings.
- What evidence would resolve it: Development of diagnostics that quantify the trade-off between stateful optimization gains and leakage risk, or algorithms achieving differentially private buffer turnover.

## Limitations

- **Causal validity:** The paired-seed design assumes that seed-matched runs differ only in the intervention, but residual nondeterminism (parallel data loading, GPU atomic ops) can break this. Confidence is Medium for mechanism claims because validation relies on strong RNG discipline but lacks exhaustive ablation.
- **Path dependence quantification:** Non-commutativity is theoretically grounded but empirical attribution across diverse architectures remains sparse. Confidence is Low for generality across architectures and tasks.
- **Scope of auxiliary sources:** Teacher EMA, contrastive queues, and BN stats are included, but other forms of state (LR schedulers, adaptive regularization) are not explicitly measured. Confidence is Medium for completeness of the source taxonomy.

## Confidence

- **Mechanism 1 (Optimizer State):** High confidence in the half-life framework; direct empirical evidence in corpus (FMR=0.54).
- **Mechanism 2 (Order Policy):** Medium confidence; theoretical basis strong, but empirical corpus evidence thin.
- **Mechanism 3 (Causal Attribution):** Medium confidence; protocol is rigorous but requires strict RNG control to avoid leakage.

## Next Checks

1. **RNG Isolation Test:** Run a paired seed experiment with order-window swap, log byte-identical batch indices during the hold window, and verify zero divergence except from the intervention.
2. **Multi-Source Confounding:** Apply simultaneous perturbations (momentum reset + order swap) to confirm that the ATE framework can detect and disentangle overlapping effects.
3. **Architectural Scaling:** Repeat Mechanism 1 on a vision transformer (e.g., ViT-Small) to test if optimizer-state memory effects persist in architectures with different gradient dynamics.