---
ver: rpa2
title: Social Simulations with Large Language Model Risk Utopian Illusion
arxiv_id: '2510.21180'
source_url: https://arxiv.org/abs/2510.21180
tags:
- social
- llms
- human
- similarity
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a systematic framework to analyze large language
  models (LLMs) in social simulations by examining multi-agent chatroom conversations
  across five linguistic dimensions. Experiments with eight representative LLMs across
  three families show that LLM-generated dialogues are overly idealized and lack the
  complexity of real human interactions.
---

# Social Simulations with Large Language Model Risk Utopian Illusion

## Quick Facts
- arXiv ID: 2510.21180
- Source URL: https://arxiv.org/abs/2510.21180
- Reference count: 40
- Primary result: LLM-generated dialogues lack human-like complexity, showing idealized "utopian" social patterns shaped by social desirability bias

## Executive Summary
This study systematically evaluates large language models in social simulations by analyzing multi-agent chatroom conversations across five linguistic dimensions. Experiments with eight representative LLMs reveal that LLM-generated dialogues are overly idealized and lack the complexity of real human interactions. Compared to human conversations, LLM outputs exhibit higher semantic similarity between utterances, reduced diversity of social roles, stronger persistence of early-introduced topics, and more positive sentiment. These findings suggest that LLMs simulate a "utopian" version of society shaped by social desirability bias, raising concerns for social science studies and real-world applications.

## Method Summary
The study employs a multi-agent chatroom framework where 2-32 participants engage in turn-based conversations on various topics. Agents are assigned roles (either LLM-generated or sampled from PersonaChat) and prompted to speak or stay silent based on their persona and the conversation history. The simulation runs for up to 30 turns with cyclic turn-taking. Utterances are evaluated across five dimensions: semantic similarity (using BERT embeddings), keyword persistence (via KeyBERT), sentiment (VADER scores), linguistic patterns (LIWC), and social role distribution (ISCO-08 classification). Human baselines come from PersonaChat and TopicalChat datasets.

## Key Results
- LLM conversations show significantly higher semantic similarity between utterances compared to human conversations
- Social role distribution is heavily skewed toward high-status occupations, underrepresenting elementary/agricultural workers
- Sentiment becomes increasingly positive as group size increases, showing positivity bias amplification
- Topics introduced early in conversations show stronger persistence (primacy effect) compared to human discussions

## Why This Works (Mechanism)

### Mechanism 1: Social Desirability Bias from Alignment Training
- Claim: LLMs produce overly positive, agreeable outputs because preference alignment rewards socially desirable responses over authentic ones
- Mechanism: RLHF implicitly favors responses with higher sentiment scores and greater semantic coherence with prompts. Human annotators prefer outputs that affirm rather than challenge, narrowing the expressive range toward "likability and comfort"
- Core assumption: Preference datasets are representative of what RLHF optimizes for in deployed models
- Evidence anchors:
  - [abstract] "LLMs do not faithfully reproduce genuine human behavior but instead reflect overly idealized versions of it, shaped by the social desirability bias"
  - [section 2.7] "Preferred responses showed significantly higher average positivity than dispreferred ones (p < 0.001)... preferred responses were more semantically aligned with their prompts"
  - [corpus] Weak direct evidence; corpus neighbors address affective hallucination and gambling addiction but not RLHF-driven social desirability

### Mechanism 2: Reporting Bias Inheritance from Training Corpora
- Claim: LLMs over-represent high-status social roles because textual corpora over-document prestigious occupations relative to real-world distributions
- Mechanism: LLMs learn statistical associations between occupation terms and their frequency in training data. Google Books Ngram shows professionals (52.2%) and managers (13.6%) account for >65% of occupation mentions, while elementary/agricultural workers (1.7%) are under-represented
- Core assumption: Google Books Ngram frequencies approximate the occupational distribution in LLM pretraining corpora
- Evidence anchors:
  - [section 2.1] "References to professionals (Occupation 2) account for 52.2% of all occupation-related n-grams... elementary occupations... make up only 1.7%"
  - [section 2.1] "LLMs systematically inherit reporting biases from their training corpora, giving rise to a 'social role bias'"
  - [corpus] No direct corpus evidence on training data occupational distributions

### Mechanism 3: Conversational Convergence Through Recursive Amplification
- Claim: Multi-agent LLM interactions amplify social desirability bias because each agent responds to already-idealized outputs, creating a reinforcing loop
- Mechanism: In group chat, Agent A produces a socially desirable utterance; Agent B perceives this as the conversational norm and produces an even more aligned response. Semantic similarity increases across turns (Fig. 4 shows "drift toward higher similarity" in later columns), and sentiment rises with group size (Fig. 7)
- Core assumption: The cyclic turn-taking design approximates natural multi-agent dynamics
- Evidence anchors:
  - [section 2.6] "Sentiment shows a consistent upward trend for group sizes above 5, indicating that the overall emotional tone becomes increasingly positive in larger groups"
  - [section 3] "In multi-agent settings, this bias is further amplified: each agent responds to already desirable utterances from others, reinforcing conformity and politeness through recursive feedback"
  - [corpus] "Are Human Interactions Replicable by Generative Agents?" examines hierarchical pronoun usage but not convergence dynamics

## Foundational Learning

- **Social Desirability Bias (Psychology)**
  - Why needed here: The paper frames LLM behavior through this lens; understanding the human concept helps interpret why "likability" optimization produces systematically skewed outputs
  - Quick check question: Can you distinguish between social desirability bias (self-presentation) and reporting bias (documentation frequency)?

- **RLHF and Preference Alignment**
  - Why needed here: The mechanism section links preference datasets to observed biases; understanding how scalar rewards shape output distributions is critical
  - Quick check question: What happens to output diversity when preference data systematically favors high-sentiment responses?

- **Semantic Similarity Metrics (BERT Embeddings, Cosine Similarity)**
  - Why needed here: The paper uses inter-agent and intra-agent semantic similarity as primary divergence metrics; interpreting Fig. 3-4 requires understanding what cosine similarity on BERT embeddings captures
  - Quick check question: If two utterances have cosine similarity of 0.95, what can you conclude about their semantic content?

## Architecture Onboarding

- **Component map:** Topic source → Role assignment → Multi-agent chatroom → Utterance generation → Evaluation layer
- **Critical path:** Role assignment → Topic injection → Turn-by-turn generation with history → Accumulated conversation → Five-dimensional analysis against human baselines (PersonaChat, TopicalChat)
- **Design tradeoffs:**
  - Controlled turn-taking (internal validity) vs. natural conversation dynamics (ecological validity)
  - Role diversity through PersonaChat vs. LLM-generated roles (authenticity vs. coverage)
  - Group size scaling vs. computational cost (LLaMA 3.1 extended to 32 participants locally)
- **Failure signatures:**
  1. "Hollow spiral" — conversation circles same themes without development (primacy effect)
  2. Sentiment inflation — positivity bias increases with group size
  3. Occupational distortion — <7% elementary/agricultural workers vs. 40.2% global reality
- **First 3 experiments:**
  1. Replicate the 2-agent baseline with a single model, comparing LLM-generated roles vs. PersonaChat roles on semantic similarity. Verify the gap reproduces.
  2. Introduce a contrarian agent instruction (e.g., "disagree with the previous speaker if you see assumptions") and measure whether sentiment inflation and primacy effect persist.
  3. Scale to 16 participants with explicit turn-skipping permissions; test whether reduced mandatory participation affects convergence patterns.

## Open Questions the Paper Calls Out

- **Question:** To what extent do specific alignment procedures, particularly Reinforcement Learning from Human Feedback (RLHF), causally contribute to social desirability bias compared to pre-training data alone?
- **Basis in paper:** [explicit] The authors state that while their analysis suggests alignment plays a role, "the underlying mechanisms remain correlational rather than causal." They explicitly call for future work to "disentangle how specific training or alignment procedures give rise to SDB-like tendencies" (Page 15)
- **Why unresolved:** The study compares existing models with different architectures and training mixes but does not isolate alignment as a single independent variable (e.g., by comparing a base model vs. its RLHF-tuned version on these specific social metrics)
- **What evidence would resolve it:** Controlled ablation studies tracking bias development across training stages, specifically comparing base models against their instruction-tuned and RLHF-aligned counterparts on the five linguistic dimensions defined in the paper

- **Question:** How does the "Utopian" bias in LLMs shift when simulations involve high-stakes resource constraints, competition, or collective decision-making?
- **Basis in paper:** [explicit] The authors note a key limitation: "the interaction setting was tension-free and non-goal-oriented, leaving open the question of how biases might emerge under resource constraints, competition, or collective decision-making" (Page 15)
- **Why unresolved:** The current framework relies on open-ended chatroom discussions. Without tangible goals or resource scarcity (e.g., negotiating a limited budget), the agents' propensity for politeness and agreement has not been stress-tested against realistic conflict scenarios
- **What evidence would resolve it:** Re-running the multi-agent simulation framework in game-theoretic environments (e.g., multi-agent resource allocation tasks) to measure if the positivity bias persists or if agents can adopt more realistic, self-interested conflict behaviors

- **Question:** Can enforcing structured reasoning (e.g., Chain-of-Thought) systematically mitigate social desirability bias in non-reasoning models?
- **Basis in paper:** [inferred] The paper observes that reasoning-oriented models (GPT-o3, DeepSeek-Reasoner) exhibit "weaker sentiment inflation and greater thematic diversity," suggesting that structured reasoning may "disrupt the automatic reinforcement of socially desirable patterns" (Page 15). However, this was observed as a correlation between model types rather than tested as an intervention
- **Why unresolved:** It is unclear if the reasoning capability itself reduces bias or if other architectural differences in those specific models are responsible
- **What evidence would resolve it:** An intervention study where standard LLMs are prompted to use explicit reasoning steps before generating social responses, compared against their default behavior, to see if reasoning depth forces a departure from "hollow spiral" consensus

## Limitations
- The use of proprietary LIWC software creates dependency on commercial tools that may not be universally accessible for replication
- Exact sampling parameters (temperature, top_p, max_tokens) were not specified beyond "default configurations," which vary across model families and implementations
- Analysis relies on Google Books Ngram data as a proxy for training corpus distributions, but this may not accurately represent actual pretraining data composition
- The study did not test whether observed biases persist when using adversarial or contrarian agent instructions

## Confidence
- **High Confidence**: Semantic similarity differences between LLM and human conversations; occupational distribution bias showing over-representation of high-status professions
- **Medium Confidence**: Positivity bias increasing with group size; primacy effect showing stronger keyword persistence from early turns
- **Low Confidence**: Attribution of all biases to social desirability bias from RLHF training; the mechanism is plausible but not definitively proven

## Next Checks
1. **Adversarial Agent Intervention**: Introduce an agent with explicit instructions to disagree with previous speakers and challenge assumptions. Measure whether this intervention reduces semantic similarity convergence, positivity bias amplification, and conversational repetition compared to standard agents.

2. **Balanced Corpus Training Test**: Train or fine-tune a small model on explicitly balanced occupational data (matching real-world distributions) and compare social role bias against baseline models. This would test whether reporting bias inheritance is the primary mechanism versus other factors.

3. **Model Family Architecture Comparison**: Test reasoning-optimized models (like GPT-O3 or DeepSeek-Reasoner) that underwent different training paradigms without traditional RLHF. Compare their social bias patterns to RLHF-trained models to determine if the mechanisms are training-method dependent or model-architecture dependent.