---
ver: rpa2
title: On the Hardness of Bandit Learning
arxiv_id: '2506.14746'
source_url: https://arxiv.org/abs/2506.14746
tags:
- action
- algorithm
- such
- function
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the fundamental limitations of bandit
  learning by addressing two key questions: (1) which reward function classes are
  learnable, and (2) how can they be learned efficiently? The authors demonstrate
  that unlike classical PAC learning, no combinatorial dimension can fully characterize
  bandit learnability, even for finite classes, using a simple argument based on the
  finite-character property.'
---

# On the Hardness of Bandit Learning
## Quick Facts
- arXiv ID: 2506.14746
- Source URL: https://arxiv.org/abs/2506.14746
- Reference count: 40
- Primary result: No combinatorial dimension can fully characterize bandit learnability, and identifying optimal actions can be computationally hard despite requiring only two queries

## Executive Summary
This paper investigates fundamental limitations in bandit learning, revealing that bandit learnability fundamentally differs from classical PAC learning. The authors demonstrate that no combinatorial dimension can fully characterize bandit learnability, even for finite function classes, using a simple argument based on the finite-character property. They construct a reward function class where identifying the optimal action requires at most two queries but is computationally intractable unless RP equals NP. The paper also explores how observation noise affects learnability, showing that certain classes have constant query complexity in noise-free settings but become unlearnable with any positive noise variance.

## Method Summary
The authors employ complexity-theoretic arguments and combinatorial analysis to establish their results. They construct specific reward function classes and analyze their properties under different noise conditions. The main approach involves demonstrating that certain bandit problems require solving computationally hard problems, establishing connections between bandit learning and complexity classes like RP and NP. They also analyze the relationship between query complexity and regret, showing that minimizing one necessarily leads to linear growth in the other.

## Key Results
- No combinatorial dimension can fully characterize bandit learnability, even for finite classes
- A reward function class exists where optimal action identification requires at most two queries but is computationally hard unless RP=NP
- Certain classes have constant query complexity in noise-free settings but become unlearnable with any positive noise variance
- Achieving optimal query complexity necessarily incurs linear regret, proving no algorithm can minimize both objectives simultaneously

## Why This Works (Mechanism)
The paper's results stem from the fundamental difference between bandit learning and classical PAC learning. In bandit settings, the learner only observes rewards for chosen actions, creating an exploration-exploitation tradeoff that doesn't exist in the supervised learning setting. This partial observability, combined with computational complexity considerations, creates scenarios where learning becomes intractable despite seemingly simple query requirements.

## Foundational Learning
- **Finite-character property**: A combinatorial property used to prove that no universal dimension can characterize bandit learnability. Needed to establish fundamental limitations on what can be learned efficiently.
- **RP vs NP complexity classes**: Understanding the relationship between randomized polynomial time and nondeterministic polynomial time. Critical for establishing computational hardness results.
- **Query complexity**: The number of queries needed to identify optimal actions. Essential for understanding the fundamental limits of bandit learning.
- **Regret bounds**: Measures of how much worse an algorithm performs compared to always choosing the optimal action. Important for evaluating algorithm performance.
- **Observation noise models**: Different noise assumptions that affect learnability. Crucial for understanding practical limitations of bandit algorithms.

## Architecture Onboarding
- **Component map**: Reward function class -> Query complexity analysis -> Computational hardness proof -> Regret bounds
- **Critical path**: The chain from constructing reward function classes through proving hardness results to establishing regret-complexity tradeoffs
- **Design tradeoffs**: The fundamental tradeoff between query complexity and regret - minimizing one necessarily increases the other linearly
- **Failure signatures**: Classes where optimal action requires few queries but is computationally intractable; noise-free learnability that disappears with any positive noise
- **First experiments**:
  1. Verify the complexity-theoretic reduction showing optimal action identification is NP-hard under RP≠NP assumption
  2. Test observation noise claims with alternative noise models
  3. Empirically validate the query complexity vs regret tradeoff through simulations

## Open Questions the Paper Calls Out
None

## Limitations
- Central hardness claims rely on unproven assumption that RP ≠ NP
- Construction of computationally hard reward function class needs verification of combinatorial arguments
- Noise model assumptions may not generalize to all practical settings
- Complexity-theoretic reductions may not capture all possible algorithmic approaches

## Confidence
High confidence in: The fundamental difference between bandit learnability and classical PAC learnability being characterized by the absence of a universal combinatorial dimension.

Medium confidence in: The hardness results based on complexity-theoretic assumptions and the specific construction of the reward function class demonstrating computational intractability.

Low confidence in: The universality of the observation noise results and their implications for all bandit learning scenarios.

## Next Checks
1. Verify the correctness of the complexity-theoretic reduction showing that identifying the optimal action is NP-hard under the assumption that RP ≠ NP, checking whether all possible algorithmic strategies are properly accounted for.

2. Test the observation noise claims by constructing alternative noise models and verifying whether the constant query complexity property holds or fails under these variations.

3. Examine the relationship between query complexity and regret bounds through empirical simulations across different reward function classes to validate the theoretical claim that optimal query complexity necessarily incurs linear regret.