---
ver: rpa2
title: 'TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER'
arxiv_id: '2503.22714'
source_url: https://arxiv.org/abs/2503.22714
tags:
- tridis
- test
- script
- lines
- medieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TRIDIS, a comprehensive corpus of medieval
  and early modern manuscripts designed to advance Handwritten Text Recognition (HTR)
  and Named Entity Recognition (NER). TRIDIS aggregates multiple open-source sub-corpora,
  ensuring rich metadata, diverse languages, and challenging document types.
---

# TRIDIS: A Comprehensive Medieval and Early Modern Corpus for HTR and NER

## Quick Facts
- **arXiv ID:** 2503.22714
- **Source URL:** https://arxiv.org/abs/2503.22714
- **Reference count:** 22
- **One-line primary result:** TRIDIS introduces an outlier-based test split strategy for HTR evaluation, showing a 2.2% increase in CER over random splits.

## Executive Summary
TRIDIS is a comprehensive corpus of medieval and early modern manuscripts designed to advance Handwritten Text Recognition (HTR) and Named Entity Recognition (NER). The corpus aggregates multiple open-source sub-corpora with rich metadata, diverse languages, and challenging document types. A key contribution is the introduction of an outlier-driven test split strategy, which identifies and isolates challenging examples based on their deviation from the corpus's central tendency in a joint embedding space. This approach provides a more realistic evaluation of HTR model robustness compared to traditional random splits. Preliminary baseline experiments using TrOCR and MiniCPM-Llama3-V 2.5 demonstrate that the outlier-based test set significantly increases difficulty, with higher Character Error Rates (CER) and Word Error Rates (WER) observed compared to random splits. TRIDIS is publicly available and aims to stimulate joint robust HTR and NER research across medieval and early modern textual heritage.

## Method Summary
TRIDIS unifies multiple legacy medieval and early modern manuscript datasets into a single corpus of ~200k lines with semi-diplomatic transcriptions. The corpus uses a novel outlier-based test split strategy that identifies challenging examples by computing joint image-text embeddings, calculating a centroid, and selecting the 5% of samples furthest from this center. Baseline HTR experiments use TrOCR and MiniCPM-Llama3-V 2.5 architectures, with the outlier split showing higher CER and WER than random splits, demonstrating more realistic evaluation of model generalization.

## Key Results
- Outlier-based test split strategy increases CER from 9.1% to 11.3% compared to random splits
- TRIDIS corpus contains ~200k lines and 2M tokens with rich metadata including language, century, and script family
- Semi-diplomatic transcription rules (abbreviation expansion, normalization, punctuation) facilitate downstream NLP while challenging HTR models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: An outlier-based test split, defined by distance in a joint image-text embedding space, creates a more rigorous and realistic benchmark for model generalization than random splits.
- Mechanism: The authors compute a joint embedding for each line by concatenating mean-pooled outputs from a vision encoder and a text encoder. They calculate a centroid (median) of these embeddings and measure the Euclidean distance of each sample from this central point. The 5% of samples furthest from the centroid are selected as outliers to form the test set. This isolates data points that are atypical in both visual features (e.g., script style, layout) and textual content (e.g., vocabulary).
- Core assumption: Assumption: Euclidean distance from the corpus centroid in this specific joint embedding space correlates directly with transcription difficulty and represents "out-of-domain" characteristics likely to be encountered in new historical document collections.
- Evidence anchors:
  - [abstract]: "...novel outlier-based test split strategy, using joint image-text embeddings, identifies challenging out-of-domain examples for realistic evaluation."
  - [section]: "Lines with the largest Euclidean distances from this centroid (the top 5%) are labeled as 'outliers' and form the test partition." (Section IV.A)
  - [section]: Baseline experiments show TrOCR CER increases from 9.1% on a random split to 11.3% on the outlier split (Table II).
  - [corpus]: The corpus provides the scale (~200k lines) and diversity necessary for this distance-based split to be statistically meaningful.
- Break condition: This will fail if the embedding space does not capture features relevant to HTR difficulty (e.g., if outliers are just noisy images rather than challenging scripts) or if the test set becomes so different from the training distribution that the task is impossible rather than just difficult.

### Mechanism 2
- Claim: Using a semi-diplomatic transcription standard that silently expands abbreviations and modernizes punctuation improves the utility of HTR output for downstream NLP tasks.
- Mechanism: The transcription guidelines normalize the target text. Abbreviations are expanded to full word forms (e.g., "dms" → "dominus"), allographic variants are collapsed (e.g., long-s to short-s), and punctuation is modernized. This aligns the historical text closer to modern orthographic conventions, reducing the vocabulary gap between the historical manuscript content and modern NLP models (like those used for NER), thereby facilitating easier integration with standard NLP pipelines.
- Core assumption: Assumption: The interpretative layer introduced by editors (e.g., deciding how to expand an abbreviation or punctuate a sentence) is consistent and accurate, and that this modernized format is more valuable for downstream tasks than a strictly paleographic transcription.
- Evidence anchors:
  - [abstract]: "...its semi-diplomatic transcription rules (expansion, normalization, punctuation)..."
  - [section]: "This transcription mode has the advantage of facilitating usage of modern NLP pipelines with the content while introducing an interpretative layer that challenge HTR models trained on such data." (Section II)
  - [corpus]: N/A (This mechanism is a methodological choice applied to the corpus, not an intrinsic property of the source documents).
- Break condition: This approach is inappropriate for research focused on paleographic analysis, the study of abbreviation practices, or any task requiring strict adherence to the original manuscript's literal glyphs and spacing.

### Mechanism 3
- Claim: Aggregating diverse sub-corpora under a unified schema with rich metadata (language, script family, century) enables more robust model training and targeted failure analysis.
- Mechanism: TRIDIS unifies multiple legacy datasets into a single Parquet file with a standardized metadata schema. This allows a model to train on a much broader distribution of scripts (Cursiva, Textualis, etc.), languages, and time periods than any single sub-corpus could provide. The structured metadata then allows researchers to slice performance data, identifying specific weaknesses (e.g., poor performance on 12th-century Textualis or 16th-century German).
- Core assumption: Assumption: Increasing the scale and diversity of the training data in this manner will induce better cross-domain generalization, and the metadata is accurate enough to support meaningful sub-group analysis.
- Evidence anchors:
  - [abstract]: "It unifies multiple legacy sub-corpora... and incorporates large metadata descriptions."
  - [section]: "TRIDIS merges them under a uniform data model that indicates the rules and transcription style in the metadata." (Section II.A)
  - [corpus]: Figure 1 shows the distribution of languages, centuries, and script families; Table I details the sub-corpora composition.
- Break condition: The mechanism weakens if metadata is noisy or inconsistent across the aggregated sub-corpora, leading to incorrect stratification. It also assumes the model has sufficient capacity to learn the diverse patterns without the different data sources confusing the learning process (negative transfer).

## Foundational Learning

- Concept: **Handwritten Text Recognition (HTR) vs. OCR**
  - Why needed here: Understanding that HTR deals with the high variability of human handwriting (script styles, ligatures, abbreviations) compared to the more uniform glyphs in printed OCR is fundamental to grasping why this corpus and its challenges exist.
  - Quick check question: What are three key factors that make historical handwritten documents more difficult for models to transcribe than modern printed text?

- Concept: **Embedding Space & Centroids**
  - Why needed here: The core innovation of this paper's evaluation strategy relies on understanding what an "embedding" is (a vector representation of data) and what a "centroid" represents (the average or center of a cluster of data points).
  - Quick check question: In the context of this paper, what does it mean for a document line to be "far from the centroid" in the joint embedding space?

- Concept: **Semi-Diplomatic Transcription**
  - Why needed here: The target output of the models is not a literal character-by-character copy of the manuscript but a normalized, interpreted version. Understanding this distinction is critical for data preparation and model evaluation.
  - Quick check question: According to the paper, how is the abbreviation for "dominus" handled in a semi-diplomatic transcription, and what is the trade-off of this choice?

## Architecture Onboarding

- Component map: The data pipeline begins with a unified Parquet file containing line images (RGB), UTF-8 text, and metadata. For the outlier split strategy, the pipeline has two encoder branches: a Vision Encoder (for image features) and a Text Encoder (for text features), whose outputs are concatenated to form a joint embedding. For the HTR task itself, the architecture is a standard line-level encoder-decoder (e.g., TrOCR's ViT encoder + RoBERTa decoder).

- Critical path: The most critical path for a new engineer is understanding the data loading and the outlier split logic. The outlier selection (`Algorithm 1`) determines the test set, which directly impacts all reported performance metrics. An engineer must be able to reproduce this split to validate baseline results.

- Design tradeoffs: The main tradeoff is between **evaluation realism** and **model feasibility**. The outlier split provides a more realistic ("pessimistic") view of model performance but makes the benchmark significantly harder. Another tradeoff is in the transcription style: semi-diplomatic transcriptions make downstream NLP easier but may obscure paleographic details and increase the complexity of the HTR task itself (learning to expand abbreviations).

- Failure signatures: A model may fail with a low CER on a random test split but a high CER on the outlier split, indicating poor generalization. Failure analysis (Section IV.C) identifies specific signatures: (1) **Names**: high error rates on lines with exclusive proper names/abbreviations; (2) **Physical Defects**: failures on faded, stained, or transparent pages; (3) **Long/Complex Lines**: failures on lines with >20 words or variable geometry. These should be the first places to check during error analysis.

- First 3 experiments:
  1. **Establish a Random-Split Baseline:** Train a standard HTR model (like TrOCR-base) on the TRIDIS training set and evaluate on a *randomly* selected test set. Record CER, WER. This establishes an "optimistic" performance upper-bound.
  2. **Implement and Evaluate on Outlier Split:** Use the provided pre-computed outlier test set (or re-implement `Algorithm 1` to generate it from the embeddings). Evaluate the same model from experiment 1 on this split. Quantify the performance drop (the "domain gap") to understand the model's generalization limits.
  3. **Targeted Error Analysis:** Use the metadata to slice the results from experiment 2. Calculate and compare the CER specifically for the different failure signatures (e.g., compare CER on lines tagged with 'Praegotica' vs 'Cursiva', or lines from the 12th century vs 15th century). This identifies which "outlier" features the model struggles with most.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted data augmentation strategies for underrepresented script families (e.g., Carolingian minuscule, Praegotica) significantly reduce the performance gap between random and outlier-based test splits?
- Basis in paper: [explicit] The authors state that "analyzing these outliers provides insights into scribal diversity, suggesting targeted augmentation strategies—e.g., for underrepresented scripts—or specialized normalization approaches for complex abbreviations."
- Why unresolved: The paper identifies underrepresented scripts as problematic but does not implement or evaluate any augmentation techniques to address this.
- What evidence would resolve it: Experiments comparing baseline models trained on TRIDIS with and without augmented data for rare script families, measuring CER/WER changes on outlier subsets.

### Open Question 2
- Question: How does the choice of joint embedding architecture (vision and text encoders) affect the composition and difficulty of the outlier-based test partition?
- Basis in paper: [inferred] The outlier detection uses mean-pooled outputs from image and text encoders, but the paper does not compare alternative embedding strategies or analyze sensitivity to encoder choice.
- Why unresolved: Only one embedding method is described; the impact of encoder selection on outlier identification remains unexplored.
- What evidence would resolve it: Ablation studies using different encoder architectures (e.g., CLIP vs. domain-specific HTR encoders) and comparing resulting outlier distributions and model performance.

### Open Question 3
- Question: To what extent does semi-diplomatic transcription normalization (abbreviation expansion, allograph collapsing, modern punctuation) hinder or help downstream NER performance compared to diplomatic transcriptions?
- Basis in paper: [inferred] The paper notes that semi-diplomatic transcription "facilitates usage of modern NLP pipelines" but also "introduces an interpretative layer that challenge HTR models." NER annotations are available only in the Alcar-HOME sub-corpus.
- Why unresolved: No experiments compare NER performance under different transcription paradigms, and only one sub-corpus has NER labels.
- What evidence would resolve it: Comparative NER experiments on documents transcribed in both semi-diplomatic and diplomatic styles, measuring precision/recall differences for entity types.

### Open Question 4
- Question: Can multimodal models like MiniCPM leverage the rich metadata (century, script family, language) more effectively than encoder-decoder architectures like TrOCR for out-of-domain generalization?
- Basis in paper: [explicit] The authors note that "many downstream tasks require robust multimodal representations" and that the corpus "becomes instrumental for developing advanced HTR modules," but baseline experiments do not incorporate metadata during training.
- Why unresolved: The metadata exists but was not used as conditioning input in the reported baseline experiments.
- What evidence would resolve it: Experiments where metadata is injected as conditional prompts or features, comparing generalization to the outlier test set against metadata-agnostic baselines.

## Limitations

- The effectiveness of the outlier split depends entirely on the chosen embedding space adequately capturing features that make lines difficult for HTR models
- Semi-diplomatic transcription introduces an interpretative layer that may not be consistent across the entire corpus and makes the data unsuitable for paleographic analysis
- The full impact of the corpus on NER performance is unknown as baseline NER experiments are not provided

## Confidence

- **High Confidence**: The corpus is a large, diverse aggregation of open-source medieval and early modern manuscript data with unified metadata. The description of the sub-corpora, transcription rules, and metadata schema is clear and verifiable. The reported performance drop (CER from 9.1% to 11.3%) on the outlier split is a reproducible, measurable result.

- **Medium Confidence**: The claim that the outlier-based split provides a "more realistic" evaluation of model generalization is well-reasoned and mechanistically sound, but its general applicability depends on the robustness of the embedding space. The assertion that semi-diplomatic transcription "facilitates" downstream NLP is logical but not empirically proven within the paper.

- **Low Confidence**: The full impact of the corpus on NER performance is an unknown. The paper sets the stage for this research but does not provide NER baseline results or a detailed analysis of how the transcription rules affect entity recognition accuracy.

## Next Checks

1. **Embed and Verify the Outlier Split**: Re-implement the joint embedding calculation (using CLIP ViT-B/32 for images and RoBERTa-base for text as reasonable defaults) and the centroid-based outlier selection algorithm. Verify that the pre-computed test set in the HuggingFace dataset matches the one generated by your implementation. Analyze the distribution of distances for the test set to confirm they are indeed the farthest 5%.

2. **Evaluate a Different HTR Architecture**: Train a different, well-established HTR model (e.g., Calamari or a simple CNN-LSTM-CTC model) on the TRIDIS training data. Evaluate it on both the random and outlier test splits. This will test if the performance drop is consistent across architectures and not an artifact of the TrOCR model's specific design.

3. **Perform a Targeted NER Experiment**: Using the provided NER annotations, train a named entity recognition model (e.g., a BERT-based sequence tagger) on the semi-diplomatic transcriptions. Compare its performance to a baseline trained on a modern, non-historical text corpus. This will provide empirical evidence for the paper's claim about the transcriptions' utility for downstream NLP tasks.