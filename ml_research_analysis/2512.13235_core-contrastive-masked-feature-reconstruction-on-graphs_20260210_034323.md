---
ver: rpa2
title: 'CORE: Contrastive Masked Feature Reconstruction on Graphs'
arxiv_id: '2512.13235'
source_url: https://arxiv.org/abs/2512.13235
tags:
- graph
- masked
- learning
- nodes
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges generative and contrastive approaches in self-supervised
  graph learning by showing their theoretical convergence under specific conditions.
  The authors propose Contrastive Masked Feature Reconstruction (CORE), which integrates
  contrastive learning into masked feature reconstruction by using masked nodes as
  both positive and negative samples.
---

# CORE: Contrastive Masked Feature Reconstruction on Graphs

## Quick Facts
- arXiv ID: 2512.13235
- Source URL: https://arxiv.org/abs/2512.13235
- Authors: Jianyuan Bo; Yuan Fang
- Reference count: 40
- Key outcome: CORE achieves up to 3.76% improvement on graph classification and 3.72% on node classification compared to GraphMAE/GraphMAE2

## Executive Summary
This paper bridges generative and contrastive approaches in self-supervised graph learning by showing their theoretical convergence under specific conditions. The authors propose Contrastive Masked Feature Reconstruction (CORE), which integrates contrastive learning into masked feature reconstruction by using masked nodes as both positive and negative samples. This eliminates the need for complex graph augmentations while improving model discrimination. CORE outperforms GraphMAE and GraphMAE2 by up to 3.76% on graph classification and up to 3.72% on node classification tasks, demonstrating superior performance across multiple benchmarks.

## Method Summary
CORE combines masked feature reconstruction with contrastive learning by treating masked nodes as anchors. During training, a subset of nodes is masked and replaced with learnable tokens. The encoder-decoder architecture (GAT for node tasks, GIN for graph tasks) reconstructs features for these masked nodes. Positive pairs are formed between reconstructed features and original raw features of masked nodes, while negative pairs are created by sampling from other masked node reconstructions. The contrastive loss encourages the model to distinguish between true positives and negatives while maintaining the reconstruction objective. This unified framework theoretically converges to both generative and contrastive objectives under specific conditions.

## Key Results
- CORE outperforms GraphMAE and GraphMAE2 by up to 3.76% on graph classification tasks
- CORE achieves up to 3.72% improvement on node classification tasks
- Temperature parameter T critically affects performance, with optimal range around 0.1-1.0

## Why This Works (Mechanism)
CORE leverages the theoretical equivalence between masked feature reconstruction and contrastive learning objectives. By using masked nodes as both anchors and negative samples, the framework creates informative contrastive pairs without requiring explicit graph augmentations. The learnable mask tokens allow the model to adaptively represent missing information, while the contrastive component encourages the encoder to capture discriminative features. This dual objective simultaneously learns to reconstruct masked features accurately and distinguish between different node representations, resulting in more robust and discriminative embeddings.

## Foundational Learning
- **Graph neural networks**: Why needed - CORE uses GNN encoders (GAT/GIN) to learn node/graph representations; Quick check - Understand how message passing aggregates neighbor information
- **Masked language modeling**: Why needed - CORE masks node features similar to BERT; Quick check - Verify understanding of how masking creates pretext tasks
- **Contrastive learning**: Why needed - CORE uses contrastive loss to distinguish positive from negative pairs; Quick check - Understand InfoNCE loss formulation and temperature scaling
- **Self-supervised learning**: Why needed - CORE learns representations without labels; Quick check - Know difference between generative and contrastive self-supervised approaches
- **Graph classification vs node classification**: Why needed - CORE is evaluated on both tasks with different architectures; Quick check - Understand when to use GAT vs GIN

## Architecture Onboarding

**Component Map**: Graph -> GNN Encoder -> Masking Layer -> Learnable Tokens -> Decoder -> Reconstruction

**Critical Path**: Input graph → GNN encoder → masked node embeddings → decoder → reconstructed features → contrastive loss computation

**Design Tradeoffs**: 
- Masking ratio vs reconstruction quality: Higher masking improves contrastive signal but may hurt reconstruction
- Temperature T: Lower values increase contrastive pressure but risk instability
- Negative sample size: More negatives improve discrimination but increase computation

**Failure Signatures**:
- Very low temperature (T < 1e-4) causes unstable training and degraded performance
- Using all nodes (not just masked) as anchors hurts contextual learning
- Insufficient negative samples lead to suboptimal discrimination

**First Experiments**:
1. Verify masking implementation by checking that masked nodes are correctly replaced with learnable tokens
2. Test CORE loss computation with synthetic positive/negative pairs to ensure contrastive objective works
3. Run baseline GraphMAE training to establish performance reference point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dimensionality reduction techniques like Principal Component Analysis (PCA) effectively resolve the performance degradation of CORE on graphs with high-dimensional raw features?
- Basis in paper: The authors note CORE underperforms on CS and Physics datasets, attributing this to large raw feature dimensions (6,805 and 8,415), and suggest "preprocessing techniques like principal component analysis could help address this issue."
- Why unresolved: The paper identifies the limitation but does not experimentally validate whether PCA or other feature reduction methods would restore the performance gains seen in other datasets.
- Evidence to resolve: Empirical results on the CS and Physics datasets comparing the performance of CORE with and without PCA preprocessing to determine if the overfitting issue is mitigated.

### Open Question 2
- Question: Can the theoretical convergence of MFR and GCL objectives be maintained for heterogeneous or dynamic graphs where node types and temporal information are crucial?
- Basis in paper: The theoretical analysis (Theorem 4.1) and empirical validation rely on static, homogenous graph structures and adjacency matrices.
- Why unresolved: The current masking strategy and theoretical assumptions do not account for multiple node/edge types or temporal evolution, leaving the generalizability of the CORE framework to complex networks unexplored.
- Evidence to resolve: An extension of the theoretical proof to typed graphs and experimental results on heterogeneous graph benchmarks (e.g., HGB) showing whether specific masking of semantic types improves performance.

### Open Question 3
- Question: Would incorporating hard negative mining strategies for masked nodes improve the discriminative capability of CORE compared to the current random selection method?
- Basis in paper: The framework selects negative samples randomly from the set of masked nodes, a choice shown to be effective, but the authors do not explore whether selecting more informative "hard" negatives could further optimize the learning process.
- Why unresolved: While the paper analyzes the effect of the number of negative samples, it leaves the quality or selection strategy of these negatives as an open design space.
- Evidence to resolve: Ablation studies comparing the random negative sampling strategy against hard negative mining approaches on classification benchmarks.

## Limitations
- Performance degrades on graphs with very high-dimensional raw features (6,805-8,415 dimensions) due to potential overfitting
- The framework is designed for static, homogeneous graphs and may not generalize well to heterogeneous or dynamic networks
- Computational cost increases with the number of negative samples, though the paper claims efficiency

## Confidence

**Confidence Labels:**
- CORE theoretical framework and loss formulation: **High**
- Reported performance improvements over GraphMAE/GraphMAE2: **Medium**
- Claims about computational efficiency on large graphs: **Low** (insufficient implementation details provided)

## Next Checks
1. Reproduce core experimental results on Cora/Citeseer/PubMed node classification using the linear evaluation protocol, verifying that CORE consistently outperforms GraphMAE across multiple random seeds
2. Conduct ablation studies to confirm that using only masked nodes as anchors (not all nodes) is critical for performance gains
3. Systematically vary the temperature parameter T and negative sample size |M| to identify optimal ranges and verify the claimed sensitivity patterns shown in Figures 2-3