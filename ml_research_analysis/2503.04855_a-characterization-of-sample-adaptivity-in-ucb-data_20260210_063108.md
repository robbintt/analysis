---
ver: rpa2
title: A characterization of sample adaptivity in UCB data
arxiv_id: '2503.04855'
source_url: https://arxiv.org/abs/2503.04855
tags:
- sample
- lemma
- bandit
- mean
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes a novel joint central limit theorem (CLT)
  characterizing the correlation structure between the number of pulls and sample
  mean rewards in two-armed stochastic bandit environments under generalized UCB algorithms.
  The result reveals a non-standard CLT for the number of pulls that smoothly interpolates
  between standard and slow-concentration regimes depending on the arm gap, with typical
  deviation scaling as $n2^ T/f(T)$ rather than the standard $\sqrt{n2^ T}$.
---

# A characterization of sample adaptivity in UCB data

## Quick Facts
- arXiv ID: 2503.04855
- Source URL: https://arxiv.org/abs/2503.04855
- Authors: Yilun Chen; Jiaqi Lu
- Reference count: 40
- Key outcome: Establishes a joint CLT characterizing the correlation between pulls and sample means in two-armed stochastic bandits under generalized UCB algorithms, revealing non-standard concentration rates and enabling bias characterization.

## Executive Summary
This paper establishes a novel joint central limit theorem (CLT) characterizing the correlation structure between the number of pulls and sample mean rewards in two-armed stochastic bandit environments under generalized UCB algorithms. The result reveals a non-standard CLT for the number of pulls that smoothly interpolates between standard and slow-concentration regimes depending on the arm gap, with typical deviation scaling as $n_2^* T/f(T)$ rather than the standard $\sqrt{n_2^* T}$. This finding also yields a non-standard CLT for pseudo-regret with typical scaling $n_2^* T \Delta_T$ and deviation $\Theta(\sqrt{n_2^* T}/f(T))$. The correlation structure enables a heuristic derivation of sample bias up to leading order, showing it vanishes at a slow rate of $1/\sqrt{\log T}$ after normalization in small-gap regimes.

## Method Summary
The analysis employs a fluid approximation approach combined with perturbation analysis around the fluid limit to establish the joint CLT. The framework characterizes the number of pulls through a martingale difference sequence and leverages stochastic approximation theory to handle the correlation structure. The perturbation analysis allows tracking how small deviations from the fluid approximation affect the asymptotic behavior, enabling the characterization of both the number of pulls and sample mean rewards jointly.

## Key Results
- Establishes a joint CLT for the number of pulls and sample mean rewards under generalized UCB algorithms
- Reveals non-standard concentration rates for the number of pulls scaling as $n_2^* T/f(T)$ instead of $\sqrt{n_2^* T}$
- Derives a non-standard CLT for pseudo-regret with deviation $\Theta(\sqrt{n_2^* T}/f(T))$
- Characterizes sample bias vanishing at rate $1/\sqrt{\log T}$ through correlation structure analysis

## Why This Works (Mechanism)
The joint CLT emerges from the interplay between the adaptive sampling strategy and the reward uncertainty in UCB algorithms. The fluid approximation captures the deterministic trend of the system, while the perturbation analysis quantifies the stochastic deviations. The correlation between pulls and sample means arises because the UCB algorithm uses the sample means to decide which arm to pull, creating a feedback loop that the joint CLT captures. The non-standard concentration rates reflect the adaptive nature of the algorithm, which slows down exploration as it becomes more confident about the better arm.

## Foundational Learning

Stochastic Approximation Theory: Needed to analyze the convergence of the algorithm's state to its fluid limit and characterize the joint behavior of multiple quantities. Quick check: Verify the stability of the ODE associated with the fluid limit.

Martingale Central Limit Theorem: Required to establish the asymptotic normality of the deviation terms from the fluid approximation. Quick check: Confirm the Lindeberg condition for the martingale difference sequence.

Fluid Approximation: Essential for capturing the deterministic backbone of the adaptive sampling process in bandit algorithms. Quick check: Validate the fluid limit by showing convergence of the scaled state process.

Perturbation Analysis: Needed to quantify how small stochastic deviations affect the system's behavior and establish the joint CLT. Quick check: Verify that the perturbation remains bounded in the appropriate topology.

## Architecture Onboarding

Component Map: Fluid approximation -> Martingale CLT -> Joint CLT for pulls and means -> Bias characterization
Critical Path: The key technical flow is establishing the fluid limit, then applying martingale CLT to the deviation process, and finally using the joint CLT to characterize the correlation structure and derive bias properties.
Design Tradeoffs: The analysis prioritizes mathematical rigor over generality, focusing on two-armed stochastic bandits with stationary rewards rather than extending to more complex settings.
Failure Signatures: The perturbation analysis may break down if the gap-dependent concentration rate $f(T)$ is not well-behaved or if the fluid approximation does not capture the algorithm's dynamics accurately.
First Experiments:
1. Verify the predicted non-standard deviation scaling through numerical simulations across different gap regimes
2. Test the sensitivity of the joint CLT to variations in the UCB algorithm (different confidence bounds)
3. Empirically validate the $1/\sqrt{\log T}$ bias vanishing rate through Monte Carlo experiments

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis framework may have limited applicability to non-UCB algorithms or structured bandit settings
- The characterization assumes stationary reward distributions and may not extend to non-stationary or heavy-tailed environments
- The heuristic derivation of sample bias lacks rigorous error bounds and may not capture higher-order correction terms accurately
- The result focuses on the asymptotic regime $T \to \infty$, and finite-sample behavior is not addressed

## Confidence

High: The main technical results establishing the non-standard CLT for the number of pulls and pseudo-regret are mathematically rigorous within the stated assumptions.
Medium: The characterization of the correlation structure between pulls and sample means, while theoretically sound, may require further empirical validation to confirm its practical relevance.
Low: The heuristic derivation of sample bias and its vanishing rate of $1/\sqrt{\log T}$ lacks rigorous justification and may be sensitive to higher-order effects.

## Next Checks

1. Validate the theoretical correlation structure through numerical simulations across different gap regimes and algorithm variants to confirm the predicted non-standard deviation scaling.
2. Extend the perturbation analysis framework to non-UCB algorithms such as Thompson Sampling or explore its applicability to structured bandit problems with context or Markovian rewards.
3. Derive and verify explicit error bounds for the heuristic sample bias characterization, including higher-order correction terms, through both theoretical analysis and empirical testing.