---
ver: rpa2
title: Session-Level Spoken Language Assessment with a Multimodal Foundation Model
  via Multi-Target Learning
arxiv_id: '2509.16025'
source_url: https://arxiv.org/abs/2509.16025
tags:
- language
- speech
- learning
- overall
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal foundation model for session-level
  spoken language assessment (SLA) using multi-target learning. The model jointly
  predicts holistic and part-level proficiency scores from the entire L2 speech session,
  integrating acoustic and semantic evidence in a single pass.
---

# Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning

## Quick Facts
- arXiv ID: 2509.16025
- Source URL: https://arxiv.org/abs/2509.16025
- Reference count: 0
- Primary result: State-of-the-art RMSE of 0.360 on Speak & Improve benchmark for session-level SLA

## Executive Summary
This paper presents a multimodal foundation model for session-level spoken language assessment (SLA) using multi-target learning. The model jointly predicts holistic and part-level proficiency scores from the entire L2 speech session, integrating acoustic and semantic evidence in a single pass. A Whisper-based Acoustic Proficiency Prior is introduced to enhance calibration with non-lexical cues like prosody and fluency. Experiments on the Speak & Improve benchmark show the proposed approach achieves state-of-the-art performance (RMSE 0.360), outperforming ensemble and cascaded baselines, especially on long and multi-response parts. The method improves discourse-level reasoning, reduces error propagation, and offers a compact, deployable solution for CALL applications.

## Method Summary
The method employs a Phi-4-multimodal-instruct backbone (frozen) with LoRA adapters and a frozen Whisper-large-v3 encoder for acoustic proficiency priors. The model processes entire L2 speech sessions as dialog-style sequences with interleaved audio placeholders and text instructions. It jointly predicts five scores (P1, P3, P4, P5, and overall) using a unified 5-output regression head trained with MSE loss. The Whisper encoder provides an 8-class acoustic proficiency prior prepended to the sequence as a calibration signal. Training uses AdamW optimizer with specific hyperparameters, and the model achieves state-of-the-art performance on the Speak & Improve benchmark.

## Key Results
- Achieves state-of-the-art RMSE of 0.360 on Speak & Improve benchmark
- Outperforms ensemble and cascaded baselines by more than 12% relative
- Shows consistent improvements across all parts, with clear gains on multi-response parts (P1/P5)
- Demonstrates effective integration of delivery with content and cross-part information transfer

## Why This Works (Mechanism)

### Mechanism 1: Session-Level Attention Aggregation
Processing all session responses in a single forward pass enables cross-utterance reasoning that utterance-level models cannot achieve. The model constructs a unified multimodal sequence with interleaved audio placeholders and text instructions. Self-attention operates across all responses, allowing the model to weight and integrate evidence from multiple parts when predicting each score. Core assumption: Proficiency evidence is distributed across responses; aggregating at the model level (vs. post-hoc averaging) captures dependencies that external fusion misses.

### Mechanism 2: Multi-Target Learning for Shared Representations
Joint prediction of part-level and holistic scores induces representations that generalize across tasks. A single five-output regression head is trained with a unified MSE loss over all targets. Shared backbone parameters must encode features useful for all predictions, encouraging transfer. Core assumption: Part-level proficiency shares underlying linguistic and acoustic evidence.

### Mechanism 3: Acoustic Proficiency Prior as Calibration Signal
A Whisper-derived prior injects non-lexical proficiency cues (prosody, fluency) that complement the MLLM's semantic bias. A frozen Whisper-large-v3 encoder produces pooled hidden states, passed through an MLP to yield an 8-class probability vector. This is projected to a token embedding prepended to the multimodal sequence, providing an acoustic "hint" before the model processes audio. Core assumption: Whisper encoder representations encode proficiency-relevant prosodic patterns despite being trained for ASR.

## Foundational Learning

- **Multi-target regression with shared backbone**
  - Why needed here: The model predicts 5 correlated scores simultaneously; understanding how shared representations emerge from joint optimization is essential.
  - Quick check question: Can you explain why training multiple regression heads together might outperform training separate models for each target?

- **Attention over long multimodal sequences**
  - Why needed here: Sessions span multiple audio segments; the model must attend across ~128k token context windows.
  - Quick check question: What is the computational bottleneck for self-attention over long sequences, and how does FlashAttention address it?

- **PEFT/LoRA for foundation model adaptation**
  - Why needed here: Only LoRA adapters and the regression head are trained; the backbone remains frozen.
  - Quick check question: How does low-rank adaptation reduce trainable parameters while preserving model capacity?

## Architecture Onboarding

- **Component map:**
  - Input: Dialog-style sequence with interleaved text prompts + `<|audio_i|>` placeholders
  - Audio pathway: 16kHz waveforms → speech adapter → token embeddings
  - APP branch: Whisper-large-v3 encoder (frozen) → mean pooling → 2-layer MLP → softmax → projector → prior token
  - Backbone: Phi-4-Multimodal (frozen) + LoRA adapters (trainable)
  - Output: 5-head linear regression layer → score vector [P1, P3, P4, P5, overall]

- **Critical path:**
  1. Format session as dialog sequence with audio placeholders
  2. Generate APP token from Whisper encoder (parallel branch)
  3. Prepend APP token to multimodal sequence
  4. Forward through Phi-4 with LoRA adapters
  5. Extract final hidden state → linear head → 5 scores

- **Design tradeoffs:**
  - APP frozen vs. fine-tuned: Frozen preserves ASR-learned features but cannot adapt to SLA-specific prosody patterns.
  - Part-mean overall vs. direct overall head: Paper reports part-mean for comparability; direct head may capture non-linear score interactions but lacks ground-truth alignment.
  - Single model vs. ensemble: Single model is deployable but cannot specialize per-part.

- **Failure signatures:**
  - Systematic under/over-prediction on specific parts → possible APP miscalibration or LoRA rank insufficient
  - High variance across sessions with similar proficiency → attention not aggregating effectively; check sequence formatting
  - P3/P4 errors significantly higher than P1/P5 → long-audio processing issues; verify FlashAttention enabled

- **First 3 experiments:**
  1. **Ablate APP**: Train Phi-4-MTL without APP to isolate prior contribution; expect RMSE increase per Table 1 (0.362 → 0.360 delta).
  2. **Vary LoRA rank**: Test ranks {4, 8, 16, 32} to find capacity ceiling; monitor per-part RMSE for underfitting/overfitting.
  3. **Session truncation study**: Randomly drop 1-2 responses per session to test robustness; if performance collapses, model relies on full-session aggregation.

## Open Questions the Paper Calls Out
- **Generalization to new tasks**: The paper explicitly identifies extending the framework to "cross-task generalization" as a primary goal for future work. Experiments were confined to Speak & Improve benchmark; performance stability on external datasets or different task types remains unverified.
- **Fairness considerations**: The conclusion lists "fairness considerations" as a key direction, implying current results are not disaggregated by subgroup. No analysis on potential performance biases across specific demographic or linguistic subgroups is provided.
- **APP complementarity**: It is unclear if the APP is necessary because the backbone is frozen, or if it provides unique prosodic evidence that a fully fine-tuned LLM would still fail to capture. The paper does not compare APP contribution under full fine-tuning versus parameter-efficient setup.

## Limitations
- Applicability to zero-shot or few-shot deployment remains unclear as results are based on single dataset with 3 epochs of full fine-tuning.
- Silent architecture choices include LoRA rank/alpha, exact projection dimensions for APP prior, and prompt template for dialog formatting.
- Overall score design uses part-mean as label despite having access to direct overall ratings in corpus.
- Computational constraints of full multimodal context window (~128k tokens) not discussed for practical deployment.

## Confidence
- **High confidence** in: multi-target learning mechanism, session-level attention aggregation, empirical improvement over baselines.
- **Medium confidence** in: Whisper-based Acoustic Proficiency Prior contribution, claim of generalization without further tuning.
- **Low confidence** in: precise impact of APP on calibration, robustness to missing or noisy parts.

## Next Checks
1. **Ablate APP and test robustness**: Retrain without the Whisper prior and measure per-part and overall RMSE, especially on long-audio parts (P3/P4), to isolate the acoustic calibration effect. Verify that gains persist without the prior.
2. **Probe generalization to new domains**: Evaluate the trained model on an out-of-domain SLA dataset (e.g., a different L2 oral proficiency corpus) or on held-out parts not seen during training. This tests the claim of generalizability beyond Speak & Improve.
3. **Vary LoRA rank and test over/underfitting**: Systematically sweep LoRA rank {4, 8, 16, 32} and monitor both in-domain (S&I) and out-of-domain generalization. This will reveal whether the current choice is optimal or if higher capacity is needed for robustness.