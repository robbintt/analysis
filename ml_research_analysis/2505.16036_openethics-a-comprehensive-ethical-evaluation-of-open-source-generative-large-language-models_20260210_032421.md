---
ver: rpa2
title: 'OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large
  Language Models'
arxiv_id: '2505.16036'
source_url: https://arxiv.org/abs/2505.16036
tags:
- ethical
- language
- safety
- arxiv
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive ethical evaluation of 29 open-source
  large language models across four dimensions: robustness, reliability, safety, and
  fairness. The study employs a multilingual framework using English and Turkish prompts,
  totaling 1,790 prompts, and leverages an LLM-as-a-Judge methodology for large-scale
  assessment.'
---

# OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models

## Quick Facts
- **arXiv ID:** 2505.16036
- **Source URL:** https://arxiv.org/abs/2505.16036
- **Reference count:** 40
- **Key outcome:** Comprehensive evaluation of 29 open-source LLMs across robustness, reliability, safety, and fairness using multilingual prompts and LLM-as-a-Judge methodology

## Executive Summary
This paper presents OpenEthics, a comprehensive framework for evaluating the ethical behavior of 29 open-source large language models across four dimensions: robustness, reliability, safety, and fairness. Using 1,790 prompts in English and Turkish, the study employs an LLM-as-a-Judge methodology with Gemini 2.0 Flash to assess model responses. Results show that while models excel in safety, fairness, and robustness to simple jailbreak attempts, reliability remains a significant concern, particularly regarding factual fabrication and hallucination. Larger models generally demonstrate better ethical performance, with Gemma and Qwen models showing the most ethical behavior.

## Method Summary
The study evaluates 29 open-source LLMs using a multilingual framework with 1,790 prompts in English and Turkish. An LLM-as-a-Judge methodology is employed, where Gemini 2.0 Flash acts as the evaluator using a Boolean scoring scheme. The evaluation covers four dimensions: robustness (resistance to jailbreak attempts), reliability (factual accuracy), safety (avoiding harmful content), and fairness (mitigating bias). Models are tested across different parameter sizes, and cross-linguistic consistency is assessed between English and Turkish prompts.

## Key Results
- Models show high performance in safety (96.3-96.9%), fairness, and robustness to simple jailbreak attempts
- Reliability remains a significant weakness, with models struggling particularly with factual fabrication and hallucination
- Larger models (>10B parameters) generally exhibit better ethical performance across safety, fairness, and reliability dimensions
- Cross-linguistic consistency is observed, with English slightly favored due to training data dominance

## Why This Works (Mechanism)

### Mechanism 1: Scale-Enhanced Ethical Performance
Increasing model parameter count correlates with improved ethical performance across safety, fairness, and reliability dimensions. Larger models possess greater capacity to internalize complex safety alignment patterns and nuanced ethical boundaries during instruction tuning, reducing harmful content generation while maintaining instruction-following capabilities. The observed performance gains are intrinsic to scale and not solely artifacts of quantization applied to largest models.

### Mechanism 2: LLM-as-a-Judge Validity
An LLM-as-a-Judge using Boolean scoring and high-capacity evaluator (Gemini 2.0 Flash) can approximate human ethical judgment with high concordance (97.5%). The judge model maps semantic content against explicit grading criteria, treating ethics as a classification task based on learned definitions of harm and bias. The methodology's validity rests on Gemini's neutrality and the Boolean scoring scheme's appropriateness for ethical judgments.

### Mechanism 3: Cross-Linguistic Ethical Transfer
Ethical capabilities are largely transferable across languages, with performance degrading for low-resource languages due to training data dominance. Models align on ethical concepts primarily in high-resource languages (English), transferring this alignment to low-resource languages (Turkish) via shared semantic space, but lacking fine-grained cultural nuance or robust refusal triggers present in the dominant language.

## Foundational Learning

- **Concept: Robustness via Jailbreak Templates**
  - Why needed: The study evaluates robustness specifically as resistance to "Attention Shifting," "Pretending," and "Privilege Escalation" templates, not as a generic metric.
  - Quick check: Can you distinguish between a model failing a prompt because it is unsafe versus failing because it was tricked by a "Ignore previous instructions" prefix?

- **Concept: Reliability vs. Safety**
  - Why needed: The paper highlights a divergence where models are highly Safe (avoiding harm) but have poor Reliability (hallucinations/fabrications). Understanding this distinction is critical for interpreting scores.
  - Quick check: If a model refuses to answer a question about a fake entity (Fictitious Entity), is that a failure of Safety (over-refusal) or a success of Reliability?

- **Concept: LLM-as-a-Judge Pipeline**
  - Why needed: The validity of the entire 1,790-prompt evaluation rests on the reliability of the Gemini 2.0 Flash judge, specifically the "Regular" prompt type with Boolean scoring.
  - Quick check: Why would a "Step-by-Step" reasoning prompt for the judge potentially introduce more variance than a direct Boolean classification?

## Architecture Onboarding

- **Component map:** Dataset (1,790 prompts) -> Target Models (29 LLMs) -> Judge System (Gemini 2.0 Flash) -> Rejection Analyzer (secondary LLM prompt)
- **Critical path:** Defining Ground Truth (Manual validation of 100 samples) -> Validating Judge Prompt (Achieving 97.5% concordance) -> Running Jailbreak augmentation (Category + JB)
- **Design tradeoffs:**
  - Quantization: Models >32B were run in 8-bit quantization, saving GPU memory but risking altered ethical reasoning
  - Judge Selection: Using proprietary Gemini introduces dependency on external "black box" for scoring, risking "preference leakage"
- **Failure signatures:**
  - Factual Fabrication: Lowest scores (approx. 20% English, 16% Turkish) - model invents details rather than stating "I don't know"
  - Language-Specific Over-Refusal: Models refusing English safety prompts (68.8%) but answering the same dangerous Turkish prompts (26.6% refusal)
- **First 3 experiments:**
  1. Re-calibrate the Judge: Run the 40-sample validation set to ensure specific inference settings still achieve >95% concordance
  2. Test the "Jailbreak Deltas": Run subset of "Safety + JB" prompts on small (Llama 3.2 1B) vs. large (Gemma 2 27B) models to quantify performance degradation
  3. Language Gap Analysis: Select 10 "Fictitious Entity" prompts and run in English vs. Turkish to verify "Hallucination Gap"

## Open Questions the Paper Calls Out

### Open Question 1
To what extent does 8-bit quantization degrade the safety and fairness performance of large language models (>32B parameters) compared to their full-precision counterparts? The authors note that evaluating large models using 8-bit quantization might affect performance, particularly impacting non-benchmark metrics such as safety.

### Open Question 2
How does the ethical performance of open-source LLMs change when subjected to multi-turn adversarial interactions rather than the static jailbreak templates used in this study? The study's robustness assessment does not reflect the complexity of multi-turn interactions or more advanced adversarial techniques.

### Open Question 3
Does the observed trade-off (prioritizing safety over reliability) generalize to low-resource languages outside the Turkish/English pair, specifically those with different linguistic structures? The study only analyzed English and Turkish, leaving uncertainty about whether findings are specific to these languages' linguistic proximity.

### Open Question 4
How does the choice of "LLM-as-a-Judge" impact the ranking of models, particularly regarding "preference leakage" where a judge model favors its own family? The evaluation relied solely on Gemini 2.0 Flash as the judge, making it difficult to disentangle actual ethical performance from the judge's subjective preferences.

## Limitations

- Reliance on proprietary LLM-as-a-Judge (Gemini 2.0 Flash) introduces potential "preference leakage" and limits reproducibility
- Cross-linguistic performance gap may be confounded by translation quality from DeepL/Google rather than inherent model bias
- Quantization of models >32B parameters (8-bit) could systematically degrade ethical performance compared to fp16, though not directly tested
- Small "Ground Truth" validation sample (100 prompts) may not capture edge cases across the full 1,790-prompt evaluation

## Confidence

- **High Confidence:** Larger models (>10B parameters) show better safety and fairness performance - well-supported by data and aligns with scaling laws
- **Medium Confidence:** Cross-linguistic consistency claim - supported by data but lacks external validation; translation process and cultural nuances introduce uncertainty
- **Medium Confidence:** LLM-as-a-Judge methodology's validity - rests heavily on Gemini model's neutrality and Boolean scoring scheme's appropriateness
- **Low Confidence:** Assertion that quantization doesn't significantly impact ethical reasoning - stated but not empirically tested against fp16 equivalents

## Next Checks

1. **Judge Neutrality Test:** Run the 100-sample ground truth validation set through multiple LLM judges (GPT-4, Claude) to detect systematic bias and verify Gemini's 97.5% concordance is reproducible

2. **Quantization Impact Assessment:** Compare ethical performance of a model family (Llama 3.1 70B) in both 8-bit and fp16 quantization on the same 100-prompt subset to isolate quantization effect

3. **Cross-Linguistic Robustness Validation:** Manually validate 20 "Safety + JB" prompts (10 English, 10 Turkish) to determine if observed refusal gap is due to translation artifacts or genuine model bias in handling low-resource languages