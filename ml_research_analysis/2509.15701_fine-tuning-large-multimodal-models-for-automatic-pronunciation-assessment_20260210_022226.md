---
ver: rpa2
title: Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment
arxiv_id: '2509.15701'
source_url: https://arxiv.org/abs/2509.15701
tags:
- score
- assessment
- accuracy
- pronunciation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores fine-tuning Large Multimodal Models (LMMs)
  for Automatic Pronunciation Assessment (APA) across multiple granularities and aspects.
  Using Qwen2-Audio-7B-Instruct, the authors fine-tune the model on the Speechocean762
  dataset and a private corpus, combining LoRA adaptation with Simple Preference Optimization
  (SimPO) and cross-entropy loss.
---

# Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment

## Quick Facts
- arXiv ID: 2509.15701
- Source URL: https://arxiv.org/abs/2509.15701
- Reference count: 0
- LMM fine-tuning achieves PCC ~0.9 at word/sentence levels, but only PCC 0.38 at phoneme level

## Executive Summary
This paper presents a fine-tuning approach for Large Multimodal Models (LMMs) to perform Automatic Pronunciation Assessment (APA) across multiple granularities. Using Qwen2-Audio-7B-Instruct, the authors combine LoRA adaptation with Simple Preference Optimization (SimPO) and cross-entropy loss, training on the Speechocean762 dataset and a private corpus. The approach significantly outperforms zero-shot baselines and achieves competitive performance with public and commercial systems at word and sentence levels. However, phoneme-level assessment remains challenging, and the authors identify Spearman's rank correlation coefficient (SCC) as a more appropriate metric than Pearson correlation coefficient (PCC) due to large discrepancies between these metrics.

## Method Summary
The authors fine-tune Qwen2-Audio-7B-Instruct using LoRA parameter-efficient adaptation combined with SimPO preference optimization and cross-entropy loss. The model is trained on the Speechocean762 dataset (3,100 utterances with native English speaker ratings) and a private corpus with phoneme-level annotations. Training employs a weighted loss combining SimPO (with simulated preference pairs) and cross-entropy (λ=0.1) to balance preference learning with format adherence. The approach supports multi-granularity assessment through structured text generation prompts, evaluating performance across word, phoneme, and sentence levels with multiple aspects (accuracy, fluency, completeness, prosody).

## Key Results
- Word-level and sentence-level PCC scores reach approximately 0.9, outperforming zero-shot baselines (PCC -0.03)
- Phoneme-level assessment remains challenging with PCC of only 0.38 and SCC of 0.34
- Significant PCC-SCC discrepancy (PCC ~0.9 vs SCC ~0.6) suggests SCC is more suitable for APA evaluation
- Completeness scores show severe class imbalance issues, with NaN values due to only 8 utterances scoring 5-8 out of 2,500 total

## Why This Works (Mechanism)

### Mechanism 1
Parameter-efficient LoRA adaptation enables LMMs to acquire pronunciation assessment capabilities without full fine-tuning. LoRA injects low-rank trainable matrices into target modules of Qwen2-Audio, allowing the pretrained audio-text representations to be steered toward APA-specific scoring behaviors while preserving base capabilities. The core assumption is that the pretrained LMM already encodes sufficient acoustic-phonetic knowledge that can be redirected via low-rank updates. Evidence anchors include the abstract statement about LoRA adaptation and section 2's description of parameter-efficient fine-tuning, with neighbor paper arxiv:2509.02915 reporting similar success with LoRA on Phi-4-multimodal for APA/MDD.

### Mechanism 2
Combining SimPO preference optimization with cross-entropy loss stabilizes scoring alignment and instruction following. SimPO creates a reference-free preference signal by comparing log-likelihoods of score-adjusted samples, while cross-entropy maintains format adherence. The weighted sum (λ=0.1) balances ordinal preference learning with generation accuracy. The core assumption is that simulated preference pairs (±2-4 score adjustments) capture meaningful ordinal relationships for pronunciation quality. Evidence anchors include section 2.2's explanation of combining SimPO with cross-entropy to prevent format drift, and section 3.3's report of slight improvements in phoneme-level performance from incorporating SimPO with simulated data.

### Mechanism 3
Native audio-text integration in LMMs simplifies APA architecture versus cascaded ASR-plus-classifier systems. Qwen2-Audio processes raw audio and reference text jointly through shared attention mechanisms, eliminating need for separate audio encoder or GOP-style acoustic feature extraction. The model learns to map acoustic patterns directly to scoring outputs via instruction prompts. The core assumption is that end-to-end audio-text attention can capture fine-grained phonetic distinctions comparable to explicit phone-level acoustic features. Evidence anchors include section 2's description of native audio-text support and competitive performance at word/sentence levels, though phoneme-level PCC=0.38 reveals limitations.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Enables practical fine-tuning of 7B-parameter models on single consumer GPUs by reducing trainable parameters from billions to millions.
  - Quick check question: If LoRA rank increases from 8 to 32, would you expect better phoneme-level performance or just overfitting risk?

- Concept: Preference Optimization (DPO/SimPO)
  - Why needed here: Provides supervision signal for ordinal scoring without requiring explicit reward model training; SimPO variant reduces memory overhead.
  - Quick check question: Why does SimPO remove the reference model requirement, and what tradeoff does this introduce?

- Concept: Correlation Metrics for Ordinal Data (PCC vs SCC)
  - Why needed here: Paper reveals PCC~0.9 vs SCC~0.6 discrepancy, suggesting models may achieve linear correlation without proper ordinal ranking—critical for real-world pass/fail decisions.
  - Quick check question: For a CALL system that assigns proficiency levels (A/B/C/D), which metric better reflects practical utility?

## Architecture Onboarding

- Component map: Raw audio + reference text/phone sequence -> Qwen2-Audio-7B-Instruct (with LoRA adapters) -> Structured scores via text generation

- Critical path: 1. Audio preprocessing into LMM-compatible format 2. Prompt construction with reference text/phone sequence 3. LoRA fine-tuning on SO762 (3 epochs) or combined data (2 epochs) 4. SimPO training with simulated preference pairs 5. Evaluation via PCC/SCC/RMSE against human expert scores

- Design tradeoffs:
  - Multi-granularity vs single-granularity prompts: Multi-task reduces performance (phoneme PCC 0.38) vs single-task (word accuracy PCC 0.62)
  - SimPO-only vs SimPO+CE: Pure SimPO risks format drift; adding CE (λ=0.1) stabilizes output structure
  - Data pooling: Adding private corpus improves word/sentence scores but phoneme annotations unavailable

- Failure signatures:
  - Completeness scores return NaN due to severe class imbalance (only 8/2500 utterances with scores 5-8)
  - Phoneme-level SCC drops to 0.34 indicating poor ordinal consistency despite PCC=0.38
  - Zero-shot performance near zero (PCC -0.03) confirming pretrained LMM lacks native APA capability

- First 3 experiments:
  1. Baseline LoRA fine-tuning on SO762 only: Establish word/sentence performance ceiling; expect PCC 0.57-0.78 depending on granularity
  2. Ablation: LoRA vs LoRA+SimPO vs LoRA+DPO: Isolate preference optimization contribution on phoneme-level scoring
  3. Metric validation on held-out private corpus: Confirm PCC/SCC discrepancy pattern (PCC~0.9, SCC~0.6) persists across datasets

## Open Questions the Paper Calls Out
None

## Limitations

- Limited generalization to diverse learner populations due to evaluation restricted to Chinese EFL learners, with substantial gap between word/sentence-level success and phoneme-level struggles
- Severe class imbalance in completeness scores (only 8/2500 utterances with scores 5-8) raises questions about reliability of aspect-level evaluations
- Fundamental limitations in phoneme-level assessment (PCC 0.38, SCC 0.34) suggest architectural constraints in sub-word acoustic resolution despite native audio-text integration

## Confidence

- High confidence: Claims about LoRA's parameter efficiency and combination of SimPO with cross-entropy improving format adherence are well-supported by ablation results and consistent with established literature
- Medium confidence: Competitive performance at word and sentence levels is supported by PCC values reaching 0.9, but metric discrepancy raises questions about practical utility
- Low confidence: Assertions about architectural advantages of native audio-text integration for fine-grained modeling are contradicted by phoneme-level performance collapse; suitability of SCC over PCC as evaluation metric is proposed but not rigorously validated

## Next Checks

1. Cross-population generalization test: Evaluate the fine-tuned model on pronunciation assessment data from non-Chinese EFL learners and native speakers with varying accents. Measure whether the PCC-SCC discrepancy pattern persists and whether phoneme-level performance improves with more diverse acoustic-phonetic patterns.

2. Metric behavior under controlled conditions: Create synthetic datasets with known ordinal relationships but varying score distributions. Test whether the model's PCC-SCC gap emerges in controlled settings, and validate whether SCC better predicts actual pass/fail decisions than PCC in simulated proficiency classification tasks.

3. Architecture ablation with explicit acoustic features: Implement a hybrid model combining Qwen2-Audio with explicit phone-level acoustic feature extraction (MFCCs, formants) and compare phoneme-level performance. Determine whether the architecture itself or the learning approach causes the fine-grained modeling limitations.