---
ver: rpa2
title: 'Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual'
arxiv_id: '2510.26271'
source_url: https://arxiv.org/abs/2510.26271
tags:
- teacher
- student
- multilingual
- distillation
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study addresses the challenge of maintaining multilingual\
  \ performance when compressing vision-language models using knowledge distillation.\
  \ The core method involves comparing five distillation strategies\u2014feature distillation,\
  \ English-control distillation, soft-logit distillation, multilingual contrastive\
  \ learning, and distributional replication\u2014across CLIP and SigLIP2 architectures."
---

# Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual

## Quick Facts
- arXiv ID: 2510.26271
- Source URL: https://arxiv.org/abs/2510.26271
- Reference count: 28
- Primary result: Some distillation configurations preserve or improve multilingual retrieval robustness despite halving model size

## Executive Summary
This paper addresses the challenge of maintaining multilingual performance when compressing vision-language models using knowledge distillation. The study compares five distillation strategies—feature distillation, English-control distillation, soft-logit distillation, multilingual contrastive learning, and distributional replication—across CLIP and SigLIP2 architectures. Results show that distributional replication preserves multilingual retrieval performance while English-control distillation better maintains visual question answering capabilities, revealing critical design trade-offs between different downstream tasks.

## Method Summary
The method involves knowledge distillation from multilingual vision-language teachers (CLIP-ViT-L/14 or SigLIP2-L/16) to smaller multilingual student encoders (XLM-R Base, DistilBERT, MiniLM). Five distillation strategies are evaluated: feature distillation (MSE), English-control distillation (dual MSE with English alignment), soft-logit distillation (KL divergence), multilingual contrastive learning (CLIP-style contrastive loss), and distributional replication (similarity-based distributions with FIFO queue). Training uses ImageCaptioning7M dataset (7M multilingual-English pairs) and evaluates on multilingual retrieval (Multi30k, COCO, WIT, xFlickr, XM3600) and VQA tasks (CVQA, ALM-Bench).

## Key Results
- Distributional Replication achieves higher MRR@k in multilingual retrieval than other methods
- Student models outperform SigLIP2-L/16 on multilingual T2I retrieval despite being smaller
- English-Control Distillation improves VQA performance by 2.97 points over Distributional Replication
- Text anchors provide 14.79 retrieval improvement over image anchors
- DR+FD combination improves retrieval but hurts VQA (-3.46 points)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distributional Replication (DR) preserves multilingual retrieval performance by enforcing distributional consistency between teacher and student similarity structures.
- **Mechanism:** DR constructs similarity-based probability distributions using a FIFO queue of teacher representations. The student learns to match the teacher's distributional relationships over many samples rather than point-wise embeddings. This preserves relative ranking structure across languages even when model capacity is halved.
- **Core assumption:** The ranking structure captured by similarity distributions transfers more robustly than direct embedding alignment.
- **Evidence anchors:** DR-distilled models achieve higher MRR@k than other methods; student outperforms SigLIP2-L/16 on multilingual T2I retrieval.

### Mechanism 2
- **Claim:** English-Control Distillation (ED) maintains out-of-domain VQA performance by adding explicit English self-alignment to prevent representation collapse.
- **Mechanism:** ED extends feature distillation by aligning both student's multilingual outputs AND student's English outputs to the teacher's English representation. This creates a stable anchor that prevents all languages from collapsing toward a single point when the teacher only provides English supervision.
- **Core assumption:** Preventing intra-student representation drift is critical for tasks requiring compositional reasoning (VQA) rather than just similarity matching (retrieval).
- **Evidence anchors:** ED achieves 35.54 VQA average vs DR's 32.57, despite lower retrieval scores.

### Mechanism 3
- **Claim:** Monolingual teachers can train multilingual students because text-anchored distillation transfers cross-modal alignment rather than language-specific representations.
- **Mechanism:** The distillation objective aligns student multilingual embeddings with teacher English embeddings based on paired training data. The student's pre-trained multilingual encoder provides language understanding; distillation adds vision-language alignment. Text anchors provide precise semantic grounding compared to ambiguous image anchors.
- **Core assumption:** The student encoder already captures cross-lingual semantic similarity; distillation primarily transfers visual grounding.
- **Evidence anchors:** Text anchors achieve 42.15 retrieval average vs 27.36 for image anchors.

## Foundational Learning

- **Concept:** Knowledge Distillation (KD)
  - **Why needed here:** All five methods are KD variants; understanding the general paradigm is prerequisite.
  - **Quick check question:** Can you explain why matching soft probability distributions can transfer more information than matching hard labels?

- **Concept:** Vision-Language Contrastive Learning
  - **Why needed here:** The teacher models are trained contrastively; the MCL and DR methods build on contrastive objectives.
  - **Quick check question:** How does contrastive learning create aligned embedding spaces for images and text?

- **Concept:** Multilingual Representation Spaces
  - **Why needed here:** The paper's core problem is maintaining cross-lingual consistency under compression; understanding how multilingual encoders represent different languages is essential.
  - **Quick check question:** Why might a multilingual encoder struggle to maintain equal quality across all languages after distillation?

## Architecture Onboarding

- **Component map:** Teacher (CLIP-ViT-L/14 or SigLIP2-L/16) -> Student (XLM-R Base, DistilBERT, or MiniLM) -> Loss modules (FD, ED, SD, MCL, DR) -> Training data (ImageCaptioning7M)

- **Critical path:** Choose task priority (retrieval → DR; VQA → ED); match student capacity (XLM-R Base for moderate, MiniLM for extreme); select teacher based on available training languages (SigLIP2 if multilingual data, CLIP if only English).

- **Design tradeoffs:** DR vs ED (DR wins retrieval +2.83 avg, ED wins VQA +2.97 avg); single vs multi-objective (DR+FD improves retrieval +0.13 but hurts VQA -3.46); text vs image anchors (+14.79 retrieval for text).

- **Failure signatures:** Representation collapse (all languages map to similar embeddings → ED not used); language-specific degradation (Bengali, Telugu, Swahili perform poorly → training data lacks those languages); cross-task instability (strong retrieval but poor VQA → DR used alone).

- **First 3 experiments:** 1) Train XLM-R Base with FD from SigLIP2-L/16, evaluate on Multi30k retrieval and CVQA. 2) Same setup with DR and ED separately. 3) Replace text anchors with image anchors on subset, confirm retrieval drops ~15 points.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a unified distillation objective be developed to jointly optimize for both in-domain multilingual retrieval and out-of-domain visual question answering (VQA) without the trade-offs observed in current multi-objective configurations?
- **Basis in paper:** The authors state that combining objectives (DR+ED+FD) yields a performance penalty in VQA tasks, concluding the need to develop a new training objective that effectively addresses both retrieval and VQA tasks.
- **Why unresolved:** DR optimizes retrieval while ED optimizes VQA, and combining them leads to conflicting gradients that fail to maximize performance on both simultaneously.
- **What evidence would resolve it:** A new loss function that achieves state-of-the-art results on both Multi30k and CVQA benchmarks concurrently.

### Open Question 2
- **Question:** Why does using non-English languages as the distillation anchor result in significant performance degradation, and is this degradation primarily an artifact of machine translation quality or the teacher's representation geometry?
- **Basis in paper:** Section 5.3.1 shows using German or Chinese as anchors causes performance drops compared to English. The authors hypothesize "linguistic proximity" causes information loss but do not isolate the root cause.
- **Why unresolved:** The paper utilizes machine-translated data for non-English anchors, making it unclear if the drop is due to the student's inability to align or noise introduced by the translation process.
- **What evidence would resolve it:** Experiments using human-curated parallel corpora as anchors, or analysis of the teacher's latent space geometry.

### Open Question 3
- **Question:** What specific mechanisms allow smaller student models to outperform larger teacher models in multilingual clustering and ranking tasks despite having lower raw accuracy?
- **Basis in paper:** Section 6.2 and 6.3 reveal student models achieve better purity scores and MRR than the teacher, a phenomenon the paper observes but does not fully explain mechanistically.
- **Why unresolved:** The paper attributes this to the KD method's focus on "multilingual consistency," but it's counter-intuitive that a compressed model would organize a latent space more effectively than the teacher without explicit regularization for clustering.
- **What evidence would resolve it:** Spectral analysis of embedding spaces to determine if distillation acts as a regularizer that reduces noise or isotropy.

## Limitations
- Evaluation scope focuses predominantly on retrieval tasks and a single VQA benchmark, not exploring generation tasks
- Limited exploration of low-resource languages beyond mentioned examples (Bengali, Telugu, Swahili)
- Reliance on English-translation pairs raises questions about scalability when multilingual data is scarce

## Confidence
- **High confidence:** Core experimental results showing DR's superiority in multilingual retrieval and the fundamental mechanism of distributional alignment
- **Medium confidence:** Claim that DR+FD improves retrieval while hurting VQA (represents a tradeoff dependent on specific protocols)
- **Medium confidence:** Assertion that multilingual teachers are not required (primarily demonstrated through one experiment)

## Next Checks
1. **Cross-task robustness test:** Evaluate DR-distilled models on visual question generation (VQG) and image captioning tasks beyond retrieval and VQA to assess generalization across the full vision-language task spectrum.

2. **Low-resource language stress test:** Conduct controlled experiments reducing training data availability for specific languages (e.g., Swahili, Bengali) to quantify the minimum viable data requirements for maintaining performance across linguistic diversity.

3. **Architecture scaling study:** Systematically vary student model capacity to identify the point at which multilingual representation quality begins to degrade significantly, establishing practical limits for model compression.