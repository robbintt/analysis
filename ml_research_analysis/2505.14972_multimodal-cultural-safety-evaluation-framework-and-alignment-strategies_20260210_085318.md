---
ver: rpa2
title: 'Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies'
arxiv_id: '2505.14972'
source_url: https://arxiv.org/abs/2505.14972
tags:
- cultural
- safety
- culturally
- evaluation
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CROSS, a benchmark for evaluating the cultural
  safety of large vision-language models (LVLMs) across 16 countries, 3 domains, and
  14 languages. It features 1,284 visually grounded queries where cultural norm violations
  only emerge in context.
---

# Multimodal Cultural Safety: Evaluation Framework and Alignment Strategies

## Quick Facts
- **arXiv ID**: 2505.14972
- **Source URL**: https://arxiv.org/abs/2505.14972
- **Reference count**: 40
- **Primary result**: Evaluates 21 leading LVLMs on cultural safety across 16 countries, 14 languages, and 3 domains; shows significant safety gaps (as low as 37.73% compliance) and demonstrates safety improvements via alignment strategies

## Executive Summary
This paper introduces CROSS, a benchmark for evaluating the cultural safety of large vision-language models (LVLMs) across 16 countries, 3 domains, and 14 languages. It features 1,284 visually grounded queries where cultural norm violations only emerge in context. A theory-driven framework, CROSS-Eval, measures four dimensions: awareness, education, compliance, and helpfulness. Evaluations of 21 leading LVLMs show significant safety gaps, with top models scoring as low as 61.79% in awareness and 37.73% in compliance. To address these gaps, the authors develop two enhancement strategies—supervised fine-tuning and preference tuning—which substantially improve GPT-4o's cultural safety metrics while preserving general multimodal performance.

## Method Summary
The method constructs CROSS, a benchmark with 1,284 image-query pairs across 16 countries, 14 languages, and 3 domains, where cultural norm violations emerge only when images are interpreted in context. CROSS-Eval uses GPT-4o as an automatic evaluator with dimension-specific prompts to measure Awareness, Education, Compliance, and Helpfulness. Training data is derived from CVQA MCQs converted to safety-oriented open-ended QA (1,094 overlapped + 1,152 exclusive examples for SFT; same counts for DPO contrastive pairs). Two enhancement strategies are employed: supervised fine-tuning with culturally grounded responses, and DPO with contrastive pairs spanning four negative types. Both use text-only fine-tuning for 1 epoch, with GPT-4o using OpenAI API and open-source models using 1-2 A100 80GB GPUs at LR 5e-4.

## Key Results
- Top LVLMs score as low as 61.79% in awareness and 37.73% in compliance on cultural safety dimensions
- 40+ point gaps exist between Awareness and Education in several countries (Nigeria, Brazil, Iran, Egypt, Morocco), indicating norm recognition without cultural contextualization capability
- DPO with mixed negative types achieves best safety-general capability tradeoff (+25.88 awareness, -1.10 MMMU) versus single-type training
- Models with weak cultural pretraining (CVQA accuracy <60%) show minimal safety improvements from alignment methods regardless of data quality

## Why This Works (Mechanism)

### Mechanism 1: Visual-Context-Dependent Cultural Reasoning
Image-query pairs designed to appear culturally neutral in isolation force models to perform genuine multimodal reasoning rather than relying on lexical shortcuts. Queries are crafted without explicit cultural keywords (e.g., "glass-made items" rather than "alcohol"), requiring the model to recognize visual content (wine glasses), retrieve relevant cultural norms (alcohol prohibition in Iran), and reason about their interaction. The core assumption is that models that correctly avoid violations do so through integrated visual-semantic reasoning, not pattern matching on surface features. Evidence includes the abstract's claim that "cultural norm violations emerge only when images are interpreted in context" and the corpus reference to MMA-ASIA paper's similar challenges with cultural grounding. Break condition occurs if models learn to recognize image content but lack cultural norm representations.

### Mechanism 2: Dimension-Decomposed Safety Assessment
Decomposing cultural safety into four orthogonal dimensions (Awareness, Education, Compliance, Helpfulness) enables targeted diagnosis of model failures. Each dimension maps to a distinct capability from Intercultural Sensitivity Scale: recognizing norms, explaining rationales, respecting boundaries, and providing actionable guidance. High awareness does not guarantee high education (e.g., Nigeria shows 40+ point gaps). The core assumption is that these dimensions capture meaningfully independent aspects of culturally safe behavior. Evidence includes §4.1's finding that "Norm Awareness and Compliance are highly correlated (r≈0.96), indicating that once a norm is recognized, it is usually followed. Education is only moderately correlated with Compliance (r≈0.60)." Break condition occurs if dimensions are not truly independent.

### Mechanism 3: Contrastive Preference Tuning with Structured Negative Types
DPO using contrastive pairs spanning four negative types (missing awareness, missing education, non-compliance, unhelpful) improves cultural safety with minimal general capability degradation. Each negative type provides distinct supervision signals. Mixed-type training yields best results (+25.88 awareness, -1.10 MMMU) vs. single-type training (Type 2: +31.04 awareness but -6.88 MMMU). The core assumption is that the four negative types cover the space of culturally unsafe behaviors, and mixing them prevents overfitting to specific failure modes. Evidence includes §5.3's ablation showing mixed negatives achieve best safety-general capability tradeoff. Break condition occurs if foundation models lack sufficient cultural grounding (CVQA accuracy <60%), as alignment methods yield minimal gains regardless of data quality.

## Foundational Learning

- **Concept: Vision-Language Model Cross-Modal Grounding**
  - **Why needed here:** The benchmark explicitly requires integrating visual perception with cultural knowledge. Without understanding how LVLMs bind visual features to semantic concepts, you cannot diagnose whether failures stem from perception or reasoning.
  - **Quick check question:** Can you explain why a model might correctly identify "wine glasses" in an image but fail to connect this to alcohol-related cultural norms?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** Safety-DPO is the preferred alignment method due to its minimal capability degradation. Understanding the objective function helps interpret why mixed negative types outperform single types.
  - **Quick check question:** How does DPO differ from supervised fine-tuning in terms of what the model learns about the preference boundary?

- **Concept: Intercultural Sensitivity Scale (Chen & Starosta, 2000)**
  - **Why needed here:** The four evaluation dimensions are theory-grounded adaptations of ISS traits. Interpreting results requires understanding the underlying constructs being measured.
  - **Quick check question:** Which ISS trait does the "Education" dimension adapt, and what does it require beyond mere norm recognition?

## Architecture Onboarding

- **Component map:** CVQA MCQs → norm extraction → scenario generation → query-response construction → contrastive pair generation → text-only SFT/DPO training → CROSS-Eval evaluation

- **Critical path:**
  1. Validate cultural norms using geo-diverse annotators + cross-model verification
  2. Construct image-query pairs where violations are visual-context-dependent
  3. Generate contrastive response pairs covering all four negative types
  4. Train with mixed-type DPO to balance safety gains against capability preservation

- **Design tradeoffs:**
  - **Text-only vs. multimodal fine-tuning:** Paper uses text-only SFT/DPO, citing prior work on cross-modal generalization. Tradeoff is weaker visual grounding but simpler implementation.
  - **Overlapped vs. exclusive country data:** Safety-SFT-Excl. achieves comparable gains to Overlapped (+60% vs. +58% awareness), suggesting generalization. DPO shows similar pattern.
  - **SFT vs. DPO:** SFT yields larger safety gains (Awareness 80%+ vs. 48%) but worse general performance (MMMU -5.55 vs. -1.10).

- **Failure signatures:**
  1. **Inflated compliance from perception failure:** Small models (Qwen2.5-VL-3B/7B) show artificially high compliance because they fail to recognize image content and default to avoidance behavior
  2. **Explanation gaps:** High awareness but low education (40+ point gaps in Nigeria, Brazil, Iran, Egypt, Morocco) indicates norm recognition without cultural contextualization capability
  3. **Multilingual degradation:** Open-source models show >20% compliance drops from English to target languages

- **First 3 experiments:**
  1. **Baseline evaluation:** Run CROSS-Eval on your target LVLM across all four dimensions in both English and target languages; identify which dimensions fail first
  2. **Ablate negative types:** Train separate DPO models with each negative type (1-4) plus mixed; measure safety gains vs. MMMU/MME degradation to find optimal tradeoff
  3. **Cross-country generalization test:** Train on Safety-SFT-Exclusive (non-overlapping countries) and evaluate on CROSS-Country to assess whether learned cultural reasoning transfers across regions

## Open Questions the Paper Calls Out
None

## Limitations
- **Domain generalizability:** The benchmark focuses on three consumer-facing domains (shopping, meal planning, outdoor activities). Claims about universal cultural safety improvements may not extend to professional or sensitive domains (healthcare, legal, security).
- **Visual grounding reliability:** The safety gains from alignment methods may be overestimated for models with weak cultural pretraining (e.g., InternVL2.5), as evidenced by minimal improvements despite substantial training.
- **Language coverage asymmetry:** While 14 languages are supported, evaluation and training data are heavily English-centric, with only post-hoc translation.

## Confidence

- **High confidence:** The CROSS benchmark construction methodology (image-query pairing, dimension decomposition) and the core finding that top LVLMs show significant cultural safety gaps (61.79% awareness, 37.73% compliance) are well-supported by systematic evaluation.
- **Medium confidence:** The alignment strategy effectiveness (SFT and DPO improvements) is demonstrated but primarily on GPT-4o. Generalization to other model architectures and the pretraining dependency effects require further validation.
- **Low confidence:** Claims about cultural safety improvements in real-world deployment scenarios lack empirical validation beyond the controlled benchmark environment.

## Next Checks

1. **Pretraining Dependency Test:** Systematically evaluate alignment method effectiveness across models with varying levels of cultural pretraining (using CVQA as proxy) to quantify the threshold below which alignment provides minimal benefit.
2. **Domain Transfer Experiment:** Apply the alignment strategies to models tested on professional domains (medical imaging interpretation, legal document analysis) to assess cross-domain generalization of cultural safety improvements.
3. **Longitudinal Capability Monitoring:** Track general capability metrics (MMMU/MME) across extended deployment periods post-alignment to detect gradual capability degradation that may not appear in short-term evaluations.