---
ver: rpa2
title: 'VisionCAD: An Integration-Free Radiology Copilot Framework'
arxiv_id: '2511.00381'
source_url: https://arxiv.org/abs/2511.00381
tags:
- images
- diagnostic
- visioncad
- image
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'VisionCAD introduces a camera-based framework that captures medical
  images directly from displays, bypassing hospital IT integration requirements. The
  system uses a six-component pipeline: Vision Capturer acquires images, Screen Detector
  isolates medical content, Quality Enhancer restores image quality using Restormer,
  Modality Router identifies imaging type, Diagnostic Engine applies specialized models
  (Ark+ for chest X-rays, fine-tuned ViT for other modalities), and Report Assistant
  generates clinical reports.'
---

# VisionCAD: An Integration-Free Radiology Copilot Framework

## Quick Facts
- **arXiv ID**: 2511.00381
- **Source URL**: https://arxiv.org/abs/2511.00381
- **Reference count**: 40
- **Primary result**: Integration-free camera-based CAD system achieving diagnostic performance within 2% of original-image baselines across four medical imaging datasets

## Executive Summary
VisionCAD introduces a camera-based framework that captures medical images directly from displays, bypassing hospital IT integration requirements. The system uses a six-component pipeline: Vision Capturer acquires images, Screen Detector isolates medical content, Quality Enhancer restores image quality using Restormer, Modality Router identifies imaging type, Diagnostic Engine applies specialized models (Ark+ for chest X-rays, fine-tuned ViT for other modalities), and Report Assistant generates clinical reports. Across four medical imaging datasets, VisionCAD achieves diagnostic performance comparable to conventional CAD systems, with F1-score degradation typically less than 2% and automated report generation metrics within 1% of original-image baselines.

## Method Summary
The VisionCAD framework captures medical images from display screens using Azure Kinect camera (2560×1440 resolution, 50-60cm viewing distance) and processes them through a six-stage pipeline. Screen detection uses RTMDet and fine-tuned YOLOv5 on synthetic RIS interfaces. Quality enhancement employs Restormer trained on 22,975 paired degraded/clean images across five modalities. Modality routing uses BiomedCLIP zero-shot classification. Diagnostic engines include Ark+ for chest X-rays and fine-tuned ViT for knee OA. Report generation converts diagnostic probabilities to structured clinical text for VLM input (GPT-4.1 or Gemini-2.5-pro at temperature 0.2).

## Key Results
- Diagnostic performance comparable to conventional CAD systems with F1-score degradation typically less than 2% across classification tasks
- Quality Enhancer achieves top PSNR and SSIM on PneumoniaMNIST, best SSIM on MIMIC-CXR using Restormer
- Zero-shot modality classification using BiomedCLIP enables automatic selection of optimal diagnostic models
- Automated report generation metrics within 1% of original-image baselines across four medical imaging datasets

## Why This Works (Mechanism)

### Mechanism 1: Capture-Induced Degradation Recovery
The Quality Enhancer module recovers diagnostic-quality images from camera-captured displays with sufficient fidelity for downstream analysis. Restormer learns to correct perspective distortions, illumination artifacts, and moiré patterns by training on paired degraded/clean image datasets (22,975 pairs across five modalities). The model maps captured images back toward original distribution via supervised pixel-level correspondence established through SIFT alignment.

### Mechanism 2: Modality-Guided Model Routing
Zero-shot modality classification using BiomedCLIP embeddings enables automatic selection of optimal diagnostic models. The captured image and computes cosine similarity against text embeddings of modality descriptions. The highest similarity triggers the corresponding specialized model (Ark+ for chest, fine-tuned ViT for knee).

### Mechanism 3: Structured Diagnostic-to-Report Synthesis
Converting diagnostic probabilities to standardized clinical text before VLM input improves report accuracy and consistency. The Diagnostic Engine outputs probability distributions; these are mapped to textual descriptions via threshold-based rules (e.g., [0.9, 1.0] → "Definitely have {disease}"). This structured text is concatenated with the image and fed to a VLM with a fixed prompt template.

## Foundational Learning

- **Encoder-Decoder Image Restoration**: Understanding how Restormer reconstructs degraded medical images from learned representations. Quick check: Given a noisy chest X-ray captured at 45° viewing angle, would a UNet or Transformer-based restoration better preserve fine pulmonary markings?

- **Zero-Shot Vision-Language Classification**: The Modality Router relies on CLIP-style embeddings for routing without task-specific training. Quick check: If BiomedCLIP has never seen camera-captured CT images during pretraining, will its embeddings still cluster by modality type?

- **Probabilistic Thresholding for Clinical Decision Support**: Converting model outputs to actionable clinical language requires calibrated probability interpretation. Quick check: A model outputs 0.73 probability for pneumonia—is this "small possibility," "likely," or should it trigger immediate clinical review?

## Architecture Onboarding

- **Component map**: Vision Capturer (Azure Kinect) → RTMDet screen bounds → affine correction → YOLOv5 medical region → Restormer restoration → BiomedCLIP modality score → modality-specific model → probability-to-text → VLM report synthesis

- **Critical path**: Captured image → RTMDet screen bounds → affine correction → YOLOv5 medical region → Restormer restoration → BiomedCLIP modality score → modality-specific model → probability-to-text → VLM report synthesis. Latency bottleneck is typically Quality Enhancer (Restormer inference on high-resolution images).

- **Design tradeoffs**: Restormer vs. SwinIR: Restormer selected for balanced performance across modalities; SwinIR faster but lower SSIM on MIMIC-CXR. Ark+ vs. fine-tuned ViT: Ark+ provides SOTA chest X-ray but requires 768×768 input; ViT offers flexibility for tasks without foundation models. GPT-4.1 vs. Gemini-2.5-pro: Thinking model (Gemini) achieves higher F1 (+5.77 average) but at higher latency and cost.

- **Failure signatures**: PSNR <18 dB after restoration → check camera positioning or lighting conditions. Modality misclassification → verify BiomedCLIP prompt templates cover all expected modalities. Report missing key findings → inspect probability-to-text thresholds; may need adjustment for subtle pathologies. F1 degradation >2% → review Quality Enhancer training data coverage for this modality.

- **First 3 experiments**: 1) Capture 100 images from standard workstation, measure PSNR/SSIM before and after Restormer across three lighting conditions. Target: PSNR improvement >5 dB. 2) Test BiomedCLIP zero-shot classification on 500 images across 5 modalities. Target: >95% top-1 accuracy. 3) Run Ark+ on original vs. VisionCAD-processed MIMIC-CXR subset (n=200). Target: AUC difference <0.02 across 14 CheXpert labels.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the image restoration pipeline be optimized to prevent the loss of subtle, low-contrast features (e.g., early pneumonia signs) that currently show variable diagnostic performance? The current Restormer-based enhancement improves general metrics but specific conditions still show performance variability.

- **Open Question 2**: To what extent does the framework's performance degrade when deployed on resource-constrained edge devices (smart glasses) rather than the high-fidelity Azure Kinect setup? The paper states the prototype uses a Kinect but "practical deployment envisions integration into lighter and more compact smart glasses."

- **Open Question 3**: Can the framework maintain diagnostic parity when processing images from diverse display types (e.g., high-resolution medical vs. consumer-grade) under uncontrolled clinical lighting? The authors list "variable lighting conditions" and "diverse display types" as limitations requiring "more validation."

- **Open Question 4**: How can the system mitigate data privacy risks associated with transmitting captured medical images to cloud-based VLMs (GPT-4/Gemini) for report generation? The Conclusion states that "reliance on cloud-based models raises concerns about data privacy, security, and connectivity requirements."

## Limitations

- Quality Enhancer's ability to generalize beyond controlled capture conditions with specific viewing angles (±30° horizontal, ±15° vertical) and lighting conditions may not capture real-world clinical variability
- Zero-shot modality routing through BiomedCLIP lacks explicit performance benchmarks and accuracy metrics
- Structured probability-to-text mapping assumes linear relationships between model confidence and clinical significance that may not capture nuanced cases

## Confidence

- **High confidence**: The integration-free architecture and six-component pipeline design
- **Medium confidence**: Diagnostic performance claims (supported by F1-score degradation metrics but limited by dataset diversity)
- **Low confidence**: Modality routing accuracy (zero-shot claims without explicit benchmarks)

## Next Checks

1. **Capture robustness test**: Systematically vary capture conditions (viewing angles ±45°, lighting extremes, screen reflections) and measure PSNR/SSIM degradation and downstream diagnostic accuracy across all four modalities

2. **Modality routing stress test**: Evaluate BiomedCLIP zero-shot classification on 100+ images per modality including edge cases (small lesions, poor contrast, atypical anatomy) with confusion matrix analysis

3. **Clinical decision boundary validation**: Compare structured probability thresholds against radiologist consensus on borderline cases to identify where automated mapping fails clinical nuance