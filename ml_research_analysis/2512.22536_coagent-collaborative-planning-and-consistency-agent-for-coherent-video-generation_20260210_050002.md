---
ver: rpa2
title: 'CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation'
arxiv_id: '2512.22536'
source_url: https://arxiv.org/abs/2512.22536
tags:
- coagent
- video
- generation
- wang
- consistency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoAgent, a collaborative planning and consistency
  agent for coherent video generation. The authors address the challenge of maintaining
  narrative coherence and visual consistency in long-form videos by formulating video
  synthesis as a plan-synthesize-verify-edit pipeline.
---

# CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation

## Quick Facts
- arXiv ID: 2512.22536
- Source URL: https://arxiv.org/abs/2512.22536
- Reference count: 40
- Outperforms state-of-the-art methods in subject consistency (94.70% vs 89.95%), background consistency (96.50% vs 92.89%), and text-video alignment (2.731 vs 2.550)

## Executive Summary
This paper proposes CoAgent, a collaborative planning and consistency agent for coherent video generation. The authors address the challenge of maintaining narrative coherence and visual consistency in long-form videos by formulating video synthesis as a plan-synthesize-verify-edit pipeline. The framework integrates a Storyboard Planner that decomposes high-level prompts into structured shot plans, a Global Context Manager that maintains entity-level memory across shots, a Synthesis Module that generates each shot with adaptive modes (T2V, FF2V, FLF2V), and a Verifier Agent that evaluates visual consistency and triggers selective regeneration. Experiments show that CoAgent significantly outperforms state-of-the-art methods in subject consistency (94.70% vs 89.95%), background consistency (96.50% vs 92.89%), and text-video alignment (2.731 vs 2.550), demonstrating its effectiveness in generating coherent, style-aligned, and rhythm-consistent videos.

## Method Summary
CoAgent formulates video synthesis as a plan-synthesize-verify-edit pipeline. The system first uses a Storyboard Planner (Gemini-2.5-Flash) to decompose high-level prompts into structured shot plans and maintain a character registry. A Global Context Manager (GCM) then initializes entity memory by generating "Master Portraits" using an ImageGen model, storing canonical embeddings for key entities. The Synthesis Module (Wan2.1) consumes GCM memory plus Shot Plan to generate video clips, adaptively selecting between Text-to-Video (T2V), First-Frame-to-Video (FF2V), or First-Last-Frame-to-Video (FLF2V) modes. After synthesis, a Verifier Agent (GPT-4o) audits each clip against the plan and GCM portrait, triggering selective regeneration if quality scores fall below threshold τ. The orchestrator manages this loop and handles final pacing assembly.

## Key Results
- Subject consistency improvement: 94.70% vs 89.95% (state-of-the-art)
- Background consistency improvement: 96.50% vs 92.89% (state-of-the-art)
- Text-video alignment improvement: 2.731 vs 2.550 (state-of-the-art)

## Why This Works (Mechanism)

### Mechanism 1: Explicit Statefulness via Global Context Manager (GCM)
The paper proposes that maintaining a persistent, external memory for entity visual features reduces identity drift better than implicit conditioning. The GCM stores canonical embeddings $(v_k, a_k, t_k)$ for key entities (characters/props). During synthesis, these embeddings are retrieved and fused into the diffusion process via cross-attention, explicitly injecting identity features rather than relying on the model's latent memory. Core assumption: The visual encoder can capture distinctive identity features, and the synthesis module can correctly attend to these features during denoising. Evidence: "Global Context Manager that maintains entity-level memory to preserve appearance and identity consistency." Break condition: High visual similarity between distinct entities (e.g., two characters in similar suits) may cause retrieval confusion or feature bleeding.

### Mechanism 2: Closed-Loop Error Correction
A "generate-verify-correct" loop likely improves semantic alignment and temporal quality by filtering errors that open-loop systems cannot detect. After synthesis, a Verifier Agent (VLM) assesses the shot against the plan and GCM. If the quality score $V_i < \tau$, the system triggers specific interventions: prompt refinement (for semantic errors) or mode switching (for visual/attribute errors). Core assumption: The Verifier VLM has sufficient visual reasoning to detect specific failure modes (e.g., "brown hair" vs. "blonde") and the correction strategy effectively maps the error to a solution. Evidence: "Verifier Agent evaluates intermediate results... and triggers selective regeneration." Break condition: "Physical hallucinations" (e.g., object clipping) often escape detection because VLMs lack spatial reasoning for fine-grained physics.

### Mechanism 3: Bidirectional Temporal Anchoring (FLF2V)
Conditioning generation on both start and end frames appears to enforce stronger narrative pacing and motion continuity than first-frame-only conditioning. The Storyboard Planner generates a "goal frame" description for complex shots. The Synthesis Module uses FLF2V (First-Last-Frame-to-Video) mode, anchoring the diffusion trajectory to both the start and end states, ensuring the motion flows toward a specific narrative target. Core assumption: The Planner can infer a logical "end state" that aligns with the subsequent shot's "start state." Evidence: "FLF2V... using both starting and goal frames for bidirectional continuity." Break condition: If the planned "goal frame" is physically impossible to reach from the "start frame" within the shot duration, the model may generate distorted morphing artifacts.

## Foundational Learning

- **Concept: Cross-Attention in Diffusion Models**
  - Why needed here: The GCM injects identity embeddings into the synthesis process. Understanding how text/image embeddings modulate the self-attention layers in DiT/UNet models is required to debug why an identity might be ignored.
  - Quick check question: Can you explain how a reference image is injected into a standard ControlNet or IP-Adapter workflow?

- **Concept: Vision-Language Model (VLM) Reasoning**
  - Why needed here: The Verifier Agent acts as a "judge." You must understand the limitations of VLMs (e.g., hallucination, lack of spatial depth perception) to design effective verification prompts.
  - Quick check question: How does a VLM typically process a video frame sequence versus a static image for logical consistency?

- **Concept: Video Diffusion Modes (I2V vs. T2V)**
  - Why needed here: CoAgent dynamically switches between T2V, FF2V, and FLF2V. Understanding the structural differences in how these models consume conditioning signals (noise vs. image latents) is vital for the Synthesis Module.
  - Quick check question: What is the difference in the denoising loop initialization between a standard Text-to-Video and an Image-to-Video model?

## Architecture Onboarding

- **Component map:** Storyboard Planner (Gemini-2.5-Flash) -> Global Context Manager (GCM) -> Synthesis Module (Wan2.1) -> Verifier Agent (GPT-4o) -> Orchestrator
- **Critical path:** The GCM Initialization. If the "Master Portrait" generated at the start is low quality or inconsistent with the style, all subsequent shots will inherit this flaw or fail the consistency check, triggering infinite regeneration loops.
- **Design tradeoffs:** Efficiency vs. Consistency (reported ~8 minutes per shot due to agentic loop; Verifier threshold τ controls this tradeoff), Control vs. Motion (FLF2V mode ensures strict transitions but may constrain dynamic motion freedom compared to standard T2V).
- **Failure signatures:** Physics Hallucinations (objects clipping through each other due to 2D backbone's lack of 3D rigidity), Metric Hacking (watermarks or specific visual patterns triggering false "High Quality" scores), Verifier Blindness (VLM failing to spot rapid temporal artifacts due to sparse frame sampling).
- **First 3 experiments:** 1) GCM Ablation: Run the "CS Student" prompt with GCM disabled and verify visually if the character's face changes between Shot 1 and Shot 4. 2) Verifier Stress Test: Deliberately provide a prompt with a semantic contradiction (e.g., "burning ice") and verify if the Verifier catches the logical failure and triggers regeneration. 3) Mode Swapping: Force the system to use only T2V mode for a continuous action sequence and measure the drop in Motion Smoothness compared to the adaptive FLF2V mode.

## Open Questions the Paper Calls Out
None

## Limitations
- Computational Efficiency: ~8 minutes per shot due to agentic loop, representing significant scalability challenge for longer videos
- VLM Verification Limitations: Verifier Agent relies on GPT-4o which lacks spatial reasoning for fine-grained physics detection
- Dataset and Generalization: Performance on diverse content types (non-human subjects, abstract concepts, highly dynamic scenes) not discussed

## Confidence

**High Confidence**: Subject consistency improvements (94.70% vs 89.95%) and background consistency gains (96.50% vs 92.89%) - quantitative metrics with clear ground truth comparisons

**Medium Confidence**: Text-video alignment improvements (2.731 vs 2.550) - measured against standard benchmarks but subjective nature of alignment quality warrants caution

**Low Confidence**: Motion smoothness claims - paper mentions this metric but provides insufficient detail on measurement methodology or comparative baselines

## Next Checks

1. **GCM Retrieval Robustness Test**: Create a dataset with visually similar but semantically distinct entities (e.g., multiple characters wearing similar clothing) and measure how often the GCM retrieves incorrect identity features, causing visual confusion between characters.

2. **Verifier Error Detection Rate**: Systematically inject known failure modes (physics violations, temporal inconsistencies, semantic contradictions) into generated videos and measure the Verifier Agent's detection rate, particularly for spatially complex errors that VLMs typically miss.

3. **Computational Cost Scaling Analysis**: Generate videos of varying lengths (5 shots, 10 shots, 20 shots) while measuring total generation time, number of regeneration cycles, and quality degradation to establish practical limits on video length and identify bottlenecks in the agentic loop.