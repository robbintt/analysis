---
ver: rpa2
title: A New Type of Adversarial Examples
arxiv_id: '2510.19347'
source_url: https://arxiv.org/abs/2510.19347
tags:
- adversarial
- uni00000013
- uni00000010
- examples
- uni0000002c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel type of adversarial example that
  is significantly different from the original input but still correctly classified
  by the target deep neural network. Unlike traditional adversarial examples that
  lie in the neighborhood of data points, these new examples are distributed far from
  the original input.
---

# A New Type of Adversarial Examples

## Quick Facts
- arXiv ID: 2510.19347
- Source URL: https://arxiv.org/abs/2510.19347
- Reference count: 7
- A novel type of adversarial example exists that is visually distinct from the original but still correctly classified by the target DNN.

## Executive Summary
This paper introduces a novel type of adversarial example that is significantly different from the original input but still correctly classified by the target deep neural network. Unlike traditional adversarial examples that lie in the neighborhood of data points, these new examples are distributed far from the original input. The authors propose four iterative gradient-based methods—NI-FGSM, NI-FGM, NMI-FGSM, and NMI-FGM—to generate such examples by minimizing the loss function under large perturbation constraints. Experiments on multiple networks (Inception-v3, Inception-v4, Inception-ResNet-v2, and ResNet-v2-152) trained on ImageNet show that these adversarial examples can achieve high success rates (up to 99% in white-box settings) while appearing visually distinct from the originals. The work demonstrates that adversarial examples are not limited to local neighborhoods but are distributed extensively in the sample space, suggesting that decision boundaries should be contracted to exclude such outliers.

## Method Summary
The authors propose four iterative gradient-based methods to generate adversarial examples that are visually distinct from the original input but still classified as the original class. The core idea is to minimize the loss function under large perturbation constraints. The methods include NI-FGSM, NI-FGM, NMI-FGSM, and NMI-FGM, which use different norms ($L_\infty$ and $L_2$) and incorporate momentum for improved stability. The update rule for NMI-FGSM is $X_{n+1} = X_n - \alpha \cdot \text{sign}(g_{n+1})$, where $g_{n+1} = \mu \cdot g_n + \nabla_X J(X_n, y) / \|\nabla_X J\|_1$. Experiments were conducted on a subset of 1,000 images from the ILSVRC2012 validation set, using four pre-trained models: Inception-v3, Inception-v4, Inception-ResNet-v2, and ResNet-v2-152.

## Key Results
- Adversarial examples can be generated that are visually distinct from the original input but still correctly classified by the target DNN.
- The proposed methods achieve high success rates (up to 99% in white-box settings) on multiple networks.
- These adversarial examples are distributed extensively in the sample space, not just in the neighborhood of data points.
- The success rate drops significantly when using larger perturbations (e.g., to 65% for NI-FGM with $\delta=10,000$).

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Perturbing an input iteratively in the direction that minimizes the model's loss function can generate inputs that are visually distinct from the original but are still classified as the original class.
- **Mechanism:** The method (e.g., NI-FGSM) iteratively updates an input $X$ by subtracting a scaled sign of the gradient of the loss with respect to $X$: $X_{adv}^{n+1} = X_{adv}^n - \alpha \cdot \text{sign}(\nabla_X J(X_{adv}^n, y_{true}))$. This process finds points in the input space that move deeper into the region classified as the target class, even if they are far from the original data distribution.
- **Core assumption:** The linear approximation of the loss function holds sufficiently well in the direction of the negative gradient over small step sizes, allowing the iterative process to converge to a minimum within the class's decision boundary.
- **Evidence anchors:**
  - [abstract]: "...minimizing the loss function under large perturbation constraints."
  - [section]: "We linearize the loss function and perturb the input iteratively along the gradients to solve the constrained optimization problem."
  - [corpus]: "Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers" relates to vulnerabilities in such iterative processes.
- **Break condition:** The linear approximation fails over large distances, causing the iterative path to cross decision boundaries and leading to misclassification (as seen with larger perturbations).

### Mechanism 2
- **Claim:** Integrating a momentum term into the iterative update rule improves the stability and success rate of generating these new adversarial examples, especially in white-box settings.
- **Mechanism:** Momentum methods (e.g., NMI-FGSM) accumulate a velocity vector from past gradients, which helps to smooth out oscillations and propel the optimization over small local minima or flat regions in the loss landscape. The update uses this accumulated direction ($g_{n+1}$) instead of just the current gradient.
- **Core assumption:** The loss landscape contains local optima or flat regions that would trap a simple gradient descent, and the accumulated history of gradients points toward a better minimum.
- **Evidence anchors:**
  - [section]: "The accumulation helps to accelerate gradient descent algorithms and barrel through local optimum..."
  - [section]: "When the decay factor is 1.0, both attack methods achieve an attack success rate of 99%."
  - [corpus]: "Optimizing the Adversarial Perturbation with a Momentum-based Adaptive Matrix" confirms momentum is a key technique in this domain.
- **Break condition:** Excessive momentum (decay factor > 1.0) can cause the update to overshoot, as the accumulated history obscures important new gradient information from the current location.

### Mechanism 3
- **Claim:** Adversarial examples for a given class are not confined to a small local neighborhood around natural data points but are distributed extensively throughout the sample space.
- **Mechanism:** DNNs learn complex, high-dimensional decision boundaries that can form large, contiguous regions for a single class. The paper's method demonstrates that by starting from a point and moving in a specific loss-minimizing direction, one can find points arbitrarily far from the start that still lie within the same decision region.
- **Core assumption:** The learned decision boundaries for a class are expansive and potentially overly so, capturing regions of the input space that are far from the true data manifold.
- **Evidence anchors:**
  - [abstract]: "...adversarial examples are not merely distributed in the neighbourhood of the examples from the dataset; instead, they are distributed extensively in the sample space."
  - [section]: "...the results show that the distribution of adversarial examples is extremely wide, extending not only to the neighborhood of the data points but also to regions far from them."
  - [corpus]: Corpus evidence on the specific shape of decision boundaries is weak; related papers focus on optimization techniques.
- **Break condition:** This extensive subspace appears to be highly model-dependent. The generated examples have very low transferability (black-box success rates <4%), suggesting the specific shape of these expansive decision regions is unique to each model.

## Foundational Learning

- **Concept: Gradient-Based Optimization**
  - **Why needed here:** The core methods proposed (NI-FGSM, NMI-FGSM) are iterative optimization algorithms that rely on computing the gradient of a loss function with respect to an input.
  - **Quick check question:** If the gradient of the loss is zero, in what direction would NI-FGSM update the input?
- **Concept: Decision Boundaries in High-Dimensional Space**
  - **Why needed here:** The paper's central finding is about the nature and extent of the decision boundaries learned by DNNs. Understanding that these are not simple lines but complex, high-dimensional surfaces is crucial.
  - **Quick check question:** Why would an "expansive" decision boundary be considered a potential vulnerability in a classifier?
- **Concept: White-Box vs. Black-Box Attacks**
  - **Why needed here:** The paper distinguishes between these two settings in its experiments. The methods require white-box access (model weights/gradients), while the low success in black-box settings highlights the model-specific nature of the generated examples.
  - **Quick check question:** Why do the generated adversarial examples have such low transferability to other models (black-box attack success)?

## Architecture Onboarding

- **Component map:** The Loss Function ($J$) acts as the compass, its Gradient ($\nabla_X J$) as the engine, and the Update Rule as the vehicle that modifies the Input ($X$) across the Model's ($f$) decision landscape.
- **Critical path:** A new engineer should first implement NI-FGSM (Equation 8). The steps are: 1) Load a pre-trained model and an image. 2) Compute the gradient of the loss with respect to the image. 3) Update the image by subtracting a small step in the direction of the sign of the gradient. 4) Repeat for $N$ iterations.
- **Design tradeoffs:** There is a direct tradeoff between perturbation size and success rate; larger perturbations risk crossing the decision boundary. Momentum improves stability but requires tuning the decay factor ($\mu$), where too high a value can degrade performance.
- **Failure signatures:**
  - **Low Success Rate (White-Box):** May indicate a bug in gradient calculation or an overly aggressive step size.
  - **High Success Rate (Black-Box):** Would contradict the paper's findings and could suggest the models are very similar.
  - **Visually Identical Output:** Indicates the total perturbation is too small; increase iterations or step size.
- **First 3 experiments:**
  1. **Baseline Reproduction (Inc-v3):** Implement NI-FGSM and reproduce the white-box success rate curve from Figure 2 on a subset of the ILSVRC2012 validation set.
  2. **Momentum Ablation (NMI-FGSM):** Run the experiment from Figure 6 by varying the decay factor ($\mu$). Verify the peak success rate is near $\mu=1.0$.
  3. **Transferability Check:** Generate adversarial examples using Inc-v3 and test them against Inc-v4 and ResNet-v2-152. Confirm that the black-box success rates are very low (<5%), aligning with Table 1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more effective generation methods be developed to improve the success rates of these adversarial examples, particularly under extremely large perturbation constraints?
- Basis in paper: [explicit] The authors state in the conclusion that they will "focus on developing more effective methods for generating the new type of adversarial examples."
- Why unresolved: Current success rates drop significantly (e.g., to 65% for NI-FGM) when perturbation sizes become very large ($\delta=10,000$), indicating the optimization struggles to maintain the target class over vast distances in sample space.
- What evidence would resolve it: Novel algorithms that sustain >95% success rates even as the $L_p$ distance constraint increases substantially beyond the current experimental maximums.

### Open Question 2
- Question: How can the properties of these "far-field" adversarial examples be practically utilized for secure image encryption or information hiding?
- Basis in paper: [explicit] The paper suggests these examples can be utilized to "hide image information since they almost look like meaningless noise images," proposing encryption as a potential application direction.
- Why unresolved: The paper demonstrates the generation of the examples but does not implement or test a specific encryption scheme or information retrieval mechanism based on this phenomenon.
- What evidence would resolve it: A functional prototype where meaningful data is reconstructed from these noise-like images using a specific key or model, demonstrating robustness against interception.

### Open Question 3
- Question: How can defense mechanisms or training protocols be modified to "contract" decision boundaries to exclude these distant outliers without degrading performance on natural inputs?
- Basis in paper: [inferred] The paper concludes that "decision boundary should be appropriately contracted to exclude these outliers," yet provides no methodology for achieving this contraction.
- Why unresolved: While the paper identifies that boundaries are too expansive, standard regularization or adversarial training typically expands boundaries to include local perturbations; the inverse operation (excluding distant points) is unexplored.
- What evidence would resolve it: A modified training regimen that results in a classifier which correctly rejects the "negative" adversarial examples (treating them as unrecognizable/noise) while maintaining accuracy on the validation set.

## Limitations
- The findings may be highly model-dependent, as evidenced by the low black-box transferability rates.
- The paper does not provide a detailed analysis of the specific shape of decision boundaries learned by different DNN architectures.
- The proposed methods rely on linear approximations that may fail over large distances in the input space.

## Confidence

- **High Confidence:** The existence of adversarial examples far from the original input and the effectiveness of the proposed iterative gradient-based methods (NI-FGSM, NMI-FGSM) in generating them within a specific model's decision region.
- **Medium Confidence:** The claim that these adversarial examples are "distributed extensively in the sample space" is supported by the experimental results but requires further investigation to generalize across different model architectures and datasets.
- **Low Confidence:** The paper's assertion that decision boundaries should be "contracted" to exclude such outliers is a theoretical suggestion based on the findings, but the practical methods for achieving this are not explored.

## Next Checks

1. **Decision Boundary Analysis:** Conduct a detailed analysis of the decision boundaries learned by different DNN architectures (e.g., Inception, ResNet) on ImageNet to quantify the extent and shape of contiguous regions for each class. Use techniques like gradient ascent from the adversarial examples to map out the boundaries.
2. **Model Transferability Study:** Systematically evaluate the transferability of the generated adversarial examples across a wider range of models (e.g., different architectures, training procedures) to determine if the low black-box success rates are consistent and to identify any patterns or correlations.
3. **Robustness to Dataset Shift:** Test the proposed methods on datasets with different characteristics (e.g., CIFAR-10, medical imaging) to assess the generalizability of the findings and to identify any dataset-specific vulnerabilities or strengths in the decision boundaries.