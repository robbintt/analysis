---
ver: rpa2
title: 'Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings'
arxiv_id: '2506.15001'
source_url: https://arxiv.org/abs/2506.15001
tags:
- text
- embedding
- sequence
- llama
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces memory tokens, a method to generate reversible\
  \ sentence embeddings by training a special token\u2019s embedding to reconstruct\
  \ a fixed text sequence. Using a frozen large language model (LLM), the memory token\
  \ embedding is optimized via autoregressive training on a single sequence."
---

# Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings

## Quick Facts
- **arXiv ID:** 2506.15001
- **Source URL:** https://arxiv.org/abs/2506.15001
- **Reference count:** 11
- **Key outcome:** Memory tokens enable exact reconstruction of text sequences up to ~240 tokens using frozen LLMs, with Llama 3.1 8B achieving perfect results while smaller models show degradation on longer sequences.

## Executive Summary
This work introduces memory tokens, a novel method for generating reversible sentence embeddings using frozen large language models. By training a special token's embedding to reconstruct a fixed text sequence, the method enables exact text reconstruction through greedy decoding when the memory token is provided as input. The approach demonstrates successful reconstruction across English and Spanish datasets using models ranging from 100M to 8B parameters, with Llama 3.1 8B achieving perfect reconstruction of all test sequences.

The method has potential applications in memory-based retrieval, text compression, and controlled generation, but also raises concerns about misuse in adversarial attacks. The performance varies significantly with model size, with smaller models showing degraded accuracy on longer sequences. This technique represents a new approach to reversible embeddings that could impact both practical applications and security considerations in NLP systems.

## Method Summary
Memory tokens work by training a special token's embedding to reconstruct a fixed text sequence while keeping the underlying large language model frozen. The memory token embedding is optimized through autoregressive training on a single sequence, allowing the model to learn a compressed representation that can later be used to exactly reconstruct the original text. When this trained memory token is provided as input to the frozen LLM, the model can regenerate the original sequence through greedy decoding. The method was evaluated across multiple model sizes (100M to 8B parameters) and languages (English and Spanish), with sequences up to approximately 240 tokens in length.

## Key Results
- Llama 3.1 8B successfully reconstructed all test sequences exactly using memory tokens
- Smaller models (100M-3B parameters) showed accuracy degradation on longer sequences
- Method worked across both English and Spanish datasets with consistent results
- Exact reconstruction was achieved through greedy decoding when memory token was provided as input

## Why This Works (Mechanism)
The mechanism works by leveraging the frozen LLM's existing knowledge while training only the memory token embedding to encode the target sequence. Since the LLM parameters remain fixed, the model cannot learn new capabilities but can optimize the memory token to trigger the correct generation sequence through its existing pathways. The autoregressive training on a single sequence forces the memory token embedding to capture the essential information needed for reconstruction, creating a reversible mapping between the token embedding and the original text.

## Foundational Learning
- **Autoregressive training:** Why needed - enables sequential generation of text from learned representations. Quick check - model can predict next token given previous context.
- **Token embeddings:** Why needed - represent discrete text units in continuous vector space. Quick check - embedding space supports meaningful semantic relationships.
- **Greedy decoding:** Why needed - deterministic generation method for exact reconstruction verification. Quick check - highest probability token selected at each step.
- **Frozen model training:** Why needed - prevents catastrophic forgetting while allowing embedding optimization. Quick check - model parameters remain unchanged during memory token training.
- **Sequence reconstruction:** Why needed - validates reversible nature of learned embeddings. Quick check - original text can be exactly regenerated from memory token.

## Architecture Onboarding

**Component map:** Memory token embedding -> Frozen LLM -> Text sequence reconstruction

**Critical path:** Training phase: memory token embedding optimization -> Inference phase: memory token input -> Greedy decoding output

**Design tradeoffs:** Training only embeddings (fast, memory-efficient) vs. full model fine-tuning (potentially better performance but computationally expensive and risks catastrophic forgetting)

**Failure signatures:** Degraded reconstruction quality on longer sequences with smaller models, suggesting capacity limitations in learned representations

**Three first experiments:**
1. Test exact reconstruction capability on shorter sequences (â‰¤50 tokens) with various model sizes
2. Evaluate reconstruction accuracy degradation as sequence length increases
3. Compare reconstruction quality using different decoding strategies (greedy vs. beam search)

## Open Questions the Paper Calls Out
The paper highlights potential misuse of memory tokens in adversarial attacks, raising security and ethical concerns that require further investigation. The scalability of the method to smaller models and longer sequences also remains an open question, particularly regarding the performance degradation observed with increased sequence length.

## Limitations
- Performance degradation in smaller models (100M-3B parameters) on longer sequences suggests scalability challenges
- Reliance on freezing large language models may limit adaptability to different domains or languages without retraining
- Potential misuse in adversarial attacks raises ethical and security concerns requiring further investigation

## Confidence

**High confidence:** Feasibility of generating reversible sentence embeddings using memory tokens with large models like Llama 3.1 8B, demonstrated by successful exact reconstruction

**Medium confidence:** Method's scalability to smaller models and longer sequences, given observed accuracy degradation patterns

**Low confidence:** Robustness of memory tokens across diverse domains and languages without additional training or fine-tuning

## Next Checks
1. Test memory tokens on multilingual datasets beyond English and Spanish to assess cross-lingual performance
2. Evaluate the method's robustness against adversarial attacks to quantify potential misuse risks
3. Investigate scalability of memory tokens to models smaller than 8B parameters and sequences longer than 240 tokens to identify performance limits