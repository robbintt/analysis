---
ver: rpa2
title: 'ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT'
arxiv_id: '2506.04929'
source_url: https://arxiv.org/abs/2506.04929
tags:
- translation
- context
- image
- machine
- product
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ConECT, a new Czech-to-Polish e-commerce
  product translation dataset consisting of 11,400 sentence pairs with images and
  product metadata. The authors evaluate three approaches for context-aware machine
  translation: (1) fine-tuning a vision-language model (PaliGemma) with images, (2)
  incorporating category paths as special instruction tokens in text-to-text models,
  and (3) adding synthetic image descriptions as prefixes.'
---

# ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT

## Quick Facts
- arXiv ID: 2506.04929
- Source URL: https://arxiv.org/abs/2506.04929
- Authors: Mikołaj Pokrywka; Wojciech Kusa; Mieszko Rutkowski; Mikołaj Koszowski
- Reference count: 19
- Primary result: ConECT dataset (11,400 Czech-Polish pairs) shows VLM fine-tuning with images improves translation quality; category paths help, but synthetic image descriptions degrade performance.

## Executive Summary
This paper introduces ConECT, a new Czech-to-Polish e-commerce product translation dataset consisting of 11,400 sentence pairs with images and product metadata. The authors evaluate three approaches for context-aware machine translation: (1) fine-tuning a vision-language model (PaliGemma) with images, (2) incorporating category paths as special instruction tokens in text-to-text models, and (3) adding synthetic image descriptions as prefixes. Results show that visual context in VLM improves translation quality, and category paths enhance performance in text-to-text models, particularly for product names. However, using synthetic image descriptions as prefixes degraded translation quality, highlighting that context integration can sometimes impair model performance. The dataset is made publicly available for future research on context-aware translation.

## Method Summary
The authors created ConECT by crawling 20,000 products from a Polish e-commerce platform, then extracting product names, descriptions, and offer titles in Czech and Polish, aligning them using sentence embeddings. They trained three context-aware MT systems: a VLM (PaliGemma-3b) with LoRA fine-tuning on image-text pairs, a text-only Transformer-big with category paths as special tokens, and a version with synthetic image descriptions as prefixes. They evaluated on 10,295 test sentences using chrF and COMET metrics, comparing against strong baselines trained on 53M parallel sentences.

## Key Results
- PaliGemma VLM with real images outperforms text-only baseline (COMET 0.9152 vs. 0.9091)
- Category path tokens improve translation quality, especially for product names (COMET 0.9385 vs. 0.9372)
- Synthetic image descriptions as prefixes severely degrade performance (COMET 0.8219 vs. 0.9341 baseline)
- VLM visual context provides consistent gains across product names and offer titles

## Why This Works (Mechanism)

### Mechanism 1: Visual Context Grounding in VLM Fine-tuning
- Claim: Visual context in VLMs improves translation quality for domain-specific content when images are semantically aligned with text.
- Mechanism: Fine-tuning PaliGemma with product images enables the visual encoder to ground textual tokens to visual features, which aids disambiguation of lexically ambiguous terms (e.g., "pen" as writing instrument vs. animal enclosure).
- Core assumption: The visual encoder retains meaningful image-text alignments during fine-tuning and does not suffer from catastrophic forgetting.
- Evidence anchors:
  - [abstract]: "We test a vision-language model (VLM), finding that visual context aids translation quality."
  - [Table 2]: PaliGemma with real images at both train and inference achieves COMET 0.9152 vs. 0.9091 with black images—a consistent improvement across product names and offer titles.
  - [corpus]: Weak direct corpus support; neighbor papers address e-commerce product generation and search but not multimodal MT mechanisms.
- Break condition: If images are absent or mismatched at inference (black image with real-image training), performance degrades but does not collapse, suggesting partial context reliance.

### Mechanism 2: Category Path Disambiguation via Special Tokens
- Claim: Encoding product category paths as special instruction tokens improves translation quality in text-to-text NMT, especially for short, terminology-heavy text.
- Mechanism: The category path provides hierarchical semantic context (e.g., "Sports » Bicycles » Tires") that constrains the model's lexical choices, reducing ambiguity.
- Core assumption: Category paths contain disambiguating information not already present in source text, and the model learns to attend to them.
- Evidence anchors:
  - [abstract]: "incorporating category paths as special instruction tokens in text-to-text models" enhances performance.
  - [Table 2]: Category context model achieves COMET 0.9362 vs. 0.9354 without—most notable for product names (COMET 0.9385 vs. 0.9372).
  - [corpus]: No direct corpus validation; related work on e-commerce metadata exists but does not confirm this specific tokenization mechanism.
- Break condition: If category paths are noisy, inconsistent, or missing for rare products, the model may over-rely on prefix tokens and under-utilize source text.

### Mechanism 3: Synthetic Image Description Prefixes (Negative Result)
- Claim: Prefixing source text with synthetic image descriptions degrades translation quality.
- Mechanism: Generated descriptions introduce noise, hallucinations, or irrelevant details that mislead the NMT model, causing it to attend to distractor content.
- Core assumption: The image description model captures translation-relevant visual information—this appears violated.
- Evidence anchors:
  - [abstract]: "using synthetic image descriptions as prefixes degraded translation quality."
  - [Table 2]: Description context model collapses to COMET 0.8219 vs. 0.9341 without—especially severe on product descriptions (COMET 0.7243 vs. 0.9358).
  - [corpus]: No corpus validation for this negative finding.
- Break condition: This mechanism consistently degrades performance; any use requires reformulation (e.g., better description quality, selective application).

## Foundational Learning

- Concept: Transformer Cross-Attention and Context Disambiguation
  - Why needed here: The paper addresses ambiguity in sentence-level NMT; understanding how attention mechanisms can incorporate external context (images, metadata) is prerequisite.
  - Quick check question: Can you explain how cross-attention allows a decoder to condition on multiple input modalities?

- Concept: Vision-Language Model Alignment
  - Why needed here: PaliGemma combines a SigLIP visual encoder with a Gemma LLM; understanding image-text alignment is critical for interpreting VLM MT results.
  - Quick check question: What is the role of the visual encoder output tokens in a VLM's language generation process?

- Concept: Special Token Design for Context Injection
  - Why needed here: The paper uses <SC>, <EC>, <SD>, <ED> tokens to demarcate context; improper tokenization can cause context leakage or neglect.
  - Quick check question: Why must special tokens be added to the vocabulary before SentencePiece training rather than post-hoc?

## Architecture Onboarding

- Component map: PaliGemma-3b (SigLIP + Gemma) -> LoRA fine-tuning -> Image-conditioned translation; Transformer-big (Marian) -> SentencePiece vocab -> Text-to-text NMT; Context prefixes -> Special tokens (<SC>, <EC>, <SD>, <ED>) -> Context injection

- Critical path:
  1. Data preparation: Align parallel sentences with images and category metadata
  2. Vocabulary extension: Add special tokens (<SC>, <EC>, etc.) before training
  3. Fine-tuning: LoRA for VLM (r=8, α=8); full fine-tuning for NMT with prefix data
  4. Inference: Ensure context alignment (real images or correct category paths)

- Design tradeoffs:
  - VLM vs. NMT: VLM requires ~8x more GPU memory (3B params + image processing) but provides visual grounding; NMT is lightweight but context-limited
  - Category vs. image context: Category paths are deterministic and cheap; images require storage and compute but capture visual attributes
  - Synthetic descriptions: Attractive for pure-text pipelines but risk quality collapse—avoid without validation

- Failure signatures:
  - COMET drops sharply with description prefixes (0.82 vs. 0.93 baseline): Indicates context noise or model confusion
  - VLM chrF increases but COMET decreases on product descriptions: Signals surface-level n-gram matching without semantic improvement
  - Category context chrF decreases while COMET improves: May indicate over-regularization to category terms at the cost of lexical diversity

- First 3 experiments:
  1. Ablation on image presence: Train VLM with real images, evaluate with real vs. black images to isolate visual contribution
  2. Category path noise injection: Randomly shuffle or mask category paths to test robustness of context reliance
  3. Description quality filtering: Filter synthetic descriptions by confidence or length; re-evaluate prefix approach to identify if noise is the primary failure mode

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the integration of image descriptions as context be refined to prevent the degradation of translation quality observed in simple prefix-based approaches?
- Basis in paper: [explicit] The authors state in the conclusion that their experiments with image description prefixes yielded negative results, highlighting that "this straightforward approach requires further refinement."
- Why unresolved: The paper demonstrated that adding synthetic descriptions via prefixes hurts performance (COMET score drop), but did not test alternative integration architectures or denoising strategies.
- What evidence would resolve it: A study comparing prefix-based integration against attention-based fusion mechanisms, or an analysis showing performance recovery when using ground-truth human descriptions instead of synthetic ones.

### Open Question 2
- Question: Can a mechanism be developed to determine when contextual information (visual or metadata) is beneficial versus when it introduces noise or hallucinations?
- Basis in paper: [inferred] The limitations section notes that "text alone is sufficient" in many cases and that "incorporating extra context can sometimes reduce translation quality, especially with LLMs."
- Why unresolved: The current methodology involves appending context indiscriminately; the paper does not explore dynamic context gating or relevance filtering.
- What evidence would resolve it: Experiments implementing a context-actuation gate (e.g., based on textual ambiguity scores) that shows improved performance by selectively disabling context for unambiguous inputs.

### Open Question 3
- Question: Does the utility of category paths as context generalize to languages with lower-resource status or non-Slavic linguistic structures?
- Basis in paper: [inferred] The study focuses on a specific Czech-to-Polish pair (closely related Slavic languages) and notes the pair is "under-researched," leaving the transferability of the category-path findings unresolved.
- Why unresolved: The morphological similarities between Czech and Polish might facilitate category-path transfer in ways that do not apply to morphologically distant or structurally different language pairs.
- What evidence would resolve it: Replicating the category-path fine-tuning experiments on a diverse set of language pairs (e.g., English-Japanese or Czech-German) to observe if the COMET score improvements persist.

## Limitations
- Dataset size and scope: 11,400 sentence pairs is relatively small and focused only on Czech-Polish e-commerce products
- Limited explanation of negative results: The paper shows synthetic descriptions degrade performance but doesn't explore why or potential remedies
- Language pair specificity: Results may not generalize to non-Slavic or lower-resource language pairs

## Confidence
- **High confidence**: VLM fine-tuning with real images improves translation quality over text-only baselines (consistent COMET improvements across test sets)
- **Medium confidence**: Category path tokens improve translation quality, particularly for product names (effect size is small but consistent)
- **Medium confidence**: Synthetic image descriptions degrade translation quality (strong negative result, but mechanism unexplained)

## Next Checks
1. Investigate the mechanism behind synthetic description failure by analyzing the generated descriptions for hallucinations, irrelevance, or translation noise
2. Test category path robustness by evaluating performance when category metadata is noisy, missing, or randomly shuffled
3. Validate VLM context contribution by conducting a human evaluation study to determine whether visual context actually improves semantic understanding or merely surface-level matching