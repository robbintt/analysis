---
ver: rpa2
title: Performance Evaluation of Large Language Models in Bangla Consumer Health Query
  Summarization
arxiv_id: '2505.05070'
source_url: https://arxiv.org/abs/2505.05070
tags:
- bangla
- llms
- arxiv
- summarization
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the zero-shot performance of nine large language
  models (LLMs) for summarizing Bangla consumer health queries, a low-resource language
  task. Using the BanglaCHQ-Summ dataset of 2,350 annotated query-summary pairs, the
  models were assessed using ROUGE metrics and compared against a fine-tuned Bangla
  T5 model.
---

# Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization

## Quick Facts
- arXiv ID: 2505.05070
- Source URL: https://arxiv.org/abs/2505.05070
- Authors: Ajwad Abrar; Farzana Tabassum; Sabbir Ahmed
- Reference count: 36
- Key outcome: Mixtral-8x22b-Instruct achieves ROUGE-1 (51.36) and ROUGE-L (49.17) performance comparable to fine-tuned Bangla T5, demonstrating zero-shot LLMs can rival fine-tuned models in low-resource language summarization

## Executive Summary
This study evaluates nine large language models (LLMs) on zero-shot Bangla consumer health query summarization using the BanglaCHQ-Summ dataset of 2,350 annotated query-summary pairs. The evaluation compares proprietary models (GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro) and open-source models (Llama3-70B-Instruct, Mixtral-8x22B-Instruct, Qwen2-72B-Instruct, Gemma-2-27B, Athene-70B) against a fine-tuned Bangla T5 baseline. Mixtral-8x22b-Instruct emerged as the top performer, achieving ROUGE-1 (51.36) and ROUGE-L (49.17) scores that rival the fine-tuned Bangla T5 model, while Bangla T5 excelled in ROUGE-2 (29.11). The results demonstrate that zero-shot LLMs can generate high-quality summaries without task-specific training, offering a viable approach for low-resource language applications.

## Method Summary
The study employs zero-shot inference on nine LLMs using a standardized prompt: "Provide a concise summary of the following long Bangla consumer health query..." The BanglaCHQ-Summ dataset provides 2,350 query-summary pairs across 32 health categories. Models are evaluated using ROUGE-1, ROUGE-2, and ROUGE-L metrics against gold standard summaries. The evaluation compares zero-shot performance against a fine-tuned Bangla T5 model trained specifically on this dataset. Some models initially generated English outputs instead of Bangla, requiring explicit language instructions in the prompt.

## Key Results
- Mixtral-8x22b-Instruct achieved the highest ROUGE-1 (51.36) and ROUGE-L (49.17) scores among zero-shot models
- Bangla T5 outperformed all zero-shot models in ROUGE-2 (29.11 vs. Mixtral's 14.41)
- Zero-shot LLMs achieved competitive ROUGE-1 and ROUGE-L performance compared to the fine-tuned baseline
- Summary length alignment with gold standards correlated positively with ROUGE performance

## Why This Works (Mechanism)

### Mechanism 1: Cross-Lingual Transfer Enables Zero-Shot CHQ Summarization
Large pre-trained LLMs encode generalizable summarization capabilities and cross-lingual representations during multilingual pre-training. When prompted with Bangla CHQs, these patterns activate to identify salient medical information and generate coherent summaries without task-specific training. This works because pre-training data contains sufficient Bangla text and summarization patterns to support transfer. Break condition: Insufficient Bangla medical terminology in pre-training data degrades performance.

### Mechanism 2: Fine-Tuning Captures Task-Specific Bi-Gram Patterns (ROUGE-2 Gap)
Fine-tuned models like Bangla T5 learn domain-specific word collocations and summary style patterns that preserve local sequence coherence. Zero-shot LLMs generate semantically valid summaries but produce different word sequences (synonyms, alternative phrasings) that reduce bi-gram matches while maintaining unigram overlap. Break condition: If evaluation shifts to semantic similarity metrics, the fine-tuning advantage in ROUGE-2 may not reflect actual quality differences.

### Mechanism 3: Summary Length Alignment Correlates with ROUGE Performance
ROUGE metrics compute overlap relative to reference length. Models whose output length distribution matches the gold standard optimize the precision-recall trade-off inherent in ROUGE. Break condition: Inconsistent reference summary lengths or failed prompt length constraints weaken the correlation.

## Foundational Learning

- **Concept: ROUGE Metrics Family (ROUGE-1, ROUGE-2, ROUGE-L)**
  - Why needed here: The entire evaluation framework relies on ROUGE; understanding what each variant measures is essential for interpreting performance differences.
  - Quick check question: Why does Bangla T5 achieve 29.11 ROUGE-2 while Mixtral achieves only 14.41, despite Mixtral outperforming on ROUGE-1 and ROUGE-L?

- **Concept: Zero-Shot vs. Fine-Tuned Learning Paradigm**
  - Why needed here: The paper's central claim—that zero-shot LLMs can rival fine-tuned models—hinges on understanding the trade-offs between generalization without task-specific training versus specialization through supervised fine-tuning.
  - Quick check question: What practical advantages do zero-shot LLMs offer for deploying CHQ summarization in low-resource languages, and what performance trade-offs should practitioners expect?

- **Concept: Low-Resource Language Challenges in NLP**
  - Why needed here: The paper's motivation stems from Bangla's underrepresentation in NLP resources; understanding this context explains why these findings matter for ~300 million Bangla speakers.
  - Quick check question: Why might zero-shot LLMs be particularly valuable for languages like Bangla compared to English, where fine-tuned biomedical models are readily available?

## Architecture Onboarding

- **Component map**: Bangla CHQs from daktarbhai.com -> Standardized prompt with language/length constraints -> Nine LLMs (GPT-3.5-Turbo, GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro, Llama3-70B-Instruct, Mixtral-8x22B-Instruct, Qwen2-72B-Instruct, Gemma-2-27B, Athene-70B) -> ROUGE-1/2/L computation against gold summaries

- **Critical path**: Dataset ingestion -> Prompt design iteration -> Batch inference across all models -> ROUGE computation and length analysis

- **Design tradeoffs**: Zero-shot vs. fine-tuned (scalability vs. task-specific accuracy), Proprietary vs. open-source (API accessibility vs. self-hosted inference), Summary verbosity (detail capture vs. ROUGE precision)

- **Failure signatures**: Language switching (models initially responding in English), Over-verbose outputs, Pronunciation variability (different spellings for same words), ROUGE-2 underperformance (50% gap vs. fine-tuned model)

- **First 3 experiments**: 1) Baseline prompt evaluation to identify language-switching issues, 2) Prompt iteration for language/length control, 3) Error analysis on pronunciation variants to distinguish semantic errors from spelling mismatches

## Open Questions the Paper Calls Out
- Does text normalization to handle pronunciation variability improve ROUGE scores in Bangla summarization? (The authors identify this as warranting further research but don't implement mitigation strategies)
- How do zero-shot LLM summaries compare to fine-tuned models in terms of factual consistency and semantic accuracy? (The study relies exclusively on ROUGE metrics, which may not capture semantic equivalence)
- Can supervised fine-tuning of top-performing LLMs close the performance gap with Bangla T5 on the ROUGE-2 metric? (The study's zero-shot restriction leaves this unexplored)

## Limitations
- Evaluation relies exclusively on ROUGE metrics, which capture n-gram overlap but not semantic fidelity, factual consistency, or clinical utility
- Results may not generalize across different prompt formulations since only one prompt template was tested
- Analysis doesn't account for potential biases in gold standard annotations or whether reference summaries represent optimal condensation

## Confidence
- **High confidence**: Zero-shot LLMs can achieve competitive ROUGE-1 and ROUGE-L performance compared to fine-tuned models for Bangla CHQ summarization
- **Medium confidence**: The ROUGE-2 gap reflects genuine limitations in capturing domain-specific bi-gram patterns rather than semantic equivalence
- **Low confidence**: The specific ranking of LLMs represents robust, generalizable performance differences across diverse CHQ types

## Next Checks
1. **Human evaluation study**: Recruit medical experts to rate 100 summaries (50 from top zero-shot LLM, 50 from fine-tuned Bangla T5) on clinical accuracy, completeness, and readability; compare expert rankings against ROUGE scores
2. **Semantic similarity analysis**: Compute BERTScore and BLEURT scores for the same 100-summary sample to assess whether zero-shot models achieve semantic equivalence despite lower ROUGE-2
3. **Prompt ablation experiment**: Systematically vary the prompt across language instruction strength, length constraint, and conciseness emphasis; measure performance variance to quantify prompt impact vs. inherent model capabilities