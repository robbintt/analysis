---
ver: rpa2
title: 'InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented
  Generation'
arxiv_id: '2510.21538'
source_url: https://arxiv.org/abs/2510.21538
tags:
- hallucination
- detection
- context
- knowledge
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a mechanistic interpretability-based approach
  for detecting hallucinations in retrieval-augmented generation (RAG) systems. The
  method computes external context scores (ECS) and parametric knowledge scores (PKS)
  across model layers and attention heads to quantify reliance on retrieved information
  versus internal knowledge.
---

# InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation

## Quick Facts
- **arXiv ID:** 2510.21538
- **Source URL:** https://arxiv.org/abs/2510.21538
- **Authors:** Likun Tan; Kuan-Wei Huang; Joy Shi; Kevin Wu
- **Reference count:** 32
- **Primary result:** Proposes ECS/PKS mechanistic signals for RAG hallucination detection, achieving 75.36% F1 on GPT-4.1-mini responses via proxy-model evaluation

## Executive Summary
This paper introduces InterpDetect, a mechanistic interpretability approach for detecting hallucinations in retrieval-augmented generation (RAG) systems. The method computes External Context Scores (ECS) and Parametric Knowledge Scores (PKS) across model layers and attention heads to quantify reliance on retrieved information versus internal knowledge. Using Qwen3-0.6b, these scores are computed at the span level and used as features to train regression-based classifiers (SVC, Logistic Regression, Random Forest, XGBoost) for hallucination detection. The approach achieves F1 scores of 74.68% in self-evaluation and 75.36% in proxy-based evaluation, outperforming several commercial detection systems and demonstrating effective proxy-model generalization.

## Method Summary
InterpDetect extracts mechanistic signals from RAG systems to detect hallucinations. For each (response, context) pair, it computes ECS by measuring attention-weighted semantic alignment between response chunks and retrieved context chunks, and PKS by measuring vocabulary distribution divergence before/after FFN layers. These 476 features (28 layers × 16 heads + 28 PKS) are reduced to 341 via correlation filtering, then used to train classifiers on span-level hallucination labels. The method demonstrates effective proxy-model generalization, with classifiers trained on Qwen3-0.6b signals detecting hallucinations in GPT-4.1-mini outputs with 75.36% F1.

## Key Results
- Achieves 74.68% F1 in self-evaluation on Qwen3-0.6b responses
- Achieves 75.36% F1 in proxy-based evaluation on GPT-4.1-mini responses
- Outperforms commercial systems (TruLens, RefChecker) and smaller open-source models
- Demonstrates effective proxy-model generalization from 0.6B to 1.5B parameter models

## Why This Works (Mechanism)

### Mechanism 1: External Context Score (ECS) for Grounding Quantification
ECS computes cosine similarity between response chunk embeddings and the most-attended context chunk per attention head. Lower attention-mediated semantic alignment between response chunks and retrieved context correlates with higher hallucination likelihood. Core assumption: attention weights meaningfully reflect information routing decisions that influence output grounding.

### Mechanism 2: Parametric Knowledge Score (PKS) for Internal Knowledge Injection
PKS measures JSD between vocabulary distributions before and after FFN layers. Higher divergence indicates greater FFN-driven token probability shifts, reflecting parametric knowledge overriding contextual grounding. Core assumption: the unembedding projection meaningfully captures which tokens the FFN "proposes."

### Mechanism 3: Proxy-Model Generalization via Architecture-Agnostic Signals
The relative balance of ECS vs. PKS is assumed to be a general property of hallucinations, not model-specific. A 0.6B model extracts these relative signals from any (response, context) pair. Core assumption: the mechanistic relationship between grounding and parametric knowledge holds across architectures.

## Foundational Learning

- **Residual Stream Architecture:** Why needed: ECS/PKS computation requires understanding how attention and FFN outputs are combined into the residual stream. Quick check: Can you trace how a token's hidden state is updated across one transformer layer?
- **Mechanistic Interpretability Hooks:** Why needed: Extracting attention patterns and FFN intermediate states requires access to internal activations beyond standard APIs. Quick check: What library does InterpDetect use to access internal model states?
- **Feature Selection for High-Dimensional Signals:** Why needed: 476 raw features require correlation filtering to prevent overfitting. Quick check: Why does XGBoost overfit while SVC generalizes better with limited data?

## Architecture Onboarding

- **Component map:** Data Ingestion → Response Generation (Qwen3-0.6b or GPT-4.1-mini) → Span Chunking → ECS/PKS Computation (TransformerLens) → Feature Selection → SVC Classifier → Span/Response Labels
- **Critical path:** Hook into model forward pass to capture attention weights and residual stream states; project residual states through unembedding matrix; aggregate token-level scores to span-level features; apply SmartCorrelatedSelection to reduce 476→341 features; train SVC on span-level labels.
- **Design tradeoffs:** Span-level vs. token-level (span chosen for efficiency); all heads vs. copying heads (all heads retained at cost of dimensionality); SVC vs. XGBoost (SVC preferred for better validation generalization).
- **Failure signatures:** Low ECS + high PKS in later layers → high hallucination confidence; low correlation between ECS and hallucination labels in new model → ECS features may be uninformative; XGBoost showing 99.75% train F1 but 75.08% val F1 → overfitting.
- **First 3 experiments:** Replicate correlation analysis on your target model to verify ECS/PKS-hallucination relationships; ablate late-layer vs. early-layer features to test if late-layer PKS alone suffices; test proxy transfer on your production model outputs using pre-trained SVC.

## Open Questions the Paper Calls Out
None

## Limitations
- ECS mechanism assumes attention weights meaningfully reflect grounding decisions, which may not hold across architectures
- PKS relies on the assumption that FFN-driven vocabulary distribution shifts indicate parametric knowledge injection, but causal link isn't independently established
- Proxy-model generalization only demonstrated within same model family (0.6B to 1.5B), not across fundamentally different architectures

## Confidence

- **High Confidence:** Computational pipeline for extracting ECS/PKS features and training classifiers is clearly specified and reproducible; correlation patterns in Qwen3-0.6b are empirically demonstrated
- **Medium Confidence:** Self-evaluation results (74.68% F1) are directly supported by methodology; claim that classifiers outperform TruLens/RefChecker relies on paper's own comparisons
- **Low Confidence:** Proxy-based evaluation (75.36% F1) and architecture-agnostic claim remain under-validated; paper doesn't test transfer to substantially different model architectures or domains

## Next Checks

1. **Architecture Transfer Test:** Apply InterpDetect's trained SVC classifier to hallucinations in non-transformer models (e.g., Mamba, RWKV) or mixture-of-experts architectures to validate the architecture-agnostic claim.

2. **Signal Consistency Across Domains:** Replicate the ECS/PKS-hallucination correlation analysis on models trained on different domains (e.g., medical, legal) to test whether the grounding-knowledge balance patterns generalize beyond financial data.

3. **Causal Validation of PKS:** Design ablation studies that systematically vary FFN contributions (e.g., through layer dropping or attention masking) to establish whether the observed PKS-hallucination correlation reflects causal knowledge injection or spurious correlation.