---
ver: rpa2
title: A General Close-loop Predictive Coding Framework for Auditory Working Memory
arxiv_id: '2503.12506'
source_url: https://arxiv.org/abs/2503.12506
tags:
- memory
- audio
- auditory
- working
- recalled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a close-loop predictive coding framework
  for auditory working memory, addressing the challenge of modeling sequential auditory
  information storage and retrieval. The proposed approach uses two-layer neural networks
  with distinct weight matrices to encode and reconstruct audio sequences through
  predictive coding.
---

# A General Close-loop Predictive Coding Framework for Auditory Working Memory

## Quick Facts
- **arXiv ID**: 2503.12506
- **Source URL**: https://arxiv.org/abs/2503.12506
- **Reference count**: 33
- **Primary result**: Semantic similarity scores consistently exceeding 0.7 on both ESC-50 and LibriSpeech datasets

## Executive Summary
This paper introduces a close-loop predictive coding framework for auditory working memory that addresses the challenge of modeling sequential auditory information storage and retrieval. The approach uses two-layer neural networks with distinct weight matrices to encode and reconstruct audio sequences through predictive coding. A key innovation is the incorporation of close-loop feedback, which significantly improves recall accuracy compared to open-loop methods. The framework is evaluated on two benchmark datasets—ESC-50 for environmental sounds and LibriSpeech for speech—using semantic similarity as the primary metric.

## Method Summary
The framework implements a two-layer neural network with WH and Wout matrices that encode sequential audio segments (10-200ms chunks) during a write phase. During training (100 epochs), weight matrices minimize prediction errors between actual and predicted hidden states and outputs. For retrieval, weights are frozen and the model reconstructs sequences iteratively (500 steps) using close-loop feedback, where the recalled output serves as the next input. The approach is tested on 4-second audio sequences from ESC-50 and LibriSpeech, with performance measured via semantic similarity using MiniLM embeddings applied to CLAP/Whisper-generated captions.

## Key Results
- Semantic similarity scores consistently exceed 0.7 across both ESC-50 and LibriSpeech datasets
- Close-loop feedback mechanism significantly outperforms open-loop approaches, preventing rapid signal degradation
- The framework successfully preserves both acoustic and semantic features during recall
- Class-specific performance varies, with some sounds (e.g., "vacuum cleaner," "fireworks") showing higher variance

## Why This Works (Mechanism)

### Mechanism 1: Predictive Coding with Dual Weight Matrices
The model uses WH to predict subsequent hidden states and Wout to predict next output observations. During the write phase, weight matrices are trained to minimize the ℓ2-norm of prediction errors for both pathways. The hidden state is iteratively updated via partial differential gradients until convergence. This approach assumes auditory sequences can be represented as discrete segments where each segment's prediction error carries sufficient information for reconstruction.

### Mechanism 2: Close-loop Feedback for Error Correction
During retrieval, ex,μ is computed as the Euclidean distance between the stored segment and the most recently recalled segment—not the original. This creates a self-correcting loop where retrieval errors compound more slowly. The hidden state updates using this feedback, proving particularly effective in preserving both acoustic and semantic features during recall.

### Mechanism 3: Frozen-Weight Retrieval with Iterative Refinement
After the write phase completes, WH and Wout are frozen. During read, only the hidden states and output estimates are iteratively updated to minimize the frozen loss function. This approach assumes the weight matrices encode a sufficient representation of the sequence such that iterative hidden-state optimization can reconstruct it.

## Foundational Learning

- **Predictive Coding Theory**: Why needed - The entire framework builds on the principle that neural systems minimize prediction errors between top-down predictions and bottom-up sensory inputs. Quick check - Can you explain why prediction error (not raw input) drives learning in this framework?

- **Working Memory vs. Long-Term Memory**: Why needed - The paper explicitly targets auditory working memory—temporary storage and manipulation—distinct from permanent storage. This constrains sequence length and justifies the write/read phase separation. Quick check - What distinguishes the 4-second sequences tested here from long-term memory storage?

- **Semantic Similarity Metrics (Cosine Similarity on Embeddings)**: Why needed - The primary evaluation metric uses MiniLM sentence embeddings and cosine similarity. Understanding this is critical for interpreting the >0.7 SS scores and why acoustic fidelity alone doesn't determine success. Quick check - Why might two acoustically different signals still achieve high semantic similarity?

## Architecture Onboarding

- **Component map**:
```
Input Audio (xμ) → Segment Extraction (10-200ms)
                          ↓
         ┌─────────────────────────────────┐
         │   WRITE PHASE (training)        │
         │   Hidden State hμ ← iterative   │
         │   WH: hμ → hμ+1 prediction      │
         │   Wout: hμ → xμ+1 prediction    │
         │   Loss: ||ex,μ||² + ||eh,μ||²   │
         └─────────────────────────────────┘
                          ↓
              Frozen Weights (WH, Wout)
                          ↓
         ┌─────────────────────────────────┐
         │   READ PHASE (retrieval)        │
         │   Initial cue → h0              │
         │   Loop: predict → recall →      │
         │         feedback to next step   │
         │   Output: Reconstructed x̂μ     │
         └─────────────────────────────────┘
                          ↓
         Evaluation: CLAP/Whisper → Text → SS
```

- **Critical path**: Segmentation parameters (10ms vs 200ms) directly impact memory capacity and computational cost; write phase epochs (N1=100) determine encoding quality; read phase iterations (N2=500) determine reconstruction precision; close-loop vs open-loop toggle is the single most impactful design choice.

- **Design tradeoffs**: Segment length - shorter = finer granularity but higher compute; 200ms chosen for tractability. Hidden neurons (1,600) - larger = more capacity but diminishing returns. Learning rate (0.0001) - conservative choice for stability; faster rates may destabilize convergence.

- **Failure signatures**: Open-loop mode shows waveform collapses to noise after ~0.2s. Insufficient write epochs produce low semantic similarity (<0.5) across all classes. Semantic drift occurs when SS = 0.39, showing acoustic preservation but meaning loss. Class-specific failures include high variance for "vacuum cleaner," "fireworks."

- **First 3 experiments**:
1. Ablate close-loop feedback: Run read phase in open-loop mode on 10 LibriSpeech samples; compare waveform alignment and SS scores to paper benchmarks
2. Sweep segment length: Test 50ms, 100ms, 200ms, 500ms segments on ESC-50; plot SS vs. segment size to validate 200ms choice
3. Capacity test: Progressively increase sequence length (2s, 4s, 6s, 8s) on LibriSpeech; identify where SS drops below 0.7 threshold

## Open Questions the Paper Calls Out

### Open Question 1
Does the proposed close-loop predictive coding framework accurately reflect the neural mechanisms observed in biological auditory working memory? The Introduction states that "the neural mechanisms underpinning auditory working memory... remain an area of active investigation." This remains unresolved because the paper establishes a computational model inspired by biological predictive coding but evaluates it solely on artificial benchmarks without comparing the model's internal dynamics to neuroimaging data (e.g., fMRI or EEG). A study correlating the model's hidden state dynamics and error terms with neuroimaging data from human subjects performing analogous auditory working memory tasks would resolve this.

### Open Question 2
How does the recall performance of the framework degrade as the duration of the auditory sequence increases beyond the tested 4-second window? The "Experimental settings" section specifies a constraint where "the model retains information for the first 20 steps (equivalent to 4 seconds)." This remains unresolved because the paper demonstrates success on short sequences, but the stability of the close-loop error correction mechanism over longer, continuous streams (e.g., minutes of audio) remains untested. Evaluation of the model's semantic similarity scores and error accumulation rates on datasets containing significantly longer sequential audio segments would resolve this.

### Open Question 3
To what extent does the framework preserve low-level acoustic features compared to high-level semantic content? The caption of Figure 2 notes that the recalled waveform suffers from "increased amplitude" and "additional echo and background noise," despite achieving high semantic similarity. This remains unresolved because the primary evaluation metric is semantic similarity derived from text captions (CLAP/Whisper), which may mask acoustic distortions that would fail objective measures of signal fidelity. Quantitative evaluation using signal-level metrics (e.g., SNR, spectral convergence, or MSE) alongside the current semantic similarity measures would resolve this.

## Limitations
- Model capacity unknown - the paper does not define the maximum sequence length the framework can handle before performance degradation
- Activation functions unspecified - critical details about non-linear functions f(·) and h(·) are missing
- Segmentation inconsistency - the Methods section mentions 10ms segments while Experiments uses 200ms
- Single run results - all performance metrics appear to come from single runs without statistical validation

## Confidence
- **High confidence**: The close-loop feedback mechanism demonstrably improves recall quality compared to open-loop (supported by Fig. 2 waveform comparisons)
- **Medium confidence**: Semantic similarity scores >0.7 indicate meaningful retention, but the metric's sensitivity to acoustic vs semantic preservation is not fully characterized
- **Medium confidence**: Frozen-weight retrieval is theoretically sound and supported by related work [9], but empirical validation beyond the tested datasets is limited
- **Low confidence**: Claims about generalization to longer sequences or different audio types are not empirically tested

## Next Checks
1. **Ablation study**: Run the framework in both close-loop and open-loop modes on identical 10 LibriSpeech samples; compare waveform alignment and SS scores to confirm the ~0.2s degradation threshold
2. **Segmentation sweep**: Test 50ms, 100ms, 200ms, and 500ms segments on ESC-50; plot SS scores against segment length to validate the 200ms choice and identify optimal range
3. **Capacity stress test**: Progressively increase sequence length from 2s to 8s on LibriSpeech samples; identify the point where SS drops below 0.7 and characterize failure modes