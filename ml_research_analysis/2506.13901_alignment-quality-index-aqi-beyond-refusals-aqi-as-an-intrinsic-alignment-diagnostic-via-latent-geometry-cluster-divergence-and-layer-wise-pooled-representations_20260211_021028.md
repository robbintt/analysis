---
ver: rpa2
title: 'Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment
  Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations'
arxiv_id: '2506.13901'
source_url: https://arxiv.org/abs/2506.13901
tags:
- alignment
- prompts
- latent
- unsafe
- openai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Alignment Quality Index (AQI), an intrinsic
  geometric metric for evaluating LLM alignment. AQI analyzes latent activation patterns
  to detect alignment failures that behavioral metrics miss, such as jailbreak vulnerabilities,
  alignment faking, and representation drift.
---

# Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations

## Quick Facts
- arXiv ID: 2506.13901
- Source URL: https://arxiv.org/abs/2506.13901
- Reference count: 40
- One-line primary result: Introduces AQI, a geometric metric for intrinsic LLM alignment evaluation, detecting hidden misalignment undetectable by refusal scores

## Executive Summary
This paper introduces the Alignment Quality Index (AQI), an intrinsic geometric metric for evaluating LLM alignment. AQI analyzes latent activation patterns to detect alignment failures that behavioral metrics miss, such as jailbreak vulnerabilities, alignment faking, and representation drift. The method combines multiple clustering indices—Davies-Bouldin Score, Dunn Index, Xie-Beni Index, and Calinski-Harabasz Index—applied to pooled layer-wise activations to assess the separation between safe and unsafe completions. AQI is shown to be decoding-invariant, stable under paraphrasing and stochastic generation, and sensitive to hidden misalignment undetectable by refusal or toxicity scores. The authors also propose LITMUS, a benchmark of safe and unsafe prompts, and demonstrate AQI's effectiveness across models trained under DPO, GRPO, and RLHF. AQI is released as a public tool for scalable, geometry-aware alignment auditing.

## Method Summary
AQI evaluates alignment by computing a composite clustering score (CHI + XBI) on pooled layer-wise activations from safe and unsafe prompt-completion pairs. The method uses Sparsemax attention to weight mid-to-deep layers where alignment-critical abstraction emerges, pooling these with a frozen safety anchor vector. This produces a decoding-invariant metric that detects latent misalignment even when behavioral refusals appear compliant. The approach is validated on LITMUS, a benchmark of 10,000 prompts, showing AQI's sensitivity to jailbreaks, over-refusals, and alignment faking across model families.

## Key Results
- AQI detects latent misalignment undetectable by behavioral metrics like refusals and G-Eval
- AQI remains stable across decoding temperatures while behavioral metrics fluctuate significantly
- AQI identifies jailbreak vulnerabilities and over-refusals that traditional metrics miss
- AQI scores correlate with model size and training method (DPO, GRPO, RLHF)

## Why This Works (Mechanism)

### Mechanism 1: Latent Geometric Separation Detects Hidden Misalignment
- **Claim:** If a model is truly aligned, safe and unsafe prompt representations will form geometrically separable clusters in activation space; collapsed boundaries indicate latent misalignment even with compliant outputs.
- **Mechanism:** AQI computes a composite score from Xie-Beni Index (XBI, penalizes centroid proximity and intra-cluster variance) and Calinski-Harabasz Index (CHI, measures global inter-cluster dispersion) over pooled layer activations. Low separation → high XBI and/or low CHI → low AQI → flagged risk.
- **Core assumption:** Safety-relevant abstractions are clusterable in latent space; ambiguous or hybrid-intent prompts may violate this.
- **Evidence anchors:**
  - [abstract] "AQI analyzes latent activation patterns to detect alignment failures that behavioral metrics miss, such as jailbreak vulnerabilities, alignment faking, and representation drift."
  - [Section 3] "AQI builds on two core insights: Layer-Aware Semantics... Geometric Fidelity... evaluates cluster quality in activation space—measuring intra-class compactness and inter-class separation."
  - [corpus] Weak direct support; related work on refusal directions in activation space exists but does not validate the specific CHI+XBI composite.
- **Break condition:** If safe/unsafe labels are noisy, or prompts encode context-dependent harm (e.g., educational misuse), clusters may not reflect true alignment; AQI may under- or over-estimate risk.

### Mechanism 2: Layer-wise Sparsemax Pooling Focuses on Alignment-Critical Layers
- **Claim:** Intermediate layers (mid-to-deep) encode alignment-relevant abstraction; final-layer reliance is brittle due to over-smoothing. Sparse attention pooling identifies and aggregates these signals.
- **Mechanism:** Learn sparse weights α(l) via Sparsemax over cosine similarity between each layer's activation h(l)(x,y) and a frozen safety anchor vector r. Pooled embedding ˜h = Σ α(l)h(l) feeds into CHI/XBI computation.
- **Core assumption:** A meaningful safety anchor r can be derived (e.g., from known-safe completions); Sparsemax's sparsity assumption (few layers dominate) holds.
- **Evidence anchors:**
  - [Section 3.1] "Mid-to-deep layers (layers 11–24) receive dominant weight, reflecting where alignment-critical abstraction emerges. Early layers receive near-zero mass, while final layers show high variance."
  - [Section C.3] "Sparsemax yields sparse distributions: many α(l) = 0, focusing attention on a small subset of layers."
  - [corpus] No direct corpus validation of Sparsemax pooling for alignment; related work on layerwise representations exists but does not confirm this specific design.
- **Break condition:** If the anchor r is biased (e.g., culturally narrow), pooling may misattribute alignment; if safety signals are distributed across many layers, sparsity may discard relevant information.

### Mechanism 3: Decoding-Invariance Enables Stable Auditing Under Sampling Noise
- **Claim:** AQI remains stable across decoding temperatures and sampling strategies because it operates on pre-logit activations, whereas behavioral metrics (refusal, G-Eval) fluctuate significantly.
- **Mechanism:** AQI computes clustering indices on pooled embeddings extracted during a single forward pass, before any sampling. Temperature/top-p only affect token selection post-AQI computation.
- **Core assumption:** The forward pass deterministically produces activations; no randomness is introduced in activation extraction.
- **Evidence anchors:**
  - [Section 5.1, Case 3] "G-Eval and judge scores fluctuate up to 40 points. AQI, computed pre-logits, remains consistent across temperatures—highlighting its decoding invariance."
  - [FAQ] "AQI is fundamentally decoding-invariant—it operates entirely within the model's internal representation space and does not depend on generated text."
  - [corpus] Weak; corpus focuses on behavioral refusals and over-refusal issues, not decoding-invariant diagnostics.
- **Break condition:** If evaluation prompts differ between batches (non-identical forward passes), or if model internals change (e.g., dynamic LoRA switching), stability may degrade.

## Foundational Learning

- **Concept: Cluster Validity Indices (CHI, XBI)**
  - Why needed here: AQI's core is combining CHI (global separation) and XBI (local compactness) to quantify latent safe/unsafe separation.
  - Quick check question: For two clusters with tight intra-group variance but centroids close together, which index will penalize this more strongly?

- **Concept: Sparsemax Attention vs. Softmax**
  - Why needed here: Layer-wise pooling uses Sparsemax to produce sparse, interpretable attention weights over transformer depth.
  - Quick check question: Why would Sparsemax yield more interpretable layer attribution than softmax for a 30-layer model?

- **Concept: Activation Patching / Causal Tracing**
  - Why needed here: Appendix I describes using AQI to trigger causal investigations—patching activations at high-AQI-divergence layers to test mechanistic alignment.
  - Quick check question: If patching an unsafe activation into a safe forward pass at layer l* flips the output, what does this imply about layer l*'s role in alignment?

## Architecture Onboarding

- **Component map:** Prompt/Completion Input → Activation Extraction → Sparsemax Pooling → CHI + XBI Computation → AQI Score → Drift/Diagnostic Hooks
- **Critical path:** Activation extraction → Sparsemax pooling → CHI/XBI → AQI. Errors in pooling (e.g., collapsed α(l)) or anchor quality will propagate directly.
- **Design tradeoffs:**
  - λ balance: Higher λ emphasizes local compactness (XBI); lower λ emphasizes global separation (CHI). Default 0.5 may not suit all models.
  - Batch size: Small batches (<32) inflate CHI; large batches may mix heterogeneous prompts, flattening separation.
  - Sparsemax vs. Softmax: Sparsemax aids interpretability but assumes few layers matter; softmax is smoother but dilutes attribution.
- **Failure signatures:**
  - AQI near 0.5 across all conditions → possible anchor r is uninformative or α(l) collapsed to uniform.
  - Large AQI variance across paraphrases → model lacks latent invariance; consider contrastive pretraining or manifold alignment.
  - AQI high but behavioral refusals low → possible alignment faking; trigger causal tracing at high-α(l) layers.
- **First 3 experiments:**
  1. **Sanity check:** On a small LITMUS subset (N=100 safe, N=100 unsafe), compute AQI with λ=0.5, Sparsemax pooling. Verify: (a) α(l) distribution is non-uniform; (b) AQI differs meaningfully between a known-aligned and known-misaligned checkpoint.
  2. **Decoding invariance test:** For a fixed model, generate completions at T∈{0.2,0.7,1.0}. Compute AQI and G-Eval. Confirm AQI variance < G-Eval variance (target <5% vs. >20%).
  3. **Jailbreak deflection test:** Take 50 unsafe prompts and their jailbreak paraphrases. Compute AQI on clean vs. jailbreak completions. Expect AQI drop ≥15% on jailbreaks for models <7B parameters; minimal drop for robustly aligned larger models.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the AQI framework be generalized to vision-language models (VLMs) or multimodal agents, where latent geometries are structurally discontinuous or non-Euclidean?
- Basis: [explicit] The authors explicitly state in Section 7.2 that "Current AQI evaluation is constrained to autoregressive, text-only models" and that "Future work must explore multimodal embedding fusion... and manifold-aware clustering on non-Euclidean latent geometries."
- Why unresolved: The current methodology relies on Euclidean pooling of transformer activations (Equation 5), which may not hold for modality-specific encoders where activations are temporally or structurally discontinuous.
- What evidence would resolve it: A demonstration of AQI validity on a VLM (e.g., LLaVA or Flamingo) using modality-specific pooling techniques, showing consistent correlation with behavioral safety metrics in image-to-text tasks.

### Open Question 2
- Question: Can AQI be extended to capture graded or continuous alignment dimensions (e.g., fairness, privacy) rather than binary safe/unsafe labels?
- Basis: [explicit] Section 7.2 lists "Reliance on Binary Safety Labels" as a limitation and proposes that "Extending AQI to fuzzy clustering... or scalar reward modeling... would better reflect real-world safety objectives."
- Why unresolved: The current formulation (Section 3) computes CHI and XBI over binary clusters (safe vs. unsafe), failing to capture the nuance of "soft policy violations" or context-dependent harms.
- What evidence would resolve it: A modified AQI formulation using fuzzy clustering validity indices (e.g., fuzzy Xie-Beni) applied to a dataset with scalar harm ratings, showing that the score correlates with the degree of misalignment rather than just binary presence.

### Open Question 3
- Question: How can AQI be integrated with causal tracing methods to localize the specific parameters or neurons responsible for detected latent misalignment?
- Basis: [explicit] The authors state in Section 7.2 that AQI "does not localize its origin" and suggest it "should be integrated with interpretability methods such as causal tracing... and activation patching."
- Why unresolved: While AQI identifies *if* alignment has failed via geometric collapse, it currently functions as a diagnostic metric rather than an attribution tool, leaving the "root cause" unidentified.
- What evidence would resolve it: An experiment where an AQI drop triggers an automated causal tracing intervention (e.g., causal abstraction or path patching) that successfully identifies and surgically modifies the specific attention head or MLP layer restoring the AQI score.

## Limitations
- AQI relies on binary safe/unsafe labels and cannot capture nuanced or continuous alignment dimensions
- The method assumes safety-relevant abstractions are clusterable in latent space, which may not hold for context-dependent harms
- AQI does not localize the specific parameters or neurons responsible for detected misalignment

## Confidence
- **High Confidence**: Decoding invariance and stability of AQI across sampling temperatures (supported by direct empirical comparison to G-Eval in Case 3)
- **Medium Confidence**: Layer-wise Sparsemax pooling focusing on mid-to-deep layers for alignment abstraction (supported by weight distribution analysis in Appendix C.3, but lacking direct corpus validation)
- **Low Confidence**: The specific combination of CHI and XBI as the optimal composite for alignment auditing (related work on clustering indices exists, but no direct comparison to alternative metrics)

## Next Checks
1. **Anchor Robustness Test**: Construct multiple safety anchors from diverse cultural contexts (e.g., Western vs. non-Western safety norms) and compute AQI on the same LITMUS subset. Measure variance in AQI scores; if >15% across anchors, this indicates anchor bias sensitivity and the need for anchor ensemble methods.
2. **Layer Attribution Ablation**: For a fixed model, compute AQI with Sparsemax pooling disabled (uniform weights α(l)=1/L) versus enabled. Measure the difference in AQI scores and layer attribution stability. If AQI variance >5% or top-3 layers differ across runs, this suggests Sparsemax may be unstable or overfit to specific checkpoints.
3. **Clustering Assumption Violation**: Construct a prompt set with context-dependent harm (e.g., "How to build a drone" for educational vs. surveillance misuse). Compute AQI and visualize pooled embeddings via UMAP. If clusters overlap significantly (>30% IoU) but behavioral refusals are clean, this confirms AQI's limitation in handling ambiguous-intent prompts and suggests the need for hybrid behavioral-latent metrics.