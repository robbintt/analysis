---
ver: rpa2
title: Benchmarking the Medical Understanding and Reasoning of Large Language Models
  in Arabic Healthcare Tasks
arxiv_id: '2508.15797'
source_url: https://arxiv.org/abs/2508.15797
tags:
- llms
- medical
- arabic
- task
- gemini
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of large language models (LLMs)
  in Arabic medical tasks, focusing on understanding and reasoning capabilities. Using
  the AraHealthQA dataset from the MedArabiQ2025 challenge, the study benchmarks various
  LLMs on multiple-choice questions (MCQs), fill-in-the-blank scenarios, and open-ended
  questions.
---

# Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks

## Quick Facts
- **arXiv ID:** 2508.15797
- **Source URL:** https://arxiv.org/abs/2508.15797
- **Reference count:** 9
- **Key outcome:** Proposes majority voting ensemble achieving 77% MCQ accuracy and BERTScore up to 86.44% for Arabic medical understanding tasks

## Executive Summary
This study evaluates large language models on Arabic medical tasks using the AraHealthQA dataset from the MedArabiQ2025 challenge. The paper benchmarks both proprietary models (Gemini Pro 2.5, GPT-o3) and open-source Arabic models (Allam, Falcon 3) across multiple-choice questions, fill-in-the-blank scenarios, and open-ended questions. Proprietary models demonstrated superior performance, with majority voting ensembles achieving up to 77% accuracy on MCQs. For open-ended questions, models showed high semantic alignment scores up to 86.44% BERTScore. The study identifies critical limitations including small dataset size, need for bias detection, and metric reliability concerns, while proposing future work on retrieval-augmented generation and larger Arabic medical datasets.

## Method Summary
The paper employs zero-shot prompting with temperature=0 and top_p=1 to evaluate LLMs on the AraHealthQA dataset containing 700 Arabic medical samples. Three prompt variants are tested: simple instructions, chain-of-thought reasoning, and concise MSA-focused responses. For MCQs, majority voting ensembles combine predictions from Gemini Pro 2.5, Gemini Flash 2.5, and GPT-o3. Classification tasks are evaluated using exact match accuracy while open-ended questions use XLM-RoBERTa-Large BERTScore for semantic alignment. The study compares proprietary models against open-source Arabic models without any fine-tuning or RAG approaches.

## Key Results
- Majority voting ensemble of three top models achieves 77% MCQ accuracy, outperforming individual models (72-76%)
- Gemini Pro 2.5 and GPT-o3 demonstrate superior performance on MCQs using chain-of-thought reasoning prompts
- Open-ended question generation achieves BERTScore up to 86.44% with concise MSA prompts
- Open-source Arabic models (Allam: 39%, Falcon 3: 45%) significantly underperform proprietary models

## Why This Works (Mechanism)

### Mechanism 1: Majority Voting Ensemble for Answer Selection
- **Claim:** Aggregating predictions from multiple reasoning-capable LLMs via majority voting improves classification accuracy on Arabic medical MCQs compared to any single model.
- **Mechanism:** Three models independently predict answers; the final prediction is the one with majority agreement. This reduces individual model errors by leveraging complementary strengths—where one model may fail due to knowledge gaps, another may succeed through different learned representations.
- **Core assumption:** The models make uncorrelated errors and possess sufficiently overlapping medical knowledge for consensus to correlate with correctness.
- **Evidence anchors:** Abstract shows majority voting achieving 77% accuracy vs. individual models at 72-76%. MedArabiQ benchmark provides dataset foundation.

### Mechanism 2: Step-by-Step Reasoning for Medical Question Decomposition
- **Claim:** Prompts that elicit chain-of-thought reasoning improve MCQ accuracy by enabling LLMs to decompose complex clinical scenarios before selecting answers.
- **Mechanism:** Reasoning-capable models (GPT-o3, Gemini 2.5 series) simulate diagnostic thinking by combining multiple facts and systematically eliminating plausible distractors, rather than pattern-matching on surface cues.
- **Core assumption:** The model's embedded medical knowledge is sufficient and retrievable when prompted appropriately; reasoning traces improve retrieval and integration.
- **Evidence anchors:** Table 1 shows reasoning prompts improving performance over simple prompts. AraReasoner paper demonstrates reasoning improves Arabic NLP tasks.

### Mechanism 3: Concise MSA Prompting for Semantic Alignment
- **Claim:** Prompts that explicitly constrain responses to Modern Standard Arabic (MSA) and demand conciseness improve semantic alignment scores with expert answers.
- **Mechanism:** By instructing models to avoid explanations and focus on medically correct, concise MSA responses, the output distribution shifts toward reference answer structures that BERTScore evaluates more favorably.
- **Core assumption:** Expert reference answers are themselves concise, MSA-formatted, and that semantic similarity metrics like BERTScore validly capture medical correctness.
- **Evidence anchors:** Table 2 shows Prompt 3 achieving highest BERTScores (0.8581-0.8644). Corpus evidence on Arabic prompt optimization is limited.

## Foundational Learning

- **Concept: Medical MCQ vs. Open-Ended Evaluation Paradigms**
  - **Why needed here:** The paper evaluates two fundamentally different tasks with different metrics (accuracy vs. BERTScore). Understanding this distinction is critical for interpreting results and selecting appropriate approaches.
  - **Quick check question:** If a model achieves 77% MCQ accuracy but 86% BERTScore on open-ended questions, which metric better reflects clinical readiness for patient-facing applications—and why?

- **Concept: Zero-Shot Inference with Deterministic Decoding**
  - **Why needed here:** All experiments use temperature=0, top_p=1 for reproducibility. Understanding zero-shot evaluation reveals what knowledge is already embedded vs. what requires fine-tuning.
  - **Quick check question:** What does it mean when an open-source Arabic model (Allam) achieves only 39% on MCQs in zero-shot mode—is this a prompting problem or a knowledge problem?

- **Concept: Semantic Similarity Metrics and Their Limitations**
  - **Why needed here:** The paper explicitly notes BERTScore's limitations in capturing subtle semantic nuances. Understanding metric validity is essential for translating benchmark results to deployment decisions.
  - **Quick check question:** If Model A achieves BERTScore 0.86 and Model B achieves 0.84, but Model A's answers contain clinically dangerous hallucinations that happen to use similar vocabulary to references, what evaluation gap exists?

## Architecture Onboarding

- **Component map:** AraHealthQA dataset (700 samples: 300 MCQs, 400 open-ended) -> Proprietary LLM APIs (GPT-o3, Gemini 2.5 series) and open-source Arabic models (Falcon 3, Fanar, Allam) -> Majority voting ensemble (3 models) for MCQs -> Exact match accuracy (MCQs) or BERTScore (open-ended) evaluation

- **Critical path:** 1) Select task type (MCQ vs. open-ended) 2) Choose model(s) based on reasoning capability and Arabic proficiency 3) Apply task-specific prompt 4) For MCQs: aggregate multiple model predictions via majority voting 5) Evaluate against reference answers using appropriate metric

- **Design tradeoffs:**
  - **Proprietary vs. open-source:** Proprietary models (Gemini Pro 2.5: 75% accuracy) vastly outperform open-source Arabic models (Allam: 39%), but raise deployment cost, latency, and data privacy concerns
  - **Single model vs. ensemble:** Majority voting improves accuracy by 1-2% but triples inference cost and latency
  - **Prompt complexity vs. reliability:** Chain-of-thought prompts improve reasoning but may increase token usage and failure modes on edge cases

- **Failure signatures:**
  - Low MCQ accuracy (<50%) despite reasoning prompts suggests fundamental knowledge gaps in Arabic medical terminology
  - High BERTScore with clinically incorrect answers indicates metric gaming through vocabulary matching rather than genuine medical understanding
  - High variance across prompt templates signals unstable knowledge retrieval

- **First 3 experiments:**
  1. **Baseline replication:** Run Gemini Flash 2.5 and GPT-o3 on 50-sample subset with Prompts 1-3; verify you can reproduce reported accuracy ranges (±3%)
  2. **Ablation study:** Test majority voting with different model combinations (2-model vs. 3-model ensembles) to quantify marginal accuracy gains vs. inference cost
  3. **Error analysis:** Manually review 20 incorrect MCQ predictions from the best single model to categorize failure modes (misunderstanding questions vs. knowledge gaps vs. reasoning failures) and determine whether fine-tuning or RAG would be more effective

## Open Questions the Paper Calls Out

- **Question 1:** What evaluation metrics can more accurately capture semantic nuances and factual correctness in Arabic medical open-ended responses than current BERTScore methods?
  - **Basis:** The authors explicitly identify BERTScore as a limitation, noting it "often yields similar values across different responses and fails to reflect subtle nuances in semantic alignment."
  - **Why unresolved:** High BERTScores may not distinguish between medically precise answers and plausible but incorrect generations.
  - **What evidence would resolve it:** Development of a novel evaluation framework demonstrating higher correlation with human expert judgment than BERTScore on Arabic medical generative tasks.

- **Question 2:** To what extent can Retrieval-Augmented Generation (RAG) or fine-tuning on larger datasets improve LLM accuracy beyond the current 77% majority-voting benchmark?
  - **Basis:** The limitations section states there is a "clear need for larger, high-quality Arabic medical datasets" and suggests employing "retrieval-augmented generation (RAG) techniques" to enhance performance.
  - **Why unresolved:** The current benchmark relies on a small dataset (700 samples) and zero-shot prompting; potential performance gains from external knowledge retrieval remain unquantified.
  - **What evidence would resolve it:** Comparative results showing accuracy improvements when models utilize RAG or fine-tuning on proposed larger Arabic medical corpora.

- **Question 3:** How does the integration of bias detection and mitigation techniques impact the reliability of LLMs in culturally sensitive Arabic healthcare scenarios?
  - **Basis:** The authors list the "absence of bias detection and mitigation techniques" as a specific limitation and acknowledge the dataset includes questions with "bias injected" to test sensitive scenarios.
  - **Why unresolved:** While the paper measures raw accuracy on bias-injected questions, it does not evaluate the effectiveness of preprocessing interventions to mitigate ethical or cultural risks before inference.
  - **What evidence would resolve it:** A study measuring performance disparities and safety compliance on the bias-injected subset before and after applying specific debiasing algorithms.

## Limitations
- Small dataset size (700 samples) raises concerns about statistical power and overfitting to specific question patterns
- BERTScore limitations in capturing medical semantic correctness may overestimate alignment when models generate medically incorrect but lexically similar answers
- Zero-shot inference approach doesn't explore whether task-specific adaptation (fine-tuning, RAG) could significantly improve performance, especially for open-source models

## Confidence
- **High Confidence:** Claims about proprietary models outperforming open-source Arabic models on MCQs (Gemini Pro 2.5 at 75% vs. Allam at 39%) are well-supported by experimental results
- **Medium Confidence:** Effectiveness of majority voting ensembles and chain-of-thought prompting is demonstrated but relies on specific model combinations
- **Low Confidence:** Claims about BERTScore reflecting true semantic medical understanding are weakest, given explicit acknowledgment of metric limitations

## Next Checks
1. **Dataset Expansion Validation:** Conduct statistical power analysis to determine minimum dataset size needed for reliable benchmarking. Test whether current results hold when evaluating on stratified subsets (50-sample random samples vs. full 200-sample test set).
2. **Metric Robustness Testing:** Implement human evaluation for a random sample of 50 open-ended answers, comparing BERTScore rankings against clinical expert assessments to quantify metric reliability and identify systematic discrepancies.
3. **Zero-Shot vs. Fine-Tuned Comparison:** Evaluate whether fine-tuning the best-performing open-source Arabic model (Allam) on a subset of the training data significantly closes the performance gap with proprietary models, determining if zero-shot limitations are knowledge-based or prompting-based.