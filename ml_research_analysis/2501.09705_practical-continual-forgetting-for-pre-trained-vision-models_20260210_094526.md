---
ver: rpa2
title: Practical Continual Forgetting for Pre-trained Vision Models
arxiv_id: '2501.09705'
source_url: https://arxiv.org/abs/2501.09705
tags:
- forgetting
- learning
- classes
- arxiv
- uni000003ec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses continual forgetting for pre-trained vision
  models, aiming to remove unwanted information (e.g., privacy data, toxic data) while
  maintaining remaining knowledge. The problem is defined as sequential erasure requests
  from users and model owners in real-world scenarios, with three key challenges:
  efficient and effective deletion of unwanted knowledge, minimal impact on remaining
  knowledge, and robustness to scarce or missing training samples.'
---

# Practical Continual Forgetting for Pre-trained Vision Models

## Quick Facts
- arXiv ID: 2501.09705
- Source URL: https://arxiv.org/abs/2501.09705
- Authors: Hongbo Zhao; Fei Zhu; Bolin Ni; Feng Zhu; Gaofeng Meng; Zhaoxiang Zhang
- Reference count: 40
- Primary result: Achieves efficient continual forgetting for pre-trained vision models using Group Sparse LoRA (GS-LoRA) and GS-LoRA++ with prototype regularization, demonstrating high data efficiency and parameter efficiency (<1% of parameters) across face recognition, object detection, and image classification tasks.

## Executive Summary
This paper addresses the problem of continual forgetting in pre-trained vision models, where specific classes (e.g., privacy data, toxic data) need to be erased while preserving knowledge of remaining classes. The authors propose Group Sparse LoRA (GS-LoRA), which uses LoRA modules with group sparse regularization to efficiently identify and modify only the necessary parameters for forgetting. They extend this with GS-LoRA++ that incorporates prototype information as additional supervision, improving forgetting performance especially in few-shot and missing class scenarios. Experiments demonstrate effective forgetting with minimal impact on remaining classes across multiple vision tasks.

## Method Summary
The method uses LoRA modules inserted into Feed-Forward Network (FFN) layers of Transformer blocks, with group sparse regularization to automatically select specific LoRA groups for modification. The core optimization minimizes a data loss combining retention loss on remaining classes and forgetting loss on target classes, with gradient ascent used for the forgetting component bounded by a ReLU function. Group Lasso regularization enforces sparsity in the LoRA weights. GS-LoRA++ extends this by adding prototype-based regularization that maximizes KL divergence between forgotten class logits and prototypes while minimizing it for remaining classes. The approach is evaluated on face recognition (CASIA-Face100), image classification (ImageNet100), and object detection (COCO) tasks under various scenarios including single-step, continual, few-shot, and missing class settings.

## Key Results
- Achieves high forgetting performance (Acc_f → 0) while maintaining retention performance (Acc_r → pre-trained levels) across multiple vision tasks
- Demonstrates superior performance compared to continual learning and machine unlearning baselines in both few-shot and missing class scenarios
- Shows parameter efficiency by using less than 1% of total model parameters while achieving effective forgetting
- Exhibits data efficiency by working effectively even with minimal training data (4-shot per class scenarios)

## Why This Works (Mechanism)
The method works by strategically modifying only a small subset of model parameters through LoRA modules while leveraging group sparse regularization to identify which parameters are most relevant for forgetting specific classes. The gradient ascent approach for the forgetting loss, combined with the ReLU-bounded lower limit, ensures that the model actively pushes forgotten class representations away from their original decision boundaries. The prototype regularization in GS-LoRA++ provides additional supervision by explicitly pulling forgotten class logits away from their prototypes while pushing remaining class logits closer to theirs, creating a more robust forgetting mechanism that works even with limited training data.

## Foundational Learning
- **Group Sparse Regularization**: Why needed: To automatically identify and select only the most relevant LoRA groups for forgetting specific classes, improving parameter efficiency. Quick check: Verify that Zero Group Ratio increases as α increases while maintaining forgetting performance.
- **Gradient Ascent for Forgetting**: Why needed: Standard gradient descent would reinforce knowledge, so gradient ascent is used to actively remove information about forgotten classes. Quick check: Monitor that Acc_f decreases during training while Acc_r remains stable.
- **Prototype-Based Supervision**: Why needed: Provides additional guidance for forgetting when training data is scarce, using class prototypes as reference points. Quick check: Verify that KL divergence between forgotten class logits and prototypes increases while it decreases for remaining classes.
- **LoRA Parameter Efficiency**: Why needed: Enables efficient fine-tuning by modifying only a small subset of parameters rather than full model weights. Quick check: Confirm that total LoRA parameters represent <1% of total model parameters.
- **ReLU-Bounded Loss**: Why needed: Prevents gradient explosion and ensures stable optimization during the forgetting process. Quick check: Monitor loss curves for stability and absence of NaN values.

## Architecture Onboarding

Component Map: Pre-trained Model -> LoRA Modules (FFN layers) -> Group Sparse Regularization -> Optimization Loop -> Forgotten/Retained Classes

Critical Path: Data sampling → Prototype calculation → Forward pass (compute losses) → Backward pass (update LoRA weights only) → Sparsity enforcement → Evaluation

Design Tradeoffs: The method trades off between forgetting effectiveness and retention preservation, requiring careful balancing of hyperparameters. The use of LoRA provides parameter efficiency but may limit the magnitude of modifications possible compared to full fine-tuning.

Failure Signatures: Overfitting in few-shot settings (Acc_f remains high on test set), gradient explosion (loss becomes NaN or spikes), catastrophic forgetting of retained classes (Acc_r drops significantly).

First Experiments:
1. Run single-step forgetting on ImageNet100 with 10 forgotten classes using standard GS-LoRA to verify basic functionality
2. Test GS-LoRA++ on the same setup with 4-shot per class to evaluate prototype-based improvements
3. Apply the method to face recognition task (CASIA-Face100) to verify cross-task applicability

## Open Questions the Paper Calls Out
- Can the GS-LoRA++ framework be effectively adapted for instance-wise unlearning rather than class-wise unlearning? The current method relies on class prototypes and group sparse regularization optimized for category-level removal; it is unclear if these mechanisms can precisely target isolated data points without degrading the model's generalization on similar instances.
- How robust is the GS-LoRA++ method against sophisticated membership inference attacks (MIAs)? While the paper demonstrates "real forgetting" via feature reconstruction attacks, it has not yet quantified the protection against statistical attacks designed to infer whether a specific data point was used in training.
- Does GS-LoRA++ maintain its efficiency and efficacy when scaled to Large Vision-Language Models (LVLMs) with billions of parameters? The interaction between low-rank updates and group sparsity may differ in extremely high-dimensional spaces or in models where vision and language modules are deeply entangled.

## Limitations
- Hyperparameter sensitivity significantly affects performance, with no comprehensive sensitivity analysis provided
- Prototype quality dependency may limit effectiveness in missing class scenarios where prototypes must be estimated from limited data
- Scalability to larger vision models (ViT-L, Swin Transformers) and more complex architectures remains unverified

## Confidence
- **High Confidence**: Core methodology using group-sparse LoRA modules for efficient forgetting is well-validated through ablation studies and comparisons with baselines
- **Medium Confidence**: Claims about robustness to few-shot and missing class scenarios are supported by experiments but may be sensitive to hyperparameter settings and prototype quality
- **Medium Confidence**: Performance comparisons with continual learning and machine unlearning baselines are methodologically sound but baseline selection could influence relative metrics

## Next Checks
1. Conduct systematic ablation studies varying α, β, γ, and BND across multiple orders of magnitude to identify stable operating regions and quantify hyperparameter sensitivity
2. Design experiments where prototype quality is deliberately degraded (e.g., using fewer samples, adding noise) to measure degradation in GS-LoRA++ performance and establish minimum prototype quality requirements
3. Test GS-LoRA on larger vision models (ViT-L, Swin-L) and different architectures (ConvNeXt, CLIP) to verify scalability and identify any architecture-specific limitations or adjustments needed