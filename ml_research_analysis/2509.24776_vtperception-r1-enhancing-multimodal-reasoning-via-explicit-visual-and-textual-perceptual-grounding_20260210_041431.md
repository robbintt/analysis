---
ver: rpa2
title: 'VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual
  Perceptual Grounding'
arxiv_id: '2509.24776'
source_url: https://arxiv.org/abs/2509.24776
tags:
- reasoning
- visual
- perception
- textual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VTPerception-R1, a two-stage training framework
  that explicitly decouples visual and textual perception from reasoning in multimodal
  large language models. The first stage employs perception-augmented supervised fine-tuning
  to train models to generate concise, task-relevant perceptual descriptions before
  reasoning.
---

# VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding

## Quick Facts
- **arXiv ID:** 2509.24776
- **Source URL:** https://arxiv.org/abs/2509.24776
- **Reference count:** 38
- **Key outcome:** Explicit visual and textual perceptual grounding improves multimodal reasoning accuracy and robustness, especially for smaller models, achieving SOTA results on 4/6 benchmarks.

## Executive Summary
VTPerception-R1 introduces a two-stage training framework that explicitly decouples visual and textual perception from reasoning in multimodal large language models. The first stage uses perception-augmented supervised fine-tuning to train models to generate concise, task-relevant perceptual descriptions before reasoning. The second stage applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards to ensure grounded reasoning. Through systematic experiments on four benchmarks using Qwen2.5-VL-32B and Qwen2.5-VL-7B, the study finds that explicit perception consistently yields the largest performance gains, particularly for smaller models. VTPerception-R1 significantly improves reasoning accuracy and robustness, achieving new state-of-the-art results on four out of six evaluated benchmarks, demonstrating the critical importance of balanced visual and textual grounding for multimodal reasoning.

## Method Summary
VTPerception-R1 employs a two-stage training approach. Stage I uses supervised fine-tuning on structured data with `<description>→<thinkimg>→<answer>` format, training models to first generate task-relevant perceptual descriptions before reasoning. Stage II applies perception-aware reinforcement learning using DAPO optimizer with a composite reward combining accuracy, format compliance, visual key-info coverage, textual key-info coverage, repetition penalty, and description-reasoning consistency. The framework processes data from LLaVA-CoT, Vision-SR1, MMK12, Vision-R1-rl, and Mulberry datasets, with key information extracted via teacher models and verified through a budgeted annotation pipeline.

## Key Results
- Explicit perception consistently yields the largest performance gains, particularly for smaller models (7B)
- VTPerception-R1 achieves SOTA results on four out of six evaluated benchmarks (MMMU, MathVista, EMMA, OlympiaBench)
- Ablation studies show perception rewards are critical, with SFT-only models showing 4.6-6.5 point gaps vs. full RL
- Smaller models are more sensitive to perception strategy design, benefiting more from explicit annotations than larger models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly decoupling perception from reasoning via a structured `<description>` stage improves multimodal reasoning, particularly for smaller models.
- **Mechanism:** The model is trained to first generate a concise, task-relevant description summarizing visual/textual evidence before producing chain-of-thought reasoning and an answer. This intermediate representation reduces spurious correlations and provides an auditable grounding layer.
- **Core assumption:** Models can learn to distill multimodal inputs into compact perceptual summaries that are more useful for downstream reasoning than raw embeddings.
- **Evidence anchors:** [abstract] "explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models"; [Page 5] "Stage I trains the model to produce concise, task-relevant <description> before reasoning and answering"; [corpus] GeoPQA identifies similar perception gaps in geometric reasoning tasks.

### Mechanism 2
- **Claim:** A composite perception-aware reward (visual key-info, textual key-info, consistency) enforces evidence-grounded reasoning during RL.
- **Mechanism:** Three rewards operate jointly: R_vkey rewards coverage of annotated visual cues in `<description>`; R_tkey rewards coverage of textual facts in `<thinkimg>`; R_cons penalizes reasoning that references entities/attributes not grounded in the description or question.
- **Core assumption:** Annotated key information can be extracted (via teacher models or automated pipelines) and used as verifiable supervision signals.
- **Evidence anchors:** [Page 6-7] Full reward formulation with discretized recall thresholds for R_vkey and R_tkey; [Page 9, Table 3] Ablation shows removing any component degrades performance; consistency removal causes largest drops on C-MMBench (-3.26).

### Mechanism 3
- **Claim:** Smaller models exhibit greater sensitivity to perception strategy design than larger models.
- **Mechanism:** Model capacity limits the ability to self-generate accurate perceptual descriptions under structured prompting. Smaller models benefit more from explicit external annotations and textual perception cues.
- **Core assumption:** Larger models have better intrinsic perceptual capabilities and can recover from weaker prompting strategies.
- **Evidence anchors:** [Page 4, Table 1] Qwen2.5-VL-7B shows greater sensitivity to perception prompts; structured prompts can *reduce* performance on challenging tasks; [Page 4] "smaller models are more prone to self-induced perceptual errors when required to generate their own interpretations".

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** Stage II uses DAPO-style RL with rule-based rewards (accuracy, format, perception). Understanding policy gradients, advantage normalization, and clipped objectives is essential.
  - **Quick check question:** Can you explain why DAPO uses asymmetric clipping and dynamic sampling instead of standard PPO?

- **Concept: GRPO/DAPO Group-Relative Advantage Estimation**
  - **Why needed here:** Rewards are computed relative to groups of sampled responses; understanding group normalization is critical for implementing the perception-aware objective.
  - **Quick check question:** How would removing group-relative advantage computation affect reward signal quality for perception rewards?

- **Concept: Multimodal Perception Grounding**
  - **Why needed here:** The core hypothesis is that reasoning failures often originate in perception; you need to understand how visual/textual grounding differs from pure language reasoning.
  - **Quick check question:** What distinguishes R_vkey from a generic image captioning reward, and why does task-relevance matter?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-7B-Instruct → Stage I SFT (12K structured samples) → Stage II RL (22K samples with perception rewards) → Qwen2.5-VL-32B evaluation
- **Critical path:** 1. Convert raw CoT data to structured format with quality filtering (Appendix A.2) 2. Run Stage I SFT for 3 epochs to establish perception-to-reasoning interface 3. Prepare RL data with extracted key information annotations 4. Run Stage II RL with perception-first weighting schedule
- **Design tradeoffs:** Generic captions vs. task-relevant descriptions (paper chooses relevance over completeness); Annotation cost vs. reward quality (budgeted verification selects top-B candidates by log-probability); Model scale vs. prompting strategy (smaller models require more explicit supervision; structured prompts can backfire)
- **Failure signatures:** SFT-only models show +4.6 to +6.5 point gaps vs. full RL; Removing consistency reward causes largest drops on reasoning-intensive benchmarks; For small models, structured grounding prompts reduce performance on complex tasks (Table 1, OlympiaBench)
- **First 3 experiments:** 1. Validate SFT format compliance: Check that Stage I produces well-formed `<description>` outputs on held-out samples before proceeding to RL 2. Ablate single reward components: Train three RL variants (no R_vkey, no R_tkey, no R_cons) on identical data to reproduce Table 3 ablation patterns 3. Scale sensitivity test: Compare Qwen2.5-VL-7B vs. Qwen2.5-VL-32B under identical perception strategies to confirm smaller models benefit more from explicit annotations

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the systematic study of inference strategies for the 32B model and the restriction of VTPerception-R1 training to the 7B model, several scalability questions emerge regarding whether the strict decoupling of perception and reasoning remains beneficial for significantly larger models.

## Limitations
- Perception-aware RL framework depends critically on high-quality key-info annotations, with scalability and reliability of automated annotation pipeline uncertain for diverse real-world scenarios
- Performance gains primarily demonstrated on specific benchmark datasets (MMMU, MathVista, EMMA, OlympiaBench), with generalization to broader applications unverified
- Structured description format imposes additional generation step that may introduce latency and may not be optimal for all reasoning tasks

## Confidence
- **High Confidence:** The core finding that explicit perceptual grounding improves multimodal reasoning performance, particularly for smaller models (7B), is well-supported by systematic ablation studies showing consistent gains across four benchmarks
- **Medium Confidence:** The claim that composite perception rewards (R_vkey + R_tkey + R_cons) are superior to individual components is supported by ablation results, but exact contribution of each reward component and optimal combination weights remain unclear
- **Medium Confidence:** The observation that smaller models are more sensitive to perception strategy design is observed in controlled experiments, but underlying mechanism (self-induction of perceptual errors) requires further investigation

## Next Checks
1. **Annotation Quality Validation:** Conduct human evaluation of automatically extracted key visual and textual information to measure annotation accuracy and completeness, particularly for complex reasoning tasks where perceptual grounding is critical
2. **Generalization Benchmark Testing:** Evaluate VTPerception-R1 on additional multimodal reasoning datasets not included in training corpus (e.g., VQA-CP, ScienceQA) to assess robustness across different visual and textual distributions
3. **Model Scale Sensitivity Analysis:** Systematically test perception-augmented training framework across broader range of model scales (3B, 13B, 70B) to better understand relationship between model capacity and perception strategy effectiveness