---
ver: rpa2
title: 'Generative Binary Memory: Pseudo-Replay Class-Incremental Learning on Binarized
  Embeddings'
arxiv_id: '2503.10333'
source_url: https://arxiv.org/abs/2503.10333
tags:
- binary
- memory
- learning
- classes
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of class-incremental learning
  (CIL) in dynamic environments where new concepts continuously emerge, requiring
  deep neural networks to adapt by learning new classes while retaining previously
  acquired ones. The proposed Generative Binary Memory (GBM) is a novel CIL pseudo-replay
  approach that generates synthetic binary pseudo-exemplars in a latent binary space
  using Bernoulli Mixture Models (BMMs).
---

# Generative Binary Memory: Pseudo-Replay Class-Incremental Learning on Binarized Embeddings

## Quick Facts
- **arXiv ID**: 2503.10333
- **Source URL**: https://arxiv.org/abs/2503.10333
- **Reference count**: 40
- **Primary result**: Achieves SOTA average accuracy on CIFAR100 (+2.9%) and TinyImageNet (+1.5%) with ResNet-18

## Executive Summary
This paper presents Generative Binary Memory (GBM), a novel approach for class-incremental learning that generates synthetic binary pseudo-exemplars in a latent binary space using Bernoulli Mixture Models. The method addresses catastrophic forgetting in dynamic environments where new concepts continuously emerge. GBM is designed to work with both conventional deep neural networks and Binary Neural Networks, offering a memory-efficient solution for embedded systems. The approach demonstrates superior performance compared to state-of-the-art methods while maintaining a lower memory footprint.

## Method Summary
GBM introduces a generative approach for class-incremental learning by operating in a latent binary space. The method uses Bernoulli Mixture Models to capture the multi-modal characteristics of class distributions in binary embeddings. A specifically-designed feature binarizer enables compatibility with conventional DNNs, while the approach natively supports BNNs for highly-constrained model sizes. The generative process creates synthetic binary pseudo-exemplars that can be used for replay during incremental learning, helping to mitigate catastrophic forgetting without requiring storage of original training data.

## Key Results
- Achieves higher than state-of-the-art average accuracy on CIFAR100 (+2.9%) and TinyImageNet (+1.5%) for ResNet-18 with proposed binarizer
- For BNNs, outperforms emerging CIL methods with +3.1% in final accuracy and 4.7x memory reduction on CORE50 benchmark
- Effectively addresses catastrophic forgetting while providing superior incremental performance with lower memory footprint

## Why This Works (Mechanism)
The effectiveness of GBM stems from its ability to model class distributions in a binary latent space using Bernoulli Mixture Models. By operating in this binary space, the approach leverages the computational efficiency of binary representations while maintaining discriminative power. The generative nature of the method allows for synthetic sample creation without storing original data, addressing privacy and storage constraints. The combination of binary embeddings with mixture modeling captures the multi-modal characteristics of class distributions, enabling better separation between classes during incremental learning phases.

## Foundational Learning

**Class-Incremental Learning**: Learning new classes over time while retaining knowledge of previously learned classes. *Why needed*: Addresses catastrophic forgetting in dynamic environments. *Quick check*: Does the model maintain performance on old classes when learning new ones?

**Catastrophic Forgetting**: The tendency of neural networks to rapidly forget previously learned information when trained on new tasks. *Why needed*: Fundamental challenge in incremental learning scenarios. *Quick check*: Measure accuracy drop on old classes after learning new classes.

**Bernoulli Mixture Models**: Probabilistic models that represent data distributions using mixtures of Bernoulli distributions. *Why needed*: Enables modeling of binary feature distributions for synthetic sample generation. *Quick check*: Verify model captures multi-modal characteristics of class distributions.

**Binary Neural Networks**: Neural networks with weights and activations constrained to binary values (-1, +1). *Why needed*: Enables highly efficient inference on resource-constrained devices. *Quick check*: Compare model size and inference speed against full-precision counterparts.

**Feature Binarization**: The process of converting continuous feature representations into binary form. *Why needed*: Enables compatibility with BNNs and reduces memory footprint. *Quick check*: Assess information loss from binarization process.

## Architecture Onboarding

**Component Map**: Data → Feature Extractor → Binarizer → Bernoulli Mixture Model → Synthetic Sample Generator → Replay Memory → Incremental Learner

**Critical Path**: The essential processing flow involves extracting features from input data, binarizing them, modeling the binary distribution with BMMs, generating synthetic samples, and using these samples during incremental training to prevent forgetting.

**Design Tradeoffs**: The binary representation reduces memory requirements and computational cost but may introduce information loss. The generative approach avoids storing original data (addressing privacy concerns) but requires careful modeling to maintain discriminative power. BMMs capture multi-modality but increase model complexity.

**Failure Signatures**: Poor performance on fine-grained distinctions between classes, degraded accuracy on older classes after learning new ones, failure to generate diverse synthetic samples, or inability to scale to large numbers of classes.

**First Experiments**:
1. Benchmark GBM against existing CIL methods on CIFAR100 with ResNet-18
2. Evaluate memory efficiency by comparing storage requirements with exemplar-based methods
3. Test BNN compatibility by implementing GBM on a binary ResNet architecture

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation primarily conducted on standard image classification benchmarks, which may not capture performance on more diverse real-world datasets
- Binary embedding space may introduce information loss affecting fine-grained classification tasks
- Comprehensive analysis across different model sizes and architectures is lacking
- Real-world deployment scenarios and computational trade-offs require further validation

## Confidence

**Performance improvements over SOTA**: High - Well-supported by experimental results on CIFAR100 and TinyImageNet
**Memory efficiency claims**: Medium - Demonstrated on CORE50 but needs broader validation
**Compatibility with both conventional DNNs and BNNs**: Medium - Shows flexibility but requires real-world deployment validation

## Next Checks

1. Evaluate GBM performance on domain-specific datasets (medical imaging, satellite imagery) to assess generalization beyond standard benchmarks
2. Conduct ablation studies isolating the contribution of the Bernoulli Mixture Model component versus the binary embedding strategy
3. Perform runtime analysis on embedded hardware platforms to verify claimed memory and computational benefits in practical deployment scenarios