---
ver: rpa2
title: Large Language Models for Oral History Understanding with Text Classification
  and Sentiment Analysis
arxiv_id: '2508.06729'
source_url: https://arxiv.org/abs/2508.06729
tags:
- prompt
- oral
- sentiment
- history
- concise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a scalable framework using large language\
  \ models (LLMs) to automatically annotate Japanese American oral histories for semantic\
  \ classification and sentiment analysis. A high-quality dataset of 558 sentences\
  \ was manually labeled, and four prompt strategies\u2014zero-shot, few-shot, and\
  \ retrieval-augmented generation\u2014were evaluated across three models (ChatGPT,\
  \ Llama, and Qwen)."
---

# Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis

## Quick Facts
- **arXiv ID:** 2508.06729
- **Source URL:** https://arxiv.org/abs/2508.06729
- **Reference count:** 40
- **Primary result:** LLM framework achieves 88.71% F1 for semantic classification and 82.87% F1 for sentiment analysis on Japanese American oral histories

## Executive Summary
This study introduces a scalable framework using large language models (LLMs) to automatically annotate Japanese American oral histories for semantic classification and sentiment analysis. A high-quality dataset of 558 sentences was manually labeled, and four prompt strategies—zero-shot, few-shot, and retrieval-augmented generation—were evaluated across three models (ChatGPT, Llama, and Qwen). ChatGPT achieved the highest F1 score (88.71%) for semantic classification, while Llama slightly outperformed for sentiment analysis (82.87%). The best prompt configurations were applied to annotate 92,191 sentences from 1,002 interviews. Results show LLMs can effectively preserve contextual nuance and narrative integrity in large-scale, culturally sensitive archival analysis. This framework supports ethical, scalable NLP use in digital humanities and collective memory preservation.

## Method Summary
The framework uses expert-annotated sentences (558 from 15 narrators) to develop and evaluate prompt strategies across three LLMs: ChatGPT (GPT-4o), Llama-3.3-70B, and Qwen-2.5-32B. Five prompt variants were tested: Foundational, Structured, Comprehensive, Refined, and Concise. Four strategies were evaluated: zero-shot, few-shot (using 40% of labeled data as examples), zero-shot RAG, and few-shot RAG. The best configurations (ChatGPT few-shot refined for semantic, Llama few-shot concise for sentiment) were then applied to annotate 92,191 sentences from 1,002 interviews. RAG context was retrieved via sentence embedding similarity and appended to prompts. Final annotation used Llama with few-shot concise for sentiment (82.87% F1) and few-shot refined for semantic (84.99% F1).

## Key Results
- ChatGPT few-shot refined achieved 88.71% F1 for semantic classification
- Llama few-shot concise achieved 82.87% F1 for sentiment analysis
- Few-shot prompting improved semantic F1 by 5.36% over zero-shot refined
- Concise prompts consistently yielded lower variance than longer variants (SD up to 3.45 for structured few-shot)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting with domain-specific examples improves semantic classification accuracy over zero-shot approaches
- Mechanism: In-context examples provide task-specific reasoning patterns that help LLMs recognize category boundaries in culturally sensitive narratives
- Core assumption: The labeled examples are representative of the target distribution and don't introduce annotation bias
- Evidence anchors:
  - [section 4.2.1]: "Few shot Setting – In the few-shot setting... The refined prompt reached 88.71%... a +5.24% gain over the foundational prompt"
  - [abstract]: "evaluated zero-shot, few-shot, and retrieval-augmented generation—across three models"
- Break condition: When annotation budget cannot support manual labeling of representative examples; when domain shifts between example set and target corpus

### Mechanism 2
- Claim: Concise prompts can match or exceed comprehensive prompts while reducing token costs and variance
- Mechanism: Information density matters more than prompt length. The paper found concise prompts achieved strong performance with lower standard deviation
- Core assumption: The compression process (LLM-assisted refinement with human verification) preserves task-critical semantics
- Evidence anchors:
  - [section 4.1.3]: "shorter prompts (concise) consistently yield lower variance than longer ones (comprehensive)"
  - [section 4.2.2]: "Prompt length alone does not determine performance; semantic precision and clear instructions are essential"
- Break condition: When tasks require extensive background context that cannot be compressed without semantic loss; when working with models poorly aligned to instruction following

### Mechanism 3
- Claim: RAG augmentation shows mixed results and can introduce noise that degrades performance without careful prompt design
- Mechanism: Retrieval adds context but may retrieve irrelevant content
- Core assumption: Retrieved documents are relevant and authoritative; embedding similarity correlates with task utility
- Evidence anchors:
  - [section 4.2.1]: "Zero shot RAG Setting – The foundational prompt performed poorly with an F1 score 13.08%"
  - [section 4.1.2]: "RAG based strategies exhibit varying robustness... ChatGPT (SD = 1.46) shows higher variance, reflecting sensitivity to noisy retrieval"
- Break condition: When retrieval corpus is small or low-quality; when embedding model poorly matches domain semantics; when inference cost constraints prohibit retrieval overhead

## Foundational Learning

- **Concept: Prompt engineering as task specification**
  - Why needed here: The paper demonstrates 75-point F1 variance (13.08% to 88.71%) based solely on prompt design, showing this is the primary performance lever
  - Quick check question: Can you articulate the difference between foundational, structured, comprehensive, refined, and concise prompt variants?

- **Concept: In-context learning (zero-shot vs. few-shot)**
  - Why needed here: Understanding how examples change model behavior is essential for deciding annotation investment. Few-shot improved semantic F1 by 5.36% but required labeled data
  - Quick check question: Given a budget for 100 manually labeled sentences, would you allocate them to few-shot examples or hold-out evaluation?

- **Concept: Cross-model transferability limitations**
  - Why needed here: Prompts developed on ChatGPT showed degraded performance on Llama and Qwen for semantic classification (88.71% → 84.99% → 83.72%), indicating prompts aren't universally portable
  - Quick check question: If deploying to an open-source model, how would you adapt prompts initially designed for GPT-4?

## Architecture Onboarding

- **Component map:** Data layer (Densho transcripts → 92,191 filtered sentences) -> Annotation layer (558 labeled sentences → LLM inference) -> Prompt layer (5 variants × 4 strategies) -> Model layer (ChatGPT, Llama, Qwen) -> Evaluation layer (F1 + SD) -> Analysis layer (BERTopic, NER)

- **Critical path:**
  1. Expert annotation of representative sample (domain knowledge essential)
  2. Prompt variant design with iterative refinement
  3. Strategy evaluation (zero-shot → few-shot → RAG) with stability testing
  4. Model selection per task (ChatGPT for semantic, Llama for sentiment based on F1+stability)
  5. Large-scale inference with selected configuration
  6. Downstream validation via topic modeling/entity analysis

- **Design tradeoffs:**
  - Accuracy vs. cost: ChatGPT highest accuracy but API costs; Llama/Qwen lower cost but 3-5% F1 drop
  - Prompt complexity vs. stability: Longer prompts can improve accuracy but increase variance (SD up to 3.45 for structured few-shot)
  - Few-shot vs. zero-shot: Few-shot improves semantic (+5.36%) but requires labeled data; zero-shot sufficient for sentiment (82.29% vs. 81.72%)
  - RAG overhead vs. gain: RAG added minimal improvement over few-shot alone while increasing complexity and variance

- **Failure signatures:**
  - Very low F1 with RAG foundational prompts (<20%) → prompt lacks sufficient constraints for retrieved context
  - High variance across runs (SD >1.5) → prompt too verbose or retrieval noisy
  - Cross-model performance collapse → prompts overfit to source model's instruction tuning
  - Semantic classification much worse than sentiment → likely missing domain background in prompt

- **First 3 experiments:**
  1. Establish baseline: Run zero-shot concise prompt on manually labeled 50-sentence subset across all three models; report F1 and SD to establish model ranking for your specific domain
  2. Few-shot ablation: Add 5, 10, 20 examples to prompt; measure F1 curve and token cost to find efficiency frontier before diminishing returns
  3. Stability diagnostic: Run best configuration 10 times; if SD >1.0, simplify prompt by removing one component (background, keywords, or examples) and retest

## Open Questions the Paper Calls Out

- **Open Question 1:** How well do the optimized prompt strategies transfer to oral history archives from different cultural, linguistic, and historical backgrounds beyond Japanese American incarceration narratives?
  - Basis in paper: [explicit] Authors state: "its historical and cultural specificity may limit the generalizability of the results. Further validation is needed to assess whether the proposed annotation approach performs equally well on other oral history collections with different narrative styles, linguistic features, or cultural contexts."
  - Why unresolved: The framework was only tested on the JAIOH corpus; no cross-domain validation has been conducted
  - What evidence would resolve it: Applying the same prompt configurations to other oral history corpora (e.g., Holocaust testimonies, Indigenous narratives) and comparing F1 scores and annotation consistency

- **Open Question 2:** Can automated prompt optimization techniques (e.g., reinforcement learning, gradient-based search) outperform human-designed prompts for culturally sensitive semantic classification?
  - Basis in paper: [explicit] Authors note: "there is still significant room to improve the precision of sentiment and semantic annotation... future work should explore systematic methods to refine instructions"
  - Why unresolved: All prompts were manually designed through iterative experimentation; no automated optimization methods were tested
  - What evidence would resolve it: Comparing F1 scores of automatically optimized prompts versus manually refined prompts on held-out oral history sentences

## Limitations
- The framework's effectiveness is tied to Japanese American oral history context and may not generalize to other cultural or historical narratives
- Cross-model prompt transferability is limited, with 3-5% F1 degradation when moving from ChatGPT to Llama/Qwen
- RAG augmentation showed inconsistent results with high variance, requiring careful prompt design to avoid performance degradation

## Confidence

- **High confidence:** Few-shot prompting effectiveness (+5.36% F1 gain), concise prompt stability (lower variance), and model selection findings (ChatGPT for semantic, Llama for sentiment)
- **Medium confidence:** RAG strategy performance is inconsistent and highly sensitive to prompt design; results show high variance (SD up to 3.45) and mixed effectiveness
- **Low confidence:** Cross-domain generalization of the framework and prompt portability across different LLM architectures are not validated

## Next Checks

1. **Domain transfer validation:** Apply the best-performing prompt configurations to a different oral history collection (e.g., Holocaust survivor testimonies or civil rights movement interviews) and measure F1 degradation compared to the Japanese American corpus

2. **Prompt portability test:** Systematically test prompt variants across 5+ different LLM families (including open-source models like Mistral, Gemma) to quantify transferability limits and develop model-specific adaptation guidelines

3. **RAG robustness evaluation:** Vary retrieval corpus size (100 to 10,000 documents), similarity thresholds (0.5 to 0.9), and embedding models to identify conditions where RAG augmentation provides consistent improvement versus introducing noise