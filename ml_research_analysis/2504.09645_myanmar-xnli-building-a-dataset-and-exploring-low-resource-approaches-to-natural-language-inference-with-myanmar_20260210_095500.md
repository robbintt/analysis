---
ver: rpa2
title: 'Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches to
  Natural Language Inference with Myanmar'
arxiv_id: '2504.09645'
source_url: https://arxiv.org/abs/2504.09645
tags:
- myanmar
- english
- language
- translation
- xnli
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Myanmar XNLI (myXNLI) dataset to address
  the lack of low-resource language benchmarks in cross-lingual natural language inference
  (XNLI). The authors construct myXNLI by translating English XNLI data into Myanmar
  using community crowdsourcing followed by expert verification, demonstrating the
  value of expert review in improving data quality.
---

# Myanmar XNLI: Building a Dataset and Exploring Low-resource Approaches to Natural Language Inference with Myanmar

## Quick Facts
- arXiv ID: 2504.09645
- Source URL: https://arxiv.org/abs/2504.09645
- Authors: Aung Kyaw Htet; Mark Dras
- Reference count: 40
- Key outcome: Introducing myXNLI dataset and demonstrating expert verification plus augmentation strategies improve low-resource Myanmar NLI performance by up to 2 percentage points

## Executive Summary
This paper addresses the critical gap in low-resource language benchmarks for cross-lingual natural language inference (XNLI) by introducing the Myanmar XNLI (myXNLI) dataset. The authors construct myXNLI through a two-stage process: community crowdsourcing followed by expert verification, demonstrating that expert review significantly improves data quality and downstream model performance. They evaluate state-of-the-art multilingual language models on myXNLI, establishing initial performance baselines for Myanmar and showing that data augmentation techniques including multilingual training, cross-matched language pairs, and genre-based side inputs can improve model accuracy by up to 2 percentage points. The study confirms these strategies generalize across low-resource languages, highlighting the importance of high-quality data and targeted training methods in advancing NLP for underrepresented languages.

## Method Summary
The paper introduces myXNLI by translating English XNLI data into Myanmar using community crowdsourcing followed by expert verification to ensure quality. The authors evaluate state-of-the-art multilingual language models (mDeBERTa-v3-base) on myXNLI, establishing initial performance baselines. They explore data augmentation strategies including cross-matched NLI pairs (mixing English premises with Myanmar hypotheses and vice versa) and genre-based inputs where explicit genre metadata is added as special tokens. The fine-tuning procedure uses 1 epoch on the training set, with the combination method (cross-matched EN-MY pairs with genre prefixes) achieving the best results.

## Key Results
- Expert verification stage improves data quality and model performance by up to 2 percentage points
- Cross-matched NLI data augmentation (EN-MY pairs) achieves 80.99% accuracy vs 79.46% baseline for Myanmar
- Fine-tuning multilingual models on target language data (translate-train) outperforms cross-lingual transfer approaches
- Genre-based side inputs provide additional performance gains when consistent genre labels are available

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Two-stage translation (community + expert verification) improves low-resource dataset quality and model performance more effectively than single-stage crowd-sourcing
- **Mechanism**: Community translators provide volume but make errors in polysemy, cultural context, and arbitrary transliteration. Expert verification corrects these semantic "drifts," aligning the text closer to the logical intent of source premise-hypothesis pairs
- **Core assumption**: Bilingual experts using low-resource language domestically but English professionally possess cultural and linguistic nuance missing in general crowd-workers
- **Evidence anchors**: Expert verification significantly improves data quality and model performance; expert review reduces mistranslations of polysemous words and inadequate cultural knowledge; revised myXNLI provides better Myanmar results by up to 2 percentage points
- **Break condition**: If expert reviewers lack specific domain knowledge (e.g., Western cultural concepts), errors persist

### Mechanism 2
- **Claim**: Fine-tuning multilingual models on cross-matched NLI pairs (English premise with Myanmar hypothesis) improves performance over monolingual-only fine-tuning
- **Mechanism**: Mixing languages within training examples forces the model to align representations across languages explicitly, reinforcing semantic relationships regardless of input language
- **Core assumption**: Pre-trained multilingual model (mDeBERTa) possesses sufficient cross-lingual alignment in embedding space to leverage mixed-language pairs
- **Evidence anchors**: Cross-matched augmentation achieves 80.99% accuracy vs 79.46% baseline; quadruple training data in en-en, en-my, my-en, and my-my combinations improves performance
- **Break condition**: If model capacity is too small, adding quadruple data might lead to overfitting or capacity dilution

### Mechanism 3
- **Claim**: Injecting explicit genre metadata as special tokens helps disambiguate context in low-resource NLI tasks
- **Mechanism**: Genre tokens act as conditional signals, allowing attention mechanism to weigh words differently based on register
- **Core assumption**: Training data contains consistent genre labels correlating with distinct linguistic patterns relevant to NLI task
- **Evidence anchors**: Genre tokens added as prefix to both sentences improve classification; genre-aware model focuses on repeated short utterances differently; requires mapping lookup for genre consistency
- **Break condition**: If test data contains unseen genres or mapping errors, special token may confuse model

## Foundational Learning

- **Concept**: **Natural Language Inference (NLI)**
  - **Why needed here**: Core benchmark task requiring 3-way classification (Entailment, Contradiction, Neutral) between Premise and Hypothesis
  - **Quick check question**: If Premise is "You don't have to stay there" and Hypothesis is "You can leave," is this Entailment, Contradiction, or Neutral? (Answer: Entailment)

- **Concept**: **Cross-Lingual Transfer (Zero-Shot vs. Fine-Tuning)**
  - **Why needed here**: Paper evaluates knowledge transfer between languages; must distinguish Zero-Shot (train English, test Myanmar) from Translate-Train (train translated Myanmar, test Myanmar)
  - **Quick check question**: Which approach yields higher accuracy: training multilingual model on English data only, or fine-tuning on target language data? (Answer: Fine-tuning on target language data)

- **Concept**: **Data Augmentation Strategies**
  - **Why needed here**: Paper tests several ways to enrich dataset; must know difference between Adversarial NLI (hard negatives) and Genre-based inputs (metadata conditioning)
  - **Quick check question**: Does adding Adversarial NLI data from English hurt Myanmar performance due to "forgetting"? (Answer: No, provides slight uplift)

## Architecture Onboarding

- **Component map**: Myanmar text → Tokenizer (Unicode compliance) → mDeBERTa-v3-base model → Classification head (3 classes)
- **Critical path**: 
  1. Data Prep: Ensure Unicode compliance (Myanmar has legacy encoding issues like Zawgyi)
  2. Tokenization: Implement logic to prepend mapped Genre token to both Premise and Hypothesis sentences
  3. Training: Fine-tune using "Combination Method" (Cross-matched EN-MY pairs with Genre prefixes)
- **Design tradeoffs**:
  - Multilingual models (mDeBERTa) outperform monolingual models (MyanBERTa) even on monolingual task
  - Improving data quality (expert revision) yields roughly same gain (~2%) as complex augmentation methods
- **Failure signatures**:
  - Cultural mistranslation: Model fails on "Scotch on the Rocks" or "The whole nine yards" if translated literally
  - Space sensitivity: Removing spaces from test set drops performance despite Myanmar not using spaces officially
  - Arbitrary transliteration: Inconsistent spelling of named entities breaks generalization
- **First 3 experiments**:
  1. Baseline Establishment: Fine-tune mDeBERTa-base on English XNLI, evaluate on Myanmar test set (Cross-Lingual Transfer)
  2. Translate-Train: Fine-tune mDeBERTa-base on myXNLI training set (Myanmar only), evaluate on revised test set
  3. Augmentation Ablation: Implement "Combination Method" (EN-MY cross-matched + Genre prefixes), compare against Translate-Train baseline

## Open Questions the Paper Calls Out
- How do newer transformer architectures (mT5, Aya) perform on myXNLI benchmark compared to current state-of-the-art
- To what extent do Myanmar-specific morphological variations impact model tokenization and inference accuracy
- Can prompt-based data augmentation techniques (e.g., PromDA) effectively generate synthetic training data to improve low-resource Myanmar NLI
- How does model performance degrade when there are significant domain shifts between training and test data

## Limitations
- Reliance on crowdsourced translations without full specification of translator qualification criteria
- Evaluation framework only tests on revised test set, not original crowd-sourced version
- Generalization claims across low-resource languages not systematically validated beyond Myanmar

## Confidence
- **High Confidence**: Baseline performance of multilingual models on Myanmar XNLI (~79.5%) and improvement from translate-train fine-tuning
- **Medium Confidence**: Expert verification significantly improves data quality, but quantitative comparison against original dataset lacking
- **Low Confidence**: Generalization claim that cross-matched augmentation benefits "all low-resource languages" extends beyond empirical scope

## Next Checks
1. Replicate with Original Dataset: Obtain and evaluate on both original crowd-sourced test set and revised expert-verified version to quantify exact contribution of dataset revision versus training methodology
2. Multi-Language Ablation Study: Apply cross-matched augmentation and genre-based input methods to at least two additional low-resource languages (e.g., Swahili, Hindi) to determine if Myanmar gains generalize
3. Expert Selection Protocol Documentation: Document and test impact of different expert qualification criteria by creating parallel test sets reviewed by different expert pools