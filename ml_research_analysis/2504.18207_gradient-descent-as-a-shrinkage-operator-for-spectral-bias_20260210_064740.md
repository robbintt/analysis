---
ver: rpa2
title: Gradient Descent as a Shrinkage Operator for Spectral Bias
arxiv_id: '2504.18207'
source_url: https://arxiv.org/abs/2504.18207
tags:
- activations
- singular
- relu
- components
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates spectral bias in shallow neural networks
  by establishing explicit connections between activation functions, gradient descent
  (GD) hyperparameters, and implicit regularization. The key contributions are: (1)
  Activation functions can be related to spline regression regularization, where the
  activation choice controls the order of gradient smoothing; (2) GD acts as a shrinkage
  operator on the singular values of a network''s Jacobian, with learning rate and
  iterations controlling which principal components are preserved; (3) An explicit
  relationship is derived between GD hyperparameters and the number of active principal
  components, with monotonic activations (Heaviside, tanh) being significantly more
  iteration-efficient than non-monotonic ones (ReLU, GELU, SiLU); (4) Non-monotonic
  activations like sinc and Gaussian are shown to be better suited for scale-based
  regularization rather than GD hyperparameter tuning.'
---

# Gradient Descent as a Shrinkage Operator for Spectral Bias

## Quick Facts
- arXiv ID: 2504.18207
- Source URL: https://arxiv.org/abs/2504.18207
- Reference count: 40
- One-line primary result: Gradient descent acts as a soft-shrinkage operator on network Jacobians, with monotonic activations (tanh, ReLU) being efficiently regularized by GD iterations while non-monotonic activations (sinc, Gaussian) require scaling adjustments instead.

## Executive Summary
This paper establishes a rigorous connection between gradient descent hyperparameters and spectral bias in shallow neural networks by characterizing GD as a soft-shrinkage operator on the singular values of the network Jacobian. The key insight is that monotonic activation functions produce singular value spectra amenable to GD regularization, while non-monotonic activations require different control mechanisms. The analysis reveals that GD iterations preserve only the top K principal components, with the number K controlled by the learning rate and iteration count. The paper provides explicit formulas linking these hyperparameters to the desired bandwidth of the learned function, demonstrating that GD's implicit regularization is effective only with monotonic activations like tanh and ReLU.

## Method Summary
The method analyzes 1D shallow networks f(x) = Σ w_m · η(x - b_m) where biases b are fixed equally-spaced points and weights w are optimized via full-batch gradient descent on L2 loss. The core technique involves computing the SVD of the network Jacobian matrix A to obtain singular values, then deriving the explicit shrinkage operator m(s; α, q) = [1 - exp(-α · q · s²)] that describes how GD masks these values. Two hyperparameter strategies are proposed: for monotonic activations, set σ ≈ M and q ≈ s_max² · s_K⁻²; for non-monotonic activations, fix q=100 and set σ = K. The framework is validated through signal reconstruction experiments comparing to discrete sine transform baselines.

## Key Results
- GD acts as a soft-shrinkage operator on singular values with threshold κ ≈ sqrt(-log(ϵ) / (α · q)), preserving only top K principal components
- Monotonic activations (tanh, ReLU) produce spectra suitable for GD regularization; non-monotonic activations (sinc, Gaussian) require scaling factor control
- Principal components of monotonic activations approximate Discrete Sine Transform basis, linking index K to frequency bandwidth B
- Tuning learning rate and iterations works effectively for monotonic activations, while non-monotonic activations need scaling parameter σ = K

## Why This Works (Mechanism)

### Mechanism 1
Gradient descent implicitly regularizes a shallow network by acting as a shrinkage operator on the singular values of its Jacobian, preserving only the top K principal components. GD iterations apply a soft-thresholding filter m(s; α, q) = [1 - exp(-α · q · s²)] to the inverse of the Jacobian's singular values. This mask is near 1 for large singular values (low-frequency components) and near 0 for small ones (high-frequency components), with transition width controlled by q. The analysis relies on gradient flow approximations and least-squares loss in an overparameterized regime (M > N). Assumption: Biases b are fixed and equally spaced. Evidence: Proposition 4.1 defines the masking function, Figure 2 visualizes how q controls the active window. Breaks if network is not overparameterized, biases are learned, or loss landscape is highly non-convex.

### Mechanism 2
The effectiveness of GD's implicit regularization depends entirely on the activation function's spectral profile. Monotonic activations (tanh, ReLU) produce a rapid initial drop-off in singular values, creating clear separation for GD's soft mask to exploit. Non-monotonic activations (sinc, Gaussian) produce a flat plateau followed by precipitous drop, making GD's slow transition mask ineffective; their regularization is instead controlled by scaling factor σ. The singular value distributions observed for 1D signals generalize. Assumption: Primary goal is controlling spectral bandwidth. Evidence: Figure 3 contrasts spectra of monotonic vs. non-monotonic activations, Figure 6 demonstrates empirical result: tuning q works for tanh/ReLU, tuning σ works for sinc/Gaussian. Break condition: This control mechanism is derived for 1D inputs; link between principal component index and frequency may be lost in higher dimensions.

### Mechanism 3
Principal components of Jacobian for monotonic activations approximate Discrete Sine Transform (DST) frequency basis. Eigenvectors of matrix AA^T for monotonic activations η correspond to increasing spatial frequencies, allowing index K of retained principal components to serve as direct proxy for signal's maximum frequency (bandwidth). Assumption: Biases b are equally spaced; approximation to true frequency basis strictest for Heaviside/tanh and less so for higher-order activations like ReLU. Evidence: Figure 4 visualizes principal components showing sinusoidal shape, Discussion section formalizes link between K and bandwidth B. Break condition: Approximation degrades for finite network widths and higher-order activations; frequency relationship not preserved in multi-dimensional settings without specific initialization.

## Foundational Learning

**Singular Value Decomposition (SVD)**: Why needed here: The paper's core insight is that GD acts on the *singular values* of the network Jacobian. Understanding that these values represent the gain of the linear transformation along its principal axes is essential for grasping the shrinkage mechanism. Quick check question: If a matrix has singular values [10, 1, 0.1], and we apply a shrinkage operator that masks values below 0.5, how many principal components will be preserved?

**Spectral Bias**: Why needed here: This is the central problem the paper addresses. You must understand that neural networks tend to learn low-frequency (smooth) functions first, treating high-frequency details as noise unless properly regularized. Quick check question: A model fits a smooth curve to data with high-frequency noise. Is this an example of spectral bias hurting or helping generalization?

**Gradient Flow**: Why needed here: The paper uses gradient flow (continuous-time approximation of discrete GD steps) to derive the explicit shrinkage operator formula m(s; α, q). This mathematical tool bridges gap between iterative optimization and closed-form spectral analysis. Quick check question: In derived relationship κ ≈ sqrt(-log(ϵ) / (α · q)), if we increase number of iterations q, does singular value threshold κ increase or decrease?

## Architecture Onboarding

**Component map**: Activation (η) -> Biases (b) -> Weights (w) -> Optimizer (GD with α, q)

**Critical path**:
1. **Initialization:** Fix bias grid b (e.g., 1024 equally spaced points in [0,1]). Choose activation η.
2. **Hyperparameter Strategy:**
    - **If Monotonic:** Set scaling σ ≈ M. Determine desired bandwidth B. Use paper's heuristic to set iterations q ≈ s_max² · s_K⁻² and α ≈ s_max⁻².
    - **If Non-monotonic:** Set scaling σ = K to control bandwidth directly. Use fixed, modest number of iterations (e.g., q=100).
3. **Training:** Run full-batch GD on MSE loss. Implicit regularization handled by chosen strategy (iterations or scaling).

**Design tradeoffs**:
- **Tanh (Monotonic):** Best match to frequency basis (DST). Efficiently regularized by GD. *Tradeoff:* Performance sensitive to scaling factor σ.
- **ReLU (Monotonic):** Scale-equivariant (robust to σ). Forms smooth basis. *Tradeoff:* Less strict frequency alignment than tanh. Iteration-inefficient for regularization.
- **Sinc (Non-monotonic):** Optimal frequency basis. *Tradeoff:* Cannot be regularized via GD iterations. Spatial decay can be slow.
- **Heaviside / ReLU2:** Avoid. Heaviside has discontinuous interpolation; ReLU2 extremely iteration-inefficient (orders of magnitude more iterations needed).

**Failure signatures**:
- **Symptom:** Using non-monotonic activation (e.g., Gaussian) and tuning GD iterations q has no effect on generalization. **Fix:** Switch to tuning scaling factor σ to control bandwidth.
- **Symptom:** Using monotonic activation (e.g., tanh) and getting poor or noisy reconstruction. **Fix:** Check scaling σ ≈ M and ensure q set appropriately for desired bandwidth.
- **Symptom:** Training 2D or higher-dimensional network and finding no relationship between singular values and frequency. **Fix:** Ensure using full-rank initialization for bias directions V (Appendix E), not rank-1 initialization.

**First 3 experiments**:
1. **Reproduce Figure 6a:** Train shallow network with tanh activation on 1D signal. Fix σ. Demonstrate that increasing GD iterations q systematically increases bandwidth (and PSNR) of reconstruction, similar to DST baseline.
2. **Reproduce Figure 6b:** Train shallow network with sinc activation on same signal. Fix q=100. Demonstrate that increasing scaling factor σ systematically controls bandwidth, while changing q has little effect.
3. **Validate the Shrinkage Operator:** For synthetic problem, compute ground truth singular value mask m_gd using Prop 4.1's formula. Compare to effective mask learned by GD after q iterations. Plot both to visually confirm "active window" and "grey zone" behavior.

## Open Questions the Paper Calls Out

**Open Question 1**: Does the interpretation of gradient descent as a singular value shrinkage operator generalize effectively to deep neural networks? The paper suggests insights from 1D shallow case could have ramifications for deep networks, but theoretical derivations rely specifically on shallow network architecture where Jacobian ∇f(x) equates to matrix A^T. Deep networks introduce non-linear layer interactions that complicate direct application of this linear shrinkage model. Evidence would be empirical validation showing varying learning rate and iterations in deep networks results in masking effect on singular values of deep network's Jacobian or NTK.

**Open Question 2**: How can explicit relationship between GD hyperparameters and spectral bandwidth be adapted for multi-dimensional signals (D > 1) where frequency basis is not strictly preserved? Appendix E demonstrates that for 2D signals (D=2), principal component index loses direct relationship to frequency/bandwidth, unlike 1D case which adheres to Discrete Sine Transform. This mapping breaks down in higher dimensions, making current heuristic for selecting iteration count q inapplicable. Evidence would be theoretical extension of shrinkage operator accounting for mixed spectral properties of 2D bases, or modified hyperparameter heuristic controlling smoothness in higher dimensions.

**Open Question 3**: How does optimization of bias terms b (rather than keeping them fixed) alter singular value distribution and resulting implicit regularization? Section 4 explicitly assumes "we are only optimizing for weights w" and Section 3 notes approximation holds as M → ∞ with fixed biases. Allowing b to be learned makes problem non-linear and non-convex, potentially changing singular value spectrum of Jacobian dynamically during training. Evidence would be analysis of how singular value distribution of A shifts when biases are updated via GD, and whether shrinkage effect on spectral bias persists or degrades.

## Limitations

- Analysis is rigorously derived and experimentally validated only for 1D shallow networks; frequency interpretation of principal components may not hold for higher-dimensional data
- Theoretical results depend on key assumptions including gradient flow approximation, overparameterized regime (M > N), fixed-bias assumption, and full-rank initialization requirements
- Empirical validation limited to single image patch (peppers) and synthetic signals; doesn't address generalization to other signal types or real-world datasets

## Confidence

**High Confidence**: The characterization of gradient descent as a soft-shrinkage operator (Proposition 4.1) is mathematically rigorous and empirical validation is strong. The dichotomy between monotonic and non-monotonic activations regarding GD regularization effectiveness is clearly demonstrated and theoretically justified.

**Medium Confidence**: The frequency interpretation of principal components for monotonic activations is well-supported in 1D but extension to higher dimensions is not thoroughly validated. The specific hyperparameter recommendations (σ ≈ M for monotonic, σ = K for non-monotonic) are based on empirical observations that may be dataset-dependent.

**Low Confidence**: The paper doesn't provide validation for interaction between activation choice and data characteristics beyond simple 1D case. The recommendation to avoid Heaviside and ReLU² is based on iteration inefficiency rather than convergence or generalization issues, which may be context-dependent.

## Next Checks

1. **Multi-dimensional Extension Validation**: Reproduce core experiment (Figure 6a/b) on 2D image patch using full-rank initialization strategy from Appendix E. Verify whether singular value-frequency relationship and GD regularization effectiveness persist. Compare principal components to actual 2D Fourier modes.

2. **Assumption Breakage Test**: Implement shallow network with learned biases (not fixed) and train on 1D signal reconstruction task. Measure whether GD shrinkage behavior (Proposition 4.1) still manifests. Document deviation from theoretical predictions and identify which assumptions are most critical.

3. **Data Distribution Sensitivity**: Test framework on different 1D signal types (smooth sinusoids, piecewise polynomials, noise-corrupted signals). For each, measure optimal scaling factors and iteration counts for various monotonic activations. Determine whether heuristic σ ≈ M and q ∝ (s_max/s_K)² are robust across signal characteristics or require dataset-specific tuning.