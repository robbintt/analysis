---
ver: rpa2
title: 'Small Language Models for Efficient Agentic Tool Calling: Outperforming Large
  Models with Targeted Fine-tuning'
arxiv_id: '2512.15943'
source_url: https://arxiv.org/abs/2512.15943
tags:
- arxiv
- language
- tool
- evaluation
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates replacing expensive Large Language Models
  (LLMs) with Small Language Models (SLMs) for tool-calling tasks. The authors fine-tuned
  a 350M parameter OPT model using the ToolBench dataset, focusing on instruction-following
  and API interaction.
---

# Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning

## Quick Facts
- arXiv ID: 2512.15943
- Source URL: https://arxiv.org/abs/2512.15943
- Reference count: 25
- Primary result: 350M parameter SLM achieves 77.55% pass rate on ToolBench, outperforming larger models including ChatGPT-CoT (26.00%)

## Executive Summary
This paper demonstrates that small language models (SLMs) can outperform significantly larger general-purpose models on specialized tool-calling tasks through targeted fine-tuning. The authors fine-tuned a 350M parameter OPT model on the ToolBench dataset using a single epoch of supervised fine-tuning, achieving a 77.55% pass rate compared to 26-30% for baseline models. The approach leverages parameter efficiency, format adherence, and aggressive regularization to create a cost-effective alternative for enterprise AI deployment. The results challenge the assumption that larger models are always superior for specialized tasks.

## Method Summary
The authors fine-tuned facebook/opt-350m using Hugging Face TRL's SFT trainer on the ToolBench dataset, which was transformed into 187,542 instruction-following examples with structured Thought-Action-Action Input patterns. Training occurred on Amazon SageMaker with a single epoch, learning rate of 5×10⁻⁵, gradient clipping (max norm=0.3), and FP16 mixed precision. The model was evaluated on 1,100 test queries across 6 categories using ToolEval with ChatGPT-based scoring, achieving a 77.55% pass rate that significantly outperformed larger baselines.

## Key Results
- 350M parameter SLM achieved 77.55% pass rate on ToolBench evaluation
- Outperformed ChatGPT-CoT (26.00%), ToolLLaMA-DFS (30.18%), and ToolLLaMA-CoT (16.27%)
- Demonstrated superior performance for structured tool-calling tasks compared to general-purpose models

## Why This Works (Mechanism)

### Mechanism 1: Parameter Efficiency via Task Specialization
The paper posits that general LLMs suffer from "parameter dilution," where capacity is wasted on capabilities irrelevant to tool calling. By constraining the model to 350M parameters and training exclusively on tool-use data, the system forces "parameter efficiency," utilizing the entire weight space for targeted API manipulation logic.

### Mechanism 2: Format Adherence via Structured SFT
Supervised Fine-Tuning on high-quality, structured chains (Thought-Action-Observation) aligns the model's generation distribution strictly to expected agentic format. The model is trained on "Thought-Action-Action Input" patterns, suppressing overgeneralization and forcing precise syntax required for machine-readable API calls.

### Mechanism 3: Stable Convergence via Aggressive Regularization
A "high-stability" hyperparameter configuration (low learning rate, aggressive clipping) allows a small model to converge effectively in a single epoch without catastrophic forgetting. The conservative learning rate ($5 \times 10^{-5}$) with high gradient clipping (max norm=0.3) prevents weight oscillation when learning sparse tool-specific tokens.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) vs. Zero-Shot Prompting**: Why needed? The paper argues specialized adaptation (SFT) is superior to generic capability for fixed tasks. Quick check: Can you explain why a smaller fine-tuned model might outperform a larger prompted model on a specific API schema?

- **The ReAct Paradigm (Reasoning + Acting)**: Why needed? The target format "Thought-Action-Action Input" derives from ReAct framework where model generates reasoning step before API call. Quick check: How does interleaving "Thoughts" with "Actions" theoretically improve tool-calling accuracy?

- **ToolBench/ToolEval Evaluation**: Why needed? The paper relies on "Pass Rate" and "Win Rate" metrics from ChatGPT-as-judge evaluations. Understanding this is offline evaluation rather than live API execution is critical. Quick check: Does a "Pass Rate" imply the API was successfully called in reality, or that the generated plan was logically correct according to an evaluator?

## Architecture Onboarding

- **Component map**: facebook/opt-350m (Base Model) -> Hugging Face TRL SFTTrainer (Training Framework) -> ToolBench (Data) -> Amazon SageMaker ml.g5.8xlarge (Infrastructure) -> ToolEval (Evaluation)

- **Critical path**:
  1. Data Transformation: Convert ToolBench data into structured sequences with system/user/assistant delimiters
  2. SFT Configuration: Initialize with lr=5e-5, grad_norm=0.3, epochs=1, batch_size=32
  3. Evaluation: Run inference on 1,100 test queries and pass outputs to ToolEval for Pass Rate calculation

- **Design tradeoffs**: Size vs. Specialization (350M minimizes inference cost but risks complex contextual nuance), Single Epoch vs. Overfitting (avoids overfitting but assumes high data quality)

- **Failure signatures**: Overgeneralization (verbose text instead of JSON), Catastrophic Forgetting (loss of syntactic coherence), Low Pass Rate on Unseen Tools (memorization vs. reasoning)

- **First 3 experiments**:
  1. Reproducibility Run: Fine-tune OPT-350M on ToolBench subset using exact hyperparameters to verify 77% baseline
  2. Generalization Test: Evaluate specifically on "G1-tool" (unseen tools) category to verify reasoning vs. memorization
  3. Hyperparameter Ablation: Increase gradient clipping or learning rate to test stability claim

## Open Questions the Paper Calls Out

- **Generalizability to diverse tool-calling frameworks**: The authors state "Future work should explore the generalizability of our approach across different domains" and note potential limitations with other tool-calling frameworks or real-world API ecosystems.

- **Task complexity vs. optimal parameter count**: The paper acknowledges "The optimal parameter count likely varies across different specialized domains, warranting systematic investigation of task-complexity to model-capacity relationships."

- **Hybrid architectures combining SLMs and LLMs**: The authors propose "Future research should... develop hybrid approaches that combine the efficiency of targeted models with the adaptability of larger systems."

- **Memorization vs. generalized reasoning**: The authors flag concerns about "The tight coupling between training data and evaluation metrics raises questions about performance on novel tool domains."

## Limitations

- Data transformation opacity: Training data was "generated using Amazon Q" but exact scripts and delimiters are not provided
- Evaluation framework dependence: Primary evaluation relies on ToolEval using ChatGPT as automated scorer with undisclosed prompts and criteria
- Single epoch generalizability: The aggressive single-epoch approach may not scale to noisier datasets or more complex scenarios

## Confidence

- **High confidence**: Core claim that targeted fine-tuning of SLMs outperforms larger models on structured tool-calling tasks (supported by empirical results)
- **Medium confidence**: Mechanism explanations (parameter efficiency, format adherence, stability) are logically coherent but need deeper validation
- **Low confidence**: Generalizability to domains beyond API tool-calling remains uncertain without additional validation

## Next Checks

1. **Ablation on training duration**: Systematically train the same 350M model for 1, 2, 3, and 5 epochs while monitoring validation loss and test pass rate to verify single-epoch optimality.

2. **Evaluator independence test**: Run fine-tuned model outputs through ToolEval with ChatGPT, human expert evaluation, and rule-based validator to quantify evaluator bias.

3. **Parameter scaling study**: Fine-tune OPT models at 125M, 350M, and 1.3B parameters using identical hyperparameters and datasets to empirically determine optimal model size.