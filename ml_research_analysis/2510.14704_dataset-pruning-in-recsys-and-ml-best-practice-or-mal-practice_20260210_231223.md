---
ver: rpa2
title: 'Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?'
arxiv_id: '2510.14704'
source_url: https://arxiv.org/abs/2510.14704
tags:
- pruning
- core
- datasets
- dataset
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This thesis investigates how data pruning \u2013 removing users\
  \ with fewer than a specified number of interactions \u2013 affects both dataset\
  \ characteristics and algorithm performance in recommender systems. Five widely\
  \ used datasets were examined in their unpruned form and at five progressive pruning\
  \ levels (5, 10, 20, 50, 100)."
---

# Dataset Pruning in RecSys and ML: Best Practice or Mal-Practice?
## Quick Facts
- **arXiv ID:** 2510.14704
- **Source URL:** https://arxiv.org/abs/2510.14704
- **Reference count:** 0
- **Primary result:** Dataset pruning (removing users with few interactions) artificially inflates performance metrics when both training and testing on pruned data.

## Executive Summary
This thesis investigates how user-based core pruning affects dataset characteristics and algorithm performance in recommender systems. Five datasets were pruned at progressive levels (5, 10, 20, 50, 100 interactions per user minimum) and evaluated using eleven algorithms. The study reveals that training and testing on equally pruned data inflates performance metrics by systematically excluding cold-start users, while evaluation on unpruned test sets shows substantial performance declines. Traditional algorithms benefit from increased density in pruned datasets, while deep learning models suffer from reduced training volume. The findings question whether commonly applied pruning practices represent best practice or mal-practice in RecSys research.

## Method Summary
The study examines five datasets (Amazon CD, Amazon Toys, Gowalla, MovieLens Full Latest, Yelp) through a two-phase evaluation. Datasets were converted from explicit to implicit feedback (ratings ≥4.0 → 1), downsampled to 3M interactions, and pruned using user-based core filtering at thresholds [0, 5, 10, 20, 50, 100]. Each coreset underwent 80/20 user-based train-test splitting. Eleven algorithms from LensKit (BiasedMF, ImplicitMF, UserKNN, ItemKNN, PopScore) and RecBole (BPR, DiffRec, DMF, MultiVAE, SimpleX, Random) were trained with default hyperparameters. Phase 1 evaluated models on equally pruned test sets; Phase 2 evaluated pruned-model performance on unpruned test sets (disjoint, equal-sized, containing low-activity users).

## Key Results
- Training and testing on pruned data artificially inflates nDCG@10 scores, with traditional algorithms showing inflated performance that largely disappears when evaluated on unpruned test sets.
- Progressive pruning increases dataset density, benefiting traditional algorithms (ImplicitMF improved by 96% on 100-core vs 0-core) while harming deep learning models (DiffRec declined by 80% at 100-core).
- Identical pruning thresholds produce vastly different user retention rates across datasets (5-core pruning retained 75% of Gowalla users but only 14% of Yelp users).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training and testing on equally pruned data artificially inflates performance metrics by systematically excluding the most challenging evaluation cases.
- Mechanism: Core pruning removes users with fewer interactions than a threshold (cold-start users). When both train and test sets undergo identical pruning, test sets contain only high-activity users with rich interaction histories. Recommendations for these users are inherently easier, inflating metrics like nDCG@10. When models trained on pruned data are evaluated on unpruned test sets that reintroduce cold-start users, the performance advantage largely disappears.
- Core assumption: Low-activity users are harder to generate accurate recommendations for than high-activity users due to sparse interaction histories.
- Evidence anchors:
  - [abstract]: "Traditional algorithms achieved higher nDCG@10 scores when both training and testing on pruned data; however, this advantage largely disappeared when evaluated on unpruned test sets."
  - [section 6.1.2.2]: "These findings suggest that the improved performance on progressively pruned datasets stems from the test set composition, as the test sets were the only element that differed between the two evaluation settings."
  - [corpus]: Weak relevance. Corpus papers address LLM parameter pruning, not RecSys dataset pruning effects on evaluation validity.
- Break condition: When test sets include unpruned data containing low-activity users, the observed performance gains from training on pruned data diminish substantially. For traditional algorithms tested on unpruned data, the thesis reports "a decline of an average 95% across the pruning levels" compared to pruned-test evaluation.

### Mechanism 2
- Claim: Progressive pruning increases dataset density, which benefits traditional algorithms (MF, KNN) but harms modern deep learning algorithms that require larger training volumes.
- Mechanism: Removing users with few interactions reduces space size (|U| × |I|) faster than it reduces interaction count, increasing density. Traditional matrix factorization and neighborhood-based methods improve as sparsity decreases—User KNN improved by 70% and Implicit MF by 96% on 100-core versus 0-core when tested on pruned data. Conversely, deep learning models like DiffRec declined by 80% and MultiVAE by over 50% at 100-core, as they depend on training data volume.
- Core assumption: Traditional and modern algorithms have fundamentally different sensitivity profiles—traditional methods benefit from density increases, while deep learning methods require data volume.
- Evidence anchors:
  - [section 5.1.3.1]: "The density of all datasets continued to increase with each Core-level."
  - [section 6.1.2.1]: "Density increased with each level of pruning applied, thus Sparsity decreased and performance for Biased MF and Implicit MF improved."
  - [corpus]: Weak relevance. Corpus focuses on model weight pruning for efficiency, not dataset density effects on RecSys algorithms.
- Break condition: At extreme pruning levels (e.g., 100-core), even density-benefiting algorithms may decline if training data volume falls below a critical threshold. The thesis shows MovieLens at 100-core retained only 30% of interactions, causing BPR performance to drop 44% compared to 50-core.

### Mechanism 3
- Claim: The same pruning threshold produces vastly different retention rates across datasets depending on their underlying user interaction distribution.
- Mechanism: Datasets with high average interactions per user and moderate GiniUser (interaction inequality) retain more users at each pruning level. Gowalla (Avg. Int/User = 28.72) retained 75% of users at 5-core, while Yelp (Avg. Int/User = 3.16) retained only 14%. This asymmetry means the same "5-core" label describes fundamentally different evaluation conditions across datasets, compromising comparability.
- Core assumption: User retention rates directly determine how representative a pruned dataset is of its source population.
- Evidence anchors:
  - [section 5.1.4.3]: "5-core pruning revealed a gap of 61% between the datasets with the highest and lowest user retention rates."
  - [section 5.1.4.3]: "5-core pruning removed 86% of Yelp's users while preserving 58% of its interactions, whereas Gowalla experienced a 25% loss in users, retaining 98% of its interactions."
  - [corpus]: Weak relevance. Corpus papers do not address dataset retention rate asymmetry in RecSys evaluation.
- Break condition: When comparing algorithms across datasets pruned at identical thresholds, results may not be comparable due to radically different user population compositions.

## Foundational Learning

- Concept: Cold-start problem in recommender systems
  - Why needed here: The thesis hinges on understanding that removing low-activity users eliminates the most challenging recommendation scenarios, making evaluation easier but less realistic.
  - Quick check question: If a dataset has 100,000 users but 80,000 have fewer than 5 interactions, what proportion of the recommendation challenge is being removed by 5-core pruning?

- Concept: nDCG@k (Normalized Discounted Cumulative Gain at cutoff k)
  - Why needed here: This is the primary evaluation metric used throughout the thesis. Understanding that it rewards ranking relevant items higher in the recommendation list is essential for interpreting the results.
  - Quick check question: If Algorithm A achieves nDCG@10 = 0.08 on pruned data but 0.002 on unpruned data, what does this suggest about the composition of the pruned test set?

- Concept: Gini coefficient for interaction distribution
  - Why needed here: The thesis uses GiniUser and GiniItem to characterize how evenly interactions are distributed. A Gini of 0 indicates perfect equality; 1 indicates one user/item receives all interactions.
  - Quick check question: If GiniUser decreases from 0.70 to 0.18 as pruning increases from 0-core to 100-core, what does this tell you about the remaining user population?

## Architecture Onboarding

- Component map:
  Data preprocessing layer -> Pruning module -> Split generator -> Algorithm layer -> Evaluation layer

- Critical path:
  1. **Retention analysis first**: Before training any model, compute user/item/interaction retention rates for your dataset at each pruning level. If 5-core removes >70% of users, reconsider the threshold.
  2. **Dual evaluation setup**: Always evaluate on both pruned and unpruned test sets. The gap between these reveals inflation magnitude.
  3. **Dataset characteristic tracking**: Monitor density, GiniUser, and average interactions per user across pruning levels to predict which algorithms will benefit or suffer.

- Design tradeoffs:
  - **Computational efficiency vs. generalizability**: Pruning reduces training time (beneficial for sustainability) but produces results that may not transfer to real-world scenarios with cold-start users.
  - **Reproducibility vs. realism**: Using standard pruned benchmarks (e.g., MovieLens 20-core) enables comparison with prior work but excludes 85%+ of users.
  - **Algorithm fairness**: Evaluating only on high-activity users may favor algorithms that perform well for engaged users while penalizing those optimized for cold-start scenarios.

- Failure signatures:
  - **Inflated performance on pruned test, collapse on unpruned test**: Indicates model has overfit to high-activity user patterns. Traditional algorithms showed this pattern most strongly.
  - **Algorithm ranking changes dramatically across pruning levels**: Signals dataset-specific favoritism. The thesis observed "more frequent and more extreme" rank changes on dense datasets (MovieLens, Yelp).
  - **Retention rate below 15% at moderate pruning (20-core)**: Dataset is too sparse for meaningful comparison. Yelp at 20-core retained only 1.86% of users.

- First 3 experiments:
  1. **Establish retention baseline**: For your dataset, compute and plot user/item/interaction retention at [0, 5, 10, 20, 50, 100]-core. Identify the threshold where user retention drops below 30%.
  2. **Dual-path evaluation on a single algorithm**: Train UserKNN on progressively pruned data. Evaluate on both pruned and unpruned test sets. Plot nDCG@10 for both paths to visualize inflation.
  3. **Cross-algorithm sensitivity test**: Train one traditional (ImplicitMF) and one modern (SimpleX) algorithm at 0-core, 10-core, and 50-core. Evaluate on unpruned test sets only. Compare the slope of performance decline to determine which category is more sensitive to your dataset's pruning behavior.

## Open Questions the Paper Calls Out

- **Question:** How does pruning based on item interactions (removing items with few interactions) compare to user-based pruning in terms of dataset characteristics and algorithm performance?
  - **Basis in paper:** [explicit] The authors state in the Future Work section: "It is interesting to examine how pruning based on item interactions, e.g. pruning all interactions with an item if it is interacted with less than a specific number of times, impacts both dataset characteristics and algorithm performance."
  - **Why unresolved:** The thesis exclusively focused on user-based core pruning (removing users with low interaction counts) and did not analyze the structural or performance effects of removing sparse items.
  - **What evidence would resolve it:** A replication of the study's methodology using item-based interaction thresholds for pruning, followed by a comparison of structural changes (e.g., density, Gini coefficients) and algorithm performance.

- **Question:** Does dataset pruning affect beyond-accuracy metrics (e.g., fairness, diversity, novelty) differently than accuracy metrics?
  - **Basis in paper:** [explicit] The paper notes in Future Work that "researchers could examine the impact data pruning has on, e.g. beyond-accuracy metrics for deep learning models."
  - **Why unresolved:** The study was limited to ranking-based accuracy metrics (nDCG, Precision, Recall) and did not evaluate whether pruning exacerbates popularity bias or reduces recommendation diversity.
  - **What evidence would resolve it:** Evaluation of the pruned models using metrics such as catalog coverage, Shannon entropy, or Gini indices specifically measuring the distribution of recommended items.

- **Question:** To what extent does hyperparameter optimization mitigate the performance degradation observed when training on pruned datasets?
  - **Basis in paper:** [explicit] The authors acknowledge a limitation where "All models were trained with their default parameter values" and suggest "Future work should consider implementing hyperparameter-tuning."
  - **Why unresolved:** It is unclear if the performance decline in Phase 2 (testing on unpruned data) was due to the data reduction itself or because the default parameters were suboptimal for the altered data distributions.
  - **What evidence would resolve it:** A comparative analysis where models are tuned specifically for each core-pruned training set before being evaluated on the unpruned test sets.

## Limitations
- Only five datasets were examined, all with different domain characteristics, limiting generalizability.
- No hyperparameter tuning was performed, which may mask algorithmic strengths in pruned settings.
- The analysis focuses exclusively on user-based core pruning without exploring item-based or interaction-based alternatives.

## Confidence
- **High Confidence**: The core finding that training and testing on pruned data artificially inflates performance (Mechanism 1) is supported by clear quantitative evidence showing 95% average decline when evaluated on unpruned test sets.
- **Medium Confidence**: The differential algorithmic responses to pruning (Mechanism 2) is well-documented but may vary with hyperparameter optimization.
- **Medium Confidence**: The asymmetric retention rates across datasets (Mechanism 3) is empirically demonstrated but requires validation on additional datasets.

## Next Checks
1. **Cross-domain validation**: Apply the same pruning and evaluation methodology to non-sequential datasets (e.g., Last.fm music listening history) to test if findings generalize beyond location-based and product recommendation domains.
2. **Algorithm-specific pruning analysis**: For each algorithm category (traditional vs. modern), identify the optimal pruning threshold that maximizes performance on unpruned test sets, then compare these thresholds across categories to determine if different pruning strategies are warranted.
3. **Cold-start user simulation**: Create synthetic low-activity users by sampling from existing user interaction patterns, then evaluate whether models trained on pruned data perform worse on these simulated cold-start users compared to models trained on unpruned data.