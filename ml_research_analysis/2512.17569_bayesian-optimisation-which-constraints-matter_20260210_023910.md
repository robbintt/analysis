---
ver: rpa2
title: 'Bayesian Optimisation: Which Constraints Matter?'
arxiv_id: '2512.17569'
source_url: https://arxiv.org/abs/2512.17569
tags:
- function
- constraints
- objective
- cost
- dckg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Bayesian optimisation for expensive black-box
  optimisation problems with decoupled black-box constraints, where subsets of the
  objective and constraint functions can be evaluated independently. The core method
  idea is to extend the constrained Knowledge Gradient (cKG) acquisition function
  to handle decoupled constraints by computing the expected improvement of evaluating
  each constraint individually and supplementing it with coupled cKG to avoid sampling
  bias.
---

# Bayesian Optimisation: Which Constraints Matter?

## Quick Facts
- arXiv ID: 2512.17569
- Source URL: https://arxiv.org/abs/2512.17569
- Reference count: 14
- The paper proposes decoupled Bayesian optimisation methods that selectively evaluate objective and constraint functions independently, outperforming coupled approaches on opportunity cost across various benchmark problems.

## Executive Summary
This paper addresses Bayesian optimisation for expensive black-box problems with decoupled constraints, where subsets of objective and constraint functions can be evaluated independently. The core contribution extends constrained Knowledge Gradient (cKG) to handle decoupled constraints by computing expected improvement from evaluating each constraint individually, supplemented with coupled cKG to avoid sampling bias. An alternative, computationally faster approach (cEI+) uses constrained Expected Improvement for location selection and dcKG for function selection. The methods demonstrate superior opportunity cost performance across benchmark problems, particularly when dealing with redundant or expensive constraints.

## Method Summary
The method extends constrained Knowledge Gradient (cKG) to decoupled constraints by computing expected improvement from evaluating each constraint individually (dcKG_k for k ∈ {0,...,K}), selecting the constraint that maximizes this value per unit cost. A coupled cKG fallback prevents sampling collapse into well-explored regions by proposing joint evaluations when they yield higher expected improvement per total cost. An alternative approach (cEI+) uses constrained Expected Improvement to select the evaluation location and dcKG to decide which function to evaluate. Both methods use separate GP models for objective and each constraint, trained on decoupled data, with initial 6 Latin Hypercube samples evaluating all functions.

## Key Results
- dcKG and cEI+ outperform existing approaches (cKG, cEI, PESC) in opportunity cost across benchmark problems
- Performance gap is largest when objective or active constraints are more expensive to evaluate
- dcKG effectively stops evaluating redundant constraints after initial sampling, freeing budget for informative evaluations
- cKG fallback mechanism reduces variance in performance and prevents sampling collapse into known feasible regions

## Why This Works (Mechanism)

### Mechanism 1
Selective constraint evaluation improves sample efficiency by focusing computational budget on binding constraints. The dcKG acquisition computes expected improvement from evaluating each constraint individually, then selects the constraint that maximizes this value per unit cost. This creates a feedback loop where GP models learn which constraints are inactive at promising regions and receive fewer evaluations. Core assumption: only a subset of constraints are binding near the optimum.

### Mechanism 2
Including coupled cKG as a fallback prevents sampling collapse into well-explored regions. Purely decoupled optimizers suffer from "chicken and egg" pathology—they prefer known feasible regions because evaluating objective alone won't improve feasibility estimates, and evaluating constraints alone won't improve objective estimates. Adding cKG as a competing acquisition allows the algorithm to escape by proposing coupled evaluations when they yield higher expected improvement per total cost.

### Mechanism 3
Cost-weighting the acquisition function naturally prioritizes cheaper evaluations, improving budget efficiency under heterogeneous costs. Each dcKG_k value is divided by the evaluation cost b_k, transforming the acquisition from "maximum expected improvement" to "maximum expected improvement per unit cost." This implicitly solves a resource allocation problem across functions with different costs.

## Foundational Learning

- **Gaussian Process Regression**: GP provides surrogate models for both objective and constraints, enabling mean predictions and uncertainty quantification. The dcKG acquisition requires GP posteriors to compute expected improvements analytically. Quick check: Can you explain why the Matérn 5/2 kernel might be preferred over RBF for modeling physical simulations?

- **Expected Improvement and Knowledge Gradient**: EI selects points maximizing expected improvement over current best. KG extends this by considering expected improvement in the posterior recommendation after updating the model—critical for constrained problems where feasibility matters. Quick check: What is the fundamental difference between EI (myopic) and KG (non-myopic) in terms of what they optimize?

- **Probability of Feasibility**: For constrained BO, each candidate has probability of satisfying all constraints, computed as product of individual constraint feasibility probabilities. This weights the expected recommendation quality. Quick check: If PF(x) = Π PF_k(x), what happens to overall feasibility probability as number of constraints grows?

## Architecture Onboarding

- **Component map**: GP Models (K+1 independent GPs) -> dcKG Acquisition (K+1 values) -> cKG Fallback (1 value) -> Recommendation Rule (argmax μ_f(x) · PF(x))

- **Critical path**: 1) Initialize with 6 Latin Hypercube samples (all functions evaluated); 2) Fit K+1 independent GP models; 3) Compute K+1 dcKG values + 1 cKG value; 4) Select max; if cKG wins, evaluate all constraints with PF_k < 1-δ plus objective; else evaluate single function at its optimal location; 5) Update relevant GP(s); repeat until budget exhausted; 6) Return argmax_x μ_f(x) · PF(x)

- **Design tradeoffs**: Fantasy sampling uses 7 quantile fantasies for dcKG, 35 (7×5) for cKG—more fantasies improve accuracy but increase computation. Threshold δ=10⁻⁷ determines when to skip constraint evaluation in coupled mode. cEI+ is faster (compute dcKG at only cEI-selected location) but theoretically less principled.

- **Failure signatures**: Slow convergence on problems where all constraints are active—decoupled methods may not outperform coupled methods. High variance in results—check if cKG fallback is being triggered. Budget exhausted on objective-only evaluations—may indicate constraints are already well-learned or constraint models are overconfident.

- **First 3 experiments**: 1) Reproduce Mystery function baseline (1 constraint, equal costs) with 50 random seeds to verify median OC convergence; 2) Ablate cKG fallback on Test Function 2 to confirm variance reduction; 3) Test heterogeneous cost scenario where redundant constraint is 10× more expensive to verify dcKG allocates <5% budget to it after initial samples.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the dcKG framework be adapted for partially coupled settings where only specific subsets of constraints or objective functions can be evaluated independently? Basis: Section 5 states it should be straightforward to adapt algorithms to cases where certain subsets are coupled. Unresolved because current implementation assumes binary distinction between fully coupled and fully decoupled.

- **Open Question 2**: Can the dcKG acquisition function be effectively extended to handle multi-objective optimization problems? Basis: Section 5 lists extending method to solve multi-objective optimization problems as future work. Unresolved because current formulation relies on scalar opportunity cost that doesn't translate directly to Pareto fronts.

- **Open Question 3**: How does observation noise in constraint and objective evaluations affect the performance and theoretical guarantees of the proposed dcKG method? Basis: Section 5 includes "noisy optimization problems" in relevant extensions for future research. Unresolved because current method uses deterministic fantasy sampling assuming noiseless observations.

## Limitations
- The cKG fallback mechanism's effectiveness depends heavily on the δ threshold (1e-7), which is stated but not rigorously validated across problem types.
- Computational overhead comparison between dcKG and cEI+ is incomplete - while cEI+ is described as faster, no runtime benchmarks are provided.
- The quantile-based fantasy sampling scheme is referenced but not fully specified, relying on external literature for implementation details.

## Confidence

- **High confidence**: The decoupled evaluation mechanism (dcKG) demonstrably reduces opportunity cost when constraints vary in binding importance (Fig. 5, 6, 7, 8)
- **Medium confidence**: The cKG fallback prevents sampling collapse (Fig. 9 shows variance reduction, but mechanism's necessity across all problem types is not fully established)
- **Medium confidence**: Cost-weighting improves efficiency under heterogeneous costs (Fig. 6-8 show improvements, but threshold where costs must differ significantly is not quantified)

## Next Checks
1. **Ablation study**: Run dcKG with cKG fallback disabled across all test functions to quantify variance increase and identify problem types where fallback is essential.
2. **Threshold sensitivity**: Sweep δ from 1e-5 to 1e-10 on Test Function 2 to determine optimal range and identify when fallback mechanism becomes detrimental.
3. **Runtime profiling**: Measure acquisition optimization time for dcKG vs cEI+ on 2D problems to quantify computational savings and identify when cEI+ becomes preferable despite theoretical limitations.