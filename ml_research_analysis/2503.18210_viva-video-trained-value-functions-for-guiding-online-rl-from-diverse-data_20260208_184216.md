---
ver: rpa2
title: 'ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data'
arxiv_id: '2503.18210'
source_url: https://arxiv.org/abs/2503.18210
tags:
- uni00000087
- data
- uni00000021
- value
- uni0000000f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of online reinforcement learning
  with sparse rewards by proposing a method to leverage widely available video data
  for guidance. The core idea is to learn a goal-conditioned value function from diverse
  video sources, including internet recordings and environment interactions, to estimate
  temporal distances between states.
---

# ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data

## Quick Facts
- arXiv ID: 2503.18210
- Source URL: https://arxiv.org/abs/2503.18210
- Authors: Nitish Dashora; Dibya Ghosh; Sergey Levine
- Reference count: 34
- Primary result: Video-trained value functions improve online RL performance, especially in low-data regimes

## Executive Summary
ViVa addresses the challenge of online reinforcement learning with sparse rewards by leveraging widely available video data. The method learns a goal-conditioned value function from diverse video sources to estimate temporal distances between states, which is then incorporated into the reward signal during online RL. ViVa demonstrates generalization to unseen goals, positive transfer from human video pre-training, and improved performance with increased dataset size, particularly in low-data regimes.

## Method Summary
ViVa trains an Intent-Conditioned Value Function (ICVF) to estimate temporal distance between states and goals using expectile regression. The method pre-trains on internet-scale video data (Ego4D), fine-tunes on environment-specific interaction data, and uses the learned values to augment sparse rewards during online RL. The ICVF is trained with a soft optimality assumption that transitions reducing estimated distance to goal approximate optimal behavior. During online RL, the value function is frozen and provides dense guidance through additive reward shaping.

## Key Results
- ViVa outperforms baselines in AntMaze and RoboVerse tasks
- Demonstrates positive transfer from human video pre-training, particularly in low-data regimes
- Shows improved performance with increased dataset size
- Achieves 2x performance increase in low-data regime with Internet-scale video pre-training

## Why This Works (Mechanism)

### Mechanism 1
Video-trained value functions provide dense guidance for sparse-reward online RL without action-labeled data. The ICVF learns V(s,g) that estimates temporal distance between any state s and goal g by applying expectile regression with an advantage heuristic. This relaxes the max operator in standard TD learning by assuming transitions with positive advantage are acting "optimally" toward the goal. The core assumption is that transitions in video data that reduce estimated distance to goal approximate optimal behavior.

### Mechanism 2
Pre-training on internet-scale human video improves low-data regime performance via transfer of goal-reaching priors. Ego4D pre-training develops visual features and manipulation priors that transfer to robotics domains. A -1 reward shift makes the learned values correspond to negative temporal distance, directly usable as reward penalty. The core assumption is that visual and causal structure in egocentric human video transfers to robotic manipulation domains.

### Mechanism 3
Environment-specific fine-tuning is necessary to specialize the value function; pre-training alone is insufficient. Fine-tuning on task-agnostic environment data brings the value function into the target domain by developing setting-specific features. The frozen value function then augments online RL reward. The core assumption is that some environment-specific interaction data is available, even if it contains only failures or undirected exploration.

## Foundational Learning

- **Goal-Conditioned Value Functions**
  - Why needed here: ViVa relies on V(s,g) representing temporal distance to goal; understanding how goal-conditioning works is prerequisite.
  - Quick check question: Can you explain why V(s,g) ≈ ICVF(s,g,g) estimates the time to reach g from s?

- **Temporal-Difference Learning with Expectile Regression**
  - Why needed here: The training objective uses expectile-weighted TD error rather than standard TD; understanding IQL-style learning is essential.
  - Quick check question: Why does expectile α=0.9 bias updates toward "advantageous" transitions?

- **Reward Shaping Theory**
  - Why needed here: The method augments sparse rewards with learned values; understanding policy invariance under shaping is relevant.
  - Quick check question: Why might the authors choose additive value penalty over potential-based shaping despite theoretical guarantees?

## Architecture Onboarding

- **Component map:** ResNet-v2 encoder (26-layer) -> encodes (s, s+, g) images -> concatenated latents -> Ensemble of 2 MLPs (256x256, LayerNorm) -> value estimate -> Online RL: SAC/DrQ with 50/50 batch mixing

- **Critical path:**
  1. Pre-train ICVF on Ego4D (3000h, 128×128, 1.5 days on v4 TPU)
  2. Fine-tune on D_env using same objective
  3. Freeze ICVF, run online RL with augmented reward

- **Design tradeoffs:**
  - Monolithic ICVF vs. multilinear factorization: Authors found monolithic produces higher-quality values
  - Potential-based shaping vs. direct value addition: Potential-based showed higher variance in returns
  - Expectile 0.9: Balances soft assumption vs. convergence guarantees

- **Failure signatures:**
  - Zero-shot generalization fails when domain gap is large
  - Value errors can force agent into states erroneously estimated as near-goal
  - ICVF training collapse with multilinear formulation

- **First 3 experiments:**
  1. Sanity check (state-based): Train ICVF on corrupted AntMaze, verify value function generalizes to unseen goal
  2. Ablation study: Compare ViVa with vs. without Ego4D pre-training across varying D_env sizes
  3. Data scaling test: Train ViVa with increasing on-task data amounts on RoboVerse pick-and-place

## Open Questions the Paper Calls Out

- Can the intent-conditioned value function be modified to encode explicit abstractions that enable successful zero-shot transfer to a target domain without any environment-specific finetuning data?
- Can the bi-level training pipeline be simplified by integrating the finetuning process directly into the online RL phase using an exploration algorithm?
- How can ViVa robustly handle value estimation errors to prevent the agent from getting stuck in states erroneously predicted to be near the goal?

## Limitations
- The method fails when video data contains no trajectories that meaningfully reduce distance to the target goal
- Large domain gap between pre-training video and target environment remains a significant challenge
- Value estimation errors can mislead the agent into suboptimal states

## Confidence
- **High Confidence**: ICVF learns meaningful temporal distance estimates; pre-training + fine-tuning architecture works; method scales with data
- **Medium Confidence**: Soft optimality assumption holds across domains; additive value penalty outperforms potential-based shaping
- **Low Confidence**: Zero-shot generalization is feasible; expectile 0.9 is optimal

## Next Checks
1. Cross-dataset generalization test: Pre-train ICVF on RoboVerse human demos and evaluate zero-shot transfer to AntMaze
2. Value error sensitivity analysis: Systematically vary value function noise levels during online RL
3. Shaping method comparison: Implement and compare potential-based shaping with additive penalty across multiple domains