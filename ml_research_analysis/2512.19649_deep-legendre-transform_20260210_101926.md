---
ver: rpa2
title: Deep Legendre Transform
arxiv_id: '2512.19649'
source_url: https://arxiv.org/abs/2512.19649
tags:
- convex
- training
- functions
- legendre
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep Legendre Transform (DLT), a deep learning
  method for computing convex conjugates of differentiable convex functions. Traditional
  numerical methods suffer from the curse of dimensionality and become computationally
  intractable in high dimensions, while neural network-based approaches scale better
  but require solving complicated optimization problems.
---

# Deep Legendre Transform

## Quick Facts
- **arXiv ID:** 2512.19649
- **Source URL:** https://arxiv.org/abs/2512.19649
- **Reference count:** 40
- **Key outcome:** Introduces Deep Legendre Transform (DLT) for computing convex conjugates of differentiable convex functions using implicit Fenchel formulation and gradient-based minimization

## Executive Summary
This paper introduces Deep Legendre Transform (DLT), a deep learning method for computing convex conjugates of differentiable convex functions. Traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, while neural network-based approaches scale better but require solving complicated optimization problems. DLT uses an implicit Fenchel formulation of convex conjugation, allowing efficient gradient-based minimization of approximation errors.

DLT approximates f*(∇f(x)) ≈ ⟨x,∇f(x)⟩ − f(x) for x∈C by minimizing empirical squared error over a training set Xtrain ⊆C. This formulation provides several advantages: exact agreement with true values at training points, scalability to high dimensions with neural networks, guaranteed convexity when using input convex neural networks (ICNNs), and a posteriori error estimates.

## Method Summary
The Deep Legendre Transform method uses an implicit Fenchel formulation to compute convex conjugates efficiently. The approach minimizes the empirical squared error between the approximated convex conjugate values and the target values at training points. By leveraging input convex neural networks and gradient-based optimization, DLT achieves scalable computation in high dimensions while maintaining theoretical guarantees of convexity. The method provides exact agreement with true values at training points and includes a posteriori error estimation capabilities.

## Key Results
- DLT achieves equivalent L2 errors to direct learning methods without requiring closed-form Legendre transforms
- Outperforms classical grid-based approaches like Lucet's Linear Time Legendre Transform in high dimensions
- Enables applications to Hamilton-Jacobi equations through the Hopf formula with superior accuracy compared to Deep Galerkin Methods
- Provides unbiased a posteriori error estimators for evaluating numerical methods

## Why This Works (Mechanism)
DLT works by reformulating the convex conjugation problem using the implicit Fenchel formulation. Instead of directly approximating the convex conjugate, it minimizes the error in the relationship f*(∇f(x)) ≈ ⟨x,∇f(x)⟩ − f(x). This approach allows gradient-based optimization to efficiently find the optimal approximation. The use of input convex neural networks ensures the output remains convex, while the implicit formulation provides exact agreement at training points and enables a posteriori error estimation.

## Foundational Learning

**Convex Analysis**: Understanding convex functions and their conjugates is fundamental to this method. Why needed: The entire approach relies on convex conjugation properties. Quick check: Can you verify that f** = f for proper closed convex functions?

**Input Convex Neural Networks (ICNNs)**: These architectures guarantee convexity with respect to inputs. Why needed: Ensures the output remains convex, which is essential for valid convex conjugation. Quick check: Verify that the network output is convex in the input variables.

**Fenchel Duality**: The theoretical foundation connecting primal and dual problems. Why needed: Provides the mathematical framework for the implicit formulation. Quick check: Can you derive the Fenchel-Young inequality from first principles?

## Architecture Onboarding

**Component Map**: Data Generation -> ICNN Training -> Error Minimization -> A Posteriori Estimation
**Critical Path**: Training data generation → ICNN optimization → Convex conjugate approximation → Error estimation
**Design Tradeoffs**: Uses ICNNs for guaranteed convexity vs. flexibility of standard architectures; implicit formulation vs. direct approximation
**Failure Signatures**: Poor performance on non-smooth functions; overfitting to training data; violation of convexity constraints
**First Experiments**: 1) Test on simple convex functions with known conjugates (quadratic, exponential); 2) Verify convexity preservation in trained models; 3) Compare against grid-based methods on low-dimensional examples

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes convexity of target function, limiting applicability to non-convex problems
- Reliance on ICNNs may restrict architectural flexibility and expressiveness
- Performance in extremely high dimensions (d > 100) and with non-smooth convex functions remains unverified
- Limited comparison set against modern operator learning approaches

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical framework connecting Fenchel formulation to efficient gradient-based minimization | High |
| Empirical demonstrations showing strong performance in tested cases | Medium |
| A posteriori error estimation provides useful uncertainty quantification | Medium |

## Next Checks
1. Test DLT on non-smooth convex functions (absolute value, max functions) to evaluate robustness beyond smooth cases
2. Compare performance against modern operator learning approaches in extreme high dimensions (d > 100)
3. Validate the error estimation framework against ground truth in cases where analytical convex conjugates are known