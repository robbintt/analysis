---
ver: rpa2
title: 'Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign
  Language Translation'
arxiv_id: '2601.21128'
source_url: https://arxiv.org/abs/2601.21128
tags:
- paraphrases
- language
- evaluation
- training
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the use of Large Language Models (LLMs)
  to generate paraphrased references for sign language translation (SLT) evaluation.
  It finds that naively using paraphrases in training does not improve translation
  performance and may even be detrimental.
---

# Beyond a Single Reference: Training and Evaluation with Paraphrases in Sign Language Translation

## Quick Facts
- arXiv ID: 2601.21128
- Source URL: https://arxiv.org/abs/2601.21128
- Reference count: 7
- The paper finds that using paraphrased references during evaluation improves automatic metrics and better aligns with human judgments, but naively incorporating paraphrases during training is detrimental.

## Executive Summary
This paper investigates the use of Large Language Models (LLMs) to generate paraphrased references for sign language translation (SLT) evaluation. The study finds that while naive incorporation of paraphrases during training degrades translation performance, using paraphrases during evaluation improves automatic metrics like BLEU and better aligns with human judgments. The authors introduce BLEUpara, an extension of BLEU that evaluates translations against multiple paraphrased references, and release all generated paraphrases along with generation and evaluation code to support reproducible and more reliable evaluation of SLT systems.

## Method Summary
The authors train a pose-based T5 encoder-decoder model on sign language datasets (YouTubeASL and How2Sign) and generate K=5 paraphrased references per sentence using GPT-4o-mini with a specific prompt. They evaluate three training strategies: no paraphrases, random sampling from 6 alternatives, and minimum-loss paraphrase selection. For evaluation, they propose BLEUpara, which computes BLEU scores against the canonical reference plus all paraphrases and selects the maximum score. Paraphrase quality is assessed using ParaScore, a metric combining BERTScore (semantic similarity) and Normalized Levenshtein Distance (lexical divergence).

## Key Results
- BLEUpara shows improved correlation with human judgments (Pearson r=0.697, Spearman ρ=0.685) compared to standard BLEU (r=0.688, ρ=0.657)
- Training with paraphrases (both random and minimum-loss strategies) yields worse BLEU scores than canonical-only training
- GPT-4o-mini achieves the highest average ParaScore (0.87) for paraphrase generation, with quality threshold set at 0.7
- Sequential paraphrase prompting outperforms iterative prompting and is faster to compute

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-reference evaluation with paraphrases improves correlation with human judgment of SLT quality.
- Mechanism: Sign language translations admit multiple valid written realizations due to non-isomorphic mapping; by expanding from one canonical reference to K paraphrased alternatives, BLEUpara selects the maximum n-gram match across references, capturing synonymous phrasings and word-order variations that single-reference BLEU penalizes unfairly.
- Core assumption: LLM-generated paraphrases preserve semantic meaning while introducing lexically diverse surface forms that represent plausible human translations.
- Evidence anchors:
  - [abstract] "using paraphrases during evaluation leads to higher automatic scores and better alignment with human judgments"
  - [Section 6, Table 2] BLEUpara shows Pearson r=0.697 vs BLEU's 0.688; Spearman ρ=0.685 vs 0.657; extreme-case ρ=0.659 vs 0.578
  - [corpus] Related paper "A Critical Study of Automatic Evaluation in Sign Language Translation" confirms text-based metrics have unclear reliability for SLT, supporting the need for improved evaluation approaches
- Break condition: Paraphrases that drift semantically or are too lexically similar to the original would degrade rather than improve evaluation; ParaScore threshold <0.7 may indicate poor paraphrase quality.

### Mechanism 2
- Claim: ParaScore effectively ranks paraphrase quality by balancing semantic preservation against lexical diversity.
- Mechanism: The metric combines BERTScore (contextual embedding similarity) with Normalized Levenshtein Distance (character-level edit divergence), weighted by hyperparameters γ=0.35 and ω=0.5. This prevents near-copies from scoring highly while penalizing semantic drift.
- Core assumption: High-quality paraphrases exist in a "sweet spot" where meaning is preserved but surface forms differ substantially enough to add evaluation value.
- Evidence anchors:
  - [Section 3.2] "A successful paraphrase must balance two critical criteria: semantic similarity and lexical divergence"
  - [Section 4] Manual evaluation established quality threshold of 0.7; GPT-4o-mini achieved highest average ParaScore
  - [corpus] Weak corpus evidence—no direct comparison of paraphrase evaluation metrics found in neighbors
- Break condition: If the semantic similarity and lexical divergence components are not properly balanced for SLT domain language, the ranking may not reflect true paraphrase utility.

### Mechanism 3
- Claim: Naive paraphrase incorporation during training degrades SLT model performance.
- Mechanism: Multiple paraphrased targets per source introduce optimization ambiguity—the model receives conflicting supervision signals for identical pose inputs, potentially preventing convergence to a coherent output distribution. Both "random sampling" and "minimum loss" strategies underperform canonical-only training.
- Core assumption: The detrimental effect stems from ambiguous supervision rather than paraphrase quality itself.
- Evidence anchors:
  - [abstract] "naively incorporating paraphrases during training does not improve translation performance and can even be detrimental"
  - [Table 1] No-paraphrase training: BLEU-4=7.46; Random paraphrase training: BLEU-4=6.17; Min-loss paraphrase training: BLEU-4=6.49
  - [Section 5.2] "exposure to multiple paraphrased targets during training may introduce ambiguity that negatively impacts the performance"
- Break condition: More sophisticated training protocols (curriculum learning, hard example mining, or latent variable models) might overcome this limitation—current experiments only test naive approaches.

## Foundational Learning

- Concept: **BLEU and n-gram matching metrics**
  - Why needed here: BLEUpara extends BLEU by computing maximum n-gram overlap across multiple paraphrased references; understanding BLEU's precision-based scoring is essential to grasp why paraphrases help evaluation.
  - Quick check question: Given a hypothesis "The cat sat" and references ["The cat sat down", "A cat was sitting"], which reference yields higher BLEU-4 and why?

- Concept: **Pose-based sign language representation**
  - Why needed here: The T5 model processes extracted keypoints (not raw video); understanding how MediaPipe/MMPose extract normalized pose features explains the input modality and preprocessing requirements.
  - Quick check question: What preprocessing steps convert raw sign language video into T5-compatible input sequences?

- Concept: **Sequence-to-sequence with T5**
  - Why needed here: The architecture uses a modified T5 encoder-decoder for pose-to-text translation; understanding transformer-based seq2seq training (teacher forcing, cross-attention) is prerequisite to analyzing training dynamics.
  - Quick check question: How does T5's text-to-text framework accommodate continuous pose embeddings rather than discrete tokens?

## Architecture Onboarding

- Component map: Raw video -> Keypoint extraction (MediaPipe/MMPose) -> SignSpace normalization -> Missing keypoint imputation (-10) -> T5 encoder-decoder -> Text output -> BLEUpara evaluation

- Critical path: Keypoint extraction quality -> pose normalization -> model pretraining convergence -> paraphrase generation quality -> BLEUpara evaluation correlation with human judgment

- Design tradeoffs:
  - Sequential vs. iterative paraphrase prompting: Sequential is faster with slightly higher ParaScore (Figure 3)
  - Context-augmented paraphrasing: Adding video context degrades ParaScore (Figure 2)—avoid unless future work proves otherwise
  - Training with paraphrases: Currently detrimental—do not use for training until improved protocols are validated

- Failure signatures:
  - BLEUpara scores inconsistent across runs: Check paraphrase generation determinism (temperature=0.7 introduces stochasticity)
  - Training loss plateauing early with paraphrases: Ambiguous supervision detected—revert to canonical-only targets
  - ParaScore distribution shifted left: LLM model or prompt mismatch—verify GPT-4o-mini and exact prompt formatting

- First 3 experiments:
  1. **Reproduce BLEUpara correlation**: Generate GPT-4o-mini paraphrases for How2Sign test set, compute BLEU vs BLEUpara on existing model outputs, verify Pearson/Spearman improvements match Table 2
  2. **Ablate paraphrase count K**: Test BLEUpara with K∈{1,2,3,5,10} to measure marginal correlation gains vs computational cost
  3. **Cross-dataset generalization**: Apply YouTubeASL-trained model to How2Sign test set with How2Sign paraphrases to verify evaluation improvement transfers across domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can discourse-level context improve paraphrase quality for sign language translation when using more sophisticated prompting strategies?
- Basis in paper: [explicit] Authors state that adding video-level context worsened ParaScore and note "This is something we are planning to explore further in our future work."
- Why unresolved: The naive approach of simply prepending preceding sentences degraded performance, but this does not rule out more nuanced context integration methods.
- What evidence would resolve it: Experiments with structured context representations, attention-based context selection, or discourse-aware prompting showing improved ParaScore compared to sentence-only paraphrasing.

### Open Question 2
- Question: Does BLEUpara generalize to sign languages beyond American Sign Language?
- Basis in paper: [explicit] The conclusion states: "we would like to test if the proposed metric BLEUpara generalizes well also for other sign languages other than American Sign Language."
- Why unresolved: The metric was validated only on ASL datasets (YouTubeASL, How2Sign); cross-linguistic validity is unknown given structural differences across sign languages.
- What evidence would resolve it: Human evaluation experiments on datasets like DGS (German Sign Language) or BSL (British Sign Language) showing comparable or improved correlation with human judgments versus standard BLEU.

### Open Question 3
- Question: Can selective training strategies such as hard example mining effectively leverage paraphrases for SLT model improvement?
- Basis in paper: [explicit] The discussion notes that naive augmentation failed and suggests "More selective strategies, such as hard example mining, may therefore be a more promising direction for future research."
- Why unresolved: Random sampling and minimum-loss selection both underperformed the baseline, but curriculum-based or difficulty-aware paraphrase selection remains unexplored.
- What evidence would resolve it: Training experiments where paraphrases are filtered or weighted by quality metrics, semantic divergence, or model uncertainty, demonstrating BLEU/BLEURT improvements over non-paraphrased training.

### Open Question 4
- Question: Why does adding contextual information degrade paraphrase quality, and does this reflect fundamental limitations or prompt design issues?
- Basis in paper: [inferred] Figure 2 shows context shifts the ParaScore distribution left with higher variance, but the mechanism is unexplained. This matters because sign languages are highly context-dependent.
- Why unresolved: The paper observes the degradation but does not analyze whether it stems from LLM attention dilution, noise from misaligned context, or ParaScore's sensitivity to longer inputs.
- What evidence would resolve it: Ablation studies varying context length, source, and relevance; alternative semantic preservation metrics; qualitative analysis of semantic drift in context-conditioned paraphrases.

## Limitations
- Training with paraphrases is shown to be detrimental, but only naive approaches were tested. More sophisticated training protocols might yield different results.
- ParaScore combines semantic and lexical metrics, but the weighting parameters (γ=0.35, ω=0.5) were not extensively validated across different domains or LLM models.
- The study relies on proprietary LLM outputs (GPT-4o-mini) which may not be reproducible with other models or in different licensing contexts.

## Confidence
- High confidence: BLEUpara improves correlation with human judgment over standard BLEU; paraphrases preserve semantic meaning while providing lexical diversity; training with paraphrases is currently detrimental
- Medium confidence: ParaScore effectively ranks paraphrase quality; human evaluation confirms qualitative improvements; proposed evaluation framework is more reliable
- Low confidence: Claims about why paraphrase-based training fails; generalizability of ParaScore parameters; reproducibility with non-proprietary models

## Next Checks
1. Test alternative training strategies with paraphrases (curriculum learning, hard example mining, or latent variable models) to determine if sophisticated approaches can overcome the ambiguity issue observed with naive methods.
2. Conduct ablation studies on ParaScore parameters (γ, ω) across different LLM models and domains to validate the robustness of the semantic-lexical balance.
3. Verify cross-dataset generalization by applying YouTubeASL-trained models to How2Sign test sets with How2Sign paraphrases, and vice versa, to test domain transfer of the evaluation improvements.