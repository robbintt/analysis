---
ver: rpa2
title: 'Deconstructing Generative Diversity: An Information Bottleneck Analysis of
  Discrete Latent Generative Models'
arxiv_id: '2512.01831'
source_url: https://arxiv.org/abs/2512.01831
tags:
- diversity
- generative
- codebook
- latent
- subset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an information-theoretic framework for analyzing
  generative diversity in discrete latent models, based on the Information Bottleneck
  principle. It decomposes diversity into path diversity (Hpath) and execution diversity
  (Hexec) and uses zero-shot inference-time interventions to probe each component.
---

# Deconstructing Generative Diversity: An Information Bottleneck Analysis of Discrete Latent Generative Models

## Quick Facts
- arXiv ID: 2512.01831
- Source URL: https://arxiv.org/abs/2512.01831
- Reference count: 40
- Primary result: Introduces Information Bottleneck-based framework decomposing generative diversity into path diversity (Hpath) and execution diversity (Hexec) via zero-shot inference interventions

## Executive Summary
This paper presents an information-theoretic framework for analyzing generative diversity in discrete latent generative models, decomposing diversity into path diversity (Hpath) and execution diversity (Hexec) based on the Information Bottleneck principle. The authors introduce three inference-time interventions - Codebook Subset, Argmax, and Paraphrase - to probe these components across AR, MIM, and Diffusion models. The analysis reveals three archetypal strategies: MIM models prioritize diversity through high Hexec, AR models compress information with near-zero Hpath and Hexec, and Diffusion models decouple path and execution stochasticity.

## Method Summary
The framework uses three zero-shot inference probes to decompose diversity: Codebook Subset (restricting to top-k frequent tokens), Argmax (deterministic decoding), and Paraphrase (semantic variations). Diversity is measured using LPIPS, pixel cosine distance, SSIM, FID, and CLIP scores computed as pairwise metrics across generated samples. The decomposition follows Information Bottleneck bounds where Hpath represents information retained through the latent bottleneck and Hexec represents execution-level stochasticity. The analysis is validated on DALL-E 3 synthetic captions, MSCOCO2014, and multiple model architectures including LlamaGen (AR), aMUSEd (MIM), VQ-Diffusion, DeepSeek Janus Pro 1B, and Show-O.

## Key Results
- MIM models exhibit large diversity drops under Argmax and Subset probes, confirming high Hexec dominance
- LlamaGen shows near-zero diversity under Argmax, demonstrating Hexec dependence
- VQ-Diffusion maintains stable diversity across interventions, reflecting its decoupled design
- Proposed diversity enhancement method (mixed paraphrases + disabling high-frequency tokens) increases LPIPS by measurable margins on DeepSeek and Show-O models

## Why This Works (Mechanism)
The framework works by exploiting the Information Bottleneck principle to separate two sources of diversity: the information retained through the latent representation (path diversity) and the stochasticity in execution (execution diversity). By systematically removing each source through targeted interventions, the authors can quantify their individual contributions. The Codebook Subset intervention removes path diversity by limiting the available latent codes, Argmax removes execution diversity by forcing deterministic generation, and Paraphrase probes semantic robustness. This separation reveals fundamental architectural differences between generative paradigms.

## Foundational Learning

**Information Bottleneck Theory**: Framework for quantifying information compression and relevance in latent variable models. Why needed: Provides theoretical foundation for decomposing diversity into path and execution components. Quick check: Verify that the IB bounds used (I(X;T) ≥ I(X;Y)) hold for the discrete latent setting.

**VQ-VAE Codebook Structure**: Discrete latent space represented by finite set of embedding vectors. Why needed: Core mechanism for both representation and generation in all three model types. Quick check: Confirm codebook size and usage statistics across models to understand capacity constraints.

**Diffusion Sampling Dynamics**: Iterative denoising process with temporal stochasticity schedule. Why needed: Explains how VQ-Diffusion achieves decoupled path/execution diversity. Quick check: Verify that noise schedule parameters match those in the analysis.

## Architecture Onboarding

**Component Map**: Input Text → Encoder → Discrete Latent Codes → Decoder → Output Image, with stochastic sampling at various stages depending on model type

**Critical Path**: For diversity analysis, the critical path is: Text → Encoder → (Latent Bottleneck) → Decoder → Image, where interventions target the bottleneck and sampling stages

**Design Tradeoffs**: AR models compress information for coherence (low Hpath/Hexec), MIM models maintain execution diversity (high Hexec/low Hpath), Diffusion models separate concerns (moderate Hpath/moderate Hexec)

**Failure Signatures**: Diversity collapse under Argmax indicates Hexec dependence; minimal change under Subset suggests high Hpath; VQ-Diffusion stability indicates successful decoupling

**First Experiments**:
1. Generate samples with and without codebook subsetting to verify direction of diversity change
2. Compare Argmax diversity drops at early vs late stages for MIM models
3. Test paraphrase robustness across all model types to establish baseline semantic stability

## Open Questions the Paper Calls Out
None

## Limitations
- Information Bottleneck decomposition relies on theoretical bounds that may not reflect actual information flow in trained models
- The orthogonality assumption between Hpath and Hexec is not empirically validated across all model types
- Diversity enhancement method lacks ablation studies to isolate the contribution of paraphrase mixing vs token disabling

## Confidence

High: MIM archetype characterization (clear empirical support for Hexec dominance)
Medium: AR archetype characterization (based on single model LlamaGen with potential architectural peculiarities)
Medium: Diffusion claims (qualitatively supported but quantitative separation between path and execution effects not fully established)

## Next Checks

1. Verify codebook subset construction by running controlled experiments removing either most vs least frequent tokens and measuring the opposite effect on diversity
2. Perform ablation on the diversity enhancement method by testing paraphrase-only vs token-disabling-only conditions on the same models
3. Cross-validate the Information Bottleneck decomposition by training models with explicit entropy regularization on Hpath vs Hexec components and comparing to intervention-based estimates