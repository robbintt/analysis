---
ver: rpa2
title: 'Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning'
arxiv_id: '2507.22887'
source_url: https://arxiv.org/abs/2507.22887
tags:
- qwen
- prompt
- llama3
- position
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uncovers a previously unexplored positional bias in
  in-context learning (ICL) called DPP bias, where the placement of demonstrations
  within a prompt significantly impacts LLM performance. The authors systematically
  evaluate four canonical demo positions across eight tasks and ten LLMs, introducing
  accuracy-change and prediction-change metrics to quantify performance shifts.
---

# Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning

## Quick Facts
- arXiv ID: 2507.22887
- Source URL: https://arxiv.org/abs/2507.22887
- Reference count: 40
- Key outcome: Placement of demonstrations within prompts significantly impacts LLM performance, with early positions (ssp/esp) yielding more stable and accurate outputs than later positions (eum), and no single position optimal across all models and tasks.

## Executive Summary
This paper uncovers a previously unexplored positional bias in in-context learning (ICL) called DPP bias, where the placement of demonstrations within a prompt significantly impacts LLM performance. The authors systematically evaluate four canonical demo positions across eight tasks and ten LLMs, introducing accuracy-change and prediction-change metrics to quantify performance shifts. Results show that placing demos at the start of the prompt (ssp/esp) yields more stable and accurate outputs, while end-of-message placement (eum) can flip over 30% of predictions without improving correctness. Smaller models are most sensitive to this bias, though even large models remain affected on complex tasks. No single position is universally optimal, highlighting the need for model- and task-specific prompt tuning.

## Method Summary
The study evaluates four canonical positions for demonstration placement: ssp (start of system prompt), esp (end of system prompt), sum (start of user message), and eum (end of user message). Using eight benchmarks across classification, QA, and summarization tasks, the authors sample 200 test examples per task and 5 demonstrations from train splits. For each model and position, they compute accuracy-change (∆metric = Metricposition − Metriczero-shot) and prediction-change (∆pred = #answer flips / #Q) metrics. Experiments run inference-only at temperature=0 with 4-bit quantized models via vLLM, serving results through paired statistical tests (Wilcoxon signed-rank with FDR correction).

## Key Results
- Placing demos at the start of the prompt (ssp/esp) yields more stable and accurate outputs with gains up to +6 points
- Eum placement flips over 30% of predictions without improving correctness on QA tasks
- Smaller models show the strongest positional sensitivity, though even large models remain affected on complex tasks
- No single demo position is universally optimal; best placement depends on both model scale and task type

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Earlier demonstration positions (ssp, esp) yield more stable and accurate ICL outputs than later positions (sum, eum).
- Mechanism: Causal-decoder LLMs process tokens autoregressively; early tokens disproportionately influence subsequent hidden states and predictions through attention mechanisms, particularly via "induction heads" that concentrate on early context.
- Core assumption: Architectural primacy bias and attention patterns drive positional sensitivity more than content quality.
- Evidence anchors:
  - [abstract] "placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points"
  - [section 5.1] "earlier tokens exert disproportionate influence on the hidden state that conditions all subsequent predictions... attention weights concentrate on early and sink tokens"
  - [corpus] Related work on positional encoding benefits in transformers supports the role of position in ICL (Vocabulary In-Context Learning paper, FMR=0.60)
- Break condition: If model architecture shifts to bidirectional attention or position-invariant training, this mechanism may weaken.

### Mechanism 2
- Claim: Position effects are modulated by model scale and task type, with no universally optimal placement.
- Mechanism: Larger models exhibit greater context integration capacity, reducing positional fragility; task-specific inductive biases (e.g., arithmetic reasoning) may favor different positions regardless of scale.
- Core assumption: Model capacity and task structure interact to determine optimal demo placement.
- Evidence anchors:
  - [abstract] "No single demo position is universally optimal; the best placement depends on both model scale and task type"
  - [section 4.2] "larger models generally exhibit reduced prediction volatility... but the degree of robustness is task-dependent and not uniformly monotonic with size"
  - [section 4.4] "LLAMA3 70B shows a consistent preference for placing demonstrations at the sum position... suggesting larger models may benefit from having demonstrations placed in closer proximity to the query"
  - [corpus] Weak direct evidence; related papers focus on ICL improvement methods, not positional variance across scales
- Break condition: If models are trained with explicit position-randomized ICL data, scale-dependent patterns may shift.

### Mechanism 3
- Claim: Later-positioned demos (especially eum) induce prediction volatility without improving correctness, acting as a confounder rather than learning signal.
- Mechanism: When demos appear after the query, they may interfere with already-formed predictions or introduce competing signals, causing flips without net accuracy gains.
- Core assumption: The timing of demo exposure relative to query processing determines whether demos are integrated as guidance or noise.
- Evidence anchors:
  - [abstract] "placing demos at the end of the user message flips over 30% of predictions without improving correctness on QA tasks"
  - [section 3.3] "Prediction Change ∆pred measures the volatility of individual model outputs induced by demonstration placement"
  - [section 4.3] "later-positioned demos (eum) cause significantly more answer flips than earlier positions... placing demonstrations after the query can inject instability"
  - [corpus] No direct corpus evidence for this specific mechanism
- Break condition: If models are trained to condition on context holistically regardless of position, this volatility may reduce.

## Foundational Learning
- Concept: In-Context Learning (ICL)
  - Why needed here: The entire paper investigates how positional placement of demos affects ICL performance; understanding ICL fundamentals is prerequisite.
  - Quick check question: Can you explain how ICL differs from fine-tuning and why demo order might matter?
- Concept: Transformer Attention Mechanisms
  - Why needed here: The proposed mechanism relies on how attention distributes across token positions (primacy bias, induction heads).
  - Quick check question: Why might early tokens receive more attention in causal-decoder architectures?
- Concept: Prompt Structure (System vs. User Roles)
  - Why needed here: The four canonical positions depend on understanding chat-style prompt templates with system/user separation.
  - Quick check question: What is the difference between system prompt and user message in instruction-tuned LLMs?

## Architecture Onboarding
- Component map: Input: Task τ, demonstration set Dτ (k demos), query set Qτ → Four DPP positions (ssp, esp, sum, eum) → Metrics: Accuracy-Change (∆metric = Metricposition − Metriczero-shot), Prediction-Change (∆pred = #answer flips / #Q) → Output: Position-specific performance deltas and volatility measures

- Critical path:
  1. Fix demo content and order; vary only position
  2. For each position, compute task-specific metrics (accuracy, F1, ROUGE-L, etc.)
  3. Calculate ∆metric vs. zero-shot baseline and ∆pred vs. default sum position
  4. Conduct statistical tests (Wilcoxon signed-rank with FDR correction)

- Design tradeoffs:
  - Controlled isolation of position vs. real-world prompt complexity
  - Five demos per task balances signal vs. context length limits
  - Temperature=0 ensures reproducibility but may not reflect production settings

- Failure signatures:
  - High ∆pred with low/negative ∆metric → position causes instability without benefit (e.g., eum on QA tasks)
  - Large accuracy variance across positions → model is position-fragile; smaller models show this more
  - Non-monotonic scale behavior → task-specific inductive biases override general robustness trends

- First 3 experiments:
  1. **Baseline replication**: Run ssp, esp, sum, eum on AG News with QWEN-1.5B; verify ∆pred ~45% and accuracy variance matches Figure 1.
  2. **Scale test**: Compare ssp vs. eum on MMLU across three model sizes (e.g., 3B, 8B, 70B); confirm prediction-change declines with scale (Figure 4a pattern).
  3. **Task contrast**: Run all four positions on GSM8K vs. CNN/DailyMail with a single mid-sized model; observe how eum causes near-100% prediction changes in generation but not reasoning tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do architectural constraints (e.g., attention initialization) or training data regularities (e.g., instruction tuning templates) primarily drive the DPP bias?
- Basis in paper: [explicit] The authors call for "deeper interpretability work" to distinguish between causes like "decoder primacy" versus "instruction tuning templates" in the Future Work section.
- Why unresolved: The paper establishes the existence of the bias but only hypothesizes the mechanism without isolating the specific contribution of each factor.
- What evidence would resolve it: Ablation studies intervening on model attention mechanisms or training models on datasets with randomized positional structures to observe changes in bias severity.

### Open Question 2
- Question: Can fine-tuning on randomly permuted demonstration contexts effectively induce position-invariant representations in LLMs?
- Basis in paper: [explicit] Section 5.2 proposes "post-training on randomly permuted contexts" as a specific direction to counteract structural preferences.
- Why unresolved: The paper outlines this mitigation strategy but does not implement or validate its efficacy in the reported experiments.
- What evidence would resolve it: Empirical results showing reduced accuracy-change and prediction-change metrics in models fine-tuned on position-scrambled data compared to standard instruction-tuned baselines.

### Open Question 3
- Question: Does the DPP bias persist or intensify when using Chain-of-Thought (CoT) rationales within demonstrations?
- Basis in paper: [explicit] The authors state that extending the analysis to "few-shot chain-of-thought prompts" is necessary to help generalize the insights.
- Why unresolved: CoT prompts significantly alter the length and internal structure of demonstrations, potentially shifting token importance distributions in untested ways.
- What evidence would resolve it: Evaluations of prediction volatility across DPP configurations on reasoning benchmarks (e.g., GSM8K) specifically using CoT templates.

## Limitations
- The study fixes demonstration content and order across positions, leaving open whether content quality or sequence variability could interact with placement effects
- Temperature=0 setting ensures reproducibility but may underrepresent stochastic behaviors relevant to production deployments
- Inference-only approach means observed positional sensitivities might differ under fine-tuning or reinforcement learning

## Confidence
- **High confidence**: Claims about overall positional variance across tasks and models, supported by consistent statistical patterns and replication across ten LLMs
- **Medium confidence**: Mechanistic explanations linking positional bias to architectural attention patterns
- **Medium confidence**: Scale-dependent robustness patterns, particularly for larger models
- **Low confidence**: The universal claim that no single position is optimal across all configurations

## Next Checks
1. **Cross-content validation**: Repeat the main experiments with randomized demonstration content and order within each task to test whether positional effects persist independent of content quality or sequence patterns
2. **Temperature sweep validation**: Run ssp vs. eum comparisons at temperature=0.7 and temperature=1.5 on two representative tasks to assess whether positional volatility increases under stochastic sampling
3. **Bidirectional architecture validation**: Test the four DPP positions on an encoder-decoder or bidirectional model using equivalent demonstration blocks to determine if positional encoding mechanisms are architecture-specific