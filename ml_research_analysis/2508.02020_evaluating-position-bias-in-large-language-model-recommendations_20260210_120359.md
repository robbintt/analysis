---
ver: rpa2
title: Evaluating Position Bias in Large Language Model Recommendations
arxiv_id: '2508.02020'
source_url: https://arxiv.org/abs/2508.02020
tags:
- bias
- candidate
- position
- llms
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that LLM-based recommenders are highly
  sensitive to the order of candidate items in prompts, exhibiting position bias that
  affects recommendation stability. The authors introduce Ranking via Iterative Selection
  (RISE), a method that mitigates this bias by iteratively selecting and ranking items
  one at a time rather than all at once.
---

# Evaluating Position Bias in Large Language Model Recommendations

## Quick Facts
- **arXiv ID:** 2508.02020
- **Source URL:** https://arxiv.org/abs/2508.02020
- **Reference count:** 29
- **Key outcome:** LLM-based recommenders are highly sensitive to item order, exhibiting position bias that affects recommendation stability.

## Executive Summary
This paper identifies and addresses position bias in LLM-based recommendation systems, where the order of candidate items in prompts disproportionately influences recommendations. The authors introduce RISE (Ranking via Iterative Selection), which mitigates this bias by decomposing the ranking task into iterative single-item selections rather than ranking all items simultaneously. RISE achieves up to 25% reduction in position bias while maintaining competitive ranking accuracy on MovieLens and Amazon Books datasets.

## Method Summary
The paper evaluates position bias in LLM recommenders by comparing rankings from original vs. reversed candidate lists. RISE addresses this by iteratively selecting one item at a time from the candidate pool until empty, requiring K LLM calls for K items. The method uses stratified sampling to create candidate lists of varying sizes (10, 20, 30), where items are drawn from popularity-based bins. Users with ≥3 ratings in the candidate list serve as ground truth for evaluation. Baselines include standard prompting (rank all at once) and bootstrapping with Borda count aggregation.

## Key Results
- RISE@1 reduces position bias by up to 25% compared to standard prompting and bootstrapping baselines
- RISE consistently improves positional consistency (PC) and output similarity (Sim) while maintaining Recall@5 and NDCG@5
- Position bias effects strengthen with larger candidate lists (K=30 shows stronger bias than K=10)
- Bootstrapping improves output similarity but not positional consistency

## Why This Works (Mechanism)

### Mechanism 1: Position Bias from Sequential Attention
- Claim: LLMs exhibit systematic preference for items based on their position in the candidate list, producing inconsistent rankings when input order changes.
- Mechanism: The autoregressive nature of LLMs creates attention patterns that differentially weight items by position. When asked to rank all candidates simultaneously, the model's attention distribution favors certain positions, causing the same item set to yield different rankings under different orderings.
- Core assumption: Position bias stems from the transformer architecture's sequential processing rather than task-specific learned behavior.
- Evidence anchors:
  - [abstract] "LLM-based recommendation models suffer from position bias, where the order of candidate items in a prompt can disproportionately influence the recommendations produced by LLMs."
  - [Section 3.2] "A high variance across the outputs indicates a strong sensitivity to item order, which reflects the degree of position bias in the model."
  - [corpus] Related work documents similar phenomena: "Lost in the Middle" [Liu et al. 13] and in-context learning order sensitivity [Xu et al. 23].
- Break condition: If candidate list contains only 1-2 items (no comparative positions exist); if the model architecture fundamentally changes to provide uniform positional attention.

### Mechanism 2: RISE's Reduce-and-Conquer Decomposition
- Claim: Iteratively selecting single items from the candidate pool reduces position bias by eliminating comparative positional effects.
- Mechanism: RISE simplifies the decision space at each step—instead of ranking K items simultaneously (where relative positions compete for attention), the model selects one item from K, then one from K-1, etc. This decomposition prevents position-based preferences from accumulating across a single ranking pass. Each selection operates on a smaller, more tractable comparison set.
- Core assumption: Position bias manifests primarily in comparative judgments across multiple items rather than absolute positional preference for single-item selection.
- Evidence anchors:
  - [abstract] "RISE addresses position bias by reducing the original ranking task into smaller, more manageable subtasks, which are then solved in an iterative manner."
  - [Section 3.3] "The core idea of our prompting technique is to reduce the size of the ranking task and solve it iteratively, following a 'reduce-and-conquer' strategy."
  - [corpus] Weak corpus evidence—no direct comparison to other decomposition approaches; related work [Hou et al. 9] uses aggregation via bootstrapping rather than task decomposition.
- Break condition: If single-item selection still shows positional preference; if the iterative process introduces compounding errors that degrade ranking quality.

### Mechanism 3: Selection Depth (N) Inversely Correlates with Bias Reduction
- Claim: Smaller batch selections (N=1) maximize positional consistency; increasing N degrades bias mitigation.
- Mechanism: When the model selects N>1 items per iteration, comparative positional effects re-emerge within each batch. The model must still rank N items against each other, reintroducing the attention competition that drives position bias. N=1 eliminates intra-batch comparison entirely.
- Core assumption: Assumption: The computational cost of K LLM calls (for N=1) is acceptable given the bias reduction benefit.
- Evidence anchors:
  - [abstract] Not explicitly addressed.
  - [Section 4.5] "We see clear degradation in positional consistency and ranking quality as the value of N increases. RISE@1 consistently produces the greatest results compared to other values of N, confirming that more precise step-wise selection is most effective."
  - [corpus] No direct corpus evidence for this specific trade-off.
- Break condition: If latency constraints require N>1; if domain-specific ranking quality degrades significantly with single-item selection.

## Foundational Learning

- **Kendall's Tau Coefficient**
  - Why needed here: The paper uses Kendall's tau as the primary metric for measuring ranking similarity and positional consistency across shuffled inputs.
  - Quick check question: If two rankings have Kendall's tau = 0.75, what proportion of item pairs are ordered identically in both lists?

- **Learning-to-Rank Formulation**
  - Why needed here: The paper frames recommendation as listwise ranking (ranking all candidates jointly) rather than pointwise scoring or pairwise comparison.
  - Quick check question: In listwise ranking, does the model score items independently or must it consider the full candidate set context?

- **Position Bias vs. Popularity Bias**
  - Why needed here: The paper explicitly investigates whether position bias is confounded with popularity bias (items appearing earlier may be more popular).
  - Quick check question: If an LLM preferentially ranks popular items higher, is this position bias or popularity bias?

## Architecture Onboarding

- **Component map:**
  - Candidate Sampling -> Prompt Constructor -> RISE Loop Controller -> Evaluation Layer

- **Critical path:**
  1. Sample candidate list C of size K; identify eligible user with interaction history I
  2. For RISE: Initialize empty ranking list R; repeat K times: (a) prompt LLM to select one item from C given I, (b) append to R, (c) remove from C
  3. For baselines: Single-prompt ranking (Standard) or T=9 shuffled aggregations (Bootstrapping with Borda Count)
  4. Compute metrics: PC via Algorithm 1 (compare original vs. reversed outputs), Sim via pairwise Kendall's tau

- **Design tradeoffs:**
  - Latency vs. Consistency: RISE@1 requires K LLM calls per ranking vs. 1 call for Standard; RISE@N reduces calls by factor of N but degrades PC (Table 3 shows PC drops from 0.72 at N=1 to 0.61 at N=5 for K=20)
  - Candidate list size (K): Standard prompting degrades sharply as K increases (MovieLens PC: 0.67→0.47 from K=10→30); RISE maintains stable PC (0.75→0.69)
  - Aggregation vs. Decomposition: Bootstrapping (T=9 calls with aggregation) improves output similarity but not positional consistency; RISE improves both

- **Failure signatures:**
  - PC < 0.5 with Standard prompting confirms significant position bias
  - High variance (±0.19 std dev in Table 1) in PC across users indicates inconsistent bias patterns
  - RISE showing no improvement suggests position bias may not be the dominant issue in your domain
  - Recall@5 or NDCG@5 dropping >10% with RISE indicates over-correction sacrificing relevance

- **First 3 experiments:**
  1. Baseline measurement: On your target LLM and domain, compute PC by comparing rankings from original vs. reversed candidate lists (n=100 samples, K=20); if PC < 0.6, position bias is significant
  2. RISE vs. Standard comparison: Implement RISE@1; measure both PC and Recall@5 to confirm bias reduction doesn't sacrifice ranking accuracy
  3. Selection depth ablation: Test RISE@1, @3, @5 on held-out set; plot PC vs. latency to find acceptable operating point for your constraints

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- Temperature hyperparameter's impact on bias metrics is acknowledged but not systematically explored
- Single-study validation on MovieLens and Amazon Books limits generalizability to other domains
- Lacks direct comparison to alternative decomposition approaches like pairwise ranking

## Confidence
- **High Confidence:** Position bias exists and affects LLM recommendation stability
- **Medium Confidence:** RISE@1 reduces position bias by up to 25% while maintaining ranking accuracy
- **Low Confidence:** The "reduce-and-conquer" mechanism is the optimal decomposition approach compared to alternatives

## Next Checks
1. Test RISE on additional domains (news recommendation, product recommendations) to verify bias reduction generalizes beyond movies/books
2. Compare RISE against alternative decomposition methods like pairwise ranking or iterative refinement to validate the specific approach
3. Conduct ablation studies varying temperature and top-k sampling parameters to quantify their interaction with position bias effects