---
ver: rpa2
title: 'LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware
  Minimization'
arxiv_id: '2509.03110'
source_url: https://arxiv.org/abs/2509.03110
tags:
- lsam
- learning
- distributed
- sharpness-aware
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LSAM (Landscape-Smoothed Sharpness-Aware
  Minimization), a novel distributed optimizer that addresses the inefficiency of
  Sharpness-Aware Minimization (SAM) in large-batch training scenarios. LSAM combines
  SAM's adversarial robustness with asynchronous distributed sampling, generating
  a smoothed sharpness-aware loss landscape that eliminates synchronization bottlenecks
  and accelerates convergence.
---

# LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization

## Quick Facts
- arXiv ID: 2509.03110
- Source URL: https://arxiv.org/abs/2509.03110
- Authors: Yunfei Teng; Sixin Zhang
- Reference count: 40
- Primary result: Novel distributed optimizer combining SAM with asynchronous sampling parallelism to eliminate synchronization bottlenecks and accelerate convergence in large-batch training

## Executive Summary
This paper introduces LSAM (Landscape-Smoothed Sharpness-Aware Minimization), a distributed optimizer that addresses Sharpness-Aware Minimization's (SAM) inefficiency in large-batch training scenarios. LSAM combines SAM's adversarial robustness with asynchronous distributed sampling, generating a smoothed sharpness-aware loss landscape that eliminates synchronization bottlenecks and accelerates convergence. The method introduces sampling parallelism to overcome SAM's batch-size sensitivity, integrating it with Gaussian smoothing to achieve both wide and deep minima.

Empirically, LSAM demonstrates superior performance across multiple architectures and datasets (SVHN, CIFAR-10, CIFAR-100), consistently achieving lower final test errors and faster convergence compared to data-parallel SAM, LSGD, EASGD, and standard SGD. The approach scales effectively in distributed settings while maintaining SAM's generalization advantages.

## Method Summary
LSAM addresses SAM's batch-size sensitivity in distributed training by introducing asynchronous sampling parallelism. Workers independently sample from a conditional distribution q(x|y) using Langevin dynamics, synchronizing only periodically every τ steps. The method combines SAM's adversarial perturbation with Gaussian smoothing through convolution, producing a landscape that targets both wide and deep minima. LSAM theoretically matches SGD convergence rates, with proofs showing convergence to stationary points for both constant and decaying perturbations. The algorithm uses a double-loop structure: fast inner sampling loop (multiple τ iterations) and slow outer optimization loop, with different step sizes for each.

## Key Results
- LSAM consistently achieves lower final test errors than data-parallel SAM across CNN-5, ResNet, VGG, and WRN architectures on SVHN, CIFAR-10, and CIFAR-100
- LSAM demonstrates faster convergence, requiring fewer epochs to reach target accuracy compared to DP-SAM, LSGD, EASGD, and standard SGD
- The method scales effectively in distributed settings while maintaining SAM's generalization advantages, with asynchronous communication every τ=16 steps
- Theoretical analysis shows LSAM matches SGD's O(log T/√T) convergence rate to stationary points with decaying perturbations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling parallelism overcomes SAM's batch-size sensitivity by decoupling worker count from global batch size
- Mechanism: Workers independently sample from q(x|y) and synchronize only periodically (every τ steps), avoiding forced batch inflation that degrades SAM's sharpness-aware gradients
- Core assumption: Periodic synchronization frequency τ and mixing coefficient α preserve sufficient coupling between local samplers to maintain convergence
- Evidence anchors: Data parallelism forces unfavorable trade-off between shrinking per-worker batches or ballooning global batch sizes; each worker independently generates samples in parallel with synchronization after nτ iterations; AsyncSAM (2503.11147) addresses similar synchronization bottlenecks

### Mechanism 2
- Claim: Convolving SAM distribution with Gaussian kernel produces smoothed landscape targeting wide and deep minima
- Mechanism: πSAM penalizes sharp minima regardless of depth; πESGD spreads mass to deep minima regardless of width; πLSAM combines both through convolution πLSAM(y) ∝ ∫ πSAM(x) πkernel(x,y) dx
- Core assumption: Kernel bandwidth (controlled via λ) is appropriately tuned to balance exploration (smoothing) and exploitation (convergence)
- Evidence anchors: πSAM suppresses probability mass at sharp minima regardless of depth; log-density gradient equals negative posterior mean under q (Theorem 1); no direct corpus validation of SAM+kernel combination

### Mechanism 3
- Claim: Decaying perturbation radius ρt = ρ0/√(t+1) enables convergence to exact stationary points
- Mechanism: Constant ρ introduces O(ρ²) bias in gradient estimate; decaying ρ eliminates this bias asymptotically, matching SGD's O(log T/√T) convergence rate
- Core assumption: Decay schedule is appropriately timed—decaying too fast loses sharpness-aware benefits; too slow leaves residual bias
- Evidence anchors: Theorem 3 shows constant ρ converges to neighborhood of size O(ρ²); Theorem 4 shows decaying ρ converges to zero as T→∞ with rate O(log T/√T); corpus papers on SAM variants generally use constant ρ

## Foundational Learning

- Concept: **Sharpness-Aware Minimization (SAM) basics**
  - Why needed here: LSAM builds directly on SAM's two-step procedure (adversarial perturbation → gradient at perturbed point)
  - Quick check question: Can you explain why SAM uses f(x + ε(x)) rather than f(x), and what ε(x) = ρ·∇f(x)/‖∇f(x)‖ approximates?

- Concept: **Langevin dynamics / SGLD**
  - Why needed here: The inner loop uses stochastic gradient Langevin dynamics to sample from q(x|y)
  - Quick check question: What is the role of injected Gaussian noise in Langevin dynamics, and how does it differ from standard SGD noise?

- Concept: **Two-time-scale stochastic approximation**
  - Why needed here: LSAM runs fast inner sampling loop and slow outer optimization loop with different step sizes
  - Quick check question: Why must inner loop run multiple steps (τ iterations) before outer loop updates, and what happens if both run at same frequency?

## Architecture Onboarding

- Component map: Workers (GPUs) -> Local servers (CPU) -> Global server (CPU) -> Workers (GPUs)

- Critical path:
  1. Workers sample independently using local gradients
  2. After nτ total iterations, workers push samples to local servers
  3. Global server aggregates, computes g't = (1/nτ)Σ(xi - yt), applies momentum: gt = g't + β(g't - g't-1)
  4. Optimizer updates yt, broadcasts new anchor point
  5. Workers reset counters and resume sampling

- Design tradeoffs:
  - **Sync period τ**: Larger τ → less communication overhead, more worker autonomy, but higher risk of divergence
  - **Momentum β**: Higher β → faster convergence on smooth landscapes, but may overshoot in rugged regions
  - **Perturbation ρ**: Larger ρ → stronger sharpness penalty, but introduces more noise and potential instability

- Failure signatures:
  - **Divergence of ∥xt - yt∥**: Anchor gap grows unbounded → check λ is large enough (λ ≥ C/ρ₀L per Lemma 5)
  - **No improvement over baseline**: Likely ρ too small or decaying too fast → monitor if sharpness-aware signal is active
  - **Communication deadlock**: Workers waiting indefinitely → verify shared queue implementation avoids race conditions

- First 3 experiments:
  1. **Sanity check**: Run LSAM on SVHN with CNN-5, n=1 worker, τ=16, ρ=0.1. Verify test error decreases and is comparable to Table 1 (~4.6%). If not, debug learning rate η and mixing α.
  2. **Scaling test**: Fix per-worker batch=128, increase workers n∈{1,2,4}. Compare convergence speed (epochs to target error) vs DP-SAM. Expect LSAM to maintain or improve speed while DP-SAM degrades at larger n.
  3. **Ablation on ρ schedule**: Compare constant ρ=0.1 vs decaying ρt=ρ₀/√(t+1). Measure final test error and training stability. Expect decaying schedule to reach slightly lower error but with longer tail convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the double communication overhead inherent to LSAM be reduced to match the cost of standard SGD while preserving the theoretical benefits of landscape smoothing?
- Basis in paper: Remark 1 states, "our method still requires double the communication cost, same as the original SAM."
- Why unresolved: The authors identify this as a specific constraint of their current framework but do not propose a method to mitigate the additional forward-backward pass cost.
- What evidence would resolve it: A variant of the algorithm that utilizes gradient compression, local steps, or error feedback to reduce communication volume without diverging from the πLSAM distribution.

### Open Question 2
- Question: Does the decaying perturbation radius (ρt) required for theoretical convergence to a stationary point compromise the generalization benefits typically associated with fixed perturbation SAM?
- Basis in paper: Theorem 4 guarantees convergence to zero only if ρt decays, whereas the empirical success of SAM is generally attributed to finding flat minima using a fixed ρ.
- Why unresolved: The paper empirically tests a fixed ρ but theoretically proves convergence for a decaying ρ, leaving the performance trade-off of the theoretically "correct" schedule unexplored.
- What evidence would resolve it: Comparative experiments on CIFAR/ImageNet using the decaying ρ schedule to measure if test accuracy degrades compared to the fixed ρ baseline.

### Open Question 3
- Question: Is the practice of ignoring synchronization for Batch Normalization statistics theoretically sound in the asynchronous distributed setting proposed?
- Basis in paper: Appendix C.2 notes that Batch Normalization statistics are not synchronized and "empirical results show no degradation," but no theoretical justification is provided.
- Why unresolved: While empirically functional on the tested datasets, the lack of synchronization could induce a distribution shift between workers that destabilizes training on more complex datasets or architectures.
- What evidence would resolve it: A theoretical analysis or ablation study on larger datasets (e.g., ImageNet) measuring the divergence of BN statistics across workers and its impact on final accuracy.

## Limitations

- The asynchronous sampling mechanism lacks direct empirical validation in the literature, with only AsyncSAM as a conceptual parallel
- The Gaussian smoothing component, while theoretically sound, has no established precedent in distributed SAM training
- The proof framework assumes bounded gradients and smoothness which may not hold for modern deep networks with batch normalization and skip connections

## Confidence

- **High confidence**: Core algorithmic framework, SGD convergence rates, CIFAR-10/SVHN results
- **Medium confidence**: Asynchronous convergence guarantees, scaling behavior claims, CIFAR-100 results
- **Low confidence**: Gaussian smoothing mechanism, decaying perturbation schedule, large-scale distributed performance

## Next Checks

1. Reproduce ResNet-20 on CIFAR-10 with n=4 workers, τ=16, ρ=0.1; verify test error matches Table 1 (12.47%) and shows faster convergence than DP-SAM
2. Conduct ablation study: compare LSAM with constant ρ=0.1 vs decaying ρt=0.1/√(t+1) to validate the theoretical advantage of decaying schedules
3. Test extreme scaling: run LSAM with n=8 workers (per-worker batch=64) to verify scaling claims and identify communication bottlenecks