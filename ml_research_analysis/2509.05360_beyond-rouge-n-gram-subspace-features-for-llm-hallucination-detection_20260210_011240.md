---
ver: rpa2
title: 'Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection'
arxiv_id: '2509.05360'
source_url: https://arxiv.org/abs/2509.05360
tags:
- arxiv
- n-gram
- https
- tensor
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel hallucination detection method using
  N-Gram tensor decomposition. The approach constructs N-Gram frequency tensors from
  LLM-generated text, capturing semantic co-occurrence patterns, then applies tensor
  decomposition to extract singular values as features for an MLP binary classifier.
---

# Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection

## Quick Facts
- arXiv ID: 2509.05360
- Source URL: https://arxiv.org/abs/2509.05360
- Reference count: 35
- Primary result: Tensor decomposition of N-gram frequency tensors extracts singular values that, when used as features for an MLP classifier, outperform traditional baselines and match state-of-the-art LLM judges on HaluEval hallucination detection.

## Executive Summary
This paper introduces a novel hallucination detection method using N-Gram tensor decomposition. The approach constructs N-Gram frequency tensors from LLM-generated text, capturing semantic co-occurrence patterns, then applies tensor decomposition to extract singular values as features for an MLP binary classifier. Evaluated on the HaluEval dataset, the method significantly outperforms traditional baselines (ROUGE, BERTScore, Perplexity) and matches or exceeds state-of-the-art LLM judges. Performance improves with larger text group sizes, demonstrating the method's effectiveness and transferability. Alternative N-Gram matrix representations confirm the approach's robustness against textual bias, highlighting its potential for reliable hallucination detection across domains.

## Method Summary
The method groups texts by label, constructs N-gram frequency tensors from these groups, and applies tensor decomposition (SVD for matrices, Tucker/CP for tensors) to extract singular values as fixed-length features. These features are fed into a 4-layer MLP binary classifier trained to distinguish hallucinated from factual text. The approach leverages the hypothesis that hallucinated and factual text exhibit different co-occurrence patterns captured by tensor decomposition. The paper evaluates performance across group sizes (1, 5, 20, 40) and uses binary and log-frequency variants to test robustness against textual bias.

## Key Results
- SVD-G40 achieves AUROC 0.994 on Summary subset, significantly outperforming baselines
- Performance consistently improves with larger group sizes, with SVD-G40 outperforming SVD-G1 across all subsets
- Binary and log-frequency matrix variants maintain comparable performance, demonstrating robustness to textual bias
- Method matches or exceeds state-of-the-art LLM judges on HaluEval dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-Gram frequency tensors capture discriminative co-occurrence patterns that differ structurally between hallucinated and factual text.
- Mechanism: By constructing an order-N tensor where each entry counts N-gram occurrences across grouped documents, the tensor encodes multi-way co-occurrence relationships. Decomposing this tensor extracts singular values that summarize the latent "shape" of these co-occurrence patterns—hallucinated text exhibits different statistical regularities than factual text.
- Core assumption: Hallucinated and factual text have measurably different N-gram co-occurrence signatures that survive tensor decomposition.
- Evidence anchors:
  - [abstract] "This tensor captures richer semantic structure by encoding co-occurrence patterns, enabling better differentiation between factual and hallucinated content."
  - [section 2] "We observe that as the reference text length increases, the N-Gram tensor captures more meaningful and discriminative information."
  - [corpus] Related work (HARP, CoHalLo) also exploits latent subspaces for hallucination detection, suggesting subspace structure is a recurring signal.
- Break condition: If hallucinated text is statistically indistinguishable from factual text in N-gram co-occurrence (e.g., high-quality hallucinations or domain-shifted data), singular values will not be discriminative.

### Mechanism 2
- Claim: Aggregating multiple documents into a single tensor densifies the signal, making singular values more stable and informative.
- Mechanism: Grouping M texts before tensor construction creates a larger, denser tensor where co-occurrence patterns are more reliably estimated. Singular values from denser matrices/tensors are less noisy and better capture true latent structure rather than sparsity artifacts.
- Core assumption: The discriminative signal is cumulative across documents of the same class (hallucinated or factual).
- Evidence anchors:
  - [section 3.3.2] "The results consistently improve as the group size increases, highlighting an underlying trend of denser and more populated matrices yielding better performances."
  - [table 2] SVD-G1 underperforms Perplexity on some subsets; SVD-G40 achieves AUROC 0.994 on Summary.
  - [corpus] Weak corpus evidence on density effects specifically; no direct external validation of this grouping mechanism found.
- Break condition: If grouped documents are heterogeneous (mixed quality, mixed domains, or mixed hallucination types), averaging may dilute rather than strengthen the signal.

### Mechanism 3
- Claim: The method's effectiveness is robust to length/frequency biases, as shown by alternative matrix representations.
- Mechanism: Testing binary (presence/absence) and log-frequency variants of the N-Gram matrix controls for simple length or frequency artifacts. Comparable performance across variants suggests the classifier learns from structural co-occurrence patterns, not just surface-level frequency differences.
- Core assumption: Binary and log-frequency transformations preserve the discriminative co-occurrence structure while attenuating frequency bias.
- Evidence anchors:
  - [section 3.3.3] "These experiments suggest that our method using frequency based N-Gram matrices is robust enough and minimally influenced by category-specific textual biases."
  - [figure 3] Shows comparable or improved performance for binary/log-frequency variants across subsets.
  - [corpus] "The Illusion of Progress" paper critiques ROUGE-based evaluations, reinforcing the need for bias-aware validation.
- Break condition: If hallucinated texts are systematically longer or shorter with distinct vocabulary (as in QA subset excluded for bias), even structural features may capture bias indirectly.

## Foundational Learning

- Concept: Tensor Decomposition (SVD, Tucker, CP)
  - Why needed here: The method extracts singular values from N-Gram tensors as classifier features; understanding what these values represent (latent factors, rank, energy) is essential for interpreting results.
  - Quick check question: Can you explain why the singular values of a matrix/tensor summarize its "shape" and why the top-k values are often sufficient for classification?

- Concept: N-Gram Language Models
  - Why needed here: The core representation is an N-Gram frequency tensor; knowing how N-grams capture local context and their limitations (sparsity, out-of-vocabulary issues) grounds the approach.
  - Quick check question: What happens to the tensor size and sparsity as N increases from 2 to 4 for a vocabulary of 10,000 tokens?

- Concept: Classifier Evaluation Metrics (AUROC, AUPR, F1)
  - Why needed here: The paper reports multiple metrics across imbalanced subsets; understanding trade-offs (e.g., AUROC vs. F1 under class imbalance) is critical for fair comparison.
  - Quick check question: If hallucinated samples are 10% of the dataset, which metric would you prioritize for real-world deployment—AUROC or F1?

## Architecture Onboarding

- Component map: Text Preprocessor -> Vocabulary Builder -> Tensor Constructor -> Decomposition Engine -> Feature Extractor -> MLP Classifier
- Critical path: Text grouping → Tensor construction → Decomposition → Feature extraction → MLP inference. The grouping strategy and tensor density directly determine feature quality.
- Design tradeoffs:
  - Group size vs. latency: Larger groups improve accuracy but require batching multiple documents, increasing latency.
  - N value vs. sparsity: Higher N captures richer context but exponentially increases tensor sparsity; the paper focuses on 2-grams for matrices.
  - Tucker vs. CP decomposition: Tucker preserves multi-mode structure but is memory-intensive; CP is lighter but may lose mode-specific information.
- Failure signatures:
  - Near-random AUROC (~0.5): Check if tensors are too sparse (group size too small) or if vocabulary is misaligned between train/eval.
  - Perfect accuracy on training but poor eval: Likely overfitting to dataset-specific N-grams; verify held-out split integrity.
  - Memory errors at large group sizes: Tensor dimensionality explodes; switch to sparse tensor representations or reduce vocabulary size.
- First 3 experiments:
  1. **Baseline sanity check**: Reproduce Table 2 results for SVD-G1 and SVD-G5 on HaluEval General subset. Verify AUROC is within ±0.02 of reported values.
  2. **Group size ablation**: Run SVD-G10 and SVD-G20 on Dialogue subset to interpolate the performance curve between G5 and G40. Plot AUROC vs. group size.
  3. **Robustness test**: Train on HaluEval Summary, evaluate on a held-out domain (e.g., CNN/DailyMail held-out split not in HaluEval). Measure performance drop to assess transferability claims.

## Open Questions the Paper Calls Out
- How can the singular values from N-Gram tensors be interpreted to explain the specific linguistic mechanisms of hallucinations? The conclusion states, "Future work should focus on improving the interpretability of our N-Gram Matrices and Tensors to better understand how they capture differences between hallucinated and factual text." The paper demonstrates that singular values serve as effective statistical features for classification but does not map these mathematical abstractions back to semantic error types.
- Can the method be adapted to detect hallucinations in single-instance generation without relying on large text groupings? Table 2 and Figure 2 show that performance at group size 1 is significantly weaker than at group size 40. The method currently relies on the density of co-occurrence patterns found in aggregated text, limiting its utility for real-time, single-response verification.
- Do the N-Gram subspace features transfer effectively to domains excluded from this study, such as QA or code generation? The authors suggest "investigating how hallucinations differ across text categories could offer valuable insights." The QA subset was explicitly excluded due to "strong textual bias." It is unclear if the singular value signatures are universal or if they overfit to the narrative structures of the General, Dialogue, and Summary subsets.

## Limitations
- Textual bias risk: The exclusion of QA subsets due to textual bias hints at domain-specific vulnerabilities that may not generalize, though the paper addresses this via ablation studies.
- Dataset dependence: All evaluations use HaluEval; performance on out-of-domain data is not reported, limiting claims of transferability.
- N-gram rigidity: The approach relies on fixed-length N-grams, which may miss longer-range dependencies or semantic nuances that transformer-based methods capture.

## Confidence
- **High**: The method's superiority over ROUGE/BERTScore baselines on HaluEval; the positive correlation between group size and performance; the robustness of binary/log-frequency variants.
- **Medium**: Claims of matching or exceeding state-of-the-art LLM judges; transferability across domains; the causal link between tensor density and singular value discriminativeness.
- **Low**: The mechanism's generalizability to non-text modalities (e.g., multimodal LLMs); performance on extremely sparse tensors (group size 1, high N).

## Next Checks
1. **Cross-dataset validation**: Evaluate SVD-G20 on CNN/DailyMail held-out summaries (not in HaluEval) to measure domain transfer performance.
2. **Bias audit**: Compare length, vocabulary overlap, and perplexity distributions between hallucinated and factual text in each subset; report how much variance in singular values is explained by these factors.
3. **Tensor sparsity analysis**: Quantify tensor density (non-zero entries / total entries) across group sizes and N values; correlate sparsity with AUROC to confirm the density-performance link.