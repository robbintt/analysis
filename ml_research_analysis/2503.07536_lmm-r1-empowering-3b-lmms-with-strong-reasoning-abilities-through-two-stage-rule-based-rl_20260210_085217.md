---
ver: rpa2
title: 'LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage
  Rule-Based RL'
arxiv_id: '2503.07536'
source_url: https://arxiv.org/abs/2503.07536
tags:
- reasoning
- multimodal
- arxiv
- training
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LMM-R1, a two-stage rule-based reinforcement
  learning framework that enhances reasoning in 3B-parameter Large Multimodal Models
  (LMMs) by first strengthening foundational reasoning via text-only data, then generalizing
  to multimodal domains. The method addresses challenges of multimodal reasoning data
  scarcity and degraded reasoning from multimodal pretraining.
---

# LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL

## Quick Facts
- arXiv ID: 2503.07536
- Source URL: https://arxiv.org/abs/2503.07536
- Reference count: 40
- Key outcome: 4.83% average improvement over baselines on multimodal benchmarks and 4.5% on text-only benchmarks using two-stage rule-based RL

## Executive Summary
LMM-R1 introduces a two-stage reinforcement learning framework that enhances reasoning in 3B-parameter Large Multimodal Models by first strengthening foundational reasoning through text-only data, then generalizing to multimodal domains. The method addresses the challenge of multimodal reasoning data scarcity and prevents reasoning degradation that occurs when training directly on simpler multimodal data. Experiments with Qwen2.5-VL-Instruct-3B demonstrate that text-based reasoning enhancement enables effective multimodal generalization, achieving significant improvements across both text-only and multimodal reasoning benchmarks.

## Method Summary
LMM-R1 employs a two-stage rule-based reinforcement learning approach. The first stage (FRE) strengthens foundational reasoning using text-only verifiable data through PPO optimization with format and accuracy rewards. The second stage (MGT) generalizes these reasoning capabilities to multimodal domains using domain-specific datasets while continuing PPO training. This progressive approach prevents the degradation of complex reasoning abilities that occurs when training directly on simpler multimodal data, achieving superior performance compared to supervised fine-tuning approaches.

## Key Results
- 4.83% average improvement over baselines on multimodal benchmarks (vs. 3.37% for FRE-Multi)
- 4.5% average improvement over baselines on text-only benchmarks (vs. 0.45% for FRE-Multi)
- 3.63% improvement on complex Football Game agent tasks
- FRE-Text improves OlympiadBench by 5.34%, MathVision by 2.17%, and MathVerse by 4.19%

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Reasoning Transfer from Text to Multimodal Domains
- Claim: Strengthening reasoning abilities on text-only data creates foundational capabilities that generalize to multimodal reasoning tasks.
- Mechanism: Logical deduction patterns learned in text domain are modality-agnostic and apply when processing multimodal inputs.
- Evidence anchors: FRE-Text improved text benchmarks significantly while FRE-Multi degraded them; cross-modal generalization supported by experimental results.
- Break condition: If multimodal tasks require fundamentally different reasoning patterns not present in text-only training data.

### Mechanism 2: Progressive Two-Stage Training Prevents Reasoning Degradation
- Claim: Establishing a strong reasoning foundation before multimodal training prevents degradation that occurs when training directly on simpler multimodal data.
- Mechanism: FRE stage builds robust reasoning patterns using complex text-only problems, preventing optimization for simpler patterns during multimodal training.
- Evidence anchors: FRE-Multi showed 1.6% decline on MATH500 while FRE-Text showed 2.0% gain; response length analysis showed multimodal RL encourages brevity over reasoning depth.
- Break condition: If high-quality, complex multimodal reasoning data were available at scale.

### Mechanism 3: Rule-Based RL Preserves Reasoning Capacity Better Than SFT
- Claim: Reinforcement learning with rule-based rewards avoids catastrophic forgetting and transfers reasoning capabilities more effectively than supervised fine-tuning.
- Mechanism: RL optimizes for outcome correctness through exploration, maintaining reasoning repertoire while SFT constrains to specific training trajectories.
- Evidence anchors: Direct SFT with text data led to significant performance degradation while RL training resulted in substantial improvements; SFT-Text underperformed FRE-Text across all benchmarks.
- Break condition: If SFT demonstrations exhaustively cover all required reasoning patterns with high-quality reasoning chains.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**: Used as the RL algorithm for both FRE and MGT stages with KL penalty to ensure training stability. Quick check: Why does the KL divergence penalty β in equation (1) prevent the policy from deviating too far from the initial policy during training?

- **Reward Function Design (Format + Accuracy)**: Two-part reward system with format reward (structured reasoning in <elevation> tags) and accuracy reward (answer correctness). The balance parameter α controls format importance. Quick check: If α is set too high, what behavior would you expect in the trained model's outputs?

- **Transfer Learning and Catastrophic Forgetting**: Core insight that reasoning skills transfer from text to multimodal domains, but improper training order causes forgetting. Quick check: Why does training on simpler multimodal data before establishing a strong reasoning foundation degrade text-only reasoning performance?

## Architecture Onboarding

- **Component map**:
```
LMM-R1 Pipeline
├── Stage 1: FRE (Foundational Reasoning Enhancement)
│   ├── Input: Text-only verifiable data (DeepScaler-40K)
│   ├── PPO Optimizer with KL penalty (β=1e-3)
│   └── Reward: Format (α-weighted) + Accuracy (symbolic verification)
│
├── Stage 2: MGT (Multimodal Generalization Training)
│   ├── Input: FRE-Text model checkpoint
│   ├── Domain-specific datasets:
│   │   ├── Geo: VerMulti-Geo15K
│   │   ├── PerceReason: VerMulti-65K
│   │   └── Agent: Sokoban environments
│   └── Continue PPO training with same reward structure
│
└── Evaluation: LMMs-Eval + LightEval frameworks
```

- **Critical path**:
  1. Start with Qwen2.5-VL-Instruct-3B baseline
  2. FRE-Text training: ~40K text-only problems, 1 epoch, actor_lr=1e-6
  3. Validate FRE-Text on text benchmarks (MATH500, GPQA) to confirm reasoning improvement
  4. MGT training on target domain with continued PPO (lower lr=4e-7 for Geo/PerceReason)
  5. Evaluate on domain-specific multimodal benchmarks

- **Design tradeoffs**:
  - FRE data source: Text-only (better reasoning transfer) vs. multimodal (better immediate visual skills but degrades reasoning) — paper recommends text-only
  - Response length budget: FRE-Text produces 600-800 token responses (deep reasoning); FRE-Multi produces ~80 tokens (shallow reasoning)
  - MGT domain order: Geo (easiest) → PerceReason (moderate) → Agent (hardest)

- **Failure signatures**:
  - Text-only reasoning degrades after multimodal training → likely skipped FRE stage or used FRE-Multi
  - Model produces very short responses without reasoning chains → likely trained with multimodal RL only
  - Catastrophic forgetting on held-out benchmarks → used SFT instead of RL, or trained without KL penalty
  - Poor agent task performance → may need longer MGT training (MGT-Sokoban used 4 episodes vs. 2 for other domains)

- **First 3 experiments**:
  1. Reproduce FRE-Text vs. FRE-Multi comparison: Train two models on text-only (40K) vs. multimodal (65K) data and measure both on MATH500 and MathVista
  2. Ablate MGT stage necessity: Take FRE-Text and evaluate directly on multimodal benchmarks without MGT training
  3. Test α (format reward weight) sensitivity: Run FRE-Text with α ∈ {0, 0.5, 1.0, 2.0} and measure both format compliance and accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about cross-modal reasoning transfer rely heavily on a single model architecture (Qwen2.5-VL-Instruct-3B) and may not generalize to other LMMs
- Evaluation focuses on reasoning benchmarks but provides limited analysis of real-world multimodal reasoning scenarios
- Two-stage approach adds complexity and training time, and optimal sequence for different domain combinations is not systematically explored

## Confidence
- **High Confidence**: The FRE-Text vs FRE-Multi degradation effect, where text-only RL improves text reasoning while multimodal RL degrades it; the response length analysis showing multimodal RL produces shorter responses
- **Medium Confidence**: The cross-modal transfer mechanism is theoretically sound but relies on the assumption that reasoning patterns are truly modality-agnostic; improvement magnitudes are modest and may vary
- **Low Confidence**: The specific choice of text-only data for FRE stage lacks systematic comparison with other data curation strategies; the claim that rule-based RL is superior to SFT for reasoning preservation is demonstrated but not extensively validated

## Next Checks
1. **Cross-Modal Transfer Boundary Test**: Apply LMM-R1 to a reasoning task that requires fundamentally different cognitive patterns in visual vs. text domains and measure whether the claimed transfer breaks down when task structure diverges.

2. **Rule-Based RL vs. SFT Generalization Gap**: Train identical models using SFT vs. rule-based RL on the same FRE-Text dataset, then evaluate on both seen and unseen reasoning problem types to quantify whether RL's advantage extends to novel problem formulations.

3. **Multimodal Data Quality Threshold**: Systematically vary the complexity and quality of multimodal training data in the MGT stage while holding FRE-Text constant to determine the minimum data quality threshold where direct multimodal training matches or exceeds the two-stage approach.