---
ver: rpa2
title: Limitations of Large Language Models in Clinical Problem-Solving Arising from
  Inflexible Reasoning
arxiv_id: '2502.04381'
source_url: https://arxiv.org/abs/2502.04381
tags:
- reasoning
- medical
- arxiv
- preprint
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces M-ARC, a medical abstraction and reasoning
  corpus designed to probe failure modes in LLM clinical problem-solving arising from
  inflexible reasoning patterns. The benchmark exploits the Einstellung effect by
  incorporating long-tail reasoning patterns underrepresented in medical texts to
  disrupt predictable problem-solving strategies.
---

# Limitations of Large Language Models in Clinical Problem-Solving Arising from Inflexible Reasoning

## Quick Facts
- **arXiv ID:** 2502.04381
- **Source URL:** https://arxiv.org/abs/2502.04381
- **Reference count:** 40
- **Primary result:** LLMs including o1 and Gemini models scored below 50% on M-ARC, while human physicians averaged 66%, highlighting inflexible reasoning patterns and overconfidence in clinical problem-solving.

## Executive Summary
This paper introduces M-ARC, a medical reasoning benchmark designed to expose LLM failure modes in clinical problem-solving by exploiting the Einstellung effect through long-tail reasoning patterns underrepresented in medical texts. The benchmark reveals that current state-of-the-art LLMs perform significantly worse than human physicians (below 50% vs 66% accuracy) when faced with questions requiring flexible reasoning rather than pattern matching. Models demonstrated systematic errors including hallucinations and fundamental reasoning failures, such as asserting medically implausible procedures or failing to apply logical negation. The findings suggest that LLM limitations stem from training biases toward pattern matching, raising concerns about their deployment in clinical settings where flexible reasoning is essential.

## Method Summary
The study evaluates LLMs on M-ARC, a 100-question medical reasoning benchmark specifically designed to probe inflexible reasoning patterns through adversarial distractors and long-tail reasoning scenarios underrepresented in medical texts. The benchmark follows USMLE multiple-choice format and includes a "seek more clinical data" option in 53% of questions. Human physician performance (n=5) served as baseline at 66% ±5.3% SE. Models tested include GPT-4o, o1, Claude-Sonnet/Opus, Gemini, Medalpaca, Meditron-7b, and Mistral via API access. For uncertainty estimation, each question was evaluated 15 times with subject age perturbed by ±10 days, and accuracy plus Brier scores were calculated using sample consistency. The methodology focuses on identifying specific failure modes related to inflexible reasoning rather than general medical knowledge.

## Key Results
- LLMs scored below 50% accuracy on M-ARC benchmark compared to human physicians at 66% ±5.3% SE
- Models exhibited systematic reasoning errors including hallucinations (e.g., measuring blood pressure on forehead) and failure to apply logical negation
- Overconfidence was observed despite limited accuracy, with Brier scores indicating poor calibration
- Performance gaps suggest LLM limitations stem from training biases favoring pattern matching over flexible reasoning

## Why This Works (Mechanism)
The M-ARC benchmark exploits the Einstellung effect by incorporating reasoning patterns that are underrepresented in medical texts, forcing models to rely on flexible problem-solving rather than familiar pattern matching. This approach reveals fundamental limitations in how LLMs process clinical reasoning tasks that require thinking beyond established patterns.

## Foundational Learning
- **Einstellung effect**: Cognitive bias where repeated exposure to a problem-solving pattern makes alternative solutions less accessible. Needed to understand why models fail on long-tail reasoning scenarios; check by identifying when models apply familiar solutions to novel problems.
- **Chain-of-thought prompting**: Technique where models verbalize reasoning steps before answering. Needed to elicit model reasoning processes; check by examining whether verbalized steps lead to correct conclusions.
- **Brier score calibration**: Metric measuring the accuracy of probabilistic predictions. Needed to assess model confidence calibration; check by comparing predicted probabilities with actual accuracy rates.
- **Adversarial distractors**: Incorrect answer choices designed to exploit predictable reasoning patterns. Needed to create challenging benchmark questions; check by analyzing error patterns on questions with these distractors.
- **Long-tail reasoning patterns**: Rare but valid problem-solving approaches underrepresented in training data. Needed to create benchmark questions that test flexible thinking; check by identifying questions requiring novel reasoning.
- **Selective prediction strategies**: Approaches where models defer to human experts when uncertain. Needed to explore safe deployment pathways; check by testing whether models can recognize out-of-distribution scenarios.

## Architecture Onboarding
**Component Map:** M-ARC questions -> LLM inference (temperature=0) -> Answer parsing -> Accuracy calculation -> Uncertainty estimation (15 samples) -> Brier score calculation -> Human baseline comparison

**Critical Path:** Question selection → Model inference → Answer extraction → Performance evaluation → Uncertainty analysis

**Design Tradeoffs:** The benchmark prioritizes ecological validity in exposing reasoning failures over generalizability to all medical scenarios, accepting that artificial adversarial questions may not reflect real-world clinical encounters.

**Failure Signatures:** Hallucinations of medically implausible procedures, failure to apply logical negation, selection of familiar but incorrect answers matching training data patterns, overconfidence in incorrect responses.

**First Experiments:**
1. Run M-ARC questions with temperature=0 across target LLMs and compare answer distributions
2. Perform uncertainty estimation by varying subject age across 15 samples per question
3. Analyze Brier scores to assess model calibration across correct and incorrect responses

## Open Questions the Paper Calls Out
The authors explicitly state that future work will aim to increase the size of the M-ARC dataset to improve its robustness, acknowledging that the current 100-question limitation reflects the difficulty of crafting novel adversarial scenarios. The paper also suggests that selective prediction strategies may offer a pathway for safe deployment by forcing deferral in out-of-distribution contexts, though this remains untested.

## Limitations
- The exact prompt template and chain-of-thought format used during evaluation remain unspecified, potentially affecting reproducibility
- Scoring methodology for "seek more clinical data" responses is unclear, complicating performance assessment
- The benchmark targets artificial adversarial scenarios that may not generalize to typical clinical reasoning tasks
- Small human baseline sample size (n=5) limits statistical power for comparing human vs. LLM performance

## Confidence
- **High confidence:** LLMs demonstrate systematic reasoning failures on M-ARC benchmark questions requiring flexible thinking
- **Medium confidence:** Training biases favoring pattern matching over flexible reasoning explain observed performance gaps
- **Medium confidence:** Overconfidence in responses despite limited accuracy is a consistent finding across models

## Next Checks
1. Replicate the study using standardized medical reasoning benchmarks beyond M-ARC to determine if performance gaps persist in more conventional clinical scenarios
2. Conduct ablation studies varying prompt formats and chain-of-thought instructions to isolate the impact of evaluation methodology on observed performance differences
3. Test model performance on M-ARC questions with medical expert-verified distractors to confirm that failures stem from inflexible reasoning rather than adversarial question design