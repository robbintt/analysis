---
ver: rpa2
title: Online Multi-LLM Selection via Contextual Bandits under Unstructured Context
  Evolution
arxiv_id: '2506.17670'
source_url: https://arxiv.org/abs/2506.17670
tags:
- context
- cost
- user
- regret
- linucb
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a contextual bandit framework for online multi-LLM
  selection under unstructured prompt evolution. It addresses the challenge of selecting
  the best LLM in a sequential, multi-step interaction where the prompt context evolves
  dynamically based on prior LLM outputs.
---

# Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution

## Quick Facts
- **arXiv ID**: 2506.17670
- **Source URL**: https://arxiv.org/abs/2506.17670
- **Reference count**: 40
- **Primary result**: 74.84% average accuracy achieved with positionally-aware knapsack heuristic

## Executive Summary
This paper introduces a contextual bandit framework for online multi-LLM selection under unstructured prompt evolution. The framework addresses the challenge of selecting the best LLM in sequential, multi-step interactions where prompt context evolves dynamically based on prior LLM outputs. The proposed method employs a myopic regret minimization strategy using Greedy LinUCB algorithm, maintaining separate linear models for each LLM and selecting the one with highest upper confidence bound at each step.

The framework includes budget-aware and positionally-aware extensions to handle variable costs and prioritize early-stage user satisfaction. Experiments across diverse benchmarks demonstrate that the proposed algorithms outperform existing LLM routing methods, with the positionally-aware knapsack heuristic achieving the highest average accuracy of 74.84% while the budget-aware variant offers strong cost-efficiency. The results validate the effectiveness of contextual bandits for real-time, adaptive LLM selection in interactive environments.

## Method Summary
The proposed method uses a contextual bandit framework with myopic regret minimization for online multi-LLM selection. At each step of the multi-step interaction, the system observes the current context (prompt) and selects an LLM using the Greedy LinUCB algorithm. Each LLM maintains its own linear model that maps contexts to expected rewards. The algorithm selects the LLM with the highest upper confidence bound, balancing exploration and exploitation. After receiving the LLM's output, the context evolves based on this output, creating a dynamic selection environment. The framework includes extensions for budget constraints (variable LLM costs) and positional awareness (prioritizing early-stage user satisfaction) through knapsack-based heuristics.

## Key Results
- Positionally-aware knapsack heuristic achieves highest average accuracy of 74.84%
- Budget-aware variant provides strong cost-efficiency trade-offs
- Proposed algorithms outperform existing LLM routing methods across diverse benchmarks
- Framework successfully handles unstructured prompt evolution in multi-step interactions

## Why This Works (Mechanism)
The contextual bandit approach works because it enables real-time adaptation to evolving contexts while maintaining separate performance models for each LLM. By using upper confidence bounds, the algorithm balances exploration of potentially better LLMs with exploitation of known good performers. The myopic regret minimization ensures decisions at each step consider both immediate reward and future context evolution. The linear model structure allows efficient updates as new context-reward pairs are observed. The knapsack-based extensions for budget and positional constraints provide practical mechanisms to incorporate real-world limitations while maintaining the core selection logic.

## Foundational Learning
- **Contextual Bandits**: Sequential decision-making framework where actions are chosen based on context features; needed because LLM selection depends on prompt characteristics that evolve over time; quick check: verify the algorithm updates context features appropriately after each LLM response.
- **Greedy LinUCB Algorithm**: Upper confidence bound method for linear contextual bandits; needed to balance exploration vs exploitation in LLM selection; quick check: confirm confidence bounds properly scale with uncertainty in each LLM's performance estimate.
- **Myopic Regret Minimization**: Strategy that optimizes immediate reward while considering future consequences; needed because each selection affects subsequent context evolution; quick check: validate that short-term greedy choices don't consistently lead to poor long-term outcomes.
- **Knapsack Heuristics**: Optimization approach for constrained resource allocation; needed for budget-aware and positionally-aware extensions; quick check: ensure the heuristic correctly handles the trade-off between LLM quality and cost constraints.
- **Linear Contextual Models**: Regression models mapping contexts to expected rewards for each LLM; needed to predict performance without exhaustive testing; quick check: verify model updates properly incorporate new context-reward observations.
- **Unstructured Context Evolution**: Dynamic changes in prompt context based on prior LLM outputs; needed to model realistic multi-step interactions; quick check: test with varying degrees of context complexity to ensure robust performance.

## Architecture Onboarding

**Component Map**: User Prompt -> Context Processor -> LLM Selector (Greedy LinUCB) -> Selected LLM -> Output Processor -> Reward Calculator -> Context Updater -> (Budget/Positional Constraints)

**Critical Path**: The core selection loop follows: observe context → select LLM using upper confidence bounds → execute LLM → process output → calculate reward → update context → update linear models. This loop must complete within acceptable latency bounds for real-time applications.

**Design Tradeoffs**: The framework trades computational complexity (maintaining separate models for each LLM) for better adaptation to individual LLM strengths. The myopic approach simplifies planning but may miss global optimization opportunities. Linear models offer efficiency but may not capture complex non-linear relationships between contexts and LLM performance.

**Failure Signatures**: Poor performance may manifest as: consistently selecting suboptimal LLMs (exploration failure), getting stuck in local optima (exploitation failure), slow adaptation to context changes (model update issues), or budget exhaustion without quality improvement (constraint handling failure).

**First Experiments**:
1. **Ablation Study**: Remove budget-aware and positionally-aware components to quantify their individual contributions to the 74.84% accuracy.
2. **Adversarial Context Testing**: Introduce deliberately complex or rapidly changing context evolution patterns to stress-test the unstructured evolution assumption.
3. **Latency Analysis**: Measure end-to-end computational overhead including model updates, selection decisions, and context processing under varying complexity.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- The assumption of "unstructured context evolution" remains somewhat abstract in experimental validation, with relatively controlled prompt variations rather than truly unpredictable context structures.
- Performance metrics focus primarily on accuracy without comprehensive analysis of failure modes or error types.
- The claim of "real-time, adaptive LLM selection" effectiveness needs more temporal analysis regarding computational overhead and adaptation speed.

## Confidence

**High Confidence**: The theoretical foundation of using contextual bandits for LLM selection is sound, and the Greedy LinUCB algorithm implementation is technically coherent. The budget-aware and positionally-aware extensions follow logically from the base framework.

**Medium Confidence**: Experimental results showing superiority over existing LLM routing methods are promising but require scrutiny regarding dataset characteristics, query distributions, and tested LLM variations.

**Low Confidence**: The effectiveness claim for real-time adaptation needs more temporal analysis, including computational overhead, latency implications, and adaptation speed to new context patterns.

## Next Checks
1. Conduct ablation studies removing the positionally-aware and budget-aware components to quantify their individual contributions to the 74.84% accuracy figure.

2. Test the framework with deliberately adversarial context evolution patterns where prompt complexity increases unpredictably to stress-test the "unstructured context evolution" assumption.

3. Measure end-to-end latency and computational overhead in realistic deployment scenarios, including time to update linear models and make selection decisions under varying context complexity.