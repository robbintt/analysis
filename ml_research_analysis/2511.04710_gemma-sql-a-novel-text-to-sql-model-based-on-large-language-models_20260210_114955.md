---
ver: rpa2
title: 'GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models'
arxiv_id: '2511.04710'
source_url: https://arxiv.org/abs/2511.04710
tags:
- schema
- query
- gemma-sql
- language
- gemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GEMMA-SQL, a lightweight, open-source text-to-SQL
  model built on the Gemma 2B architecture. The model is designed to translate natural
  language queries into executable SQL statements efficiently, using few-shot prompting
  and schema-aware encoding.
---

# GEMMA-SQL: A Novel Text-to-SQL Model Based on Large Language Models

## Quick Facts
- **arXiv ID**: 2511.04710
- **Source URL**: https://arxiv.org/abs/2511.04710
- **Reference count**: 40
- **Primary result**: Achieves 64.5% Test-Suite accuracy on Spider with 2B parameter model

## Executive Summary
This paper introduces GEMMA-SQL, a lightweight text-to-SQL model built on the Gemma 2B architecture. The model leverages LoRA fine-tuning, schema-aware few-shot prompting, and iterative confidence-based refinement to translate natural language queries into executable SQL statements. Despite its small size, GEMMA-SQL achieves competitive results on the Spider benchmark, demonstrating that effective prompt design and domain-specific tuning can yield strong performance even on resource-constrained hardware.

## Method Summary
The authors fine-tune Gemma-2B using Low-Rank Adaptation (LoRA) with rank 8 on the Spider dataset. Inputs are formatted into a structured prompt template containing instruction, schema, and response components. The model uses schema-aware few-shot prompting to ground SQL generation in valid database structure. A post-processing loop implements iterative refinement by calculating confidence scores from token log-probabilities and regenerating queries when confidence falls below a threshold, up to 5 attempts.

## Key Results
- GEMMA-SQL achieves 64.5% Test-Suite accuracy and 60.7% Exact Match accuracy on Spider dev set
- GEMMA-SQL Instruct variant achieves 66.8% Test-Suite and 63.3% Exact Match accuracy
- Outperforms significantly larger models like CodeGen-16B despite having only 2B parameters
- Iterative refinement improves accuracy by approximately 6% at the cost of increased latency

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific adaptation via LoRA aligns a general-purpose LLM with SQL syntax and schema logic. The authors fine-tune Gemma 2B using LoRA (rank=8) on the Spider dataset, allowing the model to learn the mapping from natural language to SQL structure efficiently on consumer-grade hardware. Core assumption: The base Gemma 2B model possesses sufficient latent reasoning capability to handle logical mapping, requiring only lightweight weight updates to specialize for SQL. Evidence: Table 3 shows GEMMA-SQL Instruct outperforming significantly larger models despite having only 2B parameters.

### Mechanism 2
Schema-aware few-shot prompting reduces ambiguity by explicitly grounding the query generation in database structure. The model uses a structured prompt template: {Instruction, Schema, Response}. By explicitly feeding the schema (tables, columns, foreign keys) and illustrative examples into the context window, the model is constrained to generate SQL using valid identifiers rather than hallucinating non-existent columns. Core assumption: The model's context window is large enough to hold the schema definition and few-shot examples without catastrophic forgetting of the input query. Evidence: Figure 22 shows "Schema-Aware Few-Shot" outperforming "Zero-Shot" by significant margins (EM: 88% vs 65%).

### Mechanism 3
Iterative confidence-based refinement filters out low-probability token generations to improve reliability. The system calculates a confidence score for a generated SQL query by averaging token log-probabilities. If the score falls below a threshold, the system re-prompts the model with modified instructions, retrying up to 5 times. Core assumption: Low token probability correlates strongly with semantic or syntactic error in SQL generation, and re-sampling will eventually yield a high-confidence correct answer. Evidence: Section 3.7 states this iterative refinement process improves overall model accuracy by approximately 6%.

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - **Why needed here**: Understanding how the authors fine-tuned a Large Language Model on a single GPU without training the full 2B parameters
  - **Quick check question**: Does LoRA update the pre-trained weights of the Gemma model directly, or does it train separate adapter matrices?

- **Concept: Schema Linking**
  - **Why needed here**: The core challenge in Text-to-SQL is mapping natural language entities (e.g., "biggest city") to specific database columns (e.g., `population`)
  - **Quick check question**: How does providing the "Schema" in the prompt help the model distinguish between a column named `name` and a column named `title`?

- **Concept: Exact Match (EM) vs. Test Suite (TS)**
  - **Why needed here**: Evaluating SQL requires knowing if the query is textually identical (EM) or functionally equivalent (TS)
  - **Quick check question**: Why might a generated SQL query have a low Exact Match score but a high Test Suite accuracy?

## Architecture Onboarding

- **Component map**: Input Processor -> Prompt Builder -> Core Engine (Gemma 2B + LoRA Adapter) -> Post-Processor
- **Critical path**: The data preparation phase is critical; the prompt must strictly follow the `Instruction -> Schema -> SQL` format for the fine-tuned model to recognize the pattern
- **Design tradeoffs**:
  - Latency vs. Accuracy: The iterative refinement loop improves accuracy by ~6% but multiplies inference time by up to 5x for low-confidence queries
  - Model Size vs. Context: Using Gemma 2B allows local deployment but limits the context window and reasoning depth compared to 70B+ models
- **Failure signatures**:
  - Operation Hallucination: Model inserts "ascending/descending" incorrectly
  - Schema Misalignment: Model uses query terms as column names instead of actual schema names
- **First 3 experiments**:
  1. Baseline LoRA Validation: Fine-tune Gemma-2B on the Spider training set using the exact LoRA config (Rank 8, LR 5e-5) and verify convergence
  2. Prompt Ablation: Compare zero-shot vs. schema-aware few-shot inference on the dev set to replicate the accuracy jump shown in Figure 22
  3. Refinement Threshold Testing: Adjust the confidence score threshold in the post-processing loop to find the "sweet spot" between latency and accuracy gain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GEMMA-SQL effectively generalize to multilingual natural language inputs?
- Basis in paper: Section 5 states future work will focus on "multilingual generalization"
- Why unresolved: The training and evaluation were restricted to the English-only SPIDER dataset
- What evidence would resolve it: Benchmarks on translated datasets (e.g., multilingual Spider) showing performance retention across languages

### Open Question 2
- Question: How does the model perform on domain-specific schemas not represented in general-purpose benchmarks?
- Basis in paper: Section 5 identifies "domain adaptation" as a necessary future direction to expand utility
- Why unresolved: The SPIDER dataset provides cross-domain variety but lacks the vertical depth of specific industries
- What evidence would resolve it: Evaluation results on specialized datasets like BIRD or proprietary financial databases

### Open Question 3
- Question: Can the iterative refinement strategy effectively close the performance gap with large proprietary models on highly nested queries?
- Basis in paper: Table 3 shows GEMMA-SQL lags behind DIN-SQL (GPT-4) by ~7.4% on Test-Suite accuracy, and Section 4.4 notes specific struggles with nested queries
- Why unresolved: The paper demonstrates efficiency but does not fully resolve the accuracy trade-off inherent to smaller parameter counts
- What evidence would resolve it: A comparative error analysis focused exclusively on complex, nested SQL patterns against GPT-4 baselines

## Limitations
- Context Window Constraint: The 512-token limit may truncate complex schemas, forcing incomplete schema representation and potentially degrading performance
- Iterative Refinement Trade-off: The confidence-based re-generation loop improves accuracy by ~6% but increases inference latency up to 5x, making real-time applications impractical without optimization
- Schema Dependency: Model performance is highly dependent on accurate schema representation in the prompt; hallucination of non-existent columns or incorrect column references remains a documented failure mode

## Confidence
- **High Confidence**: The mechanism of LoRA fine-tuning for efficient parameter adaptation is well-established and clearly demonstrated through performance gains over larger models
- **Medium Confidence**: The schema-aware few-shot prompting mechanism is sound, but the specific example selection strategy and confidence threshold values are not fully specified
- **Medium Confidence**: The iterative refinement mechanism is plausible and supported by related work, but the effectiveness depends heavily on the unstated confidence threshold parameter

## Next Checks
1. **Threshold Sensitivity Analysis**: Systematically vary the confidence score threshold parameter to quantify the trade-off between accuracy improvement and latency increase across different threshold values
2. **Schema Representation Test**: Evaluate model performance on progressively larger schemas (truncated at different lengths) to determine the breaking point where schema information loss begins degrading accuracy
3. **Cross-Dataset Generalization**: Test the fine-tuned GEMMA-SQL model on a held-out Spider subset or alternative Text-to-SQL datasets (e.g., WikiSQL) to assess whether the few-shot prompting strategy generalizes beyond the training distribution