---
ver: rpa2
title: Scaling Laws for Uncertainty in Deep Learning
arxiv_id: '2506.09648'
source_url: https://arxiv.org/abs/2506.09648
tags:
- uncertainty
- data
- scaling
- learning
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether scaling laws, which describe predictable
  performance improvements with increased data and model size, also govern predictive
  uncertainties in deep learning. The authors empirically demonstrate that various
  uncertainty measures, including total, aleatoric, and epistemic uncertainty, follow
  power-law scaling trends with respect to dataset and model size across vision and
  language tasks.
---

# Scaling Laws for Uncertainty in Deep Learning

## Quick Facts
- **arXiv ID:** 2506.09648
- **Source URL:** https://arxiv.org/abs/2506.09648
- **Reference count:** 40
- **Primary result:** Epistemic uncertainty follows predictable power-law decay with dataset size across multiple architectures, modalities, and uncertainty quantification methods

## Executive Summary
This paper investigates whether scaling laws, which describe predictable performance improvements with increased data and model size, also govern predictive uncertainties in deep learning. The authors empirically demonstrate that various uncertainty measures, including total, aleatoric, and epistemic uncertainty, follow power-law scaling trends with respect to dataset and model size across vision and language tasks. These findings hold across multiple architectures, modalities, and uncertainty quantification methods, including Monte Carlo dropout, deep ensembles, and Markov chain Monte Carlo. The results show that epistemic uncertainty typically contracts as O(1/N) with increasing dataset size, though the exact power-law coefficients vary depending on design choices. This work provides practical evidence that uncertainty can be extrapolated to larger data regimes and challenges skepticism about the relevance of Bayesian approaches in data-rich scenarios.

## Method Summary
The authors empirically demonstrate power-law scaling of predictive uncertainties by training models on increasing subsets of data and measuring how uncertainty metrics evolve. They use multiple uncertainty quantification methods including MC Dropout, Deep Ensembles, MCMC sampling, IVON optimization, and linearized Laplace approximation. For each method, they train on 4-9 different data subset sizes (typically 25%, 50%, 75%, 100% of available data) with 10 seeds per configuration. Uncertainty is computed from ensemble predictions using entropy-based metrics: total uncertainty (TU) as entropy of the mean predictive, aleatoric uncertainty (AU) as average entropy, and epistemic uncertainty (EU) as their difference. Power-law exponents γ are fitted via log-log regression, and extrapolation tests validate predictions on larger held-out datasets.

## Key Results
- Epistemic uncertainty typically contracts as O(1/N) with increasing dataset size across vision and language tasks
- Power-law scaling holds across multiple architectures including ResNet, WideResNet, ViT, and MLP
- Different uncertainty quantification methods show varying power-law coefficients, with Deep Ensembles generally showing steeper decay than MC Dropout
- Scaling laws enable uncertainty extrapolation to larger data regimes not seen during training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Predictive uncertainty measures follow power-law scaling with dataset size N, enabling extrapolation to larger data regimes
- **Mechanism:** As training data increases, the posterior distribution contracts around true parameters. In identifiable Bayesian models, this follows O(1/N). The paper shows this extends empirically to over-parameterized neural networks: EU contracts predictably while AU approaches an irreducible floor representing intrinsic data noise
- **Core assumption:** Power-law form L(x) ∝ x^γ observed for test loss extends to entropy-based uncertainty metrics derived from ensemble predictions
- **Evidence anchors:** Section 4.3, Figure 11 shows MLP on two_moons with EU power-law decay (γ = −0.79, R² = 0.90) matching extrapolation to 1M samples
- **Break condition:** Pre-training saturation masks scaling behavior; fine-tuning on small datasets yields flat uncertainty curves (Section 4.2, Figure 18)

### Mechanism 2
- **Claim:** Generalization error GN decomposes into aleatoric entropy plus a KL divergence term that captures epistemic uncertainty
- **Mechanism:** The paper derives GN = H[p(y)] + KL[p(y)||qN(y)], where qN is posterior predictive after N training points. The KL term quantifies divergence from true distribution and vanishes as N→∞. This connects Watanabe's Singular Learning Theory to predictive uncertainty
- **Core assumption:** Asymptotic expansion from SLT (E[Gn] = λ/n + o(1/n)) governs uncertainty scaling in over-parameterized networks despite non-identifiability
- **Evidence anchors:** Section 5.2, Eq. 10 formal derivation showing GN = aleatoric + KL divergence (epistemic)
- **Break condition:** Exact scaling exponent γ varies with architecture, optimizer, and UQ method; no formula predicts γ from model properties

### Mechanism 3
- **Claim:** Under linearized Laplace approximation, epistemic uncertainty in logit space scales as O(1/N) because Fisher information accumulates additively with data
- **Mechanism:** GGN approximation to Hessian yields Σ ∝ (∑J^TΛJ + λI)^−1. Data precision term grows linearly with N, so posterior variance contracts as 1/N. Linearization transforms BNN into GLM, enabling closed-form analysis
- **Core assumption:** Linearized network approximates true predictive distribution sufficiently near MAP estimate
- **Evidence anchors:** Section B.3, Figure 12 shows Hessian eigenvalues scale linearly with N; EU in logit space follows O(1/N) across prior strengths
- **Break condition:** Non-linear mapping from logits to probabilities dampens observed EU in probability space; confident predictions suppress variance even when logit-space EU is non-negligible

## Foundational Learning

- **Concept: Epistemic vs. Aleatoric Uncertainty Decomposition**
  - **Why needed here:** The paper's central analysis depends on distinguishing reducible uncertainty (EU, from limited data/model knowledge) from irreducible noise (AU, intrinsic to task). TU = H[predictive mean], AU = average H[predictive], EU = TU − AU
  - **Quick check question:** Given an ensemble of predictions, can you compute which component decreases with more training data?

- **Concept: Power-Law Scaling and Exponent Interpretation**
  - **Why needed here:** All results are expressed as f(x) ∝ x^γ with γ < 0. Understanding that |γ| controls contraction rate is essential for comparing methods and predicting data requirements
  - **Quick check question:** If EU ∝ N^−0.8, how much more data is needed to halve epistemic uncertainty?

- **Concept: Approximate Bayesian Inference Methods (MCD, Ensembles, MCMC, Laplace)**
  - **Why needed here:** The paper tests whether scaling laws hold across UQ methods. Each produces ensembles differently: dropout samples, independent training runs, or posterior samples
  - **Quick check question:** Why might Deep Ensembles show different γ values than Monte Carlo Dropout for the same architecture?

## Architecture Onboarding

- **Component map:** UQ Method Layer (MCD, Deep Ensembles, MCMC, IVON, Linearized Laplace) -> Uncertainty Computation (TU/AU/EU via entropy formulas) -> Scaling Estimation (power-law fit on log-log scale) -> Extrapolation Module (predict EU at large N)
- **Critical path:** 1. Choose UQ method and architecture -> 2. Train on increasing N subsets with multiple seeds -> 3. Compute TU/AU/EU on held-out test set -> 4. Fit power-law regression (log U vs. log N) -> 5. Validate extrapolation by holding out largest N
- **Design tradeoffs:**
  - MCD vs. Deep Ensembles: MCD is cheap but may underestimate diversity; DE is expensive but typically shows steeper γ (faster EU contraction)
  - Prior strength in Laplace: Strong prior (large λ) delays transition to data-dominated 1/N regime; weak prior yields earlier power-law but risks numerical instability
  - SAM optimizer: Can increase EU (positive γ) by forcing flat minima with preserved functional diversity (Figure 3)
- **Failure signatures:**
  - Flat uncertainty curves (γ ≈ 0): Pre-training saturation, insufficient training epochs, or OOD test set far from training support
  - Positive γ (EU increasing with N): SAM + MCD interaction, ensemble collapse, or optimization dynamics favoring diverse but overconfident solutions
  - High variance in γ estimates across seeds: Unstable posterior approximation or insufficient ensemble size
- **First 3 experiments:**
  1. Replicate CIFAR-10 ResNet-18 MCD baseline: Train on [12K, 25K, 50K] subsets with p=0.5, compute TU/AU/EU, fit γ. Compare against paper's γ_EU ≈ −0.36
  2. Ablate UQ method on same architecture: Run Deep Ensembles (M=5) and MCD (p=0.2) on identical N subsets. Quantify γ difference to assess method sensitivity
  3. Extrapolation test: Train on [100, 1K, 10K] of two_moons with Bayesian MLP + HMC. Predict EU at N=100K using fitted power-law; compare against actual EU computed from full training run

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Singular Learning Theory (SLT) invariants, such as the Real Log Canonical Threshold (RLCT) or effective dimensionality, formally predict the empirical scaling exponents γ for uncertainty in deep networks?
- Basis: The authors explicitly state: "A key limitation is that we do not yet have a formal theory that predicts the empirical scaling exponents γ of deep networks from SLT invariants or other geometric quantities"
- Why unresolved: Current theoretical connections are limited to linear models, and extending these to over-parameterized, non-identifiable deep networks remains an open challenge
- What evidence would resolve it: A theoretical framework deriving power-law coefficients from model geometry that matches empirical measurements of uncertainty decay

### Open Question 2
- Question: How do optimization strategies, specifically Sharpness Aware Minimization (SAM), fundamentally alter uncertainty scaling laws to induce increasing epistemic uncertainty with dataset size?
- Basis: While the paper notes optimization effects are "only partially characterized," Figure 3 and Section 4.1.1 show that SAM can invert expected trends, causing epistemic uncertainty to increase with N
- Why unresolved: The interaction between flat minima selection (SAM) and functional diversity in Bayesian approximations creates complex dynamics not explained by standard theory
- What evidence would resolve it: A theoretical or empirical disentanglement of how flatness metrics interact with ensemble diversity to reverse the O(1/N) contraction rate

### Open Question 3
- Question: How does massive-scale pre-training saturation mask or distort uncertainty scaling laws when models are fine-tuned on smaller downstream datasets?
- Basis: The paper notes in Section 4.2 that fine-tuning the Phi-2 model resulted in "completely flat" uncertainty curves, suggesting current metrics lose sensitivity when pre-training data dwarfs fine-tuning data
- Why unresolved: It is unclear if uncertainty metrics truly saturate or if the scaling simply occurs on a logarithmic scale invisible at the fine-tuning resolution
- What evidence would resolve it: Experiments varying the ratio of pre-training to fine-tuning data to identify the threshold where scaling laws re-emerge

## Limitations
- The exact power-law exponents vary significantly across methods and architectures, with no theoretical formula to predict γ from model properties
- Pre-training saturation can mask scaling behavior when fine-tuning on small datasets relative to pre-training scale
- The linearized Laplace approximation's validity for highly non-linear networks remains unproven despite empirical success

## Confidence
- **High confidence:** Power-law scaling of EU with N holds empirically across multiple methods and datasets
- **Medium confidence:** The O(1/N) contraction rate matches theoretical expectations from Bayesian asymptotics
- **Medium confidence:** SLT-based decomposition of generalization error into aleatoric and epistemic components applies to over-parameterized networks

## Next Checks
1. **Cross-dataset transferability test:** Train scaling law models on CIFAR-10 and evaluate uncertainty extrapolation on CIFAR-100 to assess method robustness across domains
2. **OOD boundary validation:** Systematically vary test set distance from training distribution and measure how well scaling laws predict uncertainty collapse in increasingly OOD regions
3. **Architecture-agnostic prediction:** Build a meta-model that predicts γ from architecture statistics (depth, width, parameter count) and test predictions on held-out architectures not used in training