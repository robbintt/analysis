---
ver: rpa2
title: Verification-Guided Falsification for Safe RL via Explainable Abstraction and
  Risk-Aware Exploration
arxiv_id: '2506.03469'
source_url: https://arxiv.org/abs/2506.03469
tags:
- safety
- falsification
- policy
- violations
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of ensuring the safety of reinforcement
  learning (RL) policies in high-stakes environments by integrating formal verification
  with interpretable policy abstraction and risk-aware falsification. The core method,
  CAPS (Comprehensible Abstract Policy Summarization), generates a human-interpretable
  graph from offline trajectories, which is then verified using Storm probabilistic
  model checking against PCTL safety specifications.
---

# Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration

## Quick Facts
- arXiv ID: 2506.03469
- Source URL: https://arxiv.org/abs/2506.03469
- Reference count: 34
- Primary result: Integrates formal verification with interpretable policy abstraction and risk-aware falsification to detect significantly more safety violations in RL policies across navigation, maze, and medical domains.

## Executive Summary
This paper addresses the challenge of ensuring safety in reinforcement learning (RL) policies for high-stakes applications by combining formal verification with interpretable policy abstraction and risk-aware falsification. The core method, CAPS (Comprehensible Abstract Policy Summarization), generates a human-interpretable graph from offline trajectories, which is then verified using Storm probabilistic model checking against PCTL safety specifications. When no violations are found, the framework performs risk- and uncertainty-guided falsification to explore high-risk and underrepresented states, using a risk critic and ensemble-based epistemic uncertainty to detect novel safety violations. The approach also includes a runtime safety shield that switches to a fallback policy when risk exceeds a threshold.

## Method Summary
The method involves four key stages: First, the CAPS abstraction method converts offline trajectories into an interpretable policy graph. Second, the Storm model checker verifies this graph against PCTL safety specifications, returning counterexamples if violations exist. Third, if no violations are found, a risk-aware falsification strategy uses estimated risk from model checking and epistemic uncertainty from an ensemble of encoders to guide exploration of high-risk, underrepresented states through targeted rollouts and gradient-based mutation. Fourth, a runtime safety shield monitors execution and switches to a fallback policy when predicted risk exceeds a threshold.

## Key Results
- The framework detects significantly more safety violations than uncertainty-based and fuzzy-based search methods across three domains (Navigation2, Maze, insulin dosing).
- Discovered violations show higher diversity and novelty compared to baseline approaches, with violations being farther from the training data distribution.
- The runtime safety shield effectively reduces violations during execution while maintaining reasonable task performance.
- The approach maintains efficiency and interpretability, with PAC-style guarantees ensuring completeness of violation detection under abstraction and data limitations.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A formal model checker (Storm) can detect certain violations of a PCTL safety specification, but if no violation is found, the policy cannot be deemed safe due to potential gaps in the abstraction and limited offline data.
- Mechanism: The CAPS method creates an interpretable policy graph from offline trajectories, which Storm checks against PCTL specifications. A found violation is conclusive; an absence of violations is not, necessitating further steps.
- Core assumption: The CAPS graph faithfully represents the RL policy and the PCTL specification accurately models the desired safety property.
- Evidence anchors:
  - [abstract] "If the model checker identifies a violation, it will return an interpretable counterexample trace... However, if no violation is detected, we cannot conclude satisfaction due to potential limitation in the abstraction and coverage of the offline dataset."
  - [section 4.1] "If safety specification φ is violated, Storm returns a counterexample...".
- Break condition: If the CAPS abstraction is fundamentally flawed (e.g., fails to capture key policy dynamics) or the PCTL specification is incomplete, this mechanism will fail to find true violations or produce misleading counterexamples.

### Mechanism 2
- Claim: A falsification strategy guided by risk and epistemic uncertainty can uncover violations missed by formal verification by actively exploring underrepresented, high-risk state-space regions.
- Mechanism: The method estimates risk from the model-checking process and epistemic uncertainty from an ensemble of encoders. It then uses these estimates to select high-risk seed states and perform targeted rollouts, mutating states when uncertainty is low to find novel, risky regions.
- Core assumption: The epistemic uncertainty estimate is a reliable proxy for identifying states where the policy's behavior is poorly understood and potentially dangerous.
- Evidence anchors:
  - [abstract] "...we estimate associated risk during model checking to guide a falsification strategy that prioritizes searching in high-risk states and regions underrepresented in the trajectory dataset."
  - [section 4.2] "We then apply a gradient-based mutation to push toward underexplored, risky regions...".
- Break condition: If the uncertainty estimator is poorly calibrated or the risk metric does not correlate with actual safety violations, the falsification process will be misdirected, wasting exploration budget on low-risk or already-well-understood states.

### Mechanism 3
- Claim: A lightweight runtime safety shield, guided by a risk critic, can mitigate failures by switching to a fallback policy when predicted risk exceeds a threshold.
- Mechanism: A risk critic $Q_{risk}(s, a)$ is trained to estimate the expected cumulative safety cost. During execution, if this value for an action exceeds a set threshold $\epsilon$, the shield overrides the action with one from a pre-trained safe policy $\pi_{safety}$.
- Core assumption: The risk critic provides sufficiently accurate real-time risk estimates, and the safe policy is available and effective in the identified high-risk states.
- Evidence anchors:
  - [abstract] "...we incorporate a lightweight safety shield that switches to a fallback policy at runtime when such a risk exceeds a threshold..."
  - [section 4.4] "When the estimated risk of an action exceeds a safety threshold $\epsilon$, the shield intervenes and overrides the task action...".
- Break condition: If the risk critic is inaccurate (providing false positives/negatives) or the safe policy is unsafe in the specific "risky" states identified, the shield will either be overly conservative or fail to prevent the targeted failures.

## Foundational Learning

- Concept: **Probabilistic Computation Tree Logic (PCTL)**
  - Why needed here: PCTL is the formal language used to specify the safety properties (e.g., "probability of reaching an unsafe state within 100 steps is at most 0.01") that the Storm model checker verifies against the abstracted policy.
  - Quick check question: Can you write a PCTL formula that specifies the probability of being in a "goal" state within 50 time steps, while never entering a "forbidden" state, is at least 0.95?

- Concept: **Abstraction of an RL Policy**
  - Why needed here: The paper's core idea is to convert an opaque, black-box RL policy into a finite, interpretable graph (using CAPS) so that standard model checking tools can be applied.
  - Quick check question: What is the primary trade-off when creating an abstract graph of a continuous state-space policy for verification purposes?

- Concept: **Epistemic Uncertainty in RL**
  - Why needed here: The falsification strategy relies on epistemic uncertainty to identify states that are underrepresented in the offline dataset and thus may hide unknown failure modes.
  - Quick check question: How does the paper quantify epistemic uncertainty, and why is this metric used to guide the search for violations?

## Architecture Onboarding

- Component map:
    Policy Abstraction Module (CAPS) -> Formal Verification Engine (Storm) -> Risk & Uncertainty Estimators -> Guided Falsification Loop -> Runtime Safety Shield

- Critical path: The workflow begins with the **CAPS module** abstracting the policy. The resulting graph goes to **Storm** for verification. If violations are found, the process yields interpretable counterexamples. If not, the **Guided Falsification Loop** is engaged, using the **Risk & Uncertainty Estimators** to drive targeted exploration for new violations. The output is a set of discovered violations.

- Design tradeoffs:
    - **Abstraction Fidelity vs. Tractability**: A more detailed CAPS graph captures the policy more accurately but increases the state space for Storm, making verification slower. A coarser graph is faster to check but may miss subtle violations.
    - **Exploration (Falsification) Budget**: More rollouts increase the chance of finding violations but consume more computational resources. This is a tunable parameter ($N$, $L$ in Alg 1).
    - **Shield Conservativeness**: The threshold $\epsilon$ for the safety shield determines how often it intervenes. A low $\epsilon$ is safer but may hinder task performance.

- Failure signatures:
    - **Falsification runs indefinitely without finding violations**: The risk and uncertainty signals may be misguiding the search. Check their correlation with known unsafe states.
    - **Storm finds violations that are false positives**: The CAPS abstraction may be too coarse or misrepresentative of the true policy. Inspect the abstract states in the counterexample.
    - **Safety shield overly hampers task performance**: The threshold $\epsilon$ is too low or the safe policy is not well-aligned with the task, leading to constant interventions.

- First 3 experiments:
  1. **Ablation on Abstraction**: Compare violation detection and action fidelity of CAPS against a simpler abstraction method (like KMeans) to justify the use of explainable abstraction.
  2. **Falsification Efficacy**: Run the full framework on a safety-critical task (e.g., Navigation2) and measure the *count* and *novelty* (distance from dataset) of violations found compared to baseline search methods (e.g., uncertainty-only, DRLFuzz).
  3. **Shield Impact**: Integrate the runtime safety shield and measure the reduction in detected violations compared to the unshielded policy, demonstrating its mitigation effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explainable abstraction methods like CAPS be extended to handle image-based or high-dimensional observation spaces while preserving both interpretability for humans and verification-friendliness for model checkers?
- Basis in paper: [explicit] The conclusion states: "CAPS does not currently support image-based observation spaces; future work will explore extending CAPS or developing new abstractions for high-dimensional inputs."
- Why unresolved: CAPS relies on CLTree clustering on state features, which assumes structured or tabular representations; image inputs require fundamentally different abstraction mechanisms that remain unclear.
- What evidence would resolve it: A modified CAPS framework applied successfully to vision-based RL benchmarks (e.g., Atari or robotic manipulation from pixels), with quantitative metrics on abstraction fidelity, verification completeness, and human interpretability.

### Open Question 2
- Question: How can runtime safety shields be designed to handle temporally extended safety violations with delayed effects, such as those in medical domains like insulin dosing?
- Basis in paper: [explicit] The conclusion notes: "We omit shielding results for the Dosing task, as its temporally extended safety violations and partially observable dynamics make runtime repair nontrivial." The RQ5 discussion adds this motivates "future work on designing temporally-aware shields for safety-critical medical applications."
- Why unresolved: Current threshold-based shields react to immediate risk estimates, but hypoglycemia in diabetes results from cumulative dosing decisions over hours, requiring predictive or memory-augmented shielding mechanisms.
- What evidence would resolve it: A temporally-aware shield architecture (e.g., using recurrent risk critics or predictive safety margins) demonstrating significant violation reduction in the Dosing environment or similar delayed-consequence domains.

### Open Question 3
- Question: How does verification completeness degrade as the abstraction quality decreases due to sparse offline data, and can adaptive falsification budgets compensate?
- Basis in paper: [inferred] The paper acknowledges that when model checking finds no violations, "we cannot conclude satisfaction due to potential limitation in the abstraction and coverage of the offline dataset." The PAC guarantee (Theorem) depends on assumptions including risk estimator accuracy within εr, but the relationship between data sparsity, abstraction fidelity, and falsification budget requirements remains unquantified.
- Why unresolved: While the framework provides PAC-style guarantees, the practical calibration of falsification budgets (N seed states, depth L) relative to offline dataset coverage gaps is empirically determined rather than theoretically derived.
- What evidence would resolve it: Systematic experiments varying offline dataset size and coverage, measuring the relationship between abstraction error rates, falsification budget requirements, and violation detection completeness.

## Limitations
- The CAPS abstraction quality and its impact on verification completeness is not fully characterized; the paper relies on heuristic clustering without rigorous bounds on abstraction error.
- Falsification effectiveness depends on the calibration of epistemic uncertainty estimates, but the paper does not provide uncertainty calibration metrics or sensitivity analysis to the K=5 ensemble size.
- The runtime safety shield's effectiveness is only demonstrated via reduced violation counts, not via actual safety improvement in the task (e.g., fewer constraint violations during execution).

## Confidence
- High: The paper's core architecture (CAPS + Storm + falsification loop) is clearly defined and the reported results show improvements over baselines in violation count and diversity.
- Medium: The claim that epistemic uncertainty guides effective exploration is supported by results but lacks ablation on uncertainty calibration and ensemble size.
- Medium: The claim that the runtime shield mitigates failures is supported by reduced violation counts, but the actual safety benefit during execution is not demonstrated.

## Next Checks
1. **Abstraction Fidelity Analysis**: Conduct a quantitative study comparing CAPS's action fidelity against a simpler abstraction (e.g., KMeans) across multiple environments to isolate the impact of explainable abstraction.
2. **Uncertainty Calibration Study**: Evaluate the epistemic uncertainty estimator's calibration by measuring its correlation with actual safety violation density and sensitivity to ensemble size K.
3. **Shield Effectiveness in Execution**: Design an experiment where the shielded policy is executed in a safety-critical environment, measuring both task performance and actual constraint violations compared to the unshielded policy.