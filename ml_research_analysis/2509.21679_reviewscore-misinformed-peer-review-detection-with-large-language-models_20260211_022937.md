---
ver: rpa2
title: 'ReviewScore: Misinformed Peer Review Detection with Large Language Models'
arxiv_id: '2509.21679'
source_url: https://arxiv.org/abs/2509.21679
tags:
- review
- premises
- argument
- point
- reconstruction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces REVIEWSCORE, a method to detect misinformed
  peer reviews by identifying questions answerable by the paper or weaknesses with
  incorrect premises. It uses LLMs to reconstruct arguments into premises and conclusions,
  then evaluates their factuality.
---

# ReviewScore: Misinformed Peer Review Detection with Large Language Models

## Quick Facts
- **arXiv ID:** 2509.21679
- **Source URL:** https://arxiv.org/abs/2509.21679
- **Reference count:** 40
- **Primary result:** REVIEWSCORE detects misinformed peer reviews with moderate human-LLM agreement (0.4–0.5 F1, 0.3–0.4 Kappa) by reconstructing arguments into premises and evaluating their factuality.

## Executive Summary
This paper introduces REVIEWSCORE, a method to detect misinformed peer reviews by identifying questions answerable by the paper or weaknesses with incorrect premises. The approach uses LLMs to reconstruct arguments into explicit premise-conclusion structures, then evaluates their factuality using paper context and author responses. A human-annotated dataset of 657 review points from ICLR papers was created to validate the approach. Eight state-of-the-art LLMs were tested, showing moderate agreement with human annotators. Premise-level evaluation outperformed weakness-level evaluation, and providing author responses improved model performance by up to 22% in F1 score.

## Method Summary
The REVIEWSCORE method detects misinformed peer reviews through a multi-step pipeline: (1) splitting reviews into questions, claims, and arguments; (2) reconstructing arguments into explicit premises and conclusions using an LLM and validating with a SAT solver; (3) scoring premise factuality by selecting appropriate knowledge bases (paper, annotator knowledge, or referred papers); and (4) aggregating premise scores via logical conjunction or weighted average by untrivialness. The approach leverages LLM-as-a-judge at temperature=0 with deterministic decoding and evaluates multiple SOTA LLMs on a human-annotated dataset of 657 review points from 40 ICLR papers.

## Key Results
- REVIEWSCORE achieves 0.4–0.5 F1 and 0.3–0.4 Kappa agreement with human annotators for detecting misinformed reviews
- Premise-level evaluation shows significantly higher agreement (2.48× higher F1) than weakness-level evaluation
- Providing author responses improves F1 scores by up to 22.3% (0.367 to 0.449)
- Proprietary LLMs outperform open-sourced models on factuality tasks, though open models remain competitive on question detection

## Why This Works (Mechanism)

### Mechanism 1
Decomposing arguments into explicit and implicit premises improves factuality detection compared to evaluating weaknesses holistically. An LLM extracts verbatim conclusions and reasons, then reconstructs them into a premise-conclusion structure with deductive validity enforced via a SAT solver (Z3). The solver verifies that premises formally imply the conclusion. Faithfulness is then checked by translating formalized logic back to natural language and comparing against the original argument. This surfaces implicit false premises that humans miss when evaluating surface-level text.

### Mechanism 2
Providing author responses as additional context improves model agreement with human factuality judgments. Author responses clarify ambiguities, correct reviewer misunderstandings, and point to specific paper sections. LLMs use this as auxiliary context when scoring premise factuality, reducing "unanswerable" classifications. This works because author responses are themselves largely factual and relevant to the review point being evaluated.

### Mechanism 3
Multi-source knowledge base selection enables context-appropriate factuality judgment. For each premise, the model selects which knowledge source to consult: (1) the submitted paper for paper-specific claims, (2) annotator knowledge for general-domain facts, or (3) referred papers for external citations. This prevents penalizing premises that require domain knowledge not in the paper, allowing the model to distinguish when external knowledge is appropriate versus when a claim should be paper-grounded.

## Foundational Learning

- **Concept: First-order logic (FOL) and SAT solving**
  - **Why needed here:** The argument reconstruction engine formalizes natural language into FOL formulas and uses Z3 to verify deductive validity. Without this, reconstructed arguments may be logically incoherent.
  - **Quick check question:** Given premises "If A then B" and "A", can you derive "B" using a SAT solver?

- **Concept: Inter-annotator agreement metrics (Krippendorff's Alpha, Quadratic Weighted Kappa)**
  - **Why needed here:** The paper relies on human-model agreement to validate the approach. Understanding these metrics is essential to interpret whether "moderate agreement" (0.3–0.4 Kappa) is acceptable.
  - **Quick check question:** Why is Kappa preferred over raw accuracy when class distributions are imbalanced?

- **Concept: Premise-conclusion argument structure**
  - **Why needed here:** The core innovation is reconstructing reviews into explicit premises and conclusions. Understanding informal logic is prerequisite to designing or debugging the reconstruction engine.
  - **Quick check question:** What makes an argument "valid" vs. "faithful" in critical thinking literature?

## Architecture Onboarding

- **Component map:** Review Point Extractor -> Argument Reconstruction Engine -> Factuality Scorer -> Aggregation Module
- **Critical path:** Argument reconstruction → FOL formalization → SAT validation → faithfulness check → premise factuality scoring → aggregation. The SAT validation step is the correctness gate; the faithfulness check is the semantic gate.
- **Design tradeoffs:** Logical conjunction is brittle (one annotation error propagates) while weighted average is more robust but loses strict logical semantics. Excluding figures/appendices reduces API cost but causes parsing failures. Proprietary LLMs yield higher agreement; open models lag on ArgScore but are competitive on QScore.
- **Failure signatures:** Low ClaimScore agreement indicates claims lack supporting premises; over-classification of "misinformed" suggests models over-apply external knowledge; circular proofs indicate reconstruction errors; faithfulness score < 4 means reconstruction diverges from original argument.
- **First 3 experiments:**
  1. **Reconstruction quality baseline:** Run argument reconstruction engine on 50 arguments without feedback loops; measure validity (SAT solver) and faithfulness (human annotation); target validity >0.9, faithfulness >4.0/5.
  2. **Ablation on knowledge source selection:** Force all premises to use only the submitted paper as knowledge base; compare agreement with human labels vs. default multi-source selection; hypothesis: drop in agreement for domain-knowledge premises.
  3. **Author response impact by review point type:** Stratify Table 4 results by questions vs. arguments vs. claims; identify which category benefits most from author responses; verify questions benefit most as prior work suggests.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does providing full multimodal context (figures, appendices) impact the accuracy of misinformed review detection compared to the text-only approach?
  - **Basis in paper:** The authors explicitly state in the Limitations section that they excluded figures and appendices to save API costs, acknowledging this leads to information loss.
  - **Why unresolved:** The current experimental setup relies solely on parsed text and tables, failing to capture visual evidence relevant to review claims.
  - **What evidence would resolve it:** Experiments comparing model performance when ingestions include full PDF rendering or image patches versus the current text-only baseline.

- **Open Question 2:** Can the detection framework be adapted to handle or correct noise in the human ground-truth annotations, given that expert evaluators sometimes disagree or err?
  - **Basis in paper:** Appendix B explicitly notes that "human evaluations are sometimes incorrect" and lists this as a limitation of the work.
  - **Why unresolved:** The current methodology treats median human annotations as the absolute ground truth for training and evaluation.
  - **What evidence would resolve it:** Developing a robust loss function or label aggregation technique that accounts for annotator fallibility and measures performance against a "cleaned" or probabilistic ground truth.

- **Open Question 3:** Does the integration of specialized reasoning models (e.g., o1-style architectures) improve the validity and faithfulness of the automatic argument reconstruction engine?
  - **Basis in paper:** Page 10 notes that reconstruction quality improved when upgrading the base model, though reasoning models were excluded from the judge evaluation.
  - **Why unresolved:** It is unclear if extended chain-of-thought reasoning specifically aids the complex logic required for valid and faithful reconstruction more than standard LLMs.
  - **What evidence would resolve it:** Benchmarking the reconstruction engine using models with explicit test-time compute scaling capabilities.

## Limitations
- **Limited dataset size:** 657 review points is relatively small for LLM-based evaluation
- **Moderate human-LLM agreement:** 0.3–0.5 F1 suggests the task remains challenging and results may not generalize
- **Text-only processing:** Excluding figures and appendices from paper context reduces API cost but causes information loss and parsing failures

## Confidence
- **High:** The core methodology (argument reconstruction → SAT validation → premise factuality scoring) is technically sound and reproducible
- **Medium:** Empirical results show moderate human-LLM agreement and improvements from author responses, but generalization to other venues/conferences is unproven
- **Low:** The claim that premise-level evaluation is inherently superior to weakness-level evaluation lacks ablation against other factuality frameworks

## Next Checks
1. Replicate the human-LLM agreement study with a held-out test set to verify that the 0.4–0.5 F1 range is consistent
2. Test the approach on peer reviews from other conferences (e.g., NeurIPS, CVPR) to assess domain transferability
3. Conduct an ablation study: compare REVIEWSCORE performance with a baseline that skips argument reconstruction and directly evaluates weaknesses for factuality