---
ver: rpa2
title: 'Search Self-play: Pushing the Frontier of Agent Capability without Supervision'
arxiv_id: '2510.18821'
source_url: https://arxiv.org/abs/2510.18821
tags:
- search
- question
- training
- proposer
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Search Self-play (SSP) introduces a self-evolving training framework
  for deep search agents by enabling a single LLM to act as both a question proposer
  and a problem solver. The proposer generates challenging, verifiable questions through
  multi-turn search, while the solver attempts to answer them.
---

# Search Self-play: Pushing the Frontier of Agent Capability without Supervision

## Quick Facts
- arXiv ID: 2510.18821
- Source URL: https://arxiv.org/abs/2510.18821
- Authors: Hongliang Lu, Yuhang Wen, Pengyu Cheng, Ruijin Ding, Jiaqi Guo, Haotian Xu, Chutian Wang, Haonan Chen, Xiaoxi Jiang, Guanjun Jiang
- Reference count: 40
- Primary result: Qwen2.5-7B-Instruct improves by 8.0 points average across 7 benchmarks using self-evolving proposer-solver framework

## Executive Summary
Search Self-play (SSP) introduces a self-evolving training framework for deep search agents where a single LLM acts as both question proposer and problem solver. The system generates challenging, verifiable questions through multi-turn search, with correctness ensured by a retrieval-augmented generation (RAG) pipeline. Through competitive and cooperative dynamics, both roles co-evolve without human supervision, achieving substantial performance gains across multiple benchmarks.

## Method Summary
SSP enables autonomous capability improvement by creating an adversarial game between two roles: a proposer that generates challenging questions and a solver that attempts to answer them. The proposer uses backward reasoning from ground-truth answers and multi-turn search to create questions, while the solver uses multi-turn exploration to find answers. Questions must pass rule-based filters and RAG verification, where the solver must answer correctly using only the proposer's collected documents plus noise. The game uses asymmetric reinforcement learning: REINFORCE for the proposer (rewarding failures) and GRPO for the solver (rewarding successes), with periodic replay buffer resets to prevent overfitting.

## Key Results
- Qwen2.5-7B-Instruct improves by 8.0 points average across 7 benchmarks
- Qwen2.5-32B-Instruct improves by 3.4 points average
- Works in both from-scratch and continual training settings
- Co-evolution outperforms fixed-opponent variants that show reward saturation

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Co-evolution via Min-Max Objective
The proposer and solver compete in a zero-sum game where the proposer is rewarded when the solver fails (reward = 1 - solver_success_rate). This creates an implicit curriculum where task difficulty automatically adapts—when the solver improves, the proposer must generate harder questions to maintain its reward, preventing the overfitting seen in fixed-opponent training.

### Mechanism 2: RAG-Based Verification as Cooperative Constraint
Questions are verified by requiring the solver to answer correctly using only the proposer's collected search results plus noise documents. This ensures questions are solvable with complete evidence from the proposer's trajectory, converting the cooperative objective into a hard filter via rejection sampling rather than joint optimization.

### Mechanism 3: Asymmetric RL with Role-Specific Algorithms
The proposer uses REINFORCE with single-sample rewards to encourage exploration of question space, while the solver uses GRPO with 5 trajectories per question and group mean baseline to reduce variance. This balances exploration (proposer) against stable credit assignment (solver).

## Foundational Learning

- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Needed because SSP builds on RLVR but removes the need for human-curated ground truth by having the model generate its own verifiable questions. Quick check: Can you explain why RLVR requires "verifiable" rewards rather than learned reward models?

- **Self-play in RL (min-max games, AlphaGo Zero)**: Needed because SSP treats question-answering as a zero-sum game between proposer and solver, adapting AlphaGo-style self-play to language agents. Quick check: What failure mode does a purely adversarial self-play setup risk without cooperative constraints?

- **Group Relative Policy Optimization (GRPO)**: Needed because solver training uses GRPO to reduce variance by comparing trajectories within groups rather than against a separate baseline model. Quick check: How does GRPO differ from PPO in its baseline computation?

## Architecture Onboarding

- **Component map**: Ground-truth answer → Proposer multi-turn search → Question generation → Rule filter → RAG verification (solver attempts with proposer's docs + 4 noise) → Valid questions → Solver multi-turn exploration (n=5) → Reward computation → Policy updates

- **Critical path**: The proposer generates questions via multi-turn search, which are filtered and verified through RAG. Valid questions are then used to train the solver through multiple exploration attempts, with rewards computed based on correctness.

- **Design tradeoffs**: GRPO-GRPO vs RF-GRPO offers higher accuracy (50.9 vs 49.5) but 6× generation cost; paper chooses RF-GRPO for efficiency. Replay buffer full-reuse vs periodic-reset: full-reuse causes solver overfitting; periodic reset (every 10 steps) balances density and novelty. Noise documents (0 vs 4 vs 7): 0 allows RAG-hacking; 4 optimal; 7 causes confusion.

- **Failure signatures**: Reward collapse occurs when proposer policy entropy rises and valid question rate approaches zero (caused by punitive rewards for format errors). Solver overfitting happens when evaluation accuracy drops while in-game reward stays high (caused by static question pool). RAG-hacking occurs when questions are easy with specific documents but ambiguous for general search.

- **First 3 experiments**:
  1. Ablate RAG verification: Train without RAG filter on Qwen2.5-7B-Instruct; expect ~5-10 point average drop, especially on GeneralQA
  2. Compare fixed-opponent vs co-evolution: Train solver-only and proposer-only variants; plot in-game reward and held-out evaluation over 150 steps; expect solver-only to saturate early and degrade
  3. Test noise document sensitivity: Run with 0, 1, 4, 7 noise documents; measure GeneralQA and Multi-HopQA separately; expect optimal at 4 with degradation at extremes

## Open Questions the Paper Calls Out
None

## Limitations
- RAG verification reliability depends heavily on retrieval system robustness to noisy documents and solver's ability to avoid exploiting document bias
- Self-evolving dynamics may be sensitive to task domain characteristics and may not transfer to non-QA domains
- 6× slowdown of GRPO-GRPO versus RF-GRPO represents significant computational overhead for marginal accuracy gains

## Confidence

- **High confidence**: The core architecture and training framework is well-defined and reproducible. The use of RLVR with proposer-generated questions is novel and theoretically sound.
- **Medium confidence**: Empirical results showing 8.0-point improvement are compelling but based on a relatively small sample size (7 benchmarks).
- **Medium confidence**: Mechanism explanations for adversarial co-evolution are plausible given ablation results, but exact reasons why self-play outperforms fixed-opponent training remain partially speculative.

## Next Checks
1. Apply SSP to a non-QA domain (e.g., code generation or mathematical reasoning) to measure whether self-evolving dynamics transfer or collapse
2. Systematically corrupt the RAG retrieval with adversarial documents to measure how easily the verification filter can be fooled and whether this corrupts the adversarial game
3. Run SSP for 500+ training steps to monitor whether proposer and solver continue to improve or enter a degenerative cycle where questions become either trivially verifiable or unsolvable