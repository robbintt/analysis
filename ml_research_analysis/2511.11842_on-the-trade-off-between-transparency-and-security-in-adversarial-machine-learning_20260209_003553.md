---
ver: rpa2
title: On the Trade-Off Between Transparency and Security in Adversarial Machine Learning
arxiv_id: '2511.11842'
source_url: https://arxiv.org/abs/2511.11842
tags:
- adversarial
- attacks
- surrogate
- game
- defended
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the trade-off between transparency and
  security in adversarial machine learning. The authors focus on transferable adversarial
  example attacks, where attackers generate adversarial perturbations using surrogate
  models to fool a defender's target model.
---

# On the Trade-Off Between Transparency and Security in Adversarial Machine Learning

## Quick Facts
- **arXiv ID**: 2511.11842
- **Source URL**: https://arxiv.org/abs/2511.11842
- **Reference count**: 40
- **Primary result**: Transparency about defense status can reduce adversarial accuracy by up to 1.04% on CIFAR10 and 0.64% on ImageNet by enabling attackers to match surrogate defense types.

## Executive Summary
This paper investigates how transparency about AI defense mechanisms affects security against transferable adversarial attacks. Using a large-scale empirical evaluation across 181 models and 9 attack methods, the authors demonstrate that attackers are significantly more successful when they match the defender's choice of using defended versus undefended models. They model this as both Nash and Stackelberg games, quantifying the security cost of transparency. The findings reveal that existing benchmarks substantially underestimate the potency of transferable attacks against defended models, and suggest that mixed defense strategies can enhance robustness.

## Method Summary
The authors conduct a comprehensive empirical study evaluating transferable adversarial attacks across CIFAR-10 and ImageNet datasets. They evaluate 9 different attack methods using both defended and undefended surrogate models against all target models, excluding the surrogate itself. Defended models are selected from RobustBench and ranked by white-box AutoAttack robustness (worst/median/best). The analysis is then framed as game-theoretic problems: a 2×2 Surrogate game and an extended Attack & Surrogate game, where Nash equilibria are computed via vertex enumeration and Stackelberg equilibria are solved analytically with the defender as leader. Payoff differences between equilibria quantify the transparency cost.

## Key Results
- Attackers achieve higher success rates when matching the defender's defense status (defended vs undefended surrogate models)
- Transparency reduces adversarial accuracy by up to 1.04% on CIFAR10 and 0.64% on ImageNet
- Existing benchmarks underestimate transferable attack potency against defended models by up to 3.73×
- Mixed defense strategies can enhance robustness against adaptive attacks

## Why This Works (Mechanism)
The core mechanism is that transferable adversarial examples are more effective when the surrogate model's architecture and defense characteristics closely match those of the target model. When defenders reveal their use of defenses, attackers can select appropriate surrogate models that incorporate similar defenses, dramatically improving attack success rates. This violates the common assumption that transferable attacks are less effective against defended models.

## Foundational Learning

**Transferable adversarial attacks**: Why needed: Core threat model being evaluated. Quick check: Can an attacker generate perturbations on one model that transfer to another without white-box access?

**Game theory (Nash/Stackelberg)**: Why needed: Framework for analyzing strategic interactions between attackers and defenders. Quick check: Does the equilibrium analysis correctly identify optimal strategies?

**RobustBench models**: Why needed: Standardized benchmark for evaluating defenses. Quick check: Are the defended models correctly ranked by white-box robustness?

**l∞ norm constraints**: Why needed: Standard adversarial attack metric. Quick check: Are perturbations properly bounded during attack generation?

## Architecture Onboarding

**Component map**: Datasets (CIFAR-10, ImageNet) → Models (98 undefended + 92 defended) → Surrogate models (18) → Attacks (9 methods) → Evaluation (accuracy degradation) → Game theory analysis

**Critical path**: Model selection → Attack execution → Accuracy measurement → Payoff matrix construction → Equilibrium computation

**Design tradeoffs**: Binary defense choice (defended/undefended) vs. more nuanced defense strategies; empirical evaluation vs. theoretical guarantees

**Failure signatures**: Memory errors with complex attacks on specialized architectures; white-box attacks when surrogate attacks itself; incorrect ranking of defended models affecting surrogate selection

**First experiments**: 1) Verify transferability improvement when matching defense status using simple attacks like BIA; 2) Test payoff matrix construction with a small subset of models; 3) Validate Nash equilibrium computation on a 2×2 game with known solution

## Open Questions the Paper Calls Out

**Open Question 1**: How does using mixed strategies with orthogonal defenses impact robustness compared to binary defended/undefended choices? The authors note this is outside the current scope but could offer better improvements.

**Open Question 2**: Does the transparency-security trade-off persist in non-image modalities like NLP? The current analysis is limited to image classification.

**Open Question 3**: In what specific contexts does transparency strengthen trust enough to justify the security cost? The paper quantifies security costs but doesn't evaluate corresponding trust gains.

## Limitations
- Does not disclose specific model identities or attack hyperparameters, making exact replication difficult
- Game-theoretic analysis assumes rational actors with perfect information about defense implementations
- Limited to image classification; results may not generalize to other modalities

## Confidence

**High**: Finding that matching surrogate defense status to target defense status improves attack success (directly observable from accuracy metrics)

**Medium**: Quantified transparency costs of 1.04% (CIFAR10) and 0.64% (ImageNet) due to specification gaps

**Low**: Stackelberg equilibrium predictions without validation of defender's pure strategy commitments

## Next Checks

1. Replicate core finding using clearly specified surrogate models (e.g., "ResNet50 undefended" vs "TRADES defended") to verify matched defense status improves transferability

2. Test whether 3.73× underestimation of attack potency against defended models holds using alternative defense benchmarks beyond RobustBench

3. Validate game-theoretic predictions by conducting ablation study where defenders randomize between defended and undefended models to measure robustness gains compared to predicted mixed-strategy equilibrium