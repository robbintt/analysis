---
ver: rpa2
title: 'KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold
  Networks with Accelerated Failure Time Analysis'
arxiv_id: '2512.20305'
source_url: https://arxiv.org/abs/2512.20305
tags:
- data
- kan-aft
- survival
- time
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KAN-AFT integrates Kolmogorov-Arnold Networks with the Accelerated
  Failure Time model to address the limitations of traditional survival analysis models.
  It handles right-censored data using Buckley-James, IPCW, and Transform methods
  while learning complex nonlinear relationships between covariates and survival time.
---

# KAN-AFT: An Interpretable Nonlinear Survival Model Integrating Kolmogorov-Arnold Networks with Accelerated Failure Time Analysis

## Quick Facts
- arXiv ID: 2512.20305
- Source URL: https://arxiv.org/abs/2512.20305
- Reference count: 3
- Primary result: KAN-AFT achieves C-index values of 0.73-0.88 on real datasets while providing interpretable symbolic expressions

## Executive Summary
KAN-AFT extends the Accelerated Failure Time model by replacing its linear predictor with Kolmogorov-Arnold Networks that learn nonlinear relationships between covariates and survival time. The model handles right-censored data through three strategies (Buckley-James, IPCW, and Transform methods) while maintaining interpretability by converting learned spline functions into closed-form symbolic equations. On both synthetic and real-world datasets, KAN-AFT demonstrates performance comparable to or better than DeepAFT, with the added benefit of transparent mathematical expressions of covariate effects.

## Method Summary
KAN-AFT combines Kolmogorov-Arnold Networks with Accelerated Failure Time analysis to model survival data with right censoring. The core innovation replaces the linear AFT predictor with learnable univariate spline functions that can capture complex nonlinear relationships. Three censoring strategies handle incomplete data: Buckley-James iterative imputation, IPCW weighting, and Transform methods. Regularization (L1 on activations and entropy) enables pruning of irrelevant connections, while post-hoc symbolic regression converts surviving splines into interpretable closed-form expressions. The model uses a shallow architecture [n_cov, 1] to preserve interpretability while achieving competitive performance across synthetic and real datasets.

## Key Results
- Achieves C-index values ranging from 0.73 to 0.88 on real-world datasets (PBC, GBSG, Veteran, Heart failure)
- Outperforms traditional AFT models and matches or exceeds DeepAFT performance
- Successfully captures nonlinear effects including quadratic, exponential, and periodic relationships
- Converts learned spline functions into closed-form symbolic equations (e.g., "log T = 0.5113z₁ - 0.3081z₂ + 0.9483z₃")
- Demonstrates lower MSE than competing models on benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Learnable Spline Activation Functions Replace Linear Predictors
Replacing the traditional AFT linear predictor with KAN's learnable univariate spline functions enables modeling of arbitrary nonlinear covariate-time relationships while preserving interpretability. The model transforms log(T_i) = KAN(z_i) + σε_i, where each covariate z_ij passes through a unique B-spline activation function φ₁,₁,j(·). These splines can approximate smooth functions (quadratic, exponential, periodic) during training, then be converted to symbolic form post-hoc.

### Mechanism 2: Three-Strategy Censoring Handling for Right-Censored Data
The three censoring strategies (Buckley-James iterative imputation, IPCW weighting, Transform method) each provide different bias-variance tradeoffs when training KANs on incomplete survival data. Buckley-James iteratively imputes censored residuals, IPCW weights uncensored observations inversely by their censoring probability, and Transform maps observed times to adjusted values enabling standard regression.

### Mechanism 3: Regularization-Driven Sparsity Enables Symbolic Regression
L₁ regularization on activation function magnitudes and spline coefficients, combined with entropy regularization, enables pruning of irrelevant connections and produces simple enough spline functions for symbolic regression. The total loss includes terms that prune weak connections, promote modularity, and smooth spline shapes, resulting in parsimonious symbolic expressions after training.

## Foundational Learning

- **Concept: Accelerated Failure Time (AFT) Model**
  - Why needed here: KAN-AFT extends this foundational framework; understanding the time-scaling interpretation (log T = μ + β'z + σε) is essential before grasping the nonlinear extension.
  - Quick check question: Given an acceleration factor exp(β'x) = 1.5, does survival time increase or decrease by 50%?

- **Concept: Right-Censoring in Survival Data**
  - Why needed here: All three censoring methods (B-J, IPCW, Transform) address the fundamental problem that event times are only partially observed; understanding why standard regression fails is prerequisite.
  - Quick check question: If δ_i = 0, do we know the exact event time, only a lower bound, or only an upper bound?

- **Concept: B-Spline Basis Functions**
  - Why needed here: The KAN activation functions are parameterized as B-spline expansions; understanding how smooth curves are built from piecewise polynomial basis functions clarifies the approximation power.
  - Quick check question: What happens to spline flexibility as the number of grid intervals G increases? What tradeoff does this introduce?

## Architecture Onboarding

- **Component map:**
  Input covariates z → [KAN Layer: n_cov splines φ(·)] → Sum → log(T) prediction → exp() → Ŝ
                                    ↓
                         [Regularization: L₁ on |Φ|, |C| + entropy]
                                    ↓
                         [Post-hoc: Symbolic regression on learned φ]

- **Critical path:**
  1. Initialize KAN with shallow architecture [n_cov, 1]
  2. Choose censoring strategy based on data characteristics
  3. Train with combined loss (survival + regularization)
  4. Prune near-zero connections using |Φ| threshold
  5. Apply symbolic regression to surviving splines
  6. Validate C-index on held-out test set before deploying symbolic formula

- **Design tradeoffs:**
  - **Buckley-James vs. IPCW vs. Transform**: B-J highest accuracy but iterative; IPCW fast but unstable under heavy censoring; Transform simpler but requires α-tuning
  - **Shallow vs. Deep KAN**: Paper uses [n_cov, 1] for interpretability; deeper architectures could capture interactions but complicate symbolic extraction
  - **Regularization strength**: Higher values improve interpretability but risk underfitting; paper does not report hyperparameter sensitivity

- **Failure signatures:**
  - C-index near 0.5: Model failed to learn predictive signal; check for data leakage or insufficient training
  - Overly complex symbolic expressions: Regularization too weak; increase λ₁ or prune more aggressively
  - IPCW loss diverges: High censoring rate causing numerical instability; switch to Buckley-James or Transform
  - Large train-test C-index gap: Overfitting; increase regularization or reduce spline grid resolution

- **First 3 experiments:**
  1. **Reproduce linear synthetic benchmark**: Generate data from log T = 0.5z₁ - 0.3z₂ + z₃ + ε with ~30% censoring; verify recovered coefficients within 10% of true values and C-index > 0.85
  2. **Ablation on censoring methods**: On PBC dataset, compare all three methods (B-J, IPCW, Transform) with identical hyperparameters; document C-index, MSE, and training time
  3. **Symbolic recovery stress test**: Generate data with known nonlinear effects (e.g., log T = 0.5z₁² + 0.3 exp(z₂)); verify symbolic regression correctly identifies functional forms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the KAN-AFT framework be theoretically and algorithmically extended to handle time-dependent covariates while preserving its symbolic interpretability?
- Basis in paper: [explicit] The authors state in the Discussion that "Future work could focus on extending KAN-AFT to handle time-dependent covariates."
- Why unresolved: The current KAN-AFT formulation assumes fixed baseline covariates z_i to predict a single log-survival time. Time-varying covariates require modeling the relationship between feature trajectories and event times, which complicates the single closed-form symbolic output the model currently provides.

### Open Question 2
- Question: Can formal statistical inference procedures, such as confidence intervals and p-values, be developed for the learned symbolic coefficients in KAN-AFT?
- Basis in paper: [explicit] The authors identify the need for "developing formal inference procedures for the learned functional effects" as a direction for future work.
- Why unresolved: While KAN-AFT extracts symbolic equations, it currently lacks the rigorous uncertainty quantification standard in parametric AFT models. Deriving the asymptotic distribution for spline-based neural network weights to construct valid confidence intervals remains an open theoretical challenge.

### Open Question 3
- Question: How does the performance and interpretability of KAN-AFT scale in high-dimensional "omics" settings where the number of covariates significantly exceeds the sample size?
- Basis in paper: [explicit] The paper lists "high-dimensional settings" as a specific area for future extension.
- Why unresolved: The current study evaluates KAN-AFT on datasets with a small to moderate number of covariates. It is unclear if the B-spline parameterization and symbolic regression pipeline remain computationally efficient and resistant to overfitting when p >> n.

## Limitations
- Hyperparameter sensitivity: The paper does not report sensitivity analyses for regularization weights, spline grid resolution, or censoring method selection criteria
- Computational costs: No runtime or iteration counts provided for the Buckley-James method, making practical feasibility assessment difficult
- Symbolic regression fidelity: Conversion process from learned splines to closed-form expressions lacks validation against ground truth in real datasets

## Confidence
- **High confidence**: Performance improvements over traditional AFT models (C-index 0.73-0.88) are well-supported by comparison tables across multiple datasets
- **Medium confidence**: The three censoring strategies are theoretically sound and show performance variation, but independent validation of DeepAFT implementation details is lacking
- **Low confidence**: The symbolic interpretability claim requires further validation - while synthetic examples show recovered formulas, real-world applications lack ground truth for verification

## Next Checks
1. **Hyperparameter ablation study**: Systematically vary λ₁, λ₂, and G across datasets to identify robust default settings and quantify performance sensitivity
2. **Runtime benchmarking**: Measure wall-clock time for each censoring method (B-J, IPCW, Transform) on PBC and GBSG datasets to establish practical tradeoffs
3. **Symbolic fidelity test**: Generate synthetic data with known nonlinear effects, train KAN-AFT, and quantitatively compare recovered symbolic expressions against true formulas using distance metrics (e.g., integrated squared error)