---
ver: rpa2
title: Fast and close Shannon entropy approximation
arxiv_id: '2505.14234'
source_url: https://arxiv.org/abs/2505.14234
tags:
- entropy
- approximation
- learning
- gradient
- shannon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational cost and numerical instability
  associated with Shannon entropy (SE) calculations, particularly due to the singularity
  of SE's gradient at zero probabilities. The authors propose Fast Entropy Approximation
  (FEA), a non-singular rational approximation of Shannon entropy and its gradient.
---

# Fast and close Shannon entropy approximation

## Quick Facts
- arXiv ID: 2505.14234
- Source URL: https://arxiv.org/abs/2505.14234
- Reference count: 17
- Proposed FEA achieves 50% computational speedup with mean absolute error of 10^-3

## Executive Summary
This paper addresses the computational cost and numerical instability associated with Shannon entropy (SE) calculations, particularly due to the singularity of SE's gradient at zero probabilities. The authors propose Fast Entropy Approximation (FEA), a non-singular rational approximation of Shannon entropy and its gradient. FEA achieves a mean absolute error of 10^-3, approximately 20 times lower than state-of-the-art methods, while requiring only 5-6 elementary computational operations compared to tens for existing fast algorithms. The method is Lipschitz-continuous with a relatively small Lipschitz constant (L ≤ 31.616), enabling faster convergence in optimization.

## Method Summary
The Fast Entropy Approximation (FEA) method introduces a rational approximation function for Shannon entropy that remains non-singular at zero probabilities. The approximation uses a carefully constructed rational function with numerator and denominator coefficients optimized to minimize error across the probability interval [0,1]. FEA provides both the entropy value and its gradient through the same approximation framework, ensuring computational efficiency. The method achieves Lipschitz continuity with L ≤ 31.616, which improves optimization convergence rates. Implementation tests demonstrate approximately 50% faster computation across Python, Julia, and MATLAB platforms while maintaining mean squared approximation error of 3 × 10^-6.

## Key Results
- FEA computes entropy approximately 50% faster than standard methods across Python, Julia, and MATLAB platforms
- Mean squared approximation error of 3 × 10^-6 with mean absolute error of 10^-3
- Two to three orders of magnitude faster model reduction in machine learning feature selection tasks while maintaining comparable model quality
- Only 5-6 elementary computational operations required versus tens for existing fast algorithms

## Why This Works (Mechanism)
FEA works by replacing the singular logarithmic function in Shannon entropy with a smooth rational approximation that maintains accuracy while eliminating numerical instabilities. The rational function is designed to have well-behaved derivatives across the entire probability domain, providing a Lipschitz-continuous gradient that improves optimization convergence. By reducing the computational complexity from O(n log n) to O(n) operations for entropy calculation, FEA achieves significant speedups without sacrificing accuracy.

## Foundational Learning
- Shannon entropy formula: H(X) = -Σ p(x) log p(x) - required for understanding the mathematical foundation being approximated
- Rational function approximation: why needed for creating smooth, computationally efficient substitutes for logarithmic functions
- Lipschitz continuity: why needed for ensuring stable gradient behavior in optimization algorithms
- Numerical stability: why needed for handling near-zero probability values without computational errors
- Gradient-based optimization: why needed for understanding how smoother gradients improve convergence in machine learning

## Architecture Onboarding
Component map: Probability input -> FEA rational function -> Entropy value + Gradient
Critical path: Input probability vector → FEA evaluation → Entropy computation → Optimization step
Design tradeoffs: Accuracy vs. computational speed, smooth vs. exact gradient behavior
Failure signatures: Large approximation errors near probability boundaries, numerical instability with very small values
First experiments:
1. Verify FEA speedup on uniform and skewed probability distributions
2. Test FEA gradient continuity across the [0,1] interval
3. Benchmark FEA vs exact entropy in simple logistic regression optimization

## Open Questions the Paper Calls Out
### Open Question 1
Can the FEA rational approximation be effectively extended to matrix-based von Neumann entropy for quantum computing applications?
The abstract lists von Neumann entropy as a key target area alongside Shannon entropy, but the method and validation focus exclusively on scalar probability inputs. The proposed approximation is a scalar rational function; extending it to the spectral domain of density matrices requires handling matrix logarithms and eigenvalue decompositions not discussed in the current formulation. A formulation of FEA applicable to density matrices and benchmarks showing computational speedup in quantum state tomography or entanglement quantification would resolve this.

### Open Question 2
Does the non-singular gradient of FEA improve optimization convergence in non-convex energy-based models like Boltzmann machines or Hopfield networks?
The introduction identifies Boltzmann machines and Hopfield networks as primary tools utilizing SE; the experimental validation is restricted to the convex SPARTAn feature selection algorithm. The benefits of the Lipschitz-continuous gradient are demonstrated only for convex regression; behavior in non-convex landscapes with potential local minima remains untested. Comparative training benchmarks of Boltzmann machines using FEA versus exact entropy, analyzing convergence speed and stability, would resolve this.

### Open Question 3
What are the theoretical worst-case error bounds for FEA across the probability interval [0,1]?
The paper reports a mean absolute error of 10^-3 and visual error plots, but lacks a formal theorem defining the maximum approximation error. While the mean error is low, the rational approximation may exhibit larger deviations at specific points (e.g., near 0 or 1) that could be critical for sparse solution finding. A mathematical proof establishing the supremum of |h_S(x) - h_FEA(x)| on the interval [0,1] would resolve this.

## Limitations
- Validation focuses primarily on artificial probability distributions and specific ML feature selection tasks without comprehensive testing across diverse real-world datasets
- Does not address potential numerical precision issues with extremely small probability values approaching machine epsilon
- Comparative analysis is limited to "state-of-the-art" methods without specifying exact algorithms used for comparison

## Confidence
High: Cross-platform validation showing consistent 50% speedup across Python, Julia, and MATLAB
Medium: Two to three orders of magnitude speedup claim in feature selection tasks based on single application domain
Low: Theoretical error bounds not formally established

## Next Checks
1. Benchmark FEA against multiple competing entropy approximation methods (Taylor series, rational approximations) on diverse datasets spanning text, image, and time series domains
2. Stress-test FEA with edge cases including near-zero probability distributions and highly skewed distributions to verify numerical stability
3. Conduct ablation studies to isolate the contribution of FEA's speedup versus other implementation optimizations in the reported feature selection experiments