---
ver: rpa2
title: 'SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise
  Use'
arxiv_id: '2505.17332'
source_url: https://arxiv.org/abs/2505.17332
tags:
- your
- language
- words
- have
- swear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SweEval, a novel benchmark to evaluate how
  Large Language Models (LLMs) handle swearing and inappropriate language in real-world
  enterprise scenarios. The benchmark includes 2,725 prompts per language across eight
  languages, using multilingual and transliterated swear words in formal and informal
  contexts with both positive and negative tones.
---

# SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use

## Quick Facts
- **arXiv ID:** 2505.17332
- **Source URL:** https://arxiv.org/abs/2505.17332
- **Reference count:** 17
- **Primary Result:** A novel benchmark revealing significant safety alignment gaps in LLMs when handling swear words and inappropriate language across multilingual contexts, especially in low-resource and transliterated Indic languages.

## Executive Summary
This paper introduces SweEval, a comprehensive benchmark designed to evaluate how Large Language Models handle swearing and inappropriate language in real-world enterprise scenarios. The benchmark comprises 2,725 prompts per language across eight languages, incorporating multilingual and transliterated swear words in both formal and informal contexts with varying tones. Through experiments with 13 models ranging from 7B to 141B parameters, the study reveals that LLMs demonstrate significant performance gaps when processing swear words in low-resource and transliterated languages compared to English. The findings highlight that models often fail to recognize cultural and contextual nuances, particularly in multilingual settings, underscoring the need for improved data curation and training methodologies to enhance safety alignment across all supported languages.

## Method Summary
The SweEval benchmark was developed to comprehensively assess LLM behavior with swearing and inappropriate language across multilingual contexts. The benchmark includes 2,725 prompts per language distributed across eight languages, featuring both multilingual and transliterated swear words. Prompts were designed to cover formal and informal contexts with both positive and negative tones to capture a wide range of real-world scenarios. The evaluation framework tests models' ability to recognize and appropriately handle swear words while considering cultural nuances and contextual variations. Experiments were conducted across 13 different models with parameter sizes ranging from 7B to 141B, providing insights into how model scale affects performance on this safety-critical task.

## Key Results
- LLMs show significantly higher harmful rates when processing swear words in low-resource and transliterated Indic languages compared to English
- Models struggle with cultural and contextual nuances, particularly in multilingual settings
- Performance gaps persist across model sizes (7B to 141B parameters), indicating fundamental challenges in safety alignment

## Why This Works (Mechanism)
The benchmark works by exposing models to realistic, context-rich scenarios where swear words appear in both expected and unexpected ways. By using transliterated content and mixing formal/informal registers, the benchmark forces models to demonstrate true language understanding rather than pattern matching. The multi-tone approach (positive/negative) reveals whether models can distinguish between harmful and non-harmful uses of similar language. This comprehensive exposure helps identify where models fail to grasp cultural and linguistic subtleties that are critical for enterprise safety applications.

## Foundational Learning

**Cultural Context Awareness**
- Why needed: Swear words carry different meanings and severity across cultures
- Quick check: Test model responses to identical words in different cultural contexts

**Multilingual Processing**
- Why needed: Enterprises operate globally and need consistent safety standards
- Quick check: Compare model performance across high-resource vs low-resource languages

**Transliteration Handling**
- Why needed: Users often mix scripts in multilingual communication
- Quick check: Evaluate model responses to transliterated vs native script inputs

## Architecture Onboarding

**Component Map:**
Benchmark Generator -> Prompt Distribution -> Model Evaluation -> Safety Scoring -> Result Aggregation

**Critical Path:**
Prompt Generation -> Context Classification -> Model Response Generation -> Harm Assessment -> Performance Analysis

**Design Tradeoffs:**
- Comprehensive coverage vs. evaluation efficiency
- Cultural specificity vs. generalization
- Technical complexity vs. practical applicability

**Failure Signatures:**
- High false positives in low-resource languages
- Context blindness in formal/informal distinction
- Cultural misinterpretation of mild vs severe content

**First 3 Experiments:**
1. Test English-only model performance vs multilingual models on identical swear word sets
2. Evaluate context sensitivity by varying tone while keeping vocabulary constant
3. Compare transliterated vs native script handling within the same language

## Open Questions the Paper Calls Out
None

## Limitations
- Cultural specificity of swear words may limit generalizability across all global contexts
- Focus on eight languages may not capture full linguistic and cultural diversity
- Transliteration handling may miss regional variations and dialectical differences

## Confidence

**High Confidence:**
- Performance gaps between high-resource and low-resource languages are well-supported
- Model size does not eliminate multilingual safety alignment challenges

**Medium Confidence:**
- Cultural nuance recognition failures are observed but may vary by specific context
- Transliteration handling difficulties are documented but may have language-specific variations

## Next Checks

1. Conduct extensive cross-cultural validation with native speakers to verify cultural sensitivity and appropriateness of harmful content classification

2. Test the benchmark with additional low-resource languages and regional dialects to assess scalability and identify potential blind spots

3. Perform longitudinal studies to evaluate how model performance changes with different fine-tuning approaches and safety training methods across all supported languages