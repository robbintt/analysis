---
ver: rpa2
title: Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation
arxiv_id: '2505.21941'
source_url: https://arxiv.org/abs/2505.21941
tags:
- multilingual
- llama-3
- sampling
- reward
- repeated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates repeated sampling with verifiers for multilingual
  text generation. The approach generates multiple samples and selects the best using
  perplexity- or reward-based scoring.
---

# Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation

## Quick Facts
- arXiv ID: 2505.21941
- Source URL: https://arxiv.org/abs/2505.21941
- Authors: Ashim Gupta; Vivek Srikumar
- Reference count: 40
- This paper evaluates repeated sampling with verifiers for multilingual text generation, showing up to 35% gains on the Aya Evaluation Suite.

## Executive Summary
This paper explores test-time scaling through repeated sampling for multilingual text generation, demonstrating that generating multiple candidate responses and selecting the best using verifier models significantly improves output quality. The approach works by sampling n times (up to 100) and using either perplexity or reward-based scoring to select the optimal response. Results show substantial improvements across nine languages in the Aya Evaluation Suite, with reward models proving particularly effective for reasoning-heavy tasks where perplexity-based verification can actually degrade performance.

## Method Summary
The method generates n samples (n âˆˆ {5, 10, 15, 20, 25, 50, 75, 100}) with temperature=0.8 and top-p=0.95 for each prompt, then selects the best output using either perplexity verifiers (Gemma/Llama-base) or reward models (Gemma/Llama-tuned). The approach is tested on the Aya Evaluation Suite (9 languages: ar, en, hi, pa, pt, ru, te, tr, zh) and m-ArenaHard (7 languages: ar, cs, en, hi, ja, pt, vi). Performance is measured using win/loss rate deltas computed by an LLM-as-a-Judge (Gemini-2.0-flash), averaging across languages. Models tested include Qwen-2.5, Llama-3.1/3.3, and Aya-expanse, with verifier sizes ranging from 2B to 8B parameters.

## Key Results
- Up to 35% increase in win rates on the Aya Evaluation Suite when using reward-based verifiers
- Reward models outperform perplexity verifiers on reasoning-heavy m-ArenaHard tasks
- Even small 2B-parameter reward models can enhance outputs from much larger 72B-parameter generators
- No single verifier type is universally optimal across all languages and task types

## Why This Works (Mechanism)

### Mechanism 1: Sample Diversity Increases Quality Probability
Generating multiple candidates increases the likelihood of producing high-quality outputs in low-confidence settings. By sampling n times with temperature-based decoding, the system explores a wider solution space. For underrepresented languages where initial outputs may be suboptimal, selecting the best candidate from a larger pool increases the probability of finding a fluent and contextually appropriate response.

### Mechanism 2: Reward Models Capture Reasoning Constraints
Reward models score prompt-response pairs, allowing them to evaluate semantic correctness and instruction following beyond simple fluency. This distinction allows RMs to succeed in reasoning tasks (math/code) where fluent but incorrect answers are common, while perplexity-based verification can actually hurt generation quality for such tasks.

### Mechanism 3: Decoupled Generation and Verification
Small, specialized verifiers can effectively steer larger, general-purpose generators by acting as critics. A smaller reward model (e.g., 2B parameters) acts as a critic for a much larger generator (e.g., 72B), allowing efficient compute use without sacrificing the generation capacity of the large model.

## Foundational Learning

**Test-Time Scaling (Repeated Sampling):** This is the core strategy. Understanding that inference compute can substitute for or augment training compute is necessary to grasp why generating n=100 samples is viable. *Quick check:* How does increasing the number of samples (n) affect the probability of finding the correct answer compared to increasing model size?

**Perplexity vs. Reward Model Verification:** The paper hinges on the failure of PPL for reasoning. You must distinguish between "Is this text likely?" (PPL) and "Is this text correct/preferred?" (Reward). *Quick check:* Why would a mathematically incorrect but grammatically perfect sentence receive a low perplexity score but a low reward score?

**LLM-as-a-Judge:** The results (win rates) rely on an automated evaluator (Gemini). Understanding the subjectivity and potential bias of this judge is critical for interpreting the "35% gain" claim. *Quick check:* What biases might an LLM judge introduce when evaluating low-resource languages like Punjabi or Telugu?

## Architecture Onboarding

**Component map:** Generator (multilingual LLM) -> Verifier Pool (PPL/RM scoring) -> Aggregator (argmax/argmin selection)

**Critical path:** The latency bottleneck is the n generation steps. Verification is typically a single forward pass per sample but becomes expensive at n > 50.

**Design tradeoffs:** Verifier Size vs. Efficacy (2B faster but may lack nuance of 8B); PPL vs. RM (PPL is "free" but fails on reasoning, RM requires separate model and specialized training data).

**Failure signatures:** Reasoning Degradation (using PPL verifiers on code/math tasks); Language Mismatch (high variance across languages); Compute Floor (smaller model with heavy sampling fails to match larger model with n=1).

**First 3 experiments:** 1) Verifier Ablation on Reasoning (run m-ArenaHard with PPL vs. RM); 2) Scaling Saturation (plot delta scores for n=5, 10, 25, 50, 100); 3) Cross-architecture Verification (test if Llama verifier improves Qwen outputs).

## Open Questions the Paper Calls Out

**Open Question 1:** Does training verifiers on multilingual preference data yield significant performance gains over English-only verifiers for repeated sampling? The authors note their verifiers were trained exclusively on English data and state "further improvements may be possible with multilingual training."

**Open Question 2:** Can an adaptive system effectively select the optimal verifier based on the input language or task type? The authors observe that no single verifier is universally optimal and suggest "future work could explore strategies for selecting verifiers based on language, model architecture, or task type."

**Open Question 3:** To what extent do "LLM-as-a-Judge" evaluations align with human judgments for low-resource languages in this context? The authors list reliance on Gemini-2.0-flash as a judge as a limitation, noting it "may not fully align with human evaluations, particularly in multilingual or culturally nuanced contexts."

## Limitations

- Limited generalization across language families, with verifier effectiveness varying significantly (Gemma-PPL works for Hindi but Llama-PPL for Arabic)
- Assumption of single-correct-response paradigm that may not apply to creative or open-ended tasks
- English-centric reward model training creates untested assumptions about cross-linguistic preference transfer

## Confidence

**High Confidence:** The core finding that repeated sampling with verifiers improves multilingual text generation quality (supported by measurable win-rate improvements)

**Medium Confidence:** The claim that reward models outperform perplexity for reasoning tasks (supported by m-ArenaHard results but underlying mechanism remains speculative)

**Low Confidence:** The assertion that small reward models can effectively steer large generators across diverse languages (shown to work in practice but lacks theoretical guarantees)

## Next Checks

**Check 1:** Cross-linguistic reward model bias analysis - Evaluate the same reward model's scoring patterns across languages for equivalent reasoning tasks to quantify systematic bias.

**Check 2:** Sample efficiency saturation curves - Generate detailed plots of delta scores versus sample count (n=5, 10, 25, 50, 100) for each language pair to identify optimal sampling budgets.

**Check 3:** Zero-shot transfer robustness - Test verifier performance on languages and domains completely absent from training data (e.g., code-switching, minority languages) to establish true boundaries of generalization.