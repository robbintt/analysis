---
ver: rpa2
title: 'CLUE: Conflict-guided Localization for LLM Unlearning Framework'
arxiv_id: '2509.20977'
source_url: https://arxiv.org/abs/2509.20977
tags:
- forget
- neurons
- retain
- circuit
- unlearning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of large language model (LLM)
  unlearning, aiming to remove undesirable knowledge while preserving non-target capabilities.
  Existing localization methods struggle to disentangle neurons responsible for forgetting
  versus retaining information, leading to over-forgetting or incomplete erasure.
---

# CLUE: Conflict-guided Localization for LLM Unlearning Framework

## Quick Facts
- arXiv ID: 2509.20977
- Source URL: https://arxiv.org/abs/2509.20977
- Authors: Hang Chen; Jiaying Zhu; Xinyu Yang; Wenya Wang
- Reference count: 40
- Key outcome: Proposes CLUE framework for LLM unlearning using circuit discovery and CNF satisfiability to classify neurons into forget, retain, and conflict categories, achieving superior forget efficacy and retain utility with fewer parameter modifications

## Executive Summary
The paper addresses the challenge of LLM unlearning by proposing CLUE (Conflict-guided Localization for LLM Unlearning Framework), which leverages circuit discovery and CNF satisfiability to accurately identify neurons responsible for forgetting versus retaining information. Unlike existing methods that struggle with over-forgetting or incomplete erasure, CLUE classifies neurons into three categories and solves the CNF derived from forget and retain circuits to determine which neurons to modify for each objective. The framework demonstrates improved performance on three unlearning tasks with better forget efficacy, retain utility, and parameter efficiency.

## Method Summary
CLUE introduces a novel approach to LLM unlearning by combining circuit discovery with CNF satisfiability solving. The method first identifies forget and retain circuits through neuron activation analysis on unlearning and general tasks respectively. These circuits are then converted into conjunctive normal form, and a CNF satisfiability solver is used to classify neurons into three categories: forget neurons (only in forget circuits), retain neurons (only in retain circuits), and conflict neurons (in both). By solving the CNF constraints, CLUE determines which neurons should be modified to achieve the unlearning objective while preserving general capabilities, addressing the key challenge of disentangling conflicting objectives in parameter modification.

## Key Results
- Outperforms existing methods in forget efficacy with 1-accuracy up to 0.724 on WMDP Cyber task
- Achieves retain utility up to 0.91 on PKU-SafeRLHF task while modifying fewer parameters (58.16% on WMDP Cyber)
- Demonstrates robustness across different forget ratios and improves interpretability of unlearning effects

## Why This Works (Mechanism)
CLUE works by leveraging the complementary nature of forget and retain objectives through CNF satisfiability solving. The key insight is that forget and retain neurons can be treated as complementary constraints in a logical formula, where satisfying the CNF ensures that the right neurons are modified for each objective. By explicitly identifying conflict neurons and handling them through the satisfiability framework, CLUE avoids the over-forgetting problem common in existing methods while ensuring effective erasure of target knowledge.

## Foundational Learning
- **Circuit Discovery**: Technique to identify sets of neurons responsible for specific behaviors or knowledge representation in neural networks. Why needed: Essential for understanding which neurons encode target knowledge to be forgotten versus general capabilities to be retained. Quick check: Verify that discovered circuits correspond to expected behaviors through ablation studies.
- **CNF Satisfiability**: Method of expressing logical constraints as conjunctions of disjunctions and finding solutions that satisfy all constraints simultaneously. Why needed: Provides a principled way to handle the conflicting objectives of forgetting specific knowledge while retaining general capabilities. Quick check: Confirm that the CNF solver can handle the scale of neuron sets in practical LLM architectures.
- **Neuron Classification**: Process of categorizing neurons based on their involvement in different computational pathways or knowledge representations. Why needed: Enables targeted parameter modification by distinguishing between neurons that should be altered for forgetting versus those that must be preserved. Quick check: Validate classification accuracy through targeted interventions and behavioral testing.

## Architecture Onboarding

**Component Map:**
Raw Model -> Circuit Discovery (Forget/Retain) -> CNF Formulation -> Satisfiability Solving -> Neuron Classification -> Parameter Modification -> Unlearned Model

**Critical Path:**
Circuit Discovery -> CNF Formulation -> Satisfiability Solving -> Parameter Modification

**Design Tradeoffs:**
- Circuit discovery complexity vs. localization accuracy
- CNF solver scalability vs. neuron set size
- Parameter modification extent vs. unlearning effectiveness
- Computational overhead vs. unlearning performance

**Failure Signatures:**
- Over-forgetting indicated by significant drop in general task performance
- Incomplete erasure shown by persistence of target knowledge in probing tasks
- High conflict neuron count suggesting fundamental incompatibility between objectives
- Solver timeouts indicating CNF formulation scalability issues

**3 First Experiments to Run:**
1. Ablation study removing CNF satisfiability step to assess its contribution to performance
2. Varying forget ratios to evaluate robustness across different unlearning intensities
3. Comparison of circuit discovery methods to determine optimal approach for neuron identification

## Open Questions the Paper Calls Out
None

## Limitations
- Limited testing to biomedical and safety-related unlearning tasks, raising questions about domain generalizability
- Absolute performance metrics show only 91% retain utility even with the best method, indicating room for improvement
- Scalability to very large models (GPT-4 class) and computational overhead of circuit discovery/CNF solving not fully characterized

## Confidence
- **High Confidence**: The core methodology of using circuit discovery combined with CNF satisfiability for neuron classification is technically sound and well-explained.
- **Medium Confidence**: Claims about parameter efficiency and interpretability benefits are supported by experiments but would benefit from additional ablation studies.
- **Low Confidence**: Claims about robustness across different forget ratios and general utility improvements require testing on broader range of tasks and models.

## Next Checks
1. Test CLUE on diverse unlearning tasks beyond biomedical and safety domains, including legal knowledge removal, financial information unlearning, and general knowledge forgetting to assess domain generalizability.

2. Conduct scalability experiments on larger models (30B+ parameter models) to evaluate computational overhead and performance degradation with model size, particularly focusing on circuit discovery and CNF solving components.

3. Perform security analysis to evaluate the robustness of unlearning against fine-tuning attacks, knowledge probing, and other adversarial techniques that might recover forgotten information.