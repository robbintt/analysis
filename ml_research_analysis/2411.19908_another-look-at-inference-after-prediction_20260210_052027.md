---
ver: rpa2
title: Another look at inference after prediction
arxiv_id: '2411.19908'
source_url: https://arxiv.org/abs/2411.19908
tags:
- inference
- data
- efficiency
- estimator
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of conducting valid and efficient
  statistical inference using machine learning-imputed outcomes, where predictions
  are used to complement costly gold-standard data. The authors revisit the popular
  Prediction-Powered Inference (PPI) method and show that it can be viewed as an augmented
  inverse probability weighting estimator with a specific choice of augmenting function.
---

# Another look at inference after prediction

## Quick Facts
- arXiv ID: 2411.19908
- Source URL: https://arxiv.org/abs/2411.19908
- Reference count: 40
- This paper revisits the Prediction-Powered Inference (PPI) method, shows it's equivalent to an augmented IPW estimator, and proposes the CC estimator which guarantees efficiency gains over classical inference regardless of prediction quality.

## Executive Summary
This paper addresses the problem of conducting valid and efficient statistical inference when outcomes are partially observed but machine learning predictions are available for all observations. The authors revisit the popular Prediction-Powered Inference (PPI) method and show it can be viewed as an augmented inverse probability weighting estimator. They then propose an alternative approach based on two-phase sampling literature, called the CC estimator, which guarantees efficiency gains over classical inference regardless of prediction quality by adding a carefully designed augmentation term. The CC estimator is shown to be an AIPW estimator with an influence function that approximates the semiparametric efficiency bound via orthogonal projection.

## Method Summary
The CC estimator addresses prediction-based inference by augmenting classical Z-estimation with a term derived from predicted outcomes. For Z-estimation problems where parameters β* satisfy E[ϕ(Y,X;β*)]=0, the CC estimator takes the form β̂_CC = β̂_lab - ÂŴ_CC(γ̂_lab - γ̂_all), where β̂_lab is the classical estimator on labeled data, γ̂_lab and γ̂_all are prediction-based estimators, and Ŵ_CC is a weight matrix constructed from covariances of the estimating functions. The method assumes MCAR missingness and that the estimating functions admit unique solutions. For linear regression, ϕ(y,x;β) = x(y - x^Tβ).

## Key Results
- The CC estimator guarantees efficiency at least equal to classical estimation by adding a zero-mean augmentation term correlated with classical estimator's sampling error
- The method achieves efficiency gains that scale with the squared partial correlation between residuals of Y and Ŷ after adjusting for X
- UK Biobank application shows 11-20% reduction in standard error compared to classical inference when using strong ML predictions
- The CC estimator outperforms both classical inference and PPI extensions across various settings including linear and Poisson regression

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The CC estimator guarantees efficiency at least equal to classical estimation by augmenting the labeled estimator with a zero-mean term derived from predicted outcomes.
- **Mechanism:** The augmentation term (γ̂_lab - γ̂_all) is constructed to be mean-zero (thus introducing no asymptotic bias) while being correlated with the classical estimator's sampling error. This correlation allows the weighted subtraction to reduce variance without compromising consistency.
- **Core assumption:** Missing Completely at Random (MCAR) — labeling indicator R is independent of (Y, X, Z). Also assumes the estimating function ϕ satisfies regularity conditions (C.1-C.7 in Appendix).
- **Evidence anchors:**
  - [abstract] "a simple modification can be applied to guarantee improvements in efficiency beyond yielding valid inferences when the ML predictions are imperfect"
  - [Section 4] "The CC estimator adds an additional term to the classical estimator that can be estimated from the proxy measurements and that is consistent for zero. This term is carefully designed to be correlated with the classical estimator in a way that guarantees a variance reduction."
- **Break condition:** If labeling is not MCAR (e.g., MAR dependent on Y or X), the zero-mean property of the augmentation term may fail, breaking the guarantee.

### Mechanism 2
- **Claim:** The CC estimator achieves its efficiency gains through an AIPW (augmented inverse probability weighting) structure with an influence function that approximates the semiparametric efficiency bound via orthogonal projection.
- **Mechanism:** The augmenting function ω_CC(Ŷ,X) is constructed as the orthogonal projection of ϕ(Y,X;β*) onto the Hilbert space spanned by ϕ(Ŷ,X;γ*). This projection minimizes the distance to the optimal augmenting function ω_opt(Ŷ,X) = E[ϕ(Y,X;β*)|Ŷ,X] within the linear subspace defined by the predicted data's estimating equations.
- **Core assumption:** The estimating equations for both Y (with β*) and Ŷ (with γ*) admit unique solutions, and the covariance matrix C₂₂ = Var[ϕ(Ŷ,X;γ*)] is invertible.
- **Evidence anchors:**
  - [Section 4] "β̂_CC is therefore an AIPW estimator with ω_CC(Ŷ,X) = Cov[ϕ(Y,X;β*),ϕ(Ŷ,X;γ*)]{Var[ϕ(Ŷ,X;γ*)]}⁻¹ϕ(Ŷ,X;γ*)"
  - [Section 4] "The augmenting function is the orthogonal projection of ϕ(Y,X;β*) onto the subspace spanned by ϕ(Ŷ,X;γ*) in the Hilbert space of mean-zero, square-integrable random variables."
- **Break condition:** If C₂₂ is singular (predictions perfectly collinear with estimating function), weight matrix estimation fails.

### Mechanism 3
- **Claim:** For homoskedastic linear regression, efficiency gains scale with the squared partial correlation between residuals of Y and Ŷ after adjusting for X, and inversely with the labeling probability.
- **Mechanism:** When ρ²_{Y,Ŷ} > 0, the predicted outcome captures residual variation in Y that classical estimation ignores. The weight W_CC = σ_{Y,Ŷ}/σ²_Ŷ optimally transfers this predictive information. Lower labeling fractions (smaller π) amplify gains because the unlabeled data contribution grows.
- **Core assumption:** Homoskedasticity of residuals for both Y and Ŷ given X.
- **Evidence anchors:**
  - [Section 4, Example 1] "∆Var(β̂_lab,β̂_CC) = (π⁻¹−1)(σ²_Yρ²_{Y,Ŷ})Σ⁻¹_X" — explicitly derives the relationship between variance reduction and residual correlation
  - [Section 4] "The improvement is stronger with increasing residual correlation of the residuals between Y and Ŷ, greater variability in the residuals for Y, and a lower probability of labeling."
  - [corpus] UK Biobank application (Section 7) empirically confirms 20% SE reduction with strong predictor (R²=0.8), minimal gains with weak predictor (R²=0.5).
- **Break condition:** If predictions are uncorrelated with Y after adjusting for X (ρ_{Y,Ŷ}=0), the augmentation term adds noise without benefit; CC reduces to classical estimator.

## Foundational Learning

- **Concept: Z-estimation and Estimating Equations**
  - Why needed here: The entire CC framework is built on Z-estimators, where parameters are defined as solutions to E[ϕ(Y,X;β)] = 0. Understanding this abstraction is essential to see why the method generalizes beyond OLS to MLE, GEE, and other estimators.
  - Quick check question: If ϕ(y,x;β) = x(y − x^Tβ), what parameter β* solves the estimating equation?

- **Concept: Influence Functions and Asymptotic Linearity**
  - Why needed here: The paper proves efficiency gains by comparing influence functions (IF) of competing estimators. The IF determines asymptotic variance; smaller IF norm → more efficient estimator.
  - Quick check question: Why does a "smaller" influence function translate to lower asymptotic variance?

- **Concept: Augmented Inverse Probability Weighting (AIPW)**
  - Why needed here: CC is shown to be an AIPW estimator. AIPW combines inverse probability weighting (IPW) with an augmentation term that leverages auxiliary information to reduce variance.
  - Quick check question: In the AIPW formula IF = A[ϕ(Y,X;β*)R/π + ω(Ŷ,X)(1−R/π)], what happens to variance if ω(Ŷ,X) perfectly predicts ϕ(Y,X;β*)?

## Architecture Onboarding

- **Component map:**
  Input Data → [Labeled subset (R=1): Y, X, Ŷ] + [Unlabeled subset (R=0): X, Ŷ] → Classical Estimator (β̂_lab) → Prediction Estimator (γ̂_lab, γ̂_all) → Weight Matrix (Ŵ_CC) → CC Estimator (β̂_CC) → Inference

- **Critical path:**
  1. Verify MCAR or estimate propensity scores if MAR
  2. Fit classical estimator on labeled data
  3. Fit prediction-based estimators (γ̂_lab on labeled, γ̂_all on full data)
  4. Estimate weight matrix Ŵ_CC using empirical covariances from labeled data
  5. Compute augmented estimator and its influence function
  6. Construct confidence intervals using asymptotic normality

- **Design tradeoffs:**
  - **Weight matrix complexity:** Full q×q matrix (optimal) vs. diagonal (PSPA) vs. scalar (PPI++) — full matrix maximizes efficiency but requires more labeled data to estimate stably
  - **Prediction model quality:** Strong predictions (high ρ²_{Y,Ŷ}) yield larger gains; weak predictions may not justify complexity
  - **Labeling fraction:** Lower π amplifies gains but also increases weight matrix estimation variance

- **Failure signatures:**
  - Weight matrix unstable (e.g., C₂₂ near-singular) → consider diagonal or scalar constraints
  - PPI outperforming CC → check if augmentation term mean is truly zero; possible MAR violation
  - Coverage below nominal → verify influence function variance estimation; may need bootstrap

- **First 3 experiments:**
  1. **Sanity check on simulated data:** Generate Y = X^Tβ + ε, Ŷ = ρY + noise, vary ρ ∈ {0, 0.3, 0.6, 0.9} and π ∈ {0.05, 0.1, 0.2}. Confirm CC never loses to OLS and gains increase with ρ.
  2. **Weight matrix structure comparison:** Compare full matrix (CC), diagonal (PSPA-style), and scalar (PPI++-style) on same data. Track: (a) efficiency loss relative to full matrix, (b) stability of weight estimates at n_lab = 50, 100, 500.
  3. **Distribution shift robustness:** Train prediction model on data with X₁ distribution shifted (e.g., different mean). Apply CC vs. PPI vs. PDC. Verify CC maintains efficiency advantage under moderate shift (following simulation settings in Table 1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Chen-Chen (CC) estimator be theoretically and empirically adapted to maintain efficiency guarantees under general missingness patterns, such as Missing At Random (MAR), rather than the current Missing Completely At Random (MCAR) assumption?
- Basis in paper: [explicit] The Discussion section states, "Extending the CC approach to accommodate more general missingness patterns would greatly enhance its practical applicability and has been a focus of recent work."
- Why unresolved: The current theoretical derivations and simulations (Section 6) explicitly assume outcomes are MCAR to facilitate comparisons with existing PB inference methods.
- What evidence would resolve it: Theoretical proofs of consistency and asymptotic normality under MAR, alongside simulation results demonstrating robustness when missingness depends on covariates.

### Open Question 2
- Question: Does restricting the CC estimator's weight matrix to a scalar or diagonal structure yield better finite-sample performance than the full matrix approach when labeled data are limited?
- Basis in paper: [explicit] The Discussion notes that "estimation of a $q \times q$ weight matrix... can be unstable when labeled data are limited," suggesting restricted structures may be beneficial.
- Why unresolved: While the full matrix is asymptotically efficient, the paper acknowledges that variance reduction in finite samples is not guaranteed due to estimation instability.
- What evidence would resolve it: Simulation studies comparing the mean squared error (MSE) and confidence interval coverage of full vs. restricted weight matrices across varying labeling fractions (π).

### Open Question 3
- Question: Under what specific conditions does the Prediction De-Correlated (PDC) estimator outperform the CC estimator in logistic regression settings with misclassified outcomes?
- Basis in paper: [explicit] The Discussion clarifies that "The CC estimator is not guaranteed to outperform PDC in general," specifically noting Proposition 5 fails for logistic regression with non-differentially misclassified Ŷ.
- Why unresolved: The paper establishes CC efficiency for canonical GLMs with correctly specified means but leaves the theoretical hierarchy ambiguous for this specific misclassification scenario.
- What evidence would resolve it: A theoretical derivation of the asymptotic variance difference between CC and PDC for logistic regression with misclassification, supported by targeted simulations.

## Limitations
- The CC estimator's efficiency guarantees rely on MCAR missingness, but real-world applications often exhibit MAR mechanisms
- The numerical experiments use specific simulation settings with unknown parameter values (σ²_ε, σ²_X1, σ²_X2), limiting direct reproducibility
- The UK Biobank application demonstrates practical benefits but lacks comparison with established post-prediction inference methods beyond PPI and PDC

## Confidence

- **High confidence:** The CC estimator's advantage over classical inference under MCAR (theoretical derivation is rigorous)
- **Medium confidence:** Numerical simulations showing relative performance (implementation details uncertain)
- **Medium confidence:** UK Biobank application results (real data but limited methodological comparisons)

## Next Checks
1. **MCAR vs MAR robustness test:** Simulate data where labeling probability depends on X but not Y. Verify CC maintains efficiency gains while PPI may fail.

2. **Weight matrix stability analysis:** Systematically vary labeled sample size (πn) and dimension q to identify when full matrix estimation becomes unstable. Compare against diagonal/scalar alternatives.

3. **Real-world prediction quality impact:** Apply CC to datasets with varying ML prediction quality (e.g., R² from 0.3 to 0.9) to quantify efficiency gains across the quality spectrum.