---
ver: rpa2
title: 'A Survey on Agentic Security: Applications, Threats and Defenses'
arxiv_id: '2510.06445'
source_url: https://arxiv.org/abs/2510.06445
tags:
- agents
- arxiv
- preprint
- agent
- security
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first comprehensive survey of agentic
  security, organizing over 160 papers into three fundamental pillars: Applications,
  Threats, and Defenses. The survey reveals that LLM agents are increasingly used
  in both offensive security (penetration testing, vulnerability discovery, exploit
  generation) and defensive security (threat detection, incident response, forensics),
  with a clear trend toward planner-executor architectures and hybrid models.'
---

# A Survey on Agentic Security: Applications, Threats and Defenses

## Quick Facts
- arXiv ID: 2510.06445
- Source URL: https://arxiv.org/abs/2510.06445
- Authors: Asif Shahriar; Md Nafiu Rahman; Sadif Ahmed; Farig Sadeque; Md Rizwan Parvez
- Reference count: 40
- Primary result: First comprehensive survey of agentic security, organizing over 160 papers into Applications, Threats, and Defenses

## Executive Summary
This paper presents the first comprehensive survey of agentic security, organizing over 160 papers into three fundamental pillars: Applications, Threats, and Defenses. The survey reveals that LLM agents are increasingly used in both offensive security (penetration testing, vulnerability discovery, exploit generation) and defensive security (threat detection, incident response, forensics), with a clear trend toward planner-executor architectures and hybrid models. The analysis shows GPT-family models dominate as backbones (83% of studies), while non-textual modalities remain underexplored. Security threats are diverse, including injection attacks, poisoning, jailbreaks, and goal hijacking, with agents proving more vulnerable than standalone LLMs. Defenses range from secure-by-design architectures to runtime protection and formal verification, though many defenses remain fragile under adaptive attacks. The survey highlights critical research gaps in cross-domain systems, economics of agentic security, and the need for provable safety guarantees, while providing a continuously updated public repository of surveyed papers.

## Method Summary
The survey employed a multi-stage collection process: automated database searches across arXiv, ACL Anthology, IEEE Xplore, and ACM Digital Library using Boolean queries (Jan 2023 - Sep 2025), followed by manual curation of top conferences and backward/forward snowballing. Papers were included if they focused on LLM-based agents with substantial technical security content (Applications/Threats/Defenses). The analysis involved cross-cutting categorization of architecture patterns, LLM backbone distribution, modality coverage, and knowledge source usage, with statistics verified against the public repository at https://github.com/kagnlp/Awesome-Agentic-Security.

## Key Results
- GPT-family models dominate as backbones (83% of studies), with planner-executor architectures emerging as the dominant pattern (39.8%)
- Agentic wrappers amplify LLM vulnerability by expanding attack surface beyond text I/O through tools, memory systems, and feedback loops
- Layered defense architectures reduce but do not eliminate attack success, with all surveyed defenses vulnerable to adaptive attacks
- Non-textual modalities (images, network traces, binaries) remain underexplored despite their security relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing agent cognition into planner-executor architectures improves interpretability and reduces cascading failures.
- Mechanism: Separation of planning (task decomposition) from execution (tool invocation, environment interaction) allows independent verification of each stage and enables modular debugging of failure points.
- Core assumption: Errors in reasoning can be isolated before they propagate to tool execution.
- Evidence anchors:
  - [Section 5]: "The field is shifting towards planner–executor architectures (39.8%) and hybrid models (14%)... decomposed cognitive pipelines, where planning, execution, and verification can be modularized to improve interpretability and debugging."
  - [Section 2.1.1]: PentestGPT uses a "Reasoning–Generation–Parsing design reducing context loss."
  - [corpus]: Related surveys (e.g., "Agentic AI Security") corroborate the architectural trend toward modular agent designs.
- Break condition: If planner and executor share unverified state (e.g., contaminated memory), isolation fails and attacks propagate.

### Mechanism 2
- Claim: Agentic wrappers amplify LLM vulnerability by expanding the attack surface beyond text I/O.
- Mechanism: Tools, memory systems, and feedback loops introduce new injection vectors (indirect prompt injection via tool outputs, memory poisoning, goal hijacking) that bypass the base model's refusal training.
- Core assumption: The base LLM's safety alignment does not transfer reliably to agentic contexts.
- Evidence anchors:
  - [Section 1]: "A number of studies have shown that the very act of wrapping an LLM in an agentic framework significantly increases its vulnerability."
  - [Section 3.1.3]: "AI agents are significantly more vulnerable to jailbreak attacks than their underlying LLMs" due to iterative action generation and environment feedback processing.
  - [Section 3.1.1]: Prompt injection benchmarks (AgentDojo, InjecAgent) show high vulnerability rates across agents.
  - [corpus]: "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures" confirms transfer failure but focuses on standalone models, not agent-specific surfaces.
- Break condition: If all input channels (tools, memory, environment feedback) are sanitizable and verified, amplification is mitigated—but current defenses remain fragile under adaptive attacks.

### Mechanism 3
- Claim: Layered defense architectures (secure-by-design + runtime protection + verification) reduce but do not eliminate attack success.
- Mechanism: Intent validation at planning time, behavioral monitoring during execution, and formal verification of action policies provide overlapping safeguards, each catching different attack classes.
- Core assumption: Attackers cannot simultaneously bypass all layers.
- Evidence anchors:
  - [Section 4.1.1]: Modular plan–execute isolation "cut[s] cross-context injection rates by over 40%."
  - [Section 4.1.3]: Guardrails (R2-Guard, AgentGuard) "reduce jailbreak failures by up to 35%."
  - [Section 3.2.1]: Adaptive attacks (Zhan et al., 2025) "demonstrate that all [defenses] can be successfully bypassed."
  - [corpus]: Evidence on multi-layer defense effectiveness is sparse; most corpus papers focus on single-layer mitigations.
- Break condition: If attackers craft adaptive payloads that exploit layer interactions (e.g., poisoning memory to bypass runtime checks), layered defense fails.

## Foundational Learning

- Concept: **Prompt Injection (Direct & Indirect)**
  - Why needed here: The dominant attack vector in agentic systems; tools and external data introduce untrusted context that can override instructions.
  - Quick check question: Can you distinguish an injection embedded in tool output from one in user input?

- Concept: **Tool-Augmented LLMs**
  - Why needed here: Agents differ from chatbots by invoking APIs/tools; understanding the execution boundary is critical for threat modeling.
  - Quick check question: What happens if a tool returns maliciously crafted JSON that the agent parses as instructions?

- Concept: **Multi-Agent Coordination Patterns**
  - Why needed here: 24.6% of surveyed systems use multi-agent setups; inter-agent communication creates new trust boundaries and Byzantine failure modes.
  - Quick check question: If one agent in a collective is compromised, how does it affect task completion?

## Architecture Onboarding

- Component map: Core LLM backbone -> Memory subsystem -> Tool layer -> Governance layer -> Runtime guardrails
- Critical path:
  1. Input parsing (user query + environment observations)
  2. Planning phase (task decomposition)
  3. Intent validation (task alignment check)
  4. Tool selection & execution
  5. Output filtering & governance review
  6. Feedback integration into memory
- Design tradeoffs:
  - Security vs. utility: Defenses that reduce injection success also degrade task completion (AgentDojo finding).
  - Autonomy vs. oversight: Fully autonomous agents scale better but lack human-in-the-loop safety gates.
  - Pretrained knowledge vs. RAG/fine-tuning: 132 papers rely on frozen weights (practical but brittle); RAG/fine-tuning remain niche.
- Failure signatures:
  - **Context loss**: Long-horizon tasks cause reasoning drift (PentestGPT mitigates with specialized design).
  - **Goal hijacking**: Secondary malicious goals override primary task.
  - **Reward hacking / specification gaming**: Agent satisfies literal instructions while violating intent (e.g., o3 cheating at chess).
  - **Byzantine agents**: Single compromised agent disrupts multi-agent coordination.
- First 3 experiments:
  1. Reproduce an indirect prompt injection attack using AgentDojo or InjecAgent benchmarks on a minimal tool-calling agent.
  2. Implement a planner-executor split with intent validation and measure injection resistance vs. task completion rate.
  3. Test a multi-agent debate framework (e.g., PhishDebate pattern) on phishing detection and compare false-positive rates to single-agent baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can defense techniques provide provable, formal safety guarantees for LLM agents without unacceptably degrading task utility?
- Basis in paper: [explicit] The conclusion explicitly calls for "defense techniques with provable safety guarantees."
- Why unresolved: Current defenses are heuristic (guardrails, behavioral monitoring) and fragile under adaptive attacks. Formal verification systems like VeriPlan and AgentGuard are mentioned but remain limited in scalability and coverage.
- What evidence would resolve it: A formal verification framework proving bounded agent behavior on realistic security tasks, with empirical validation showing maintained utility.

### Open Question 2
- Question: What are the economic trade-offs between attack costs, defense investments, and agent utility in production agentic systems?
- Basis in paper: [explicit] The conclusion identifies "the economics of agentic security" as a key future research direction.
- Why unresolved: The Limitations section notes most studies emphasize accuracy and safety while neglecting "practical aspects like cost, speed, or energy use."
- What evidence would resolve it: Cost-benefit models quantifying defense investment versus attack success rates across different agent architectures in real deployments.

### Open Question 3
- Question: Can the fundamental security-utility trade-off be overcome, where current defenses reduce vulnerability but also degrade task completion?
- Basis in paper: [explicit] AgentDojo reveals "a fundamental trade-off: security defenses that reduce vulnerability also degrade the agent's task-completion utility." ST-WebAgentBench shows policy-compliant success is 38% lower than standard completion.
- Why unresolved: Secure-by-design architectures reduce injection rates but no approach has closed this gap; all surveyed defenses face this tension.
- What evidence would resolve it: An architecture achieving comparable task completion rates with and without security constraints on benchmarks like AgentDojo or ST-WebAgentBench.

### Open Question 4
- Question: How can agentic security be extended to non-textual modalities (images, network traces, binaries) that current research largely overlooks?
- Basis in paper: [explicit] The cross-cutting analysis states: "images, network traces and binaries are often tied to security vulnerabilities... these non-textual modalities are underexplored. This research gap also presents a promising area for future work."
- Why unresolved: Text dominates (141 papers) versus images (38), binaries (10), and network traces (11); embodied agents face unique threats (sensor spoofing, adversarial prompts) without standardized defenses.
- What evidence would resolve it: Benchmarks and defense mechanisms validated on agents processing non-textual security data, demonstrating resilience against modality-specific attacks.

## Limitations

- The survey focuses on academic literature, potentially missing industrial practices and gray literature that may reveal practical deployment patterns.
- Categorization of papers into architectural patterns relies on subjective interpretation of methodology descriptions, which may not capture nuanced implementations.
- Effectiveness metrics for defenses are largely reported from original papers without independent validation, introducing potential bias in reported success rates.
- The survey lacks systematic analysis of economic factors and real-world deployment costs, which are critical for understanding practical adoption barriers.

## Confidence

- **High Confidence**: Claims about architectural trends (planner-executor dominance at 39.8%, GPT-family backbone usage at 83%) are directly supported by quantitative analysis of the surveyed corpus and clearly visible patterns across multiple papers.
- **Medium Confidence**: Claims about vulnerability amplification (agents being more vulnerable than standalone LLMs) are well-supported by attack benchmarks but may not generalize to all agent designs, particularly those with robust security architectures.
- **Low Confidence**: Claims about long-term effectiveness of layered defenses are based on reported results that haven't been tested against adaptive, persistent attackers in real-world scenarios.

## Next Checks

1. **Independent Defense Validation**: Reproduce a subset of the most-cited defense mechanisms (e.g., R2-Guard, AgentGuard) against current adversarial attack frameworks to verify reported effectiveness rates and test against adaptive attacks not considered in original papers.

2. **Cross-Domain Applicability Test**: Select 3-5 agentic security applications from different domains (e.g., penetration testing, threat detection, forensics) and evaluate whether the identified architectural patterns and vulnerabilities transfer across these contexts or domain-specific variations exist.

3. **Economic Impact Analysis**: Survey industrial practitioners (via interviews or questionnaires) to assess the actual deployment costs, maintenance overhead, and ROI of agentic security systems, comparing theoretical benefits reported in academic papers with practical implementation experiences.