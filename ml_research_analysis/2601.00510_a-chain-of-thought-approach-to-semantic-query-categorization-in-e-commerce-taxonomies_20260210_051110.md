---
ver: rpa2
title: A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce
  Taxonomies
arxiv_id: '2601.00510'
source_url: https://arxiv.org/abs/2601.00510
tags:
- query
- category
- categories
- semantic
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses semantic query categorization in e-commerce
  taxonomies, aiming to improve search relevance by mapping user queries to relevant
  leaf categories. The proposed Chain-of-Thought Breadth-First Search (CoT BFS) method
  combines tree search with LLM semantic scoring to navigate hierarchical taxonomies
  and identify semantically aligned categories.
---

# A Chain-of-Thought Approach to Semantic Query Categorization in e-Commerce Taxonomies

## Quick Facts
- **arXiv ID**: 2601.00510
- **Source URL**: https://arxiv.org/abs/2601.00510
- **Reference count**: 13
- **Primary result**: Chain-of-Thought Breadth-First Search achieves 89.8% higher F1 score than k-NN baseline on human-labeled e-commerce query categorization

## Executive Summary
This paper addresses the challenge of mapping user search queries to relevant leaf categories in hierarchical e-commerce taxonomies. The proposed Chain-of-Thought Breadth-First Search (CoT BFS) method leverages large language models to semantically score category relevance during tree traversal, outperforming traditional embedding-based approaches. The method achieves substantial improvements in precision, recall, and F1 scores while maintaining computational efficiency by visiting only a small fraction of taxonomy nodes.

## Method Summary
The CoT BFS method combines tree search with LLM semantic scoring to navigate hierarchical taxonomies and identify semantically aligned categories. At each taxonomy level, the LLM scores children categories based on query relevance, then applies dual-threshold pruning (selection-threshold=9, minimum-threshold=8) to reduce the search space. The process iterates to leaf nodes, which are then rescored to produce final predictions. The method uses Mixtral-8x7B as the primary LLM and demonstrates significant performance improvements over k-NN baselines using sentence-BERT embeddings across multiple evaluation metrics.

## Key Results
- CoT BFS outperforms k-NN baseline by +89.8% F1, +86.1% precision, and +109.7% F1 in sample aggregation on human-judged data
- On LLM-judged data, CoT BFS achieves +96.7% F1 improvement
- Retrieval tests show +72% higher recall and +34% better relevance scores
- Method visits only 1.7%-24.8% of taxonomy nodes, demonstrating high efficiency

## Why This Works (Mechanism)
CoT BFS works by leveraging the semantic understanding capabilities of LLMs to navigate hierarchical taxonomies more intelligently than embedding-based approaches. Unlike k-NN methods that rely solely on vector similarity, CoT BFS uses contextual semantic scoring that considers the hierarchical relationships between categories. The dual-threshold pruning mechanism ensures that only the most relevant branches are explored, reducing computational overhead while maintaining high precision. This approach is particularly effective for queries requiring contextual understanding (e.g., "iPhone charger" vs. "iPhone case") where traditional methods struggle with ambiguity.

## Foundational Learning
- **Tree search algorithms**: Why needed - to systematically navigate hierarchical taxonomies; Quick check - can traverse any tree structure using BFS or DFS
- **Semantic scoring with LLMs**: Why needed - to evaluate category relevance beyond vector similarity; Quick check - LLM can score category relevance on 1-10 scale given query and context
- **Dual-threshold pruning**: Why needed - to balance precision and recall by controlling search space; Quick check - can implement selection and minimum thresholds for branch pruning
- **Taxonomy structure**: Why needed - understanding hierarchical relationships between categories; Quick check - can represent taxonomy as tree with parent-child relationships
- **Evaluation metrics (F1, precision, recall)**: Why needed - to quantify classification performance; Quick check - can compute micro/macro averages from confusion matrix
- **Sentence embeddings**: Why needed - baseline comparison using traditional NLP methods; Quick check - can generate and compare embeddings using cosine similarity

## Architecture Onboarding

**Component Map**
Query -> CoT BFS (LLM scoring + dual-threshold pruning) -> Taxonomy traversal -> Leaf category predictions

**Critical Path**
1. Receive user query
2. Start at root category, score children using LLM
3. Apply dual-threshold pruning
4. Traverse pruned branches to leaves
5. Rescore final leaf candidates
6. Return predictions

**Design Tradeoffs**
- LLM-based scoring vs. embedding similarity: Higher semantic understanding vs. computational cost
- Dual-threshold pruning vs. exhaustive search: Better precision/recall balance vs. potential missed categories
- Top-down traversal vs. other strategies: Systematic exploration vs. possible local optima

**Failure Signatures**
- LLM outputs text instead of scores: Indicates prompt engineering issues
- Empty predictions: Thresholds too restrictive, need adjustment
- Low precision despite high recall: Minimum threshold too low, allowing irrelevant categories

**3 First Experiments**
1. Implement CoT BFS on a small sample taxonomy (5-10 categories) with 20 test queries to validate basic functionality
2. Compare CoT BFS vs. k-NN baseline on a subset of 100 queries to verify performance trends
3. Test threshold sensitivity by varying selection-threshold from 7 to 10 and measuring impact on precision/recall

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary eBay taxonomy structure unavailable, requiring reconstruction with alternative taxonomies
- Exact LLM prompt template and score standardization procedure unspecified
- Evaluation uses LLM-generated pseudo-references for majority of queries, introducing potential bias
- Apples-to-oranges comparison with k-NN baseline (semantic scoring vs. vector similarity)

## Confidence
- **High confidence**: Core method innovation and dual-threshold pruning mechanism are well-specified
- **Medium confidence**: Performance improvements are well-documented but depend on proprietary data
- **Low confidence**: Retrieval test methodology and PEGFB model details are insufficiently described

## Next Checks
1. Implement CoT BFS using Google Product Taxonomy and evaluate on held-out test set with human judgments
2. Conduct ablation studies varying selection-threshold and minimum-threshold parameters
3. Test method robustness across different e-commerce domains (retail vs. electronics)