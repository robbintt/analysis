---
ver: rpa2
title: Implementing a Logical Inference System for Japanese Comparatives
arxiv_id: '2509.13734'
source_url: https://arxiv.org/abs/2509.13734
tags:
- heavy
- semantic
- jiro
- comparatives
- than
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a logic-based system for Japanese Natural Language
  Inference (NLI) involving comparative expressions. Unlike previous English-focused
  approaches, the authors develop ccg-jcomp, a system that handles morphological and
  semantic differences specific to Japanese comparatives using Combinatory Categorial
  Grammar (CCG) and degree semantics.
---

# Implementing a Logical Inference System for Japanese Comparatives

## Quick Facts
- **arXiv ID:** 2509.13734
- **Source URL:** https://arxiv.org/abs/2509.13734
- **Reference count:** 17
- **Primary result:** Logic-based ccg-jcomp system achieves 84.5% accuracy on Japanese NLI comparatives, outperforming GPT-4o (77.4%), Swallow 8B (54.9%), and Swallow 70B (71.2%).

## Executive Summary
This paper presents ccg-jcomp, a logic-based system for Japanese Natural Language Inference (NLI) involving comparative expressions. Unlike previous English-focused approaches, the authors develop a system that handles morphological and semantic differences specific to Japanese comparatives using Combinatory Categorial Grammar (CCG) and degree semantics. The system compositionally derives semantic representations through syntactic and semantic parsing and uses theorem proving to judge entailment. Tested on a Japanese NLI dataset (JSeM), ccg-jcomp achieved 84.5% accuracy, outperforming GPT-4o (77.4%), Swallow 8B (54.9%), and Swallow 70B (71.2%). The system successfully handles Japanese-specific phenomena such as the absence of overt comparative morphemes, equatives, clausal comparatives, and presupposition triggers, demonstrating the effectiveness of logic-based approaches for comparative NLI in Japanese.

## Method Summary
The ccg-jcomp system implements a pipeline approach: Japanese sentences are tokenized, parsed into CCG trees using depccg, modified with Tsurgeon rules (60 entries) to handle comparative-specific constructions and insert empty categories, semantically parsed using ccg2lambda with 222 semantic templates (58 for comparatives), converted to TPTP format, and evaluated using Vampire 4.9 theorem prover with a 20-second timeout per proof. The system uses degree semantics following the A-not-A analysis with 5 axioms (CP, ANT, UP, DOWN, DELTA) and handles Japanese-specific phenomena through empty category insertion and multidimensional semantics for presupposition projection.

## Key Results
- ccg-jcomp achieves 84.5% accuracy on JSeM comparatives dataset
- Outperforms GPT-4o (77.4%), Swallow 8B (54.9%), and Swallow 70B (71.2%)
- Successfully handles Japanese-specific phenomena: morphologically silent comparatives, equatives, clausal comparatives, and presupposition triggers
- Error analysis reveals limitations with adjective-like verbs and comparative standard ambiguities

## Why This Works (Mechanism)

### Mechanism 1: Compositional Semantic Derivation via Degree Semantics
The system maps Japanese comparative sentences to logical formulas by assigning semantic templates to lexical items and composing them via CCG combinatory rules. Each word receives a lambda-calculus semantic template, with adjectives treated as binary predicates `heavy(x, d)` where `d` is a degree. The comparative marker "yori" receives a template introducing existential quantification over degrees with the A-not-A analysis: `∃d.(A(x, d) ∧ ¬A(y, d))`. CCG forward/backward application rules compose these compositionally.

### Mechanism 2: Empty Category Insertion for Morphologically Silent Comparatives
Japanese lacks overt comparative morphemes, so the system inserts an unpronounced `cmp` category to unify semantic representations between comparative and non-comparative uses. When no comparative expression is detected, Tsurgeon rules insert an empty category `cmp` with semantic representation `λS.S(λAx.A(x, θ))`, where θ is a contextually-determined comparison criterion. This makes "Taro-wa omoi" receive `heavy(taro, θ)` while "Taro-wa Jiro yori omoi" receives `∃d.(heavy(taro, d) ∧ ¬heavy(jiro, d))`.

### Mechanism 3: Multidimensional Semantics for Presupposition Projection
Japanese comparatives with "izyoo-ni", "to onaji kurai", and "hodo" carry presuppositions that project through negation. The sentence "Taro-wa Jiro izyoo-ni omoi" receives representation `⟨∃d.(heavy(taro, d) ∧ ¬heavy(jiro, d)), heavy(jiro, θ)⟩` where the first element is at-issue content and the second is the presupposition that Jiro is heavy. Under negation, only the at-issue content is negated; the presupposition persists. Theorem proving conjoins both dimensions.

## Foundational Learning

- **Combinatory Categorial Grammar (CCG)**
  - Why needed here: CCG provides the typed syntactic framework that enables semantic composition. Without understanding categories like `S\NP` (verb phrase) and combinatory rules, you cannot debug parsing failures or extend semantic templates.
  - Quick check question: Given category `S\NP` for "omoi" and `NP` for "Taro", what category results from backward application?

- **Degree Semantics (A-not-A Analysis)**
  - Why needed here: This is the theoretical foundation for comparative meaning. The system's semantic templates encode this analysis directly—you need to understand why "heavier than Bob" becomes `∃d.(heavy(john,d) ∧ ¬heavy(bob,d))` to modify or extend templates.
  - Quick check question: Why does the A-not-A representation use negation (`¬heavy(bob, d)`) rather than inequality (`d > d_bob`)?

- **First-Order Theorem Proving (Resolution)**
  - Why needed here: The system relies on Vampire to determine entailment. Understanding how axioms like CP (`∀xy.(∃d.(A(x,d) ∧ ¬A(y,d)) → ∀d.(A(y,d) → A(x,d)))`) enable inference is essential for adding new axioms or debugging proof failures.
  - Quick check question: Given premises `heavy(jiro, 70)` and `∀d.(heavy(jiro,d) → heavy(taro,d))`, can a resolution prover derive `heavy(taro, 70))`?

## Architecture Onboarding

- **Component map:** Input Sentence → Janome Tokenizer → depccg CCG Parser → CCG Tree → Tsurgeon Tree Conversion → Modified CCG Tree → ccg2lambda Semantic Parsing → Logical Formulas → TPTP Format Converter + Axiom Selection → Vampire Theorem Prover → Yes/No/Unknown

- **Critical path:** CCG parsing quality determines everything downstream. If depccg produces incorrect trees (e.g., misattaching "yori"), Tsurgeon fixes may not apply, semantic templates won't match, and the pipeline fails silently (treated as incorrect answers).

- **Design tradeoffs:**
  - Tsurgeon vs. Parser Retraining: Authors chose post-hoc tree modification (60 Tsurgeon rules) over fine-tuning depccg. Trade-off: faster iteration but fragile pattern-matching that may not generalize to unseen constructions.
  - Single Templates vs. Ambiguity Handling: "APCOM-no keiyaku" ambiguity causes errors. System currently picks one reading; implementing multiple readings would require disambiguation logic.
  - Modal Replacement: Modals replaced with negation to fit non-modal semantic framework. This sidesteps modal-semantics complexity but may not preserve all inference patterns.

- **Failure signatures:**
  - Type mismatch errors: Adjective-like verbs ("magatte-i-ru") don't match `S\NP` template expectations.
  - Ambiguity under-specification: Genitive ambiguities ("APCOM-no keiyaku") receive wrong interpretations.
  - Parser coverage gaps: Novel syntactic patterns not covered by depccg or Tsurgeon rules.
  - Timeout: 20-second theorem proving limit may insufficient for complex multi-premise problems.

- **First 3 experiments:**
  1. **Pipeline validation on gold trees:** Bypass depccg by manually creating correct CCG trees for 10 test sentences. This isolates whether failures are parsing-induced vs. semantic/template bugs.
  2. **Ablation on axioms:** Run system with subsets of axioms {CP only, CP+ANT, CP+ANT+UP/DOWN, all} to measure contribution of each axiom class to accuracy.
  3. **Error taxonomy on JSeM:** Categorize all 71 problems by linguistic phenomenon (equatives, clausal comparatives, presuppositions, measure phrases) and compute per-category accuracy to identify weakest components.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the semantic type system be adapted to handle Japanese "adjective-like verbs" (e.g., *magatte-i-ru*) that currently cause parsing errors due to type mismatches?
- **Basis in paper:** [explicit] Section 6.2 (Error Analysis) and Section 7 (Conclusion) identify "adjective-like verbs" as a specific failure mode where the system fails to match the semantic type required for the argument of "yori".
- **Why unresolved:** The system currently treats these verbs as standard verbs rather than predicates compatible with degree semantics, requiring exceptional semantic representations that have not yet been implemented.
- **What evidence would resolve it:** A modified semantic template for aspectual forms (like *te-i-ru*) that successfully parses and proves entailments for sentences containing these specific verbs without generating type errors.

### Open Question 2
- **Question:** Can a mechanism be implemented to effectively disambiguate comparative standards (e.g., "APCOM-no keiyaku") based on syntactic features or context?
- **Basis in paper:** [explicit] Section 6.2 and Section 7 (Conclusion) cite ambiguity in phrases like "APCOM-no keiyaku" as a source of error, noting the need for a system to distinguish between interpretations.
- **Why unresolved:** The current system lacks a method to select between multiple valid semantic interpretations when syntactic structure alone is insufficient, leading to inference failures.
- **What evidence would resolve it:** A disambiguation module that correctly identifies the semantic role of the comparative standard in ambiguous constructions, validated by correct entailment judgments on previously problematic examples.

### Open Question 3
- **Question:** Can degree semantics be compositionally integrated with dynamic semantics to create a unified system capable of handling both comparatives and distinct linguistic phenomena like anaphora?
- **Basis in paper:** [explicit] Section "Limitations" explicitly states that integrating mechanisms like dynamic semantics for anaphora with degree semantics is "not trivial" and is left for future work.
- **Why unresolved:** The current system focuses exclusively on comparatives and lacks the architectural framework to process anaphora or other phenomena included in the broader JSeM dataset.
- **What evidence would resolve it:** A unified logic-based inference system that maintains high accuracy on comparative benchmarks while simultaneously resolving coreference chains found in the non-comparative sections of JSeM.

### Open Question 4
- **Question:** Would fine-tuning the CCG parser on comparative-specific data yield more robust performance than the current post-hoc tree modification approach using Tsurgeon?
- **Basis in paper:** [explicit] Section 3.1 notes that while revising the parser is possible, it is "costly," leading the authors to use Tsurgeon scripts and leave parser re-training for future work.
- **Why unresolved:** It is unclear if the rule-based tree manipulation introduces errors or limitations that a parser inherently trained on comparative structures would avoid.
- **What evidence would resolve it:** A comparative evaluation showing the accuracy delta between a fine-tuned CCG parser and the current Tsurgeon-based pipeline on the JSeM dataset.

## Limitations
- The Tsurgeon-based tree modification approach introduces brittleness that may not generalize to novel comparative constructions
- The modal-to-negation replacement strategy is a simplification that may not preserve all inference patterns for sentences containing modality
- The system currently lacks a mechanism to handle adjective-like verbs (e.g., *magatte-i-ru*) that cause type mismatches in semantic parsing

## Confidence
- **High Confidence:** The core degree semantics approach (A-not-A analysis) and its implementation through semantic templates is theoretically sound and well-established in the literature. The superiority over LLM baselines (84.5% vs 77.4% for GPT-4o) is clearly demonstrated.
- **Medium Confidence:** The multidimensional semantics framework for handling presuppositions is correctly implemented for the specific cases tested, though the generalization to more complex presupposition projection scenarios remains unverified.
- **Medium Confidence:** The CCG-based compositional approach works for the JSeM dataset, but the brittleness of the Tsurgeon-based tree modification strategy raises concerns about scalability to broader Japanese NLI tasks.

## Next Checks
1. **Parser Isolation Test:** Manually construct correct CCG trees for 10 test sentences to determine whether failures stem from depccg parsing errors versus semantic/template issues.
2. **Axiom Contribution Analysis:** Systematically ablate different axiom sets (CP only, CP+ANT, CP+ANT+UP/DOWN, all) to quantify their individual contributions to accuracy.
3. **Per-Category Performance Analysis:** Categorize all 71 JSeM problems by linguistic phenomenon (equatives, clausal comparatives, presuppositions, measure phrases) and compute accuracy per category to identify specific weaknesses in the system.