---
ver: rpa2
title: 'SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization
  for Large Language Models'
arxiv_id: '2602.01027'
source_url: https://arxiv.org/abs/2602.01027
tags:
- sfmp
- quantization
- mixed-precision
- weight
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SFMP introduces a search-free and hardware-friendly mixed-precision
  quantization framework for large language models. It extends integer bit-width to
  fractional values, enabling direct precision allocation based on global weight salience
  without solving discrete optimization problems.
---

# SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models

## Quick Facts
- arXiv ID: 2602.01027
- Source URL: https://arxiv.org/abs/2602.01027
- Reference count: 40
- Key outcome: Search-free mixed-precision quantization framework achieving up to 98.90% full-precision zero-shot accuracy at 3.5 bits per weight for LLaMA3.1-70B

## Executive Summary
SFMP introduces a novel search-free mixed-precision quantization framework for large language models that extends integer bit-widths to fractional values. The method enables direct precision allocation based on global weight salience without solving discrete optimization problems, combining block-wise mixed-precision with row-column weight reordering to aggregate salient weights. SFMP achieves up to 98.90% of full-precision zero-shot accuracy at 3.5 bits per weight for LLaMA3.1-70B while significantly improving inference speed compared to group-wise mixed-precision methods.

## Method Summary
SFMP introduces a search-free mixed-precision quantization framework that extends integer bit-widths to fractional values, enabling direct precision allocation based on global weight salience. The method combines block-wise mixed-precision with row-column weight reordering to aggregate salient weights and align them with hardware-friendly memory layouts. A unified GEMM kernel eliminates weight dequantization overhead through a one-bit LUT-based design. The framework requires only 0.05-0.15 hours to compress models versus 44 hours for AMQ, while maintaining competitive or better perplexity and downstream task performance across multiple model scales (8B, 14B, 32B, 70B).

## Key Results
- Achieves up to 98.90% of full-precision zero-shot accuracy at 3.5 bits per weight for LLaMA3.1-70B
- Improves inference speed by 30-50% compared to group-wise mixed-precision (SliM-LLM)
- Reduces compression time to 0.05-0.15 hours versus 44 hours for AMQ under the same memory constraints

## Why This Works (Mechanism)
The fractional bit-width approach allows for fine-grained precision allocation by representing precision as continuous values rather than discrete integers. This enables optimal allocation of precision resources based on weight salience without the computational overhead of solving discrete optimization problems. The block-wise mixed-precision combined with row-column weight reordering aggregates salient weights together, creating memory layouts that are naturally compatible with hardware acceleration. The unified GEMM kernel with one-bit LUT design eliminates dequantization overhead, maintaining efficiency even at very low bit-widths.

## Foundational Learning

**Fractional Bit-Width Quantization**
- Why needed: Traditional quantization methods use discrete integer bit-widths, limiting precision allocation flexibility
- Quick check: Verify that fractional precision values can be mapped to actual hardware quantization parameters

**Weight Salience Analysis**
- Why needed: Identifies which weights contribute most to model accuracy for optimal precision allocation
- Quick check: Confirm salience scores correlate with actual weight importance through ablation studies

**Block-wise Mixed-Precision Layout**
- Why needed: Aggregates high-precision weights together for hardware efficiency
- Quick check: Measure memory access patterns and cache utilization with different block sizes

**Unified GEMM Kernel with LUT**
- Why needed: Eliminates dequantization overhead that typically slows down low-bit quantization
- Quick check: Compare inference latency with and without the unified kernel implementation

## Architecture Onboarding

**Component Map**
Input Weights -> Salience Analysis -> Fractional Precision Allocation -> Block-wise Reordering -> Row-Column Reordering -> Unified GEMM Kernel -> Quantized Weights

**Critical Path**
Salience Analysis -> Fractional Precision Allocation -> Block-wise Reordering -> Unified GEMM Kernel

**Design Tradeoffs**
- Precision granularity vs. hardware compatibility
- Compression speed vs. optimization quality
- Memory layout efficiency vs. implementation complexity

**Failure Signatures**
- Accuracy degradation when block size is too small or too large
- Speed slowdown when LUT size exceeds shared memory capacity
- Suboptimal compression when salience estimation is inaccurate

**3 First Experiments**
1. Test salience analysis accuracy on a small subset of weights
2. Verify fractional precision allocation maintains accuracy compared to uniform quantization
3. Benchmark the unified GEMM kernel against standard integer kernels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the group size ($n_b$) be made adaptive rather than fixed to better optimize the accuracy-memory trade-off?
- Basis in paper: Appendix H.1.2 observes a non-monotonic relationship between group size and accuracy, and Appendix J lists "adaptive group sizes" as a promising direction
- Why unresolved: Currently fixed hyperparameter may waste memory budget at low bits or fail to capture heterogeneity at high bits
- What evidence would resolve it: A search-free algorithm that dynamically selects optimal group sizes per layer under strict memory constraints

### Open Question 2
- Question: How can the proposed unified GEMM kernel be optimized to maintain efficiency for batch sizes greater than 1?
- Basis in paper: Appendix J states the work targets memory-constrained edge scenarios with batch size = 1
- Why unresolved: The one-bit LUT-based GEMM design is tailored for batch-1 decode phase
- What evidence would resolve it: Latency and throughput benchmarks at batch sizes of 4, 8, or higher

### Open Question 3
- Question: Can the SFMP framework be effectively ported to alternative hardware architectures such as CPUs, NPUs, or TPUs?
- Basis in paper: Appendix J lists "Broader hardware support" as a limitation
- Why unresolved: The unified kernel relies heavily on specific CUDA features that may not map to other architectures
- What evidence would resolve it: Implementation on CPU/NPU showing comparable speedups over standard integer kernels

## Limitations
- Fractional bit-width approach may face practical implementation challenges in real hardware systems
- Performance claims require careful interpretation due to different optimization approaches in baseline methods
- Inference speed improvements methodology could benefit from more detailed hardware specifications and batch size variations

## Confidence

**High**: Hardware-friendly GEMM implementation and basic quantization framework
**Medium**: Fractional bit-width concept and its practical implementation
**Medium**: Performance comparisons against baseline methods

## Next Checks

1. Test SFMP across diverse model architectures beyond LLaMA3.1 to verify generalization of fractional precision allocation
2. Implement and benchmark the fractional bit-width approach on actual hardware platforms (not just simulated)
3. Conduct ablation studies to isolate the contribution of each component (fractional bits, reordering, GEMM optimization) to overall performance