---
ver: rpa2
title: 'Extending CREAMT: Leveraging Large Language Models for Literary Translation
  Post-Editing'
arxiv_id: '2504.03045'
source_url: https://arxiv.org/abs/2504.03045
tags:
- translation
- creativity
- post-editing
- editing
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the feasibility of post-editing literary
  translations generated by large language models (LLMs). Four professional translators
  post-edited translations produced by GPT-4, GPT-3.5, and a literary-adapted Mistral-7B
  model, with editing time, quality, and creativity measured.
---

# Extending CREAMT: Leveraging Large Language Models for Literary Translation Post-Editing

## Quick Facts
- **arXiv ID**: 2504.03045
- **Source URL**: https://arxiv.org/abs/2504.03045
- **Reference count**: 10
- **Primary result**: Post-editing LLM-generated literary translations significantly reduces editing time while maintaining creativity levels comparable to human translation

## Executive Summary
This study investigates the feasibility of post-editing literary translations generated by large language models (LLMs). Four professional translators post-edited translations produced by GPT-4, GPT-3.5, and a literary-adapted Mistral-7B model, with editing time, quality, and creativity measured. Results show that post-editing LLM-generated translations significantly reduces editing time compared to human translation, while maintaining similar levels of creativity. The quality-to-time ratio was highest for GPT-4 (0.38), followed by Mistral-60k (0.29) and GPT-3.5 (0.28). Creativity scores were comparable across all translation variants (ranging from 20.1% to 26.5%), suggesting LLMs can support literary translators without compromising creativity. The domain-adapted Mistral model demonstrated promising performance, achieving a quality-to-time ratio higher than GPT-3.5 despite requiring more edits.

## Method Summary
The study employed a controlled experiment where four professional translators post-edited translations of eight literary excerpts generated by three different LLMs: GPT-4, GPT-3.5, and a literary-adapted Mistral-7B model. Editing time was measured using screen recording software, while quality was assessed using a custom CREAMT framework that evaluates Creativity, Reproducibility, Error, Acceptability, Meaning, and Terminology. Creativity was specifically measured using a creativity-to-time ratio. Translators worked in three sessions, post-editing outputs from each model across different text genres to control for fatigue effects and ensure comprehensive evaluation.

## Key Results
- Post-editing LLM-generated translations reduced editing time by 35-40% compared to human translation
- Quality-to-time ratio was highest for GPT-4 (0.38), followed by Mistral-60k (0.29) and GPT-3.5 (0.28)
- Creativity scores were comparable across all translation variants (20.1% to 26.5%)
- The domain-adapted Mistral model outperformed GPT-3.5 in quality-to-time ratio despite requiring more edits

## Why This Works (Mechanism)
Assumption: The efficiency gains from post-editing LLM-generated translations stem from the models' ability to produce coherent and stylistically appropriate initial drafts, reducing the cognitive load on translators. The literary-adapted Mistral model's domain-specific training likely enhanced its performance by better capturing nuanced literary expressions, contributing to its competitive quality-to-time ratio despite requiring more edits.

## Foundational Learning
Unknown: The study does not explicitly detail the foundational learning principles behind the LLM models used. However, it can be inferred that the models leverage large-scale pretraining on diverse text corpora, with the Mistral model additionally fine-tuned on literary texts to enhance its domain-specific capabilities.

## Architecture Onboarding
Unknown: The report does not provide specific details on the architecture onboarding process for the LLM models. It is assumed that the models underwent standard pretraining and fine-tuning procedures, with the Mistral model receiving additional literary adaptation to improve its performance in the translation task.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's findings are constrained by a small sample size of only four professional translators
- The evaluation focused on a limited number of literary excerpts (eight texts), potentially affecting the robustness of quality and creativity assessments
- The study did not account for long-term consistency in post-editing practices or potential fatigue effects on translators working with AI-generated content

## Confidence
- **High Confidence**: The reduction in editing time when post-editing LLM-generated translations compared to human translation is a robust finding, supported by consistent measurements across translators and models
- **Medium Confidence**: The similarity in creativity scores across translation variants is plausible but may be influenced by the subjective nature of creativity assessment and the specific evaluation methodology used
- **Medium Confidence**: The performance ranking of GPT-4, Mistral-60k, and GPT-3.5 based on quality-to-time ratios is supported by the data but could vary with different literary texts or evaluation criteria

## Next Checks
1. **Expand Sample Size**: Conduct the study with a larger group of professional translators to assess the generalizability of the findings across different skill levels and literary genres
2. **Longitudinal Study**: Implement a longitudinal study to evaluate the consistency and potential fatigue effects of post-editing LLM-generated translations over extended periods
3. **Diverse Literary Texts**: Test the models with a broader range of literary texts, including different genres and languages, to validate the robustness of the quality and creativity assessments