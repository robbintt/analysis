---
ver: rpa2
title: 'CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences'
arxiv_id: '2511.07691'
source_url: https://arxiv.org/abs/2511.07691
tags:
- capo
- reward
- preference
- languages
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAPO, a multilingual preference optimization
  method that improves upon DPO by incorporating a relative reward margin into the
  loss function. The approach dynamically scales the learning signal based on confidence
  in preference pairs, addressing the limitation of treating all preference margins
  equally.
---

# CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences

## Quick Facts
- **arXiv ID**: 2511.07691
- **Source URL**: https://arxiv.org/abs/2511.07691
- **Reference count**: 16
- **Primary result**: 16-33% higher reward accuracy than baselines on multilingual benchmarks

## Executive Summary
CAPO addresses limitations in multilingual preference optimization by introducing a Relative Reward Margin (RRM) term that dynamically scales the learning signal based on confidence in preference pairs. The method improves upon DPO by replacing fixed-margin treatment with a ratio-based scaling mechanism that adjusts for tokenization-induced variability across languages. Evaluated on MT-Bench, XLSum, and M-IFEval benchmarks, CAPO demonstrates significant improvements in alignment quality while reducing overfitting to ambiguous preference pairs.

## Method Summary
CAPO modifies DPO by incorporating a Relative Reward Margin term that scales the loss based on the ratio of log-probabilities between preferred and dispreferred responses. The method uses LoRA-based parameter-efficient fine-tuning with a base Llama-3.1-8B-Instruct model, training on MTPE datasets converted to preference pairs across 8 language directions. The approach dynamically adjusts learning signal strength per example, emphasizing clear preferences while downweighting ambiguous pairs, and omits length normalization to avoid multilingual unfairness from tokenization rate differences.

## Key Results
- Achieves 16-33% higher reward accuracy than DPO, SimPO, and DPONLL baselines on multilingual benchmarks
- Improves alignment across languages by widening the gap between preferred and dispreferred responses
- Demonstrates particular effectiveness in languages with lower BLEU scores between preferred and dispreferred samples
- Shows stable convergence with reduced overfitting to ambiguous preference pairs

## Why This Works (Mechanism)

### Mechanism 1: Relative Reward Margin (RRM) as Dynamic Confidence Scaling
CAPO dynamically adjusts learning signal strength per example based on the relative reward ratio between preferred and dispreferred responses, rather than treating all preference margins uniformly. The RRM term α · logπ(yw|x) / logπ(yl|x) scales the loss: when the preferred log-likelihood substantially exceeds dispreferred, RRM increases and loss diminishes (signaling good alignment); when margins are small, RRM downweights the signal to avoid overfitting to noisy/ambiguous pairs. Core assumption: The ratio of log-probabilities reflects meaningful preference confidence that absolute differences fail to capture, especially when reward magnitudes vary significantly.

### Mechanism 2: Per-Example Calibration for Tokenization-Induced Variability
CAPO reduces unfair bias toward languages with shorter tokenizations or higher per-token log-probs by scaling loss on a per-example basis rather than applying uniform treatment. Because tokenization rates vary across languages (e.g., English vs. Nepali), the same absolute reward difference can represent different relative preference strengths. RRM adjusts per-example to account for these disparities, ensuring reward signals reflect true preference strength rather than tokenization artifacts. Core assumption: Tokenization rate differences cause systematic reward signal distortions that relative scaling can correct.

### Mechanism 3: Noise Robustness via Downweighting Ambiguous Pairs
CAPO reduces overfitting to low-confidence or ambiguous preference pairs by softening gradients when the reward margin is uncertain. The RRM term acts as a confidence-weighted modulator: ambiguous pairs (small margin) receive lower learning signal intensity, while clear preferences receive stronger optimization pressure. This is particularly beneficial for multilingual settings where preferences are harder to model. Core assumption: Ambiguous pairs are noisier and less reliable; downweighting them improves generalization without discarding data.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** CAPO is a modification of DPO; understanding DPO's fixed-margin Bradley-Terry objective is prerequisite to grasping what CAPO changes.
  - **Quick check question:** Can you write out the standard DPO loss LBT = −logσ(β[logπ(yw|x) − logπ(yl|x)]) and explain what each term represents?

- **Concept: Log-likelihood ratios vs. differences**
  - **Why needed here:** CAPO introduces a ratio-based term (RRM) alongside the difference-based DPO term; understanding why ratios capture relative preference strength differently is essential.
  - **Quick check question:** Given two preference pairs with the same absolute reward difference (Δr=1), explain why r=[5,4] vs. r=[1.2,0.2] might represent different preference strengths.

- **Concept: Multilingual tokenization disparities**
  - **Why needed here:** A core motivation for CAPO is language-specific tokenization rate differences affecting reward distributions.
  - **Quick check question:** Why might the same sentence in English vs. Vietnamese produce different tokenized lengths, and how could this affect per-token log-probability-based rewards?

## Architecture Onboarding

- **Component map:** Input preference pairs → Log-likelihood computation → RRM term calculation → CAPO loss aggregation → Gradient update
- **Critical path:** Preference data preparation → log-likelihood extraction → RRM computation → loss aggregation → gradient update
- **Design tradeoffs:** Higher α emphasizes clear preferences but risks over-reweighting; paper finds α=2.0 optimal, sharp drop beyond. No reference model reduces memory/compute but removes explicit KL constraint. No length normalization avoids multilingual unfairness but may affect other domains differently.
- **Failure signatures:** Negative reward differences persisting after training (Figure 3 shows DPO has this issue; CAPO should shift distribution rightward). Training loss not decreasing monotonically (CAPO should show stable convergence per Figure 4). Per-language reward accuracy not improving (CAPO should show ≥16% improvement over DPO).
- **First 3 experiments:**
  1. **Baseline replication:** Run DPO on MTPE preference data (DIVEMT + MLQE-PE) with Llama-3.1-8B-Instruct; verify reward accuracy and Δr distribution match paper baselines
  2. **α sweep:** Train CAPO with α ∈ {1.0, 1.5, 2.0, 2.5, 3.0}; plot training loss vs. evaluation accuracy to confirm α=2.0 peak
  3. **Cross-lingual generalization test:** Evaluate trained CAPO on held-out languages (fr, es, ja per paper's unseen set) on MT-Bench and XLSum; verify win rates over DPO

## Open Questions the Paper Calls Out

**Question 1:** How can high-quality MTPE (Machine Translated Post-Edited) data be automatically generated to expand language coverage and better reflect true human preferences across diverse languages? The conclusion states: "While we relied on existing MTPE data for training, future work should consider automatically generating high-quality MTPE data to expand language coverage and better reflect true human preferences." Current work repurposes existing MTPE datasets covering only 8 language directions, limiting scalability and generalizability to low-resource languages.

**Question 2:** How does CAPO perform under full fine-tuning compared to the LoRA-based parameter-efficient approach used in this study? Limitations section states: "While full fine-tuning may yield different insights, our results demonstrate that even with efficient, lightweight training, CAPO achieves meaningful improvements." All experiments use LoRA (rank=16, alpha=32) with small training sets (100 samples per direction); the trade-offs between parameter efficiency and alignment quality under full fine-tuning remain unexplored.

**Question 3:** Why does CAPO underperform on English compared to other languages, and how does the source language of MTPE data affect preference alignment? The paper notes: "CAPO's poor performance in en may be due to using EN → other languages MTPE data, which might have introduced learned preferences that are subtly different from native English data." The hypothesis about cross-lingual artifact transfer from MTPE data is not empirically validated; alternative explanations (e.g., English data saturation, reward model bias) are not ruled out.

## Limitations

- **RRM formulation validity:** No direct experimental ablation showing the ratio formulation is superior to alternative confidence metrics like absolute margin variance or entropy-based uncertainty.
- **Cross-lingual tokenization impact:** Lacks empirical validation of the claim that tokenization disparities cause unfair reward signal distortions; no analysis of magnitude of tokenization-rate differences in datasets.
- **Ambiguous pair handling:** May over-downweight low-margin pairs that could contain valuable signal, particularly for languages where preferences are inherently harder to distinguish.

## Confidence

**High confidence:** The core architectural contribution (RRM term integration into DPO loss) is clearly specified and the mathematical formulation is sound. The 16-33% reward accuracy improvements over baselines on MT-Bench, XLSum, and M-IFEval are empirically demonstrated with reasonable statistical significance.

**Medium confidence:** The multilingual alignment improvements and the claim that CAPO widens the gap between preferred/dispreferred responses are supported by experimental results, though the mechanism (tokenization rate correction) lacks direct validation. The α=2.0 optimal value finding is based on validation performance but the search range and methodology are underspecified.

**Low confidence:** The noise robustness claims and the specific advantage for languages with lower BLEU scores between preferred and dispreferred samples are asserted but not rigorously tested. No controlled experiments examine CAPO's performance on deliberately noisy or ambiguous preference pairs versus clean pairs.

## Next Checks

1. **RRM ablation study:** Implement alternative confidence metrics (absolute margin variance, entropy-based uncertainty) and compare their performance against the ratio-based RRM formulation on the same multilingual datasets to isolate the specific contribution of the ratio approach.

2. **Tokenization impact quantification:** Measure per-language tokenization rates in the DIVEMT and MLQE-PE datasets and analyze the correlation between tokenization length and reward distributions. Compare reward accuracy before and after CAPO application stratified by tokenization rate to validate the claimed correction mechanism.

3. **Ambiguous pair sensitivity analysis:** Create controlled experiments with varying levels of preference ambiguity (synthetic low-margin pairs, human-annotated uncertainty scores) and test whether CAPO's downweighting improves or harms performance compared to treating all pairs uniformly, particularly for languages with inherently noisier preferences.