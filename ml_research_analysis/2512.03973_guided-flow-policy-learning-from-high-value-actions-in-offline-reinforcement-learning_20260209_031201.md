---
ver: rpa2
title: 'Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement
  Learning'
arxiv_id: '2512.03973'
source_url: https://arxiv.org/abs/2512.03973
tags:
- tasks
- policy
- actor
- learning
- minari
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Guided Flow Policy (GFP) introduces value-aware behavior cloning
  (VaBC) to address the challenge of offline reinforcement learning with suboptimal
  datasets. The method couples a multi-step flow-matching policy with a distilled
  one-step actor through bidirectional guidance, where VaBC selectively clones high-value
  actions from the dataset rather than indiscriminately imitating all state-action
  pairs.
---

# Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.03973
- Source URL: https://arxiv.org/abs/2512.03973
- Reference count: 29
- Key outcome: State-of-the-art performance on 144 offline RL tasks by selectively cloning high-value actions via value-aware behavior cloning

## Executive Summary
Guided Flow Policy (GFP) addresses the challenge of learning from suboptimal offline datasets by introducing value-aware behavior cloning (VaBC). Unlike standard behavior cloning that indiscriminately imitates all state-action pairs, VaBC selectively weights dataset actions based on their estimated value, prioritizing high-value transitions while down-weighting poor ones. This is achieved through a bidirectional guidance system where a flow-matching policy learns to generate actions conditioned on dataset quality, and an actor policy distills this flow policy while maintaining expressivity. The method achieves state-of-the-art performance across OGBench, Minari, and D4RL benchmarks, particularly excelling on challenging manipulation and navigation tasks where previous methods struggle.

## Method Summary
GFP couples a multi-step flow-matching policy with a distilled one-step actor through bidirectional guidance. The flow policy π_ω learns a velocity field via conditional flow matching, while VaBC π_ω selectively clones high-value actions from the dataset using a guidance function g_η(s,a) that compares Q-values between dataset actions and actor proposals. The actor π_θ maximizes Q-values while staying aligned with VaBC through a distillation term. This architecture enables selective regularization—the actor can exploit critic signals while VaBC anchors it to dataset quality—avoiding the distribution shift problem common in offline RL. Training alternates between updating a critic network, updating the flow policy with weighted BC loss, and updating the actor with Q-maximization plus VaBC distillation.

## Key Results
- Achieves state-of-the-art performance across 144 state and pixel-based tasks from OGBench, Minari, and D4RL
- Substantial gains on suboptimal datasets: cube-triple-noisy (24.5 vs 4.0 FQL, 3.5 ReBRAC)
- Superior navigation performance: humanoidmaze-large-navigate (17.8 vs 6.5 FQL, 2.7 ReBRAC)
- Effective across both state and pixel-based inputs, demonstrating broad applicability

## Why This Works (Mechanism)

### Mechanism 1: Value-Aware Weighting
Value-aware weighting improves policy quality when datasets contain suboptimal actions by comparing Q-values of dataset actions against actor-sampled actions via a softmax. High-value dataset actions receive higher weight in the flow-matching BC loss, while low-value actions are down-weighted. This selectively shapes the VaBC policy toward the dataset's better transitions without full exclusion. The guidance function g_η ∈ (0,1) prevents degeneracy early on when the critic is unreliable.

### Mechanism 2: Bidirectional Guidance
Bidirectional guidance between flow policy and actor stabilizes learning while maintaining expressiveness. VaBC π_ω regularizes the actor π_θ through distillation, keeping the actor near high-value dataset support. The actor simultaneously maximizes Q_ϕ and its Q-values feed back into g_η, shaping what VaBC emphasizes. This creates mutual constraint: actor can exploit critic signals while VaBC anchors it to dataset quality.

### Mechanism 3: One-Step Actor Distillation
One-step actor distillation avoids backpropagation through time while preserving multi-step flow policy benefits. The flow policy requires M Euler integration steps to sample actions. By distilling to a one-step actor π_θ that directly maps noise z → action, inference is fast and gradients flow cleanly, while VaBC still benefits from flow model expressiveness during training.

## Foundational Learning

- **Flow Matching for Conditional Generation**: VaBC uses conditional flow matching to learn a velocity field v_ω(t,s,a_t) that transforms noise z → action. Understanding ODE-based generative models is essential for grasping how VaBC represents multimodal action distributions. *Quick check: Can you explain why flow matching avoids the iterative denoising of diffusion models while achieving similar expressiveness?*

- **Actor-Critic with Behavior Regularization (BRAC)**: GFP is a BRAC-family method. The core trade-off is between maximizing Q-values (exploitation) and staying close to the behavior policy (safety). GFP modifies the regularization to be value-aware rather than uniform. *Quick check: Why does standard BC regularization fail to distinguish high-value from low-value actions in a suboptimal dataset?*

- **Offline RL Distributional Shift Problem**: The fundamental challenge is that Q-functions can overestimate out-of-distribution actions. GFP's solution constrains the actor to dataset support while using value-awareness to prioritize better in-dataset actions. *Quick check: If the learned policy selects actions outside the dataset support, what goes wrong with the Q-function estimates?*

## Architecture Onboarding

- **Component map**: Critic Q_ϕ → Flow Policy π_ω → Actor π_θ → Guidance g_η (bidirectional feedback)
- **Critical path**: 1) Sample batch from dataset D, 2) Update critic with TD loss, 3) Sample actions from both actor and VaBC, 4) Compute guidance weights g_η for dataset actions, 5) Update VaBC with weighted flow-matching loss, 6) Update actor with Q-maximization + distillation loss, 7) Repeat for 1M gradient steps (OGBench) or 500K (D4RL)
- **Design tradeoffs**:
  - Temperature η: Low η (≤10⁻⁵) creates sharp binary filtering (risk: over-concentration, instability). High η (≥10⁻¹) weak filtering (risk: clones low-value actions). Moderate η (~10⁻³) balances selectivity with diversity.
  - BC coefficient α: Primary sensitivity knob. Too low → actor escapes dataset support. Too high → actor cannot exploit critic.
  - Batch size: Larger batches (1024) help on challenging tasks; default 256 works for most.
  - Discount γ: Task-specific; antmaze-large benefits from 0.995, humanoidmaze-large from 0.999.
- **Failure signatures**:
  - Critic overestimation: Actor exploits OOD actions → high training Q-values but low evaluation returns. Check: actor actions far from VaBC outputs.
  - VaBC collapse: Temperature too low → g_η near-binary, VaBC over-concentrates on few actions, loses diversity.
  - Weak regularization: α too low → actor diverges from VaBC, Q-values explode.
  - Conservative underperformance: Modified Bellman target y_VaBC can help on cube/humanoidmaze but may hurt elsewhere.
- **First 3 experiments**:
  1. Hyperparameter sweep on α: Fix η=10⁻³, sweep α logarithmically (10⁻² to 10²). This is the most critical parameter. Verify actor stays close to VaBC outputs.
  2. Temperature ablation on a noisy task: Run cube-double-noisy with η ∈ {10⁻⁵, 10⁻³, 10⁻¹}. Observe filtering behavior via P(g_η > δ) statistics (Fig. 4 pattern).
  3. Component ablation: Compare (a) full GFP, (b) GFP without value-awareness (g_η = 1, equivalent to FQL), (c) GFP without distillation (use flow policy directly). Quantify contribution of each mechanism.

## Open Questions the Paper Calls Out
- **Question 1**: Can the reliance on an accurate critic be reduced while maintaining GFP's performance advantages? The conclusion states GFP depends on the availability of a sufficiently accurate critic, and improvements are limited when the critic cannot reliably evaluate actions.
- **Question 2**: How can GFP be extended to settings with weaker or sparse reward signals? The authors explicitly propose this as a future research direction, noting the current guidance function may provide poor gradients when rewards are sparse.
- **Question 3**: Can the temperature parameter η be automatically adapted during training or selected without task-specific tuning? Section 4.2 shows optimal temperature varies substantially across tasks, with extreme values causing instability or poor filtering.

## Limitations
- Highly sensitive to hyperparameter tuning, particularly α and η, requiring careful calibration for optimal performance
- Relies heavily on accurate critic estimates, limiting effectiveness when Q-value estimates are unreliable
- Does not fully solve challenging tasks like humanoidmaze-large-navigate, suggesting limitations with very complex control problems

## Confidence
- **High confidence**: The mechanism of value-aware weighting improving performance on suboptimal datasets (supported by extensive OGBench/Minari results)
- **Medium confidence**: The bidirectional guidance architecture's stability benefits (demonstrated but sensitive to hyperparameters)
- **Low confidence**: Generalizability to arbitrary offline RL settings (results highly task-dependent, optimal η varies by 2 orders of magnitude)

## Next Checks
1. **Hyperparameter robustness study**: Systematically evaluate GFP across a grid of α and η values on cube-double-noisy to map the stability region and identify failure modes (over-concentration vs. weak filtering).
2. **Dataset quality sensitivity**: Test GFP on datasets with varying levels of suboptimal actions (e.g., 10%, 50%, 90% suboptimal) to quantify when value-awareness provides the most benefit versus standard regularization.
3. **Distributional shift analysis**: Measure actor action distance from VaBC outputs during training and evaluate Q-value overestimation when the actor approaches dataset boundaries to verify the regularization prevents OOD exploitation.