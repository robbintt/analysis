---
ver: rpa2
title: Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks
arxiv_id: '2510.00962'
source_url: https://arxiv.org/abs/2510.00962
tags:
- english
- rules
- questions
- accuracy
- grammatical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models (LLMs) perform
  worse when answering multiple-choice questions written in non-Standard American
  English (SAE) dialects versus SAE. Using grammatical perturbation tools, questions
  from three benchmark QA datasets (BoolQ, SciQ, MMLU) are transformed into six English
  dialects (African American, Appalachian, Chicano, Indian, Singaporean, Southern).
---

# Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks

## Quick Facts
- **arXiv ID:** 2510.00962
- **Source URL:** https://arxiv.org/abs/2510.00962
- **Reference count:** 39
- **Key outcome:** LLMs show up to 20 percentage point accuracy drops on non-Standard American English (SAE) dialect questions, with existential "it", zero copula, and "y'all" rules explaining most degradation

## Executive Summary
This paper reveals systematic performance gaps in large language models when answering questions written in non-Standard American English dialects versus SAE. Using automated grammatical perturbation tools, questions from three benchmark QA datasets are transformed into six English dialects, revealing accuracy drops of up to 20 percentage points. The study identifies three specific grammatical structures—existential "it," zero copula, and "y'all"—as primary drivers of dialectal bias across multiple dialects, accounting for 64-85% of degradation in American dialects.

## Method Summary
The study applies Multi-VALUE, an automated grammatical perturbation tool based on eWAVE dialect rules, to transform questions from BoolQ, SciQ, and MMLU datasets into six non-SAE dialects. The perturbations are applied only to question text while keeping answers unchanged. Three LLMs (Gemma-2B, Mistral-7B, GPT4o-mini) are evaluated zero-shot using LM Eval Harness. Performance is measured as accuracy drop conditioned on SAE-correct responses, with statistical significance via McNemar's test (p<0.05). The analysis isolates individual grammatical rules' contributions by applying perturbations at 100% frequency for specific rules.

## Key Results
- Accuracy drops of up to 20 percentage points for non-SAE dialects compared to SAE
- Singaporean English and African American English show the largest degradation (16-21%)
- Three grammatical rules (existential "it", zero copula, "y'all") explain 64-85% of degradation in American dialects
- Obligatory grammatical rules correlate with higher degradation across dialects
- Perplexity increases correlate with accuracy drops, with Singaporean English showing 3517% increase

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training data underrepresentation of non-SAE dialects causes systematic parsing failures in LLMs.
- Mechanism: LLMs trained predominantly on Standard American English (SAE) develop statistical representations that assign higher perplexity—and lower comprehension—to dialectal grammatical structures not seen during training. The paper reports Singaporean English showing perplexity increases of up to 3517% (Table 11), correlating with the highest accuracy drops.
- Core assumption: The performance gap stems from data distribution skew rather than architectural limitations.
- Evidence anchors:
  - [abstract] "up to a 20% reduction in accuracy" for non-SAE dialects
  - [section 3.1] "substantial increases in perplexity when SAE is transformed into dialectal variants, with Singaporean English demonstrating the most dramatic increases"
  - [corpus] Related work (Hofmann et al., 2024, cited in paper) confirms AI generates "covertly racist decisions about people based on their dialect"
- Break condition: If models were trained on balanced dialectal corpora without SAE dominance, this mechanism would not hold.

### Mechanism 2
- Claim: A small subset of grammatical rules (existential "it", zero copula, "y'all") causes disproportionate performance degradation across multiple dialects.
- Mechanism: These rules alter fundamental syntactic structures (subject-verb agreement, copula deletion, pronoun variation) that appear frequently enough in questions to create systematic failures. The paper shows these three rules alone recover 64-85% of degradation in American dialects (Figure 2).
- Core assumption: Rule-specific failures compound additively rather than canceling out.
- Evidence anchors:
  - [abstract] "three specific grammar rules... can explain the majority of performance degradation"
  - [section 3.2] "each of these three rules individually account for at least 45% of the degradation in performance"
  - [corpus] "Analysis of LLM as a grammatical feature tagger for African American English" (neighbor paper) examines LLM detection of habitual be and zero copula—supporting that specific AAE features are poorly handled
- Break condition: If grammatical perturbations were applied to answer choices rather than just questions, the mechanism's contribution might shift.

### Mechanism 3
- Claim: Obligatory grammatical rules (always applied when applicable) are stronger predictors of dialectal degradation than optional rules.
- Mechanism: Dialects like Chicano English with few obligatory rules show minimal degradation (~4-6% on conditioned accuracy), while Singaporean English with many obligatory divergences shows 16-21% drops. Optional rules applied probabilistically dilute their individual impact.
- Core assumption: Obligatory rules in eWAVE database accurately reflect dialectal usage patterns.
- Evidence anchors:
  - [section 3.2] "obligatory dialect rules are correlated with overall performance of a dialect, recovering similar trends in degradation"
  - [Figure 3] Chicano English shows least degradation, consistent with "infrequency of obligatory rules"
  - [corpus] Limited direct corpus evidence on obligatory vs. optional rule distinction—related work focuses on overall dialect performance rather than rule classifications
- Break condition: If Multi-VALUE's rule application frequencies don't match real dialectal usage, the obligatory/optional distinction becomes unreliable.

## Foundational Learning

- Concept: **Zero copula** (e.g., "She studying" vs. "She is studying")
  - Why needed here: This is one of three high-impact rules; understanding the transformation helps interpret why it causes ~1% accuracy drop per Table 8
  - Quick check question: Can you identify which dialects in the paper use zero copula as an obligatory rule?

- Concept: **Perplexity as a distributional distance metric**
  - Why needed here: The paper uses perplexity to validate that dialectal variants are "further" from training distribution; Table 11 shows Singlish at 3517% increase
  - Quick check question: Why might Singaporean English show higher perplexity than African American English despite both being non-SAE?

- Concept: **Conditioned accuracy analysis** (filtering to questions answered correctly in SAE)
  - Why needed here: Table 2 isolates "capability gaps" where models could answer in SAE but fail in dialect—this is the core fairness measure
  - Quick check question: Why does conditioning on SAE-correct questions amplify apparent degradation compared to raw accuracy?

## Architecture Onboarding

- Component map:
  - BoolQ/SciQ/MMLU datasets -> Multi-VALUE perturbation tool -> Dialectal variants -> LM Eval Harness -> Accuracy evaluation -> Rule attribution analysis

- Critical path:
  1. Identify which questions can be perturbed by each grammar rule
  2. Generate dialectal variants (questions only; answers unchanged)
  3. Run inference, collect accuracy deltas
  4. Attribute degradation to individual rules vs. rule combinations

- Design tradeoffs:
  - **Automated translation vs. human-written dialect**: Multi-VALUE provides consistency but may not capture authentic usage patterns; the paper acknowledges this limitation
  - **Multiple choice vs. open-ended**: Restricting to MCQA provides cleaner measurement but may underestimate real-world degradation
  - **Rule isolation vs. full dialect**: Testing individual rules enables attribution but ignores interaction effects (Tables 9-10 show non-additive interactions)

- Failure signatures:
  - Low rule applicability count (e.g., "y'all" only applies to 1,377 questions) → insufficient statistical power
  - Model already at ceiling on SAE (SciQ: 97.7% for GPT4o-mini) → floor effects mask degradation
  - Dialect with few obligatory rules (Chicano) → minimal signal to analyze

- First 3 experiments:
  1. **Replicate single-rule perturbation on a held-out dataset** (e.g., CommonsenseQA) to verify that existential "it", zero copula, and "y'all" remain high-impact across different question types
  2. **Apply perturbations to answer choices, not just questions** to test whether the mechanism is input-specific or generalizes to full prompt context
  3. **Fine-tune a small model on QA pairs augmented with just the three high-impact rules** to test the paper's hypothesis that targeted training could reduce bias across dialects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted fine-tuning on the three identified high-impact grammatical structures (existential "it," zero copula, and "y'all") reduce dialectal performance degradation across multiple dialects?
- Basis in paper: [explicit] The abstract and discussion explicitly call for "future work to investigate bias mitigation methods focused on individual, high-impact grammatical structures."
- Why unresolved: The paper only identifies and quantifies problematic rules; it does not test interventions.
- What evidence would resolve it: Fine-tune LLMs on QA pairs perturbed with these specific rules and measure accuracy recovery across dialects.

### Open Question 2
- Question: Do the observed dialectal performance degradations generalize to open-ended generation tasks beyond multiple-choice QA?
- Basis in paper: [explicit] The discussion states "it will be also be important to extend our findings from multiple choice QA tasks to open-ended responses."
- Why unresolved: Multiple-choice responses are less variable and potentially easier; open-ended tasks may reveal greater or different bias patterns.
- What evidence would resolve it: Evaluate the same LLMs on open-ended QA benchmarks using dialect-perturbed prompts with human or automated evaluation.

### Open Question 3
- Question: How do interactions between co-occurring grammatical rules affect cumulative performance degradation—is it additive, sub-additive, or super-additive?
- Basis in paper: [inferred] Appendix A.4 documents non-additive interaction effects for Singaporean English rules (Tables 9-10 show interaction gaps ranging from -0.79% to +2.48%), but these patterns are not systematically characterized.
- Why unresolved: The paper flags interaction effects but only analyzes three rules in one dialect as a case study.
- What evidence would resolve it: Systematically test rule combinations across all dialects to characterize interaction patterns (synergistic vs. compensatory).

## Limitations
- Automated perturbation via Multi-VALUE may not capture authentic dialectal usage patterns
- Analysis limited to multiple-choice QA tasks, potentially underestimating bias in open-ended scenarios
- Focus on grammatical rules from eWAVE database may miss lexical or phonological dialect features

## Confidence
- **High**: The overall finding that non-SAE dialects show systematic accuracy degradation (up to 20 percentage points) is well-supported by consistent results across multiple models and benchmarks
- **Medium**: The identification of three specific grammatical rules (existential "it", zero copula, "y'all") as primary drivers is compelling but may not generalize to all dialectal features or model architectures
- **Medium**: The correlation between obligatory rule frequency and degradation is suggestive but relies on eWAVE's rule classification, which may not perfectly reflect actual dialectal usage patterns

## Next Checks
1. Apply the same methodology to open-ended QA datasets (e.g., Natural Questions) to test whether multiple-choice format limitations affect the measured bias
2. Conduct human evaluation of Multi-VALUE-generated dialectal variants to assess authenticity and identify systematic errors in automated perturbation
3. Test whether fine-tuning on dialect-augmented data reduces bias more effectively for high-impact rules versus random grammatical perturbations