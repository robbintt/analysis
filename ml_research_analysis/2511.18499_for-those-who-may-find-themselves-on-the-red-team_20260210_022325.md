---
ver: rpa2
title: For Those Who May Find Themselves on the Red Team
arxiv_id: '2511.18499'
source_url: https://arxiv.org/abs/2511.18499
tags:
- language
- interpretability
- tokens
- about
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that literary scholars should engage
  with large language model (LLM) interpretability research, particularly through
  participation in "red teams" - groups that conduct adversarial testing of AI systems.
  The author contends that current approaches to interpretability in AI are narrowly
  instrumental, focused on optimizing model performance and addressing failures, rather
  than exploring broader questions about language and meaning.
---

# For Those Who May Find Themselves on the Red Team

## Quick Facts
- arXiv ID: 2511.18499
- Source URL: https://arxiv.org/abs/2511.18499
- Reference count: 7
- This position paper argues that literary scholars should engage with large language model (LLM) interpretability research, particularly through participation in "red teams" - groups that conduct adversarial testing of AI systems.

## Executive Summary
This position paper contends that literary scholars should actively engage with large language model interpretability research, specifically by participating in "red teams" that conduct adversarial testing of AI systems. The author argues that current interpretability approaches are narrowly instrumental, focused on optimizing performance and addressing failures rather than exploring broader questions about language and meaning. Through analysis of "glitch tokens" in GPT models, the paper demonstrates how adversarial attacks reveal unexpected behaviors and prompt deeper questions about semantic representation in high-dimensional embedding spaces.

The paper acknowledges that engagement with interpretability research involves ideological complicity with certain rationalist and corporate frameworks but argues this engagement is necessary to prevent LLM interpretation from remaining solely the domain of computer and data sciences focused on profit generation and existential risk mitigation. The author suggests that literary scholars can bring valuable theoretical perspectives to questions about language, meaning, and interpretation that complement technical approaches.

## Method Summary
The paper presents a conceptual argument rather than empirical research, drawing on analysis of existing interpretability literature and a case study of "glitch tokens" in GPT models. The author examines how adversarial attacks reveal unexpected behaviors in language models and considers what literary theoretical frameworks might contribute to understanding these phenomena. The methodology is primarily analytical and theoretical, focusing on identifying opportunities for interdisciplinary collaboration rather than conducting original research.

## Key Results
- Literary scholars can contribute valuable theoretical perspectives to LLM interpretability research through participation in red teams
- Adversarial testing reveals unexpected behaviors in language models that prompt deeper questions about semantic representation
- Current interpretability approaches are narrowly instrumental, focused on optimization rather than broader questions about language and meaning
- Engagement with interpretability research involves ideological complicity but is necessary to prevent monopolization by technical fields
- "Glitch tokens" demonstrate how high-dimensional embedding spaces can produce semantically coherent but unexpected outputs

## Why This Works (Mechanism)
The paper argues that adversarial testing through red teams works by revealing unexpected behaviors and edge cases in language models that expose limitations in current interpretability approaches. These failures prompt deeper questions about how meaning is constructed and represented in high-dimensional spaces, which literary scholars are uniquely positioned to address through their theoretical frameworks about language, interpretation, and meaning-making.

## Foundational Learning
- Red team methodology: Understanding adversarial testing approaches is needed to identify how literary scholars can contribute to interpretability research; quick check: review existing red team frameworks and their objectives
- High-dimensional embedding spaces: Knowledge of how language models represent semantic meaning in vector spaces is needed to understand where literary theory can provide insights; quick check: examine how specific tokens map to embedding coordinates
- Interpretability research paradigms: Understanding current technical approaches is needed to identify gaps where humanities perspectives could contribute; quick check: survey recent interpretability papers for their methodological assumptions
- Literary theory frameworks: Familiarity with theories of meaning, interpretation, and language is needed to articulate potential contributions; quick check: identify specific theoretical concepts applicable to LLM analysis
- Corporate AI development contexts: Understanding the institutional frameworks that shape interpretability research is needed to navigate ideological tensions; quick check: analyze funding sources and publication venues in interpretability research
- Interdisciplinary collaboration challenges: Knowledge of how different academic fields approach shared problems is needed to identify practical collaboration pathways; quick check: examine case studies of successful humanities-technical collaborations

## Architecture Onboarding
Component map: Literary scholars -> Red team participation -> Adversarial testing -> Interpretability insights -> Theoretical contributions
Critical path: Literary theory application to technical interpretability challenges
Design tradeoffs: Ideological complicity with corporate frameworks vs. practical engagement with technical research
Failure signatures: Reproducing existing technical paradigms without contributing novel theoretical insights
First experiments:
1. Analyze a specific "glitch token" case study using literary theoretical frameworks
2. Participate in a red team exercise focused on semantic edge cases
3. Conduct a comparative analysis of interpretability approaches across humanities and technical fields

## Open Questions the Paper Calls Out
- How can literary scholars practically navigate the ideological tensions involved in engaging with corporate AI research frameworks?
- What specific theoretical contributions can literary studies make that computer scientists are not already investigating?
- How can interdisciplinary collaboration be structured to ensure genuine knowledge transfer rather than token participation?
- What are the power dynamics at play when humanities scholars engage with technical research communities?

## Limitations
- The practical feasibility of meaningful interdisciplinary collaboration remains uncertain
- The paper lacks systematic analysis of whether literary frameworks could provide novel insights beyond existing technical investigations
- Ideological tensions are acknowledged but not fully addressed in terms of practical navigation strategies
- The case study of "glitch tokens" provides illustrative examples but lacks depth in demonstrating theoretical contributions

## Confidence
High: The core argument that interdisciplinary engagement is valuable and necessary
Medium: The reasoning about how literary scholars can contribute to interpretability research
Low: Assertions about specific epistemological contributions literary studies could make

## Next Checks
1. Conduct a pilot collaborative project where literary scholars and AI researchers jointly analyze a specific interpretability challenge, documenting both technical insights and theoretical contributions from each perspective
2. Survey existing interdisciplinary work at the intersection of humanities and AI to identify concrete examples of successful knowledge transfer and methodological innovation
3. Analyze the actual participation patterns and outcomes of current red team initiatives to assess whether they create genuine opportunities for humanities perspectives or primarily reproduce existing technical paradigms