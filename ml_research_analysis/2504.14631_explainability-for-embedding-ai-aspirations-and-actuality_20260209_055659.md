---
ver: rpa2
title: 'Explainability for Embedding AI: Aspirations and Actuality'
arxiv_id: '2504.14631'
source_url: https://arxiv.org/abs/2504.14631
tags:
- systems
- developers
- software
- survey
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how software developers interact with explainable
  AI (XAI) systems. The author conducted three online surveys with 96 to 116 participants,
  including both students and professional developers, to assess the demand for explainability
  and the effectiveness of popular XAI methods like LIME and SHAP.
---

# Explainability for Embedding AI: Aspirations and Actuality

## Quick Facts
- arXiv ID: 2504.14631
- Source URL: https://arxiv.org/abs/2504.14631
- Reference count: 31
- Key outcome: Software developers see high need for explainability but current XAI tools (LIME, SHAP) do not effectively support debugging or understanding AI models

## Executive Summary
This paper investigates how software developers interact with explainable AI (XAI) systems through three online surveys with 96-116 participants. While developers generally perceive a high need for explainability in AI systems, the study found that popular XAI tools like LIME and SHAP do not effectively support developers in debugging or understanding AI models. Experienced developers benefit more from these tools but still struggle to detect faults in manipulated datasets, with very few identifying deliberately added errors. The findings highlight that XAI systems are often designed for narrow use cases and require significant prior knowledge, limiting their utility for a broader developer audience.

## Method Summary
The study conducted three online surveys via an institutional survey platform, recruiting participants through university mailing lists and professional contacts with $10/hour compensation. Survey 1 assessed demand for explainability across 14 application scenarios (9 AI-powered, 5 non-AI). Survey 2 evaluated participants' understanding of LIME and SHAP through multiple-choice questions. Survey 3 used a manipulated California Housing Dataset with deliberately introduced faults (median income replacement for 25% of high-value points and swapped longitude/household labels) to test fault detection with both XAI tools.

## Key Results
- Developers see high need for explainability but current XAI tools don't effectively support debugging
- Only 2-3 out of 21 participants detected deliberately added faults using LIME or SHAP
- Experienced developers benefit more from XAI but still struggle with fault detection
- XAI tools appear designed for narrow use cases rather than systematic debugging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XAI tool utility is mediated by domain expertise—prior knowledge enables interpretation, not the reverse.
- Mechanism: Experienced developers with multiple years of data science background reported higher confidence in interpreting XAI output and found tools easier to understand. Novices remained uncertain or skeptical. XAI appears to amplify existing mental models rather than construct new ones from scratch.
- Core assumption: The benefit stems from matching XAI outputs to pre-existing hypotheses about model behavior.
- Evidence anchors:
  - [abstract]: "Experienced developers benefit more but still struggle to detect faults"
  - [section 4]: "participants with multiple years of data science experience were generally more confident in their ability to interpret explanatory output... the more experienced participants found the XAI systems generally easy to understand while only a single less experienced participant considered them easy"
  - [corpus]: Weak direct evidence; neighbor papers focus on extending XAI beyond developers, not expertise gradients.
- Break condition: If users lack prior mental models of the domain or model architecture, XAI outputs may be misinterpreted or ignored.

### Mechanism 2
- Claim: Subjective confidence in XAI-assisted understanding does not correlate with objective fault detection.
- Mechanism: Participants reported high perceived understanding, plausibility, and confidence in models (Survey 3), yet fault detection rates were near floor—only 2/21 with SHAP and 1/21 with LIME detected deliberately introduced data/model faults. This suggests XAI can create an illusion of comprehension.
- Core assumption: Assumption: The positive subjective ratings reflect surface-level interpretability cues rather than deep causal understanding.
- Evidence anchors:
  - [abstract]: "very few identifying deliberately added errors"
  - [section 4]: "only two participants were able to detect one of the deliberately added faults when using SHAP, and only one participant found a fault using LIME"
  - [corpus: arXiv:2508.05792]: Notes XAI often focuses on model justification over actionable transparency, consistent with confidence-fault gap.
- Break condition: If XAI is used as a verification tool without independent validation methods, false confidence may propagate.

### Mechanism 3
- Claim: Current XAI tools are designed for narrow initial use cases, limiting generalization to debugging and maintenance tasks.
- Mechanism: LIME and SHAP were built for local explanation and feature attribution, not systematic fault detection. Their design assumptions (e.g., surrogate model locality, Shapley value fidelity) may not expose dataset-level anomalies like skewed feature distributions or semantic inconsistencies from label swaps.
- Core assumption: Assumption: Tool design scope, not just user expertise, constrains effectiveness.
- Evidence anchors:
  - [abstract]: "XAI systems are often designed for narrow use cases and require significant prior knowledge"
  - [section 5]: "the XAI systems in our study were not designed as dedicated debugging tools but to get a general understanding of the AI models"
  - [section 2]: "many current XAI systems insufficiently address users' needs... very narrow in scope and focus only on use cases from their conception"
  - [corpus]: Limited direct corroboration; neighbor papers emphasize multi-stakeholder XAI but do not test debugging scope.
- Break condition: If developers treat general-purpose XAI as a debugging solution without complementary tooling, fault coverage will remain low.

## Foundational Learning

- Concept: **Feature attribution interpretation (LIME/SHAP fundamentals)**
  - Why needed here: The paper assumes familiarity with how LIME uses surrogate models and SHAP uses Shapley values to assign feature importance. Without this, participants cannot map outputs to model behavior.
  - Quick check question: Given a SHAP plot showing "median income" with high positive attribution for high-valued predictions, what would you conclude if this feature were artificially inflated in a subset of data?

- Concept: **Data and model fault taxonomies**
  - Why needed here: Survey 3 introduced skewed data (replaced median income) and semantic inconsistency (swapped longitude/household labels). Recognizing these requires knowing what faults look like in ML pipelines.
  - Quick check question: A model relies heavily on "longitude" but treats "latitude" as unimportant. What type of fault might this indicate, and how would you confirm it?

- Concept: **XAI scope and limitations**
  - Why needed here: The paper critiques XAI tools as narrow in scope. Understanding what questions LIME/SHAP can and cannot answer prevents over-reliance.
  - Quick check question: If SHAP indicates a feature is important for a specific prediction, does this guarantee the feature is causally relevant or that no data leakage exists?

## Architecture Onboarding

- Component map:
  - XAI front-end -> Explanation engine (LIME/SHAP) -> Data/model substrate -> Developer mental model layer

- Critical path:
  1. Developer forms hypothesis about potential issue (e.g., "data might be skewed")
  2. Developer runs XAI tool on suspect subset
  3. Developer compares XAI output to expected feature behavior
  4. Developer confirms or revises hypothesis (paper suggests confirmation bias risk here)
  5. Developer validates with independent method (not provided by current XAI tools per paper findings)

- Design tradeoffs:
  - **Breadth vs. depth**: General-purpose XAI (LIME/SHAP) provides local explanations but lacks systematic debugging workflows
  - **Accessibility vs. rigor**: Tools require prior knowledge to be useful (per paper), creating a barrier for novice developers
  - **Confidence vs. calibration**: High subjective plausibility (Survey 3) does not ensure accurate fault detection

- Failure signatures:
  - **High confidence, zero detection**: Participants report understanding and trust but fail to identify introduced faults
  - **Split reception**: Equal positive/negative ratings across users suggests inconsistent utility
  - **Experience-dependent outcomes**: Novices uniformly skeptical; only experts derive marginal benefit

- First 3 experiments:
  1. **Baseline calibration test**: Before using XAI tools, have developers predict which features should be important for a known dataset. Then compare predictions to XAI outputs to measure prior knowledge alignment.
  2. **Fault injection detection drill**: Provide a dataset with a documented fault type (e.g., label swap, distribution shift). Measure detection rate with and without XAI assistance to quantify added value.
  3. **Hypothesis confirmation vs. discovery test**: Present two scenarios—one where XAI confirms a pre-stated hypothesis, one where it reveals an unexpected pattern. Track whether developers notice the unexpected finding or only the confirmation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do XAI tools primarily confirm developers' existing hypotheses rather than helping them discover novel faults in AI systems?
- Basis in paper: [explicit] The paper states "it is unclear whether they simply support existing mental models or actually provide a broader benefit for finding unknown issues in AI systems" and suggests experienced users may use XAI to confirm pre-existing suspicions.
- Why unresolved: The study design (participants inspecting unfamiliar datasets) could not distinguish between hypothesis confirmation and novel fault discovery.
- What evidence would resolve it: A controlled study comparing fault detection rates when participants are primed with specific hypotheses versus given no prior expectations, using both known and novel fault types.

### Open Question 2
- Question: What minimum level of prior knowledge or experience is required for developers to effectively utilize XAI tools for debugging?
- Basis in paper: [explicit] The authors note "these tools require prior knowledge to be useful in the first place" and observed that less experienced participants were "uncertain to skeptical" while experienced developers showed some benefit.
- Why unresolved: The study had heterogeneous experience levels and did not systematically vary or measure the knowledge threshold needed for effective XAI use.
- What evidence would resolve it: A longitudinal training study that measures specific XAI competencies and correlates them with fault detection performance across controlled experience levels.

### Open Question 3
- Question: Can XAI systems be redesigned to support exploratory debugging rather than only explaining individual predictions?
- Basis in paper: [explicit] The paper notes "popular XAI tools... appear to be designed more towards a specific initial use case and less for generally exploring potential faults in an AI system or the underlying data" and that "XAI systems in our study were not designed as dedicated debugging tools."
- Why unresolved: The study evaluated existing off-the-shelf tools (LIME, SHAP) rather than developing or testing debugging-specific XAI interfaces.
- What evidence would resolve it: Design and evaluation of novel XAI interfaces explicitly built for systematic fault exploration, compared against current tools on realistic debugging tasks.

### Open Question 4
- Question: Does the perceived high confidence and understanding from XAI tools lead to over-reliance, where developers miss faults they would otherwise detect?
- Basis in paper: [inferred] The paper warns "care needs to be taken that developers do not over-rely on XAI just to support their existing beliefs" and notes participants had high subjective confidence despite very low actual fault detection rates (only 2-3 participants found faults).
- Why unresolved: The study did not include a control condition without XAI tools to compare fault detection rates.
- What evidence would resolve it: A between-subjects experiment comparing fault detection with and without XAI tools, plus qualitative analysis of reasoning patterns in each condition.

## Limitations
- Small sample sizes (96-116 participants) across three surveys limit statistical power
- Study focused on only two XAI methods (LIME, SHAP) and a single dataset (California Housing)
- XAI tools were not designed as debugging solutions, which may have influenced results
- Artificial nature of fault detection task may not reflect real-world debugging scenarios

## Confidence
- **High confidence**: XAI tools are designed for narrow use cases and require significant prior knowledge
- **Medium confidence**: XAI can create an illusion of understanding (confidence-fault detection gap observed)
- **Medium confidence**: XAI amplifies existing mental models rather than constructing new ones (experience gradient interpretation)

## Next Checks
1. **Cross-tool validation**: Repeat the fault detection experiment with debugging-focused XAI tools (e.g., SHAP interaction values for systematic pattern detection) to isolate whether LIME/SHAP limitations drive low detection rates.

2. **Skill calibration study**: Measure participants' actual ML knowledge (via pre-survey assessment) rather than relying on self-reported experience, then correlate calibrated expertise with XAI effectiveness.

3. **Mixed-method validation**: Conduct think-aloud sessions during XAI tool use to identify specific points of confusion or misinterpretation that quantitative surveys miss, particularly for novice developers.