---
ver: rpa2
title: Learning Longitudinal Health Representations from EHR and Wearable Data
arxiv_id: '2601.12227'
source_url: https://arxiv.org/abs/2601.12227
tags:
- wearable
- wang
- data
- chen
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multimodal foundation model that jointly
  learns representations from sparse, irregular electronic health records (EHR) and
  dense, continuous wearable sensor data by modeling them as a unified continuous-time
  latent process. The model uses modality-specific encoders, a shared temporal backbone
  with continuous-time attention, and pretraining objectives that align dense physiological
  signals with sparse clinical events.
---

# Learning Longitudinal Health Representations from EHR and Wearable Data

## Quick Facts
- arXiv ID: 2601.12227
- Source URL: https://arxiv.org/abs/2601.12227
- Reference count: 40
- Primary result: Multimodal foundation model jointly learns from sparse EHR and dense wearable data, outperforming baselines on clinical event forecasting, physiological state estimation, and longitudinal risk modeling

## Executive Summary
This paper presents a multimodal foundation model that learns unified representations from irregular electronic health records (EHR) and continuous wearable sensor data by modeling them as a continuous-time latent process. The model employs modality-specific encoders, a shared temporal backbone with continuous-time attention, and pretraining objectives that align dense physiological signals with sparse clinical events. The approach demonstrates significant improvements over EHR-only and wearable-only baselines, particularly at long prediction horizons and under missing-modality scenarios, with better calibration and smoother latent trajectories.

## Method Summary
The model processes EHR data (diagnosis codes, medications, labs, procedures) and wearable data (accelerometry, PPG-derived heart rate) from 48,732 UK Biobank patients. A 6-layer wearable encoder processes 30-second windows, while an EHR encoder handles clinical concepts. Both modalities are mapped to a shared 1024-dimensional space using a 24-layer transformer backbone with continuous-time attention that uses a mixture-of-exponentials kernel to model temporal decay. The model is pretrained using masked reconstruction, predictive coding, and asymmetric cross-modal alignment objectives, with curriculum learning to stabilize training. Evaluation uses frozen linear probes on downstream tasks including clinical event forecasting, physiological state estimation, and longitudinal risk modeling.

## Key Results
- Outperforms strong EHR-only and wearable-only baselines across all evaluated tasks
- Shows particularly large gains at long prediction horizons (365-day forecasting)
- Demonstrates improved calibration (lower ECE) and smoother latent trajectories
- Maintains robustness under missing-modality scenarios
- Representation analyses reveal the model successfully separates dynamic physiological trends from discrete clinical events

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Temporal Disambiguation
Dense wearable signals resolve the temporal uncertainty inherent in sparse, irregular EHR data. EHR events provide semantic anchors while wearables provide high-frequency trajectories between those anchors, allowing the latent representation to learn underlying physiological dynamics rather than just clinical code sequences.

### Mechanism 2: Asymmetric Predictive Grounding
The model uses asymmetric prediction where dense wearable context predicts sparse future EHR events. This acts as strong regularization, forcing the wearable encoder to learn clinically relevant features since predicting semantic events from dense context is more informative than the reverse.

### Mechanism 3: Continuous-Time Attention Decay
A learnable kernel $K(\Delta t)$ (mixture of exponentials) explicitly models the decay of information relevance over time. This allows the model to learn multi-timescale dependencies, retaining critical lab results for months while variably weighting recent physiological measurements.

## Foundational Learning

- **Latent Stochastic Processes**: Understanding how a model infers a continuous function from discrete, irregular observations is crucial since health is framed as a continuous-time latent process rather than a sequence of tokens.
  - *Quick check*: How does the model define the "latent health state" at a time $t$ where no data was recorded?

- **Self-Supervised Learning (SSL) Objectives**: The model is pretrained on unlabeled data using masked reconstruction and prediction. Understanding what is being masked and what is predicted is key to grasping the alignment.
  - *Quick check*: Why is predicting future EHR tokens considered a "cross-modal" objective rather than just a generative one?

- **Modality Heterogeneity**: EHR and wearable data have vastly different statistical properties (sparse/categorical vs. dense/continuous). The architecture uses modality-specific front-ends to resolve this before the shared backbone.
  - *Quick check*: Why can't you simply concatenate raw wearable vectors with EHR token embeddings?

## Architecture Onboarding

- **Component map**: Wearable encoder -> EHR encoder -> Continuous positionalization -> Shared temporal backbone (Time-Kernel attention) -> Pretraining heads
- **Critical path**: The Continuous-Time Attention Kernel within the shared backbone is the critical innovation. If implemented incorrectly, the model fails to handle irregular sampling.
- **Design tradeoffs**: 
  - Compute vs. Resolution: Wearable data is downsampled or windowed to make attention tractable, potentially missing sub-second arrhythmias
  - Curriculum Training: Intra-modal reconstruction is weighted heavily early on, then cross-modal alignment is increased
- **Failure signatures**:
  - Modality Collapse: Dense wearable signals can overwhelm EHR representations if losses aren't balanced
  - Kernel Decay to Zero: Learnable decay rates can become too large, causing the attention mechanism to lose long-term context
- **First 3 experiments**:
  1. Ablate the kernel: Replace continuous-time kernel with standard sinusoidal positional embeddings to verify time-awareness advantage
  2. Missing Modality Test: Run inference with only EHR data and only Wearable data to verify robustness claims
  3. Probe the Latent Space: Visualize latent trajectory before and after major clinical events to confirm smooth trajectory claims

## Open Questions the Paper Calls Out

### Open Question 1
How do scaling laws apply to multimodal health foundation models when one modality is dense (wearables) and the other sparse (EHR)? The paper questions whether familiar scaling relationships hold and if there are diminishing returns to adding more wearable days versus more patients.

### Open Question 2
To what extent does fine-tuning improve performance over frozen probes, and what is the risk of catastrophic forgetting of cross-modal alignment? The paper isolates pretraining effects via frozen evaluations, leaving full fine-tuning dynamics unexplored.

### Open Question 3
Can integrating principled generative pretraining (e.g., latent SDEs via ELBO) with discriminative cross-modal objectives improve uncertainty quantification? The current model uses discriminative and reconstruction losses, treating the ELBO view only as an optional ablation.

### Open Question 4
Under what theoretical conditions does cross-modal prediction yield representations that are sufficient for downstream clinical tasks? The paper calls for characterizing identifiability and sufficiency of latent representations, formalizing conditions where cross-modal prediction reduces Bayes risk.

## Limitations

- The continuous-time attention kernel requires careful hyperparameter tuning and can collapse to near-zero decay rates, reverting to standard transformer behavior
- Modality imbalance can cause EHR-specific clinical semantics to be washed out if dense wearable signals dominate
- The model assumes stationary physiological dynamics, which may not generalize to patients with frequent acute events or rapidly progressing chronic diseases
- Limited quantitative evaluation of robustness under missing modalities, with only qualitative mentions of performance in single-modality scenarios

## Confidence

**High Confidence**: Joint EHRâ€“wearable pretraining improves long-horizon forecasting, supported by consistent ablation results and directly measured calibration improvements

**Medium Confidence**: Smoothness and interpretability of learned representations, based on compelling qualitative visualizations but lacking systematic clinical outcome linkage

**Low Confidence**: Robustness under missing modalities, as the paper lacks quantitative degradation measurements when only one modality is present

## Next Checks

1. **Disentanglement Probe**: Run controlled experiments with progressively corrupted wearable data to measure how quickly predictive performance degrades and whether EHR signals compensate for lost physiological information

2. **Temporal Generalization**: Hold out the last 6 months of each patient's history as a test set, fine-tune on only the first N months, and measure 365-day forecasting performance to test whether the model learns temporal priors or memorizes patterns

3. **Calibration Stress Test**: Generate reliability diagrams for high-risk patients (e.g., 90-day mortality risk >20%) to check whether improved overall ECE is driven by well-calibrated predictions in high-risk subpopulations or simply better calibration at lower risk levels