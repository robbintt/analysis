---
ver: rpa2
title: 'Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis
  of BERT'
arxiv_id: '2501.08053'
source_url: https://arxiv.org/abs/2501.08053
tags:
- bert
- neural
- schilling
- data
- krauss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether BERT, a transformer-based large
  language model, exhibits localized neural processing for sequential data analogous
  to biological neural systems. Using a dataset of 1000 narratives with diverse authorial
  styles and content, BERT's layerwise activations were analyzed through dimensionality
  reduction techniques (PCA and MDS).
---

# Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT

## Quick Facts
- arXiv ID: 2501.08053
- Source URL: https://arxiv.org/abs/2501.08053
- Authors: Awritrojit Banerjee; Achim Schilling; Patrick Krauss
- Reference count: 36
- Key outcome: BERT exhibits strong clustering of narrative content in later layers while showing minimal clustering for authorial style, demonstrating prioritized semantic processing over stylistic features

## Executive Summary
This study investigates whether BERT, a transformer-based large language model, exhibits localized neural processing for sequential data analogous to biological neural systems. Using a dataset of 1000 narratives with diverse authorial styles and content, BERT's layerwise activations were analyzed through dimensionality reduction techniques (PCA and MDS). Results revealed strong clustering of narrative content in later layers, with progressively compact and distinct clusters, while minimal clustering was observed for authorial style. Layerwise Generalized Discrimination Value (GDV) trends showed highly negative values for narrative content clustering, becoming more negative in later layers, but remained close to zero for authorial style.

## Method Summary
The study employed a dataset of 1000 narratives representing diverse authorial styles and content. BERT's layerwise activations were extracted for each narrative and analyzed using Principal Component Analysis (PCA) and Multidimensional Scaling (MDS) to reduce dimensionality and visualize clustering patterns. The Generalized Discrimination Value (GDV) was calculated to quantify clustering quality across layers. The analysis focused on comparing clustering patterns between narrative content and authorial style dimensions, tracking how these patterns evolved through BERT's hierarchical layers.

## Key Results
- Strong clustering of narrative content observed in later BERT layers with progressively compact and distinct clusters
- Minimal clustering detected for authorial style across all layers
- Layerwise GDV values highly negative for narrative content, becoming more negative in later layers, while remaining near zero for authorial style

## Why This Works (Mechanism)
BERT's attention mechanisms and feedforward networks create hierarchical representations where lower layers capture surface-level features while higher layers encode more abstract semantic relationships. The progressive clustering in later layers reflects BERT's ability to form coherent representations of narrative content through iterative refinement. The differential clustering between content and style indicates that BERT's architecture naturally prioritizes semantic understanding over stylistic features during pretraining on general language corpora.

## Foundational Learning
- **Transformer architecture**: Why needed - understanding the basic building blocks of BERT; Quick check - identify encoder-decoder structure and attention mechanisms
- **Layerwise analysis**: Why needed - tracking information processing progression; Quick check - map activation patterns across transformer layers
- **Dimensionality reduction techniques**: Why needed - visualizing high-dimensional activation spaces; Quick check - understand PCA and MDS limitations
- **Clustering metrics**: Why needed - quantifying representational organization; Quick check - interpret GDV values and clustering quality measures
- **Narrative structure analysis**: Why needed - framing linguistic feature extraction; Quick check - distinguish content from style features

## Architecture Onboarding
- **Component map**: Input -> Embedding Layer -> N Transformer Blocks (Multi-head Attention -> Feedforward Networks) -> Output Layer
- **Critical path**: Input text → Tokenization → Embedding → Multi-head attention computation → Feedforward processing → Layerwise activation extraction → Dimensionality reduction → Clustering analysis
- **Design tradeoffs**: BERT prioritizes bidirectional context understanding over autoregressive generation efficiency; uses masked language modeling rather than causal attention
- **Failure signatures**: Non-clustering patterns may indicate insufficient model capacity, poor pretraining, or dataset limitations; uniform activation distributions across layers suggest lack of hierarchical processing
- **First experiments**: 1) Test activation extraction on single-layer vs. full model outputs; 2) Compare clustering patterns on synthetic vs. natural text; 3) Validate GDV calculations on controlled synthetic datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on BERT without comparative analysis across transformer architectures
- Use of PCA and MDS may oversimplify complex activation spaces and obscure subtle patterns
- Relatively small dataset (1000 narratives) for drawing generalizable conclusions
- Does not account for potential confounds like narrative length, genre effects, or tokenization strategies

## Confidence
- BERT layerwise clustering patterns: High
- Content vs. style processing hierarchy: Medium
- Biological neural system analogy: Low
- GDV metric reliability: Medium

## Next Checks
1. Replicate findings using alternative dimensionality reduction techniques (t-SNE, UMAP) and compare clustering patterns
2. Conduct cross-model validation by testing similar layerwise analyses on other transformer architectures (RoBERTa, GPT variants)
3. Perform ablation studies varying narrative content complexity and authorial style markers to isolate specific feature representations