---
ver: rpa2
title: 'seqme: a Python library for evaluating biological sequence design'
arxiv_id: '2511.04239'
source_url: https://arxiv.org/abs/2511.04239
tags:
- metrics
- sequences
- sequence
- embedding
- urlhttps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "seqme is a Python library for evaluating biological sequence design,\
  \ addressing the lack of unified metrics to assess fidelity to target distributions\
  \ and attainment of desired properties. It offers three types of metrics\u2014sequence-based,\
  \ embedding-based, and property-based\u2014applicable to diverse biological sequences."
---

# seqme: a Python library for evaluating biological sequence design

## Quick Facts
- arXiv ID: 2511.04239
- Source URL: https://arxiv.org/abs/2511.04239
- Reference count: 40
- Primary result: Provides unified framework for evaluating biological sequence design across three metric categories with caching, diagnostics, and visualization tools.

## Executive Summary
seqme addresses the critical need for standardized evaluation of biological sequence design methods by providing a comprehensive Python library that implements model-agnostic metrics across three categories: sequence-based, embedding-based, and property-based. The library fills a gap in the field where no unified framework existed for assessing fidelity to target distributions and achievement of desired properties across diverse biological sequences including proteins, DNA, RNA, and peptides. By offering standardized metrics, caching mechanisms, and diagnostic tools, seqme enables fair benchmarking and accelerates progress in biological sequence design research.

## Method Summary
seqme is a Python library implementing model-agnostic metrics for evaluating biological sequence design methods across three categories: sequence-based (novelty, uniqueness, diversity), embedding-based (FBD, MMD, precision, recall, authenticity), and property-based (hit-rate, hypervolume). The library supports diverse sequence types including small molecules, DNA, ncRNA, mRNA, peptides, and proteins. Users can compute metrics through a standardized interface, optionally using caching to store embeddings and properties for efficiency, and employ Fold functionality to split sequences into K equal groups for bias mitigation and variance estimation. The library includes embedding models (ESM-2, RNA-FM, GENA-LM, Hyformer) and property predictors, along with diagnostic tools for embedding quality assessment and visualization functions for results.

## Key Results
- Provides unified framework for evaluating biological sequence design across three metric categories
- Includes tools for embedding and property models, diagnostics, and visualizations
- Supports both one-shot and iterative design evaluation with caching and Fold functionality
- Enables fair benchmarking across diverse biological sequence types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-category metric evaluation captures complementary failure modes that single metrics miss.
- Mechanism: seqme decomposes evaluation into sequence-based, embedding-based, and property-based metrics. Each category detects different failures—e.g., overfitting reduces novelty/diversity while random baselines maximize diversity without optimizing properties.
- Core assumption: No single metric reliably detects all generative model failures; comprehensive evaluation requires multiple perspectives.
- Evidence anchors: [abstract] "seqme considers three groups of metrics: sequence-based, embedding-based, and property-based"; [section] "A typical failure mode is overfitting to a small set of biological sequences that satisfy the target properties at the expense of novelty, diversity, and uniqueness".

### Mechanism 2
- Claim: Embedding-based distribution comparison enables semantic similarity assessment beyond string-level metrics.
- Mechanism: seqme maps sequences to fixed-length vectors via pretrained models, then compares generated vs. reference distributions in embedding space using FBD, MMD, and improved Precision/Recall measures.
- Core assumption: Embedding models capture biologically meaningful structure; distances in embedding space correlate with functional similarity.
- Evidence anchors: [section] "FBD is sensitive to both the number of sequences and their distribution in the embedding space"; [section] "Selecting an appropriate embedding model is a non-trivial task as the embeddings must capture the biological domain of interest".

### Mechanism 3
- Claim: Caching and Fold functionality enable efficient, unbiased evaluation across iterative design workflows.
- Mechanism: Caching stores computed embeddings/properties, reducing time complexity from O(nk) to O(n+k) when computing m metrics sharing the same embedding model. Fold splits sequences into K equal groups to mitigate sample-size bias and estimate variance.
- Core assumption: Embeddings/properties are deterministic given the same model; sample-size bias materially affects metric comparisons.
- Evidence anchors: [section] "With caching enabled, the time-complexity of computing m metrics with the same embedding model for n sequences is reduced to O(n+k) instead of O(nk)"; [section] "Several metrics... can become biased when there is a discrepancy in the number of designed sequences and reference sequences".

## Foundational Learning

- Concept: **Embedding spaces and distributional metrics**
  - Why needed here: All embedding-based metrics (FBD, MMD, Precision/Recall) assume understanding of how fixed-length vector representations enable distribution comparison.
  - Quick check question: Can you explain why Fréchet Distance assumes Gaussian distributions and when MMD would be preferred?

- Concept: **Generative model failure modes (memorization, mode collapse, diversity loss)**
  - Why needed here: seqme's metrics are designed to detect specific failures—Authenticity for memorization, Diversity for mode collapse, Novelty for overfitting.
  - Quick check question: What is the difference between a model that memorizes training data vs. one that produces low-diversity outputs?

- Concept: **Multi-objective optimization and Pareto fronts**
  - Why needed here: Property-based metrics like Hypervolume Indicator evaluate tradeoffs between conflicting objectives (e.g., binding affinity vs. stability).
  - Quick check question: How does the hypervolume indicator quantify multi-objective performance?

## Architecture Onboarding

- Component map: Metrics module (three classes: Sequence, Embedding, Property) -> Models module (embedding models and property predictors) -> Cache layer (stores computed embeddings/properties) -> Fold utility (K-fold splitting) -> Visualization (tables, parallel-coordinates, barplots, trajectory plots)

- Critical path: 1) Define sequences (dict of named sequence lists) 2) Select embedding model(s) and instantiate Cache 3) Choose metrics aligned with failure modes of concern 4) Call `sm.evaluate(sequences, metrics)` 5) Inspect with `sm.show()` or visualization functions

- Design tradeoffs: Embedding model selection (domain-specific vs. general-purpose; verify alignment with diagnostics), metric completeness vs. computational cost (more metrics = better coverage but slower evaluation), reference set size (too small = unreliable estimates; too large = memory/compute burden)

- Failure signatures: High Precision + low Recall (model covers only subset of reference distribution), Low Authenticity + low FBD (memorization of training data), High Novelty/Diversity + low Hit-rate (random exploration without property optimization), Inconsistent metric values across Folds (sample-size bias or unstable embedding models)

- First 3 experiments: 1) Baseline comparison: Evaluate random sequences vs. your generated sequences using Diversity, FBD, and a property metric to establish failure mode profile 2) Embedding model validation: Run KNN feature-alignment and Spearman diagnostics on your chosen embedding model with labeled property data to verify alignment 3) Iterative tracking: Use Fold with caching to plot metric trajectories across training epochs or design iterations, watching for divergence between distributional and property metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should users optimally select combinations of metrics to achieve robust evaluation of biological sequence design methods?
- Basis in paper: [explicit] The authors state "only the use of several carefully chosen metrics will yield a robust evaluation" and note that "each evaluation metric is imperfect and can fail to detect a failure in a given generative AI model."
- Why unresolved: The library provides many metrics but no principled guidance on which combinations work best for different design scenarios or sequence types.
- What evidence would resolve it: Systematic benchmarking of metric combinations against known failure modes across diverse sequence design tasks.

### Open Question 2
- Question: How can appropriate embedding models be systematically selected for specific biological domains and properties of interest?
- Basis in paper: [explicit] The authors state "Selecting an appropriate embedding model is a non-trivial task as the embeddings must capture the biological domain of interest."
- Why unresolved: While diagnostic tools (k-NN feature-alignment, Spearman alignment) are provided, criteria for model selection remain user-defined and no benchmarks exist.
- What evidence would resolve it: Comparative studies linking diagnostic scores to downstream metric reliability across embedding models and tasks.

### Open Question 3
- Question: Can seqme metrics serve as reliable stopping criteria in machine learning training loops for sequence design?
- Basis in paper: [explicit] The authors "envision seqme will... offer more comprehensive tools for defining stopping criteria in machine-learning training loops."
- Why unresolved: This is stated as a future vision without empirical demonstration or methodology for implementation.
- What evidence would resolve it: Experiments showing metric trajectories during training correlate with final design quality and can define optimal stopping points.

## Limitations
- Effectiveness depends critically on embedding model selection with no automated guidance for optimal choice beyond diagnostic tools
- Computational requirements for large models like ESM-2 are not specified, potentially limiting accessibility
- Interdependence and redundancy between metrics from different categories is not fully characterized

## Confidence
- **High confidence**: The decomposition into three metric categories captures complementary failure modes, supported by both theoretical justification and practical examples of model failures
- **Medium confidence**: Embedding-based metrics provide meaningful semantic similarity assessment, contingent on proper embedding model selection and alignment diagnostics
- **Medium confidence**: Caching and Fold functionality meaningfully improve evaluation efficiency and reduce bias, though computational benefits depend on model determinism

## Next Checks
1. **Diagnostic validation**: Apply KNN feature-alignment and Spearman diagnostics to verify embedding model alignment before running full metric suites, comparing results across different embedding models for the same sequence set
2. **Benchmark consistency**: Evaluate the same generated sequences across multiple metric combinations to identify correlations and potential redundancies between sequence-, embedding-, and property-based metrics
3. **Resource profiling**: Characterize memory and time requirements for large-scale evaluations with ESM-2 and similar models across different hardware configurations to establish practical limits