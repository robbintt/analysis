---
ver: rpa2
title: 'BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language
  Models'
arxiv_id: '2510.00307'
source_url: https://arxiv.org/abs/2510.00307
tags:
- bias
- selection
- tool
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses bias in LLM tool selection, where models systematically
  favor certain APIs over functionally equivalent alternatives due to superficial
  cues like metadata or prompt order, harming fairness and user experience. The authors
  construct a benchmark of 10 API clusters (5 tools each) and 1,000 user queries,
  then measure selection bias using total variation from uniform.
---

# BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models

## Quick Facts
- arXiv ID: 2510.00307
- Source URL: https://arxiv.org/abs/2510.00307
- Reference count: 40
- Primary result: Proposes a lightweight subset-selector mitigation that reduces tool-selection bias by ~75% while maintaining high coverage.

## Executive Summary
Large language models exhibit systematic bias when selecting among functionally equivalent APIs, favoring certain tools based on superficial cues like metadata or prompt order. This bias harms fairness and user experience, particularly when powerful tools are overlooked in favor of less capable ones. The authors construct a benchmark of 10 API clusters with 5 tools each and measure selection bias using total variation from uniform. Across seven models, all show substantial bias (δ_model ~ 0.3–0.4), with semantic similarity between queries and tool descriptions being the strongest predictor. To address this, they propose a mitigation approach using a subset selector to identify all valid APIs for a task, then sampling uniformly among them, reducing bias by ~75% while maintaining ~89% recall.

## Method Summary
The authors first construct a benchmark of 10 API clusters, each containing 5 functionally equivalent tools from different providers. They collect 1,000 user queries and measure selection bias using total variation distance from uniform distribution across equivalent APIs. To identify factors driving bias, they analyze semantic similarity between queries and tool descriptions, along with metadata features like parameter complexity and familiarity. They then conduct controlled experiments by corrupting tool metadata and measuring shifts in selection. For mitigation, they propose a lightweight module that uses a subset selector (Qwen3-14B) to identify all valid APIs for a task, then samples uniformly among them. The approach is evaluated across seven LLMs, showing significant bias reduction while maintaining high task completion rates.

## Key Results
- All seven tested models show substantial selection bias (δ_model ~ 0.3–0.4) when choosing among functionally equivalent APIs
- Semantic similarity between queries and tool descriptions is the strongest predictor of selection bias
- Controlled metadata corruption experiments show description changes have the largest impact on selection
- The subset-selector mitigation reduces bias by ~75% while maintaining ~89% recall across all models

## Why This Works (Mechanism)
The bias occurs because LLMs rely on superficial cues rather than functional equivalence when selecting tools. These cues include semantic similarity between task descriptions and tool documentation, as well as metadata features like name familiarity and promotional language. The proposed mitigation works by first identifying all valid tools for a task using a separate LLM, then uniformly sampling among them, which breaks the correlation between superficial cues and selection probability.

## Foundational Learning
- **Total variation distance**: Measures how much a selection distribution deviates from uniform; needed to quantify bias magnitude; quick check: δ = 0.5 means maximum bias, δ = 0 means no bias
- **Semantic similarity metrics**: Used to predict tool selection; needed to identify bias drivers; quick check: higher cosine similarity between query and description correlates with higher selection probability
- **API equivalence clusters**: Groups of functionally identical tools; needed to define fair selection baseline; quick check: all tools in cluster should have equal selection probability in unbiased model
- **Subset selector pattern**: LLM-based module that identifies valid options; needed for mitigation; quick check: recall should exceed 85% for practical deployment
- **Controlled metadata perturbation**: Systematic changes to tool descriptions/names; needed to test causality; quick check: changes should produce measurable shifts in selection rates
- **Uniform sampling from valid set**: Mitigation technique that ensures equal selection probability; needed to eliminate bias; quick check: final selection distribution should approach uniform within cluster

## Architecture Onboarding

### Component Map
Query -> Subset Selector (Qwen3-14B) -> Valid API Set -> Uniform Sampler -> Selected API -> Task Execution

### Critical Path
The critical path is: Query processing → Subset selector inference → Uniform sampling → API call. The subset selector must accurately identify all valid APIs; failure here directly impacts recall and task completion.

### Design Tradeoffs
The authors chose uniform sampling over ranking-based approaches to ensure fairness, accepting potential efficiency losses. The subset selector adds computational overhead but is necessary to identify valid tools. The approach trades some precision for significant bias reduction.

### Failure Signatures
- Low recall (<85%) indicates the subset selector misses valid APIs
- Persistent selection skew within clusters suggests uniform sampling isn't working
- High computational latency from subset selector inference
- Degradation in task completion when valid APIs are incorrectly excluded

### 3 First Experiments
1. Measure baseline selection bias across 10 API clusters with 5 tools each using total variation distance
2. Test controlled metadata perturbations by corrupting descriptions and measuring selection shifts
3. Evaluate mitigation effectiveness by comparing pre/post bias metrics and recall rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can richer semantic and structural features beyond the seven used (e.g., syntactic complexity, domain-specific terminology, implicit sentiment) significantly improve predictive power for tool selection rates?
- Basis in paper: Future work section states: "Future work should scale queries and clusters and enrich features with deeper semantic and structural signals to raise predictive power beyond the modest observed R²."
- Why unresolved: Linear regression achieved only R² < 0.4, and random forests failed to improve predictions, leaving substantial variance unexplained.
- What evidence would resolve it: Construct expanded feature sets incorporating syntactic, semantic, and structural signals; demonstrate R² improvement >0.2 on held-out API selection data.

### Open Question 2
- Question: Do higher-order interactions between tool features (e.g., description length × parameter complexity, name familiarity × promotional language) drive selection bias in ways linear models cannot capture?
- Basis in paper: Future work section states: "deploying more expressive models (e.g., boosted trees or deep nets) with cross-validation could capture higher-order interactions between tool features, further increasing their explanatory power."
- Why unresolved: Random forests underperformed even linear baselines, possibly due to limited feature set or data scale (only 50 API examples per model).
- What evidence would resolve it: Train gradient-boosted models on larger feature sets with interaction terms; validate via cross-validation and feature-importance ablation.

### Open Question 3
- Question: Does the subset-selector LLM introduce its own systematic biases (e.g., favoring certain API names or descriptions) that propagate into the final selection distribution?
- Basis in paper: The mitigation relies on a separate LLM (Qwen3-14B) to identify valid APIs, but the paper notes "bias can persist if the subset selector itself is biased." While Table 2 shows high recall (~89%), systematic underselection of specific APIs could remain undetected.
- Why unresolved: Precision and recall metrics don't capture whether false negatives cluster around specific API characteristics.
- What evidence would resolve it: Analyze false-negative patterns across API attributes; report selection-rate distributions for the subset selector itself on equivalent-API clusters.

## Limitations
- Benchmark coverage is narrow, testing only 10 API clusters from 5 providers, which may not capture real-world tool ecosystem diversity
- Evaluation relies on simulated queries rather than actual user interactions, potentially missing contextual factors
- Mitigation method may face scalability challenges when applied to hundreds or thousands of tools due to subset selector computational demands

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Systematic bias exists across models | High |
| Semantic similarity drives selection bias | Medium |
| Mitigation maintains high coverage | High |
| Real-world impact of mitigation is significant | Low |

## Next Checks
1. Test the mitigation method on a larger, more diverse set of APIs to assess scalability and robustness
2. Conduct user studies to compare the quality and fairness of tool selections made by biased versus debiased models in real-world tasks
3. Analyze the trade-offs between bias reduction and other performance metrics (e.g., response time, accuracy) when applying the subset selector in production environments