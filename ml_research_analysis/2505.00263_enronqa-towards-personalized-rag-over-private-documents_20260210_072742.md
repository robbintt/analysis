---
ver: rpa2
title: 'EnronQA: Towards Personalized RAG over Private Documents'
arxiv_id: '2505.00263'
source_url: https://arxiv.org/abs/2505.00263
tags:
- question
- email
- emails
- https
- enronqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EnronQA is a large-scale benchmark for retrieval augmented generation
  over private documents, containing 103,638 emails with 528,304 question-answer pairs
  across 150 user inboxes. The dataset is designed to enable personalized and private
  RAG benchmarking, addressing the gap where most existing benchmarks use public data
  like Wikipedia.
---

# EnronQA: Towards Personalized RAG over Private Documents

## Quick Facts
- **arXiv ID**: 2505.00263
- **Source URL**: https://arxiv.org/abs/2505.00263
- **Reference count**: 40
- **Primary result**: EnronQA is a large-scale benchmark for retrieval augmented generation over private documents, containing 103,638 emails with 528,304 question-answer pairs across 150 user inboxes.

## Executive Summary
EnronQA addresses the gap in RAG benchmarking by providing a large-scale dataset of private emails with synthetically generated QA pairs. Unlike Wikipedia-based benchmarks where models can rely on memorized knowledge, EnronQA forces genuine retrieval because the emails are not in LLM pretraining corpora. The dataset contains 103,638 emails and 528,304 QA pairs across 150 user inboxes, generated through a rigorous multi-stage LLM pipeline ensuring specificity, objectivity, groundedness, and quality. Experimental results show that EnronQA provides better calibration for retrieval quality compared to public benchmarks, with BM25 achieving 87.5% accuracy without query rewriting.

## Method Summary
The dataset is constructed through a four-stage filtering pipeline applied to the Enron email corpus, followed by a multi-stage QA generation pipeline using Llama-3.1-70B and Mixtral-8x7B models. The QA generation process involves generating questions, evaluating them for specificity (can an LLM distinguish the correct email from 10 similar ones?), objectivity (are they definitively answerable?), groundedness (do they require context?), and quality, with iterative refinement cycles. The dataset is publicly released with train/test splits for benchmarking RAG pipelines and memorization techniques.

## Key Results
- EnronQA provides superior calibration between retrieval quality and downstream accuracy compared to public benchmarks like NaturalQuestions and TriviaQA
- BM25 achieves 87.5% accuracy on EnronQA without query rewriting, outperforming dense retrievers on this benchmark
- LoRA-based memorization can match long-context performance but underperforms retrieval at all scales tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EnronQA provides superior calibration between retrieval quality and downstream accuracy compared to public benchmarks.
- Mechanism: Because Enron emails are private data not present in LLM pretraining corpora, models cannot answer questions using parametric knowledge alone. This forces genuine reliance on retrieval, making accuracy gains directly proportional to retriever improvements (≈0.6% accuracy gain per 1% recall increase).
- Core assumption: The tested LLMs have not been pretrained on the Enron email corpus.
- Evidence anchors:
  - [abstract] "Experiments show that EnronQA is more sensitive to retrieval quality—unlike Wikipedia-based benchmarks where models can perform well without retrieval"
  - [section 4.2] "EnronQA is the only benchmark where adding context is always better than the no-context baseline... For NaturalQuestions it takes a retriever with a Recall@1 above 0.5 in order to outperform the no context baseline"
- Break condition: If future foundation models are trained on Enron emails, the calibration advantage degrades as parametric knowledge contaminates the benchmark.

### Mechanism 2
- Claim: The multi-stage LLM-driven QA generation pipeline produces questions that are specific, objective, grounded, and resistant to guessing.
- Mechanism: A 4-stage evaluation pipeline (Specificity → Objectivity → Groundedness → Quality) with iterative refinement ensures questions can only be answered with the exact source email. Specificity is tested by requiring an LLM to select the correct email from 10 similar candidates; groundedness is verified by confirming models cannot answer without context.
- Core assumption: The evaluation models (Llama-3.1-70B, Mixtral-8x7B) are sufficiently capable and unbiased to serve as reliable judges.
- Evidence anchors:
  - [section 3.2] "Producing a single high-quality question takes 10-50 distinct LLM calls... Our pipeline asserts that questions are specific, objective, grounded, and high-quality"
  - [section 3.2] "Our Llama3.1 70B Instruct LLM judge achieves a 0.98 F1-score with human evaluation on a small study of 200 generations"
- Break condition: If evaluation models have systematic blind spots or biases, low-quality questions could pass all filters.

### Mechanism 3
- Claim: LoRA-based memorization can match long-context performance for factual recall but underperforms retrieval.
- Mechanism: Fine-tuning LoRA adapters on QA pairs encodes factual knowledge into a small number of additional parameters (~8-2048 rank). This allows the model to "memorize" private facts without storing raw documents, but retrieval still outperforms because it provides exact, relevant context rather than relying on distributed parametric encoding.
- Core assumption: LoRA adapters can faithfully encode factual associations without catastrophic interference or overfitting.
- Evidence anchors:
  - [section 6.2] "LoRA memorization can match long-context performance at almost all scales and continue beyond the 1000 QA-pair cap"
  - [section 6.2] "At all scales, RAG outperforms memorization and long-context"
- Break condition: Performance degrades significantly around 20,000+ facts, suggesting capacity limits in low-rank adapters.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: EnronQA is specifically designed to benchmark RAG systems over private documents. Understanding why RAG is preferred over fine-tuning for private data (no data leakage, inference-time context) is foundational.
  - Quick check question: Why would an enterprise prefer RAG over fine-tuning for querying internal documents?

- Concept: **Parametric vs. Contextual Knowledge in LLMs**
  - Why needed here: The paper's core thesis depends on the distinction between knowledge encoded in model weights (parametric) and knowledge provided at inference time (contextual). Wikipedia benchmarks fail because parametric knowledge suffices.
  - Quick check question: On a benchmark like TriviaQA, why might a model achieve 60%+ accuracy with zero retrieval?

- Concept: **LoRA (Low-Rank Adaptation) Fine-Tuning**
  - Why needed here: The memorization case study uses LoRA adapters to encode private facts. Understanding how LoRA works (freezing base weights, training small adapter matrices) is necessary to interpret Table 5 results.
  - Quick check question: If a LoRA adapter with rank 256 can memorize 5,000 facts but degrades at 20,000, what does this suggest about knowledge density in parameter space?

## Architecture Onboarding

- Component map:
  Raw Enron Corpus (517k emails) -> Filtering Pipeline (dedup, quality, language, toxicity) -> Cleaned Corpus (103,638 emails, 150 users) -> QA Generation Pipeline (4-stage: generate → evaluate → feedback → refine) -> EnronQA Benchmark (528,304 QA pairs) -> Evaluation Rigs (RAG pipelines, memorization adapters, calibration tests)

- Critical path: The QA generation pipeline (Section 3.2) is the most complex and failure-prone component. Each question requires 10-50 LLM calls across evaluation stages; optimizing the pipeline with DSPy reduced average refinement iterations from 1.94 to 1.64.

- Design tradeoffs:
  - Single-hop vs. multi-hop: EnronQA uses single-hop queries to isolate retrieval quality; ConcurrentQA offers multi-hop for reasoning evaluation.
  - Synthetic vs. human questions: Fully synthetic generation enables scale (528k pairs) but requires rigorous automated quality checks.
  - Memorization vs. retrieval: LoRA adapters offer privacy benefits (no raw document exposure) but currently underperform retrieval.

- Failure signatures:
  - Questions that fail specificity check: LLM cannot distinguish correct email from 10 similar candidates.
  - Questions that fail groundedness check: Model answers correctly without any email context (indicates memorization or guessability).
  - LoRA memorization collapse: Accuracy drops sharply beyond ~20k facts (Table 5, rank 256+ shows degradation).

- First 3 experiments:
  1. **Calibration validation**: Replicate Figure 4 on your own infrastructure—test no-context vs. retrieved-context accuracy across simulated recall levels to confirm EnronQA's sensitivity advantage over NaturalQuestions/TriviaQA.
  2. **Retriever baseline sweep**: Benchmark BM25 vs. ColBERTv2 with multiple LLMs (Llama-8B, Llama-70B, GPT-4o) using the Table 4 protocol. Expect BM25 to outperform on this benchmark due to lexical overlap with proper nouns.
  3. **Memorization capacity probe**: Train LoRA adapters at ranks {32, 128, 512} on subsets of {100, 1000, 5000} QA pairs. Plot accuracy vs. facts memorized to identify the degradation threshold in your setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental limits of LLM memorization on private document corpora, and can memorization-based methods eventually match or surpass retrieval-based approaches in accuracy and efficiency?
- Basis in paper: [explicit] The conclusion states, "we hope that EnronQA can serve as a resource for testing these sorts of methods and exploring the limits of LLM memorization in the future."
- Why unresolved: Current memorization methods (e.g., LoRA adapters) underperform RAG on EnronQA, but the authors suggest improvements in pretraining techniques could narrow this gap.
- What evidence would resolve it: Benchmark results on EnronQA comparing state-of-the-art memorization methods (e.g., continued pretraining, parameter-efficient tuning) against RAG, with analysis of scaling laws and failure modes.

### Open Question 2
- Question: How does training memorization models directly on full documents (e.g., emails) rather than on derived QA pairs affect performance on private document QA tasks?
- Basis in paper: [explicit] Section 6 notes, "we hope future work can also explore training on the documents," as the current case study simplifies by using QA pairs.
- Why unresolved: Document-level training may better capture context and inter-document relationships but poses challenges in data efficiency and scalability, which are untested.
- What evidence would resolve it: Experiments training LLMs or adapters on the EnronQA email corpus (not just QA pairs) and evaluating on the same QA pairs, with metrics for factual recall and hallucination rates.

### Open Question 3
- Question: How can personalized RAG systems be developed to effectively leverage user-segmented private data while respecting privacy boundaries, and what metrics best evaluate personalization in retrieval?
- Basis in paper: [explicit] The introduction states that EnronQA enables "experimentation on the introduction of personalized retrieval settings over realistic data" for developing "measured approaches to private and personalized RAG."
- Why unresolved: The paper provides initial multi-user retrieval baselines but does not explore advanced personalization techniques or privacy-preserving constraints beyond segmentation.
- What evidence would resolve it: Systems that dynamically tailor retrieval to user context (e.g., user embeddings, federated retrieval) evaluated on EnronQA with new metrics for personalization accuracy and privacy leakage.

## Limitations

- The synthetic QA generation pipeline depends entirely on LLM judges, which may have systematic biases affecting question quality
- LoRA memorization shows capacity limits around 20,000 facts, suggesting fundamental constraints in low-rank parameter adaptation
- The calibration advantage over public benchmarks assumes tested LLMs have not been exposed to Enron email data during training

## Confidence

- **High confidence**: The dataset construction methodology (filtering pipeline, QA generation stages) is well-documented and reproducible. The retrieval baseline results (BM25 achieving 87.5% accuracy) are straightforward to verify.
- **Medium confidence**: The calibration advantage over public benchmarks is theoretically sound but depends on untested assumptions about LLM pretraining corpora. The LoRA memorization performance claims are specific but show capacity degradation that requires careful interpretation.
- **Low confidence**: The synthetic QA generation pipeline's quality depends entirely on the judgment capability of evaluation LLMs, which introduces potential blind spots not detectable through automated evaluation.

## Next Checks

1. **Pretraining contamination test**: Analyze whether leading LLMs (GPT-4, Claude, Llama-3) have been exposed to Enron email data during training by testing zero-shot performance on held-out EnronQA questions and comparing against control datasets.

2. **Question quality audit**: Sample 100 EnronQA questions and manually verify the multi-stage LLM pipeline's effectiveness by testing specificity (can humans distinguish correct email from 10 similar ones?), objectivity (are questions definitively answerable?), and groundedness (do questions require email context?).

3. **Memorization scaling experiment**: Systematically test LoRA adapter performance across 5 different ranks (32, 128, 512, 1024, 2048) and 4 fact scales (100, 1000, 5000, 10000) to precisely characterize the degradation threshold and identify whether the 20,000 fact limit is fundamental or architecture-dependent.