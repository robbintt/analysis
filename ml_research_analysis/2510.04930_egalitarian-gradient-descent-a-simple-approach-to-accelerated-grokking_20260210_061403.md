---
ver: rpa2
title: 'Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking'
arxiv_id: '2510.04930'
source_url: https://arxiv.org/abs/2510.04930
tags:
- grokking
- gradient
- which
- descent
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the phenomenon of "grokking" in deep learning,
  where models achieve perfect training accuracy early but only generalize to unseen
  data much later. The authors propose a new optimization method called Egalitarian
  Gradient Descent (EGD) that addresses this issue.
---

# Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking

## Quick Facts
- arXiv ID: 2510.04930
- Source URL: https://arxiv.org/abs/2510.04930
- Authors: Ali Saheb Pasand; Elvis Dohmatob
- Reference count: 14
- One-line primary result: EGD eliminates the long grokking plateau in modular arithmetic tasks, achieving high test accuracy after only a few epochs instead of thousands.

## Executive Summary
This paper addresses the phenomenon of "grokking" in deep learning, where models achieve perfect training accuracy early but only generalize to unseen data much later. The authors propose Egalitarian Gradient Descent (EGD), a method that modifies gradients so optimization dynamics evolve at the same speed along all principal directions. EGD achieves this by normalizing the gradient matrix through spectral decomposition, effectively equalizing all singular values to unity. The method is simple, hyperparameter-free, and can be seen as a modified version of natural gradient descent.

## Method Summary
EGD transforms the gradient matrix $G$ into $\tilde{G} = (GG^\top)^{-1/2}G$, which preserves the singular vectors while normalizing all singular values to 1. This equalizes optimization speed across all directions, preventing the stalling in slow directions that causes grokking. The method is applied only to hidden layer gradients in a two-layer ReLU network architecture. Training uses weight decay and switches from EGD to vanilla SGD once validation accuracy improves, avoiding potential instability.

## Key Results
- EGD eliminates the grokking plateau on modular arithmetic tasks (addition/multiplication modulo p ∈ {79,97,127}), achieving high test accuracy after only a few epochs versus thousands for vanilla SGD
- The method successfully accelerates generalization on sparse parity problems (Parity(n,k) with k-bit XOR on n-bit strings)
- EGD maintains final performance parity with vanilla SGD while drastically reducing training epochs required for generalization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Delayed generalization (grokking) is induced by anisotropic convergence speeds along the singular directions of the gradient matrices.
- **Mechanism:** In standard Gradient Descent, directions with large singular values (fast directions) optimize quickly, leading to rapid memorization. However, generalization often requires learning "slow" directions associated with small singular values. The optimization stalls until these slow modes catch up, creating the plateau.
- **Core assumption:** The generalizing solution lies in the subspace spanned by the singular vectors corresponding to small singular values.
- **Evidence anchors:**
  - [abstract]: "...grokking can be induced by asymmetric speeds of (stochastic) gradient descent, along different principal (i.e singular directions) of the gradients."
  - [section]: Page 6, Insight #1 states that for the toy problem, grokking is "sole due to a delayed convergence... caused by ill-conditioned gradients."
  - [corpus]: "To Grok Grokking..." supports the view that grokking is a distinct phase transition amenable to theoretical analysis via optimization dynamics.

### Mechanism 2
- **Claim:** Forcing all singular values of the gradient matrix to unity equalizes optimization speed across all directions, collapsing the grokking delay.
- **Mechanism:** EGD transforms the gradient matrix $G$ into $\tilde{G} = (GG^\top)^{-1/2}G$, which preserves the singular vectors but normalizes all singular values to 1. This prevents the optimizer from stalling in slow directions, allowing the model to form generalizing circuits immediately rather than waiting for the slow modes to evolve.
- **Core assumption:** Accelerating the slow modes does not destabilize the learning of fast modes (isotropy assumption).
- **Evidence anchors:**
  - [abstract]: "...normalizes the gradients so that dynamics along all the principal directions evolves at exactly the same speed."
  - [section]: Page 7, Eq. (11) and surrounding text prove that $\tilde{G} = UV^\top$, ensuring all singular values are equal.
  - [corpus]: "The Geometry of Grokking..." links grokking to norm minimization, while EGD provides a spectral method to accelerate this process.

### Mechanism 3
- **Claim:** EGD functions as a whitened variant of Natural Gradient Descent (NGD).
- **Mechanism:** NGD preconditions gradients by the inverse Fisher information matrix $F^{-1}$ to account for curvature. EGD uses $F^{-1/2}$. This "whitening" effect stabilizes the update magnitude (Fisher norm of the gradient becomes a constant of motion, equal to 1), acting as a specific form of natural gradient that is hyperparameter-free and avoids the complexities of inverting potentially degenerate Fisher matrices.
- **Core assumption:** The empirical Fisher matrix $GG^\top$ accurately captures the local geometry relevant for generalization.
- **Evidence anchors:**
  - [abstract]: "...can be seen as a carefully modified form of natural gradient descent..."
  - [section]: Page 8, Eq. (13) explicitly relates EGD $\tilde{G}$ and NGD $\bar{G}$ via $\tilde{G} = F^{1/2}\bar{G}$.

## Foundational Learning

- **Concept:** **Singular Value Decomposition (SVD)**
  - **Why needed here:** EGD relies on decomposing the gradient matrix $G$ into $U \Sigma V^\top$ to identify principal directions and normalize the singular values $\Sigma$.
  - **Quick check question:** Can you explain the geometric meaning of the matrices $U$, $\Sigma$, and $V^\top$ in terms of rotation and scaling?

- **Concept:** **Condition Number**
  - **Why needed here:** The paper attributes grokking to "ill-conditioned" gradients (high condition number), where the ratio of largest to smallest singular values is large, causing disparate convergence speeds.
  - **Quick check question:** How does the condition number of the Hessian (or gradient covariance) affect the trajectory and speed of Gradient Descent?

- **Concept:** **Grokking**
  - **Why needed here:** This is the target phenomenon. Understanding that it is a phase transition from memorization to generalization is essential to valuing the acceleration EGD provides.
  - **Quick check question:** What distinguishes the "memorization" phase from the "grokking" phase in terms of training vs. test accuracy?

## Architecture Onboarding

- **Component map:** Standard Backprop -> Spectral Decomposer -> Gradient Normalizer -> Weight Updater
- **Critical path:** The calculation of the matrix square root inverse $(GG^\top)^{-1/2}$. This is the computational bottleneck. Implementation must decide between exact SVD (costly) or randomized approximations.
- **Design tradeoffs:**
  - **Compute vs. Epochs:** EGD adds per-step overhead (SVD) but drastically reduces total epochs (from $10^3$ to $\sim 10^1$).
  - **Precision vs. Speed:** The paper suggests randomized SVD works fine, implying the exact spectral structure is less important than the general equalization of magnitudes.
  - **Memory:** Unlike Grokfast (which buffers past gradients), EGD is stateless, costing no extra memory.
- **Failure signatures:**
  - **Rank Deficiency:** If $G$ is rank-deficient, $(GG^\top)^{-1}$ does not exist. Implementation must use Moore-Penrose pseudo-inverse (Remark 1).
  - **Numerical Instability:** Small singular values can cause instability during inversion; numerical safeguards (epsilon additions) are needed.
- **First 3 experiments:**
  1. **Baseline Reproduction (Toy Data):** Implement the anisotropic Gaussian classification (Figure 5) to verify that vanilla GD plateaus for $k \propto 1/\epsilon$ epochs while EGD converges immediately.
  2. **Ablation on Approximation:** Implement EGD using exact SVD vs. randomized SVD to measure the trade-off in per-step latency vs. convergence speed on Modular Addition (Figure 1).
  3. **Early Stopping Test:** Verify the "switch-off" strategy. Train with EGD until test accuracy rises, then switch to vanilla SGD to confirm stability and final performance parity.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can low-cost spectral approximations (e.g., randomized SVD, sketching) maintain EGD's acceleration benefits while reducing computational overhead for large-scale models?
- **Basis:** [explicit] Page 9 states the intent to "explore optional low-cost spectral surrogates" like "randomized SVD" and "streaming or online PCA" to improve efficiency.
- **Why unresolved:** The authors note that computing the SVD is the main computational cost, but they have not yet tested approximate methods to mitigate this.
- **What evidence would resolve it:** Experiments comparing training time and generalization speed between exact EGD and approximation variants on large-scale datasets.

### Open Question 2
- **Question:** Does EGD effectively accelerate grokking in non-algorithmic domains such as computer vision or natural language processing?
- **Basis:** [explicit] Page 9 lists "broader benchmarks beyond algorithmic tasks" as a specific direction for future work.
- **Why unresolved:** The paper's empirical validation is restricted to algorithmic problems (sparse parity, modular arithmetic).
- **What evidence would resolve it:** Empirical results demonstrating reduced grokking plateaus on standard vision or language benchmarks when using EGD.

### Open Question 3
- **Question:** How does EGD interact with adaptive optimizers (like Adam) and weight decay in complex training loops?
- **Basis:** [explicit] Page 9 proposes studying "plug-and-play combinations with adaptive optimizers and weight decay."
- **Why unresolved:** The current experiments focus primarily on SGD/GD, leaving the interaction with adaptive learning rate mechanisms unexplored.
- **What evidence would resolve it:** Comparative studies analyzing training dynamics when EGD is applied within Adam or other adaptive optimization frameworks.

### Open Question 4
- **Question:** How does EGD behave under non-stationary data distributions or curriculum learning schedules?
- **Basis:** [explicit] Page 9 calls for the study of "behavior under non-stationary data and curriculum schedules."
- **Why unresolved:** The experimental setup assumes static data distributions, whereas practical training often involves shifting data.
- **What evidence would resolve it:** Analysis of EGD's ability to maintain accelerated generalization in continual learning scenarios with distribution shifts.

## Limitations

- The theoretical claims primarily demonstrate grokking causation on a simplified toy problem rather than the complex modular arithmetic tasks where practical benefits are shown
- The mechanism connecting singular value spectra to generalization performance in deep networks remains largely empirical
- The computational overhead of SVD remains a practical limitation for large-scale applications

## Confidence

- **High Confidence:** The mathematical derivation of EGD as gradient normalization (Ẽ = (GG⊤)^(-1/2)G) is rigorous and the computational experiments demonstrating acceleration on modular arithmetic are convincing.
- **Medium Confidence:** The claim that EGD is a whitened variant of Natural Gradient Descent is theoretically sound but the practical equivalence requires more empirical validation.
- **Low Confidence:** The assertion that grokking is "solely due to" anisotropic gradient convergence in general deep learning scenarios needs broader validation beyond the specific toy problem.

## Next Checks

1. Test EGD on CIFAR-10/100 with standard architectures to verify whether the acceleration generalizes beyond synthetic tasks.
2. Conduct ablation studies measuring the singular value spectra of gradient matrices during training to directly observe the hypothesized ill-conditioning.
3. Compare EGD's performance against other preconditioning methods (Adam, Shampoo) on tasks where grokking is observed to isolate the specific contribution of isotropic gradient normalization.