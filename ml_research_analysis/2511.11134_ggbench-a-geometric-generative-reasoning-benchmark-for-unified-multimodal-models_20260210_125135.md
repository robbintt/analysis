---
ver: rpa2
title: 'GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal
  Models'
arxiv_id: '2511.11134'
source_url: https://arxiv.org/abs/2511.11134
tags:
- reasoning
- geometric
- multimodal
- arxiv
- construction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GGBench introduces the first benchmark for geometric generative
  reasoning, evaluating whether models can construct verifiable solutions by integrating
  language understanding, reasoning, and visual generation. Each problem includes
  aligned text, executable GeoGebra code, and rendered diagrams, enabling holistic,
  process-level evaluation.
---

# GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models

## Quick Facts
- arXiv ID: 2511.11134
- Source URL: https://arxiv.org/abs/2511.11134
- Authors: Jingxuan Wei; Caijun Jia; Xi Bai; Xinglong Xu; Siyuan Li; Linzhuang Sun; Bihui Yu; Conghui He; Lijun Wu; Cheng Tan
- Reference count: 40
- Primary result: First benchmark evaluating geometric generative reasoning through verifiable code execution

## Executive Summary
GGBench introduces the first benchmark for geometric generative reasoning, evaluating whether models can construct verifiable solutions by integrating language understanding, reasoning, and visual generation. Each problem includes aligned text, executable GeoGebra code, and rendered diagrams, enabling holistic, process-level evaluation. Experiments across UMMs and LLMs show that models generating and executing code (e.g., GPT-5, Claude Sonnet 4.5) significantly outperform end-to-end image generators in geometric correctness, planning quality, and human ratings, with GPT-5 achieving the highest overall VLM-I score of 57.08 and human score of 83.06. Performance declines sharply on complex reasoning categories and higher difficulty levels, highlighting the need for symbolic-geometric alignment. The benchmark provides a rigorous foundation for advancing grounded, verifiable multimodal reasoning.

## Method Summary
GGBench evaluates unified multimodal models on geometric generative reasoning by requiring construction of verifiable geometric figures from text descriptions. The benchmark includes 1,411 construction problems with tri-modal alignment (text, executable GeoGebra code, rendered images), spanning three difficulty levels (Easy/Medium/Hard) and eight reasoning categories. Evaluation uses a four-stage pipeline: VLM-T (planning quality), VLM-I-Mid (intermediate accuracy), VLM-I-Res (final correctness), and VLM-I (overall score). Two tracks are supported: end-to-end UMMs generating images directly, and LLMs generating executable GeoGebra code. GPT-4o serves as VLM judge with fixed prompts, achieving high correlation with human evaluation (r=0.9295).

## Key Results
- GPT-5 achieves highest overall VLM-I score of 57.08 and human score of 83.06
- Code-driven models (GPT-5, Claude Sonnet 4.5) significantly outperform end-to-end image generators in geometric correctness
- Performance drops sharply on complex reasoning categories (Measurement & Ratios, Applications of Theorems) and higher difficulty levels
- VLM-based evaluation shows high correlation with human judgment (r=0.9295), validating the automated assessment approach

## Why This Works (Mechanism)
The benchmark works by enforcing tri-modal alignment—each problem links text descriptions to executable GeoGebra code and rendered images. This creates a verifiable feedback loop where geometric correctness can be objectively assessed through code execution rather than subjective visual inspection. The multi-stage evaluation (planning, intermediate steps, final result) captures the complete reasoning process, not just the final output. By separating code generation from image rendering, the benchmark isolates geometric reasoning from visual generation capabilities, revealing that symbolic representation and execution are critical for geometric accuracy.

## Foundational Learning
- **Geometric Construction Principles**: Understanding classical compass-and-straightedge constructions as the foundation for representing geometric problems in executable form. Needed because modern models often conflate calculation with construction. Quick check: Can the model construct a 40° angle using the central angle theorem rather than direct rotation?
- **Tri-Modal Alignment**: The alignment between natural language descriptions, symbolic GeoGebra code, and rendered diagrams as a framework for verifiable reasoning. Needed because text alone lacks precision and images alone lack executability. Quick check: Does the generated code accurately reflect the textual constraints when executed?
- **Process-Level Evaluation**: Multi-stage assessment (planning, intermediate, final) rather than binary correct/incorrect judgments. Needed because geometric reasoning is sequential and errors compound. Quick check: Are intermediate construction steps geometrically valid even if the final result fails?
- **Symbolic vs. Perceptual Reasoning**: Distinguishing between geometric correctness (theorem-based) and visual plausibility (pixel-based). Needed because models can generate visually convincing but mathematically incorrect figures. Quick check: Does the figure satisfy the stated theorems or just appear correct?
- **Executable Code as Ground Truth**: Using GeoGebra code execution as an objective correctness criterion. Needed because geometric problems have definitive solutions that can be verified algorithmically. Quick check: Does the code execute without errors and produce the described figure?
- **Difficulty Progression**: Structured escalation from basic constructions to complex theorem applications. Needed to identify specific reasoning capabilities and limitations. Quick check: Does performance decline predictably with increased complexity?

## Architecture Onboarding

**Component Map**
Text Input -> Model (LLM/UMM) -> GeoGebra Code -> Execution Engine -> Rendered Image -> VLM Judge -> Evaluation Metrics

**Critical Path**
Text Description → GeoGebra Code Generation → Code Execution → Image Rendering → VLM Evaluation → Score Aggregation

**Design Tradeoffs**
- Code generation vs. end-to-end image generation: Code provides verifiability but requires external execution; images are self-contained but lack objective correctness criteria
- Symbolic representation vs. perceptual similarity: Symbolic ensures mathematical correctness; perceptual matches human intuition but can miss subtle errors
- Multi-stage vs. single-stage evaluation: Multi-stage captures reasoning process but increases complexity; single-stage is simpler but less diagnostic

**Failure Signatures**
- GeoGebra syntax errors (reserved keywords, undefined objects)
- Logical errors where visual output appears correct but violates geometric constraints
- Construction errors where models calculate instead of construct
- Planning failures where the construction sequence is invalid

**First Experiments**
1. Run a simple construction (e.g., equilateral triangle) through both code generation and end-to-end models to verify basic functionality
2. Test a problem with known logical pitfalls (e.g., inscribed angle construction) to identify whether models confuse calculation with construction
3. Evaluate a multi-step construction requiring intermediate object references to test planning and variable management

## Open Questions the Paper Calls Out

**Open Question 1**: To what extent does the tri-modal (text-code-image) evaluation framework generalize to other domains requiring generative reasoning, such as physics or architectural design? The paper establishes the framework for geometry but hasn't validated it in domains with different constraint types.

**Open Question 2**: Can end-to-end Unified Multimodal Models be trained to enforce precise spatial constraints without relying on intermediate executable code? The paper shows current UMMs fail at constraint enforcement, but doesn't determine if this is a fundamental limitation or training deficiency.

**Open Question 3**: What specific improvements in symbolic-geometric alignment are required to close the performance gap in "Measurement & Ratios" and "Applications of Theorems" categories? The paper identifies these as most challenging but doesn't propose concrete mechanisms for improvement.

**Open Question 4**: How robust is the VLM-based evaluation metric against visual "hallucinations" that appear geometrically plausible but are mathematically incorrect? While correlation with humans is high, the risk of missing subtle mathematical errors remains unquantified.

## Limitations

- Evaluation relies heavily on GPT-4o as VLM judge, introducing potential systematic biases despite high human correlation
- GPT-5 results cannot be independently verified as the model is not publicly accessible
- Tri-modal alignment assumes perfect consistency between text, code, and images, but geometric descriptions can have inherent ambiguities
- The benchmark focuses on classical geometric constructions, potentially limiting generalizability to other mathematical domains

## Confidence

- **High Confidence**: Benchmark construction methodology and code execution evaluation pipeline are well-specified and reproducible
- **Medium Confidence**: Relative performance rankings between model categories are robust, though absolute scores depend on VLM judge consistency
- **Low Confidence**: Specific numerical results for GPT-5 and exact performance gaps to other models cannot be independently verified

## Next Checks

1. Implement independent validation using multiple VLM judges (including open-source alternatives) to assess whether GPT-4o's judgments align with broader consensus on geometric correctness

2. Systematically test GeoGebra code generation with reserved keyword conflicts and undefined object references to quantify syntax failures versus geometric reasoning errors

3. Conduct detailed error analysis for each of the eight reasoning categories to identify whether performance gaps stem from geometric understanding, planning, or execution limitations