---
ver: rpa2
title: 'Evaluating Large Language Models for Diacritic Restoration in Romanian Texts:
  A Comparative Study'
arxiv_id: '2511.13182'
source_url: https://arxiv.org/abs/2511.13182
tags:
- restoration
- diacritics
- llms
- diacritic
- romanian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of large language models (LLMs)
  in restoring diacritics in Romanian texts. Using a comprehensive corpus, models
  including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama
  2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's
  RoLlama 2 7B were tested under multiple prompt templates ranging from zero-shot
  to complex multi-shot instructions.
---

# Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study

## Quick Facts
- **arXiv ID:** 2511.13182
- **Source URL:** https://arxiv.org/abs/2511.13182
- **Reference count:** 0
- **One-line primary result:** Proprietary models (e.g., GPT-4o) achieve high diacritic restoration accuracy in Romanian, outperforming open-source models and baselines.

## Executive Summary
This study evaluates large language models (LLMs) on the task of diacritic restoration in Romanian texts, where accents and special characters (ă, â, î, ș, ț) are stripped from input sentences and must be restored. Using a curated subset of 1,000 statements from the dexonline Project, the authors test a range of proprietary and open-source LLMs—including OpenAI's GPT series, Google's Gemini, Meta's Llama family, and Mistral's Mixtral—under multiple prompt templates from zero-shot to three-shot. Results show that models like GPT-4o achieve high restoration accuracy and consistently surpass a neutral echo baseline, while others, such as Meta's Llama models, exhibit more variability. The findings highlight the importance of model architecture, training data, and prompt design in diacritic restoration, with implications for improving NLP tools for diacritic-rich languages.

## Method Summary
The study uses two subsets of 1,000 statements each from the dexonline Project corpus, with input text stripped of diacritics and ground truth being the original. Twelve LLMs—including GPT-3.5, GPT-4, GPT-4o, Gemini 1.0 Pro, Llama 2, Llama 3, Mixtral 8x7B Instruct, airoboros 70B, and RoLlama 2 7B—are evaluated under five prompt templates (zero-shot to three-shot) provided in Appendix B. Restoration Accuracy (RA) and Restoration Error Rate (RER) are calculated at character and word levels, with case-sensitive and case-insensitive modes, using Levenshtein distance. The study focuses on inference-only evaluation, without task-specific fine-tuning.

## Key Results
- GPT-4o consistently achieves the highest diacritic restoration accuracy, surpassing all other models and the neutral echo baseline.
- Meta's Llama family shows significant variability in performance, with some models underperforming compared to proprietary alternatives.
- Open-weight models like Mixtral 8x7B Instruct and RoLlama 2 7B exhibit moderate accuracy, indicating room for improvement through fine-tuning.
- Prompt design significantly impacts restoration performance, with multi-shot instructions generally yielding better results than zero-shot.

## Why This Works (Mechanism)
None specified.

## Foundational Learning
- **Diacritic restoration task:** Removing and restoring special characters (ă, â, î, ș, ț) in Romanian text is crucial for readability and searchability.
  - *Why needed:* Romanian orthography relies on diacritics for correct meaning and pronunciation; their omission leads to ambiguity.
  - *Quick check:* Verify that the input corpus is correctly stripped of all diacritics and that the ground truth includes them.
- **Levenshtein distance for evaluation:** Measures the minimum number of edits (insertions, deletions, substitutions) needed to transform one string into another.
  - *Why needed:* Provides a quantitative metric to compare restored text against ground truth at character and word levels.
  - *Quick check:* Confirm that the evaluation script correctly calculates RA and RER using Levenshtein distance.
- **Prompt engineering (zero-shot to few-shot):** Designing instructions to guide LLMs in restoring diacritics without or with few examples.
  - *Why needed:* Influences model performance, especially for open-weight models that may require more explicit guidance.
  - *Quick check:* Review Appendix B for prompt templates and ensure they are correctly formatted and applied.
- **Model architecture impact:** Different LLM architectures (e.g., transformer-based GPT vs. Llama) affect restoration accuracy.
  - *Why needed:* Identifies which model families are better suited for language-specific tasks like diacritic restoration.
  - *Quick check:* Compare RA scores across model families to identify architectural strengths and weaknesses.
- **Open-source vs. proprietary models:** Trade-offs between accessibility, performance, and need for fine-tuning.
  - *Why needed:* Guides decisions on which models to deploy or improve for diacritic restoration in resource-constrained settings.
  - *Quick check:* Evaluate whether open-weight models can achieve comparable accuracy with fine-tuning on the dexonline corpus.

## Architecture Onboarding

**Component map:** dexonline corpus -> preprocessing (diacritic stripping) -> LLM inference (prompt templates) -> Levenshtein evaluation (RA/RER) -> results aggregation

**Critical path:** Data preparation (corpus extraction and diacritic stripping) -> Model inference (LLM API or local weights) -> Evaluation (RA/RER calculation) -> Results analysis

**Design tradeoffs:** The study prioritizes inference-only evaluation over fine-tuning, allowing for rapid benchmarking but limiting exploration of model improvement strategies. The choice of prompt templates balances simplicity (zero-shot) and guidance (multi-shot), but may not capture all possible instructions for optimal performance.

**Failure signatures:** 
- Llama 2 models often ignore "respond strictly with restored text" instructions, returning conversational filler instead of raw output.
- Mixtral 8x7B Instruct may over-generate diacritics, increasing error rates by hallucinating characters where none exist.

**First experiments:**
1. Verify data extraction and preprocessing by checking that input sentences are correctly stripped of diacritics and ground truth includes them.
2. Test inference pipeline with a small subset of sentences and GPT-4o to confirm API connectivity and output format.
3. Run evaluation script on a known test case to ensure RA and RER are calculated correctly using Levenshtein distance.

## Open Questions the Paper Calls Out
1. **What specific fine-tuning strategies and data volumes are required for underperforming open-source models (e.g., Llama 2) to achieve state-of-the-art performance in Romanian diacritic restoration?**
   - *Basis:* The authors note in Section 6 that the underperformance of models like Llama 2 highlights a need for determining the "extent of finetuning and the strategies that underperforming open-source models... would need to adopt."
   - *Why unresolved:* The study evaluated models primarily in zero-shot and few-shot inference modes without performing task-specific fine-tuning on the diacritic restoration corpus.
   - *What evidence would resolve it:* A comparative study measuring the Restoration Accuracy (RA) of open-source models before and after supervised fine-tuning on the DLRLC and CRAWLER datasets.

2. **How effectively do LLMs restore diacritics in historical Romanian texts that adhere to pre-1993 orthographic rules?**
   - *Basis:* Section 3.1 and Section 10.1 note that the current focus is on post-reform orthography, and that "Analysis of pre-1993 reform data... is planned for future research."
   - *Why unresolved:* The study explicitly filtered or utilized datasets reflecting contemporary standards, leaving the handling of the complex "î" vs. "â" historical shifts untested.
   - *What evidence would resolve it:* Evaluating the current model prompt configurations on a corpus of texts verified to follow pre-1993 orthographic standards.

3. **Are the high accuracy levels achieved by proprietary models like GPT-4o generalizable to other diacritic-rich languages with different morphological complexities?**
   - *Basis:* Section 10.6 lists "Language-Specific Challenges" as a limitation, stating that "findings of this study may not directly translate to other languages."
   - *Why unresolved:* The experimental scope was restricted to Romanian, which has specific diacritical and orthographic properties that may not represent other languages.
   - *What evidence would resolve it:* Replicating the methodology (prompts and evaluation metrics) on similar corpora for languages such as Vietnamese, Polish, or Turkish.

## Limitations
- **Undisclosed test set construction:** The exact sentence indices and random seed for the 1,000-statement subset are not disclosed, complicating precise replication.
- **Unspecified inference parameters:** Generation parameters (e.g., temperature, max_tokens) are not provided, introducing variability in model outputs.
- **Proprietary model version drift:** Future API updates to models like GPT-4o may alter performance, making exact score replication challenging.
- **Limited language scope:** Findings are based solely on Romanian, limiting generalizability to other diacritic-rich languages.

## Confidence
- **High Confidence:** The overall ranking of model performance (e.g., GPT-4o consistently outperforming others) and the identification of architecture-specific limitations (e.g., Llama 2's instruction-following issues) are well-supported by the experimental design and results.
- **Medium Confidence:** The impact of prompt design on performance is demonstrated, but the specific mechanisms driving variability in models like Mixtral 8x7B remain underexplored.
- **Low Confidence:** The reproducibility of exact scores is low due to undisclosed test splits and generation parameters.

## Next Checks
1. **Validate Test Set Reproducibility:** Confirm the exact sentence indices or random seed used to select the 1,000-statement subset from the dexonline corpus to ensure test set consistency.
2. **Benchmark Generation Parameters:** Systematically test different inference settings (e.g., temperature=0 vs. higher values) to determine their impact on diacritic restoration accuracy, particularly for open-weight models.
3. **Monitor API Model Drift:** Track performance of proprietary models (e.g., GPT-4o) over time to quantify the effect of version updates on diacritic restoration accuracy.