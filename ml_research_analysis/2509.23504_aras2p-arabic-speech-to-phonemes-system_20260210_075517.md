---
ver: rpa2
title: 'AraS2P: Arabic Speech-to-Phonemes System'
arxiv_id: '2509.23504'
source_url: https://arxiv.org/abs/2509.23504
tags:
- arabic
- arxiv
- pretraining
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents AraS2P, a speech-to-phonemes system that ranked
  first in the Iqra''Eval 2025 shared task for Quranic pronunciation assessment. The
  system adapts Wav2Vec2-BERT through a two-stage training strategy: first, task-adaptive
  pretraining on large-scale Arabic speech-phoneme datasets generated using the MSA
  Phonetiser from Common Voice, SADA, and MASC; second, fine-tuning on the official
  task data augmented with XTTS-v2 synthesized recitations featuring varied Ayat segments,
  speaker embeddings, and textual perturbations to simulate human errors.'
---

# AraS2P: Arabic Speech-to-Phonemes System
## Quick Facts
- **arXiv ID**: 2509.23504
- **Source URL**: https://arxiv.org/abs/2509.23504
- **Reference count**: 11
- **Primary result**: Ranked first in Iqra'Eval 2025 shared task for Quranic pronunciation assessment

## Executive Summary
AraS2P presents a speech-to-phonemes system designed for Arabic mispronunciation detection in Quranic recitation. The system adapts Wav2Vec2-BERT through a two-stage training strategy, combining task-adaptive pretraining on large-scale Arabic speech-phoneme datasets with fine-tuning on augmented task-specific data. The approach addresses challenges posed by Arabic's complex phonemic inventory and diacritic significance through strategic data augmentation and phoneme-aware pretraining.

## Method Summary
The system employs a two-stage training strategy: first, task-adaptive pretraining on large-scale Arabic speech-phoneme datasets generated using the MSA Phonetiser from Common Voice, SADA, and MASC; second, fine-tuning on official task data augmented with XTTS-v2 synthesized recitations featuring varied Ayat segments, speaker embeddings, and textual perturbations. This methodology leverages both synthetic data generation and phoneme-aware pretraining to improve performance on phoneme-level mispronunciation detection in Quranic Arabic.

## Key Results
- Achieved first place ranking in Iqra'Eval 2025 shared task
- Final F1-score of 0.4726 on official leaderboard
- Demonstrated effectiveness of phoneme-aware pretraining combined with targeted augmentation

## Why This Works (Mechanism)
The system's success stems from addressing Arabic's complex phonemic challenges through two key mechanisms: first, the two-stage training strategy that bridges the gap between general Arabic speech and Quranic recitation-specific patterns; second, the comprehensive augmentation pipeline that simulates diverse pronunciation errors and speaker variations. The combination of Wav2Vec2-BERT adaptation with synthetic data generation creates a robust system capable of detecting subtle mispronunciations in Quranic Arabic.

## Foundational Learning
- **Wav2Vec2-BERT adaptation**: Needed to capture Arabic phonetic patterns; quick check: verify pretraining dataset coverage of Arabic phonemes
- **Speech-phoneme alignment**: Required for accurate mispronunciation detection; quick check: assess alignment accuracy between speech and phoneme sequences
- **Synthetic data generation**: Essential for creating diverse training examples; quick check: validate quality of XTTS-v2 generated speech
- **Text augmentation techniques**: Critical for simulating human errors; quick check: measure augmentation's impact on error pattern diversity
- **MSA Phonetiser usage**: Necessary for converting text to phonemes; quick check: verify phonetiser accuracy across Arabic dialects
- **Task-adaptive pretraining**: Important for domain-specific performance; quick check: compare with standard fine-tuning approaches

## Architecture Onboarding
- **Component map**: MSA Phonetiser -> Dataset generation -> Wav2Vec2-BERT pretraining -> XTTS-v2 synthesis -> Fine-tuning -> Evaluation
- **Critical path**: Pretraining on generated datasets → Augmentation with synthetic data → Fine-tuning on task data → Evaluation
- **Design tradeoffs**: Synthetic data quality vs. quantity, augmentation complexity vs. training efficiency
- **Failure signatures**: Poor generalization to new speakers, sensitivity to diacritic variations, over-reliance on specific error patterns
- **First experiments**:
  1. Baseline fine-tuning without pretraining
  2. Pretraining with reduced augmentation
  3. Ablation of XTTS-v2 synthetic data

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- Moderate F1-score of 0.4726 suggests room for improvement in practical deployment
- Heavy reliance on synthetic data raises concerns about real-world generalization
- Limited validation beyond the specific Iqra'Eval 2025 evaluation framework

## Confidence
- **High confidence**: First place ranking in Iqra'Eval 2025 shared task; technical soundness of two-stage training methodology
- **Medium confidence**: Effectiveness of augmentation strategy; competitive performance within evaluation framework
- **Low confidence**: Generalization to broader Arabic contexts; quality of automatically generated training data; real-world deployment across diverse speakers

## Next Checks
1. Conduct cross-dataset evaluation using independent Quranic recitation corpora and non-Quranic Arabic speech datasets to assess generalization performance
2. Perform ablation studies systematically removing or modifying components of the augmentation pipeline to quantify individual contributions
3. Implement human evaluation studies comparing system predictions against expert Quranic reciter assessments across multiple error categories