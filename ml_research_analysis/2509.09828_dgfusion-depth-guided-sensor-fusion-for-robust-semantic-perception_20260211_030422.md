---
ver: rpa2
title: 'DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception'
arxiv_id: '2509.09828'
source_url: https://arxiv.org/abs/2509.09828
tags:
- depth
- fusion
- semantic
- sensor
- lidar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DGFusion addresses robust semantic perception for autonomous vehicles
  by combining multiple sensors under challenging environmental conditions. The core
  method leverages lidar not only as an input modality but also as ground truth for
  auxiliary depth estimation, enabling depth-aware cross-modal fusion through spatially
  varying depth tokens and a global condition token.
---

# DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception

## Quick Facts
- arXiv ID: 2509.09828
- Source URL: https://arxiv.org/abs/2509.09828
- Reference count: 34
- DGFusion achieves 61.03% PQ on MUSES and 56.7% mIoU on DeLiVER, outperforming prior methods in adverse conditions.

## Executive Summary
DGFusion addresses robust semantic perception for autonomous vehicles by combining multiple sensors under challenging environmental conditions. The core method leverages lidar not only as an input modality but also as ground truth for auxiliary depth estimation, enabling depth-aware cross-modal fusion through spatially varying depth tokens and a global condition token. The network integrates an outlier-robust depth loss with edge-aware and panoptic-edge-aware smoothness terms to handle sparse, noisy lidar measurements. DGFusion achieves state-of-the-art panoptic segmentation performance on MUSES (61.03% PQ) and DeLiVER (56.7% mIoU), significantly outperforming prior methods in adverse conditions such as fog, rain, and snow. The approach demonstrates robust generalization across datasets and environmental conditions while maintaining computational efficiency with only 2.3% parameter overhead.

## Method Summary
DGFusion is a multi-modal panoptic and semantic segmentation network that uses depth-guided cross-modal fusion. The method employs a Swin-T backbone with modality-specific feature adapters, projecting all sensor inputs (RGB, lidar, radar, events) to the camera plane. During training, an auxiliary depth head uses sparse lidar as noisy supervision with τ-quantile outlier filtering and smoothness regularization. Depth-guided fusion uses local depth tokens (DT) and a global condition token (CT) to condition cross-attention between modalities. At inference, the depth head is removed but depth-aware features remain to guide fusion. The network achieves robust performance across adverse weather conditions through this depth-aware fusion mechanism and multi-task supervision.

## Key Results
- Achieves 61.03% PQ on MUSES panoptic segmentation benchmark, outperforming prior methods
- Reaches 56.7% mIoU on DeLiVER semantic segmentation, demonstrating strong cross-dataset generalization
- Shows consistent improvements across all adverse conditions (fog, rain, snow, night) with only 2.3% parameter overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local depth tokens enable spatially adaptive cross-modal fusion by conditioning attention on depth-varying sensor reliability.
- Mechanism: Depth features are partitioned into local windows and pooled into per-window Depth Tokens (DTs). These DTs concatenate with RGB tokens and a global Condition Token (CT) to form queries in cross-attention. After fusion, DTs and CT are removed. This allows the network to weight modalities differently per-region based on local depth cues (e.g., favoring lidar nearby, camera at medium range).
- Core assumption: Sensor reliability varies spatially with depth and environmental conditions in a way that local depth information can capture.
- Evidence anchors:
  - [abstract] "spatially varying local depth tokens that condition our attentive cross-modal fusion"
  - [section III-A, page 4] Equation 4–6 describe DT extraction and query formation; "each local-window cross-attentive fusion is jointly guided both by the global condition context encoded in the CT and the localized depth cues"
  - [corpus] SDGOCC uses depth-guided BEV transformation for occupancy prediction, showing depth cues can guide multimodal fusion; CaR1 notes camera-radar complementary depth/semantic strengths. Limited direct validation of DT mechanism in corpus.
- Break condition: If sensor reliability does not correlate with depth in your domain (e.g., controlled indoor lighting), local depth conditioning may not provide signal and could add noise.

### Mechanism 2
- Claim: Multi-task supervision with an auxiliary depth head improves semantic feature representations at no inference cost.
- Mechanism: During training, lidar serves as sparse, noisy ground truth for depth prediction via an auxiliary depth head (Semantic FPN decoder). Depth features are shared with the fusion module. At inference, the depth head is discarded; only the learned depth-aware features remain to guide fusion.
- Core assumption: Depth and semantics share informative structure; depth supervision regularizes features in ways that transfer to segmentation.
- Evidence anchors:
  - [abstract] "utilizing the lidar measurements... both as one of the model's inputs and as ground truth for learning depth"
  - [section IV-B, Table IV] Adding auxiliary depth head improves PQ by +0.64% without changing inference architecture
  - [corpus] EGSA-PT explores multi-task depth/segmentation but notes potential negative cross-task interactions; not all multi-task setups help. Assumption: synergy depends on task alignment and loss design.
- Break condition: If depth labels are extremely sparse or systematically biased (e.g., transparent surfaces), auxiliary depth supervision may mislead features.

### Mechanism 3
- Claim: τ-quantile outlier filtering and panoptic-edge-aware smoothness enable robust depth learning from noisy lidar.
- Mechanism: The robust L1 loss only supervises pixels below the τ-quantile of log-depth errors (τ=0.8). Edge-aware smoothness (weighted by RGB gradients) and panoptic-edge-aware smoothness (weighted by instance boundaries) regularize depth in unsupervised regions.
- Core assumption: Adverse weather introduces outlier depth measurements; smoothness priors generalize across conditions.
- Evidence anchors:
  - [section III-B, page 5] Equations 8–15 define τ-filtering, edge-aware, and panoptic-edge-aware losses
  - [section IV-B, Table V] Combining smoothness terms with τ-filtering yields +1.05% PQ over baseline
  - [corpus] Limited direct validation of τ-filtering in corpus; SDGOCC uses depth supervision but not outlier filtering explicitly.
- Break condition: If your lidar noise is systematic rather than outlier-dominated, or if scenes lack clear panoptic edges, smoothness terms may over-regularize.

## Foundational Learning

- Concept: Cross-attention with auxiliary conditioning tokens
  - Why needed here: Fusion uses CT (global) and DT (local) as extra query tokens to modulate cross-modal attention.
  - Quick check question: Can you explain how appending tokens to queries changes which values get attended to?

- Concept: Multi-task learning with auxiliary heads
  - Why needed here: Depth head provides training-time supervision without inference-time cost; features transfer to main task.
  - Quick check question: What conditions make auxiliary tasks helpful vs. harmful for the main task?

- Concept: Robust loss design for noisy labels
  - Why needed here: Adverse-weather lidar is sparse and noisy; standard losses overfit to outliers.
  - Quick check question: Why does quantile-based filtering help when noise is non-Gaussian?

## Architecture Onboarding

- Component map:
  - Shared backbone (Swin-T) + modality-specific feature adapters → per-modality feature pyramids
  - Three branches: (1) Depth estimation (FPN decoder, training-only), (2) Segmentation (OneFormer head), (3) Condition representation (Transformer → CT)
  - Depth-Guided Fusion: local-window cross-attention per secondary modality, queries = [RGB tokens, CT, local DT], keys/values from secondary modality
  - Output: reassembled fused features → segmentation head

- Critical path:
  1. Understand how depth features d_l are computed (Eq. 1) and pooled into DTs (Eq. 4)
  2. Trace CT → FC mapping (Eq. 5) and query construction for cross-attention (Eq. 6)
  3. Verify DT/CT removal after fusion (Eq. 6–7) to ensure output alignment

- Design tradeoffs:
  - τ value: Higher τ retains more supervision but includes noisier pixels. Default τ=0.8 balances coverage vs. robustness.
  - Smoothness weights (λ_es, λ_pes): Higher values enforce smoother depth but may blur boundaries. Default both 0.05.
  - Window size K_w: Larger windows capture more context but reduce local adaptivity. Inherited from CAFuser.

- Failure signatures:
  - Over-smoothed depth near object boundaries → check λ_es/λ_pes scaling; visualize depth gradients vs. panoptic edges
  - Noisy fusion in fog/snow → inspect τ-filtering; may need lower τ or stronger smoothness
  - Poor generalization to new conditions → CT may not capture unseen conditions; consider expanding condition descriptions

- First 3 experiments:
  1. Reproduce Table IV ablation: baseline → +aux depth head → +DT. Confirm each component contributes.
  2. Visualize depth predictions with/without τ-filtering on fog/rain samples to verify outlier rejection.
  3. Test cross-dataset transfer: train on MUSES, evaluate on DeLiVER (or vice versa) to assess generalization claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DGFusion's depth-guided fusion mechanism be extended to improve detection of heavily occluded objects, where the method currently shows limitations?
- Basis in paper: [explicit] The authors state in their qualitative results that DGFusion "misses the second person, highlighting a limitation in detecting heavily occluded objects."
- Why unresolved: The current depth tokens and cross-attention fusion do not explicitly model occlusion relationships, and the paper does not propose mechanisms to handle severe occlusion scenarios.
- What evidence would resolve it: Ablation studies incorporating explicit occlusion-aware depth tokens or occlusion modeling, with quantitative evaluation on heavily occluded object subsets.

### Open Question 2
- Question: How does the τ-quantile filtering threshold in the outlier-robust depth loss need to adapt to varying noise levels across different adverse weather conditions?
- Basis in paper: [inferred] The fixed τ=0.8 threshold is selected as a hyperparameter, but lidar noise characteristics differ substantially between fog (scattering), snow (multipath), and rain (attenuation), suggesting a single threshold may be suboptimal.
- Why unresolved: The paper evaluates fixed hyperparameter settings and shows stability to scaling, but does not investigate condition-adaptive thresholding strategies.
- What evidence would resolve it: Per-condition ablation of τ values and comparison with adaptive or learned quantile selection mechanisms on condition-specific test splits.

### Open Question 3
- Question: Does projecting all modalities onto the 2D image plane discard 3D structural information that could further improve fusion, particularly for distant or small objects?
- Basis in paper: [inferred] The method projects lidar, radar, and events as 3-channel images and dilates sparse inputs, which may lose native 3D geometric structure before fusion occurs.
- Why unresolved: The paper does not compare against 3D-aware fusion baselines that preserve point cloud or volumetric representations during feature extraction