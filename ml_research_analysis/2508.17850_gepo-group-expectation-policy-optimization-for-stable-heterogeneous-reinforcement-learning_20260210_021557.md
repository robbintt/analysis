---
ver: rpa2
title: 'GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement
  Learning'
arxiv_id: '2508.17850'
source_url: https://arxiv.org/abs/2508.17850
tags:
- uni00000013
- group
- gepo
- training
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training large language models
  in decentralized, heterogeneous environments with high network latency. The authors
  propose HeteroRL, a framework that decouples rollout sampling from parameter learning,
  and introduce Group Expectation Policy Optimization (GEPO) to stabilize training
  under policy divergence.
---

# GEPO: Group Expectation Policy Optimization for Stable Heterogeneous Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.17850
- Source URL: https://arxiv.org/abs/2508.17850
- Reference count: 40
- Primary result: GEPO reduces best-to-last gap by 85% (1.8 vs 12.0) and maintains 97% performance under 1800s latency in decentralized RL for LLMs.

## Executive Summary
This paper tackles the challenge of training large language models in decentralized, heterogeneous environments with high network latency. The authors propose HeteroRL, a framework that decouples rollout sampling from parameter learning, and introduce Group Expectation Policy Optimization (GEPO) to stabilize training under policy divergence. GEPO replaces high-variance token-level importance weights with group-level expectations, exponentially reducing variance under high KL divergence. Experiments show GEPO achieves superior stability—only a 3% performance drop from online to 1800s latency—and reduces the best-to-last gap by 85% compared to GSPO (1.8 vs 12.0), while attaining the highest scores on mathematical reasoning benchmarks.

## Method Summary
The paper introduces HeteroRL, a framework for decentralized reinforcement learning that separates rollout sampling (performed by multiple sampler nodes) from parameter learning (performed by a single learner node). The core innovation is GEPO (Group Expectation Policy Optimization), which stabilizes training by replacing token-level importance weights with group-level expectations. Instead of using the ratio p/q for each token, GEPO computes an expectation over groups of tokens, reducing variance exponentially under high KL divergence. The method is evaluated on mathematical reasoning tasks using Qwen3 models, demonstrating superior stability and performance compared to baselines like GSPO when training with simulated network delays of up to 1800 seconds.

## Key Results
- GEPO achieves only 3% performance drop from online to 1800s latency training
- Best-to-last gap reduced by 85% compared to GSPO (1.8 vs 12.0)
- Highest scores on mathematical reasoning benchmarks (MATH500, AMC2023, AIME24/25)
- Maintains stability under high policy divergence with group-level importance weighting

## Why This Works (Mechanism)
GEPO addresses the instability caused by high variance in importance weights during asynchronous reinforcement learning. In decentralized training, the sampler and learner policies can diverge significantly, causing the ratio p/q (where p is the target policy and q is the behavior policy) to have extremely high variance. This variance explosion leads to training instability and poor performance. GEPO solves this by replacing token-level importance weights with group-level expectations, which exponentially reduces variance under high KL divergence. The group expectation acts as a low-variance estimator that stabilizes the policy gradient while maintaining the benefits of importance weighting.

## Foundational Learning

**Reinforcement Learning with Importance Sampling**
*Why needed:* Understanding how RL uses importance weights to correct for policy mismatch between behavior and target policies
*Quick check:* Can you explain why importance weights are needed in off-policy RL?

**KL Divergence and Policy Divergence**
*Why needed:* GEPO's variance reduction depends on high KL divergence between sampler and learner policies
*Quick check:* What happens to importance weight variance as KL divergence increases?

**Group Expectation Estimation**
*Why needed:* The core mathematical innovation that enables variance reduction
*Quick check:* Can you derive the group expectation formula from first principles?

## Architecture Onboarding

**Component Map**
Sampler Nodes (4x) -> Stale Buffer -> Learner Node (1x) -> Parameter Server

**Critical Path**
1. Sampler nodes generate trajectories using current policy
2. Trajectories stored in Stale Buffer with timestamp
3. Learner node pulls eligible data from buffer
4. Learner computes GEPO loss with group expectation weights
5. Parameters updated and broadcast back to samplers

**Design Tradeoffs**
- Decoupling sampler and learner increases latency tolerance but introduces stale data
- Group-level weighting reduces variance but adds computational overhead
- Multiple samplers improve throughput but increase synchronization complexity

**Failure Signatures**
- High variance in importance weights indicates policy divergence
- Performance degradation over training indicates instability
- Best-to-last gap increases suggest GEPO implementation issues

**Three First Experiments**
1. Implement GEPO weighting scheme and verify lower variance vs token-level weights
2. Test delayed data pipeline with sliding window to ensure eligibility
3. Validate reward signal for mathematical reasoning tasks

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Can a "defensive sampling" strategy that adaptively blends target policy probabilities into the importance weight denominator mitigate bias while maintaining GEPO's stability benefits?
- Basis: Appendix H explicitly proposes investigating this strategy as a promising direction to reduce sensitivity to high-variance estimates and bias in asynchronous settings.
- Why unresolved: While GEPO uses group expectation to reduce variance, it introduces a small bias; the proposed adaptive smoothing is hypothesized but not implemented or tested.
- Evidence: A comparative analysis in high-latency environments measuring the bias-variance trade-off and convergence speed of defensive sampling versus the standard GEPO estimator.

**Open Question 2**
- Question: Can the theoretical condition for variance reduction (DKL > log(n²+1)) be tightened to reflect practical LLM training dynamics?
- Basis: Appendix A.1 (Corollary 1) notes that while the proof requires this KL threshold, empirical data shows the actual constant Creal is much smaller, suggesting the theoretical bound is loose.
- Why unresolved: The current proof relies on worst-case bounds for the proposal distribution's probability mass, which are far more conservative than real-world long-tailed LLM distributions.
- Evidence: A refined theoretical proof or empirical characterization that closes the gap between the theoretical KL threshold and the observed variance reduction onset.

**Open Question 3**
- Question: Does GEPO's group-level importance weighting maintain its stability advantages in tasks with dense, process-based rewards or non-reasoning domains?
- Basis: Section 4.1 limits the experimental scope to mathematical reasoning tasks (MATH/AIME) which utilize sparse, outcome-based rewards.
- Why unresolved: The variance characteristics of group-level weighting may interact differently with dense reward signals (e.g., RLHF for chat) or code generation, which have distinct structural constraints.
- Evidence: Evaluation of GEPO on standard coding benchmarks (e.g., HumanEval) or RLHF alignment tasks under the HeteroRL framework to verify generalization.

## Limitations

- Critical implementation details missing: exact delay simulation mechanism, reward model definition, and log-prob recomputation procedure
- No public codebase available, making exact reproduction difficult
- Experimental scope limited to mathematical reasoning tasks, not tested on other domains
- Theoretical variance reduction bound is loose compared to empirical observations

## Confidence

- **High Confidence:** The theoretical foundation of GEPO is sound and the core formula is clearly defined
- **Medium Confidence:** The reported empirical results are internally consistent and address the paper's stated goals
- **Low Confidence:** Exact numerical results cannot be verified due to missing implementation details and lack of public codebase

## Next Checks

1. Implement GEPO weighting scheme as described in Listing 1 and verify it produces lower variance in importance weights compared to token-level (GRPO) or sequence-level (GSPO) weighting under high KL divergence conditions.

2. Implement the Stale Buffer system with simulated network delays using Log-normal distribution (bounded 60-1800s) and ensure the learner uses a sliding window of eligible data with iteration gaps never exceeding 64 steps.

3. Define and implement a binary reward function for mathematical reasoning that checks for correctly formatted and numerically correct answers within \boxed{} tokens, and verify it works stably with GEPO weighting in the GRPO loss loop.