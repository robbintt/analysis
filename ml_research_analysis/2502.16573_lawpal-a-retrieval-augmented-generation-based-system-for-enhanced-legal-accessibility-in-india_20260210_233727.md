---
ver: rpa2
title: 'LawPal : A Retrieval Augmented Generation Based System for Enhanced Legal
  Accessibility in India'
arxiv_id: '2502.16573'
source_url: https://arxiv.org/abs/2502.16573
tags:
- legal
- retrieval
- lawpal
- faiss
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces LawPal, a Retrieval-Augmented Generation
  (RAG)-based legal chatbot designed to improve legal accessibility in India. The
  system integrates DeepSeek-R1:5B for language understanding and FAISS for efficient
  vector-based document retrieval, enabling accurate and context-aware legal query
  responses.
---

# LawPal : A Retrieval Augmented Generation Based System for Enhanced Legal Accessibility in India

## Quick Facts
- arXiv ID: 2502.16573
- Source URL: https://arxiv.org/abs/2502.16573
- Authors: Dnyanesh Panchal; Aaryan Gole; Vaibhav Narute; Raunak Joshi
- Reference count: 24
- Primary result: RAG-based legal chatbot achieving >90% accuracy with 10-50ms retrieval and 800-1500ms response generation

## Executive Summary
LawPal introduces a Retrieval-Augmented Generation (RAG) system for legal query answering in India, combining DeepSeek-R1:5B for language understanding with FAISS for efficient vector-based document retrieval. The system processes legal queries by embedding them into vectors, retrieving relevant legal documents, and generating context-aware responses. LawPal demonstrates significant improvements in accuracy and efficiency over traditional rule-based legal chatbots and keyword-driven search engines, with systematic evaluation showing superior performance in handling complex and misleading legal queries while maintaining consistency and precision.

## Method Summary
LawPal employs a RAG architecture where legal documents are chunked (500-750 characters with 50-100 character overlap) and embedded using DeepSeek-R1:5B into 1,024-dimensional vectors. These vectors are indexed in FAISS with hierarchical domain categories (Criminal, Civil, Constitutional). At query time, the system embeds the query, retrieves top-k relevant chunks using cosine similarity, and passes the query plus retrieved context to DeepSeek-R1:5B for generation. The system uses prompt engineering to ensure factual accuracy and source citation.

## Key Results
- Achieved over 90% accuracy in retrieving and interpreting legal information
- Retrieval speeds of 10-50 ms and response generation times of 800-1500 ms
- Superior performance compared to traditional rule-based legal chatbots and keyword-driven search engines
- Robust handling of complex and misleading legal queries with maintained consistency and precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic vector retrieval via FAISS improves legal document relevance over keyword-based search.
- Mechanism: Legal texts are chunked, embedded into 1,024-dimensional vectors using DeepSeek-R1:5B, and indexed in FAISS. At query time, cosine similarity retrieves top-k relevant chunks. The hierarchical index groups documents by domain (Criminal, Civil, Constitutional) to improve precision.
- Core assumption: Embedding similarity correlates with legal relevance; chunk boundaries preserve sufficient context.
- Evidence anchors:
  - [abstract] "FAISS for rapid vector-based search, significantly improving retrieval speed and accuracy."
  - [section III.C] "The FAISS index is structured hierarchically, grouping legal topics into categories such as criminal law, contract law, and constitutional law."
  - [corpus] Neighbor paper "ASVRI-Legal" similarly reports RAG benefits for legal regulation tasks, but no direct replication of LawPal's specific architecture exists.
- Break condition: If chunks span article boundaries or omit definitions, retrieved context may be incomplete; embedding drift over time degrades retrieval quality.

### Mechanism 2
- Claim: RAG reduces hallucination by grounding generation in retrieved legal documents.
- Mechanism: Retrieved chunks R are concatenated with query Q and passed to DeepSeek-R1:5B: A = G(Q, R). Prompt engineering constrains the model to cite retrieved sources and refuse speculation beyond the context.
- Core assumption: The generative model respects context constraints and does not override retrieved facts with parametric knowledge.
- Evidence anchors:
  - [abstract] "trained using an extensive dataset comprising legal books, official documentation and the Indian Constitution, ensuring accurate responses."
  - [section III.B] "The model is prompt-engineered to maintain factual accuracy, ensuring that responses are aligned with constitutional laws, statutory provisions, and legal precedents."
  - [corpus] Neighbor "Bridging Legal Knowledge and AI" discusses RAG+KG for legal AI but focuses on knowledge graphs; does not validate this specific prompting strategy.
- Break condition: If prompts are underspecified or model temperature is too high, generated answers may fabricate citations or blend jurisdictions.

### Mechanism 3
- Claim: Chunk overlap and controlled segment size preserve legal argument continuity.
- Mechanism: RecursiveCharacterTextSplitter creates 500-750 character chunks with 50-100 character overlap. This maintains logical continuity across chunk boundaries, especially for multi-paragraph legal provisions.
- Core assumption: Legal meaning is not disrupted by mid-sentence splits; overlap captures cross-boundary references.
- Evidence anchors:
  - [section I] "segmented into smaller chunks using LangChain's RecursiveCharacterTextSplitter to maintain logical continuity."
  - [section III.C] "text is segmented into 500-750 character chunks with an overlap of 50-100 characters to maintain contextual integrity."
  - [corpus] No corpus paper directly validates this chunking configuration for Indian legal text.
- Break condition: Long statutory definitions or multi-section provisions may still fragment; overlap may introduce noise if not deduplicated.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: LawPal's core architecture combines retrieval with generation; understanding RAG is prerequisite to debugging retrieval-quality issues.
  - Quick check question: Given a query and two retrieved chunks, can you trace which chunk most influenced the final answer?

- Concept: **Vector Similarity Search (FAISS)**
  - Why needed here: Retrieval speed and accuracy depend on FAISS indexing; engineers must understand IVF, PQ, and HNSW tradeoffs.
  - Quick check question: What is the difference between exact search and approximate search in FAISS, and when does recall drop?

- Concept: **Legal Prompt Engineering**
  - Why needed here: The model must refuse speculative answers and cite sources; prompt structure directly affects accuracy.
  - Quick check question: How would you modify the system prompt to enforce citation of retrieved sections only?

## Architecture Onboarding

- Component map: Data Ingestion -> RecursiveCharacterTextSplitter (500-750 chars, 50-100 overlap) -> DeepSeek-R1:5B embeddings (1,024-dim) -> FAISS indexing (hierarchical domain categories) -> Query embedding -> FAISS cosine similarity retrieval -> DeepSeek-R1:5B generation -> Streamlit frontend

- Critical path: Query → embedding → FAISS retrieval → context assembly → generation → display. Latency budget: 10-50ms retrieval + 800-1500ms generation.

- Design tradeoffs:
  - FAISS vs Chroma: Paper claims FAISS provides better recall and GPU acceleration; Chroma offers stability on large retrievals but lower recall (Section IV.B).
  - Chunk size: Smaller chunks improve retrieval precision but may lose context; overlap mitigates but increases storage.

- Failure signatures:
  - Multi-jurisdictional queries return mixed or incorrect references (Section V).
  - Ambiguous legal provisions yield inconsistent answers (>5% variation due to generative rephrasing, Section IV.C).
  - Long-context arguments exceed chunk boundaries, leading to incomplete synthesis.

- First 3 experiments:
  1. Retrieval quality baseline: Measure Precision@K, MRR, NDCG on a held-out set of 100 legal queries with known relevant documents.
  2. Chunk size ablation: Compare 300-char vs 500-char vs 750-char chunks on answer completeness for multi-section queries.
  3. Adversarial robustness test: Submit misleading or ambiguous queries (e.g., "Is Section 420 applicable to civil disputes?") and verify refusal or clarification behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications are required to implement dynamic, location-based legal filtering to address multi-jurisdictional query limitations?
- Basis in paper: [explicit] The authors note that "limitations persist in handling multi-jurisdictional queries" and identify "jurisdictional adaptability, ensuring location-based legal filtering" as a specific target for future improvements.
- Why unresolved: The current system relies on hierarchical indexing by domain (e.g., Criminal vs. Civil) but lacks the mechanisms to disambiguate or apply laws based on specific regional jurisdictions within India.
- What evidence would resolve it: A demonstrated ability to correctly apply state-specific statutes (e.g., land laws) versus central laws based on user context or explicit location tags.

### Open Question 2
- Question: How can the system be extended to support regional Indian languages while maintaining high retrieval accuracy with limited legal corpora?
- Basis in paper: [explicit] The user testing section highlights that "multilingual support is a key area for future improvement" and was a specific request from users during validation.
- Why unresolved: The current model (DeepSeek-R1:5B) and dataset are primarily English-centric, and the paper does not address how to handle the scarcity of digitized legal resources in regional languages.
- What evidence would resolve it: Successful implementation of retrieval and generation for languages such as Hindi or Marathi, validated by native-speaking legal experts.

### Open Question 3
- Question: Can memory-based retrieval techniques or advanced chunking strategies improve the model's ability to synthesize long-context legal arguments?
- Basis in paper: [explicit] The conclusion lists "long-context arguments" as a current limitation and proposes "enhancing long-context understanding" to better synthesize interconnected legal provisions.
- Why unresolved: The current methodology uses fixed-size chunks (500-750 characters) which may fragment complex legal logic that spans multiple pages of a judgment.
- What evidence would resolve it: Comparative tests showing improved accuracy on complex queries requiring the synthesis of full case histories versus the current segmented approach.

## Limitations

- Single-point dependency on DeepSeek-R1:5B for both embedding and generation creates risk of compounded errors
- Chunk-based retrieval may fragment complex legal arguments spanning multiple sections
- Multi-jurisdictional query handling remains limited without location-based filtering

## Confidence

- **Medium** confidence in 90% accuracy claim due to absence of detailed validation methodology
- **Low** confidence in vector similarity mechanism without explicit precision evaluation across legal domains
- **Medium** confidence in adversarial query handling claims, but no systematic testing methodology described

## Next Checks

1. Conduct a comprehensive precision@K evaluation using a blind test set of 200 legal queries with expert-verified relevance judgments across all legal domains (Criminal, Civil, Constitutional)
2. Perform an ablation study comparing DeepSeek-R1:5B embeddings against established legal-specific embedding models (e.g., Legal-BERT) to quantify any performance degradation
3. Implement and test a comprehensive adversarial query suite designed to probe the system's handling of jurisdictional ambiguities, temporal changes in law, and intentionally misleading prompts