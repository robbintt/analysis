---
ver: rpa2
title: 'eSapiens''s DEREK Module: Deep Extraction & Reasoning Engine for Knowledge
  with LLMs'
arxiv_id: '2507.15863'
source_url: https://arxiv.org/abs/2507.15863
tags:
- esapiens
- module
- recall
- retrieval
- pipeline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The DEREK module addresses the challenge of accurate, auditable
  enterprise document QA by combining hybrid vector+BM25 retrieval, LLM-based query
  refinement, reranking, and a LangGraph verifier that enforces citation overlap.
  The pipeline ingests heterogeneous content into 1,000-token overlapping chunks,
  indexes them in a hybrid HNSW+BM25 store, and regenerates answers until every claim
  is grounded in retrieved text.
---

# eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs

## Quick Facts
- arXiv ID: 2507.15863
- Source URL: https://arxiv.org/abs/2507.15863
- Authors: Isaac Shi; Zeyuan Li; Fan Liu; Wenli Wang; Lewei He; Yang Yang; Tianyu Shi
- Reference count: 6
- Primary result: DEREK achieves TRACe Utilization >0.50 and <3 % unsupported statements on enterprise QA, improving recall by 1 pp and precision by 7 pp over baselines

## Executive Summary
DEREK (Deep Extraction & Reasoning Engine for Knowledge) is a hybrid retrieval and LLM-based QA module designed for enterprise document processing requiring auditable, context-faithful answers with citation grounding. It combines 1,000-token overlapping chunking, hybrid vector+BM25 retrieval, GPT-4o query refinement, Cohere reranking, CO-STAR prompt engineering, and a LangGraph verifier that enforces sentence-level citation overlap. On four LegalBench subsets, DEREK improves Recall@50 by ~1 pp and Precision@10 by ~7 pp; the verifier achieves TRACe Utilization >0.50 and limits unsupported statements to <3 %. A VC due diligence case study reports 50 % manual effort reduction and 20-30 % decision confidence improvement through traceable AI-backed insights.

## Method Summary
DEREK processes heterogeneous documents (PDF, Office, web) by first splitting them into 1,000-token chunks with 150-token overlap, then embedding with OpenAI text-embedding-3-large. Documents are indexed in Elasticsearch 8.x using a hybrid HNSW+BM25 setup. Query refinement via GPT-4o expands synonyms and resolves ambiguity before retrieving 200 candidates using combined vector+BM25 scores, then reranking with Cohere rerank-english-v3 to top-50. Answers are generated using a CO-STAR prompt template and passed through a LangGraph verifier loop that checks each claim's citation overlap, regenerating until all claims are grounded or a fallback is triggered.

## Key Results
- 1,000-token chunking improves Recall@50 by about 1 pp on LegalBench subsets
- Hybrid+rerank boosts Precision@10 by about 7 pp over baseline retrieval
- Verifier raises TRACe Utilization above 0.50 and limits unsupported statements to under 3 %
- VC due diligence case study: insight generation cut from weeks to hours, manual effort reduced by over 50 %, decision confidence improved by 20-30 %

## Why This Works (Mechanism)
DEREK's accuracy gains stem from multi-stage error reduction: query refinement mitigates ambiguity, hybrid retrieval balances semantic and lexical matching, reranking prioritizes relevance, and the verifier loop enforces strict citation grounding. The 1,000-token chunk size captures sufficient context for legal and contractual language while the 150-token overlap ensures continuity across boundaries. The verifier's sentence-level overlap check prevents hallucination by requiring every claim to be traceable to retrieved text, with regeneration loops ensuring compliance.

## Foundational Learning
- **Hybrid retrieval (vector+BM25)**: Combines semantic matching (vectors) with exact term matching (BM25) to balance recall and precision across varied query types
  - Why needed: Legal and contractual language often contains rare terms that pure semantic search misses
  - Quick check: Compare retrieval results on a query mixing common and rare terms with/without BM25
- **Query refinement via LLM**: Uses GPT-4o to expand synonyms and resolve ambiguities before retrieval
  - Why needed: Raw queries may be too narrow or ambiguous for effective retrieval
  - Quick check: Run same query with/without refinement and measure recall@10
- **Citation overlap verification**: LangGraph verifier checks that each sentence in the answer overlaps with retrieved snippets
  - Why needed: Enforces traceability and prevents hallucination in high-stakes domains
  - Quick check: Manually inspect verifier output on edge cases (fragmented snippets, partial matches)
- **CO-STAR prompt engineering**: Context, Objective, Style, Tone, Audience, Response template guides LLM generation
  - Why needed: Ensures consistent, domain-appropriate answers
  - Quick check: Generate answers with/without CO-STAR and compare coherence and grounding
- **Iterative regeneration loop**: Answer is regenerated until all claims are grounded or max iterations reached
  - Why needed: Guarantees citation compliance even if initial generation fails
  - Quick check: Log iteration counts and grounding scores per query

## Architecture Onboarding

**Component map:**
Document Ingestion -> 1,000-token Chunker (150 overlap) -> Hybrid Index (HNSW+BM25) -> Query Refinement (GPT-4o) -> Retrieval (200 candidates) -> Rerank (Cohere, top-50) -> CO-STAR Generation -> LangGraph Verifier -> (Regenerate if needed) -> Final Answer

**Critical path:**
Query → Refinement → Hybrid Retrieval → Rerank → Generation → Verifier Loop → Grounded Answer

**Design tradeoffs:**
- 1,000 tokens balances context richness vs. index size and retrieval latency
- 150-token overlap prevents context loss at chunk boundaries but increases storage
- 200 candidate retrieval ensures recall but increases rerank computation
- Verifier loop guarantees grounding but risks infinite regeneration (handled by max iterations)

**Failure signatures:**
- Verifier loop not converging: infinite regeneration, citation overlap scores plateauing
- Low Recall@50: fragmented documents, insufficient chunk size for domain context
- High hallucination despite verifier: strict-grounding mode disabled, verifier threshold too lenient

**Three first experiments:**
1. Test chunk size impact: Compare Recall@50 using 500 vs 1,000 tokens on a heterogeneous corpus
2. Validate hybrid gains: Measure Precision@10 with vector-only, BM25-only, and hybrid retrieval
3. Verify grounding enforcement: Run verifier on known hallucinated outputs and confirm rejection

## Open Questions the Paper Calls Out
None

## Limitations
- Recall@50 gain from 1,000-token chunking measured only on LegalBench, not heterogeneous enterprise data
- 7 pp Precision@10 improvement assumes fixed hyperparameters (200 candidates, top-50) that may not generalize
- TRACe metrics based on 100 RAGtruth questions, potentially unrepresentative of full enterprise variability
- VC case study lacks quantitative baselines and control groups, making effort and confidence claims hard to verify

## Confidence
- **High confidence**: Core architectural design (hybrid retrieval + LLM query refinement + verifier loop) and its intended purpose for auditable enterprise QA
- **Medium confidence**: Quantitative improvements on LegalBench subsets (given controlled evaluation but limited domain scope)
- **Low confidence**: Case study metrics and real-world impact claims (single anecdote, no comparative data)

## Next Checks
1. Benchmark DEREK's chunking strategy (500 vs 1,000 tokens) on a heterogeneous enterprise corpus with varying document structures to confirm generalizability beyond LegalBench
2. Measure TRACe Utilization and unsupported statement rates on a held-out test set of 500+ enterprise documents to validate statistical significance of claimed <3 % hallucination rate
3. Conduct a controlled experiment comparing DEREK vs standard RAG on the same VC due diligence dataset, measuring both accuracy and human time savings to verify the 50 % effort reduction claim