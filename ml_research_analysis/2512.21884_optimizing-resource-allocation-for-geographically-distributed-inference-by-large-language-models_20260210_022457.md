---
ver: rpa2
title: Optimizing Resource Allocation for Geographically-Distributed Inference by
  Large Language Models
arxiv_id: '2512.21884'
source_url: https://arxiv.org/abs/2512.21884
tags:
- time
- inference
- request
- server
- requests
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first systematic study of optimal resource
  allocation for distributed large language model (LLM) inference. The key problem
  is how to optimally place model blocks across geographically distributed servers
  and route inference requests to minimize average per-token inference time, subject
  to GPU memory constraints.
---

# Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models

## Quick Facts
- **arXiv ID:** 2512.21884
- **Source URL:** https://arxiv.org/abs/2512.21884
- **Reference count:** 40
- **Primary result:** 60-80% reduction in average per-token inference time compared to state-of-the-art

## Executive Summary
This paper presents the first systematic study of optimal resource allocation for distributed large language model (LLM) inference. The authors tackle the problem of placing model blocks across geographically distributed servers and routing inference requests to minimize average per-token inference time while respecting GPU memory constraints. They develop a mixed integer linear programming formulation, prove its NP-hardness, and propose a three-step polynomial-time algorithm with guaranteed performance. The solution is validated through extensive experiments showing 60-80% improvement over the state-of-the-art, primarily by optimizing GPU memory allocation between model blocks and attention caches. The authors also provide a lightweight CPU-only simulator for researchers with limited GPU access.

## Method Summary
The paper formulates the joint block placement and request routing problem as a mixed integer linear program (MILP) and proves its NP-hardness. To solve this efficiently, they propose a three-step polynomial-time algorithm: conservative block placement (reserving GPU memory for both model blocks and attention caches), greedy block placement (assigning consecutive blocks to servers based on amortized inference time), and shortest-path request routing. For online settings with dynamic request arrivals, they adapt the offline algorithm into a two-time-scale solution using robust optimization for block placement and waiting-penalized shortest-path routing for request scheduling. The approach is validated through experiments on PETALS and a lightweight CPU-only simulator.

## Key Results
- 60-80% reduction in average per-token inference time compared to PETALS baseline
- Conservative memory reservation prevents GPU memory exhaustion during concurrent inference
- Algorithm scales to real-world Internet Topology Zoo datasets (AboveNet, BellCanada)
- Lightweight CPU-only simulator enables performance evaluation without GPU resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conservative block reservation prevents GPU memory exhaustion during concurrent inference
- Mechanism: The algorithm reserves GPU memory for both model blocks and attention caches upfront. For each server j, it computes `m_j = min(⌊M_j/(s_m + s_c|R|)⌋, L)`, where s_m is block size, s_c is cache size per request, and |R| is the target concurrent request count. This guarantees that even if all requests route through a single server, memory constraints are satisfied.
- Core assumption: Request arrivals are bounded by the design parameter |R|; if exceeded, requests must wait rather than cause OOM failures.
- Evidence anchors:
  - [Section 3.2.3]: "storing min(⌊M_j/(s_m + s_c|R|)⌋, L) blocks on server j will guarantee the maximum memory consumption to be feasible"
  - [Section 4.2.1]: "PETALS places 53 blocks on A100 and 4 blocks on MIG, whereas our algorithm (CG-BP) only places 41 blocks on A100 and 3 blocks on MIG" — the paper attributes the 60-80% reduction primarily to this memory allocation difference
  - [corpus]: Weak direct evidence; DILEMMA discusses edge inference but with quantization focus, not block placement

### Mechanism 2
- Claim: Greedy block placement by amortized inference time minimizes average per-token latency
- Mechanism: Servers are sorted by `t̃_j = τ_j + t*_j/m_j` (amortized inference time per block). Each server j selects consecutive blocks {a_j, ..., a_j + m_j - 1} that maximize total "need of service" measured by ΣT_b, where T_b tracks total amortized inference time for block b. Faster servers cover earlier blocks, creating a sequential chain.
- Core assumption: The relaxed routing approximation (block-by-block routing with per-block cost) sufficiently approximates real request routing constraints.
- Evidence anchors:
  - [Section 3.2.3, Lemma 3.3]: "The block placement in lines 2–8 of Alg. 1 minimizes the average per-token inference time over all the requests under the relaxed request routing"
  - [Section 3.2.3, Theorem 3.5]: Provides bounded performance guarantee with upper bound equation (17)
  - [corpus]: HALoS paper uses similar hierarchical ordering for geo-distributed training, but for different objective

### Mechanism 3
- Claim: Waiting-penalized shortest-path routing handles dynamic request arrivals
- Mechanism: For each new request at time t, the algorithm computes link cost as `t^W_{ij}(t) + l_max * t^c_{ij}`, where t^W_{ij}(t) is the waiting time until server j has sufficient memory. This reduces to standard shortest-path when concurrent requests ≤ |R| (no waiting), and gracefully handles overload by penalizing congested paths.
- Core assumption: The maximum operator in true completion time can be upper-bounded by the sum approximation without severe overestimation.
- Evidence anchors:
  - [Section 3.3.2, Corollary 3.7]: "the cost of this path Σ_{(i,j)∈p_c(t)} (t^W_{ij}(t) + l_max*t^c_{ij}) is an upper bound on the completion time"
  - [Section 3.3.2]: "if the number of concurrent requests at time t is within |R|, then p_c(t) is optimal under the given block placement"
  - [corpus]: EPARA addresses edge inference parallelization but uses different scheduling approach

## Foundational Learning

- Concept: **Pipeline parallelism for transformer models**
  - Why needed here: The entire system architecture assumes blocks can be processed sequentially across servers. Without understanding that transformer blocks have identical structure and can be split at layer boundaries, the placement problem formulation makes no sense.
  - Quick check question: Given a 70-block BLOOM-176B model, why can server A process blocks 1-20 while server B processes blocks 21-35, but server A cannot process blocks 1, 5, 10, 15?

- Concept: **Mixed Integer Linear Programming (MILP)**
  - Why needed here: The paper formulates BPRR as MILP (equation 13) and proves NP-hardness. Understanding why auxiliary variables (α, β, γ, δ) are needed to linearize bilinear terms is essential for implementing or modifying the formulation.
  - Quick check question: Why does the objective function in (13a) use auxiliary variables instead of directly using a_j * f^r_{ij}?

- Concept: **Attention cache (KV cache) memory dynamics**
  - Why needed here: The core insight is that GPU memory must be split between static model blocks and dynamic per-request caches. Equation (2) shows `s_m*m_j + s_c*Σk^r_j` — misunderstanding this leads to OOM errors or suboptimal placement.
  - Quick check question: If s_m = 2GB per block, s_c = 10MB per request-block, and M_j = 40GB, how many blocks can server j store if it must handle 100 concurrent requests?

## Architecture Onboarding

- **Component map:**
  - Input parameters: Server set V_s with memory M_j, processing time τ_j, RTT t_{cj}; client set V_c; LLM with L blocks, block size s_m, cache size s_c
  - Logical topology G = (V, E): S-clients (sources), D-clients (destinations), servers; edges represent feasible routing links
  - CG-BP module: Computes (a, m) placement offline
  - WS-RR module: Computes per-request routing online using current system state
  - State tracker: Maintains (T^j_r(t), M^j_r(t)) for each server j and active request r

- **Critical path:**
  1. Profile servers to obtain τ_j, t_{cj} (benchmark with representative input/output lengths)
  2. Set design parameter |R| based on expected load (equation 19 provides upper bound)
  3. Run CG-BP once (or periodically if demand shifts significantly)
  4. For each incoming request, run WS-RR with current system state

- **Design tradeoffs:**
  - **Higher |R|**: More throughput capacity, but fewer blocks per server → longer routing paths → higher per-token latency
  - **Conservative vs aggressive block placement**: Conservative guarantees feasibility but may underutilize memory; aggressive placement risks OOM
  - **Rerouting frequency**: Re-running block placement adapts to demand changes but has high overhead (loading blocks takes seconds)

- **Failure signatures:**
  - **Memory exhaustion during prefill**: CG-BP parameter |R| was set too low for actual load
  - **Excessive waiting times**: Check if request rate exceeded design capacity; verify state tracker is correctly aging out completed requests
  - **Suboptimal routing despite placement**: Check if logical topology G is missing feasible edges due to incorrect block placement data

- **First 3 experiments:**
  1. **Validate performance models locally**: Deploy single server with varying blocks (10-50) and concurrent requests (1-30); plot inference time vs blocks to verify linearity assumption in equation (1); compare against Fig. 2
  2. **Test CG-BP in isolation**: Create synthetic cluster with 5 servers of heterogeneous memory; run CG-BP and verify: (a) all blocks placed, (b) memory constraints satisfied, (c) faster servers get earlier blocks
  3. **End-to-end comparison with PETALS baseline**: Replicate clustered scenario from Table 2; measure average per-token inference time with 100 requests at 0.5 req/s; target 60%+ reduction; if not achieved, profile memory allocation (blocks vs caches) to identify bottleneck

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the optimization framework be extended to minimize operational costs (e.g., GPU rental) or maximize robustness in the face of unreliable nodes?
  - Basis in paper: [explicit] Section 2.3, Remark 1 states: "Other performance measures, e.g., cost of renting GPUs or robustness in the face of unreliable nodes, are left for future work."
  - Why unresolved: The current MILP formulation and the CG-BPRR algorithm are designed exclusively to minimize average inference time under the assumption of static server availability.
  - What evidence would resolve it: A modified optimization formulation that includes cost constraints or failure probabilities, accompanied by simulations evaluating the trade-off between inference time, cost, and reliability.

- **Open Question 2:** Can the greedy block placement and myopic request scheduling strategies be refined to close the performance gap with the global optimum?
  - Basis in paper: [explicit] Section 4.3 notes "the suboptimality of the greedy block placement and myopic request scheduling strategy, which leaves potential room for improvement in future work."
  - Why unresolved: The proposed CG-BPRR algorithm relies on heuristics (greedy placement, shortest-path routing) to solve the NP-hard problem in polynomial time, which sacrifices optimality.
  - What evidence would resolve it: The development of an algorithm with a tighter approximation ratio or empirical results that consistently approach the lower bounds defined in Appendix B.4 without increasing complexity prohibitively.

- **Open Question 3:** How can the resource allocation model be adapted for tensor parallelism in geographically-distributed settings?
  - Basis in paper: [explicit] Section 1.1 notes that while the solution applies to pipeline parallelism, "the extension of our solution to tensor parallelism is nontrivial."
  - Why unresolved: Tensor parallelism requires massive all-to-all communication of activations between devices for each layer, which breaks the "chain of servers" assumption used in the current flow-based routing model.
  - What evidence would resolve it: A formulation that accounts for all-to-all communication latency and bandwidth constraints in a geo-distributed wide-area network (WAN) environment.

- **Open Question 4:** How does the algorithm perform when the workload is dominated by long input prompts (prefill phase) rather than long outputs (decoding phase)?
  - Basis in paper: [inferred] Section 2.1 explicitly scopes the work to "short-prompt long-response queries" ($l^I_{max} \ll l_{max}$), and experiments in Section 4.1 use a fixed small input length ($l^I_{max} = 20$).
  - Why unresolved: The algorithm optimizes memory allocation for attention caches (which grow with sequence length) and weights costs based on the decoding phase; massive prefill inputs could saturate network bandwidth differently.
  - What evidence would resolve it: Experimental evaluation using workloads where input length is significantly larger than output length to verify if the current routing and placement logic remains optimal.

## Limitations

- **Conservative parameter selection**: The choice of design parameter $|R|$ significantly impacts solution quality but lacks thorough sensitivity analysis
- **Approximation validity**: The relaxed routing formulation may break down for servers with heterogeneous RTT distributions across clients
- **Offline algorithm brittleness**: CG-BP is designed for static environments and doesn't quantify the overhead of re-running when demand shifts

## Confidence

- **High confidence**: Mechanism 1 (memory allocation guarantee) - The conservative reservation approach has clear mathematical bounds and direct experimental validation through the PETALS comparison showing 60-80% improvement
- **Medium confidence**: Mechanism 2 (greedy placement heuristic) - While Lemma 3.3 and Theorem 3.5 provide theoretical guarantees, these rely on the relaxed routing approximation and may not hold under all traffic patterns
- **Medium confidence**: Mechanism 3 (online routing) - Corollary 3.7 establishes theoretical bounds, but the greedy nature of WS-RR may lead to suboptimal equilibria under high load

## Next Checks

1. **Sensitivity analysis on $|R|$ parameter**: Run the full system with varying $|R|$ values (e.g., 10, 20, 30, 40) on the synthetic cluster testbed. Measure average per-token inference time and maximum waiting time. Identify the optimal tradeoff point where memory utilization is high but waiting times remain acceptable.

2. **Stress test with bursty arrivals**: Modify the Poisson arrival model to include bursty traffic (e.g., periods of high intensity followed by quiet periods). Measure system behavior during overload conditions - specifically, how well WS-RR handles temporary violations of the $|R|$ constraint and whether request queuing/prioritization strategies are needed.

3. **Cross-model validation**: Apply the algorithm to a different LLM architecture (e.g., LLaMA or GPT-2) with different block sizes and memory characteristics. Validate whether the same performance improvements hold and whether the conservative reservation mechanism scales appropriately to different model dimensions.