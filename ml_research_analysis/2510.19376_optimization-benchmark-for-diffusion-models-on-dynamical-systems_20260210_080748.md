---
ver: rpa2
title: Optimization Benchmark for Diffusion Models on Dynamical Systems
arxiv_id: '2510.19376'
source_url: https://arxiv.org/abs/2510.19376
tags:
- loss
- training
- learning
- rate
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks optimization algorithms for training diffusion
  models on dynamical system data, specifically for denoising Navier-Stokes flow trajectories.
  It evaluates Muon, SOAP, ScheduleFree, and AdamW, finding that Muon and SOAP achieve
  18% lower final loss than AdamW despite higher per-step runtime.
---

# Optimization Benchmark for Diffusion Models on Dynamical Systems

## Quick Facts
- **arXiv ID:** 2510.19376
- **Source URL:** https://arxiv.org/abs/2510.19376
- **Reference count:** 40
- **Primary result:** Muon and SOAP achieve 18% lower final loss than AdamW for diffusion model training on Navier-Stokes data

## Executive Summary
This paper benchmarks optimization algorithms for training diffusion models on dynamical system data, specifically for denoising Navier-Stokes flow trajectories. It evaluates Muon, SOAP, ScheduleFree, and AdamW, finding that Muon and SOAP achieve 18% lower final loss than AdamW despite higher per-step runtime. ScheduleFree matches AdamW's loss but shows inferior generative quality, likely due to lack of learning-rate annealing. The study also shows AdamW outperforms SGD in this task, unlike in language modeling where class imbalance explains the gap. A new sqrt learning-rate schedule is proposed as an anytime alternative to wsd and cosine. Key takeaway: Muon and SOAP are efficient alternatives to AdamW for diffusion model training, and the entire training trajectory impacts final model quality beyond just the loss value.

## Method Summary
The paper benchmarks optimization algorithms for training a score-based diffusion model on Navier-Stokes flow trajectories. The model is a U-Net architecture (22.9M parameters) trained to denoise 64×64 velocity field snapshots from 1024 trajectories. Five optimizers are compared: AdamW (baseline), Muon (spectral preconditioning), SOAP (Shampoo+Adam hybrid), ScheduleFree (constant learning rate), and Prodigy. Hyperparameters are swept across learning rates (10⁻⁴ to 10⁻³) and weight decays (10⁻³ to 10⁻²) with 3 random seeds. The primary metric is final validation loss, with visual inspection of generated vorticity fields as secondary evaluation. A new sqrt learning-rate schedule is proposed as an anytime alternative to existing schedules.

## Key Results
- Muon and SOAP achieve 18% lower final loss than AdamW (0.0082 vs 0.0100)
- ScheduleFree matches AdamW's loss but produces inferior generative quality
- AdamW outperforms SGD despite no class imbalance in this task
- The proposed sqrt schedule performs similarly to wsd while being anytime
- Muon and SOAP have 1.45-1.72× higher per-step runtime than AdamW

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Muon and SOAP achieve better convergence efficiency than AdamW for diffusion model training on dynamical systems.
- **Mechanism:** Muon performs approximate steepest descent in the spectral norm by orthogonalizing gradient matrices via Newton-Schulz iteration (for 2D weight matrices). SOAP combines Shampoo's dense preconditioning with Adam's adaptive element-wise updates. Both methods provide curvature information that AdamW's diagonal approximation cannot capture, which appears particularly beneficial for the U-Net architecture processing spatiotemporal flow data.
- **Core assumption:** The spectral structure of gradients in convolutional layers processing turbulent flow data contains exploitable geometric structure that diagonal preconditioning misses.
- **Evidence anchors:**
  - [abstract] "Muon and SOAP are highly efficient alternatives to AdamW (18% lower final loss)"
  - [section 3.1] "Muon performs approximately steepest descent in the spectral norm... SOAP combines techniques from Shampoo and Adam"
  - [corpus] Weak direct evidence—neighbor papers focus on diffusion applications rather than optimization mechanics. PAD-TRO (arXiv:2510.04436) discusses diffusion for trajectory optimization but doesn't compare optimizers.

### Mechanism 2
- **Claim:** The entire training trajectory—not just final loss—impacts generative quality of diffusion models, with learning-rate annealing playing a critical role.
- **Mechanism:** Schedule-free methods maintain constant learning rates, preventing the "cooldown" phase that appears necessary for the model to refine its score function estimation. The wsd schedule shows similar degradation despite low final loss. Assumption: Learning rate decay allows the model to settle into sharper minima that produce better denoising trajectories, even when validation loss appears similar.
- **Core assumption:** Diffusion model quality depends on optimization trajectory properties (e.g., final learning rate magnitude) beyond what the loss surface alone captures.
- **Evidence anchors:**
  - [abstract] "ScheduleFree matches AdamW's loss but shows inferior generative quality, likely due to lack of learning-rate annealing"
  - [section 4] "We conjecture that the entire training trajectory might impact the final model quality"
  - [corpus] No direct corpus support for this specific finding—this appears to be a novel observation.

### Mechanism 3
- **Claim:** AdamW outperforms SGD for diffusion model training through mechanisms beyond class imbalance.
- **Mechanism:** The Adam-SGD gap in language modeling has been attributed to heavy-tailed class distributions (Kunstner et al., 2024). This diffusion task has no class labels, yet the gap persists. Assumption: Architectural factors (U-Net convolutions, time embeddings, or multi-scale feature interactions) create optimization landscapes where adaptive methods outperform momentum-SGD, possibly due to heterogeneous gradient magnitudes across layers.
- **Core assumption:** The U-Net architecture with time embeddings creates layer-wise gradient heterogeneity that benefits from per-parameter adaptive learning rates.
- **Evidence anchors:**
  - [section 3.3] "This leads us to the conclusion that for this problem instance other factors (e.g. architecture properties) must be at play"
  - [section 3.3] "The explanation that class imbalance causes the gap between AdamW and SGD can not be applied here"
  - [corpus] Indirect support: Stochastic closure modeling paper (arXiv:2506.20771) uses similar U-Net architectures for dynamical systems but doesn't compare optimizers.

## Foundational Learning

- **Concept:** **Denoising Diffusion Probabilistic Models (DDPM)**
  - **Why needed here:** The entire benchmark trains a U-Net to learn a score function by denoising Navier-Stokes trajectories. Without understanding that the model predicts noise at each timestep, you cannot interpret loss curves or generated sample quality.
  - **Quick check question:** Can you explain why the model trains on noisy samples and how the learned score function enables trajectory generation?

- **Concept:** **Preconditioned Stochastic Optimization**
  - **Why needed here:** Muon (spectral preconditioning) and SOAP (Shampoo + Adam) differ fundamentally from AdamW's diagonal preconditioning. Understanding why preconditioning helps requires knowing how curvature estimation affects convergence in non-convex landscapes.
  - **Quick check question:** What structural information does Muon's spectral orthogonalization provide that AdamW's element-wise adaptive learning rates cannot capture?

- **Concept:** **Learning Rate Schedules and Training Dynamics**
  - **Why needed here:** The paper shows wsd and cosine schedules achieve similar losses but different generative qualities. Understanding why cooldown phases matter requires connecting schedule design to optimization trajectory properties.
  - **Quick check question:** Why might a constant learning rate (ScheduleFree) achieve low loss but produce worse generated samples than a schedule with annealing?

## Architecture Onboarding

- **Component map:**
  Input: (batch, 10, 64, 64) velocity field snapshots
          ↓
  U-Net Encoder: 3 conv blocks (96→192→384 channels)
          ↓
  Time Embedding: 64-dim sinusoidal → injected at each layer
          ↓
  U-Net Decoder: symmetric upscaling with skip connections
          ↓
  Output: Predicted noise at sampled diffusion timestep

- **Critical path:**
  1. Data pipeline: Navier-Stokes simulation → 1024 trajectories → 64×64 downsampled windows
  2. Diffusion forward process: Add noise at random timesteps
  3. U-Net training: Predict noise component (score matching)
  4. Hyperparameter sweep: Learning rate (10⁻⁴ to 10⁻³) × weight decay (10⁻³ to 10⁻²) × 3 seeds
  5. Evaluation: Validation loss + visual inspection of generated vorticity fields

- **Design tradeoffs:**
  - **Muon/SOAP vs AdamW:** 1.45–1.72× slower per step but 18% lower final loss. Choose based on whether compute budget or model quality is constrained.
  - **Schedule-free vs scheduled:** ScheduleFree eliminates tuning of schedule length but risks quality degradation. The proposed sqrt schedule offers a middle ground.
  - **SGD vs AdamW:** SGD fails to close gap despite extensive tuning (Fig 5). Not worth pursuing for this architecture.

- **Failure signatures:**
  - **Low loss, poor generation:** ScheduleFree shows this pattern (Fig 11)—loss ~0.010 but blurry/incoherent vorticity fields.
  - **Schedule mismatch:** wsd achieves minimal loss at lr=2e-4 but generates poor samples (Fig 10a). Indicates loss-quality decoupling.
  - **SGD stagnation:** Validation loss plateaus ~0.03 vs AdamW's ~0.015 (Fig 5), with visibly worse trajectory structure.

- **First 3 experiments:**
  1. **Baseline replication:** Train AdamW with cosine schedule, lr=4.5e-4, wd=1e-2 for 1024 epochs. Verify final loss ~0.015 and visually inspect generated vorticity fields against Fig 8a.
  2. **Optimizer comparison:** Run Muon (lr=1e-3, wd=3.2e-2) and SOAP (lr=1e-3, wd=3.2e-2) for same epochs. Confirm ~18% loss reduction and compare per-epoch runtime overhead (expect 1.45× and 1.72× respectively).
  3. **Schedule ablation:** Train AdamW with wsd schedule at lr=2e-4 and sqrt schedule at lr=2e-3 (both with 20% cooldown). Compare final loss and visual quality to diagnose loss-quality mismatch.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the entire training trajectory determine the final generative quality of diffusion models, independent of the final loss value?
- Basis in paper: [explicit] The authors conclude that "the entire training trajectory might impact the final model quality" and explicitly leave this conjecture open for future work.
- Why unresolved: ScheduleFree achieved loss values comparable to AdamW but produced inferior generative quality, suggesting final loss is an insufficient proxy for model performance.
- What evidence would resolve it: A theoretical framework or empirical metric correlating the path taken during optimization (not just the final checkpoint) with downstream generative fidelity.

### Open Question 2
- Question: What specific factors explain the performance gap between AdamW and SGD, and between Muon/SOAP and AdamW, in the absence of class imbalance?
- Basis in paper: [explicit] The Conclusion states it is an "open question that remains is to explain the performance difference," noting that prior explanations regarding class imbalance do not apply here.
- Why unresolved: The paper demonstrates a clear performance gap but does not isolate the underlying mathematical or architectural reasons specific to dynamical system data.
- What evidence would resolve it: Ablation studies on model architecture and loss landscape geometry that identify the specific properties requiring adaptive methods like Adam over SGD for this task.

### Open Question 3
- Question: Do the efficiency advantages of Muon and SOAP over AdamW persist when scaling to significantly larger model sizes?
- Basis in paper: [inferred] The introduction notes that computational constraints limited experiments to "relatively small-scale problems (~23M parameters)" despite the method's relevance to larger scientific applications.
- Why unresolved: Optimization dynamics and runtime overhead (1.45x–1.72x per step) may scale differently with model size, potentially negating the 18% loss reduction observed in smaller models.
- What evidence would resolve it: Benchmarking Muon and SOAP on diffusion models with parameter counts closer to state-of-the-art weather or climate models (e.g., >100M parameters).

## Limitations
- The loss-quality decoupling observed with ScheduleFree remains unexplained beyond the conjecture that annealing affects the optimization trajectory
- The specific architectural factors that make U-Net + diffusion training benefit from spectral/dense preconditioning are not isolated
- The sqrt learning-rate schedule is proposed as an anytime alternative, but its comparative benefits beyond matching wsd's performance aren't established

## Confidence
- **High:** Muon and SOAP achieve 18% lower loss than AdamW (directly measured, clear statistical comparison)
- **Medium:** ScheduleFree produces inferior generative quality despite matching loss (observed qualitatively, but mechanistic explanation is speculative)
- **Medium:** AdamW outperforms SGD for reasons beyond class imbalance (logically consistent with evidence, but architectural mechanism not proven)

## Next Checks
1. **Loss-quality correlation test:** Train multiple ScheduleFree runs with identical hyperparameters but different random seeds. Quantify variance in both loss and generated sample quality to determine if the quality degradation is consistent or stochastic.
2. **Architecture ablation:** Replace U-Net with a simpler architecture (e.g., fully connected network) and repeat the optimizer benchmark. This would test whether the AdamW advantage over SGD is architecture-dependent as claimed.
3. **Preconditioner analysis:** For SOAP, measure the condition number of preconditioned gradients over training. Compare against AdamW's diagonal preconditioning to verify that SOAP's dense preconditioning provides the hypothesized benefit.