---
ver: rpa2
title: Single-pass Detection of Jailbreaking Input in Large Language Models
arxiv_id: '2502.15435'
source_url: https://arxiv.org/abs/2502.15435
tags:
- attacks
- attack
- language
- llama
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Single Pass Detection (SPD), a method to detect
  jailbreaking attacks in Large Language Models (LLMs) with just one forward pass
  by analyzing the output logits. SPD identifies harmful prompts by detecting shifts
  in the logit distributions compared to benign inputs, using a feature matrix based
  on the negative log probabilities of the first few tokens.
---

# Single-pass Detection of Jailbreaking Input in Large Language Models

## Quick Facts
- arXiv ID: 2502.15435
- Source URL: https://arxiv.org/abs/2502.15435
- Reference count: 40
- Achieves >90% TPR and <1% FPR in detecting jailbreaking attacks using single forward pass

## Executive Summary
This paper introduces Single Pass Detection (SPD), a method that detects jailbreaking attacks on Large Language Models (LLMs) with just one forward pass by analyzing the output logits. SPD identifies harmful prompts by detecting shifts in the logit distributions compared to benign inputs, using a feature matrix based on the negative log probabilities of the first few tokens. An SVM classifier is trained to distinguish between attacked and benign sentences. Experiments show that SPD achieves high true positive rates (>90%) and low false positive rates (<1%) across multiple open-source models (Llama 2, Vicuna) and proprietary models (GPT-3.5, GPT-4, GPT-4o-mini), even with limited logit access. SPD is 12-80 times faster than existing perturbation-based or auxiliary LLM-based defenses, offering a highly efficient solution for safeguarding LLMs against jailbreaking attacks.

## Method Summary
SPD works by performing a single forward pass on the target LLM and extracting logits for the first few output tokens. The method computes softmax over the full vocabulary, selects the top 50 probabilities, and computes a feature matrix based on the negative log probabilities. This feature matrix is flattened and used to train an SVM classifier with an RBF kernel to distinguish between attacked and benign sentences. The approach leverages the observation that jailbreaking attacks cause detectable shifts in the logit distributions compared to benign inputs, allowing for efficient detection without multiple passes or auxiliary models.

## Key Results
- Achieves >90% True Positive Rate (TPR) and <1% False Positive Rate (FPR) on multiple models
- 12-80 times faster than existing perturbation-based or auxiliary LLM-based defenses
- Effective across open-source (Llama 2, Vicuna) and proprietary models (GPT-3.5, GPT-4, GPT-4o-mini)

## Why This Works (Mechanism)
SPD exploits the fundamental property that jailbreaking attacks manipulate the input prompt to trigger harmful outputs, which creates detectable shifts in the output logit distributions. By analyzing the negative log probabilities of the top logits for the first few tokens, SPD captures these distributional shifts that distinguish attacked prompts from benign ones. The method assumes that successful jailbreaks must produce output distributions that differ systematically from normal operation, making them detectable through statistical analysis of the logits.

## Foundational Learning
- **Logit distribution analysis**: Understanding how output logits reflect prompt semantics and model behavior is crucial for detecting adversarial manipulations
  - Why needed: Jailbreaks create characteristic patterns in logit distributions that differ from benign inputs
  - Quick check: Verify that attacked prompts produce systematically different top-k logit patterns than benign prompts

- **Single-pass efficiency**: The method leverages a single forward pass to extract detection features, avoiding the computational overhead of multiple passes or auxiliary models
  - Why needed: Real-time detection requires minimal computational overhead
  - Quick check: Confirm that feature extraction adds negligible latency to the forward pass

- **SVM classification on logit features**: Using traditional ML classifiers on engineered features from logits provides an efficient detection mechanism
  - Why needed: Simple classifiers can effectively distinguish between complex distributional patterns
  - Quick check: Validate that SVM achieves high accuracy with the proposed feature representation

## Architecture Onboarding

**Component Map**: Input Prompt → LLM Forward Pass → Logit Extraction → Softmax Normalization → Top-50 Selection → Negative Log Probability → Feature Matrix → SVM Classifier → Binary Decision

**Critical Path**: The forward pass through the LLM followed by immediate feature extraction and classification is the critical path. Any delay in logit processing or classifier inference directly impacts detection latency.

**Design Tradeoffs**: The method trades some detection accuracy for extreme speed by using simple SVM classifiers on engineered features rather than complex neural detectors. This makes it suitable for real-time deployment but potentially less robust to sophisticated adaptive attacks.

**Failure Signatures**: High false positive rates on code-related tasks indicate the method struggles with domains where benign prompts naturally produce unusual logit distributions. Performance degradation with limited logit access suggests the method relies heavily on comprehensive probability information.

**3 First Experiments**:
1. Test feature extraction on a small set of known attacked vs benign prompts to verify the logit distribution differences are detectable
2. Train and evaluate the SVM classifier on a minimal dataset to confirm the classification pipeline works
3. Measure the computational overhead of feature extraction relative to the base LLM inference time

## Open Questions the Paper Calls Out

**Open Question 1**: Can adaptive attacks that simultaneously minimize jailbreaking loss and maintain benign-like logit distributions successfully evade SPD detection? The paper notes preliminary experiments with KL-divergence-constrained GCG attacks failed to produce successful jailbreaks, but this does not constitute a full proof.

**Open Question 2**: What is the minimum logit access threshold (number of top-k tokens) required to achieve performance comparable to full vocabulary access on closed-source models? The authors explicitly state this threshold is not quantified.

**Open Question 3**: How can SPD be adapted to maintain effectiveness as new attack categories emerge that do not share characteristics with known attack families? The paper shows SPD struggles with unseen attack types from different families.

## Limitations

- Unreported SVM hyperparameters (C and gamma) may affect reproducibility of performance claims
- Limited evaluation on real-world jailbreaking attempts, focusing on curated benchmark datasets
- Performance relies on comprehensive logit access, with unclear degradation patterns under access constraints

## Confidence

**High confidence**: The fundamental approach of using single-pass logit analysis for jailbreak detection is technically sound and the mathematical framework is clearly specified.

**Medium confidence**: The reported TPR >90% and FPR <1% across multiple models are likely achievable with proper implementation, but exact replication depends on unreported hyperparameters and random seeds.

**Low confidence**: The claim of effective detection with "limited logit access" lacks quantitative thresholds, and the method's robustness to naturally occurring versus curated attacks is not empirically validated.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary SVM C and gamma parameters while measuring TPR/FPR to determine if performance claims are robust to hyperparameter choice.

2. **Real-world attack evaluation**: Test the SPD method on naturally occurring jailbreaking attempts from production LLM deployments rather than curated benchmark datasets.

3. **Limited access threshold testing**: Quantify the minimum logit access requirements (tokens, vocabulary subset size) needed to maintain >90% TPR, establishing concrete boundaries for "limited access" scenarios.