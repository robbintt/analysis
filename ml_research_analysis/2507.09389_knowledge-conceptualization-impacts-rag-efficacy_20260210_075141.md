---
ver: rpa2
title: Knowledge Conceptualization Impacts RAG Efficacy
arxiv_id: '2507.09389'
source_url: https://arxiv.org/abs/2507.09389
tags:
- schema
- knowledge
- queries
- enslaved
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how knowledge schema complexity and representation
  impact the performance of agentic Retrieval-Augmented Generation (RAG) systems.
  The authors conducted experiments using two knowledge graphs (KnowWhereGraph and
  Enslaved.org) with different schema representations (node-edge-node triples vs.
---

# Knowledge Conceptualization Impacts RAG Efficacy

## Quick Facts
- arXiv ID: 2507.09389
- Source URL: https://arxiv.org/abs/2507.09389
- Reference count: 40
- Primary result: Schema simplicity improves LLM SPARQL query generation accuracy (83% vs 38% correct queries)

## Executive Summary
This paper investigates how knowledge schema complexity and representation impact the performance of agentic Retrieval-Augmented Generation (RAG) systems. Through experiments with two knowledge graphs (KnowWhereGraph and Enslaved.org) using different schema representations, the authors evaluate GPT-4o's ability to generate SPARQL queries from competency questions. Results demonstrate that simpler schemas significantly improve query generation accuracy, with KWG-Lite achieving 83% correct queries versus 38% for the complex KWG schema. The study also reveals that while axiomatized representations can outperform simple triples for smaller schemas, representation method effects vary by knowledge graph, suggesting the need for balanced approaches in real-world RAG system design.

## Method Summary
The study employs a 2×2×2 factorial design comparing two knowledge graph families (KWG/KWG-Lite and Enslaved/Enslaved Wikibase) across two schema representations (node-edge-node triples vs. Manchester Syntax axioms) and three complexity levels (simple: 1 hop, moderate: 2-3 hops, complex: >3 hops). GPT-4o generates SPARQL queries from competency questions using a fixed temperature of 0.8. Query outputs are scored on a 3-point Likert scale (-1 for major flaws, 0 for minor flaws, 1 for valid) by human annotators, with results reported as mean ± standard deviation. The methodology includes full system and user prompt templates, with raw responses logged for qualitative annotation.

## Key Results
- Schema simplicity significantly improves LLM performance: KWG-Lite (83% correct queries) outperforms complex KWG schema (38% correct queries)
- Representation method effects vary by knowledge graph: axiomatized representations outperform NEN triples for smaller schemas (Wikibase) but show mixed results for larger schemas (Enslaved)
- LLM hallucination occurs primarily when competency questions lack direct mappings to schema terms, with models generating non-existent classes and relationships
- Smaller and more expressive representations can be equally effective as simpler ones, suggesting balanced approaches are optimal for real-world RAG systems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Schema simplicity (reduced reification/hops) generally improves an LLM's ability to generate syntactically and semantically correct SPARQL queries.
- **Mechanism:** Flattened graph structures reduce the cognitive load on the LLM by minimizing the number of intermediate steps (hops) required to link entities. This constrains the reasoning chain, lowering the probability of logic errors.
- **Core assumption:** LLMs have a finite effective context window and limited multi-step reasoning reliability.
- **Evidence anchors:**
  - [Abstract] "Simpler KWG-Lite schema achieving 83% correct queries versus 38% for the complex KWG schema."
  - [Page 12] "KWG-Lite's simpler structure makes a big difference in keeping the model on track."
  - [Corpus] No direct corpus validation found for schema complexity specifically; related papers focus on RAG poisoning or cultural risks.

### Mechanism 2
- **Claim:** Axiomatized representations (Manchester Syntax) can outperform simple Node-Edge-Node (NEN) triples, but primarily when the schema is small and structured (e.g., Wikibase).
- **Mechanism:** For smaller schemas, explicit logical axioms provide dense semantic constraints that guide the LLM more effectively than loose triples. However, in larger schemas, the verbosity of axioms fills the context window with noise, increasing confusion.
- **Core assumption:** The information density of axioms is beneficial only if the total token count remains within the model's processing sweet spot.
- **Evidence anchors:**
  - [Page 12] "Axiomatization did better for smaller representations (Wikibase) compared to larger (Enslaved)."
  - [Page 13] "Not just simpler, but smaller and more expressive... representation led to better performance."
  - [Corpus] Weak signal; corpus papers focus on general RAG optimization, not ontology serialization formats.

### Mechanism 3
- **Claim:** LLMs are prone to hallucinating non-existent classes and relationships when Competency Questions (CQs) lack direct lexical mappings to the provided schema terms.
- **Mechanism:** When the LLM cannot find a clear path from the natural language prompt to the schema, it leverages its parametric training data to "fill in" plausible-looking but non-existent URIs rather than deferring or admitting inability.
- **Core assumption:** The LLM prioritizes generating a "complete-looking" response over strict adherence to the provided context constraints.
- **Evidence anchors:**
  - [Page 14] "Hallucination occurs most frequently when the CQ does not contain direct mappings to the terms in the schema."
  - [Page 12] "When the model could not find relevant information... it often substituted unrelated schema elements or hallucinated new fictitious relationships."
  - [Corpus] Unverified in provided corpus.

## Foundational Learning

- **Concept: Schema Reification vs. Flattening**
  - **Why needed here:** The paper measures complexity largely by the level of reification (modeling relationships as entities), which requires complex multi-hop joins. Understanding this distinction is necessary to interpret the performance gap between KWG and KWG-Lite.
  - **Quick check question:** Can you distinguish between a direct property link (A -> relatesTo -> B) and a reified relationship (A -> hasEvent -> Event -> relatesTo -> B)?

- **Concept: Competency Questions (CQs)**
  - **Why needed here:** CQs act as the ground truth for natural language inputs in this architecture. The system's success is measured by how well it translates these domain-specific questions into SPARQL.
  - **Quick check question:** How does a Competency Question differ from a standard user prompt in terms of scope and intent validation?

- **Concept: Manchester Syntax**
  - **Why needed here:** This is the specific serialization method tested against simple triples. It attempts to make OWL logic human-readable, but the paper notes it may fail to capture all semantic nuances (e.g., General Class Axioms).
  - **Quick check question:** Why might a syntax designed for human readability (Manchester) be semantically "lossy" compared to machine-oriented RDF serializations?

## Architecture Onboarding

- **Component map:** Knowledge Graph (OWL/TTL) -> Serializer (NEN or Manchester) -> Prompt Constructor (Schema + CQ) -> LLM Agent (GPT-4o) -> SPARQL Query -> Triplestore

- **Critical path:**
  1. Schema Extraction (selecting relevant schema fragments)
  2. Serialization (converting to text/NEN/Axioms)
  3. Context Injection (fitting schema into prompt window)
  4. Query Validation (checking for hallucinations/syntax errors)

- **Design tradeoffs:**
  - Expressivity vs. Token Limits: Axiomatized representations are expressive but token-heavy (bad for large schemas). NEN triples are token-efficient but may lack semantic constraints (bad for complex reasoning)
  - Temperature Setting: Higher temperature (0.8) aids inference in simple schemas but increases hallucination risk in complex schemas

- **Failure signatures:**
  - Structural Hallucination: Generation of URIs or predicates that do not exist in the graph (detected by validation)
  - Misalignment: Query structure does not match the logic of the CQ (e.g., retrieving wrong data types)
  - Prefix Errors: Syntactic failures due to missing namespace definitions

- **First 3 experiments:**
  1. Complexity Baseline: Run the same set of CQs against a simplified (Lite) schema vs. a full schema to isolate the cost of complexity
  2. Representation A/B Test: Serialize the exact same schema subset as NEN triples vs. Manchester Syntax and compare query accuracy
  3. Hallucination Constraint: Add explicit negative constraints to the system prompt ("Do not invent URIs") and measure the reduction in "Structurally Misaligned" errors

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can Vision Language Models (VLMs) interpret graphical schema diagrams more effectively than text-based representations for generating SPARQL queries?
- **Basis in paper:** [explicit] The authors explicitly propose "exploring vision language model interpretation of schema diagrams instead of textual representation" as a future direction.
- **Why unresolved:** The current study evaluated only textual representations (NEN triples and Manchester Syntax), leaving visual modality untested.
- **What evidence would resolve it:** A comparative evaluation measuring the syntactic and semantic correctness of queries generated by VLMs analyzing schema images versus LLMs analyzing text.

### Open Question 2
- **Question:** Does the W3C Ontology Lexicon (OntoLex) offer improved semantic fidelity over Manchester Syntax for LLM-based query generation?
- **Basis in paper:** [explicit] The authors note Manchester Syntax failed to capture complex OWL constructs and suggest "tools such as the W3C Ontology Lexicon (OntoLex)... could be used for forming syntactic structures more formally."
- **Why unresolved:** The experiment revealed limitations in Manchester Syntax's ability to express general class axioms, but alternative serializations were not tested.
- **What evidence would resolve it:** Comparative experiments using OntoLex serializations to determine if they reduce hallucinations and improve query accuracy for complex schemas.

### Open Question 3
- **Question:** How does the LLM temperature parameter interact with schema complexity to influence hallucination rates in query generation?
- **Basis in paper:** [inferred] The authors speculate that their choice of a high temperature (0.8) may have caused "heightened creativity" that helped with simple schemas but induced hallucinations in complex ones, though this was not systematically tested.
- **Why unresolved:** The study used a fixed temperature, making it unclear if lower temperatures would mitigate hallucinations in complex schemas without harming performance in simple ones.
- **What evidence would resolve it:** An ablation study varying temperature settings across different schema complexities to measure the trade-off between inference flexibility and factual grounding.

## Limitations
- The study uses a relatively small set of competency questions and focuses on specific knowledge graphs, limiting generalizability to diverse real-world schemas
- Scoring methodology relies on subjective human judgment for the 3-point Likert scale, introducing potential inter-rater variability
- Fixed temperature setting of 0.8 may not represent optimal settings for all schema types and complexity levels

## Confidence
- **High Confidence:** Simpler schemas improve query generation accuracy (83% vs 38% correct queries) - well-supported by experimental data
- **Medium Confidence:** Axiomatized representations outperform NEN triples primarily for smaller schemas - supported but requires broader validation
- **Medium Confidence:** LLM hallucination when CQs lack direct schema mappings - well-documented but mechanism universality needs testing

## Next Checks
1. **Schema Expressivity Test:** Conduct experiments varying the semantic expressivity of schemas while keeping structural complexity constant to isolate whether performance gains from simplicity stem from reduced complexity or reduced expressivity
2. **Cross-Domain Replication:** Apply the same methodology to knowledge graphs from different domains (e.g., biomedical, financial) to test the generalizability of findings about complexity-performance relationships
3. **LLM Model Comparison:** Systematically test the same schema-complexity framework across multiple LLM architectures (different sizes, training paradigms) to determine whether observed effects are model-dependent or fundamental to RAG architecture