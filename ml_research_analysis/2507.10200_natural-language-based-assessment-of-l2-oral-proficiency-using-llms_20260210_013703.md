---
ver: rpa2
title: Natural Language-based Assessment of L2 Oral Proficiency using LLMs
arxiv_id: '2507.10200'
source_url: https://arxiv.org/abs/2507.10200
tags:
- assessment
- language
- descriptors
- each
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a natural language-based assessment (NLA)
  method for second language (L2) oral proficiency that leverages open-source LLMs
  with CEFR analytic descriptors in a zero-shot setting. The approach uses the Qwen
  2.5 72B LLM to evaluate learner speech transcriptions, applying established language
  proficiency descriptors across multiple dimensions to predict holistic scores.
---

# Natural Language-based Assessment of L2 Oral Proficiency using LLMs

## Quick Facts
- arXiv ID: 2507.10200
- Source URL: https://arxiv.org/abs/2507.10200
- Reference count: 0
- Primary result: NLA method achieves competitive performance on L2 oral proficiency assessment using text-only LLMs with CEFR descriptors

## Executive Summary
This paper introduces a Natural Language-based Assessment (NLA) method for evaluating L2 oral proficiency using open-source LLMs with CEFR analytic descriptors in a zero-shot setting. The approach uses the Qwen 2.5 72B LLM to assess learner speech transcriptions, applying established language proficiency descriptors across ten analytic dimensions to predict holistic scores. Experiments on the S&I Corpus demonstrate that this text-only method achieves competitive performance, surpassing a BERT-based model and matching a speech LLM fine-tuned on read-aloud data, particularly in mismatched task settings. The method offers greater interpretability through its use of explainable language descriptors and addresses data scarcity challenges in L2 assessment.

## Method Summary
The NLA method employs a zero-shot approach where the Qwen 2.5 72B LLM receives speech transcriptions and CEFR can-do descriptors for ten analytic aspects (e.g., grammatical accuracy, vocabulary range). For each aspect, the model selects the most fitting descriptor level (A1-C2) from a randomized list, and logit probabilities are converted to numerical scores. This process is repeated three times per aspect with different descriptor orderings to mitigate positional bias. The ten analytic scores are averaged to produce a holistic score for each response. The system uses OpenAI Whisper small for transcription and operates without any task-specific training data, leveraging the LLM's general language understanding capabilities.

## Key Results
- NLA achieves competitive correlation with human ratings (PCC/SRC: 0.66/0.69) compared to a BERT-based model (0.63/0.64) and speech LLM fine-tuned on read-aloud data (0.65/0.66)
- The method is particularly effective in mismatched task settings where evaluation data differs from training data
- NLA offers greater interpretability through ten separate analytic scores that align with CEFR proficiency descriptors
- The approach addresses data scarcity by using zero-shot learning rather than requiring task-specific training data

## Why This Works (Mechanism)

### Mechanism 1
A large language model can emulate the human assessment process for oral proficiency by using structured CEFR can-do descriptors as zero-shot instructions, achieving performance competitive with fine-tuned models in data-scarce scenarios. The LLM processes transcriptions with CEFR descriptors as prompts, selecting the most fitting descriptor level without requiring task-specific training examples.

### Mechanism 2
An analytic scoring framework yields more diagnostic and stable assessments than a single holistic score, as it decomposes proficiency into distinct, interpretable components. The system predicts ten separate analytic scores (e.g., vocabulary range, coherence) that are averaged to form a holistic score, providing a vector of scores that can be analyzed to understand the drivers of the final rating.

### Mechanism 3
A zero-shot instruction-following approach is more robust to task mismatch than a model fine-tuned on a different data type, as it avoids overfitting to task-specific features. The NLA method relies on general language understanding rather than features specific to read-aloud or spontaneous speech, making it inherently more generalizable when the evaluation task differs from available training data.

## Foundational Learning

- **Zero-Shot Learning with Large Language Models**
  - Why needed: This is the core innovation enabling assessment without task-specific training data
  - Quick check: How does the NLA system predict a score for a new learner's response without any training data? (Answer: It uses a pre-trained LLM and instructs it with the assessment criteria as a prompt)

- **The Common European Framework of Reference for Languages (CEFR)**
  - Why needed: CEFR's "can-do descriptors" are the instructions that define the task for the LLM
  - Quick check: In the NLA pipeline, what information from the CEFR is actually fed into the model? (Answer: The text descriptions of proficiency levels for specific skills, not just level names)

- **Analytic vs. Holistic Scoring**
  - Why needed: The paper argues for analytic scoring's superiority for interpretability
  - Quick check: Why does the paper's method produce a more explainable output than a BERT-based model that predicts a single holistic score? (Answer: Because it produces scores for ten separate linguistic aspects that can be inspected and analyzed)

## Architecture Onboarding

- **Component map:** Audio input -> ASR transcription (Whisper small) -> LLM prompt construction with CEFR descriptors -> Qwen 2.5 72B inference -> Logit extraction and softmax -> Fair Average score calculation -> Aggregation across 10 aspects -> Holistic score

- **Critical path:** ASR quality affects transcription accuracy; logit extraction from 4-bit quantized model must be stable; positional bias mitigation through descriptor randomization is essential; aggregation of ten analytic scores determines final output

- **Design Tradeoffs:** Text-only approach simplifies implementation but cannot assess pronunciation/acoustic features; zero-shot learning prioritizes generalization over peak performance of fine-tuned models; computational cost is higher due to ten separate analytic prompts per response

- **Failure signatures:** High variance in scores across different descriptor orderings indicates positional bias; systematic differences between dev and eval set performance suggest data sensitivity; poor correlation with human ratings indicates fundamental model limitations

- **First 3 experiments:**
  1. Implement NLA pipeline on graded dataset to confirm non-random correlation with human ratings
  2. Compare scores with fixed vs. randomized descriptor order to measure positional bias mitigation effectiveness
  3. Measure performance degradation when using noisy vs. clean transcriptions to quantify ASR dependency

## Open Questions the Paper Calls Out

- **Multilingual adaptation:** The approach could be adapted to multilingual assessment tasks across different languages using CEFR descriptors, though no experiments have validated cross-linguistic portability
- **Data sensitivity analysis:** The paper observes degraded performance on evaluation sets compared to development sets but hasn't investigated root causes (data distribution shifts, transcription quality, prompt sensitivity)
- **Acoustic feature integration:** The text-only framework cannot assess phonological control and pronunciation, despite these being important CEFR dimensions; multimodal integration remains unexplored
- **Diagnostic utility validation:** While analytic scores align with holistic ratings, the paper hasn't validated whether these scores provide actionable feedback to learners or align with human analytic ratings

## Limitations
- The method relies entirely on ASR quality, with no analysis of how transcription errors affect assessment accuracy
- The approach cannot evaluate acoustic features like pronunciation, intonation, and phonological control that are important for oral proficiency
- The interpretability claims lack empirical validation showing how practitioners actually use or benefit from the analytic scores compared to holistic predictions

## Confidence

**High confidence:** The claim that NLA achieves competitive performance with fine-tuned models in mismatched task settings is well-supported by experimental results showing comparable correlation metrics