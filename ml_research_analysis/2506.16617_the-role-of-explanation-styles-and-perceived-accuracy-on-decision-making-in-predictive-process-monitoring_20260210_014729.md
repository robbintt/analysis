---
ver: rpa2
title: The Role of Explanation Styles and Perceived Accuracy on Decision Making in
  Predictive Process Monitoring
arxiv_id: '2506.16617'
source_url: https://arxiv.org/abs/2506.16617
tags:
- explanation
- explanations
- accuracy
- styles
- perceived
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how explanation styles and perceived AI
  accuracy influence decision-making in predictive process monitoring (PPM). Using
  loan application data from BPIC 2017, the authors trained a random forest model
  and generated explanations using three styles: feature importance (LIME), rule-based
  (Anchor), and counterfactual (DiCE).'
---

# The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring

## Quick Facts
- arXiv ID: 2506.16617
- Source URL: https://arxiv.org/abs/2506.16617
- Reference count: 38
- Primary result: Counterfactual explanations significantly improve task performance, particularly in low-accuracy conditions

## Executive Summary
This study investigates how explanation styles and perceived AI accuracy influence decision-making in predictive process monitoring using loan application data from BPIC 2017. Through a user experiment with 179 participants across six groups, the research found that counterfactual explanations improved task performance most effectively, especially when AI accuracy was perceived as low. Rule-based explanations achieved highest satisfaction scores but didn't improve performance, while feature importance explanations reduced agreement with AI predictions. Counterintuitively, participants in low-accuracy groups outperformed high-accuracy groups, suggesting critical engagement when AI reliability is questioned.

## Method Summary
The study used BPIC 2017 event log data for loan application process monitoring, training a Random Forest classifier with approximately 85% accuracy. Researchers generated explanations using three styles: LIME (feature importance), Anchor (rule-based), and DiCE (counterfactual). They conducted a user experiment where 179 participants made loan approval decisions across six groups (high/low perceived accuracy Ã— three explanation styles), evaluating task performance, agreement with AI predictions, and decision confidence before and after explanations.

## Key Results
- Counterfactual explanations significantly improved task performance, particularly in low-accuracy groups
- Rule-based explanations achieved highest satisfaction and confidence scores but didn't significantly improve task performance
- Participants in low-accuracy groups outperformed high-accuracy groups, suggesting critical engagement with explanations
- Feature importance explanations reduced agreement with AI predictions compared to other styles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Counterfactual explanations improve decision quality by enabling error detection in low-trust environments
- **Core assumption:** Users possess sufficient domain knowledge to evaluate counterfactual scenario validity
- **Evidence anchors:** Counterfactuals enabled low-accuracy group to identify and correct AI errors; cognitive capabilities moderate effects
- **Break condition:** If counterfactuals generate unrealistic scenarios, performance benefits collapse

### Mechanism 2
- **Claim:** Lower perceived system accuracy triggers critical engagement, increasing task performance
- **Core assumption:** Low accuracy label drives evaluation mode vs compliance mode (automation bias)
- **Evidence anchors:** Low-accuracy group outperformed high-accuracy group; counterintuitive result indicates critical engagement
- **Break condition:** If accuracy label is too low (< random chance), users dismiss AI entirely

### Mechanism 3
- **Claim:** Rule-based explanations boost subjective satisfaction without improving objective performance
- **Core assumption:** User satisfaction driven by cognitive ease rather than diagnostic value
- **Evidence anchors:** Rule-based explanations had highest satisfaction but no performance improvement; satisfaction-efficacy gap exists
- **Break condition:** If task requires strict policy compliance, rule-based might outperform others

## Foundational Learning

- **Concept: Automation Bias & Over-reliance**
  - Why needed: Explains why high-accuracy label led to worse performance through excessive trust
  - Quick check: Why did the "96% accurate" group make more mistakes than the "63% accurate" group?

- **Concept: Explanation Fidelity vs. Comprehensibility**
  - Why needed: Distinguishes between explanations being mathematically faithful vs. human-understandable
  - Quick check: Why might a satisfying explanation fail to catch mistakes that a difficult explanation catches?

- **Concept: Predictive Process Monitoring (PPM) Context**
  - Why needed: PPM deals with sequences and event logs, unlike static classification tasks
  - Quick check: How does sequential loan application process differ from single-frame image classification for explanations?

## Architecture Onboarding

- **Component map:** Data Layer (BPIC 2017) -> Model Layer (Random Forest) -> XAI Interface (LIME/Anchor/DiCE adapters) -> Presentation Layer (decision interface with accuracy label and explanation UI)

- **Critical path:** DiCE Adapter is most sensitive; requires solving optimization for feasible counterfactuals, unlike LIME's local approach or Anchor's binary rules

- **Design tradeoffs:**
  - Simplicity vs. Efficacy: Rule-based yields higher satisfaction (easier to sell) but counterfactual needed for actual decision-quality improvement
  - Perception Management: Hiding high-accuracy labels might improve performance over showing them

- **Failure signatures:**
  - "Confidence Trap": High confidence with Rule-based but incorrect decisions (monitor high confidence/low accuracy)
  - "Agreement Drop": LIME caused users to disagree more even when AI was right (watch for agreement metric drops)

- **First 3 experiments:**
  1. **Calibration Test:** Verify 50 DiCE counterfactuals are actionable vs. nonsensical
  2. **Label Sensitivity Check:** A/B test "96% Acc" vs "63% Acc" labels with identical model to confirm engagement patterns
  3. **Performance vs. Satisfaction Correlation:** Plot internal users' satisfaction against error detection rate to verify satisfaction-performance divergence

## Open Questions the Paper Calls Out

- **Open Question 1:** How does true model accuracy (vs perceived accuracy) influence user reliance and decision-making performance with different explanation styles?
  - Why unresolved: Study manipulated accuracy label rather than underlying model, using single 85% accuracy model
  - What evidence would resolve it: Comparative study with objectively high vs low-performing models

- **Open Question 2:** Do explanation style effects hold for domain experts vs student/crowd-worker populations used in this study?
  - Why unresolved: Participants lacked specific domain knowledge (banking risk assessment), limiting generalizability
  - What evidence would resolve it: Replication with professional loan officers or process analysts

- **Open Question 3:** How do LLM-generated natural language explanations compare to visual/logic-based styles in user satisfaction and performance?
  - Why unresolved: Study only tested LIME, Anchor, and DiCE; unknown if conversational explanations bridge performance-satisfaction gap
  - What evidence would resolve it: Comparison of current baselines against LLM-based explanation condition

## Limitations
- Random Forest hyperparameters not specified, affecting baseline accuracy and explanation quality
- Synthetic feature generation rules incompletely described, complicating exact reproduction
- Psychological mechanism for "critical engagement" inferred but not directly measured (no cognitive load or eye-tracking data)

## Confidence
- **High:** Counterfactual explanations improve task performance in low-trust environments
- **Medium:** Rule-based explanations boost satisfaction without improving performance
- **Medium:** Lower perceived accuracy triggers critical engagement

## Next Checks
1. **Explanation Feasibility Audit:** Manually verify 50 counterfactuals generated by DiCE to confirm they are actionable vs. nonsensical
2. **Perception Manipulation Test:** A/B test interface with "96% Acc" vs "63% Acc" labels using identical model to confirm low-accuracy labels trigger deeper engagement
3. **Satisfaction-Performance Gap Analysis:** Have internal users complete tasks with both explanation styles and plot satisfaction scores against actual error detection rates