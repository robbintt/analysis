---
ver: rpa2
title: 'Optimal Condition for Initialization Variance in Deep Neural Networks: An
  SGD Dynamics Perspective'
arxiv_id: '2508.12834'
source_url: https://arxiv.org/abs/2508.12834
tags:
- loss
- initialization
- function
- distribution
- variance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study establishes a theoretical optimal condition for initialization
  variance in deep neural networks using stochastic gradient descent (SGD) dynamics.
  By approximating SGD through a continuous-time framework and deriving the corresponding
  Fokker-Planck equation, the authors connect the quasi-stationary distribution of
  weights to the initial Gaussian distribution via Kullback-Leibler divergence.
---

# Optimal Condition for Initialization Variance in Deep Neural Networks: An SGD Dynamics Perspective

## Quick Facts
- arXiv ID: 2508.12834
- Source URL: https://arxiv.org/abs/2508.12834
- Authors: Hiroshi Horii; Sothea Has
- Reference count: 25
- Establishes theoretical optimal condition for initialization variance in deep neural networks using SGD dynamics

## Executive Summary
This study derives a theoretically optimal condition for initialization variance in deep neural networks through an analysis of SGD dynamics. By approximating SGD with continuous-time stochastic differential equations and deriving the corresponding Fokker-Planck equation, the authors connect the quasi-stationary distribution of weights to the initial Gaussian distribution. The key finding is that optimal initialization variance should equal the steady-state variance of weights (σ²₀ = V(W)), providing a mathematically grounded alternative to heuristic initialization methods. Numerical experiments on MNIST and Fashion-MNIST datasets validate that this optimal initialization consistently achieves lower training loss and higher test accuracy compared to conventional He-normal initialization.

## Method Summary
The authors develop a theoretical framework by approximating SGD dynamics with continuous-time stochastic differential equations. They derive the corresponding Fokker-Planck equation to characterize the evolution of the weight distribution over time. By analyzing the Kullback-Leibler divergence between the quasi-stationary distribution and the initial Gaussian distribution, they establish a connection between initialization variance and expected loss. The theoretical analysis reveals that for small initialization variances, the loss scales proportionally to the steady-state variance, while for large variances, it scales logarithmically. This leads to the derivation of an optimal initialization condition where the initial variance equals the steady-state variance of weights.

## Key Results
- Derives optimal initialization variance condition: σ²₀ = V(W), where V(W) is the steady-state weight variance
- For small variances, expected loss scales proportionally to steady-state variance
- For large variances, expected loss scales logarithmically with initialization variance
- Numerical validation on MNIST and Fashion-MNIST shows optimal initialization outperforms He-normal initialization in both training loss and test accuracy

## Why This Works (Mechanism)
The mechanism works by connecting initialization variance to the long-term behavior of SGD through a continuous-time approximation. The Fokker-Planck equation describes how the weight distribution evolves during training, and the KL divergence measures how initialization affects this evolution. When initialization variance matches the steady-state variance, the system minimizes the expected loss by balancing exploration (variance) with convergence stability. This mathematical framework provides a principled way to determine initialization variance rather than relying on empirical heuristics.

## Foundational Learning
1. **Continuous-time SGD approximation** - Needed to apply differential equation analysis to discrete optimization; quick check: verify that time discretization doesn't significantly alter the theoretical predictions
2. **Fokker-Planck equation** - Required to model the evolution of weight distributions under stochastic dynamics; quick check: confirm the equation accurately captures the gradient noise statistics
3. **Kullback-Leibler divergence** - Essential for quantifying the relationship between initial and steady-state distributions; quick check: validate that KL divergence minimization correlates with loss minimization
4. **Steady-state variance analysis** - Critical for determining the optimal initialization point; quick check: measure actual steady-state variance during training across different architectures

## Architecture Onboarding
**Component map:** Initialization variance → SGD dynamics → Fokker-Planck evolution → KL divergence → Expected loss → Optimal condition
**Critical path:** Initial distribution → Training dynamics → Quasi-stationary state → Performance metric
**Design tradeoffs:** Continuous-time approximation simplifies analysis but may miss discrete effects; theoretical framework provides generality but may not capture all practical nuances
**Failure signatures:** Poor generalization despite optimal initialization suggests mismatch between theoretical assumptions and practical training conditions; performance degradation on certain architectures indicates limitations of the continuous-time approximation
**First experiments:** 1) Test optimal initialization on convolutional networks for image classification, 2) Evaluate on recurrent networks for sequence modeling, 3) Compare against Xavier initialization under identical conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies on continuous-time approximation of SGD, which may not fully capture discrete optimization dynamics
- Theoretical framework assumes specific conditions on loss landscape and gradient noise that may not hold universally
- Numerical validation limited to fully connected networks on MNIST and Fashion-MNIST datasets

## Confidence
- High confidence in the mathematical derivation of the optimal variance condition under stated theoretical framework
- Medium confidence in practical applicability across diverse deep learning scenarios
- Medium confidence in numerical validation due to limited scope of tested architectures and datasets

## Next Checks
1. Test the optimal initialization condition on convolutional neural networks for image classification and on recurrent networks for sequence modeling tasks
2. Evaluate performance across diverse datasets including CIFAR-10/100, ImageNet, and natural language processing benchmarks
3. Compare the proposed initialization against other theoretically motivated methods like Xavier and He initialization under identical training conditions to quantify practical improvements