---
ver: rpa2
title: Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware
  Fairness
arxiv_id: '2502.17921'
source_url: https://arxiv.org/abs/2502.17921
tags:
- fairness
- bias
- recommendation
- metrics
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a set of category-aware metrics to evaluate\
  \ gender bias in recommendation systems, addressing the limitations of existing\
  \ fairness metrics that overlook item categories and ranking. The proposed metrics\u2014\
  such as Category Coverage, Relative Category Representation, and ranking-based measures\
  \ like Category MAP and CDCG\u2014enable a nuanced assessment of bias across different\
  \ genres."
---

# Unmasking Gender Bias in Recommendation Systems and Enhancing Category-Aware Fairness

## Quick Facts
- **arXiv ID:** 2502.17921
- **Source URL:** https://arxiv.org/abs/2502.17921
- **Reference count:** 40
- **Primary result:** Category-aware fairness metrics and regularization reduce gender bias in recommendations by up to 77% without significantly harming accuracy.

## Executive Summary
This paper introduces a set of category-aware metrics to evaluate gender bias in recommendation systems, addressing the limitations of existing fairness metrics that overlook item categories and ranking. The authors propose six new metrics—including Category Coverage, Relative Category Representation, and ranking-based measures like Category MAP and CDCG—that enable a nuanced assessment of bias across different genres. By incorporating Category Coverage as a regularization term in the loss function during model training, the approach effectively reduces gender bias in recommendations without significantly degrading overall performance. Experiments on three real-world datasets using five baseline models demonstrate that the proposed approach significantly reduces bias, with NeuMF achieving an average bias reduction of 77%, 53%, and 50% across MovieLens 100K, MovieLens 1M, and Yelp datasets respectively.

## Method Summary
The authors propose a category-aware fairness framework that introduces six new metrics to measure gender bias in recommendation systems. The core innovation is incorporating Category Coverage as a regularization term in the model's loss function during training. The method calculates the absolute difference in category distributions between male and female users (Gender Balance Score) and adds this to the recommendation loss. The combined loss function is optimized to simultaneously maximize recommendation accuracy and minimize gender disparity across categories. The approach was implemented using Cornac framework with MF, VAE-CF, and NeuMF models, and validated on MovieLens 100K, MovieLens 1M, and Yelp datasets.

## Key Results
- The proposed category-aware metrics revealed significant gender bias that traditional accuracy metrics missed, particularly in genre distributions (e.g., Action vs. Romance recommendations).
- Incorporating Category Coverage as a regularization term reduced gender bias by an average of 77% for