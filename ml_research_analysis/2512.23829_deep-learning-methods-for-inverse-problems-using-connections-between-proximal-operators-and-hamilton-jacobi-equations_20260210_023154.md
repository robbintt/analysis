---
ver: rpa2
title: Deep learning methods for inverse problems using connections between proximal
  operators and Hamilton-Jacobi equations
arxiv_id: '2512.23829'
source_url: https://arxiv.org/abs/2512.23829
tags:
- convex
- proximal
- prior
- learning
- equations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep learning method for solving inverse
  problems by directly learning priors for proximal operators, avoiding the need to
  invert priors post-training. The method leverages the connection between proximal
  operators and Hamilton-Jacobi (HJ) partial differential equations (PDEs), particularly
  the backward viscosity solution (BVS) characterization of priors.
---

# Deep learning methods for inverse problems using connections between proximal operators and Hamilton-Jacobi equations

## Quick Facts
- arXiv ID: 2512.23829
- Source URL: https://arxiv.org/abs/2512.23829
- Reference count: 40
- Key outcome: Introduces a deep learning method for inverse problems by learning priors for proximal operators through connections to Hamilton-Jacobi equations, avoiding post-training inversion

## Executive Summary
This paper presents a novel approach to solving inverse problems by directly learning priors for proximal operators using connections to Hamilton-Jacobi partial differential equations. The method leverages the backward viscosity solution characterization of priors and employs convex neural networks to approximate the convex function corresponding to the prior plus a quadratic term. Theoretical analysis provides error bounds based on max-plus algebra, while numerical experiments demonstrate effectiveness across various prior types including ℓ1 norm, min-plus algebra, concave, and negative ℓ1 norm priors.

## Method Summary
The method establishes a connection between proximal operators and Hamilton-Jacobi PDEs, specifically using the backward viscosity solution characterization. By approximating the HJ PDE solution, the prior can be recovered through convex optimization. The approach employs Learned Proximal Networks (convex neural networks) to learn the convex function representing the prior plus a quadratic term. Theoretical analysis draws on max-plus algebra to derive sample complexity bounds, providing a rigorous foundation for understanding approximation error in high-dimensional settings.

## Key Results
- Demonstrates effectiveness in learning various priors (ℓ1, min-plus algebra, concave, negative ℓ1 norm) in high dimensions
- Shows mean squared errors generally decrease with increasing sample size across different priors
- Proves method particularly effective for forward solution (HJ PDE solution) with reasonable performance for learning priors
- Achieves good performance even for nonconvex priors through the convex neural network framework

## Why This Works (Mechanism)
The approach works by exploiting the mathematical relationship between proximal operators and Hamilton-Jacobi PDEs, where priors can be characterized through backward viscosity solutions. This connection allows the use of convex neural networks to approximate the underlying convex function that defines the prior. The max-plus algebra framework provides theoretical guarantees on approximation quality, while the learned representation avoids the computational burden of post-training inversion required by traditional methods.

## Foundational Learning
- Hamilton-Jacobi PDEs: Needed to understand the mathematical foundation connecting proximal operators to differential equations; Quick check: Verify understanding of viscosity solutions and backward HJ equations
- Backward viscosity solution characterization: Essential for how priors are represented mathematically; Quick check: Confirm grasp of how priors relate to HJ PDE solutions
- Convex neural networks: Core architecture for learning convex functions; Quick check: Understand max-affine spline properties and convexity constraints
- Max-plus algebra: Provides theoretical bounds on approximation error; Quick check: Review max-plus semiring properties and their application to sample complexity
- Proximal operators: Central to inverse problem formulation; Quick check: Verify understanding of proximal operator definition and properties
- Viscosity solutions: Mathematical framework for weak solutions to HJ PDEs; Quick check: Confirm understanding of comparison principle and uniqueness

## Architecture Onboarding

Component Map:
Learned Proximal Network -> Convex Optimization Solver -> Prior Approximation -> Inverse Problem Solver

Critical Path:
The critical path flows from learning the convex function via the neural network, through solving the resulting convex optimization problem to recover the prior, which is then applied to solve the inverse problem. The accuracy of the learned prior directly impacts the quality of the inverse problem solution.

Design Tradeoffs:
- Expressiveness vs. computational efficiency: More complex networks can better approximate priors but increase training time
- Sample complexity vs. approximation accuracy: More samples improve accuracy but increase data requirements
- Convexity constraints vs. flexibility: Convex networks ensure tractable optimization but may limit representation of certain priors

Failure Signatures:
- Poor generalization when sample size is insufficient relative to problem dimension
- Numerical instability in solving the convex optimization problem if the learned function is poorly conditioned
- Convergence issues when applying the learned prior to solve inverse problems, indicating inadequate prior approximation

First Experiments:
1. Verify HJ PDE solution approximation quality for a simple convex prior before proceeding to learning
2. Test learned proximal operator on a low-dimensional inverse problem with known solution
3. Compare approximation error vs. sample size for different prior types to validate theoretical bounds

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Numerical experiments are limited in scope and primarily use synthetic data rather than real-world applications
- Method effectiveness appears prior-dependent, with varying accuracy across different prior types and dimensions
- Computational efficiency and scalability to truly large-scale problems are not extensively addressed
- Limited comparison with existing state-of-the-art proximal operator learning methods

## Confidence

**Major Claim Clusters Confidence:**
- Theoretical connection between proximal operators and HJ PDEs: High
- Sample complexity bounds via max-plus algebra: High
- Generalizability to nonconvex priors: Medium
- Numerical effectiveness across all tested priors: Medium
- Scalability to high-dimensional practical problems: Low

## Next Checks
1. Conduct systematic experiments varying the number of samples across different dimensionalities to empirically verify the predicted error bounds.
2. Test the method on real-world inverse problem datasets (e.g., medical imaging or compressed sensing applications) rather than synthetic data.
3. Benchmark computational time and memory requirements against existing state-of-the-art proximal operator learning methods for direct comparison.