---
ver: rpa2
title: 'Large Language Model-Based Agents for Automated Research Reproducibility:
  An Exploratory Study in Alzheimer''s Disease'
arxiv_id: '2505.23852'
source_url: https://arxiv.org/abs/2505.23852
tags:
- agents
- data
- studies
- methods
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored the use of Large Language Model (LLM) agents
  to automate reproducibility of published Alzheimer's Disease research. Using GPT-4o
  agents with defined roles (Planner, Engineer, Scientist, Critic, Executor), the
  system attempted to reproduce findings from five highly cited NACC-based studies
  using only study abstracts, methods, and dataset descriptions.
---

# Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease

## Quick Facts
- arXiv ID: 2505.23852
- Source URL: https://arxiv.org/abs/2505.23852
- Reference count: 40
- Agents reproduced approximately 53.2% of key findings across five Alzheimer's Disease studies

## Executive Summary
This exploratory study investigates the use of LLM agents to automate scientific reproducibility assessment, focusing on Alzheimer's Disease research using the NACC dataset. The system employs GPT-4o agents with specialized roles (Planner, Engineer, Scientist, Critic, Executor) to reproduce study findings from abstracts, methods sections, and dataset descriptions alone. Across five highly cited studies, agents achieved approximately 53.2% reproduction of key findings, successfully replicating some trends and statistical significances while struggling with exact numeric matches. The work demonstrates both the promise and current limitations of LLM-based automation for evaluating research rigor at scale.

## Method Summary
The study deployed a multi-agent system using GPT-4o, with five specialized roles working collaboratively to reproduce published Alzheimer's Disease research findings. Agents were provided only abstracts, methods sections, and dataset descriptions of five highly cited NACC-based studies, without access to original code or complete methodological details. The system attempted to replicate key findings including demographic characteristics, statistical associations, and clinical outcomes. Reproduction quality was assessed by comparing agent-generated results against original study findings, measuring both exact matches and trend reproduction. The approach tests whether LLM agents can automate reproducibility assessment when complete methodological documentation is unavailable.

## Key Results
- LLM agents reproduced approximately 53.2% of key findings per study on average
- Numeric values and statistical methods often differed from original studies
- Agents successfully replicated some study trends and statistical significances despite missing implementation details

## Why This Works (Mechanism)
LLM agents can parse and interpret scientific text to identify methodological components and research objectives. The multi-agent architecture distributes cognitive tasks (planning, execution, critique) similar to human research teams. By constraining input to published materials, the system tests real-world reproducibility scenarios where original code is unavailable. The agents' ability to generate code and perform data analysis enables end-to-end reproduction attempts. Trend and significance detection is often more robust than exact numeric replication, allowing partial success even with incomplete information.

## Foundational Learning
**Scientific reproducibility concepts**: Understanding what constitutes reproducible research and how to measure it - needed to define success criteria and evaluation metrics; quick check: compare agent output against established reproducibility frameworks.
**Multi-agent collaboration**: How specialized LLM agents can work together on complex tasks - needed to distribute analytical workload and cross-validate results; quick check: trace agent communication logs for task handoffs.
**Algorithmic bias in LLMs**: How model training affects analytical outputs - needed to contextualize systematic differences between agent and original results; quick check: analyze variance in reproduction attempts across different studies.
**Dataset characteristics**: Understanding NACC dataset structure and limitations - needed to contextualize reproduction feasibility; quick check: verify agent data preprocessing matches documented dataset properties.

## Architecture Onboarding

**Component map**: User Query -> Planner Agent -> Engineer Agent <-> Executor Agent <-> Scientist Agent <-> Critic Agent -> Final Output

**Critical path**: Planner defines reproduction strategy → Engineer designs analysis pipeline → Executor runs code on data → Scientist validates scientific validity → Critic evaluates reproducibility against original → Output generation

**Design tradeoffs**: The system prioritizes breadth (testing multiple studies) over depth (complete methodological documentation), accepting approximate reproduction as evidence of feasibility. Using only published text increases real-world applicability but reduces exact reproducibility potential.

**Failure signatures**: Agent outputs diverging significantly from original statistics suggest missing methodological details; inability to execute code indicates technical implementation gaps; critic agent flagging inconsistencies reveals flawed reproduction logic.

**First experiments**:
1. Reproduce a single well-documented study with available code to establish baseline agent performance
2. Test agent collaboration on a simple analytical task to validate role distribution
3. Compare agent reproduction of trending vs. exact numeric findings to understand success patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope testing only five studies using a single Alzheimer's Disease dataset (NACC)
- Missing original code and complete methodological details may have biased results downward
- Use of only abstracts, methods, and dataset descriptions may omit critical implementation details

## Confidence

**High confidence**: The feasibility of using LLM agents for automated reproducibility assessment is demonstrated; agents can identify and reproduce study trends and significances in some cases.

**Medium confidence**: The quantitative finding that agents reproduced approximately 53.2% of key findings per study, as this metric may be sensitive to how "reproduction" is defined and evaluated.

**Medium confidence**: The conclusion that exact replication remains challenging while trend reproduction is possible, given the inherent difficulties in matching original study conditions without complete methodological details.

## Next Checks

1. Expand evaluation to 50+ studies across multiple disease domains and datasets to assess generalizability of agent performance.
2. Compare agent reproduction results against studies with publicly available code and complete methodological documentation to isolate the impact of missing information.
3. Conduct blinded expert review of agent-generated reproductions against original studies to validate the automated assessment methodology and refine evaluation criteria.