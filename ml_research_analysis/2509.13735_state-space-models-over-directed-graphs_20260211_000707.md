---
ver: rpa2
title: State Space Models over Directed Graphs
arxiv_id: '2509.13735'
source_url: https://arxiv.org/abs/2509.13735
tags:
- graph
- directed
- graphs
- node
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DirGraphSSM, the first state space model
  specifically designed for directed graph learning. The core innovation is DirEgo2Token,
  a novel sequentialization framework that converts directed graphs into causal sequences
  via k-hop ego graphs, preserving long-range causal dependencies.
---

# State Space Models over Directed Graphs

## Quick Facts
- arXiv ID: 2509.13735
- Source URL: https://arxiv.org/abs/2509.13735
- Reference count: 40
- Primary result: Introduces DirGraphSSM, achieving SOTA performance on directed graph learning tasks with 1.5× to 2× faster training than existing models

## Executive Summary
This paper introduces DirGraphSSM, the first state space model specifically designed for directed graph learning. The core innovation is DirEgo2Token, a novel sequentialization framework that converts directed graphs into causal sequences via k-hop ego graphs, preserving long-range causal dependencies. Building on this, the authors develop an efficient message-passing-based implementation of SSMs that avoids the serial computation bottleneck of existing graph Mamba models. Experimental results demonstrate that DirGraphSSM achieves state-of-the-art performance on three representative directed graph learning tasks while attaining 1.5× to 2× faster training speeds compared to existing models on large-scale datasets.

## Method Summary
The paper presents DirGraphSSM, which extends state space models to directed graph learning through a novel sequentialization framework called DirEgo2Token. This framework converts directed graphs into causal sequences by constructing k-hop ego-networks for each target node and ordering nodes by shortest path distance. The model implements SSMs using a message-passing paradigm that avoids serial computation bottlenecks by leveraging precomputed SSM convolution kernels. Multi-head attention is used to dynamically weight messages from predecessors based on their shortest path distances. The architecture includes specialized components for handling directed graph structure, including in/out gated graph convolutional layers, DepthPlus positional encodings for cycle handling, and PageRank-guided fusion attention.

## Key Results
- Achieves state-of-the-art performance on three directed graph learning tasks
- Attains 1.5× to 2× faster training speeds compared to existing models on large-scale datasets
- Successfully handles both directed acyclic graphs and directed cyclic graphs
- Demonstrates effectiveness across diverse domains including code AST classification, neural architecture regression, and malware detection

## Why This Works (Mechanism)

### Mechanism 1: DirEgo2Token for Causal Sequentialization
- **Claim:** DirEgo2Token converts a directed graph into a permutation-invariant, causal sequence by constructing a k-hop directed ego-network for each target node and ordering nodes by shortest path distance.
- **Mechanism:** For a target node $v$, it defines a causal sequence $S_v = (L_k, L_{k-1}, ..., L_0)$, where $L_i$ is the set of nodes at a shortest path distance $i$ from $v$ via directed incoming edges. This process traces the "causal history" of a node, transforming a graph structure into a format amenable to sequential modeling.
- **Core assumption:** Long-range causal dependencies in a directed graph are primarily captured by the hierarchical structure of incoming paths (predecessors).
- **Evidence anchors:**
  - [abstract]: "...DirEgo2Token, a novel sequentialization framework that converts directed graphs into causal sequences via k-hop ego graphs, preserving long-range causal dependencies."
  - [section]: Equation (6) in Section IV-B formally defines the sequence construction.
  - [corpus]: Related work on directed acyclic graphs (DAGs) supports the use of topological ordering for capturing dependencies [ProDAG, DAG Convolutional Networks].
- **Break condition:** The mechanism assumes that causal flow aligns with the defined directionality (in-edges). In cycles or highly interconnected components where a strict causal order is ambiguous, this hierarchical decomposition may flatten complex interdependencies into a simpler linear sequence, potentially losing relational information.

### Mechanism 2: Digraph SSM Scan as a Message-Passing Kernel
- **Claim:** The `Digraph SSM Scan` module efficiently captures long-range dependencies by reframing the SSM's convolutional operation as a message-passing process over the k-hop neighborhood.
- **Mechanism:** Instead of explicitly serializing the graph (which requires padding and serial computation), it uses a precomputed SSM convolution kernel ($K = (CB, CAB, ..., CA^{L-1}B)$). For each node, it aggregates messages from its k-hop predecessors. Each message is transformed by a kernel component corresponding to the path distance ($SSM_{(spd(u,v))}$). This is formalized as $y_v = \sum_{u \in N_{in}^k(v)} \alpha_{u,v} \cdot SSM_{(spd(u,v))}(f(x_u)W_V)$.
- **Core assumption:** The SSM convolution kernel can effectively model the decay or transformation of information as it propagates across increasing path distances in the graph.
- **Evidence anchors:**
  - [abstract]: "...develop an efficient message-passing-based implementation of SSMs that avoids the serial computation bottleneck..."
  - [section]: Section IV.C and Equation (10) detail the equivalence between the SSM scan and the message-passing paradigm.
  - [corpus]: Weak. The paper claims to be the first to apply SSMs to directed graphs in this manner. General SSM literature (e.g., Mamba, S4) validates the convolutional form for sequences but not directly this graph-based application.
- **Break condition:** The mechanism's efficiency and effectiveness rely on the sparsity of the graph. On dense graphs, the k-hop neighborhood size grows rapidly, potentially creating a computational and memory bottleneck similar to quadratic attention.

### Mechanism 3: Multi-Head Attention as a Selective SSM Mechanism
- **Claim:** The model uses a standard multi-head attention mechanism not for global self-attention, but to compute attention weights ($\alpha_{u,v}$) which are then used as dynamic, data-dependent filters within the SSM message-passing process.
- **Mechanism:** This mimics the "selective scanning" of Mamba, which makes SSM parameters input-dependent. Here, attention dynamically weights the contribution of each predecessor node's message ($SSM_{(spd(u,v))}(f(x_u)W_V)$) to the final aggregation, allowing the model to selectively focus on the most relevant causal paths.
- **Core assumption:** A standard attention mechanism provides a sufficient and computationally efficient way to implement content-aware "selection" within a graph SSM framework.
- **Evidence anchors:**
  - [abstract]: "This marks the first systematic extension of state space models to the field of directed graph learning."
  - [section]: Section IV.C states: "Notably, this attention mechanism plays a role in SSM scan process analogous to the 'selective scanning' in Mamba."
  - [corpus]: Weak. The analogy to Mamba's selective scan is a design choice presented in this paper, not a proven equivalence.
- **Break condition:** This mechanism assumes that per-node attention over its local k-hop neighborhood is sufficient for selection. It lacks the global context that a full self-attention mechanism would provide, potentially limiting its "selective" capacity to local causal chains.

## Foundational Learning

- **Concept: State Space Models (SSMs)**
  - **Why needed here:** This is the core computational engine of the entire architecture. Understanding how SSMs process 1D sequences via a convolution kernel is necessary to grasp how `DirGraphSSM` generalizes this to graph structures.
  - **Quick check question:** Explain how a continuous state space model is discretized and how its recursive form relates to a convolution operation.

- **Concept: Message Passing in Graph Neural Networks (GNNs)**
  - **Why needed here:** The paper's key efficiency claim rests on reformulating the SSM scan as a message-passing algorithm. One must understand how GNNs aggregate information from neighborhoods to understand the implementation.
  - **Quick check question:** Describe the general message-passing update rule for a node in a GNN, distinguishing between directed and undirected graphs.

- **Concept: Directed Graphs and Causal Dependencies**
  - **Why needed here:** The central problem this paper solves is modeling directionality. A solid grasp of how directed edges represent information flow, precedence, or causality is required to evaluate the model's claims.
  - **Quick check question:** In a citation network modeled as a directed graph, how would information flow? What does a "k-hop ego graph" represent in this context?

## Architecture Onboarding

- **Component map:** Preprocessing (Tarjan SCC + topological sort + BFS) -> DirGatedGCN layers -> Digraph SSM Scan (message-passing with attention) -> Digraph Fusion Attention -> Output head

- **Critical path:** The central innovation and data flow is in the `Digraph SSM Scan`. Your first implementation task is to correctly build the k-hop predecessor edge index (`k_hop_edge_index`) and shortest path distances (`k_hop_spd`) during preprocessing, as these tensors are fundamental to the message-passing SSM kernel application in the forward pass.

- **Design tradeoffs:**
  - **Receptive Field vs. Efficiency (Parameter K):** A larger `K` (max sequence length/hop count) captures longer-range dependencies but increases computational cost linearly. The paper suggests tuning this parameter.
  - **Parallelism vs. Selective Scanning:** The model replaces Mamba's input-dependent scan with an attention mechanism over the k-hop neighborhood. This choice trades the theoretical efficiency of a pure scan for the parallelizability and proven selection properties of attention.

- **Failure signatures:**
  - **Performance degradation on dense graphs:** If computational cost explodes, it's likely because the k-hop neighborhood size is too large, violating the sparsity assumption.
  - **Loss of long-range dependency:** If performance on tasks requiring long-range reasoning drops, check if `K` is set too low, or if the SSM kernel parameters are not learning long-term memory.
  - **Out-of-Time (OOT) on preprocessing:** Extremely large graphs may struggle with the initial Tarjan's algorithm and BFS for all nodes. Preprocessing is a one-time cost, but could be a bottleneck.

- **First 3 experiments:**
  1. **Verify SSM Kernel Application:** Construct a small, manually defined directed graph. Perform a forward pass and check that the messages passed between nodes are correctly transformed by the SSM kernel component corresponding to their shortest path distance (e.g., a 2-hop neighbor's message should be processed by $CA^2B$).
  2. **Ablation on Sequence Length (K):** Run the model on a benchmark dataset (e.g., `ogbg-code2`) while varying `K` (e.g., 1, 2, 4, 8). Plot the trade-off between accuracy and training time per epoch to validate the parameter sensitivity analysis in the paper.
  3. **Compare with Baseline GNNs:** Implement or run a standard directed GNN (like DirGNN) and compare its training time and accuracy against `DirGraphSSM` on a medium-scale dataset to confirm the claimed efficiency-performance balance.

## Open Questions the Paper Calls Out

- **Question:** How can neighborhood sampling strategies be designed to mitigate the performance degradation of DirGraphSSM on dense directed graphs?
  - **Basis in paper:** [explicit] The authors state in the Conclusion that the model "suffers from significant performance degradation when handling dense directed graphs due to the sequentialization of K-hop directed neighborhoods" and identify "designing new strategies (e.g., neighborhood sampling strategies)" as critical future work.
  - **Why unresolved:** The current DirEgo2Token framework relies on full $k$-hop neighborhood aggregation, which becomes computationally prohibitive and noisy when the directed graph density is high.
  - **What evidence would resolve it:** A modified sampling mechanism that maintains state-of-the-art accuracy while reducing computational complexity on benchmark dense directed graph datasets.

- **Question:** Can an automated or adaptive methodology be developed to select the optimal maximum SSM sequence length ($K$) to balance efficiency and accuracy?
  - **Basis in paper:** [explicit] The Conclusion notes that "how to choose the maximum sequence length K for the SSM to balance efficiency and accuracy remains an open question."
  - **Why unresolved:** The current approach relies on heuristics or parameter studies (grid search) to set $K$, which may not generalize optimally across different graph topologies without manual tuning.
  - **What evidence would resolve it:** A learning-based or theoretical heuristic for determining $K$ that correlates with improved performance metrics without requiring exhaustive empirical search.

## Limitations

- **Theoretical gaps:** The fundamental connection between SSM convolutional kernels and causal information propagation in directed graphs is not rigorously established.
- **Efficiency constraints:** The claimed efficiency advantages are conditional on graph sparsity and may not hold for dense graphs.
- **Limited ablation studies:** The paper lacks comprehensive ablation of critical components like PageRank-guided fusion and multi-head attention within the SSM scan.

## Confidence

- **High Confidence:** The core technical implementation of the message-passing SSM scan is well-defined and reproducible.
- **Medium Confidence:** The experimental results demonstrating state-of-the-art performance are compelling but limited by the small number of datasets tested.
- **Low Confidence:** Claims about causal dependency preservation and SSM kernel selection effectiveness are primarily supported by empirical results rather than theoretical guarantees.

## Next Checks

1. **Ablation Study on Core Components:** Run controlled experiments removing or replacing the DirEgo2Token sequentialization, the PageRank-guided fusion attention, and the multi-head attention within the SSM scan. Measure the impact on both accuracy and training time to quantify the contribution of each innovation.

2. **Scalability and Sparsity Analysis:** Systematically test the model on synthetic directed graphs with varying density (from sparse DAGs to dense cyclic graphs) while measuring memory usage, training time, and accuracy. This will validate whether the claimed efficiency advantages hold across different graph structures and identify the breaking point where k-hop neighborhoods become too large.

3. **Comparison with Simpler Baselines:** Implement and test the model against standard GNNs (like GraphSAGE or GAT) and attention-based models on at least two of the benchmark datasets. This will determine whether the SSM architecture provides genuine advantages over simpler approaches for directed graph learning tasks.