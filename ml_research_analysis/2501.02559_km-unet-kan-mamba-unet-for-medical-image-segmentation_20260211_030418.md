---
ver: rpa2
title: KM-UNet KAN Mamba UNet for medical image segmentation
arxiv_id: '2501.02559'
source_url: https://arxiv.org/abs/2501.02559
tags:
- segmentation
- image
- medical
- feature
- km-unet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces KM-UNet, a U-shaped architecture combining
  Kolmogorov-Arnold Networks (KANs) and state-space models (SSMs) for improved medical
  image segmentation. It addresses the limitations of CNNs in capturing long-range
  dependencies and Transformers in computational efficiency.
---

# KM-UNet KAN Mamba UNet for medical image segmentation

## Quick Facts
- arXiv ID: 2501.02559
- Source URL: https://arxiv.org/abs/2501.02559
- Authors: Yibo Zhang
- Reference count: 38
- One-line primary result: KM-UNet achieves an average IoU of 80.45% and F1 score of 88.63% on five medical image segmentation datasets

## Executive Summary
KM-UNet introduces a U-shaped architecture that combines Kolmogorov-Arnold Networks (KANs) and state-space models (SSMs) to address limitations in medical image segmentation. The model integrates KANs for interpretable feature representation, SSMs for scalable long-range modeling, and a Selective-Scan Efficient Multi-scale (SEM) attention module for multi-directional feature extraction. Experiments on five benchmark datasets demonstrate KM-UNet outperforms existing methods while maintaining low computational cost and fewer parameters.

## Method Summary
KM-UNet is a hybrid architecture that combines convolutional layers, Kolmogorov-Arnold Networks, and state-space models in a U-shaped structure. The encoder uses a Convolution Phase with SEM modules containing S6 blocks for multi-directional scanning. The bottleneck employs a Tok-KAN Phase with learnable activation functions. The decoder mirrors the encoder structure with tokenized KAN blocks. Skip connections use simple addition rather than concatenation to reduce computational overhead. The model processes 2D feature maps by unfolding them into sequences and scanning in four directions to aggregate spatial information efficiently.

## Key Results
- Achieves average IoU of 80.45% and F1 score of 88.63% across five benchmark datasets
- Maintains low computational cost at 17.66 Gflops and only 7.35M parameters
- Outperforms existing methods on ISIC17, ISIC18, CVC, BUSI, and GLAS datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Selective-Scan Efficient Multi-scale (SEM) module enables efficient global context modeling by utilizing State Space Models (SSMs) to capture long-range dependencies.
- Mechanism: The SEM module unfolds 2D feature maps into sequences and processes them using the S6 block (an SSM variant). It scans inputs in four directions to aggregate spatial information, achieving linear complexity compared to Transformer's quadratic scaling.
- Core assumption: Scanning 2D image patches as 1D sequences across multiple directions preserves sufficient spatial context to model complex anatomical structures.
- Evidence anchors: [abstract] "KM-UNet leverages... SSMs for scalable long-range modeling"; [section] Page 3, Eq 1-2: The SEM module scans features along multiple directions and merges them using the S6 block.

### Mechanism 2
- Claim: Integrating Kolmogorov-Arnold Networks (KAN) into the bottleneck enhances non-linear feature representation and interpretability while reducing parameter count.
- Mechanism: KANs replace linear weight matrices and fixed activation functions of MLPs with learnable, parameterized activation functions (typically B-splines) on the edges, allowing flexible function approximation with fewer parameters.
- Core assumption: The function approximation capability of learnable splines is better suited for the non-linear mapping in the latent space of medical images than fixed linear projections.
- Evidence anchors: [abstract] "KM-UNet leverages the Kolmogorov-Arnold representation theorem for efficient feature representation"; [section] Page 5, Eq 7-9: Contrasts MLPs with KANs, noting KANs replace linear transformation matrices with learnable activation functions.

### Mechanism 3
- Claim: The hybrid Convolution-SSM-KAN architecture recovers fine boundary details that pure SSM or Transformer models often miss.
- Mechanism: The model uses a "Convolution Phase" for initial feature extraction, preserving local inductive bias, before transitioning to the "Tok-KAN Phase" for global reasoning. The SEM module bridges this by using multi-scale convolutions alongside SSM scanning.
- Core assumption: Standard SSMs require explicit convolutional guidance to maintain local spatial fidelity.
- Evidence anchors: [abstract] Mentions combining strengths of KANs and SSMs; [section] Page 3, "Multi-Scale Attention" section describes using parallel Conv1x1 and Conv3x3 to capture short and long-range dependencies.

## Foundational Learning

- **State Space Models (SSM / Mamba)**
  - Why needed here: To understand how the model achieves linear complexity (O(L)) for sequence modeling, serving as a faster alternative to Transformers for long-range dependencies.
  - Quick check question: How does the "Selective Scan" (S6) mechanism differ from standard RNN processing in terms of input dependency?

- **Kolmogorov-Arnold Networks (KAN)**
  - Why needed here: To grasp why the bottleneck is designed with learnable activation functions rather than linear weights, and how this impacts parameter efficiency.
  - Quick check question: In a KAN layer, where are the learnable parameters locatedâ€”on the nodes or the edges?

- **U-Shaped Skip Connections**
  - Why needed here: To understand how the model attempts to merge high-level semantic features (from the bottleneck) with low-level spatial features (from the encoder).
  - Quick check question: Does KM-UNet use concatenation or simple addition for its skip connections, and why might this reduce computational overhead?

## Architecture Onboarding

- Component map: Input -> SEM (S6 Scan) -> Tokenizer -> KAN (Bottleneck) -> Upsample -> SEM -> Output
- Critical path: The S6 block's scanning direction and the KAN's spline training are the critical variables
- Design tradeoffs:
  - Accuracy vs. Latency: The sequential nature of SSM scanning and iterative nature of KAN splines can be slower to train than pure CNNs
  - Addition vs. Concatenation: Using "simple addition" for skip connections saves parameters but may discard spatial information compared to concatenation
- Failure signatures:
  - Boundary Blurring: If the SEM multi-scale attention is misconfigured, the model may revert to coarse masks
  - Spline Overfitting: KAN layers might show high training accuracy but poor validation generalization if regularization is insufficient
- First 3 experiments:
  1. Baseline Validation: Run KM-UNet on ISIC17 and compare Dice score against standard U-Net to verify the reported ~7% uplift
  2. SEM Ablation: Disable the S6 scanning (replacing it with standard convolution) to isolate the contribution of the SSM to the "long-range" metrics
  3. KAN vs. MLP Swap: Replace the Tok-KAN block with a standard MLP block to measure the parameter reduction and accuracy delta provided specifically by KAN

## Open Questions the Paper Calls Out

- **Question 1**: How can the computational complexity of the Selective-Scan Efficient Multi-scale (SEM) attention module be reduced without compromising segmentation accuracy?
  - Basis in paper: The authors state in the Conclusion that despite the SEM module's excellent performance, "its computational complexity remains relatively high, and future work could focus on designing more efficient feature extraction."
  - Why unresolved: While the model is more efficient than Transformers, the multi-directional scanning strategy introduces overhead that may hinder real-time deployment.
  - What evidence would resolve it: A study proposing an optimized SEM variant that maintains an IoU > 80% while significantly lowering Gflops.

- **Question 2**: Can the KM-UNet architecture be effectively adapted for 3D volumetric segmentation and multimodal medical imaging (e.g., MRI/CT)?
  - Basis in paper: The Conclusion suggests that "incorporating multimodal medical imaging data... could enhance the model's ability to identify complex pathologies."
  - Why unresolved: The current implementation and tokenization strategy are tailored for 2D image inputs; the feasibility of extending the KAN-SSM fusion to 3D tensors is unverified.
  - What evidence would resolve it: Successful application of a 3D-adapted KM-UNet on a volumetric dataset (e.g., BraTS) demonstrating improved segmentation over 3D U-Net baselines.

- **Question 3**: Is the proposed "rotation from outer layers toward the center" scanning strategy robust for multi-object or highly irregular segmentation tasks?
  - Basis in paper: The authors claim the scanning approach enhances accuracy "particularly for single-object tasks or segmentation tasks involving simpler shapes," implying potential limitations in more complex scenarios.
  - Why unresolved: The paper evaluates datasets (like ISIC and GLAS) that largely feature single, centralized subjects, leaving the method's efficacy on scattered or multi-instance pathologies unstated.
  - What evidence would resolve it: A comparative analysis on a multi-instance medical dataset showing the scanning method's performance against standard raster scanning.

## Limitations

- The exact channel dimensions for each stage (C1-C5, D1-D5) are defined as hyperparameters without providing concrete values, making precise architectural replication challenging
- The ISIC dataset's input resolution is not explicitly stated, creating ambiguity in data preprocessing
- The paper mentions both a 7:3 train/test split and an 80/20 split, creating confusion about the exact data partitioning strategy

## Confidence

- **High Confidence**: The core architectural innovations (KAN bottleneck, SSM-based SEM module, hybrid convolution-SSM-KAN design) are well-described and the performance improvements over baseline methods are statistically significant across multiple datasets
- **Medium Confidence**: The reported computational efficiency metrics (17.66 Gflops, 7.35M parameters) appear reasonable for the architecture, though exact verification would require access to the full implementation
- **Low Confidence**: The specific implementation details of the KAN spline functions, the exact SEM scanning directions, and the precise transition mechanisms between convolution and tokenized phases are not fully specified

## Next Checks

1. **Baseline Verification**: Implement KM-UNet on ISIC17 using the 80/20 split and verify the reported ~7% improvement in Dice score over standard U-Net
2. **SEM Ablation Study**: Disable the S6 scanning mechanism and replace it with standard convolution to quantify the contribution of the SSM component to long-range dependency modeling
3. **Channel Dimension Sensitivity**: Test multiple configurations of the unspecified channel dimensions (C1-C5, D1-D5) to understand their impact on both accuracy and computational efficiency