---
ver: rpa2
title: 'GeLaCo: An Evolutionary Approach to Layer Compression'
arxiv_id: '2507.10059'
source_url: https://arxiv.org/abs/2507.10059
tags:
- compression
- similarity
- gelaco
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeLaCo, an evolutionary approach to large
  language model compression via layer merging. The method employs a module-wise similarity
  fitness function that captures attention, feed-forward, and hidden state representations
  during the evolutionary search for optimal layer merge configurations.
---

# GeLaCo: An Evolutionary Approach to Layer Compression

## Quick Facts
- arXiv ID: 2507.10059
- Source URL: https://arxiv.org/abs/2507.10059
- Reference count: 20
- Introduces evolutionary layer merging for LLM compression with Pareto frontier optimization

## Executive Summary
GeLaCo presents a novel evolutionary approach to compress large language models through layer merging operations. The method employs a module-wise similarity fitness function that evaluates attention, feed-forward, and hidden state representations to guide the evolutionary search for optimal layer configurations. By supporting both single and multi-objective optimization, GeLaCo establishes the first Pareto frontier along compression ratio and quality dimensions, outperforming existing state-of-the-art compression methods across multiple model sizes and benchmarks.

## Method Summary
GeLaCo uses evolutionary algorithms to search for optimal layer merge configurations in transformer models. The core innovation is a module-wise similarity fitness function that captures attention, feed-forward, and hidden state representations during the evolutionary search. The method supports both single and multi-objective optimization, where the latter simultaneously optimizes for compression ratio and model quality. The evolutionary search explores different layer merging patterns to find configurations that balance model size reduction with performance preservation, producing Pareto-optimal solutions that represent the trade-off frontier between compression and quality.

## Key Results
- GeLaCo consistently outperforms state-of-the-art methods (LaCo, LLM-Pruner, SliceGPT) on perplexity-based and generative benchmarks
- Module-wise similarity metric shows superior performance compared to KL divergence and perplexity alternatives
- Multi-objective results reveal critical compression thresholds beyond which model performance degrades significantly
- GeLaCo solutions dominate the Pareto front compared to previous approaches across Llama-2 7B/13B and Llama-3.1 8B/70B models

## Why This Works (Mechanism)
The evolutionary approach works by systematically exploring the space of possible layer merge configurations through fitness-guided search. The module-wise similarity metric captures fine-grained structural information from multiple components (attention, feed-forward, hidden states) rather than relying on coarse-grained measures. This allows the evolutionary algorithm to identify which layers can be merged without significant quality degradation. The multi-objective framework enables discovery of Pareto-optimal solutions that explicitly balance compression goals with quality preservation, rather than optimizing a single weighted objective.

## Foundational Learning
- **Evolutionary Algorithms**: Population-based optimization methods that use selection, mutation, and crossover to explore solution spaces - needed for systematic search of layer merge configurations; quick check: understand selection pressure and convergence properties
- **Module-wise Similarity**: Fine-grained comparison of transformer subcomponents (attention, feed-forward, hidden states) - needed to guide evolutionary search toward quality-preserving merges; quick check: verify similarity metric sensitivity to architectural changes
- **Pareto Optimality**: Concept where no objective can be improved without worsening another - needed to characterize trade-offs between compression and quality; quick check: confirm dominance relationships in solution sets
- **Transformer Layer Structure**: Understanding of attention heads, feed-forward networks, and residual connections - needed to implement effective merging operations; quick check: validate merge patterns preserve essential information flow
- **Compression Ratio Metrics**: Quantitative measures of model size reduction - needed to evaluate compression effectiveness; quick check: verify calculation methods across different model architectures

## Architecture Onboarding
**Component Map**: Input Model -> Evolutionary Search -> Layer Merge Operations -> Compressed Model
**Critical Path**: Model loading → Module-wise similarity calculation → Evolutionary fitness evaluation → Merge operation selection → Compressed model generation
**Design Tradeoffs**: Computational cost of evolutionary search vs. compression quality, single vs. multi-objective optimization, fine-grained vs. coarse-grained similarity metrics
**Failure Signatures**: Degraded performance on specific tasks, loss of long-range dependencies, failure to maintain semantic coherence in generated text
**First Experiments**:
1. Verify baseline model performance on standard benchmarks before compression
2. Test evolutionary search convergence on small model subset
3. Validate module-wise similarity metric sensitivity to known architectural changes

## Open Questions the Paper Calls Out
None

## Limitations
- Exclusive focus on decoder-only transformer architectures (LLama family) without validation on encoder-decoder models
- Evolutionary search requires significant computational resources during optimization phase (exact requirements not quantified)
- Similarity metric evaluation limited to three alternatives (KL divergence, perplexity, module-wise similarity)
- Ablation study on similarity metrics uses only one model size (Llama-2 7B), limiting generalizability

## Confidence
- **High Confidence**: Core claims about GeLaCo's effectiveness relative to baselines and Pareto frontier results
- **Medium Confidence**: Module-wise similarity metric superiority claim (limited comparison scope)
- **Low Confidence**: Generalizability to non-LLama architectures and real-world deployment scenarios

## Next Checks
1. Validate GeLaCo's effectiveness on encoder-decoder architectures (e.g., T5, BART) and vision-language models to assess architectural generalizability
2. Conduct resource profiling of the evolutionary search phase to quantify computational overhead and identify optimization opportunities
3. Extend the similarity metric ablation to include additional distance measures (e.g., cosine similarity, Earth Mover's Distance) across multiple model sizes to strengthen the metric comparison