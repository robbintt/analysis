---
ver: rpa2
title: A Survey on Large Language Models for Automated Planning
arxiv_id: '2502.12435'
source_url: https://arxiv.org/abs/2502.12435
tags:
- planning
- llms
- language
- large
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey critically examines the use of large language models
  (LLMs) in automated planning, analyzing both their potential and limitations. While
  LLMs show promise in multi-step reasoning and generalization across domains, they
  struggle with long-horizon planning tasks and often produce plans with arbitrarily
  poor costs.
---

# A Survey on Large Language Models for Automated Planning

## Quick Facts
- arXiv ID: 2502.12435
- Source URL: https://arxiv.org/abs/2502.12435
- Authors: Mohamed Aghzal; Erion Plaku; Gregory J. Stein; Ziyu Yao
- Reference count: 10
- Primary result: Large language models struggle with long-horizon planning tasks and often produce plans with arbitrarily poor costs, though they show promise when integrated with traditional planning frameworks

## Executive Summary
This survey critically examines the use of large language models (LLMs) in automated planning, analyzing both their potential and limitations. While LLMs show promise in multi-step reasoning and generalization across domains, they struggle with long-horizon planning tasks and often produce plans with arbitrarily poor costs. The paper categorizes existing approaches into two main categories: using LLMs as standalone planners and integrating them with traditional planning frameworks. Standalone methods include hierarchical task breakdown, plan refinement, search-based techniques, and finetuning, each with significant limitations in computational efficiency, reliability, and optimality. The paper advocates for integrating LLMs with traditional planners through text-to-formal language translation, commonsense knowledge enhancement, and plan evaluation. This hybrid approach leverages LLMs' natural language understanding and commonsense knowledge while maintaining the rigor and cost-effectiveness of traditional planning methods, offering a more promising direction for practical applications.

## Method Summary
The survey provides a comprehensive literature review of LLM applications in automated planning, systematically categorizing approaches into standalone LLM planners and LLM-integrated frameworks. The analysis examines four primary standalone methods (hierarchical task breakdown, plan refinement, search-based techniques, and finetuning) and three integration mechanisms (text-to-formal translation, commonsense knowledge augmentation, and plan evaluation). The survey evaluates each approach based on its ability to handle long-horizon tasks, computational efficiency, reliability, and plan optimality. The analysis is supported by theoretical arguments, case studies, and references to existing research, though empirical benchmarking data is limited.

## Key Results
- LLMs struggle with long-horizon planning tasks and often produce plans with arbitrarily poor costs when used as standalone planners
- Text-to-formal language translation shows promise as a hybrid approach, leveraging LLMs' natural language understanding while maintaining planning rigor
- Commonsense knowledge augmentation can improve planning efficiency but reliability depends on domain relevance
- Qualitative plan evaluation by LLMs can capture human preferences but suffers from inconsistent explanations

## Why This Works (Mechanism)

### Mechanism 1: Text-to-Formal Language Translation
- Claim: LLMs can serve as effective interfaces between natural language task specifications and symbolic planners when functioning as translators rather than planners.
- Mechanism: The LLM receives natural language descriptions of goals and constraints, then outputs formal representations (PDDL, temporal logic, or executable code) that classical planners can process. The symbolic planner then computes solutions using guaranteed algorithms.
- Core assumption: The translation accuracy is sufficiently high that semantic errors do not propagate into malformed planning problems.
- Evidence anchors:
  - [abstract] "This hybrid approach leverages LLMs' natural language understanding and commonsense knowledge while maintaining the rigor and cost-effectiveness of traditional planning methods."
  - [Section 4] "Building on the success of LLMs in machine translation and semantic parsing, several studies have explored their ability to translate task specifications expressed in natural language into a formal language that traditional symbolic planners can process."
  - [corpus] Related survey "LLMs as Planning Formalizers" (arXiv:2503.18971) explicitly examines this translation paradigm, corroborating the mechanism's prominence.
- Break condition: Translation fails when task specifications contain ambiguities that the LLM cannot disambiguate without iterative feedback loops (Section 5 identifies this as an open problem).

### Mechanism 2: Commonsense Knowledge Augmentation
- Claim: LLMs can improve planning efficiency by contributing domain knowledge that would otherwise require manual engineering.
- Mechanism: During planning, the LLM is queried for heuristics such as: likelihood of finding objects in locations, valid action preconditions, or high-level subgoal decomposition. These semantic cues prune the search space or guide hierarchical decomposition.
- Core assumption: The LLM's pre-trained commonsense knowledge is relevant and accurate for the target domain.
- Evidence anchors:
  - [Section 4] "LLMs have been utilized to identify whether preconditions for specific actions are satisfied and to monitor the resulting effects of those actions... [and] to provide statistical insights, such as estimating the likelihood of finding objects."
  - [Section 4] References LLM-modulo framework [Kambhampati et al., 2024] which advocates for leveraging LLM commonsense to "guess" high-level plans for external verification.
  - [corpus] Corpus signals are weak for direct comparative evidence on commonsense augmentation effectiveness; no neighbor papers directly address this mechanism with quantitative benchmarks.
- Break condition: Performance degrades in specialized domains where the LLM lacks coverage, potentially producing hallucinated constraints or incorrect probability estimates.

### Mechanism 3: Qualitative Plan Evaluation
- Claim: LLMs can evaluate plans along human-preference dimensions that are difficult to formalize as objective functions.
- Mechanism: After a traditional planner proposes candidate plans, the LLM scores them based on stylistic, safety, or preference criteria encoded implicitly in pre-training data. This can also extend to reward function generation for RL-based planners.
- Core assumption: The LLM's implicit preferences accurately reflect the user's intended qualitative criteria, and its evaluations are consistent across similar plans.
- Evidence anchors:
  - [Section 4] "As they are trained on large amounts of human-generated data, LLMs implicitly encode human stylistic and qualitative preferences. Thus, LLMs can also function as evaluators, assessing plans based on qualitative and stylistic criteria."
  - [Section 4] Cites work using LLMs/VLMs as scoring modules to detect undesirable behavior and provide feedback [Guan et al., 2024; Song et al., 2024].
  - [corpus] No strong corpus corroboration specific to plan evaluation; related work focuses primarily on planning generation rather than evaluation.
- Break condition: Evaluation becomes unreliable when explanations are unfaithful to internal reasoning (Section 5 cites [Turpin et al., 2024]), or when numerical cost assessment is required (Section 5 notes LLM limitations in metric reasoning).

## Foundational Learning

- Concept: **Classical Planning Formalization (S, A, T, s_init, G)**
  - Why needed here: Understanding the tuple structure of planning problems is prerequisite to comprehending why LLMs fail at standalone planning—they must implicitly simulate state transitions without explicit representations.
  - Quick check question: Given a navigation task, can you identify the state space, action set, transition function, initial state, and goal conditions?

- Concept: **Long-Horizon vs. Short-Horizon Planning**
  - Why needed here: The paper's central claim hinges on LLMs performing acceptably on short-horizon tasks but degrading on long-horizon problems; distinguishing these regimes is essential for architectural decisions.
  - Quick check question: Does your task require reasoning about immediate next steps (short-horizon) or anticipating effects multiple actions into the future (long-horizon)?

- Concept: **PDDL (Planning Domain Definition Language)**
  - Why needed here: The text-to-formal translation mechanism relies on LLMs outputting PDDL; understanding its syntax and semantics is necessary to evaluate translation quality.
  - Quick check question: Can you read a simple PDDL domain file and identify the predicates, actions, preconditions, and effects?

## Architecture Onboarding

- Component map: NL Interface → LLM Translator → Symbolic Planner → (if multi-candidate) LLM Evaluator → Executor
- Critical path: NL Interface → LLM Translator → Symbolic Planner → (if multi-candidate) LLM Evaluator → Executor. The translator output quality is the primary bottleneck.
- Design tradeoffs:
  - Standalone LLM planning vs. hybrid: Standalone offers flexibility but produces arbitrarily poor costs and fails on long-horizon tasks (Section 3). Hybrid adds latency but guarantees formal correctness.
  - Search-based methods (ToT, GoT) vs. direct translation: Search explores more solution space but incurs exponential computational cost; translation is faster but may miss edge cases.
  - Fine-tuning vs. in-context learning: Fine-tuning improves performance on in-distribution tasks but generalizes poorly; in-context learning requires no training data but depends heavily on prompt engineering.
- Failure signatures:
  - **Long-horizon degradation**: Plans become incoherent or incomplete as task depth increases (Section 3).
  - **Arbitrarily poor plan costs**: Plan succeeds but uses far more steps than optimal (Section 1, 3).
  - **Translation semantic drift**: Generated PDDL compiles but does not match user intent (Section 5 on ambiguity).
  - **Unfaithful evaluation explanations**: LLM provides post-hoc justifications that do not reflect actual evaluation logic (Section 5).
- First 3 experiments:
  1. **Translation accuracy benchmark**: Measure PDDL generation quality on PlanBench or similar; compare exact match, executability, and goal achievement rates against ground-truth formalizations.
  2. **Horizon scaling test**: Evaluate standalone LLM planning vs. hybrid approach across progressively longer planning horizons; plot success rate and plan cost vs. horizon depth.
  3. **Evaluator consistency check**: Generate multiple equivalent plans for the same task; verify LLM evaluator produces consistent preference rankings across semantically identical solutions.

## Open Questions the Paper Calls Out
- How to improve LLM performance on long-horizon planning tasks while maintaining computational efficiency
- How to quantify and improve translation accuracy between natural language and formal planning languages
- How to ensure reliable commonsense knowledge augmentation across diverse domains
- How to make plan evaluation more consistent and explain explanations that are faithful to internal reasoning processes

## Limitations
- Limited empirical benchmarking data to support claims about LLM performance across different planning depths
- Weak corpus corroboration for specific mechanisms like commonsense knowledge augmentation and plan evaluation
- Anecdotal rather than systematic evidence for long-horizon planning failures
- Insufficient quantitative analysis of translation accuracy and semantic drift

## Confidence
- **High confidence**: The general framework for integrating LLMs with traditional planners is logically sound and methodologically coherent
- **Medium confidence**: Comparative advantages of hybrid approaches over standalone LLM planning are reasonably supported by existing literature
- **Low confidence**: Claims about reliability and effectiveness of individual mechanisms (translation accuracy, commonsense augmentation, plan evaluation) lack robust empirical validation

## Next Checks
1. **Translation accuracy benchmark**: Implement systematic evaluation of PDDL generation quality using PlanBench or similar datasets, measuring exact match rates, executability, and goal achievement against ground-truth formalizations.
2. **Horizon scaling experiment**: Design controlled experiments comparing standalone LLM planning vs. hybrid approaches across progressively longer planning horizons, measuring success rates and plan costs as functions of task depth.
3. **Evaluator consistency validation**: Generate multiple semantically equivalent plans for identical tasks and verify that LLM evaluators produce consistent preference rankings, testing both reliability and faithfulness of explanations.