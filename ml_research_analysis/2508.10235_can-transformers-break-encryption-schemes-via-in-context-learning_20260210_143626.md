---
ver: rpa2
title: Can Transformers Break Encryption Schemes via In-Context Learning?
arxiv_id: '2508.10235'
source_url: https://arxiv.org/abs/2508.10235
tags:
- learning
- in-context
- ciphers
- cipher
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformer models can learn to
  break classical encryption schemes via in-context learning (ICL). The authors frame
  the problem as learning to decrypt given ciphertext-plaintext pairs without parameter
  updates.
---

# Can Transformers Break Encryption Schemes via In-Context Learning?

## Quick Facts
- **arXiv ID**: 2508.10235
- **Source URL**: https://arxiv.org/abs/2508.10235
- **Reference count**: 10
- **Primary result**: Transformers can learn to break mono-alphabetic substitution ciphers via in-context learning, achieving high accuracy on both in-distribution and out-of-distribution text, but struggle with variable-length Vigenère ciphers.

## Executive Summary
This paper investigates whether transformer models can learn to break classical encryption schemes via in-context learning without parameter updates. The authors train GPT-2 models (9.5M parameters) to predict plaintext from ciphertext given example pairs in the prompt. The models successfully learn mono-alphabetic substitution ciphers, achieving high accuracy on both natural and random text, outperforming frequency-based baselines. For fixed-length Vigenère ciphers, models achieve 100% accuracy with 32 examples, but struggle with variable-length keys, reaching only ~70% accuracy due to periodicity generalization limitations.

## Method Summary
The authors train transformer models using OpenWebText corpus to learn decryption via in-context learning. They construct prompts with alternating (ciphertext, plaintext) pairs and train the model to predict the plaintext character following a given ciphertext sequence. For mono-alphabetic substitution, random permutation keys are used. For Vigenère ciphers, both fixed-length (32 characters) and variable-length keys are tested. The GPT-2 architecture uses 12 layers, 8 attention heads, and 256-dimensional embeddings with a custom 26-token vocabulary. Training uses AdamW optimizer with batch size 64 for 20k steps, weight decay of 0.1, and learning rate of 1e-3.

## Key Results
- Transformers achieve high accuracy on mono-alphabetic substitution ciphers, outperforming frequency-based and lookup baselines on both in-distribution and out-of-distribution (random) text
- Fixed-length Vigenère ciphers (key length 32) achieve 100% accuracy with 32 in-context examples
- Variable-length Vigenère ciphers plateau at ~70% accuracy even with maximum context, failing to generalize to unseen key lengths
- The model learns to infer substitution mappings rather than memorizing patterns, but shows structural limitations with increased complexity

## Why This Works (Mechanism)

### Mechanism 1: In-Context Substitution Mapping via Attention-Based Pattern Matching
- **Claim**: The transformer builds an internal representation of bijective character mappings by attending to alternating ciphertext-plaintext pairs in the prompt, without memorizing specific keys.
- **Mechanism**: Attention aggregates information across all (cipher, plain) pairs in context. When predicting m[j+1], the model conditions on all previous pairs, effectively constructing a substitution table in hidden states. The 26! key space makes memorization infeasible, forcing the model to learn the alternating pattern structure.
- **Core assumption**: The model learns the (cipher, plain) alternation format during training and generalizes this parsing to unseen keys.
- **Evidence anchors**:
  - [abstract]: "For mono-alphabetic substitution ciphers, the transformer achieves high accuracy on both in-distribution and out-of-distribution (uniformly random) text, outperforming frequency-based and lookup-based baselines."
  - [section 4.1]: "we find that the model learns to use the alternating (cipher, plain) pattern in the prompt to build an internal representation of the key, and apply it to unseen ciphertext characters during inference"
  - [corpus]: "ICL CIPHERS" (arxiv 2504.19395) studies ICL modes via substitution ciphers, supporting this as a recognized mechanism class.
- **Break condition**: Non-alternating prompt formats or key spaces requiring more than 26 distinct mappings may degrade performance.

### Mechanism 2: Statistical Prior Exploitation for Partial-Key Extrapolation
- **Claim**: The model leverages learned English letter frequency distributions to generalize when full key information is unavailable in context.
- **Mechanism**: Under partial context (fewer examples than key length for Vigenère), the model combines inferred partial offsets with statistical priors (e.g., 'e' frequency) to predict unseen positions, outperforming naive lookup baselines.
- **Core assumption**: OpenWebText pretraining provides sufficient signal to internalize English character distributions.
- **Evidence anchors**:
  - [section 5.1]: "when provided with fewer than 32 examples, it achieves significantly higher accuracy than the baselines, suggesting that it can partially infer or generalize key offsets"
  - [section 5.2]: "This contrast highlights that the model's strong performance under partial context in the natural language setting stems largely from its implicit knowledge of English letter frequencies"
  - [corpus]: "ALICE" (arxiv 2509.07282) shows interpretable architectures for cryptogram generalization, but no direct corpus evidence for this specific prior-exploitation mechanism in ICL.
- **Break condition**: Uniformly random plaintext removes linguistic priors; performance drops to baseline levels.

### Mechanism 3: Fixed-Periodicity Induction Bias
- **Claim**: Training on fixed-length keys induces a rigid periodicity expectation that limits generalization to different key lengths.
- **Mechanism**: The model learns position-specific representations modulo the training key length (e.g., 32). Shorter keys require multiple observation cycles; non-divisor lengths cause complete failure due to misaligned periodicity expectations.
- **Core assumption**: Positional encodings develop strong coupling to the training key period.
- **Evidence anchors**:
  - [section 5.3]: "On the length-16 distribution, the model eventually achieves high accuracy but only after observing 32 in-context examples. This is twice the number theoretically required"
  - [section 5.3]: "On the length-20 distribution, the model fails entirely to learn the key. Its predictions remain near chance"
  - [corpus]: Weak direct evidence; "Can Transformers Learn Full Bayesian Inference in Context?" (arxiv 2501.16825) discusses ICL limitations but not periodicity specifically.
- **Break condition**: Key lengths that don't evenly divide the training period; variable-length training still plateaus at ~70% (Figure 6).

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: The paper frames cipher breaking entirely as ICL—learning decryption from inference-time examples without gradient updates. Understanding Equation 1's loss over prompt prefixes is essential.
  - Quick check question: How does ICL differ from meta-learning, and what does the objective L(Mθ(P^j), m[j+1]) optimize?

- **Concept: Private-Key Encryption Formalism**
  - Why needed here: The prompt construction relies on the formal definition (message space M, key space K, Enc/Dec functions). Without this, the "correctness condition" Deck(Enck(m)) = m is opaque.
  - Quick check question: For a Vigenère cipher with key "ABC" (shifts 0,1,2), compute Enck("BAD").

- **Concept: Causal Attention Masking**
  - Why needed here: The model uses a causal mask so prediction at position j only attends to positions < j. This constrains what key information can be aggregated at each step.
  - Quick check question: In a 4-layer decoder, can position 10 attend to position 5's hidden state? Can position 5 attend to position 10?

## Architecture Onboarding

- **Component map**: Tokenizer (26-token vocab a-z) -> Embeddings (26x256) -> GPT-2 Decoder (12 layers, 8 heads, 256-dim) -> Output (softmax over 26 chars) -> Loss (only on plaintext positions)

- **Critical path**:
  1. Sample key k ~ Uniform(K), message m ~ DM (OpenWebText, lowercased, no punctuation/spaces)
  2. Compute ciphertext c = Enck(m)
  3. Build prompt with alternating pairs, masking loss on ciphertext tokens
  4. Single forward pass computes predictions for all prefixes simultaneously
  5. AdamW optimizer, batch size 64, lr=1e-3, weight decay 0.1, 20k steps

- **Design tradeoffs**:
  - Fixed vs. variable key length: Fixed achieves 100% on known lengths but fails OOD; variable training plateaus at 70%
  - Model scale: 9.5M may be insufficient—authors suggest scaling depth/width as future work
  - Context window: Longer contexts help fixed-length tasks but don't resolve variable-length limitations

- **Failure signatures**:
  - OOD key lengths: Length-32 model gets ~chance accuracy on length-20 keys
  - Variable-length Vigenère: Plateaus at ~70% even with 1535 context tokens
  - Uniform text + partial context: Performance matches naive baselines (prior exploitation disabled)

- **First 3 experiments**:
  1. **Mono-alphabetic baseline replication**: Train from scratch on OpenWebText with random permutation keys (1-100 in-context examples). Confirm accuracy exceeds lookup baseline on held-out keys.
  2. **OOD robustness test**: Evaluate mono-alphabetic model on uniformly random character sequences. Verify accuracy tracks frequency-based attack (Figure 2).
  3. **Periodicity ablation**: Train three fixed-length Vigenère models (key lengths 16, 24, 32). Cross-evaluate each on all three lengths to quantify periodicity overfitting.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can scaling model capacity or employing curriculum learning enable transformers to achieve perfect accuracy on variable-length Vigenère ciphers?
- **Basis in paper**: [explicit] Section 6.1 states "several directions remain open for exploration," specifically suggesting scaling model depth/width or curriculum learning to help the model "interpolate between key lengths."
- **Why unresolved**: The current 9.5M parameter model plateaus at ~70% accuracy on variable-length keys, failing to generalize over the structural ambiguity of different periodicities.
- **What evidence would resolve it**: Training larger models or using curriculum learning strategies on the variable-key task and observing if accuracy reaches 100% with reasonable context.

### Open Question 2
- **Question**: How does the in-context learning capability for decryption scale with model architecture and size?
- **Basis in paper**: [explicit] The Conclusion explicitly calls for "more investigation on how model size or architecture impacts performance."
- **Why unresolved**: The current study only evaluates a single GPT-2 architecture with 9.5M parameters, leaving the scaling laws for this specific structured inference task unknown.
- **What evidence would resolve it**: A systematic evaluation of varying model sizes (e.g., 125M to 7B parameters) and architectures (e.g., looped transformers) on the substitution and Vigenère tasks.

### Open Question 3
- **Question**: Can transformers be effectively integrated as assistants in classical cryptanalytic workflows?
- **Basis in paper**: [explicit] The Conclusion suggests future work should "consider how transformers might be used as assistants in classical cryptanalytic workflows."
- **Why unresolved**: The current work focuses on isolated decryption tasks via ICL but does not explore interactive or hybrid human-model workflows.
- **What evidence would resolve it**: Experiments where transformers augment or guide traditional cryptanalysis (e.g., identifying cipher types or suggesting partial keys) rather than solving the entire task independently.

## Limitations
- Variable-length Vigenère ciphers remain challenging, with models plateauing at ~70% accuracy despite extensive context and training
- The model's success relies heavily on exact key length matching from training, showing rigid periodicity expectations rather than general decryption principles
- Limited to 9.5M parameters, leaving open questions about scalability and whether larger models could overcome current limitations
- Focus on simplified cipher settings (no spaces, punctuation, or digits) limits real-world applicability

## Confidence
- **High confidence**: The transformer achieves superior performance on mono-alphabetic substitution ciphers compared to frequency-based and lookup baselines, both on in-distribution and out-of-distribution data. This is well-supported by quantitative results and multiple evaluation settings.
- **Medium confidence**: The model's success with fixed-length Vigenère ciphers (100% accuracy) is demonstrated, but this relies heavily on exact key length matching from training. The mechanism appears to be position-specific periodicity rather than general key inference.
- **Low confidence**: Claims about the model's ability to generalize to variable-length Vigenère ciphers are weak. The plateau at ~70% accuracy across extensive experiments suggests fundamental architectural limitations rather than optimization issues. The paper's framing of these results as "learning" is questionable given the failure to generalize.

## Next Checks
1. **Cross-length Vigenère evaluation**: Train fixed-length Vigenère models (key lengths 16, 24, 32) and systematically evaluate each on all three lengths plus intermediate values (20, 28). This would definitively test whether the model learns position-specific periodicity versus general Vigenère decryption.
2. **Statistical prior ablation**: Train models on both natural language and uniformly random character sequences with varying context sizes. Compare performance to isolate the contribution of English letter frequency priors versus learned cipher structure inference.
3. **Architecture scaling study**: Replicate mono-alphabetic and fixed-length Vigenère experiments with models of 50M, 200M, and 500M parameters. Measure accuracy improvements and test whether variable-length Vigenère performance increases beyond the 70% plateau, establishing whether this represents a fundamental ICL limitation or simply insufficient model capacity.