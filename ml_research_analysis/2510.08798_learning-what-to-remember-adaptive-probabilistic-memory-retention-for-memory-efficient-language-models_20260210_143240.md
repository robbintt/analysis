---
ver: rpa2
title: 'Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient
  Language Models'
arxiv_id: '2510.08798'
source_url: https://arxiv.org/abs/2510.08798
tags:
- arxiv
- retention
- memory
- https
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Adaptive Retention is a probabilistic token selection method for
  memory-efficient language models. It learns per-token retention probabilities using
  a lightweight gating network trained via Hard-Concrete relaxation, with a Lagrange
  multiplier enforcing a strict global budget.
---

# Learning What to Remember: Adaptive Probabilistic Memory Retention for Memory-Efficient Language Models

## Quick Facts
- **arXiv ID**: 2510.08798
- **Source URL**: https://arxiv.org/abs/2510.08798
- **Reference count**: 40
- **Primary result**: 30-50% token retention preserves ≥95% accuracy while reducing peak memory by 35-45% and improving throughput up to 1.8×

## Executive Summary
This paper introduces Adaptive Retention, a probabilistic token selection method that learns per-token retention probabilities for memory-efficient language models. The approach uses a lightweight gating network trained via Hard-Concrete relaxation with a Lagrange multiplier enforcing strict global budget constraints. During inference, only the top-M tokens are retained, reducing computation and memory without modifying base attention mechanisms. Extensive evaluation on six benchmarks shows that keeping 30-50% of tokens maintains ≥95% of full-model accuracy while achieving 35-45% peak memory reduction and up to 1.8× throughput improvement. The method outperforms heuristic pruning and matches sparse-attention models, offering practical long-context efficiency with minimal accuracy loss.

## Method Summary
Adaptive Retention learns token-specific retention probabilities through a lightweight gating network that conditions on token embeddings. The gating network parameters are trained using Hard-Concrete relaxation, which allows gradient-based optimization while maintaining discrete selection at inference time. A Lagrange multiplier enforces a strict global budget on the number of retained tokens, preventing over-pruning. At inference, the system ranks tokens by their learned retention scores and keeps only the top-M tokens based on the budget constraint. This approach integrates seamlessly with existing attention mechanisms without requiring architectural modifications, making it applicable across different transformer variants.

## Key Results
- 30-50% token retention preserves ≥95% of full-model accuracy across six benchmarks
- Achieves 35-45% peak memory reduction through selective token retention
- Improves throughput by up to 1.8× compared to baseline models
- Outperforms heuristic pruning methods and matches sparse-attention baselines

## Why This Works (Mechanism)
The method works by learning token-specific importance scores that reflect each token's contribution to downstream task performance. Hard-Concrete relaxation enables differentiable approximation of discrete selection, allowing the gating network to be trained end-to-end with the base model. The Lagrange multiplier formulation provides a principled way to enforce global budget constraints while maintaining gradient flow. By ranking and selecting only the most informative tokens, the approach reduces both computational complexity and memory footprint without sacrificing representational capacity for the retained information.

## Foundational Learning
- **Hard-Concrete relaxation**: A continuous relaxation of discrete random variables that enables gradient-based optimization; needed to train the gating network end-to-end; quick check: verify gradients flow through relaxed samples
- **Lagrange multiplier optimization**: A constrained optimization technique for enforcing budget constraints; needed to maintain strict token retention limits; quick check: confirm dual variable converges during training
- **Transformer attention mechanisms**: The self-attention operation that scales quadratically with sequence length; needed as the target for optimization; quick check: verify attention scores match expected patterns
- **KV cache management**: Efficient storage of key-value pairs during autoregressive generation; needed to realize memory savings; quick check: measure cache size reduction empirically
- **Sparse attention patterns**: Techniques for reducing attention computation by focusing on subsets of tokens; needed for performance comparison; quick check: verify attention sparsity matches implementation

## Architecture Onboarding
- **Component map**: Input tokens -> Gating network -> Retention scores -> Top-M selection -> Attention computation -> Output
- **Critical path**: Token embeddings flow through gating network to produce retention scores, which are ranked to select top-M tokens for attention computation
- **Design tradeoffs**: Probabilistic selection vs. deterministic heuristics (better adaptability vs. simplicity); Hard-Concrete vs. other relaxations (gradient quality vs. implementation complexity); Lagrange multiplier vs. heuristic budgeting (principled vs. empirical tuning)
- **Failure signatures**: Over-pruning leading to accuracy collapse; under-pruning negating memory benefits; unstable Lagrange multiplier causing budget violations; poor gating network generalization to unseen data
- **3 first experiments**: 1) Ablation study removing Hard-Concrete relaxation to test its necessity; 2) Training with fixed vs. adaptive budget to evaluate Lagrange multiplier effectiveness; 3) Cross-dataset evaluation to assess generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Method evaluated only on encoder-decoder transformer variants; decoder-only architecture performance unverified
- Lagrange multiplier requires per-task tuning and assumes stable training dynamics
- Hard-Concrete relaxation introduces stochasticity that may affect reproducibility across hardware seeds
- Effectiveness at extremely long sequences (>4096 tokens) and in multi-lingual settings unexplored
- Memory savings depend on KV cache patterns and may not translate to all downstream applications

## Confidence
- **High confidence**: Memory savings claims (35-45% peak memory reduction) and throughput improvements (up to 1.8×) are well-supported by ablation studies across six benchmarks
- **Medium confidence**: Accuracy preservation (≥95% at 30-50% retention) relies on fixed thresholds that may require re-tuning for different datasets or model sizes
- **Medium confidence**: Comparison to sparse-attention baselines is methodologically sound but limited to a narrow set of competing approaches

## Next Checks
1. Evaluate Adaptive Retention on decoder-only models (e.g., LLaMA, GPT-Neo) to confirm cross-architecture applicability
2. Test robustness across diverse language families and low-resource datasets to assess generalization beyond English-centric benchmarks
3. Conduct ablation studies on Lagrange multiplier sensitivity and Hard-Concrete temperature parameters to determine optimal configurations for different sequence lengths and model scales