---
ver: rpa2
title: 'ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced
  Legal Regulation'
arxiv_id: '2511.03563'
source_url: https://arxiv.org/abs/2511.03563
tags:
- legal
- arxiv
- language
- dataset
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study fine-tuned Large Language Models (LLMs) to enhance their
  ability to understand, analyze, and draft legal regulations. The approach involved
  training LLMs with a supervised dataset of legal instruction-output pairs and integrating
  Retrieval-Augmented Generation (RAG) to access current legal knowledge.
---

# ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation

## Quick Facts
- arXiv ID: 2511.03563
- Source URL: https://arxiv.org/abs/2511.03563
- Reference count: 40
- Primary result: Fine-tuned models achieved BLEU 0.13-0.15 and METEOR 0.34-0.37, outperforming baselines but trailing commercial models like GPT-3.5 Turbo (BLEU 0.24)

## Executive Summary
This study fine-tuned Large Language Models (LLMs) to enhance their ability to understand, analyze, and draft legal regulations. The approach involved training LLMs with a supervised dataset of legal instruction-output pairs and integrating Retrieval-Augmented Generation (RAG) to access current legal knowledge. Two models—ASVRI-Legal-Llama 2 and ASVRI-Legal-WizardLM—were developed and evaluated against baseline LLMs and commercial models. Quantitative results showed BLEU scores of 0.13 and 0.15 and METEOR scores of 0.34 and 0.37, respectively, outperforming baseline models but still trailing larger commercial models like GPT-3.5 Turbo and GPT-4. Qualitative assessment indicated improved legal coherence, yet limitations in complex legal tasks persisted. The study concludes that while the fine-tuned models are promising tools for legal professionals, further refinement is needed to achieve full practical utility.

## Method Summary
The study employed supervised fine-tuning with parameter-efficient methods (PEFT/LoRA) on LLaMA2-7B and WizardLM-13B models using a dataset of 8,507 instruction-output pairs generated via GPT-3.5-turbo from Indonesian government regulations on educational standards. The fine-tuning process used 3 epochs with batch sizes of 2 and 1 respectively, and token limits of 2048/1024 for source/target. A RAG pipeline was implemented with vector database storage and top-k retrieval, combining retrieved context with user queries and system prompts for final generation.

## Key Results
- ASVRI-Legal-WizardLM achieved BLEU score of 0.15 and METEOR of 0.37
- ASVRI-Legal-Llama 2 achieved BLEU score of 0.13 and METEOR of 0.34
- Both models outperformed baseline LLMs but trailed GPT-3.5 Turbo (BLEU 0.24) and GPT-4

## Why This Works (Mechanism)
The approach works by combining domain-specific knowledge through supervised fine-tuning with dynamic knowledge access via RAG. Fine-tuning adapts general LLMs to legal domain tasks using instruction-output pairs, while RAG mitigates hallucination by retrieving relevant legal documents at inference time. The PEFT/LoRA method enables efficient adaptation without full fine-tuning, making the approach computationally feasible.

## Foundational Learning
- **Retrieval-Augmented Generation (RAG)**: Why needed here: Allows static models to access dynamic, external legal knowledge, addressing outdated information and hallucination. Quick check: How does providing retrieved documents to an LLM at inference time differ from the knowledge it learned during pre-training or fine-tuning?
- **Parameter-Efficient Fine-Tuning (PEFT) & LoRA**: Why needed here: Adapts large models (7B/13B parameters) to legal domain without prohibitive retraining costs. Quick check: What are the two main problems with full fine-tuning that PEFT methods like LoRA are designed to solve? (Hint: One is computational, the other relates to the model's existing knowledge)
- **N-gram Based Evaluation Metrics (BLEU/METEOR)**: Why needed here: Primary quantitative tools for judging model success, so understanding their limitations is crucial. Quick check: A model achieves high BLEU score but summary is legally nonsensical. What key limitation of BLEU explains this?

## Architecture Onboarding

### Component Map
Training Subsystem: Data Curation -> Fine-Tuning Engine -> Specialized Model Weights
Inference Subsystem: Retrieval-Augmented Generation -> Augmented Generation Engine -> Final Response

### Critical Path
The success depends on the SFT Dataset Generation -> Fine-Tuning -> RAG Context Injection pipeline. The most critical step is the quality of generated instruction-output pairs, as this data fundamentally shapes the model's legal reasoning capabilities.

### Design Tradeoffs
- PEFT/LoRA vs. Full Fine-Tuning: PEFT reduces computational cost and catastrophic forgetting risk but may yield lower performance on complex tasks
- Smaller Specialized Models vs. Large General-Purpose Models: Fine-tuning smaller, open-source models provides cost and data privacy benefits versus raw performance (GPT-3.5/4 significantly outperform smaller models)
- Evaluation Metrics: BLEU/METEOR provide standardized benchmarks but may not fully capture legal reasoning intricacies

### Failure Signatures
- Poor Retrieval Relevance: RAG returns keyword-similar but legally irrelevant documents, causing contextually inappropriate answers
- Catastrophic Forgetting/Overfitting: Model loses general reasoning ability or becomes too rigid for queries outside training distribution
- Hallucination: Despite RAG, model generates plausible but legally incorrect statements, especially with ambiguous context

### First 3 Experiments
1. Baseline vs. RAG vs. Fine-Tuning Ablation: Evaluate base LLaMA-2 model, model with only RAG, and model with only fine-tuning to isolate component contributions
2. Dataset Quality Sensitivity Analysis: Fine-tune on smaller, higher-quality expert-reviewed subset of training data and compare performance to full synthetic set model
3. Retrieval Top-k Impact: Vary RAG retriever k parameter (k=1, 3, 5, 10) and evaluate effects on metrics and qualitative legal coherence

## Open Questions the Paper Calls Out
- How can evaluation metrics be specifically designed to capture semantic nuances and logical consistency of legal text generation better than standard n-gram overlap? The authors note BLEU and METEOR may not fully capture legal text intricacies and suggest future work should incorporate metrics specifically designed for legal text.
- What specific architectural enhancements or training strategies allow smaller, domain-specific LLMs (7B-13B parameters) to outperform generalist giants like GPT-4 in legal reasoning tasks? The Conclusion notes specialized models still fall short of GPT-3.5 and GPT-4, necessitating future focus on enhancing model architectures.
- Does relying on synthetic data generated by general-purpose LLMs (e.g., GPT-3.5) for supervised fine-tuning limit the model's ability to handle nuanced legal interpretation compared to expert-curated data? The authors used GPT-3.5 to generate their instruction dataset, but Related Works notes that "Lawyer LLaMA" found expert-written data more effective than model-generated data.

## Limitations
- Performance improvements remain modest (BLEU: 0.13-0.15; METEOR: 0.34-0.37) and test set size (20% of 8,507 examples) limits generalizability
- Synthetic data generation process using GPT-3.5-turbo introduces potential quality variability that isn't quantified
- Insufficient hyperparameter details provided for exact reproduction and training stability metrics not reported

## Confidence
- High Confidence: RAG integration architecture is sound and quantitative improvement over baseline models is reproducible
- Medium Confidence: Qualitative assessment of improved legal coherence, though promising, relies on subjective evaluation
- Low Confidence: Claims about practical utility for legal professionals given persistent performance gap with commercial models

## Next Checks
1. Ablation Study Replication: Isolate RAG contribution by testing base LLM with and without RAG on same held-out set to verify reported 0.05-0.07 BLEU improvement
2. Cross-Domain Generalization: Evaluate fine-tuned models on legal text from jurisdictions outside Indonesia to test generalizability beyond PSKP domain
3. Expert Legal Review: Have practicing legal professionals assess whether model's responses meet professional standards for accuracy and completeness on complex regulatory scenarios