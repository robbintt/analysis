---
ver: rpa2
title: Enhancing Performance and Calibration in Quantile Hyperparameter Optimization
arxiv_id: '2509.17051'
source_url: https://arxiv.org/abs/2509.17051
tags:
- quantile
- performance
- hyperparameter
- search
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates enhancing conformalized quantile hyperparameter
  optimization (HPO) through acquisition function diversification, expanded surrogate
  architectures, and improved conformal calibration techniques. Key contributions
  include benchmarking quantile regression with adaptive conformal intervals (DtACI)
  and sample-efficient cross-validation (CV+), exploring additional acquisition functions
  like Expected Improvement and Optimistic Bayesian Sampling, and introducing novel
  surrogate architectures including quantile ensembles and Gaussian Processes.
---

# Enhancing Performance and Calibration in Quantile Hyperparameter Optimization

## Quick Facts
- arXiv ID: 2509.17051
- Source URL: https://arxiv.org/abs/2509.17051
- Authors: Riccardo Doyle
- Reference count: 35
- Primary result: Ensemble quantile models with Optimistic Bayesian Sampling achieve state-of-the-art performance in hyperparameter optimization across diverse datasets

## Executive Summary
This study advances conformalized quantile hyperparameter optimization (HPO) through acquisition function diversification, expanded surrogate architectures, and improved conformal calibration techniques. The research benchmarks novel quantile regression methods including adaptive conformal intervals and sample-efficient cross-validation, while exploring additional acquisition functions like Expected Improvement and Optimistic Bayesian Sampling. Novel surrogate architectures including quantile ensembles and Gaussian Processes are introduced. Experiments demonstrate that ensemble quantile models with Optimistic Bayesian Sampling significantly outperform traditional methods like Gaussian Processes, Tree Parzen Estimators, and SMAC, particularly in challenging heteroskedastic and asymmetric settings.

## Method Summary
The study enhances conformalized quantile HPO through three main innovations: acquisition function diversification (adding Expected Improvement and Optimistic Bayesian Sampling to existing Thompson Sampling), expanded surrogate architectures (introducing quantile ensembles and Gaussian Processes), and improved conformal calibration techniques. The research benchmarks adaptive conformal intervals (DtACI) and sample-efficient cross-validation (CV+) for quantile regression. Experiments span diverse tabular and image datasets, comparing performance against traditional HPO methods including Gaussian Processes, Tree Parzen Estimators, and SMAC. The methodology emphasizes robustness to non-normal, heteroskedastic, and categorical hyperparameter spaces while maintaining strong calibration properties.

## Key Results
- Ensemble quantile models (QE) with Optimistic Bayesian Sampling achieve state-of-the-art performance
- Significant improvements over Gaussian Processes, Tree Parzen Estimators, and SMAC
- Conformalization provides strong calibration benefits under Expected Improvement but mixed results under Thompson Sampling
- Methods demonstrate robustness in heteroskedastic and asymmetric settings

## Why This Works (Mechanism)
The success stems from combining ensemble methods' ability to capture diverse predictive patterns with Optimistic Bayesian Sampling's exploration-exploitation balance. Quantile ensembles reduce variance by aggregating multiple models' predictions, while conformal calibration ensures reliable uncertainty estimates. The diversification of acquisition functions prevents premature convergence to suboptimal solutions by maintaining exploration in challenging search spaces.

## Foundational Learning
- Quantile Regression: Models conditional quantiles rather than mean predictions, crucial for capturing asymmetric error distributions and heteroskedasticity in HPO landscapes
- Conformal Prediction: Provides distribution-free uncertainty quantification, essential for reliable exploration in unknown hyperparameter spaces
- Bayesian Optimization: Framework for sequential model-based optimization, where acquisition functions balance exploration-exploitation trade-offs
- Thompson Sampling: Probabilistic acquisition function that samples from posterior, effective but potentially unstable without proper calibration
- Expected Improvement: Acquisition function that explicitly optimizes expected performance gain, benefiting from conformal calibration
- Optimistic Bayesian Sampling: Enhanced Thompson Sampling variant that maintains exploration while improving stability

## Architecture Onboarding
**Component Map:** Data → Surrogate Model (Quantile Ensemble/GP) → Conformal Calibration → Acquisition Function (EI/Obs/TS) → Candidate Selection → Evaluation → Feedback Loop

**Critical Path:** The surrogate model's prediction quality directly determines acquisition function effectiveness, which drives the search efficiency. Conformal calibration quality impacts the reliability of uncertainty estimates used in acquisition decisions.

**Design Tradeoffs:** Ensemble methods provide robustness but increase computational overhead versus single-model approaches. Conformal calibration improves reliability but requires additional validation data, potentially reducing budget for actual optimization.

**Failure Signatures:** Poor surrogate accuracy leads to suboptimal acquisition decisions. Inadequate conformal calibration results in over/underestimated uncertainties, causing premature convergence or excessive exploration. Acquisition function instability manifests as oscillating or non-convergent search behavior.

**First Experiments:**
1. Compare single quantile model versus ensemble performance on simple benchmark functions
2. Evaluate acquisition function stability across different hyperparameter distributions
3. Test conformal calibration effectiveness on known heteroskedastic problems

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation focused primarily on tabular and image datasets with continuous hyperparameters
- Computational overhead of ensemble methods and conformal calibration may limit scalability
- Limited testing on discrete/categorical hyperparameter spaces

## Confidence
- High confidence in QE+OBS outperforming baseline methods on benchmark datasets
- Medium confidence in calibration benefits under Expected Improvement, given mixed results across experimental settings
- Medium confidence in conformalization robustness for non-normal distributions, as heteroskedastic behavior was tested but not extensively characterized

## Next Checks
1. Test ensemble quantile methods on real-world HPO problems with large hyperparameter spaces and mixed (continuous/discrete) parameter types
2. Conduct ablation studies isolating the contribution of conformal calibration versus acquisition function diversification to overall performance gains
3. Evaluate computational efficiency trade-offs between single-model quantile approaches and ensemble methods across varying budget constraints