---
ver: rpa2
title: Statistical Inference for Online Algorithms
arxiv_id: '2505.17300'
source_url: https://arxiv.org/abs/2505.17300
tags:
- asgd
- coverage
- hulc
- regression
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of constructing confidence intervals
  for parameters estimated via online algorithms like stochastic gradient descent
  (SGD), where the standard Wald interval is computationally infeasible due to data
  access limitations. Existing approaches rely on online variance estimation, which
  is computationally expensive or memory-intensive.
---

# Statistical Inference for Online Algorithms

## Quick Facts
- **arXiv ID:** 2505.17300
- **Source URL:** https://arxiv.org/abs/2505.17300
- **Reference count:** 40
- **Primary result:** HulC method constructs valid confidence intervals for online algorithms without variance estimation by dividing data into buckets and computing min/max across independent estimates

## Executive Summary
This paper addresses the fundamental challenge of constructing confidence intervals for parameters estimated via online algorithms like stochastic gradient descent (SGD) when data access is limited to sequential observations. Standard Wald intervals require storing intermediate quantities for variance estimation, which is computationally infeasible in streaming settings. The authors propose the HulC method, which divides data into independent buckets and computes parameter estimates on each bucket separately, then constructs confidence intervals by taking the minimum and maximum across buckets. This approach requires no variance estimation and has minimal computational overhead. Theoretical analysis shows that if the online estimator is asymptotically normal, HulC intervals achieve correct coverage with only a modest width penalty compared to optimal Wald intervals.

## Method Summary
The HulC method constructs confidence intervals by splitting streaming data into B* = ⌈log₂(2/α)⌉ buckets and running the online algorithm independently on each bucket. For each bucket j, the algorithm computes θ̄(j)_T using data points at positions j, j+B*, j+2B*, etc. The final confidence interval is the convex hull (min/max) of these B* estimates. The method requires no variance estimation and works under the assumption that bucket estimates are approximately independent and asymptotically normal. In simulations on linear and logistic regression, HulC achieves correct coverage while maintaining comparable width to alternative methods like the t-statistic approach, though it is more sensitive to step-size hyperparameter tuning than some alternatives.

## Key Results
- HulC confidence intervals achieve correct coverage (within statistical uncertainty) compared to target 95% levels
- Width ratios: HulC intervals are typically 1.6-2.5× wider than Wald intervals, within 0.3 ratio difference of t-statistic method
- ASGD plug-in variance method consistently undercovers across all tested configurations
- The valid range of step-size hyperparameter c (the "basin of attraction") narrows as dimension increases, especially for logistic regression
- HulC requires no stored intermediate quantities, making it computationally efficient for streaming data

## Why This Works (Mechanism)

### Mechanism 1: Bucket-based Independent Estimation Eliminates Variance Requirements
Dividing data into independent buckets and computing estimates separately allows confidence interval construction without variance estimation. The HulC method splits streaming data into B* buckets (e.g., B* ≈ 5-6 for 95% CI) with each bucket running the online algorithm independently on non-overlapping data points. The confidence interval is the convex hull (min/max) of these B* estimates. This works when bucket estimates are approximately independent and identically distributed with median-bias close to zero.

### Mechanism 2: Asymptotic Normality Provides Coverage Guarantees
If the underlying online estimator converges to a normal distribution, the HulC interval achieves the nominal coverage rate (1−α). Theorem 1 bounds coverage error in terms of maximum median-bias ∆_T across buckets. When ∆_T → 0 (which occurs under asymptotic normality or symmetry), P(θ∞ ∉ CI_T,α) → α. The expansion in Theorem 2 shows √T(θ̄_T − θ∞) ≈ T^(−1/2) Σ J^(−1)ξ_t + remainder, where ξ_t forms a martingale difference sequence → CLT applies.

### Mechanism 3: Width Penalty is Bounded Relative to Wald Intervals
HulC intervals are wider than optimal Wald intervals by only a constant factor ≈ √(2 log log₂(1/α)) ≈ 1.4 for α=0.05. Since HulC uses min/max over B* ≈ log₂(2/α) estimates rather than a variance-scaled mean, the interval width scales with the spread of individual estimates rather than their standard error. This adds a logarithmic penalty in (1/α) but preserves the same convergence rate.

## Foundational Learning

- **Concept: Stochastic Gradient Descent with Polyak-Ruppert Averaging**
  - **Why needed here:** The paper's primary application is ASGD; understanding how θ(t) = θ(t−1) − η_t∇ℓ(Z_t; θ(t−1)) and θ̄_T = (1/T)Σθ(t) produce asymptotically normal estimators is essential.
  - **Quick check question:** Can you explain why averaging iterates (rather than using the final iterate) improves asymptotic efficiency?

- **Concept: Martingale Central Limit Theorem**
  - **Why needed here:** The proof of asymptotic normality (Theorem 2) relies on recognizing that {ξ_t} forms a martingale difference sequence, enabling CLT application without independence.
  - **Quick check question:** If E[ξ_t | Z₁,...,Z_{t−1}] = 0, why does this matter for inference on streaming data?

- **Concept: Median Bias vs. Mean Bias**
  - **Why needed here:** HulC's validity depends on median-bias (∆_T = max|P(θ̂ > θ∞) − 0.5|), not mean-bias. Asymmetric sampling distributions are acceptable if median-unbiased.
  - **Quick check question:** For a skewed estimator, why might median-unbiasedness be easier to verify than mean-unbiasedness?

## Architecture Onboarding

- **Component map:** Streaming Data (Z₁, Z₂, ...) → Bucket Router (assigns Z_t to bucket j if t mod B* = j) → B* Parallel ASGD Instances (each sees ~T/B* samples) → Bucket Estimates {θ̄(1), ..., θ̄(B*)} → Confidence Interval: [min_k θ̄(k), max_k θ̄(k)]

- **Critical path:**
  1. **Bucket count selection:** B* = ⌈log₂(2/α)⌉ (5 for 90% CI, 6 for 95% CI)
  2. **Step-size tuning:** η_t = c·t^(−γ) with γ ∈ (0.5, 1); c is critical and problem-dependent
  3. **Initialization:** Each bucket can use different θ(0,j); warm-start with constant-step SGD for ⌊T/3⌋ iterations recommended
  4. **Aggregation:** Take elementwise min/max across bucket estimates for each coordinate

- **Design tradeoffs:**
  - **Fewer buckets (smaller B*):** Tighter intervals but requires stronger normality assumptions
  - **More buckets:** More robust to non-normality but wider intervals
  - **Larger step-size c:** Faster convergence per bucket but narrower "basin of attraction" (hyperparameter sensitivity)
  - **Memory vs. computation:** HulC requires no stored intermediate quantities; alternative methods (plug-in variance) require O(d²) memory for covariance tracking

- **Failure signatures:**
  - **Consistent undercoverage (all coordinates):** Step-size c too small (slow convergence) or too large (divergence)
  - **Coverage varying wildly by coordinate:** Initialization bias; warm-start with fixed-step SGD
  - **Width ratios >> 2.5× Wald:** Non-convergence or extremely high variance in bucket estimates
  - **In high dimensions (d ≥ 100):** "Basin of attraction" for c becomes very narrow; systematic grid search required

- **First 3 experiments:**
  1. **Sanity check on linear regression with d=5, T=10⁴, identity covariance:** Set c ∈ {0.1, 0.5, 1.0}, γ=0.505. Verify HulC coverage ≈ 95% and width ratio < 2× Wald. This establishes baseline performance.
  2. **Step-size sensitivity sweep:** For d ∈ {5, 20, 100} and c ∈ {0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1.0, 2.0}, plot coverage vs. c. Identify the "basin of attraction" range for each dimension.
  3. **Comparison with t-statistic and plug-in methods:** On logistic regression (d=100, Toeplitz covariance, T=10⁵), compare all four methods. Confirm: (a) plug-in undercovers; (b) t-stat and HulC achieve similar coverage; (c) HulC width within 0.3 of t-stat. Note: Wald may fail due to ill-conditioning.

## Open Questions the Paper Calls Out

- **Open Question 1:** What are principled, data-driven methods for selecting the step-size hyperparameter c in ASGD that ensure convergence and valid inference across varying dimensions and model classes?
  - **Basis in paper:** [explicit] The authors state "Due to the lack of enough resources on the 'right' way to choose c, we present our results over a range of c values" and observe empirically that the "basin of attraction" for c narrows as dimension increases, especially for logistic regression.
  - **Why unresolved:** Current practice relies on grid search without theoretical guidance; the paper shows coverage and width are highly sensitive to c, yet no adaptive selection rule is proposed.
  - **What evidence would resolve it:** Theoretical analysis characterizing the valid range of c as a function of dimension, condition number, and sample size; or an online algorithm that adaptively tunes c with provable guarantees.

- **Open Question 2:** Can SGD variants using local non-quadratic expansions of the loss function alleviate the poor practical performance of standard ASGD while maintaining asymptotic normality required for HulC-based inference?
  - **Basis in paper:** [explicit] "SGD iterations are derived using a local quadratic expansion of the loss function, and some authors have proposed a local non-quadratic expansion to alleviate the poor performance issues. The exploration of such variants of SGD is beyond the scope of the current manuscript and would be an interesting research topic to consider in the future."
  - **Why unresolved:** The theoretical framework (Theorem 2) relies on quadratic approximations; non-quadratic expansions may change the asymptotic distribution form.
  - **What evidence would resolve it:** Derivation of asymptotic expansions for non-quadratic SGD variants; empirical demonstration that HulC achieves correct coverage with such variants.

- **Open Question 3:** Do the empirical observations about the "basin of attraction" for hyperparameter c have theoretical justification, particularly regarding how it narrows with increasing dimension and differs between linear and logistic regression?
  - **Basis in paper:** [explicit] "This basin of attraction becomes narrower as the dimension d increases, and is narrower for logistic regression than for linear regression. We do not know of any theoretical results supporting these empirical observations."
  - **Why unresolved:** The simulations reveal systematic patterns that existing convergence theory does not predict or explain.
  - **What evidence would resolve it:** Theoretical analysis linking problem geometry (condition number, loss landscape curvature) to valid hyperparameter ranges; finite-sample bounds characterizing the c-region where bias remains negligible.

## Limitations

- The method requires careful tuning of the step-size hyperparameter c, with the valid range (basin of attraction) narrowing as dimension increases
- Performance depends on achieving asymptotic normality of the online estimator, which may not hold for non-convex losses without PL-conditions
- Width ratios can exceed theoretical bounds if bucket estimates have high variance due to small per-bucket sample sizes
- The paper does not provide principled, data-driven methods for selecting c, relying instead on grid search

## Confidence

- **Theoretical validity of HulC method:** High - Theorems 1 and 2 provide rigorous coverage guarantees under well-specified conditions
- **Empirical coverage results:** Medium - Simulations show correct coverage, but hyperparameter sensitivity may limit practical applicability
- **Width ratio claims:** Medium - Theoretical bounds suggest √(2 log log₂(1/α)) factor, but empirical ratios show higher variability
- **Step-size sensitivity observations:** High - Extensive simulations demonstrate systematic narrowing of valid c-range with increasing dimension

## Next Checks

1. Reproduce the linear regression sanity check (d=5, T=10⁴, identity covariance) to verify HulC achieves ~95% coverage and width ratio < 2× Wald
2. Conduct step-size sensitivity sweep across dimensions d ∈ {5, 20, 100} to identify and characterize the "basin of attraction" for c
3. Compare HulC against t-statistic and plug-in methods on logistic regression (d=100, Toeplitz covariance, T=10⁵) to confirm coverage patterns and width relationships