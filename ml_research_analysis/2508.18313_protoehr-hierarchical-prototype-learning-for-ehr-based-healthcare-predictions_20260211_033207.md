---
ver: rpa2
title: 'ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions'
arxiv_id: '2508.18313'
source_url: https://arxiv.org/abs/2508.18313
tags:
- medical
- prediction
- patient
- prototype
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ProtoEHR is a hierarchical prototype learning framework for healthcare
  prediction using electronic health records (EHRs). It integrates medical knowledge
  graphs and prototype learning to capture both within-level similarities and cross-level
  hierarchies in EHR data.
---

# ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions

## Quick Facts
- arXiv ID: 2508.18313
- Source URL: https://arxiv.org/abs/2508.18313
- Authors: Zi Cai; Yu Liu; Zhiyao Luo; Tingting Zhu
- Reference count: 40
- One-line primary result: ProtoEHR consistently outperforms state-of-the-art baselines on MIMIC-III and MIMIC-IV for five clinically significant tasks.

## Executive Summary
ProtoEHR introduces a hierarchical prototype learning framework that leverages large language models to construct medical knowledge graphs for electronic health record (EHR)-based healthcare predictions. The framework integrates medical knowledge graphs and prototype learning to capture both within-level similarities and cross-level hierarchies in EHR data, resulting in superior accuracy and interpretability compared to existing methods.

## Method Summary
ProtoEHR employs a three-stage pipeline to construct a medical knowledge graph using open-source LLM (Llama3-70B) for retrieval, closed-source LLM (GPT-4) for cleaning, and agglomerative clustering for refinement. The framework then uses a hierarchical structure with prototype-based encoders at code, visit, and patient levels, where cross-attention mechanisms update learnable prototypes and perform prototype infusion. A task-aware hierarchical fusion layer with gating weights determines the relative importance of each hierarchical level for specific prediction tasks.

## Key Results
- ProtoEHR consistently outperforms state-of-the-art baselines on MIMIC-III and MIMIC-IV datasets
- Achieves significant performance gains across five clinically significant tasks: mortality prediction, readmission prediction, length of stay prediction, drug recommendation, and phenotype classification
- Demonstrates superior accuracy and interpretability through hierarchical fusion weights that align with clinical expectations

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Prototype Infusion
Capturing intrinsic similarities within hierarchical levels (code, visit, patient) via learnable prototypes enhances representation generalization compared to instance-only learning. The model employs a "Prototype-based Encoder" at each level, using cross-attention to update "learnable prototypes" based on batch inputs, then performing "prototype infusion" where individual representations are augmented by a similarity-weighted sum of these enhanced prototypes.

### Mechanism 2: LLM-Enhanced Knowledge Graph Construction
Utilizing a hybrid LLM pipeline to extract semantic relationships creates a more expressive Medical Knowledge Graph than static ontologies, directly improving code-level representations. The authors use a three-stage pipeline: Open-source LLM (Llama3-70B) retrieves triplets → Closed-source LLM (GPT-4) filters/cleans → Agglomerative clustering refines relations.

### Mechanism 3: Task-Aware Hierarchical Fusion
Different prediction tasks rely differently on the code, visit, and patient hierarchies; explicitly learning these weights improves accuracy and interpretability. The final patient representation is a fusion of the patient embedding and learned prototypes from all three levels, with a gating mechanism learning contribution weights for each level.

## Foundational Learning

**Concept: Graph Convolutional Networks (CompGCN)**
- **Why needed here:** Used to process the constructed Medical KG, updating medical code embeddings by aggregating information from neighbor entities and relations
- **Quick check question:** Can you explain how Eq. (1) uses the circular correlation φ to combine entity and relation embeddings?

**Concept: Cross-Attention Mechanisms**
- **Why needed here:** Central to the Prototype-based Encoder, allowing the model to query input objects to update prototypes and vice-versa during infusion
- **Quick check question:** How does the Softmax normalization in Eq. (3) differ from the similarity calculation in Eq. (4)?

**Concept: Transformer Encoders**
- **Why needed here:** Used at the Patient Level to model the temporal sequence of visits, capturing dependencies between visits better than simple RNNs for long sequences
- **Quick check question:** Why does the model use only the representation of the *last* visit after the Transformer encoding for the patient summary?

## Architecture Onboarding

**Component map:**
1. Input: Medical Codes (diagnosis, procedure, drug)
2. KG Builder: Llama3 (Retrieve) → GPT-4 (Clean) → Cluster (Refine)
3. Code Layer: CompGCN (Embed) → Code Proto-Encoder (Refine)
4. Visit Layer: Avg Pooling → Visit Proto-Encoder (Refine)
5. Patient Layer: Transformer (Temporal) → Patient Proto-Encoder (Refine)
6. Fusion: Cross-Attention over all prototypes → Weighted Sum → Linear Head

**Critical path:** The flow moves strictly upwards (Code → Visit → Patient). Crucially, the quality of the top-level prediction depends heavily on the lowest level (Code) KG construction. A failure in the LLM KG stage propagates through all levels.

**Design tradeoffs:**
- Prototype Count (m): Higher m captures finer granularity but risks overfitting and slower convergence
- LLM Choice: Using Llama3 for retrieval is cost-effective but noisier than GPT-4; the paper compensates by training a classifier on GPT-4 labels to clean the Llama output

**Failure signatures:**
- Low Performance on Rare Classes: If patient-level prototypes are too few, minority patient cohorts may be "absorbed" by majority prototypes
- Stagnant Fusion Weights: If βₜ doesn't change across epochs, the fusion layer is effectively linear and not learning task-specific hierarchy
- Noisy KG: High edge count (269 relation types) without proper refinement introduces noise

**First 3 experiments:**
1. KG Ablation: Run ProtoEHR with w/o Medical KG to establish baseline for semantic knowledge contribution
2. Prototype Sensitivity: Vary number of patient-level prototypes (2 vs. 8 vs. 16) on Mortality task to visualize trade-off between granularity and generalization
3. Level Importance Check: Run inference on test set and plot average fusion weights β for Drug Recommendation vs. Mortality to verify if model correctly identifies Code vs. Patient importance

## Open Questions the Paper Calls Out
- To what extent does the integration of unstructured data, such as clinical notes or laboratory test results, enhance ProtoEHR's predictive accuracy over the current structured code-only approach?
- Does the hierarchical prototype framework generalize effectively to non-critical care or geographically distinct datasets that possess different coding standards and patient distributions?
- How sensitive is the model's stability and performance to the specific number of prototypes initialized at the code, visit, and patient levels?
- Can the prototypes learned for one clinical task (e.g., mortality) be effectively transferred to improve performance on a distinct task (e.g., readmission) without retraining?

## Limitations
- The Medical KG construction pipeline relies on LLM outputs, introducing potential hallucinations and dataset-specific bias
- The claim that prototype learning captures "intrinsic similarities" is validated internally but needs external validation on diverse datasets
- While hierarchical fusion is shown to be task-aware, the interpretability of learned weights is not deeply explored

## Confidence
- **High Confidence:** Overall hierarchical architecture and ability to outperform baselines on MIMIC datasets
- **Medium Confidence:** Specific contribution of LLM-constructed KG to performance
- **Medium Confidence:** Claim that prototype learning captures clinically meaningful clusters

## Next Checks
1. Cross-Dataset Generalization: Train and evaluate ProtoEHR on a non-MIMIC EHR dataset (e.g., eICU) to test generalizability
2. Prototype Cluster Analysis: Visualize learned prototypes using t-SNE/UMAP and compute silhouette scores to assess cluster quality
3. KG Hallucination Test: Run KG construction pipeline on held-out subset and manually verify sample triplets to quantify hallucination rate