---
ver: rpa2
title: 'Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining
  BabyLMs'
arxiv_id: '2510.20475'
source_url: https://arxiv.org/abs/2510.20475
tags:
- hard
- babylm
- tokens
- training
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors address the challenge of improving sample efficiency\
  \ in language model pretraining, specifically within the constrained data budget\
  \ of the BabyLM Challenge. They propose Adaptive Masked Language Modeling (AMLM),\
  \ a method that dynamically adjusts token masking probabilities during training\
  \ based on model prediction performance\u2014using either accuracy or loss metrics\
  \ to focus on tokens that are difficult to predict."
---

# Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs

## Quick Facts
- arXiv ID: 2510.20475
- Source URL: https://arxiv.org/abs/2510.20475
- Authors: Lukas Edman; Alexander Fraser
- Reference count: 4
- Primary result: AMLM with accuracy-based scoring and decaying mask ratio improves BabyLM aggregate scores over standard MLM baseline

## Executive Summary
This paper addresses sample efficiency in language model pretraining for the BabyLM Challenge by introducing Adaptive Masked Language Modeling (AMLM). The method dynamically adjusts token masking probabilities during training based on model prediction difficulty, using either accuracy or loss metrics. Additionally, token-level n-hot embeddings are proposed to incorporate sub-token information for improved morphological generalization. Experiments show AMLM consistently outperforms standard MLM, with accuracy-based scoring yielding the best results and decaying mask ratio providing additional benefits. The n-hot embeddings significantly boost performance on adjective nominalization tasks while showing limited benefits elsewhere.

## Method Summary
The authors propose Adaptive Masked Language Modeling (AMLM), which tracks per-token prediction performance (via accuracy or loss) and adjusts masking probabilities accordingly - increasing difficulty for tokens the model struggles with and decreasing for mastered tokens. They use a smoothed accuracy metric with decay factor λ=0.2, updating probabilities every 200 batches. The method includes a decaying mask ratio schedule from 40% to 15% over training. Token-level n-hot embeddings are also introduced, encoding substrings present in the vocabulary as n-hot vectors projected via learned linear layers and added to standard token embeddings. The approach is evaluated on the BabyLM benchmark using Deberta-V2 architecture with BPE tokenization.

## Key Results
- AMLM-hard consistently outperforms standard MLM across BabyLM evaluation tasks
- Decaying mask ratio (40%→15%) improves performance in 55% of tasks compared to constant 15%
- N-hot embeddings boost adjective nominalization performance by 20-30 points but show limited benefits elsewhere
- AMLM achieves higher aggregate scores in the BabyLM strict-small track

## Why This Works (Mechanism)

### Mechanism 1
Dynamically adjusting token masking probabilities based on prediction difficulty improves sample efficiency in low-resource pretraining. AMLM tracks per-token prediction performance and up-weights masking probability for difficult tokens while down-weighting easy ones, concentrating learning capacity on informative tokens. Core assumption: tokens with low current prediction performance carry more learnable signal per training step. Break condition: if scoring estimator is too noisy or λ is set too high, probabilities may oscillate or collapse to narrow token subsets.

### Mechanism 2
Accuracy-based ("hard") difficulty scoring outperforms loss-based ("soft") scoring in constrained settings. Hard metric uses smoothed accuracy, providing a more stable curriculum signal than per-token loss variance. Core assumption: binary correctness aggregated over predictions is a less noisy indicator of token-level learning progress than raw cross-entropy loss. Break condition: if a token type is systematically mispredicted despite low loss, soft scoring may provide more nuanced signal.

### Mechanism 3
Decaying overall masking ratio from 40% to 15% during training improves convergence compared to constant 15% ratio. Higher early masking provides denser supervision when representations are uninitialized; later reduction eases the prediction task as the model matures, reducing gradient noise from overly difficult targets. Core assumption: optimal masking density decreases as model competence increases. Break condition: if decay is too aggressive or schedule misaligned, late-stage pretraining may lack sufficient gradient signal.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)** - Why needed: AMLM modifies standard MLM; understanding baseline objective is prerequisite. Quick check: Why does standard MLM typically use 15% masking, and what are the roles of [MASK], random replacement, and no-change tokens?

- **Concept: Curriculum Learning** - Why needed: Adaptive masking implicitly creates difficulty-based curriculum. Quick check: How does reordering training examples by difficulty typically affect convergence speed and final performance?

- **Concept: Sub-token and Morphological Representations** - Why needed: N-hot embedding approach encodes sub-token morphological information. Quick check: How would a BPE tokenizer with 40k vocabulary likely split "wugability" into sub-tokens?

## Architecture Onboarding

- **Component map:** Tokenize input → standard token embeddings + optional n-hot projection → apply adaptive masking → encode with Deberta-V2 → prediction head → compute loss → update per-token statistics → recalculate mask probabilities every 200 batches

- **Critical path:** 1) Tokenize input → standard token embeddings + optional n-hot projection 2) Apply adaptive masking (probabilities normalized to average p_mlm) 3) Encode with Deberta-V2 → prediction head 4) Compute loss → update per-token statistics 5) Every 200 batches, recalculate mask probabilities via scoring function

- **Design tradeoffs:** Hard vs soft scoring (hard more stable, soft potentially more nuanced but noisier); decaying vs constant mask ratio (decaying improves early learning but adds schedule complexity); n-hot embeddings (boost morphological generalization but may hurt BLiMP, adds precomputation and small runtime overhead)

- **Failure signatures:** Masking distribution collapses (check variance of per-token probabilities), training loss spikes and never recovers (Section C notes this with mismatched batch size and LR), n-hot model underperforms on BLiMP/grammar benchmarks while improving morphology tasks

- **First 3 experiments:** 1) Baseline AMLM validation: Compare standard MLM vs AMLM-hard on held-out 1M token split; track BLiMP, (Super)GLUE fine-tuning average, and masking probability evolution 2) Scoring ablation: Run AMLM-hard vs AMLM-soft; record per-POS masking probability trends and final aggregate scores 3) N-hot isolation test: Enable n-hot embeddings with AMLM-hard; measure delta on adjective nominalization vs BLiMP

## Open Questions the Paper Calls Out

### Open Question 1
Can a learned "neural masker" outperform the heuristic AMLM approach by dynamically identifying the most challenging tokens or spans for the model? The current AMLM method relies on manually tuned hyperparameters and simple score accumulation, which the authors admit is "likely far from optimized." Comparative experiments on BabyLM benchmark between current statistical AMLM and policy-gradient-based masking network could resolve this.

### Open Question 2
Is it possible to integrate n-hot sub-token embeddings without causing degradation in syntactic generalization observed in this study? The authors note a trade-off where morphological gains come at cost of syntactic performance, but don't determine if this is inherent or fixable. Ablation studies using gated or attention-based fusion mechanisms for n-hot embeddings could preserve BLiMP scores alongside adjective nominalization improvements.

### Open Question 3
Does applying adaptive masking probabilities to groups of tokens (spans) force the model to learn more global language properties than single-token adaptive masking? The current study only validates adaptive masking on individual tokens; the utility of word-level or phrase-level adaptive masking remains untested. Implementing a span-level AMLM variant and comparing its performance on global reasoning tasks against token-level baseline could resolve this.

## Limitations
- Data composition ambiguity: exact mixing ratio between original BabyLM data and synthetic triplets is unspecified
- N-hot embedding integration: specific projection dimension and integration method are not detailed
- Architecture specification gaps: Deberta-V2 configuration incompletely specified (only hidden/intermediate dimensions given)
- Curriculum scheduling assumptions: optimal decay schedule may vary across domains and dataset sizes

## Confidence
- High confidence: Adaptive masking with accuracy-based scoring improves sample efficiency over standard MLM in low-resource settings
- Medium confidence: Accuracy-based scoring outperforms loss-based scoring specifically within BabyLM context
- Medium confidence: Decaying mask ratio from 40% to 15% provides benefits over constant 15%
- Medium confidence: N-hot embeddings significantly boost morphological generalization while potentially hurting syntactic generalization

## Next Checks
1. Run AMLM-hard with constant vs decaying mask schedules across 3 random seeds; measure training stability and learning curves to isolate decay effect
2. Implement AMLM with hard and soft scoring on held-out 1M token validation split; track per-POS masking probability evolution and correlation with task performance improvements
3. Create variants of n-hot integration: (a) learnable gate between standard and n-hot embeddings, (b) separate projection dimensions, (c) frequency-based n-hot activation; evaluate on both morphology and syntax tasks