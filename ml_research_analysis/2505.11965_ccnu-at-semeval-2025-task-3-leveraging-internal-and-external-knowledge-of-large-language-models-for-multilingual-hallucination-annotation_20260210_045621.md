---
ver: rpa2
title: 'CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of
  Large Language Models for Multilingual Hallucination Annotation'
arxiv_id: '2505.11965'
source_url: https://arxiv.org/abs/2505.11965
tags:
- knowledge
- hallucinations
- answer
- system
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes CCNU's approach to SemEval-2025 Task 3 (Mu-SHROOM),
  which focuses on multilingual hallucination detection in question-answering systems.
  The key innovation is leveraging multiple Large Language Models (LLMs) in parallel,
  each assigned different expert roles, to simulate a crowdsourced annotation process.
---

# CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation

## Quick Facts
- arXiv ID: 2505.11965
- Source URL: https://arxiv.org/abs/2505.11965
- Reference count: 4
- Primary result: First place in Hindi, top 5 in 7 other languages for multilingual hallucination detection

## Executive Summary
This paper presents CCNU's approach to SemEval-2025 Task 3 (Mu-SHROOM), focusing on multilingual hallucination detection in question-answering systems. The system uses multiple Large Language Models (LLMs) working in parallel, each assigned different expert roles to simulate crowdsourced annotation. The methodology integrates both internal knowledge (LLM-generated reference answers) and external knowledge (Wikipedia retrieval and summarization) without requiring any fine-tuning or language-specific optimization. Using DeepSeek-V3 as the backbone model, the system achieved top rankings across multiple languages, though performance varied significantly between language groups.

## Method Summary
The approach employs an ensemble of LLMs working in parallel, each assigned different expert roles to simulate a crowdsourced annotation process. Each LLM generates reference answers using its internal knowledge while simultaneously retrieving and summarizing relevant Wikipedia content for external knowledge integration. The system uses DeepSeek-V3 as the primary model and requires no fine-tuning or language-specific optimization. The methodology combines internal and external knowledge sources to improve hallucination detection accuracy across multiple languages, with performance evaluated through the SemEval-2025 Task 3 benchmark.

## Key Results
- Achieved first place ranking for Hindi data in SemEval-2025 Task 3
- Ranked in top five positions for seven other languages in the competition
- Demonstrated effectiveness of combining internal and external knowledge sources through ablation studies
- Showed significant performance variation across languages, with Chinese data presenting particular challenges

## Why This Works (Mechanism)
The system's effectiveness stems from leveraging multiple LLMs with different expert roles to create a crowdsourced-style annotation process. By combining internal knowledge (model-generated answers) with external knowledge (Wikipedia retrieval), the system can cross-validate responses and identify potential hallucinations more effectively. The parallel processing of multiple expert perspectives helps reduce individual model biases and improves overall detection accuracy across diverse languages.

## Foundational Learning
- Multilingual QA systems: Understanding how different languages affect model performance and hallucination detection accuracy
- LLM ensemble methods: Learning how multiple models working in parallel can improve annotation quality and reduce bias
- Knowledge integration techniques: Understanding how to effectively combine internal model knowledge with external information sources
- Wikipedia retrieval systems: Understanding how to efficiently retrieve and process relevant external knowledge for validation
- Role-based model assignment: Understanding how assigning different expert roles to individual LLMs can simulate crowdsourced annotation

## Architecture Onboarding
- Component map: Multiple LLMs (DeepSeek-V3 based) -> Internal knowledge generation -> External knowledge retrieval (Wikipedia) -> Annotation fusion -> Hallucination detection
- Critical path: Question input → Parallel LLM processing → Reference answer generation → Wikipedia retrieval → Knowledge integration → Annotation decision
- Design tradeoffs: No fine-tuning approach prioritizes generalizability over potential performance gains from adaptation; parallel LLM processing increases computational cost but improves accuracy
- Failure signatures: Performance degradation on Chinese data suggests dataset quality issues; language-specific variations indicate potential bias in external knowledge sources
- First experiments: 1) Test system on additional multilingual QA datasets beyond competition benchmarks, 2) Conduct human evaluation studies comparing LLM-generated annotations against expert annotators, 3) Perform cross-lingual transfer experiments to quantify performance transfer between language groups

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on task-specific benchmarks without broader generalization testing
- Performance gaps exist between languages, with Chinese data showing particular challenges likely due to dataset quality
- No-fine-tuning approach limits understanding of potential performance improvements through model adaptation
- Crowdsourced LLM annotation methodology may introduce subjectivity and inconsistency between different model judgments

## Confidence
- Task performance results: High confidence (based on official task rankings)
- LLM ensemble methodology effectiveness: Medium confidence (requires ablation validation)
- No fine-tuning requirement: High confidence (clearly stated methodology)
- Wikipedia retrieval contribution: Medium confidence (based on ablation studies only)

## Next Checks
1. Test the system on additional multilingual QA datasets beyond the competition to assess generalization
2. Conduct human evaluation studies comparing LLM-generated annotations against expert annotators for consistency
3. Perform cross-lingual transfer experiments to quantify how well performance in high-resource languages (like Hindi) transfers to truly low-resource languages not in the original evaluation set