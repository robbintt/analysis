---
ver: rpa2
title: 'AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security'
arxiv_id: '2601.18491'
source_url: https://arxiv.org/abs/2601.18491
tags:
- agent
- safety
- risk
- unsafe
- trajectory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentDoG is a diagnostic guardrail framework for AI agent safety
  that addresses the challenge of monitoring autonomous agents in complex, tool-augmented
  environments. It introduces a unified three-dimensional taxonomy categorizing risks
  by source (where risk originates), failure mode (how it manifests), and real-world
  harm (what consequences occur).
---

# AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security

## Quick Facts
- arXiv ID: 2601.18491
- Source URL: https://arxiv.org/abs/2601.18491
- Reference count: 31
- AgentDoG achieves state-of-the-art performance on trajectory-level safety evaluation, outperforming existing guardrail models on benchmarks like R-Judge, ASSE-Safety, and ATBench.

## Executive Summary
AgentDoG introduces a diagnostic guardrail framework for monitoring AI agent safety in complex, tool-augmented environments. The framework addresses the challenge of detecting unsafe behaviors that may arise from intermediate actions rather than just final outputs. By introducing a three-dimensional taxonomy categorizing risks by source, failure mode, and real-world harm, AgentDoG provides fine-grained, contextual monitoring of agent trajectories and diagnoses root causes of unsafe actions. Trained on a large-scale synthetic dataset with over 10,000 tools and 100,000+ trajectories, the framework achieves state-of-the-art performance in trajectory-level safety evaluation while offering transparency through detailed risk attribution.

## Method Summary
AgentDoG employs a three-stage synthetic data generation pipeline guided by a unified three-dimensional taxonomy. The framework uses planner-based systems to sample risk configurations and generate execution plans, which are then orchestrated to create synthetic trajectories with tool interactions. A quality control layer filters generated data for structural and semantic validity. The guard model, based on finetuned Llama/Qwen architectures (4B-8B parameters), performs binary safety classification and outputs fine-grained taxonomy labels for risk source, failure mode, and harm. The model uses negative log-likelihood loss during training with a learning rate of 1e-5.

## Key Results
- Achieves 92.8% accuracy on ATBench trajectory-level safety evaluation benchmark
- Outperforms existing guardrail models with 91% F1 on R-Judge and 80% F1 on ASSE-Safety benchmarks
- Demonstrates superior fine-grained attribution with 82.0% accuracy on risk source classification versus 36.8% for Gemini-3-Pro baseline

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Taxonomy Enables Controllable Risk Coverage
The three-dimensional taxonomy (risk source, failure mode, real-world harm) transforms agentic safety from an enumerative problem into a compositional one, enabling systematic coverage rather than case-by-case coverage. By orthogonally decomposing risks into where/what/how dimensions, the framework can independently sample categories along each dimension during data synthesis, yielding "controllable, per-class synthesis" instead of generating data that only exhibits canonical risk phenomena.

### Mechanism 2: Trajectory-Level Monitoring Captures Process Failures
Evaluating the full trajectory rather than only final outputs detects unsafe intermediate actions that would be invisible to output-only guardrails. The model predicts binary safety at trajectory level: y=unsafe iff ∃i where step ti is unsafe. This explicitly models that "unsafe behavior may arise from intermediate actions (e.g., thinking content, tool calls) or intermediate environment feedback, even when the final response appears benign."

### Mechanism 3: Taxonomy-Guided Supervision Improves Attribution Quality
Training with explicit fine-grained taxonomy labels improves the model's ability to diagnose root causes, not just detect unsafe behavior. Fine-grained supervision on (risk source, failure mode, harm) labels provides explicit training signal for attribution. Without this, "general models typically lack explicit supervision on fine-grained risk taxonomies during training, making attribution difficult even if they can detect unsafe trajectories."

## Foundational Learning

- **Trajectory-Level vs. Turn-Level Evaluation:**
  - Why needed here: The core innovation is evaluating complete agent trajectories, not individual turns. Without this concept, you'll misinterpret the task definition and benchmark comparisons.
  - Quick check question: Given a 10-step agent trajectory where only step 3 is unsafe but the final output is benign, would a turn-level guardrail mark it safe or unsafe?

- **Perturbation-Based Attribution:**
  - Why needed here: The XAI module uses probability drop/hold scores to identify which sentences drive actions. Understanding this is necessary to interpret the attribution case studies.
  - Quick check question: If removing sentence x from context reduces action probability by 0.5 log-likelihood units, but x alone increases probability by 0.3 units relative to empty context, what is the final attribution score?

- **Taxonomy-Guided Data Synthesis:**
  - Why needed here: The training data is synthetically generated by sampling risk configurations from the taxonomy. Understanding this pipeline is critical for reproducing or extending the approach.
  - Quick check question: If you want to generate 1000 trajectories covering all 8 risk sources uniformly, how many trajectories should you sample per source?

## Architecture Onboarding

- **Component map:** Taxonomy Layer (3D risk categorization) -> Data Synthesis Pipeline (Planner → Orchestrator → QC) -> Guard Model (SFT-finetuned Llama/Qwen) -> Attribution Module (temporal information gain + perturbation scoring)
- **Critical path:** Taxonomy definition → Synthesis pipeline configuration → Quality control thresholds → SFT training → Binary evaluation → Fine-grained evaluation. Errors in early stages compound through the pipeline.
- **Design tradeoffs:** Synthetic vs. real trajectories (controlled coverage vs. real deployment patterns), Binary vs. fine-grained outputs (evaluation ease vs. diagnostic value), QC strictness (52% pass rate indicates aggressive filtering).
- **Failure signatures:** Low recall with high precision (over-conservative model), Fine-grained accuracy near random (taxonomy supervision failed), Attribution highlights irrelevant steps (perturbation scoring dominated by surface features).
- **First 3 experiments:**
  1. Reproduce binary classification on ATBench: Run AgentDoG-Qwen3-4B on the 500-trajectory benchmark, verify ~92.8% accuracy matches reported results
  2. Ablate training data scale: Train on 10%/50%/100% of synthetic data to verify performance scales with coverage
  3. Cross-benchmark generalization: Evaluate on R-Judge and ASSE-Safety without further training to assess transfer; compare against reported ~91% and ~80% F1 scores

## Open Questions the Paper Calls Out

### Open Question 1
Can AgentDoG be effectively extended to support multimodal inputs (e.g., images, GUI elements) for safeguarding computer-use agents? The current architecture and training data are designed exclusively for textual trajectories; adapting the diagnostic attention mechanisms to visual contexts requires architectural changes. Evidence needed: Modified AgentDoG model that processes visual tokens alongside text, evaluated on a GUI-based agent safety benchmark (e.g., OSWorld).

### Open Question 2
Can the fine-grained diagnostic outputs of AgentDoG serve as effective reward signals to align agent behavior through reinforcement learning? The current framework functions as a post-hoc monitor; it is unproven whether discrete taxonomy labels provide a sufficient gradient for RL policy optimization. Evidence needed: Training results showing that agents trained via RLHF using AgentDoG diagnostics achieve lower failure rates compared to those trained with standard safety reward models.

### Open Question 3
Does training on taxonomy-guided synthetic data generalize robustly to human-generated adversarial attacks that do not strictly adhere to the defined taxonomic categories? Synthetic trajectories may lack the creative nuance or distributional diversity of human red-teaming, potentially leaving blind spots in the "long-tail" risk space. Evidence needed: Performance comparison on a held-out dataset of human-expert red-teaming trajectories versus the synthesized ATBench.

## Limitations
- The synthetic data generation approach raises questions about real-world deployment performance, with a 52% QC pass rate indicating significant filtering
- The taxonomy assumes orthogonal risk dimensions, but hidden dependencies between risk sources, failure modes, and harms could limit compositional coverage
- The framework is currently confined to text-based trajectories and requires architectural changes to support multimodal inputs

## Confidence
- **High Confidence:** Binary trajectory-level safety evaluation performance (ATBench accuracy ~92.8%, R-Judge ~91% F1) - these are direct benchmark measurements with clear methodology
- **Medium Confidence:** Fine-grained taxonomy classification performance (risk source 82.0% accuracy) - results are reported but attribution quality is harder to validate independently
- **Medium Confidence:** Attribution module effectiveness - the perturbation-based approach is described but validation relies on case studies rather than systematic evaluation

## Next Checks
1. Evaluate AgentDoG on a small set of real agent trajectories from production systems to assess synthetic-to-real generalization
2. Systematically test whether certain risk source × failure mode combinations are impossible or overrepresented in the synthetic data
3. Re-run data synthesis with relaxed (60% pass) and strict (40% pass) QC thresholds to measure impact on model performance and coverage