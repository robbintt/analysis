---
ver: rpa2
title: Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative
  Preference Alignment
arxiv_id: '2512.04210'
source_url: https://arxiv.org/abs/2512.04210
tags:
- safety
- healthcare
- alignment
- cycle
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce an iterative post-deployment alignment framework that
  applies KTO and DPO to refine healthcare LLMs against domain-specific safety signals.
  Evaluated on CARES-18K, our approach achieves up to 42% improvement in safety-related
  metrics for harmful query detection across four models (Llama-3B/8B, Meditron-8B,
  Mistral-7B).
---

# Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment

## Quick Facts
- **arXiv ID:** 2512.04210
- **Source URL:** https://arxiv.org/abs/2512.04210
- **Reference count:** 40
- **Primary result:** Up to 42% safety improvement on CARES-18K via KTO→DPO iterative alignment

## Executive Summary
This paper introduces an iterative post-deployment alignment framework that refines healthcare LLMs by balancing safety (refusing harmful queries) and helpfulness (answering benign ones). The approach applies KTO and DPO sequentially to improve safety metrics while monitoring for over-refusal. Tested on CARES-18K with four models (Llama-3B/8B, Meditron-8B, Mistral-7B), the method achieves substantial safety gains with architecture-dependent calibration effects requiring human oversight for optimal deployment.

## Method Summary
The framework iteratively aligns healthcare LLMs using KTO (Kahneman-Tversky Optimization) followed by DPO (Direct Preference Optimization) on domain-specific safety signals. It operates on the CARES-18K dataset, generating responses that are judged for safety (Accept/Caution/Refuse), then constructing KTO datasets (unary labels) and DPO datasets (preference pairs). Training uses LoRA fine-tuning with separate learning rates for each method. The process iterates up to five cycles, selecting checkpoints based on an Overall Metric balancing safety score and error rate. External or fine-tuned judges are used when self-evaluation shows calibration bias.

## Key Results
- Achieves up to 42% improvement in safety-related metrics for harmful query detection
- Sequential KTO→DPO alignment produces more stable gains than either method alone
- Architecture-dependent calibration biases determine self-evaluation viability; external judges partially correct bias but introduce systematic drifts
- Ablation studies show external or fine-tuned judges are necessary when self-evaluation is unreliable

## Why This Works (Mechanism)

### Mechanism 1
Sequential KTO→DPO alignment produces more stable safety gains than either method alone. KTO uses unary safety scores on the full training set for stable gradient signals, followed by DPO sharpening boundaries using contrastive pairs. This sequential application (KTO first, then DPO) leverages KTO's stable full-dataset signals for reliable gains while DPO refines with preference pairs. Break condition occurs when DPO dataset shrinks below critical mass.

### Mechanism 2
Architecture-dependent calibration biases determine whether self-evaluation is viable for iterative alignment. Initial agreement between model self-judgment and external reference (GPT-4o-mini) predicts trajectory reliability. Well-calibrated models (Llama-3B: κ=0.59) show parallel improvement curves; miscalibrated models (Meditron-8B: κ=0.35; Llama-8B: κ=0.29) diverge, amplifying refusal or permissiveness biases. Break condition triggers when self-evaluation κ < 0.4.

### Mechanism 3
External judges partially correct bias tendencies but introduce systematic calibration drifts requiring monitoring. Decoupling judge from target model prevents self-reinforcing spirals, but external judges carry their own biases: base Llama-3B overestimates refusal rates while finetuned Llama-3B underestimates them. Smaller transparent models can approximate larger model judgments for practical supervision, but human oversight remains essential. Break condition when divergence exceeds 10% absolute difference on ERR.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed: DPO reformulates RLHF as supervised learning on preferred/rejected pairs, eliminating reward model training. Essential for constructing valid training sets.
  - Quick check: Given a single model response, how would you construct the complementary rejected response if the primary response is "preferred"?

- **Kahneman-Tversky Optimization (KTO)**
  - Why needed: KTO operates on unary feedback rather than pairs, enabling use of full datasets without synthetic pair construction. Critical for understanding KTO's stable signal advantage.
  - Quick check: Why might KTO be more robust than DPO when preference pairs are sparse or noisy?

- **Safety-Helpfulness Trade-off (ERR vs Safety Score)**
  - Why needed: The framework balances two competing objectives. ERR measures over-refusal of benign queries; Safety Score measures appropriate non-compliance with harmful requests. The policy weight α controls this balance.
  - Quick check: If α=0.8 in Equation (2), which objective receives higher priority, and what behavior would you expect in checkpoint selection?

## Architecture Onboarding

- **Component map:** Target LLM → Response Generation → Judge LLM (self/external/finetuned) → KTO Dataset (unary: Safety Score → binary label) → DPO Dataset (pairwise: preferred vs rejected, with conditional generation) → LoRA Fine-tuning → Validation Evaluation → Best checkpoint selection (via Overall Metric) → Next cycle or deployment

- **Critical path:**
  1. Prompt-response collection from target LLM (greedy decoding, temp=0)
  2. Safety judgment extraction (3-way: Accept/Caution/Refuse)
  3. Safety Score computation per Table 1 mapping
  4. KTO label assignment (score=1 → desirable, else undesirable)
  5. DPO pair construction via conditional generation + verification
  6. Sequential KTO→DPO fine-tuning (1 epoch each, separate learning rates)
  7. Validation-based checkpoint selection using Overall Metric (α=0.6 default)

- **Design tradeoffs:**
  - Self vs external judge: Self-evaluation is cost-efficient but unreliable for miscalibrated architectures; external judges add compute cost and introduce their own calibration biases
  - α policy weight: Higher α prioritizes safety (risk: over-refusal, user frustration); lower α prioritizes helpfulness (risk: unsafe compliance)
  - Stopping criterion: Early stopping reduces compute but may miss plateau-region gains relevant in high-stakes settings

- **Failure signatures:**
  - ERR spike > 50% on validation set (indicates aggressive over-refusal)
  - DPO dataset collapse < 30% of KTO size (insufficient contrastive pairs)
  - Self vs external judge divergence > 15% on Safety Score (miscalibration)
  - Safety Score plateau for 3+ consecutive cycles (diminishing returns)

- **First 3 experiments:**
  1. Baseline calibration audit: Run base model on 500 calibration prompts, collect self-judgment and GPT-4o-mini judgment. Compute Cohen's κ. If κ < 0.4, proceed directly to external judge configuration.
  2. Single-cycle KTO vs DPO comparison: Train one cycle each with identical data split. Compare Safety Score, ERR, F1 on held-out validation. Expect KTO to show more stable gains; DPO may excel on specific cycles.
  3. α sensitivity sweep: Run checkpoint selection with α ∈ {0.4, 0.6, 0.8}. Identify α* thresholds where KTO/DPO preference flips. Document context-dependent α recommendations.

## Open Questions the Paper Calls Out

- **Judge Ensembles:** Can an ensemble of judges combining self-evaluation and external supervision mitigate single-model calibration biases more effectively than single judges? The current framework relies on single proxy judges that exhibit specific, architecture-dependent over- or under-refusal biases. Comparative experiments showing reduced variance and higher alignment stability across cycles would resolve this.

- **Larger Models:** Does the iterative alignment framework maintain efficacy when applied to larger industry-scale models (e.g., Llama-70B)? The study focused on small-to-mid-sized models (3B–8B); it is unknown if the same safety-helpfulness trade-offs and calibration issues persist in larger, more capable architectures. Experimental results on Llama-70B would resolve this.

- **Pre-deployment Integration:** How can pre-deployment alignment techniques, such as Constitutional AI or reasoning-based approaches, be integrated with this post-deployment framework? The current framework is strictly post-deployment; it is unclear how it interacts with or improves upon safety behaviors established during pre-training or instruction tuning. A study comparing pre-deployment versus post-deployment alignment or a hybrid approach would resolve this.

## Limitations
- Reliance on GPT-4o-mini as safety judgment oracle may introduce distributional shift risks when deployed in real-world healthcare settings
- Framework assumes CARES-18K represents a comprehensive safety landscape, but healthcare harm categories evolve continuously
- 5-cycle stopping criterion represents a heuristic that may not be optimal for all deployment scenarios

## Confidence
- **High confidence:** KTO→DPO sequential training produces measurable safety gains (42% improvement empirically documented)
- **Medium confidence:** Architecture-dependent calibration biases meaningfully affect self-evaluation reliability (correlational evidence, limited architectural breadth)
- **Medium confidence:** External judges correct bias tendencies with systematic calibration drifts (observed patterns, no systematic error quantification)
- **Low confidence:** The 5-cycle stopping criterion represents optimal trade-off (no comparative analysis of stopping strategies)

## Next Checks
1. **Expert-consensus validation:** Run 100 random samples through the final aligned model, collect human expert judgments on safety/helpfulness trade-off, compute inter-rater reliability against GPT-4o-mini
2. **Distributional robustness test:** Construct adversarial prompt sets targeting known failure modes (e.g., implicit harm, temporal reasoning) and measure safety score degradation
3. **Longitudinal drift assessment:** Implement continuous monitoring protocol to detect safety score degradation over 3+ months of post-deployment use