---
ver: rpa2
title: Plan Verification for LLM-Based Embodied Task Completion Agents
arxiv_id: '2509.02761'
source_url: https://arxiv.org/abs/2509.02761
tags:
- actions
- judge
- plan
- verification
- embodied
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce a language-based plan verification framework
  for cleaning embodied AI task plans, using a Judge LLM to critique action sequences
  and a Planner LLM to apply revisions iteratively. Tested on TEACh dataset with four
  LLMs, the method achieves up to 90% recall and 100% precision, converging within
  three iterations for 96.5% of cases.
---

# Plan Verification for LLM-Based Embodied Task Completion Agents

## Quick Facts
- arXiv ID: 2509.02761
- Source URL: https://arxiv.org/abs/2509.02761
- Reference count: 29
- One-line result: Achieves up to 90% recall and 100% precision on TEACh dataset with 96.5% convergence within three iterations

## Executive Summary
This paper introduces a language-based plan verification framework for cleaning embodied AI task plans by identifying irrelevant, redundant, and missing actions. The approach uses a Judge LLM to critique action sequences and a Planner LLM to apply revisions iteratively, achieving high precision and recall without requiring environment simulation or formal specifications. Tested across four LLMs on the TEACh dataset, the method converges quickly and preserves human error-recovery patterns, making it suitable for improving imitation learning data quality.

## Method Summary
The framework implements an iterative two-agent loop where a Judge LLM evaluates action sequences against task goals and outputs structured critiques with #REMOVE/#MISSING tags, while a Planner LLM revises the plan based on these critiques. The process repeats until no critiques remain or a maximum of five iterations is reached. The method uses zero-shot natural language prompting without training, operating on abstract action tokens and task goals extracted from human demonstrations. Four LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout) are evaluated for their verification behaviors, with different models exhibiting systematic precision-recall tradeoffs.

## Key Results
- Achieves up to 90% recall and 100% precision across different LLM pairings
- 96.5% of sequences converge within three iterations, with diminishing returns beyond
- Outperforms rule-based baselines significantly (22% recall vs. 68-80% for LLM judges)
- Different LLMs exhibit systematic verification behaviors enabling task-appropriate selection

## Why This Works (Mechanism)

### Mechanism 1: Iterative Critique-Revision Convergence
Multi-round Judge-Planner interaction systematically reduces plan errors while preserving valid action sequences through geometric convergence under conservative assumptions. The verification operator composes critique and revision functions into a non-increasing error sequence.

### Mechanism 2: Natural Language Prompt-Based Error Detection
Zero-shot NL prompts encoding common-sense criteria suffice for detecting multiple error categories without domain-specific rules, leveraging pretrained world knowledge rather than formal specifications.

### Mechanism 3: Precision-Recall Tradeoff via Model Selection
Different LLMs exhibit systematic verification behaviors enabling task-appropriate Judge selection, with conservative models achieving high precision but lower recall, and aggressive models showing the opposite pattern.

## Foundational Learning

- **Embodied Task Planning**: Understanding that action sequences must be grounded in physical constraints is prerequisite to evaluating what makes a plan "clean" vs. "noisy." Quick check: Can you explain why "PickUp(Mug)" followed immediately by "Place(Mug)" might be redundant in one context but necessary in another?

- **LLM-as-a-Judge Paradigm**: The framework relies on casting an LLM as an evaluator rather than generator; understanding known limitations contextualizes why precision-recall tradeoffs matter. Quick check: What are two failure modes of LLM judges mentioned in the literature that could affect verification reliability?

- **Imitation Learning Data Quality**: The paper's downstream goal is improving training data for imitation learning; noisy demonstrations propagate into learned policies, making verification a data-curation step. Quick check: How does preserving "human error-recovery patterns" differ from simply removing all anomalous actions?

## Architecture Onboarding

- **Component map**: Goal + Action Sequence -> Judge LLM -> Critiques -> Planner LLM -> Revised Plan -> Reindex -> (repeat until convergence)
- **Critical path**: Goal and action sequence formatted into Judge prompt → Judge outputs line-by-line annotations with #REMOVE/#MISSING tags → Planner receives original sequence + critiques, outputs revised sequence → Reindex action positions → Repeat until no critiques or iteration limit
- **Design tradeoffs**: Conservative vs. Aggressive Judges (high-precision minimizes false removals but leaves errors vs. high-recall catches more but risks over-correction); Self-pairing vs. Cross-model pairing (diagonal performance often best, but cross-model can achieve highest recall); Iteration depth (3 iterations covers 96.5% of cases)
- **Failure signatures**: Recall failures (context-dependent redundancies, long-range dependencies); Precision failures (multi-step preparations incorrectly flagged as redundant); Non-convergence (3.5% of sequences still have critiques after 5 iterations)
- **First 3 experiments**: 1) Baseline reproduction on 20 TEACh episodes with each of 4 Judge LLMs measuring precision/recall; 2) Iteration ablation comparing convergence at 1, 2, 3, 5 iterations; 3) Cross-domain probe applying framework to 10 episodes from VirtualHome or ALFRED datasets

## Open Questions the Paper Calls Out

- Can the framework maintain high precision and recall when extended to vision-grounded settings where the Judge LLM receives visual input alongside action sequences? The paper states this as future work, as current methodology operates independently of visual inputs.
- Do hybrid ensembles of Judge LLMs (combining high-precision and high-recall models) outperform single-model approaches in F1-score for plan verification? The paper suggests this as a promising direction to manage precision-recall tradeoffs.
- How robust is the natural language verification prompt against adversarial attacks or distributional shifts in plan formatting? The paper identifies the need to explore adversarial robustness of judge prompts.
- Does the framework generalize to domains outside of household tasks, such as industrial or healthcare environments, without task-specific heuristics? The paper notes generalizability to non-household domains remains untested.

## Limitations
- Context-dependent redundancies and long-range dependencies remain undetected by current NL-only prompting
- Framework's reliance on household task knowledge may not transfer to domains requiring precise spatial reasoning
- Conservative convergence assumption lacks formal proof; 3.5% non-convergence cases uncharacterized

## Confidence

- **High confidence**: Convergence rates (96.5% within 3 iterations), precision-recall tradeoffs across models, relative baseline performance
- **Medium confidence**: Generalization to non-household tasks and cross-domain robustness claims
- **Low confidence**: Theoretical convergence guarantees and error amplification bounds

## Next Checks

1. Run verification on TEACh episodes with maximum iteration depth (5) to characterize the 3.5% non-convergence cases and analyze critique patterns that prevent termination.
2. Apply framework to 10 episodes from VirtualHome or ALFRED datasets to assess performance degradation and identify domain-specific limitations beyond household tasks.
3. Construct action sequences with deliberate long-range dependencies and context-dependent redundancies to measure detection rates and establish bounds on NL-only verification capabilities.