---
ver: rpa2
title: 'Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location
  Selection for Prostate Cancer Localisation'
arxiv_id: '2508.03953'
source_url: https://arxiv.org/abs/2508.03953
tags:
- segmentation
- policy
- image
- cancer
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a reinforcement learning framework to optimise
  prostate cancer localisation on multiparametric MRI. The approach trains a policy
  network to iteratively select the most informative imaging modality (T2-weighted,
  diffusion-weighted, or both) and local anatomical regions for segmentation.
---

# Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation

## Quick Facts
- arXiv ID: 2508.03953
- Source URL: https://arxiv.org/abs/2508.03953
- Reference count: 19
- This work introduces a reinforcement learning framework to optimise prostate cancer localisation on multiparametric MRI.

## Executive Summary
This paper presents a reinforcement learning framework for iterative prostate cancer segmentation on multiparametric MRI. The approach trains a policy network to select the most informative imaging modality and anatomical region at each step, using a pre-trained SwinUNETR-v2 segmentation model as a simulated radiologist to provide feedback. Evaluated on 1325 clinical cases, the method achieves improved Dice scores (up to 0.44) compared to standard segmentation baselines, particularly for challenging cases. Notably, the learned policy sometimes deviates from established PI-RADS guidelines, suggesting novel strategies that could inform clinical practice.

## Method Summary
The method formulates prostate cancer segmentation as a Markov Decision Process (MDP) where a policy network iteratively selects which anatomical portion of the prostate and which imaging modality (T2-weighted, diffusion-weighted, or both) to process next. A pre-trained SwinUNETR-v2 segmentation model acts as a simulated radiologist, processing the selected region and modality to provide local segmentation. The policy is trained using reinforcement learning (REINFORCE or GRPO) to maximize Dice score improvements, with rewards calculated as the difference in loss between current and updated segmentation masks. The volume is divided into 8 portions along depth, and training involves 60 steps per episode over 30 epochs.

## Key Results
- Achieved improved Dice scores (up to 0.44) compared to standard segmentation baselines
- Demonstrated effectiveness particularly for challenging clinical cases
- Learned policy sometimes deviates from established PI-RADS guidelines, suggesting novel strategies
- Enhanced segmentation accuracy and efficiency

## Why This Works (Mechanism)

### Mechanism 1: Iterative State Refinement via MDP
The system uses MDP to iteratively update a global segmentation map by focusing on local improvements. The current segmentation mask is fed back as part of the state, and the policy selects a local region and modality. The simulated radiologist re-infers only that region, and the local result replaces the corresponding part of the global mask. This allows recovery from initial errors by processing the full volume in multiple focused passes rather than a single feed-forward pass.

### Mechanism 2: Surrogate Reward via Simulated Radiologist
A pre-trained segmentation network acts as a surrogate for human feedback, providing dense reward signals based on Dice loss improvements. The policy network is trained via REINFORCE or GRPO, computing the Dice loss improvement between the current state and ground truth. This eliminates the need for online human intervention during training.

### Mechanism 3: Disentangled Modality Selection
Explicitly modeling modality selection as a discrete action forces the model to learn the specific diagnostic value of T2-weighted vs. DW imaging for different anatomical contexts. The action space includes a modality mask, and by zeroing out non-selected channels, the segmentation network must process the image using only the selected view, mimicking a radiologist focusing on a specific sequence.

## Foundational Learning

**Concept: Markov Decision Processes (MDP)**
- Why needed: The problem requires determining the optimal order to look at image parts to maximize accuracy, not just segmenting a static image
- Quick check: If you removed the iterative loop and fixed the number of steps to 1, would this still be an MDP?

**Concept: Policy Gradient Methods (REINFORCE/GRPO)**
- Why needed: The action space (discrete selection of location + modality) is non-differentiable, requiring RL to estimate gradients of the reward
- Quick check: Why is the reward defined as the difference in loss rather than just the raw loss at step t+1?

**Concept: Decoder-only / Segmentation Architectures