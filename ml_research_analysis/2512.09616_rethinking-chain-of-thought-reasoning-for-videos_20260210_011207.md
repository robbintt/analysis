---
ver: rpa2
title: Rethinking Chain-of-Thought Reasoning for Videos
arxiv_id: '2512.09616'
source_url: https://arxiv.org/abs/2512.09616
tags:
- reasoning
- video
- concise
- token
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work challenges the assumption that long, human-like chain-of-thought
  reasoning is essential for video understanding. The authors show that concise reasoning
  traces combined with compressed visual tokens can be sufficient and more efficient.
---

# Rethinking Chain-of-Thought Reasoning for Videos

## Quick Facts
- arXiv ID: 2512.09616
- Source URL: https://arxiv.org/abs/2512.09616
- Authors: Yiwu Zhong; Zi-Yuan Hu; Yin Li; Liwei Wang
- Reference count: 40
- Key outcome: Concise reasoning traces combined with compressed visual tokens achieve competitive video understanding performance while being 10× more efficient than chain-of-thought approaches.

## Executive Summary
This work challenges the assumption that lengthy, human-like chain-of-thought reasoning is essential for video understanding. The authors demonstrate that concise reasoning traces combined with compressed visual tokens can achieve competitive performance while significantly improving inference efficiency. By leveraging reinforcement learning (GRPO) without requiring chain-of-thought annotations or supervised fine-tuning, the proposed framework enables models to operate on compressed visual tokens and generate brief reasoning traces. The resulting models deliver improved inference efficiency and competitive performance across general, long, and complex video understanding benchmarks, outperforming existing chain-of-thought methods.

## Method Summary
The method employs reinforcement learning via GRPO to align pre-trained video MLLMs with concise reasoning without requiring chain-of-thought annotations or supervised fine-tuning. Token compression is integrated during training, allowing the model to learn reasoning from reduced visual tokens while enabling 6× more frames to be processed within the same computational budget. The framework uses a hybrid attention scheme where FlashAttention is disabled in specific layers for token pruning, and concise reasoning traces are generated within structured tags rather than verbose human-like pondering patterns.

## Key Results
- Achieves +5.7 points on VideoMME and +8.1 points on MLVU over Video-R1 baselines
- Inference is 10× faster than chain-of-thought approaches while maintaining competitive accuracy
- Outperforms existing chain-of-thought methods on general, long, and complex video understanding benchmarks
- Concise reasoning with token compression enables processing 6× more frames within fixed compute budget

## Why This Works (Mechanism)

### Mechanism 1: Direct GRPO Alignment Without SFT
Reinforcement learning via GRPO alone can elicit concise reasoning from pre-trained video MLLMs, bypassing the need for CoT annotation and supervised fine-tuning. Pre-trained models possess latent reasoning capability but are misaligned with concise reasoning mode. GRPO optimizes policy through group-relative advantage comparisons, rewarding correct answer formats and accuracy, aligning the model toward brief, informative reasoning traces rather than verbose human-like pondering patterns.

### Mechanism 2: Token Compression Adaptation via Training
Integrating token compression during GRPO training enables the model to learn robust reasoning from reduced visual tokens, whereas inference-only compression degrades performance. Token compression (merging similar tokens, pruning uninformative ones) is applied during training. The model learns to reason from compressed representations, making inference-time compression transparent. Physical token removal enables actual memory and runtime savings.

### Mechanism 3: Frame Multiplication Under Fixed Compute Budget
Token compression creates computational slack that can be reallocated to process more frames, improving temporal coverage for long-video understanding. By reducing per-frame token count, the model can ingest 6× more frames within the same prefilling budget. Dense temporal sampling captures critical frames that sparse sampling misses, with GRPO training ensuring the model can integrate information across more frames without degradation.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: The core RL algorithm enabling direct post-training without a critic model. Understanding how it normalizes rewards within sampled groups is essential for debugging reward design. Quick check: Given 4 sampled responses with rewards [0.8, 0.6, 0.4, 0.2], what are their relative advantages after normalization?

- **Transformer Prefilling vs. Decoding Phases**: The paper's efficiency gains decompose into reducing prefilling cost (via token compression) and decoding cost (via concise reasoning). Understanding this distinction is necessary for profiling where optimization efforts should focus. Quick check: Which phase scales with input sequence length, and which scales with output sequence length?

- **Token Compression via Merging and Pruning**: The method physically removes tokens (not just attention masking) and requires disabling FlashAttention in select layers. Implementation requires understanding how to identify "visually similar" tokens and "uninformformative" tokens. Quick check: If token compression merges k tokens into 1, what is the reduction in self-attention complexity for that segment?

## Architecture Onboarding

- **Component map**: Pre-trained Video MLLM -> Token Compression Module -> GRPO Training Loop -> Decoding Mode Controller

- **Critical path**: Input video → visual encoder → raw visual tokens → initial LLM layers (FlashAttention enabled) → token compression module merges/prunes (FlashAttention disabled for these layers only) → compressed tokens continue through remaining layers → GRPO samples multiple response candidates → reward computation → policy update → model generates concise reasoning trace within `<think/>` tags, then final answer

- **Design tradeoffs**:
  - Compression ratio vs. information retention: Higher compression enables more frames but risks losing spatial detail
  - Number of pruning layers vs. efficiency: More pruning layers increase savings but reduce compatibility with FlashAttention
  - GRPO group size vs. training stability: Larger groups provide better advantage estimates but increase compute per step

- **Failure signatures**:
  - Concise reasoning collapses to direct answering: Model learns to skip `<think/>` section entirely; reward function may be missing format reward
  - Token compression causes accuracy crash: Model wasn't trained with compression; apply compression during training, not just inference
  - GRPO training diverges: KL coefficient too low; model deviates too far from pre-trained weights
  - Long videos still fail: Frame count increased but key frames still missed; consider uniform sampling vs. keyframe detection

- **First 3 experiments**:
  1. Baseline sanity check: Run pre-trained Qwen2.5-VL with direct answer vs. concise reason prompt on VideoMME subset. Confirm the performance gap to establish your baseline.
  2. GRPO-only training: Train with GRPO (no SFT, no compression) for 500 steps. Measure improvement in concise reason mode. This isolates the alignment effect.
  3. Compression ablation: Compare inference-only compression vs. training-with-compression on a held-out benchmark. Measure both accuracy and wall-clock runtime to quantify the tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
Can the concise reasoning paradigm with token compression generalize to image-only reasoning and multi-image reasoning tasks? The authors explicitly state that their conclusions are constrained by the scope of current video benchmarks and believe insights may generalize beyond these datasets. The paper validates the hypothesis only on video benchmarks, leaving untested whether the same efficiency-accuracy trade-off holds for static images or interleaved multi-image tasks.

### Open Question 2
What are the theoretical mechanisms that make concise reasoning sufficient for video understanding, and why do human-like pondering patterns ("Hmm," "Wait") provide limited value? The authors note that CoT outputs frequently contain human-like "pondering" patterns that contribute little to reasoning but do not explain why. The paper demonstrates that concise reasoning works empirically but does not investigate whether the benefit comes from reduced distraction, better token utilization, inherent model knowledge, or other factors.

### Open Question 3
What is the optimal trade-off frontier between reasoning conciseness and visual token compression, and do these dimensions interact? The paper separately shows that concise reasoning and token compression each work, and combines them in the final model. However, it does not explore whether certain compression rates require correspondingly longer reasoning traces, or if joint optimization reveals different optima than independent tuning.

### Open Question 4
Why does the pre-trained model perform poorly on concise reasoning before GRPO fine-tuning, and to what extent does this depend on pre-training data composition? The authors observe that prompting the base pre-trained MLLM to generate concise reasoning chains leads to major performance drop, significantly worse than direct answering. They conjecture that the model possesses necessary knowledge but is not well aligned with the concise reasoning paradigm, but this conjecture remains unverified.

## Limitations
- Generalization across tasks and modalities remains untested, with uncertainty about whether concise reasoning suffices for tasks requiring extensive intermediate calculation
- Model architecture constraints include specific choices of base model (Qwen2.5-VL-7B) and token compression ratios that may degrade with different model sizes or non-video tasks
- Reward function sensitivity could be problematic, as the paper does not provide exhaustive ablation studies on reward function hyperparameters

## Confidence
- Generalization Across Tasks and Modalities: Medium
- Model Architecture Constraints: Medium
- Reward Function Sensitivity: Low
- Compression Quality vs. Information Loss: Medium

## Next Checks
1. Apply the trained model to non-video reasoning tasks (e.g., mathematical word problems, scientific reasoning) to test whether concise reasoning generalizes beyond the video domain, comparing performance against both direct answering and traditional CoT approaches.

2. Systematically vary token compression ratios and measure the accuracy-efficiency tradeoff curve to identify the compression point where accuracy degradation becomes unacceptable and determine if this varies by video type.

3. Conduct controlled experiments removing either the format reward or accuracy reward from GRPO training to measure the impact on concise reasoning generation quality and overall task performance, isolating the contribution of each reward component.