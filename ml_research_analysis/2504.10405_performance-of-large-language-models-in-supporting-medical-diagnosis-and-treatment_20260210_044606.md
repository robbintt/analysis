---
ver: rpa2
title: Performance of Large Language Models in Supporting Medical Diagnosis and Treatment
arxiv_id: '2504.10405'
source_url: https://arxiv.org/abs/2504.10405
tags:
- performance
- medical
- available
- llms
- accessed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of 21 contemporary Large Language
  Models (LLMs) on the 2024 Portuguese National Exam for medical specialty access
  (PNA), a standardized medical knowledge assessment. Models were tested using a strict
  pass@1 methodology without prompt engineering to provide a conservative estimate
  of their generalized medical knowledge and reasoning capabilities.
---

# Performance of Large Language Models in Supporting Medical Diagnosis and Treatment

## Quick Facts
- arXiv ID: 2504.10405
- Source URL: https://arxiv.org/abs/2504.10405
- Reference count: 40
- Primary result: Several LLMs achieved accuracy exceeding human student benchmarks on the 2024 Portuguese National Exam for medical specialty access.

## Executive Summary
This study evaluates 21 contemporary Large Language Models (LLMs) on the 2024 Portuguese National Exam for medical specialty access (PNA), a standardized 150-question multiple-choice medical knowledge assessment. Using a strict pass@1 methodology without prompt engineering, the research provides a conservative estimate of LLMs' generalized medical knowledge and reasoning capabilities. Several models, including Google's experimental Gemini 2.5 Pro and open-source DeepSeek R1, demonstrated accuracy exceeding human student benchmarks, with some achieving the highest raw scores. The study developed a novel scoring metric balancing accuracy with cost-effectiveness and potential data contamination risk.

## Method Summary
The study tested 21 LLMs on the 2024 PNA exam using 150 multiple-choice medical questions covering diverse medical fields. Models were presented in batches of 10 questions (15 batches total) with no prompt engineering or explicit reasoning prompts. A strict pass@1 protocol was used, recording only the first response per question. A custom composite scoring metric was developed: Score = 100 × (Correct/150)³ × 1/√(1+log₁₀(Price+1)) × C_risk. The methodology deliberately avoided any optimization techniques to provide conservative estimates of baseline medical knowledge and reasoning capabilities.

## Key Results
- Multiple LLMs achieved accuracy scores exceeding human benchmarks, with top student scores at 135/150 and median at 101/150
- Several models, including Google's Gemini 2.5 Pro and DeepSeek R1, demonstrated reasoning capabilities surpassing human performance on standardized medical exams
- The study established cost-effectiveness rankings alongside accuracy, highlighting models that balance performance with practical deployment considerations

## Why This Works (Mechanism)
The study's methodology works by providing a standardized, controlled evaluation environment that isolates generalized medical knowledge from prompt engineering effects. By using a strict pass@1 protocol and avoiding any optimization techniques, the research captures baseline reasoning capabilities rather than cherry-picked results. The multiple-choice format with clear correct answers enables objective measurement, while the diverse question coverage across medical specialties tests comprehensive knowledge rather than narrow expertise.

## Foundational Learning
- **Pass@1 Protocol**: Required for measuring baseline performance without retries or model selection; Quick check: Verify that only first response per question is recorded
- **Cost-Effectiveness Scoring**: Balances accuracy with deployment practicality; Quick check: Confirm all models' pricing data is current and consistently applied
- **Data Contamination Risk**: Critical for interpreting high scores in medical domains; Quick check: Assess overlap between model training data and exam content availability
- **Chain-of-Thought Reasoning**: Enables complex medical reasoning through step-by-step analysis; Quick check: Compare performance between models with and without explicit reasoning prompts
- **Standardized Testing Framework**: Provides objective measurement across diverse medical knowledge areas; Quick check: Verify question coverage matches intended medical specialty breadth
- **Composite Scoring Metrics**: Combines multiple evaluation dimensions for holistic assessment; Quick check: Validate scoring formula weights reflect practical deployment priorities

## Architecture Onboarding
Component Map: Question Input -> LLM API Call -> Answer Extraction -> Scoring Evaluation
Critical Path: The model response generation and answer extraction stages are most critical, as errors here directly impact accuracy measurements and subsequent cost-effectiveness calculations.
Design Tradeoffs: The study chose strict pass@1 over iterative refinement to maintain conservative estimates, sacrificing potential accuracy gains for methodological rigor. Cost calculations prioritize simplicity over precise per-token accounting, accepting some imprecision for comparability.
Failure Signatures: Incorrect answer parsing from verbose model outputs, API rate limiting causing incomplete submissions, and language-specific degradation for non-English content are primary failure modes that could skew results.
First Experiments: (1) Validate answer extraction accuracy by manually reviewing 50 random model responses; (2) Test API submission with small batches to identify rate limiting thresholds; (3) Compare model performance on Portuguese vs. English medical questions to quantify language effects.

## Open Questions the Paper Calls Out
The paper explicitly calls for a prospective "PNA 2025 LLM-Student Showdown" to eliminate data contamination risks by having models take the exam simultaneously with students. It also highlights the need for real-world integration studies to assess how LLM performance on standardized tests translates to actual clinical diagnostic accuracy and patient outcomes. Additionally, the paper identifies the need for systematic qualitative error analysis to categorize incorrect responses and distinguish between knowledge gaps, logical errors, and hallucinations.

## Limitations
The study does not specify how free-form model responses were mapped to single-choice answers, creating uncertainty about answer extraction methodology. The contamination risk penalty component in the scoring formula is not clearly defined, limiting reproducibility. Language effects on model performance are acknowledged but not quantitatively assessed, leaving uncertainty about cross-linguistic generalizability.

## Confidence
High: Ranking of models by accuracy under pass@1 protocol
Medium: Cost-effectiveness conclusions due to unclear pricing and risk calculations
Low: Cross-linguistic generalizability without quantitative language effect assessment

## Next Checks
1. Replicate answer extraction using raw model outputs and multiple parsing heuristics
2. Clarify and apply the contamination risk penalty consistently
3. Test a subset of questions with English translations to quantify language effects