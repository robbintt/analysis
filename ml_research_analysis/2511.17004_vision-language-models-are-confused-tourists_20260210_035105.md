---
ver: rpa2
title: Vision Language Models are Confused Tourists
arxiv_id: '2511.17004'
source_url: https://arxiv.org/abs/2511.17004
tags:
- image
- country
- cultural
- perturbation
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces CONFUSEDTOURIST, a new benchmark for evaluating
  the robustness of Vision-Language Models (VLMs) to cultural concept mixing. The
  key innovation is the use of adversarial image perturbations that insert conflicting
  geographical cues (flags and landmarks) alongside a target cultural item (e.g.,
  cuisine, attire, or music).
---

# Vision Language Models are Confused Tourists

## Quick Facts
- arXiv ID: 2511.17004
- Source URL: https://arxiv.org/abs/2511.17004
- Reference count: 40
- Primary result: All tested VLMs suffer substantial accuracy drops when cultural items are paired with conflicting geographical cues

## Executive Summary
This paper introduces CONFUSEDTOURIST, a benchmark evaluating VLM robustness to cultural concept mixing through adversarial perturbations. The evaluation suite contains 5,451 images covering 243 unique cultural items from 57 countries, with perturbations applied at two difficulty levels using image stacking and generative methods. Results show that all 14 tested VLMs experience substantial accuracy drops when faced with conflicting geographical cues, with flags causing the most disruption. Interpretability analysis reveals that failures stem from systematic attention shifts toward distracting cues like flags and landmarks, diverting focus from intended cultural items. The findings highlight a critical vulnerability in VLMs' ability to maintain stable, culturally-aware multimodal understanding.

## Method Summary
The evaluation uses 5,451 images covering 243 unique cultural items from 57 countries, with each item paired with adversarial geographical cues (flags and landmarks from different countries). Perturbations are applied at two difficulty levels: image stacking (resizing adversarial image to â‰¤20% and placing in corners) and generative perturbation using Gemini-2.5-Flash-Image. The benchmark evaluates 14 VLMs including proprietary and open-source models on multi-target accuracy (substring match for item/country prediction) and model distraction likelihood (probability of predicting adversarial country when wrong).

## Key Results
- All 14 tested VLMs experience accuracy drops of 15-30% when faced with cultural concept mixing
- Flag perturbations cause more severe performance degradation than landmark perturbations
- Generative perturbations create worse performance drops than simple image stacking
- Model failures are driven by attention shifts toward adversarial cues rather than target cultural items
- All models show fallback bias toward over-represented countries (India, China, Japan) when uncertain

## Why This Works (Mechanism)

### Mechanism 1: Attention Hijacking by Salient Distractors
- VLMs disproportionately attend to easily interpretable visual cues (flags, landmarks) rather than target cultural objects, causing systematic grounding failures. The visual attention layer gets captured by prominent contextual features that have strong learned associations in training data. Visual attention patterns correlate with prediction focus and reasoning priority.

### Mechanism 2: Training Distribution Fallback Bias
- When models encounter uncertainty, they revert to predicting culturally over-represented countries present in their training data (India, China, Japan), regardless of actual visual evidence. The model's learned probability distribution over countries has stronger priors for certain regions due to training data imbalances. Training data contains geographic/cultural imbalances that create systematic prediction biases.

### Mechanism 3: Contextual Reasoning Corruption via Flag Priority
- Flags, more than landmarks, corrupt the model's reasoning chain by being processed first and treated as authoritative geographic context, bypassing object-level analysis. Flags have high visual distinctiveness and strong country associations. The model's reasoning process treats them as primary geographic indicators, leading to confirmation bias where subsequent visual analysis is interpreted through the flag's geographic lens. VLM reasoning traces reflect actual cognitive priority.

## Foundational Learning

- **Concept: Visual Grounding in Multimodal Models**
  - Why needed here: Understanding that VLMs must map linguistic concepts to specific visual regions, and that this mapping can be disrupted by competing visual elements.
  - Quick check question: Can you explain why a VLM might correctly identify an object in isolation but fail when a flag is present nearby?

- **Concept: Attention Mechanisms and Saliency**
  - Why needed here: The paper's interpretability analysis relies on attention heatmaps; understanding how attention is computed and what it represents is essential for diagnosing the failure mode.
  - Quick check question: If a model's attention heatmap shows high activation on a flag rather than the target cuisine, what does this imply about its prediction process?

- **Concept: Adversarial Evaluation in Vision-Language Tasks**
  - Why needed here: The paper constructs adversarial examples (cultural mixing) rather than using natural data; understanding adversarial methodology helps interpret whether findings reflect real-world failures or edge cases.
  - Quick check question: What is the difference between image-stacking perturbation and generative perturbation, and why might the latter cause worse performance?

## Architecture Onboarding

- **Component map**: Image encoder (ViT-based) -> Vision-language connector -> Attention mechanism -> Reasoning/generation (LLM backbone)
- **Critical path**: 1) Image encoded into visual tokens, 2) Prompt text tokenized and embedded, 3) Cross-modal attention weights visual tokens based on text prompts, 4) **Failure point**: High-saliency distractors receive disproportionate attention weight, 5) LLM generates prediction based on attended features, 6) **Failure point**: Reasoning chain incorporates geographic priors from attended distractors
- **Design tradeoffs**: Attention breadth vs. focus (broader attention captures context but risks distraction; narrow attention may miss relevant cultural markers), Prompt specificity vs. generalization (more specific prompts can help but may not generalize), Training diversity vs. over-representation (more cultural data may reduce fallback bias but could introduce confusing overlaps)
- **Failure signatures**: Accuracy drops of 15-30% on country prediction when flags are present, Distraction likelihood correlates negatively with accuracy (R=-0.76), Reasoning traces mention geographic cues early and treat them as authoritative, Fallback predictions cluster around India, China, Japan
- **First 3 experiments**:
  1. Baseline attention analysis: Run open-source VLM on CONFUSEDTOURIST samples, extract attention heatmaps for correct and incorrect predictions to verify incorrect predictions show attention concentration on flags/landmarks
  2. Prompt ablation with attention tracking: Test refined prompt across 50 samples per category, track whether attention shifts to target object and whether accuracy improves
  3. Fallback bias quantification: Measure distribution of incorrect country predictions on unperturbed baseline images, correlate with training data prevalence or frequency of country mentions in common VLM training corpora

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training modifications can reduce VLMs' susceptibility to geographical distractor cues while preserving their ability to use legitimate contextual information?
- Basis in paper: The authors state "These findings highlight a critical challenge: visual cultural concept mixing can substantially impair even state-of-the-art VLMs, underscoring the urgent need for more culturally robust multimodal understanding."
- Why unresolved: The paper diagnoses the problem but does not propose or test solutions; prompt ablation showed inconsistent improvements
- What evidence would resolve it: A study comparing different training objectives, architectural modifications, or data augmentation strategies on CONFUSEDTOURIST performance

### Open Question 2
- Question: To what extent do the observed cultural grounding failures generalize to other cultural domains beyond cuisine, attire, and musical instruments?
- Basis in paper: The authors explicitly limit their categories to three "well-constrained" domains with "single, clear, object-based" cues, excluding multi-concept categories like festivals or broad categories like artifacts
- Why unresolved: The dataset covers only 3 of many possible cultural domains; whether VLMs exhibit similar attention-shift behavior with intangible cultural elements remains untested
- What evidence would resolve it: Extension of the evaluation suite to additional cultural categories with appropriate adversarial pairing methods

### Open Question 3
- Question: Does improving geographic diversity in training data directly mitigate the "fallback bias" toward over-represented cultures (e.g., India, China, Japan)?
- Basis in paper: The authors hypothesize that "this over-representation may arise because these regions are common subjects of online imagery" and note that Southeast Asia showed relatively stable performance "possibly reflecting improved data coverage"
- Why unresolved: The observed correlation between training data coverage and regional robustness is suggestive but not causally established
- What evidence would resolve it: Controlled experiments comparing model variants trained on balanced vs. imbalanced geographic distributions, evaluated on region-stratified subsets of CONFUSEDTOURIST

## Limitations
- Interpretability analysis relies on attention heatmaps from open-source models, which may not generalize to proprietary systems with different architectures
- Cultural item selection depends on Wikipedia/Wikimedia Commons which may have geographic representation biases
- Prompt refinement showing inconsistent improvement suggests the solution may not be robust to varying adversarial conditions

## Confidence
- High confidence: Empirical finding that VLMs experience substantial accuracy drops (15-30%) when faced with cultural concept mixing, and that flags are more disruptive than landmarks
- Medium confidence: Mechanism explanation that attention hijacking drives failures, based on open-source model analyses that may not fully represent proprietary systems
- Medium confidence: Training distribution bias claim regarding India/China/Japan fallback predictions, pending verification of training corpus statistics

## Next Checks
1. **Attention mechanism validation**: Test whether the refined prompt consistently shifts attention to target objects across all perturbation types and models, documenting cases where it fails
2. **Cultural representation audit**: Analyze the Wikipedia/Wikimedia Commons source data for geographic representation biases that could influence both baseline performance and perturbation effectiveness
3. **Proprietary model analysis**: Apply the same interpretability pipeline (attention heatmap analysis) to at least one proprietary model to verify that the attention hijacking mechanism generalizes beyond open-source architectures