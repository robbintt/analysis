---
ver: rpa2
title: An Open-Source and Reproducible Implementation of LSTM and GRU Networks for
  Time Series Forecasting
arxiv_id: '2504.18185'
source_url: https://arxiv.org/abs/2504.18185
tags:
- series
- lstm
- time
- dataset
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an open-source and reproducible implementation
  of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks for time
  series forecasting. The authors evaluated LSTM and GRU networks because of their
  strong performance reported in related work for time series forecasting.
---

# An Open-Source and Reproducible Implementation of LSTM and GRU Networks for Time Series Forecasting

## Quick Facts
- arXiv ID: 2504.18185
- Source URL: https://arxiv.org/abs/2504.18185
- Reference count: 16
- Primary result: LSTM and GRU networks significantly outperform baseline on synthetic Activities dataset but perform similarly to baseline on stock market data

## Executive Summary
This paper presents an open-source and reproducible implementation of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks for time series forecasting. The authors evaluate these architectures on two datasets: a synthetic Activities dataset with repeating weekly patterns and a stock market dataset (S&P BSE BANKEX). Results show that LSTM and GRU networks significantly outperform a naive baseline on the synthetic dataset in terms of RMSE and Directional Accuracy, but perform similarly to baseline on stock data, likely due to the non-repetitive nature of financial time series. The authors release both datasets and implementation code to enable future comparisons and ensure reproducibility.

## Method Summary
The method involves training LSTM and GRU networks on normalized time series data using a sliding window approach. Each series is normalized to [0,1] using its own min/max values. Input windows of size w=60 are mapped to target values f steps ahead (f=1 or f=20). The networks consist of one recurrent layer with 128 units followed by a dense output layer. Training uses only the first series from each dataset, with evaluation on all 10 series. The Adam optimizer minimizes MSE loss over 200 epochs. Performance is measured using RMSE and Directional Accuracy against a baseline that repeats the last available value.

## Key Results
- On the Activities dataset, LSTM and GRU significantly outperform baseline for both 1-step and 20-step forecasts (p < 0.05 via Mann-Whitney U test)
- GRU achieves lower RMSE than LSTM for 1-step forecasts (0.0313 vs 0.0395) while LSTM performs better for 20-step forecasts (0.0555 vs 0.0585)
- On the BANKEX stock dataset, both networks perform similarly to baseline with no statistically significant difference in RMSE
- Directional Accuracy on BANKEX is approximately 50% for all methods, indicating no directional prediction advantage

## Why This Works (Mechanism)

### Mechanism 1: Gated Memory for Selective Information Retention
LSTM and GRU architectures can selectively preserve relevant temporal patterns over extended sequences when those patterns exhibit regular repetition. Gating units (forget/input gates in LSTM; reset/update gates in GRU) modulate information flow by learning which features to retain or discard via sigmoid-activated scalar multipliers, enabling gradient flow across long distances without vanishing. The memory cell `c_t` in LSTM updates as `c_t = f_t * c_{t-1} + i_t * \tilde{c}_t`, where gates control the balance between past context and new input. Core assumption: The underlying time series contains repeatable structure that gating can learn to recognize and exploit. Break condition: When series lack repeating structure, gates cannot discover exploitable patterns.

### Mechanism 2: Single-Series Training via Pattern Generalization
Training on one normalized series can generalize to unseen series within the same dataset if those series share structural patterns. The network learns the shape of repeating patterns rather than series-specific values. Normalization maps all series to [0,1], so the network sees pattern templates. When applied to new series with similar structure but different amplitude/phase, the learned pattern detectors transfer. Core assumption: Dataset series are pattern-aligned or rotationally equivalent. Break condition: If test series have fundamentally different pattern structures, transfer fails.

### Mechanism 3: Baseline Parity Under Information-Theoretic Limits
LSTM/GRU cannot extract predictive signal from series that approximate random walks, leading to baseline-equivalent performance. When price changes are near-unpredictable, the optimal forecast is the last observed value. RNNs with MSE loss converge to predicting the conditional mean, which ≈ last value for near-Martingale series. Core assumption: Financial closing prices contain minimal exploitable autocorrelation beyond short horizons. Break condition: If additional features provide signal beyond price history, this mechanism may not hold.

## Foundational Learning

- **Concept: Recurrent Hidden State Dynamics**
  - Why needed here: Understanding `h_t = g(Wx_t + Uh_{t-1})` is prerequisite to grasping why vanilla RNNs suffer from vanishing gradients and why LSTM/GRU were invented.
  - Quick check question: Given sequence `[x1, x2, x3]`, can you trace how information from `x1` influences the output at `x3` through matrix multiplications?

- **Concept: Gating as Learnable Scalar Modulation**
  - Why needed here: Gates are sigmoid-activated scalars ∈ (0,1) that act as learned switches. Without this, you cannot interpret forget gate values near 0 vs. near 1.
  - Quick check question: If forget gate `f_t = 0.1` and `c_{t-1} = 5.0`, what portion of past memory is retained in `c_t`?

- **Concept: Sliding Window Supervision for Sequence-to-Vector Prediction**
  - Why needed here: The paper constructs input matrix `X` (windows of size `w`) mapping to target matrix `Y` (next `f` values). This framing converts time series to supervised learning.
  - Quick check question: For a sequence `[0.1, 0.2, 0.3, 0.4, 0.5]` with `w=3` and `f=1`, what is `X[0]` and `Y[0]`?

- **Concept: Directional Accuracy vs. RMSE Tradeoffs**
  - Why needed here: RMSE penalizes magnitude errors; DA measures trend direction correctness. A model can have low RMSE but DA≈50% (no better than random direction).
  - Quick check question: If predictions are always 1% below actuals but direction matches, which metric is high and which is low?

## Architecture Onboarding

- **Component map**: Input (window of w=60 normalized values) → RNN Layer (128 LSTM or GRU units, sigmoid+tanh activations) → Dense Layer (f units, linear activation for regression) → Output (f-step ahead predictions)

- **Critical path**:
  1. Normalize each series independently to [0,1] using its own min/max
  2. Construct sliding-window tensors `X` (shape: `[N, 60]`) and `Y` (shape: `[N, f]`)
  3. Train on first series only; evaluate on all 10 series using last 251 samples as test set
  4. Denormalize predictions for visual inspection using test series min/max

- **Design tradeoffs**:
  - Single-series training vs. multi-series: Single series is simpler but risks overfitting to idiosyncrasies; multi-series may improve generalization if patterns are truly shared
  - Window size (w=60): Longer windows capture more context but increase computation and may include irrelevant history
  - LSTM vs. GRU: GRU has fewer parameters (2 gates vs. 3), often trains faster; LSTM may excel on longer dependencies
  - 200 epochs: Chosen empirically; early stopping not mentioned. Risk of overfitting on small data

- **Failure signatures**:
  - RMSE ≈ baseline, DA ≈ 50%: Model learned to output last value (random walk behavior)
  - Predictions show lagged tracking: Model predicts `y_t ≈ x_{t-1}`
  - Loss plateaus early with high error: Possible vanishing gradients, learning rate too high, or normalization issues

- **First 3 experiments**:
  1. Reproduce Activities dataset results: Train LSTM and GRU on first series, evaluate RMSE/DA on all 10 series for `f=1`
  2. Ablate window size: Test `w ∈ {20, 40, 60, 80}` on Activities to characterize sensitivity
  3. Multi-series training test: Train on all 9 series (exclude 1 for testing) and compare to single-series training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do statistical and other machine learning forecasting methods compare to the proposed LSTM and GRU implementation on the provided datasets?
- Basis in paper: [explicit] The Conclusion states the authors plan to "benchmark different forecasting methods against the method presented here," specifically evaluating "statistical methods as well as other machine learning methods."
- Why unresolved: The current study limits its comparison to a naive baseline, omitting comparisons with standard statistical models (e.g., ARIMA) or other architectures mentioned in the literature review.
- What evidence would resolve it: A comparative performance analysis (RMSE and DA) on the BANKEX and Activities datasets using algorithms like ARIMA, CNN, or ELM.

### Open Question 2
- Question: Can specific hyperparameter optimization enable LSTM or GRU networks to significantly outperform the naive baseline on the BANKEX stock market dataset?
- Basis in paper: [explicit] The Conclusion notes the poor performance on stock data, suggesting the authors "failed at optimizing the hyperparameters of the networks."
- Why unresolved: The architecture (128 units, 200 epochs) was set empirically based on the synthetic dataset and may not be suitable for the non-repetitive nature of the financial data.
- What evidence would resolve it: Results showing significantly lower RMSE or higher Directional Accuracy on the BANKEX dataset following a systematic search for optimal network depth, learning rates, or window sizes.

### Open Question 3
- Question: Does incorporating exogenous variables (extra information) allow these networks to successfully forecast stock market time series?
- Basis in paper: [explicit] The Conclusion posits that the failure to beat the baseline on stocks implies the authors "would need extra information that is not reflected in stock market series alone."
- Why unresolved: The current implementation relies solely on historical closing prices (univariate), which may be insufficient for the "nature" of financial series.
- What evidence would resolve it: Improved forecasting metrics resulting from training the networks on multivariate data (e.g., trading volume, open prices, or macroeconomic indicators) rather than closing prices alone.

## Limitations
- Small sample size with only two datasets (10 series each) limits generalizability
- Single-series training approach lacks theoretical justification for pattern transfer
- No exploration of hyperparameter sensitivity or feature engineering
- Synthetic dataset may not reflect real-world complexity

## Confidence
- **High Confidence**: LSTM/GRU architectures correctly implemented; Activities dataset results reproducible and statistically significant
- **Medium Confidence**: Single-series training claims rely on strong assumptions about pattern uniformity not fully validated
- **Low Confidence**: Specific reasons for GRU vs LSTM performance differences on different forecast horizons not explained

## Next Checks
1. **Cross-dataset generalization test**: Apply single-series trained models from Activities dataset to a third real-world dataset with similar repeating patterns
2. **Feature ablation study**: Add technical indicators to BANKEX dataset to test whether enriched features enable RNNs to extract signal
3. **Hyperparameter sensitivity analysis**: Systematically vary window size and number of units on Activities dataset to identify optimal configurations