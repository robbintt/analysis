---
ver: rpa2
title: A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification
  and Sentiment Analysis of Bangla News
arxiv_id: '2511.18618'
source_url: https://arxiv.org/abs/2511.18618
tags:
- sentiment
- headline
- dataset
- classification
- bangla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of simultaneously classifying
  Bangla news headlines and analyzing their sentiment, a task complicated by limited
  resources, class imbalance, and the morphological complexity of Bangla. The proposed
  BERT-CNN-BiLSTM hybrid model combines transformer-based contextual embeddings (BERT)
  with convolutional feature extraction (CNN) and bidirectional LSTM with attention
  to capture both local and global linguistic patterns.
---

# A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News

## Quick Facts
- arXiv ID: 2511.18618
- Source URL: https://arxiv.org/abs/2511.18618
- Reference count: 40
- Primary result: Achieves 81.37% accuracy and 81.54% F1-score for Bangla headline classification, 73.43% accuracy and 73.71% F1-score for sentiment analysis

## Executive Summary
This study introduces a novel BERT-CNN-BiLSTM hybrid model to simultaneously classify Bangla news headlines and analyze their sentiment. The framework addresses the challenges of limited resources, class imbalance, and morphological complexity in Bangla language processing by combining transformer-based contextual embeddings with CNN feature extraction and BiLSTM with attention mechanisms. The proposed approach achieves state-of-the-art performance on the BAN-ABSA dataset of 9,014 headlines, demonstrating superior results compared to existing methods while providing a strong baseline for future research in low-resource language processing.

## Method Summary
The proposed BERT-CNN-BiLSTM framework integrates three key components: BERT for contextual word embeddings that capture semantic relationships, CNN for extracting local n-gram features, and BiLSTM with attention to model sequential dependencies and identify salient information. The model employs undersampling and oversampling strategies before and after dataset splitting, with 5-fold cross-validation for robust evaluation. The hybrid architecture leverages the strengths of each component—BERT's contextual understanding, CNN's pattern recognition, and BiLSTM's sequential modeling—to achieve simultaneous headline classification and sentiment analysis in Bangla.

## Key Results
- Achieved 81.37% accuracy and 81.54% F1-score for Bangla headline classification
- Achieved 73.43% accuracy and 73.71% F1-score for sentiment analysis
- Oversampling before dataset splitting yielded the best performance metrics
- Demonstrated strong generalization on external validation datasets

## Why This Works (Mechanism)
The model's effectiveness stems from its multi-layered approach to capturing linguistic patterns. BERT provides deep contextual embeddings that understand word meanings in context, CNN layers identify local patterns and n-gram features crucial for headline classification, and BiLSTM with attention mechanisms model long-range dependencies while focusing on sentiment-bearing words. The simultaneous learning of both tasks allows the model to leverage shared linguistic features, improving overall performance through task-specific fine-tuning of the combined architecture.

## Foundational Learning
- **BERT embeddings**: Transform contextual word representations needed for understanding Bangla's morphological complexity; quick check: verify embedding dimensions match input requirements
- **CNN feature extraction**: Captures local patterns and n-grams essential for headline classification; quick check: confirm filter sizes align with linguistic units
- **BiLSTM with attention**: Models sequential dependencies and identifies salient features for sentiment; quick check: validate attention weights focus on sentiment-bearing tokens
- **Class imbalance handling**: Undersampling/oversampling techniques required due to skewed class distributions; quick check: verify class distributions post-sampling
- **5-fold cross-validation**: Ensures robust evaluation across dataset partitions; quick check: confirm each fold maintains representative class proportions
- **Hybrid architecture integration**: Combines complementary strengths of transformer, CNN, and RNN components; quick check: test individual component contributions via ablation

## Architecture Onboarding

**Component Map**: Input Text → BERT Embeddings → CNN Feature Extraction → BiLSTM with Attention → Headline Classification and Sentiment Analysis Outputs

**Critical Path**: The most critical sequence is BERT → CNN → BiLSTM → Output, as this captures contextual understanding, local features, and sequential dependencies in order.

**Design Tradeoffs**: The model trades computational complexity for improved performance by combining multiple architectures rather than using a simpler single-model approach. The hybrid design requires careful parameter tuning but provides superior feature extraction capabilities.

**Failure Signatures**: Performance degradation may occur when encountering out-of-vocabulary words, complex syntactic structures, or when class imbalance persists despite sampling techniques. The model may also struggle with sarcasm or nuanced sentiment expressions common in news headlines.

**3 First Experiments**:
1. Evaluate individual component performance (BERT-only, CNN-only, BiLSTM-only) to establish baseline contributions
2. Test different oversampling strategies (SMOTE, random oversampling) to optimize class balance
3. Compare attention mechanism effectiveness against non-attention BiLSTM variants

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The dataset size of 9,014 headlines may not capture full linguistic diversity of Bangla news content
- Oversampling techniques may introduce bias affecting generalizability to real-world news
- Model complexity limits interpretability of component contributions
- Focus on headline-level analysis rather than full article content

## Confidence
- Headline classification performance: High
- Sentiment analysis component: Medium
- Joint learning framework superiority: Medium

## Next Checks
1. External validation on larger, more diverse Bangla news datasets to assess generalizability across different news domains and time periods
2. Ablation studies to systematically evaluate the contribution of each architectural component (BERT, CNN, BiLSTM, attention)
3. Comparative analysis with other state-of-the-art multilingual transformer models specifically fine-tuned for Bangla