---
ver: rpa2
title: 'REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM'
arxiv_id: '2505.04673'
source_url: https://arxiv.org/abs/2505.04673
tags:
- safety
- harm
- language
- policy
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces REVEAL, a scalable automated framework for
  evaluating vision language models (VLLMs) across three harm categories: sexual harm,
  violence, and misinformation. The framework generates multi-turn adversarial conversations
  by mining real-world images, creating topic seeds, and applying crescendo attack
  strategies to progressively escalate harmfulness.'
---

# REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM

## Quick Facts
- arXiv ID: 2505.04673
- Source URL: https://arxiv.org/abs/2505.04673
- Reference count: 20
- Primary result: Multi-turn adversarial conversations reveal significantly higher safety defects in VLLMs compared to single-turn evaluations

## Executive Summary
This paper introduces REVEAL, a scalable automated framework for evaluating vision language models (VLLMs) across three harm categories: sexual harm, violence, and misinformation. The framework generates multi-turn adversarial conversations by mining real-world images, creating topic seeds, and applying crescendo attack strategies to progressively escalate harmfulness. Evaluations across five state-of-the-art VLLMs (GPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtral) revealed that multi-turn interactions result in significantly higher defect rates compared to single-turn evaluations, with Llama-3.2 showing the highest multi-turn defect rate (16.55%) and Qwen2-VL the highest refusal rate (19.1%). GPT-4o demonstrated the most balanced performance as measured by the Safety-Usability Index (SUI), followed closely by Pixtral.

## Method Summary
REVEAL employs a five-component pipeline to evaluate VLLM safety. First, it takes harm policy definitions as input, then mines relevant real-world images via Bing Image Search API. Next, it generates conversational seeds using GPT-4o based on the image-policy pairs, expands these seeds into multi-turn crescendo conversations (5-7 turns), and finally uses GPT-4o as an automated evaluator to judge policy violations. The framework was validated against five VLLMs using a dataset of ~320 expanded conversations per policy, with human validation achieving Cohen's Kappa >0.8 for evaluator consistency.

## Key Results
- Multi-turn defect rates exceeded single-turn rates across all models, with Llama-3.2 showing 16.55% defect rate in multi-turn settings
- GPT-4o achieved the highest Safety-Usability Index (SUI), balancing safety and usability better than other models
- Misinformation was identified as the most challenging harm category, requiring enhanced contextual defenses
- The framework demonstrated modularity and adaptability to new harm policies

## Why This Works (Mechanism)

### Mechanism 1: Progressive Contextual Escalation (Crescendo Attacks)
Gradual escalation of harmful intent within conversation history bypasses safety filters more effectively than direct attacks. The framework generates user turns that begin benign, integrate a mined image, and incrementally shift toward prohibited content, leveraging the model's context window to normalize harmful trajectories.

### Mechanism 2: Cross-Modal Misalignment Exploitation
Pairing text inputs with contextually relevant real-world images induces safety failures that text-only inputs would not trigger. The visual modality may override or confuse text-based safety classifiers, creating a "distraction" or implicit instruction that the model fails to refuse.

### Mechanism 3: Synthetic Policy-Guided Evaluation
Automating adversarial generation and evaluation based on specific, granular harm policies provides a scalable alternative to static human-curated datasets. The framework uses GPT-4o to both generate attacks targeting policy definitions and judge compliance, validated by human agreement (Cohen's Kappa > 0.8).

## Foundational Learning

- **Concept: Adversarial Alignment vs. Capability**
  - Why needed: The paper distinguishes between a model's ability to perform tasks (usability) and its ability to refuse harmful ones (safety), requiring understanding of the "Safety-Usability Index" (SUI).
  - Quick check: If a model refuses 100% of requests, is it "safe"? Why might the authors consider this a failure mode?

- **Concept: Cross-Modal Context Windows**
  - Why needed: REVEAL relies on multi-turn interactions where images and text build up history, requiring understanding that the model processes the entire sequence to predict the next token.
  - Quick check: How does the introduction of an image in turn 3 affect the probability distribution of the response in turn 4 compared to a text-only history?

- **Concept: Black-Box Evaluation**
  - Why needed: The framework evaluates VLLMs without access to internal weights or gradients, treating the model as an opaque function.
  - Quick check: Why is "black-box" evaluation critical for proprietary models, and what data can you not access in this paradigm?

## Architecture Onboarding

- **Component map:** Harm Policy -> Image Mining -> Seed Generation -> Conversational Expansion -> Target VLLM -> Evaluator
- **Critical path:** The Seed Generation and Expansion steps are critical; if the seed is not contextually relevant to the mined image or expansion fails to escalate naturally, the target VLLM will likely refuse immediately.
- **Design tradeoffs:** Automation vs. ground truth (relying on GPT-4o creates model-in-the-middle risk), modularity (allows swapping image sources but depends heavily on prompt engineering for attack quality).
- **Failure signatures:** High refusal rate indicates over-safety/low usability (e.g., Qwen2-VL), high defect rate indicates low safety (e.g., Llama-3.2), hallucination is harder to detect than explicit violence.
- **First 3 experiments:**
  1. Run REVEAL dataset against text-only model version to quantify "image-input" contribution to defect rate
  2. Modify harm policy to be stricter and observe impact on defect vs. refusal rates
  3. Manually evaluate 10% sample of conversations to verify automated evaluator's Cohen's Kappa

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does REVEAL's effectiveness generalize to non-English languages and diverse cultural contexts?
- Basis: All experiments are currently limited to English despite framework extensibility
- Why unresolved: Adversarial strategies and harm interpretations may vary significantly across languages
- Evidence needed: Defect and refusal rates for VLLMs tested with non-English adversarial conversations

### Open Question 2
- Question: Can synthetic image generation replace web-mined images without reducing evaluation efficacy?
- Basis: Framework relies solely on web-mined images without exploring image generation substitutes
- Why unresolved: Unclear if AI-generated images capture necessary visual nuances to elicit same compliance
- Evidence needed: Comparative study of defect rates using generated vs. mined images for same seed sets

### Open Question 3
- Question: What safety training interventions are required to mitigate high VLLM susceptibility to misinformation in multi-turn settings?
- Basis: Misinformation emerged as significant challenge requiring enhanced contextual defenses
- Why unresolved: Paper identifies vulnerability but doesn't investigate technical solutions
- Evidence needed: Experiments measuring performance improvements in VLLMs fine-tuned with contextual defense strategies

## Limitations
- Framework relies on GPT-4o as both generator and evaluator, creating potential model-in-the-middle bias propagation
- Exact prompt templates and few-shot examples for evaluator are not fully specified in paper
- Image mining depends on Bing API with disabled safe search, but exact parameters are not detailed
- Experiments are limited to English language, restricting generalizability

## Confidence
- **High confidence:** Multi-turn defect rates exceed single-turn rates across all models; SUI metric is clearly defined and consistently applied
- **Medium confidence:** Crescendo attacks more effective than direct attacks (theoretically sound but not explicitly validated)
- **Medium confidence:** Cross-modal misalignment exploitation demonstrated but specific modality contributions not isolated

## Next Checks
1. **Modality Isolation Test:** Run REVEAL dataset against text-only model version to quantify "image-input" contribution to defect rate
2. **Escalation Strategy Comparison:** Compare crescendo attacks against direct escalation strategies by measuring relative defect rates
3. **Evaluator Consistency Verification:** Manually evaluate 10% sample of conversations to verify automated evaluator's performance across different target models