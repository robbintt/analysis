---
ver: rpa2
title: 'QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question
  Answering'
arxiv_id: '2508.05197'
source_url: https://arxiv.org/abs/2508.05197
tags:
- query
- image
- search
- answer
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QA-Dragon, a query-aware dynamic RAG system
  for knowledge-intensive visual question answering that improves over traditional
  methods by integrating domain-specific reasoning and adaptive retrieval strategies.
  It features a domain router for subject classification, a search router for dynamic
  retrieval strategy selection, and specialized image and text search agents that
  support multimodal, multi-turn, and multi-hop reasoning.
---

# QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering

## Quick Facts
- arXiv ID: 2508.05197
- Source URL: https://arxiv.org/abs/2508.05197
- Reference count: 21
- Outperforms baselines by 5.06% (single-source), 6.35% (multi-source), and 5.03% (multi-turn) accuracy on Meta CRAG-MM Challenge

## Executive Summary
QA-Dragon introduces a query-aware dynamic RAG system for knowledge-intensive visual question answering that improves over traditional methods by integrating domain-specific reasoning and adaptive retrieval strategies. It features a domain router for subject classification, a search router for dynamic retrieval strategy selection, and specialized image and text search agents that support multimodal, multi-turn, and multi-hop reasoning. Evaluated on the Meta CRAG-MM Challenge, QA-Dragon significantly improves answer accuracy and knowledge overlap, demonstrating its effectiveness in handling complex VQA scenarios.

## Method Summary
QA-Dragon is a multimodal RAG pipeline that processes image-question pairs through a domain router (fine-tuned BLIP-2), domain-specific chain-of-thought reasoning, and a search router that dynamically selects retrieval strategies. The system employs specialized image and text search agents with a coarse-to-fine multimodal reranker, followed by post-answer generation and dual verification. It supports single-source, multi-source, and multi-turn reasoning tasks while maintaining latency constraints through adaptive routing and evidence filtering.

## Key Results
- Improves single-source accuracy by 5.06% over baseline
- Improves multi-source accuracy by 6.35% over baseline
- Improves multi-turn accuracy by 5.03% over baseline
- Achieves 41.42%/41.65% overlap scores on single/multi-source tasks with full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1: Query-Aware Dynamic Routing
Conditional routing based on reasoning traces improves retrieval efficiency and answer accuracy compared to static retrieval strategies. The Search Router analyzes features from the D-CoT reasoning trace—answerability flags, task heuristics (OCR, arithmetic, named entities), and uncertainty patterns—to dispatch queries to one of three branches: Direct Output (self-contained), Search Verify (needs evidence checking), or RAG-Augment (requires external knowledge).

### Mechanism 2: Domain-Specialized Chain-of-Thought (D-CoT)
Domain-aware prompting with in-domain few-shot examples generates more useful preliminary reasoning traces for downstream routing decisions. BLIP-2 is fine-tuned on competition domain annotations to classify queries into 14 domains, then selects specialized prompt templates with curated ICL examples to constrain the MLLM to identify exact objects, reason step-by-step, and signal uncertainty explicitly.

### Mechanism 3: Coarse-to-Fine Multimodal Reranking with Fusion
Cascaded reranking filtering noisy evidence and improves final answer quality without proportional latency increase. Stage 1 uses Q-Former to compute coarse relevance scores via max-similarity between query tokens and evidence embeddings. Stage 2 uses Qwen3-Reranker for pointwise fine-grained scoring. Final evidence is assembled from top-K2 chunks sorted by combined score (s_coarse × s_fine).

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG) fundamentals
  - Why needed here: QA-Dragon is a RAG system; understanding the baseline pattern (retrieve → augment → generate) is prerequisite to grasping the added complexity of dynamic routing and multimodal retrieval.
  - Quick check question: Can you explain why naive RAG (retrieve once, generate once) fails on multi-hop queries?

- Concept: Multimodal embeddings and cross-modal retrieval
  - Why needed here: The Image Search Agent uses CLIP embeddings; the reranker uses Q-Former for vision-language alignment. Without understanding how text and image embeddings relate, the fusion mechanism is opaque.
  - Quick check question: What does CLIP optimize during pretraining, and why does cosine similarity work for cross-modal retrieval?

- Concept: Chain-of-Thought (CoT) prompting and uncertainty quantification
  - Why needed here: D-CoT generates reasoning traces for routing; the Verifier uses token probabilities (s_min, s_mean) to detect hallucinations. Understanding CoT and uncertainty signals is essential for debugging routing failures.
  - Quick check question: How would you interpret a low s_min but high s_mean in the verifier output?

## Architecture Onboarding

- Component map: Input image+query → Domain Router (BLIP-2) → D-CoT (MLLM) → Search Router → branches into Direct Output, Search Verify, or RAG path via Tool Router → Image/Text Search Agents → Coarse-to-Fine Reranker → Post-Answer Generator + Verifier → Final output

- Critical path: The RAG-Augment branch exercises all components: Domain Router → D-CoT → Search Router (classifies as RAG) → Tool Router (decides image+text) → Image Search (object extraction → segmentation → search) → Fusion Search (combines object identity with text query) → Text Search → Reranker → Post-Answer → Verifier. Latency accumulates at each stage; Table 2 shows RAG path elapse of 4.15–5.97s vs. 0.71s for LLM-only baseline.

- Design tradeoffs:
  - Accuracy vs. latency: QA-Dragon★ (with fine-tuning) achieves best overlap (41.42%/41.65%) but highest latency (5.97s). QA-Dragon without fine-tuning balances accuracy (21.31%/23.22%) with lower latency (4.15s/4.79s).
  - Retrieval modality: Image search is essential for object identification; text search provides factual attributes. Ablation shows removing Tool Router drops accuracy to 18.32% (single-source).
  - Verification overhead: Dual verification (white-box + MLLM-based) adds inference passes; the paper does not report ablation on verification alone.

- Failure signatures:
  - Low accuracy on aggregation queries (5.8% single-source, 6.7% multi-source) indicates multi-hop cross-source reasoning remains a weakness.
  - Real-time queries yield 0% accuracy, suggesting temporal grounding is unsolved.
  - Plant domain accuracy is lowest (7.5%/9.8%), reflecting fine-grained visual discrimination challenges.
  - Removing Query Splitting degrades multi-source performance, indicating complex queries are under-served by monolithic retrieval.

- First 3 experiments:
  1. Reproduce baseline comparison: Run LLM-only, CoT, and Direct RAG on a subset of validation data to calibrate your pipeline against Table 2 baselines.
  2. Ablate the Search Router: Force all queries through the RAG branch (skip routing) and measure accuracy/latency delta vs. full QA-Dragon.
  3. Stress-test the Reranker thresholds: Vary τ_coarse and τ_fine on a held-out set and plot accuracy vs. evidence count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multimodal RAG systems be enhanced to handle temporally sensitive (real-time) queries, where current systems achieve near-zero accuracy?
- Basis in paper: Section 5 states: "Accuracy decreases steadily as query dynamism increases, with real-time queries yielding no correct answers."
- Why unresolved: Real-time queries require up-to-date information beyond static knowledge bases, and existing retrieval mechanisms lack temporal indexing or recency-aware ranking.
- What evidence would resolve it: Demonstrating improved accuracy on real-time query subsets through temporal metadata integration, recency-weighted retrieval, or live API augmentation within latency constraints.

### Open Question 2
- Question: What architectural improvements can address the severe performance gap in aggregation-type queries, which achieve only 5.8–6.7% accuracy?
- Basis in paper: Section 5 reports Aggregation as the most difficult query category, attributing failure to "its multi-hop and cross-source nature."
- Why unresolved: Aggregation requires synthesizing information across multiple entities, sources, and reasoning steps—current retrieval and reranking may return fragmented evidence insufficient for cross-source synthesis.
- What evidence would resolve it: Showing significant accuracy gains on aggregation queries via explicit multi-hop reasoning modules, cross-source evidence fusion mechanisms, or structured knowledge graph traversal.

### Open Question 3
- Question: Can QA-Dragon's domain-specific routing and retrieval strategies generalize to domains and query types beyond the CRAG-MM benchmark without task-specific fine-tuning?
- Basis in paper: The domain router is fine-tuned on CRAG-MM's 14 domain annotations; all experiments are confined to this benchmark.
- Why unresolved: The system's reliance on domain-specific prompts, in-domain few-shot examples, and benchmark-specific tuning raises concerns about out-of-distribution generalization.
- What evidence would resolve it: Evaluation on additional multimodal VQA benchmarks (e.g., OK-VQA, A-OKVQA, E-VQA) demonstrating competitive performance without architecture changes or domain-specific retraining.

### Open Question 4
- Question: How can the latency-performance trade-off be optimized to achieve near-baseline latency while retaining the accuracy gains of the full QA-Dragon pipeline?
- Basis in paper: Section 4.2 notes: "The most powerful variant, QA-Dragon★, achieves the best performance but incurs the highest latency (5.97s)."
- Why unresolved: Adding components improves accuracy but cumulatively increases inference time; the system lacks adaptive computation or early-exit mechanisms.
- What evidence would resolve it: Demonstrating that a latency-constrained variant (e.g., <2s) achieves comparable accuracy through cascaded early exit, parallel retrieval, or lightweight router alternatives.

## Limitations

- The Search Router classifier performance and training setup are not specified, raising questions about routing reliability.
- Domain Router fine-tuning details (dataset split, learning rate, class balance) are omitted, limiting generalizability assessment.
- Reranker thresholds appear tuned for the competition dataset; their transferability to other VQA tasks is untested.
- The system achieves near-zero accuracy on real-time queries and only 5.8–6.7% on aggregation queries, indicating fundamental limitations in temporal grounding and multi-hop reasoning.

## Confidence

- High: Accuracy and overlap improvements over baselines (5.03–6.35%) are directly measurable from Table 2 and supported by ablation in Table 4.
- Medium: The mechanism of query-aware routing is logically coherent and partially validated by routing ablation, but the internal classifier performance and failure modes are not quantified.
- Low: Claims about domain specialization improving reasoning traces rely on the absence of ablation on D-CoT alone; the specific impact of domain-specific prompts versus generic CoT is not isolated.

## Next Checks

1. Profile the Search Router classifier: Train a simple logistic regression or small MLP on the published reasoning-trace features (answerability, OCR, arithmetic, entity, uncertainty) and evaluate per-class accuracy. This isolates whether the routing logic is robust or brittle.

2. Ablate the reranker thresholds: Systematically sweep τ_coarse, τ_fine, and τ_white on a held-out set and plot accuracy vs. evidence count and latency. This quantifies the tradeoff and tests whether the paper's thresholds are overfit.

3. Stress-test domain generalization: Hold out one domain (e.g., Plant) during Domain Router fine-tuning and measure routing accuracy and downstream QA performance on the held-out domain. This validates whether the 14-domain classification transfers or collapses on rare categories.