---
ver: rpa2
title: Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models
arxiv_id: '2601.16378'
source_url: https://arxiv.org/abs/2601.16378
tags:
- tokens
- spatial
- perspective-taking
- reasoning
- orientation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates the persistent egocentric bias in multimodal\
  \ language models (MLMs) during spatial reasoning tasks that require adopting another\
  \ agent's visual perspective. To address this, the authors introduce perspective\
  \ tokens\u2014specialized embeddings that encode orientation information through\
  \ either embodied body-keypoint cues or abstract rotation representations."
---

# Cognitively-Inspired Tokens Overcome Egocentric Bias in Multimodal Models

## Quick Facts
- arXiv ID: 2601.16378
- Source URL: https://arxiv.org/abs/2601.16378
- Authors: Bridget Leonard; Scott O. Murray
- Reference count: 4
- The study introduces perspective tokens that encode orientation information to overcome egocentric bias in multimodal language models during spatial reasoning tasks.

## Executive Summary
This study addresses a fundamental limitation in multimodal language models (MLMs): their persistent egocentric bias when performing spatial reasoning tasks that require adopting another agent's visual perspective. The authors introduce perspective tokensâ€”specialized embeddings that encode orientation information through either embodied body-keypoint cues or abstract rotation representations. These tokens are integrated into LLaVA-1.5-13B through fine-tuning, enabling the model to reason over non-egocentric viewpoints. The approach demonstrates substantial improvements across both synthetic and naturalistic benchmarks, with embodiment tokens achieving 95% accuracy on unaligned perspective-taking tasks and rotation tokens enabling generalization to non-human references.

## Method Summary
The authors developed perspective tokens as specialized embeddings that encode orientation information, which can be integrated into multimodal models through fine-tuning. Two types of tokens were explored: embodiment tokens using body-keypoint cues and rotation tokens using abstract rotation representations. These tokens were added to LLaVA-1.5-13B and fine-tuned on datasets requiring perspective-taking abilities. The fine-tuning process allows the model to learn how to incorporate these orientation signals when reasoning about spatial relationships from non-egocentric viewpoints.

## Key Results
- Embodiment tokens achieved 95% accuracy on unaligned perspective-taking tasks, up from 0% without the tokens
- Rotation tokens enabled generalization to non-human references like animals and furniture
- Representational analyses showed that fine-tuning enhanced latent orientation sensitivity already present in the base model
- The approach demonstrated effectiveness across synthetic (Isle Bricks V2, perspective-taking benchmark) and naturalistic (COCO, 3DSRBench) benchmarks

## Why This Works (Mechanism)
The perspective tokens work by providing explicit orientation information that the model can condition on when reasoning about spatial relationships. This addresses the fundamental challenge that MLMs typically reason from an egocentric viewpoint, making it difficult to understand spatial relationships from other perspectives. The embodied body-keypoint tokens leverage human-like spatial reasoning frameworks, while rotation tokens provide abstract geometric information. Through fine-tuning, the model learns to integrate these signals into its reasoning process, effectively shifting from an egocentric to an allocentric frame of reference when needed.

## Foundational Learning
- **Egocentric vs. allocentric spatial reasoning**: Why needed - Understanding the distinction between self-centered and other-centered spatial representations; Quick check - Can the model distinguish between "to the left of me" versus "to the left of the person in the image"?
- **Multimodal embeddings**: Why needed - Understanding how visual and textual information are combined in MLMs; Quick check - How are visual features transformed into the model's embedding space?
- **Fine-tuning vs. training from scratch**: Why needed - Recognizing when additional training can enhance model capabilities; Quick check - Does the model maintain its original capabilities while gaining perspective-taking abilities?
- **Spatial attention mechanisms**: Why needed - Understanding how models focus on relevant spatial information; Quick check - How does the model attend to different regions when reasoning about perspective?

## Architecture Onboarding
**Component map**: Input image -> Visual encoder -> Fusion with text embeddings -> Perspective token injection -> Reasoning module -> Output response
**Critical path**: Visual input flows through the visual encoder, merges with text embeddings at the fusion layer, perspective tokens are injected at this fusion point, then the combined representation flows through the reasoning module to produce the output
**Design tradeoffs**: The token-based approach is lightweight and model-agnostic but requires careful design of the token embeddings and may not capture all nuances of perspective-taking compared to more comprehensive architectural changes
**Failure signatures**: Models may revert to egocentric reasoning when tokens are ambiguous or missing, struggle with complex scenes involving multiple agents, or fail to generalize beyond trained perspectives
**3 first experiments**:
1. Test perspective token performance on simple two-object scenes with clear reference points
2. Evaluate token effectiveness when the reference agent is partially occluded
3. Compare embodiment tokens versus rotation tokens on tasks requiring human vs. non-human perspective-taking

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on synthetic datasets (Isle Bricks V2, perspective-taking benchmark) and specific image captioning datasets (COCO, 3DSRBench), which may not capture full real-world complexity
- The mechanism by which tokens enable allocentric reasoning remains somewhat opaque despite representational analyses
- Model-agnostic claims are not extensively validated across different MLM architectures beyond LLaVA-1.5-13B

## Confidence
**High confidence in**: The effectiveness of perspective tokens in reducing egocentric bias on tested benchmarks, particularly the Isle Bricks V2 and unaligned perspective-taking tasks.

**Medium confidence in**: The generalization of findings to naturalistic datasets (COCO, 3DSRBench) and the model-agnostic nature of the approach.

**Low confidence in**: The precise cognitive mechanisms underlying token-based allocentric reasoning and the scalability of this approach to more complex spatial reasoning tasks beyond relative positioning.

## Next Checks
1. Test the perspective token approach on more diverse, real-world datasets with varied spatial reasoning challenges, including outdoor scenes and dynamic environments, to assess robustness beyond synthetic and curated benchmarks.

2. Conduct ablation studies comparing embodied body-keypoint tokens versus abstract rotation tokens across different MLM architectures (e.g., GPT-4V, Flamingo) to validate model-agnostic claims and identify architectural dependencies.

3. Perform neuroimaging-inspired analyses to better characterize how perspective tokens modify internal representations, potentially using techniques like probing classifiers or attention visualization to map the emergence of allocentric reasoning pathways.