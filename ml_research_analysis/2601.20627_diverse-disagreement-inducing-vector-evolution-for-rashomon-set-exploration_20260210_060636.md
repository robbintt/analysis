---
ver: rpa2
title: 'DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration'
arxiv_id: '2601.20627'
source_url: https://arxiv.org/abs/2601.20627
tags:
- rashomon
- diverse
- film
- latent
- cma-es
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DIVERSE is a gradient-free method for exploring the Rashomon set
  of deep neural networks by augmenting a pretrained model with Feature-wise Linear
  Modulation (FiLM) layers and using Covariance Matrix Adaptation Evolution Strategy
  (CMA-ES) to search a latent modulation space. This approach generates diverse model
  variants without retraining or gradient access.
---

# DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration
## Quick Facts
- **arXiv ID:** 2601.20627
- **Source URL:** https://arxiv.org/abs/2601.20627
- **Reference count:** 40
- **Primary result:** A gradient-free method for exploring the Rashomon set by augmenting models with FiLM layers and using CMA-ES to search a latent modulation space.

## Executive Summary
DIVERSE introduces a gradient-free approach for exploring the Rashomon set of deep neural networks by augmenting pretrained models with Feature-wise Linear Modulation (FiLM) layers and using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space. This method generates diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models, achieving comparable diversity to retraining at reduced computational cost. The approach demonstrates robustness to hyperparameter settings and shows that diversity can be localized to specific FiLM layers, suggesting opportunities for targeted search.

## Method Summary
DIVERSE augments a pretrained neural network with FiLM layers that modulate intermediate activations using a shared latent vector z. The FiLM parameters are computed as γ(z) = 1 + tanh(zW_γ) and β(z) = tanh(zW_β), where W_γ and W_β are frozen random projection matrices. The method uses CMA-ES to optimize z by maximizing a fitness function that balances predictive disagreement (measured via Total Variation Distance and hard label mismatch) with loss tolerance constraints. This gradient-free approach enables efficient exploration of the Rashomon set without retraining or gradient access to the base model.

## Key Results
- DIVERSE generates diverse model variants with performance comparable to retraining while reducing computational cost
- The method successfully uncovers multiple high-performing yet functionally distinct models across MNIST, PneumoniaMNIST, and CIFAR-10
- Diversity can be effectively localized to specific FiLM layers through sensitivity analysis
- DIVERSE maintains robustness across different hyperparameter settings and achieves good diversity metrics under principled validation constraints

## Why This Works (Mechanism)
The method works by creating a continuous search space where small perturbations to the latent vector z induce significant changes in model predictions while maintaining similar loss values. The FiLM layers act as a differentiable interface that allows CMA-ES to explore this space efficiently without requiring gradients from the base model. By constraining the search to maintain performance within a tolerance ε, the method ensures that discovered models remain valid members of the Rashomon set while the disagreement-inducing objective drives diversity.

## Foundational Learning
- **Rashomon Set:** The set of models with similar performance but different predictions. Understanding this concept is crucial as it frames the exploration goal.
  - *Why needed:* Provides the theoretical foundation for why diverse models matter
  - *Quick check:* Verify that different models can achieve similar accuracy but make different predictions on the same inputs
- **FiLM (Feature-wise Linear Modulation):** A conditioning technique that scales and shifts feature maps using learned parameters
  - *Why needed:* Enables parameter-efficient model variation without retraining
  - *Quick check:* Confirm that FiLM layers can be inserted after dense/conv blocks and modulated by a shared vector
- **CMA-ES (Covariance Matrix Adaptation Evolution Strategy):** An evolutionary algorithm for continuous optimization that adapts its search distribution
  - *Why needed:* Provides gradient-free optimization suitable for black-box fitness functions
  - *Quick check:* Ensure CMA-ES can optimize the latent vector while maintaining population diversity

## Architecture Onboarding
- **Component Map:** Pretrained Model → FiLM Layers → Fitness Function → CMA-ES Optimizer → Diverse Models
- **Critical Path:** Latent vector z → FiLM parameter computation → Model prediction → Fitness evaluation → CMA-ES update
- **Design Tradeoffs:** 
  - FiLM vs. weight perturbation: FiLM offers more structured, interpretable modifications
  - CMA-ES vs. gradient methods: Avoids gradient access requirements but may be slower
  - Loss tolerance ε: Stricter tolerance ensures better performance but may reduce diversity
- **Failure Signatures:**
  - Rashomon Ratio near zero: ε too strict or initialization too far from reference
  - Low diversity: Search space too constrained or CMA-ES step size too small
  - Computational inefficiency: Latent dimension too high for standard CMA-ES
- **First Experiments:**
  1. Implement FiLM layers on a simple MLP and verify parameter modulation works
  2. Test fitness function with synthetic data to ensure disagreement is correctly measured
  3. Run CMA-ES with small latent dimension (d=2) on MNIST to validate the complete pipeline

## Open Questions the Paper Calls Out
- Can targeted search strategies focusing on sensitivity-identified layers improve exploration efficiency compared to network-wide modulation?
- Can scalable CMA-ES variants like DD-CMA-ES overcome dimensionality constraints for high-latent-dimension searches?
- How do interactions between FiLM layers at different depths influence diversity, and does joint modulation differ from marginal effects?
- Does modulating attention mechanisms in Vision Transformers yield more diverse Rashomon sets than modulating only dense layers?

## Limitations
- The method is currently limited to moderate-sized latent dimensions (d < 16) due to CMA-ES scalability constraints
- Performance depends on the choice of λ weighting in the fitness function, which may not be optimal for all datasets
- The approach has only been validated on image classification tasks and may not generalize to other domains
- Random seed dependencies for FiLM initialization and CMA-ES make reproducibility challenging

## Confidence
- **High confidence:** The core algorithmic approach is technically sound and produces diverse models within specified constraints
- **Medium confidence:** Computational efficiency claims and scalability to larger latent dimensions need more extensive validation
- **Medium confidence:** Claims about localizing diversity to specific layers are supported but require more systematic investigation

## Next Checks
1. **Reproducibility check:** Implement the method with explicit random seeds and verify that diversity metrics match reported values within ±5% tolerance across all three datasets
2. **Sensitivity analysis:** Systematically vary λ in the fitness function (0.3, 0.5, 0.7) and ε tolerance to test robustness of reported results
3. **Architecture generalization test:** Apply the method to a non-image domain (e.g., tabular data with MLP) to verify broader applicability beyond image classification tasks