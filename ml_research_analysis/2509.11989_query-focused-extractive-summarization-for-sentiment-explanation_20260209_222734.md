---
ver: rpa2
title: Query-Focused Extractive Summarization for Sentiment Explanation
arxiv_id: '2509.11989'
source_url: https://arxiv.org/abs/2509.11989
tags:
- query
- sentiment
- summarization
- task
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of explaining the causes of sentiment
  in feedback documents through a specialized Query-Focused Summarization (QFS) task
  called Explicative Sentiment Summarization (ESS). The main difficulty lies in the
  linguistic dissonance between user queries and source documents, which can vary
  in language register and information content.
---

# Query-Focused Extractive Summarization for Sentiment Explanation

## Quick Facts
- **arXiv ID:** 2509.11989
- **Source URL:** https://arxiv.org/abs/2509.11989
- **Reference count:** 17
- **Primary result:** MBTR significantly outperforms baselines, achieving ROUGE-SU4 F1 of 25.64 vs 16.69 for best baseline.

## Executive Summary
This paper addresses the challenge of explaining sentiment causes in feedback documents through Query-Focused Extractive Summarization (QFS). The authors propose a Multi-Bias TextRank (MBTR) model that combines multiple query formulations and incorporates sentiment classifier outputs to bridge linguistic gaps between user queries and source documents. Experiments on a proprietary dataset show MBTR significantly outperforms baseline models including MMR, QuerySum, and Biased TextRank, with the best configuration achieving ROUGE-SU4 F1 scores of 25.64.

## Method Summary
The method extends TextRank with a multi-bias framework that aggregates semantic similarity scores from multiple query encodings. The MBTR model incorporates three main biases: sentiment bias from classifier probabilities, query bias from expanded sentiment phrases, and Information Content Regularization that penalizes sentences based on their embedding norm distance from a target specificity level. The model uses asymmetric encoders for query expansion and symmetric encoders for sentence similarity, with hyperparameters α=0.1 and β=0.2 tuned for optimal performance.

## Key Results
- MBTR with optimal parameters (α=0.1, β=0.2) achieves ROUGE-SU4 F1 of 25.64, significantly outperforming baselines.
- Best baseline (Biased TextRank) achieves ROUGE-SU4 F1 of 16.69.
- Information Content Regularization with β=0.1 performs best among tested configurations.
- Sentiment-based query expansion improves performance over standard query formulations.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining multiple query formulations into a compound bias appears to bridge the "linguistic dissonance" (register and information content gaps) between user intent and source text better than single-query approaches.
- **Mechanism:** The Multi-Bias TextRank (MBTR) aggregates similarity scores from multiple query encodings (e.g., frequent reference words, expanded phrases) via summation. This compound bias vector replaces the single bias vector in the PageRank recursion, effectively allowing a sentence to be boosted if it matches *any* of the query formulations.
- **Core assumption:** The specific information need is distributed across different linguistic registers (colloquial vs. formal) and granularities; a single query cannot capture this variance.
- **Evidence anchors:**
  - [abstract] "propose a multi-bias framework to help bridge this gap... supporting multiple query formulations"
  - [section 3.1] "Intuitively, this is analogous to humans reformulating questions from multiple perspectives..."
  - [corpus] Related papers like *FG-RAG* and *DETQUS* focus on graph structures or decomposition for QFS, but do not explicitly validate the specific "multi-bias summation" approach, suggesting this is a distinct architectural choice.
- **Break condition:** If the query formulations are semantically redundant or if the aggregation method (summation) disproportionately amplifies noisy signals from weaker queries.

### Mechanism 2
- **Claim:** Penalizing candidate sentences based on the difference between their embedding norm and a target "reference" norm (Information Content Regularization) likely guides the summary toward a desired level of specificity.
- **Mechanism:** The model calculates a penalty term ($\beta \Delta IC$) derived from the absolute difference between a candidate sentence's vector norm and the average vector norm of a guide summary. This penalty is subtracted from the bias vector in the ranking equation, suppressing sentences that are either too generic or too specific relative to the reference.
- **Core assumption:** The vector norm of a sentence embedding correlates strongly with "Information Content" (specificity), as defined by Amigó et al. (2022), and the development set references represent the ideal specificity.
- **Evidence anchors:**
  - [section 3.3] "We leverage this feature to disfavor candidate sentences by their distance from the targeted level of specificity."
  - [section 5] "Dampening ICR performs best at $\beta=0.1$... suggests ERT's lesser regularization requirement as benefiting from its inherent proximity..."
  - [corpus] No direct validation of the "norm-as-information-content" hypothesis was found in the provided corpus signals; related work generally focuses on content retrieval rather than specificity regularization via norms.
- **Break condition:** If the underlying sentence encoder (e.g., SBERT) does not encode specificity monotonically with vector magnitude in the target domain, the regularization may arbitrarily suppress relevant sentences.

### Mechanism 3
- **Claim:** Incorporating a "Sentiment Bias" derived from classifier probabilities allows the extractive process to function as an explanation engine rather than just a retrieval engine.
- **Mechanism:** A sentiment classifier predicts the probability of the target sentiment (e.g., "negative") for each sentence. This probability forms a bias vector that is combined with the semantic query biases. This forces the centrality algorithm to prefer sentences that not only match the query keywords but also contain the targeted affective signal.
- **Core assumption:** The sentences containing the highest concentration of the target sentiment polarity are the ones that best "explain" the cause of that sentiment.
- **Evidence anchors:**
  - [abstract] "...use sentiment classifier outputs as an additional bias."
  - [section 3.4.2] "...we can utilize the probabilistic confidence in this sentiment for every input sentence to construct a sentiment bias vector."
  - [corpus] The corpus contains "Multilingual Sentiment Analysis of Summarized Texts," suggesting a known interplay between summarization and sentiment, but limited evidence for using classifier *confidence* as a ranking bias in graph-based summarization.
- **Break condition:** If the sentiment classifier is poorly calibrated or if the "cause" of sentiment is discussed in neutral terms (e.g., "The wait time was 4 hours" is a factual cause of negative sentiment but may be classified as neutral).

## Foundational Learning

- **Concept: Graph-based Centrality (TextRank/PageRank)**
  - **Why needed here:** The proposed MBTR model is built directly on top of TextRank. Understanding that nodes (sentences) are ranked based on their connections (similarity) to other important nodes is critical.
  - **Quick check question:** How does adding a "bias" vector modify the standard random surfer model in PageRank?

- **Concept: Vector Space Models & Norms**
  - **Why needed here:** The paper relies on cosine similarity for semantic matching and, critically, on the vector *norm* (length) for the Information Content Regularization mechanism.
  - **Quick check question:** Does normalizing vectors to unit length (common in semantic search) destroy the "Information Content" signal required for this specific architecture?

- **Concept: Extractive vs. Abstractive Summarization**
  - **Why needed here:** The authors explicitly frame their work as "extractive" to ensure "output traceability" and avoid hallucinations, contrasting it with generative models.
  - **Quick check question:** Why would an extractive approach be preferred over an abstractive one in high-stakes feedback analysis, even if the resulting summary is less fluent?

## Architecture Onboarding

- **Component map:**
  - Input: Source Documents + Query (Entity) + Sentiment Label (Pos/Neg) -> Preprocessing: Sentence segmentation (NLTK) + Phrase extraction (spaCy noun/verb chunks) -> Bias Generators: 1) Semantic Biases: Reference-term expansion (MPB2/BTR) using asymmetric encoders, 2) Sentiment Bias: Propagated classifier confidence scores -> Core Solver: Multi-Bias TextRank (MBTR) + Information Content Regularization (ICR) -> Output: Top-ranked sentences concatenated into a summary

- **Critical path:**
  1. **Encoder Selection:** You must use distinct encoders for specific tasks (asymmetric `msmarco-distilbert-base-v4` for short query-to-phrase matching vs. symmetric `xlm-r-distilroberta` for sentence-to-sentence similarity).
  2. **Parameter Tuning:** The paper highlights extreme sensitivity in alpha ($\alpha$) and beta ($\beta$). The standard TextRank recommendation ($\alpha=0.85$) failed here; $\alpha=0.1$ was required to prioritize query-focus over centrality.

- **Design tradeoffs:**
  - **Reference-dependence:** The best-performing configuration (ERT) requires a development set to extract "Frequent Reference-Words," which may not exist in zero-shot scenarios. The "Sentiment Biases" configuration is more general but performed slightly lower in experiments.
  - **Sentence vs. Phrase:** The system indexes sentences for the final summary but must index phrases (NP/VP) for the query expansion step, increasing preprocessing complexity.

- **Failure signatures:**
  - **Generic Summaries:** If $\beta$ is too low, the model may extract high-centrality but low-specificity sentences (e.g., "The product is great").
  - **Topic Drift:** If the sentiment classifier is inaccurate, the "Sentiment Bias" will pull the summary toward sentences that are sentiment-heavy but irrelevant to the specific entity query.

- **First 3 experiments:**
  1. **Baseline Validation:** Implement standard Biased TextRank (BTR) to verify the $\alpha=0.85$ vs $\alpha=0.1$ performance drop on your own data.
  2. **Ablation on Regularization:** Test MBTR with $\beta=0$ (off) vs. $\beta=0.2$ to confirm if "Information Content" norms actually correlate with summary quality in your domain.
  3. **Encoder Swap:** Swap the symmetric SBERT encoder for the asymmetric one in the main ranking loop to observe if query-document dissonance improves or if semantic coherence breaks.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the Explicative Sentiment Summarization (ESS) task and the Multi-Bias TextRank (MBTR) model be effectively adapted to publicly available Aspect-Based Sentiment Analysis (ABSA) datasets?
- **Open Question 2:** How does the MBTR framework compare to modern Large Language Models (LLMs) regarding factuality and hallucination rates in sentiment explanation?
- **Open Question 3:** Can the Compound Bias-Focused Summarization (CBFS) framework be integrated into non-TextRank QFS models without performance degradation?

## Limitations
- The proprietary dataset prevents independent verification of the claimed performance gains.
- The model relies on a single performance metric (ROUGE-SU4) without human evaluation of explanatory quality.
- Information Content Regularization assumes monotonic correlation between vector norms and specificity without empirical validation.

## Confidence
- **High Confidence:** The MBTR architecture and its general approach of combining multiple query formulations is well-specified and follows established graph-based ranking principles. The improvement over baseline models (BTR, MMR, QuerySum) on the reported metric is clearly demonstrated.
- **Medium Confidence:** The specific contribution of the Information Content Regularization component is plausible but relies on an unvalidated assumption about the relationship between vector norms and specificity. The sensitivity to hyperparameters (α=0.1, β=0.2) is noted but the underlying reasons are not fully explained.
- **Low Confidence:** The quality and impact of the sentiment classifier on the final summary are uncertain, as its architecture, training data, and performance are not disclosed. The claim that MBTR effectively bridges "linguistic dissonance" is supported by the results but lacks a detailed linguistic analysis.

## Next Checks
1. **IC Regularization Ablation:** Reproduce the MBTR model with β=0 (no regularization) and β=0.2 on a public dataset to measure the specific impact of Information Content Regularization on summary quality and specificity.
2. **Sentiment Classifier Impact:** Replace the paper's unspecified sentiment classifier with a standard, pre-trained model (e.g., a BERT-based sentiment analyzer) and measure the variance in summary performance to quantify the classifier's contribution.
3. **Human Evaluation:** Conduct a small-scale human evaluation on a subset of generated summaries to assess whether the extractive summaries are truly explanatory of the sentiment cause, beyond their ROUGE scores.