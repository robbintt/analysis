---
ver: rpa2
title: Criteria-Based LLM Relevance Judgments
arxiv_id: '2507.09488'
source_url: https://arxiv.org/abs/2507.09488
tags:
- relevance
- criteria
- judgments
- multi-criteria
- criterion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Multi-Criteria, a framework for LLM-based\
  \ relevance judgments that decomposes the concept of relevance into four distinct\
  \ criteria\u2014Exactness, Coverage, Topicality, and Contextual Fit. Each criterion\
  \ is evaluated independently via dedicated prompts, and the results are aggregated\
  \ into a final relevance label."
---

# Criteria-Based LLM Relevance Judgments
## Quick Facts
- arXiv ID: 2507.09488
- Source URL: https://arxiv.org/abs/2507.09488
- Reference count: 40
- Key outcome: Multi-criteria LLM framework achieves Spearman correlation of 0.99 on TREC datasets, outperforming baselines.

## Executive Summary
This paper introduces Multi-Criteria, a framework for LLM-based relevance judgments that decomposes the concept of relevance into four distinct criteria—Exactness, Coverage, Topicality, and Contextual Fit. Each criterion is evaluated independently via dedicated prompts, and the results are aggregated into a final relevance label. This approach improves transparency and interpretability compared to single-prompt methods. Experiments on TREC Deep Learning datasets show that Multi-Criteria achieves a Spearman's rank correlation of 0.99 on LLMJudge, outperforming strong baselines like UMBRELA and RUBRIC. The method is particularly effective with smaller LLMs, offering a scalable and accessible alternative for automatic relevance assessment.

## Method Summary
The Multi-Criteria framework evaluates relevance through four independent dimensions using dedicated prompts for each criterion. These criteria—Exactness, Coverage, Topicality, and Contextual Fit—are designed to capture different aspects of relevance. The framework processes each criterion separately through the LLM, then aggregates the individual scores into a final relevance label. This decomposition approach contrasts with single-prompt methods by providing greater transparency and interpretability in the judgment process. The evaluation demonstrates strong performance on TREC Deep Learning datasets, particularly when using smaller LLMs, suggesting the framework offers a practical and scalable solution for automatic relevance assessment.

## Key Results
- Achieved Spearman's rank correlation of 0.99 on LLMJudge benchmark
- Outperformed established baselines UMBRELA and RUBRIC
- Demonstrated effectiveness particularly with smaller LLMs

## Why This Works (Mechanism)
The framework works by decomposing the complex, multifaceted concept of relevance into four orthogonal dimensions that can be evaluated independently. This decomposition allows the LLM to focus on specific aspects of relevance rather than attempting to capture all dimensions simultaneously in a single judgment. By evaluating Exactness, Coverage, Topicality, and Contextual Fit separately, the system can identify specific strengths and weaknesses of documents relative to queries. The aggregation of these independent assessments provides a more nuanced and comprehensive relevance judgment than monolithic approaches. This method leverages the LLM's ability to perform fine-grained analysis while avoiding the confusion that can arise when multiple relevance aspects are conflated in a single evaluation prompt.

## Foundational Learning
- Relevance decomposition: Breaking relevance into orthogonal criteria enables more precise and interpretable judgments; check by verifying each criterion captures a distinct aspect of relevance.
- Multi-prompt evaluation: Using separate prompts for different criteria reduces cognitive load on the LLM; check by comparing single-prompt versus multi-prompt performance.
- Criteria aggregation: Combining individual criterion scores requires careful methodology; check by analyzing how different aggregation strategies affect final judgment quality.

## Architecture Onboarding
**Component map**: Query -> [Exactness prompt, Coverage prompt, Topicality prompt, Contextual Fit prompt] -> Individual criterion scores -> Aggregation module -> Final relevance label

**Critical path**: The most critical sequence is the evaluation of all four criteria followed by their aggregation, as the quality of the final judgment depends on both accurate individual assessments and effective combination strategy.

**Design tradeoffs**: The framework trades computational overhead (multiple prompts versus single prompt) for improved interpretability and potentially more accurate judgments. This design favors transparency and nuanced assessment over speed and simplicity.

**Failure signatures**: The system may fail when criteria are not truly orthogonal, leading to redundant or conflicting assessments. It may also struggle with criteria that have inherent conflicts (e.g., high exactness but low coverage). Aggregation failures can occur when combining criteria with incompatible scales or when trade-offs between criteria are not well-defined.

**First experiments**:
1. Ablation study removing one criterion at a time to assess each dimension's contribution to overall performance
2. Comparison of different aggregation strategies (weighted averaging, voting, learned combinations) to optimize final judgment quality
3. Cross-domain validation testing the framework on retrieval tasks outside TREC to evaluate generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- The four-criteria framework may not generalize well across different retrieval domains and tasks
- Evaluation is limited to synthetic query-document pairs, not real-world scenarios
- The aggregation method for combining criteria into final relevance labels is not explicitly detailed

## Confidence
- High confidence in the technical implementation of multi-criteria decomposition and experimental methodology
- Medium confidence in generalizability across different domains and retrieval tasks
- Medium confidence in practical scalability claims for smaller LLMs

## Next Checks
1. Cross-domain validation: Test the Multi-Criteria framework on diverse retrieval tasks beyond TREC, such as web search, e-commerce, or academic paper recommendation, to assess generalizability.

2. Real-world data evaluation: Apply the approach to real user queries and authentic document collections to validate performance in practical scenarios, not just synthetic pairs.

3. Aggregation method analysis: Conduct ablation studies on different aggregation strategies (weighted averaging, voting, learned combinations) to determine optimal methods for combining criterion scores into final relevance labels.