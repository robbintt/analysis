---
ver: rpa2
title: Robust Fake News Detection using Large Language Models under Adversarial Sentiment
  Attacks
arxiv_id: '2601.15277'
source_url: https://arxiv.org/abs/2601.15277
tags:
- news
- fake
- sentiment
- detection
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of fake news detectors
  to sentiment manipulation using large language models (LLMs). The authors propose
  AdSent, a framework that detects fake news in a sentiment-agnostic manner by training
  models on sentiment-neutralized variants of news articles.
---

# Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks

## Quick Facts
- arXiv ID: 2601.15277
- Source URL: https://arxiv.org/abs/2601.15277
- Reference count: 40
- Key outcome: AdSent framework achieves macro-F1 scores of 87.76% and 78.56% on two benchmark datasets while maintaining robustness against sentiment manipulation attacks.

## Executive Summary
This paper addresses the vulnerability of fake news detection systems to adversarial sentiment manipulation. The authors propose AdSent, a framework that neutralizes sentiment in news articles before classification, making detection more robust to sentiment-based attacks. Through extensive experiments on two benchmark datasets, the framework demonstrates significant improvements over state-of-the-art models when subjected to sentiment manipulation. The approach leverages large language models to generate sentiment-neutralized variants of news articles, enabling more reliable fake news detection.

## Method Summary
The AdSent framework operates by first generating sentiment-neutralized variants of news articles using large language models (LLMs). These neutralized versions are then used to train fake news detection models in a sentiment-agnostic manner. The approach involves a two-step process: sentiment neutralization using LLMs followed by training detection models on the neutralized data. This design aims to decouple sentiment information from the actual content that indicates whether news is fake or real, thereby reducing the effectiveness of sentiment-based adversarial attacks.

## Key Results
- AdSent achieves macro-F1 scores of 87.76% and 78.56% on two benchmark datasets
- State-of-the-art models show performance drops of up to 21.51% in F1-score when subjected to sentiment manipulation
- The framework demonstrates good generalization to unseen datasets and adversarial scenarios beyond sentiment manipulation

## Why This Works (Mechanism)
The framework works by removing sentiment as a predictive feature for fake news detection. By neutralizing sentiment through LLM-based transformations, the detection models are forced to focus on other linguistic and structural features that more reliably indicate fake news. This approach addresses the core vulnerability where sentiment alone can be manipulated to fool detectors without changing the actual factual content of the news.

## Foundational Learning
- **Sentiment Neutralization**: The process of removing emotional tone from text using LLMs. Needed to create sentiment-agnostic training data. Quick check: Verify that sentiment scores of neutralized text approach neutral values.
- **Adversarial Attacks**: Deliberate manipulations designed to fool machine learning models. Needed to understand vulnerabilities in fake news detection. Quick check: Test model performance under various attack types.
- **Macro-F1 Score**: An evaluation metric that averages precision and recall across all classes. Needed to assess model performance on imbalanced datasets. Quick check: Calculate both macro and weighted F1 scores for comparison.

## Architecture Onboarding

Component Map: LLM Sentiment Neutralization -> Fake News Detection Model -> Evaluation Pipeline

Critical Path: News Article -> LLM Neutralization -> Detection Model -> Classification Output

Design Tradeoffs: The approach trades computational overhead (LLM-based neutralization) for improved robustness. This may limit real-time deployment but provides better protection against adversarial attacks.

Failure Signatures: Performance degradation under non-sentiment adversarial attacks, inconsistent LLM neutralization outputs, and potential loss of nuanced content information during sentiment removal.

First Experiments:
1. Test detection performance on original vs. sentiment-manipulated versions of the same articles
2. Evaluate robustness against sentiment-neutralized adversarial examples
3. Compare performance across different LLM models for sentiment neutralization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on sentiment-based adversarial attacks, not testing other attack types
- Reliance on LLM-based sentiment neutralization introduces dependency on LLM quality and consistency
- Limited to English datasets, raising questions about multilingual effectiveness

## Confidence
- High Confidence: Effectiveness of AdSent in mitigating sentiment-based attacks and superior performance compared to baselines
- Medium Confidence: Generalizability to unseen datasets and adversarial scenarios beyond sentiment manipulation
- Medium Confidence: Robustness in practical, real-world settings with diverse linguistic nuances

## Next Checks
1. Test AdSent's performance against a broader range of adversarial strategies including entity substitution, syntactic paraphrasing, and context manipulation
2. Validate effectiveness on multilingual datasets and specialized domains (political, scientific, financial news)
3. Measure computational overhead of sentiment neutralization process and assess feasibility for real-time applications