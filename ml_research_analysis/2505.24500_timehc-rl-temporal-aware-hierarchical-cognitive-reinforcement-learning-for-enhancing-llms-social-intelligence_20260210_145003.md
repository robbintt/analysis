---
ver: rpa2
title: 'TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for
  Enhancing LLMs'' Social Intelligence'
arxiv_id: '2505.24500'
source_url: https://arxiv.org/abs/2505.24500
tags:
- social
- arxiv
- reasoning
- data
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TimeHC-RL, a Temporal-aware Hierarchical
  Cognitive Reinforcement Learning method designed to enhance LLMs' social intelligence.
  The approach addresses the gap between LLMs' strong performance in IQ-related domains
  (math, coding) and their underdeveloped social cognition abilities.
---

# TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence

## Quick Facts
- arXiv ID: 2505.24500
- Source URL: https://arxiv.org/abs/2505.24500
- Reference count: 26
- This paper introduces TimeHC-RL, a Temporal-aware Hierarchical Cognitive Reinforcement Learning method designed to enhance LLMs' social intelligence.

## Executive Summary
This paper addresses the gap between LLMs' strong performance in IQ-related domains (math, coding) and their underdeveloped social cognition abilities. TimeHC-RL introduces a temporal-aware hierarchical cognitive reinforcement learning framework that incorporates explicit temporal rewards and a three-level cognitive mode selection (intuitive System 1, surface-level, and deliberate System 2 thinking). The method was evaluated across eight diverse datasets, demonstrating a 29.0-point improvement over the backbone model and achieving 80.0% comprehensive performance. The 7B parameter model shows strong capabilities in social situation cognition and interpersonal reasoning, with systematic exploration revealing insights about RL generalization, interpersonal reasoning depth extrapolation, and test-time scaling limitations.

## Method Summary
TimeHC-RL uses a temporal-aware hierarchical cognitive framework built on Qwen2.5-7B-Instruct-1M backbone with GRPO reinforcement learning. The method incorporates three cognitive modes (System 1 intuitive, surface-level, and System 2 deliberate) with behavioral tags, and a three-component reward function including format validation (+/-1), answer accuracy (+2/-1.5), and temporal contrastive rewards (+0.4 when ordered response accuracy exceeds shuffled by threshold). Training combines datasets focusing on social situation cognition and interpersonal reasoning with specific depth-based splits. The framework was compared against five post-training paradigms and two test-time intervention approaches across eight benchmark datasets.

## Key Results
- 29.0-point improvement over backbone model on social intelligence benchmarks
- 80.0% comprehensive performance matching advanced models like DeepSeek-R1 and OpenAI-O3
- RL method significantly outperforms SFT on interpersonal reasoning problems with depths 3-4 (0.68, 0.64 vs 0.56, 0.50 for SFT)
- Temporal reward introduction brings performance advantages of 1.0-3.0 points in in-domain and OOD evaluation
- Test-time scaling fails to improve social situation cognition, highlighting the need for explicit training

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Cognitive Mode Selection
Social intelligence requires adaptive selection between intuitive (System 1), surface-level, and deliberate (System 2) reasoning modes, unlike math domains that predominantly use System 2. A training template prompts the model to choose among three cognitive modes, learning to select simpler modes for straightforward social-situation cognition and deliberate reasoning for complex interpersonal inference chains. Different social tasks naturally map to different cognitive depths, and forcing uniform deliberation is suboptimal.

### Mechanism 2: Temporal-aware Contrastive Reward
Explicitly incentivizing correct temporal ordering through contrastive rewards improves reasoning over social-event sequences. For each question, responses with ordered and shuffled event sequences are generated. If accuracy on ordered (p) exceeds threshold relative to shuffled (ṽp), positive temporal reward coefficient (α=0.4) is applied. Meaningful social reasoning depends on temporal sequence; shuffled sequences should degrade performance when the model uses temporal cues correctly.

### Mechanism 3: RL Generalization Over SFT Memorization
Rule-based RL enables extrapolation to deeper interpersonal reasoning (depths 3-4) even when trained only on shallower problems (depths 1-2), whereas SFT fails to generalize. GRPO explores solution space through policy gradients rather than token-level imitation, enabling the model to discover transferable reasoning strategies. The reasoning patterns learned for shallow mental-state inference compose into deeper inference capabilities.

## Foundational Learning

- **Concept: GRPO (Group Relative Policy Optimization)**
  - Why needed here: TimeHC-RL uses GRPO for RL training; understanding how group-sampled responses normalize into advantages is essential for debugging reward signals.
  - Quick check question: Given 8 sampled responses with rewards [2, 2, 2, 2, -1.5, -1.5, -1.5, -1.5], what is the advantage for a response with reward 2?

- **Concept: Dual-System Theory (System 1 vs System 2)**
  - Why needed here: The hierarchical cognitive framework is built on this distinction; selecting the wrong cognitive mode for a task type will degrade performance.
  - Quick check question: For a fourth-order belief inference ("Where does A think B thinks C thinks D thinks the object is?"), which cognitive mode is likely appropriate?

- **Concept: Theory of Mind (ToM) Reasoning Depths**
  - Why needed here: Dataset construction and evaluation explicitly vary interpersonal reasoning depth (1-4); misunderstanding depth leads to incorrect train/test splits.
  - Quick check question: A question asking "Where does Alice think the object is?" is what order of mental-state inference?

## Architecture Onboarding

- **Component map:**
  - Qwen2.5-7B-Instruct-1M -> TRL (SFT) and VeRL (RL) -> GRPO with group size 8 -> Three-component reward (format + accuracy + temporal) -> Hierarchical cognitive modes with behavioral tags

- **Critical path:**
  1. Prepare datasets with correct train/eval splits (depth 1-2 for train, 3-4 for eval on HiToM)
  2. Implement reward function with format validation, answer extraction matching, and temporal contrastive logic
  3. Configure GRPO with group size 8, learning rate 3e-7, KL coefficient 0.001
  4. Train with hierarchical cognitive template, monitoring per-dataset accuracy

- **Design tradeoffs:**
  - Temporal reward requires 2× sampling (ordered + shuffled), increasing compute cost
  - Hierarchical modes add output token overhead for deliberative responses
  - GRPO eliminates critic model but requires sufficient group size for stable advantage estimation

- **Failure signatures:**
  - Temporal reward collapsing to zero: ordered and shuffled accuracy too similar; check if shuffle actually disrupts temporal logic
  - Model always selecting one cognitive mode: template guidance may be insufficient; check mode distribution across validation batches
  - SFT outperforming RL on OOD: indicates overfitting or insufficient exploration; increase group size or learning rate

- **First 3 experiments:**
  1. Ablate temporal reward (HC-RL vs TimeHC-RL) on a held-out ToMi subset to confirm +1-3 point gain
  2. Compare mode selection distribution on ToMBench (situational) vs HiToM depth 4 (interpersonal) to verify adaptive behavior
  3. Test depth extrapolation: train only on HiToM depth 1-2, evaluate on depth 3-4, compare RL vs Direct SFT vs Long-thought SFT

## Open Questions the Paper Calls Out

- **How can the TimeHC-RL framework be extended to address behavioral intelligence (interaction and acting) rather than just situational and cognitive intelligence?**
  - Basis: Authors explicitly list "Beyond Situational Intelligence and Cognitive Intelligence" as a limitation, noting that "The ability to behave and interact (behavioral intelligence) is also very important."
  - Why unresolved: Current study exclusively focused on assessing and training reasoning and cognition via question-answering formats, not on generative action or interaction.
  - Evidence: Successful application in interactive environments (e.g., Sotopia) where agents must generate dialogue and actions, not just select answers.

- **How does the efficacy of TimeHC-RL vary across different model sizes, particularly regarding the trade-off between inherent knowledge and cognitive capacity?**
  - Basis: "Limitations and Future Works" states "Experiments with multiple model sizes... might reveal more valuable insights," noting that larger models have higher cognitive capacity.
  - Why unresolved: Study restricted to 7B parameter backbone, leaving scaling laws for social reinforcement learning unexplored.
  - Evidence: Comparative study applying TimeHC-RL to 7B, 32B, and 70B models to measure if larger models achieve social intelligence with less explicit temporal reward engineering.

- **Does increasing model size alone enable social situation cognition, or is diverse training data strictly necessary?**
  - Basis: Section 4.5 speculates that developing social situational cognition "may necessarily require either incorporating diverse social situations in the training data or increasing the model size" since test-time scaling failed.
  - Why unresolved: Paper establishes test-time compute does not solve social cognition, but disentangles roles of model scale vs. data diversity only speculatively.
  - Evidence: Ablation study testing frozen large-scale models (without RL) on ToMBench versus smaller TimeHC-RL models to determine if scale or training method is primary driver.

## Limitations

- The study focuses exclusively on situational and cognitive intelligence, not addressing behavioral intelligence (interaction and acting capabilities)
- Experiments are limited to 7B parameter models, leaving scaling effects on social intelligence unexplored
- The necessity of diverse training data versus model size for social situation cognition remains unclear, with test-time scaling shown ineffective but other factors not fully disentangled

## Confidence

- **High confidence:** Backbone model choice (Qwen2.5-7B-Instruct-1M), dataset composition and splits, GRPO hyperparameters, and three-component reward structure are explicitly specified and reproducible.
- **Medium confidence:** Hierarchical cognitive framework's effectiveness, temporal reward's contribution to performance gains, and RL's advantage over SFT for generalization are supported by experimental results but would benefit from additional ablation studies and cross-domain validation.
- **Low confidence:** Exact temporal reward implementation details (shuffling strategy), answer extraction logic for reward matching, and long-thought SFT generation specifications are insufficiently detailed for complete replication.

## Next Checks

1. **Temporal Reward Signal Validation:** Implement multiple shuffling strategies (random permutation, block shuffle, reverse order) and measure distribution of p/ṽp ratios. Verify temporal reward provides meaningful gradients by confirming p > 0.9ṽp occurs frequently and correlates with temporal reasoning tasks.

2. **Cross-Domain Generalization Test:** Evaluate TimeHC-RL on non-ToM social reasoning tasks (e.g., social norm reasoning, emotional intelligence tests) that weren't part of training distribution to confirm hierarchical cognitive framework and temporal awareness transfer beyond Theory of Mind.

3. **Mode Selection Robustness Analysis:** Systematically vary social context complexity and measure consistency of hierarchical mode selection. Identify failure cases where System 1 is selected for tasks requiring System 2 deliberation, and vice versa, to quantify adaptive selection mechanism's reliability.