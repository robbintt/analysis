---
ver: rpa2
title: 'CLASH: A Benchmark for Cross-Modal Contradiction Detection'
arxiv_id: '2511.19199'
source_url: https://arxiv.org/abs/2511.19199
tags:
- conflict
- caption
- object
- question
- conflicting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLASH addresses the critical gap in evaluating multimodal models'
  ability to detect contradictions across visual and textual inputs. While existing
  benchmarks assume input consistency or treat one modality as authoritative, CLASH
  tests spontaneous conflict detection in dual ground truth scenarios where both modalities
  are valid but contradictory.
---

# CLASH: A Benchmark for Cross-Modal Contradiction Detection

## Quick Facts
- **arXiv ID**: 2511.19199
- **Source URL**: https://arxiv.org/abs/2511.19199
- **Reference count**: 40
- **Primary result**: Most open-source multimodal models fail at detecting cross-modal contradictions, while leading closed-source models achieve >85% accuracy

## Executive Summary
CLASH introduces a benchmark for evaluating multimodal models' ability to detect contradictions between visual and textual inputs. Unlike existing benchmarks that assume input consistency or treat one modality as authoritative, CLASH tests spontaneous conflict detection in scenarios where both modalities are valid but contradictory. The benchmark features controlled object- and attribute-level contradictions in MS COCO images paired with modified captions, accompanied by targeted questions in multiple-choice and open-ended formats. Results reveal a stark performance divide between closed-source models (GPT-5, Gemini 2.5 Pro) achieving over 85% accuracy and most open-source models struggling with near-zero detection rates, along with systematic modality biases across architectures.

## Method Summary
The benchmark uses MS COCO images with contradictory captions where one element (object or attribute) is modified. Targeted questions are generated to probe the contradictory elements without revealing the conflict. The evaluation includes both multiple-choice and open-ended formats. For improving open-source model performance, LoRA-based fine-tuning is applied using ~15k filtered training samples. The fine-tuning process uses specific hyperparameters for different model architectures and includes both conflicting and non-conflicting samples to prevent bias toward conflict detection.

## Key Results
- Leading closed-source models (GPT-5, Gemini 2.5 Pro) achieve over 85% conflict detection accuracy, while most open-source models struggle with near-zero detection rates
- Systematic modality biases emerge across architectures, with some models favoring text over images and vice versa
- Targeted fine-tuning on CLASH dramatically improves conflict detection capabilities, with models like LLaVa-1.5-7b improving from 0% to 77% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Targeted fine-tuning on cross-modal contradiction examples can dramatically improve conflict detection in open-source models
- **Mechanism**: LoRA-based supervised fine-tuning on a dataset containing carefully constructed image-text pairs with controlled contradictions (object and attribute level). The training data includes both conflicting and non-conflicting samples to prevent a "conflict-bias." By learning from explicit examples of contradiction and consistency, the model's weights are updated to recognize patterns of cross-modal inconsistency.
- **Core assumption**: The learned conflict detection capability represents generalizable improvement in cross-modal reasoning that can transfer to unseen conflicts
- **Evidence anchors**: Abstract states fine-tuning substantially improves conflict detection; LLaVa-1.5-7b improves from 0% to 76.86% accuracy with filtered training data; related work supports supervised intervention for multimodal failure modes

### Mechanism 2
- **Claim**: Systematic modality bias (favoring text or image) is a primary failure mode in cross-modal contradiction detection for open-source models
- **Mechanism**: In the absence of robust cross-modal integration, a model defaults to a dominant modality. When presented with a contradiction, it resolves the conflict by choosing the answer consistent with its preferred modality rather than identifying the contradiction.
- **Core assumption**: The observed modality preference is a stable trait of the model architecture and training, not random output variation
- **Evidence anchors**: Abstract notes systematic modality biases; Table 1 shows stark differences in bias (LLaVa-1.5-7b is text-biased, LLaVa-OneVision-7b is image-biased); related work directly supports existence of this bias as a known issue

### Mechanism 3
- **Claim**: Closed-source frontier models exhibit emergent cross-modal reasoning capabilities that allow for strong (>85%) contradiction detection without task-specific training
- **Mechanism**: Advanced capabilities likely stem from larger scale, more sophisticated training data, and more robust cross-modal attention mechanisms. These models can weigh information from both visual and textual inputs, compare them against each other, and reason that a logical inconsistency exists.
- **Core assumption**: Performance of closed-source models reflects genuine reasoning and conflict detection, not data contamination
- **Evidence anchors**: Abstract states top closed-source models achieve strong conflict detection performance (>85%); GPT-5 achieves 86.78% and Gemini 2.5 Pro 88.48% in multiple-choice conflict detection; related work notes similar performance gaps between frontier and open-source models

## Foundational Learning

- **Concept: Multimodal Large Language Model (MM-LLM) Architecture**
  - **Why needed here**: The paper evaluates failure modes of these specific architectures. Understanding the general encoder-LLM structure is critical to diagnose where cross-modal fusion or attention may be failing
  - **Quick check question**: In a typical MM-LLM, which component is responsible for integrating visual features with textual tokens for joint reasoning?

- **Concept: Cross-Modal Reasoning**
  - **Why needed here**: This is the core capability being evaluated. It's not just visual question answering but the higher-order cognitive task of comparing and contrasting information from two distinct modalities
  - **Quick check question**: How does the CLASH task differ fundamentally from a standard Visual Question Answering (VQA) task?

- **Concept: LoRA (Low-Rank Adaptation) Fine-Tuning**
  - **Why needed here**: This is the intervention method used to improve model performance. A new engineer must understand how it works to reproduce or extend the results
  - **Quick check question**: Why is LoRA preferred over full fine-tuning for adapting large models, and what key hyperparameters would you monitor?

## Architecture Onboarding

- **Component Map**: MS COCO images/captions -> LLM (Gemini 2.5 Flash) generates single controlled contradiction in caption (object or attribute) -> LLM generates targeted question -> Creates multiple-choice answers including "Conflict" option -> Automated quality filters -> LLM-as-judge checks -> Human verification -> Evaluation suite runs samples through MM-LLMs and classifies responses

- **Critical Path**:
  1. **Contradiction Generation**: Must be a single, unambiguous, and objective change
  2. **Question Generation**: Must subtly target the contradictory element without revealing the conflict
  3. **Answer Validation**: Distractors must be plausible but non-synonymous, and "Conflict" option must be the only logically correct answer
  4. **Model Evaluation**: Model's output must be correctly classified as "Conflict", "Image", "Text", "Distractor", or "Incorrect" via string matching or LLM-as-judge

- **Design Tradeoffs**:
  - **Synthetic vs. Real-World Contradictions**: Synthetic allows for precise, controlled experiments but may lack nuance of real-world errors
  - **Automated vs. Human Verification**: Main dataset filtered automatically for scale, while diagnostic test set is human-verified
  - **Multiple-Choice vs. Open-Ended Evaluation**: Multiple-choice offers clean metric but may test instruction-following; open-ended is more realistic but requires complex evaluation

- **Failure Signatures**:
  - **Near-Zero Conflict Detection**: Indicates fundamental lack of cross-modal reasoning or strong modality bias
  - **High "Incorrect" Response Rate**: Suggests poor instruction-following capability rather than reasoning failure
  - **Asymmetrical Modality Bias**: A model that answers "Image" 80% of the time and "Conflict" 0% of the time has severe visual grounding bias
  - **Category-Specific Failure**: A model detecting color conflicts but failing on environmental attributes points to weaknesses in visual encoder's feature representations

- **First 3 Experiments**:
  1. **Establish a Baseline**: Run CLASH diagnostic set on target open-source MM-LLM to quantify baseline conflict detection accuracy and modality bias profile
  2. **Perform Targeted Fine-Tuning**: Using CLASH training set (~15k samples), perform LoRA fine-tuning on the underperforming model. Evaluate performance gain on held-out diagnostic set
  3. **Probe for Generalization**: Evaluate fine-tuned model on spatial conflict subset or out-of-distribution contradictions to test if learned skill generalizes beyond CLASH patterns

## Open Questions the Paper Calls Out

- **Can multimodal models be trained to detect spatial, relational, temporal, or causal cross-modal contradictions, or are current architectural limitations fundamental barriers to these more complex conflict types?**
  - Basis: The authors note CLASH focuses on object- and attribute-level contradictions rather than spatial, relational, temporal or causal conflicts, and since most open-source models fail on simple mismatches, introducing more complex conflicts would conflate multiple failure modes
  - Why unresolved: The exploratory spatial conflict experiment showed substantially worse performance—GPT-5 dropped from 86.78% to 68.24%—but the paper does not investigate whether this is a training data gap or architectural limitation
  - What evidence would resolve it: Training models on a spatial/relational contradiction benchmark and measuring whether performance gaps narrow; architectural probing studies examining whether cross-modal attention mechanisms support relational reasoning

- **Do the conflict detection capabilities and biases observed on everyday COCO images transfer to specialized domains such as medical imaging, scientific figures, or technical documentation?**
  - Basis: The authors note that extending it to specialized domains like medical, scientific, or technical content remains an important direction for future work
  - Why unresolved: All experiments use MS COCO images of common objects and scenes; domain-specific terminology, visual conventions, and error patterns may differ substantially
  - What evidence would resolve it: Constructing domain-specific contradiction benchmarks and evaluating whether fine-tuned models maintain their conflict detection capabilities across domains

- **Why does model scaling not consistently improve conflict detection, and what training or architectural factors better predict performance on this task?**
  - Basis: The Qwen family results show inconsistent scaling: larger models do not consistently improve conflict detection—while Qwen2.5VL-32b outperforms the 7b variant, Qwen3VL-30b underperforms the 8b model
  - Why unresolved: The paper demonstrates the phenomenon but does not investigate whether instruction-tuning data, visual encoder quality, or cross-modal fusion architecture explain the variance better than parameter count
  - What evidence would resolve it: Controlled ablation studies varying training data composition, visual encoders, and fusion mechanisms while holding scale constant; correlation analysis between architectural features and conflict detection performance

## Limitations
- Benchmark relies on synthetic contradictions generated from MS COCO data, which may not capture real-world cross-modal conflict complexity
- Performance gap between closed-source and open-source models could partially reflect data contamination if frontier models were exposed to similar contradiction detection tasks during pre-training
- Quality control pipeline processed 10,000+ samples through LLM-as-judge system, introducing potential systematic biases in evaluation itself

## Confidence
- **High**: Fundamental finding that cross-modal contradiction detection represents significant blind spot in current open-source MM-LLMs (consistent near-zero performance across multiple architectures)
- **Medium**: Systematic modality bias analysis (clear patterns observed but potential confounds from varying visual grounding capabilities)
- **Medium**: Fine-tuning results (dramatic improvements compelling but may reflect overfitting to specific CLASH contradiction patterns rather than generalizable reasoning transfer)

## Next Checks
1. **Out-of-Distribution Testing**: Evaluate fine-tuned models on real-world contradiction datasets or more complex conflict types (spatial, causal, temporal) not present in CLASH training set to assess generalization beyond memorized patterns

2. **Component Ablation**: Systematically disable either visual or textual modality during inference to quantify how much performance depends on modality fusion versus simple text/image classification

3. **Temporal Analysis**: Track learning curves during fine-tuning to determine whether conflict detection emerges gradually through pattern recognition or appears suddenly, indicating whether models learn generalizable reasoning or surface-level heuristics