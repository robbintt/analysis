---
ver: rpa2
title: 'MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM
  Embedded Knowledge'
arxiv_id: '2505.24493'
source_url: https://arxiv.org/abs/2505.24493
tags:
- emotion
- melt
- speech
- data
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of costly and inconsistent human\
  \ annotation in speech emotion recognition (SER) by proposing an automated annotation\
  \ method using GPT-4o without direct access to audio data. The method, called MELT,\
  \ leverages GPT-4o\u2019s embedded knowledge to annotate a multimodal dataset from\
  \ the TV series Friends using only textual cues, employing structured prompts with\
  \ Chain-of-Thought reasoning and cross-validation."
---

# MELT: Towards Automated Multimodal Emotion Data Annotation by Leveraging LLM Embedded Knowledge

## Quick Facts
- arXiv ID: 2505.24493
- Source URL: https://arxiv.org/abs/2505.24493
- Reference count: 0
- This paper proposes MELT, an automated annotation method using GPT-4o to generate emotion labels for speech data without direct audio access, achieving improved generalization in speech emotion recognition models.

## Executive Summary
This paper addresses the challenge of costly and inconsistent human annotation in speech emotion recognition (SER) by proposing MELT, an automated annotation method using GPT-4o without direct access to audio data. The method leverages GPT-4o's embedded knowledge about culturally significant content, specifically the TV series "Friends," to generate accurate emotion labels using only textual cues. Subjective experiments show MELT annotations align better with human preferences (>70% agreement on emotions like anger and surprise), and objective experiments demonstrate that models trained on MELT achieve improved generalization and robustness across multiple SER datasets, with up to 23.74% UAR improvement on IEMOCAP. The approach significantly reduces annotation costs (<$10) while highlighting the potential of LLMs for scalable, context-aware multimodal annotation.

## Method Summary
MELT uses GPT-4o to annotate emotion data from the TV series Friends using only textual transcripts, without accessing audio signals. The method employs structured prompts with Chain-of-Thought reasoning and cross-validation to generate emotion labels. The annotation pipeline filters utterances <1s, excludes context-poor characters, and formats input as "[speaker] at s[season]e[episode] said: [utterance]". The resulting labels train SER models using SSL backbones (wav2vec2, HuBERT, WavLM) with two FC layers. The approach demonstrates that text-derived labels can effectively train audio-based SER models, achieving comparable or superior performance to human-annotated labels across multiple out-of-domain datasets.

## Key Results
- MELT-trained models achieved up to 23.74% UAR improvement on IEMOCAP compared to MELD-trained models
- Subjective MOS evaluation showed >70% participant agreement for MELT annotations on 'anger' and 'surprise' emotions
- Total annotation cost was less than $10 for the entire dataset
- MELT annotations generalized better than human annotations across 3 of 4 out-of-domain SER datasets

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Annotation via Embedded Knowledge
GPT-4o can generate accurate emotion labels for multimodal data using only textual transcripts by leveraging knowledge encoded during pre-training about culturally significant content. The LLM accesses implicit "collective knowledge" from its training corpus about the TV series "Friends"—character relationships, situational context, and emotional patterns—which enables contextually grounded annotations without accessing audio signals. This mechanism degrades when source material is obscure or not in GPT-4o's training data; authors explicitly filtered characters lacking clear identifiers because context was insufficient.

### Mechanism 2: Structured Prompting with Cross-Validation and Chain-of-Thought
A carefully designed prompting framework with CoT reasoning and cross-validation produces more consistent and accurate annotations than naive prompting. The prompt architecture enforces explicit contextual framing, multi-step reasoning through emotional descriptors, cross-validation requests for verifiable information, and structured JSON output for consistency. Ambiguous or context-poor inputs (minor characters, generic dialogue) lead to inconsistent outputs; authors excluded ~260 characters lacking sufficient identifiers.

### Mechanism 3: Text-Derived Labels Transfer to Audio Emotion Recognition
Emotion labels derived purely from text transcripts can effectively train audio-based SER models, achieving comparable or superior performance to human-annotated labels. The semantic-emotional content encoded in text annotations captures sufficient signal about the underlying emotional state that transfers across modalities when SSL audio backbones learn to associate acoustic features with these labels during fine-tuning. Emotions heavily dependent on acoustic expression rather than semantic content (low-arousal emotions like 'fear', 'sadness') show lower annotation quality—MOS results indicate MELD was preferred for these categories, and many 'sad' samples were relabeled as 'neutral'.

## Foundational Learning

- **Self-Supervised Learning (SSL) for Speech**: The evaluation architecture relies on SSL-pretrained backbones (wav2vec2, HuBERT, WavLM) to extract audio representations. Understanding how these models learn acoustic features without labeled data is essential for interpreting why MELT labels transfer effectively. Quick check: Can you explain why SSL representations might be more robust to label noise than handcrafted acoustic features?

- **Chain-of-Thought (CoT) Prompting**: CoT is a core component of the annotation pipeline, explicitly required in the prompt design. Understanding how step-by-step reasoning affects LLM output quality is critical for reproducing and extending this work. Quick check: How does CoT prompting differ from standard prompting, and what types of tasks benefit most from it?

- **Unweighted Average Recall (UAR) vs. Accuracy in SER**: The paper uses UAR as a primary metric because emotion datasets are class-imbalanced. Understanding why UAR is preferred over standard accuracy is necessary for correctly interpreting the results. Quick check: Why might a model achieve high accuracy but low UAR on an imbalanced emotion dataset?

## Architecture Onboarding

- **Component map**: MELD utterance → format prompt string → GPT-4o API call → parse JSON response → extract emotion label → create MELT dataset → SSL backbone → classification head → fine-tune 100 epochs → evaluate on target datasets

- **Critical path**: 1) Format utterance with speaker/season/episode context, 2) Call GPT-4o with structured prompt (temperature=1.0), 3) Parse JSON response for emotion label, 4) Train SSL backbone + FC layers on MELT data, 5) Evaluate on IEMOCAP/TESS/RAVDESS/CREMA-D

- **Design tradeoffs**: Context depth vs. coverage (restricting to 42 well-identified characters improves consistency but reduces dataset size by ~30%), temperature=1.0 (increases diversity but may reduce reproducibility), text-only vs. multimodal annotation (eliminates audio input requirement but may miss acoustic-only emotional cues), cost vs. validation (<$10 cost but no human validation loop)

- **Failure signatures**: Low-arousal emotion collapse ('Sad' and 'fear' frequently relabeled as 'neutral'), model-specific inconsistency (TESS performance varies from -0.03% to +23.74% UAR across backbones), hallucination risk (no quantitative analysis of hallucination rate provided)

- **First 3 experiments**: 1) Reproduce annotation pipeline on held-out MELD subset (100 utterances) to verify label change rate and distribution, 2) Ablate prompting components (test with/without CoT, cross-validation, JSON formatting) on small subset, 3) Cross-corpus baseline comparison using single SSL backbone (WavLM-base+ recommended) on both MELT and MELD, evaluate on all four out-of-domain datasets

## Open Questions the Paper Calls Out
- How can hybrid annotation pipelines effectively combine LLM capabilities with human-in-the-loop methods to maximize both scalability and annotation reliability?
- To what extent does LLM-based annotation generalize to domains, cultures, or content not well-represented in the model's pre-training data?
- What is the impact of excluding multimodal audio features during LLM-based annotation on the accuracy and nuance of emotion labels?

## Limitations
- The approach relies on GPT-4o's pre-training corpus containing sufficient information about the "Friends" TV series for accurate emotion annotation, which may not generalize to obscure content
- Text-only annotation shows particular weakness for low-arousal emotions (sadness, fear) which are frequently relabeled as neutral, suggesting the mechanism fails when acoustic expression dominates over semantic content
- No quantitative analysis of hallucination rates is provided, and the lack of human validation in the annotation loop is noted as a limitation

## Confidence
- **High Confidence**: The cost-effectiveness claim (<$10 for annotation) and MOS subjective preference results (>70% agreement for anger and surprise) are well-supported by direct measurements
- **Medium Confidence**: Cross-corpus generalization results show significant variation across SSL backbones (TESS performance varies from -0.03% to +23.74% UAR), suggesting complex interactions between annotation quality and backbone architecture
- **Low Confidence**: The mechanism that GPT-4o's embedded knowledge about "Friends" enables accurate emotion annotation without audio access is plausible but not directly validated

## Next Checks
1. Reproduce annotation pipeline on held-out MELD subset: Use the provided prompt template to annotate 100 random utterances, compare label distribution and inter-annotator agreement with the paper's reported 46-47% label change rate from MELD to MELT
2. Ablate prompting components: Test annotation quality with and without Chain-of-Thought reasoning, cross-validation, and JSON formatting on a small subset to isolate the contribution of each prompt engineering principle
3. Cross-corpus baseline comparison: Train a single SSL backbone (WavLM-base+ recommended based on Table 3 results) on both MELT and MELD, evaluate on all four out-of-domain datasets, verify the reported UAR/ACC/F1 patterns and test the claim that MELT annotations generalize better than human annotations