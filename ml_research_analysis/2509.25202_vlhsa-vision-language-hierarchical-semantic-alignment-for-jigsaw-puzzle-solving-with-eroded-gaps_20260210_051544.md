---
ver: rpa2
title: 'VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving
  with Eroded Gaps'
arxiv_id: '2509.25202'
source_url: https://arxiv.org/abs/2509.25202
tags:
- visual
- global
- puzzle
- semantic
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of solving jigsaw puzzles with
  eroded gaps, where traditional visual-only methods struggle due to insufficient
  edge and visual coherence information. The proposed VLHSA framework introduces hierarchical
  vision-language semantic alignment, combining Vision Mamba and BLIP for dual visual
  encoding with CLIP-based text encoding.
---

# VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps

## Quick Facts
- **arXiv ID**: 2509.25202
- **Source URL**: https://arxiv.org/abs/2509.25202
- **Reference count**: 16
- **Key outcome**: VLHSA achieves state-of-the-art performance on jigsaw puzzles with eroded gaps, improving piece accuracy by 14.2 percentage points over vision-only methods.

## Executive Summary
VLHSA introduces a novel framework for solving jigsaw puzzles where traditional visual edge-matching fails due to eroded gaps between pieces. The key innovation is hierarchical vision-language semantic alignment that combines dual visual encoding (Vision Mamba + BLIP) with CLIP-based text encoding. By establishing semantic correspondences at token, region, and global levels, the framework compensates for missing visual information. The approach achieves 66.9% piece accuracy on 5×5 puzzles, outperforming all prior methods including vision-only approaches by significant margins.

## Method Summary
VLHSA addresses jigsaw puzzles with eroded gaps by introducing hierarchical vision-language semantic alignment. The framework uses Vision Mamba for spatial feature extraction, BLIP for semantic visual encoding, and CLIP for text encoding. These representations are aligned at three hierarchical levels: token (patch-to-word), region (group-to-phrase), and global (overall semantic coherence). The aligned features are fused and used to predict piece-to-position assignments via the Hungarian algorithm. This multi-granularity approach compensates for missing edge information by leveraging textual semantic constraints.

## Key Results
- **14.2 percentage point improvement** in piece accuracy over previous best vision-only approach (66.9% vs 52.7%)
- **19.0% Perfect Accuracy** on challenging 5×5 puzzles, outperforming all prior methods
- **85.4% Piece Accuracy** on 3×3 puzzles, demonstrating strong performance across scales
- **Global alignment contributes most** to performance, with token and region levels providing complementary benefits

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Semantic Alignment Compensates for Eroded Visual Cues
- **Claim**: Multi-granularity vision-language alignment enables puzzle reconstruction when edge information is missing.
- **Mechanism**: The VLHSA module establishes correspondences at three levels simultaneously: (1) token-level cross-attention aligns individual patches with specific caption words; (2) region-level alignment groups adjacent patches and matches them to phrases via sliding windows; (3) global-level alignment uses gated fusion to enforce overall semantic coherence. These fused representations then predict piece-to-position assignments via Hungarian algorithm optimization.
- **Core assumption**: Textual descriptions of the original image are available and accurately capture salient semantic content that disambiguates visually similar fragments.
- **Evidence anchors**:
  - [abstract]: "Ablation studies confirm that hierarchical alignment is critical, with global alignment providing the largest contribution while token and region levels offer complementary fine-grained reasoning."
  - [section - Table 7]: Global alignment alone achieves 17.50% Perfect Accuracy; combining all three levels achieves 19.00% Perfect and 66.90% Piece Accuracy.
  - [corpus]: Weak direct validation. Related work (HiMo-CLIP) supports hierarchical vision-language alignment in principle, but no corpus paper validates this specific three-level scheme for puzzles.
- **Break condition**: When puzzle pieces share nearly identical visual and semantic properties (e.g., repetitive backgrounds), neither visual nor textual cues provide discriminative power—the paper acknowledges this limitation explicitly in Figure 2 discussion.

### Mechanism 2: Dual Visual Encoding Captures Complementary Spatial and Semantic Features
- **Claim**: Combining Vision Mamba with BLIP visual encoder provides richer representations than either alone.
- **Mechanism**: Vision Mamba processes patches sequentially through selective state space mechanisms, capturing long-range spatial dependencies across non-adjacent pieces. BLIP's pre-trained vision encoder provides cross-modally aligned features that already encode semantic content. A residual adapter injects puzzle-specific patterns into Mamba features, then BLIP features are projected and added via residual connection.
- **Core assumption**: Pre-trained BLIP visual features transfer effectively to fragmented, scrambled puzzle pieces despite domain shift from natural images.
- **Evidence anchors**:
  - [section - Table 6]: Vision Mamba alone: 3.55% Perfect, 49.29% Piece. Adding BLIP: 5.95% Perfect, 53.98% Piece. Adding CLIP: 8.85% Perfect, 58.57% Piece.
  - [section - Methodology]: "This design preserves spatial structure from Vision Mamba while incorporating semantic richness from BLIP."
  - [corpus]: No corpus paper validates dual Mamba-BLIP encoding specifically.
- **Break condition**: If BLIP's pre-training on natural images fails to extract meaningful semantics from heavily eroded or abstract puzzle fragments, the projected features may add noise rather than signal.

### Mechanism 3: Textual Semantic Constraints Reduce Assignment Ambiguity
- **Claim**: Natural language descriptions provide priors that constrain the combinatorial search space for piece placement.
- **Mechanism**: CLIP encodes captions at both global (mean-pooled) and token levels. During alignment, visual features attend to semantically relevant text tokens, effectively asking "which pieces should contain the object described by this word?" This creates soft constraints that supplement visual edge-matching, particularly valuable when gaps erode boundary information.
- **Core assumption**: Captions are manually verified or of sufficient quality (paper notes "approximately 10 words each, with manual editing to address model-generated errors").
- **Evidence anchors**:
  - [section - Introduction]: "knowledge such as 'A woman wearing a crown seated on a chair' or 'A painting depicts a landscape with mountains and trees' provides strong constraints that effectively supplement visual edge matching."
  - [section - Table 3]: Largest improvements on engravings and artifacts (+15 percentage points over ERL-MPP), suggesting semantic guidance helps most on structured, semantically rich content.
  - [corpus]: Agentic Jigsaw Interaction Learning paper confirms VLMs struggle with jigsaw tasks, but doesn't validate language-guided solutions.
- **Break condition**: If captions are generic ("an image with various colors") rather than specific to image content, semantic constraints become uninformative.

## Foundational Learning

- **Concept**: Multi-head Cross-Attention
  - **Why needed here**: Token and region alignment use cross-attention where visual patches query textual keys/values. Without understanding Q/K/V mechanics, the alignment module is opaque.
  - **Quick check question**: Given query vectors from image patches and key/value vectors from caption tokens, what does the attention matrix represent?

- **Concept**: Selective State Space Models (Mamba)
  - **Why needed here**: Vision Mamba is the primary spatial encoder; understanding its selective scan mechanism explains how it captures long-range dependencies without quadratic attention cost.
  - **Quick check question**: How does Mamba's selective mechanism differ from standard attention in handling variable-distance relationships?

- **Concept**: Hungarian Algorithm for Bipartite Matching
  - **Why needed here**: Final assignment requires one-to-one piece-to-position mapping; the Hungarian algorithm guarantees optimal permutation in O(n³).
  - **Quick check question**: Why can't simple argmax over position logits produce valid puzzle assignments?

## Architecture Onboarding

- **Component map**: Input puzzle image + caption → Vision Mamba Encoder → adapted spatial features → BLIP Vision Encoder → projected semantic features → Feature Integration → combined visual features → CLIP Text Encoder → global and token text features → VLHSA Module → hierarchical aligned features → MLP Prediction Head → position logits → Hungarian Algorithm → optimal assignment

- **Critical path**: Vision Mamba encoding → dual feature integration → global alignment (largest ablation impact) → hierarchical fusion → Hungarian assignment. The paper states global alignment contributes most; prioritize understanding Equation 11-13 (global encoder + gating mechanism).

- **Design tradeoffs**:
  - Model size (27.1M) vs. expressiveness: Smaller than ERL-MPP (102.1M) but achieves higher accuracy—suggests semantic priors compensate for capacity.
  - Caption dependency: Requires ground-truth or high-quality generated captions; real archaeological applications may lack this.
  - Three-level hierarchy: Adds complexity; ablation shows global alone achieves 17.5% vs 19.0% with all levels—marginal gain may not justify full hierarchy in resource-constrained settings.

- **Failure signatures**:
  - High visual similarity pieces (repetitive textures, uniform backgrounds) → model outputs near-identical logits for multiple positions.
  - Generic captions → semantic alignment loss provides weak gradients; performance degrades toward vision-only baseline.
  - Large puzzle scales (5×5 vs 3×3): Perfect Accuracy drops from implied higher levels to 19.0% on 5×5; Piece Accuracy drops from 85.4% (3×3) to 66.9% (5×5).

- **First 3 experiments**:
  1. **Reproduce ablation Table 7**: Run global-only alignment vs. full hierarchy to validate the 1.5 percentage point Perfect Accuracy gain before investing in full implementation.
  2. **Caption quality stress test**: Replace manually-edited captions with raw BLIP-generated captions without editing; measure Piece Accuracy degradation to quantify caption quality sensitivity.
  3. **Cross-dataset transfer**: Train on JPwLEG-3, test on JPwLEG-5 without fine-tuning to assess whether learned semantic alignment generalizes across puzzle scales.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the VLHSA framework be extended to resolve ambiguity in cases where puzzle fragments share nearly identical visual textures and semantic properties?
- **Basis in paper**: [explicit] The paper explicitly states in the "Visual Comparison" section that "Our hierarchical semantic alignment cannot fully resolve cases where pieces look nearly identical. When fragments share similar visual and semantic properties, neither visual nor textual cues provide enough discriminative power."
- **Why unresolved**: The current hierarchical alignment strategy relies on distinguishing features; when features are indistinguishable, the model defaults to ambiguity. The authors acknowledge this limitation but offer no proposed mechanism (e.g., higher-resolution analysis or third-party modalities) to address it.
- **What evidence would resolve it**: Successful reconstruction results on a specialized dataset of adversarial examples containing near-duplicate fragments, or the introduction of a distinctiveness-weighting loss that forces the model to prioritize subtle differences.

### Open Question 2
- **Question**: How robust is the VLHSA framework when deployed without the manual verification and editing of image captions?
- **Basis in paper**: [inferred] The methodology section describes the captions as "BLIP... generated (approximately 10 words each, with manual editing to address model-generated errors and improve clarity)" (Page 2). The paper does not evaluate performance using raw, potentially erroneous captions.
- **Why unresolved**: It is unclear if the state-of-the-art results rely on "clean" semantic signals that would not exist in a fully automated real-world pipeline. If the alignment module is sensitive to caption noise, the practical applicability is limited.
- **What evidence would resolve it**: An ablation study comparing the "Piece Accuracy" of the model when using raw BLIP captions versus the manually edited ground-truth captions used in the main experiments.

### Open Question 3
- **Question**: Does the reliance on the Hungarian algorithm for assignment prediction limit the scalability of VLHSA to large-scale, real-world archaeological reconstruction?
- **Basis in paper**: [inferred] The paper mentions archaeological reconstruction as a key motivation (Page 1) but only evaluates the method on 3×3 and 5×5 grids (N=9 and N=25). The assignment module uses the Hungarian algorithm (Page 4), which has a time complexity of O(N³).
- **Why unresolved**: While efficient for small grids, the cubic complexity of the assignment solver may become a computational bottleneck for puzzles with hundreds of fragments (common in archaeology), a scenario not tested in the paper.
- **What evidence would resolve it**: Complexity benchmarks and accuracy retention metrics on synthetic datasets with significantly higher piece counts (e.g., 10×10 or 20×20 grids).

## Limitations
- **Citation gap**: JPwLEG datasets appear newly introduced; claimed superiority may be dataset-specific rather than demonstrating fundamental advances
- **Domain transfer risk**: 14.2 percentage point improvement depends critically on caption quality; archaeological applications may lack detailed metadata
- **Combinatorial scalability**: Perfect Accuracy drops significantly from 3×3 to 5×5 puzzles, suggesting fundamental limitations as search space grows

## Confidence

**High Confidence**:
- Dual visual encoding (Vision Mamba + BLIP) improves over single encoders
- Vision-language alignment outperforms vision-only approaches on JPwLEG benchmarks
- Global alignment level contributes more than token/region levels

**Medium Confidence**:
- Hierarchical alignment provides complementary benefits beyond global alignment alone
- The 14.2 percentage point improvement represents meaningful real-world impact
- Caption quality requirements are manageable in practical applications

**Low Confidence**:
- State-of-the-art claims generalize beyond JPwLEG datasets
- The approach scales effectively to puzzles larger than 5×5
- Real archaeological applications can satisfy caption quality requirements

## Next Checks

1. **Cross-Dataset Generalization Test**: Train VLHSA on JPwLEG-3, then evaluate on established puzzle datasets like CCHW or Pclean without fine-tuning. Measure whether the 66.9% Piece Accuracy transfers or degrades significantly, which would indicate dataset-specific rather than general methodological advances.

2. **Caption Quality Degradation Analysis**: Systematically replace the manually-edited captions with three conditions: (a) raw BLIP-generated captions, (b) captions with 50% word deletion, (c) generic captions like "an image with colors and shapes." Track Piece Accuracy degradation to quantify the framework's sensitivity to caption quality and establish minimum requirements.

3. **Scalability Boundary Test**: Implement VLHSA for 6×6 and 7×7 puzzles (729 and 5,040 possible arrangements respectively). Measure Perfect and Piece Accuracy at these scales to identify the point where semantic alignment alone can no longer compensate for exponential combinatorial growth, revealing fundamental limitations of the approach.