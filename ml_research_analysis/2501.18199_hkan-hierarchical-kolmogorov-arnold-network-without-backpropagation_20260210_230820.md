---
ver: rpa2
title: 'HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation'
arxiv_id: '2501.18199'
source_url: https://arxiv.org/abs/2501.18199
tags:
- e-02
- hkan
- functions
- e-03
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Hierarchical Kolmogorov-Arnold Network
  (HKAN), a novel architecture that eliminates the need for backpropagation by using
  randomized learning and a hierarchical multi-stacking design. Instead of learning
  all parameters through gradient descent, HKAN fixes the parameters of its basis
  functions (either randomly or data-driven) and optimizes linear aggregations using
  least-squares regression at each layer.
---

# HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation

## Quick Facts
- **arXiv ID:** 2501.18199
- **Source URL:** https://arxiv.org/abs/2501.18199
- **Reference count:** 40
- **Primary result:** HKAN eliminates backpropagation by using randomized learning and hierarchical stacking, achieving accuracy comparable to or exceeding standard KANs and MLPs with reduced training time and enhanced stability.

## Executive Summary
This paper introduces HKAN, a novel neural network architecture that replaces gradient-based learning with randomized basis functions and least-squares regression. By fixing the parameters of non-linear basis functions and optimizing only the linear combination weights, HKAN eliminates the need for backpropagation while maintaining strong approximation capabilities. The hierarchical multi-stacking design progressively refines predictions through multiple layers, treating each layer as a stacking ensemble that combines outputs from previous layers. Experimental results demonstrate that HKAN achieves state-of-the-art performance on various regression tasks with significantly reduced training times compared to standard KANs and MLPs.

## Method Summary
HKAN replaces backpropagation with a hierarchical architecture that uses randomized or data-driven basis functions whose parameters are fixed, then optimizes linear weights through least-squares regression at each layer. The training process involves computing basis function outputs, solving convex optimization problems for block weights using Ridge Regression, and aggregating these through h-functions in subsequent layers. This approach decomposes the complex non-convex optimization problem into multiple convex subproblems, guaranteeing optimal linear weights for fixed basis configurations. The architecture incorporates a built-in mechanism for assessing input variable importance while maintaining strong approximation accuracy.

## Key Results
- HKAN achieves accuracy comparable to or exceeding standard KANs and MLPs across regression tasks
- Training times are significantly reduced compared to backpropagation-based methods
- The hierarchical stacking design progressively refines predictions through multiple layers
- Built-in input variable importance assessment enhances model interpretability

## Why This Works (Mechanism)

### Mechanism 1: Convex Optimization via Least-Squares Decomposition
HKAN replaces non-convex gradient descent with convex linear regression subproblems, guaranteeing optimal linear weights for fixed basis configurations. By fixing basis function parameters and solving for linear combinations using Moore-Penrose pseudoinverse, the model avoids local minima issues inherent in backpropagation. The assumption is that sufficient random or data-distributed fixed basis functions can span the necessary functional space, leaving only linear aggregation to be optimized.

### Mechanism 2: Hierarchical Multi-Stacking (Refinement)
Prediction accuracy improves through deep structure where each layer acts as a "stacking" ensemble, refining outputs of the previous layer. Layer 1 blocks approximate the target using single variables, while subsequent layers take previous layer outputs, re-project through new randomized non-linear bases, and re-combine them. This treats previous layer outputs as "weak learners" and current layer as "meta-learner," successively reducing residual error.

### Mechanism 3: Randomized Feature Diversity
Randomly selecting basis function parameters enforces diversity in feature space, allowing linear aggregation to act as powerful ensemble combiner. By distributing parameters randomly or using support points from data, the model creates wide array of non-linear "views" of input. The least-squares optimization then selects best linear combination of these diverse views, mimicking Extreme Learning Machine or Random Vector Functional Link networks.

## Foundational Learning

**Concept:** Kolmogorov-Arnold Representation Theorem (KART)
- **Why needed here:** HKAN's structure (summing univariate functions of inputs) is rooted in KART, distinguishing it from MLPs which use fixed activations on nodes.
- **Quick check question:** Can you explain why KART allows placing "learnable" functions on edges rather than nodes?

**Concept:** Least Squares / Ridge Regression
- **Why needed here:** This is the *only* training algorithm used, replacing backpropagation. Understanding Moore-Penrose pseudoinverse is essential.
- **Quick check question:** If you have design matrix $G$ of BaF outputs, how does adding regularization ($\lambda I$) prevent overfitting when calculating weights $c$?

**Concept:** Ensemble Stacking
- **Why needed here:** The paper frames hierarchical layers as "stacking" models, where meta-learners combine weak learners.
- **Quick check question:** In a stacking ensemble, why is diversity among base learners (or blocks) important for meta-learner's performance?

## Architecture Onboarding

**Component map:** Input $x$ → Layer $l$ with BaFs → BlFs (linear combination) → h-functions (aggregation) → Output

**Critical path:**
1. Initialize BaF parameters ($\mu, \sigma$) using "Data-driven" or "Random" strategy
2. Compute BaF outputs for training batch
3. Learn BlF weights ($c$) using Ridge Regression to fit target
4. Learn h-function weights ($w$) using Ridge Regression to fit target using BlF outputs
5. Pass outputs to next layer and repeat

**Design tradeoffs:**
- **Random vs. Data-driven Locations:** Random is faster but may create gaps; Data-driven ensures coverage but couples architecture to training set distribution
- **Depth ($L$) vs. BaF count ($m$):** Complex functions approximated by many BaFs in one layer (width) or stacking many refining layers (depth)
- **Smoothing ($\sigma$):** High $\sigma$ makes BaFs smoother (less overfitting, higher bias); low $\sigma$ makes them spiky (high variance)

**Failure signatures:**
- **Ill-conditioned Matrix:** If BaFs are too similar, design matrix becomes singular, causing numerical instability in Least Squares (requires higher regularization $\lambda$)
- **Stagnant Error:** If error doesn't drop after layer 1, random bases likely failed to capture primary signal (check $\sigma$ and distribution method)
- **Overfitting:** Very low training error but high test error indicates $m$ too high or $\lambda$ too low

**First 3 experiments:**
1. **Sanity Check (Toy Function):** Train 1-layer HKAN on $y = \sin(x)$. Verify Least Squares solution recovers curve using random Gaussian bases.
2. **Location Strategy Comparison:** Train 2-layer HKAN on 2D synthetic dataset. Compare "Random Uniform" vs. "Data-driven" locations for $\mu$. Measure Test RMSE.
3. **Refinement Test:** Run model with $L=1$ vs $L=3$ on complex function. Plot Test RMSE to observe if "stacking" effect actually reduces error.

## Open Questions the Paper Calls Out
- Can HKAN be effectively adapted for classification tasks and time-series forecasting?
- How can HKAN be integrated with other advanced architectures, such as Transformers or Convolutional Neural Networks?
- Is there a theoretical or heuristic method to determine optimal type and distribution of basis functions without relying on extensive hyperparameter search?

## Limitations
- Reliance on randomized basis functions introduces uncertainty about generalization when training distributions differ from test distributions
- Scalability to very high-dimensional problems (thousands of features) remains unexplored
- Lack of theoretical bounds on approximation error as function of basis function count and distribution method

## Confidence
- **High Confidence:** Elimination of backpropagation through least-squares optimization is mathematically sound
- **Medium Confidence:** Claims about comparable or superior accuracy to standard KANs and MLPs are supported by experimental results
- **Low Confidence:** Assertion that hierarchical multi-stacking guarantees progressive refinement is primarily supported by empirical results

## Next Checks
1. **Distribution Shift Test:** Train HKAN on synthetic data with one distribution and evaluate on data with different distribution to assess robustness
2. **Ablation Study on Architecture:** Compare HKAN performance with variations: single-layer vs. multi-layer, random vs. data-driven basis function locations, different numbers of basis functions per block
3. **Theoretical Error Bound Derivation:** Develop and validate theoretical bounds on approximation error as function of number of basis functions, their distribution, and number of layers