---
ver: rpa2
title: World Model Agents with Change-Based Intrinsic Motivation
arxiv_id: '2503.21047'
source_url: https://arxiv.org/abs/2503.21047
tags:
- dreamerv3
- learning
- cbet
- environment
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies applying Change Based Exploration Transfer (CBET)
  - a method combining intrinsic motivation and transfer learning - to world model
  agents like DreamerV3. CBET adds intrinsic rewards based on state novelty and transfers
  policies between environments.
---

# World Model Agents with Change-Based Intrinsic Motivation

## Quick Facts
- **arXiv ID:** 2503.21047
- **Source URL:** https://arxiv.org/abs/2503.21047
- **Reference count:** 40
- **Key outcome:** CBET improves DreamerV3 in Crafter but reduces performance in Minigrid, highlighting environment-dependent effectiveness of change-based intrinsic motivation.

## Executive Summary
This paper investigates applying Change-Based Exploration Transfer (CBET) to world model agents like DreamerV3. CBET combines intrinsic motivation based on state novelty with transfer learning capabilities. The authors adapted CBET for DreamerV3 and compared it with IMPALA in sparse-reward environments (Crafter and Minigrid). Results showed environment-dependent performance: CBET improved DreamerV3 in Crafter but reduced performance in Minigrid, suggesting that CBET's impact depends on environment characteristics and model architecture. The study highlights the importance of carefully selecting exploration strategies and provides insights into CBET's effectiveness across different algorithms and environments.

## Method Summary
The paper adapts CBET for DreamerV3 by using two separate world model instances: one pre-trained with intrinsic rewards (exploration agent) and one trained with extrinsic rewards (task agent). The final policy aggregates outputs from both agents via softmax summation. Intrinsic rewards are calculated as $r_i(s) = \frac{1}{n(s) + n(c)}$ based on state visitation counts and environmental changes. The method was compared against IMPALA across tabula rasa and transfer learning settings in Minigrid (Unlock task) and Crafter environments. Key hyperparameters included intrinsic scaling coefficients (α) and a planning ratio of 64 for DreamerV3.

## Key Results
- CBET improved DreamerV3 performance in Crafter but reduced it in Minigrid environments
- DreamerV3 underperformed IMPALA in Minigrid, converging to suboptimal policies
- Transfer learning with pre-trained intrinsic rewards did not immediately lead to optimal policies in Minigrid
- CBET's effectiveness depends on environment characteristics and alignment between exploration and task objectives

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Change-based intrinsic rewards drive exploration in sparse feedback environments by assigning value to novel state transitions.
- **Mechanism:** The agent calculates an intrinsic reward inversely proportional to state visitation count and environmental change count, generating bonus signals for rare states or unexpected changes.
- **Core assumption:** "Interestingness" correlates with strategically valuable states for solving tasks.
- **Evidence anchors:** Abstract highlights CBET addresses sparse feedback; sections 2.3-2.4 define the reward formula and pseudocount hashing; Neighbor 5 confirms effectiveness but warns of behavioral misalignment risks.
- **Break condition:** If "interesting" changes promoted by intrinsic rewards don't align with task goals, the agent may converge to suboptimal policies.

### Mechanism 2
- **Claim:** Policy transfer via softmax aggregation allows pre-trained exploration policies to guide task-specific policies without direct weight interference.
- **Mechanism:** The method freezes the pre-trained exploration policy network and combines its output logits with the task policy network via softmax summation.
- **Core assumption:** The exploration policy provides a useful prior that steers the task policy toward productive behaviors faster than random exploration.
- **Evidence anchors:** Section 2.3 describes the fixed exploration policy and softmax strategy; section 4 notes pre-training doesn't immediately lead to optimal policies, suggesting sensitivity to exploration-task alignment.
- **Break condition:** If the exploration policy confidently suggests suboptimal actions, it may hinder convergence requiring the task policy to "fight" the bias.

### Mechanism 3
- **Claim:** Decoupling world model instances allows CBET to function in model-based architectures without modifying internal latent dynamics.
- **Mechanism:** Two separate DreamerV3 instances are used, with the final policy aggregating outputs from both distinct latent states.
- **Core assumption:** The latent representations of both world models remain sufficiently compatible for their policy logits to be meaningfully summed.
- **Evidence anchors:** Section 2.5 details the architectural modification to maintain original DreamerV3 structure; section 4 discusses the doubling of VRAM requirements as a major caveat.
- **Break condition:** If latent spaces diverge significantly, policy heads may output logits based on incompatible "views" of the state.

## Foundational Learning

- **Concept:** World Models (RSSM)
  - **Why needed here:** DreamerV3 learns by simulating trajectories in a learned latent space. Understanding this is vital because CBET must operate on the outputs of this model, not just raw states.
  - **Quick check question:** Does the agent learn purely from environment interaction, or does it learn from "dreams" generated by its internal model?

- **Concept:** Intrinsic Reward Scaling (α)
  - **Why needed here:** The paper highlights that balancing intrinsic exploration and extrinsic task completion is delicate. Too much intrinsic reward causes the agent to "play" rather than work.
  - **Quick check question:** If the agent explores constantly but never achieves the goal, is the intrinsic coefficient α likely too high or too low?

- **Concept:** Count-Based Exploration
  - **Why needed here:** CBET relies on counting states and changes to determine novelty. One must understand that these counts are estimated rather than exact tabular counts in complex environments.
  - **Quick check question:** How does the system punish revisiting the same state?

## Architecture Onboarding

- **Component map:**
  Exploration Agent ($Agent_i$) -> Task Agent ($Agent_e$) -> Aggregator -> Environment
  (World Model $w_i$ + Policy $f_i$) (World Model $w_e$ + Policy $f_e$) (Softmax summation)

- **Critical path:**
  1. **Pre-training:** Run $Agent_i$ in exploration environment using only intrinsic rewards. Save the model.
  2. **Initialization:** Initialize $Agent_e$ randomly.
  3. **Fine-tuning:** For every environment step, pass observation through both agents. Sum logits via Softmax. Update only $Agent_e$ using extrinsic reward.

- **Design tradeoffs:**
  - **VRAM vs. Simplicity:** The paper notes this architecture doubles VRAM usage. A complex alternative would be modifying DreamerV3 to accept two policy heads into a single world model, but the authors chose decoupled instances to preserve the algorithm's original integrity.
  - **Generalization:** The paper suggests this method works better in "complex" environments (Crafter) but fails in "simple" grid environments (Minigrid), implying a tradeoff between exploration complexity and task alignment.

- **Failure signatures:**
  - **Suboptimal Convergence:** In Minigrid, DreamerV3+CBET converged to lower returns than the baseline. Look for agents that interact with "interesting" objects without completing the specific goal sequence.
  - **High Variance:** The paper notes increased variance in Minigrid with CBET.
  - **Early stagnation:** In transfer learning, returns near zero for the first 200k steps if pre-trained behaviors are incompatible.

- **First 3 experiments:**
  1. **Scaling Grid Search (Tabula Rasa):** Run DreamerV3 in the target environment with varying intrinsic coefficients (α) to establish baseline tolerance.
  2. **Component Ablation:** Compare DreamerV3 (standard) vs. DreamerV3+CBET (Tabula Rasa) vs. DreamerV3+CBET (Transfer) to isolate whether the issue is intrinsic reward itself or transfer mechanism.
  3. **Policy Disagreement Analysis:** Visualize/Log logits of $f_i$ vs $f_e$ during training. If $f_i$ consistently votes against optimal actions identified by $f_e$, it confirms "behavioral misalignment" hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can an intrinsic reward coefficient scheduler effectively automate the balance between exploration and exploitation in world model agents?
- **Basis in paper:** The authors suggest future research could focus on developing a scheduler for intrinsic reward strength to reduce the need for extensive parameter tuning.
- **Why unresolved:** The current study relied on manual grid search to set coefficient α, which is inefficient and may be suboptimal.
- **What evidence would resolve it:** Demonstrating that a dynamic scheduler outperforms fixed coefficients across both Minigrid and Crafter environments.

### Open Question 2
- **Question:** Can fractional transfer learning methods mitigate the high computational overhead of applying CBET to world model agents?
- **Basis in paper:** The authors note that their adaptation doubles VRAM requirements and propose investigating fractional transfer methods that selectively reset model components.
- **Why unresolved:** The current adaptation requires running two separate DreamerV3 instances, making it resource-intensive.
- **What evidence would resolve it:** A modified transfer architecture that maintains performance while significantly reducing memory consumption compared to the dual-instance approach.

### Open Question 3
- **Question:** Why does CBET improve DreamerV3 performance in Crafter but significantly hinder it in Minigrid?
- **Basis in paper:** The authors observe that CBET behaviors do not align with Minigrid objectives, but the specific environment features or model dynamics causing this divergence remain unidentified.
- **Why unresolved:** The paper identifies the performance discrepancy and hypothesizes about behavioral alignment, but does not isolate the causal factors.
- **What evidence would resolve it:** A systematic analysis correlating specific environment characteristics with the success or failure of intrinsic motivation signals in DreamerV3.

## Limitations

- CBET's effectiveness is highly environment-dependent, improving performance in complex environments (Crafter) but reducing it in simpler ones (Minigrid)
- The method requires doubling VRAM resources by maintaining two separate world model instances
- CBET can converge to suboptimal policies when intrinsic rewards misalign with task objectives
- Transfer learning with intrinsic rewards does not immediately lead to optimal policies, requiring further fine-tuning

## Confidence

**High confidence:** The observation that CBET improved DreamerV3 in Crafter but reduced performance in Minigrid is well-supported by experimental results with multiple seeds and consistent trends across runs.

**Medium confidence:** The claim that transfer learning with intrinsic rewards does not immediately lead to optimal policies is supported, but the single-run nature of transfer experiments limits generalization claims.

**Low confidence:** The assertion that behavioral misalignment between exploration and task objectives is the primary cause of Minigrid failure, while plausible, lacks direct mechanistic validation through ablation studies of the exploration component alone.

## Next Checks

1. **Ablation study on exploration component:** Run DreamerV3 with only the extrinsic reward but with the architectural modifications (two world model instances) to isolate whether the performance degradation in Minigrid is due to CBET specifically or the architectural changes.

2. **Latent space analysis:** Compare the latent representations $w_i(x)$ and $w_e(x)$ in both environments to determine if the "incompatible views" hypothesis explains the Minigrid failure, using techniques like similarity metrics or visualization.

3. **Intrinsic reward ablation in Crafter:** Remove the intrinsic reward component while maintaining the two-instance architecture to test whether the Crafter improvements come from CBET's exploration or simply from architectural modifications that provide additional representational capacity.