---
ver: rpa2
title: Identifying the Supply Chain of AI for Trustworthiness and Risk Management
  in Critical Applications
arxiv_id: '2511.15763'
source_url: https://arxiv.org/abs/2511.15763
tags:
- data
- supply
- chain
- risk
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap in systematic risk assessment of AI
  supply chains, particularly for critical applications. The authors propose a lightweight
  taxonomy to categorize AI supply chain entities, helping stakeholders identify dependencies
  and apply risk management frameworks.
---

# Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications

## Quick Facts
- arXiv ID: 2511.15763
- Source URL: https://arxiv.org/abs/2511.15763
- Authors: Raymond K. Sheh; Karen Geappen
- Reference count: 9
- Primary result: A lightweight taxonomy categorizing AI supply chain entities into four components to help stakeholders identify dependencies and apply risk management frameworks

## Executive Summary
This paper addresses the gap in systematic risk assessment of AI supply chains, particularly for critical applications. The authors propose a lightweight taxonomy to categorize AI supply chain entities, helping stakeholders identify dependencies and apply risk management frameworks. The taxonomy divides the supply chain into four components (data, models, programs, infrastructure) with specific roles for each. It enables end-users, especially those without AI expertise, to "consider the right questions" about their AI system's dependencies and potential risks. The taxonomy is designed to bridge the gap between current AI governance and the need for actionable risk assessment, facilitating the application of existing and emerging frameworks like the NIST AI Risk Management Framework.

## Method Summary
The paper introduces a four-component taxonomy for AI supply chains: data creators/aggregators, model creators/aggregators, program creators, and infrastructure providers. Each component is defined with specific roles and relationships to help stakeholders systematically identify dependencies and potential risks. The taxonomy is presented as a conceptual framework rather than an implementation guide, with the authors acknowledging the need for future validation through empirical studies and pilot programs in critical sectors.

## Key Results
- A lightweight taxonomy that divides AI supply chains into four components: data, models, programs, and infrastructure
- Each component has defined roles (creator vs. aggregator) to help identify dependencies and risks
- The taxonomy enables non-expert end-users to ask the right questions about AI system dependencies
- Designed to bridge the gap between AI governance and actionable risk assessment
- Facilitates application of existing and emerging risk management frameworks like NIST AI RMF

## Why This Works (Mechanism)
The taxonomy works by providing a structured framework that maps the complex relationships between different entities in an AI supply chain. By categorizing entities into data, models, programs, and infrastructure with specific creator/aggregator roles, it helps users systematically identify where risks might originate and how they propagate through the system. This structured approach enables users to apply appropriate governance controls and risk management frameworks by providing a clear map of dependencies and potential failure points.

## Foundational Learning
- AI Supply Chain Complexity: Understanding that modern AI systems involve multiple interconnected entities beyond just the end product, why needed to identify where risks originate, quick check: Can you list all entities involved in your AI system?
- Creator vs Aggregator Distinction: Differentiating between entities that create original components versus those that combine existing ones, why needed to understand risk propagation paths, quick check: For each component in your system, identify if it's created or aggregated
- Risk Management Framework Integration: How the taxonomy enables application of existing frameworks like NIST AI RMF, why needed to move from identification to mitigation, quick check: Can you map taxonomy components to specific NIST RMF controls?
- Critical Application Context: The specific needs of AI systems in high-stakes environments like healthcare and law, why needed to prioritize risk assessment efforts, quick check: Does your system operate in a safety-critical or rights-impacting domain?
- Black Box Problem: The challenge of opaque AI models that don't disclose training data or sub-components, why needed to understand limitations of the taxonomy, quick check: Do you have visibility into all upstream components of your AI system?

## Architecture Onboarding

Component Map:
Data Creators -> Data Aggregators -> Model Creators -> Model Aggregators -> Program Creators -> Infrastructure Providers

Critical Path:
The critical path for risk assessment flows from data through to infrastructure, with each component potentially introducing vulnerabilities that affect downstream components.

Design Tradeoffs:
- Granularity vs Usability: More detailed taxonomy increases precision but may overwhelm non-expert users
- Transparency vs Practicality: Full supply chain visibility is ideal but often impractical due to proprietary concerns
- Standardization vs Flexibility: Strict definitions ensure consistency but may not capture all AI system variations

Failure Signatures:
- Incomplete Entity Identification: Missing key dependencies in the supply chain
- Misclassification of Components: Confusing programs with models or creators with aggregators
- Overlooked Interdependencies: Failing to recognize how risks in one component affect others

Three First Experiments:
1. Map an existing AI system using the taxonomy and identify all dependencies
2. Apply the taxonomy to a critical application case study (e.g., medical AI) and assess risk coverage
3. Test the taxonomy with non-expert stakeholders to evaluate usability and effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effectively does the proposed taxonomy facilitate end-user adoption of AI governance controls and frameworks (such as the NIST AI RMF) in live critical application contexts?
- Basis in paper: The conclusion explicitly states that "Future work will include evaluating the applicability and effectiveness of this taxonomy in facilitating the end-user adoption of various AI governance controls and frameworks as they mature."
- Why unresolved: The paper introduces the taxonomy as a conceptual bridge but does not provide empirical data or case study results regarding its actual performance in real-world risk management scenarios.
- What evidence would resolve it: Results from user studies or pilot programs in critical sectors (e.g., healthcare, law) showing that non-experts successfully used the taxonomy to identify dependencies and apply appropriate governance controls.

### Open Question 2
- Question: How can stakeholders rigorously distinguish between "models" (learned components) and "programs" (engineered code) in systems that utilize AI-assisted coding or "vibe coding"?
- Basis in paper: The paper defines "programs" as engineered code and "models" as learned components, but acknowledges that "the distinction between 'program' and 'model' blurs in the current climate of AI-assisted code generation."
- Why unresolved: While the taxonomy relies on this distinction to categorize entities, the paper offers no strict mechanism or heuristic for classifying components in modern development environments where code is generated rather than written.
- What evidence would resolve it: A defined methodology or automated analysis tool capable of accurately decomposing a "vibe coded" system into the taxonomy's four components.

### Open Question 3
- Question: What specific mechanisms can verify supply chain integrity when model creators explicitly restrict visibility into upstream entities (the "black box" problem)?
- Basis in paper: The authors note that many "foundational models have adopted a black box 'trust us' approach," which limits the taxonomy's ability to trace risks to data creators or aggregators.
- Why unresolved: The taxonomy presumes that identifying entities is possible, but does not solve the practical barrier where dominant market players refuse to disclose the provenance of training data or sub-components.
- What evidence would resolve it: Technical standards or third-party certification processes that validate supply chain claims for opaque models without requiring the full disclosure of proprietary training data.

## Limitations
- The taxonomy has not been empirically validated through user studies or pilot programs in real-world critical applications
- No specific implementation guidance or tools are provided for applying the taxonomy in practice
- The paper focuses on conceptual framework development rather than practical deployment considerations
- Does not address how to handle AI-assisted coding environments where the program/model distinction becomes blurred

## Confidence
- **High Confidence**: The identification of AI supply chain complexity and the need for systematic risk assessment in critical applications
- **Medium Confidence**: The proposed taxonomy structure and its potential to help non-expert users understand AI dependencies
- **Low Confidence**: The practical effectiveness and usability of the taxonomy without empirical validation

## Next Checks
1. Conduct user studies with stakeholders in critical applications to evaluate the taxonomy's usability and effectiveness in identifying relevant risks
2. Apply the taxonomy to real-world AI supply chains in critical sectors to test its comprehensiveness and practical utility
3. Validate the taxonomy's alignment with existing risk management frameworks (e.g., NIST AI RMF) through case studies