---
ver: rpa2
title: 'Mitigating Semantic Drift: Evaluating LLMs'' Efficacy in Psychotherapy through
  MI Dialogue Summarization'
arxiv_id: '2511.22818'
source_url: https://arxiv.org/abs/2511.22818
tags:
- llms
- annotation
- summaries
- https
- dialogues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the performance of three state-of-the-art
  LLMs (ChatGPT, Gemini, DeepSeek) in generating and classifying summaries of motivational
  interviewing (MI) dialogues across six MITI-based dimensions. A two-stage annotation
  framework was used, comparing LLM-generated summaries against expert-annotated MI
  dialogues using a five-point Likert scale.
---

# Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization

## Quick Facts
- arXiv ID: 2511.22818
- Source URL: https://arxiv.org/abs/2511.22818
- Reference count: 28
- Three state-of-the-art LLMs (ChatGPT, Gemini, DeepSeek) evaluated for generating and classifying MI dialogue summaries across six MITI-based dimensions

## Executive Summary
This study evaluates three LLMs (ChatGPT, Gemini, DeepSeek) on their ability to generate and classify summaries of motivational interviewing (MI) dialogues across six MITI dimensions. Using a two-stage annotation framework with expert-annotated ground truth, the research introduces the AnnoSUM-MI dataset and demonstrates that ChatGPT with one-shot prompting achieves the lowest semantic drift across all dimensions. The work highlights the importance of tailored prompting strategies in maintaining contextual fidelity in sensitive therapeutic domains.

## Method Summary
The study uses a two-stage framework: (1) expert annotation of original MI dialogues from the AnnoMI dataset (131 sessions) across six MITI dimensions using 5-point Likert scales, and LLM summary generation with one-shot and few-shot prompting strategies; (2) LLM-based classification of all generated summaries across the same dimensions. Three LLMs (ChatGPT 4.0, Gemini 2.0 Flash, DeepSeek V3) generate summaries for 34 test sessions, which are then classified by all three models, creating a cross-model evaluation matrix. Deviation from ground truth is measured as absolute difference between predicted and expert Likert scores.

## Key Results
- ChatGPT (one-shot) demonstrated the lowest deviation from ground truth across all six dimensions
- Gemini showed the highest deviation, with summaries described as "brief and not detailed enough to reflect the intensity of emotion involved"
- One-shot prompting outperformed few-shot prompting for ChatGPT, contrary to typical expectations
- Moderate inter-annotator agreement (κ=0.50) was achieved on the annotation scheme

## Why This Works (Mechanism)

### Mechanism 1
- Claim: One-shot prompting with domain-grounded examples reduces semantic drift more effectively than few-shot for certain model architectures
- Mechanism: A single high-quality exemplar provides sufficient context-window anchoring without introducing conflicting pattern signals that emerge from multiple examples with varying quality levels. The exemplar acts as a "semantic anchor" that constrains the model's generative space toward the target domain's conceptual boundaries
- Core assumption: The quality and representativeness of the single exemplar determines effectiveness; poor exemplars may increase drift
- Evidence anchors: [abstract] "ChatGPT (one-shot) demonstrated the lowest deviation from ground truth across all dimensions"; [section V] "ChatGPT has shined among all the LLMs and has consistently performed better for all sets of experiments"
- Break condition: When dialogue complexity exceeds the exemplar's representational scope, or when model architecture exhibits "extreme" interpretation tendencies that override exemplar guidance

### Mechanism 2
- Claim: Structured annotation schemes derived from clinical frameworks (MITI) enable quantifiable evaluation of subjective therapeutic constructs
- Mechanism: The six MITI dimensions (evocation, collaboration, autonomy, direction, empathy, non-judgmental attitude) operationalize abstract therapeutic qualities into discrete 5-point Likert ratings. This transforms qualitative assessment into a multi-output multi-class classification problem, creating a shared metric space between human experts and LLM outputs
- Core assumption: Expert annotations on the same scale represent valid ground truth; inter-rater reliability (κ=0.50) is sufficient for meaningful comparison
- Evidence anchors: [section III-A] "We utilize a five-point Likert scale to assess attitudes, perceptions, and behaviors across the six attributes"; [section IV-A] "The inter-annotation agreement (Cohen's Kappa) score for these 15 MI sessions is 0.50"
- Break condition: When constructs are too nuanced for discrete categorization, or when annotator expertise varies significantly across dimensions

### Mechanism 3
- Claim: Cross-model evaluation (using one LLM to classify another's outputs) reveals model-specific biases in summarization quality
- Mechanism: The experimental design uses each LLM to classify all generated summaries (including its own and competitors'), creating a matrix that isolates generation quality from classification ability. Consistent deviation patterns across classifiers indicate genuine summarization differences rather than evaluation artifacts
- Core assumption: Classification capability is relatively consistent across models; deviation reflects summarization quality rather than classifier bias
- Evidence anchors: [section IV-B] "Each LLM is then tasked with classifying all six sets covering both its own outputs and those of the other models"; [section V] "Gemini's summaries are brief and not detailed enough to reflect the intensity of the emotion involved"
- Break condition: When classifier and generator share systematic biases (same model family), leading to artificially low deviation scores that mask quality issues

## Foundational Learning

- Concept: **Motivational Interviewing Treatment Integrity (MITI) Framework**
  - Why needed here: The entire evaluation schema is built on MITI's six dimensions. Without understanding what "evocation" or "autonomy" mean therapeutically, you cannot interpret deviation scores or design effective prompts
  - Quick check question: Can you explain why "evocation" is distinct from "direction" in a therapeutic context?

- Concept: **Semantic Drift in Generative Models**
  - Why needed here: The paper's core contribution is measuring and mitigating drift. Understanding that drift is gradual deviation from source meaning—not just factual errors—is essential for interpreting the deviation metric
  - Quick check question: How would you distinguish semantic drift from hallucination in a therapy summary?

- Concept: **Multi-Output Multi-Class Classification**
  - Why needed here: The evaluation task is framed as predicting six independent Likert-scale values per summary. This differs from single-label classification and affects how you interpret performance metrics
  - Quick check question: Why would accuracy be misleading for this task compared to deviation analysis?

## Architecture Onboarding

- Component map: AnnoMI dataset (131 sessions) → stratified split (97 train / 34 test) → Expert annotation Stage 1 → LLM summary generation (3 models × 2 prompting strategies) → Expert annotation Stage 2 → LLM classification (3 models × 6 summary variants) → Deviation calculation

- Critical path: 1. Establish ground truth via expert annotation (Cohen's κ ≥ 0.40 threshold) 2. Generate summaries with consistent prompt templates across models 3. Run cross-model classification matrix (3 × 6 = 18 experiments) 4. Compute deviation radar plots per dimension per model combination

- Design tradeoffs:
  - Dataset size vs. annotation quality: 34-session test set limits statistical power but enables thorough expert review
  - Model-as-evaluator vs. human-evaluator: Using LLMs for classification introduces potential bias but scales evaluation; paper acknowledges this limitation
  - One-shot vs. few-shot: One-shot reduces prompt engineering overhead but may underutilize context window capacity for complex domains

- Failure signatures:
  - Extreme interpretation bias: Model assigns only extreme values (1 or 5), indicating failure to capture nuance (observed in Gemini)
  - Context loss in long prompts: Model loses mid-prompt information, producing inconsistent attribute descriptions (observed in DeepSeek)
  - Cross-model bias inflation: Model rates its own outputs more favorably than competitors' (assumption: monitor for this pattern)

- First 3 experiments:
  1. Baseline deviation mapping: Run all three models with zero-shot prompting on 5 held-out sessions to establish drift baselines before adding exemplars
  2. Exemplar sensitivity analysis: Systematically vary the quality of the one-shot exemplar (high-quality vs. low-quality MI summary) to quantify exemplar impact on ChatGPT's deviation scores
  3. Dimension-specific prompt refinement: Isolate the "empathy" dimension where all models showed higher deviation (per radar plots) and test dimension-specific prompt instructions targeting empathetic language preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using the same LLM for both summary generation and evaluation introduce systematic bias, and would independent human evaluation or cross-model validation yield different performance rankings?
- Basis in paper: [explicit] The authors state in Section VI: "In our approach, LLMs are used for both generating and evaluating summaries, which may introduce bias and affect the objectivity of performance assessment. Future work will explore independent human evaluations or cross-model validation to mitigate this."
- Why unresolved: The current experimental design has LLMs classify their own and other models' summaries, creating potential circularity in evaluation. Without independent human evaluation of summaries, it remains unclear whether ChatGPT's superior performance reflects genuine quality or evaluator-generator alignment
- What evidence would resolve it: A study where human MI experts, blinded to which model generated which summary, rate the summaries using the same six-dimension Likert scale, with results compared against LLM-based classification scores

### Open Question 2
- Question: Why does one-shot prompting outperform few-shot prompting for some LLMs in this domain, contrary to typical expectations that more examples improve performance?
- Basis in paper: [inferred] The results section states "ChatGPT (one-shot) demonstrated the lowest deviation from ground truth across all dimensions," and the radar plot (Fig. 7) shows one-shot often outperforming few-shot. The paper does not explain this counterintuitive finding
- Why unresolved: Standard LLM behavior suggests few-shot should improve contextual understanding by providing more examples. The reversal of this pattern in therapeutic dialogue summarization suggests domain-specific dynamics not yet understood—possibly related to prompt length, context window issues, or overfitting to example styles
- What evidence would resolve it: Systematic ablation studies varying the number and content of shots, combined with attention analysis to understand how models weight exemplar information versus target dialogue content in therapeutic contexts

### Open Question 3
- Question: Can the moderate inter-annotator agreement (Cohen's Kappa = 0.50) be improved through annotation scheme refinement, or does it reflect inherent subjectivity in rating therapeutic constructs?
- Basis in paper: [inferred] The paper reports "The inter-annotation agreement (Cohen's Kappa) score for these 15 MI sessions is 0.50" and notes this is "moderate" but does not investigate whether this ceiling stems from the scheme's design or the intrinsic ambiguity of MI dimensions
- Why unresolved: Human experts achieve only moderate agreement, yet this human-level agreement serves as ground truth for evaluating LLMs. If the annotation task itself has inherent ambiguity, then "semantic drift" may be partially attributable to ambiguous criteria rather than LLM limitations
- What evidence would resolve it: Analysis of specific disagreement patterns between annotators, followed by annotation scheme refinements (e.g., more detailed rubrics, behavioral anchors, or dimension-specific guidelines) tested through new annotation rounds with the same expert pool

## Limitations
- The exact prompt templates for one-shot and few-shot conditions are not fully specified, limiting reproducibility
- Using LLMs to evaluate other LLMs introduces potential circularity and bias, though cross-model evaluation partially mitigates this
- Results are based on 34 test sessions from a single MI dataset, limiting generalizability

## Confidence

- **High confidence**: The finding that ChatGPT outperforms Gemini and DeepSeek across all dimensions using one-shot prompting is well-supported by the deviation metrics presented
- **Medium confidence**: The mechanism that one-shot prompting provides better semantic anchoring than few-shot prompting is plausible but not definitively proven due to prompt template ambiguity
- **Medium confidence**: The MITI framework effectively operationalizes therapeutic constructs for LLM evaluation, though the moderate inter-annotator agreement suggests inherent subjectivity

## Next Checks

1. **Prompt template replication**: Reconstruct and test the exact one-shot and few-shot prompt templates to verify if ChatGPT's performance advantage persists with identical prompts
2. **Human-in-the-loop validation**: Have independent clinicians evaluate a subset of LLM-generated summaries to confirm that LLM classification patterns align with expert judgment
3. **Cross-domain generalization**: Apply the same evaluation framework to non-MI therapeutic dialogues (e.g., cognitive behavioral therapy) to test if the one-shot prompting advantage generalizes beyond the MI domain