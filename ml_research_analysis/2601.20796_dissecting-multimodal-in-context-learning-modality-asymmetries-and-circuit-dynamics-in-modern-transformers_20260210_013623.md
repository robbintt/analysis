---
ver: rpa2
title: 'Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit
  Dynamics in modern Transformers'
arxiv_id: '2601.20796'
source_url: https://arxiv.org/abs/2601.20796
tags:
- multimodal
- accuracy
- data
- learning
- in-context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically dissects multimodal in-context learning
  (ICL) in transformers by training small, architecturally realistic models on synthetic
  tasks with precise control over data statistics and model components. Key findings
  include: (1) Modern transformer architectures preserve the statistical drivers of
  unimodal ICL, but rotary position embeddings (RoPE) raise the data complexity threshold
  by weakening induction circuits.'
---

# Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers

## Quick Facts
- arXiv ID: 2601.20796
- Source URL: https://arxiv.org/abs/2601.20796
- Reference count: 40
- Primary result: Multimodal ICL exhibits fundamental asymmetry: pretraining decoder on high-diversity primary modality enables surprisingly low data complexity in secondary modality for strong ICL

## Executive Summary
This paper systematically investigates multimodal in-context learning (ICL) in transformers through controlled synthetic experiments. The authors identify a fundamental learning asymmetry where pretraining the decoder on a high-diversity primary modality enables strong multimodal ICL with surprisingly low data complexity in the secondary modality. They show this asymmetry emerges from pretrained circuit reuse rather than memorization, with scaling improving ICL by mapping the secondary modality into existing circuits. Mechanistically, both unimodal and multimodal ICL rely on induction circuits that copy labels from matching in-context exemplars, with multimodal training refining rather than creating new circuits.

## Method Summary
The study uses two-layer transformer decoders with RMSNorm, SiLU activation, and either RoPE or APE positional encodings. Models are trained on synthetic Gaussian mixture data with precisely controlled class diversity (K), burstiness (B), and skew (Î±). Multimodal training follows a two-stage approach: pretrain decoder on primary modality M1 with high diversity, then add an MLP projector (optionally preceded by a pretrained encoder for M2) and jointly train on interleaved sequences. Progress measurements track induction circuit formation through previous-token head strength, induction head strength, target label association, and context-label accuracy metrics.

## Key Results
- Modern transformer architectures preserve unimodal ICL statistical drivers, but RoPE raises data complexity threshold by weakening induction circuits
- Multimodal ICL exhibits fundamental asymmetry: pretraining decoder on high-diversity primary modality enables strong ICL with surprisingly low secondary modality data complexity
- Pretrained encoder is critical for cross-modal alignment, with encoder quality strongly predicting downstream ICL performance
- Both unimodal and multimodal ICL rely on induction circuits; multimodal training primarily refines label-matching induction head to accommodate new modality

## Why This Works (Mechanism)

### Mechanism 1: Induction Circuit Formation and Refinement
ICL relies on a two-layer induction circuit installed during unimodal pretraining and refined during multimodal training. Layer 1 learns a "previous-token head" that copies information forward, while Layer 2 learns an "induction head" that matches query features to context exemplars and retrieves their labels. Multimodal training refines the label-matching induction head rather than creating new circuits. RoPE weakens but doesn't eliminate induction patterns, though higher data complexity can compensate.

### Mechanism 2: Modality Asymmetry via Pretrained Circuit Reuse
When the decoder is pretrained on high-diversity primary modality data, the secondary modality requires surprisingly low data complexity for strong ICL. Primary modality pretraining installs the ICL circuit, and the secondary modality only needs to provide distinguishable signals that the projector maps onto the decoder's existing feature space. Scaling improves multimodal ICL by leveraging capacity for this mapping rather than memorization. This curriculum effect is confirmed by early-fusion joint training, which reverses the asymmetry.

###