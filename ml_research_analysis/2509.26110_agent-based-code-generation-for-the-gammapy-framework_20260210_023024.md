---
ver: rpa2
title: Agent-based code generation for the Gammapy framework
arxiv_id: '2509.26110'
source_url: https://arxiv.org/abs/2509.26110
tags:
- code
- gammapy
- generation
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an agent-based approach for code generation
  tailored to the Gammapy framework in gamma-ray astronomy. Specialized scientific
  libraries like Gammapy often lack the resources and stable APIs that foundational
  models rely on, making it challenging to generate reliable analysis scripts.
---

# Agent-based code generation for the Gammapy framework

## Quick Facts
- arXiv ID: 2509.26110
- Source URL: https://arxiv.org/abs/2509.26110
- Reference count: 12
- Agent-based code generation for Gammapy framework achieves 100% pass rates on benchmark tasks

## Executive Summary
This paper introduces an agent-based approach for generating reliable Gammapy code, addressing the challenge that specialized scientific libraries often lack the resources and stable APIs that foundational models rely on. The system uses an iterative loop where the agent writes, executes, and validates Gammapy scripts in a controlled sandbox environment, automatically repairing code until successful execution. The framework enforces strict rules through prompting, integrates tightly with the Gammapy stack, and includes a minimal web demo and benchmarking suite. Evaluation using OpenAI's o3 and GPT-5 models shows both achieve 100% pass rates on smaller tasks, with recent models slightly faster.

## Method Summary
The agent-based approach works through an iterative loop where the agent generates Gammapy code, executes it in a controlled sandbox environment, and validates the results. If execution fails or validation criteria aren't met, the agent repairs the code and repeats the process until success. The system enforces strict prompting rules and integrates closely with the Gammapy framework to ensure code reliability. The framework includes a minimal web demonstration and comprehensive benchmarking suite for evaluation. The approach is designed to handle the challenges of specialized scientific libraries that often lack the extensive documentation and stable APIs that general-purpose code generation systems rely upon.

## Key Results
- OpenAI's o3 and GPT-5 models both achieve 100% pass rates on benchmark tasks
- Recent model versions demonstrate slightly faster execution times
- Framework supports reproducibility and extensibility for future enhancements
- Includes plans for open-weight model integration and multi-agent collaboration

## Why This Works (Mechanism)
The system succeeds by combining three key mechanisms: first, the iterative write-execute-validate-repair loop allows the agent to self-correct errors through direct feedback from execution rather than relying solely on model predictions. Second, the strict prompting framework with enforced rules guides the agent toward Gammapy-specific best practices and API usage patterns. Third, the sandbox execution environment provides a safe, controlled space where code can be tested without risking system integrity while capturing comprehensive execution traces for validation.

## Foundational Learning

**Gammapy Framework** - A Python package for gamma-ray astronomy analysis
*Why needed*: The target domain requires specialized knowledge of gamma-ray data analysis workflows and API patterns
*Quick check*: Can identify correct Gammapy classes for common analysis tasks (e.g., event selection, spectrum extraction)

**Agent-based Code Generation** - AI agents that write, test, and iteratively improve code
*Why needed*: Traditional code generation lacks the feedback loop necessary for scientific software reliability
*Quick check*: Can describe the write-execute-validate-repair cycle and its benefits over single-pass generation

**Sandbox Execution Environment** - Isolated runtime for safe code testing
*Why needed*: Prevents system damage while allowing comprehensive execution trace capture
*Quick check*: Can explain how sandbox isolation differs from standard execution and why it's critical for automated testing

## Architecture Onboarding

**Component Map**: User Request -> Prompt Generator -> Code Generator -> Sandbox Executor -> Validator -> (Feedback Loop to Code Generator)

**Critical Path**: The write-execute-validate-repair cycle forms the core execution path, with each iteration refining the code until validation passes. This loop must maintain state between iterations to track changes and avoid infinite cycles.

**Design Tradeoffs**: The framework trades execution speed for reliability by using iterative validation rather than attempting perfect generation in one pass. Closed-weight models provide better performance but limit reproducibility, while open-weight alternatives would enable independent verification but may have lower accuracy.

**Failure Signatures**: Common failures include infinite repair loops when validation criteria are too strict, sandbox environment misconfiguration leading to false negatives, and model inability to interpret complex validation feedback for subtle logical errors.

**3 First Experiments**:
1. Test single Gammapy function generation (e.g., event selection) to verify basic agent capability
2. Execute benchmark suite with open-weight models to compare performance against closed models
3. Run generated scripts in standard Gammapy installation (without sandbox) to test real-world robustness

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but implies several areas for future work: performance with open-weight models, scalability to complex multi-file analysis pipelines, integration with other scientific frameworks beyond Gammapy, and the potential for multi-agent collaboration to handle increasingly complex workflows.

## Limitations

- Benchmark results rely exclusively on closed-weight models, preventing independent verification
- Evaluation only tests relatively small-scale tasks, leaving uncertainty about performance on complex real-world pipelines
- Sandbox environment may not capture all edge cases and dependencies present in actual research workflows

## Confidence

**High confidence**: The agent-based architecture and sandbox execution approach are technically sound and well-implemented

**Medium confidence**: The 100% pass rates on benchmark tasks, given limited task complexity and closed-model dependency

**Low confidence**: Generalizability to production-scale Gammapy workflows and performance with open-weight alternatives

## Next Checks

1. Implement and test the framework with open-weight models (e.g., DeepSeek-R1, Qwen2.5-Coder) to assess performance parity and enable independent verification
2. Expand benchmark suite to include multi-file, end-to-end gamma-ray analysis pipelines that mirror actual research use cases
3. Conduct blind validation where generated scripts are executed in standard Gammapy installations without the specialized sandbox environment to test real-world robustness