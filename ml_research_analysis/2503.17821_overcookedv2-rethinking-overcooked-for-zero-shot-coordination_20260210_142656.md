---
ver: rpa2
title: 'OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination'
arxiv_id: '2503.17821'
source_url: https://arxiv.org/abs/2503.17821
tags:
- agents
- environment
- coordination
- layouts
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that Overcooked, a popular benchmark for zero-shot
  coordination (ZSC), primarily tests state coverage rather than true coordination
  challenges. The authors introduce a state-augmentation algorithm that mixes in states
  encountered with unknown partners during training, nearly closing the SP-XP gap
  across all layouts.
---

# OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination

## Quick Facts
- arXiv ID: 2503.17821
- Source URL: https://arxiv.org/abs/2503.17821
- Reference count: 40
- Key outcome: State augmentation nearly closes SP-XP gap on original Overcooked; OvercookedV2 introduces asymmetric information and partial observability for true coordination challenges.

## Executive Summary
This paper reveals that Overcooked, a popular benchmark for zero-shot coordination (ZSC), primarily tests state coverage rather than true coordination challenges. The authors introduce a state-augmentation algorithm that mixes in states encountered with unknown partners during training, nearly closing the SP-XP gap across all layouts. This demonstrates that ZSC failures in Overcooked are largely due to poor state coverage rather than more sophisticated coordination challenges. To address these shortcomings, the authors introduce OvercookedV2, a new benchmark featuring asymmetric information, stochasticity, and meaningful partial observability. OvercookedV2 includes three handcrafted coordination challenges: grounded communication, test-time protocol formation, and implicit communication through actions. Experiments show that state-of-the-art ZSC methods struggle with these new challenges, highlighting the need for novel algorithms capable of adaptive coordination.

## Method Summary
The paper introduces two key contributions: a state-augmentation mechanism for improving ZSC in the original Overcooked environment, and OvercookedV2 as a new benchmark for evaluating coordination algorithms. The state-augmentation algorithm iteratively collects trajectories from all policy pairings, stores every 10th state in a buffer, and samples initial states from this buffer for subsequent training episodes. This forces agents to learn robust recovery behaviors for states that standard self-play might rarely visit. For OvercookedV2, the authors introduce partial observability with limited view radius, asymmetric information through recipe indicators visible only to co-located agents, and handcrafted coordination challenges requiring grounded communication, test-time protocol formation, and implicit communication. Agents are trained using PPO with a specific network architecture including 1x1 convolutions, 2D convolutions, and a GRU for handling partial observability.

## Key Results
- State-augmentation mechanism nearly closes the SP-XP gap on all original Overcooked layouts by improving state coverage
- Self-Play, State-Augmented, Other-Play, and FCP all fail to achieve high cross-play scores on OvercookedV2 coordination challenges
- OvercookedV2 successfully creates coordination challenges requiring information exchange and test-time protocol formation
- Standard Overcooked architectures fail to condition on recipe indicators in V2 layouts, requiring the proposed 1x1 Conv + GRU architecture

## Why This Works (Mechanism)

### Mechanism 1: State-Augmentation for Distribution Coverage
If agents are trained on a distribution of starting states that includes states likely to be encountered by unknown partners, the performance gap between self-play (SP) and cross-play (XP) significantly decreases. An iterative algorithm collects trajectories from all policy pairings during training, extracts every $n$-th state into a buffer, and samples initial states from this buffer for subsequent training episodes. This forces the agent to learn robust recovery behaviors for states that standard self-play might rarely visit, thereby treating ZSC failure primarily as an out-of-distribution generalization problem.

### Mechanism 2: Asymmetric Information Inducing Communication
If the environment hides critical state information (like the current recipe) from specific agents while making it visible to others, agents are forced to develop grounded communication strategies to coordinate effectively. OvercookedV2 introduces a limited "view radius" and specific objects (Recipe Indicators) that provide information only to co-located agents. To solve the task, the informed agent must signal the recipe using "Grounded Communication" (buttons) or "Implicit Communication" (actions), moving the challenge from state coverage to information exchange.

### Mechanism 3: Test-Time Protocol Formation via Feedback
If agents receive grounded feedback (success/failure of a delivery) but lack a pre-established communication protocol, successful ZSC requires the ability to adapt and form protocols online during execution. In layouts like "Test-Time Protocol Formation," agents receive a reward signal only upon correct delivery. The mechanism relies on the agent updating its policy or internal state based on this feedback to align with the partner's current signaling strategy.

## Foundational Learning

- **Concept: Decentralized Partially Observable Markov Decision Process (Dec-POMDP)**
  - Why needed here: OvercookedV2 is formally defined as a Dec-POMDP. Understanding that each agent receives a distinct observation $o_i$ and cannot see the full state $S$ is critical to grasping why communication is necessary.
  - Quick check question: Can Agent A determine the current recipe solely from its own observation in the "Grounded Coordination" layout?

- **Concept: The SP-XP Gap**
  - Why needed here: This is the primary evaluation metric. It measures the difference in reward when an agent plays with itself (Self-Play) versus when it plays with an independently trained agent (Cross-Play). A high gap indicates overfitting to specific partner behaviors.
  - Quick check question: If an agent achieves high SP reward but near-zero XP reward, what does this imply about its learned policy?

- **Concept: Arbitrary vs. Grounded Conventions**
  - Why needed here: The paper distinguishes between arbitrary conventions (e.g., "spin twice means go left") which fail in ZSC, and grounded conventions (e.g., "press button to reveal recipe") which transfer better.
  - Quick check question: Why does the "Other-Play" algorithm attempt to break symmetries, and how does that relate to arbitrary conventions?

## Architecture Onboarding

- **Component map:** Sparse grid observation (Width × Height × Channels) -> Three 1x1 Convolutional layers (128, 128, 8 channels) -> Three 2D Convolutional layers (16, 32, 32 channels) -> GRU (128 hidden units) -> Actor (categorical distribution) and Critic (value estimate)

- **Critical path:** The addition of 1x1 Convolutions and the GRU is the critical deviation from standard Overcooked baselines. The 1x1 convolutions are explicitly added because the observations in OvercookedV2 are sparse (mostly empty grid), and standard architectures failed to condition correctly on the recipe indicator.

- **Design tradeoffs:**
  - 1x1 Convs vs. Larger Kernels: 1x1 convolutions are computationally cheaper for mixing channel information on sparse grids but may lose immediate spatial context, hence the follow-up 3x3 layers.
  - GRU vs. Feed-forward: The GRU adds computational overhead but is strictly necessary for the partial observability introduced in V2 (to remember recipes or partner actions).

- **Failure signatures:**
  - Stalling/Repetitive Actions: Agents stuck in corners or picking up/placing the same item repeatedly
  - Negative Reward Accumulation: In V2 layouts with negative rewards for wrong delivery, agents might repeatedly deliver the wrong recipe if they fail to interpret partner signals
  - Convention Mismatch: Agents ignoring each other or moving in circles in "Test-Time Protocol" layouts due to mismatched arbitrary conventions

- **First 3 experiments:**
  1. **State-Coverage Ablation (Original Layouts):** Train agents on the original "Counter Circuit" layout with and without the state-augmentation buffer. Verify that XP scores improve purely through state diversity.
  2. **Architectural Validation (V2 Layouts):** Train an agent on "Grounded Coordination Simple" using a standard feed-forward net vs. the proposed 1x1 Conv + GRU net. Confirm that the standard net fails to condition on the recipe.
  3. **ZSC Baseline (Cross-Play):** Train two populations of agents (Self-Play vs. Other-Play) on "Test-Time Protocol Simple." Run the cross-play matrix to observe if Other-Play reduces the SP-XP gap compared to Self-Play.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can novel algorithms be developed that successfully perform test-time protocol formation and online adaptation to solve the coordination challenges in OvercookedV2?
- Basis in paper: Appendix E states: "The formation of protocols at test time requires novel ZSC methods capable of adapting online to other agents," noting that current methods struggle with this specific challenge.
- Why unresolved: The authors demonstrate that existing state-of-the-art ZSC methods (Self-Play, State-Augmented, Other-Play, FCP) fail to close the SP-XP gap in layouts requiring test-time adaptation, as they rely on fixed conventions.
- What evidence would resolve it: The development of an algorithm that achieves high cross-play scores (closing the SP-XP gap) specifically in the "Test-Time Protocol Formation" layouts without relying on pre-trained conventions.

### Open Question 2
- Question: How do current ZSC agents perform when coordinating with human partners in the OvercookedV2 environment?
- Basis in paper: Appendix E notes: "We already provide an interactive interface for humans to interact with our new environment, and future work should focus on benchmarking human-AI coordination in OvercookedV2."
- Why unresolved: The paper primarily evaluates AI-AI cross-play as a proxy for human coordination, but the introduction of asymmetric information and partial observability may create unique challenges when interacting with human reasoning.
- What evidence would resolve it: Empirical results from experiments pairing trained agents with human participants, measuring performance and coordination success rates.

### Open Question 3
- Question: Can Unsupervised Environment Design (UED) be effectively utilized to automatically generate diverse coordination scenarios in OvercookedV2?
- Basis in paper: Appendix E suggests: "The grid-world nature of OvercookedV2... provides an ideal platform for exploring UED... This also opens up new avenues for investigating the generalisation capabilities of ZSC methods."
- Why unresolved: The current benchmark relies on handcrafted layouts; it is unknown if automated generation can produce valid, challenging scenarios that improve agent generalization.
- What evidence would resolve it: A UED implementation that generates solvable layouts and agents trained via UED demonstrating superior zero-shot transfer to unseen layouts compared to agents trained on fixed sets.

## Limitations

- The state-augmentation mechanism, while effective for improving state coverage, may not generalize to domains where coordination failures stem from incompatible strategies rather than unseen states.
- The paper does not explore whether the proposed state-augmentation mechanism can be extended to handle the more complex coordination challenges in OvercookedV2, where information asymmetry and test-time protocol formation dominate.
- The effectiveness of the benchmark relies on handcrafted coordination challenges, and it remains unclear how well these challenges capture the full spectrum of real-world coordination difficulties.

## Confidence

- **High Confidence**: State-augmentation mechanism effectively closes SP-XP gap on original Overcooked layouts by improving state coverage.
- **Medium Confidence**: OvercookedV2 successfully introduces novel coordination challenges requiring information exchange and test-time protocol formation.
- **Medium Confidence**: Standard ZSC methods (Self-Play, Other-Play, FCP) perform poorly on OvercookedV2 coordination challenges, validating the need for new algorithms.

## Next Checks

1. **Cross-Domain Transfer**: Apply state-augmentation to another popular ZSC benchmark (e.g., CoinRun, Hanabi) to test whether poor state coverage is a generalizable bottleneck.

2. **Information-Theoretic Analysis**: Measure the mutual information between agent observations and hidden states (recipes) in OvercookedV2 to quantify the communication burden and validate that asymmetric information creates genuine coordination challenges.

3. **Protocol Formation Baseline**: Implement and evaluate a simple test-time adaptation algorithm (e.g., policy distillation from cross-play rollouts) on the "Test-Time Protocol Formation" layout to establish a performance floor for online coordination.