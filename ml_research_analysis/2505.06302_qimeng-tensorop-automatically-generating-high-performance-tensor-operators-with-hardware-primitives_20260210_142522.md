---
ver: rpa2
title: 'QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators
  with Hardware Primitives'
arxiv_id: '2505.06302'
source_url: https://arxiv.org/abs/2505.06302
tags:
- hardware
- tensor
- performance
- llms
- qimeng-tensorop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces QiMeng-TensorOp, a framework that automatically
  generates high-performance tensor operators (GEMM, Conv) using hardware primitives
  across diverse hardware (RISC-V, ARM, NVIDIA GPUs) with a single-line user prompt.
  The method leverages large language models (LLMs) to comprehend hardware architectures,
  auto-extract hardware factors, generate optimized code, and tune parameters using
  an LLM-assisted MCTS algorithm.
---

# QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives

## Quick Facts
- **arXiv ID**: 2505.06302
- **Source URL**: https://arxiv.org/abs/2505.06302
- **Reference count**: 12
- **Key outcome**: Automatically generates high-performance tensor operators (GEMM, Conv) across diverse hardware using a single-line user prompt, achieving up to 1291× improvement over vanilla LLMs and 251% of OpenBLAS on RISC-V CPUs.

## Executive Summary
QiMeng-TensorOp is a framework that leverages large language models to automatically generate optimized tensor operator implementations across diverse hardware platforms. The system uses structured hardware optimization hints to enable LLMs to map abstract optimization techniques to concrete hardware factors, decomposing the generation task into high-level sketch creation and low-level kernel emission. Through LLM-assisted Monte Carlo Tree Search tuning, it achieves significant performance improvements over both baseline LLM outputs and traditional hand-optimized libraries while dramatically reducing development costs compared to human experts.

## Method Summary
The framework employs a three-stage pipeline: (1) Hardware Architecture Comprehending, where LLMs extract key hardware factors from technical manuals using natural language optimization hints; (2) Tensor Operator Generation, where LLMs first create C/CUDA sketches with tiling and reordering logic, then generate Python scripts that emit hardware-specific assembly or PTX kernels; (3) Auto-Tuning, where LLM-guided MCTS optimizes sketch parameters and instruction ordering based on compilation feedback and performance metrics. The system supports multiple hardware targets including RISC-V, ARM, and NVIDIA GPUs, and handles operators like GEMM and convolution through a single-line prompt interface.

## Key Results
- Achieves 251% of OpenBLAS performance on RISC-V CPUs and 124% of cuBLAS on NVIDIA GPUs
- Demonstrates 1291× performance improvement over vanilla LLM generation
- Reduces development costs by 200× compared to human expert implementation
- Outperforms established libraries like TVM across multiple hardware platforms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured hardware optimization hints enable LLMs to map abstract optimization techniques to concrete hardware factors
- Mechanism: The framework provides natural-language descriptions of five optimization primitives (Tiling, Reordering, Vectorization, Layout, Pipeline) and their relationships to four hardware factors (Memory Hierarchy, Instructions, Registers, SMs). This activates domain knowledge in pretrained LLMs, allowing them to reason about which optimizations apply to which hardware characteristics.
- Core assumption: LLMs have sufficient latent knowledge about hardware architecture concepts from pretraining; the hints primarily activate rather than teach this knowledge.

### Mechanism 2
- Claim: Decomposing tensor operator generation into sketch (high-level structure) and kernel (hardware primitives) improves correctness by leveraging LLMs' relative strengths at different abstraction levels
- Mechanism: LLMs generate C/CUDA sketches with tiling and reordering logic, then generate Python scripts that emit assembly or PTX kernels. The two-stage approach separates control flow from instruction-level optimization, reducing the complexity each generation step must handle.
- Core assumption: LLMs can reliably generate correct Python metaprogramming code even when they struggle with direct assembly/PTX generation.

### Mechanism 3
- Claim: LLM-guided MCTS expansion using search history improves tuning efficiency by injecting semantic priors into action selection
- Mechanism: During MCTS expansion, the LLM receives path history (tuning actions and performance feedback) and global tree history to select promising actions and generate candidate action spaces. This replaces fixed expansion rules with adaptive, context-aware exploration.
- Core assumption: Performance feedback patterns in search history contain learnable structure that LLMs can extrapolate to propose better candidate configurations.

## Foundational Learning

- Concept: **Memory Hierarchy and Tiling**
  - Why needed here: Understanding how L1/L2/L3 cache sizes determine optimal tile dimensions (BM, BN, BK parameters) is essential for interpreting the sketch generation outputs and tuning search space
  - Quick check question: Given a CPU with 32KB L1 cache and 4-byte floats, what is the maximum tile dimension for a square submatrix that fits entirely in L1?

- Concept: **Vector Instructions and Register Allocation**
  - Why needed here: The kernel generation step assumes familiarity with vector ISAs (RVV, NEON) and how register count/width constrains loop unrolling and software pipelining
  - Quick check question: If a RISC-V vector unit has 32 vector registers of 128 bits each and you need to hold three 32-bit element vectors (A, B, C accumulator), what is the maximum vector length (VL) you can achieve without spilling?

- Concept: **Monte Carlo Tree Search (MCTS)**
  - Why needed here: The auto-tuning component extends standard MCTS with LLM-guided expansion; understanding selection, expansion, simulation, and backpropagation phases is prerequisite to debugging or modifying the search
  - Quick check question: In UCB-based selection, what happens to exploration vs. exploitation balance as the exploration constant C increases?

## Architecture Onboarding

- Component map:
  Hardware Factor Extractor -> Sketch Generator -> Kernel Generator -> MCTS Tuner -> Feedback Loop

- Critical path:
  1. User prompt → Hardware Factor Extractor → factor dictionary
  2. Factors + optimization hints → Sketch Generator → C/CUDA sketch with kernel calls
  3. Sketch + factors + few-shot examples → Kernel Generator → Python script → assembly/PTX kernel
  4. Sketch + kernel → compile and test → initial performance baseline
  5. Baseline → MCTS Tuner → parameter adjustments and instruction reordering → optimized implementation

- Design tradeoffs:
  - Python intermediate vs. direct assembly: Trading one-step generation complexity for two-step pipeline overhead; improves correctness at cost of more prompt engineering
  - LLM-guided vs. random MCTS expansion: Trading inference cost per expansion for reduced total search iterations; beneficial when LLM priors are accurate, harmful when they encode biases toward local optima
  - Single-line prompt vs. detailed specification: Trading user convenience for potential ambiguity; error recovery depends on LLM correctly interpreting underspecified requests

- Failure signatures:
  - Generated kernel fails to compile: Likely prompt/template mismatch for target hardware variant
  - Performance degrades during tuning: LLM expansion proposing actions that violate hidden hardware constraints (e.g., register pressure, cache alignment)
  - Huge variance across LLM runs: Insufficient prompt structure; add more few-shot examples or explicit constraints
  - MCTS exhausts budget without improvement: Search space poorly initialized; check sketch parameter ranges against hardware factors

- First 3 experiments:
  1. Reproduce a single result from Table 1 (e.g., 1024×1024×1024 GEMM on an available RISC-V or ARM CPU) to validate the full pipeline on your hardware and LLM access configuration
  2. Ablate the MCTS history component by running with history disabled; compare convergence speed and final performance against the full system to quantify the LLM guidance contribution
  3. Test a different tensor operator (e.g., Conv from Figure 3) on the same hardware to assess whether optimization hints and kernel generation generalize beyond GEMM without manual prompt adjustment

## Open Questions the Paper Calls Out

- Can the framework effectively generalize its generation and auto-tuning capabilities to a broader range of tensor operators beyond dense GEMM and Convolution?
- How robust is the hardware factor extraction process when documentation is incomplete, unstructured, or unavailable for niche or proprietary hardware?
- Can the system maintain high performance when deployed on smaller, local open-source models (e.g., 7B-13B parameters) rather than SOTA models like GPT-4o or Llama-3.1-405B?

## Limitations

- Performance claims depend on specific hardware manual parsing capabilities and optimization hints that may not generalize to novel architectures
- The single-line prompt claim is difficult to verify without access to the exact prompt engineering approach
- The 200× cost reduction versus human experts is based on development time estimates that may not account for debugging and iteration cycles

## Confidence

- **High Confidence**: The decomposition of tensor operator generation into sketch and kernel stages is well-motivated and aligns with established code generation patterns
- **Medium Confidence**: The hardware factor extraction mechanism assumes LLMs can reliably parse technical manuals, which may work for standardized documentation but could fail on poorly formatted or proprietary manuals
- **Low Confidence**: The LLM-assisted MCTS tuning mechanism's effectiveness is demonstrated only on tested benchmarks and may not scale to more complex operator variants

## Next Checks

1. Replicate the RISC-V GEMM performance claim (251% of OpenBLAS) on a different RISC-V system to verify hardware factor extraction and sketch generation robustness across implementations
2. Conduct an ablation study on the LLM-guided MCTS expansion by comparing against random expansion with identical search budget to quantify the semantic guidance contribution
3. Test the framework on a tensor operator variant not included in the original evaluation (e.g., depthwise convolution or attention mechanism) to assess generalization of optimization hints and kernel generation templates