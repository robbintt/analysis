---
ver: rpa2
title: 'WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification
  Under Distribution Shifts'
arxiv_id: '2505.19587'
source_url: https://arxiv.org/abs/2505.19587
tags:
- prediction
- shifts
- distribution
- coverage
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces two methods to address distribution shifts
  in conformal prediction (CP) for robust uncertainty quantification. The first method,
  RLSCP, scales prediction sets using reconstruction losses from a Variational Autoencoder
  (VAE) to handle distribution shifts.
---

# WQLCP: Weighted Adaptive Conformal Prediction for Robust Uncertainty Quantification Under Distribution Shifts

## Quick Facts
- arXiv ID: 2505.19587
- Source URL: https://arxiv.org/abs/2505.19587
- Reference count: 40
- Primary result: WQLCP achieves 0.7402 coverage with set size 9.501 on ImageNetA, outperforming APS baseline's 0.7023 coverage and 16.3921 set size

## Executive Summary
This paper addresses the critical challenge of distribution shifts in conformal prediction (CP) by introducing two novel methods: RLSCP and WQLCP. RLSCP scales prediction sets using VAE reconstruction losses as uncertainty metrics, while WQLCP refines this by incorporating weighted exchangeability based on calibration-test loss ratios. Experiments on ImageNet variants demonstrate WQLCP's ability to maintain coverage while significantly reducing prediction set sizes compared to existing baselines.

## Method Summary
WQLCP builds upon split conformal prediction by using VAE reconstruction losses to quantify uncertainty under distribution shifts. The method trains a β-VAE (β=1.2) jointly with the base classifier on in-distribution data, then uses calibration reconstruction losses to compute weights that reflect distribution alignment. At test time, these weights adjust the quantile threshold, while scores are scaled by reconstruction loss quantiles to recover coverage under shift.

## Key Results
- WQLCP achieves 0.7402 coverage with 9.501 set size on ImageNetA (vs APS: 0.7023 coverage, 16.3921 set size)
- On ImageNetV2, WQLCP reaches 0.9503 coverage at 1.80 set size (vs RLSCP: 0.9301 coverage, 2.20 set size)
- WQLCP reduces prediction set size by ~66% compared to RLSCP on shifted datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** VAE reconstruction loss correlates with distribution shift severity and serves as a proxy for epistemic uncertainty.
- **Mechanism:** Under distribution shift, the VAE decoder struggles to reconstruct out-of-distribution inputs, yielding elevated reconstruction losses. The paper uses $L_{rec}(x) = \|x - \hat{x}(\theta)\|^2$ as an uncertainty metric. Higher loss signals greater discrepancy between test and training distributions.
- **Core assumption:** Reconstruction loss reliably reflects shift severity rather than unrelated factors (e.g., inherent input complexity). Assumption: VAE latent space generalizes sufficiently to rank-shift severity.
- **Evidence anchors:** [abstract] "utilizes reconstruction losses derived from a Variational Autoencoder (VAE) as an uncertainty metric to scale score functions"; [Page 4, Figure 2] Empirical correlation between reconstruction loss and softmax mean of true label across ImageNet variants; [corpus] AdaptNC (arXiv:2602.01629) similarly uses nonconformity scores adapted to dynamic environments, supporting uncertainty-driven adaptation
- **Break condition:** If reconstruction loss is dominated by noise or data complexity rather than distributional discrepancy, the uncertainty signal degrades.

### Mechanism 2
- **Claim:** Scaling conformal scores by reconstruction loss quantiles ($RL_{test}$) adaptively enlarges prediction sets under shift, recovering coverage.
- **Mechanism:** RLSCP computes $RL_{test} = q_{1-\alpha}(L_{test})$ and scales scores via $s(x, y') \cdot \max(1, RL_{test})$. When $RL_{test} > 1$, the effective threshold for label inclusion relaxes, systematically enlarging prediction sets to counter under-coverage.
- **Core assumption:** Elevated reconstruction loss warrants larger prediction sets; the relationship is monotonic and approximately linear via the scaling factor.
- **Evidence anchors:** [Page 4, Eq. 5] Formal definition of scaled score criterion; [Page 7, Table 1] RLSCP achieves 0.9102 coverage on ImageNetV2 vs. 0.8789 for APS baseline; [corpus] Weak direct corpus support for reconstruction-loss scaling specifically; related methods use likelihood ratio reweighting (WCP) or self-supervised learning (SSCP)
- **Break condition:** If reconstruction loss scaling is too aggressive, prediction sets become excessively large, reducing practical utility (observed in Table 1: RLSCP set sizes 7.8–50.5 on shifted datasets).

### Mechanism 3
- **Claim:** Weighted quantile calibration based on calibration-to-test reconstruction loss ratios reduces prediction set size while maintaining coverage.
- **Mechanism:** WQLCP assigns weights $w(x_j) \propto \frac{L_{cal}(x_j)}{L_{test}(x_i) + \epsilon}$, prioritizing calibration samples with reconstruction losses similar to test samples. The weighted quantile $\hat{q}$ is computed via Eq. 8, refining the threshold without inflating sets.
- **Core assumption:** The calibration-test loss ratio approximates distribution alignment; higher weights for in-distribution-like calibration samples yield more efficient thresholds.
- **Evidence anchors:** [Page 5, Eq. 7–8] Formal weight definition and weighted quantile estimation; [Page 7, Table 1] WQLCP (ViT-VAE) achieves 0.9503 coverage at 1.80 set size on ImageNet vs. RLSCP's 0.9301 / 2.20; [corpus] Distribution-informed Online CP (arXiv:2512.07770) uses distribution-informed weighting for adaptive prediction, conceptually aligned
- **Break condition:** If calibration and test loss distributions diverge sharply (extreme adversarial shifts), weight estimation becomes unstable; observed in ImageNetA where coverage drops to 0.7402 (vs. 0.9503 on ImageNet).

## Foundational Learning

- **Concept: Conformal Prediction (SplitCP)**
  - **Why needed here:** WQLCP builds on SplitCP's calibration-threshold framework; understanding prediction set construction via score quantiles is prerequisite.
  - **Quick check question:** Given calibration scores $\{s_i\}_{i=1}^n$, how is the threshold $\tau_D$ computed for coverage $1-\alpha$?

- **Concept: Variational Autoencoders (VAEs) and ELBO**
  - **Why needed here:** Reconstruction loss derives from VAE's ELBO decomposition; distinguishing reconstruction term from KL regularization clarifies why $L_{rec}$ reflects epistemic uncertainty.
  - **Quick check question:** In the ELBO $= \mathbb{E}_{q(z|x)}[\log p(x|z)] - KL(q(z|x) \| p(z))$, which term measures reconstruction fidelity?

- **Concept: Exchangeability and Distribution Shift**
  - **Why needed here:** CP guarantees assume exchangeability; understanding its violation under covariate shift motivates weighted adaptive methods.
  - **Quick check question:** Why does i.i.d. test data violate exchangeability when the test distribution differs from calibration?

## Architecture Onboarding

- **Component map:** Base classifier -> VAE encoder/decoder -> Calibration set processor -> Weighted quantile estimator -> Score scaler -> Prediction set generator

- **Critical path:**
  1. Train β-VAE jointly with base classifier on in-distribution data (100 epochs, AdamW, lr=1e-4)
  2. Compute and store calibration losses and conformal scores
  3. At test time, compute $L_{test}$ for each input; compute $RL_{test}$ quantile
  4. Compute weights $w(x_j)$ and weighted quantile $\hat{q}$
  5. Scale test scores and generate prediction sets

- **Design tradeoffs:**
  - **ViT-VAE vs. RN-VAE:** ViT-VAE yields lower coverage drop under shift ($\Delta=0.21$ vs. $0.29$) but requires more compute
  - **Calibration set size:** 25k samples achieve 95% of max coverage with 2.4× faster calibration vs. 50k
  - **β value:** $\beta=1.2$ balances reconstruction fidelity vs. latent regularization (Fig. 4); higher β risks over-regularization under adversarial shift

- **Failure signatures:**
  - **Under-coverage on extreme shift:** 12.1% of ImageNetA samples with $L_{rec} > 3\sigma$ show coverage 0.42 vs. average 0.74 (linked to β-VAE over-regularization)
  - **Inflated sets for fine-grained classes:** 4.8% of predictions require $|C(x)| > 15$ (e.g., dog breeds at 18.2)
  - **Weight instability:** When $L_{test} \approx 0$ or highly variable, weights become noisy (mitigated by $\epsilon$ term)

- **First 3 experiments:**
  1. **Validate reconstruction loss–shift correlation:** Plot $L_{rec}$ vs. softmax score across ImageNet variants; verify monotonic relationship per Fig. 2
  2. **Ablate calibration size:** Compare coverage/set size at 10k, 25k, 50k calibration samples; confirm 25k efficiency claim
  3. **Compare RLSCP vs. WQLCP on ImageNetA:** Measure coverage/set size tradeoff; expect WQLCP to reduce set size by ~66% (28.51 → 9.50) with ~3.9% coverage drop per Table 1

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive $\beta$-VAE scheduling mitigate the under-coverage observed in extreme distribution shifts (e.g., ImageNet-A) caused by latent space rigidity?
- **Basis in paper:** [explicit] Section 4.6 identifies "under-coverage in extreme shifts" linked to "$\beta$-VAE over-regularization" and proposes "adaptive $\beta$-VAE scheduling" as a future direction.
- **Why unresolved:** The current implementation uses a fixed $\beta=1.2$ (determined via grid search), which leads to a coverage drop for samples with high reconstruction loss ($L_{rec} > 3\sigma$).
- **What evidence would resolve it:** Experiments comparing fixed vs. adaptive $\beta$ schedules on adversarial datasets (like ImageNet-A), demonstrating improved coverage for high-loss samples without inflating average set sizes.

### Open Question 2
- **Question:** Does hierarchical label grouping effectively reduce the inflated prediction set sizes observed in fine-grained classes without compromising coverage guarantees?
- **Basis in paper:** [explicit] Section 4.6 identifies "inflated set sizes in fine-grained classes" (e.g., dog breeds) as a failure mode and suggests "hierarchical label grouping" to address semantic ambiguity.
- **Why unresolved:** The current method treats all classes as independent, resulting in 4.8% of predictions requiring sets $>15$ labels, significantly exceeding the average size (9.50 on ImageNet-A).
- **What evidence would resolve it:** Implementation of a hierarchical WQLCP variant on fine-grained datasets showing a reduction in set size variance and average size for ambiguous classes while maintaining target coverage.

### Open Question 3
- **Question:** How can WQLCP be adapted for online, single-instance prediction scenarios where the global test-distribution statistics (required for scaling) are unavailable?
- **Basis in paper:** [inferred] The method (Eq. 4, Eq. 7) computes thresholds ($R_{Ltest}$) and weights ($w(x_j)$) by aggregating reconstruction losses over the *entire* test dataset $D_{test}$, requiring a batch setting despite the introduction mentioning "real-world data streams".
- **Why unresolved:** The algorithm (Steps 1-2) assumes access to the full set of test losses to calculate the necessary quantiles and ratios, preventing deployment in sequential or real-time environments where test data arrives one at a time.
- **What evidence would resolve it:** A modified algorithm utilizing a sliding window or recursive estimation for test statistics, validated on a continuously shifting time-series stream to show it maintains coverage without batch processing.

## Limitations
- Under-coverage on extreme adversarial shifts (ImageNetA) with 12.1% of samples showing coverage 0.42 vs. target 0.74
- VAE inference computational overhead during test-time prediction limits real-time applications
- Weight estimation instability when calibration and test loss distributions diverge sharply

## Confidence
- **High confidence:** Core mechanism of using reconstruction loss as uncertainty proxy and basic RLSCP scaling approach
- **Medium confidence:** WQLCP's weighted quantile refinement and VAE architecture choices
- **Low confidence:** Claims about real-time deployment and performance on completely unseen domains

## Next Checks
1. Conduct stress tests on adversarial distribution shifts by systematically corrupting ImageNet samples (Gaussian noise, occlusion, color shifts) and measuring coverage/set size tradeoffs
2. Validate the VAE latent space generalization by testing reconstruction loss correlation on out-of-domain datasets (e.g., CIFAR-10, Places365) to confirm it captures distributional discrepancy rather than complexity
3. Implement ablation studies on weight computation stability by varying the ε term and analyzing weight distribution sensitivity across different shift severities