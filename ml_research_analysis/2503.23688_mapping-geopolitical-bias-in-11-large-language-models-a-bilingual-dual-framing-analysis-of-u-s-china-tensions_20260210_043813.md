---
ver: rpa2
title: 'Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing
  Analysis of U.S.-China Tensions'
arxiv_id: '2503.23688'
source_url: https://arxiv.org/abs/2503.23688
tags:
- chinese
- china
- reverse
- across
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated geopolitical bias across 11
  Large Language Models using a bilingual (English/Chinese) and dual-framing methodology
  on seven U.S.-China topics. Models were tested with 19,712 prompts, scored on a
  normalized scale from -2 (strongly Pro-China) to +2 (strongly Pro-U.S.), and analyzed
  for stance consistency, refusal, and neutrality rates.
---

# Mapping Geopolitical Bias in 11 Large Language Models: A Bilingual, Dual-Framing Analysis of U.S.-China Tensions

## Quick Facts
- arXiv ID: 2503.23688
- Source URL: https://arxiv.org/abs/2503.23688
- Reference count: 14
- Primary result: U.S.-based LLMs consistently favored Pro-U.S. stances while Chinese-origin models exhibited strong Pro-China biases, especially in Chinese-language reverse prompts.

## Executive Summary
This study systematically evaluated geopolitical bias across 11 Large Language Models using a bilingual (English/Chinese) and dual-framing methodology on seven U.S.-China topics. Models were tested with 19,712 prompts, scored on a normalized scale from -2 (strongly Pro-China) to +2 (strongly Pro-U.S.), and analyzed for stance consistency, refusal, and neutrality rates. U.S.-based models consistently favored Pro-U.S. stances, while Chinese-origin models exhibited strong Pro-China biases, especially in Chinese-language reverse prompts. Framing and language significantly influenced responses, with several models reversing positions based on prompt structure. The findings reveal clear ideological alignments by model origin and underscore the importance of evaluating LLM behavior under varied linguistic and framing conditions in politically sensitive applications.

## Method Summary
The researchers evaluated 11 LLMs across 7 U.S.-China topics using 1,792 prompts per model (7 topics × 2 framings × 2 languages × 64 iterations). Prompts combined fixed templates with randomized prefix-suffix pairs to prevent memorization. Responses were classified into 6 stance categories using GPT-4o-mini, with manual verification for neutral/refusal cases. Scores were normalized to a -2 to +2 scale and aggregated by topic, language, and framing to measure bias patterns.

## Key Results
- U.S.-based models (GPT-4o, Claude-3.7-sonnet) consistently leaned Pro-U.S. with high effect sizes and low variability
- Chinese-origin models (Qwen, Yi) showed pronounced Pro-China biases, especially in Chinese-language reverse prompts
- Several models exhibited stance reversals based on prompt framing, with Chinese models showing the most dramatic shifts
- High refusal rates (100% for some models) on sensitive topics like Xinjiang and Taiwan highlight moderation vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit geopolitical bias correlated with their geographic origin through training data and alignment processes.
- Mechanism: Pre-training corpora and RLHF/safety fine-tuning embed culturally-specific ideological patterns that persist at inference time.
- Core assumption: Training data composition and fine-tuning reflect the values of the producing organization's regulatory and cultural environment.
- Evidence anchors:
  - [abstract] "U.S.-based models predominantly favored Pro-U.S. stances, while Chinese-origin models exhibited pronounced Pro-China biases"
  - [section p.17] "U.S. models like GPT-4o and Claude-3.7-sonnet consistently leaned Pro-U.S. across all topics"
  - [corpus] "Made-in China, Thinking in America" (arXiv:2512.13723) finds "U.S. Values Persist in Chinese LLMs"

### Mechanism 2
- Claim: Prompt framing (affirmative vs. reverse) drives larger response shifts than language alone.
- Mechanism: LLMs exhibit acquiescence or anchoring to prompt-embedded claims rather than independently evaluating semantic content.
- Core assumption: Models lack robust reasoning that abstracts stance from surface phrasing.
- Evidence anchors:
  - [abstract] "language and prompt framing substantially influenced model responses, with several LLMs exhibiting stance reversals based on prompt polarity or linguistic context"
  - [section p.6-7] "GPT-4o also flipped from neutrality (μ ≈ 0.02) in most prompts to strongly Pro-China under Chinese reverse (μ = −0.97, d = −3.88)"

### Mechanism 3
- Claim: Content moderation triggers are topic-language combination dependent, not purely semantic.
- Mechanism: Safety layers use heuristics that activate under specific linguistic + topical patterns.
- Core assumption: Moderation systems implement rule-based or classifier-based filters that are not semantically invariant across languages.
- Evidence anchors:
  - [abstract] "High refusal rates and inconsistent responses in sensitive topics like Xinjiang and Taiwan highlight vulnerabilities in model reliability"
  - [section p.6] "Qwen, also a consistent neutrality rate of Yi (Neu. = 16.4%), in Chinese prompts (Ref. = 100.0%)" on Xinjiang

## Foundational Learning

- **Dual-framing methodology (affirmative vs. reverse prompts)**
  - Why needed here: Single-prompt evaluation conflates model stance with acquiescence bias; reverse framing isolates genuine ideological preference.
  - Quick check question: If a model agrees with "X is justified" and also agrees with "X is unjustified," what does that reveal about its stance?

- **Bias scale normalization (-2 to +2)**
  - Why needed here: Raw agreement rates are not comparable across framing directions; normalization maps both prompt types to a common ideological axis.
  - Quick check question: Why must reverse-prompt agreement be inverted before aggregation?

- **Repeated randomized testing (64 iterations per condition)**
  - Why needed here: Single responses are noisy; statistical power requires sufficient samples to distinguish stable bias from random variation.
  - Quick check question: What does zero variance across 64 iterations (as seen in Qwen-max on tariffs) suggest about the model's behavior?

## Architecture Onboarding

- **Component map:**
  - Prompt Generator → OpenRouter API (model queries) → GPT-4o-mini (stance classifier) → Human review (edge cases) → Aggregation/Scoring → Visualization
  - Each prompt variant: 7 topics × 2 framings × 2 languages × 64 random prefix-suffix pairs = 1,792 prompts per model

- **Critical path:**
  1. Ensure randomization prevents memorization (unique prefix-suffix per iteration)
  2. Independent prompt submission (no conversational context carryover)
  3. Automated classification with manual verification on Neutral/Refusal categories

- **Design tradeoffs:**
  - Automated stance detection (scalable, consistent) vs. human annotation (more nuanced, costly)
  - Equal topic weighting in aggregate scores vs. sensitivity-weighted aggregation
  - Binary ideological axis (Pro-U.S./Pro-China) vs. multidimensional stance modeling

- **Failure signatures:**
  - Ceiling effects: Zero variance (e.g., Qwen-max μ = −1.00, σ = 0.00) indicates templated responses
  - Framing reversals: Same model scoring +0.5 in one condition, −0.5 in another signals unstable alignment
  - Refusal spikes: 100% refusal on specific topic-language combinations suggests hard-coded filters

- **First 3 experiments:**
  1. Replicate one topic with monolingual-only prompts to isolate framing vs. language effects
  2. Test a neutral topic (not in the 7) to establish baseline refusal/neutrality rates
  3. Compare stance classification accuracy: run GPT-4o-mini classifier vs. human annotator on 200-sample subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the observed geopolitical bias patterns persist in conflicts outside the U.S.-China dynamic?
- Basis in paper: [explicit] The authors identify "Topic Selection Scope" as a limitation and suggest future studies could "broaden this scope to assess whether similar alignment patterns hold in other geopolitical or domestic political contexts."
- Why unresolved: The current study intentionally restricted its scope to seven specific U.S.-China topics to ensure depth.
- What evidence would resolve it: Replicating the bilingual, dual-framing methodology on a new set of topics involving other geopolitical powers or regional conflicts.

### Open Question 2
- Question: What specific factors drive Mistral's distinct Pro-China alignment compared to other non-Chinese models?
- Basis in paper: [explicit] In the results for "U.S. Tech Restrictions," the authors note Mistral diverged from its U.S. and Chinese counterparts, stating "The underlying reasons for the general convergence among U.S. and Chinese models and Mistral's distinct behavior, remain open questions for further investigation."
- Why unresolved: The "black-box nature" of the models prevents the authors from determining if the cause is unique training data, architecture, or fine-tuning techniques.
- What evidence would resolve it: A comparative analysis of Mistral's training corpus and alignment protocols against the other U.S.-based models evaluated.

### Open Question 3
- Question: Are the high refusal rates and stance reversals in Chinese models caused by hard-coded safety filters or semantic misinterpretation?
- Basis in paper: [inferred] The authors observe "rigid outputs" and high refusal rates (e.g., Qwen at 100% for Xinjiang), suggesting this "reflects built-in safety filters... or inability to handle politically sensitive content."
- Why unresolved: The study relies on API outputs without access to internal model states or system prompts.
- What evidence would resolve it: White-box testing or ablation studies that isolate the specific layers or rules triggering refusals in sensitive linguistic contexts.

## Limitations

- High refusal rates on sensitive topics may reflect API-level filtering rather than model-level content moderation
- Automated stance classification may misclassify subtle ideological positions, particularly for nuanced responses
- Focus on seven specific U.S.-China topics limits generalizability to other geopolitical contexts

## Confidence

**High Confidence**: The correlation between model geographic origin and ideological alignment in English-language affirmative prompts (GPT-4o, Claude-3.7-sonnet consistently Pro-U.S.; Qwen, Yi consistently Pro-China).

**Medium Confidence**: The framing effect magnitude (stance reversals in Chinese models), as this depends on the automated classifier's sensitivity to prompt polarity cues.

**Low Confidence**: The refusal rate patterns as indicators of content moderation effectiveness, since API-level filtering and model-level moderation are not clearly distinguished.

## Next Checks

1. **Cross-linguistic semantic equivalence validation**: Manually translate a subset of affirmative prompts to Chinese and back to English to verify semantic content remains equivalent across languages.

2. **Independent stance annotation**: Recruit human annotators to classify 200 randomly selected responses (spanning all models, topics, and languages) using the same six-category scheme, then compute inter-annotator agreement.

3. **API-level filtering isolation**: Test the same prompts using local model deployments (where available) or alternative API endpoints to determine whether refusal spikes reflect model-level content policies or platform-level filtering mechanisms.