---
ver: rpa2
title: Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion
arxiv_id: '2601.03213'
source_url: https://arxiv.org/abs/2601.03213
tags:
- diffusion
- unlearning
- cgru
- critic
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents Critic-Guided Reinforcement Unlearning (CGRU),
  an RL-based framework for concept removal in diffusion models that addresses the
  challenge of sparse reward signals and high-variance updates common in prior approaches.
  CGRU treats denoising as a sequential decision process and introduces a timestep-aware
  critic that provides dense, informative signals throughout the denoising trajectory.
---

# Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion

## Quick Facts
- arXiv ID: 2601.03213
- Source URL: https://arxiv.org/abs/2601.03213
- Reference count: 40
- Primary result: CGRU achieves 95.55% unlearning accuracy and 78.47% retain accuracy on UnlearnCanvas benchmark while maintaining FID=98.43

## Executive Summary
The paper introduces Critic-Guided Reinforcement Unlearning (CGRU), an RL-based framework for removing targeted concepts from diffusion models. CGRU addresses the challenge of sparse reward signals and high-variance updates common in prior approaches by treating denoising as a sequential decision process and introducing a timestep-aware critic. The critic provides dense, informative signals throughout the denoising trajectory, enabling more effective credit assignment and policy optimization. CGRU integrates with standard text-to-image backbones without architectural changes and supports off-policy learning for sample efficiency.

## Method Summary
CGRU implements a two-phase training approach. First, a critic network V_ϕ(x_t,c,t) is trained to predict the expected terminal reward from noisy latents using FiLM layers with sinusoidal timestep embeddings. The critic minimizes MSE loss on shuffled timesteps from generated denoising trajectories. Second, policy optimization uses advantage-weighted gradients A_t = r(x_0,c) - V_ϕ(x_t,c,t) with importance sampling for off-policy reuse. The method employs LoRA for memory efficiency and operates on 50 denoising steps over 100 epochs with a learning rate of 3e-4.

## Key Results
- CGRU achieves 95.55% unlearning accuracy and 78.47% retain accuracy on UnlearnCanvas benchmark
- Outperforms strong baselines including DDPO (43.25% UA, 29.75% IRA) and specialized unlearning methods
- Maintains reasonable image quality with FID=98.43 compared to 81.51 for original Stable Diffusion 1.5
- Ablation studies confirm timestep-aware critic is critical for performance

## Why This Works (Mechanism)
CGRU addresses the fundamental challenge of sparse rewards in diffusion unlearning by converting the denoising process into a sequential decision-making problem. The timestep-aware critic provides dense feedback signals at each denoising step, enabling the policy to learn which modifications most effectively reduce the target concept's presence. By predicting expected terminal reward from intermediate latents, the critic creates a bridge between immediate denoising actions and final image quality, allowing for more effective credit assignment than reward-only approaches.

## Foundational Learning

**Diffusion denoising as sequential decision process**
Why needed: Traditional diffusion unlearning treats denoising as a black box, missing opportunities for intervention during the gradual refinement process
Quick check: Verify that each denoising step can be modeled as a discrete decision with measurable impact on concept presence

**Timestep-aware value estimation**
Why needed: Without timestep information, critics cannot distinguish between early (high-noise) and late (fine-detail) modifications, leading to poor credit assignment
Quick check: Compare performance of timestep-aware vs timestep-agnostic critics on simple unlearning tasks

**Advantage-weighted policy gradients**
Why needed: Raw rewards in unlearning are sparse and binary, making standard RL approaches ineffective
Quick check: Monitor advantage magnitudes during training to ensure meaningful signal propagation

## Architecture Onboarding

**Component map:**
Classifier -> Reward function -> Policy network (SD 1.5) -> Image generation
Policy network -> Critic network (value estimation) -> Advantage computation -> Policy update
Data generator -> Trajectory storage -> Critic training -> Value predictions

**Critical path:**
Classifier → Reward function → Critic training → Advantage computation → Policy optimization

**Design tradeoffs:**
- Uses LoRA instead of full fine-tuning for memory efficiency vs potential performance ceiling
- Employs importance sampling for off-policy learning vs potential weight instability
- Balances unlearning accuracy vs retain accuracy vs image quality degradation

**Failure signatures:**
- High gradient variance during policy updates indicates poor critic predictions
- Low retain accuracy suggests overly aggressive concept suppression
- Exploding importance weights cause training instability

**First experiments:**
1. Train critic alone on synthetic trajectories to verify it learns to predict terminal rewards
2. Test policy updates with critic guidance vs reward-only baseline on simple concept removal
3. Measure importance weight distributions during off-policy training to establish baseline stability

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: What are the fundamental causes of the inherent tension between unlearning accuracy (UA) and retain accuracy (IRA) in diffusion unlearning, and can this trade-off be fundamentally improved or only shifted?
- Basis in paper: "We believe investigating the fundamental causes of this tension is a substantial research direction that warrants a dedicated study, which we leave for future work."
- Why unresolved: The paper observes the UA-IRA trade-off across methods but does not investigate its mechanistic origins in diffusion model representations or the denoising process.
- What evidence would resolve it: Layer-wise or attention-head analyses showing whether forgotten and retained concepts share representational substrates; interventions that decouple the metrics.

**Open Question 2**
- Question: How robust is CGRU against adversarial prompts designed to re-elicit erased concepts, compared to parameter-editing unlearning methods?
- Basis in paper: "Future work should explore adaptive reward functions, investigate robustness against adversarial prompts, and extend the framework to other unlearning types."
- Why unresolved: The paper cites prior work showing erased concepts can be re-elicited via adversarial phrasing but does not evaluate CGRU's susceptibility to such attacks.
- What evidence would resolve it: Benchmarking CGRU against adversarial prompt suites and comparing evasion rates to ESD, UCE, and other baselines.

**Open Question 3**
- Question: Does CGRU generalize effectively to non-object unlearning tasks such as style removal, identity erasure, or unsafe content suppression?
- Basis in paper: "While our experiments demonstrate effectiveness on object removal tasks, the generalizability of CGRU to other types of unlearning remains to be investigated."
- Why unresolved: All reported experiments use 20 object classes from UnlearnCanvas; no results for styles, celebrities, or explicit content are provided.
- What evidence would resolve it: Applying CGRU to established style/identity unlearning benchmarks and reporting UA/IRA metrics.

## Limitations

- Performance trade-off: CGRU achieves better unlearning accuracy than baselines but with higher FID (98.43 vs 81.51), indicating image quality degradation
- Architectural ambiguities: Critical implementation details including critic network architecture, LoRA configuration, and dataset partitioning are not fully specified
- Limited scope: Experiments only demonstrate effectiveness on object removal tasks, leaving generalization to style, identity, and unsafe content unvalidated

## Confidence

**High confidence** in method's conceptual validity and quantitative performance claims
**Medium confidence** in exact architectural specifications and reproducibility
**Medium confidence** in generalizability beyond the UnlearnCanvas benchmark

## Next Checks

1. Implement critic network with FiLM layers and sinusoidal timestep embeddings, then validate through ablation study comparing with timestep-agnostic critic (expected: significant performance drop without timestep awareness)

2. Conduct importance weight analysis during policy training to ensure stability and verify that weight clipping prevents explosion while maintaining learning efficacy

3. Test robustness by training on subset of UnlearnCanvas classes (e.g., 5 classes instead of 20) and measuring degradation in unlearning/retain accuracy to assess scalability limits