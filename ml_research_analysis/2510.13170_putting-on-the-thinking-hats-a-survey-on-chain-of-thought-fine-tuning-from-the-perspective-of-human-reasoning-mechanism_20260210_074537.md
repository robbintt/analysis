---
ver: rpa2
title: 'Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from
  the Perspective of Human Reasoning Mechanism'
arxiv_id: '2510.13170'
source_url: https://arxiv.org/abs/2510.13170
tags:
- reasoning
- https
- online
- available
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey provides the first comprehensive review of Chain of
  Thought (CoT) fine-tuning through the lens of human reasoning theory. It systematically
  categorizes CoT fine-tuning methods using the Six Thinking Hats framework, which
  represents six key human reasoning abilities: planning, divergent thinking, intuitive
  judgment, reflection, internal thinking, and factual perception.'
---

# Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism

## Quick Facts
- arXiv ID: 2510.13170
- Source URL: https://arxiv.org/abs/2510.13170
- Reference count: 40
- Key outcome: First comprehensive review of Chain of Thought fine-tuning using Six Thinking Hats framework to categorize techniques and identify future research directions

## Executive Summary
This survey provides the first comprehensive review of Chain of Thought (CoT) fine-tuning through the lens of human reasoning theory. It systematically categorizes CoT fine-tuning methods using the Six Thinking Hats framework, which represents six key human reasoning abilities: planning, divergent thinking, intuitive judgment, reflection, internal thinking, and factual perception. The survey organizes CoT fine-tuning techniques into two levels: top-level (thinking hats) and base-level (techniques), and analyzes their development trajectories. It identifies key challenges and future research directions while providing a curated overview of datasets and model performances. A continuously updated GitHub repository is maintained to track recent advances in this field.

## Method Summary
The survey systematically reviews Chain of Thought fine-tuning methods by first establishing the CoT fine-tuning pipeline (data acquisition → SFT → RFT), then categorizing techniques using the Six Thinking Hats framework. It analyzes each hat's contribution to reasoning: Blue Hat (planning), Green Hat (diversity), Red Hat (intuition/preference), Black Hat (reflection), Yellow Hat (efficiency/internalization), and White Hat (fact perception). The analysis covers development trajectories from basic SFT methods to advanced RFT techniques, examining both theoretical foundations and practical implementations across various benchmarks.

## Key Results
- CoT fine-tuning enables LLMs to learn intermediate reasoning steps rather than direct input-output mappings
- The Six Thinking Hats framework provides systematic organization of CoT techniques across six reasoning dimensions
- Development progresses from Reflex Model (direct answers) → Thinking Model (single-path CoT) → Insight Model (multi-faceted reasoning)
- Key challenges include unfaithfulness, reward hacking, and balancing conflicting reasoning traits
- Future directions focus on meta-planning, genuine diversity, and dynamic trait balancing

## Why This Works (Mechanism)

### Mechanism 1: CoT Fine-tuning as Structured Reasoning Supervision
- Claim: Explicit chain-of-thought supervision during training can guide LLMs to learn intermediate reasoning steps rather than direct input-output mappings, improving logical clarity and generalization on complex tasks.
- Mechanism: CoT fine-tuning optimizes model parameters using training data annotated with intermediate reasoning steps. The objective (Eq. 2) trains the model to generate reasoning chains $c$ concatenated with final answers $y$, encouraging the model to "think" before answering.
- Core assumption: The reasoning capabilities can be learned from data containing explicit intermediate steps, and this learned capability generalizes to new problems.
- Evidence anchors:
  - [abstract] "Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs) with reasoning capabilities by training them on curated reasoning traces."
  - [section III] "CoT fine-tuning is a learning paradigm that directly optimizes the model's parameters using training data annotated with intermediate reasoning steps."
  - [corpus] Related work supports that CoT-based methods improve reasoning performance, though theoretical gaps remain (CoT-Space paper notes limitations in scalability of intermediate steps).
- Break condition: If training data contains noisy, incorrect, or unfaithful reasoning chains, the model may learn flawed reasoning patterns (unfaithfulness problem in Section III-A). Also, if tasks require reasoning that cannot be decomposed into verbalizable steps, explicit CoT supervision may be insufficient.

### Mechanism 2: Six Thinking Hats as a Cognitive Decomposition Framework
- Claim: Human reasoning abilities can be decomposed into six complementary thinking modes (planning, divergent thinking, intuition, reflection, internal thinking, factual perception), and CoT fine-tuning methods can be systematically organized and developed along these dimensions.
- Mechanism: The framework provides a taxonomy to categorize CoT techniques: Blue Hat (planning), Green Hat (diversity), Red Hat (intuition/preference), Black Hat (reflection), Yellow Hat (efficiency/internalization), White Hat (fact perception). This enables targeted enhancement of specific reasoning capabilities.
- Core assumption: Human reasoning modes as described by the Six Thinking Hats are relevant and applicable to LLM reasoning, and techniques developed for one mode can be integrated or combined with others.
- Evidence anchors:
  - [abstract] "we classify and examine CoT fine-tuning methods through this lens [Six Thinking Hats]."
  - [section IV] Detailed categorization of techniques under each hat, e.g., Blue Hat methods include self-planning and external planning.
  - [corpus] Weak or missing direct evidence; neighboring papers focus on specific aspects (e.g., meta-thinking via MARL) but not the holistic Six Hats decomposition.
- Break condition: If a task primarily requires a thinking mode not well-covered by existing techniques (e.g., strong ethical judgment not explicitly addressed), or if different hats' objectives conflict (e.g., Green Hat creativity vs. Blue Hat rigid planning), the framework's guidance may be insufficient.

### Mechanism 3: Progressive Model Evolution from Reflex to Insight via Fine-tuning
- Claim: LLMs' reasoning capabilities evolve through stages: Reflex Model (direct answers), Thinking Model (single-path CoT), and Insight Model (multi-faceted, human-like reasoning), with CoT fine-tuning as the primary driver.
- Mechanism: SFT establishes basic CoT generation (Thinking Model). RFT (using PPO-like, GRPO-like, DPO-like methods) strengthens reasoning. Advanced techniques under Six Hats enable the Insight Model, incorporating planning, diversity, reflection, etc.
- Core assumption: This staged progression is both descriptive of model development and prescriptive for improving reasoning; later stages subsume and extend earlier ones.
- Evidence anchors:
  - [section I] "the development of CoT fine-tuning in LLMs can be broadly categorized into two stages: the Think Model and the Insight Model stages."
  - [figure 1] Visual evolution from Reflex to Thinking to Insight models.
  - [corpus] Related surveys discuss long CoT and reasoning models (e.g., "Towards Reasoning Era" survey) but don't explicitly frame it as this three-stage progression.
- Break condition: If a model skips SFT and is trained directly with RFT, or if techniques from the Insight Model (e.g., reflection) are applied to a model without foundational CoT ability, performance may degrade. The evolution path may not hold for all model architectures or tasks.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Fine-tuning Basics
  - Why needed here: Central technique the entire survey analyzes; distinguishes from prompt-based CoT.
  - Quick check question: How does CoT fine-tuning differ from CoT prompting in terms of model optimization?

- Concept: Reinforcement Learning from Human Feedback (RLHF) and Variants
  - Why needed here: RFT methods (PPO, GRPO, DPO, STaR-like) are core to advancing beyond SFT, as explained in Section III-B.
  - Quick check question: What is the key difference between PPO-like and DPO-like RFT in terms of required models during training?

- Concept: The Six Thinking Hats Framework (de Bono)
  - Why needed here: The survey's entire taxonomy is based on this human reasoning framework.
  - Quick check question: Which thinking hat is most associated with generating multiple solution paths, and what is its formal notation in CoT (Eq. 4)?

## Architecture Onboarding

- Component map:
  - Data Acquisition (Manual/Automatic CoT) → SFT (Pre-thinking/Post-thinking/Multi-task) → RFT (Reward Modeling + Policy Optimization) → Insight Model Modules (Six Hats)

- Critical path:
  1. **Start with a pre-trained LLM** (e.g., Llama, Qwen, Mistral).
  2. **Acquire CoT data** (manual annotation or automatic generation via LLMs/rules).
  3. **Perform SFT** using pre-thinking paradigm for basic reasoning ability (Thinking Model).
  4. **Implement RFT** (choose method based on resources: PPO (high), GRPO (medium), DPO (low)) using reward models (ORM/PRM).
  5. **Select Insight Model capabilities** based on task: Identify which hats are most relevant (Fig. 11) and integrate corresponding techniques (e.g., add a reflection loop for medical QA).
  6. **Evaluate** on appropriate benchmarks (Table III).

- Design tradeoffs:
  - **SFT Paradigm**: Pre-thinking offers problem decomposition but risks error accumulation; Post-thinking is faster but lacks decomposition.
  - **RFT Method**: PPO-like requires most resources (4 models) but is most established; DPO-like needs fewest (2 models) but relies heavily on preference data quality.
  - **Reward Modeling**: PRM provides finer-grained feedback but is costlier and more susceptible to reward hacking; ORM is simpler and more stable for large-scale training.
  - **Hat Integration**: Integrating multiple hats (e.g., planning + reflection) increases reasoning robustness but also computational overhead and potential for conflicting objectives.

- Failure signatures:
  - **SFT**: Unfaithful CoT (conclusion contradicts reasoning), error accumulation in long chains.
  - **RFT**: Reward hacking (model exploits reward model flaws), entropy collapse, instability in PPO.
  - **Green Hat (Diversity)**: Generating only superficially different reasoning paths, not methodologically diverse ones.
  - **Black Hat (Reflection)**: Over-reflection on simple tasks, consuming resources without benefit.
  - **Yellow Hat (Efficiency)**: Misrouting between fast/slow thinking leading to performance drops; semantic loss in latent thinking.
  - **General**: Poor generalization to new tasks (planning hat methods are often task-specific).

- First 3 experiments:
  1. **Baseline SFT**: Take a 7B parameter model (e.g., Llama-3-8B), acquire CoT data for math (GSM8K), and fine-tune using pre-thinking SFT. Evaluate on GSM8K and MATH to establish a Thinking Model baseline.
  2. **RFT Comparison**: Using the SFT model from #1, implement GRPO-like RFT with an ORM (rule-based for math). Compare performance and training stability against a DPO-like baseline using the same data.
  3. **Single Hat Enhancement**: For a task requiring planning (e.g., HotpotQA multi-hop reasoning), integrate a Blue Hat technique (e.g., global planning via pseudocode as in CodePlan) into your model. Compare against the baseline from #1 to measure improvement in planning-dependent tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can LLMs be trained to acquire abstract, task-independent "meta-planning" capabilities rather than task-specific planning skills?
- **Basis in paper:** [explicit] Section VI-A notes that current planning research is task-dependent, failing to establish general strategies for selecting decomposition methods or correcting trajectories in unknown environments.
- **Why unresolved:** Existing studies focus on "how to plan for a specific task," requiring retraining for new domains.
- **What evidence would resolve it:** A model demonstrating robust planning performance across unseen domains without task-specific fine-tuning.

### Open Question 2
- **Question:** How can reasoning diversity be shifted from surface-level expression variations to fundamental methodological differences (e.g., inductive vs. deductive reasoning)?
- **Basis in paper:** [explicit] Section VI-B observes that current "diverse thinking" relies on surface-level token variations (paraphrasing) rather than distinct solution approaches.
- **Why unresolved:** Models currently lack the intrinsic ability to autonomously select or generate structurally different reasoning strategies for the same problem.
- **What evidence would resolve it:** A framework where the model autonomously employs distinct logical frameworks (e.g., abduction vs. deduction) based on problem constraints.

### Open Question 3
- **Question:** How can models be designed to dynamically balance the conflicting requirements of different "hats," such as the rigor of the Black Hat (reflection) versus the efficiency of the Yellow Hat (optimism)?
- **Basis in paper:** [explicit] Section VI-G ("Clash of Caps") highlights that reasoning traits like systematic planning and creative exploration often conflict, and existing methods struggle to coordinate them optimally.
- **Why unresolved:** Current systems lack effective mechanisms to switch between or integrate these conflicting modes dynamically during reasoning.
- **What evidence would resolve it:** An architecture capable of resolving these trade-offs to maintain high accuracy while minimizing unnecessary computational overhead.

### Open Question 4
- **Question:** How can reflective mechanisms be refined to distinguish "necessary reflection" from "redundant reflection" to prevent overthinking?
- **Basis in paper:** [explicit] Section VI-D identifies "over-reflection" as a key challenge, where models perform comprehensive checks even on simple or deterministic tasks.
- **Why unresolved:** Models lack the judgment to detect high-certainty steps where self-correction is unnecessary.
- **What evidence would resolve it:** A model that selectively triggers reflection steps based on confidence thresholds, reducing latency without accuracy loss.

## Limitations

- The Six Thinking Hats framework, while comprehensive, may not fully capture all dimensions of LLM reasoning, potentially limiting its predictive power for novel reasoning techniques.
- The extent to which fine-tuned reasoning capabilities generalize to truly novel, real-world problems remains uncertain, as benchmarks used are curated datasets.
- Integrating multiple techniques (e.g., planning + reflection + diversity) may lead to conflicting objectives or emergent behaviors not captured in individual technique analyses.

## Confidence

**High Confidence**:
- The basic mechanism of CoT fine-tuning (training on intermediate reasoning steps) is well-established and supported by empirical evidence.
- The categorization of techniques into SFT vs. RFT and the progression from Reflex to Thinking to Insight Models accurately describes the field's development trajectory.

**Medium Confidence**:
- The Six Thinking Hats framework provides a useful taxonomy for organizing existing CoT fine-tuning methods, though its completeness for capturing all LLM reasoning capabilities is uncertain.
- The identified challenges (unfaithfulness, reward hacking, etc.) are real concerns based on literature, but their relative importance and prevalence across different tasks and models may vary.

**Low Confidence**:
- Specific performance claims for individual techniques and their combinations are largely absent from the survey, requiring readers to consult the original papers.
- The survey's analysis of future directions is speculative, as the field is rapidly evolving and unforeseen challenges or breakthroughs may emerge.

## Next Checks

1. **Framework Validation**: Apply the Six Thinking Hats framework to a newly proposed CoT technique not covered in the survey. Evaluate whether the framework can meaningfully categorize and analyze this technique, and identify any reasoning dimensions it fails to capture.

2. **Integration Experiment**: Select three distinct techniques from different hats (e.g., Blue Hat planning, Green Hat diversity, Black Hat reflection) and implement their integration into a single model. Systematically evaluate whether their combination provides additive benefits or leads to conflicts/confusion in reasoning outputs.

3. **Generalization Study**: Fine-tune a model using CoT techniques from the survey on a standard benchmark (e.g., GSM8K). Then evaluate its performance on a qualitatively different reasoning task (e.g., legal reasoning, medical diagnosis) that requires similar but not identical reasoning capabilities. Quantify the degree of knowledge transfer and identify any failure patterns.