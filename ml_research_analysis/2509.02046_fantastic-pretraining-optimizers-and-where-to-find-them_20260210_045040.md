---
ver: rpa2
title: Fantastic Pretraining Optimizers and Where to Find Them
arxiv_id: '2509.02046'
source_url: https://arxiv.org/abs/2509.02046
tags:
- loss
- chinchilla
- hyperparameter
- data
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper systematically benchmarks 11 optimizers for LLM pretraining\
  \ across 4 model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8\xD7\
  \ Chinchilla optimum). The authors identify two critical methodological shortcomings\
  \ in prior work: insufficient hyperparameter tuning and limited evaluation setups."
---

# Fantastic Pretraining Optimizers and Where to Find Them

## Quick Facts
- **arXiv ID**: 2509.02046
- **Source URL**: https://arxiv.org/abs/2509.02046
- **Authors**: Kaiyue Wen; David Hall; Tengyu Ma; Percy Liang
- **Reference count**: 40
- **Key outcome**: Systematic benchmarking reveals claimed optimizer speedups are largely artifacts of poor hyperparameter tuning, with actual matrix optimizer advantages shrinking from 1.4× to 1.1× as model scale increases

## Executive Summary
This paper presents a comprehensive benchmark of 11 optimizers for large language model pretraining, identifying critical methodological flaws in prior work. Through rigorous hyperparameter tuning and evaluation across multiple model scales (0.1B-1.2B parameters) and data-to-model ratios, the authors demonstrate that many claimed speedups over AdamW are artifacts of insufficient tuning rather than genuine optimizer superiority. The study reveals that matrix-based optimizers like Muon, Soap, and Kron consistently outperform scalar-based alternatives for small models, but this advantage diminishes with scale. The optimal optimizer choice also shifts with data-to-model ratios, with Muon being overtaken by Soap and Kron at high ratios. Early-stage loss curves can be misleading as optimizer rankings change during training due to learning rate decay.

## Method Summary
The authors conduct systematic benchmarks across four model scales (0.1B to 1.2B parameters) and four data-to-model ratios (1× to 8× Chinchilla optimum), testing 11 optimizers including AdamW, LAMB, Muon, Soap, and Kron. Each optimizer undergoes coordinate-descent hyperparameter sweeps with three random seeds, using 2K training steps per configuration. The evaluation employs a fixed pretraining recipe with a single learning rate schedule and identical weight initialization across all runs. To measure robustness, they introduce an adversarial hyperparameter setup with large learning rates and weight decay, then rank optimizers by performance in both ideal and adversarial settings. The study uses masked autoregressive language modeling on datasets ranging from 70B to 560B tokens, with 1.1-4.4 billion training tokens per run depending on the data ratio.

## Key Results
- Many claimed 1.4-2× speedups over AdamW are artifacts of weak baselines; actual matrix optimizer advantages shrink from 1.4× for 0.1B models to 1.1× for 1.2B models
- Matrix-based optimizers (Muon, Soap, Kron) consistently outperform scalar-based ones for small models but lose this advantage at larger scales
- Optimizer rankings shift with data-to-model ratios; Muon is overtaken by Soap and Kron at high ratios (8× Chinchilla optimum)
- Early-stage loss curves can be misleading as optimizer rankings change during training due to learning rate decay

## Why This Works (Mechanism)
The diminishing advantage of matrix-based optimizers with increasing model scale likely stems from the increased complexity of optimization landscapes in larger models. As parameter counts grow, the benefits of adaptive per-parameter scaling that matrix-based methods provide become less pronounced relative to the computational overhead they introduce. The shift in optimal optimizer choice with data-to-model ratios suggests that different optimizers have varying sensitivities to data scarcity or abundance, with some methods better handling the trade-off between exploration and exploitation in different data regimes.

## Foundational Learning

**Coordinate Descent Optimization**: Iterative hyperparameter optimization where each parameter is tuned while holding others fixed. Needed to systematically explore high-dimensional hyperparameter spaces without combinatorial explosion. Quick check: Verify that parameter updates follow monotonic improvement pattern across iterations.

**Data-to-Model Ratio Scaling**: The relationship between training tokens and model parameters, where optimal performance requires sufficient data relative to model size. Needed to understand how pretraining efficiency varies with dataset size. Quick check: Confirm that performance degrades when operating below Chinchilla-optimal ratios.

**Adversarial Hyperparameter Testing**: Evaluating optimizer robustness by deliberately choosing extreme hyperparameter values. Needed to assess real-world reliability beyond ideal conditions. Quick check: Compare performance variance between standard and adversarial configurations.

## Architecture Onboarding

**Component Map**: Data Loader -> Model Architecture -> Optimizer Core -> Parameter Update -> Loss Computation -> Backpropagation -> Gradient Accumulation

**Critical Path**: Token Sequence Input -> Embedding Layer -> Transformer Blocks -> Output Projection -> Cross-Entropy Loss -> Gradient Computation -> Optimizer Update

**Design Tradeoffs**: Matrix-based optimizers offer per-parameter adaptation but incur computational overhead that grows with model size; scalar-based methods are computationally efficient but may struggle with heterogeneous parameter sensitivities across different layers and attention heads.

**Failure Signatures**: Optimizer divergence manifests as exploding gradients or NaN values, typically triggered by excessive learning rates or insufficient weight decay; poor convergence appears as plateaued loss curves despite continued training.

**3 First Experiments**:
1. Run identical 2K step sweeps with AdamW baseline to establish reference performance
2. Compare matrix vs scalar optimizer performance on smallest (0.1B) model scale
3. Test single data-to-model ratio (4× Chinchilla) to isolate optimizer effects from data scaling

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Study restricted to models up to 1.2B parameters, leaving uncertainty about findings at industrial scales
- Exclusive focus on LAMB- and AdamW-style optimizers excludes adaptive methods like Lion or AdaHessian
- Hyperparameter tuning methodology may miss global optima due to bounded search spaces and coordinate-descent approach
- Fixed pretraining recipe and data distribution may not capture real-world curriculum learning and data mixing scenarios

## Confidence

**High Confidence**: The systematic hyperparameter tuning methodology and multiple data-to-model ratio evaluations provide strong evidence for claims about overestimated speedups and diminishing matrix optimizer advantages.

**Medium Confidence**: Specific optimizer rankings and relative performance metrics are well-supported within tested parameter range but may not generalize to larger models.

**Low Confidence**: Implications for industrial-scale pretraining (hundreds of billions of parameters) are speculative given the study's explicit avoidance of this regime.

## Next Checks

1. Extend experimental framework to models beyond 1.2B parameters (e.g., 10B+ scale) to verify whether diminishing matrix optimizer advantages continue and identify the scale threshold where AdamW matches or exceeds their performance.

2. Evaluate additional optimizer families (Lion, AdaHessian, Adafactor) under identical hyperparameter tuning protocols to determine whether relative performance patterns hold across different optimization paradigms.

3. Test robustness of findings across different data distributions and curriculum strategies, including mixed-dataset pretraining scenarios and varying token composition ratios to assess whether optimizer rankings remain stable under realistic training conditions.