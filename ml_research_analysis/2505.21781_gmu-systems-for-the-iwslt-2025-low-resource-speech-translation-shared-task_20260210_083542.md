---
ver: rpa2
title: GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task
arxiv_id: '2505.21781'
source_url: https://arxiv.org/abs/2505.21781
tags:
- speech
- fine-tuning
- language
- text
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper describes GMU''s participation in the IWSLT 2025 low-resource
  speech translation shared task. They fine-tuned the SeamlessM4T-v2 model for all
  language pairs except Levantine Arabic, using four different fine-tuning strategies:
  end-to-end ST, cascaded ST, multi-task training, and initialization with pre-trained
  ASR/MT components.'
---

# GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task

## Quick Facts
- arXiv ID: 2505.21781
- Source URL: https://arxiv.org/abs/2505.21781
- Reference count: 20
- Primary result: GMU fine-tuned SeamlessM4T-v2 using four strategies (E2E ST, cascaded ST, multi-task, ASR-init) for 9 low-resource language pairs, achieving best performance in 6 languages with direct E2E fine-tuning

## Executive Summary
This paper presents GMU's submission to the IWSLT 2025 low-resource speech translation shared task, fine-tuning the pre-trained SeamlessM4T-v2 model for nine language pairs. The authors evaluate four fine-tuning strategies: end-to-end speech translation (E2E ST), cascaded speech recognition followed by machine translation (ASR→MT), multi-task training combining all three tasks with knowledge distillation, and initialization with pre-trained ASR encoders for languages not seen during SeamlessM4T-v2 pre-training. Results show direct E2E fine-tuning yields strong performance across most languages, with multi-task training providing modest improvements when MT significantly outperforms ST. For languages absent from SeamlessM4T-v2's pre-training, initializing the speech encoder with ASR-fine-tuned weights substantially improves ST performance.

## Method Summary
The paper fine-tunes the SeamlessM4T-v2-Large model (2B parameters) for nine low-resource language pairs using four distinct strategies. For end-to-end speech translation, they optimize the standard cross-entropy loss on translation output. Cascaded systems first transcribe speech to text using ASR fine-tuning, then translate using MT fine-tuning, combining outputs at inference. Multi-task training jointly optimizes all three tasks with weighted losses, incorporating knowledge distillation from the MT model to the ST model. For languages not supported by SeamlessM4T-v2's pre-training (bem, bho, fon, aeb, que), they first fine-tune ASR on in-domain data, then initialize the ST model's speech encoder with these ASR weights before ST fine-tuning. All models use AdamW optimization with 1e-4 learning rate (6e-5 for ASR/MT-initialized models), batch sizes of 120 for speech and 256 for text, and label smoothing of 0.2.

## Key Results
- Direct E2E fine-tuning achieved the best performance in 6 out of 9 languages, demonstrating the effectiveness of fine-tuning pre-trained models for low-resource ST
- Initializing speech encoders with ASR-fine-tuned weights improved performance by +3-5 BLEU for languages not seen during SeamlessM4T-v2 pre-training (aeb, bho)
- Multi-task training provided modest gains (2 BLEU for aeb, 0.7 for que) when MT models significantly outperformed baseline E2E ST models
- The cascaded system was competitive but generally underperformed end-to-end fine-tuning, with some exceptions (est-spa cascaded achieved +0.1 BLEU over E2E)

## Why This Works (Mechanism)

### Mechanism 1: ASR Encoder Initialization for Unseen Languages
Initializing the speech translation encoder with weights from an ASR-fine-tuned model improves ST performance for languages absent from SeamlessM4T-v2's pre-training. The acoustic-semantic mappings learned during ASR training transfer positively to the speech-to-translation task without interference from different output objectives. For languages already covered by SeamlessM4T-v2 pre-training (gle, est, mlt, mar), ASR encoder initialization showed negligible or slightly negative effects (<1 BLEU change).

### Mechanism 2: Multi-Task Training with Knowledge Distillation
Multi-task training combining ST, MT, and knowledge distillation objectives yields modest improvements specifically when the MT teacher substantially outperforms the baseline E2E ST model. The MT model serves as a teacher via KL-divergence loss, transferring translation knowledge to the ST student. When MT performance is not substantially better than ST (e.g., bem where ST surpassed MT by ~2 BLEU), multi-task training provides little benefit.

### Mechanism 3: Direct E2E Fine-Tuning for Pre-Trained Languages
Direct end-to-end fine-tuning of SeamlessM4T-v2 produces competitively strong results without auxiliary training stages, particularly for languages with pre-training coverage. SeamlessM4T-v2's large-scale pre-training (4.5M hours speech, massive multilingual text) already establishes robust cross-modal representations that can be efficiently refined for new ST tasks.

## Foundational Learning

- **Encoder-Decoder Speech Translation Architecture**: The paper's methodology centers on fine-tuning SeamlessM4T-v2, which uses a speech encoder (processes audio to hidden states) and text decoder (generates translation tokens). Understanding this separation is essential for grasping why ASR encoder initialization works—the encoder's acoustic representations directly impact decoder output quality.
  - *Quick check*: If you freeze the speech encoder and only fine-tune the text decoder for ST, would you expect this to work for a language not in the pre-training set? Why or why not?

- **Knowledge Distillation in Sequence Models**: The multi-task training mechanism relies on KL-divergence between teacher (MT) and student (ST) probability distributions. Understanding that the teacher provides soft targets (probability distributions over vocabulary) rather than hard labels explains why this can transfer richer information than cross-entropy alone.
  - *Quick check*: Why does the paper use stop-gradient on the teacher's output? What would happen to the MT model if gradients flowed back through it during distillation?

- **Transfer Learning and Parameter Initialization**: The paper's key innovation for unseen languages is initializing ST from ASR-fine-tuned weights. This requires understanding that neural network parameters encode learned representations, and that initializing from a related task can provide a better starting point than random or generic pre-trained weights when target-domain data is scarce.
  - *Quick check*: The paper initializes the speech encoder but finds text decoder initialization less helpful. What does this suggest about where the bottleneck lies for low-resource languages new to the model?

## Architecture Onboarding

- **Component map**:
  ```
  SeamlessM4T-v2 (2B parameters total)
  ├── Speech Encoder (w2v-BERT 2.0 pre-trained, 4.5M hrs)
  │   └── Input: 16kHz audio → Output: Hidden states
  ├── Text Encoder (NLLB-initialized)
  │   └── Input: Source text → Output: Hidden states
  └── Shared Text Decoder (NLLB-initialized)
      └── Input: Encoder states + [</s>, <lang>] → Output: Translation tokens
  ```

- **Critical path**:
  1. Determine language support: Check if SeamlessM4T-v2 supports your source language for ASR (supports: est, gle, mar, mlt; does not support: aeb, bem, bho, fon, que)
  2. If NOT supported → ASR pre-training first: Fine-tune on ASR data, save encoder weights (θASR_se)
  3. Initialize ST model: Load pre-trained SeamlessM4T-v2, optionally replace speech encoder with θASR_se
  4. Fine-tune with appropriate strategy: Direct E2E (supported languages), ASR-init (unsupported languages), or multi-task (if MT >> ST baseline)
  5. Codebase choice matters: Official codebase outperformed HuggingFace due to differences in loss computation on language codes, embedding tying, and dropout configuration

- **Design tradeoffs**:
  - Direct E2E vs. ASR-init: Simpler pipeline vs. better performance on unsupported languages (+3-5 BLEU for aeb, bho)
  - Cascaded vs. E2E: Cascaded competitive for some pairs (est: cascaded +0.1 BLEU over E2E), but generally underperforms and adds inference latency
  - Multi-task overhead: Requires 3-way data (speech, transcription, translation) and careful loss weighting (α=1, β=1, γ=2); only worthwhile when MT >> ST gap is large
  - Official vs. HuggingFace codebase: Official stronger but less flexible; HF enables all training paradigms but requires fixing discrepancies (untied lm_head, dropout differences)

- **Failure signatures**:
  - Near-zero BLEU on zero-shot: Language not in pre-training (expected); requires ASR-init + more data
  - ASR-init hurts performance: Language already supported; skip initialization, use direct E2E
  - Multi-task shows no gain: MT not sufficiently stronger than ST; verify MT baseline first
  - Synthetic data degrades performance: Observed for gle with 200hr synthetic data; quality matters more than quantity
  - Cascaded underperforms E2E: ASR errors propagating; check ASR WER first (high for aeb: 41% WER)

- **First 3 experiments**:
  1. Establish baselines: Run zero-shot SeamlessM4T-v2 evaluation on your language pair; separately fine-tune and evaluate ASR (CER/WER) and MT (BLEU) to understand component quality and determine if multi-task is viable.
  2. Compare E2E strategies: For one unsupported language, compare (a) direct E2E fine-tuning vs. (b) ASR-encoder-initialized E2E fine-tuning. Measure BLEU delta to confirm the ~3-5 BLEU improvement pattern holds for your language.
  3. Ablate initialization points: Test speech encoder init only vs. speech encoder + text decoder init. The paper shows decoder init is rarely helpful for high-resource targets—verify this doesn't hurt your specific pair before skipping it.

## Open Questions the Paper Calls Out

- **Speech-Encoder Decoder Alignment**: Can specialized pre-training objectives effectively bridge the modality gap between the speech encoder and text decoder in SeamlessM4T-v2? The conclusion suggests exploring better pretraining methods to mitigate this gap as future work.

- **Speech LLM Alternatives**: Do speech-based Large Language Models (LLMs) offer advantages over SeamlessM4T-v2 for low-resource speech translation? The paper explicitly lists exploring the use of speech large language models as a future direction.

- **Synthetic Data Quality**: What specific characteristics of synthetic data cause performance degradation in Irish (gle) end-to-end fine-tuning? The authors report a 1 BLEU drop when adding synthetic 3-way ST data for Irish, contrary to gains seen in Quechua.

## Limitations

- The paper reveals significant performance differences between the official SeamlessM4T codebase and HuggingFace implementation (up to 2.3 BLEU difference for bem-spa), suggesting implementation details substantially impact results
- The reported performance improvements are based on specific low-resource language pairs, and generalization to other language families or different domain content remains untested
- Dataset split details remain unspecified, creating ambiguity in reproducing exact performance metrics, particularly for languages with limited official data

## Confidence

- **High Confidence**: The core finding that direct E2E fine-tuning of SeamlessM4T-v2 yields strong results for languages with pre-training coverage (6/9 languages achieving best performance with E2E)
- **Medium Confidence**: The ASR encoder initialization mechanism for languages absent from pre-training, demonstrated on only 4 out of 9 languages with mixed effects
- **Low Confidence**: The multi-task training benefits, with modest improvements reported only when MT significantly outperforms ST, but no clear threshold established

## Next Checks

1. **Ablation study on multi-task loss components** - Systematically remove each component (E2E ST loss, MT loss, KD loss) from the multi-task training to quantify their individual contributions and verify the reported synergy effect.

2. **Cross-domain evaluation** - Evaluate the best-performing models (E2E fine-tuned and ASR-initialized) on held-out conversational speech datasets outside the IWSLT domain to assess robustness and identify potential overfitting to the task-specific data distribution.

3. **Implementation replication comparison** - Reproduce one language pair (e.g., bem-spa) using both the official codebase and HuggingFace implementation to precisely quantify the performance gap and verify that the identified discrepancies (lm_head tying, dropout, lang code loss) fully explain the differences.