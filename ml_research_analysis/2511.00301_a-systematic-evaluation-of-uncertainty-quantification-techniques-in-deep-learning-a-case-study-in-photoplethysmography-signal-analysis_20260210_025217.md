---
ver: rpa2
title: 'A systematic evaluation of uncertainty quantification techniques in deep learning:
  a case study in photoplethysmography signal analysis'
arxiv_id: '2511.00301'
source_url: https://arxiv.org/abs/2511.00301
tags:
- uncertainty
- reliability
- calibration
- prediction
- metrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a systematic evaluation of eight uncertainty
  quantification (UQ) techniques for deep learning models applied to photoplethysmography
  (PPG) signal analysis, specifically for atrial fibrillation (AF) classification
  and blood pressure (BP) regression tasks. The research implements and compares intrinsic
  methods (Maximum a Posteriori estimation, Monte Carlo Dropout, Quantile Regression),
  post-hoc ensemble techniques (Deep Ensembles), and post-hoc recalibration approaches
  (Temperature Scaling, Conformal Prediction, Isotonic Regression).
---

# A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis

## Quick Facts
- arXiv ID: 2511.00301
- Source URL: https://arxiv.org/abs/2511.00301
- Authors: Ciaran Bench; Oskar Pfeffer; Vivek Desai; Mohammad Moulaeifard; Loïc Coquelin; Peter H. Charlton; Nils Strodthoff; Nando Hegemann; Philip J. Aston; Andrew Thompson
- Reference count: 40
- Primary result: No single UQ technique universally outperforms others; optimal performance depends on uncertainty expression, evaluation metric, and reliability scale

## Executive Summary
This study presents a systematic evaluation of eight uncertainty quantification (UQ) techniques for deep learning models applied to photoplethysmography (PPG) signal analysis, specifically for atrial fibrillation (AF) classification and blood pressure (BP) regression tasks. The research implements and compares intrinsic methods (Maximum a Posteriori estimation, Monte Carlo Dropout, Quantile Regression), post-hoc ensemble techniques (Deep Ensembles), and post-hoc recalibration approaches (Temperature Scaling, Conformal Prediction, Isotonic Regression). A comprehensive evaluation framework is developed to assess uncertainty reliability across global, local, and adaptive scales, considering both calibration and sharpness metrics. Results reveal that no single UQ technique universally outperforms others; optimal performance depends on the chosen expression of uncertainty, evaluation metric, and scale of reliability assessment.

## Method Summary
The study evaluates UQ techniques on two medical signal analysis tasks: AF detection using 25-second PPG segments from the DeepBeat dataset and BP estimation using 10-second segments from VitalDB. Two neural architectures are used: 1D AlexNet and 1D ResNet with 50 layers. Eight UQ techniques are implemented and compared: MAP baseline, Monte Carlo Dropout (MCD), Deep Ensembles (DE), Quantile Regression (QR), Temperature Scaling (TS), Isotonic Regression (IR), and Conformal Prediction (CP). Training uses subject-level splits supporting both "calib" (overlapping subjects) and "calibfree" (distinct subjects) evaluation. The study implements a comprehensive evaluation framework assessing global metrics (CRPS, NLL, CCE), local metrics (ECE, ENCE with 15 bins), and adaptive metrics (per-class stratification). Post-hoc recalibration is applied after intrinsic UQ training, and uncertainty estimates are evaluated across multiple reliability scales.

## Key Results
- Deep Ensembles (alexnet-DE and resnet-DE) produced the most reliable uncertainties according to per-class Expected Calibration Error
- Post-hoc recalibration techniques (TS, IR) improved global calibration metrics but degraded adaptive/per-class reliability
- No single UQ technique was consistently superior across all metrics and scales; optimal choice depends on specific requirements
- Individual reliability assessment revealed significant gaps in current UQ methods, particularly for minority classes (AF vs non-AF)
- For the calibfree dataset, Deep Ensembles (alexnet-DE, resnet-DE, and resnet-MCD) performed best, highlighting the importance of model diversity for generalization

## Why This Works (Mechanism)

### Mechanism 1: Post-hoc Recalibration for Global vs. Adaptive Reliability
Post-hoc recalibration techniques improve global calibration metrics by learning monotonic transformations on held-out calibration sets, but can degrade adaptive/per-class reliability. This occurs because these methods optimize for aggregate performance rather than local consistency. The calibration set distribution must be exchangeable with the test set for optimal performance.

### Mechanism 2: Ensemble Diversity for Posterior Coverage
Deep Ensembles capture superior uncertainty estimates by sampling diverse regions of parameter space through independently trained models with different random initializations. The aggregation through Law of Total Variance disentangles epistemic (between-model) from aleatoric (within-model) uncertainty, with each ensemble member representing a sample from the posterior distribution.

### Mechanism 3: Scale-Dependent Reliability Assessment
Optimal UQ technique selection depends critically on the assessment scale: global (aggregate), local (binned by uncertainty magnitude), or adaptive (stratified by class/condition). Global metrics average across all samples, masking systematic biases toward dominant classes, while local and adaptive metrics reveal magnitude-dependent calibration drift and class-specific reliability gaps.

## Foundational Learning

- **Concept: Calibration-Sharpness Tradeoff**
  - Why needed here: Proper scoring rules reward both calibration (uncertainty matches error frequency) and sharpness (tight confidence intervals); different UQ techniques optimize different tradeoffs
  - Quick check question: A model predicts 50% confidence for every sample in a binary task with 50% class balance—it's perfectly calibrated but useless. Which property is missing?

- **Concept: Aleatoric vs. Epistemic Uncertainty Disentanglement**
  - Why needed here: Different UQ techniques model different uncertainty sources; MCD and DE can capture both via Law of Total Variance, while MAP only captures aleatoric
  - Quick check question: Your PPG model encounters signals from a patient population not represented in training data. Which uncertainty type should increase, and which UQ techniques would detect this?

- **Concept: Binning Strategies and Metric Artifacts**
  - Why needed here: The paper implements ECE (equal-width bins), ACE (equal-frequency bins), and smECE (kernel-smoothed); ENCE values scale with bin count, and sparse bins introduce variance
  - Quick check question: Your model's confidence scores cluster near 0.9 (overconfident). Why might equal-width ECE bins give misleadingly low calibration error?

## Architecture Onboarding

- **Component map**: AlexNet1D/ResNet1D50 -> MAP/MCD/DE/QR training -> Law of Total Variance for uncertainty extraction -> Optional post-hoc calibration (TS/IR) -> Multi-scale evaluation (Global → Local → Adaptive)

- **Critical path**:
  1. Train with likelihood-based loss: GNLL (regression) or custom logit-variance loss (classification)
  2. Intrinsic UQ selection: MAP baseline → add stochastic inference (MCD) or multi-model training (DE) or quantile heads (QR)
  3. Uncertainty extraction: MCD/DE use Law of Total Variance; QR directly outputs quantiles
  4. Optional post-hoc: Fit TS/IR on calibration split (40K samples for VitalDB calib)
  5. Multi-scale evaluation: Global metrics → reliability diagrams → bivariate histograms → per-class breakdown

- **Design tradeoffs**:
  - MCD vs DE: MCD is ~5x cheaper at inference but comparable reliability; DE preferred for calibfree generalization
  - Model capacity: ResNet improves predictive performance but uncertainty reliability is more mixed
  - Post-hoc calibration: IR achieves best NLL but worst per-class ECE for AF; global improvement masks local degradation
  - QR confidence levels: 1σ intervals more efficient but require distributional assumptions; 2σ more robust to Gaussian violations

- **Failure signatures**:
  - Overconfident minority class: AF per-class ECE 0.139-0.346 vs non-AF 0.013-0.060
  - Distribution shift: calibfree post-hoc methods degraded (PICP dropped below target)
  - PICP > 1.0: IR/TS at 1σ level overestimate uncertainty (alexnet DE+IR: 1.028/1.030)
  - UCE/ECE mismatch: UCE gave contradictory rankings due to comparing entropy to misclassification rate

- **First 3 experiments**:
  1. Establish MAP baseline on calib dataset: Train ResNet1D with GNLL loss, evaluate CRPS/NLL and ENCE reliability diagrams; target CRPS < 7.0 for SBP
  2. Compare intrinsic methods on adaptive reliability: Train MCD (5% dropout) and DE (5 members) for AF classification; evaluate per-class ECE with bootstrapping; expect DE to achieve <0.05 ECE for non-AF class
  3. Characterize post-hoc tradeoffs: Apply TS and IR to best DE model; plot global ECE vs per-class ECE tradeoff curve; identify if 10% global improvement justifies 50% per-class degradation for clinical use case

## Open Questions the Paper Calls Out
- How to develop robust quantitative metrics for assessing individual-level reliability, which the study identifies as the most effective measure but lacks established metrics
- How to better handle class imbalance in uncertainty quantification, particularly for minority classes like AF detection
- Whether existing UQ techniques can provide reliable uncertainty estimates at the patient level for clinical decision-making
- How to develop methods that maintain both global and adaptive reliability simultaneously without the observed tradeoffs

## Limitations
- Findings are primarily based on two specific medical datasets (DeepBeat for AF detection and VitalDB for BP regression), limiting generalizability to other domains
- The study relies on aggregate metrics that may not capture clinically meaningful failure modes in real-world deployment
- Computational cost of some UQ methods (particularly Deep Ensembles) may be prohibitive for real-time clinical applications
- Limited investigation of temporal dependencies or sequence modeling approaches that might better capture physiological signal characteristics

## Confidence
- **High confidence**: Comparative performance rankings of UQ techniques, fundamental tradeoffs between global vs. adaptive reliability, and core experimental findings
- **Medium confidence**: Claims about optimal technique selection depending on evaluation scale, given specific dataset characteristics and task definitions
- **Low confidence**: Generalization of findings to other medical imaging or signal processing domains, and specific threshold values for "reliable" uncertainty estimates

## Next Checks
1. **Cross-domain validation**: Apply the same UQ techniques to a different medical imaging dataset (e.g., chest X-rays or histopathology) to test generalizability of technique rankings
2. **Individual-level reliability analysis**: Implement and evaluate metrics for assessing uncertainty reliability at the patient level, addressing the study's identified gap in robust quantitative metrics
3. **Resource-efficiency evaluation**: Systematically quantify the computational cost-benefit tradeoff for each UQ technique under resource-constrained deployment scenarios