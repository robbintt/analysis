---
ver: rpa2
title: Cluster-Based Generalized Additive Models Informed by Random Fourier Features
arxiv_id: '2512.19373'
source_url: https://arxiv.org/abs/2512.19373
tags:
- mixture
- random
- fourier
- data
- clustering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a mixture-of-GAMs framework that combines
  random Fourier feature embeddings with interpretable local modeling. The method
  learns a latent Fourier representation from data, reduces it via PCA, and applies
  Gaussian mixture modeling to partition the input space into clusters.
---

# Cluster-Based Generalized Additive Models Informed by Random Fourier Features

## Quick Facts
- arXiv ID: 2512.19373
- Source URL: https://arxiv.org/abs/2512.19373
- Reference count: 40
- Primary result: Mixture-of-GAMs framework combining RFF embeddings with interpretable local modeling achieves performance comparable to state-of-the-art black-box and explainable models

## Executive Summary
This work introduces a mixture-of-GAMs framework that combines random Fourier feature embeddings with interpretable local modeling. The method learns a latent Fourier representation from data, reduces it via PCA, and applies Gaussian mixture modeling to partition the input space into clusters. Within each cluster, a GAM captures nonlinear univariate effects via spline-based smoothers. Numerical experiments on four regression benchmarks show that the proposed method consistently outperforms global interpretable models and mixture-of-linear baselines, achieving performance comparable to state-of-the-art black-box and explainable models on several tasks.

## Method Summary
The method operates through a pipeline: (1) Train an RFF model to approximate the regression function using learned frequencies and amplitudes, (2) Extract intermediate features S and reduce dimensionality via PCA to get latent representation Z, (3) Fit a Gaussian Mixture Model on Z to produce soft cluster assignments, (4) Train cluster-specific GAMs using spline-based smoothers, and (5) Aggregate predictions using posterior responsibilities as mixture weights. The framework balances interpretability through GAMs with the representational power of RFF embeddings.

## Key Results
- Consistently outperforms global interpretable models and mixture-of-linear baselines
- Achieves performance comparable to state-of-the-art black-box and explainable models on several tasks
- Successfully captures heterogeneous non-linear relationships better than single global models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Clustering in learned spectral latent space yields more predictive partitions than raw input space clustering
- **Core assumption:** Target regression function exhibits spectral characteristics that can be approximated by finite frequencies, with distinct data regimes manifesting as separable clusters in spectral domain
- **Evidence anchors:** Abstract mentions "random Fourier feature embeddings to uncover locally adaptive structures," page 7 defines latent space feature representation using RFF, corpus paper on adaptive RFF supports learned frequency utility
- **Break condition:** Underfitting RFF model (too few features K or poor frequency sampling) leads to uninformative clusters

### Mechanism 2
- **Claim:** Cluster-specific GAMs capture heterogeneous non-linear relationships better than global or locally linear baselines
- **Core assumption:** Residual relationship within any cluster is primarily additive, allowing GAMs to suffice locally even if global function is highly non-additive
- **Evidence anchors:** Abstract states "local GAM captures nonlinear univariate effects through interpretable spline-based smoothers," page 4 extends mixture of linear models to GAMs
- **Break condition:** Complex high-order interactions that univariate GAMs cannot model, or clusters too small for robust spline fitting

### Mechanism 3
- **Claim:** Soft gating via GMM stabilizes predictions across cluster boundaries
- **Core assumption:** Latent representation follows distribution reasonably approximated by Gaussian Mixture
- **Evidence anchors:** Page 8 describes GMM performing soft clustering to partition input space, page 11 mentions posterior responsibilities for latent space representation
- **Break condition:** Complex non-convex geometry that GMM cannot capture, leading to erratic posterior weights

## Foundational Learning

- **Concept:** Random Fourier Features (RFF)
  - **Why needed here:** Representation engine; understand how RFF approximates kernel to see why intermediate features form useful spectral embedding for clustering
  - **Quick check question:** Can you explain why RFF allows linear model to solve non-linear regression? (Answer: Maps data to feature space where linear combinations approximate kernel inner products)

- **Concept:** Generalized Additive Models (GAMs)
  - **Why needed here:** Local interpreter; distinguish between black box non-linear model and GAM which restricts complexity to sums of univariate smooth functions g_j(x_j)
  - **Quick check question:** If GAM plots curve for feature x_1, does that curve account for feature x_2? (Answer: Yes, marginally, assuming additivity holds)

- **Concept:** Curse of Dimensionality in Clustering
  - **Why needed here:** Paper applies PCA before GMM; understand why clustering directly in high-dimensional RFF space (K ≈ 4000) would fail due to distance concentration, requiring reduction to d ≈ 2-6
  - **Quick check question:** Why does pipeline compress K-dimensional RFF features down to d dimensions before fitting GMM?

## Architecture Onboarding

- **Component map:** Input -> RFF Encoder -> Latent Projector -> Gating Network -> Local Experts -> Aggregator
- **Critical path:** Quality of PCA-reduced RFF representation is bottleneck; if first d principal components don't separate data regimes well, GMM assigns poor weights and local GAMs train on garbage partitions
- **Design tradeoffs:** L (Clusters) vs Bias (high L increases flexibility but risks fragmenting data); d (PCA Dim) vs Stability (low d easier for GMM but may discard critical variance); Global vs Local (trades global stability for local precision)
- **Failure signatures:** Cluster Collapse (GMM assigns 99% to one cluster), Spline Overfit (wild oscillations in local GAM plots), Spectral Bias (RFF fails to capture high-frequency variations)
- **First 3 experiments:**
  1. Baseline Ablation: Replicate "Raw data + Local GAM" vs "RFF + Local GAM" comparison to verify RFF-based clustering adds value
  2. Hyperparameter Grid: Run (L, d) grid search on validation set to find stability plateau for your dataset
  3. Spatial Feature Check: If using spatial data, train RFF using only spatial features for clustering to see if interpretable regions emerge naturally

## Open Questions the Paper Calls Out

- **Open Question 1:** Can framework be effectively extended to spatio-temporal settings using joint spectral representations?
- **Open Question 2:** What are effects of employing sparse regularization (ℓ_p-type penalties with p ≤ 1) on random feature coefficients vs standard Tikhonov (ℓ_2) regularization?
- **Open Question 3:** Would multi-scale or hierarchical clustering strategies improve partitioning of RFF-induced latent space over standard GMM?

## Limitations

- RFF frequency optimization algorithm (reference [21]) is described as "preprint" with critical implementation details missing
- B-spline knot placement strategy lacks precise specifications for knot counts and boundary handling across datasets
- Exact mechanism by which RFF-GMM coupling discovers interpretable data regimes not fully validated against ablation baselines

## Confidence

- **High Confidence:** General methodology combining RFF embeddings, PCA reduction, GMM clustering, and cluster-specific GAMs is clearly specified and theoretically grounded
- **Medium Confidence:** Empirical performance claims supported by reported results, but exact reproduction depends on resolving frequency optimization and knot placement uncertainties
- **Low Confidence:** Exact mechanism by which specific RFF-GMM coupling discovers interpretable data regimes not fully validated against ablation baselines

## Next Checks

1. Baseline Ablation: Replicate "Raw data + Local GAM" vs "RFF + Local GAM" comparison to verify RFF-based clustering adds measurable value over simple input clustering
2. Hyperparameter Grid: Systematically run the (L, d) grid search on a validation set to identify the stability plateau for your specific dataset
3. Cluster Interpretability: If using data with spatial or categorical features, train the RFF using only those features for clustering and verify resulting partitions align with known data regimes