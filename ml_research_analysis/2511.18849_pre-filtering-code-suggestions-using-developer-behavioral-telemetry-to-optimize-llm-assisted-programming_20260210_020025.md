---
ver: rpa2
title: Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize
  LLM-Assisted Programming
arxiv_id: '2511.18849'
source_url: https://arxiv.org/abs/2511.18849
tags:
- code
- developer
- suggestion
- behavioral
- acceptance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a behavioral pre-filtering approach to optimize\
  \ Large Language Model (LLM)-based code suggestions in IDEs. By leveraging real-time\
  \ developer telemetry\u2014such as typing speed, file navigation, and editing activity\u2014\
  the method predicts whether a suggestion will likely be accepted before invoking\
  \ the LLM."
---

# Pre-Filtering Code Suggestions using Developer Behavioral Telemetry to Optimize LLM-Assisted Programming

## Quick Facts
- arXiv ID: 2511.18849
- Source URL: https://arxiv.org/abs/2511.18849
- Reference count: 23
- Primary result: Behavioral pre-filtering nearly doubled LLM code suggestion acceptance rates (18.4% → 34.2%) while suppressing 35% of low-value calls

## Executive Summary
This paper presents a behavioral pre-filtering approach to optimize Large Language Model (LLM)-based code suggestions in IDEs. By leveraging real-time developer telemetry—such as typing speed, file navigation, and editing activity—the method predicts whether a suggestion will likely be accepted before invoking the LLM. Deployed in a production Visual Studio Code plugin over four months, the system nearly doubled the acceptance rate of suggestions (18.4% → 34.2%) while suppressing 35% of low-value LLM calls. These results demonstrate that developer behavioral context can significantly enhance both the efficiency and user experience of AI-assisted programming tools, offering a lightweight, privacy-preserving solution for adaptive suggestion timing.

## Method Summary
The system captures real-time developer behavioral telemetry (typing patterns, navigation, editing activity) and aggregates it into ~15 features across five categories. A CatBoost binary classifier predicts suggestion acceptance probability using these behavioral features alone—without inspecting code content or prompts. The model uses a deliberately low threshold (τ=0.1) to maximize suppression of low-value LLM calls while preserving nearly all potentially accepted suggestions. The pre-filtering gate executes before LLM invocation, reducing unnecessary LLM calls by 35% while maintaining high acceptance rates.

## Key Results
- Acceptance rate improved from 18.4% to 34.2% after implementing behavioral pre-filtering
- Suppressed 35% of low-value LLM calls while maintaining high precision (0.981) for rejections
- Asymmetric threshold achieved 41.6% true negative rate with only 3.5% false negative rate

## Why This Works (Mechanism)

### Mechanism 1: Behavioral-Only Acceptance Prediction
Real-time developer behavioral telemetry (typing patterns, navigation, editing activity) can predict suggestion acceptance likelihood without inspecting code content or prompts. A CatBoost binary classifier trained on 2,318 suggestion events processes ~15 behavioral features across 5 categories (Interaction Fluency, Code Editing Scope, IDE Command Usage, Code State, Session Context). The model outputs an acceptance probability used to gate LLM invocation. Observable behavioral patterns serve as reliable proxies for developer cognitive readiness and task engagement state.

### Mechanism 2: Asymmetric Threshold Gating
A deliberately low decision threshold (τ=0.1) maximizes suppression of low-value calls while preserving nearly all potentially accepted suggestions. The system triggers LLM invocation only when predicted acceptance probability exceeds the threshold. At τ=0.1, the filter suppresses 41.6% of eventually-rejected suggestions (true negatives) while only mistakenly filtering 3.5% of would-be-accepted suggestions. The cost of suppressing a useful suggestion (false negative) significantly exceeds the cost of an unnecessary LLM call (false positive).

### Mechanism 3: Momentum Effect from Recent Interaction History
A developer's recent acceptance/rejection trajectory strongly predicts immediate-term receptiveness. The acceptance ratio feature `N_accepted / (N_accepted + N_rejected)` captures behavioral momentum. High recent acceptance rates indicate continued openness; rejection streaks signal disengagement or task mismatch. Developer receptiveness to AI assistance exhibits temporal continuity (momentum) rather than being stateless per request.

## Foundational Learning

- **Binary Classification Under Severe Class Imbalance (1:4.4 ratio)**: Training data contained 426 accepted vs. 1,892 rejected suggestions. Understanding weighted binary cross-entropy loss, precision-recall trade-offs, and why accuracy is misleading is essential for interpreting model design choices.
  - *Quick check question:* Why would 82% classification accuracy be trivially achievable and meaningless as an optimization target here?

- **Windowed Telemetry Aggregation**: The system aggregates behavioral metrics at one-minute rolling resolution and computes derived ratios (typing efficiency, pause frequency, edit density). Understanding temporal windowing, normalization, and alignment with discrete suggestion events is critical for feature pipeline implementation.
  - *Quick check question:* Why must `Typing Efficiency = C_typed / (T_typing + ε)` normalize by duration rather than using raw character counts?

- **Privacy-Preserving Feature Design**: The entire approach hinges on never inspecting code or prompts. Understanding what behavioral signals can and cannot proxy—and their inherent limitations—is essential for extending or debugging this architecture.
  - *Quick check question:* What contextual information (e.g., semantic intent, code quality) does behavioral telemetry inherently fail to capture that prompt/code content might reveal?

## Architecture Onboarding

- **Component map:** VS Code Extension (telemetry capture, suggestion triggering UI) -> Behavioral Feature Aggregator (1-minute rolling windows, ratio computation) -> Pre-filtering Classifier (CatBoost, ~15 features, outputs acceptance probability) -> Threshold Gate (τ=0.1 decision point, blocks or passes request) -> LLM Generation Pipeline (RAG retrieval + Qwen via vLLM inference) -> Post-processing Layer (suggestion trimming, reformatting)

- **Critical path:** Telemetry capture → Feature aggregation → Classifier inference (ms-scale) → Threshold decision → (if passed) LLM invocation → Suggestion rendering. The filter executes before any LLM call, not as a post-hoc ranker.

- **Design tradeoffs:** Behavioral-only vs. content-aware (trading potential prediction accuracy for privacy, cross-language generalizability, and zero prompt inspection overhead); Global pooled model vs. per-user personalization (current design uses one model across all developers); Low threshold (τ=0.1) (optimizes for recall at cost of fewer filtered calls)

- **Failure signatures:** Elevated false negatives on atypical developer profiles; Stale features if telemetry aggregation pipeline lags; Model drift if collective developer workflows shift; Behavioral signal may fail to distinguish between different types of engagement

- **First 3 experiments:** Establish baseline without filtering (measure raw acceptance rate); Feature ablation study (train classifier variants with feature subsets removed); Threshold sensitivity sweep (evaluate τ ∈ {0.05, 0.10, 0.15, 0.20})

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can online learning mechanisms effectively adapt filtering thresholds to individual developer working styles?
- **Basis in paper:** The authors list developing "per-user or per-session adaptive thresholds" using online learning as a key future direction.
- **Why unresolved:** The current deployed model uses a static threshold (τ=0.1) on pooled data, potentially missing individual behavioral idiosyncrasies.
- **What evidence would resolve it:** A comparative study showing improved acceptance rates with dynamically personalized thresholds versus static ones.

### Open Question 2
- **Question:** Can behavioral telemetry alone predict specific next editing actions to shape suggestion content?
- **Basis in paper:** The authors propose "behavioral-driven next edit prediction" to align content with imminent goals without inspecting code.
- **Why unresolved:** The current system only predicts timing (acceptance likelihood), not the semantic intent or specific nature of the edit.
- **What evidence would resolve it:** A model successfully predicting specific code edit types using only non-content telemetry features.

### Open Question 3
- **Question:** Do these findings generalize to larger, industrial developer populations?
- **Basis in paper:** The authors note the limitation of a small academic participant pool (n=9 active users) and plan to extend deployment to industrial cohorts.
- **Why unresolved:** The reported gains come from a small academic setting, which may not represent the complexity of professional workflows.
- **What evidence would resolve it:** Validation of the system's resource reduction and acceptance rate improvements in a large-scale enterprise environment.

## Limitations

- Single-academic-setting deployment with only nine sustained users limits generalizability to diverse real-world environments
- Fixed global threshold (τ=0.1) may systematically disadvantage developers with atypical working styles
- Behavioral telemetry cannot distinguish between different types of engagement that produce similar behavioral signatures

## Confidence

**High Confidence:** The core mechanism of behavioral-only prediction for suggestion acceptance is well-supported by empirical results (18.4% → 34.2% acceptance rate improvement).

**Medium Confidence:** The claim that this approach scales to diverse real-world settings has moderate support, though single-setting deployment and homogeneous user base limit confidence in broad applicability.

**Low Confidence:** The assertion that behavioral patterns serve as reliable proxies for cognitive readiness across all developer types lacks sufficient evidence from the small participant pool.

## Next Checks

1. **Multi-site deployment validation:** Deploy across at least three distinct development environments (academic, enterprise, open-source) with minimum 20 users per site over 8+ weeks. Compare acceptance rate improvements, false negative rates, and user satisfaction across settings.

2. **Per-user adaptive threshold evaluation:** Implement and test personalized threshold adjustment based on individual acceptance patterns. Measure whether adaptive thresholds outperform the fixed τ=0.1 across different developer profiles.

3. **Semantic context correlation study:** Conduct a controlled study where a subset of suggestions are accepted/rejected with full code and prompt content available. Compare behavioral-only predictions against ground truth semantic factors to quantify irreducible error from missing content context.