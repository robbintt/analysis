---
ver: rpa2
title: Linearity-based neural network compression
arxiv_id: '2506.21146'
source_url: https://arxiv.org/abs/2506.21146
tags:
- compression
- neurons
- layer
- weights
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces linearity-based neural network compression,
  a novel method that exploits the partial linearity of ReLU-like activation functions
  to reduce model size. The core idea is that neurons with near-constant activation
  rates behave linearly and can be replaced with shortcut connections, redistributing
  their weights to subsequent layers.
---

# Linearity-based neural network compression
## Quick Facts
- arXiv ID: 2506.21146
- Source URL: https://arxiv.org/abs/2506.21146
- Reference count: 7
- Primary result: Achieves 4x model size reduction while maintaining performance for deeper networks

## Executive Summary
This paper introduces linearity-based neural network compression, a novel method that exploits the partial linearity of ReLU-like activation functions to reduce model size. The core idea is that neurons with near-constant activation rates behave linearly and can be replaced with shortcut connections, redistributing their weights to subsequent layers. The approach demonstrates that linear neurons can be removed without changing network outputs, while practical implementation uses activation rate thresholds to approximate linearity.

The method shows promising results, compressing models to 1/4 of their original size while maintaining performance in most cases. It shows minimal interference when combined with existing importance-based pruning techniques, suggesting its potential as a complementary compression strategy. The approach works particularly well for deeper networks with varying layer sizes, achieving up to 10% compression without performance loss and up to 33% with minor accuracy degradation.

## Method Summary
The linearity-based compression method identifies neurons that behave linearly by monitoring their activation rates. Neurons with near-constant activation rates (determined by a threshold parameter) are considered linear and can be replaced with shortcut connections. The method redistributes the weights of these linear neurons to subsequent layers, effectively removing them from the network while preserving the overall function. This approach leverages the mathematical property that linear neurons can be removed without changing network outputs. The implementation uses activation rate thresholds to approximate linearity in practice, making it computationally efficient and applicable to various network architectures.

## Key Results
- Achieves up to 4x compression (1/4 original size) while maintaining performance
- Works particularly well for deeper networks with varying layer sizes
- Minimal interference with existing importance-based pruning techniques

## Why This Works (Mechanism)
The method exploits the mathematical property that neurons with near-constant activation rates behave linearly. For ReLU-like activation functions, when the input to a neuron falls into the linear region (positive for ReLU), the neuron's output is a linear function of its input. By identifying neurons that consistently operate in this linear regime (determined by activation rate thresholds), these neurons can be removed and their weights redistributed to subsequent layers via shortcut connections. This redistribution preserves the overall network function while reducing model size. The approach is particularly effective because it targets structural redundancies rather than just weight magnitudes, allowing for more aggressive compression without performance degradation.

## Foundational Learning
- **Activation rate thresholds**: Measure of how frequently a neuron activates above a certain value. Needed to identify linear neurons reliably. Quick check: Plot activation histograms across layers to visualize distribution.
- **Shortcut connections**: Direct connections that bypass certain layers or neurons. Needed for weight redistribution when removing linear neurons. Quick check: Verify gradient flow through shortcuts during training.
- **ReLU linearity property**: ReLU is piecewise linear with a linear region for positive inputs. Needed to justify why linear neurons can be removed. Quick check: Verify neuron activations stay predominantly positive.
- **Weight redistribution mechanics**: Process of transferring weights from removed neurons to subsequent layers. Needed to maintain network functionality after compression. Quick check: Compare pre/post-compression outputs on validation set.
- **Network depth effects**: How compression performance varies with network depth. Needed to understand where the method is most effective. Quick check: Test compression across shallow vs deep architectures.
- **Pruning interference analysis**: How linearity-based compression interacts with other pruning methods. Needed to establish complementarity with existing techniques. Quick check: Apply combined pruning strategies and measure interference.

## Architecture Onboarding
Component map: Input -> Linear layers with ReLU -> Activation rate monitoring -> Linear neuron identification -> Weight redistribution -> Output
Critical path: Activation rate monitoring -> Linear neuron identification -> Weight redistribution
Design tradeoffs: Threshold selection (higher = more compression but potential accuracy loss) vs model performance
Failure signatures: Performance degradation when threshold too high, insufficient compression when threshold too low
First experiments: 1) Test different activation rate thresholds on a small network, 2) Compare pre/post-compression outputs on validation set, 3) Measure interference with existing pruning techniques

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical threshold for "near-constant" activation rates is not rigorously defined
- Limited evaluation across diverse model architectures and datasets
- Claims of minimal interference with existing pruning techniques lack comprehensive ablation studies

## Confidence
High: Core mathematical framework for linearity-based compression
Medium: Experimental results showing 4x compression on specific architectures
Low: Cross-architecture generalizability and long-term stability claims

## Next Checks
1. Test the method across diverse architectures (transformers, CNNs with different activation functions, RNNs) to assess generalizability
2. Conduct systematic ablation studies varying activation rate thresholds to identify optimal compression-performance trade-offs
3. Evaluate long-term stability of compressed models through continual learning scenarios where weight redistribution might affect adaptation capacity