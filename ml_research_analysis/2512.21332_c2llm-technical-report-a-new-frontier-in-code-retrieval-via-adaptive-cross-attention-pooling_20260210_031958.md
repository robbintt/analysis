---
ver: rpa2
title: 'C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention
  Pooling'
arxiv_id: '2512.21332'
source_url: https://arxiv.org/abs/2512.21332
tags:
- code
- embedding
- zhang
- c2llm
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: C2LLM introduces a novel approach to code retrieval by integrating
  Pooling by Multihead Attention (PMA) into code embedding models. PMA aggregates
  token-level representations from a causal LLM backbone using a single learnable
  query, bypassing the limitations of EOS-based and mean pooling strategies.
---

# C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling

## Quick Facts
- arXiv ID: 2512.21332
- Source URL: https://arxiv.org/abs/2512.21332
- Reference count: 25
- Primary result: C2LLM achieves state-of-the-art performance on MTEB-Code with a 7B model scoring 80.75 and a 0.5B model scoring 75.46

## Executive Summary
C2LLM introduces a novel approach to code retrieval by integrating Pooling by Multihead Attention (PMA) into code embedding models. PMA aggregates token-level representations from a causal LLM backbone using a single learnable query, bypassing the limitations of EOS-based and mean pooling strategies. This design preserves the LLM's causal attention structure while effectively aggregating information across the sequence and allowing flexible embedding dimensionality. Trained on 3 million publicly available code datasets, C2LLM achieves state-of-the-art performance on the MTEB-Code benchmark, ranking first overall with an average score of 80.75 for the 7B variant and leading among models under 1B parameters with a score of 75.46 for the 0.5B model.

## Method Summary
C2LLM is built on top of a causal LLM backbone (Qwen2.5-Coder) and incorporates a Pooling by Multihead Attention (PMA) module. The PMA module uses a single learnable query vector that attends to all token representations from the LLM via cross-attention, creating a dynamic weighting of token importance. This approach preserves the causal attention structure of the pre-trained LLM while aggregating information across the sequence. The model is trained on 3 million code samples from diverse sources including CodeSearchNet, APPS, CodeFeedback, and CodeEditSearch, using contrastive learning to create a universal embedding space.

## Key Results
- C2LLM's 7B model achieves an average score of 80.75 on MTEB-Code, ranking first overall and surpassing both closed-source and open-source competitors
- The 0.5B model leads among models under 1B parameters with a score of 75.46, demonstrating strong performance across scales
- PMA's adaptive pooling consistently outperforms fixed EOS and mean pooling strategies across all model sizes
- The model shows strong cross-language generalization capabilities on the MTEB-Code benchmark

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Pooling by Multihead Attention (PMA) module creates more informative sequence embeddings by learning to weight token importance.
- Mechanism: A single learnable query vector attends to all token representations from the causal LLM via cross-attention. This allows the model to dynamically prioritize salient tokens (e.g., function signatures, key logic) over less important ones (boilerplate, comments).
- Core assumption: The information most relevant to code retrieval is not uniformly distributed across all tokens, nor is it fully compressed into the final token position by the causal LLM.
- Evidence anchors:
  - [abstract] "C2LLM adopts a Pooling by Multihead Attention (PMA) module... aggregating information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings."
  - [section 3] "PMA consists of a cross-attention layer with a single learnable query vector... effectively aggregating information from the token sequence into a single embedding vector."
- Break condition: Attention analysis shows the query distributes weights uniformly (like mean pooling) or focuses almost entirely on the final token (like EOS pooling), indicating the mechanism is not functioning as intended.

### Mechanism 2
- Claim: Preserving the causal attention of the pre-trained backbone is critical for performance.
- Mechanism: The architecture adds the PMA module *on top of* a standard causal LLM, using its output hidden states as key/value inputs. This avoids altering the core attention mechanism, thus fully leveraging the representations learned during the LLM's pre-training on code.
- Core assumption: The representations learned by a powerful code-focused causal LLM are highly valuable and are disrupted by changing the attention mechanism to bidirectional (which is often required for effective mean pooling).
- Evidence anchors:
  - [abstract] "...utilizing the LLM's causal representations acquired during pretraining..."
  - [section 3] "This design avoids... the architectural mismatch of mean pooling [which] is often paired with bidirectional attention."
- Break condition: An ablation shows a bidirectional model with PMA outperforms the causal model with PMA, or that the PMA gain is negligible on a non-causal backbone.

### Mechanism 3
- Claim: Large-scale contrastive training on a diverse mixture of code tasks generalizes to new retrieval scenarios.
- Mechanism: The model is trained on 3 million examples covering code-to-text, text-to-code, code-to-code, and multi-turn tasks. The contrastive loss pulls embeddings of relevant pairs closer and pushes non-relevant ones apart, creating a universal embedding space.
- Core assumption: A sufficiently diverse training dataset can teach the model to map semantically similar code concepts to nearby points in the embedding space, regardless of the specific task format.
- Evidence anchors:
  - [abstract] "Trained on three million publicly available data, C2LLM models set new records on MTEB-Code..."
  - [section 4.1] "The training data includes CodeSearchNet... APPS... CodeFeedback... CodeEditSearch... totaling 3 million samples."
- Break condition: The model fails to generalize to tasks or programming languages not represented in its training mixture, or shows poor performance on out-of-distribution benchmarks.

## Foundational Learning

- Concept: **Causal vs. Bidirectional Attention**
  - Why needed here: To understand the "architectural mismatch" problem the paper identifies. Causal models (like Qwen) attend only to previous tokens; bidirectional models (like BERT) attend to all tokens. The paper argues for preserving causal attention.
  - Quick check question: In a sequence [A, B, C], can token C attend to token A in a causal model? Can token A attend to token C?

- Concept: **Pooling Strategies (EOS, Mean, Attention)**
  - Why needed here: The core contribution is a new pooling method. You must understand the trade-offs of converting a sequence of vectors into a single embedding.
  - Quick check question: What is the output of EOS pooling? What are the potential "information bottleneck" and "architectural mismatch" issues associated with it and mean pooling, respectively?

- Concept: **Contrastive Learning**
  - Why needed here: This is the primary training objective. It learns by comparing samples.
  - Quick check question: In a contrastive loss, what happens to the distance between a query and a relevant document during training? What about a query and a hard negative?

## Architecture Onboarding

- Component map: Input -> Tokenizer -> Backbone LLM (causal attention) -> Final Hidden States -> PMA (Query attends to Keys/Values) -> Output Embedding
- Critical path: Input -> Tokenizer -> Backbone LLM (causal attention) -> Final Hidden States -> PMA (Query attends to Keys/Values) -> Output Embedding
- Design tradeoffs: The PMA module adds a small number of parameters and computation to gain expressive power over fixed pooling rules. It avoids the EOS bottleneck and the bidirectional mismatch of mean pooling. The authors use LoRA for fine-tuning, trading off potential performance for memory efficiency.
- Failure signatures: Attention collapse (query weights become uniform, effectively mean pooling), positional bias (query only attends to the last token), or overfitting (poor performance on new code tasks).
- First 3 experiments:
  1. Baseline Comparison: Run C2LLM and a vanilla Qwen2.5-Coder (with EOS pooling) on a subset of MTEB-Code tasks to measure the PMA's contribution.
  2. PMA Ablation: Replace the PMA module with mean and EOS pooling on the trained C2LLM backbone to isolate the pooling mechanism's effect.
  3. Attention Analysis: Visualize the PMA query's attention weights for different code samples (short, long, various languages) to verify it learns non-uniform, semantically meaningful patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is confined to the MTEB-Code benchmark, limiting generalizability to other retrieval scenarios.
- The paper does not provide a detailed ablation of the PMA module's contribution versus the underlying LLM architecture.
- No in-depth attention visualization is provided to confirm that PMA actually learns semantically meaningful weighting.

## Confidence

- **High Confidence**: The architectural integration of PMA with a causal LLM backbone is clearly described and logically sound. The model's first-place ranking on MTEB-Code is directly supported by the results table.
- **Medium Confidence**: The claim that PMA is more effective than EOS and mean pooling is supported by benchmark results but lacks ablation studies and attention visualizations for full validation.
- **Low Confidence**: The claim that the model generalizes across languages and tasks is not substantiated with empirical evidence beyond the MTEB-Code benchmark.

## Next Checks

1. **PMA Ablation Study**: Replace PMA with EOS and mean pooling on the trained C2LLM backbone and rerun on MTEB-Code to isolate the pooling mechanism's contribution.
2. **Attention Visualization**: Analyze and visualize the PMA query's attention weights across diverse code samples to confirm non-uniform, semantically meaningful weighting.
3. **Cross-Domain Generalization**: Test C2LLM on non-MTEB-Code retrieval tasks (e.g., natural language, cross-lingual retrieval) to evaluate true embedding generalization.