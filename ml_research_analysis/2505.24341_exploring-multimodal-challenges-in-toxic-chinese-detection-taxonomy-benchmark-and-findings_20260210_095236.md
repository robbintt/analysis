---
ver: rpa2
title: 'Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark,
  and Findings'
arxiv_id: '2505.24341'
source_url: https://arxiv.org/abs/2505.24341
tags:
- toxic
- chinese
- detection
- content
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting toxic Chinese content
  in the presence of multimodal perturbations that exploit Chinese linguistic features
  like character glyphs, phonetics, and semantics. The authors propose a comprehensive
  taxonomy of 8 perturbation methods and construct a large-scale dataset (CNTP) with
  approximately 20,000 perturbed toxic examples.
---

# Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings

## Quick Facts
- arXiv ID: 2505.24341
- Source URL: https://arxiv.org/abs/2505.24341
- Authors: Shujian Yang; Shiyao Cui; Chuanrui Hu; Haicheng Wang; Tianwei Zhang; Minlie Huang; Jialiang Lu; Han Qiu
- Reference count: 28
- Key outcome: Standard LLMs fail significantly on perturbed Chinese toxic content, with detection rates dropping below 60% for homophone and pinyin-initial perturbations, while fine-tuning with small samples causes overcorrection - incorrectly flagging normal content as toxic

## Executive Summary
This paper addresses the challenge of detecting toxic Chinese content in the presence of multimodal perturbations that exploit Chinese linguistic features like character glyphs, phonetics, and semantics. The authors propose a comprehensive taxonomy of 8 perturbation methods and construct a large-scale dataset (CNTP) with approximately 20,000 perturbed toxic examples. They benchmark 9 state-of-the-art language models and find that even advanced models struggle significantly with perturbed content, particularly with homophone and pinyin-initial perturbations where detection rates drop below 60%. When exploring enhancement strategies like in-context learning and fine-tuning with small sample sizes, the models show improved detection rates but suffer from "overcorrection" - incorrectly classifying up to 30% of normal content as toxic while still failing to truly understand the perturbed semantics.

## Method Summary
The authors constructed a large-scale benchmark dataset (CNTP) by applying 8 perturbation methods to toxic and non-toxic Chinese text from the Toxi_CN corpus. Perturbations included visual similarity replacements (VSim), radical splitting (Split), traditional-simplified conversion (Trad), pinyin-initial substitution (PY_Init), full pinyin (PY_Full), homophone replacement (Homo), character shuffling (Shuff), and emoji substitution (Emoji). They evaluated 9 LLMs on this dataset and tested mitigation strategies including in-context learning with 10 samples per perturbation type and fine-tuning with 10-40 samples. A Chain-of-Thought approach (CA-CoT) was also explored to improve semantic understanding.

## Key Results
- Even advanced models like GPT-4o and DeepSeek-V3 show detection rates below 60% for Homo and PY_Init perturbations
- Fine-tuning with 10-40 samples dramatically improves detection rates (>98%) but causes overcorrection, misclassifying 30%+ of normal content as toxic
- CA-CoT mitigation improves detection rates while reducing both error rates and misinterpretation rates compared to standard approaches
- Homophone and pinyin-initial perturbations are most effective at evading detection
- Models often make correct predictions without truly understanding the semantic content of perturbed text

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Chinese toxic content perturbed across glyph, phonetic, or semantic modalities evades detection because models fail to map obfuscated input to learned toxic concept representations
- **Core assumption:** Models were trained primarily on standard corpus text lacking robustness to multimodal obfuscation strategies common in Chinese internet culture
- **Evidence:** Detection rates consistently below 60% for Homo and PY_Init perturbations
- **Break condition:** If models are trained on massive corpora of internet slang or radical-decomposed text

### Mechanism 2
- **Claim:** Small-sample fine-tuning or ICL causes "overcorrection" - models learn to flag any obfuscated content as toxic rather than understanding perturbation logic
- **Core assumption:** Misclassification of normal content as toxic serves as proxy for lack of genuine comprehension
- **Evidence:** Fine-tuning with 10 samples achieves >98% detection but >30% error rate on non-toxic content
- **Break condition:** If fine-tuning datasets are larger or balanced with negative samples containing similar stylistic noise

### Mechanism 3
- **Claim:** Chain-of-Thought prompting that forces text recovery before judgment improves detection without proportional overcorrection
- **Core assumption:** LLMs possess intrinsic capability to denoise/construct perturbed Chinese glyphs/phonetics but need explicit instruction
- **Evidence:** CA-CoT increases detection rate while declining both error rate and misinterpretation rate
- **Break condition:** If perturbation is novel or highly ambiguous (obscure pinyin initials without context)

## Foundational Learning

- **Concept: Chinese Multimodality (Glyph/Phonetic/Semantic)**
  - **Why needed here:** Chinese characters function as logograms where meaning is tied to visual structure and sound, unlike alphabetic languages
  - **Quick check question:** Can you explain why replacing a Chinese character with a visually similar one confuses a tokenizer, whereas in English, it usually results in a typo?

- **Concept: Adversarial Perturbations & Taxonomy**
  - **Why needed here:** Understanding attack surface requires defining specific perturbation vectors derived from linguistic features
  - **Quick check question:** Differentiate between "Homophone Replacing (Homo)" and "Pinyin-Initial (PY_Init)" attacks. Which relies on sound and which relies on abbreviation?

- **Concept: The Precision-Recall Trade-off (Overcorrection)**
  - **Why needed here:** Improving detection (recall) via fine-tuning destroys precision, causing models to flag benign content
  - **Quick check question:** If a model flags "I love running 5km" as toxic because of an emoji, is this a failure of semantic understanding or a failure of the decision boundary?

## Architecture Onboarding

- **Component map:** Source Selector -> Entity Extractor (LLM) -> Perturbation Engine -> Validator (Human/LLM) -> Target System -> CA-CoT Module (Mitigation)

- **Critical path:** The Perturbation Engine is the bottleneck. If generated text is not readable to native speakers (Human Validation), the benchmark is invalid. The paper notes an average readability score of 3.94 is required.

- **Design tradeoffs:**
  - Detection vs. False Positives: Using SFT/ICL boosts detection of cloaked toxicity but drastically increases flagging of normal slang (Error Rate > 30%)
  - Zero-shot vs. CoT: Zero-shot is fast but blind to perturbations. CA-CoT requires 3x inference compute but significantly lowers Misinterpretation Rate

- **Failure signatures:**
  - The "Overcorrection" Spike: Sudden spike in toxicity flags where model output says "Toxic content is: [generic/normal word]"
  - Semantic Hallucination: Model correctly flags toxicity but explains it with wrong reasoning (e.g., "monkey" insult identified as "dog" insult)

- **First 3 experiments:**
  1. Baseline Robustness Check: Run standard prompt on CNTP dataset for GPT-4o-mini, measure F1 score drop between Base and Homo categories
  2. Overcorrection Stress Test: Fine-tune model on only 10 samples of "Split" perturbations, feed non-toxic subset of CNTP, quantify Error Rate
  3. CA-CoT Validation: Implement 3-step prompt (Detect Potential -> Recover -> Judge), compare Misinterpretation Rate against standard ICL method

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can larger-scale training data reduce the overcorrection problem while maintaining improved detection rates, or is overcorrection a fundamental limitation?
- **Basis:** Authors identify "Limited Sample Sizes in Mitigation Process" as limitation, only tested 10-40 samples producing 30%+ false positive rates
- **What evidence would resolve it:** Experiments fine-tuning with progressively larger datasets (100, 500, 1000+ samples) measuring both detection and error rates

### Open Question 2
- **Question:** What mechanisms enable correct classification without genuine understanding of perturbation semantics, and how can true semantic comprehension be achieved?
- **Basis:** High Misinterpretation Rates (73.33% for Split, 60% for PY_Init) even when detection rates improve, indicating predictions without understanding
- **What evidence would resolve it:** Development of methods that reduce both detection error AND misinterpretation rate simultaneously

### Open Question 3
- **Question:** How do Chinese text-image combinations create new forms of multimodal toxicity, and can detection models handle this cross-modal challenge?
- **Basis:** Paper focuses exclusively on text perturbations, but Chinese social media commonly combines emojis, images, and text to convey toxicity
- **What evidence would resolve it:** Construction of multimodal toxic Chinese dataset with text-image pairs, followed by benchmarking vision-language models

## Limitations
- The 28-30% perturbation rate may not reflect more aggressive obfuscation techniques deployed by malicious actors
- The taxonomy may not capture all real-world multimodal evasion tactics despite corpus analysis and expert input
- The study focuses exclusively on Chinese, limiting generalizability to other languages

## Confidence

**High Confidence** in core empirical findings: Well-supported benchmark results showing LLMs struggle with perturbed Chinese toxicity detection (particularly Homo and PY_Init <60% detection rates) and overcorrection phenomenon during fine-tuning

**Medium Confidence** in proposed taxonomy and mitigation strategies: Taxonomy appears comprehensive and CA-CoT shows promise, but evaluation focuses primarily on detection rates rather than true semantic understanding

**Low Confidence** in generalizability beyond Chinese: Mechanisms proposed may not translate directly to other languages; claim about Chinese's unique vulnerability requires further comparative analysis

## Next Checks

1. **Perturbation Coverage Validation:** Analyze real-world corpus of Chinese toxic content from social media platforms to quantify actual prevalence of each perturbation type and identify taxonomy gaps

2. **Overcorrection Mitigation Experiment:** Design fine-tuning protocol that explicitly balances toxic and non-toxic samples with similar perturbation styles to test whether overcorrection can be reduced without sacrificing detection performance

3. **Cross-linguistic Generalization Test:** Apply same perturbation taxonomy to Japanese (logographic) or Arabic (morphologically rich) to test whether detection challenges are language-specific or reflect broader LLM limitations