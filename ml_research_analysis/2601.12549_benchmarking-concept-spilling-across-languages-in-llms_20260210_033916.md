---
ver: rpa2
title: Benchmarking Concept-Spilling Across Languages in LLMs
arxiv_id: '2601.12549'
source_url: https://arxiv.org/abs/2601.12549
tags:
- language
- languages
- english
- spilling
- meanings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel comparative framework for evaluating
  multilingual semantic robustness in large language models (LLMs), focusing on "language
  spilling" - the phenomenon where models default to semantic representations from
  dominant languages (typically English) when generating content in other languages.
  The authors develop a structured methodology using 100 high-polysemy English words,
  translated into nine target languages, and prompt models to generate five distinct
  meanings with examples for each translated word.
---

# Benchmarking Concept-Spilling Across Languages in LLMs

## Quick Facts
- **arXiv ID**: 2601.12549
- **Source URL**: https://arxiv.org/abs/2601.12549
- **Reference count**: 40
- **Primary result**: Introduces a comparative framework measuring "language spilling" - when multilingual LLMs default to English semantic representations when generating content in other languages

## Executive Summary
This paper introduces a novel comparative framework for evaluating multilingual semantic robustness in large language models (LLMs), focusing on "language spilling" - the phenomenon where models default to semantic representations from dominant languages (typically English) when generating content in other languages. The authors develop a structured methodology using 100 high-polysemy English words, translated into nine target languages, and prompt models to generate five distinct meanings with examples for each translated word. These outputs are validated against authoritative dictionary definitions using a judge model, with spilling rates calculated as the percentage of invalid meanings. The study evaluates 16 diverse models and finds significant variation in semantic robustness across both models and languages. The strongest model (Apertus-70B-2509) achieved an average spilling rate of 20%, while the weakest (llama-3.1-8b-instruct) reached 41%. The methodology demonstrates high agreement with human evaluators (77.43%) and provides principled rankings for model comparison without requiring definitive attribution of error sources.

## Method Summary
The methodology evaluates cross-lingual semantic interference ("language spilling") by forcing models to generate exactly five meanings for high-polysemy words translated into target languages. The process involves: (1) selecting 100 polysemous English words from WordNet and translating them into nine languages using GPT-4o; (2) prompting each model to generate five meanings with three examples each in JSON format; (3) validating each meaning against authoritative dictionary definitions using a judge model (Gemini-2.5-Flash); and (4) calculating spilling rates as the percentage of meanings rejected by the target-language dictionary. The study evaluates 16 diverse models across 9 languages, with human agreement at 77.43% and judge model concordance (Kendall's W) at 0.9176.

## Key Results
- Significant variation in semantic robustness across models (spilling rates: 20-41%) and languages
- Strongest model (Apertus-70B-2509) achieved average spilling rate of 20%, weakest (llama-3.1-8b-instruct) reached 41%
- Judge model concordance demonstrated high reliability (Kendall's W = 0.9176)
- Human evaluation agreement with automated validation at 77.43%
- Approximately 70% of spilled meanings were valid in English, confirming English-centric interference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forcing a model to generate a fixed number of meanings for a translated, high-polysemy word acts as a stress test that reveals when its semantic knowledge in the target language is exhausted.
- Mechanism: A model is prompted in a non-English language to provide five distinct meanings and examples for a translated word. To fulfill the request, the model retrieves meanings from its internal representation. If valid, language-specific meanings are scarce, the model must either fail or "spill" meanings from a semantically related, dominant language (often English). This process translates a latent semantic interference problem into a measurable spilling rate.
- Core assumption: Models generate meanings sequentially and will default to dominant-language concepts when they run out of valid target-language meanings.
- Evidence anchors:
  - [abstract] The abstract states, "when required to generate exactly five meanings... semantically stronger models do so later in the generation sequence."
  - [section] The "Multilingual Meaning Generation" (2.2) section describes the "5 meanings" format as a "controlled stress test designed to reveal when models begin... resorting to representations from other languages."
  - [corpus] The paper "Iterative Multilingual Spectral Attribute Erasure" notes that multilingual representations create a shared space for transferring effects, making interference plausible. The paper "Multilingual LLMs Struggle to Link Orthography and Semantics in Bilingual Word Processing" directly investigates related form-meaning issues in bilingual processing.

### Mechanism 2
- Claim: A strong judge model, provided with authoritative dictionary definitions, can accurately and consistently determine if a generated meaning is valid in the target language.
- Mechanism: A judge model is given a dictionary entry for a word as its ground-truth context. It is then presented with a list of generated meanings from another model and returns a true/false judgment for each. This converts the subjective task of semantic validity into a more objective text-matching or entailment task, enabling scalable evaluation.
- Core assumption: The judge model can accurately interpret dictionary definitions and correctly assess whether a generated meaning aligns with them.
- Evidence anchors:
  - [abstract] The abstract mentions validating "generated meanings against authoritative dictionary definitions using a judge model."
  - [section] Section 4.2 ("Impact of Judge Model Variation") reports high pairwise agreement (84.59% to 92.61%) between different judge models and a Kendall's coefficient of concordance of 0.9176, indicating strong consistency.
  - [corpus] No direct evidence found for this specific validation method. Corpus papers on semantic distinction (e.g., Bhav-Net) suggest this is a difficult task generally, but do not validate the judge model's accuracy here.

### Mechanism 3
- Claim: The majority of "spilled" meanings are not random errors but are traceable to the word's meanings in English, validating the benchmark as a measure of English-centric semantic bias.
- Mechanism: Meanings flagged as invalid in a non-English language are translated into English and re-evaluated using an English dictionary. A high percentage of these "invalid" meanings being accepted as valid in English (~70%) suggests the error's source is English semantic interference rather than random failure.
- Core assumption: If a meaning is invalid in the target language but valid in English, it is evidence of cross-lingual spilling.
- Evidence anchors:
  - [section] The "Language Spilling Validation Experiment" (4.3) explicitly describes this procedure and states, "we found that approximately 70% of the meanings that were rejected in the foreign language were accepted in the English language."
  - [corpus] The corpus paper "RomanLens" discusses latent Romanization and an English-pivot mechanism, providing a plausible theoretical basis for English-centric interference.

## Foundational Learning

- **Polysemy vs. Homonymy**: The benchmark relies on high-polysemy words. Learners must understand that a single word can have multiple related meanings (e.g., "head" of a person, "head" of a company) to grasp why translation can lead to semantic spilling.
  - Quick check: The English word "bank" can mean the side of a river or a financial institution. In French, are these two meanings typically translated by a single word or different words?

- **Semantic Interference (Cross-lingual)**: This is the core problem being measured. It's not just a translation error; it's the unwanted transfer of meaning from one language's representation to another.
  - Quick check: A model uses the French word "cravate" (necktie) in a context where it means "to fasten/bind." What dominant-language meaning has likely interfered?

- **Judge Model Concordance**: The methodology's credibility hinges on the evaluation being consistent. This concept explains how the authors show their automated evaluation is reliable.
  - Quick check: Two different judge models produce rankings of generative models with a Kendall's W of 0.9176. What does this value indicate about their agreement?

## Architecture Onboarding

- **Component map**: Polysemy Word Selector -> Translation Module (GPT-4o) -> Generative Model (model under test) -> Judge Model (with dictionary context) -> Spilling Rate Calculator
- **Critical path**: The reliability of the entire system depends on the Judge Model's accuracy. If the judge cannot reliably distinguish valid from invalid meanings, the spilling rate metric becomes meaningless.
- **Design tradeoffs**: Fixed Meaning Count (5) acts as a stress test, forcing errors to make comparison possible. Dictionary as Ground Truth is authoritative but incomplete, potentially overestimating spilling. Judge Model Selection involves a trade-off between power/cost; the ablation study shows consistent rankings across judges, justifying the use of a fast model.
- **Failure signatures**: Formatting errors occur when the generative model fails to output valid JSON. Judge hallucination happens if the judge ignores the dictionary and uses its own parametric knowledge. Spurious non-JSON spilling occurs if unstructured text is misinterpreted.
- **First 3 experiments**:
  1. Reproduce the core finding: Select 10 polysemous words, translate them into German, and run the pipeline with two models of different sizes to verify that the smaller model has a higher spilling rate.
  2. Judge consistency check: Take a fixed set of generated meanings for 10 words in one language, run them through two different judge models, and calculate the pairwise agreement to confirm high concordance.
  3. Validate spilling causality: Take a sample of 20 meanings flagged as invalid by the judge in the target language. Manually check (or use a simple translation + English dictionary check) to confirm that a majority are valid in English.

## Open Questions the Paper Calls Out

- **Context-Dependent Evaluation**: The authors plan to extend evaluation to context-dependent usage, examining spilling behavior when words are presented within sentences rather than in isolation. The current benchmark abstracts away pragmatic cues available in natural communication.

- **Cross-Language Pairs Beyond English**: The study proposes exploring cross-language spilling between closely related or script-sharing languages (e.g., Russian to Bulgarian, Spanish to Basque, Hindi to Sanskrit), as the current focus is exclusively on English-centric spilling.

- **Complex Linguistic Phenomena**: The authors note that semantic spilling might manifest differently with idiomatic expressions, metaphors, or words with strong cultural connotations, as the current methodology restricts analysis to 100 high-polysemy words (mostly nouns).

## Limitations

- Judge Model Reliability: While high agreement is reported between judge models, the automated semantic matching against dictionary definitions still relies on the judge model's own biases and interpretation capabilities.
- Translation Stability: The methodology depends on GPT-4o for translating 100 polysemous words, and exact translations aren't provided, requiring re-running the translation step which may yield different results.
- Model Dependency: The benchmark assumes models generate meanings sequentially and default to dominant-language concepts when exhausted, which may not hold for future model architectures or generation strategies.

## Confidence

- **High Confidence**: The core methodology is well-specified and reproducible; the finding that smaller models exhibit higher spilling rates (41% vs 20% for the strongest model) is consistent across judge models and supported by human validation (77.43% agreement).
- **Medium Confidence**: The mechanism of sequential meaning generation leading to spilling is plausible but not definitively proven. Alternative explanations for the observed spilling patterns cannot be ruled out without further investigation.
- **Medium Confidence**: The validation that spilled meanings are primarily traceable to English sources (~70%) is methodologically sound but depends on the accuracy of both the judge model and the English dictionary validation process.

## Next Checks

1. **Translation Reproducibility Test**: Run the GPT-4o translation step for 20 randomly selected polysemous words across 3 languages. Compare the generated translations with those used in the paper (if accessible) or with human translations to assess consistency.

2. **Judge Model Ablation**: Select 50 generated meanings from a single model-language pair. Run validation using two different judge models and calculate pairwise agreement. Additionally, manually verify a random sample of 20 judge decisions to assess accuracy.

3. **Spilling Mechanism Validation**: For 10 models with varying spilling rates, analyze the actual generated outputs to determine whether spilling occurs primarily in later meanings (supporting sequential generation hypothesis) or is distributed randomly. This would validate the stress-test mechanism proposed in the paper.