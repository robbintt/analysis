---
ver: rpa2
title: 'SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency
  Thresholds'
arxiv_id: '2506.18591'
source_url: https://arxiv.org/abs/2506.18591
tags:
- detection
- patch
- attack
- spann
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SpaNN, a method for detecting adversarial patches
  in CNNs by leveraging ensemble-based binarized feature maps. Instead of relying
  on a fixed saliency threshold, SpaNN constructs an ensemble of binarized feature
  maps from the first convolutional layer using a range of thresholds, performs clustering
  on each, and trains a lightweight CNN classifier on the resulting clustering features.
---

# SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds

## Quick Facts
- **arXiv ID:** 2506.18591
- **Source URL:** https://arxiv.org/abs/2506.18591
- **Reference count:** 40
- **Primary result:** Detects adversarial patches on CNNs with up to 96.64% accuracy for image classification and 86.13% for object detection, outperforming state-of-the-art baselines by up to 27 percentage points.

## Executive Summary
SpaNN introduces a novel approach for detecting adversarial patches on CNNs by constructing an ensemble of binarized feature maps from the first convolutional layer using multiple saliency thresholds. Instead of relying on a fixed threshold, SpaNN generates clustering features from density-based clustering on each binarized map and trains a lightweight CNN classifier on these features. This design makes SpaNN effective for detecting single- and multiple-patch attacks without being sensitive to the number of patches. Evaluated across object detection (YOLOv2) and image classification (ResNet-50) tasks, SpaNN achieves state-of-the-art detection accuracy while maintaining computational efficiency independent of patch count.

## Method Summary
SpaNN extracts feature maps from the first convolutional layer of victim models, then generates an ensemble of binarized feature maps using 20 equidistant saliency thresholds. For each binarized map, DBSCAN clustering identifies spatial patterns, producing four clustering features: number of clusters, mean intra-cluster distance, standard deviation of intra-cluster distances, and number of important neurons. These features are concatenated across thresholds, normalized, and fed to a lightweight 4-channel 1D CNN classifier that distinguishes attacked from clean images. The method requires only single-patch attack samples for training and can operate in unsupervised mode with competitive accuracy.

## Key Results
- Achieves detection accuracy up to 96.64% for image classification and 86.13% for object detection
- Outperforms state-of-the-art baselines by up to 27 percentage points
- Maintains robustness to adaptive attacks while keeping computational cost independent of patch count
- Supports both supervised and unsupervised detection modes

## Why This Works (Mechanism)

### Mechanism 1
Using multiple saliency thresholds instead of a fixed threshold makes detection robust against adaptive attacks and eliminates dataset-specific threshold tuning. For each input, extract feature map M from the first convolutional layer, then generate B binarized feature maps using thresholds β_b ∈ B = {0, 0.05, ..., 0.95}. The resulting curves of clustering metrics across thresholds capture how spatial patterns change as the "importance" definition varies, which differs systematically between clean and patched images.

### Mechanism 2
Adversarial patches create dense, localized clusters of high activation distinguishable from natural salient regions via density-based clustering. Apply DBSCAN (ε=1, w_min=4) to each binarized feature map. Extract four clustering features: (1) number of clusters, (2) mean average intra-cluster distance, (3) standard deviation of intra-cluster distances, (4) number of important neurons. Patches produce fewer, denser clusters with characteristic distance distributions.

### Mechanism 3
The 4×B dimensional feature vector provides sufficient signal for a lightweight CNN to learn patch-agnostic detection. The AD classifier (4-channel 1D CNN) processes normalized clustering feature curves. Architecture: 2 conv layers (4→12→12 channels, kernel=2), average pooling, batch norm, ReLU → 2 FC layers (144→576→576) → sigmoid output. Trained on binary labels using Adam, LR=0.0001.

## Foundational Learning

- **Saliency Thresholding on Neural Activations**
  - Why needed here: Understanding how thresholding creates binary masks from continuous activation maps is fundamental to the ensemble approach
  - Quick check question: Given a feature map with max activation 2.5 and threshold β=0.6, which positions with values [1.2, 1.5, 2.1, 0.8] are marked as "important"?

- **DBSCAN Density-Based Clustering**
  - Why needed here: The algorithm choice is intentional—it doesn't require pre-specifying cluster count and handles noise naturally, essential for variable patch numbers
  - Quick check question: Why would k-means clustering fail here if the number of patches varies between 1-4 per image?

- **Adversarial Patches as Physically Realizable Attacks**
  - Why needed here: Unlike ℓ_p-bounded digital perturbations, patches are spatially constrained and locally applied—this shapes their detectable activation patterns
  - Quick check question: How does the physical realizability constraint make patches both more practical for attackers yet potentially more detectable than digital attacks?

## Architecture Onboarding

- **Component map:** Victim Model Hook → Threshold Ensemble Generator → Clustering Engine → Feature Vector Builder → Attack Detector (AD)
- **Critical path:** Input → First conv layer extraction → B parallel binarizations → B parallel DBSCAN runs → Feature concatenation → Normalization ([-1,1] per row) → AD inference → Detection score
- **Design tradeoffs:**
  - Ensemble size |B|: Accuracy increases with B but cost is linear. Figure 6a shows diminishing returns after |B|=10
  - Clustering granularity: ε=1 clusters only adjacent neurons. Larger values may merge distinct salient regions
  - First-layer extraction: Assumption that patches manifest early. Deeper layers may capture different patterns
  - Supervision level: Default needs attacked samples; OCC variant uses only clean data but sees 2-10% accuracy drop
- **Failure signatures:**
  - High FPR on texture-rich images: Natural complex textures may produce patch-like cluster patterns
  - Cross-dataset degradation: Figures 18-20 show curve shapes differ by dataset/model—AD trained on one context fails on another
  - Adaptive attack erosion: Figure 7a shows detection drops as γ (stealthiness weight) increases, though attack effectiveness drops faster
- **First 3 experiments:**
  1. Replicate Table I on INRIA/ImageNet subsets to validate clustering curves match Figure 1 separation pattern and confirm baseline accuracy
  2. Run ensemble ablation |B| ∈ {4, 10, 20, 50} to verify accuracy-time tradeoff inflection around |B|=10 per Figure 6
  3. Train adaptive attack with γ ∈ {0.01, 0.1, 0.5} to confirm detection-effectiveness tradeoff and compare against baseline vulnerability in Figure 7b

## Open Questions the Paper Calls Out

1. **Can the clustering features be leveraged for attack identification and input recovery?**
   The paper conjectures that SpaNN's clustering features could be used for attack identification and recovery, but leaves this as future work. Evidence would require implementing a recovery mechanism that uses SpaNN's clusters to mask input regions and analyzing recovery success rates.

2. **Can SpaNN detectors trained on one model/dataset generalize to other contexts?**
   The paper acknowledges that retraining is currently required to adjust SpaNN to different contexts. Cross-domain experiments where the AD classifier is trained on one dataset and evaluated on another would quantify the performance degradation compared to fully retrained models.

3. **Can more sophisticated One-Class Classification techniques close the performance gap?**
   While the unsupervised variant performs competitively, the paper suggests exploring more elaborate OCC approaches could further bridge the gap with supervised methods. Advanced OCC methods like Deep SVDD could be evaluated on SpaNN's features to measure performance improvements.

## Limitations
- Relies on patches manifesting in early-layer activation patterns, which may not generalize to all attack types
- Performance degrades across different datasets/models due to varying clustering feature distributions
- Computational efficiency claim holds for detection but not necessarily for feature extraction phase
- Adaptive attacks can reduce detection accuracy, though attacks also become less effective

## Confidence

**High confidence:** Detection accuracy claims (96.64% classification, 86.13% detection) and baseline comparisons are well-supported by quantitative results across multiple datasets and attack types.

**Medium confidence:** The claim of robustness against adaptive attacks is supported but doesn't explore the full space of adaptive strategies. Computational efficiency is maintained for detection but not necessarily feature extraction.

**Low confidence:** The unsupervised variant's competitive performance suggests inherent discriminative power, but the paper doesn't analyze which features drive separability or characterize sensitivity to hyperparameters.

## Next Checks

1. **Cross-dataset generalization test:** Train SpaNN on INRIA Person and evaluate on Pascal VOC (and vice versa) to quantify performance degradation from domain shift in clustering feature distributions.

2. **Adaptive attack stress test:** Implement a gradient-based patch optimization that explicitly minimizes clustering feature distance to clean images, measuring whether detection accuracy drops below 50% while maintaining attack effectiveness.

3. **First-layer sensitivity analysis:** Replace the first convolutional layer hook with the second layer in YOLOv2 and ResNet-50, measuring detection accuracy changes to validate the architectural assumption about early-layer activation patterns.