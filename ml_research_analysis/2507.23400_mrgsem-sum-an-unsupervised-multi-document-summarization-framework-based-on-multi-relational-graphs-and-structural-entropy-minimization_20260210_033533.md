---
ver: rpa2
title: 'MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on
  Multi-Relational Graphs and Structural Entropy Minimization'
arxiv_id: '2507.23400'
source_url: https://arxiv.org/abs/2507.23400
tags:
- mrgsem-sum
- graph
- summarization
- unsupervised
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRGSEM-Sum introduces a novel unsupervised multi-document summarization
  framework that addresses the dual challenges of complex document relationships and
  information redundancy through multi-relational graph construction and structural
  entropy minimization clustering. The method constructs a multi-relational graph
  integrating semantic and discourse relationships between sentences, then applies
  a two-dimensional structural entropy minimization algorithm to automatically determine
  optimal cluster numbers without predefined parameters.
---

# MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization

## Quick Facts
- arXiv ID: 2507.23400
- Source URL: https://arxiv.org/abs/2507.23400
- Reference count: 6
- Primary result: Unsupervised MDS framework using multi-relational graphs and structural entropy minimization clustering

## Executive Summary
MRGSEM-Sum introduces a novel unsupervised multi-document summarization framework that addresses the dual challenges of complex document relationships and information redundancy through multi-relational graph construction and structural entropy minimization clustering. The method constructs a multi-relational graph integrating semantic and discourse relationships between sentences, then applies a two-dimensional structural entropy minimization algorithm to automatically determine optimal cluster numbers without predefined parameters. A position-aware compression mechanism further distills clusters into concise summaries. Experimental results demonstrate MRGSEM-Sum consistently outperforms state-of-the-art unsupervised methods across four benchmark datasets (Multi-News, DUC-2004, PubMed, and WikiSum), achieving ROUGE-1 scores of 43.21 on Multi-News compared to 42.32 for the previous best unsupervised method. Human evaluation confirms superior performance in consistency and coverage, with MRGSEM-Sum approaching human-level quality while maintaining competitiveness with supervised methods and large language models.

## Method Summary
The framework constructs a multi-relational graph G = {V, Er1, Er2} where Er1 edges represent TF-IDF cosine similarity (content-level associations) and Er2 edges represent discourse relationships (discourse markers, coreference, entity linking). These complementary perspectives are merged via max-pooling of edge weights. The 2D structural entropy algorithm iteratively merges graph nodes by greedily selecting pairs that produce the largest negative ΔSE, automatically determining optimal cluster numbers without predefined parameters. A position-aware compression mechanism prioritizes clusters containing document-initial sentences, improving summary salience through K-shortest paths algorithm with keyphrase scoring.

## Key Results
- MRGSEM-Sum achieves ROUGE-1 score of 43.21 on Multi-News dataset, outperforming previous best unsupervised method (42.32)
- Human evaluation shows MRGSEM-Sum approaches human-level quality on fluency, consistency, and coverage metrics
- Ablation studies demonstrate each component's contribution: removing multi-relational graph causes 4.25 R-1 drop on Multi-News, removing 2D SE clustering causes 5.35 R-1 drop, removing position-aware weighting causes 14.35 R-1 drop on PubMed
- Competitive performance against supervised methods and large language models while maintaining unsupervised nature

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating semantic and discourse relations captures more complete sentence relationships than single-relation graphs alone.
- Mechanism: The framework constructs a multi-relational graph G = {V, Er1, Er2} where Er1 edges represent TF-IDF cosine similarity (content-level associations) and Er2 edges represent discourse relationships (discourse markers, coreference, entity linking). These complementary perspectives are merged via max-pooling of edge weights (Eq. 1), allowing the graph to encode both topical similarity and logical-rhetorical connections.
- Core assumption: Semantic and discourse relations provide non-redundant, complementary signals about sentence relatedness in multi-document contexts.
- Evidence anchors:
  - [abstract]: "constructs a multi-relational graph integrating semantic and discourse relationships between sentences"
  - [section]: Ablation study (Table 4) shows removing multi-relation graph causes R-1 to drop 4.25 on Multi-News and 7.19 on PubMed
  - [corpus]: Weak direct corpus evidence; neighboring papers focus on hierarchical organization and retrieval but not multi-relational graph construction specifically
- Break condition: If semantic and discourse relations are highly correlated (provide redundant signal), the multi-relational construction adds computational overhead without quality gain.

### Mechanism 2
- Claim: Structural entropy minimization automatically determines optimal cluster count without manual specification.
- Mechanism: The 2D structural entropy algorithm (Eq. 2-4) iteratively merges graph nodes by greedily selecting pairs that produce the largest negative ΔSE. The process continues until no merge yields negative ΔSE, at which point the encoding tree structure defines natural cluster boundaries. This removes the need for predefined k in traditional clustering.
- Core assumption: The optimal document cluster partition corresponds to minimum structural entropy, meaning sentences within clusters are tightly connected while inter-cluster connections are sparse.
- Evidence anchors:
  - [abstract]: "automatically determining the optimal number of clusters without predefined parameters"
  - [section]: Methodology states "automatically determining the optimal cluster numbers and effectively organizing sentences into coherent groups"; ablation (Table 4) shows removing 2D SE clustering causes 5.35 R-1 drop on Multi-News
  - [corpus]: No direct corpus evidence for structural entropy in MDS; this appears to be a novel application from information theory
- Break condition: If the greedy merge strategy converges to local minima, or if documents have uniform connectivity without clear community structure, clustering quality degrades.

### Mechanism 3
- Claim: Position-aware weighting prioritizes clusters containing document-initial sentences, improving summary salience.
- Mechanism: Each sentence receives positional importance wpos(si,j) = 1 - (posi,j - 1)/(Ni - 1), giving higher weight to earlier sentences. Clusters are ranked by aggregate positional score, and top c* clusters are selected for compression via K-shortest paths algorithm using keyphrase scoring.
- Core assumption: Important information tends to appear earlier in documents (inverted pyramid journalistic convention), so clusters with more document-initial sentences contain higher salience content.
- Evidence anchors:
  - [abstract]: "position-aware compression mechanism further distills clusters into concise summaries"
  - [section]: Table 5 shows optimal performance at c*=14 clusters; ablation shows position-aware removal hurts PubMed (long documents) more than Multi-News, suggesting positional signal is more critical for longer texts
  - [corpus]: No explicit corpus validation of position-aware mechanisms in MDS
- Break condition: For document types where important information is not front-loaded (e.g., academic papers with conclusions at the end), this weighting scheme may systematically deprioritize key content.

## Foundational Learning

- Concept: **Structural Entropy on Graphs**
  - Why needed here: The core clustering mechanism requires understanding how entropy measures quantify uncertainty in graph partitions. The 2D formulation extends classical Shannon entropy to graph structures via encoding trees.
  - Quick check question: Given a graph with 5 nodes, can you manually compute the structural entropy change (ΔSE) from merging two leaf nodes under a common parent?

- Concept: **Discourse Relations in NLP**
  - Why needed here: The Er2 edge type relies on discourse markers, coreference, and entity linking. Understanding Rhetorical Structure Theory or PDTB-style discourse parsing helps interpret what discourse edges capture.
  - Quick check question: For the sentence pair "The earthquake struck at 4:30 p.m. Residents were without power," what discourse signals would link these sentences?

- Concept: **K-Shortest Paths for Sentence Compression**
  - Why needed here: The final compression step uses K-shortest paths over word graphs to generate concise cluster summaries. Understanding word-lattice construction and path scoring is essential.
  - Quick check question: How does the K-shortest paths approach differ from simply extracting the highest-scoring sentence from each cluster?

## Architecture Onboarding

- Component map: Preprocessing Module -> Multi-Relational Graph Builder -> 2D SE Clustering Engine -> Position-Aware Compressor -> Summary concatenation
- Critical path: Graph construction → Edge merging → SE minimization clustering → Position-aware cluster ranking → K-shortest paths compression → Summary concatenation
- Design tradeoffs:
  - Multi-relation vs. single-relation graphs: Richer representation at cost of doubled edge computation and storage
  - Automatic vs. predefined k: Removes hyperparameter but adds algorithmic complexity (O(n²) merge checks per iteration)
  - Position-aware vs. uniform weighting: Leverages document structure but assumes front-loaded importance
- Failure signatures:
  - Cluster collapse: If SE minimization merges too aggressively, all sentences end in one cluster (check final cluster count distribution)
  - Empty discourse edges: If discourse parser fails or documents lack explicit markers, Er2 is sparse; verify edge density before merging
  - Position bias: On non-news domains, early-sentence weighting may miss key conclusions; monitor ROUGE drops on WikiSum vs. Multi-News
- First 3 experiments:
  1. **Edge type ablation**: Run with only Er1, only Er2, and both; measure individual contributions and interaction effects
  2. **Cluster count validation**: Compare SE-determined k against ground-truth topic counts (if available) or silhouette scores; verify automatic k matches reasonable manual inspection
  3. **Position weighting sensitivity**: Test on document types with different information ordering (news vs. scientific vs. legal); quantify when position-aware helps vs. hurts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would weighted or learned combination strategies for merging multi-relational graph edges outperform the simple max-pooling operation (Eq. 1) currently used?
- Basis in paper: [inferred] The merging formula A′[i][j] = max(AEr1[i][j], AEr2[i][j]) uses a basic heuristic that may lose complementary information from different relation types.
- Why unresolved: The paper does not compare alternative merging strategies or analyze what information is lost when taking only the maximum edge weight.
- What evidence would resolve it: Ablation experiments comparing max-pooling against weighted averaging, attention-based fusion, or learnable combination functions across the same datasets.

### Open Question 2
- Question: Can incorporating pragmatic relations (beyond semantic and discourse) further improve MRGSEM-Sum's ability to model complex multi-document relationships?
- Basis in paper: [explicit] The introduction states that "complex and heterogeneous relationships that naturally arise in multi-document scenarios, such as semantic, discourse, and pragmatic links" exist, yet the framework only implements the first two.
- Why unresolved: The authors identify pragmatic links as part of the rich relational landscape but do not implement or evaluate them, leaving this extension unexplored.
- What evidence would resolve it: Experiments adding pragmatic relation edges (e.g., speaker intent, illocutionary acts) to the multi-relational graph and measuring ROUGE/human evaluation changes.

### Open Question 3
- Question: How does MRGSEM-Sum scale computationally with increasing document cluster sizes, particularly for datasets like WikiSum with 40 documents per cluster?
- Basis in paper: [inferred] Algorithm 1 involves nested loops for pairwise SE comparisons and iterative merging, but no complexity analysis or runtime measurements are provided.
- Why unresolved: The paper reports performance across datasets with varying sizes (DUC-2004: 10 docs/cluster vs. WikiSum: 40 docs/cluster) but does not analyze computational costs or scalability limits.
- What evidence would resolve it: Runtime and memory profiling across varying cluster sizes, with complexity analysis of the 2D SE minimization algorithm.

### Open Question 4
- Question: Does the linear decay position-aware importance formula generalize well to non-news domains where sentence position may not correlate with salience?
- Basis in paper: [inferred] The formula wpos(si,j) = 1 - (posi,j - 1)/(Ni - 1) assumes earlier sentences are more important, validated primarily on news datasets; the position-aware ablation showed dramatic drops on PubMed (R-1: 37.96→23.37).
- Why unresolved: The strong position-aware effect on PubMed suggests domain-specific dynamics, but the paper does not explore whether alternative position models (e.g., section-aware, inverted pyramid) would perform better.
- What evidence would resolve it: Ablation studies with alternative position weighting schemes across different domains, and analysis of sentence position vs. gold summary coverage correlations per dataset.

## Limitations

- The structural entropy minimization clustering lacks corpus validation for multi-document summarization, with no evidence that SE-identified cluster counts match reasonable ground truth
- The position-aware weighting mechanism assumes front-loaded information importance, which may systematically fail on non-news document types like scientific papers or legal documents
- The greedy merge strategy in SE minimization may converge to local minima or fail to find clear community structure in uniformly connected documents

## Confidence

**High Confidence**: Claims about overall ROUGE performance improvements over unsupervised baselines (Multi-News 43.21 vs 42.32)

**Medium Confidence**: Claims about multi-relational graph construction improving coverage (Mechanism 1) - ablation shows benefits but complementary signal validation is weak

**Low Confidence**: Claims about automatic k determination via structural entropy (Mechanism 2) - no corpus validation of SE-identified cluster counts

## Next Checks

1. **Discourse Edge Density Validation**: Measure Er2 edge density across datasets before and after discourse parsing. If Er2 remains sparse (e.g., <5% of possible edges), the multi-relational construction adds computational overhead without meaningful signal contribution.

2. **SE Clustering Robustness**: Run Algorithm 1 with different initial n values (n=4, 8, 16) on a subset of documents. Plot final cluster counts and ΔSE convergence curves to verify the greedy strategy produces stable, reasonable partitions rather than collapsing to singleton or monolithic clusters.

3. **Position-Aware Domain Transfer**: Apply the framework to PubMed and WikiSum datasets, then compute correlation between positional importance and human-annotated salience. If early-position sentences are not systematically more important in these domains, the position-aware weighting may need adaptation or removal.