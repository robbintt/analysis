---
ver: rpa2
title: Confidence-Guided Error Correction for Disordered Speech Recognition
arxiv_id: '2509.25048'
source_url: https://arxiv.org/abs/2509.25048
tags:
- correction
- speech
- confidence
- error
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses error correction for automatic speech recognition
  (ASR) of disordered speech, where existing systems often overcorrect accurate outputs,
  harming performance. To mitigate this, the authors introduce confidence-guided prompting,
  incorporating word-level uncertainty estimates from entropy-based confidence scores
  into LLM fine-tuning.
---

# Confidence-Guided Error Correction for Disordered Speech Recognition

## Quick Facts
- arXiv ID: 2509.25048
- Source URL: https://arxiv.org/abs/2509.25048
- Reference count: 0
- Reduces WER by 10% relative on spontaneous disordered speech through confidence-guided LLM correction

## Executive Summary
This paper addresses the problem of error correction in automatic speech recognition (ASR) for disordered speech, where standard systems often overcorrect accurate outputs, harming performance. The authors introduce confidence-guided prompting, which incorporates word-level uncertainty estimates derived from entropy-based confidence scores into LLM fine-tuning. By training LLaMA 3.1 to respect these confidence signals, the system learns to preserve high-confidence words while selectively correcting low-confidence regions. Evaluations demonstrate significant WER reductions on both spontaneous and scripted disordered speech datasets, with improved selective correction behavior compared to naive LLM approaches.

## Method Summary
The method embeds word-level confidence scores (derived from Tsallis or Gibbs entropy) into LLM prompts during fine-tuning. The ASR system first generates hypotheses with frame-level token probabilities, which are converted to word-level confidence scores using entropy measures and aggregation (mean/min/product). These scores are bracketed alongside words in instruction prompts for fine-tuning LLaMA 3.1 8B with LoRA adapters. During inference, the fine-tuned model uses these confidence signals to guide selective correction, preserving high-confidence words while correcting uncertain regions. The approach is evaluated across multiple datasets and ASR architectures.

## Key Results
- 10% relative WER reduction on spontaneous disordered speech (SAP-unshared)
- 47% relative WER reduction on TORGO dataset (from 10.83% to 10.58%)
- Reduced harmful corrections on high-confidence words (1.7% harmful vs 63.8% helpful on low-confidence utterances in SAP-shared)

## Why This Works (Mechanism)

### Mechanism 1
Embedding word-level confidence scores into LLM prompts guides selective correction toward uncertain ASR regions while preserving high-confidence words. The LLM receives explicit uncertainty signals (e.g., "RAFELLES[0.61]") alongside transcript tokens. During fine-tuning, the model learns a mapping between low confidence scores and higher correction probability, and conversely, high confidence and preservation behavior. This creates a conditional correction policy rather than uniform editing.

### Mechanism 2
Tsallis entropy provides better-calibrated confidence estimates than standard Gibbs entropy for modern ASR systems prone to overconfidence. Tsallis entropy introduces an entropic index α that controls sensitivity to distribution peakedness. Smaller α values penalize overconfident (peaked) distributions more heavily, producing confidence scores that better discriminate between genuine certainty and miscalibrated high probability.

### Mechanism 3
Confidence-informed fine-tuning reduces overcorrection by constraining the LLM's tendency to apply frequency-based or grammaticality-based edits to already-accurate transcripts. Naive LLMs bring linguistic priors (common words > rare words; grammatical > ungrammatical) that cause harmful edits when ASR is correct. By training with confidence annotations, the model learns to suppress these priors when confidence is high, overriding its default correction bias.

## Foundational Learning

- **Entropy-based confidence estimation**
  - Why needed here: Understanding how frame-level token distributions are converted to word-level uncertainty scores is essential for debugging calibration issues and selecting aggregation methods
  - Quick check question: Given a 5-frame word with confidence scores [0.95, 0.92, 0.45, 0.90, 0.88], what would be the difference between using mean vs. minimum aggregation?

- **LoRA (Low-Rank Adaptation) fine-tuning**
  - Why needed here: The paper uses LoRA for parameter-efficient adaptation of LLaMA 3.1 8B. Understanding rank (r) and scaling factor (α) hyperparameters is necessary for reproduction and experimentation
  - Quick check question: If LoRA rank is increased from 16 to 32, what happens to the number of trainable parameters and potential for overfitting on limited dysarthric speech data?

- **WER (Word Error Rate) decomposition**
  - Why needed here: The paper analyzes helpful vs. harmful corrections. Understanding that WER = (S + D + I) / N (substitutions, deletions, insertions, reference words) enables deeper error analysis beyond aggregate metrics
  - Quick check question: A correction changes "THE CAT SAT" (correct) to "THE CAT IS"—does this increase WER, and by how many operations?

## Architecture Onboarding

- Component map:
  - ASR Frontend (Parakeet TDT-CTC 110M or Whisper distil-large-v3.5)
  - Confidence Extractor (frame-level entropy computation → word-level aggregation)
  - Prompt Constructor (instruction template + ASR hypothesis with bracketed confidence scores)
  - Correction Model (LLaMA 3.1 8B Instruct with LoRA adapters)
  - Evaluation Pipeline (WER computation + helpful/harmful correction categorization)

- Critical path:
  1. Generate ASR hypotheses + frame-level logits on target dataset
  2. Compute entropy-based confidence per frame, aggregate to word level
  3. Format hypothesis with confidence annotations into instruction prompt
  4. Run inference through fine-tuned LLaMA model
  5. Compute WER against reference transcripts

- Design tradeoffs:
  - Tsallis α selection: Lower α (e.g., 0.3) increases sensitivity to peaked distributions but may underperform on already-well-calibrated ASR; higher α (e.g., 0.9) approaches Gibbs entropy
  - Aggregation method: Minimum highlights localized uncertainty (conservative), mean provides balanced view, product amplifies compounding uncertainty
  - Filtering vs. prompting: Word-level filtering (post hoc) is simpler but threshold-sensitive; confidence prompting (end-to-end) requires more training data but learns adaptive thresholds
  - ASR for training data: Using same ASR for training and evaluation may overfit to specific error patterns; using different ASR (as paper does with Whisper for evaluation) tests generalization

- Failure signatures:
  - Confidence miscalibration: Model over-corrects high-confidence words or under-corrects low-confidence ones—check calibration plots of confidence vs. actual accuracy
  - Threshold sensitivity: Large WER swings with small threshold changes in filtering approaches—suggests confidence distribution is not well-separated
  - Frequency bias persistence: Model still replaces rare correct words with common alternatives—confidence signal may be too weak or training data insufficient
  - Cross-dataset collapse: Performance degrades sharply on out-of-distribution datasets (e.g., TORGO)—may need domain adaptation or confidence recalibration

- First 3 experiments:
  1. Baseline replication: Reproduce Table 2 results on SAP-shared using the provided prompt format; verify naive LLM WER (~4.69%) vs. confidence-informed WER (~4.95%) to confirm the slight tradeoff on structured speech
  2. Confidence calibration analysis: Plot confidence score distributions for correct vs. incorrect words on a validation set; assess whether Tsallis α=0.5–0.9 provides better separation than Gibbs entropy
  3. Cross-ASR generalization test: Apply the Parakeet-trained correction model to Whisper outputs on SAP-unshared; compare WER reduction to assess whether confidence-guided prompting generalizes across ASR error patterns as claimed

## Open Questions the Paper Calls Out
None

## Limitations
- Confidence score calibration remains dataset- and ASR-dependent, requiring manual tuning of Tsallis entropy parameters without a principled selection framework
- Training data generation relies on a single ASR system (Parakeet), potentially overfitting to its specific error patterns despite cross-architecture evaluation with Whisper
- The method shows strong gains on spontaneous disordered speech but minimal improvement (0.2% absolute) on the structured SAP-shared subset, suggesting limited benefit when ASR outputs are already high quality

## Confidence

| Claim | Confidence Level |
|---|---|
| Embedding confidence scores into prompts guides selective correction | High |
| Tsallis entropy provides better-calibrated estimates than Gibbs entropy | Medium |
| Method generalizes across ASR architectures | Medium |

## Next Checks
1. Confidence Calibration Validation: Plot confidence score distributions against actual error rates on held-out validation sets to verify that Tsallis entropy provides better-calibrated uncertainty estimates than Gibbs entropy across different α values
2. Cross-Dataset Robustness Test: Evaluate the correction model on additional disordered speech datasets (e.g., UASpeech, C-MUDS) to assess whether performance gains transfer to different speaker populations and acoustic conditions
3. Ablation on Confidence Integration: Compare confidence-guided prompting against a baseline that filters corrections using confidence thresholds post-hoc to determine whether end-to-end integration provides measurable advantages over simpler approaches