---
ver: rpa2
title: Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks
arxiv_id: '2511.09114'
source_url: https://arxiv.org/abs/2511.09114
tags:
- agents
- terla
- network
- agent
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying deep reinforcement
  learning for autonomous cyber defence in real-world networks with varying topology
  and size, which typically requires retraining agents for each new network configuration.
  The authors propose Topological Extensions for Reinforcement Learning Agents (TERLA),
  which extend existing PPO-based agents with heterogeneous graph neural network layers
  to create a fixed-size latent embedding of the network state, and a reduced, semantically
  meaningful action space.
---

# Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks

## Quick Facts
- arXiv ID: 2511.09114
- Source URL: https://arxiv.org/abs/2511.09114
- Reference count: 26
- Primary result: A single TERLA agent can generalize across multiple network segments without retraining, achieving 69% defensive effectiveness versus 58% for separately trained agents

## Executive Summary
This paper tackles the challenge of applying deep reinforcement learning for autonomous cyber defence across real-world networks with varying topology and size. The authors propose Topological Extensions for Reinforcement Learning Agents (TERLA), which uses heterogeneous graph neural networks to create fixed-size network state embeddings and a reduced semantic action space. TERLA agents are evaluated in the Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4 environment, demonstrating that a single trained agent can effectively defend multiple network segments without retraining, while performing actions only 6-7% of the time versus 33% for baseline PPO agents.

## Method Summary
The TERLA approach extends a PPO-based agent with heterogeneous graph neural network layers (HGT) that convert variable-size network observations into a fixed-size latent embedding. The HGT encoder processes mission, subnet, and host nodes with typed edges, producing a 70-dimensional vector through two HGT layers with ReLU activation and global sum pooling. The action space is reduced from potentially hundreds of actions to 5 semantically meaningful options, with deterministic target resolution based on IDS event analysis. Training uses RLlib with PPO hyperparameters (γ=0.97, lr=1e-4, entropy=0.01, rollout=128, batch=2048, episode=500 steps, 500 iterations) and includes action waiting and shaped reward wrappers to improve learning stability.

## Key Results
- Separate TERLA agents retain defensive performance of vanilla PPO agents while reducing action frequency from 33% to 6-7% of timesteps
- A single TERLA agent deployed across multiple network segments achieves 69% defensive effectiveness versus 58% for separately trained TERLA agents
- TERLA maintains compatibility with standard RL interfaces while demonstrating superior generalization across different network topologies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Heterogeneous graph representations enable topology-agnostic state encoding
- **Mechanism:** Raw network observations are converted into a heterogeneous graph with mission, subnet, and host nodes, then processed by two HGT layers with ReLU activation to produce a fixed-size latent embedding regardless of network size
- **Core assumption:** Graph schema remains consistent across deployments; only instance counts vary
- **Evidence anchors:** Abstract mentions fixed-size latent embedding; Section IV-B describes HGT layers generating fixed-size reduced-dimensionality latent embedding

### Mechanism 2
- **Claim:** Semantic action abstraction reduces action space complexity while preserving defensive efficacy
- **Mechanism:** Instead of outputting actions for each (subnet, host) pair, TERLA agents select from 5 semantic actions, with deterministic resolver mapping to concrete targets based on IDS event analysis
- **Core assumption:** Target resolution logic correctly identifies priority hosts using only observable IDS information
- **Evidence anchors:** Abstract mentions reduced, fixed-size, semantically meaningful action space; Section IV-C describes reduced action space with post-selection targeting

### Mechanism 3
- **Claim:** Joint training of representation and policy layers enables task-aligned feature extraction
- **Mechanism:** HGT encoder and PPO policy/value heads are trained end-to-end using the same reward signal, allowing latent embedding to learn features specifically useful for defense decisions
- **Core assumption:** Shaped rewards sufficiently approximate true objective for learning transferable policies
- **Evidence anchors:** Section IV-B states joint training enables task-aligned feature extraction; Section V shows poor PPO performance without TERLA action targeting exploiting shaped rewards

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: TERLA extends a PPO-based agent; understanding policy gradient methods, clip ratios, and value functions is prerequisite to modifying the architecture
  - Quick check question: Can you explain why PPO uses a clipped objective and how the value function relates to advantage estimation?

- **Concept: Graph Neural Networks (GNNs) and attention mechanisms**
  - Why needed here: The HGT encoder uses type-specific attention; understanding message passing, node aggregation, and heterogeneous graphs is essential for debugging representation learning
  - Quick check question: How does a GNN aggregate information from neighboring nodes, and why does HGT use type-specific attention weights?

- **Concept: Partial observability in reinforcement learning**
  - Why needed here: CC4 agents only observe their local network segment via IDS events (with false positives/negatives); this constrains what policies can learn
  - Quick check question: Why might a POMDP require different algorithms than a fully observable MDP, and how does observation noise affect policy learning?

## Architecture Onboarding

- **Component map:**
  Environment (CC4) → Observation (variable-size arrays) → [Observation Conversion] → Heterogeneous Graph (mission/subnet/host nodes) → [HGT Encoder] → Latent Embedding (70-dim fixed vector) → [PPO Model] → Policy logits (5 actions) + Value estimate → [Action Conversion] → Environment Action (subnet, host, action_type) → Environment (execute action)

- **Critical path:** Observation → Graph conversion → HGT encoding → Policy selection → Action resolution. Errors in graph schema or resolution logic will propagate silently.

- **Design tradeoffs:**
  - Fixed vs. learnable action targeting: Current approach uses deterministic rules; a learned targeting module could adapt but adds complexity
  - Encoder layer sizing: Formula (host_features + actions) × 10 limits capacity but reduces tuning; larger networks may need adjustment
  - Observation conversion location: Converting in the model (vs. environment wrapper) causes GPU→CPU→GPU transfers under RLlib

- **Failure signatures:**
  - Agent never performs actions: Check action waiting logic or IDS event parsing
  - High action frequency with poor performance: Action conversion may be selecting invalid targets
  - Training divergence: Shaped rewards may conflict with episode termination conditions
  - Variable performance across segments: Encoder may not generalize; inspect latent embedding variance

- **First 3 experiments:**
  1. **Baseline replication:** Train separate TERLA agents on each CC4 segment using provided hyperparameters; verify convergence matches paper plots (~300K steps for single agent)
  2. **Ablation on action space:** Replace semantic action resolution with random target selection within the same action type to isolate the contribution of targeting logic
  3. **Topology stress test:** Deploy the single trained TERLA agent on a modified CC4 configuration with different subnet sizes (e.g., 8-host and 32-host segments) to probe generalization bounds

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can TERLA effectively mitigate the policy learning challenges inherent in the very large action spaces found in large-scale enterprise networks?
- **Basis in paper:** The authors recommend assessing "TERLA's ability to address the challenges associated with very large action spaces in large-scale networks" to mature the approach
- **Why unresolved:** The current evaluation was limited to network segments with a maximum of 48 hosts, which may not reflect the complexity of enterprise-scale networks
- **What evidence would resolve it:** Successful training and evaluation of TERLA agents in environments specifically designed to stress test scalability and large action spaces

### Open Question 2
- **Question:** How effectively does the TERLA architecture transfer from the simulated CAGE environment to emulated networks utilizing real Intrusion Detection Systems (IDS)?
- **Basis in paper:** The authors recommend "prototype deployment in an emulated network with a real IDS" to investigate interoperability and reduce the sim-to-real gap
- **Why unresolved:** The research relied entirely on the Cyber Operations Research Gym (CybORG), which simulates IDS events rather than interfacing with real hardware or software systems
- **What evidence would resolve it:** Demonstration of TERLA agents processing live IDS alerts within an emulated network environment without significant performance degradation

### Open Question 3
- **Question:** Does the application of TERLA extensions improve the generalisability and performance of non-PPO autonomous defence agents?
- **Basis in paper:** The authors recommend applying "TERLA to existing ACD agents from Dstl and DARPA research programmes" to evaluate benefits in independently developed architectures
- **Why unresolved:** The paper only validates the TERLA extensions on a standard Proximal Policy Optimisation (PPO) model
- **What evidence would resolve it:** Comparative analysis showing that TERLA maintains its benefits (fixed latent embedding, action efficiency) when applied to alternative deep reinforcement learning algorithms

## Limitations
- The HGT encoder architecture lacks specific details (attention heads, dropout rates, normalization), making exact reproduction challenging
- TERLA's action resolution relies on deterministic rules based on IDS event combinations, which may not generalize to novel attack patterns
- Shaped rewards (local segment health) were used during training but evaluation used original CC4 shared rewards; the alignment between these reward functions is not quantified

## Confidence
- **High confidence:** Separate TERLA agents retain defensive performance while improving action efficiency
- **Medium confidence:** A single TERLA agent generalizes across multiple network segments
- **Medium confidence:** TERLA agents are compatible with standard RL interfaces

## Next Checks
1. **Architectural replication verification:** Implement the HGT encoder with multiple attention head configurations and measure impact on defensive effectiveness and action efficiency to identify critical hyperparameters
2. **Reward alignment analysis:** Quantify the correlation between shaped rewards (used during training) and original CC4 shared rewards (used for evaluation) across multiple network segments
3. **Generalization stress test:** Deploy a single trained TERLA agent on networks with substantially different topologies (e.g., tree vs. mesh structures, varying host counts beyond CC4) to identify failure modes and generalization limits