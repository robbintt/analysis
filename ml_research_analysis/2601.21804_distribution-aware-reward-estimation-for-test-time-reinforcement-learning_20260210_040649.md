---
ver: rpa2
title: Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning
arxiv_id: '2601.21804'
source_url: https://arxiv.org/abs/2601.21804
tags:
- reward
- dare
- arxiv
- rollouts
- rollout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses reward estimation in test-time reinforcement
  learning (TTRL), where models must self-improve without ground-truth supervision.
  The key problem is that majority voting (MV) over rollouts loses information about
  non-majority but correct actions and introduces systematic bias, leading to confirmation
  collapse.
---

# Distribution-Aware Reward Estimation for Test-Time Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.21804
- Source URL: https://arxiv.org/abs/2601.21804
- Reference count: 22
- Primary result: DARE achieves 25.3% relative improvement on AIME 2024 and 5.3% on AMC compared to baseline TTRL methods

## Executive Summary
This paper addresses reward estimation in test-time reinforcement learning (TTRL), where models must self-improve without ground-truth supervision. The key problem is that majority voting (MV) over rollouts loses information about non-majority but correct actions and introduces systematic bias, leading to confirmation collapse. The authors propose Distribution-Aware Reward Estimation (DARE), which estimates rewards from the full empirical rollout distribution rather than a single majority outcome. DARE augments this with an exploration bonus to encourage rare but high-quality rollouts and distribution pruning to remove low-probability noisy samples. Extensive experiments show DARE improves optimization stability and performance over TTRL baselines, achieving 25.3% relative improvement on AIME 2024 and 5.3% on AMC. The method also demonstrates better out-of-distribution generalization and faster convergence. Theoretical analysis confirms MV's limitations in information preservation and bias under correlated rollouts.

## Method Summary
DARE replaces majority voting with distribution-based rewards in TTRL. For each query, M rollouts are sampled and their empirical frequencies and uncertainties computed. Rewards are assigned proportional to uncertainty-adjusted empirical probability, with an exploration bonus for rare but confident rollouts, followed by distribution pruning of low-probability samples. The final reward combines the distribution-based reward and exploration bonus, which is then used to update the policy via GRPO. This approach preserves information about non-majority correct answers while reducing noise from low-quality rollouts.

## Key Results
- 25.3% relative improvement on AIME 2024 benchmark compared to TTRL baselines
- 5.3% improvement on AMC benchmark with better optimization stability
- 2-5 point improvement in out-of-distribution generalization when trained on one benchmark and tested on another
- Theoretical proof that MV loses information about reward-relevant outputs under correlated rollouts

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Based Reward Preserves Marginal Information
- **Claim:** Replacing majority voting (MV) with distribution-based rewards reduces information loss and avoids latent-conditioned bias.
- **Mechanism:** MV collapses all rollouts into a single pseudo-label, discarding distributional structure. DARE assigns rewards proportional to uncertainty-adjusted empirical probability $\hat{p}(\hat{y}) = \frac{n(\hat{y})/(u(\hat{y}) + \epsilon)}{\sum n(\hat{y}')/(u(\hat{y}') + \epsilon)}$, preserving marginal probability information across all candidate answers.
- **Core assumption:** Higher-probability rollouts are more likely correct (standard self-training assumption); low-uncertainty traces indicate higher quality.
- **Evidence anchors:**
  - [abstract]: "MV reduces the rollout distribution into a single outcome, discarding information about non-majority but correct actions candidates"
  - [section 2.1]: Theorem 2.1 proves $I(R(Y); R_{MV}(Y; P)) \leq I(R(Y); Y)$ with strict inequality when multiple outputs have distinct rewards
  - [corpus]: Weak direct support; related TTRL papers (ETTRL, SPINE) address exploration-exploitation but don't analyze information loss formally
- **Break condition:** If rollouts are highly correlated or model confidence is poorly calibrated, empirical distribution may still reflect spurious patterns.

### Mechanism 2: Exploration Bonus Surfaces Rare Correct Actions
- **Claim:** An exploration bonus encourages the policy to reinforce low-frequency, low-uncertainty rollouts that MV would ignore.
- **Mechanism:** Bonus $b(y_i) = (1 - \frac{n(y_i)}{M}) \cdot (1 - u(y_i))$ assigns additional reward to rollouts that are (a) less frequent and (b) internally confident. This counteracts dominance of high-frequency modes without amplifying noisy rare samples.
- **Core assumption:** Non-majority but correct rollouts tend to exhibit lower token-level entropy (low uncertainty).
- **Evidence anchors:**
  - [section 3.2]: "many of these non-majority but factual correct rollouts tend to exhibit low uncertainty"
  - [figure 1a]: Visual example showing action D (correct, rare) receives boosted reward while E/F (noisy, rare) do not
  - [corpus]: SPINE (arXiv:2511.17938) uses entropy-band regularization for similar exploration control, but with different formulation
- **Break condition:** If correct rare answers have high uncertainty (e.g., complex reasoning with legitimate ambiguity), the bonus may underweight them.

### Mechanism 3: Distribution Pruning Reduces Reward Variance
- **Claim:** Removing extremely low-probability rollouts stabilizes policy updates by filtering noisy gradients.
- **Mechanism:** Pruning threshold $\tau$ removes rollouts with $\hat{p}(y_i) < \tau$, then renormalizes. This eliminates degenerate low-quality samples that would otherwise receive small but non-zero rewards and introduce gradient noise.
- **Core assumption:** Very low empirical probability correlates with low answer quality (not just model uncertainty).
- **Evidence anchors:**
  - [section 3.3]: "extremely low-probability rollouts may still introduce noise and destabilize optimization"
  - [figure 1b]: Shows pruning removes actions E/F (very low probability) while preserving meaningful distribution
  - [corpus]: No direct corpus evidence on distribution pruning in TTRL; this appears novel to DARE
- **Break condition:** If threshold $\tau$ is set too aggressively, legitimate rare-but-correct answers may be pruned; if too loose, noise persists.

## Foundational Learning

- **Concept: Test-Time Reinforcement Learning (TTRL)**
  - **Why needed here:** DARE is a method *within* TTRL; you must understand that TTRL adapts models using only self-generated rollouts without ground-truth labels.
  - **Quick check question:** Can you explain why TTRL needs pseudo-rewards instead of true rewards, and what role majority voting plays in baseline TTRL?

- **Concept: Information Theory (Mutual Information, Data Processing Inequality)**
  - **Why needed here:** Theoretical justification relies on Theorem 2.1, which uses mutual information to prove MV is information-losing.
  - **Quick check question:** Why does $I(R(Y); R_{MV}) < I(R(Y); Y)$ imply that MV discards reward-relevant information?

- **Concept: Policy Gradient Methods (GRPO)**
  - **Why needed here:** DARE computes rewards but relies on GRPO for policy updates; understanding how rewards translate to gradients is essential.
  - **Quick check question:** How does a shaped reward $r(y_i) = \tilde{p}(y_i) + \alpha \tilde{b}(y_i)$ influence the policy gradient update direction?

## Architecture Onboarding

- **Component map:** Rollout Sampling -> Uncertainty-Aware Distribution -> Distribution Pruning -> Exploration Bonus -> Final Reward -> Policy Update
- **Critical path:** Steps 2–4 are the DARE-specific logic. Errors in uncertainty computation (Eq. 9) or pruning threshold selection will propagate directly to reward quality and optimization stability.

- **Design tradeoffs:**
  - **Higher $\alpha$ (exploration bonus weight):** More exploration of rare answers, but risk of reinforcing spurious low-frequency noise if uncertainty estimates are poor.
  - **Higher $\tau$ (pruning threshold):** Cleaner reward signal, but may discard valid minority answers; empirical results suggest modest $\tau$ works best.
  - **More rollouts M:** Better distribution estimation, but linear compute cost increase.

- **Failure signatures:**
  - **Confirmation collapse:** Accuracy plateaus or degrades early; reward distribution collapses to single mode. Check if $\alpha$ is too low or pruning is too aggressive.
  - **High variance gradients:** Loss oscillates wildly. Check if $\tau$ is too low (retaining noisy rollouts).
  - **OOD generalization drop:** Model overfits to adaptation benchmark. Check if distribution-based rewards are being computed correctly (not falling back to MV).

- **First 3 experiments:**
  1. **Reproduce ablation (Table 2):** Start from raw model, add distribution reward only, then bonus, then pruning. Confirm each component adds incremental gain on AIME/AMC.
  2. **Correlation sensitivity (Figure 5):** Vary sampling temperature to control rollout correlation. Verify DARE degrades gracefully while TTRL drops sharply at high correlation.
  3. **OOD transfer (Figures 3–4):** Train on one benchmark (e.g., AMC), evaluate on another (e.g., MATH-500). Confirm DARE improves over TTRL by 2–5 points on unseen data.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can "richer distributional designs" beyond simple empirical distributions further enhance test-time reinforcement learning performance?
- **Basis in paper:** [explicit] The conclusion states that the results "open avenues for richer distributional designs and uncertainty-aware reward estimation in the future."
- **Why unresolved:** The current work relies on an empirical distribution derived from frequency and uncertainty; it does not explore parametric, hierarchical, or latent variable models for the reward distribution.
- **What evidence would resolve it:** Implementing DARE with parametric distribution approximations (e.g., Gaussian mixtures over embeddings) and comparing optimization stability against the empirical method.

### Open Question 2
- **Question:** Does the relative benefit of DARE diminish or persist when scaling to models significantly larger than 7B parameters?
- **Basis in paper:** [inferred] The appendix notes "diminishing marginal gains for simple reward shaping" on 4B and 7B models, suggesting uncertainty about scaling behavior.
- **Why unresolved:** Experiments were limited to 1.5B through 7B models; it is unclear if larger models naturally resolve distribution bias or if the method's complexity becomes redundant at scale.
- **What evidence would resolve it:** Benchmarking DARE on frontier-scale models (e.g., 70B+) to determine if the "confirmation collapse" problem persists strongly enough to require distribution-aware rewards.

### Open Question 3
- **Question:** Can a rigorous statistical dependence measure replace the token-overlap proxy to more accurately correct correlation-induced bias?
- **Basis in paper:** [inferred] Appendix D acknowledges that "directly estimating statistical dependence... is intractable," leading to the use of a simple token-overlap proxy.
- **Why unresolved:** Token overlap serves as a heuristic for mode collapse but may fail to capture semantic or logical correlations where distinct tokens still represent highly dependent reasoning paths.
- **What evidence would resolve it:** Developing a tractable semantic dependence metric and demonstrating that it reduces the variance of reward estimates more effectively than the token-level proxy.

## Limitations

- The theoretical analysis assumes idealized conditions (discrete outputs, perfect uncertainty calibration) that don't fully translate to practical LLM rollouts with continuous token probabilities.
- The number of rollouts M per query is unspecified, making it difficult to assess computational efficiency and practical deployment costs.
- The exploration bonus mechanism's core assumption that low-uncertainty correlates with correctness needs further validation across diverse reasoning tasks and ambiguity levels.

## Confidence

- **High confidence:** The core mechanism of replacing majority voting with distribution-based rewards is well-justified both theoretically (information preservation) and empirically (25.3% relative improvement on AIME 2024). The architecture and implementation details are clearly specified.
- **Medium confidence:** The exploration bonus and distribution pruning components show consistent improvements in ablations, but their individual contributions are harder to isolate. The assumption that low-uncertainty rollouts are more likely correct needs further validation across diverse reasoning tasks.
- **Low confidence:** The theoretical guarantees assume idealized conditions that don't fully translate to practical LLM rollouts. The relationship between empirical distribution and true answer quality may be more complex than the analysis suggests.

## Next Checks

1. **Correlation sensitivity analysis:** Systematically vary rollout correlation (via sampling temperature or model parameters) and measure how DARE's performance degrades compared to MV-based TTRL. This validates whether the information-theoretic advantages hold under realistic conditions.

2. **Uncertainty calibration study:** Evaluate whether low-uncertainty rollouts actually correlate with correctness across different task types and reasoning depths. This checks the foundational assumption behind the exploration bonus mechanism.

3. **Computational efficiency benchmark:** Measure the wall-clock time and memory usage of DARE versus baseline TTRL across different values of M. This quantifies the practical cost of the improved performance and helps identify optimal rollout counts.