---
ver: rpa2
title: 'CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning'
arxiv_id: '2510.06243'
source_url: https://arxiv.org/abs/2510.06243
tags:
- referring
- arxiv
- data
- grounding
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving Referring Expression
  tasks, which require models to ground objects in images based on complex natural
  language descriptions. The authors propose CoT Referring (CoTR), a novel approach
  that enhances model reasoning by structuring referring expressions into a sequential
  chain-of-thought format.
---

# CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning

## Quick Facts
- arXiv ID: 2510.06243
- Source URL: https://arxiv.org/abs/2510.06243
- Reference count: 31
- Primary result: 2.5%+ accuracy gain on complex referring expressions

## Executive Summary
CoT Referring (CoTR) introduces a chain-of-thought framework to improve referring expression tasks by decomposing complex language descriptions into sequential grounding steps. The approach generates synthetic reasoning data, trains a multi-task model (RefLM) with adaptive loss weighting, and evaluates on both standard and composite benchmarks. Results show consistent improvements in accuracy, particularly for expressions requiring multi-hop reasoning. The method bridges explicit reasoning with visual grounding to enhance model interpretability and performance.

## Method Summary
The CoTR framework transforms complex referring expressions into sequential reasoning chains using a teacher LLM. It generates synthetic training data by decomposing queries into anchor nouns, grounding each intermediate step before identifying the target object. The RefLM model integrates detection and segmentation via a vision encoder, spatial encoder, and multi-task decoder with weighted losses for box, mask, and point predictions. Training uses adaptive loss weighting and data augmentation to improve robustness. The Composite Referring Benchmark evaluates performance on challenging expressions requiring compositional reasoning.

## Key Results
- RefLM trained on CoTR data achieves 2.5%+ accuracy gains over baselines on standard RefCOCO/+/g datasets
- Significant improvements on the Composite Referring Benchmark for multi-hop reasoning expressions
- Ablation studies confirm the effectiveness of chain-of-thought reasoning and adaptive loss weighting
- Performance degrades on simple, single-noun expressions when using CoTR format

## Why This Works (Mechanism)
The chain-of-thought approach breaks complex reasoning into manageable intermediate steps, allowing the model to ground each noun before proceeding. This explicit reasoning path mirrors human problem-solving and provides intermediate supervision signals. The adaptive loss weighting ensures balanced learning across detection and segmentation tasks, preventing task dominance. The synthetic data generation creates diverse reasoning patterns that improve generalization to complex expressions.

## Foundational Learning
- Chain-of-thought reasoning: Needed to decompose complex queries into sequential grounding steps. Quick check: Does the model correctly identify intermediate anchors before the target?
- Multi-task learning with adaptive loss: Required to balance detection and segmentation objectives. Quick check: Are box and mask losses converging at similar rates?
- Synthetic data generation: Essential for creating reasoning chains without manual annotation. Quick check: Do generated chains follow logical grounding sequences?
- Visual compositionality: Core capability for combining multiple visual elements. Quick check: Can the model locate targets when intermediate anchors are visually ambiguous?
- Spatial reasoning: Critical for understanding relative object positions. Quick check: Does the model correctly interpret "left of" or "between" spatial relationships?

## Architecture Onboarding

Component Map:
Vision Encoder -> Spatial Encoder -> Multi-task Decoder -> Box/Mask/Point Outputs

Critical Path:
Input image → Vision encoder → Spatial features → Chain-of-thought reasoning → Sequential grounding → Final target prediction

Design Tradeoffs:
- Sequential reasoning vs. end-to-end prediction: CoTR trades computational efficiency for improved accuracy on complex expressions
- Synthetic data quality vs. annotation cost: Generated data enables large-scale training but may introduce reasoning artifacts
- Task balancing vs. performance: Adaptive loss weighting prevents task dominance but adds hyperparameter complexity

Failure Signatures:
- Single noun case failures: Model misses targets in simple expressions despite CoTR format
- Anchor hallucination: Model follows textual reasoning script without verifying visual existence of intermediate objects
- Point prediction instability: Performance degrades at higher point counts due to SAM decoder limitations

First 3 Experiments:
1. Compare standard SFT vs. CoTR SFT on strictly single-noun expressions to quantify performance drop
2. Test adaptive sampling vs. fixed heuristic for SAM point prompts across different image complexities
3. Evaluate counterfactual images where visual anchors are altered to test reasoning vs. pattern matching

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the introduction of structured CoT reasoning degrade performance on simple, single-noun referring expressions?
- Basis in paper: [inferred] Figure 9 explicitly identifies "single noun case failures" as a distinct error mode, noting the model fails to detect the target even when the input is ostensibly simpler than multi-hop queries.
- Why unresolved: The authors visualize the failure but do not analyze if this is due to catastrophic forgetting of the pre-training distribution or over-constraint by the reasoning template.
- What evidence would resolve it: An ablation study comparing standard SFT versus CoTR SFT on a filtered dataset of strictly single-noun expressions to quantify the performance drop.

### Open Question 2
- Question: Can the reliance on heuristics for SAM point prompts be replaced with a dynamic mechanism to handle non-monotonic performance?
- Basis in paper: [inferred] Table 3(c) shows performance peaks at 15 points and drops at 25 points. The authors attribute this to "SAM's properties" and use a fixed heuristic without solving the instability.
- Why unresolved: It is unclear if the decline at higher point counts is due to noise in the predicted points or a fundamental limitation of the SAM decoder architecture.
- What evidence would resolve it: Testing an adaptive sampling strategy or a learned point-prediction head that conditions the number of points on image complexity.

### Open Question 3
- Question: To what extent does the model learn visual compositionality versus simply mimicking the textual reasoning patterns of the teacher LLM?
- Basis in paper: [inferred] Section 3.1.2 relies on Qwen3 to rewrite expressions. There is a risk the student model learns the "First locate..." syntax without verifying the visual existence of the intermediate anchors.
- Why unresolved: Standard IoU metrics validate the final box but do not measure if the reasoning *path* is visually grounded or hallucinated based on language priors.
- What evidence would resolve it: Evaluation on counterfactual images where visual anchors are altered or removed to test if the model blindly follows the textual script or adapts to the visual context.

## Limitations
- Performance degradation on simple, single-noun referring expressions when using CoTR format
- Reliance on heuristics for SAM point prompts introduces non-monotonic performance
- Limited evaluation of cross-lingual generalization for the CoTR data generation pipeline
- Potential for model to mimic textual reasoning patterns without genuine visual grounding

## Confidence
- High confidence: 2.5%+ accuracy improvement on Composite Referring Benchmark and standard datasets
- Medium confidence: Chain-of-thought format as primary driver of performance gains (limited ablation isolation)
- Medium confidence: Optimal integration of detection and segmentation in RefLM (limited comparative analysis)

## Next Checks
1. Test the CoTR pipeline and RefLM on multilingual referring expression datasets to assess cross-lingual robustness
2. Conduct ablation studies specifically isolating the impact of the chain-of-thought format from other model changes
3. Evaluate RefLM's performance and computational efficiency on out-of-domain images, such as medical or satellite imagery