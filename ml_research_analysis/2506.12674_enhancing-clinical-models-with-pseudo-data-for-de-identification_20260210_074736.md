---
ver: rpa2
title: Enhancing Clinical Models with Pseudo Data for De-identification
arxiv_id: '2506.12674'
source_url: https://arxiv.org/abs/2506.12674
tags:
- pseudo
- masked
- roberta
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the impact of training large language models
  on de-identified clinical text containing masked protected health information. The
  authors hypothesize that masked tokens negatively affect model performance and propose
  pretraining on pseudo datasets where masked entities are replaced with realistic
  generated text.
---

# Enhancing Clinical Models with Pseudo Data for De-identification

## Quick Facts
- **arXiv ID**: 2506.12674
- **Source URL**: https://arxiv.org/abs/2506.12674
- **Reference count**: 13
- **One-line primary result**: Pretraining on pseudo datasets with realistic generated text significantly improves clinical de-identification performance compared to masked pretraining, achieving 99.883 F1 versus 99.848 F1.

## Executive Summary
This study investigates whether pretraining clinical language models on de-identified text containing masked protected health information (PHI) negatively impacts performance compared to pretraining on realistic pseudo-text replacements. The authors create pseudo datasets by replacing masked PHI entities in MIMIC-III clinical notes with generated realistic text from gazetteers and LLM-based generators. Multiple encoder-only models (RoBERTa and XLM-RoBERTa in base and large variants) are pretrained on both masked and pseudo datasets, then fine-tuned for de-identification on the i2b2 corpus. Results show that models pretrained on pseudo data significantly outperform those trained on masked data, with the best pseudo RoBERTa large model achieving 99.883 F1 and surpassing previous baselines by 1.5 F1 points. XLM-RoBERTa pseudo models show consistent improvements across labels, while RoBERTa large pseudo models had some degradation in location and name classification, highlighting dataset quality sensitivity.

## Method Summary
The study creates two pretraining datasets from MIMIC-III clinical notes: a "masked" version retaining the original redaction syntax ([**Last Name**]) and a "pseudo" version where masked entities are replaced with realistic generated text from gazetteers and LLM-based generators. Four model architectures (RoBERTa base/large and XLM-RoBERTa base/large) are pretrained on both datasets using masked language modeling objectives. All models are then fine-tuned on the 2014 i2b2/UTHealth de-identification dataset with 296 patients and 1,304 records, training on 23 original labels but evaluating on 8 HIPAA recategorized labels. The primary evaluation metric is token-level F1 score, with comparisons to baseline models and analysis of entity-specific performance patterns.

## Key Results
- Pseudo RoBERTa large model achieves 99.883 F1, surpassing masked pretraining (99.848 F1) and previous baselines by 1.5 F1 points
- XLM-RoBERTa pseudo models consistently outperform masked models across all labels except AGE, showing multilingual robustness to pseudo data noise
- Pseudo data degrades performance for location and name entities due to low overlap between pseudo gazetteers and test set entities (only 7.2% hospital name overlap)
- Masked pretraining yields significantly better F1 scores (up to 98.4) than off-the-shelf models, validating the effectiveness of masked pretraining for de-identification

## Why This Works (Mechanism)

### Mechanism 1
Pretraining on redacted (masked) clinical text creates highly specialized representations for recognizing Protected Health Information (PHI), often outperforming pseudo-data replacements. The model learns to identify the unique syntactic signatures of mask tokens (e.g., [**Last Name**]) as distinct entities. By treating these masks as consistent vocabulary items during pretraining, the model effectively learns a "fingerprint" of what redacted data looks like, which aligns perfectly with the downstream task of identifying where PHI exists or how it is structured.

### Mechanism 2
Replacing masks with "pseudo" text can degrade performance for specific entities (Location, Name) if the pseudo data distribution lacks overlap with the evaluation set. Pseudo replacement introduces a distribution shift. If the pseudo generator samples from a list (e.g., "Dekalb Regional") that has low lexical overlap with the gold standard test set entities, the model overfits to the pseudo vocabulary. This leads to false positives when the test set contains names or locations not seen in the pseudo sampling list but semantically similar to other terms.

### Mechanism 3
Multilingual architectures (XLM-RoBERTa) are more robust to the noise introduced by synthetic pseudo data than monolingual models. Multilingual models are pre-trained on diverse linguistic patterns and orthography across languages. This forces the model to learn more generalizable morphological and contextual features rather than overfitting to English-specific name patterns, making it more adaptable to the "lower entropy" or synthetic nature of the pseudo data replacements.

## Foundational Learning

- **Concept**: **De-identification (De-ID) vs. Anonymization**
  - **Why needed here**: The paper distinguishes between "masked" (redaction) and "pseudo" (replacement). Understanding that *de-identification* often leaves syntactic clues (masks) while *anonymization* aims to remove them is crucial for interpreting why masked pretraining works so well.
  - **Quick check question**: Does the model learn to identify the *concept* of a name, or does it learn to identify the syntax [**Name**]?

- **Concept**: **Encoder-only Architectures (RoBERTa)**
  - **Why needed here**: The study focuses on BERT-style encoders for token classification, not generative LLMs. The mechanism relies on bidirectional context to classify tokens, which differs from causal generation.
  - **Quick check question**: Why is a bidirectional encoder (like RoBERTa) preferred for PHI tagging over a unidirectional decoder?

- **Concept**: **Entity Linking vs. Named Entity Recognition (NER)**
  - **Why needed here**: The paper hypothesizes that redacted text hurts entity linking. Understanding that NER finds the span (the "where") while Linking maps it to a knowledge base (the "who/what") clarifies why pseudo data might help Linking (semantic meaning) but hurt NER (lexical mismatch).
  - **Quick check question**: If I replace "Dr. Smith" with "Dr. [**Last Name**]", which task becomes impossible: recognizing there is a doctor (NER) or knowing it refers to Dr. Smith of cardiology (Linking)?

## Architecture Onboarding

- **Component map**: MIMIC-III Clinical Notes -> Regex parser to identify [**Mask Tokens**] -> Pseudo Database (Gazetteers) for replacement -> RoBERTa / XLM-RoBERTa (Base/Large) -> Fully connected linear layer for token classification (PHI tags) -> Token labels (e.g., DATE, DOCTOR, LOCATION)

- **Critical path**: The **Dataset Generation** phase is the highest risk. As noted in Section 7, low overlap between the "Pseudo List" (e.g., hospital names) and the "Test Set" directly causes failure. Ensuring high coverage in the pseudo database is critical.

- **Design tradeoffs**:
  - **Masked Pretraining**: High privacy (no real data), high performance on recognition, potentially poor on semantic linking. **Use this for pure De-ID**.
  - **Pseudo Pretraining**: More natural text flow, potentially better for downstream reasoning tasks, but requires exhaustive lists to avoid entity-specific degradation. **Use this for diverse entity coverage**.

- **Failure signatures**:
  - **High False Positives on Locations**: The model predicts "HOSPITAL" for common medical terms (e.g., "Medtronic", "TSH") when trained on Pseudo data with sparse location lists (Section 7).
  - **Name/Location Confusion**: Pseudo models mislabeling names as locations 23x more often due to lack of contextual clarity in generated names.

- **First 3 experiments**:
  1. **Baseline Validation**: Fine-tune a standard BioClinicalBERT on the 2014 i2b2 dataset to establish the performance floor (approx 98.4 F1 is achievable).
  2. **Data Leakage Check**: Compute the Jaccard similarity or intersection overlap between your Pseudo Entity Lists and your Test Set entities. If < 10%, expect degradation in that category.
  3. **Ablation on Mask Syntax**: Train one model on raw MIMIC-III (with [**Tags**]) and one with pseudo replacements. Compare specifically on the LOCATION and NAME entity classes to see if the pseudo list is sufficient.

## Open Questions the Paper Calls Out
- **Question**: Do the observed benefits of pseudo-data pretraining transfer to modern decoder-only Large Language Models (LLMs), or are they limited to encoder-only architectures?
  - **Basis in paper**: [explicit] The conclusion states: "Subsequent work includes self-supervised training LLMs on our datasets and supervised fine-tuning new clinical task models."
  - **Why unresolved**: The study exclusively evaluates encoder-only models (RoBERTa and XLM-RoBERTa); it does not test generative decoder architectures.
  - **What evidence would resolve it**: Pretraining generative LLMs on the masked vs. pseudo datasets and comparing downstream performance on clinical tasks.

- **Question**: Can the performance gap in Location and Name entities be closed by improving the domain-specific quality and overlap of the pseudo-data gazetteers?
  - **Basis in paper**: [inferred] The authors note that the pseudo model suffered from false positives due to a low intersection (7.2%) between the pseudo hospital list and the test set entities.
  - **Why unresolved**: The paper identifies the low overlap as a likely cause for error but does not experimentally validate if using higher-quality, domain-matched lists fixes the degradation.
  - **What evidence would resolve it**: An ablation study using pseudo lists curated specifically to match the statistical distribution of the target evaluation corpus.

- **Question**: Does pretraining on pseudo-data provide advantages over masked data for clinical tasks other than de-identification, such as relation extraction or clinical outcome prediction?
  - **Basis in paper**: [inferred] The introduction hypothesizes that "other tasks, such as named entity recognition and entity linking, are also negatively affected" by redacted text, and the limitations section notes results could vary across "fine-tuned tasks."
  - **Why unresolved**: The experimental scope is restricted solely to the de-identification task (token classification).
  - **What evidence would resolve it**: Fine-tuning the masked and pseudo checkpoints on a suite of diverse clinical benchmarks (e.g., MedNLI, relation extraction) to measure generalizability.

## Limitations
- The evaluation is limited to a single de-identification dataset (i2b2 2014), which may not generalize to other clinical text domains or annotation schemes.
- Pseudo data generation relies heavily on static gazetteers and simple random sampling, which may inadequately capture the full distribution of clinical entity names and locations.
- The study does not address temporal generalization—models are evaluated on data from the same timeframe as the training corpus.

## Confidence
- **High Confidence**: Core finding that pretraining on masked data significantly improves de-identification performance compared to off-the-shelf models, with consistent F1 improvements across multiple model variants.
- **Medium Confidence**: Mechanism explaining why masked pretraining works—learning syntactic signatures of PHI rather than semantic content—is plausible but not directly validated through ablation studies.
- **Low Confidence**: Hypothesis that multilingual pretraining provides robustness specifically through exposure to foreign names and romance languages is speculative without direct evidence linking multilingual pretraining objectives to improved handling of synthetic English clinical entities.

## Next Checks
1. **Generalization Test**: Evaluate the best-performing models (masked and pseudo RoBERTa large) on an independent clinical de-identification dataset such as the 2016 i2b2/UTHealth dataset or the Nursing Notes corpus to assess whether the performance gains transfer beyond the original evaluation set.

2. **Gazetteer Coverage Analysis**: Systematically measure the overlap between the pseudo entity lists and the actual entity distributions in the i2b2 test set across all entity types, then retrain pseudo models with augmented gazetteers to quantify the relationship between coverage and performance degradation.

3. **Masked vs. Pseudo Ablation on Downstream Tasks**: Fine-tune the masked and pseudo pretrained models on a clinical information extraction task that requires semantic understanding of PHI (such as entity linking or relation extraction) to test whether the hypothesized advantages of pseudo data for semantic tasks actually materialize in practice.