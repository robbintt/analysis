---
ver: rpa2
title: PARAM-1 BharatGen 2.9B Model
arxiv_id: '2507.13390'
source_url: https://arxiv.org/abs/2507.13390
tags:
- language
- training
- data
- hindi
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PARAM-1 is a 2.9B parameter bilingual Hindi-English language model
  designed to address the underrepresentation of Indian languages in LLMs. The model
  is trained from scratch with 25% Hindi corpus allocation and a custom tokenizer
  optimized for Indic morphologies.
---

# PARAM-1 BharatGen 2.9B Model

## Quick Facts
- arXiv ID: 2507.13390
- Source URL: https://arxiv.org/abs/2507.13390
- Authors: Kundeshwar Pundalik, Piyush Sawarkar, Nihar Sahoo, Abhishek Shinde, Prateek Chanda, Vedant Goswami, Ajay Nagpal, Atul Singh, Viraj Thakur, Vijay Dewane, Aamod Thakur, Bhargav Patel, Smita Gautam, Bhagwan Panditi, Shyam Pawar, Madhav Kotcha, Suraj Racha, Saral Sureka, Pankaj Singh, Rishi Bal, Rohit Saluja, Ganesh Ramakrishnan
- Reference count: 40
- Primary result: 2.9B bilingual Hindi-English model achieving strong performance on both English and Hindi benchmarks while maintaining low toxicity

## Executive Summary
PARAM-1 is a 2.9B parameter bilingual Hindi-English language model designed to address the underrepresentation of Indian languages in LLMs. The model is trained from scratch with 25% Hindi corpus allocation and a custom tokenizer optimized for Indic morphologies. It achieves strong performance on English benchmarks (e.g., 52.9% few-shot on ARC-Challenge, 71.4% on HellaSwag) and excels on Indic-specific tasks, outperforming larger multilingual models on MMLU-Hindi (36.1%) and MILU (48.3% Hindi, 49.7% English). Prometheus-Eval scores show superior fluency and coherence, especially in Hindi. Toxicity assessments confirm low harmful content generation. PARAM-1 demonstrates that culturally grounded, architecture-first design can deliver both general-purpose competence and domain-specific excellence for Indian languages.

## Method Summary
PARAM-1 was trained from scratch using a three-phase pretraining strategy on a bilingual Hindi-English corpus (25% Hindi, 75% English). The model uses a custom 128K vocabulary SentencePiece tokenizer optimized for Indic languages, a decoder-only transformer architecture with 2.9B parameters, and Grouped-Query Attention. Training was conducted on 64 nodes Ã— 8 H100s using NeMo + Megatron, followed by SFT alignment on 1M instruction-response pairs. The three-phase approach includes bootstrap training (general language competence), factual preservation (high-quality parallel corpora for knowledge retention), and long-context adaptation (extended reasoning capabilities).

## Key Results
- Achieves 52.9% few-shot accuracy on ARC-Challenge and 71.4% on HellaSwag
- Outperforms larger multilingual models on MMLU-Hindi (36.1%) and MILU (48.3% Hindi, 49.7% English)
- Shows superior fluency and coherence in Hindi according to Prometheus-Eval
- Maintains low toxicity levels according to Toxigen assessments

## Why This Works (Mechanism)

### Mechanism 1: Tokenizer Optimization for Indic Languages
A custom SentencePiece BPE tokenizer with a 128K vocabulary, trained on Indic text, reduces token fragmentation for Indian languages compared to standard English-centric tokenizers, leading to more efficient learning. By including all unique script symbols and using a large vocabulary, the model represents Indic words with fewer tokens (lower fertility score). This compresses the input sequence, allowing the self-attention mechanism to process more information within the same context window and reducing the computational steps required for language modeling. The core assumption is that lower token fertility directly correlates with improved model performance and faster convergence for Indic languages. Evidence shows BharatGen-128K v1 achieves lower fertility scores (e.g., Hindi: 1.43) than LLaMA (Hindi: 2.65) and Qwen (Hindi: 4.66). A break condition would be if the model's training data contained significant noise or inconsistencies in the Indic script encoding, preventing efficiency gains from translating to improved downstream performance.

### Mechanism 2: Data Composition and Equitable Representation
Allocating 25% of the pretraining corpus to Hindi ensures robust cross-lingual transfer and strong performance on Hindi-specific tasks without sacrificing English proficiency. A bilingual corpus (Hindi-English) forces the model to learn shared representations across two diverse language families (Indo-Aryan and Germanic). This "design-first" approach builds linguistic and cultural knowledge into the model's foundational weights, rather than relying on post-hoc fine-tuning to inject it later. The core assumption is that a 25% Hindi data ratio is the optimal balance for achieving bilingual parity. Evidence shows the 5 trillion-token dataset (3.48T English, 1.52T Hindi) was carefully curated to achieve this balance. A break condition would be if the model shows weaker performance on low-resource Indian languages not included in the primary training mix (e.g., Dravidian languages other than those with some cross-lingual transfer from Hindi).

### Mechanism 3: Phased Pretraining for Factual and Contextual Grounding
A three-phase training strategy (Bootstrap, Factual Preservation, Long-Context) allows the model to acquire general language competence before specializing in fact retention and extended reasoning. The curriculum first establishes broad syntactic understanding (Phase 1), then focuses the model on high-quality, fact-rich parallel corpora to improve knowledge retention (Phase 2), and finally exposes it to longer document sequences to enhance its ability to handle multi-turn or document-level interactions (Phase 3). The core assumption is that factual knowledge is most effectively learned in a dedicated, second training phase using curated parallel data. Evidence shows Phase 2 focuses on 2T tokens with a 50/50 split between Hindi and English. A break condition would be if the model still struggles with hallucinations on topics underrepresented in the "fact-rich" Phase 2 data.

## Foundational Learning

- **Concept: Tokenizer Fertility**
  - Why needed here: This is the core metric the paper uses to justify its custom tokenizer. Understanding it is critical to evaluating the model's efficiency claim.
  - Quick check question: If a model's tokenizer has a high fertility score for a language, what does that imply about its sequence length for a given sentence?

- **Concept: Phase-2 Curriculum Learning (Factual Preservation)**
  - Why needed here: The paper claims this is how the model addresses factual recall issues. It's a key part of their proposed solution for improving LLM reliability.
  - Quick check question: What is the primary data composition change from Phase 1 to Phase 2 intended to achieve?

- **Concept: Cross-Lingual Transfer Learning**
  - Why needed here: PARAM-1 is a bilingual model. Its performance relies on the principle that knowledge learned from the large English corpus can be applied to Hindi tasks, and vice-versa.
  - Quick check question: Why would training on a high-resource language (English) alongside a lower-resource one (Hindi) potentially improve performance on tasks in the lower-resource language?

## Architecture Onboarding

- **Component map**:
  Tokenizer (SentencePiece BPE, 128K vocab) -> Model Core (Decoder-only Transformer, 2.9B parameters) -> Attention (GQA with RoPE) -> Activation (fast-swiglu) -> Data Pipeline (NVIDIA NeMo Curator)

- **Critical path**:
  1. Data Curation: The most critical and time-consuming step. The paper emphasizes the manual and algorithmic effort to create the 5T token dataset.
  2. Tokenizer Training: Must be trained on a representative sample of the final data mixture to achieve optimal fertility scores.
  3. Three-Phase Training: Executing the training job in the specified sequence (Bootstrap -> Factual -> Long-Context) is central to their methodology.
  4. Post-Training Alignment: The SFT phase on the curated 1M instruction-response dataset is key for real-world usability.

- **Design tradeoffs**:
  - Text-Only Focus: The paper explicitly states code and math were removed. This makes the model unsuitable for coding or advanced mathematical reasoning tasks.
  - Bilingual vs. Multilingual: Focusing on only Hindi and English allowed for deep representation but may limit out-of-the-box utility for other Indian languages compared to a more broadly multilingual model.
  - Tokenizer Choice: While the custom tokenizer improves Indic efficiency, it breaks compatibility with the vast ecosystem of tools built around LLaMA or GPT tokenizers.

- **Failure signatures**:
  - Hallucination on Non-Curated Topics: The model may still confidently generate incorrect information for topics not covered in its "fact-rich" Phase 2 corpus.
  - Code/Math Refusal: Due to the explicit removal of code/math data, the model will likely fail or refuse coding and complex math queries.
  - Language Drift: The model might struggle with sustained conversation in Hinglish or other Indian languages not well-represented in the training data.

- **First 3 experiments**:
  1. Tokenizer Ablation: Compare the performance of the PARAM-1 tokenizer against the Nemotron tokenizer on a fixed Hindi dataset to quantify the efficiency gain (token count per document and training speed).
  2. Factual Recall Benchmark: Create a held-out set of factual questions based *only* on the Phase 2 corpus to measure the direct impact of the "Factual Preservation" phase.
  3. Cross-Lingual Transfer Test: Evaluate the model's zero-shot performance on a downstream task in a low-resource Indian language (e.g., Tamil) to assess the limits of its cross-lingual capabilities beyond its primary Hindi-English training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the design-first, equitable corpus allocation approach scale effectively to models with significantly larger parameter counts (e.g., 7B, 70B)?
- Basis in paper: [inferred] The authors position PARAM-1 (2.9B) as a "proof-of-concept" and "blueprint for equitable foundation modeling," but all experiments are conducted at a single, relatively small scale. Scaling behavior remains unexplored.
- Why unresolved: The paper does not report experiments at other scales, leaving open whether the 25% Hindi allocation ratio and architectural choices generalize to larger capacities where emergent capabilities may arise.
- What evidence would resolve it: Training and evaluating scaled variants (e.g., 7B, 13B) with identical data mixtures and comparing per-language performance trajectories across benchmarks.

### Open Question 2
- Question: Does the Hindi-English bilingual design-first methodology transfer to other Indic language families (e.g., Dravidian languages like Tamil, Telugu) with comparable gains?
- Basis in paper: [explicit] The abstract mentions India's "over 20 official languages" spanning "Indo-Aryan, Dravidian" families, yet PARAM-1 is strictly Hindi-English. The conclusion calls for "broader efforts toward foundation models that center linguistic plurality."
- Why unresolved: Hindi (Indo-Aryan) morphology differs substantially from Dravidian languages; the tokenizer and corpus design were optimized specifically for Hindi, and no transfer experiments are reported.
- What evidence would resolve it: Training comparable bilingual models for Tamil-English or Telugu-English using analogous pipeline choices and benchmarking on MMLU-translated subsets and MILU for those languages.

### Open Question 3
- Question: What is the performance trade-off introduced by deliberately excluding code and mathematics from pretraining?
- Basis in paper: [inferred] Section 2.6 explicitly states code/math removal to focus on natural language, but no ablation measures the cost to downstream reasoning tasks that may benefit from structured symbolic exposure (e.g., logical inference, numerical QA).
- Why unresolved: The paper does not compare against a version with code/math included, leaving unclear whether this exclusion limits performance on evaluations like LogiQA or tasks involving numerical reasoning.
- What evidence would resolve it: Ablation experiments with/without code/math data, measuring impact on both general NLU benchmarks and structured reasoning tasks (LogiQA, arithmetic subsets).

## Limitations
- Limited generalization to other Indian languages beyond Hindi and English
- Potential factual inconsistency on topics outside the specialized Phase 2 corpus
- Explicit exclusion of code and mathematics makes the model unsuitable for technical domains

## Confidence
- High Confidence: Tokenizer optimization for Indic languages, data composition and equitable representation, phased pretraining strategy
- Medium Confidence: Performance benchmarks, toxicity assessments
- Low Confidence: Generalization to other Indian languages, long-term factual consistency

## Next Checks
1. Cross-Lingual Transfer Evaluation: Test PARAM-1's zero-shot performance on downstream tasks in a low-resource Indian language (e.g., Tamil) to assess the limits of its cross-lingual capabilities beyond its primary Hindi-English training.

2. Factual Recall Stress Test: Create a held-out set of factual questions based *only* on topics not covered in the Phase 2 corpus to measure the model's factual consistency outside its specialized training data.

3. Tokenizer Efficiency Benchmarking: Compare the PARAM-1 tokenizer against the Nemotron tokenizer on a fixed Hindi dataset to quantify the efficiency gain in terms of token count per document and training speed, validating the claimed improvements in sequence length compression.