---
ver: rpa2
title: 'Identifying Explanation Needs: Towards a Catalog of User-based Indicators'
arxiv_id: '2506.16997'
source_url: https://arxiv.org/abs/2506.16997
tags:
- indicators
- explanation
- need
- system
- needs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of identifying when users need explanations
  in software systems. It addresses the challenge of eliciting explanation needs without
  introducing biases such as hypothetical or confirmation bias.
---

# Identifying Explanation Needs: Towards a Catalog of User-based Indicators

## Quick Facts
- arXiv ID: 2506.16997
- Source URL: https://arxiv.org/abs/2506.16997
- Reference count: 40
- One-line primary result: Catalog of 17 behavior-based, 8 system event-based, and 14 emotional/physical reaction indicators for identifying user explanation needs in software systems

## Executive Summary
This paper addresses the fundamental challenge of identifying when users need explanations in software systems. The authors conducted an online study with 66 participants who reported their explanation needs, behaviors, and emotional states while using three software systems. Through systematic coding procedures, they developed a comprehensive catalog of runtime indicators that can signal explanation needs without introducing survey biases. The research bridges the gap between theoretical explainability requirements and practical implementation by providing concrete, observable indicators that software engineers can use to trigger explanations at appropriate moments.

## Method Summary
The study employed a four-round coding methodology using MAXQDA software with two independent coders. Participants completed an online survey reporting up to five explanation needs for each of three recently-used software systems, followed by questions about associated behaviors and physical/emotional reactions. The coders iteratively developed taxonomies for explanation needs (using Droste et al.'s existing framework) and indicators, calculating interrater agreement using Brennan & Prediger κ statistics. The process resolved all conflicts through discussion and refined the indicator categories across multiple iterations.

## Key Results
- Established catalog containing 17 behavior-based indicators, 8 system event-based indicators, and 14 emotional/physical reaction indicators
- Workflow interruption behaviors (back-and-forth navigation, canceled actions) most strongly correlate with interaction-related explanation needs
- System anomalies (unusual loading times, design deviations) inherently create explanation needs independent of user context
- Physical reactions show promise for identifying domain knowledge needs but require further investigation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Workflow interruption behaviors—specifically back-and-forth navigation and action cancellations—correlate with a user's inability to proceed, signaling a need for explanation regarding interaction or system behavior.
- **Mechanism:** When a user's mental model of the system is insufficient, they attempt to navigate but retreat or cancel when the result does not match their expectation. This oscillation (back-and-forth) or retreat (cancellation) acts as a friction signal, indicating the user is searching for information they lack.
- **Core assumption:** Users navigating away from a task or reversing actions are doing so because of confusion or uncertainty, rather than a change in goal or preference.
- **Evidence anchors:** [abstract] The study identified "back-and-forth navigation" and "canceled actions" as key behavior-based indicators. [section] Section IV.A details "Workflow Interruption" as the most frequently reported category (85 responses), noting that "Back-and-forth navigation" was reported 44 times.
- **Break condition:** If a user is browsing aimlessly or exploring features without a specific task goal, back-and-forth navigation may indicate curiosity rather than a help requirement.

### Mechanism 2
- **Claim:** Deviations from expected system behavior (system anomalies) function as direct, context-independent triggers for explanation needs.
- **Mechanism:** Users maintain a baseline expectation of system performance (e.g., speed, layout). When the system violates this baseline (e.g., unusual loading times, design changes), it triggers an immediate need to understand the cause, independent of the user's specific navigation path.
- **Core assumption:** Users possess a stable "normalcy bias" regarding software performance, and any significant deviation is perceived as a potential error or risk requiring clarification.
- **Evidence anchors:** [abstract] Mentions "system anomalies" and "first-time function calls" as system event-based indicators. [section] Section IV.B defines "System Anomaly" (20 mentions), including "Loading time" and "Design deviation," and Section VI (Finding 4) states system events "inherently create a need for explanation."
- **Break condition:** If the anomaly is minor and self-resolving (e.g., a momentary network lag that recovers instantly), users may attribute it to external factors and not require a system-level explanation.

### Mechanism 3
- **Claim:** Gaps in domain knowledge are signaled more reliably by physical reactions (facial expressions, inactivity) than by specific software navigation patterns.
- **Mechanism:** Unlike interaction errors (which cause clicking/navigating), a lack of domain understanding often causes a cognitive "freeze" where the user stops interacting to think, or manifests non-verbal frustration (frowning) because the interface itself provides no obvious path to query the definition or concept.
- **Core assumption:** Domain confusion results in internal cognitive processing or emotional frustration rather than immediate trial-and-error interaction with the UI.
- **Evidence anchors:** [abstract] States "Physical reactions... show promise for identifying needs related to domain knowledge." [section] Section V.B notes that for domain knowledge needs, "60%... evoked a physical reaction, while only a quarter evoked a specific behavioral pattern."
- **Break condition:** If the user is multitasking or interrupted physically (e.g., looking away to answer a phone), inactivity or expressions may not relate to the software.

## Foundational Learning

- **Concept:** The "Why-Not" Mentality
  - **Why needed here:** To understand why you cannot simply ask users "Do you want an explanation?" If you treat user requests as absolute requirements, you will over-engineer explanations that clutter the interface, as users instinctively say "yes" to offered features regardless of utility.
  - **Quick check question:** If a user accepts an explanation in a survey, does that guarantee they will read it in production? (Answer: No, due to the bias described in the Introduction).

- **Concept:** Runtime Indicators vs. Self-Reports
  - **Why needed here:** This paper relies on *self-reported* past behavior to build a catalog. Engineers must understand that while this provides a broad taxonomy, it lacks the precision of observed telemetry.
  - **Quick check question:** Does "I usually click rapidly when confused" guarantee that rapid clicking in logs always means confusion? (Answer: No, it is a correlation, not a proven causal link, as noted in Section VII).

- **Concept:** Explanation Need Types (Taxonomy)
  - **Why needed here:** Not all needs are equal. An explanation for a *system error* is different from an explanation for *domain knowledge*. Effective architecture requires classifying the signal type to serve the correct content.
  - **Quick check question:** A user navigates back and forth—are they confused about *how* to use the button (Interaction) or *what* the button does (Domain)?

## Architecture Onboarding

- **Component map:** Signal Collector -> Indicator Engine -> Correlator -> Trigger
- **Critical path:**
  1. Capture high-frequency user events (navigation, input) without significant latency.
  2. Filter noise (distinguishing "exploration" from "confusion" is the hardest logic step).
  3. Link the "Workflow Interruption" (Section IV.A) to a specific help trigger.

- **Design tradeoffs:**
  - **Detection Depth vs. Privacy:** Detecting "Facial Expressions" (Section IV.C) offers high signal for domain knowledge but requires invasive camera access. Behavior-based indicators (clicks/navigation) are privacy-preserving but noisier.
  - **Real-time vs. Batch:** Real-time triggering (e.g., on "System Anomaly") helps the current user but risks annoyance. Batch analysis of "Inefficient Use" helps future versions but ignores the current user's struggle.

- **Failure signatures:**
  - **False Positives:** Triggering explanations during "Click spamming" that is actually just impatient gaming behavior or rapid data entry.
  - **Alert Fatigue:** Triggering on "First-time function call" (Section IV.B) for every button, overwhelming the user.
  - **Privacy Failure:** Attempting to implement physical reaction detection (e.g., verbal expressions) without explicit, granular user consent.

- **First 3 experiments:**
  1. **Back-and-Forth Navigation validation:** Implement a log analysis script to detect users visiting Page A -> Page B -> Page A within < 10 seconds. Correlate this with help menu usage or session abandonment rates.
  2. **System Anomaly Probes:** In a sandbox environment, induce a "Design Deviation" (move a button) and measure if "Inefficient Use" (hovering/time deviation) increases, validating the link between anomaly and behavioral change.
  3. **Cancellation Flow analysis:** Track "Canceled Action" events in a transaction flow. Follow up with a non-intrusive micro-survey ("Was something unclear?") to verify if the cancellation was due to confusion (need for explanation) or simply a change of mind.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the precision and recall rates of the identified indicators (e.g., back-and-forth navigation) when detecting explanation needs in deployed software systems?
- **Basis in paper:** [explicit] The authors state in the Future Work section that they plan to "test the indicators for their applicability and to evaluate them using various criteria such as precision, recall and implementation effort."
- **Why unresolved:** The current study relied on self-reported behaviors from a survey rather than empirical validation of the indicators in a functional system.
- **What evidence would resolve it:** Empirical data from telemetry or user studies in live systems showing the statistical correlation between the trigger of an indicator and a confirmed user need for explanation.

### Open Question 2
- **Question:** Which specific facial expressions or physiological changes are most reliably correlated with distinct types of explanation needs (e.g., domain knowledge vs. system errors)?
- **Basis in paper:** [explicit] The authors note that while facial expressions were the most frequently mentioned physical reaction, they "were not able to determine what type of facial expression indicated a need for explanation, which needs to be investigated in further studies."
- **Why unresolved:** The survey method captured general categories of reactions (e.g., "facial expressions") rather than the specific granular data required to train recognition models.
- **What evidence would resolve it:** A controlled experiment using video analysis or biometric sensors to map specific physical markers to specific explanation contexts.

### Open Question 3
- **Question:** How does the effectiveness of explanation need indicators vary across different software domains, such as productivity software versus entertainment applications?
- **Basis in paper:** [inferred] Under External Validity, the authors suggest it is "unclear whether the identified indicators apply equally across different types of software," noting that needs in productivity tools may differ from games.
- **Why unresolved:** The study utilized a general convenience sample without stratifying results by software type or analyzing domain-specific interaction patterns deeply.
- **What evidence would resolve it:** A comparative study isolating specific indicators within distinct software categories to measure their predictive power in each context.

### Open Question 4
- **Question:** To what extent do unconscious user behaviors serve as indicators for explanation needs compared to the self-reported behaviors identified in the catalog?
- **Basis in paper:** [explicit] The authors acknowledge that "it can be assumed that more indicators exist, especially indicators that users perform unconsciously," and list "exploratory experiments" to observe these as future work.
- **Why unresolved:** Self-reported surveys are limited by participant awareness and memory, potentially missing subtle behavioral cues.
- **What evidence would resolve it:** Observational user studies where participants are monitored for micro-behaviors (e.g., hesitation, micro-facial expressions) during system use without relying on self-reporting.

## Limitations

- Reliance on self-reported data introduces recall bias and potential misinterpretation of behaviors
- Convenience sample (n=66) from specific geographic and institutional contexts limits generalizability
- Catalog development based on past experiences rather than real-time observations

## Confidence

- **High confidence:** The existence of 17 behavior-based indicators, 8 system event-based indicators, and 14 emotional/physical reaction indicators as self-reported categories
- **Medium confidence:** The relationships between specific indicators and explanation need types, particularly for interaction-related needs and workflow interruption behaviors
- **Low confidence:** The practical utility of physical reaction indicators (facial expressions, inactivity) for identifying domain knowledge needs, given privacy constraints and lack of real-time validation

## Next Checks

1. **Real-time behavior validation:** Deploy telemetry in a production system to capture actual user navigation patterns (back-and-forth movement, cancellations) and compare these with self-reported behaviors from the catalog.

2. **Cross-cultural generalizability:** Replicate the study with participants from diverse geographic regions and technical backgrounds to determine if the indicator patterns hold across different user populations and software contexts.

3. **Threshold optimization study:** Conduct controlled experiments to determine optimal thresholds for behavioral indicators (e.g., minimum clicks for "click spamming," time thresholds for "inefficient use") that balance sensitivity with specificity across different application domains.