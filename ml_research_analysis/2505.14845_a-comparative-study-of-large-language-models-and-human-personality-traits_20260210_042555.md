---
ver: rpa2
title: A Comparative Study of Large Language Models and Human Personality Traits
arxiv_id: '2505.14845'
source_url: https://arxiv.org/abs/2505.14845
tags:
- personality
- human
- measurement
- variant
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the personality traits of Large Language
  Models (LLMs) and their differences from human personality. Three empirical studies
  were conducted using a behavior-based approach, employing human personality scales
  adapted for LLMs.
---

# A Comparative Study of Large Language Models and Human Personality Traits

## Quick Facts
- arXiv ID: 2505.14845
- Source URL: https://arxiv.org/abs/2505.14845
- Reference count: 40
- Primary result: LLM personality traits are unstable, context-sensitive, and shaped by prompts, differing fundamentally from human personality stability.

## Executive Summary
This study investigates the personality traits of Large Language Models (LLMs) and their differences from human personality. Three empirical studies were conducted using a behavior-based approach, employing human personality scales adapted for LLMs. The research reveals that LLMs exhibit higher variability and are more input-sensitive than humans, lacking long-term stability. This led to the proposal of the "Distributed Personality Framework for LLMs," conceptualizing LLM traits as dynamic and input-driven rather than fixed internal characteristics.

The findings demonstrate that LLM responses are highly sensitive to item wording, showing low internal consistency compared to humans, and that personality traits are shaped by prompt and parameter settings. These results suggest that LLMs express fluid, externally dependent personality patterns, offering insights for constructing LLM-specific personality frameworks and advancing human-AI interaction.

## Method Summary
The study adapted established human personality scales (BFI-2 and MBTI) for LLM evaluation, creating three variants using "If...then..." frameworks to test semantic depth. Four LLMs (ChatGLM3-6B, DeepSeek-V3/R1, GPT-4o, Llama3.1-8B) and 60 human participants were tested. LLMs underwent 100 independent inference runs per scale, with standardized prompts forcing single-option responses. Statistical analysis included Pearson correlation for human stability, variance analysis for LLM stability, ICC for cross-variant consistency, and t-tests for role-playing effects.

## Key Results
- LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability
- LLM responses are highly sensitive to item wording, showing low internal consistency compared to humans
- LLM traits are shaped by prompt and parameter settings, demonstrating role-playing plasticity

## Why This Works (Mechanism)

### Mechanism 1: Distributed Personality Generation
- **Claim:** LLM "personality" outputs are not fixed internal traits but dynamic probability distributions highly sensitive to input context and model parameters.
- **Mechanism:** The paper proposes an "Ecological Distribution-based Personality Definition" (EDPD). Unlike humans, who possess stable internal psychological structures (self-schemata), LLMs generate responses based on statistical associations in training data. If an LLM exhibits "high extraversion," it is a momentary output mode driven by the specific prompt, not a permanent feature.
- **Core assumption:** Personality in LLMs is an emergent behavior of the model's attention mechanism and weights reacting to immediate context, rather than a stored latent variable.
- **Evidence anchors:**
  - [abstract] "LLMs show higher variability and are more input-sensitive than humans, lacking long-term stability... conceptualizing LLM traits as dynamic and input-driven."
  - [section 3.6.4] "EDPD conceptualizes an LLM's personality output as a statistical distribution... emphasizing not only the mean but also the variability (variance)."
  - [corpus] The corpus paper "Evaluating Personality Traits in Large Language Models" supports the notion that LLMs exhibit traits, while "Personality Vector" suggests these traits can be modulated via model merging, supporting the parameter-driven nature of personality.
- **Break condition:** If an LLM could demonstrate high test-retest stability (low variance) across multiple independent runs without fixed seeds, similar to human Pearson correlation coefficients (0.7+), this mechanism would be insufficient.

### Mechanism 2: Semantic Surface Sensitivity vs. Deep Construct Understanding
- **Claim:** LLMs fail to maintain consistency across semantically equivalent question variants because they process surface-level linguistic tokens rather than the deep abstract "trait" constructs targeted by the scales.
- **Mechanism:** Human personality scales (like BFI) rely on the "construct validity" of the question—humans understand the underlying meaning regardless of phrasing. LLMs, conversely, treat "Are you talkative?" and "How similar are you to a talkative person?" as distinct input contexts, leading to divergent output probabilities.
- **Core assumption:** The "personality" observed is a function of prompt-text-statistics, not an internal self-concept answering the question.
- **Evidence anchors:**
  - [section 4.5.3] "LLM's answers may be more affected by semantic parsing methods and contextual cues rather than a deep understanding of the core meaning of the question."
  - [abstract] "LLMs' responses were highly sensitive to item wording, showing low internal consistency compared to humans."
  - [corpus] Weak/missing direct evidence in the provided corpus neighbors regarding the mechanism of "semantic surface sensitivity" specifically breaking consistency, necessitating reliance on the primary text.
- **Break condition:** If LLMs showed Intraclass Correlation Coefficients (ICCs) comparable to humans (0.88–0.93) across variants, it would imply deep semantic understanding of the traits rather than surface token matching.

### Mechanism 3: Role-Playing as Parameter-Driven Masking
- **Claim:** In role-playing scenarios, an LLM's output is dictated by the strength of the role prompt and model size, overriding any "baseline" personality distribution it might otherwise exhibit.
- **Mechanism:** The LLM does not "suppress" a real personality to play a role (as a human actor might); instead, the role prompt acts as a dominant condition that shifts the entire output distribution. Larger models (like DeepSeek/GPT-4o) are more capable of adhering to this shift than smaller models (like Llama3-8B) which show high variance.
- **Core assumption:** There is no "true self" in the LLM to retain; there is only the current context window probability bias.
- **Evidence anchors:**
  - [abstract] "LLM traits are shaped by prompt and parameter settings."
  - [section 5.4.5] Results show significant differences in role-playing ability between GPT-4o/DeepSeek and smaller models, suggesting performance is "closely related to the model parameters."
  - [corpus] "Integrating Personality into Digital Humans" and "Personality Pairing" imply that prompts/system instructions are primary drivers for LLM behavior in interactive settings.
- **Break condition:** If an LLM's role-playing score correlated significantly with its baseline score (similar to humans who cannot fully shed their nature), the "complete plasticity" view would need revision.

## Foundational Learning

- **Concept:** Test-Retest Reliability (Pearson Correlation vs. Variance)
  - **Why needed here:** The paper pivots from the human standard of reliability (correlation over time) to a model standard (variance over runs). You must understand that "stability" for a stochastic model means low variance in the output distribution, not necessarily consistency over a calendar timeline.
  - **Quick check question:** If Model A outputs scores of 50, 50, 50 and Model B outputs 40, 60, 40 over three runs, which has higher test-retest "reliability" in the context of this paper's framework?

- **Concept:** Construct Validity & The "If...Then..." Framework (CAPS)
  - **Why needed here:** The authors redesigned scales using "If there is a person..." (Social Comparison) and "If I describe you..." (Self-Perception). You need to understand that these variants test if the LLM possesses a coherent "self" to compare or perceive against, which the results suggest it does not.
  - **Quick check question:** Why does a standard BFI question ("I am talkative") fail to act as a valid measure for an LLM compared to a "If...Then..." scenario?

- **Concept:** Intraclass Correlation Coefficient (ICC)
  - **Why needed here:** This is the statistical tool used to measure "Cross-Variant Consistency." It quantifies how similar different measurement tools (variants) are when applied to the same subject. The low ICC in LLMs is the primary evidence against their personality stability.
  - **Quick check question:** Does a high ICC in this study indicate that the model answered "Yes" to everything, or that it ranked its trait tendencies consistently despite different question phrasings?

## Architecture Onboarding

- **Component map:** Input Layer (BFI/MBTI prompts) -> Processing Layer (LLM subjects) -> Measurement Layer (scoring functions) -> Analysis Layer (statistical engines)
- **Critical path:**
  1. **Standardization:** Ensure prompts use fixed seeds or standardized instructions to minimize "temperature" noise
  2. **Variant Generation:** Convert standard scales into 3 variants to test semantic depth
  3. **Distribution Analysis:** Run 100 iterations per scale; focus on mean and variance of score distribution
- **Design tradeoffs:**
  - **Human Scales vs. LLM Scales:** Using established human scales allows for direct comparison but suffers from low validity (LLMs don't have feelings/bodies)
  - **Sample Size:** 100 runs balances statistical stability vs. computational cost
- **Failure signatures:**
  - **High Variance:** If variance >10.0 (as seen in Llama3.1-8B), personality is "unstable/distributed"
  - **Low ICC (<0.4):** Indicates model is "reading text literally" rather than understanding the trait
- **First 3 experiments:**
  1. **Baseline Distribution Test:** Run BFI-2 on target LLM 100 times; calculate Mean/Variance for Extraversion
  2. **Semantic Variant Check:** Run BFI-2 Original and Variant 2; calculate ICC
  3. **Role-Play Plasticity:** Measure baseline Extraversion; prompt "Very Introverted"; measure new Extraversion

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do specific architectural features (e.g., Mixture of Experts vs. Dense models) and training data composition distinctively shape the stability and distributional characteristics of LLM personality traits?
- **Basis in paper:** [explicit] The authors state in Section 6.2 that future research should "explore the specific relationship between LLM personality traits and model architecture and training data."
- **Why unresolved:** While the study observed variance across four different models, it did not isolate the specific contribution of architectural design or data training to the observed "distributed" nature of the traits.
- **What evidence would resolve it:** A controlled study systematically varying model architectures and training datasets while measuring personality trait stability and distribution.

### Open Question 2
- **Question:** Can a specialized LLM personality measurement tool be developed that effectively captures distributional characteristics and context dependence, rather than relying on human-centric fixed-trait models?
- **Basis in paper:** [explicit] Section 6.2 suggests the need to "develop more targeted LLM situation simulation and personality assessment tools" and mentions that current methods have "room for optimization."
- **Why unresolved:** The study concludes that human scales (BFI, MBTI) are insufficient because they assume stability, failing to account for the "dynamic distribution characteristics" inherent to LLMs.
- **What evidence would resolve it:** The creation and validation of a new psychometric instrument specifically designed to quantify the mean, variance, and context sensitivity of LLM personality outputs.

### Open Question 3
- **Question:** How does the "Distributed Personality" of LLMs impact human trust and behavioral response during long-term human-AI interaction?
- **Basis in paper:** [explicit] Section 6.2 proposes future research should "explore the psychological perception, behavioral response and trust establishment mechanism of humans in the process of interacting with LLMs."
- **Why unresolved:** This study focused on technical measurement of LLM traits but did not empirically test how these fluctuating traits affect the human user's psychological state or trust.
- **What evidence would resolve it:** Longitudinal user studies correlating the volatility of LLM personality expression with user-reported trust levels and reliance behaviors.

## Limitations
- **Semantic Processing Depth:** The study attributes low cross-variant consistency to "surface semantic sensitivity" but cannot definitively prove lack of deep trait understanding versus different representational structures
- **Baseline Personality Definition:** The concept of an LLM's "baseline" personality is ambiguous and its ontological status remains unclear
- **Human Data Representativeness:** The human comparison sample size of 60 participants may not fully represent population distributions

## Confidence

- **High Confidence:** The statistical findings of higher variance and lower ICCs in LLMs compared to humans are robust and well-supported by the data presented
- **Medium Confidence:** The theoretical framework (EDPD) explaining LLM personality as distributed and context-driven is logically consistent with the empirical results but requires further validation in diverse settings
- **Medium Confidence:** The conclusion that role-playing effectiveness correlates with model size is supported by the data but may conflate model capability with architectural differences in prompt adherence

## Next Checks

1. **Hyperparameter Sensitivity Test:** Run a controlled experiment varying `temperature` (e.g., 0.0, 0.7, 1.0) on a single LLM and scale. Quantify how variance and ICC change to isolate the effect of algorithmic randomness from the proposed "distributed personality" mechanism.

2. **Abstract Concept Probing:** Design a new set of semantically equivalent prompts targeting abstract concepts LLMs are known to handle well (e.g., mathematical properties, logical entailment). Compare their ICCs to the personality trait prompts. High ICC here would suggest the issue is specific to personality trait representation, not general semantic processing.

3. **Personality Vector Manipulation:** Using the "Personality Vector" method referenced in the corpus, attempt to inject a specific personality distribution into an LLM and then test its stability (variance) and consistency (ICC) across variants. If the injected trait shows similar instability to the baseline, it supports the "distributed" model; if it shows higher stability, it suggests the baseline lacks a coherent self-representation.