---
ver: rpa2
title: 'DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling'
arxiv_id: '2510.21712'
source_url: https://arxiv.org/abs/2510.21712
tags:
- search
- planning
- value
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DecoupleSearch addresses challenges in Agentic Retrieval-Augmented
  Generation by decoupling planning and search processes using dual value models.
  The framework employs Monte Carlo Tree Search to construct reasoning trees and evaluate
  step quality, while Hierarchical Beam Search refines planning and search candidates
  during inference.
---

# DecoupleSearch: Decouple Planning and Search via Hierarchical Reward Modeling

## Quick Facts
- **arXiv ID:** 2510.21712
- **Source URL:** https://arxiv.org/abs/2510.21712
- **Reference count:** 17
- **Primary result:** Using a 7B policy model, DecoupleSearch achieves 25.8% relative average improvement over baselines

## Executive Summary
DecoupleSearch addresses the challenge of scaling Agentic Retrieval-Augmented Generation (RAG) systems by decoupling planning and search processes. The framework uses dual value models—one for evaluating planning steps and another for search results—combined with Monte Carlo Tree Search (MCTS) to construct reasoning trees and evaluate step quality. Through Hierarchical Beam Search, the system refines planning and search candidates during inference, effectively managing the exponentially large candidate space while maintaining accuracy in complex reasoning tasks.

## Method Summary
DecoupleSearch employs a Qwen2.5-7B/14B-Instruct backbone with dual auxiliary value models for planning and search. The framework uses MCTS annotation with Qwen-Turbo to generate training data, creating reasoning trees with backpropagated value labels. During training, the policy and value models are fine-tuned jointly using a multi-task loss function. At inference, Hierarchical Beam Search iteratively expands, evaluates, and prunes planning and search candidates using the trained value models. The system operates with beam sizes B₁=3 for planning and B₂=3 for search, using a DPR retriever to fetch top-5 documents per query.

## Key Results
- DecoupleSearch achieves 25.8% relative average improvement over baselines using a 7B policy model
- Performance comparable to larger 14B models when using inference-time scaling
- Demonstrates effectiveness across both single-hop (TriviaQA) and multi-hop (HotpotQA, 2WikiMultiHopQA, Bamboogle, MuSiQue) question-answering tasks

## Why This Works (Mechanism)

### Mechanism 1: Value-Guided Pruning of Exponential Search Space
Hierarchical Beam Search with dual value models efficiently prunes the exponentially large candidate space for planning and search. Two value models evaluate planning steps and search results, ranking candidates and pruning low-scoring ones at each step. The core assumption is that learned value models can reliably estimate the expected utility of a step toward the correct answer based on MCTS-derived training signals.

### Mechanism 2: Decoupled Optimization of Planning and Search
Separating optimization for planning and search allows targeted error correction in each distinct phase. The framework uses two separate value models, V_φ for planning and V_ψ for search, trained on distinct reward signals from MCTS simulations. This enables independent penalization of flawed plans or poor retrievals.

### Mechanism 3: MCTS for Dense, Goal-Distant Supervision
MCTS annotates training data by generating dense, step-level supervision from sparse final-answer rewards. MCTS builds a reasoning tree, with an LLM judge assigning intermediate rewards that are corrected via backpropagation from the final answer's correctness, creating reliable value targets for training.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS)**
  - Why needed: Core annotation engine for generating step-level value labels
  - Quick check: What is the role of the backpropagation step in the MCTS loop?

- **Value Functions / Reward Modeling**
  - Why needed: Framework relies on training value models to estimate "goodness" of steps
  - Quick check: How is the signal used to train the policy model different from the signal for the value models?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed: DecoupleSearch improves "Agentic RAG," requiring understanding of basic RAG
  - Quick check: How does the "Agentic RAG" paradigm differ from a standard, single-turn RAG system?

## Architecture Onboarding

- **Component map:**
  - Policy Model (π_θ) -> Planning Value Model (V_φ) -> Search Value Model (V_ψ) -> Retriever -> MCTS Annotation Module -> Hierarchical Beam Search (HBS)

- **Critical path:**
  1. Offline Data Generation: MCTS creates reasoning trees with backpropagated value labels
  2. Model Training: Policy and dual value models are trained jointly on this data
  3. Inference: HBS uses the trained models to iteratively build a reasoning path

- **Design tradeoffs:**
  - Annotation Cost vs. Quality: MCTS is expensive (18+ hours) but produces dense signals
  - Beam Size vs. Inference Cost: Larger beams improve recall but increase latency and ranking difficulty
  - Shared vs. Separate Weights: Sharing LLM body is efficient but may limit representational power

- **Failure signatures:**
  - Value Model Collapse: Value models fail to discriminate, causing random walk behavior
  - Premature Pruning: Aggressive value model prunes correct path early
  - Query Formulation Failure: Policy generates poor queries, leading to irrelevant retrievals
  - Hallucinated Judgments: MCTS LLM judge is unreliable, creating miscalibrated training data

- **First 3 experiments:**
  1. Validate Data Generation: Run MCTS annotation on small dataset subset, verify backpropagation assigns high values to logically sound steps on correct paths
  2. Value Model Ablation: Disable one or both value models during inference to quantify individual contributions
  3. Scaling Analysis: Systematically vary beam sizes (planning and search) on validation set to plot accuracy vs. latency and find optimal operating point

## Open Questions the Paper Calls Out

1. Does retaining multiple promising candidates (plans and search results) at each step yield better accuracy than the current single-candidate retention strategy? The current approach focuses on retaining only the single most promising plan and search result, with exploration of retaining multiple candidates left for future work.

2. Can the computational cost of the MCTS annotation phase be reduced without degrading the quality of value model supervision? The MCTS annotation process requires multiple simulations, which can lead to additional labeling costs.

3. How can the planning value model be improved to handle the inherent difficulty of evaluating abstract plans compared to concrete search results? Evaluating the quality of a plan is more challenging, as there are no obvious patterns to determine its effectiveness.

## Limitations

- Performance scalability beyond 7B/14B models remains unclear
- Exact computational overhead of dual value models during inference not quantified
- Impact of different retriever qualities on overall framework performance unknown

## Confidence

- High confidence: Dual value models improve planning/search optimization
- Medium confidence: MCTS-derived supervision quality
- Low confidence: Real-world applicability to non-Wikipedia domains

## Next Checks

1. Run ablation study comparing single vs. dual value model configurations across all datasets
2. Test framework with alternative retrievers (BM25, dense vs. sparse) to measure dependency
3. Evaluate performance on out-of-domain QA datasets to assess generalizability