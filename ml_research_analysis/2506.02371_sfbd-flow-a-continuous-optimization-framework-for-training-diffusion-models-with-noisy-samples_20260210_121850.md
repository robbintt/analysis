---
ver: rpa2
title: 'SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models
  with Noisy Samples'
arxiv_id: '2506.02371'
source_url: https://arxiv.org/abs/2506.02371
tags:
- sfbd
- samples
- data
- diffusion
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training diffusion models
  on corrupted data while avoiding memorization of sensitive content. The authors
  reinterpret SFBD as an alternating projection algorithm and introduce a continuous
  variant, SFBD flow, which eliminates the need for iterative denoising and fine-tuning.
---

# SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples

## Quick Facts
- arXiv ID: 2506.02371
- Source URL: https://arxiv.org/abs/2506.02371
- Authors: Haoye Lu; Darren Lo; Yaoliang Yu
- Reference count: 40
- Primary result: Online SFBD achieves FID scores of 3.22 and 3.23 on CIFAR-10 and CelebA respectively when training on data with 20% Gaussian noise, outperforming models trained on clean data.

## Executive Summary
This paper addresses the challenge of training diffusion models on corrupted data while avoiding memorization of sensitive content. The authors reinterpret SFBD as an alternating projection algorithm and introduce a continuous variant, SFBD flow, which eliminates the need for iterative denoising and fine-tuning. They further develop Online SFBD, a practical method that maintains cached denoised samples and trains end-to-end. The paper establishes connections between SFBD flow and consistency constraint-based methods, showing that Online SFBD can be seen as a more accurate variant. Empirical results demonstrate that Online SFBD consistently outperforms strong baselines across CIFAR-10 and CelebA benchmarks.

## Method Summary
The paper presents SFBD flow as a continuous optimization framework for training diffusion models on noisy data. It reinterprets the Stochastic Feature Back-Door (SFBD) approach as an alternating projection algorithm, where the model projects between clean and noisy data manifolds. The key innovation is the continuous variant that eliminates iterative denoising steps by integrating the denoising process directly into the training objective. Online SFBD implements this by maintaining a cache of denoised samples that are updated during training, allowing end-to-end optimization without separate fine-tuning phases. The method connects to consistency constraint approaches by showing that the continuous formulation enforces implicit consistency between noisy inputs and their denoised counterparts.

## Key Results
- Online SFBD achieves FID scores of 3.22 and 3.23 on CIFAR-10 and CelebA respectively when training on data with 20% Gaussian noise
- Outperforms models trained on clean data in both benchmarks
- Consistently outperforms strong baselines across different noise levels and corruption types
- Demonstrates robustness to various synthetic noise models while maintaining competitive performance

## Why This Works (Mechanism)
The framework works by integrating denoising directly into the training process rather than treating it as a separate step. By reinterpreting SFBD as alternating projection between clean and noisy manifolds, SFBD flow creates a continuous path that guides the model toward learning robust representations. The Online SFBD variant maintains cached denoised samples that serve as anchors, preventing the model from memorizing noise patterns while still leveraging corrupted data for training. This approach effectively balances the need to learn from noisy samples without overfitting to the corruption.

## Foundational Learning
1. **Diffusion Models**: Why needed - Understanding the denoising diffusion process is crucial for grasping how SFBD flow modifies the training objective. Quick check - Can you explain the forward and reverse processes in diffusion models?
2. **Alternating Projection Algorithms**: Why needed - The reinterpretation of SFBD as alternating projection provides the theoretical foundation for SFBD flow. Quick check - What are the convergence properties of alternating projection methods?
3. **Feature Back-Door Mechanisms**: Why needed - SFBD relies on backdoor features that connect clean and noisy data representations. Quick check - How do backdoor features differ from regular learned features?
4. **Consistency Constraints**: Why needed - The connection to consistency-based methods helps understand the implicit regularization in SFBD flow. Quick check - What role do consistency constraints play in semi-supervised learning?
5. **Feature Space Regularization**: Why needed - The method operates primarily in feature space rather than pixel space. Quick check - How does feature space regularization differ from pixel-level denoising?

## Architecture Onboarding
**Component Map**: Noisy data -> Denoising network -> Cached denoised samples -> Diffusion model training loop -> Final model
**Critical Path**: The denoising network and cached samples form the core of the approach, with the training loop integrating both noisy and denoised information.
**Design Tradeoffs**: 
- Cached samples require additional memory but eliminate iterative denoising
- Continuous optimization trades off some precision for training efficiency
- Feature-level operations reduce computational overhead compared to pixel-level denoising
**Failure Signatures**: 
- Model memorizing noise patterns (indicated by poor FID on clean test data)
- Cached samples becoming stale or diverging from true denoised representations
- Training instability due to conflicting gradients from noisy and denoised samples
**First Experiments**:
1. Train baseline diffusion model on clean data for comparison
2. Implement SFBD flow with synthetic Gaussian noise on CIFAR-10
3. Compare Online SFBD with traditional SFBD using iterative denoising

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical evaluation focuses on standard benchmarks (CIFAR-10, CelebA) with synthetic Gaussian noise, which may not represent real-world corruption patterns
- Theoretical analysis assumes idealized conditions that may not hold with limited computational resources
- Reported FID improvements raise questions about potential overfitting to specific noise models and dataset combinations

## Confidence
- **High**: The mathematical formulation of SFBD flow and its connection to alternating projection algorithms is well-grounded and theoretically sound
- **Medium**: The empirical performance gains, while consistently demonstrated, require further validation across diverse corruption types and real-world datasets
- **Medium**: The relationship between SFBD flow and consistency constraint methods is clearly established but may have practical limitations not fully explored

## Next Checks
1. Test the framework on real-world corrupted datasets (e.g., medical imaging with sensor noise, satellite imagery with atmospheric interference) to verify generalization beyond synthetic Gaussian noise
2. Conduct ablation studies varying the noise levels and types (Poisson, salt-and-pepper, speckle) to assess robustness across different corruption models
3. Evaluate computational efficiency and memory requirements for the Online SFBD implementation at scale, particularly the cached denoised samples mechanism in resource-constrained settings