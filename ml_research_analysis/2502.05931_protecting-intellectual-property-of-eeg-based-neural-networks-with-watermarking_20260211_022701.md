---
ver: rpa2
title: Protecting Intellectual Property of EEG-based Neural Networks with Watermarking
arxiv_id: '2502.05931'
source_url: https://arxiv.org/abs/2502.05931
tags:
- watermark
- embedding
- watermarking
- accuracy
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a cryptographic watermarking framework for
  EEG-based neural networks, addressing intellectual property protection challenges.
  The method employs a wonder filter-based approach using cryptographic hashing and
  public-key encryption to embed watermarks during training.
---

# Protecting Intellectual Property of EEG-based Neural Networks with Watermarking

## Quick Facts
- arXiv ID: 2502.05931
- Source URL: https://arxiv.org/abs/2502.05931
- Authors: Ahmed Abdelaziz; Ahmed Fathi; Ahmed Ahmed Fares
- Reference count: 35
- One-line primary result: A cryptographic watermarking framework achieves 100% watermark detection reliability while maintaining <5% drop in EEG task accuracy across three models.

## Executive Summary
This paper introduces a cryptographic watermarking framework for EEG-based neural networks to protect intellectual property. The method employs a wonder filter-based approach using cryptographic hashing and public-key encryption to embed watermarks during training. Evaluation on the DEAP dataset across three models (CCNN, EEGNet, TSception) demonstrates robust performance: watermark detection achieves 100% reliability, null-embedding accuracy exceeds 99.4%, and the watermark remains intact against adversarial attacks including fine-tuning, transfer learning, and neuron pruning. The framework provides authentication, persistence, and piracy resistance while maintaining minimal distortion (<5% drop in EEG task accuracy), offering a secure solution for protecting sensitive neurophysiological models in healthcare and biometric applications.

## Method Summary
The framework generates wonder filters from owner signatures using cryptographic hashing, creating normal embeddings (filter → target label) and null embeddings (inverted filter → original labels). These are overlaid onto training samples with extreme pixel values (±2000) to create persistent, tamper-proof behavior. The model is trained from scratch on augmented data combining original samples with watermark samples. Verification involves testing classification accuracy on filtered inputs and authenticating the cryptographic signature. The approach balances watermark robustness against primary task performance, achieving >99% detection while maintaining <5% accuracy drop on EEG tasks.

## Key Results
- Watermark detection achieves 100% reliability across CCNN, EEGNet, and TSception models
- Null-embedding accuracy exceeds 99.4% while primary task accuracy drops <5%
- Watermark remains intact against fine-tuning (30 epochs), transfer learning, and neuron pruning attacks
- Secondary watermark embedding fails with >10% accuracy loss on primary task

## Why This Works (Mechanism)

### Mechanism 1
Out-of-bound pixel values create persistent, unremovable watermarks in trained models. During training, wonder filter pixels are set to extreme values (±2000 instead of normalized 0-1). These create classification rules with ~100% confidence. When cross-entropy loss is computed, the gradient approaches log(1) ≈ 0, preventing subsequent weight updates from modifying the embedded rule—even under fine-tuning. Core assumption: The model processes out-of-range inputs without saturation or rejection, and trained extreme-value patterns generalize to modified versions of the model. Break condition: If a model architecture clips or normalizes out-of-range inputs before the first convolutional layer, the extreme-value pattern will be lost, and persistence fails.

### Mechanism 2
Null embedding locks the model against secondary watermark injection. Alongside normal embedding (filter W → label y_w), the model is trained on null embeddings using the bitwise-inverted filter W⁻ mapped to original labels. This trains the network to treat the filter's pixel locations as non-informative for classification. Because null embeddings require input-space reshaping during initial training, they cannot be added post-hoc without degrading primary task accuracy. Core assumption: Null embeddings cannot be learned after initial training completes; fine-tuning cannot reshape the input space without catastrophic accuracy loss. Break condition: If an attacker retrains from scratch with sufficient data, they can overwrite both normal and null embeddings—piracy resistance assumes resource-constrained adversaries.

### Mechanism 3
Cryptographic binding of wonder filter to owner identity enables authentication and prevents ownership disputes. The owner creates a signature sig = Encrypt(private_key, verifier_string), then deterministically derives filter W, position, and target label using collision-resistant hash functions. Verification requires the public key to recover the verifier string, proving the watermark originated from the key holder. Core assumption: Hash collisions are computationally infeasible, and brute-force attacks on the filter pattern require exponentially many attempts. Break condition: If the private key is compromised or hash functions have exploitable collisions, authentication is broken.

## Foundational Learning

- **Public-key cryptography and digital signatures**: Understanding how private/public key pairs enable the owner to prove watermark origin without revealing the secret key during verification. Quick check: Can you explain why verification uses the public key while embedding requires the private key?

- **Backdoor-based watermarking in neural networks**: The wonder filter operates similarly to backdoor triggers—training the model to respond predictably to specific input patterns—so understanding this analogy clarifies the embedding process. Quick check: How does a backdoor trigger differ from adversarial perturbation in terms of training-time vs. inference-time manipulation?

- **EEG signal representation as tensors**: EEG data has unique structure (channels × time or channels × frequency × time) that differs from images; adapting wonder filters requires reshaping to match model input dimensions (e.g., 4×9×9 for CCNN, 28×512 for TSception). Quick check: Why might temporal dependencies in EEG signals complicate the placement of spatial wonder filter patterns?

## Architecture Onboarding

- **Component map**: Watermark Generator -> Data Augmentation Module -> Model Training Pipeline -> Verification Module
- **Critical path**: 1) Generate cryptographic signature from private key, 2) Derive filter position and bit pattern via hash functions, 3) Create normal embedding samples (W → y_w) and null embedding samples (W⁻ → original labels), 4) Train model from scratch with augmented data, 5) Verify by testing model behavior on filtered inputs and authenticating signature
- **Design tradeoffs**: Filter size vs. stealth (larger filters embed more reliably but may corrupt more EEG data); number of embedded watermarks (k) vs. verification exposure (multiple watermarks allow revealing one during disputes while keeping others hidden, but increase training complexity); from-scratch vs. pretrain embedding (from-scratch shows stronger persistence but requires more compute)
- **Failure signatures**: Watermark not detected (check filter placement aligns with input shape; verify out-of-bound values (±2000) are preserved through preprocessing); Secondary watermark successfully embedded (null embedding may have failed—confirm W⁻ was correctly generated and trained); Primary task accuracy drops >5% (reduce filter coverage or verify normal/null embedding balance in training data)
- **First 3 experiments**: 1) Baseline verification: Train CCNN on DEAP without watermark, then train CCNN with wonder filter watermark; confirm >99% null-embedding accuracy and <5% EEG accuracy drop, 2) Persistence stress test: Apply fine-tuning (RTAL setting, 30 epochs) and L1 pruning (up to 50% neurons) to watermarked models; verify watermark accuracy remains >80% while EEG accuracy degrades faster, 3) Piracy resistance test: Attempt to embed a second watermark (W_A) using the same scheme on an already-watermarked model; confirm >10% accuracy drop on primary task for CCNN/EEGNet

## Open Questions the Paper Calls Out

- **Generalizability across tasks**: Does the framework generalize to larger, more complex EEG architectures and diverse clinical tasks beyond emotion recognition? The Conclusion states that extending the method to tasks like Alzheimer's detection or sleep monitoring is necessary to validate versatility. Current evaluation is limited to the DEAP dataset (emotion) and three specific models.

- **Hybrid security approaches**: Can hybrid approaches combining wonder filters with secure hardware or advanced encryption enhance protection against emerging attack vectors? The authors suggest investigating hybrid approaches to address evolving security threats. The current implementation relies solely on the software-based wonder filter mechanism without hardware integration.

- **Real-world deployment robustness**: How does the watermarking scheme perform in dynamic, real-world clinical environments regarding long-term robustness? The Conclusion notes that "real-world deployment studies are needed to assess long-term robustness." The study relies on controlled experiments with static datasets rather than continuous, live clinical data streams.

## Limitations

- The cryptographic authentication claims assume hash collision resistance without quantifying the actual attack surface—an adversary with modest computational resources might still achieve brute-force success given the relatively small search space of filter patterns and positions.

- The claim that null embeddings create irreversible "locks" against secondary watermarking is supported only by internal experiments—no third-party validation exists for this mechanism.

- The assertion that out-of-bound pixel values (±2000) create gradient-locking behavior depends on specific model implementations that may not generalize across architectures.

## Confidence

- **Watermark persistence under fine-tuning and pruning**: Medium-High - The 100% detection accuracy claim is supported by internal experiments across three models, though only one dataset (DEAP) was tested.

- **Piracy resistance via null embedding**: Low-Medium - This is the most speculative claim, supported only by internal experiments showing >10% accuracy drops. No external validation exists for the "locking" mechanism.

- **Cryptographic authentication**: High - Standard public-key cryptography assumptions are well-established in the literature.

- **Generalizability across EEG datasets and tasks**: Low - Only the DEAP dataset with valence classification was tested.

## Next Checks

1. **Cross-dataset validation**: Test the watermark framework on at least two additional EEG datasets (e.g., SEED for emotion recognition, BCI Competition IV for motor imagery) to verify generalizability. Measure whether watermark persistence and piracy resistance hold when model architectures and input characteristics differ from DEAP.

2. **Formal attack analysis**: Conduct a systematic study of null embedding vulnerability by attempting to embed secondary watermarks using different strategies: (a) increasing null embedding sample count during retraining, (b) using alternative filter patterns not blocked by existing null embeddings, and (c) fine-tuning with knowledge distillation to preserve original task accuracy while embedding new triggers.

3. **Preprocessing robustness test**: Evaluate how different input preprocessing pipelines affect watermark persistence. Specifically, test models with: (a) input normalization to [0,1] range, (b) clipping layers that bound values to [-1,1], and (c) batch normalization immediately after input layers. Measure watermark detection accuracy across these configurations to identify architectural vulnerabilities.