---
ver: rpa2
title: Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with
  Infant Egocentric Video
arxiv_id: '2511.11725'
source_url: https://arxiv.org/abs/2511.11725
tags:
- video
- learning
- masking
- dataset
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores word-referent mapping in infants by proposing
  a biologically plausible masking strategy for self-supervised visual representation
  learning from egocentric video data. The proposed BlindSpotMAE model incorporates
  knowledge about human eye blind spots into a masked autoencoder framework, replacing
  standard random masking with a biologically motivated approach.
---

# Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video

## Quick Facts
- arXiv ID: 2511.11725
- Source URL: https://arxiv.org/abs/2511.11725
- Reference count: 40
- Key outcome: Blind spot masking performs comparably to random masking for word-referent mapping from egocentric infant video

## Executive Summary
This study investigates whether biologically motivated visual masking strategies can effectively support word-referent mapping in infants. The authors propose BlindSpotMAE, a masked autoencoder that incorporates human eye blind spots into the masking strategy for self-supervised visual representation learning from egocentric video data. Using longitudinal egocentric recordings from a child's perspective, the model learns to reconstruct masked regions corresponding to human blind spots (5° × 7° visual angle, centered ~15° temporally) plus peripheral vision. The pretrained encoder is then used in a contrastive learning framework to align video representations with temporally co-occurring utterances, enabling word-referent mapping without explicit supervision.

## Method Summary
The method employs VideoMAEv2 with a ViT-B backbone, processing 16-frame clips (~2.1 seconds) using cube patch embeddings (2×16×16). Blind spot masking replaces standard random masking, targeting regions corresponding to human blind spots and peripheral vision based on camera field-of-view assumptions. Pretraining uses MSE reconstruction loss with AdamW (lr=1.5e-4, batch=16, weight_decay=0.05) on 8-second chunks sampled every 4 frames. The multimodal training phase freezes the video encoder and trains a text encoder using contrastive learning with InfoNCE loss (symmetric formulation, temperature=0.07) to align video-text pairs.

## Key Results
- BlindSpotMAE-109×70 achieves 57.31% accuracy on Labeled-S object classification vs VideoMAEv2's 58.82%
- VideoMAEv2 achieves 60.44% on Toybox Transformation classification vs DINO's 30.39% (+30% improvement)
- BlindSpotMAE multimodal model achieves 78.86% on Labeled-S retrieval, comparable to VideoMAEv2 (79.59%) and DINO ViTB (82.50%)
- BlindSpotMAE achieves competitive results on CVCL evaluation trials (82.41% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Biologically motivated blind spot masking can learn visual representations as effectively as random masking for word-referent mapping
- Mechanism: The model masks regions corresponding to human blind spots (5° × 7° visual angle, centered ~15° temporally) plus peripheral vision, forcing reconstruction of these gaps similar to how the human brain fills in missing visual information. This constrains the model to learn from the same visual information available to infants.
- Core assumption: The camera field-of-view approximates what the infant visually perceives; binocular vision can be approximated by masking blind spots from both eyes.
- Evidence anchors:
  - [abstract] "Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings"
  - [Section 6.1, Table 4] BlindSpotMAE-200×135 achieves 41.24% on Toybox Object vs VideoMAEv2's 33.44%; BlindSpotMAE-109×70 achieves 57.31% on Labeled-S vs VideoMAEv2's 58.82%
  - [corpus] No direct corpus evidence for blind spot masking; related work focuses on grounded word learning, not masking strategies

### Mechanism 2
- Claim: Contrastive learning over temporally-aligned video-text pairs enables word-referent mapping without explicit supervision
- Mechanism: The InfoNCE loss pulls together video representations with their temporally co-occurring utterance embeddings while pushing apart mismatched pairs. Over repeated cross-situational exposures, the model associates specific words with their visual referents.
- Core assumption: Temporally co-occurring utterances refer to objects/events visible in the corresponding video clips; sufficient cross-situational exposure exists in the longitudinal data.
- Evidence anchors:
  - [abstract] "The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings"
  - [Section 6.4, Figure 2] BlindSpotMAE multimodal model achieves 78.86% on Labeled-S retrieval, comparable to VideoMAEv2 (79.59%) and DINO ViTB (82.50%)
  - [corpus] Related work (arxiv 2511.18824) supports that infants learn object labels from co-occurrences of words and visible referents

### Mechanism 3
- Claim: Temporal video representations enable spatio-temporal understanding that static image models cannot achieve
- Mechanism: The video encoder processes 16-frame clips (~2.1 seconds) with joint space-time attention, integrating information across time to learn transformations (rotation, translation) unavailable in single frames.
- Core assumption: Spatio-temporal patterns relevant to infant learning require temporal integration beyond single frames.
- Evidence anchors:
  - [Section 6.2, Table 6] VideoMAEv2 achieves 60.44% on Toybox Transformation vs DINO's 30.39% (+30% improvement); BlindSpotMAE-109×70 achieves 58.53%
  - [Section 7] "Learning from still images is not sufficient for spatio-temporal understanding"
  - [corpus] No direct corpus evidence on spatio-temporal learning; related work focuses on visual concept discovery

## Foundational Learning

- Concept: **Masked Autoencoding (MAE)**
  - Why needed here: BlindSpotMAE uses MAE pretraining where masked patches are reconstructed from visible ones; understanding this reconstruction objective is essential for grasping how blind spot masking differs from random masking.
  - Quick check question: Can you explain why high masking ratios (90%) typically work better for video MAE, and what changes when using lower ratios?

- Concept: **Contrastive Learning with InfoNCE Loss**
  - Why needed here: The multimodal training phase uses InfoNCE to align video-text pairs; this loss formulation determines how word-referent associations are learned.
  - Quick check question: Given a batch of B video-text pairs, how does the symmetric InfoNCE loss ensure both video-to-text and text-to-video alignment?

- Concept: **Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: The masking strategy operates on 16×16 patches; understanding patch-based processing is necessary for implementing blind spot regions that align with patch boundaries.
  - Quick check question: If a video frame is 224×224 pixels with 16×16 patches, how many patch tokens does the ViT encoder receive per frame?

## Architecture Onboarding

- Component map:
  - **Video Encoder (VideoMAEv2 + ViT-B)**: Processes 16-frame clips with cube patch embedding (2×16×16), applies blind spot masking, reconstructs via CNN decoder during pretraining
  - **Blind Spot Masking Module**: Calculates mask positions based on camera FOV (109°×70° for SAYCam), human central vision (60°×60°), and blind spot size (5°×7°)
  - **Text Encoder**: Single embedding layer mapping word tokens to d-dimensional vectors, averaged for multi-word utterances
  - **Contrastive Learning Head**: Linear projection replacing decoder during multimodal training, outputs d-dimensional embeddings for InfoNCE loss

- Critical path:
  1. Pretrain video encoder on SAYCam (Child S, 194 hours) with blind spot masking and MSE reconstruction loss
  2. Discard decoder, freeze video encoder
  3. Train multimodal model with frozen video encoder + trainable text encoder using contrastive loss on video-utterance pairs

- Design tradeoffs:
  - **Masking ratio**: BlindSpotMAE-109×70 uses 53% vs standard 90%; lower ratio may cause information leakage but works comparably
  - **FOV assumption**: BlindSpotMAE-200×135 assumes full binocular FOV vs BlindSpotMAE-109×70 uses camera FOV; camera FOV variant is more stable across tasks
  - **Peripheral masking**: BlindSpotMAE-NoPeripheral (6% masking) performs significantly worse (45.36% vs 57.31% on Labeled-S)

- Failure signatures:
  - **Basket class failure**: All multimodal models perform poorly on "basket" class (~222 samples vs 1000-10000 for others)
  - **NoPeripheral degradation**: Removing peripheral masking causes 12% accuracy drop; reconstruction task becomes too simple
  - **Image-video gap**: Video models underperform image models on static classification (~14% gap) but narrow gap with language supervision

- First 3 experiments:
  1. Train VideoMAEv2 baseline with standard random tube masking (90% ratio) on Child S data; evaluate on Labeled-S, Toybox Object, and Toybox Transformation
  2. Train three BlindSpotMAE variants (200×135 FOV, 109×70 FOV, Center crop) with 53% masking; compare reconstruction loss curves and downstream task performance
  3. Freeze pretrained video encoder, train text encoder with contrastive loss; evaluate video-text retrieval and analyze per-class performance to identify data imbalance issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would blind spot masking yield greater benefits over random masking when trained on data from multiple children with more diverse visual experiences?
- Basis in paper: [explicit] The paper notes "the video encoder is first pretrained on recordings from child S only" (194 hours from one child), and states that "children share a highly uniform grammar" despite "small overlap of less than 20% and about 40% in their initial 100 and 1000 word vocabulary."
- Why unresolved: The study uses single-child training, limiting conclusions about generalizability. Blind spot masking performs comparably to random masking, but whether this holds or changes with multi-child, more diverse training data remains unknown.
- What evidence would resolve it: Pretraining BlindSpotMAE on multi-child datasets (e.g., full SAYCam with children S, A, Y; or BabyView with 28 families) and comparing performance gaps between blind spot and random masking across different training scales and diversities.

### Open Question 2
- Question: What are the essential components of blind spot masking for effective representation learning—is peripheral masking necessary, and what is the optimal masking ratio?
- Basis in paper: [inferred] The paper reports that BlindSpotMAE-NoPeripheral (6% masking ratio, no peripheral masking) performed "worse across all object classification datasets," while BlindSpotMAE-109x70 achieved comparable results to VideoMAEv2 despite using "substantially lower masking ratio (53%) compared to the standard VideoMAE masking ratio of 90%."
- Why unresolved: The paper hypothesizes the NoPeripheral model's poor performance stems from "extremely small masking ratio" and "potential information leakage," but does not disentangle whether peripheral masking, masking ratio, or their interaction drives the results.
- What evidence would resolve it: Ablation studies varying masking ratios independently for blind spot regions versus peripheral regions, and comparing reconstruction task difficulty across configurations.

### Open Question 3
- Question: Can contrastive video-text learning achieve better data efficiency for rare word-referent pairs, or do such models fundamentally require more co-occurrence data than supervised approaches?
- Basis in paper: [inferred] The paper notes all multimodal models "perform poorly in classifying the basket class" due to limited training data (~222 images), and states "the result could potentially suggest that the contrastive learning model require more data compared to a supervised model" since linear decoding performed well on this class.
- Why unresolved: The paper does not systematically vary training data per class or compare data efficiency curves between contrastive learning and supervised alternatives.
- What evidence would resolve it: Controlled experiments varying the number of word-referent co-occurrences per class and measuring accuracy curves for both contrastive and supervised approaches, ideally validated against infant vocabulary acquisition timelines for high- versus low-frequency words.

## Limitations
- The biologically plausible masking strategy relies on assumptions about camera FOV approximating infant visual perception, which may not hold across different egocentric datasets
- The temporal alignment between video and utterances assumes perfect co-occurrence, though infants likely learn from more complex temporal relationships
- The multimodal training relies on class-balanced curated datasets that may not reflect naturalistic language exposure

## Confidence
- **High confidence**: Blind spot masking performs comparably to random masking across multiple tasks (supported by consistent results across Labeled-S, Toybox, and CVCL evaluations)
- **Medium confidence**: Biologically plausible masking provides learning signals comparable to random masking (mechanism is clear but relies on FOV assumptions)
- **Medium confidence**: Word-referent mapping is learnable from egocentric video without explicit supervision (strong empirical results but limited by curated dataset biases)

## Next Checks
1. Test blind spot masking on non-egocentric datasets (e.g., Kinetics, Something-Something) to verify if biological plausibility provides advantages beyond infant learning scenarios
2. Implement a curriculum learning approach where masking ratio increases from 50% to 90% during pretraining to combine biological plausibility with standard reconstruction strength
3. Conduct ablation studies varying blind spot positions and sizes to determine if specific anatomical configurations provide unique advantages versus simpler peripheral-only masking