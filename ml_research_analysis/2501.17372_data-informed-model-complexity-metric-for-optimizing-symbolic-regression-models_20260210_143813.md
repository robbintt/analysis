---
ver: rpa2
title: Data-Informed Model Complexity Metric for Optimizing Symbolic Regression Models
arxiv_id: '2501.17372'
source_url: https://arxiv.org/abs/2501.17372
tags:
- complexity
- data
- metric
- dimensionality
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting models from genetic
  programming (GP) that generalize well beyond training data. The authors propose
  using intrinsic dimensionality (ID) estimation to guide model selection, aligning
  model complexity with the underlying structure of the data.
---

# Data-Informed Model Complexity Metric for Optimizing Symbolic Regression Models

## Quick Facts
- arXiv ID: 2501.17372
- Source URL: https://arxiv.org/abs/2501.17372
- Authors: Nathan Haut; Zenas Huang; Adam Alessio
- Reference count: 40
- Models selected within target dimensionality range achieved median normalized fitness of 0.010 ± 0.013

## Executive Summary
This paper addresses the challenge of selecting models from genetic programming (GP) that generalize well beyond training data. The authors propose using intrinsic dimensionality (ID) estimation to guide model selection, aligning model complexity with the underlying structure of the data. The core method involves two components: first, estimating the ID of datasets using 12 different algorithms from the scikit-dimension library to establish target complexity ranges; second, measuring model complexity through Hessian rank-based metric that approximates the effective dimensionality (ED) of evolved models. The approach selects models whose ED falls within the ID-estimated range, balancing model expressiveness and accuracy.

## Method Summary
The method uses StackGP to evolve symbolic regression models with R² fitness and combined stack length complexity. After evolution, a post-processing step filters models based on a data-informed complexity metric. First, the intrinsic dimensionality (ID) of each dataset is estimated using 12 algorithms from scikit-dimension, establishing a target range (mean ID ± 1 standard deviation). Second, each model's effective dimensionality (ED) is approximated by computing the rank of the average Hessian matrix at three strategic points (min, mean, and max target values). Models with ED within the target ID range are selected as they balance expressiveness and accuracy without the bias toward zero complexity common in traditional Pareto tournament selection.

## Key Results
- Models in the "Ideal" complexity range (ED within ID target window) achieved median normalized fitness of 0.010 ± 0.013
- All pairwise comparisons between complexity groups showed statistically significant differences (p < 0.05)
- Strategic sampling with 3 points provided reliable complexity estimates comparable to 100 random points, offering computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
Models whose effective dimensionality (ED) aligns with the dataset's intrinsic dimensionality (ID) generalize better than models selected by traditional complexity metrics. The ID estimate establishes a target complexity window (mean ID ± 1 std dev). Models with Hessian-rank ED within this window are selected, avoiding the implicit "zero complexity is optimal" assumption in standard Pareto tournament selection. This works under the assumption that data lies on or near a d-dimensional manifold with constant dimensionality almost everywhere.

### Mechanism 2
Hessian rank provides a computationally tractable approximation of model effective dimensionality that captures functional rather than syntactic complexity. The rank of the average Hessian matrix at strategic points counts independent directions of second-order variation. Higher rank → model sensitive in more independent parameter directions → more complex. This approximation works if the rank of the averaged Hessian converges to the manifold dimension under smooth tangent space and non-degenerate second derivative conditions.

### Mechanism 3
Strategic sampling using 3 points (min/mean/max of target values) provides model ED estimates comparable to 100 random points at ~30x lower computational cost. Rather than densely sampling the model surface, 3 points spanning the target value range capture diverse curvature behavior. The average Hessians at these points, when ranked, approximate global complexity. This works under the assumption that min/mean/max target points provide sufficient coverage of the model's curvature directions.

## Foundational Learning

- Concept: Intrinsic Dimensionality (ID) Estimation
  - Why needed here: Core to establishing target complexity ranges; must understand that different ID algorithms make different geometric assumptions and produce varying estimates on the same data.
  - Quick check question: Given a 100-feature dataset, why might ID estimates range from 5 to 50 across different algorithms?

- Concept: Hessian Matrix and Rank
  - Why needed here: Must understand second-order partial derivatives, matrix rank as counting independent directions, and why rank matters for complexity measurement.
  - Quick check question: Why would sin(x) have Hessian rank 1 despite high curvature, while x₁ + x₂ + x₃ has rank 0?

- Concept: Pareto Front Model Selection in GP
  - Why needed here: Context for why standard complexity metrics bias toward zero complexity and risk underfitting; the proposed method is a post-processing filter on Pareto front models.
  - Quick check question: In Pareto tournament selection, what percentage of models fall into the "simple but inaccurate" region, and why does this reduce evolutionary efficiency?

## Architecture Onboarding

- Component map: ID Estimation Module -> GP Evolution Engine -> Model Complexity Evaluator -> Selection Filter
- Critical path:
  1. Pre-compute ID profile for dataset (one-time, ~seconds with scikit-dimension)
  2. Run StackGP evolution (200 generations, standard GP pipeline)
  3. Extract Pareto front models by fitness/size
  4. For each Pareto model: compute average Hessian rank at 3 strategic points
  5. Filter to models with ED within target ID range
  6. Return filtered model set for deployment

- Design tradeoffs:
  - **Functional vs. syntactic complexity**: Hessian rank captures how model behaves, not tree size. More expensive but semantically meaningful.
  - **ID range vs. single ID value**: Multiple estimators produce varying results; using range (±1 std dev) accounts for manifold multiplicity and estimator disagreement.
  - **Post-processing vs. in-evolution selection**: Current method is post-hoc filter; integrating as selection pressure during evolution is computationally prohibitive (Hessian computation per model per generation).

- Failure signatures:
  - **Wide ID estimate variance** (std dev approaching or exceeding mean): Indicates dataset may violate constant-dimension manifold assumption; target range becomes uninformative.
  - **All filtered models have poor fitness**: Target ID range may be misestimated, or evolution failed to produce models in the correct complexity regime.
  - **Hessian rank computation errors**: Non-differentiable expressions may cause numerical instability in finite difference approximation.
  - **Strategic points miss key behavior**: If model complexity is concentrated in regions not covered by min/mean/max target points, ED will be underestimated.

- First 3 experiments:
  1. **Validate ID range sensitivity**: On 3 PMLB datasets with varying feature counts, compute ID using all 12 estimators; compare variance. If std dev/mean > 0.5, flag dataset as potentially problematic for this method.
  2. **Reproduce strategic sampling result**: For a single dataset, compute ED for 20 models using both 3 strategic points and 100 random points; verify ~70% exact match and ~98% within ±1 dimension as reported.
  3. **Compare selection quality**: Evolve models on 5 datasets; extract Pareto front; filter by ID-aligned ED vs. standard size-based selection; compare held-out test fitness distributions using Mann-Whitney test. Expected: ID-aligned models show statistically significantly better median test fitness.

## Open Questions the Paper Calls Out

- Can the Hessian rank-based complexity metric be computationally optimized to enable its integration as a selection pressure during evolution rather than only as a post-processing step?
- How should local intrinsic dimensionality differences and manifold multiplicity be incorporated to refine target complexity ranges?
- Why does the three-point strategic sampling strategy achieve 98% agreement with 100-point random sampling for Hessian rank estimation?
- Does the Hessian rank metric correctly capture complexity for models with primarily first-order variation (e.g., linear functions)?

## Limitations

- The method relies on the manifold hypothesis holding across all datasets, which may break down for heterogeneous or multi-scale data structures
- Strategic sampling with only 3 points may miss localized complexity regions in datasets with sharp transitions or discontinuities
- The method's dependence on differentiable expressions limits applicability to GP populations containing non-smooth functions
- High variance among the 12 ID estimators on complex datasets makes the target range potentially too wide or too narrow

## Confidence

- **High Confidence**: The core mechanism of using Hessian rank to approximate effective dimensionality - supported by well-established mathematical foundations
- **Medium Confidence**: The strategic sampling approach - computationally efficient but lacks extensive validation beyond reported match rates
- **Medium Confidence**: The overall ID-alignment hypothesis - statistical results show significant improvements, but depends on the strong manifold assumption

## Next Checks

1. **Dataset Manifold Validation**: For 5 PMLB datasets with varying feature counts, compute ID estimates from all 12 algorithms and calculate variance-to-mean ratio. Flag datasets where std_dev/mean > 0.5 as potentially violating constant-dimension manifold assumption.

2. **Strategic Sampling Robustness**: For 3 datasets spanning low (5-10 features) to high (50+ features) dimensionality, compare ED estimates from 3 strategic points vs. 100 random points across 50 models. Verify that ~70% show exact match and ~98% fall within ±1 dimension range.

3. **Generalization Across GP Variants**: Reproduce results using alternative GP frameworks (e.g., standard tree-based GP, LinearGP) with identical ID-alignment filtering. Compare held-out test fitness distributions to verify that improvements are not StackGP-specific.