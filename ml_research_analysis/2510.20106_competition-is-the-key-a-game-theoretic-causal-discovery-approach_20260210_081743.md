---
ver: rpa2
title: 'Competition is the key: A Game Theoretic Causal Discovery Approach'
arxiv_id: '2510.20106'
source_url: https://arxiv.org/abs/2510.20106
tags:
- causal
- discovery
- score
- learning
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of causal discovery from observational
  data, where existing methods either lack finite-sample guarantees or fail to scale.
  The authors propose a game-theoretic reinforcement learning framework, DDQN-CD,
  that frames causal discovery as a sequential game between a Double DQN agent and
  a strong baseline (GES or GraN-DAG).
---

# Competition is the key: A Game Theoretic Causal Discovery Approach

## Quick Facts
- arXiv ID: 2510.20106
- Source URL: https://arxiv.org/abs/2510.20106
- Reference count: 17
- One-line primary result: DDQN-CD guarantees never worse than warm-start opponent while achieving 30-40% SHD reduction on large networks

## Executive Summary
This paper addresses causal discovery from observational data by framing it as a sequential game between a Double DQN agent and a strong baseline (GES or GraN-DAG). The agent refines the opponent's solution through local edge edits guided by BIC-based rewards, while guaranteeing the output is never worse than the warm-start opponent. The method achieves near-perfect recovery on small networks, competitive performance on mid-sized graphs, and significant improvements on large networks, reducing SHD by 30-40% compared to GraN-DAG.

## Method Summary
The method formulates causal discovery as an MDP where a DDQN agent proposes edge edits (Add, Remove, Reverse) to refine a warm-start DAG from GES or GraN-DAG. Actions are masked to enforce acyclicity and edge budget constraints. The agent is trained with BIC-based rewards that balance likelihood fit against model complexity. The output is the maximizer between the agent's best graph and the warm-start opponent, guaranteeing the result is never worse than the baseline initialization.

## Key Results
- Near-perfect recovery on small networks (Asia, Lucas) with TPR=1.0, FDR=0.0, SHD=0
- Competitive performance on mid-sized graphs (Alarm, Hepar2)
- 30-40% SHD reduction on large networks (Dream, Andes) compared to GraN-DAG
- Theoretical guarantees: output never worse than opponent, warm-start accelerates convergence geometrically, finite-sample champion selection with high probability

## Why This Works (Mechanism)

### Mechanism 1: Champion-Challenger Safety via Maximization
The algorithm maintains a champion snapshot and returns the maximizer between the agent's best graph and the warm-start opponent. This trivial maximization guarantee ensures the output is never worse than the baseline. Core assumption: BIC scorer is computable and comparable across graphs. Break condition: If scorer produces NaN/Inf values or opponent graph is corrupted.

### Mechanism 2: Geometric Hitting-Time Acceleration from Warm-Start
If the shortest improving path from warm-start to 1-optimal DAG has length d, then P(hitting optimal in one episode) ≥ (ε*/A_max)^d. Better warm-starts have smaller d, exponentially improving expected hitting time. Core assumption: Persistent exploration with probability ε* > 0, finite action space, existence of strictly improving path. Break condition: If episode horizon < d, exploration collapses, or no improving path exists.

### Mechanism 3: Finite-Sample Champion Selection via Sub-Gaussian Concentration
With sufficient samples, the algorithm selects the true best candidate with probability ≥ 1-δ. Score differences are sums of i.i.d. sub-Gaussian variables, yielding P(error) ≤ |C|·exp(-n∆²_n/(8L²)). Core assumption: Gaussian data after preprocessing, L-Lipschitz score functions, unique population optimum. Break condition: If Lipschitz constant is misestimated, gap is tiny, or data violates Gaussian assumption.

## Foundational Learning

- **Concept: Double DQN (DDQN)**
  - Why needed here: Decouples action selection from evaluation to prevent Q-value overestimation in large discrete action spaces (O(p²) edges).
  - Quick check question: Can you explain why using the same network for both selection and evaluation causes overestimation bias?

- **Concept: BIC (Bayesian Information Criterion)**
  - Why needed here: Serves as reward signal balancing likelihood fit against model complexity. The log(n) penalty is critical for theoretical guarantees.
  - Quick check question: How does BIC's complexity penalty scale with sample size, and why does this matter for consistency?

- **Concept: DAG Acyclicity Constraints**
  - Why needed here: Action masking must prevent cycles. The paper uses implicit masking rather than continuous relaxation.
  - Quick check question: Given adjacency matrix A, how would you efficiently check if adding edge (i,j) creates a cycle?

## Architecture Onboarding

- **Component map**: Preprocess data (rank-Gaussian if non-binary) -> Run opponent (GES/GraN-DAG) -> Get A₀ -> For each episode: reset to A₀, run T steps with ε-greedy action selection -> Update Q-network via DDQN loss -> Track champion Ĝ -> Return max(Ĝ, A₀) after E episodes

- **Critical path**: 1) Preprocess data 2) Run opponent to get A₀ 3) Reset to A₀ each episode 4) ε-greedy action selection for T steps 5) DDQN update with Polyak target 6) Track champion 7) Return maximizer

- **Design tradeoffs**: GES warm-start: Faster, more stable, may inherit local optima. GraN-DAG warm-start: Potentially better initial graph, more expensive. Copula-BIC vs DiscreteBIC: Handles non-Gaussian data but adds preprocessing complexity.

- **Failure signatures**: SHD not improving → check ε or step cost c. Cycles appearing → action mask bug. Q-values diverging → target update rate too high or replay buffer too small. Worse than opponent → champion update logic issue.

- **First 3 experiments**: 1) Sanity check: Run on Asia with GES warm-start; expect TPR=1.0, FDR=0.0, SHD=0. 2) Ablation: Compare GES warm-start vs random initialization on Alarm; measure episodes to plateau. 3) Sample size scaling: Vary n ∈ {200, 400, 600, 800, 1000} on synthetic 30-node SEMs; plot P(mis-selection) vs n.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DDQN-CD be extended to temporal/dynamic causal graphs where DAG structure evolves over time?
- Basis in paper: [explicit] Conclusion mentions extending to temporal graphs as promising direction.
- Why unresolved: Current formulation assumes static DAG structure without time-varying relationships.
- What evidence would resolve it: Formulation extending action space and reward for time-indexed edges, validated on time-series benchmarks.

### Open Question 2
- Question: How can domain priors be systematically incorporated into DDQN-CD?
- Basis in paper: [explicit] Conclusion lists incorporating domain priors as promising direction; KCRL uses priors but DDQN-CD does not integrate this.
- Why unresolved: Current reward and action masking lack mechanism for soft/hard prior constraints beyond warm-start.
- What evidence would resolve it: Extending reward function or action mask to penalize/restrict prior violations, with controlled experiments.

### Open Question 3
- Question: How can Lipschitz constant L be practically estimated for real-world datasets?
- Basis in paper: [inferred] Theorem 3 depends critically on L, but paper doesn't discuss estimation strategies.
- Why unresolved: Without practical method to bound L, sample complexity bound cannot be applied.
- What evidence would resolve it: Systematic study estimating L empirically across datasets, or deriving dataset-dependent upper bounds.

## Limitations
- Q-network architecture unspecified (layers, dimensions, GNN usage)
- Hyperparameter opacity (λ, c, ε schedule, B, T values not stated)
- Scalability concerns with O(p²) action space growth causing memory issues on large graphs

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical guarantees (Theorems 1-3) | High |
| Experimental results on small/mid graphs | High |
| Scalability claims on large networks | Medium |
| Finite-sample performance with n≥600 | Medium |
| Hyperparameter impact on performance | Low |

## Next Checks

1. **Architecture verification**: Reconstruct DDQN agent using standard MLP with shared parameters across actions; compare SHD on Asia to reported values.

2. **Hyperparameter sensitivity**: Run ablation studies on λ, c, and ε values for Alarm network; identify performance degradation thresholds.

3. **Finite-sample validation**: On synthetic 30-node SEMs, measure P(mis-selection) across n ∈ {200, 400, 600, 800, 1000}; verify exponential decay matches Figure 3 predictions.