---
ver: rpa2
title: 'SCARE: A Benchmark for SQL Correction and Question Answerability Classification
  for Reliable EHR Question Answering'
arxiv_id: '2511.17559'
source_url: https://arxiv.org/abs/2511.17559
tags:
- question
- questions
- ambiguous
- unanswerable
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCARE, the first benchmark designed to evaluate
  post-hoc verification layers in EHR QA systems that perform both SQL correction
  and question answerability classification. SCARE contains 4,200 triples of questions,
  candidate SQL queries, and expected outputs across three major EHR databases (MIMIC-III,
  MIMIC-IV, eICU), with queries generated by seven different text-to-SQL models.
---

# SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering

## Quick Facts
- arXiv ID: 2511.17559
- Source URL: https://arxiv.org/abs/2511.17559
- Reference count: 40
- Introduces SCARE benchmark for evaluating SQL correction and answerability classification in EHR QA systems

## Executive Summary
This paper introduces SCARE (SQL Correction And Answerability Recognition for EHR QA), the first benchmark designed to evaluate post-hoc verification layers in EHR question answering systems that perform both SQL correction and question answerability classification. The benchmark contains 4,200 triples of questions, candidate SQL queries, and expected outputs across three major EHR databases (MIMIC-III, MIMIC-IV, eICU), with queries generated by seven different text-to-SQL models. The evaluation reveals critical trade-offs between preserving correct SQL and identifying problematic questions, with hybrid approaches combining iterative refinement and explicit classification signals performing best, though methods struggle significantly with nuanced ambiguity detection and global SQL error correction.

## Method Summary
SCARE was created through a comprehensive data collection process involving 1,152 MIMIC-III data questions, 864 MIMIC-IV data questions, and 1,152 eICU data questions, each paired with candidate SQL queries generated by seven text-to-SQL models (davinci-003, gpt-3.5-turbo, ChatGLM, ERNIE, TAPEX, Text2SQL-SA, and FROST). Each question-SQL pair was annotated for three answerability scenarios: answerable-correct (SQL query correctly retrieves the expected output), answerable-incorrect (SQL query retrieves incorrect output), and ambiguous (question has multiple valid interpretations). The dataset was then expanded by including 1,920 unanswerable questions from MIMIC-IV and eICU. The benchmark evaluates four scenarios: answerable-correct, answerable-incorrect, ambiguous, and unanswerable, providing a comprehensive framework for assessing verification methods.

## Key Results
- Iterative refinement approaches achieve the highest accuracy (77.2%) for binary answerability detection but struggle with nuanced ambiguity cases (F1 score 54.0%)
- Hybrid approaches combining iterative refinement and explicit classification signals perform best overall for SQL correction tasks
- Methods achieve only 30.8% correction rate for complex structural errors in SQL queries, indicating significant room for improvement
- Performance gap between identifying clearly unanswerable questions and detecting nuanced ambiguity represents a fundamental challenge in EHR QA verification

## Why This Works (Mechanism)
The benchmark works by providing a standardized evaluation framework that captures the complex trade-offs inherent in EHR QA verification. By including diverse question types across multiple EHR databases and text-to-SQL models, SCARE enables systematic comparison of methods that must balance between preserving correct SQL queries and identifying problematic questions. The four-scenario evaluation framework (answerable-correct, answerable-incorrect, ambiguous, and unanswerable) allows researchers to understand where different approaches succeed and fail, particularly in distinguishing between clearly wrong SQL and subtly ambiguous questions that require domain expertise.

## Foundational Learning

**SQL query generation from natural language**: Converting medical questions into executable SQL queries requires understanding both clinical terminology and database schema relationships.

*Why needed*: Medical questions often contain complex temporal reasoning and clinical concepts that must be accurately translated into database operations.

*Quick check*: Verify that generated SQL correctly handles temporal constraints and joins across multiple clinical tables.

**Answerability classification**: Determining whether a question can be answered given the available data and whether the SQL query correctly retrieves the expected output.

*Why needed*: Prevents both false negatives (missing answerable questions) and false positives (executing incorrect queries that could lead to wrong medical decisions).

*Quick check*: Ensure classification methods maintain high precision while achieving reasonable recall across all four answerability scenarios.

**Iterative refinement**: Progressive improvement of SQL queries through multiple refinement steps based on intermediate feedback.

*Why needed*: Allows correction of both syntactic errors and semantic misunderstandings that may occur in initial query generation.

*Quick check*: Track improvement in query correctness across refinement iterations to identify optimal stopping points.

## Architecture Onboarding

**Component map**: Question -> Text-to-SQL Model -> Candidate SQL -> Verification Layer -> (Corrected SQL or Rejection) -> Database

**Critical path**: Question input → SQL generation → Answerability classification → SQL correction (if needed) → Query execution → Result validation

**Design tradeoffs**: The benchmark reveals tension between preserving correct SQL queries and catching errors. Methods that aggressively correct queries may inadvertently modify correct ones, while conservative approaches may miss opportunities for correction. The ambiguity detection challenge shows that simple heuristics struggle with nuanced cases that require clinical understanding.

**Failure signatures**: Methods show systematic failures in detecting subtle ambiguities in medical questions, achieving only 54.0% F1 score. Global structural errors in SQL queries prove particularly challenging, with only 30.8% correction success rate. Binary classification approaches miss nuanced cases that fall between clearly answerable and clearly unanswerable.

**First 3 experiments**:
1. Evaluate binary answerability classification performance on the answerable-incorrect vs. answerable-correct scenarios to establish baseline detection capabilities
2. Test ambiguity detection methods on questions with multiple valid interpretations to assess ability to identify nuanced cases
3. Apply iterative refinement to SQL queries with known structural errors to measure correction effectiveness across different error types

## Open Questions the Paper Calls Out

The paper identifies several open questions regarding the scalability of the benchmark methods to additional EHR databases beyond the three included (MIMIC-III, MIMIC-IV, eICU), the need for clinical domain expert input to establish ground truth for nuanced ambiguity cases, and the potential for extending the framework to handle more complex clinical reasoning tasks that go beyond simple SQL query generation.

## Limitations

- Performance gap between identifying clearly unanswerable questions and detecting nuanced ambiguity represents a fundamental challenge, with ambiguity detection achieving only 54.0% F1 score
- Limited generalizability to other clinical data sources or newer generation text-to-SQL models due to focus on specific seven models and three EHR databases
- Evaluation primarily considers syntactic and semantic correctness without extensive assessment of clinical appropriateness of generated SQL queries
- Methods struggle significantly with global SQL error correction, achieving only 30.8% correction rate for complex structural errors

## Confidence

- **High confidence**: The benchmark creation methodology and dataset characteristics are well-documented and reproducible
- **Medium confidence**: Performance numbers for specific baseline methods, as they depend on implementation details and evaluation criteria
- **Medium confidence**: The claim that hybrid approaches perform best, given the relatively small number of compared methods and scenarios

## Next Checks

1. Test the benchmark methods on additional EHR databases beyond MIMIC-III, MIMIC-IV, and eICU to assess generalizability across different clinical data schemas
2. Evaluate the ambiguity detection methods with input from clinical domain experts to establish ground truth for nuanced cases that automated methods struggle with
3. Implement ablation studies on the iterative correction approach to quantify the contribution of individual refinement steps versus the overall correction framework