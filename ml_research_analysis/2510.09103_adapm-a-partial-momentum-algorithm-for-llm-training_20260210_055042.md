---
ver: rpa2
title: 'AdaPM: a Partial Momentum Algorithm for LLM Training'
arxiv_id: '2510.09103'
source_url: https://arxiv.org/abs/2510.09103
tags:
- momentum
- uni00000013
- adapm
- uni00000011
- low-rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes AdaPM, a memory-efficient optimizer for large
  language model training that achieves over 90% reduction in momentum memory while
  maintaining performance comparable to AdamW. The key insight is that different transformer
  blocks have varying momentum requirements: embedding and attention output blocks
  need no momentum, query/key/MLP blocks need low-rank momentum with bias correction,
  and value blocks require full momentum.'
---

# AdaPM: a Partial Momentum Algorithm for LLM Training

## Quick Facts
- **arXiv ID**: 2510.09103
- **Source URL**: https://arxiv.org/abs/2510.09103
- **Reference count**: 34
- **Primary result**: AdaPM achieves >90% reduction in momentum memory while maintaining AdamW-level performance across GPT-2 and Llama models (60M-1.5B parameters)

## Executive Summary
This paper introduces AdaPM, a memory-efficient optimizer for large language model training that dramatically reduces momentum memory requirements while preserving convergence performance. The key innovation is a non-uniform momentum design that assigns different momentum strategies to different transformer blocks based on their gradient characteristics. By combining this partitioning with a debiased low-rank estimator, AdaPM achieves over 90% memory reduction compared to AdamW. The method is validated across multiple model families and scales, demonstrating both comparable or better convergence and significant GPU time savings through memory-enabled larger batch sizes.

## Method Summary
AdaPM implements a three-part strategy: (1) non-uniform momentum partitioning that disables momentum for embedding and attention output blocks, uses low-rank momentum with bias correction for query/key/MLP blocks, and full momentum for value blocks; (2) a debiased low-rank estimator that tracks approximation residuals and applies correction terms to prevent bias accumulation from rank truncation; and (3) integration with Adam-mini for second-order statistics compression to achieve up to 95% total optimizer state memory reduction. The low-rank factorization is computed via warm-started gradient descent (K=5 iterations) and updated every T=100 steps with rank ratio r=5%.

## Key Results
- Achieves >90% reduction in momentum memory while maintaining AdamW-level convergence performance
- Reduces GPU hours by over 30% for GPT-2 1.5B pretraining through memory-enabled larger batch sizes
- Maintains comparable or better validation loss across GPT-2 and Llama model families (60M-1.5B parameters)
- When combined with Adam-mini, achieves up to 95% memory savings in optimizer states

## Why This Works (Mechanism)

### Mechanism 1: Non-uniform momentum partitioning
Different transformer blocks have varying momentum requirements: embedding and attention output blocks have sparse gradients with low temporal persistence, making momentum ineffective; query, key, and MLP blocks have gradients concentrated in low-rank subspaces; value blocks require full momentum. By selectively disabling or compressing momentum per block type, memory is drastically reduced while preserving optimization dynamics where momentum matters most.

### Mechanism 2: Debiased low-rank momentum estimator
Standard low-rank approximation discards residual components, causing accumulated bias. AdaPM tracks the approximation residual r_t and adds a correction term β₁r_t/(1-β₁) to the momentum estimate. Under the assumption that residuals are approximately stationary, this correction asymptotically eliminates bias, enabling aggressive rank reduction (to 5% of original) without convergence degradation.

### Mechanism 3: Memory-enabled throughput gains
Memory reduction enables larger batch sizes, which combined with reduced optimizer overhead, reduces total GPU hours by 30%+. By reducing momentum memory to ~5% of baseline, AdaPM frees GPU memory that can be reallocated to larger batch sizes, improving throughput and reducing communication overhead in distributed training.

## Foundational Learning

- **Concept**: Adam optimizer's first and second moments
  - Why needed here: AdaPM modifies Adam's first-order moment (momentum); understanding what Adam stores is prerequisite to understanding what AdaPM compresses.
  - Quick check question: What is the update rule for Adam's momentum term m_t, and what does β₁ control?

- **Concept**: Low-rank matrix approximation (SVD/factorization)
  - Why needed here: AdaPM projects momentum into a low-rank subspace; understanding why this loses information and how to factorize m×n matrices is essential.
  - Quick check question: If a matrix has rank r, how many parameters are needed to represent it in factored form L×R^T?

- **Concept**: Exponential moving average (EMA) bias
  - Why needed here: EMA estimates are biased toward zero at early steps; the correction term β₁r_t/(1-β₁) is analogous to Adam's own bias correction but applied to low-rank approximation error.
  - Quick check question: Why does Adam's m_t need to be divided by (1-β₁^t) for unbiased estimation?

## Architecture Onboarding

- **Component map**: Block classifier -> Momentum allocator -> Low-rank projector -> Residual tracker -> Bias corrector -> Second-moment updater

- **Critical path**: 
  1. Identify parameter block type during initialization
  2. For low-rank blocks: project momentum, track residual, apply correction at each step
  3. For full-momentum blocks: standard EMA update
  4. For no-momentum blocks: use raw gradient directly

- **Design tradeoffs**:
  - **Rank ratio** (r=2%-50%): Lower rank saves more memory but increases approximation error; paper finds 5% is sufficient with debiasing
  - **Update frequency** (T=10-5000): How often to recompute L, R; T=100 is default, balancing overhead and accuracy
  - **Block granularity**: Paper uses standard transformer blocks; finer partitioning (e.g., per-head) could further reduce memory but adds complexity

- **Failure signatures**:
  - **Convergence plateau at higher loss than AdamW**: Likely missing/incorrect bias correction; verify r_t computation and application
  - **Training instability**: May indicate value block is mistakenly assigned low-rank; verify block classification
  - **No memory reduction observed**: Check that low-rank blocks are identified and L, R are stored instead of full m_t

- **First 3 experiments**:
  1. **Ablation on bias correction**: Train GPT-2 124M with and without the correction term at rank=5%. Expect ~2x slower convergence without correction.
  2. **Rank sensitivity sweep**: Train GPT-2 124M with r={2%, 5%, 10%, 50%} and T={10, 100, 1000}. Plot final validation loss to confirm robustness.
  3. **Memory/throughput benchmark**: Train GPT-2 1.5B with AdamW vs AdaPM vs AdaPM-mini on identical hardware. Measure peak memory and GPU hours, targeting 44% and 94% reductions.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the non-uniform momentum partitioning strategy be successfully generalized to other prevalent architectures, such as diffusion models?
- **Open Question 2**: Can the bias-corrected low-rank estimator be applied to activation memory to further reduce training costs?
- **Open Question 3**: What is the theoretical justification for why Value blocks strictly require full momentum while Query and Key blocks do not?

## Limitations

- The non-uniform partitioning strategy is empirically derived but lacks theoretical justification for why specific transformer blocks require different momentum strategies
- The debiasing correction assumes stationary residuals, but this is not validated empirically under non-stationary conditions
- Experiments are limited to transformer-based models up to 1.5B parameters, with no validation on 70B+ parameter models
- No comparison against other memory-efficient optimizers on identical hardware and model scales

## Confidence

- **High confidence**: The 90%+ memory reduction claim is well-supported by the technical design and experimental measurements
- **Medium confidence**: The convergence performance claim is supported by experiments but limited to specific model families and scales
- **Low confidence**: The 30%+ GPU hours reduction claim combines memory savings with batch size scaling effects that depend heavily on hardware constraints

## Next Checks

1. **Validate bias correction sensitivity**: Train GPT-2 124M with rank=5% but systematically disable the correction term at different intervals (first 10% of steps, middle 50%, last 10%). Measure convergence slowdown to quantify the correction's impact across training phases.

2. **Test non-stationary conditions**: Design an experiment where the training distribution shifts abruptly (e.g., fine-tuning on a different domain after pretraining). Monitor whether the bias correction maintains effectiveness or if residuals become non-stationary.

3. **Compare against state-of-the-art**: Implement FMR (Low-rank Momentum Factorization) and FOAM (Blocked State Folding) on the same GPT-2 1.5B pretraining task. Measure both memory usage and final validation loss to establish relative performance.