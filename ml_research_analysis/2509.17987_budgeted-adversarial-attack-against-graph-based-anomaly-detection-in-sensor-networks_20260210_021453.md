---
ver: rpa2
title: Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor
  Networks
arxiv_id: '2509.17987'
source_url: https://arxiv.org/abs/2509.17987
tags:
- anomaly
- attack
- nodes
- detection
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BETA, a novel grey-box adversarial evasion
  attack against graph neural network (GNN) based anomaly detection systems in sensor
  networks. The attack operates under realistic budget constraints, where the attacker
  can only perturb a limited number of sensors while aiming to either suppress true
  anomalies or trigger false alarms at a target node.
---

# Budgeted Adversarial Attack against Graph-Based Anomaly Detection in Sensor Networks

## Quick Facts
- **arXiv ID:** 2509.17987
- **Source URL:** https://arxiv.org/abs/2509.17987
- **Reference count:** 40
- **Primary result:** Introduces BETA attack framework that reduces GNN-based anomaly detection accuracy by 30.62–39.16% under realistic budget constraints

## Executive Summary
This paper presents BETA, a novel grey-box adversarial evasion attack targeting graph neural network (GNN) based anomaly detection systems in sensor networks. The attack operates under realistic budget constraints, where an attacker can only perturb a limited number of sensors while attempting to either suppress true anomalies or trigger false alarms at target nodes. BETA combines GAFExplainer for identifying influential nodes, eigenvector centrality for budget-aware pruning, and constrained Projected Gradient Descent (PGD) for crafting stealthy perturbations. Experiments on three real-world datasets demonstrate significant performance degradation against state-of-the-art GNN detectors while maintaining attack stealthiness.

## Method Summary
BETA operates as a grey-box adversarial attack framework that targets GNN-based anomaly detection in sensor networks under budget constraints. The attack uses GAFExplainer to identify the most influential nodes affecting the anomaly score at a target node, then applies eigenvector centrality to prune these nodes within the given budget. Constrained PGD is then employed to craft imperceptible perturbations to the selected nodes' features. The framework aims to either suppress true anomalies or trigger false alarms while remaining stealthy by keeping perturbations below a threshold. The attack is evaluated against GNN detectors including DGI, GraphSage, and GIN on three real-world datasets.

## Key Results
- BETA reduces detection accuracy by 30.62–39.16% on average across SWaT, WADI, and SJVAir datasets
- Significantly outperforms baseline methods including Nettack and random attacks
- Maintains attack stealthiness through constrained perturbation magnitude
- Successfully suppresses true anomalies and triggers false alarms at target nodes under budget constraints

## Why This Works (Mechanism)
BETA exploits the inherent vulnerability of GNN-based anomaly detection systems to adversarial perturbations. By identifying and modifying the most influential nodes in the graph topology, the attack can effectively manipulate the anomaly scores computed by GNNs. The constrained PGD approach ensures that perturbations remain imperceptible while still being effective. The combination of GAFExplainer for node influence identification and eigenvector centrality for budget-aware pruning allows the attack to operate efficiently under realistic resource constraints, making it particularly practical for real-world sensor network scenarios.

## Foundational Learning
- **GNN-based anomaly detection**: Uses graph neural networks to identify anomalous behavior in sensor networks by learning representations from graph-structured data - needed to understand the attack target
- **Grey-box attack setting**: Attacker has limited knowledge of the model but full access to graph topology and anomaly labels - needed to establish attack assumptions
- **Budget-constrained optimization**: Attack operates under constraints on number of sensors that can be perturbed - needed to model realistic attack scenarios
- **GAFExplainer**: Method for identifying influential nodes in GNNs - needed for selecting attack targets
- **Eigenvector centrality**: Graph measure used for pruning influential nodes within budget - needed for efficient attack selection
- **Constrained PGD**: Optimization technique for crafting adversarial perturbations within bounds - needed for generating stealthy attacks

## Architecture Onboarding

**Component Map:** Input data -> GAFExplainer (node influence) -> Eigenvector centrality (budget pruning) -> Constrained PGD (perturbation) -> Modified graph -> GNN detector

**Critical Path:** Data preprocessing → GAFExplainer analysis → Budget-constrained node selection → Perturbation generation → Attack execution

**Design Tradeoffs:** The grey-box assumption balances attack realism with practical feasibility; budget constraints ensure physical plausibility but may limit attack effectiveness; constrained PGD ensures stealth but requires careful parameter tuning.

**Failure Signatures:** If perturbations exceed the threshold, the attack becomes detectable; if budget is too restrictive, attack effectiveness diminishes; if GAFExplainer fails to identify truly influential nodes, attack impact is reduced.

**First Experiments:** 1) Test attack effectiveness with varying budget sizes to find optimal trade-off point; 2) Evaluate attack performance against different GNN architectures (DGI, GraphSage, GIN); 3) Measure perturbation stealthiness by comparing attack success rate vs. detection rate.

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes grey-box setting which may not reflect all real-world scenarios
- Budget constraint model relies on assumptions about physically tamperable sensors
- Evaluation focuses on specific datasets with particular characteristics
- Limited generalizability to different network topologies or application domains

## Confidence

**High confidence:** Mathematical formulation of attack objective and constrained optimization approach are well-defined and technically sound

**Medium confidence:** Empirical results showing performance degradation against GNN-based detectors are convincing within tested datasets, though sample size is limited

**Medium confidence:** Stealthiness claims based on perturbation magnitude comparisons are reasonable but could benefit from more extensive evaluation across diverse metrics

## Next Checks
1. Test BETA against GNN models with different architectures and hyperparameters to assess robustness across model variations
2. Evaluate attack transferability to unseen datasets with different graph structures and anomaly distributions
3. Conduct physical feasibility analysis of the assumed budget constraints with domain experts to validate the attack's real-world applicability