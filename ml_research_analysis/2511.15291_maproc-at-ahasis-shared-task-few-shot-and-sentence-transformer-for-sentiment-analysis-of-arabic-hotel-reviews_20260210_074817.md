---
ver: rpa2
title: 'MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment
  Analysis of Arabic Hotel Reviews'
arxiv_id: '2511.15291'
source_url: https://arxiv.org/abs/2511.15291
tags:
- arabic
- sentiment
- training
- sentence
- dialects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses sentiment analysis of Arabic dialects in hotel
  reviews, which presents challenges due to linguistic diversity and data scarcity.
  The proposed solution employs SetFit (Sentence Transformer Fine-tuning), a few-shot
  learning method that fine-tunes sentence transformers using contrastive learning
  and trains a simple classification head.
---

# MAPROC at AHaSIS Shared Task: Few-Shot and Sentence Transformer for Sentiment Analysis of Arabic Hotel Reviews

## Quick Facts
- arXiv ID: 2511.15291
- Source URL: https://arxiv.org/abs/2511.15291
- Reference count: 8
- Primary result: 73% F1 on official AHaSIS evaluation set using 64-shot SetFit

## Executive Summary
This paper tackles the challenge of sentiment analysis for Arabic dialects in hotel reviews, specifically Moroccan (Darija) and Saudi dialects. The proposed solution employs SetFit, a few-shot learning framework that fine-tunes sentence transformers using contrastive learning followed by a simple classification head. Using Arabic-SBERT-100K as the base model and only 64 examples per class, the system achieved 73% F1 on the official evaluation set, ranking 12th among 26 participants. The approach demonstrates that high-quality sentence transformers, combined with efficient few-shot fine-tuning, can effectively handle nuanced dialectal Arabic text despite limited training data.

## Method Summary
The method employs SetFit (Sentence Transformer Fine-tuning), a data-efficient few-shot learning technique. The process involves two stages: first, fine-tuning the Arabic-SBERT-100K sentence transformer using contrastive learning on positive and negative text pairs to adapt embeddings to the target task; second, training a frozen classification head (logistic regression) on top of the adapted embeddings. The model was trained on 860 examples (430 per dialect) with 64 samples per class, using 3 epochs, batch size 16, and 20 iterations for text pair generation. Preprocessing included punctuation removal and normalization of Arabic letters (alif and hamza). All experiments were conducted on CPU to demonstrate computational efficiency.

## Key Results
- Achieved 73% F1 score on official AHaSIS evaluation set, ranking 12th out of 26 participants
- Arabic-SBERT-100K outperformed ArabicBERT_Finetuned-AR-500 (67.48% vs 52.83% F1 at 8 shots)
- Model showed strong performance with minimal data: 64 examples per class (192 total)
- Performance varied significantly by dialect and class: Darija neutral class scored 61.3% F1 vs Saudi neutral at 80% F1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SetFit's contrastive learning stage adapts pre-trained embeddings to the target task with minimal examples.
- Mechanism: The framework generates positive pairs (same-label sentences) and negative pairs (different-label sentences) from training data. The model learns to pull similar-sentiment embeddings closer while pushing dissimilar ones apart using CosineSimilarityLoss.
- Core assumption: The pre-trained Arabic-SBERT-100K already captures meaningful semantic representations that can be refined rather than learned from scratch.
- Evidence anchors:
  - [abstract] "SetFit (Sentence Transformer Fine-tuning) framework, a data-efficient few-shot learning technique"
  - [section 4.1] "positive pairs (similar sentences) and negative pairs (dissimilar sentences) are created from the training examples to teach the model to produce similar embeddings for sentences with the same label"
  - [corpus] Limited direct corpus evidence on contrastive learning efficacy for Arabic dialects specifically.
- Break condition: If base model embeddings are too dissimilar from target domain (dialectal hotel reviews), contrastive fine-tuning may fail to bridge the gap with limited examples.

### Mechanism 2
- Claim: Two-stage decoupling (embedding fine-tuning → classification head) enables efficient learning with limited compute.
- Mechanism: Stage 1 fine-tunes the sentence transformer on contrastive objective. Stage 2 freezes embeddings and trains a simple classification head (logistic regression). This avoids backpropagation through the full transformer during classification training.
- Core assumption: The embeddings after Stage 1 are sufficiently task-adapted that a linear classifier can separate sentiment classes.
- Evidence anchors:
  - [section 4.1] "The second stage involves training a classification head... on top of the frozen embeddings"
  - [section 5.1] "all experiments were conducted using CPU (i7-11850H) with 8 cores and 32GB of RAM to prove that this approach is not computationally expensive"
  - [corpus] Similar efficiency patterns observed in other Arabic NLP tasks using transformer fine-tuning.
- Break condition: If sentiment boundaries are highly non-linear in embedding space, a simple classification head will underfit.

### Mechanism 3
- Claim: Pre-trained Arabic sentence transformer quality strongly influences final performance.
- Mechanism: Arabic-SBERT-100K (based on AraBERT, fine-tuned on 100K Arabic sentences) provides domain-agnostic semantic representations. The model's exposure to diverse Arabic text during pre-training determines its ability to handle unseen dialectal expressions.
- Core assumption: The pre-training corpus contains sufficient dialectal variation to generalize to Moroccan and Saudi hotel reviews.
- Evidence anchors:
  - [section 4.2] "Arabic-SBERT-100K... is based on AraBERT base model and has been finetuned on a large corpus of Arabic text (100K sentences)"
  - [section 5.2] "The quality of the pre-trained sentence transformer is crucial"
  - [corpus] Adjacent work on Arabic dialects (AHaSIS shared task, Revisiting Common Assumptions about Arabic Dialects) confirms dialect generalization remains challenging.
- Break condition: If pre-training data under-represents specific dialects (e.g., Darija), downstream performance degrades—evidenced by lower neutral-class F1 (61.3%) for Darija vs. Saudi (80%).

## Foundational Learning

- Concept: **Sentence Transformers**
  - Why needed here: SetFit's entire architecture depends on understanding how sentence embeddings differ from token-level representations and how similarity is computed.
  - Quick check question: Can you explain why CosineSimilarityLoss is appropriate for embedding-level contrastive learning vs. cross-entropy for classification?

- Concept: **Few-Shot Learning Paradigm**
  - Why needed here: The paper's core claim is achieving competitive performance with only 64 examples per class. Understanding why this works requires grasping transfer learning and sample efficiency.
  - Quick check question: Why does increasing from 8 to 64 samples per class improve F1 from 67.48% to 78.87% (internal test), and what are the diminishing returns?

- Concept: **Arabic Dialectal Variation**
  - Why needed here: The task specifically targets Moroccan (Darija) and Saudi dialects, which differ significantly from MSA and each other in syntax, lexicon, and sentiment expression.
  - Quick check question: What linguistic features might explain the 18.7 point gap in neutral-class F1 between Darija (61.3%) and Saudi (80%)?

## Architecture Onboarding

- Component map: Raw Arabic text → Preprocessing (punctuation removal, letter normalization) → Arabic-SBERT-100K (Sentence Transformer) → Contrastive Fine-tuning (3 epochs, batch=16, iterations=20) → Frozen Embeddings → Classification Head (logistic regression) → Sentiment Label (positive/negative/neutral)

- Critical path: The sentence transformer selection (Arabic-SBERT-100K vs. alternatives) and shot count (samples per class) are the highest-impact decisions. Epochs and batch size showed secondary effects.

- Design tradeoffs:
  - **Shot count vs. data efficiency**: More samples improve performance but reduce few-shot benefits. Paper tested 8/16/32/64 with clear gains up to 64.
  - **Model selection**: Arabic-SBERT-100K outperformed ArabicBERT_Finetuned-AR-500 (67.48% vs. 52.83% at 8 shots), but no multilingual models were tested.
  - **Epochs**: 3 epochs optimal; 1 underfits, 5 shows degradation at low shot counts (64.22% at 8 shots).

- Failure signatures:
  - **Darija neutral class underperformance** (61.3% F1): Suggests dialect underrepresentation in pre-training or subtler sentiment expressions in Moroccan Arabic.
  - **Gap between internal test (78.87%) and official evaluation (73%)**: Possible distribution shift or overfitting to internal split.
  - **Alternative model failure**: ArabicBERT_Finetuned-AR-500's 52.83% indicates base model quality is non-negotiable.

- First 3 experiments:
  1. **Reproduce baseline with Arabic-SBERT-100K at 64 shots**: Verify 73% F1 on a held-out set matching official evaluation distribution. Log training time and memory.
  2. **Ablate shot count**: Test 8/16/32/64 samples per class on identical split. Plot F1 trajectory to confirm paper's trend and identify inflection point.
  3. **Dialect-stratified error analysis**: Isolate Darija neutral-class failures. Manually inspect 20-30 misclassified examples for patterns (code-switching, negation handling, implicit sentiment).

## Open Questions the Paper Calls Out

- Question: What specific linguistic characteristics cause the significant performance drop in the "neutral" sentiment class for the Moroccan dialect (Darija) compared to the Saudi dialect?
  - Basis: [inferred] The paper notes the model struggles with Darija neutral sentiment (61.3% F1) versus Saudi (80% F1), suggesting difficulty in capturing subtler expressions, but does not analyze the specific linguistic features causing this gap.
  - Why unresolved: The authors provided quantitative results but did not conduct a qualitative linguistic analysis of the errors specific to the Darija dialect.
  - What evidence would resolve it: A manual error analysis of false negatives/positives in the Darija neutral class, identifying ambiguous vocabulary or syntax.

- Question: Does increasing the few-shot sample count beyond 64 examples per class yield diminishing returns or continued linear gains for this specific dialectal task?
  - Basis: [explicit] The conclusion states, "Future work could explore... increasing the number of few-shot training examples could potentially enhance performance," as experiments were capped at 64 samples.
  - Why unresolved: The study was limited to a maximum of 64 samples per class; the performance ceiling for this specific SetFit configuration on Arabic dialects remains unknown.
  - What evidence would resolve it: Experimentation results using 128, 256, and full training set samples plotted against the F1 score.

- Question: Can alternative multilingual or Arabic-specific sentence transformers outperform Arabic-SBERT-100K in the SetFit framework for low-resource dialects?
  - Basis: [explicit] The authors explicitly list "experimenting with a wider range of pre-trained Arabic or multilingual sentence transformers" as an avenue for future work.
  - Why unresolved: The study primarily relied on Arabic-SBERT-100K and one alternative (ArabicBERT_Finetured-AR-500), leaving the potential of newer or multilingual models unexplored.
  - What evidence would resolve it: Benchmarks of the same SetFit pipeline using models like MARBERT or multilingual BERT on the AHaSIS dataset.

## Limitations

- Pre-training corpus dialectal coverage is unspecified, making it difficult to assess whether performance gaps stem from inherent linguistic differences or data scarcity.
- Lack of statistical significance testing across different shot counts and model variants leaves observed improvements uncertain.
- Evaluation set composition is not fully characterized - whether it represents balanced dialectal distribution or mirrors the training skew (35.81% positive, 39.06% negative, 25.11% neutral).

## Confidence

- **High confidence**: SetFit's two-stage architecture (contrastive fine-tuning + frozen classification head) efficiently adapts sentence transformers for few-shot learning, as evidenced by the consistent F1 improvement from 67.48% to 78.87% across internal experiments and the 73% official score.
- **Medium confidence**: Arabic-SBERT-100K is the optimal pre-trained model for this task, given its superior performance over ArabicBERT_Finetuned-AR-500 (67.48% vs 52.83% at 8 shots). However, without testing multilingual alternatives or other Arabic-specific models, this conclusion remains provisional.
- **Low confidence**: The specific mechanisms causing Darija neutral-class underperformance cannot be definitively isolated without access to the actual misclassified examples and a detailed linguistic analysis of sentiment expression differences between dialects.

## Next Checks

1. **Dialect-stratified error analysis**: Collect and manually annotate 50-100 misclassified examples from the official evaluation set, stratified by dialect and true label. Analyze whether Darija failures correlate with specific linguistic phenomena (code-switching, negation patterns, implicit sentiment) or data quality issues.

2. **Cross-dataset generalization test**: Evaluate the 64-shot trained model on an independent Arabic sentiment dataset (e.g., ArSarcasm-v2 or SemEval-2017 Arabic Sentiment) to assess whether the model overfits to AHaSIS-specific dialectal patterns or generalizes to broader Arabic sentiment expression.

3. **Ablation on preprocessing pipeline**: Systematically remove preprocessing steps (punctuation removal, alif/hamza normalization) and measure their individual impact on F1 score. This quantifies whether the claimed benefits are additive or whether certain transformations may harm dialectal understanding by removing dialect-specific markers.