---
ver: rpa2
title: 'Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned
  AI'
arxiv_id: '2512.03072'
source_url: https://arxiv.org/abs/2512.03072
tags:
- logical
- cognitive
- weight
- value
- atoms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes \"Weight-Calculatism,\" a cognitive architecture\
  \ aimed at addressing the challenges of explainability and value alignment in AI.\
  \ It deconstructs cognition into fundamental Logical Atoms and two core operations\u2014\
  Pointing and Comparison\u2014and formalizes decision-making through an interpretable\
  \ Weight-Calculation model (Weight = Benefit x Probability)."
---

# Beyond the Black Box: A Cognitive Architecture for Explainable and Aligned AI

## Quick Facts
- **arXiv ID:** 2512.03072
- **Source URL:** https://arxiv.org/abs/2512.03072
- **Reference count:** 19
- **Key outcome:** Proposes "Weight-Calculatism," a cognitive architecture addressing explainability and value alignment by decomposing cognition into Logical Atoms and two core operations—Pointing and Comparison—formalized through an interpretable Weight-Calculation model (Weight = Benefit × Probability).

## Executive Summary
This paper introduces Weight-Calculatism, a cognitive architecture designed to overcome the black-box problem in AI by creating transparent, traceable decision-making processes. The architecture decomposes cognition into fundamental Logical Atoms and two core operations—Pointing and Comparison—enabling decisions to be traced back to a finite set of auditable Initial Weights. Through experiments on moral dilemmas and novel situations like alien ecosystems, the framework demonstrates superior explainability and value alignment compared to existing paradigms, establishing a theoretical foundation for building trustworthy Artificial General Intelligence.

## Method Summary
Weight-Calculatism implements decision-making through a symbolic cognitive architecture where all reasoning is traceable to a finite set of Initial Weights. The system uses Logical Atoms (nodes) connected by Pointing relations (edges) to propagate activation signals, with the Central Computation Area integrating these activations to compute weights for candidate actions using the formula Weight = Σ(InitialWeight_i × Relevance_i). When encountering novel situations, the architecture employs subgraph matching and analogical reasoning to dynamically create new relations without requiring pre-compiled rules, enabling generalization while maintaining interpretability.

## Key Results
- Demonstrated interpretable decision-making in fire escape moral dilemma, with "carry scientific-notes" achieving total weight of 20.80 versus "carry canned-food" at 0.80
- Successfully handled novel alien ecosystem scenario through analogical reasoning, achieving similarity score of 0.106 to Earth biology and selecting "remote-monitoring" as optimal decision
- Established framework for building trustworthy, value-aligned, and generalizable Artificial General Intelligence through transparent weight-based decision arbitration

## Why This Works (Mechanism)

### Mechanism 1
Decomposing cognition into Logical Atoms with Pointing operations enables traceable reasoning chains from decisions back to foundational values. Each cognitive object is stored as an indivisible node that propagates signals along explicit "Pointing" edges, allowing any final decision to be traced backward through the activation path. The Central Computation Area integrates these activations, making all decisions auditable to an Initial Weight set.

### Mechanism 2
The Weight-Calculation formula (Weight = Benefit × Probability) provides human-interpretable decision arbitration by recursively decomposing benefit into Initial Weights. For any candidate action, the system computes Weight as the sum of (InitialWeight_i × Relevance_i), where benefit traces along causal chains back to axiomatic values like survival or knowledge. Probability is derived from connection strengths, and the action with highest weight is selected.

### Mechanism 3
Comparison operations on subgraph structures enable analogical reasoning for novel situations without pre-compiled rules. When encountering unfamiliar inputs, the system performs subgraph matching between current situations and stored cognitive patterns, using similarity scores to drive hypothesis generation. New Pointing relations can be dynamically created from partial matches, allowing the system to address novel domains through composition rather than statistical pattern matching.

## Foundational Learning

- **Concept: Graph-based knowledge representation**
  - **Why needed here:** The entire Logical Atom system is implemented as a dynamic layered graph where nodes are atoms and edges are Pointing relations
  - **Quick check question:** Can you explain how activation propagation differs from gradient-based backpropagation in neural networks?

- **Concept: Symbolic vs. sub-symbolic reasoning**
  - **Why needed here:** Weight-Calculatism explicitly rejects purely statistical interpolation in favor of interpretable symbolic operations
  - **Quick check question:** What is the tradeoff between rule interpretability and handling noisy real-world data?

- **Concept: Value alignment and instrumental convergence**
  - **Why needed here:** The Initial Weight library is the axiomatic foundation for all decisions; understanding alignment failure modes is critical for safe design
  - **Quick check question:** If Initial Weights specify "survival" at high priority, what instrumental goals might an agent derive, and what failure modes could emerge?

## Architecture Onboarding

- **Component map:** Cognitive Library -> Preprocessors -> Central Computation Area -> Weight Library -> Decision Engine
- **Critical path:**
  1. Define minimal Initial Weight set (e.g., survival, harm-avoidance, knowledge)
  2. Construct core Logical Atom layer (100-500 fundamental atoms: space, time, causality)
  3. Implement Pointing relation schema and activation propagation algorithm
  4. Build Central Computation Area with Weight-Calculation arbitration
  5. Validate on constrained moral dilemma before domain expansion

- **Design tradeoffs:**
  - **Transparency vs. complexity:** Larger cognitive graphs increase capability but complicate traceability
  - **Reflexive vs. deliberative channels:** Fast channel improves real-time response but reduces explainability for edge cases
  - **Manual vs. automated atom construction:** Manual ensures semantic clarity; automated scales faster but risks semantic drift

- **Failure signatures:**
  - Decisions that cannot be traced to Initial Weights indicate incomplete benefit decomposition
  - High activation entropy (many nodes activated with similar weights) suggests underspecified relevance calculations
  - Repeated selection of low-weight actions indicates probability estimation errors or graph connectivity gaps

- **First 3 experiments:**
  1. **Fire escape dilemma replication:** Verify weight calculation traceability and compare against paper's reported values (scientific-notes: 20.80 vs. canned-food: 0.80)
  2. **Ablation on Initial Weight set:** Remove one value (e.g., "knowledge") and measure decision divergence to test sensitivity
  3. **Novel domain injection:** Introduce 10 new Logical Atoms from an unseen domain (e.g., medical diagnosis) and verify analogical reasoning without retraining

## Open Questions the Paper Calls Out

### Open Question 1
**Can the proposed hybrid strategy (manual core + LLM extraction) generate "Pointing" relations with sufficient symbolic precision to avoid the correlation errors found in deep learning?**
- **Basis:** The section "Feasible Path for Cognitive Library Construction" proposes using LLMs for automation but acknowledges the need for a "layered strategy" and manual refinement
- **Why unresolved:** LLMs are probabilistic and prone to hallucinations; the paper provides no method for automatically verifying the logical validity of machine-generated relations
- **What evidence would resolve it:** Empirical validation showing an automated pipeline producing a logical graph with high semantic consistency and low error rates

### Open Question 2
**Does the partitioned Global Workspace architecture maintain real-time decision-making speeds as the cognitive graph scales to AGI-level complexity?**
- **Basis:** The paper claims a "feasible path" for scalability via partitioned sub-graphs but provides no complexity analysis or benchmarks for large-scale graphs
- **Why unresolved:** Graph traversal and subgraph matching are computationally expensive; the theoretical "sparsity" assumption may not hold in complex, interconnected domains
- **What evidence would resolve it:** Performance benchmarks demonstrating sub-second decision latency on cognitive graphs containing millions of Logical Atoms

### Open Question 3
**How does the system ground abstract Logical Atoms in raw sensory data without requiring the massive datasets the author critiques in the "Infinite Data Trap"?**
- **Basis:** The paper defines Logical Atoms as "conscious experiences" derived from perception but explicitly criticizes embodied AI for needing millions of trials to learn basic interactions
- **Why unresolved:** The mechanism for converting high-dimensional sensory noise into discrete, symbolic "atoms" efficiently remains undefined
- **What evidence would resolve it:** A demonstration of the system learning a sensorimotor concept (e.g., "obstacle avoidance") with significantly fewer training iterations than model-free RL agents

## Limitations
- Logical Atom graph specification is incomplete in main paper, requiring supplementary materials for full reproducibility
- Weight calculation parameters (Relevance, Probability normalization) lack explicit mathematical definitions
- No empirical validation against real-world datasets or comparison with established XAI benchmarks

## Confidence
- **High confidence:** The theoretical decomposition of cognition into Logical Atoms + Pointing operations is internally consistent and traceable
- **Medium confidence:** The Weight-Calculation formula provides interpretable decision arbitration, but sensitivity to Initial Weight specification remains untested at scale
- **Low confidence:** Analogical reasoning claims for novel situations are supported by a single illustrative experiment without systematic validation across diverse domains

## Next Checks
1. Replicate the fire escape scenario with full Logical Atom specification and verify the reported weight ordering (scientific-notes: 20.80 > canned-food: 0.80)
2. Conduct systematic ablation studies removing individual Initial Weights to measure impact on decision robustness
3. Test analogical reasoning transfer on 10+ novel domains with quantitative similarity metrics and blind human evaluation of explanation quality