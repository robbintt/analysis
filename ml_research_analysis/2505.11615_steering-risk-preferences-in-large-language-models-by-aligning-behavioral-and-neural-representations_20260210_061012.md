---
ver: rpa2
title: Steering Risk Preferences in Large Language Models by Aligning Behavioral and
  Neural Representations
arxiv_id: '2505.11615'
source_url: https://arxiv.org/abs/2505.11615
tags:
- steering
- risk
- vectors
- risky
- certainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a principled method for steering large language
  models (LLMs) by aligning behavioral and neural representations of latent constructs.
  The core idea is to use Markov Chain Monte Carlo (MCMC) with an LLM to elicit behavioral
  representations of risk preference, then align these with neural activations to
  derive steering vectors.
---

# Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations

## Quick Facts
- arXiv ID: 2505.11615
- Source URL: https://arxiv.org/abs/2505.11615
- Authors: Jian-Qiao Zhu; Haijiang Yan; Thomas L. Griffiths
- Reference count: 40
- One-line primary result: Self-alignment method enables precise steering of LLM risk preferences by aligning behavioral and neural representations, outperforming Contrastive Activation baselines.

## Executive Summary
This paper introduces a principled method for steering large language models (LLMs) by aligning behavioral and neural representations of latent constructs. The core idea is to use Markov Chain Monte Carlo (MCMC) with an LLM to elicit behavioral representations of risk preference, then align these with neural activations to derive steering vectors. This self-alignment approach enables precise control of LLM outputs without retraining or fine-tuning. The authors demonstrate their method across three domains: risky decision-making, risk perception, and text generation, showing consistent improvements over existing techniques while maintaining interpretability through alignment with behavioral measures.

## Method Summary
The method consists of two main phases: behavioral elicitation and neural alignment. First, MCMC is used to sample binary gamble choices from the LLM, constructing a Markov chain that converges to a stationary distribution reflecting latent risk preferences. Second, neural activations from gamble evaluation prompts are aligned with these behavioral representations via Lasso regression. The resulting coefficients form steering vectors that, when injected into the residual stream at inference time, successfully modulate the model's risk-related outputs. The approach requires access to model weights for activation extraction but enables steering without fine-tuning or retraining.

## Key Results
- Steering vectors derived from aligned representations (MCMC and Certainty Equivalent methods) consistently outperform Contrastive Activation baselines across three domains.
- MCMC-derived vectors achieved significantly higher steerability compared to alternatives when steering risky choices in Gemma-2-9B-Instruct.
- When steering risk perception across 150 real-world events, MCMC vectors showed the largest effects, significantly outperforming both Certainty Equivalent and Contrastive Activation methods.
- The approach reveals that earlier layers are more effective for steering perception while later layers work better for decisions, aligning with cognitive processing hierarchies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM risk preferences can be characterized as probabilistic representations recoverable through repeated behavioral sampling
- Mechanism: MCMC with LLMs constructs a Markov chain from sequential binary gamble choices. The Barker acceptance function (resembling Luce's choice rule) ensures the chain converges to a stationary distribution reflecting latent risk preferences within a probability triangle space.
- Core assumption: LLM risk preferences are context-dependent and probabilistic, requiring non-parametric estimation rather than point estimates.
- Evidence anchors:
  - [abstract] "Markov chain Monte Carlo with LLMs to elicit behavioral risk representations"
  - [section 3] "the resulting sequence of samples produced by the Markov chain converges to a stationary distribution that reflects the model's latent representations"
  - [corpus] Related steering papers focus on activation-space methods; this behavioral elicitation approach is not directly comparable in corpus.
- Break condition: If LLM choices violate detailed balance or ergodicity, the Markov chain will not converge to a meaningful preference distribution.

### Mechanism 2
- Claim: Aligning behavioral and neural representations via regression produces effective steering vectors.
- Mechanism: Neural activations from gamble evaluation prompts are regressed onto behaviorally elicited preference scores using Lasso regression (L1=10). The resulting coefficients—each corresponding to a residual stream dimension—form the steering vector, interpreted as encoding the model's risk preference.
- Core assumption: Neural activations during gamble evaluation contain risk-relevant information that correlates with behaviorally elicited representations, and linear combinations of activations can capture this relationship.
- Evidence anchors:
  - [abstract] "aligns these with neural activations from the same model to derive steering vectors"
  - [section 3] "we aligned the two representations by regressing the behavioral estimates onto the neural activations... The resulting regression coefficients... are interpreted as reflecting the LLM's risk preference"
  - [corpus] "steering vectors are a promising approach to aligning language model behavior at inference time" (arXiv:2505.01162), but limitations exist.
- Break condition: If behavioral and neural representations are not linearly related, or if Lasso regularization discards relevant features, steering efficacy will degrade.

### Mechanism 3
- Claim: Residual stream injection at appropriate layers modulates risk-related outputs in predictable directions.
- Mechanism: At inference, the steering vector is scaled by a multiplier (positive for risk-seeking, negative for risk-averse) and added to the residual stream at a chosen layer at each token position. The model continues its forward pass, producing steered outputs.
- Core assumption: The residual stream contains behaviorally relevant directions that can be linearly perturbed without corrupting other model capabilities.
- Evidence anchors:
  - [abstract] "When injected into the residual stream during inference, these vectors successfully modulate the model's risk-related outputs"
  - [section 5] "steering vectors (scaled by a predefined multiplier ranging between -900 to +900) were added to the model's residual stream at each token position"
  - [corpus] "how steering effectiveness varies across different behavior types and whether the nature of the targeted behavior affects control reliability" (arXiv:2511.18284)
- Break condition: If applied at suboptimal layers or with excessive multipliers, steering may produce incoherent outputs or fail to affect behavior.

## Foundational Learning

- Concept: Markov Chain Monte Carlo (MCMC) and detailed balance
  - Why needed here: The paper uses MCMC to elicit behavioral representations from LLMs. Understanding why the Barker acceptance function satisfies detailed balance—and why this matters for convergence—is essential.
  - Quick check question: Why does the symmetric proposal distribution (q(z'|z) = q(z|z')) simplify the detailed balance condition, and what role does the Barker acceptance function play?

- Concept: Transformer residual streams and activation engineering
  - Why needed here: Steering vectors are injected into residual streams at inference time. Understanding how residual streams propagate information across layers informs layer selection.
  - Quick check question: If you modify the residual stream at layer l, which subsequent computations are affected, and what does this imply for choosing steering layers?

- Concept: Lasso regression (L1 regularization) for feature selection
  - Why needed here: The alignment step uses Lasso to identify which neural dimensions encode risk preference. L1 regularization drives irrelevant coefficients to zero.
  - Quick check question: Why might L1 regularization be preferred over L2 when the goal is to identify a sparse set of behaviorally relevant neurons?

## Architecture Onboarding

- Component map:
  MCMC sampler -> Behavioral representation -> Neural activation extractor -> Alignment module -> Steering injector

- Critical path:
  1. Run MCMC with LLM over gamble space (3,000 trials).
  2. Extract neural activations for each sampled gamble.
  3. Fit Lasso regression; extract coefficients as steering vector.
  4. Identify optimal layer by maximizing steerability metric under extreme multipliers.
  5. Inject vector at inference with task-appropriate multiplier and layer.

- Design tradeoffs:
  - **MCMC vs. Certainty Equivalent**: MCMC yields richer, more nuanced representations but requires more sampling.
  - **Layer selection**: Earlier layers better for risk perception; later layers better for risky decisions.
  - **Multiplier magnitude**: Larger magnitudes increase steerability but may degrade output quality.

- Failure signatures:
  - **Low steerability score**: Steering vector does not capture a behaviorally relevant direction.
  - **Inconsistent transfer**: Vector effective on training gambles but not on held-out tasks suggests overfitting.
  - **Ceiling effects**: Model already at 100% choice probability for risky option (e.g., Gemma-2-2B) prevents steering evaluation.

- First 3 experiments:
  1. **Layer sweep with extreme multipliers**: For each candidate layer, compute steerability metric (Equation 3) using multipliers ±900 on a held-out gamble set. Select layer with maximum steerability.
  2. **Multiplier titration**: At optimal layer, vary multiplier (e.g., -900, -600, -300, 0, +300, +600, +900) and measure choice probability shifts. Characterize dose-response curve.
  3. **Cross-domain transfer**: Apply steering vectors derived from gamble choices to (a) risk perception ratings and (b) free-text generation about risky events. Assess whether vectors generalize beyond training domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical and mechanistic links between behaviorally elicited representations and their corresponding neural activations in LLMs?
- Basis in paper: [explicit] "Further theoretical and mechanistic interpretability research is needed to establish a more concrete link between behaviorally elicited representations and their corresponding neural activations."
- Why unresolved: The method empirically aligns behavioral and neural representations via regression, but does not explain *why* this alignment enables effective steering or what underlying mechanism connects the two.
- What evidence would resolve it: Causal mediation analyses, ablation studies, or mechanistic interpretability methods that trace how behavioral preferences map onto specific neural circuits.

### Open Question 2
- Question: How can optimal layers for steering be systematically identified across different tasks and model architectures?
- Basis in paper: [explicit] "Identifying optimal layers for steering remains an open question for future research."
- Why unresolved: The paper finds that risk perception steers best at earlier layers while risky decisions require later layers, but provides no general principle for layer selection.
- What evidence would resolve it: Systematic studies across tasks, model sizes, and architectures to identify predictive factors (e.g., task type, representation depth) for optimal steering layers.

### Open Question 3
- Question: Can new descriptive frameworks be developed to characterize and predict LLM risky choice behavior, given that classical human models (EUT, CPT) do not capture it?
- Basis in paper: [explicit] "Future research should consider developing new descriptive frameworks tailored to characterizing and predicting the risky choices of LLMs."
- Why unresolved: Neither Expected Utility Theory nor Cumulative Prospect Theory qualitatively matches the risk preferences exhibited by Gemma-2-9B-Instruct.
- What evidence would resolve it: Large-scale behavioral elicitation across diverse LLMs, followed by model-fitting to identify regularities unique to machine decision-making.

### Open Question 4
- Question: Does the self-alignment approach generalize to proprietary models where weights are inaccessible?
- Basis in paper: [explicit] "This form of steering is limited in scope for proprietary models where weights are not accessible, restricting its broader applicability."
- Why unresolved: The method requires access to residual stream activations, which are unavailable in black-box API-only models.
- What evidence would resolve it: Development of input-level or decoding-level interventions that approximate the steering effect without direct activation access.

## Limitations
- The behavioral elicitation method relies on MCMC convergence assumptions that may not hold for all LLMs or domains.
- The self-alignment approach assumes that behavioral and neural representations are linearly related and discoverable via Lasso regression.
- Layer selection optimization through extreme multipliers may produce results that don't generalize to moderate steering scenarios.

## Confidence
**High confidence**: The MCMC behavioral elicitation produces coherent preference distributions that align with expected patterns (higher variance preferred, higher EV gambles preferred). The steering methodology is technically sound and implementation details are clearly specified. Layer differences for perception versus decision steering are consistently observed.

**Medium confidence**: Cross-domain transfer results show MCMC vectors outperforming alternatives, but the effect sizes vary considerably across domains and tasks. The superiority of MCMC over Certainty Equivalent methods is demonstrated but the margin may be task-dependent. Layer selection optimization reliably identifies effective steering layers, though the relationship between multiplier magnitude and steerability isn't fully characterized.

**Low confidence**: The generalizability of steering vectors across different LLM architectures beyond Gemma models remains untested. The method's robustness to prompt variations, temperature settings, or different elicitation protocols is not established. Long-term behavior effects and potential degradation of other model capabilities during steering are not investigated.

## Next Checks
1. **Convergence diagnostics**: Implement trace plots, Gelman-Rubin statistics, and autocorrelation analysis for MCMC chains across multiple LLM runs to empirically validate stationarity assumptions.

2. **Nonlinear alignment exploration**: Replace Lasso regression with nonlinear methods (e.g., neural network regression) to determine whether behavioral-neural alignment captures nonlinear relationships and whether this improves steering efficacy.

3. **Cross-architecture transfer**: Apply steering vectors trained on Gemma models to other LLM architectures (e.g., Llama, Mistral) to assess whether behavioral-neural alignment generalizes across model families or requires model-specific calibration.