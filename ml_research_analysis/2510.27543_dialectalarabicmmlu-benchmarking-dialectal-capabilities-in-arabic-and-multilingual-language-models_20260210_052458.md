---
ver: rpa2
title: 'DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual
  Language Models'
arxiv_id: '2510.27543'
source_url: https://arxiv.org/abs/2510.27543
tags:
- arabic
- dialectal
- language
- dialects
- dialect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DialectalArabicMMLU, a new benchmark for
  evaluating large language models (LLMs) across Arabic dialects. While recent Arabic
  benchmarks have advanced evaluation for Modern Standard Arabic (MSA), dialectal
  varieties remain underrepresented despite their prevalence in everyday communication.
---

# DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and Multilingual Language Models

## Quick Facts
- arXiv ID: 2510.27543
- Source URL: https://arxiv.org/abs/2510.27543
- Reference count: 0
- This paper introduces DialectalArabicMMLU, a new benchmark for evaluating large language models (LLMs) across Arabic dialects.

## Executive Summary
This paper introduces DialectalArabicMMLU, a new benchmark for evaluating large language models (LLMs) across Arabic dialects. While recent Arabic benchmarks have advanced evaluation for Modern Standard Arabic (MSA), dialectal varieties remain underrepresented despite their prevalence in everyday communication. DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding 15K QA pairs across 32 academic and professional domains.

The benchmark enables systematic assessment of LLM reasoning and comprehension beyond MSA. We evaluate 19 open-weight Arabic and multilingual LLMs (1B-13B parameters) and report substantial performance variation across dialects, revealing persistent gaps in dialectal generalization. Models show significant performance drops when handling dialectal Arabic compared to MSA and English, with accuracy ranging from 34.5% to 58.7% across dialects versus 51.9% for MSA and 62.8% for English on average. DialectalArabicMMLU provides the first unified, human-curated resource for measuring dialectal understanding in Arabic, promoting more inclusive evaluation and future model development.

## Method Summary
DialectalArabicMMLU extends the MMLU-Redux framework through manual translation and adaptation of 3K multiple-choice question-answer pairs into five major dialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding 15K QA pairs across 32 academic and professional domains. The benchmark was created by professional translators who manually translated and adapted questions from the original MMLU-Redux dataset, ensuring dialectal authenticity while preserving semantic meaning. The evaluation covers 19 open-weight Arabic and multilingual LLMs ranging from 1B to 13B parameters, testing their ability to handle dialectal Arabic across diverse domains including humanities, STEM, social sciences, and professional topics.

## Key Results
- Models show significant performance drops when handling dialectal Arabic compared to MSA and English, with accuracy ranging from 34.5% to 58.7% across dialects versus 51.9% for MSA and 62.8% for English on average.
- Substantial performance variation across dialects reveals persistent gaps in dialectal generalization, with Egyptian and Syrian dialects showing relatively better performance compared to Moroccan and Emirati varieties.
- The benchmark exposes systematic weaknesses in current Arabic and multilingual LLMs, demonstrating that even models with billions of parameters struggle with dialectal understanding despite strong MSA performance.

## Why This Works (Mechanism)
The benchmark works by providing a standardized evaluation framework that isolates dialectal comprehension from other linguistic factors. By using multiple-choice questions with consistent answer options across dialects, it enables direct comparison of model performance while controlling for question difficulty and domain knowledge requirements. The manual translation process ensures that dialectal variations are authentic and representative of real-world usage patterns, rather than artificial constructions. The large number of question-answer pairs (15K total) provides statistical power to detect meaningful differences in model capabilities across dialects.

## Foundational Learning
- Arabic diglossia (why needed: to understand why dialectal Arabic evaluation is critical for real-world applications; quick check: verify that models can handle both MSA and dialectal varieties)
- MMLU benchmark framework (why needed: to ensure compatibility with established evaluation standards; quick check: confirm question domains map correctly to MMLU categories)
- Dialectal variation patterns (why needed: to ensure benchmark covers major dialectal differences; quick check: validate that translations capture genuine dialectal features)
- Multiple-choice QA evaluation methodology (why needed: to enable consistent cross-dialect comparison; quick check: verify answer consistency across dialect versions)
- Human translation quality assurance (why needed: to ensure benchmark reliability and authenticity; quick check: have multiple translators review samples for consistency)

## Architecture Onboarding
Component map: DialectalArabicMMLU dataset -> Multiple-choice QA format -> LLM inference pipeline -> Performance metrics -> Comparative analysis
Critical path: Translation and adaptation -> Question verification -> LLM evaluation -> Result aggregation -> Analysis and reporting
Design tradeoffs: Manual translation ensures quality but limits scalability; multiple-choice format enables consistency but may not capture full dialectal understanding; coverage of 5 dialects provides breadth but excludes other important varieties
Failure signatures: Inconsistent translation quality across dialects; domain-specific performance drops; generalization failures between closely related dialects
First experiments: 1) Evaluate single model across all dialects to establish baseline performance patterns, 2) Compare model performance on MSA vs dialectal versions of identical questions, 3) Test model sensitivity to dialect-specific vocabulary and grammatical features

## Open Questions the Paper Calls Out
The paper notes that this work provides a valuable first step toward evaluating dialectal Arabic capabilities in LLMs, but several important limitations and uncertainties remain. The benchmark covers only five dialects, excluding others like Iraqi, Tunisian, and Sudanese varieties that together represent substantial portions of Arabic speakers. The manual translation process, while careful, introduces potential consistency issues across translators and dialects. The 3K question set, though substantial, represents a limited sample of the 15K possible MMLU-Redux questions, raising questions about comprehensive coverage across all domains.

## Limitations
- Benchmark covers only five dialects, excluding others like Iraqi, Tunisian, and Sudanese varieties that together represent substantial portions of Arabic speakers
- Manual translation process introduces potential consistency issues across translators and dialects
- The 3K question set represents a limited sample of the 15K possible MMLU-Redux questions, raising questions about comprehensive coverage across all domains
- Evaluation focuses exclusively on multiple-choice QA format, which may not fully capture dialectal understanding capabilities needed for real-world applications

## Confidence
- High confidence: The benchmark construction methodology and evaluation results are sound and reproducible
- Medium confidence: The interpretation that performance gaps reflect dialectal understanding limitations rather than other factors
- Medium confidence: The claim that this is the "first unified, human-curated resource" for dialectal Arabic evaluation

## Next Checks
1. Evaluate additional Arabic dialects beyond the five covered to assess benchmark generalizability
2. Test whether performance differences persist across multiple question subsets and sampling strategies from MMLU-Redux
3. Conduct ablation studies varying question formats (e.g., open-ended responses) to validate that multiple-choice format captures the full scope of dialectal understanding