---
ver: rpa2
title: 'When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents'
arxiv_id: '2512.02445'
source_url: https://arxiv.org/abs/2512.02445
tags:
- padding
- refusal
- harmful
- benign
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how long context affects both capability and
  safety of LLM agents on multi-step tasks. The authors extend AgentHarm to test models
  with 1M-2M token context windows using various padding lengths (1K-200K tokens),
  types (random, coherent non-relevant/relevant, multi-task), and positions (before/after
  task).
---

# When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents

## Quick Facts
- arXiv ID: 2512.02445
- Source URL: https://arxiv.org/abs/2512.02445
- Reference count: 40
- This study shows that long context windows do not ensure robustness and introduce concrete safety and reliability risks for agentic systems, with capability degradation beginning at just 100K tokens despite advertised 1M-2M token context windows.

## Executive Summary
This study examines how long context affects both capability and safety of LLM agents on multi-step tasks. The authors extend AgentHarm to test models with 1M-2M token context windows using various padding lengths (1K-200K tokens), types (random, coherent non-relevant/relevant, multi-task), and positions (before/after task). They find severe performance degradation starting at just 100K tokens, with >50% drops for both benign and harmful tasks. Refusal rates shift unpredictably—some models increase refusals while others decrease at the same context lengths. Coherent text padding performs better than random, which outperforms multi-task. Context placed after the task description causes more degradation than before. The results reveal that long context windows do not ensure robustness and introduce concrete safety and reliability risks for agentic systems.

## Method Summary
The study uses the AgentHarm benchmark to evaluate LLM agents on multi-step tasks with controlled context padding. Models tested include GPT-4.1-nano (1M), GPT-5 (400K), DeepSeek-V3.1 (128K), and Grok 4 Fast (2M). The evaluation varies padding length (1K-200K tokens), type (random tokens, non-relevant fiction text, relevant Wikipedia articles, multi-task descriptions), and position (before or after task description). The simplest task subset (hint + detailed prompt) is used to isolate padding effects. A two-judge evaluator scores both semantic quality and refusal behavior across ≥3 random seeds.

## Key Results
- Capability degradation begins at ~100K tokens, with >50% performance drops for both benign and harmful tasks
- Refusal rates shift unpredictably: GPT-4.1-nano increases from ~5% to ~40% while Grok 4 Fast decreases from ~80% to ~10% at 200K tokens
- Padding position matters: "after" task placement causes more severe degradation than "before" placement
- Coherent padding performs better than random, which outperforms multi-task descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Placing padding after task descriptions causes more severe capability degradation than placing it before.
- Mechanism: With "after" padding, the model must attend across a large gap between instruction (task description) and execution (tool calls). Modern attention mechanisms like RoPE and sliding windows may handle distant attention patterns poorly because such patterns are rare in training distributions.
- Core assumption: Attention mechanisms trained primarily on shorter sequences treat long-range dependencies as out-of-distribution.
- Evidence anchors:
  - [Section 4]: "When padding is placed after, the model must attend across a large gap between instruction and execution, potentially explaining the more severe degradation."
  - [Figure 4]: Shows GPT-4.1-nano and Grok 4 Fast both degrade faster with "after" vs "before" padding positions.
  - [Corpus]: Scale-invariant Attention paper discusses generalization challenges from short-context training to long-context inference.
- Break condition: If models were fine-tuned specifically on instruction-execution pairs separated by long filler, this effect should diminish.

### Mechanism 2
- Claim: Long context causes unpredictable shifts in refusal rates that are not uniform across models.
- Mechanism: Safety training appears sensitive to context perturbations. The paper suggests refusals may be triggered by factors other than safety training itself—potentially attention saturation or distribution shift affecting the refusal classifier's activation patterns.
- Core assumption: Refusal behavior is mediated by learned representations that can be disrupted by distributional shift from long context.
- Evidence anchors:
  - [Abstract]: "Refusal rates shift unpredictably: GPT-4.1-nano increases from ~5% to ~40% while Grok 4 Fast decreases from ~80% to ~10% at 200K tokens."
  - [Section 5]: "These refusals could be triggered by something other than safety training. This questions the usefulness of refusal rate as a safety metric."
  - [Corpus]: EVOREFUSE and related work discuss over-refusal phenomena, but direct mechanistic evidence for long-context refusal shift is limited.
- Break condition: If refusal were purely a function of semantic content recognition, padding should not affect rates.

### Mechanism 3
- Claim: Padding semantic coherence affects degradation severity—coherent text > random tokens > multi-task descriptions.
- Mechanism: Coherent padding stays closer to training distribution, producing more predictable attention patterns. Multi-task padding introduces semantically confounding signals that compete for attention with the primary task. Random tokens are minimally confounding but out-of-distribution.
- Core assumption: Attention heads allocate capacity based on semantic salience, and competing task-relevant signals fragment this capacity.
- Evidence anchors:
  - [Figure 3]: Shows clear ordering: non-relevant > relevant > random > multi-task for GPT-4.1-nano harmful task performance.
  - [Section 3]: "Multi-task padding tests robustness to semantically confounding context."
  - [Corpus]: LongFuncEval examines function calling in long contexts but doesn't directly address padding type effects.
- Break condition: If models used task-delimiter tokens or explicit segment boundaries, multi-task interference should decrease.

## Foundational Learning

- Concept: **Context window capacity vs. capability distinction**
  - Why needed here: Models with 1M-2M token context windows show degradation at 100K tokens—capacity ≠ capability.
  - Quick check question: Can you explain why a model might successfully accept 2M tokens yet fail to act coherently at 100K?

- Concept: **Attention recency bias and "lost in the middle" phenomenon**
  - Why needed here: Padding position effects stem from how attention weights distribute across long sequences.
  - Quick check question: Where in a 200K-token sequence would you expect weakest recall, and how does padding position exploit this?

- Concept: **Agent evaluation vs. static prompt evaluation**
  - Why needed here: Agentic setups involve multi-turn tool use; prior long-context benchmarks (NIAH, LongBench) test single-turn recall.
  - Quick check question: Why might a model succeed at "needle in haystack" retrieval but fail at maintaining task focus across tool calls?

## Architecture Onboarding

- Component map:
  - Padding generator -> AgentHarm task suite -> Two-judge evaluator -> Context position injector

- Critical path:
  1. Select task subset (hint + detailed prompt = simplest, isolates padding effects)
  2. Generate padding at target length (1K–200K tokens)
  3. Inject at position (before/after)
  4. Execute agent multi-turn tool calls
  5. Score with both judges

- Design tradeoffs:
  - Using simplest task subset provides upper bound on capability; real deployments may degrade faster
  - Random padding is reproducible but not realistic; coherent text is realistic but introduces confounds
  - API-based models conflate provider filters with base model behavior (OpenAI vs. OpenRouter)

- Failure signatures:
  - Gradual capability decay starting ~50K tokens (GPT-4.1-nano pattern)
  - Sudden collapse between 50K–100K tokens (Grok 4 Fast pattern)
  - Refusal rate inversion: models that refuse aggressively may refuse less; low-refusal models may refuse more
  - Delayed refusals: model executes partial tool calls before refusing (see Figure 11)

- First 3 experiments:
  1. Replicate random padding degradation curve on your target model at 0K, 10K, 50K, 100K, 200K tokens with "after" position.
  2. Test padding position hypothesis: compare "before" vs "after" at 100K tokens to quantify attention gap effect.
  3. Measure refusal rate shift for your model's specific harm categories to identify which degrade first (category heterogeneity is significant per Section C).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training procedures or architectural components (e.g., attention mechanisms, position embeddings) are responsible for the observed performance collapse at context lengths significantly lower than the advertised maximum window size?
- Basis in paper: [explicit] Section I (Limitations) states that understanding the reasons for degradation requires open-weight models to isolate the effects of different parameters, which was impossible with the API-based models used in this study.
- Why unresolved: The authors relied on proprietary API models, preventing internal mechanistic analysis of why models with 1M-2M token windows fail at 100K tokens.
- What evidence would resolve it: Ablation studies on open-weight models with varying attention patterns and position embedding strategies to identify the structural cause of the degradation.

### Open Question 2
- Question: How can evaluation metrics be redesigned to differentiate between immediate safety refusals and "delayed" refusals where models execute partial harmful steps before stopping?
- Basis in paper: [explicit] Section 5 (Discussion) notes that "Refusal rate alone doesn't distinguish two failure modes" and concludes that "a more holistic approach is required for measuring agent safety."
- Why unresolved: Current binary refusal metrics conflate models that refuse upfront with those that execute harmful tool calls before refusing, the latter of which poses a distinct safety risk.
- What evidence would resolve it: A new benchmarking rubric that tracks and penalizes partial tool execution prior to refusal, validated against human safety judgments.

### Open Question 3
- Question: Does the performance degradation trend hold when context padding consists of structured technical data, such as code or tool output logs, rather than natural language text?
- Basis in paper: [explicit] Section I (Limitations) notes the study focused on "simplest context padding (random tokens and coherent text)," which differs from "real-world cases where context may consist of code or tool output logs."
- Why unresolved: The experiments used random tokens or literature articles for padding; it is unknown if structured data (common in agentic workflows) exacerbates or mitigates the "lost in the middle" effect.
- What evidence would resolve it: Re-running the evaluation protocol using software repositories or system logs as padding material to compare degradation curves.

### Open Question 4
- Question: Do models exhibit faster capability degradation in long-context scenarios that require complex planning compared to the simple, hint-based tasks used in this study?
- Basis in paper: [explicit] Section I (Limitations) explains that using the easiest subset of AgentHarm (with hints and detailed prompts) represents an "upper bound on performance," and "scenarios requiring complex planning may degrade sooner."
- Why unresolved: The study controlled for planning difficulty by providing explicit instructions; thus, the interaction between reasoning complexity and context length remains unmeasured.
- What evidence would resolve it: Evaluating models on the "no hint" or "only task description" subsets of AgentHarm across various padding lengths to compare degradation velocity.

## Limitations

- The study cannot definitively separate model-specific safety filters from base-model behavior, as different API providers introduce varying refusal patterns
- Using hints + detailed prompt subset provides an upper bound on capability, meaning real-world performance may degrade more severely than reported
- While the paper identifies unpredictability in refusal rates, it lacks mechanistic explanation for why different models exhibit opposite refusal behaviors under identical context perturbations

## Confidence

- **High confidence**: The observation that capability degradation begins at ~100K tokens despite 1M-2M context windows is reproducible and directly measurable. The position effect (after > before degradation) is supported by consistent patterns across multiple models.
- **Medium confidence**: The ranking of padding types (coherent > random > multi-task) is supported by empirical data but may be influenced by specific text choices in padding generation. The unpredictability of refusal rate shifts is well-documented but the underlying mechanism remains speculative.
- **Low confidence**: The claim that refusal shifts are "triggered by something other than safety training" is asserted but not mechanistically validated. The paper demonstrates correlation between context length and refusal changes but cannot definitively prove causation separate from safety filter interference.

## Next Checks

1. Test the position hypothesis with controlled experiments varying only padding position (before vs after) at multiple fixed context lengths to quantify the attention gap effect on both capability and refusal rates.
2. Conduct ablation studies isolating model-level behavior from provider-level safety filters by testing identical models across multiple API endpoints and measuring benign task refusal rates to establish baseline interference.
3. Investigate the multi-task padding interference mechanism by testing models with explicit segment boundaries or task delimiters to determine if semantic competition drives the observed degradation pattern.