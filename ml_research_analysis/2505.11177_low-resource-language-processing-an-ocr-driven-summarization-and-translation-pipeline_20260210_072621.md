---
ver: rpa2
title: 'Low-Resource Language Processing: An OCR-Driven Summarization and Translation
  Pipeline'
arxiv_id: '2505.11177'
source_url: https://arxiv.org/abs/2505.11177
tags:
- language
- summarization
- text
- translation
- processing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an end-to-end pipeline for multilingual information
  extraction from image-based documents, addressing the challenge of processing low-resource
  Indian languages. The system integrates Optical Character Recognition (Tesseract)
  for text extraction, Natural Language Processing (NLP) modules for text classification,
  sentiment analysis, and topic classification, and machine translation APIs for cross-lingual
  translation.
---

# Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline

## Quick Facts
- arXiv ID: 2505.11177
- Source URL: https://arxiv.org/abs/2505.11177
- Reference count: 36
- Primary result: End-to-end multilingual document processing pipeline achieving 88% classification accuracy, ROUGE-1 0.41-0.56, and BLEU 18.7-32.1 across Indian languages

## Executive Summary
This paper presents a comprehensive pipeline for processing image-based documents in low-resource Indian languages, integrating OCR, text classification, sentiment analysis, summarization, and translation. The system addresses the critical challenge of multilingual information extraction in settings like India where documents span multiple scripts and languages. Using Tesseract for OCR, classical machine learning for classification, and LLM APIs for summarization and translation, the pipeline demonstrates practical performance across six Indian languages. The evaluation shows strong classification accuracy (88%) but reveals significant challenges with complex scripts where OCR errors propagate to degrade downstream tasks.

## Method Summary
The pipeline processes images through sequential stages: Tesseract OCR extracts text in English, Hindi, Tamil, Urdu, Bengali, and Telugu; text preprocessing applies normalization and cleaning; classification uses either logistic regression on TF-IDF features or zero-shot BART-MNLI; abstractive summarization employs Gemini or Cohere APIs; translation uses Google Translate or Gemini APIs; finally, a Gradio interface presents results. The system was evaluated using standard metrics including accuracy for classification, ROUGE scores for summarization, and BLEU scores for translation, with performance varying significantly across languages and document qualities.

## Key Results
- Logistic Regression classification achieved 88% accuracy on BBC Sport and World News datasets
- Summarization component reached ROUGE-1 scores of 0.41-0.56 and ROUGE-L averaging 0.39
- Translation stage achieved BLEU scores ranging from 18.7 to 32.1 across different language pairs
- OCR Word Error Rate averaged 18.4%, with Santali reaching 26.5% and Hindi at 11.2%
- Post-OCR correction techniques improved average WER by approximately 3.4 percentage points

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** OCR quality creates a performance ceiling for all downstream tasks.
- **Mechanism:** Tesseract extracts machine-readable text from images; this text becomes the sole input for classification, summarization, and translation. Higher OCR accuracy yields lower Word Error Rate (WER), which reduces noise propagation through the pipeline.
- **Core assumption:** The relationship between OCR WER and downstream metric degradation is approximately linear within the observed error range (11-27% WER).
- **Evidence anchors:**
  - [abstract] "The system uses Optical Character Recognition (Tesseract) to extract text in languages such as English, Hindi, and Tamil"
  - [section: OCR Performance Impact] "Word Error Rate (WER) averaged 18.4%, with performance varying by language (e.g., 11.2% for Hindi... 26.5% for Santali)"; "Implementing post-OCR correction techniques improved the average WER by approximately 3.4 percentage points"
  - [corpus] Neighbor paper "Seeing Justice Clearly" confirms OCR-then-MT cascade sensitivity for handwritten Marathi legal documents
- **Break condition:** If OCR WER exceeds ~30%, downstream BLEU/ROUGE scores likely become unusable without aggressive correction or human-in-the-loop validation.

### Mechanism 2
- **Claim:** Logistic Regression was selected because it balances accuracy with computational efficiency for text classification in resource-constrained deployment.
- **Mechanism:** TF-IDF vectorization transforms text into sparse features; Logistic Regression learns linear decision boundaries. The model achieved 88% accuracy, matching SVM but with lower inference cost.
- **Core assumption:** The classification task (document categorization into domains like Medical, Politics, Sports, Technology) is approximately linearly separable in TF-IDF space.
- **Evidence anchors:**
  - [abstract] "The Logistic Regression model achieved 88% accuracy for text classification"
  - [section: Results] Table 1 shows Logistic Regression at 88% with limitation "Assumes linear relationships"; Figure 3a shows t-SNE visualization of TF-IDF features with "potential sentiment separation"
  - [corpus] Weak direct corpus evidence for this specific classifier choice; neighbor papers focus on LLM-based approaches rather than classical ML comparisons
- **Break condition:** If document categories require fine-grained semantic distinctions (e.g., "Technology" vs. "AI Policy"), linear models may fail; transformer-based zero-shot (BART-large-MNLI, also mentioned) would be needed.

### Mechanism 3
- **Claim:** Abstractive summarization quality is constrained by lexical overlap with references more than semantic fidelity.
- **Mechanism:** LLM APIs (Gemini, Cohere) generate abstractive summaries; evaluation via ROUGE-1 (0.41-0.56) and ROUGE-L (avg 0.39) measures n-gram overlap and longest common subsequence. Human evaluation (3.7/5) captures semantic adequacy not reflected in ROUGE.
- **Core assumption:** ROUGE scores correlate moderately with human judgments of summary quality for Indic languages, though this correlation is less validated than for English.
- **Evidence anchors:**
  - [abstract] "summarization component achieved ROUGE-1 scores ranging from 0.41 to 0.56 and ROUGE-L scores averaging 0.39"
  - [section: Summarization Evaluation] "Human evaluation ratings averaged 3.7 out of 5 for summary quality and informativeness"
  - [corpus] "Large Language Models for the Summarization of Czech Documents" notes ROUGE limitations for morphologically rich languages; relevant but not directly cited
- **Break condition:** If reference summaries are unavailable or poorly aligned with user needs, ROUGE becomes a misleading optimization target; human-in-the-loop or task-specific evaluation required.

## Foundational Learning

- **Concept: Word Error Rate (WER) vs. Character Error Rate (CER)**
  - **Why needed here:** OCR performance is reported in both metrics; understanding their tradeoffs is critical for diagnosing pipeline failures (e.g., script complexity affects CER more, word segmentation affects WER).
  - **Quick check question:** If CER is low but WER is high for a language, what is the likely cause? (Answer: Poor word boundary detection or agglutinative morphology.)

- **Concept: ROUGE vs. BLEU evaluation paradigms**
  - **Why needed here:** Summarization uses ROUGE (recall-oriented, compares to reference); translation uses BLEU (precision-oriented, compares to reference). Mixing these up leads to incorrect system tuning.
  - **Quick check question:** Why might BLEU scores be lower for Santali→English than Hindi→English? (Answer: Lower resource availability means less training data for the translation model, plus higher OCR errors compounding.)

- **Concept: Zero-shot classification with NLI models**
  - **Why needed here:** The pipeline uses `facebook/bart-large-mnli` for zero-shot topic classification without task-specific training, enabling adaptation to new domains.
  - **Quick check question:** What is the primary tradeoff of zero-shot vs. fine-tuned classification? (Answer: Zero-shot generalizes immediately but typically underperforms fine-tuned models on in-domain data.)

## Architecture Onboarding

- **Component map:**
[Image Input] → [OCR (Tesseract/pytesseract)] → [Text Preprocessing] → [Classification (Logistic Regression or BART-MNLI zero-shot)] → [Summarization (Gemini/Cohere API)] → [Translation (Google/Gemini API)] → [Gradio Interface Output]

- **Critical path:** OCR accuracy → preprocessing quality → classification/summarization input cleanliness → translation fluency. The OCR stage is the highest-leverage failure point.

- **Design tradeoffs:**
  - Logistic Regression vs. XGBoost/SVM: Selected LR for 88% accuracy with lower inference cost and simpler deployment, accepting limitation on non-linear pattern capture.
  - API-based summarization/translation (Gemini, Cohere, Google) vs. local models: APIs provide immediate multilingual capability but introduce latency, cost, and data privacy concerns; not addressed in paper.
  - Tesseract vs. transformer-based OCR (TrOCR, LayoutLM): Tesseract chosen for accessibility and Indic script support; transformer alternatives (cited in references) may improve accuracy but require more compute.

- **Failure signatures:**
  - Santali and other complex scripts show WER >25% → expect BLEU <20 and summary incoherence.
  - If classification accuracy drops suddenly, check for OCR output degradation (e.g., font changes, image quality).
  - If ROUGE scores are acceptable but human ratings are low → summary may be extractive rather than truly abstractive, or missing key domain-specific entities.

- **First 3 experiments:**
  1. **OCR ablation:** Measure WER/CER across all supported languages with controlled image quality; identify minimum DPI threshold for acceptable downstream performance.
  2. **Post-OCR correction impact:** Before/after comparison of BLEU/ROUGE with the 3.4pp WER improvement technique mentioned; quantify downstream gains.
  3. **Classifier swap test:** Replace Logistic Regression with BART-large-MNLI zero-shot on the same dataset; compare accuracy and inference time to validate the paper's design choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can abstractive summarization models maintain semantic fidelity when input text suffers from the high Word Error Rates (WER) observed in complex scripts?
- Basis in paper: [explicit] The paper reports an average WER of 18.4% (peaking at 26.5% for Santali) and explicitly notes that "OCR quality... significantly influences downstream tasks."
- Why unresolved: The results report ROUGE scores for summarization but do not isolate the degradation caused specifically by OCR noise versus model limitations.
- What evidence would resolve it: A comparative evaluation of summarization ROUGE scores using ground-truth text versus OCR-extracted text for the identified low-resource languages.

### Open Question 2
- Question: To what extent can transfer learning improve Machine Translation BLEU scores for the lowest-performing language pairs compared to the commercial APIs used?
- Basis in paper: [explicit] The Literature Survey states "Transfer learning holds out the potential to enhance MT performance," while Results show lower BLEU scores (18.7) for complex scripts using standard APIs.
- Why unresolved: The pipeline currently relies on general-purpose Google/Gemini APIs rather than the transfer learning techniques suggested in the literature review.
- What evidence would resolve it: Benchmarking fine-tuned transfer learning models against the current API baseline for the Santali-English language pair.

### Open Question 3
- Question: Does the chosen Logistic Regression classifier maintain 88% accuracy when processing features derived from noisy OCR text rather than clean datasets?
- Basis in paper: [inferred] The paper selects Logistic Regression for its balance of performance and speed, but Table 1 notes its limitation in handling "complex, non-linear patterns," which characterizes noisy OCR data.
- Why unresolved: The classification accuracy was verified on specific datasets, but the impact of the documented OCR character errors (CER 12.7%) on classification confidence was not quantified.
- What evidence would resolve it: Accuracy metrics for the Logistic Regression model run directly on the noisy output of the Tesseract module.

## Limitations

- OCR quality significantly impacts downstream performance, with complex scripts like Santali showing WER >25% leading to poor translation and summarization
- Custom TensorFlow sentiment analysis model lacks architectural transparency and performance metrics
- API-based summarization and translation introduce cost, latency, and privacy concerns not addressed in evaluation
- Limited human evaluation beyond single Likert-scale rating for summarization quality

## Confidence

- **High Confidence:** OCR-WER as primary performance bottleneck; logistic regression achieving 88% accuracy on BBC Sport/World News classification; BLEU/ROUGE metric selection appropriateness
- **Medium Confidence:** Human evaluation rating of 3.7/5 for summarization quality (single metric, small sample); post-OCR correction improving WER by 3.4pp (reported but methodology unspecified)
- **Low Confidence:** Sentiment analysis module performance (no metrics reported); long-term API availability and cost stability; generalizability to non-news domains

## Next Checks

1. **OCR Quality Threshold Validation:** Systematically measure WER/CER across controlled image quality variations (different DPI, noise levels, fonts) for all six target languages. Determine minimum quality thresholds where BLEU/ROUGE scores drop below usable levels (>25% WER for Santali, >20% for Tamil).

2. **Human Evaluation Expansion:** Conduct structured human evaluation of summarization quality across multiple dimensions (coherence, relevance, factual consistency) for at least 50 documents per language. Compare against ROUGE/BLEU scores to quantify metric-human alignment.

3. **Classifier Robustness Testing:** Replace the logistic regression classifier with the BART-large-MNLI zero-shot approach mentioned in the paper. Run identical classification tasks to validate the claimed 88% accuracy and compare computational overhead for resource-constrained deployment scenarios.