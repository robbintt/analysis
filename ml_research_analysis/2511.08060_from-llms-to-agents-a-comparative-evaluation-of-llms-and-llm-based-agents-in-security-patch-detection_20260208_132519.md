---
ver: rpa2
title: 'From LLMs to Agents: A Comparative Evaluation of LLMs and LLM-based Agents
  in Security Patch Detection'
arxiv_id: '2511.08060'
source_url: https://arxiv.org/abs/2511.08060
tags:
- security
- patch
- detection
- llms
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study conducts the first systematic evaluation of large language\
  \ models (LLMs) and LLM-based agents for security patch detection. We investigate\
  \ three methods\u2014Plain LLM, Data-Aug LLM, and ReAct Agent\u2014using both commercial\
  \ (e.g., GPT-4o, DeepSeek-R1) and open-source LLMs (e.g., Llama-3.1, Gemma-3) on\
  \ a comprehensive security patch dataset."
---

# From LLMs to Agents: A Comparative Evaluation of LLMs and LLM-based Agents in Security Patch Detection

## Quick Facts
- arXiv ID: 2511.08060
- Source URL: https://arxiv.org/abs/2511.08060
- Reference count: 40
- Primary result: Data-Aug LLM achieves best overall performance while ReAct Agent exhibits lowest false positive rate in security patch detection

## Executive Summary
This study systematically evaluates three approaches—Plain LLM, Data-Aug LLM, and ReAct Agent—for detecting security patches in code commits using both commercial (GPT-4o, DeepSeek-R1) and open-source LLMs (Llama-3.1, Gemma-3). The comprehensive evaluation on PatchDB dataset reveals that context augmentation through metadata (CVE/CWE identifiers, commit messages) provides the best overall detection performance, while the iterative reasoning mechanism of ReAct Agents minimizes false positives. Commercial models consistently outperform open-source counterparts across all methods. The research establishes foundational baselines for LLM-based security patch detection and identifies critical design tradeoffs between precision, false positive rates, and computational costs.

## Method Summary
The paper investigates three inference methods for binary classification of security patches: (1) Plain LLM using raw code diffs with system prompts, (2) Data-Aug LLM adding contextual metadata including commit messages and vulnerability identifiers, and (3) ReAct Agent employing iterative thought-action-observation loops with external tools for diff and message analysis. Models tested include GPT-4o, GPT-4o-mini, GPT-5, DeepSeek-R1 (commercial) and Llama-3.1-8B, Gemma-3-7B (open-source), using temperature=0.1, seed=42, and max_output=512 tokens. Four prompting strategies (Vanilla, Chain-of-Thought, Few-Shot, CoT+FS) are evaluated across all combinations. The PatchDB dataset provides 12,000 security patches and 23,000 non-security patches from 313 OSS repositories.

## Key Results
- Data-Aug LLM achieves best overall performance with highest precision and accuracy across model types
- ReAct Agent exhibits lowest false positive rate (14.14%) due to iterative reasoning mechanism
- Commercial LLMs (GPT-4o) consistently outperform open-source models in all metrics
- Chain-of-Thought prompting proves most effective for improving model performance
- CWE-20 (Input Validation) shows optimal precision while CWE-476 and CWE-125 show lowest false positive rates

## Why This Works (Mechanism)

### Mechanism 1: Semantic Enrichment via Context Augmentation
Enriching code diffs with metadata (commit messages, CVE/CWE identifiers) improves detection performance by providing semantic cues that help models distinguish security patches from functional updates. The Data-Aug LLM method gives models context often implicit in raw diffs. Evidence shows this approach achieves best overall performance. Break condition occurs when metadata is misleading or missing.

### Mechanism 2: Iterative Reasoning for False Positive Reduction
ReAct Agent's thought-action-observation loop reduces false positive rates by allowing intermediate reasoning steps before final classification. This multi-step process acts as self-correction, progressively filtering non-security patches. The approach achieves lowest FPR (14.14%). Break condition occurs when initial thoughts are flawed or tools retrieve noisy context.

### Mechanism 3: Reasoning Step-Decomposition (Chain-of-Thought)
CoT prompting improves precision by forcing explicit decomposition of security analysis rather than superficial pattern matching. This prevents impulsive errors by requiring identification of vulnerable code segments and verification of fixes. CoT proves most effective overall, though shows mixed results within ReAct Agent architecture.

## Foundational Learning

- **Concept: ReAct Agent Framework (Reason + Act)**
  - Why needed: Achieves lowest FPR through interleaving reasoning traces with external tool use
  - Quick check: Can you explain the difference between standard LLM prompt-response and Thought-Action-Observation loop?

- **Concept: Security Patch vs. Vulnerability Detection**
  - Why needed: Task involves identifying patches (code changes that fix bugs) not just finding bugs in static code
  - Quick check: How does input data structure for "patch detection" (unified diff) differ from "vulnerability detection" (full source file)?

- **Concept: False Positive Rate (FPR) vs. Precision**
  - Why needed: Paper highlights trade-off between Data-Aug's high precision and ReAct's low FPR
  - Quick check: Why might security team prefer model with slightly lower accuracy but significantly lower FPR?

## Architecture Onboarding

- **Component map:** Diff Code + Commit ID -> (Augmenter: adds Commit Message, Source, CVE/CWE) -> LLM Core -> (Agent Tools: get-diff-analysis, get-message-analysis) -> Binary Classification
- **Critical path:** 1. Load PatchDB record, 2. Context setup (fetch metadata or initialize tools), 3. Apply prompting strategy, 4. Inference (single pass or iterative loop), 5. Calculate Precision, Accuracy, F1, FPR
- **Design tradeoffs:** Commercial models cost more but outperform open-source; ReAct minimizes FPR but requires multiple steps; truncation kills performance with small context windows
- **Failure signatures:** High FPR suggests Plain LLM without reasoning; hallucination with Few-Shot prompting; truncation errors with small context windows
- **First 3 experiments:** 1. Implement Plain LLM baseline with provided prompt to establish baseline, 2. Add Data-Aug LLM by including commit messages, verify precision increase, 3. Implement ReAct Agent with tools, verify FPR reduction

## Open Questions the Paper Calls Out

- **Question 1:** Can hybrid architecture combining Data-Aug LLM's contextual augmentation with ReAct Agent's iterative reasoning achieve both high precision and low false positive rates?
  - Basis: Discussion section suggests exploring hybrid models balancing contextual information and iterative reasoning
  - Why unresolved: Data-Aug has best performance but higher FPR (23.96%), ReAct has lowest FPR (14.14%) but lower F1 score

- **Question 2:** To what extent do LLM-based security patch detection methods generalize to programming languages beyond C/C++?
  - Basis: Section VII acknowledges PatchDB focuses exclusively on C/C++ and conclusions may not generalize
  - Why unresolved: All experiments conducted on C/C++ patches; language-specific syntax and vulnerability patterns may affect detection differently

- **Question 3:** Can integrating LLMs with structured code representations (ASTs, control flow graphs) improve detection over textual-only approaches?
  - Basis: Discussion suggests exploring deep integration of LLMs with structured code representations
  - Why unresolved: RepoSPD baseline showed competitive accuracy but higher FPR; whether LLM-graph synergy could retain structural understanding while lowering FPR remains untested

- **Question 4:** How can Few-Shot example selection be optimized to reduce noise and improve security patch detection performance?
  - Basis: Finding 5 reports FS strategy performs worst due to noise introduction
  - Why unresolved: Random example selection may include task-irrelevant patches; systematic selection criteria for representative security patch examples not established

## Limitations

- Performance gap between commercial and open-source models raises questions about whether differences stem from model quality or implementation details
- Lack of transparency regarding dataset splits and example selection for Few-Shot prompting impacts reproducibility
- Paper does not address potential security risks from exposing raw code diffs to external APIs during evaluation

## Confidence

- **High Confidence:** Comparative performance ranking (Data-Aug LLM > Plain LLM > ReAct Agent in overall metrics; ReAct Agent lowest FPR) is well-supported by experimental results
- **Medium Confidence:** Mechanism explanations for Data-Aug precision improvement and ReAct FPR reduction are plausible but rely on assumptions about metadata quality
- **Low Confidence:** Claim that Chain-of-Thought is "most effective" is contradicted within paper where CoT showed lower precision than Vanilla for ReAct Agent

## Next Checks

1. **Dataset Split Validation:** Reproduce Plain LLM baseline with multiple random seed splits to determine if performance variance exceeds claimed precision differences between methods

2. **Metadata Quality Assessment:** Implement ablation study removing CVE/CWE identifiers from Data-Aug prompts while keeping commit messages to isolate whether metadata enrichment genuinely improves security reasoning

3. **Open-Source Agent Fidelity:** Implement ReAct Agent with open-source models using exact tool specifications and compare FPR reduction magnitude against commercial models, controlling for context window size and tool response quality