---
ver: rpa2
title: Data Readiness for Scientific AI at Scale
arxiv_id: '2507.23018'
source_url: https://arxiv.org/abs/2507.23018
tags:
- data
- scientific
- readiness
- datasets
- preprocessing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a two-dimensional data readiness framework
  to address challenges in preparing scientific datasets for AI at leadership computing
  scales. The framework combines Data Readiness Levels (raw to AI-ready) with Data
  Processing Stages (ingest to shard), tailored for HPC environments.
---

# Data Readiness for Scientific AI at Scale

## Quick Facts
- **arXiv ID**: 2507.23018
- **Source URL**: https://arxiv.org/abs/2507.23018
- **Reference count**: 40
- **Primary result**: Introduces a 2D data readiness framework (Levels 1-5 × Ingest→Shard) for AI at leadership computing scales, based on cross-domain analysis of climate, fusion, bio/health, and materials workflows.

## Executive Summary
This paper addresses the challenge of preparing scientific datasets for AI training at exascale computing facilities by proposing a two-dimensional data readiness framework. The framework combines Data Readiness Levels (from raw to AI-ready) with Data Processing Stages (ingest to shard) to characterize dataset maturity for AI workloads. Through expert interviews and workflow analysis across four scientific domains, the authors identify common preprocessing bottlenecks and propose standardized pipelines to enable efficient training of foundation models. The work emphasizes the need for scalable, reproducible data infrastructure that bridges domain-specific requirements with HPC performance constraints.

## Method Summary
The study employs a cross-domain analysis approach, examining representative datasets from climate (CMIP6/ERA5), fusion (DIII-D/IMAS), bio/health (clinical/genomic), and materials (OMat24/AFLOW) domains. Researchers conducted expert interviews and workflow analysis to identify preprocessing patterns, then mapped these onto a conceptual 2D matrix combining Data Readiness Levels with Data Processing Stages. The method focuses on characterizing current practices rather than developing new algorithms, aiming to create a taxonomy that can guide infrastructure development and standardization efforts at leadership computing facilities.

## Key Results
- The framework successfully identifies common preprocessing bottlenecks across scientific domains, particularly around data curation, normalization, and sharding for HPC I/O efficiency.
- Domain-specific constraints (PHI compliance for bio/health, physics preservation for fusion) require careful handling during the Transform stage to maintain data utility while meeting regulatory or scientific requirements.
- Sharding to high-performance binary formats (HDF5, TFRecords, ADIOS) consistently emerges as critical for achieving training efficiency at leadership computing scales.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The framework facilitates infrastructure planning by decoupling data maturity (Levels) from operational workflow steps (Stages).
- **Mechanism:** By mapping datasets onto a 2D matrix (Readiness Level vs. Processing Stage), the framework isolates specific bottlenecks—such as a lack of labels or inefficient I/O formatting—allowing facility operators to target investments (e.g., automated labeling pipelines vs. high-throughput storage) rather than treating "AI-readiness" as a monolithic problem.
- **Core assumption:** Assumes that distinct scientific domains share enough structural similarities in their pipelines that a generalized matrix can meaningfully categorize them.
- **Evidence anchors:**
  - [abstract] "...two-dimensional readiness framework composed of Data Readiness Levels (raw to AI-ready) and Data Processing Stages (ingest to shard)..."
  - [Page 6, Section 3.5] "...readiness is not a binary state, but a spectrum shaped by domain constraints..."
  - [corpus] Weak direct validation of this specific 2D mechanism; however, related work like *SciHorizon* confirms the broader need for benchmarking AI-for-Science readiness.
- **Break condition:** If a specific domain requires a processing stage that does not fit into {Ingest, Preprocess, Transform, Structure, Shard}, the matrix misclassifies the data maturity.

### Mechanism 2
- **Claim:** Standardizing the final "Shard" stage drives training efficiency at leadership scales.
- **Mechanism:** The framework enforces a transition from domain-specific standard formats (NetCDF, MDSplus) to high-performance binary formats (HDF5, TFRecords, ADIOS). This aligns data physics with HPC architecture (parallel file systems, GPU memory), reducing I/O latency which is often the primary bottleneck for large Transformer models.
- **Core assumption:** Assumes that the computational cost of converting data to sharded formats is amortized over repeated model training runs.
- **Evidence anchors:**
  - [Page 2, Section 1] "Efficient training at this scale requires high-throughput, parallel file I/O."
  - [Page 4, Table 1] Consistently lists "Shard to binary formats" or "TFRecord/HDF5" as the final step across Climate, Fusion, and Materials.
  - [corpus] *Foundation Models for Zero-Shot Segmentation...* supports the premise that data readiness bottlenecks exist but proposes zero-shot learning as an alternative to heavy preprocessing.
- **Break condition:** If the model training is strictly data-loaded on-the-fly (streaming) without needing to revisit samples, the overhead of sharding may introduce unnecessary storage complexity.

### Mechanism 3
- **Claim:** Separating "Transform" from "Structure" enables cross-domain reusability of pipeline components.
- **Mechanism:** By distinguishing *Transform* (domain-specific logic, e.g., anonymization, regridding) from *Structure* (generic tensor layouts, graph building), the framework allows HPC facilities to build standard tools for the "Structure" and "Shard" stages while encapsulating the complexity of the "Transform" stage for domain experts.
- **Core assumption:** Assumes that "Structure" logic (e.g., creating a graph from atoms or a tensor from a grid) is sufficiently generic to be standardized.
- **Evidence anchors:**
  - [Page 6, Section 3.5] "The transform stage captures domain-specific conversions... while structure refers to organizing the data into standardized formats."
  - [Page 8, Section 5] "Fragmentation Across Domains... hinders transferability... Leadership facilities are well-positioned to define common readiness templates."
  - [corpus] *Bangladesh AI Readiness* highlights the need for ecosystem coordination, supporting the paper's goal of unifying isolated pipelines.
- **Break condition:** If the "Structure" of the data is inextricably linked to the physics of the "Transform" (e.g., adaptive mesh refinement where structure changes based on physics), modular pipeline tools will fail.

## Foundational Learning

- **Concept: HPC I/O Hierarchy (Parallel File Systems vs. Local Burst Buffers)**
  - **Why needed here:** The paper emphasizes "sharding" and "high-throughput I/O" because standard file formats (like many small NetCDF files) cause metadata storms on parallel file systems like Lustre or GPFS.
  - **Quick check question:** Why is loading 100,000 small files slower than loading one large 100GB file on a supercomputer? (Answer: Metadata overhead/seek time vs. sequential bandwidth).

- **Concept: Foundation Models vs. Domain-Specific Models**
  - **Why needed here:** The paper targets "Foundation Models" (e.g., ClimaX, AlphaFold). These models require massive, uniform datasets to learn general representations, which is why the "Data Readiness Level" must be high (normalized, labeled) compared to traditional models which might handle rawer data.
  - **Quick check question:** Why does a Foundation Model require stricter data normalization than a specialized regression model? (Answer: Cross-domain transfer learning requires consistent statistical distributions).

- **Concept: Scientific Data Modalities (Gridded vs. Graph vs. Sequence)**
  - **Why needed here:** The framework abstracts across Climate (Gridded/Spatial), Materials (Graph), and Bio (Sequence). Understanding these base structures is necessary to understand the "Structure" stage of the pipeline.
  - **Quick check question:** How does converting a molecule to a graph representation change the data loading requirements compared to a 2D satellite image?

## Architecture Onboarding

- **Component map:**
  1.  **Ingest:** Raw data adapters (MDSplus, GRIB, DFT outputs).
  2.  **Preprocess:** Cleaning/Alignment (Regridding, Noise reduction).
  3.  **Transform:** Domain logic (Anonymization, Physics-based feature extraction).
  4.  **Structure:** ML Formalization (Tensorization, Graph construction, One-hot encoding).
  5.  **Shard:** I/O Optimization (Writing to HDF5/ADIOS/TFRecords with chunking).

- **Critical path:** The path from **Transform** to **Structure**. The paper notes that "Transform" is where domain-specific complexity lives (e.g., physics constraints, PHI compliance), while "Structure" determines if the data can actually be fed to a GPU efficiently. Failure to standardize the "Structure" results in training-time crashes.

- **Design tradeoffs:**
  - **Precision vs. Throughput:** Keeping data in 64-bit float (scientific standard) halves GPU memory bandwidth compared to 32-bit (AI standard). The framework suggests normalizing to AI-ready formats, implying a potential precision tradeoff.
  - **Compliance vs. Accessibility:** Bio/Health data requires "Secure enclaves" (Section 5), which complicates the "Ingest" and "Shard" stages by preventing data movement to standard high-performance scratch spaces.

- **Failure signatures:**
  - **The "70% Curation" Trap:** Spending the majority of project time stuck between "Raw" and "Cleaned" levels due to inconsistent formats (Section 3.2).
  - **I/O Stall:** GPU utilization drops to 0% while waiting for data; indicates failure at the "Shard" stage (data is likely in too many small files or non-chunked formats).
  - **Physics Violation:** Model outputs break conservation laws; indicates failure at "Transform" stage (normalization destroyed physical relationships).

- **First 3 experiments:**
  1.  **Matrix Audit:** Select 3 existing datasets (one Climate, one Fusion, one Bio). Map them to the (Level, Stage) matrix to identify the lowest common readiness level.
  2.  **Sharding Benchmark:** Convert a representative 100GB dataset from its raw format (e.g., NetCDF) to a sharded format (e.g., HDF5/TFRecord) and measure the throughput difference during a dummy training loop.
  3.  **Provenance Trace:** Implement a basic metadata tag pass-through from "Ingest" to "Shard" for a single fusion shot file to verify that data lineage is preserved after transformation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the proposed two-dimensional readiness framework be empirically validated and iteratively refined to operationalize it across different leadership computing facilities?
- **Basis in paper:** [explicit] The authors state in the Introduction and Conclusion that this is an "initial step" and "further iteration and validation are needed to refine and operationalize these insights across domains."
- **Why unresolved:** The current work is a conceptual survey and taxonomy based on interviews and literature; it has not yet been tested as an operational standard in a live environment.
- **What evidence would resolve it:** A longitudinal study applying the framework to new datasets at scale, resulting in quantitative improvements in data preparation time or model performance.

### Open Question 2
- **Question:** What specific domain-agnostic API standards or "readiness templates" are required to reduce fragmentation and enable cross-domain transferability of AI-ready data pipelines?
- **Basis in paper:** [explicit] The Conclusion explicitly calls for future work to "broaden to more fields and tools, developing standardized domain-specific preprocessing templates for wider adoption."
- **Why unresolved:** Current pipelines are bespoke and isolated by discipline, leading to the fragmentation identified in the cross-cutting challenges.
- **What evidence would resolve it:** The development and adoption of common templates that successfully automate the "ingest to shard" pipeline for climate, fusion, and materials data simultaneously.

### Open Question 3
- **Question:** Does the current readiness framework generalize effectively to scientific disciplines or simulation codes with data modalities fundamentally different from the four domains analyzed?
- **Basis in paper:** [explicit] The Conclusion notes the survey does not capture the "full breadth of scientific disciplines or the diversity of simulation codes" and suggests future work should "broaden to more fields."
- **Why unresolved:** The study was restricted to Climate, Fusion, Bio/Health, and Materials, potentially missing unique constraints found in other fields like high-energy physics or astronomy.
- **What evidence would resolve it:** Successful mapping of the readiness levels to distinct scientific fields without requiring structural changes to the core framework.

## Limitations

- The framework lacks quantitative validation and defined scoring criteria for classifying datasets into Data Readiness Levels.
- Implementation details for preprocessing pipelines are absent, making systematic application difficult.
- The assumption that sharding universally improves training efficiency may not hold for streaming workloads or specialized architectures.

## Confidence

- **High confidence**: The identification of common preprocessing patterns across domains (ingest→preprocess→transform→structure→shard) and the general need for scalable data pipelines at leadership computing scales.
- **Medium confidence**: The proposed 2D framework's ability to guide infrastructure investment decisions, though evidence is limited.
- **Low confidence**: The precise boundaries between Data Readiness Levels (1-5) and how to systematically classify datasets into these levels due to lack of quantitative thresholds.

## Next Checks

1. **Reproducibility Test**: Apply the framework to three datasets from different domains, explicitly documenting the classification process and scoring criteria used at each (Level, Stage) intersection. Compare results across evaluators to assess inter-rater reliability.
2. **Performance Benchmark**: Measure training throughput and GPU utilization differences between datasets processed through the framework's recommended pipeline versus their native formats across multiple HPC architectures.
3. **Generalizability Audit**: Identify a scientific domain not covered in the analysis (e.g., computational fluid dynamics or astrophysics) and assess whether its preprocessing requirements fit cleanly into the five-stage pipeline or require extensions to the framework.