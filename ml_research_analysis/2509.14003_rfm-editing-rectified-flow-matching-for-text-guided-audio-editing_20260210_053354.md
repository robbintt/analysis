---
ver: rpa2
title: 'RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing'
arxiv_id: '2509.14003'
source_url: https://arxiv.org/abs/2509.14003
tags:
- audio
- editing
- diffusion
- rfm-editing
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RFM-Editing, a novel rectified flow matching-based
  diffusion framework for text-guided audio editing. The method addresses the challenge
  of modifying specific audio content while preserving the rest, without requiring
  auxiliary captions or masks.
---

# RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing

## Quick Facts
- **arXiv ID:** 2509.14003
- **Source URL:** https://arxiv.org/abs/2509.14003
- **Reference count:** 0
- **Primary result:** Novel RFM framework achieves CLAP score of 0.4250 with competitive audio quality, outperforming existing approaches in distributional consistency without auxiliary captions or masks

## Executive Summary
This paper proposes RFM-Editing, a rectified flow matching-based diffusion framework for text-guided audio editing. The method enables precise modification of specific audio content while preserving the rest, without requiring auxiliary captions or masks. The model uses a LoRA-tuned text encoder to understand editing instructions and a UNet to predict velocity fields for audio transformation. Experiments on a constructed dataset of overlapping multi-event audio demonstrate that RFM-Editing achieves faithful semantic alignment with target captions while maintaining competitive audio quality across multiple metrics.

## Method Summary
RFM-Editing is an end-to-end rectified flow matching framework that enables text-guided audio editing by learning a deterministic ODE process between noise and target audio latents. The model takes an original audio spectrogram and editing instruction as input, using a LoRA-tuned Flan-T5 encoder to generate instruction embeddings. These embeddings are used in cross-attention with audio features within a UNet that predicts velocity fields for the RFM process. The model operates in latent space using a frozen VAE encoder, with original audio latents concatenated channel-wise with current noisy latents to explicitly preserve unedited regions. Training uses MSE loss between predicted and target velocity fields, while inference employs an Euler solver with partial diffusion initialization at t_start=0.01.

## Key Results
- Achieves CLAP score of 0.4250, demonstrating strong semantic alignment with target captions
- Maintains competitive audio quality with FD=0.5736 and FAD=0.3261
- Shows significant improvement in distributional consistency with lower KL divergence (0.1021) compared to baselines
- Outperforms existing approaches without requiring auxiliary captions or masks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rectified flow matching enables efficient audio editing by learning straight-line ODE trajectories from noise to target edited audio.
- **Mechanism:** RFM formulates a deterministic ODE process that directly interpolates between noise ε and target latent x₀ along a straight path (Eq. 1: x_t = (1-(1-σ_min)·t)·ε + t·x₀). This eliminates the need for fine-grained time discretization required by SDE-based diffusion, yielding stable training and fewer sampling steps at inference.
- **Core assumption:** The velocity field v_target = x₀ - (1-σ_min)·ε can be accurately predicted by a UNet conditioned on instruction embeddings.
- **Evidence anchors:**
  - [abstract] "end-to-end efficient rectified flow matching-based diffusion framework"
  - [Section 2.1] "RFM formulates a deterministic ODE process that models a straight-line trajectory from noise ε to target x₀, eliminating the need for fine-grained time discretization"
  - [corpus] AudioTurbo paper confirms rectified diffusion "enhances inference speed by learning straight-line ODE paths"
- **Break condition:** If the interpolation path deviates significantly from straightness (curved trajectories), the velocity prediction becomes unstable and sample quality degrades.

### Mechanism 2
- **Claim:** Concatenating original audio latent with noisy latent provides explicit conditioning that preserves unedited regions.
- **Mechanism:** The model receives x_t ⊕ x_T (channel-wise concatenation), allowing the UNet direct access to the unedited input during denoising. This conditions the velocity field prediction on both current noisy state and original content, enabling the model to "copy through" unchanged regions while modifying instruction-relevant segments.
- **Core assumption:** The model can learn to selectively attend to x_T for preservation and ignore it for edit regions based on instruction semantics.
- **Evidence anchors:**
  - [abstract] "leveraging a LoRA-tuned text encoder and original audio features to preserve unedited content while applying targeted edits"
  - [Section 2.1] "This enables the model to directly access the unedited input during both training