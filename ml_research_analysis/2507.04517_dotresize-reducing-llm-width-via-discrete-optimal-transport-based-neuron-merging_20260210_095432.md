---
ver: rpa2
title: 'DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron
  Merging'
arxiv_id: '2507.04517'
source_url: https://arxiv.org/abs/2507.04517
tags:
- neurons
- dotresize
- width
- pruning
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DOTResize, a novel method for compressing large
  language models (LLMs) by reducing the width of model layers through neuron merging
  rather than pruning. The method frames width reduction as a discrete optimal transport
  problem, using activation similarities to redistribute signal across fewer neurons
  while preserving model function.
---

# DOTResize: Reducing LLM Width via Discrete Optimal Transport-based Neuron Merging

## Quick Facts
- arXiv ID: 2507.04517
- Source URL: https://arxiv.org/abs/2507.04517
- Reference count: 36
- Primary result: DOTResize achieves up to 3-point perplexity reduction and 10% zero-shot accuracy improvement over magnitude pruning by using optimal transport-based neuron merging instead of pruning

## Executive Summary
DOTResize introduces a novel method for compressing large language models by reducing layer width through neuron merging rather than pruning. The approach formulates width reduction as a discrete optimal transport problem, using activation similarities to redistribute signal across fewer neurons while preserving model function. Unlike pruning approaches that discard neurons, DOTResize employs Sinkhorn entropy regularization to create "soft" alignments allowing each neuron to correspond to linear combinations of others. Empirical results show DOTResize outperforms magnitude pruning and SliceGPT across multiple LLM families, achieving lower perplexity in language modeling and higher accuracy in zero-shot tasks.

## Method Summary
DOTResize compresses LLMs by reducing layer width through optimal transport-based neuron merging. The method computes activations across calibration tokens, builds pairwise activation distance cost matrices, and solves Sinkhorn-regularized optimal transport to find minimum-cost redistribution of full neuron width onto a target subset. QR decomposition transforms general invertible transport maps into orthogonal forms compatible with RMSNorm layer normalization. The resulting transformations are absorbed into adjacent weight matrices, allowing the model to function identically while using fewer neurons per layer. This approach preserves more functional signal than hard pruning by allowing soft, distributed merging of neurons.

## Key Results
- On Wikitext-2, DOTResize reduces perplexity by up to 3 points compared to magnitude pruning at 30% width reduction
- Achieves over 10% average accuracy improvement on zero-shot tasks compared to magnitude pruning for certain models
- Outperforms magnitude pruning and SliceGPT across multiple LLM families including Llama-3.1, Mistral-7B, and Qwen-7B
- Shows 20% sparsity provides compute gains while 30%+ sparsity shows larger perplexity degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft alignment via entropic regularization preserves more functional signal than hard pruning during width reduction.
- Mechanism: Sinkhorn entropy regularization (λ parameter) encourages the optimal transport map T to distribute mass across multiple target neurons rather than creating sparse one-to-one mappings. Each original neuron contributes to linear combinations of retained neurons instead of being discarded or mapped to a single destination.
- Core assumption: Redundant neurons encode distributable signal rather than pure noise; activation similarity correlates with functional interchangeability.
- Evidence anchors:
  - [abstract] "DOTResize employs Sinkhorn entropy regularization to create 'soft' alignments that allow each neuron to correspond to linear combinations of others"
  - [Page 4] "Soft' alignments allow for each neuron to correspond to linear combinations of other neurons, whereas non-regularized alignments are much sparser"
  - [corpus] Weak direct corpus evidence; related work on neuronal attribution (Neurons Speak in Ranges) suggests polysemanticity complicates discrete neuron-to-concept mapping, indirectly supporting distributed merging
- Break condition: If neurons encode fundamentally different functions despite activation similarity, soft merging will introduce interference; if λ is too high, transport maps become overly diffuse and lose discriminative structure.

### Mechanism 2
- Claim: QR decomposition extends permissible transformations from orthogonal-only to any invertible matrix while preserving RMSNorm behavior.
- Mechanism: For transport map T, decompose T = QR where Q is orthogonal and R is upper-triangular. Apply Q before RMSNorm (commutes due to norm preservation) and absorb R into the inverse transformation applied after RMSNorm. This exploits RMSNorm's invariance to orthogonal transformations while allowing non-orthogonal compression maps.
- Core assumption: RMSNorm is unweighted or can be converted to unweighted form; residual connections are the primary architectural constraint on transformation placement.
- Evidence anchors:
  - [Page 4-5] "This invariance assumes that RMSNorm is unweighted, as weighted versions can be converted to unweighted RMSNorms by pre-multiplying weight matrices following pre-norm by the RMSNorm weights"
  - [Page 5] Equations 2-3 establish the invariance relationship with mathematical proof in Appendix A
  - [corpus] No corpus papers address this specific technique; it appears novel to this work
- Break condition: If layer normalization has learned weights that cannot be absorbed (e.g., tied to other components), the QR decomposition approach may not preserve function; non-invertible transport maps (due to width reduction) require pseudoinverse approximation.

### Mechanism 3
- Claim: Activation-based cost matrices capture functional neuron relationships better than weight-norm heuristics for transport optimization.
- Mechanism: Compute activations across calibration tokens (262K recommended) for each neuron. Ground metric C uses ℓ1 pairwise distances between activation vectors. Optimal transport minimizes ⟨T, C⟩ - λH(T), finding the minimum-cost redistribution of full neuron width onto target subset. PCA pre-projection can decorrelate activations before computing distances.
- Core assumption: Neurons with similar activation patterns over representative data perform similar functions; calibration data distribution approximates deployment distribution.
- Evidence anchors:
  - [Page 3] "We define the cost matrix of transporting mass from distribution a to distribution b, also known as ground metric, as the pairwise distances between the activations"
  - [Page 7] "By firstly transforming the activations so that their dimensions are uncorrelated, first applying PCA can help DOTRESIZE more effectively identify meaningful similarities"
  - [corpus] No direct corpus comparison; A Sparsity Predicting Approach paper notes activation sparsity patterns are input-dependent, supporting calibration data importance
- Break condition: If calibration data is unrepresentative, transport maps will optimize for wrong distribution; if critical neurons activate rarely, they may be underweighted in similarity computation.

## Foundational Learning

- Concept: Discrete Optimal Transport with Entropic Regularization
  - Why needed here: Core mathematical framework; understanding Sinkhorn distances explains why "soft" merging outperforms hard pruning
  - Quick check question: Given source distribution a (10 neurons) and target b (7 neurons), what does increasing λ do to the sparsity of transport map T?

- Concept: RMSNorm and Orthogonal Invariance in Transformers
  - Why needed here: Explains why QR decomposition is necessary; without this, arbitrary transformations would break layer normalization
  - Quick check question: Why does RMSNorm(xQ) = RMSNorm(x) for orthogonal Q but not for general invertible T?

- Concept: Structured vs Unstructured Pruning Trade-offs
  - Why needed here: Motivates width reduction over weight-level pruning; explains hardware-agnostic benefits
  - Quick check question: Why do unstructured pruning methods fail to deliver real-world speedups without specialized sparse formats?

## Architecture Onboarding

- Component map: Per-layer transformation pairs {M_A, M_inv_A} for attention outputs, {M_F, M_inv_F} for feed-forward outputs; weight matrices affected include W_Q, W_K, W_V (pre-multiplied by M_inv_F), W_O (post-multiplied by M_A), W_up, W_gate (pre-multiplied by M_inv_A), W_down (post-multiplied by M_F)

- Critical path: 1) Forward pass calibration data through model, capture activations at each layer; 2) Select target neurons (highest ℓ2 norm or post-PCA selection); 3) Compute pairwise ℓ1 activation distances → cost matrix C; 4) Solve Sinkhorn OT with λ=0.1; 5) QR decompose T, store Q and RT⁻¹; 6) Apply transformations to weight matrices following equations 4-7; 7) Evaluate compressed model

- Design tradeoffs: λ (Sinkhorn regularization): Higher = softer merging, lower = sparser; paper finds 0.1-1.0 range stable, very low (0.01) hurts some models; Calibration data size: 131K tokens sufficient, 262K recommended; diminishing returns beyond; PCA pre-projection: Improves results for some models (Llama-3.1), neutral or slightly worse for others (Mistral-7B); Sparsity level: 20% removal shows compute gains; 30%+ shows larger perplexity degradation

- Failure signatures: Model divergence or NaN outputs: QR decomposition not applied correctly, or M_inv computed without pseudoinverse handling for rank-deficient T; Worse than magnitude pruning: λ too low (overly sparse transport) or calibration data mismatch; No compute improvement: Sparsity below ~20% due to residual connection overhead; Performance varies dramatically across model families: Training differences affect neuron redundancy patterns; may need λ tuning per model

- First 3 experiments: 1) Replicate Llama-3.1-8B at 20% sparsity comparing magnitude prune vs DOTResize on Wikitext-2; expect ~12-13 perplexity gap (Table 1); 2) Ablate λ ∈ {0.01, 0.1, 1.0, 5.0} on single model to verify regularization sensitivity matches Figure 4; 3) Test calibration data scaling (65K, 131K, 262K tokens) to confirm Figure 5 robustness; verify transport map computation time scales linearly

## Open Questions the Paper Calls Out

- Question: How does DOTResize perform when combined with quantization methods, and are there synergistic or interfering interactions between optimal transport-based merging and low-bit quantization?
- Basis in paper: [explicit] The limitations section states "this method is orthogonal to methods like quantization and should be combined," but no experiments explore this combination
- Why unresolved: The authors demonstrate DOTResize independently but do not test whether the transport maps interfere with quantization-aware training or post-training quantization pipelines
- What evidence would resolve it: Perplexity and zero-shot accuracy results for models compressed with DOTResize followed by INT4/INT8 quantization, compared to quantization-only baselines

## Limitations

- Performance variability across model families suggests the method's effectiveness depends heavily on training methodology and model architecture
- Computational overhead of optimal transport requires significant computation during compression time, potentially prohibitive for extremely large models
- QR decomposition approach relies on RMSNorm's invariance to orthogonal transformations, which may not hold for weighted normalization variants

## Confidence

**High Confidence (8/10)**: The core mechanism of using optimal transport with entropic regularization for neuron merging is mathematically sound and the experimental methodology is rigorous. The perplexity improvements over magnitude pruning are substantial and well-documented across multiple model families.

**Medium Confidence (6/10)**: Claims about activation-based cost matrices capturing functional relationships better than weight-norm heuristics are plausible but not definitively proven. The method shows consistent improvements but with varying magnitudes across model families.

**Low Confidence (4/10)**: The assertion that DOTResize is universally superior to all pruning-based approaches is overstated. The method shows mixed results compared to SliceGPT on certain metrics, and the optimal hyperparameters appear model-dependent.

## Next Checks

1. **Cross-architecture generalization test**: Apply DOTResize to a diverse set of 5-7 additional LLM architectures (including both decoder-only and encoder-decoder models, different training recipes) to quantify the method's robustness. Measure performance variance and identify architectural features that predict success or failure.

2. **End-to-end deployment benchmarking**: Implement DOTResize compression in a production-like pipeline measuring total time from calibration to deployed model, including all preprocessing and optimization steps. Compare against real-world pruning workflows to quantify the practical deployment overhead.

3. **Activation similarity validation**: Conduct ablation studies systematically varying the cost matrix computation (activation-based vs weight-based vs hybrid approaches) across multiple model families. Quantify the correlation between activation similarity and functional redundancy to validate the core assumption driving the method.