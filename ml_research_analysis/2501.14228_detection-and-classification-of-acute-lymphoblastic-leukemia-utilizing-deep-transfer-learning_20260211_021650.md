---
ver: rpa2
title: Detection and Classification of Acute Lymphoblastic Leukemia Utilizing Deep
  Transfer Learning
arxiv_id: '2501.14228'
source_url: https://arxiv.org/abs/2501.14228
tags:
- leukemia
- blood
- learning
- deep
- acute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study proposes a deep learning approach to detect and classify
  Acute Lymphoblastic Leukemia (ALL) across four stages: Benign, Early, Pre, and Pro.
  Two CNN models were employed: a custom CNN architecture and a transfer learning
  model using MobileNetV2 with a modified classification head.'
---

# Detection and Classification of Acute Lymphoblastic Leukemia Utilizing Deep Transfer Learning

## Quick Facts
- arXiv ID: 2501.14228
- Source URL: https://arxiv.org/abs/2501.14228
- Reference count: 32
- Two deep learning models achieved 98.6% (custom CNN) and 99.69% (MobileNetV2) accuracy on 4-class ALL classification

## Executive Summary
This study presents a deep learning approach for detecting and classifying Acute Lymphoblastic Leukemia across four stages: Benign, Early, Pre, and Pro. The authors employ two CNN architectures: a custom CNN and a transfer learning model using MobileNetV2 with ImageNet weights. Using 3,256 blood smear images from 89 individuals, the models achieved high accuracy rates of 98.6% and 99.69% respectively, with strong F1 scores and AUC values, demonstrating potential for reliable clinical application.

## Method Summary
The study uses 3,256 blood smear images (224×224 RGB) from 89 individuals, split 80/10/10 for training, validation, and testing. SMOTE oversampling balances the four-class distribution. Two models are evaluated: (1) Custom CNN with 3 Conv layers (32→64→128 filters) plus dense layers with 0.5 dropout, and (2) MobileNetV2 with ImageNet weights and modified classification head. Both models use Adam optimizer (lr=0.0001), batch_size=16, and categorical crossentropy loss over 150 epochs.

## Key Results
- Custom CNN achieved 98.6% accuracy on 4-class ALL classification
- MobileNetV2 achieved 99.69% accuracy, the highest among evaluated models
- Both models demonstrated strong performance with high F1 scores and AUC values (0.9990 for MobileNetV2)
- SMOTE successfully balanced class distributions, addressing potential overfitting issues

## Why This Works (Mechanism)

### Mechanism 1
Transfer learning from ImageNet to blood smear classification accelerates convergence and improves accuracy compared to training from scratch. MobileNetV2's pre-trained convolutional filters (originally learned on natural images) transfer low-level edge and texture detectors that remain useful for cell morphology, while the custom classification head adapts high-level features to the 4-class ALL staging task. Core assumption: ImageNet pre-training provides transferable features despite the domain shift from natural images to microscopic blood smears.

### Mechanism 2
SMOTE-based class balancing reduces overfitting to majority classes and improves minority class detection. SMOTE generates synthetic training examples for underrepresented classes by interpolating between existing minority samples, ensuring the model receives balanced gradients across all 4 ALL stages during training. Core assumption: Synthetic samples generated via feature-space interpolation represent plausible variations of minority class samples.

### Mechanism 3
Hierarchical convolutional feature extraction with progressive filter expansion (32→64→128) captures multi-scale morphological patterns for ALL staging. Early layers detect edges and textures; deeper layers with more filters capture abstract cell-shape and nuclear-morphology features relevant to distinguishing Benign, Early, Pre, and Pro stages. Core assumption: Four-class discrimination requires learning hierarchical features rather than being solvable with shallow architectures.

## Foundational Learning

- **Transfer Learning with Frozen vs. Fine-tuned Backbones**: Why needed here: MobileNetV2 was used with ImageNet weights; understanding whether backbone was frozen or fine-tuned is critical for reproducibility and deployment decisions. Quick check question: Can you explain when to freeze pre-trained weights versus fine-tune them, and what learning rate adjustments are needed for each?

- **SMOTE for Image Data (Feature-space vs. Pixel-space)**: Why needed here: Paper applies SMOTE to training data but doesn't specify whether oversampling occurs in pixel-space or learned feature-space. Quick check question: How does SMOTE generate synthetic samples, and what are the risks of applying it to image pixel values versus learned embeddings?

- **Multi-class Evaluation Metrics (AUC, F1, Confusion Matrix)**: Why needed here: Paper reports macro vs. per-class metrics; understanding multi-class AUC computation is essential for interpreting 0.9990 AUC claim. Quick check question: How is AUC computed for multi-class problems (one-vs-rest vs. micro/macro averaging), and which approach did this paper likely use?

## Architecture Onboarding

- **Component map**: Input Pipeline (224×224 RGB images → normalization → label encoding → shuffling → SMOTE) → MobileNetV2 Backbone (ImageNet weights → Global Average Pooling → Dense layers → Softmax) OR Custom CNN (Conv(32)→MaxPool→Conv(64)→MaxPool→Conv(128)→MaxPool→Flatten→Dense(128)→Dropout(0.5)→Softmax)

- **Critical path**: Data preprocessing (SMOTE on training split; normalization) → Model selection (MobileNetV2 for accuracy, Custom CNN for interpretability/speed) → Training with Adam optimizer, lr=0.0001, batch_size=16, 150 epochs → Evaluation on held-out test set (10%)

- **Design tradeoffs**: MobileNetV2: Higher accuracy (99.69%) but requires pre-trained weights and may be less interpretable; Custom CNN: Slightly lower accuracy (98.6%) but lighter weight and fully transparent architecture; SMOTE: Addresses class imbalance but introduces synthetic data risks

- **Failure signatures**: Class confusion between adjacent stages (Early vs. Pre) visible in confusion matrix—likely due to morphological similarity; Overfitting risk if SMOTE-generated samples leak into validation/test splits; Data leakage if same patient's images appear in multiple splits

- **First 3 experiments**: 1) Reproduce baseline: Implement MobileNetV2 with ImageNet weights on ALL dataset without SMOTE; compare accuracy to 99.69% claim to isolate SMOTE contribution; 2) Patient-level split validation: Re-split data ensuring no patient overlap between train/val/test; measure if accuracy degrades (potential data leakage indicator); 3) Ablation on SMOTE: Train both models with/without SMOTE and with alternative augmentation (e.g., geometric transforms only); compare per-class F1 scores to identify which classes benefit most from synthetic oversampling

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- Patient-level data splitting not mentioned, suggesting possible data leakage risk from random image splitting
- Small dataset (89 individuals) may not support robust generalization claims across different clinical settings
- SMOTE application to image data raises concerns about synthetic sample validity and whether generated features represent biologically plausible cell morphologies

## Confidence
- MobileNetV2 accuracy claim (99.69%): **High** - well-supported by evidence and consistent with transfer learning literature
- Custom CNN architecture effectiveness: **Medium** - adequate validation but lacks comparative analysis
- SMOTE contribution to performance: **Low-Medium** - mechanism described but effectiveness not independently validated
- Clinical applicability of 4-stage classification: **Medium** - technical performance strong but clinical validation absent

## Next Checks
1. **Patient-level Split Validation**: Re-split the dataset ensuring no patient appears in multiple train/validation/test sets to verify the 99.69% accuracy claim holds under proper clinical data partitioning.

2. **Ablation Study on Transfer Learning**: Train the same MobileNetV2 architecture from scratch (random initialization) versus with ImageNet weights to quantify the actual transfer learning benefit in this domain.

3. **SMOTE vs. Traditional Augmentation**: Compare model performance using SMOTE-generated synthetic samples versus traditional image augmentations (rotation, flipping, brightness) to determine which approach provides more reliable improvements.