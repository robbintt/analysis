---
ver: rpa2
title: 'SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and
  Step-Level Policy Optimization'
arxiv_id: '2512.02631'
source_url: https://arxiv.org/abs/2512.02631
tags:
- agent
- navigation
- action
- srgpo
- view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses perception errors in Vision-Language Navigation
  (VLN) agents caused by visual hallucinations and spatial understanding limitations.
  The proposed SeeNav-Agent introduces a dual-view Visual Prompt (VP) technique that
  reduces hallucinations and improves spatial understanding by combining first-person
  and bird's-eye views with bounding boxes, navigation lines, and action projections.
---

# SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization

## Quick Facts
- **arXiv ID**: 2512.02631
- **Source URL**: https://arxiv.org/abs/2512.02631
- **Reference count**: 40
- **Primary result**: GPT-4.1 with dual-view Visual Prompt achieves 86.7% success rate, surpassing previous best LVLM by 20 percentage points

## Executive Summary
SeeNav-Agent addresses perception errors in Vision-Language Navigation (VLN) agents caused by visual hallucinations and spatial understanding limitations. The paper introduces a dual-view Visual Prompt (VP) technique that combines first-person and bird's-eye views with structured annotations, and a novel Step Reward Group Policy Optimization (SRGPO) algorithm for efficient step-level credit assignment. Experimental results on EmbodiedBench Navigation benchmark show GPT-4.1 with VP achieves 86.7% success rate, while Qwen2.5-VL-3B trained with VP and SRGPO reaches 72.3%, outperforming previous best models by 5.6 percentage points.

## Method Summary
The method consists of two main components: (1) Dual-view Visual Prompt (VP) technique that augments navigation images with bounding boxes, navigation lines, agent markers, and action projections across first-person and bird's-eye views, and (2) Step Reward Group Policy Optimization (SRGPO) that uses verifiable process rewards based on distance-to-target changes and visibility, enabling efficient random grouping of navigation steps for reinforcement learning. The approach trains Qwen2.5-VL-3B-Instruct through SFT and RFT, with SRGPO using bi-level advantages combining trajectory-level and step-level signals.

## Key Results
- GPT-4.1 with dual-view VP achieves 86.7% success rate, surpassing previous best LVLM by 20 percentage points
- Qwen2.5-VL-3B trained with VP and SRGPO reaches 72.3% success rate, outperforming previous best model by 5.6 percentage points
- SRGPO demonstrates superior training stability, convergence speed, and generalization compared to existing methods like GRPO and GiGPO

## Why This Works (Mechanism)

### Mechanism 1: Dual-view visual prompts with structured annotations
LVLMs struggle with spatial reasoning due to pre-training corpora focusing on VQA-style tasks. By presenting both first-person and bird's-eye views with bounding boxes, navigation lines, agent markers, and action projections, the system provides redundant spatial cues that compensate for weak depth and spatial understanding. The combination of all VP modules is necessary—using dual-view without full VP modules degrades performance below single-view baseline.

### Mechanism 2: Action projection transforms planning to VQA selection
Instead of generating actions from scratch, candidate movements are rendered as numbered arrows on images. The LVLM selects the optimal action ID, leveraging its pre-trained visual-linguistic alignment rather than abstract planning. This converts the planning problem into a VQA-style selection task that LVLMs are better equipped to handle.

### Mechanism 3: SRGPO enables efficient step-level credit assignment
Traditional RL methods use sparse outcome rewards or require identical anchor states for grouping. SRGPO defines verifiable process rewards based only on distance-to-target change and target visibility—state-independent signals allowing random grouping across all batch steps. Bi-level advantages combine trajectory-level (sparse) and step-level (dense) signals for efficient learning.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: SRGPO builds on GRPO's advantage estimation framework. Without understanding GRPO's baseline normalization and group-based comparison, the step-level extension won't make sense. Quick check: Can you explain how GRPO computes advantages differently from PPO?

- **Visual Prompting for LVLMs**: The dual-view VP technique is the primary input-space intervention. Understanding how visual markers (bounding boxes, arrows) affect LVLM attention is essential for debugging annotation failures. Quick check: Why might adding visual markers hurt performance if they're inconsistent with the model's pre-training distribution?

- **Credit Assignment in RL**: SRGPO's core contribution is improved credit assignment via step-level rewards. Understanding the sparse vs. dense reward trade-off is critical. Quick check: In a 20-step navigation task with only final success/failure signal, which steps should receive credit for success?

## Architecture Onboarding

- **Component map**: Environment -> Dual-view Image Generator -> VP Annotation Module (BB, NL, AM, AP, VA) -> Concatenated FV+BEV Image -> LVLM Backbone -> VPR Calculator -> SRGPO Advantage Estimator -> Policy Update -> Action Decoder -> Environment Execution

- **Critical path**: VP annotation quality → action projection clarity → step reward calculation → advantage estimation grouping. Errors in VP annotation propagate through the entire pipeline.

- **Design tradeoffs**: Larger step group size (N_S) improves advantage estimation robustness but requires more memory; N_S=16 performed well, N_S=8 showed high variance. Dual-view provides more information but without all VP modules, performance degrades below single-view baseline. Process reward simplicity (distance+visibility) enables random grouping but may miss nuanced navigation strategies.

- **Failure signatures**: Success rate drops when using dual-view without full VP modules (Table 2: 0.450 vs. 0.650 baseline). High standard deviation in training curves indicates insufficient group size or unstable reward signals. Agent repeatedly selects invalid actions suggests AP visualization is unclear or action space projection is incomplete.

- **First 3 experiments**: 1) VP ablation by module: Run GPT-4.1 with single-module removals to identify critical components (replicate Table 2 logic on your environment). 2) Group size sensitivity: Train with N_S ∈ {4, 8, 16, 32} and plot convergence speed vs. final success rate to find compute-optimal setting. 3) Process reward validity: Compare SRGPO with VPR against SRGPO with oracle step rewards (human-labeled action quality) to quantify reward design gap.

## Open Questions the Paper Calls Out

### Open Question 1
Can SRGPO be effectively adapted for embodied tasks where success is not strictly correlated with spatial distance reduction? The current Verifiable Process Reward (VPR) relies specifically on distance changes and visibility of the target object, metrics that may not exist for tasks like object manipulation or complex reasoning. Successful application of SRGPO with a modified VPR on a non-navigation embodied benchmark would resolve this.

### Open Question 2
How can the Visual Prompt (VP) modules be automated to function in real-world environments lacking ground-truth annotations? The current method appears to rely on simulator-provided metadata to draw prompts, creating a sim-to-real gap. A system integration using separate, trained perception models (e.g., depth estimation, object detection) to generate the VP on raw camera input without performance loss would resolve this.

### Open Question 3
Does SeeNav-Agent maintain its performance advantage in multi-room or outdoor navigation scenarios? Experiments were restricted to the EmbodiedBench-Navigation benchmark, which consists of single-room tasks with relatively constrained complexity. Benchmarking results on large-scale VLN datasets (e.g., Habitat-Matterport 3D or R2R) demonstrating generalization to larger, multi-room environments would resolve this.

## Limitations
- The paper's core contributions rely on untested assumptions about LVLM behavior and RL reward design
- The claim that action projection converts planning to VQA-style selection hasn't been verified through ablation studies or attention analysis
- SRGPO introduces novel process rewards without corpus validation of their sufficiency as action quality proxies

## Confidence

- **High Confidence**: VP technique improves performance over baseline when all components are active (Table 2 shows 65% vs 45% success rate). Experimental setup and metrics are clearly specified.
- **Medium Confidence**: SRGPO shows superior convergence and generalization compared to GRPO/GiGPO (Fig. 4), but the superiority of VPR over alternative process rewards hasn't been established.
- **Low Confidence**: Mechanistic explanations for why VP reduces hallucinations and why SRGPO's state-independent rewards suffice for credit assignment are primarily theoretical.

## Next Checks

1. **VP Hallucination Reduction**: Run GPT-4.1 with VP on tasks where it previously hallucinated, then systematically remove each VP component (BB, NL, AM, AP, VA) to measure which most effectively reduces false target detection claims.

2. **SRGPO Reward Design Validation**: Implement SRGPO with oracle step-level rewards (expert-labeled action quality) and compare against VPR-based SRGPO to quantify how much performance depends on the specific reward design versus the grouping algorithm.

3. **Action Projection Coverage Analysis**: In complex environments with multiple potential paths, measure the frequency that optimal actions fall outside projected action sets, and analyze how this correlates with selection accuracy degradation.