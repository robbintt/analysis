---
ver: rpa2
title: 'The \textit{Questio de aqua et terra}: A Computational Authorship Verification
  Study'
arxiv_id: '2501.05480'
source_url: https://arxiv.org/abs/2501.05480
tags:
- questio
- texts
- veri
- dante
- authorship
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the authorship of the Questio de aqua et
  terra using computational authorship verification (AV). A reference corpus of 330
  Latin texts from the 13th and 14th centuries is assembled and used to evaluate different
  AV systems via leave-one-out cross-validation.
---

# The \textit{Questio de aqua et terra}: A Computational Authorship Verification Study

## Quick Facts
- **arXiv ID**: 2501.05480
- **Source URL**: https://arxiv.org/abs/2501.05480
- **Reference count**: 40
- **Primary result**: Computational AV study attributing the *Questio de aqua et terra* to Dante Alighieri with probability 0.999999967

## Executive Summary
This study investigates the authorship of the *Questio de aqua et terra* using computational authorship verification (AV). The authors assemble a reference corpus of 330 Latin texts from the 13th and 14th centuries and evaluate different AV systems via leave-one-out cross-validation. The best-performing system achieves F1=0.970 using logistic regression combined with Distributional Random Oversampling (DRO) and features including token lengths, function words, sentence lengths, POS n-grams, and character n-grams. DRO is identified as the key contributor to accuracy, improving F1 from 0.400 to 0.970. The system attributes the *Questio* to Dante Alighieri with probability 0.999999967. Additional authorship attribution experiments considering 38 candidate authors confirm Dante as the most likely author with probability 0.737. The results provide strong evidence for Dante's authorship of the *Questio* and demonstrate DRO's effectiveness in authorship verification for cultural heritage applications.

## Method Summary
The study uses a reference corpus of 330 Latin texts (13th-14th century) including 16 Dantean texts and 314 non-Dantean texts from 38 authors. Texts are segmented into ≤400-token chunks, preprocessed (lowercasing, v→u/j→i normalization, removing quoted excerpts), and represented using 12,807 features: token lengths, function words (from Corbara et al. 2022), sentence lengths, POS n-grams (n=1,2,3), and character n-grams (n=1,2,3). Distributional Random Oversampling (DRO) generates synthetic positive examples to achieve a 20/80 class ratio. Logistic regression with per-fold hyperparameter optimization is trained via leave-one-out cross-validation. The final model is trained on the full corpus and applied to the *Questio*, returning a posterior probability of authorship.

## Key Results
- DRO increases F1 from 0.400 to 0.970 in leave-one-out cross-validation
- Character n-grams are critical: removing them drops F1 to 0.455
- The *Questio* is attributed to Dante with probability 0.999999967
- Among 38 candidate authors, Dante has the highest attribution probability (0.737)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributional Random Oversampling (DRO) enables high-accuracy authorship verification when positive training examples are scarce.
- Mechanism: DRO generates synthetic minority-class examples by extending vector representations with distributionally-derived "latent" features, creating plausible variations that reflect semantic properties of existing features. This balances the training set for the binary classifier.
- Core assumption: Stylometric features (function words, POS n-grams, character n-grams) obey the distributional hypothesis sufficiently for meaningful synthetic example generation—an assumption the paper notes is unorthodox for non-content-bearing features.
- Evidence anchors:
  - [abstract] "The key contribution to the accuracy of this system is shown to come from Distributional Random Oversampling (DRO), a technique specially tailored to text classification which is here used for the first time in AV."
  - [section 4.1/Table 2] Ablation shows F1 drops from 0.970 to 0.400 without DRO (12 of 16 Dantean texts misclassified).
  - [corpus] Weak—no neighbor papers explicitly validate DRO for AV; neighboring work focuses on LLM-era attacks and detection rather than oversampling methods.
- Break condition: If stylometric features are not distributionally coherent (e.g., highly sparse, non-semantic), synthetic examples may not capture meaningful variation, degrading classifier generalization.

### Mechanism 2
- Claim: Character n-grams capture fine-grained, unconscious stylistic patterns that distinguish authors.
- Mechanism: Character n-grams (n=1,2,3) encode sub-word and morphological regularities (e.g., suffixes, prefixes, common letter combinations) that authors produce consistently without conscious control, providing a robust stylistic fingerprint.
- Core assumption: Authors cannot easily or consciously manipulate character-level frequencies to imitate another's style.
- Evidence anchors:
  - [section 4.1/Table 2] Removing character n-grams drops F1 from 0.970 to 0.455; 11 of 16 Dantean texts misclassified.
  - [section 6] "It seems virtually impossible that s/he can imitate Dante's character n-gram frequencies."
  - [corpus] Related work (Masks and Mimicry, StegoStylo) confirms character-level features are targeted by obfuscation attacks, supporting their discriminative power but also vulnerability to adversarial manipulation.
- Break condition: If a skilled forger systematically mimics character-level patterns (or uses LLM paraphrasing), the signal may degrade or be masked.

### Mechanism 3
- Claim: Leave-one-out cross-validation on a heterogeneous corpus provides a robust accuracy estimate for the authorship verifier.
- Mechanism: LOO trains on N-1 texts and tests on the held-out text, repeating for all N texts. This maximizes training data per fold and tests on the full corpus, reducing variance in accuracy estimates and ensuring the estimate reflects real deployment conditions.
- Core assumption: The held-out text is representative of the disputed text in terms of style, topic, and length.
- Evidence anchors:
  - [section 3.4] "The LOO protocol has the advantage of (a) testing the accuracy of the verification method on the largest possible set of texts (the entire set L)... and (b) testing it on verifiers trained on approximately the same number of training examples as the verifier that will be deployed."
  - [section 4] LOO yields F1=0.970 (329/330 correct), with only Boccaccio's Epistle 23 misclassified.
  - [corpus] Weak—neighbor papers do not specifically evaluate LOO for AV; evaluation protocols vary (held-out sets, adversarial robustness tests).
- Break condition: If the disputed text is stylistically or topically out-of-distribution relative to the corpus, LOO accuracy may not generalize to the real task.

## Foundational Learning

- **Concept: Binary text classification with imbalanced classes**
  - Why needed here: AV is framed as binary classification (Dante vs. notDante), but only 16 of 330 texts are Dantean, creating severe class imbalance that baseline classifiers handle poorly.
  - Quick check question: Why would a standard classifier trained on this imbalanced corpus likely predict "notDante" for most inputs?

- **Concept: Distributional hypothesis in computational linguistics**
  - Why needed here: DRO's effectiveness relies on the assumption that stylometric features behave distributionally; understanding this helps assess whether DRO is appropriate for new feature sets.
  - Quick check question: What does the distributional hypothesis state about word meaning, and why might it also apply to function word or character n-gram usage patterns?

- **Concept: Logistic regression for text classification**
  - Why needed here: The authors choose logistic regression for its efficiency, interpretability, and well-calibrated probability outputs; understanding its linear decision boundary helps interpret feature importance.
  - Quick check question: Why is logistic regression's linearity an advantage for analyzing which features drove a specific prediction?

## Architecture Onboarding

- **Component map**: Corpus assembly → Preprocessing → Feature extraction → DRO oversampling → Logistic regression → Leave-one-out cross-validation → Final training → Prediction on Questio

- **Critical path**: Corpus → Preprocessing → Feature extraction → DRO → Logistic regression → LOO evaluation → Final training → Prediction on Questio. The most fragile step is DRO: if synthetic examples are not distributionally plausible, classifier generalization fails.

- **Design tradeoffs**:
  - Feature richness vs. topic/genre bias: More features (e.g., higher-order n-grams) risk encoding topic or genre rather than pure style.
  - Corpus heterogeneity vs. comparability: Including diverse genres (epistles, cosmological treatises, poems) improves robustness but may dilute stylistic signal.
  - LOO accuracy vs. computational cost: LOO is maximally precise but requires training N models (330 here), which is expensive for large corpora.

- **Failure signatures**:
  - Low F1 on LOO (>20% misclassifications): Indicates features or classifier are inadequate, or corpus is too heterogeneous.
  - Low soft F1 (F1^s) despite high F1: Classifier is correct but uncertain; posterior probabilities are not well-calibrated.
  - Misclassification of texts similar to Questio in topic/genre: Suggests features are not style-neutral, or corpus composition is biased.
  - DRO degradation: If F1 without DRO is comparable to or better than with DRO, synthetic examples may be adding noise rather than signal.

- **First 3 experiments**:
  1. Reproduce LOO baseline with logistic regression and the five feature sets (token lengths, function words, sentence lengths, POS n-grams, character n-grams) but without DRO. Verify F1~0.4–0.5.
  2. Add DRO with 20/80 class ratio and re-run LOO. Confirm F1 rises to ~0.97; inspect latent feature counts and synthetic example quality.
  3. Perform ablation: remove character n-grams and re-run LOO with DRO. Confirm F1 drops sharply (~0.45); analyze which texts are misclassified and whether they share stylistic properties.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do stylometric features such as function words and POS n-grams obey the distributional hypothesis in the same way as content-bearing words?
- **Basis in paper**: [explicit] The authors note that DRO's success with these features suggests they obey the distributional hypothesis, a fact "that might deserve further investigation."
- **Why unresolved**: DRO was applied unorthodoxly to features usually considered distinct from semantic content, and this theoretical basis remains unproven.
- **What evidence would resolve it**: Theoretical analysis or ablation studies specifically testing the semantic distributionality of these syntactic features.

### Open Question 2
- **Question**: Can a skilled forger consciously manipulate micro-features like character n-gram frequencies to deceive computational verification systems?
- **Basis in paper**: [inferred] The authors argue that linguistic micro-phenomena "escape the conscious control of the author" and are virtually impossible to imitate, but they do not empirically test this assumption against adversarial attacks.
- **Why unresolved**: While the system detects Dante's style, the specific claim that these features are immune to forgery is an assumption central to the "hoax" debate.
- **What evidence would resolve it**: Experiments involving adversarial stylometry, where authors deliberately attempt to mimic the statistical profile of a target author.

### Open Question 3
- **Question**: How does the verification system perform when the true author of a disputed text is completely absent from the reference corpus (the "Adso da Melk" scenario)?
- **Basis in paper**: [inferred] The authors discuss the possibility of an unknown true author but rely on the high probability score for Dante to dismiss the risk, rather than testing the system's false positive behavior on out-of-set authors.
- **Why unresolved**: The study demonstrates the system can identify Dante, but does not fully validate its ability to reject a "confounder" author who is stylistically similar but absent from the training data.
- **What evidence would resolve it**: Leave-one-out tests simulating "imposter" authors by removing specific writers from the corpus and testing if their texts are misattributed to the target.

## Limitations

- DRO's theoretical justification for stylometric features is unproven; the distributional hypothesis's applicability to non-content-bearing features remains untested.
- The corpus is heterogeneous, introducing topic and genre confounds that may dilute stylistic signal or bias results.
- The study does not evaluate adversarial robustness or the system's performance on out-of-distribution texts or unknown authors.

## Confidence

- **High confidence**: DRO is the key driver of LOO accuracy (F1 0.400 → 0.970), and character n-grams are critical for distinguishing Dante from other authors. The attribution probability for Dante is extremely high under the stated model.
- **Medium confidence**: DRO's effectiveness generalizes beyond this corpus and feature set; the Questio's attribution is robust to minor corpus or preprocessing changes; the LOO protocol provides a reliable estimate for the real task.
- **Low confidence**: DRO's theoretical justification for stylometric features; the model's calibration and probabilistic reliability; robustness to adversarial attacks or out-of-distribution texts; generalizability to other languages or historical periods.

## Next Checks

1. **DRO Ablation and Synthetic Example Inspection**: Re-run the LOO experiment without DRO and verify the F1 drops to ~0.400. Inspect a sample of DRO-generated synthetic examples for distributional plausibility and feature coherence. Test whether DRO's gains persist if the synthetic ratio is varied (e.g., 10/90, 30/70) or if synthetic examples are replaced with SMOTE or other oversampling methods.

2. **Adversarial Robustness and Attack Surface**: Apply known AV attacks (e.g., paraphrasing, style transfer, function word substitution) to a subset of the corpus and re-run LOO. Measure the drop in F1 and the change in attribution probabilities for Dante. This will reveal whether DRO and character n-grams are genuinely robust or vulnerable to simple manipulations.

3. **Corpus Heterogeneity and Out-of-Distribution Testing**: Hold out texts most similar to the Questio in topic or genre (e.g., other cosmological or scientific treatises) and re-run LOO. Compare the F1 and calibration (soft F1, Brier score) to the full-corpus baseline. Additionally, test the classifier on a small set of Latin texts from a different period or genre to assess out-of-distribution generalization.