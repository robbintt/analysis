---
ver: rpa2
title: GAC-Net_Geometric and attention-based Network for Depth Completion
arxiv_id: '2501.07988'
source_url: https://arxiv.org/abs/2501.07988
tags:
- depth
- completion
- features
- sparse
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of depth completion in autonomous
  driving, aiming to generate high-quality dense depth maps from sparse LiDAR measurements
  using image guidance. The core method involves a three-stage network architecture
  that incorporates PointNet++ for global 3D feature extraction from sparse point
  clouds, a channel attention-based feature fusion module for multimodal fusion, and
  residual learning with CSPN++ for depth refinement.
---

# GAC-Net_Geometric and attention-based Network for Depth Completion

## Quick Facts
- arXiv ID: 2501.07988
- Source URL: https://arxiv.org/abs/2501.07988
- Authors: Kuang Zhu; Xingli Gan; Min Sun
- Reference count: 9
- Primary Result: RMSE 680.82 mm on KITTI Depth Completion test set, outperforming BP-Net (684.90 mm) and TPVD (693.97 mm)

## Executive Summary
This paper addresses the depth completion problem in autonomous driving by proposing GAC-Net, a three-stage network architecture that combines 3D geometric feature extraction, attention-based multimodal fusion, and residual learning with CSPN++ refinement. The method leverages PointNet++ to extract global 3D features from sparse LiDAR point clouds, integrates these with RGB image features through a channel attention mechanism, and refines the depth maps while preserving sparse depth constraints. The proposed approach achieves state-of-the-art performance on the KITTI Depth Completion dataset with an RMSE of 680.82 mm, demonstrating significant improvements particularly in edge areas and large void regions.

## Method Summary
GAC-Net employs a three-stage architecture for depth completion: (1) a preprocessing stage using bilateral propagation to generate an initial dense depth map from sparse LiDAR input, (2) a multimodal fusion stage where a U-Net combines RGB images and initial depth, and integrates global 3D geometric features extracted by PointNet++ through a channel attention module (CAFFM), and (3) a refinement stage that applies CSPN++ with residual learning to further improve depth quality while preserving sparse depth constraints. The network is trained with multi-scale L2 loss using AdamW optimizer with warm-up and cosine annealing learning rate schedule, operating on 6 different scales from 1/32 to full resolution.

## Key Results
- Achieves state-of-the-art RMSE of 680.82 mm on KITTI Depth Completion test set
- Significant improvement over previous methods: BP-Net (RMSE: 684.90 mm), TPVD (RMSE: 693.97 mm)
- Ablation study shows 3D branch contributes ~16 mm RMSE reduction
- Demonstrates robustness in handling sparse and complex scenes with improved accuracy in edge areas and large void regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extracting global geometric priors from 3D point clouds improves depth completion in large voids where 2D convolutions fail.
- **Mechanism:** PointNet++ processes back-projected sparse depth into 3D points, providing global structural context that 2D convolutions miss.
- **Core assumption:** Global feature vector encodes structural regularities lost when depth is treated as 2D image channel.
- **Evidence anchors:** Abstract mentions enhancing scene perception with 3D features; ablation shows 16 mm improvement; DenseFormer validates need for strong structural handling.
- **Break condition:** Performance degrades in highly unstructured environments where geometric priors no longer match observed chaos.

### Mechanism 2
- **Claim:** Channel attention (CAFFM) outperforms simple concatenation by adaptively weighting feature reliability.
- **Mechanism:** SE blocks compute channel-wise weights, allowing suppression of ambiguous image textures in favor of geometric features based on scene context.
- **Core assumption:** Distinct, learnable utility difference exists between RGB and Depth/Geometric channels across spatial locations.
- **Evidence anchors:** Abstract highlights efficient integration via CAFFM; ablation shows RMSE reduction from 697.5 mm to 680.82 mm; All-day Depth Completion supports need for attention in low light.
- **Break condition:** SE-block pooling may destroy locality information required for fine-grained edge alignment, causing over-smoothing.

### Mechanism 3
- **Claim:** Combining residual learning with CSPN++ accelerates convergence and maintains sparse point validity better than unguided propagation.
- **Mechanism:** Refinement predicts residuals to initial dense map, then uses CSPN++ to propagate corrections while constraining output with original sparse depth values.
- **Core assumption:** Initial dense map is structurally correct but metrically noisy, requiring smooth, spatially correlated corrections.
- **Evidence anchors:** Abstract mentions combining residual learning with CSPN++; section 3.5 discusses sparse depth constraint for preserving valid points.
- **Break condition:** If initial bilateral propagation creates massive structural errors, residual assumption breaks as error becomes structural discontinuity.

## Foundational Learning

- **PointNet++ Set Abstraction**
  - **Why needed here:** GAC-Net relies on PointNet++ to process unordered 3D points and capture local structures before global aggregation.
  - **Quick check question:** How does PointNet++ handle varying point densities compared to standard 3D convolutions?

- **Squeeze-and-Excitation (SE) Blocks**
  - **Why needed here:** CAFFM module uses SE blocks for channel recalibration; understanding this is necessary to debug feature fusion failures.
  - **Quick check question:** If input image is pure noise, what should SE block theoretically do to image-derived channel weights?

- **Spatial Propagation Networks (CSPN)**
  - **Why needed here:** Final output quality relies on CSPN++ post-processing; understanding affinity weights is critical.
  - **Quick check question:** In CSPN, how does affinity matrix prevent depth values from bleeding across object boundaries?

## Architecture Onboarding

- **Component map:**
  RGB Image (I) + Sparse Depth (S) -> 3D Branch (S -> PointNet++ -> Global Vector F_3D) -> Stage 1 (S -> Bilateral Prop -> Initial Dense D_pre) -> Stage 2 (Concat(I, D_pre) -> U-Net -> F_U-Net -> CAFFM fuses F_U-Net and F_3D) -> Stage 3 (Fused Features -> CSPN++ with residual connection to D_pre)

- **Critical path:**
  Interaction between CAFFM module and CSPN++ affinity generation. If CAFFM fails to fuse 3D geometry effectively, CSPN++ lacks structural context to propagate depth correctly across large voids.

- **Design tradeoffs:**
  - PointNet++ vs 3D CNNs: PointNet++ chosen for efficiency on sparse data but may lose fine-grained local details compared to heavier 3D CNNs
  - 3-Stage Complexity: Preprocessing stage stabilizes training but introduces dependency on bilateral propagation quality

- **Failure signatures:**
  - "Blob" artifacts in large voids: Indicates 3D Global Vector not utilized or PointNet++ failed to capture scene structure
  - Blurred object boundaries: Suggests CSPN++ affinity weights not learning sharp edge constraints or CAFFM over-weighting blurry RGB features

- **First 3 experiments:**
  1. **Ablation Baseline:** Run network with 3D branch disabled (Standard 2-stage) to establish baseline RMSE (~714 mm)
  2. **Fusion Strategy Test:** Replace CAFFM with simple concatenation to quantify specific gain from attention mechanism (expected delta: ~16 mm RMSE)
  3. **Visualization of Weights:** Visualize attention maps in CAFFM to verify 3D geometric features up-weighted in texture-less regions and RGB features up-weighted at complex textures

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but presents several limitations and areas for future work that could be interpreted as implicit open questions.

## Limitations

- **Training details unspecified:** Total training epochs and precise warm-up schedule beyond the 10% rule are not provided
- **Architectural details incomplete:** Exact U-Net backbone configuration, bilateral propagation implementation, and CSPN++ iteration counts are unspecified
- **Real-time performance unknown:** No inference time or FPS metrics reported despite autonomous driving applications requiring real-time operation

## Confidence

- **High Confidence:** Core mechanism of using PointNet++ for 3D geometric feature extraction and CAFFM integration is well-described and supported by ablation results
- **Medium Confidence:** SOTA claim on KITTI DC is plausible given detailed ablation study and comparison against known methods, though training duration and exact CSPN++ settings introduce some uncertainty
- **Low Confidence:** Assertion that residual learning with CSPN++ preserves depth information is stated but not rigorously validated with qualitative or quantitative comparisons to non-residual variants

## Next Checks

1. **Ablation Baseline Verification:** Train 2-stage network (U-Net + CSPN++) without 3D branch or preprocessing stage. Confirm RMSE is approximately 714 mm to establish baseline improvement.

2. **Fusion Strategy Impact:** Replace CAFFM with simple concatenation in 3-stage network. Measure specific RMSE degradation to validate ~16 mm gain attributed to attention-based fusion.

3. **Weight Visualization:** Visualize channel attention maps from CAFFM during inference. Verify 3D geometric features are up-weighted in texture-less regions (e.g., roads) and RGB features are up-weighted at complex textures.