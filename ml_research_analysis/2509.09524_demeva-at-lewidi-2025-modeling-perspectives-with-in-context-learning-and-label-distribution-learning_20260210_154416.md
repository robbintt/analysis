---
ver: rpa2
title: 'DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and
  Label Distribution Learning'
arxiv_id: '2509.09524'
source_url: https://arxiv.org/abs/2509.09524
tags:
- label
- learning
- task
- language
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The DeMeVa team ranked second in the LeWiDi-2025 shared task by
  combining in-context learning (ICL) for perspectivist modeling with label distribution
  learning (LDL) methods. For perspectivist modeling, they prompted large language
  models using annotator-specific examples, testing both similarity-based and stratified
  label-based sampling strategies.
---

# DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning

## Quick Facts
- arXiv ID: 2509.09524
- Source URL: https://arxiv.org/abs/2509.09524
- Reference count: 16
- DeMeVa team ranked second in the LeWiDi-2025 shared task

## Executive Summary
The DeMeVa team developed a dual approach combining in-context learning (ICL) and label distribution learning (LDL) for perspectivist NLP modeling. For perspectivist modeling (Task B), they used LLMs to predict individual annotator labels by conditioning on annotator-specific examples, finding that stratified label-based sampling outperformed similarity-based sampling for ordinal scales. For soft label modeling (Task A), they adapted LDL techniques including cumulative absolute distance (CAD) loss, which proved competitive with ICL. Their best performing system used GPT-4o with stratified sampling, ranking second overall in the shared task.

## Method Summary
The team employed two complementary approaches: ICL with GPT-4o/Claude Haiku 3.5/Llama 3.1 70B for Task B (perspectivist modeling) using k=10 demonstration examples per annotator, and RoBERTa fine-tuning with CAD/CJS loss functions for Task A (soft label modeling). They tested both similarity-based sampling (using Sentence-Transformers embeddings) and stratified label-based sampling (preserving empirical label proportions). ICL predictions were aggregated across annotators to form soft labels for Task A evaluation, while LDL approaches directly predicted probability distributions over labels.

## Key Results
- Stratified label-based sampling with GPT-4o yielded the best overall performance across all datasets
- CAD loss function achieved competitive performance with ICL (0.800 vs 0.792 on CSC dataset)
- Label-based sampling improved ordinal scale predictions but offered no advantage for binary classification tasks
- Models often predicted unanimous agreement on items with actual high annotation disagreement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In-context learning can predict annotator-specific labels by conditioning on their past annotation examples.
- **Mechanism:** LLMs receive annotator-specific demonstration examples in the prompt; the model infers annotation patterns (label preferences, scale usage) and applies them to new inputs. Predictions are then aggregated across annotators to form soft labels.
- **Core assumption:** Annotators exhibit consistent labeling behaviors that can be inferred from a limited set of their past annotations.
- **Evidence anchors:**
  - [abstract] "we show that ICL can effectively predict annotator-specific annotations (perspectivist annotations), and that aggregating these predictions into soft labels yields competitive performance"
  - [section 3.1] "we propose a two-step pipeline... First, we use ICL to prompt LLMs to predict individual annotators' labels... We then use these predictions to calculate the final soft label"
  - [section 5] "models... are generally successful in mimicking specific annotators' labeling strategies"
- **Break condition:** Small annotator-specific subsets may lack sufficient examples for pattern inference (Section 3.2 notes "annotator-specific subsets can be arbitrarily small").

### Mechanism 2
- **Claim:** Stratified label-based sampling improves calibration for numeric/ordinal outputs compared to similarity-based sampling.
- **Mechanism:** By preserving empirical label proportions from each annotator's history, the model is exposed to the full range and relative frequency of their label usage, helping constrain predictions to valid ranges.
- **Core assumption:** Exposure to diverse label values in demonstrations helps models learn both the scale boundaries and annotator-specific usage patterns.
- **Evidence anchors:**
  - [section 3.3] "label-based sampling tends to improve (lower) Task A distances on the other datasets... label-based sampling offers more control of said numeric values since the model limits its outputs to within the provided label range"
  - [section 3.2] "We hypothesize that label-based sampling yields more representative examples by exposing models to diverse annotation patterns"
  - [corpus] Related work on maximizing in-context signal (Opt-ICL) suggests demonstration quality is critical; weak direct corpus evidence for this specific sampling strategy.
- **Break condition:** Binary classification tasks where stratified label-based sampling is "practically equivalent to random sampling" (Section 3.3).

### Mechanism 3
- **Claim:** Cumulative Absolute Distance (CAD) loss provides an effective training signal for ordinal label distribution prediction.
- **Mechanism:** CAD computes the sum of absolute differences between cumulative distribution functions, which equals the 1D Wasserstein distance. This respects ordinal structure by penalizing based on how far predictions are from ground truth along the ordered scale.
- **Core assumption:** The cumulative nature of the loss appropriately captures ordinal relationships between Likert-scale values.
- **Evidence anchors:**
  - [section 4.1] "CAD is equal to the Wasserstein distance in 1D" and Table 3 shows CAD achieves 0.800 on CSC, "competitive with in-context learning (0.792)"
  - [section 4.3] "CAD yields better results than CJS on the test set, given that the evaluation metric is CAD/WSD"
  - [corpus] LDL methods are explored in related work but corpus evidence specifically linking CAD to perspectivist NLP is limited.
- **Break condition:** Sparse annotations (few annotators, many scale points) where "the cumulative nature of CAD/W1 could allow small prediction errors to be diffused across subsequent labels" (Section 4.1).

## Foundational Learning

- **Concept: Label Distribution Learning (LDL)**
  - Why needed here: The paper adapts LDL techniques from computer vision to predict probability distributions over labels rather than single labels.
  - Quick check question: Can you explain how LDL differs from multi-label learning?

- **Concept: In-Context Learning (ICL) demonstration sensitivity**
  - Why needed here: The paper explicitly compares demonstration sampling strategies, showing ICL performance depends on example selection.
  - Quick check question: What are two factors that make ICL demonstrations effective for perspectivist tasks?

- **Concept: Wasserstein/Cumulative Distance Metrics**
  - Why needed here: Task A evaluation uses Wasserstein distance for ordinal scales; CAD loss directly optimizes this metric.
  - Quick check question: Why might Wasserstein distance be more appropriate than KL divergence for ordinal Likert scales?

## Architecture Onboarding

- **Component map:**
  - ICL Pipeline: Annotator history → Example sampler (similarity/stratified) → Prompt constructor → LLM → Per-annotator predictions → Aggregation → Soft labels
  - LDL Pipeline: Training data → RoBERTa encoder → Prediction head → CAD/CJS loss → Soft label output

- **Critical path:** For perspectivist modeling (Task B), the critical decision is example sampling strategy. For soft label modeling (Task A), choose between ICL aggregation (better overall performance) or LDL fine-tuning (more interpretable, no LLM inference cost).

- **Design tradeoffs:**
  - ICL: Higher inference cost, no training required, flexible across tasks; requires annotator history at inference time
  - LDL fine-tuning: Lower inference cost after training, but requires dataset-specific training; struggles with sparse distributions
  - Similarity vs. stratified sampling: Similarity works better for binary tasks; stratified better for ordinal/Likert scales

- **Failure signatures:**
  - Models predict unanimous agreement on items with actual high disagreement (Section 3.4: "models often predict unanimous agreement on instances that appear straightforward on the surface")
  - Sparse label distributions (few annotators × many scale points) cause both LDL methods to underperform
  - Binary tasks show no benefit from stratified sampling over random

- **First 3 experiments:**
  1. Replicate stratified vs. similarity sampling comparison on a binary classification dataset to confirm the paper's finding that stratification offers no advantage for binary labels.
  2. Test CAD loss on CSC with varying levels of layer freezing to understand whether the "freeze all but last six layers" choice is optimal.
  3. Measure performance degradation as the number of demonstration examples (k) decreases from 10 to find the minimum viable example count for perspectivist ICL.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent can advanced label distribution learning (LDL) techniques, beyond simple loss functions, improve soft label prediction in perspectivist NLP?
- **Basis in paper:** [explicit] The authors state they "have merely scratched the surface" by borrowing simple loss functions and clustering methods, arguing that LDL methods "merit further exploration by the perspectivist community."
- **Why unresolved:** The study only adapted cumulative distances (CAD) and clustering, leaving more complex probabilistic modeling techniques from computer vision untested in the NLP context.
- **What evidence would resolve it:** Future work applying deeper LDL architectures or probabilistic smoothing techniques to perspectivist tasks, resulting in performance gains over the baselines established in this paper.

### Open Question 2
- **Question:** Can in-context learning be modified to override LLMs' tendency to predict unanimous agreement on instances that appear "straightforward" but actually possess high annotation disagreement?
- **Basis in paper:** [inferred] The authors observe that models often predict unanimity on items that look superficially clear but are annotated ironically or subjectively (e.g., MP-dev-1597), suggesting that RLHF-induced "common sense" may hinder the recognition of pluralistic views.
- **Why unresolved:** The current prompts successfully mimic annotator styles but fail to capture "hidden" disagreement where common sense suggests a single answer.
- **What evidence would resolve it:** Experiments using counter-suggestive prompts or fine-tuning that successfully increases model uncertainty on "straightforward" items that have high annotator variance.

### Open Question 3
- **Question:** How can soft label evaluation metrics be adapted to fairly evaluate models that generate smooth probability distributions against sparse, undersampled ground truth annotations?
- **Basis in paper:** [inferred] The authors note in Section 4.3 that strict distance metrics like Wasserstein may unfairly penalize models that produce "smoother (and arguably more plausible) distributions" when the ground truth is sparse (e.g., the Par dataset).
- **Why unresolved:** The team could not apply smoothing to test sets, and current metrics penalize the very generalization (smoothing) that might represent the true underlying human opinion distribution better than the sparse data.
- **What evidence would resolve it:** The development and validation of a new evaluation metric or smoothing protocol that correlates better with human judgment of distribution quality in low-annotator-count scenarios.

## Limitations
- ICL approaches require annotator-specific history at inference time, limiting practical deployment when such history is unavailable or sparse
- Binary classification tasks show no benefit from stratified sampling over random sampling
- Models exhibit common-sense bias, predicting unanimous agreement on items with actual high annotation disagreement

## Confidence
- **High confidence**: ICL can effectively predict individual annotator labels (Task B) and stratified label-based sampling improves ordinal scale predictions compared to similarity-based sampling
- **Medium confidence**: CAD loss provides competitive performance with ICL for soft label prediction
- **Medium confidence**: Models exhibit "common-sense bias" leading to unanimous predictions on ambiguous cases

## Next Checks
1. Systematically evaluate performance degradation as the number of demonstration examples (k) decreases from 10 to 1 to identify the minimum viable example count for perspectivist ICL
2. Replicate the stratified vs. similarity sampling comparison specifically on binary classification tasks to confirm that stratification offers no advantage for binary labels
3. Measure inference time and computational cost for both ICL and LDL fine-tuning approaches, then calculate the performance-per-dollar metric to provide a more complete comparison beyond raw accuracy scores