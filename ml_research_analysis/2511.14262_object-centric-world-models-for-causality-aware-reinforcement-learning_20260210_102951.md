---
ver: rpa2
title: Object-Centric World Models for Causality-Aware Reinforcement Learning
arxiv_id: '2511.14262'
source_url: https://arxiv.org/abs/2511.14262
tags:
- stica
- world
- learning
- object-centric
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STICA, an RL agent that combines object-centric
  world models with causality-aware policy and value networks. The core innovation
  is representing observations as object-centric tokens using a slot-based autoencoder,
  enabling the model to predict token-level dynamics and interactions.
---

# Object-Centric World Models for Causality-Aware Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2511.14262
- **Source URL:** https://arxiv.org/abs/2511.14262
- **Reference count:** 13
- **Primary result:** STICA achieves up to 5.5x normalized mean return on Safety Gym tasks

## Executive Summary
This paper introduces STICA, an RL agent that combines object-centric world models with causality-aware policy and value networks. The core innovation is representing observations as object-centric tokens using a slot-based autoencoder, enabling the model to predict token-level dynamics and interactions. The policy and value networks estimate causal relationships among tokens and use them to guide attention, leading to more structured decision-making. Experiments on Safety Gym and OCVRL benchmarks show STICA outperforms state-of-the-art agents in both sample efficiency and final performance.

## Method Summary
STICA uses a slot-based autoencoder to encode observations into object-centric tokens and a static background latent. A Transformer-XL dynamics model predicts future token states and rewards. Policy and value networks estimate causal relationships among tokens and use them to modulate attention. The agent is trained via model-based RL, imagining trajectories with the world model and optimizing policy inside these imagined sequences. The method assumes 5 object slots plus one background slot, with categorical latents for slot assignment.

## Key Results
- STICA outperforms state-of-the-art agents on Safety Gym and OCVRL benchmarks
- Achieves up to 5.5x normalized mean return on Safety Gym tasks
- Ablation studies confirm both object-centric representations and causal attention contribute significantly to performance gains
- Visualizations show STICA focuses selectively on task-relevant objects while ignoring irrelevant background and non-causal elements

## Why This Works (Mechanism)

### Mechanism 1: Static Background Isolation via Dedicated Latents
The slot-based autoencoder allocates specific slots for objects and a single learnable, time-independent latent state for the background. This prevents object slots from wasting capacity on static pixels, improving dynamic object representation fidelity. The assumption is that environments can be clearly factorized into static background and dynamic objects. This works because gradient pressure focuses exclusively on dynamic entities when static reconstruction is offloaded.

### Mechanism 2: Causal Attention Modulation in Policy/Value Networks
An MLP estimates causality scores for each object token, which are used to construct a causal graph matrix that modulates Transformer attention weights. This filters non-causal noise and improves decision-making efficiency. The assumption is that task-relevant objects exert consistent, learnable influence on value or action. This works by suppressing attention between non-causal objects and policy/value query tokens.

### Mechanism 3: Token-Level Dynamics Prediction
The Transformer-based dynamics model treats object slots, actions, and rewards as distinct tokens in a sequence, predicting next states of each token independently. This contrasts with holistic models that encode the whole scene into one vector. The assumption is that objects interact in locally predictable ways distinct from global average dynamics. This works by capturing multi-body interactions and long-term dependencies at the object level.

## Foundational Learning

- **Concept: Slot Attention**
  - **Why needed:** This mechanism breaks raw pixel images into object tokens required by the world model
  - **Quick check:** Can you explain how Slot Attention uses iterative competition (attention) to bind different parts of an image feature map to different slots?

- **Concept: Model-Based Reinforcement Learning (MBRL) & Imagination**
  - **Why needed:** STICA learns from trajectories "imagined" by the world model rather than direct environment steps
  - **Quick check:** How does the agent optimize the policy using the "imagined" reward and value estimates generated by the Transformer dynamics model?

- **Concept: Transformer Attention Mechanisms**
  - **Why needed:** The core of dynamics model, policy, and value networks relies on Transformers
  - **Quick check:** In a standard self-attention layer, how do Queries and Keys interact to determine attention weights applied to Values?

## Architecture Onboarding

- **Component map:** Slot-based Autoencoder → Dynamics Model → Causal Policy/Value Networks
- **Critical path:** Replay Buffer → World Model Training → Imagination Rollout → Policy Training
- **Design tradeoffs:** Fixed slot count (5) may merge objects if insufficient or capture noise if excessive; background removal assumes static backgrounds
- **Failure signatures:** Attention smearing (low performance on precise navigation tasks), slot collapse (all slots try to reconstruct same object), permutation instability (dynamics model fails due to ordering randomness)
- **First 3 experiments:**
  1. Reconstruction Sanity Check: Visualize reconstructed images and individual slot reconstructions on Safety Gym
  2. Ablation on Causal Attention: Run STICA vs. STICA w/o CA on PointGoal task and plot attention weights
  3. Imagination Accuracy: Roll out dynamics model for 15 steps and compare predicted latent states against ground truth

## Open Questions the Paper Calls Out
- The paper explicitly limits its scope, stating the agent is aware of "causality" in the sense of token-level dependency, "not in the context of causal inference"
- The authors do not claim or investigate counterfactual reasoning capabilities
- The evaluation focuses on average return rather than safety metrics, leaving open questions about causal attention's impact on hazard avoidance

## Limitations
- The static background assumption may fail in environments with dynamic backgrounds or moving cameras
- Fixed causal scores may not adapt to context-dependent object relevance, potentially causing policy blindness
- Fixed slot count may be insufficient for environments with many objects or excessive for sparse scenes

## Confidence
- **High:** Ablation study results showing performance degradation when removing background removal or causal attention
- **Medium:** General efficacy of object-centric representations supported by corpus literature, but specific implementation impact unclear
- **Low:** Individual contribution of token-level dynamics prediction compared to holistic models not directly isolated in ablation studies

## Next Checks
1. Evaluate STICA on an environment with dynamic background (moving shadows/scrolling terrain) to assess robustness of static background isolation
2. Design a task where object relevance changes based on context (hazard appearing only under specific lighting) to test adaptability of fixed causal scores
3. Vary the number of slots in STICA and evaluate performance on environments with different object densities to determine optimal slot count