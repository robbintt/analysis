---
ver: rpa2
title: 'J&H: Evaluating the Robustness of Large Language Models Under Knowledge-Injection
  Attacks in Legal Domain'
arxiv_id: '2503.18360'
source_url: https://arxiv.org/abs/2503.18360
tags:
- attack
- llms
- crime
- legal
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates the robustness of large language models (LLMs)
  in legal domain tasks by introducing a framework called J&H that performs knowledge-injection
  attacks based on the syllogistic reasoning logic (major premise, minor premise,
  conclusion). The framework tests whether LLMs rely on domain knowledge and logical
  reasoning or are misled by small errors such as typos, synonyms, or incorrect legal
  statutes.
---

# J&H: Evaluating the Robustness of Large Language Models Under Knowledge-Injection Attacks in Legal Domain

## Quick Facts
- arXiv ID: 2503.18360
- Source URL: https://arxiv.org/abs/2503.18360
- Reference count: 8
- Primary result: LLM performance drops significantly (PDR > 30%) under syllogistic knowledge-injection attacks, especially at conclusion level

## Executive Summary
This paper introduces the J&H framework to evaluate LLM robustness in legal domain tasks by performing knowledge-injection attacks based on syllogistic reasoning logic. The framework systematically tests whether LLMs rely on domain knowledge and logical reasoning or are misled by small errors such as typos, synonyms, or incorrect legal statutes. Experiments on both general and legal-specific LLMs show significant performance drops under these attacks, with legal-specific models like Farui demonstrating better but still incomplete robustness. The study concludes that prompt-level mitigation strategies are insufficient and that deeper model training or fine-tuning with domain knowledge is needed to achieve robust legal reasoning.

## Method Summary
The J&H framework evaluates LLM robustness by decomposing legal reasoning into three components: major premise (legal statutes), minor premise (case facts), and conclusion (judgment). The framework attacks each component independently while preserving legal validity, allowing researchers to isolate which reasoning stage fails. Attacks include RAG (retrieval of different statutes), similar crime substitutions, word-level and element-level fact modifications, and conclusion-level expert opinion insertions. The evaluation uses two legal datasets (CAIL2018 and LEVEN) and measures performance using Original Accuracy, Attack Accuracy, and Performance Drop Ratio (PDR). Three mitigation methods (RAG, Chain-of-Thought, Few-shot) are tested but show only partial effectiveness.

## Key Results
- PDR exceeds 30% across most attack types, with conclusion-level attacks showing the highest vulnerability
- Legal-specific model Farui demonstrates better robustness than general models but still shows significant PDR
- RAG, Chain-of-Thought, and Few-shot mitigation methods only partially improve robustness
- Minor premise attacks targeting the four elements of criminal law are more effective than general word-level attacks
- Legal-specific pre-training provides partial robustness but fails to instill complete reasoning chains

## Why This Works (Mechanism)

### Mechanism 1: Syllogistic decomposition enables targeted probing
The J&H framework structures attacks around the three components of deductive logic (major premise = legal statutes, minor premise = case facts, conclusion = judgment). By perturbing each component independently while preserving legal validity, the framework isolates which reasoning stage fails. If models relied on genuine deductive logic, they would maintain consistency despite minor perturbations—just as legal experts do.

### Mechanism 2: Domain-specific pre-training provides partial robustness
Legal-domain models like Farui undergo additional pre-training on legal corpora, which encodes domain terminology and statute patterns. This reduces susceptibility to word-level and element-level attacks (lower PDR on minor premise attacks). However, without explicit training on reasoning structures, models remain vulnerable to conclusion-level attacks that exploit their inability to maintain logical consistency.

### Mechanism 3: Prompt-level interventions cannot inject causal reasoning structures
RAG provides relevant statutes but models may retrieve without understanding applicability conditions. Chain-of-thought elicits step-by-step outputs but the paper finds models "do not appear to understand the four elements of criminal law at all"—the generated reasoning chains can be internally inconsistent. Few-shot learning causes models to "be caught in the case details" rather than abstracting legal principles.

## Foundational Learning

- **Syllogistic reasoning structure (major/minor premise → conclusion)**
  - Why needed here: The entire attack framework is built on this three-part deductive structure. Without understanding how legal professionals chain from statutes to facts to judgments, you cannot interpret which attack level targets which reasoning failure.
  - Quick check question: Given a crime scenario, can you identify which legal statute applies (major premise), which facts are legally salient (minor premise), and how they combine to yield a charge (conclusion)?

- **Performance Drop Ratio (PDR) as robustness metric**
  - Why needed here: PDR measures relative degradation under attack. A model with 85% original accuracy and 60% attack accuracy has PDR ≈ 29%, indicating significant vulnerability even if absolute performance seems acceptable.
  - Quick check question: If Model A drops from 80% to 56% and Model B drops from 70% to 56%, which is more robust and why?

- **Four-element criminal law framework (subject, subjective aspect, objective aspect, object)**
  - Why needed here: Minor premise attacks target these four elements specifically. Understanding what each element represents is necessary to interpret why "element2element" attacks are more effective than "element2common" attacks.
  - Quick check question: In a fraud case, what is the "objective aspect" versus the "object" of the crime?

## Architecture Onboarding

- **Component map:** Dataset (CAIL2018/LEVEN) → Expert Annotation (similar crimes, four elements, synonyms) → Prompt Construction (case facts + choices + attack insertion) → Attack Layer Selection (major premise / minor premise / conclusion) → LLM Inference (general or legal-specific models) → Evaluation (Original Acc, Attack Acc, PDR)

- **Critical path:** Expert annotation quality determines attack validity. If "similar crimes" are not genuinely confusable or if synonyms alter legal meaning, the framework cannot distinguish reasoning failures from annotation errors. Annotation involved "ten law school graduate students"—replicate this expertise level for new domains.

- **Design tradeoffs:**
  - Attack granularity vs. scalability: Fine-grained four-element attacks require domain expert annotation; simpler word-attack methods scale but may not test legal reasoning specifically.
  - Dataset size vs. diversity: CAIL2018 (15,806 cases, 184 charges) provides breadth; LEVEN (3,323 cases, 61 charges) provides event-type focus. Trade coverage for annotation depth depending on evaluation goals.
  - Model selection: General models (GPT-3.5, LLaMA3) test transfer of legal knowledge; legal-specific models (Farui) test domain training effectiveness. Include both to separate knowledge encoding from reasoning capability.

- **Failure signatures:**
  - Negative PDR (attack accuracy exceeds original): Attack may be providing helpful hints rather than adversarial signals—review attack construction for inadvertent assistance.
  - Near-identical PDR across all models: Attacks may not be domain-specific enough; increase legal element targeting.
  - COT degrades performance: Models generate plausible but incorrect reasoning chains—this is expected behavior per paper findings, not an implementation bug.

- **First 3 experiments:**
  1. **Baseline replication:** Run major premise RAG attack on GPT-3.5 and Farui using LEVEN. Expect Farui PDR < GPT-3.5 PDR. Confirms framework implementation matches paper.
  2. **Ablation by attack level:** Run all three attack levels on a single model (e.g., LLaMA3). Expect conclusion-level attacks to show highest PDR, confirming logical reasoning is the weakest point.
  3. **Mitigation test:** Apply RAG enhancement to the highest-vulnerability attack type from experiment 2. Expect PDR reduction but not elimination (< 10% absolute improvement), confirming prompt-level fixes are incomplete.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific pre-training or fine-tuning methodologies are required to embed robust legal reasoning capabilities that can withstand knowledge-injection attacks better than prompt-level interventions?
- Basis in paper: The authors conclude that because "prompt-level modifications are insufficient," future "improvement should be targeted towards the pre-training or fine-tuning process of LLMs."
- Why unresolved: The paper tested prompt-based mitigations (RAG, Few-shot, COT) which failed to fully solve the robustness issue, but did not experiment with training-level interventions.
- What evidence would resolve it: A study showing that models fine-tuned with specific integration of domain knowledge and reasoning chains achieve significantly lower Performance Drop Ratios (PDR) compared to standard or prompt-enhanced models.

### Open Question 2
- Question: Can the J&H syllogistic attack framework be effectively adapted to evaluate robustness in other knowledge-intensive domains with different logical structures, such as medicine or education?
- Basis in paper: The conclusion states the framework can be "applied to more fields related to social life, such as the medical and educational domains."
- Why unresolved: The current annotation framework relies heavily on legal specificities (e.g., "four elements of criminal law," "similar crimes"); it is unproven if this logic translates directly to other domains.
- What evidence would resolve it: Successful application of the J&H framework on medical datasets (e.g., diagnosis prediction) demonstrating similar vulnerability patterns to minor premise and conclusion-level attacks.

### Open Question 3
- Question: Why does Chain-of-Thought (CoT) prompting fail to improve robustness and sometimes degrade performance in legal reasoning tasks?
- Basis in paper: The Discussion notes that "introducing COT may even make the robustness worse" because models "do not appear to understand the four elements... and may draw incorrect conclusions through incorrect logic chains."
- Why unresolved: The paper observes the failure but does not isolate whether the issue stems from the model's inability to decompose the syllogism or from hallucinations within the generated chain.
- What evidence would resolve it: An analysis of intermediate reasoning steps in CoT outputs, identifying where the logical divergence from the "four elements" occurs and how often it leads to the "incorrect logic chains" mentioned.

## Limitations
- The framework's effectiveness depends heavily on expert annotation quality, which is not directly verifiable and represents significant human effort
- The attack methodology assumes performance degradation directly indicates lack of reasoning ability, but could reflect retrieval or representation issues
- The assumption that legal experts perform consistent syllogistic reasoning may not hold across all legal traditions or jurisdictions

## Confidence

- **High Confidence:** The observation that legal-specific models show better robustness than general models is well-supported by empirical results across multiple attack types.
- **Medium Confidence:** The conclusion that prompt-level interventions cannot fully address robustness gaps is reasonable but depends on specific implementation details of RAG, Chain-of-Thought, and Few-shot methods.
- **Low Confidence:** The framework's ability to distinguish between memorized patterns and genuine deductive reasoning relies heavily on assumptions about legal expert reasoning consistency.

## Next Checks

1. **Annotation Reproducibility:** Replicate the expert annotation process for a small subset (e.g., 10 cases) with different legal experts to verify consistency in identifying synonyms, similar crimes, and four-element mappings.

2. **Cross-Domain Testing:** Apply the J&H framework to a non-legal domain with well-defined syllogistic structures (e.g., medical diagnosis) to test whether PDR patterns generalize beyond legal reasoning.

3. **Knowledge Tracing:** Implement a knowledge-tracing mechanism that tracks which legal principles models retrieve during RAG attacks versus which they generate during conclusion attacks, to distinguish retrieval failures from reasoning failures.