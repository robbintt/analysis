---
ver: rpa2
title: 'DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language
  Models'
arxiv_id: '2509.24296'
source_url: https://arxiv.org/abs/2509.24296
tags:
- safety
- mask
- arxiv
- generation
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes safety vulnerabilities in diffusion large language
  models (dLLMs) by decomposing them into intra-step and inter-step dynamics. It identifies
  that the greedy remasking strategy and denoising-path dependence are core causes
  of safety failures.
---

# DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models

## Quick Facts
- **arXiv ID:** 2509.24296
- **Source URL:** https://arxiv.org/abs/2509.24296
- **Reference count:** 32
- **Primary result:** Reduces ASR from 47.9% to 14.7% while maintaining quality and efficiency

## Executive Summary
This paper identifies and addresses unique safety vulnerabilities in diffusion large language models (dLLMs) through a two-stage training-free defense framework called DiffuGuard. The authors decompose dLLM generation into intra-step and inter-step dynamics, revealing that greedy remasking strategies and denoising-path dependence are core causes of safety failures. DiffuGuard combines stochastic annealing remasking to mitigate early harmful token selection with block-level audit and repair using internal model representations to detect and correct unsafe content. Evaluated across four dLLMs and six jailbreak attack methods, the framework significantly reduces attack success rates while preserving model performance.

## Method Summary
DiffuGuard is a training-free defense framework that intervenes in the dLLM generation process at two critical points. First, the Stochastic Annealing Remasking module modifies token selection by injecting controlled randomness into the confidence ranking, with noise strongest in early steps and decaying over time to mitigate greedy harmful token selection. Second, the Block-level Audit and Repair module computes a Safety Divergence score using internal model representations to detect potentially unsafe generations, triggering a regeneration process that suppresses previously identified harmful tokens. The framework operates without model retraining and is designed to be plug-and-play across different dLLM architectures.

## Key Results
- Reduces attack success rate from 47.9% to 14.7% across four dLLMs and six attack methods
- Maintains model utility with negligible impact on inference latency
- Achieves state-of-the-art performance compared to existing baselines including PPL-Filter and other methods

## Why This Works (Mechanism)

### Mechanism 1: Stochastic Annealing Remasking Mitigates Greedy Harmful Token Selection
The standard low-confidence remasking strategy in dLLMs exhibits a harmful bias that amplifies the selection of harmful tokens. DiffuGuard introduces controlled randomness during remasking, especially in early generation steps, by modifying the token selection with a balance factor $\alpha$ that injects random noise into the confidence ranking. This increases the probability of safe tokens with slightly lower confidence being selected. The randomness is strongest early in the generation process (when $\alpha$ is highest) and decays smoothly as generation progresses, prioritizing safety-critical early decisions while preserving later-stage coherence. Core assumption: The model's inherent safety capabilities exist but are suppressed by the decoding strategy; harmful tokens often receive spuriously high confidence scores under jailbreak prompts.

### Mechanism 2: Denoising-path Dependence and Early Token Influence
The safety of tokens generated in early steps has a decisive influence on the final output's safety, a phenomenon termed "Denoising-path Dependence." In dLLMs, once a token is fixed at a given step, it becomes permanent context for all subsequent iterations due to the iterative refinement process. This creates a strong path dependency where injecting a safe token like "Sorry" at an early step (e.g., step 1) reduces ASR far more effectively than injecting it at a later step (e.g., step 16). Core assumption: The iterative denoising process behaves like a trajectory where the initial direction strongly constrains the final outcome.

### Mechanism 3: Block-level Audit and Repair via Representation Divergence
A block-level audit can detect potentially unsafe generations by measuring the divergence between the model's internal representation for a malicious query and its representation for the query wrapped in a jailbreak template. The audit module computes a "Safety Divergence" (SD) score using the cosine distance between hidden state vectors, with the hypothesis that a successful jailbreak causes a significant deviation between the representation of the original malicious intent and the final attacked representation. If the SD score exceeds a threshold, a "Repair" process is triggered, which remasks a portion of the generated block and regenerates it with probability suppression on the original harmful tokens. Core assumption: A successful jailbreak manifests as a measurable deviation in the model's internal hidden states, and suppressing specific harmful token probabilities during regeneration can steer the model toward a safer alternative.

## Foundational Learning

- **Concept: Diffusion Large Language Models (dLLMs)**
  - **Why needed here:** The entire paper is predicated on the unique architecture of dLLMs, which differ fundamentally from Autoregressive (AR) LLMs. Understanding their iterative, parallel, and bidirectional generation paradigm is essential.
  - **Quick check question:** How does the token generation process in a dLLM differ from the left-to-right, token-by-token generation in an AR LLM?

- **Concept: Masked Discrete Diffusion and Remasking**
  - **Why needed here:** The paper's proposed defense directly intervenes in the "remasking" step of the masked diffusion process. Understanding what remasking is (reverting low-confidence tokens to a masked state for subsequent refinement) is critical to grasping DiffuGuard's mechanism.
  - **Quick check question:** In a dLLM's inference loop, what is the purpose of the remasking step, and what is the standard strategy for selecting which tokens to remask?

- **Concept: Jailbreak Attacks and Safety Alignment**
  - **Why needed here:** The paper positions itself as a defense against "jailbreak" attacks that bypass a model's "safety alignment." Grasping these concepts is necessary to understand the problem DiffuGuard is solving.
  - **Quick check question:** What is the goal of a jailbreak attack against a large language model?

## Architecture Onboarding

- **Component map:** Stochastic Annealing Remasking Module -> Block-level Audit Module -> Block-level Repair Module

- **Critical path:**
  1. Input a user query to the dLLM
  2. The Stochastic Annealing Remasking module modifies the token selection at each generation step, injecting maximal randomness early and less later
  3. Upon completion of a text block, the Block-level Audit module computes the SD score
  4. If the SD score exceeds a threshold, the Block-level Repair module triggers, remasking part of the block and forcing a regeneration that avoids the previously generated harmful tokens

- **Design tradeoffs:**
  - Safety vs. Quality: Increasing the initial randomness factor $\alpha_0$ improves safety (lower ASR) but can degrade generation quality (lower accuracy on benchmarks like GSM8K)
  - Efficiency vs. Robustness: The Repair module adds inference overhead. The authors argue it's minimal, but it involves extra forward passes. It is only applied to the first block to limit this cost

- **Failure signatures:**
  - High $\alpha_0$ with weak model: If the base model has poor inherent safety, high randomness may lead to incoherent or irrelevant outputs without improving safety
  - Undetectable jailbreaks: Attacks that manipulate behavior without causing significant hidden state divergence may evade the audit module
  - Aggressive remasking: Too much random remasking may break the logical coherence of long generations

- **First 3 experiments:**
  1. Ablation Study: Run the model with DiffuGuard, then with each module (Stochastic Remasking, Audit & Repair) removed individually to confirm their contribution on different attack types
  2. Hyperparameter Sensitivity: Vary $\alpha_0$, $\lambda$, and $\gamma$ to find the optimal balance between safety (ASR) and quality (perplexity, benchmark accuracy)
  3. Cross-Model Generalization: Apply the DiffuGuard framework with tuned hyperparameters to different dLLM families (e.g., LLaDA vs. Dream) to test its generalizability

## Open Questions the Paper Calls Out

- **Question:** How can training-free inference-time defenses like DiffuGuard be integrated with model safety alignment training (e.g., adversarial training) to create a unified safety framework?
- **Basis in paper:** The authors state in Appendix F: "We will explore how to integrate training-free defense strategies like DIFFUGUARD with model safety alignment training, for example, using attack samples detected by DIFFUGUARD to conduct adversarial training."
- **Why unresolved:** DiffuGuard is currently a plug-and-play inference module that does not update model weights. It is unclear if the safety signals derived from its detection mechanisms can effectively inform gradient-based alignment without destabilizing the diffusion generation process.
- **What evidence would resolve it:** A study measuring the performance and robustness of a dLLM fine-tuned on adversarial samples filtered or generated via DiffuGuard compared to a model using DiffuGuard solely at inference time.

## Limitations

- The query decomposition algorithm for computing Safety Divergence is not specified, blocking faithful reproduction
- The claim of "negligible impact on inference latency" lacks quantitative comparison with baseline inference times
- Corpus evidence for the novel "Denoising-path Dependence" concept is weak, with no directly cited papers using that specific terminology

## Confidence

- **High confidence:** The core experimental results showing DiffuGuard's effectiveness in reducing ASR from 47.9% to 14.7% across four dLLMs and six attack methods. The methodology for the Stochastic Annealing Remasking is clearly defined with specific equations.
- **Medium confidence:** The theoretical mechanisms (Stochastic Annealing Remasking mitigates greedy harmful token selection; Denoising-path Dependence exists; Block-level Audit and Repair can detect and correct unsafe content). While the paper provides strong internal evidence, some foundational concepts have limited direct support in the cited corpus.
- **Low confidence:** The exact implementation details of the Block-level Audit and Repair, specifically the query decomposition step for p_origin, and the precise quantification of the claimed inference latency benefits.

## Next Checks

1. **Reproduce the query decomposition algorithm:** Implement and validate the method for splitting a jailbreak prompt into its original malicious core and adversarial template. This is a prerequisite for computing the Safety Divergence baseline and is critical for the audit module to function.

2. **Benchmark inference overhead quantitatively:** Measure and report the absolute and relative increase in inference time and the number of additional forward passes required when the Block-level Repair module is triggered. Compare this overhead directly with the baseline dLLM inference time to verify the "negligible impact" claim.

3. **Test cross-model generalizability of hyperparameters:** Apply the DiffuGuard framework to a held-out dLLM model (not one of the four used in the original evaluation) using the same hyperparameters (α_0=0.3, λ=0.1, γ=0.9). This will test whether the defense is truly model-agnostic or if it requires extensive per-model tuning.