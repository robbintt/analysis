---
ver: rpa2
title: Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model
  Reasoning
arxiv_id: '2505.07527'
source_url: https://arxiv.org/abs/2505.07527
tags:
- krpo
- policy
- grpo
- group
- kalman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes KRPO, a Kalman filter-enhanced version of\
  \ Group Relative Policy Optimization (GRPO) for improving language model reasoning.\
  \ The method replaces GRPO\u2019s group mean baseline with a Kalman-filtered baseline\
  \ and uncertainty estimate, providing more accurate and adaptive advantage estimation\
  \ without adding new model parameters."
---

# Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning

## Quick Facts
- arXiv ID: 2505.07527
- Source URL: https://arxiv.org/abs/2505.07527
- Reference count: 31
- Key outcome: Kalman-filtered baseline improves RL training of small language models on math reasoning by 2.3-17.9% accuracy

## Executive Summary
This paper introduces KRPO, a Kalman filter-enhanced version of Group Relative Policy Optimization (GRPO) for reinforcement learning of language models. The key innovation replaces GRPO's group-mean baseline with a Kalman-filtered baseline and uncertainty estimate, providing more adaptive and stable advantage estimation without adding model parameters. Evaluated on math reasoning tasks (Arithmetic, OpenMath-Instruct, MATH500, AIME) using small models (Llama-3.2-1B, Qwen2.5-0.5B, Qwen2.5-1.5B), KRPO consistently outperforms GRPO and other baselines. For example, on Arithmetic, KRPO improves accuracy by 2.3-5.4% across difficulty levels; on OpenMath-Instruct, improvements range from 4.6% to 17.9%. Training curves show KRPO converges faster and reaches higher rewards, especially on harder tasks.

## Method Summary
KRPO enhances GRPO by replacing the group-mean baseline with a Kalman filter that estimates a latent reward baseline and its uncertainty. The Kalman filter recursively predicts and updates the baseline using observed rewards, producing a smoothed estimate that adapts to nonstationary reward landscapes. Advantages are normalized by the estimated uncertainty (posterior variance), creating an adaptive learning signal that self-regulates based on estimate confidence. The method adds negligible computational overhead and is robust to hyperparameter variations. During training, the Kalman filter is reset for each prompt, and advantages are computed using the final state after processing all group samples.

## Key Results
- On Arithmetic-normal, KRPO achieves 50.0% accuracy vs GRPO's 45.1% (+4.9%)
- On OpenMath-Hard, KRPO achieves 15.9% accuracy vs GRPO's 9.2% (+6.7%)
- Training curves show KRPO converges faster and reaches higher rewards across all difficulty levels
- KRPO is robust to hyperparameter variations (Q from 1e-5 to 1e-3, R from 1e-1 to 1e-2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the static group-mean baseline with a Kalman-filtered baseline reduces variance in advantage estimation when reward signals are noisy or nonstationary.
- Mechanism: The Kalman filter treats observed rewards as noisy measurements of a latent reward baseline. Through recursive prediction-update cycles, it smooths instantaneous fluctuations while tracking the underlying expected reward. This yields a more stable baseline than the empirical mean, which can swing dramatically with outlier samples in small groups.
- Core assumption: Observed rewards are noisy observations of some latent "true" baseline that evolves slowly; process noise Q > 0 allows the baseline to drift, measurement noise R > 0 acknowledges observation uncertainty.
- Evidence anchors:
  - [abstract]: "lightweight Kalman filtering to dynamically estimate the latent reward baseline and uncertainty"
  - [Section 3.2, Equations 1-5]: Full Kalman filter formulation showing prediction and update steps
  - [corpus]: Weak—no corpus papers directly validate Kalman filtering for advantage estimation; this is a novel contribution
- Break condition: If Q is set too small (≈0), the filter assumes a constant baseline and loses adaptivity; if R is set too small (e.g., 1e-3), the filter over-trusts noisy observations, causing unstable learning (Section 5.3.2).

### Mechanism 2
- Claim: Normalizing advantages by the Kalman-filtered uncertainty estimate (posterior variance) stabilizes the scale of policy gradient updates.
- Mechanism: The advantage is computed as A_i = (r_i - x̂_i|i) / sqrt(P_i|i + ε). When uncertainty P is high, the denominator grows, shrinking the advantage magnitude and damping gradient updates. When uncertainty is low, advantages retain full signal strength. This creates an adaptive learning signal that self-regulates based on estimate confidence.
- Core assumption: Variance of the baseline estimate is a meaningful proxy for advantage reliability; higher variance should reduce trust in that advantage signal.
- Evidence anchors:
  - [Section 3.2, Equation 6]: "This formulation has two benefits: it adaptively centers the reward using a filtered baseline, and it normalizes the advantage by the estimated uncertainty"
  - [Section 5.2.1]: "incorporating a Kalman Filter model for advantage estimation is especially beneficial in high-variance, complex reasoning environments"
  - [corpus]: NGRPO paper (arxiv 2509.18851) addresses variance in GRPO through different mechanism (negative sampling), suggesting variance reduction is a recognized problem
- Break condition: If ε is too small or P collapses to near-zero prematurely (overconfident filter), division by near-zero can cause numerical instability or explode advantages.

### Mechanism 3
- Claim: The Kalman filter naturally generalizes the group-mean baseline, making it a principled extension rather than an arbitrary modification.
- Mechanism: As stated in Section 4, when Q → 0 (constant latent state) and all observations are weighted equally, the Kalman filter reduces to computing a weighted average that converges to the empirical mean. By setting Q > 0, KRPO explicitly models the baseline as slowly time-varying, which is more appropriate for nonstationary reward landscapes in LLM training.
- Core assumption: The reward baseline is not truly constant across training—policy improvements shift the expected reward distribution over time.
- Evidence anchors:
  - [Section 4]: "This empirical mean can be interpreted as a special case of a Kalman filter with... process noise variance Q→0"
  - [Section 4]: "Kalman filter, which accounts for both observation noise (R > 0) and possible slight changes in the latent reward (Q > 0)"
  - [corpus]: Hybrid GRPO (arxiv 2502.01652) also extends GRPO with multi-sample evaluation, confirming group-based advantage is an active research direction
- Break condition: If the true reward baseline is actually constant and Q > 0 is set high, the filter may introduce unnecessary drift, potentially harming convergence.

## Foundational Learning

- Concept: **Kalman Filter (1D)**
  - Why needed here: KRPO's core innovation is literally a 1D Kalman filter applied to scalar reward sequences. Understanding prediction (state propagation + uncertainty growth) and update (measurement incorporation + uncertainty reduction) is non-negotiable.
  - Quick check question: Given Q=1e-5 and R=1e-2, does the filter trust the prior or the observation more after many updates?

- Concept: **Advantage Function in Policy Gradient**
  - Why needed here: The entire motivation is variance reduction in policy gradients via better advantage estimation. Without understanding why A(s,a) = Q(s,a) - V(s) reduces variance, the contribution is opaque.
  - Quick check question: Why does subtracting a baseline from returns not introduce bias into the policy gradient estimate?

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: KRPO is a drop-in replacement for GRPO's advantage computation. You must understand that GRPO samples n outputs per prompt, computes group mean as baseline, and uses relative advantages—KRPO only changes this last step.
  - Quick check question: In GRPO with group size 12, if all 12 rewards are identical, what is the advantage for each sample? What does KRPO do differently in this case?

## Architecture Onboarding

- Component map: Prompt → LLM Policy π_θ → Sample n outputs → Compute rewards r_i → GRPO baseline: mean(r_i) or KRPO baseline: Kalman filter state (x̂, P) → A_i = r_i - mean or A_i = (r_i - x̂) / sqrt(P + ε) → Policy loss (clipped)

- Critical path:
  1. Initialize Kalman state (x̂_0, P_0) per prompt (Section 5.1: "reset for each prompt")
  2. For each reward r_i in group: predict (x̂ ← x̂, P ← P + Q), then update (compute K, update x̂ and P)
  3. Compute advantage using final x̂ and P for the group
  4. Feed advantages into standard GRPO clipped objective

- Design tradeoffs:
  - **Q (process noise)**: Larger Q → faster adaptation but noisier baseline. Default 1e-5 is conservative. Robust from 1e-5 to 1e-3.
  - **R (measurement noise)**: Larger R → smoother baseline but slower tracking. Default 1e-2 works; 1e-3 is too aggressive (Section 5.3.2), 1e-1 is safe but slower.
  - **Group size**: Smaller groups → higher variance in mean baseline → larger KRPO advantage (Appendix E). KRPO outperforms GRPO across group sizes 5-15.

- Failure signatures:
  - Training instability or divergence with R < 1e-3: filter over-confident in noisy observations
  - No improvement over GRPO with Q >> 1e-2: baseline drifts too fast, losing smoothing benefit
  - Collapsed advantages (all near zero): P has collapsed to near-zero; check initialization or increase Q

- First 3 experiments:
  1. **Sanity check**: Replicate GRPO baseline on Arithmetic-normal with group size 12, seed 42. Verify ~45% accuracy (Table 1). Then swap in KRPO with default Q=1e-5, R=1e-2. Expect ~50% accuracy.
  2. **Ablation on R**: Fix Q=1e-5, vary R ∈ {1e-3, 1e-2, 1e-1} on same dataset. Plot training curves. Confirm 1e-3 is unstable, 1e-1 converges slower but reaches same final performance (Figure 4b).
  3. **Hard task validation**: Run both GRPO and KRPO on OpenMath-Hard. Expect larger improvement gap (17.9% per Table 1) than on easier subsets, confirming KRPO's advantage in high-variance settings.

## Open Questions the Paper Calls Out

- Does KRPO maintain its performance advantages over GRPO on tasks with highly subjective or noisy reward signals, such as creative writing or summarization? The current study restricts evaluation to mathematical reasoning with verifiable ground truths.

- Can the process noise (Q) and measurement noise (R) hyperparameters be adapted online to improve performance, rather than relying on static values? The current implementation requires manual tuning of Q and R.

- Does structuring the order of observations fed into the Kalman filter (e.g., by training step or difficulty) yield better baseline estimation than the current random shuffling approach? The current method is "insensitive to the permutation," effectively ignoring temporal dependencies.

## Limitations

- Experiments are limited to 0.5B-1.5B parameter models; effectiveness on larger models (7B-70B) remains untested.
- All tasks share similar characteristics (single correct answer, structured output evaluation); performance on open-ended reasoning, code generation, or multi-turn dialogue tasks is unknown.
- The paper reports "statistically significant" improvements with p<0.05 but doesn't specify the test used or provide confidence intervals for accuracy improvements.

## Confidence

- **High confidence**: The Kalman filter mechanism for advantage estimation is mathematically sound and ablation studies provide strong evidence that R=1e-3 causes instability while R≥1e-2 works reliably.
- **Medium confidence**: Claims about faster convergence and higher final rewards are supported by training curves, but the magnitude of improvement varies significantly across datasets.
- **Low confidence**: The claim that KRPO is "especially beneficial in high-variance, complex reasoning environments" lacks mechanistic explanation for why Kalman filtering helps more on harder problems.

## Next Checks

1. Test KRPO on 7B-13B parameter models on the same math reasoning tasks to verify the method scales and maintains relative performance improvements with larger models.

2. Apply KRPO to non-math reasoning tasks such as code generation (HumanEval) or multi-turn dialogue to determine if the Kalman filter advantage generalizes beyond structured math problems.

3. Replicate the ablation studies with 5+ random seeds and report confidence intervals for all accuracy metrics, along with the specific statistical tests used to determine significance.