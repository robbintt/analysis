---
ver: rpa2
title: 'Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video
  Generation Model'
arxiv_id: '2503.11251'
source_url: https://arxiv.org/abs/2503.11251
tags:
- motion
- step-video-ti2v
- ti2v
- video
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Step-Video-TI2V, a 30-billion-parameter open-source
  text-driven image-to-video generation model that extends the pre-trained Step-Video-T2V
  model by incorporating image conditioning and motion control. The model uses a Video-VAE
  to encode the input image into latent representations, which are concatenated with
  video latents for generation, and introduces motion embedding to enable explicit
  control over video dynamics via optical flow-based motion scores.
---

# Step-Video-TI2V Technical Report: A State-of-the-Art Text-Driven Image-to-Video Generation Model

## Quick Facts
- **arXiv ID**: 2503.11251
- **Source URL**: https://arxiv.org/abs/2503.11251
- **Reference count**: 4
- **Primary result**: State-of-the-art text-driven image-to-video generation with motion control, achieving SOTA on VBench-I2V and Step-Video-TI2V-Eval benchmarks

## Executive Summary
Step-Video-TI2V extends the 30-billion-parameter Step-Video-T2V model to support text-driven image-to-video generation by incorporating image conditioning and motion control. The model conditions video generation on input images via Video-VAE-encoded latents and enables explicit motion control through optical flow-based motion embeddings. Trained on 5 million text-image-video triples (80%+ anime-style), it generates up to 102 frames while achieving state-of-the-art performance on both VBench-I2V and a newly proposed Step-Video-TI2V-Eval benchmark. The model particularly excels at anime-style generation and camera motion tasks, though it shows a bias toward anime content due to training data composition.

## Method Summary
Step-Video-TI2V extends the pre-trained Step-Video-T2V model by adding two key components: image conditioning and motion control. For image conditioning, the input image is augmented with Gaussian noise, encoded by a Video-VAE to latent Zc, zero-padded, and concatenated with video latents Zv to form Zt = [Zv; Zc]. The DiT patch embedding layer is extended to handle 2c input channels. For motion control, optical flow is computed at 12-frame intervals on grayscale frames, and the mean magnitude of the highest values is embedded via a conditional MLP and combined with timestep embedding through AdaLN-Single. The model is trained on 5M text-image-video triples with motion scores, enabling explicit control over video dynamics while maintaining instruction adherence and subject/background consistency.

## Key Results
- Achieves state-of-the-art performance on VBench-I2V benchmark, excelling in anime-style generation and camera motion tasks
- Outperforms baselines on the new Step-Video-TI2V-Eval benchmark (298 image-prompt pairs) across instruction adherence, subject/background consistency, and physical law adherence
- Motion control effectively balances stability and dynamism, with optimal settings around 5-10 motion score
- Demonstrates strong capability in generating up to 102 frames from text and image inputs

## Why This Works (Mechanism)
Step-Video-TI2V works by conditioning video generation on both text and image inputs while explicitly controlling motion dynamics. The image conditioning pathway provides visual context by encoding the input image into latent representations that guide the generation process, ensuring subject and background consistency. The motion control mechanism uses optical flow to quantify motion intensity, which is then embedded and applied during generation to control the level of dynamism. This dual conditioning approach allows the model to maintain visual coherence with the input image while providing explicit control over the temporal evolution of the video content.

## Foundational Learning
- **Video-VAE encoding**: Compresses images into latent representations for conditioning; needed to bridge image and video domains; quick check: verify VAE reconstruction quality on test images
- **Optical flow computation**: Quantifies motion between frames; needed for motion score extraction and control; quick check: validate flow accuracy on known motion sequences
- **DiT patch embedding extension**: Adapts the diffusion transformer to handle concatenated latents; needed to process combined image-video information; quick check: confirm embedding layer accepts 2c channels
- **AdaLN-Single with motion embedding**: Integrates motion conditioning into the normalization layer; needed for fine-grained motion control during generation; quick check: verify motion embedding affects generation at different motion score levels
- **Motion score normalization**: Scales optical flow magnitudes for training stability; needed to prevent extreme values from destabilizing training; quick check: examine motion score distribution in training data

## Architecture Onboarding
- **Component map**: Input image -> Video-VAE -> Zc -> Concatenate with Zv -> DiT with extended patch embedding -> AdaLN-Single with motion embedding -> Output video
- **Critical path**: Image conditioning (Video-VAE + concatenation) and motion control (optical flow + motion embedding) are the two critical modifications that enable TI2V functionality
- **Design tradeoffs**: High anime data proportion (80%+) improves anime generation but limits real-world performance; motion control requires careful hyperparameter tuning to avoid artifacts
- **Failure signatures**: Low instruction adherence on real-world videos indicates training data bias; unstable artifacts at high motion scores indicate motion control sensitivity
- **First experiments**:
  1. Test image conditioning alone by disabling motion embedding to isolate its contribution
  2. Vary motion score from 1 to 15 to map the stability-artifacts trade-off curve
  3. Compare anime vs. real-world test performance to quantify training data bias effects

## Open Questions the Paper Calls Out
None

## Limitations
- Training data composition heavily biased toward anime content (80%+), limiting real-world video generation quality
- Motion control mechanism requires careful hyperparameter tuning (5-10 range) to balance between static results and unstable artifacts
- Critical training hyperparameters including learning rate, batch size, and noise augmentation parameters remain unspecified

## Confidence
- **High confidence**: Model architecture modifications and evaluation methodology using the new benchmark
- **Medium confidence**: Claims of state-of-the-art performance given training data bias and limited real-world content representation
- **Medium confidence**: Motion control effectiveness due to dependence on hyperparameter tuning and observed trade-offs

## Next Checks
1. Replicate the training process with different training data compositions (varying anime vs. real-world ratios) to quantify the impact on cross-domain performance
2. Systematically evaluate the motion control mechanism across the full suggested range (5-10) on both anime and real-world test sets to map the stability-artifacts trade-off curve
3. Conduct ablation studies removing the image conditioning pathway versus the motion conditioning pathway to isolate their individual contributions to overall performance improvements