---
ver: rpa2
title: 'Judging Against the Reference: Uncovering Knowledge-Driven Failures in LLM-Judges
  on QA Evaluation'
arxiv_id: '2601.07506'
source_url: https://arxiv.org/abs/2601.07506
tags:
- answer
- reference
- question
- predicted
- gold
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models are increasingly used as automatic judges\
  \ for question answering evaluation, but little is known about their ability to\
  \ adhere to a provided reference. This paper identifies a critical failure mode:\
  \ when the provided reference conflicts with the judge model\u2019s parametric knowledge,\
  \ the resulting scores become unreliable, substantially degrading evaluation fidelity."
---

# Judging Against the Reference: Uncovering Knowledge-Driven Failures in LLM-Judges on QA Evaluation

## Quick Facts
- arXiv ID: 2601.07506
- Source URL: https://arxiv.org/abs/2601.07506
- Authors: Dongryeol Lee; Yerin Hwang; Taegwan Kang; Minwoo Lee; Younhyung Chae; Kyomin Jung
- Reference count: 40
- Primary result: LLM judges produce unreliable scores when their parametric knowledge conflicts with provided reference answers, significantly degrading evaluation fidelity

## Executive Summary
Large language models are increasingly deployed as automatic judges for question answering evaluation, but their ability to adhere to provided references remains poorly understood. This paper identifies a critical failure mode: when a reference answer conflicts with an LLM judge's parametric knowledge, the resulting evaluation scores become unreliable. Through systematic experimentation with thirteen different LLM judges across four QA datasets, the authors demonstrate that grading reliability drops sharply under reference-belief conflicts, with judges over-relying on their internal knowledge rather than the provided reference. This vulnerability persists even under common prompt-based mitigation strategies, revealing a fundamental limitation in current LLM-as-a-judge evaluation paradigms.

## Method Summary
The authors introduce a controlled swapped-reference QA framework to systematically induce reference-belief conflicts. This framework replaces correct reference answers with incorrect entities and constructs diverse pairings of original and swapped references with correspondingly aligned candidate answers. By comparing judge performance on original versus swapped references, the study quantifies the impact of knowledge-reference conflicts on evaluation reliability. The methodology tests thirteen different LLM judges across four distinct QA datasets, providing a comprehensive assessment of this vulnerability across model architectures and task types.

## Key Results
- LLM judges' grading reliability drops sharply when reference answers conflict with their parametric knowledge
- The reliability degradation is driven by judges' over-reliance on internal knowledge, leading them to disregard provided references under conflict
- Common prompt-based mitigation strategies fail to resolve this vulnerability, indicating a fundamental limitation in LLM-as-a-judge evaluation

## Why This Works (Mechanism)
When evaluating answers, LLM judges face a critical decision point: should they prioritize the provided reference or their own parametric knowledge? The mechanism reveals that judges tend to overweight their internal knowledge when conflicts arise, effectively ignoring the reference. This occurs because the judges' parametric knowledge, shaped by extensive pretraining, creates strong priors that override external guidance. The swapped-reference framework exposes this by creating controlled conflicts where judges must choose between contradicting sources of truth.

## Foundational Learning
- Reference adherence vs. parametric knowledge: Why needed - judges must balance external references with internal knowledge; Quick check - does the judge's score change significantly when the reference contradicts their knowledge?
- Evaluation reliability metrics: Why needed - to quantify when judges fail to follow references; Quick check - correlation between judge scores and ground truth correctness drops under conflict
- Reference-belief conflict induction: Why needed - to systematically study knowledge-reference tensions; Quick check - swapped references create measurable score differences compared to originals

## Architecture Onboarding

**Component Map**: QA dataset -> Reference preparation (original/swapped) -> LLM judge -> Scoring output -> Reliability assessment

**Critical Path**: Reference preparation → Judge scoring → Reliability measurement. The reliability assessment depends entirely on the conflict between judge knowledge and provided reference.

**Design Tradeoffs**: The study prioritizes controlled experimental conditions (artificial conflicts) over ecological validity, sacrificing real-world complexity for systematic measurement. This enables clear attribution of score changes to knowledge-reference conflicts but may underestimate subtler real-world mismatches.

**Failure Signatures**: Sharp score degradation on swapped references compared to originals; reduced correlation with ground truth correctness; inconsistent treatment of otherwise equivalent answers based on reference alignment with judge knowledge.

**First Experiments**:
1. Establish baseline reliability on original references across all judge-model/dataset combinations
2. Measure reliability degradation under swapped-reference conditions
3. Test common prompt engineering mitigations (explicit instructions to follow reference, confidence weighting)

## Open Questions the Paper Calls Out
None

## Limitations
- Swapped-reference framework creates artificial conflicts that may not represent subtle real-world reference-answer mismatches
- Focus on entity-level conflicts may miss other knowledge-reference mismatch types (temporal, causal, conceptual)
- Experiments primarily use English-language datasets, limiting conclusions about multilingual evaluation contexts

## Confidence

**High confidence**: Empirical demonstration of score degradation under reference-belief conflicts is well-supported across thirteen models and four datasets with robust statistical significance.

**Medium confidence**: Claim that parametric knowledge "overrides" reference adherence is supported but may oversimplify the decision-making process, as judges may weigh multiple factors beyond just knowledge and reference.

**Medium confidence**: Assertion that prompt-based mitigations are ineffective is based on tested strategies but doesn't exhaustively explore the prompt engineering space.

## Next Checks

1. Test the reference-conflict vulnerability across additional knowledge domains (scientific, medical, technical) where parametric knowledge may be more deeply entrenched and reference conflicts more nuanced.

2. Evaluate whether chain-of-thought reasoning or step-by-step verification protocols can improve reference adherence compared to direct scoring approaches.

3. Investigate whether reference-confidence calibration (providing judges with explicit confidence scores for references) mitigates the knowledge-override effect.