---
ver: rpa2
title: Proactive Assistant Dialogue Generation from Streaming Egocentric Videos
arxiv_id: '2506.05904'
source_url: https://arxiv.org/abs/2506.05904
tags:
- assistant
- task
- video
- user
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of generating real-time, proactive
  task guidance from streaming egocentric videos, addressing challenges of high-cost
  data collection and lack of evaluation frameworks. It introduces PROASSIST, a large-scale
  synthetic dialogue dataset created by transforming annotated egocentric videos into
  task-oriented dialogues using LLM-based generation.
---

# Proactive Assistant Dialogue Generation from Streaming Egocentric Videos

## Quick Facts
- arXiv ID: 2506.05904
- Source URL: https://arxiv.org/abs/2506.05904
- Reference count: 40
- This paper introduces PROASSIST, a large-scale synthetic dialogue dataset for real-time task guidance from streaming egocentric videos, and develops an end-to-end model with negative frame sub-sampling and iterative progress summarization.

## Executive Summary
This paper addresses the challenge of generating real-time, proactive task guidance from streaming egocentric videos. The authors create PROASSIST, a large-scale synthetic dialogue dataset by transforming annotated egocentric videos into task-oriented dialogues using LLM-based generation. They develop an end-to-end model based on VideoLLM-Online with two key innovations: negative frame sub-sampling to handle class imbalance in speaking decisions, and iterative progress summarization to manage long video sequences. Experiments show that task-specific knowledge and memory mechanisms significantly improve performance, while improved visual perception alone provides limited gains. Human evaluations confirm the quality of synthetic dialogues, and proposed evaluation metrics align well with human judgment.

## Method Summary
The method involves creating PROASSIST by transforming annotated egocentric videos into interactive dialogues using an LLM pipeline. Videos are chunked into 2-second segments, descriptions are generated for each segment, and dialogues are synthesized across three user behavior types. The end-to-end model uses VideoLLM-Online with SigLIP-SO400M for frame encoding, LLaMA-3.1-8B-Instruct with LoRA for dialogue generation, and a binary speaking decision head. Key innovations include negative frame sub-sampling (ρ=0.1) to address class imbalance and iterative progress summarization to handle long videos by compressing context into structured summaries when approaching token limits.

## Key Results
- PROASSIST contains 30,135 dialogues (479 hours) from 6 egocentric video datasets
- Negative frame sub-sampling improves F1 from 30.1 to 58.7 for action narration
- Iterative progress summarization achieves F1=32.9 vs baseline F1=25.7
- Task-specific knowledge integration provides larger performance gains than improved visual perception
- Human evaluations confirm synthetic dialogues are preferred over baseline human-collected data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic dialogue generation from annotated videos can produce training data that matches or exceeds human-collected dialogues in quality.
- Mechanism: The pipeline transforms timestamped action annotations into goal-recipe-dialogue triplets using an LLM. Video descriptions are chunked to fit context windows, dialogues are generated incrementally across three user behavior types, then refined for naturalness. Post-filtering removes ~25% of low-quality dialogues based on timing precision and step coverage scores.
- Core assumption: Egocentric video annotations contain sufficient signal to ground temporally-aligned assistant responses, and LLMs can simulate realistic proactive guidance patterns without real interaction data.
- Evidence anchors: [abstract] "PROASSIST, a large-scale synthetic dialogue dataset created by transforming annotated egocentric videos into interactive dialogues using large language models"; [Section 3.2] "This unified representation enables LLMs to effectively understand ongoing activities at each time step"
- Break condition: If video annotations are sparse, noisy, or lack temporal granularity, the generated dialogues will misalign with visual events.

### Mechanism 2
- Claim: Negative frame sub-sampling improves the model's ability to learn when to speak by rebalancing the speaking/silence decision distribution.
- Mechanism: During training, gradients are computed only for positive frames (speaking moments) and a uniformly sampled subset (ρ=0.1) of negative frames (silence). This prevents the classifier from defaulting to silence due to overwhelming negative samples. Dynamic resampling per epoch ensures coverage across all positions over time.
- Core assumption: The speaking decision problem is fundamentally a class imbalance problem, and uniformly sampling negatives preserves sufficient diversity for learning silence patterns.
- Evidence anchors: [abstract] "negative frame sub-sampling to handle class imbalance in speaking decisions"; [Section 5.2] "directly optimizing cross-entropy on the original distribution leads to a classifier biased toward silence"; [Table 6] F1 improved from 30.1 to 58.7 for action narration
- Break condition: If speaking moments have high variance in their features, aggressive sub-sampling may discard informative negatives that define the decision boundary.

### Mechanism 3
- Claim: Iterative progress summarization enables long-horizon task tracking by compressing context while preserving task-relevant information.
- Mechanism: When approaching context length limits, the model generates a structured progress summary (time elapsed, task goal, completed steps, discussed topics, current state). This summary replaces the full history in the next segment's system prompt. During training, videos are pre-chunked with summaries carried forward; at inference, summaries are generated on-the-fly.
- Core assumption: The task state can be compressed into a fixed-format summary without losing critical information for subsequent guidance decisions.
- Evidence anchors: [abstract] "iterative progress summarization to manage long video sequences"; [Section 5.3] "enabling continuous task tracking via dynamic memory compression"; [Table 7] IPS achieves F1=32.9 vs Drop-Middle baseline F1=25.7
- Break condition: If summaries omit critical context (e.g., prior mistakes, partially completed steps), downstream guidance may become inconsistent or contradictory.

## Foundational Learning

- Concept: **Streaming video-to-text generation with timing decisions**
  - Why needed here: Unlike offline video QA, this task requires the model to decide both *when* to speak (binary decision per frame) and *what* to say, creating a joint optimization problem with severe class imbalance (few speaking moments across thousands of frames).
  - Quick check question: Given a 60-second video at 2 FPS with 3 assistant utterances, what is the positive/negative sample ratio for speaking decisions?

- Concept: **Egocentric vision and first-person activity understanding**
  - Why needed here: The input modality is egocentric video with frequent hand-object interactions, rapid camera motion, and partial observability. Standard third-person video understanding approaches may not transfer.
  - Quick check question: How does egocentric video differ from third-person video in terms of what visual features are salient for task guidance?

- Concept: **Evaluation metrics for proactive dialogue timing**
  - Why needed here: Standard dialogue metrics (BLEU, ROUGE) ignore timing. The paper proposes bipartite matching combining semantic similarity and temporal cost, plus LLM-as-judge for holistic assessment.
  - Quick check question: Why does the temporal cost function penalize late predictions more than early predictions (L > R)?

## Architecture Onboarding

- Component map:
  - Video frames -> SigLIP-SO400M frame encoder -> Visual tokens (1/5/10 per frame)
  - Interleaved with text tokens (dialogue history + optional user input)
  - LLM (LLaMA-3.1-8B-Instruct with LoRA) processes sequence
  - Binary speaking decision head at each frame's last visual token position
  - Iterative progress summarizer triggers at context limits

- Critical path:
  1. Video frames → Frame encoder → Visual tokens (1/5/10 per frame)
  2. Interleaved with text tokens (dialogue history + optional user input)
  3. LLM processes sequence, predicts [EOS] or response at each decision point
  4. At inference, threshold θ on [EOS] probability determines speaking
  5. At context limit, generate summary, restart with compressed context

- Design tradeoffs:
  - **Tokens per frame (I=1/5/10)**: More tokens improve perception but increase compute and reduce max video length. Surprisingly, more tokens didn't help dialogue generation (Table 5), suggesting perception isn't the bottleneck.
  - **Speaking threshold θ**: Higher θ = more conservative (fewer false positives), lower θ = more chatty. Must be tuned per-domain (Table 11 shows θ varies 0.2-0.5 across subsets).
  - **Chunk size for training**: L=4096 tokens forced 86% of Ego4D samples to be truncated without IPS.

- Failure signatures:
  - **Silent mode**: Model defaults to [EOS] at all positions → usually means θ is too high or NFS ratio is insufficient
  - **Over-talkative**: Model speaks at every frame → θ too low or training didn't learn the silence pattern
  - **Context drift**: Late-stage guidance contradicts early instructions → IPS summaries omitting critical state
  - **Domain mismatch**: Poor performance on Assembly101/Lab tasks → insufficient training samples in those domains (Table 8)

- First 3 experiments:
  1. **Baseline reproduction**: Train VideoLLM-Online on PROASSIST without NFS or IPS, measure F1 and LLM-judge scores. This establishes the contribution of each component.
  2. **Ablation of NFS ratios**: Test ρ ∈ {0.01, 0.1, 0.2, 0.5} on validation set to find optimal balance for your target domain. The paper found ρ=0.1 optimal, but this may vary with dataset composition.
  3. **Threshold sweep**: For your trained model, sweep θ ∈ {0.1, 0.2, ..., 0.9} on validation data and plot precision-recall curves (Figure 4). Select θ at F1 maximum, then verify with human preference correlation.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can dialogue synthesis be made scalable by generating dialogues directly from raw egocentric videos, bypassing the need for expensive pre-existing video annotations?
  - Basis in paper: [explicit] "The dataset's reliance on pre-existing video annotations limits its scalability... Recent advances in multimodal LLMs open the possibility of generating dialogues directly from raw videos, which could make data synthesis more efficient and scalable."
  - Why unresolved: Current pipeline depends entirely on annotated videos; raw video generation would require multimodal understanding without timestamped action labels.
  - What evidence would resolve it: A new pipeline demonstrating dialogue generation from unannotated videos with quality metrics matching or exceeding the current annotation-dependent approach.

- **Open Question 2**: Can adaptive threshold selection strategies be developed for speaking decisions that do not require a validation set for hyperparameter tuning?
  - Basis in paper: [explicit] "In real-world scenarios, a support set for hyperparameter tuning may not always be available. We leave the development of a better θ selection strategy for future work."
  - Why unresolved: Current threshold selection relies on validation-set optimization; real-time systems need online or calibration-free approaches.
  - What evidence would resolve it: A threshold selection method achieving comparable F1 scores without validation-set access, validated across diverse tasks and domains.

- **Open Question 3**: How can evaluation metrics be extended to incorporate multimodal video content rather than relying solely on text-based comparison?
  - Basis in paper: [explicit] "Our text-only evaluation approach could be enhanced by incorporating multimodal metrics that consider video content, to establish more robust benchmarks for interactive assistant systems."
  - Why unresolved: Current metrics compare predicted and reference dialogues textually; they cannot verify whether guidance correctly describes visual actions.
  - What evidence would resolve it: New multimodal metrics validated against human judgment with higher correlation coefficients than current text-only F1 (0.32–0.35) and LLM-based (0.44–0.47) scores.

## Limitations

- The synthetic dialogue generation pipeline relies heavily on the quality and coverage of underlying video annotations, with ~25% post-filtering suggesting significant quality variance
- Negative frame sub-sampling assumes uniform distribution of informative negative examples, which may not hold in domains with highly variable speaking patterns
- Iterative progress summarization may omit nuanced context like user confusion patterns or environmental constraints that affect subsequent guidance decisions

## Confidence

**High Confidence** (Strong empirical support, multiple validation methods):
- The synthetic data generation pipeline produces dialogues that human evaluators prefer over baseline human-collected data
- Negative frame sub-sampling significantly improves speaking decision F1 scores (30.1 → 58.7)
- Iterative progress summarization enables handling of long videos while maintaining task coherence

**Medium Confidence** (Good results but with notable limitations):
- The pairwise matching and LLM-as-judge evaluation metrics correlate well with human preferences in controlled studies
- Task-specific knowledge integration provides larger performance gains than improved visual perception
- The model generalizes across the six source domains with consistent performance patterns

**Low Confidence** (Limited validation, potential confounds):
- The absolute performance numbers on unseen domains (Assembly101, Lab) may not reflect real-world deployment scenarios
- The speaking threshold tuning process (θ optimization per subset) may not scale to truly open-ended applications
- The temporal cost function's asymmetric penalty (L > R) assumes a specific error preference that may not hold across all task types

## Next Checks

1. **Temporal Alignment Stress Test**: Generate dialogues for videos with rapid action sequences and long pauses. Measure the model's ability to maintain temporal coherence across segments separated by IPS summaries. Specifically, verify that summaries preserve critical transition points (e.g., "step completed but user struggling") that affect subsequent guidance.

2. **Domain Transfer Evaluation**: Train the full model on a subset of PROASSIST (e.g., only cooking and assembly tasks), then evaluate on held-out domains (HoloAssist surgical tasks, WTaG assembly). Compare performance degradation against training on full dataset to quantify domain-specific vs. general capability.

3. **Speaking Decision Robustness**: Create synthetic test videos with varying speaking pattern densities (0.1%, 1%, 5%, 10% speaking frames). Evaluate the model's precision-recall trade-off across these densities to verify that NFS generalizes beyond the PROASSIST distribution.