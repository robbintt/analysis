---
ver: rpa2
title: 'Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large
  Language Models'
arxiv_id: '2507.11882'
source_url: https://arxiv.org/abs/2507.11882
tags:
- languages
- llms
- data
- instruction
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Marco-Bench-MIF, a multilingual extension
  of the IFEval instruction-following benchmark covering 30 languages. The authors
  address the limitations of existing multilingual datasets that rely on simple machine
  translation, which often fails to capture linguistic and cultural nuances.
---

# Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models

## Quick Facts
- arXiv ID: 2507.11882
- Source URL: https://arxiv.org/abs/2507.11882
- Reference count: 5
- This paper introduces Marco-Bench-MIF, a multilingual extension of the IFEval instruction-following benchmark covering 30 languages.

## Executive Summary
Marco-Bench-MIF addresses the limitations of existing multilingual instruction-following datasets by introducing a hybrid pipeline that combines machine translation with human verification and cultural localization. The benchmark evaluates 20+ large language models across 30 languages, revealing significant performance gaps between high- and low-resource languages, as well as between instruction-level and prompt-level accuracy. The study finds that proprietary models significantly outperform open models, highlighting persistent challenges in multilingual instruction following, particularly for low-resource languages and culturally specific tasks.

## Method Summary
The Marco-Bench-MIF methodology involves adapting 541 English IFEval prompts into 30 languages through a three-step localization process: lexical substitution, topical transposition, and pragmatic restructuring, followed by two rounds of human verification. The evaluation uses both strict and loose metrics to assess prompt-level (all instructions followed) and instruction-level (individual instruction success) accuracy. The benchmark accounts for language-specific constraints such as script direction and cultural context, providing a more nuanced evaluation than simple machine translation approaches.

## Key Results
- Instruction-level accuracy consistently surpasses prompt-level accuracy by 10-20%, indicating compositional reasoning challenges
- Model scale strongly correlates with performance, with 70B+ models achieving 45-60% higher accuracy than 8B counterparts
- Machine-translated data underestimates accuracy by 7-22% compared to localized data, particularly for low-resource languages
- A 25-35% accuracy gap exists between high- and low-resource languages, with right-to-left scripts showing particular sensitivity

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Localization Pipeline
- **Claim:** If evaluation benchmarks utilize a hybrid pipeline (machine translation followed by human verification and localization), they may reveal higher model accuracy in low-resource languages compared to raw machine-translated data.
- **Mechanism:** The pipeline adapts linguistic constraints and cultural references, reducing false negatives where models fail translated constraints that are linguistically inapplicable in the target language.
- **Core assumption:** Human verification effectively identifies and corrects translation artifacts that would otherwise penalize model performance incorrectly.
- **Evidence anchors:** [abstract] "machine-translated data underestimates accuracy by 7-22% versus localized data."

### Mechanism 2: Scale-Dependent Compositional Reasoning
- **Claim:** Increased model scale appears to improve the capability to handle multiple constraints simultaneously, reducing the gap between single-instruction and multi-instruction performance.
- **Mechanism:** Larger models possess greater context window utilization or parameter capacity to hold multiple constraint representations active during generation.
- **Core assumption:** The performance gap is due to capacity/reasoning limits rather than just token probability errors.
- **Evidence anchors:** [abstract] "model scale strongly correlates with performance (70B+ models achieving 45-60% higher accuracy than 8B counterparts)."

### Mechanism 3: Resource-Dependent Script Interference
- **Claim:** Performance on instruction-following tasks is conditionally bounded by the resource-level of the language and the script type, independent of the model's general reasoning capability.
- **Mechanism:** High-resource languages benefit from robust tokenization and pre-training representations, while low-resource or complex script languages suffer from script-specific challenges.
- **Core assumption:** The performance drop is intrinsic to the model's language representation learning, not just the evaluation metric's strictness.
- **Evidence anchors:** [abstract] "25-35% accuracy gap between high- and low-resource languages."

## Foundational Learning

- **Concept: Instruction-Level vs. Prompt-Level Accuracy**
  - **Why needed here:** The paper distinguishes between a model's ability to follow a single instruction versus following all instructions in a complex prompt, critical for diagnosing whether failures are due to language understanding or managing multiple constraints.
  - **Quick check question:** If a model scores 80% on Instruction-level accuracy but only 60% on Prompt-level accuracy, which capability is likely the bottleneck: language understanding or compositional reasoning?

- **Concept: Localization vs. Translation**
  - **Why needed here:** The core contribution relies on the difference between converting text and adapting it to cultural/contextual norms, essential for understanding the value of the hybrid pipeline over standard MT.
  - **Quick check question:** Why would the instruction "Change all letters to uppercase" fail for Chinese if simply translated rather than localized?

- **Concept: Constraint Categorization**
  - **Why needed here:** The paper filters and adapts data based on constraint types (Content vs. Expressive, Single vs. Multi), necessary to interpret ablation studies and error analysis.
  - **Quick check question:** Does a "Content Constraint" dictate how the response looks (format) or what the response contains (keywords)?

## Architecture Onboarding

- **Component map:** Raw IFEval English data (541 pairs) -> Categorizer (Cardinality/Type) & Filter -> Localization Engine (Google Translate + LLM-based localization + 2 rounds human verification) -> Rule-based verifier (adapted for 30 languages)
- **Critical path:** The Localization Engine is the bottleneck. If human verification doesn't correctly identify culture-specific terms or linguistic constraints, evaluation data contains noise that invalidates the Localized vs. MT comparison.
- **Design tradeoffs:** Strict vs. Loose Metrics (exact adherence vs. semantic compliance), Automated vs. Manual Localization (hybrid approach balances cost and cultural nuance)
- **Failure signatures:** "Translation Artifact" False Negative (model correctly follows semantically nonsensical translated instruction), Script Drift (mixed scripts in responses causing strict evaluation failures)
- **First 3 experiments:**
  1. Sanity Check (MT vs. Localized): Select one low-resource language (e.g., Yoruba), run Llama-3-8B on both raw MT and Localized versions, verify localized accuracy is higher.
  2. Ablation on Constraint Type: Filter for Content Constraints vs. Expressive Constraints in non-Latin script (e.g., Chinese), determine which degrades faster.
  3. Scale Sensitivity Test: Compare Prompt-level vs. Instruction-level accuracy delta for 7B vs. 70B models, confirm compositional gap narrows with scale.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM instruction-following performance differ in languages with unique typological features (e.g., Ethiopic, Cherokee) or distinct dialectal variations (e.g., Arabic dialects) compared to the 30 languages currently benchmarked?
- Basis in paper: [explicit] The "Limitations" section explicitly notes that the dataset "underrepresents languages with non-Latin scripts... and dialectal variations," limiting the investigation of these specific linguistic contexts.
- Why unresolved: The current dataset covers 30 languages but excludes these complex subsets, leaving a gap in understanding how script complexity or dialectal nuance impacts instruction adherence.
- What evidence would resolve it: Evaluation results from an extended benchmark incorporating these specific underrepresented languages and dialects.

### Open Question 2
- Question: To what extent does surface-level cultural localization (e.g., changing names, dates) fail to capture deeper instruction-following errors related to cultural pragmatics, such as politeness strategies?
- Basis in paper: [explicit] The "Limitations" section states the localization focuses on "surface-level adaptations" and may miss "deeper instruction-following failures" like specific politeness strategies.
- Why unresolved: The current metric evaluates adherence to constraints but lacks a mechanism to score nuanced cultural appropriateness or pragmatic correctness.
- What evidence would resolve it: A comparative study evaluating model outputs using a rubric specifically designed for deep cultural pragmatics versus the current constraint-based accuracy.

### Open Question 3
- Question: Do LLMs demonstrate improved instruction-following capabilities in dynamic, interactive settings where users can refine prompts, compared to the static, single-turn evaluation used in Marco-Bench-MIF?
- Basis in paper: [explicit] The authors state in "Limitations" that the evaluation focuses on "static prompts rather than interactive instruction refinement," limiting insights into error recovery.
- Why unresolved: The current benchmark provides only a single-shot accuracy score, whereas real-world utility often depends on the model's ability to correct itself through multi-turn dialogue.
- What evidence would resolve it: Results from a multi-turn dialogue evaluation framework tracking success rates after user feedback, compared against the baseline static accuracy.

## Limitations
- The hybrid localization pipeline depends heavily on human annotators' cultural and linguistic expertise, which may introduce subjective biases
- The study focuses on closed-domain instruction-following tasks, limiting generalizability to open-ended or domain-specific reasoning
- The evaluation methodology assumes perfect human judgment in the localization process, which may not hold in practice

## Confidence

- **High Confidence:** The scale-dependent performance correlation (70B+ vs. 8B models) and the 25-35% accuracy gap between high- and low-resource languages are well-supported by the data.
- **Medium Confidence:** The 7-22% underestimation of accuracy in machine-translated data versus localized data is plausible but hinges on the assumption that human localization is error-free.
- **Low Confidence:** The claim that script-specific challenges are the primary driver of low-resource language underperformance is speculative without ablation studies isolating script effects from data scarcity.

## Next Checks

1. **Localization Quality Audit:** Re-run a subset of low-resource language evaluations using both raw MT and localized data with multiple annotator teams to quantify inter-annotator agreement and validate the 7-22% accuracy gap.

2. **Script Interference Isolation:** Design a controlled experiment using a low-resource language with a Latin script (e.g., Swahili) versus a high-resource non-Latin script (e.g., Chinese) to disentangle script effects from resource availability.

3. **Compositional Reasoning Ablation:** Test a small model fine-tuned on high-quality compositional data against a large model without such fine-tuning to determine whether the scale-dependent compositional gap is due to capacity or data quality.