---
ver: rpa2
title: 'MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous
  Sources'
arxiv_id: '2601.22054'
source_url: https://arxiv.org/abs/2601.22054
tags:
- depth
- metric
- estimation
- vision
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MetricAnything addresses the challenge of metric depth estimation
  scaling by developing a pretraining framework that learns from noisy, heterogeneous
  3D data sources without requiring manually engineered prompts, camera-specific modeling,
  or task-specific architectures. The core innovation is the Sparse Metric Prompt,
  created by randomly masking depth maps, which serves as a universal interface that
  decouples spatial reasoning from sensor and camera biases.
---

# MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources

## Quick Facts
- **arXiv ID:** 2601.22054
- **Source URL:** https://arxiv.org/abs/2601.22054
- **Reference count:** 40
- **One-line primary result:** Demonstrates clear scaling trends in metric depth estimation by training on 20M noisy, heterogeneous 3D data pairs using Sparse Metric Prompts

## Executive Summary
MetricAnything introduces a scalable pretraining framework for metric depth estimation that learns from noisy, heterogeneous 3D data sources without requiring manual prompts, camera-specific modeling, or task-specific architectures. The core innovation is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using approximately 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10,000+ camera models, the pretrained model achieves state-of-the-art results across multiple downstream tasks including monocular depth estimation, camera intrinsics recovery, and 3D reconstruction.

## Method Summary
The framework uses a shared ViT encoder with a conditioned DPT decoder to process RGB images paired with Sparse Metric Prompts. The prompts are created by randomly sampling valid pixels from depth maps (1-2% density) after applying PDSA/GMDR alignment. Training employs a Robust MAE loss that drops the top 20% of high-error pixels to handle heterogeneous noise. The pretrained teacher model generates pseudo-labels for distilling into a prompt-free student model using Deep-to-Deep skip connections and distance-balanced inverse-depth loss. The approach achieves clear scaling trends in metric depth estimation while maintaining generalization across diverse camera models and data sources.

## Key Results
- State-of-the-art results: AbsRel 0.085 on Sun-RGBD, 0.147 on nuScenes, 0.200 on Middlebury (prompt-free student)
- 2.34 AbsRel on KITTI depth super-resolution (pretrained model)
- Clear scaling trends demonstrated with 20M training pairs for the first time in metric depth estimation
- Significant enhancement of Multimodal Large Language Model spatial reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
Randomly masking depth maps (Sparse Metric Prompts) acts as a universal interface that isolates geometric reasoning from sensor-specific noise patterns. By sampling a sparse set of pixels from a noisy depth map, the model learns to propagate these sparse metric constraints to dense predictions, forcing it to rely on learned structural priors rather than overfitting to specific noise distributions of any single sensor type. The noise in heterogeneous 3D data sources is spatially inconsistent enough that random sparse sampling prevents the model from learning sensor-specific artifacts as valid structure.

### Mechanism 2
Scaling trends in metric depth emerge when training on massive, noisy data is paired with a noise-robust objective function. The framework uses a "Robust MAE" loss that discards the top 20% of pixels with the highest errors during training, preventing gradients from being dominated by severe misalignments or artifacts inherent in heterogeneous data. The noise in the training data is largely uncorrelated or localized, such that removing the highest-loss regions retains valid supervision for the majority of the scene.

### Mechanism 3
Transferring metric reasoning to a prompt-free student model requires architectural changes (Deep-to-Deep connections) to utilize high-fidelity pseudo-labels effectively. The student model inverts standard U-Net skip connections, feeding deep ViT semantic features to deeper decoder layers. This is necessary because teacher-generated pseudo-labels are cleaner and wider-range than raw data, and standard shallow-to-deep connections might propagate low-level texture noise that conflicts with the teacher's refined structural understanding.

## Foundational Learning

- **Concept: Metric vs. Relative Depth**
  - **Why needed here:** MetricAnything explicitly distinguishes itself from models like Depth Anything V2 by predicting absolute scale (meters) rather than relative ordering. Understanding that "metric" requires resolving the scale factor is central to the paper's contribution.
  - **Quick check question:** Does the model predict "object A is closer than B" (relative) or "object A is 1.5m away" (metric)?

- **Concept: Vision Transformer (ViT) and Dense Prediction Transformer (DPT)**
  - **Why needed here:** The architecture uses a ViT backbone and a DPT head. You must understand how patch embeddings work and how a DPT head reassembles these patches into a dense, per-pixel depth map using multi-scale fusion.
  - **Quick check question:** How does a DPT head convert a sequence of image patches from a ViT into a high-resolution dense depth map?

- **Concept: Teacher-Student Distillation**
  - **Why needed here:** The framework uses a pretrained "teacher" (with prompts) to generate training data for a "student" (without prompts). You need to understand why this transfer is necessary (removing the prompt dependency) and how pseudo-labels function as supervision.
  - **Quick check question:** Why train a separate student model instead of just running the teacher model without a prompt?

## Architecture Onboarding

- **Component map:** RGB Image + Sparse Metric Prompt (Depth Map with validity mask) -> Prompt Prep (PDSA/GMDR alignment + concatenation) -> Shared ViT Encoder -> Conditioned DPT Decoder -> Dense Metric Depth Map -> Student Distillation (Deep-to-Deep connections)

- **Critical path:** Teacher Pretraining (20M pairs with Sparse Metric Prompts + Robust MAE loss) -> Pseudo-label Generation (run teacher on dataset) -> Student Training (prompt-free with Deep-to-Deep skips + distance-balanced loss)

- **Design tradeoffs:** Uses random sparse prompts instead of simulated LiDAR scans, sacrificing sensor-specific accuracy for massive scalability and generalization. Deep-to-Deep skip connections discard low-level texture details in favor of high-level semantics, which works only if pseudo-labels are high-quality.

- **Failure signatures:** Student Blur (if output is geometrically smooth but lacks fine detail, Deep-to-Deep may be too aggressive), Metric Drift (if fails to generalize scale, Robust MAE may drop too much valid data)

- **First 3 experiments:** 1) Ablation on Prompt Sparsity (varying N=500 vs N=40,000), 2) Zero-Shot Transfer Test (held-out dataset like ETH3D), 3) Student vs. Teacher Comparison (quantify performance gap)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does increasing model capacity yield significant performance gains for metric depth estimation, or is the task primarily data-constrained?
- **Basis in paper:** Section 8 states that while data-centric scaling shows gains, "the scalability of the model architecture itself remains unexplored."
- **Why unresolved:** The current study focuses on aggregating a 20M dataset using a fixed ViT backbone, leaving the interaction between model size and metric depth fidelity unknown.
- **What evidence would resolve it:** Benchmarks showing performance curves (e.g., AbsRel) when varying the ViT encoder size (e.g., ViT-S to ViT-G) while holding the training dataset fixed.

### Open Question 2
- **Question:** Can the Sparse Metric Prompt framework be generalized to non-central or non-pinhole camera models without architectural modifications?
- **Basis in paper:** Section 8 notes the framework "maintains the central projection camera assumption and has not been extended to specialized camera models."
- **Why unresolved:** Fisheye or panoramic sensors introduce radial distortions that violate the planar projection assumptions inherent in standard metric depth maps.
- **What evidence would resolve it:** Successful zero-shot quantitative evaluation on benchmark datasets containing fisheye or spherical imagery.

### Open Question 3
- **Question:** Can the prompt preparation step be learned end-to-end to remove the dependency on hand-crafted alignment and external priors?
- **Basis in paper:** Section 3.2 details a "parameter-free (non-learning)" prompt preparation step using PDSA and GMDR that relies on an external pre-trained model (DepthPro).
- **Why unresolved:** This external dependency introduces potential biases from the prior model and adds a discrete processing step that prevents the main network from learning to correct raw prompt irregularities directly.
- **What evidence would resolve it:** An ablation study comparing the current pipeline against a fully learnable prompt encoder.

## Limitations
- The approach assumes that randomly sampled sparse prompts can effectively decouple geometric reasoning from sensor-specific noise patterns, which may not hold for correlated noise
- The effectiveness of Deep-to-Deep skip connections in the student model is validated internally but lacks external benchmarking
- The composition and quality of the 20M training pairs remain underspecified, impacting reproducibility

## Confidence
- **High Confidence:** The use of Sparse Metric Prompts and Robust MAE loss to handle heterogeneous data is well-supported by experimental results
- **Medium Confidence:** The scaling trends demonstrated are convincing, but the exact contribution of each architectural choice to these trends is not fully isolated
- **Low Confidence:** The claim that the student model achieves state-of-the-art results without prompts is based on distillation from a prompt-dependent teacher

## Next Checks
1. **Prompt Sparsity Ablation:** Train the model with varying sparse prompt sampling rates (e.g., N=500 vs N=40,000) on a subset of the data to empirically verify the robustness of the Sparse Metric Prompt mechanism
2. **Extreme Zero-Shot Transfer:** Evaluate the pretrained teacher on a held-out dataset (e.g., ETH3D) using extremely sparse prompts (e.g., 100 points) to test the limits of the "plug-and-play" capability without fine-tuning
3. **Student vs. Teacher Generalization Gap:** Compare the Student-DepthMap (prompt-free) against the Teacher (prompt-required) on a diverse set of downstream tasks to quantify the performance trade-off introduced by distillation