---
ver: rpa2
title: 'Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to
  Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal
  Distillation'
arxiv_id: '2505.06803'
source_url: https://arxiv.org/abs/2505.06803
tags:
- llms
- audio
- visual
- qwen2-audio
- classes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the sensory capabilities of audio and visual
  large language models (LLMs) by comparing their performance to humans in sound recognition
  tasks. The authors find that audio and visual LLMs exhibit modality-specific strengths
  and weaknesses that closely mirror human perception, with visual models generally
  outperforming audio models, especially in visually distinctive categories.
---

# Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation

## Quick Facts
- arXiv ID: 2505.06803
- Source URL: https://arxiv.org/abs/2505.06803
- Reference count: 40
- Primary result: Audio and visual LLMs show modality-specific strengths that mirror human perception, with cross-modal distillation reducing the sensory gap

## Executive Summary
This paper investigates the sensory capabilities of audio and visual large language models by comparing their performance to humans in sound recognition tasks. The authors find that audio and visual LLMs exhibit modality-specific strengths and weaknesses that closely mirror human perception, with visual models generally outperforming audio models, especially in visually distinctive categories. To address this gap, they propose a cross-modal distillation framework where knowledge is selectively transferred from the stronger modality to the weaker one, guided by a heuristic switch that predicts when such transfer would be beneficial. The framework significantly improves audio model performance, with the distilled model surpassing its visual teacher and approaching the performance of an audio-visual model, while also generalizing well to unseen classes.

## Method Summary
The authors compare Qwen2-Audio and Qwen2-VL performance on VGGSound to human perception, identifying modality-specific strengths. They then develop a cross-modal distillation framework that selectively transfers knowledge from the stronger to weaker modality using a heuristic switch. The switch, trained on a PANN audio classifier, predicts when visual predictions should be used to train the audio model. When the switch indicates distillation is beneficial, the audio model learns from visual predictions; otherwise, it trains on its own predictions with anti-forgetting loss. The approach is validated on VGGSound analysis/test sets and AudioSet evaluation set.

## Key Results
- Qwen2-Audio achieved 71.9% accuracy vs Qwen2-VL's 88.5% on VGGSound analysis set, mirroring human sensory gaps
- Cross-modal distillation improved Qwen2-Audio to 92.6% on analysis set, surpassing Qwen2-VL and approaching audio-visual model performance
- Distilled model generalized to AudioSet, improving from 60.0% to 72.3% accuracy on unseen classes

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Sensory Alignment Between LLMs and Humans
The performance gap between audio and visual LLMs parallels human sensory discrepancies, suggesting LLMs inherit modality-specific biases similar to biological perception systems. Visual models excel at spatially distinctive categories (sports, identifiable objects) while audio models excel at temporal/human-related sounds (coughing, sneezing, whistling) where visual cues are limited—matching human ears vs. eyes specialization.

### Mechanism 2: Heuristic-Gated Selective Knowledge Transfer
A trained binary classifier can predict when cross-modal teacher-to-student transfer is beneficial, preventing harmful knowledge transfer when teacher is unreliable. The PANN-based heuristic switch predicts whether visual > audio for a given sample; when switch is "closed" (teacher stronger), distillation occurs; when "open," student trains on own predictions to prevent catastrophic forgetting.

### Mechanism 3: Cross-Modal Representation Grounding via Same-Scene Supervision
Training an audio LLM on visual LLM predictions from the same acoustic scene (silent video vs. audio) grounds audio representations in visually-accessible knowledge. Both modalities observe identical temporal events; visual model's confident predictions provide semantic labels that audio model learns to associate with acoustic features it previously couldn't disambiguate.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student)**: The framework depends on understanding how softer probability distributions from teacher guide student learning without ground truth. Quick check: Can you explain why KL divergence with temperature > 1 might preserve more information than cross-entropy on hard labels?

- **Multimodal LLM Architecture (Encoder + LLM Backbone)**: Cross-modal distillation requires understanding that Qwen2-Audio and Qwen2-VL share the same language backbone (Qwen2), differing only in modality encoders. Quick check: Why does sharing the same LLM backbone matter for isolating modality-specific effects in this study?

- **LoRA (Low-Rank Adaptation)**: Practical implementation uses LoRA for efficient finetuning; understanding rank constraints helps debug capacity issues. Quick check: If distilled model underperforms, would increasing LoRA rank from 16 be a reasonable first diagnostic?

## Architecture Onboarding

- **Component map**: PANN Cnn14 → Heuristic switch → Binary prediction (distill/not) → Cross-entropy loss (teacher/student) + anti-forgetting loss

- **Critical path**: 1) Pre-compute teacher predictions on full training set (149K samples) 2) Train heuristic switch on analysis set (12.8K samples) 3) For each batch: query switch → apply distillation or anti-forgetting loss 4) Single epoch finetuning with cosine LR scheduling

- **Design tradeoffs**: Teacher labels vs. logits (ablation shows labels better: 89.4 vs 88.7 test accuracy); Audio-only switch for simplicity vs. audio-visual for accuracy; Class-level vs. sample-level switch labeling

- **Failure signatures**: Catastrophic forgetting on audio-strong classes (switch routing incorrectly); Student surpasses teacher but underperforms audio-visual baseline (expected); No improvement on unseen classes (overfitting to VGGSound distribution)

- **First 3 experiments**: 1) Baseline verification: Replicate analysis set evaluation, confirm Qwen2-Audio (71.9%), Qwen2-VL (88.5%), Qwen2.5-Omni (92.6%) accuracies 2) Switch-only ablation: Train heuristic switch, validate accuracy (~89% target) 3) Small-scale distillation: Run full pipeline on 10% training data, verify 10-15% accuracy gain

## Open Questions the Paper Calls Out

- Would an audio-visual heuristic switch improve the selectivity and effectiveness of cross-modal distillation compared to the audio-only switch demonstrated in this paper? The authors state this as a future direction for improved decisions.

- Does the cross-modal distillation framework transfer to more complex acoustic scene understanding tasks (e.g., temporal reasoning, causal inference, spatial localization) beyond the sound recognition task examined here? The paper suggests expanding to complex acoustic scene tasks.

- Can the alignment between LLM and human sensory gaps observed in Qwen models be replicated across different model families and a broader set of sound classes? The study's generalizability to other model families remains untested.

- Is the heuristic switch trained on one teacher-student pair transferable to other model pairs without retraining? The paper doesn't test whether the switch generalizes to different model combinations.

## Limitations

- Dataset artifact risk: The modality alignment could be driven by VGGSound's visual bias rather than fundamental modality differences, as alternative datasets weren't tested.
- Error propagation: Systematic visual model biases may be inherited by the distilled audio model, but this wasn't analyzed.
- Generalization scope: While showing improvement on AudioSet, the paper doesn't analyze performance on acoustically similar but semantically distinct classes.

## Confidence

- **High confidence**: Modality-specific performance patterns in LLMs closely mirror human sensory discrepancies (supported by direct comparison statistics across 10 classes)
- **Medium confidence**: Cross-modal distillation significantly improves audio model performance (strong empirical results but limited ablation studies on switch design choices)
- **Medium confidence**: Heuristic-gated selective transfer prevents catastrophic forgetting (supported by anti-forgetting loss results but no analysis of failure modes)

## Next Checks

1. **Dataset bias validation**: Replicate the analysis comparing Qwen2-Audio, Qwen2-VL, and human performance on an alternative sound recognition dataset (e.g., AudioSet or FSD50K) to determine whether modality alignment is dataset-specific or reflects fundamental model behavior.

2. **Error propagation analysis**: For each class where Qwen2-VL > Qwen2-Audio in the analysis set, compute the error overlap between teacher predictions and distilled student predictions on the test set to quantify how much the student inherits the teacher's systematic biases.

3. **Switch generalization stress test**: Train the heuristic switch on 80% of the analysis set and test on the remaining 20%, measuring accuracy drop. Then apply the distilled model to classes not in VGGSound but present in AudioSet, analyzing whether the switch's class-level routing remains effective for semantically similar but novel categories.