---
ver: rpa2
title: 'Unlearning''s Blind Spots: Over-Unlearning and Prototypical Relearning Attack'
arxiv_id: '2506.01318'
source_url: https://arxiv.org/abs/2506.01318
tags:
- unlearning
- spotter
- forget
- relearning
- over-unlearning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies two critical blind spots in machine unlearning\
  \ (MU): over-unlearning, where performance degrades on data near forgotten classes,\
  \ and prototypical relearning attacks, where attackers recover forgotten knowledge\
  \ using few samples. The authors introduce OU@\u03B5, a metric measuring distributional\
  \ drift near the decision boundary of forgotten classes, and demonstrate a novel\
  \ prototypical relearning attack that exploits residual feature structure to restore\
  \ forgotten classes."
---

# Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack

## Quick Facts
- **arXiv ID**: 2506.01318
- **Source URL**: https://arxiv.org/abs/2506.01318
- **Reference count**: 40
- **Primary result**: Spotter reduces over-unlearning by up to 20x and maintains near-zero prototypical relearning accuracy while preserving retained-class performance.

## Executive Summary
This paper identifies two critical blind spots in machine unlearning: over-unlearning, where performance degrades on data near forgotten classes, and prototypical relearning attacks, where attackers recover forgotten knowledge using few samples. The authors introduce OU@ε, a metric measuring distributional drift near the decision boundary of forgotten classes, and demonstrate a novel prototypical relearning attack that exploits residual feature structure to restore forgotten classes. To address both issues, they propose Spotter, a plug-and-play framework combining masked knowledge distillation on boundary-proximal perturbations with an intra-class dispersion loss. Spotter achieves state-of-the-art results across CIFAR-10, CIFAR-100, TinyImageNet, and CASIA-WebFace.

## Method Summary
Spotter is a plug-and-play framework that mitigates over-unlearning and prototypical relearning attacks through three key components: (1) a masked knowledge distillation loss applied to forget-class samples and their perturbed versions near the decision boundary, (2) an intra-class dispersion loss that scatters forget-class embeddings to prevent prototype formation, and (3) the OU@ε metric for quantifying localized over-unlearning. The framework computes a final loss combining these elements with tunable hyperparameters λ₁ and λ₂, where λ₁ controls the balance between unlearning and over-unlearning suppression, and λ₂ controls the strength of the dispersion loss.

## Key Results
- Spotter reduces over-unlearning (OU@ε) by up to 20x compared to baseline unlearning methods
- Achieves near-zero prototypical relearning accuracy (Proto-Accf) while maintaining high retained-class accuracy
- Outperforms state-of-the-art unlearning methods on CIFAR-10, CIFAR-100, TinyImageNet, and CASIA-WebFace datasets
- Validated across multiple architectures including ResNet-18 and ViT-S/DeiT-S transformers

## Why This Works (Mechanism)

### Mechanism 1: Masked Knowledge Distillation Suppresses Over-Unlearning
The knowledge distillation penalty on adversarially perturbed samples near the forget set boundary constrains the unlearned model to preserve the original decision boundaries for retained classes in the vicinity of the forget boundary. This prevents the unlearning process from "overshooting" and harming similar retained data.

### Mechanism 2: Intra-Class Feature Dispersion Neutralizes Prototypical Attacks
The dispersion loss directly minimizes cosine similarity between all pairs of samples from each forget class in the embedding space, scattering the forget-class embeddings and preventing the formation of a tight, representative cluster needed for prototypical attacks.

### Mechanism 3: OU@ε Metric Quantifies Localized Over-Unlearning
The metric uses a masked softmax to ignore forget class probabilities and focuses only on the distribution over retained classes, calculating divergence on perturbed samples to probe the boundary region where over-unlearning is hypothesized to concentrate.

## Foundational Learning

- **Knowledge Distillation Loss**: Understanding how KL divergence between teacher (original model) and student (unlearned model) output distributions works is crucial since Spotter's core method for preventing over-unlearning is a distillation loss applied to perturbed samples.
  - *Quick check*: If the original model predicts [0.9, 0.05, 0.05] and the unlearned model predicts [0.7, 0.2, 0.1], in which direction would the distillation loss push the unlearned model?

- **Prototypical Networks / Class Prototypes**: The attack being defended against is based on comparing query embeddings to class prototypes (mean embeddings), so understanding this concept is crucial to why scattering embeddings is effective.
  - *Quick check*: If you have 5 samples from a forget class, how do you compute the class prototype used in the attack?

- **Adversarial Perturbations (PGD)**: The Spotter method and its evaluation metric rely on generating perturbed samples near the decision boundary using Projected Gradient Descent (PGD), a form of adversarial example generation.
  - *Quick check*: What is the primary constraint applied during a PGD attack (e.g., on the norm of the perturbation δ)?

## Architecture Onboarding

- **Component map**: Perturbation Generator -> Masked Distillation Module -> Dispersion Loss Module -> Final Loss Computation
- **Critical path**: The Perturbation Generator and its budget ε is the most critical component, as it defines the "blind spot" region being protected. Misconfiguration here renders the over-unlearning defense ineffective.
- **Design tradeoffs**: ε (perturbation budget) - smaller values make defense more local but may miss wider effects; λ₂ (dispersion loss weight) - increasing improves attack resistance but can slightly degrade retained accuracy; PGD vs. Gaussian noise - PGD is more targeted but computationally expensive.
- **Failure signatures**: High OU@ε despite low Accf indicates masked distillation loss is failing (likely due to incorrect ε); high Proto-Accf after attack indicates dispersion loss is insufficient (increase λ₂); significant drop in Acc_r suggests over-unlearning mitigation is too aggressive.
- **First 3 experiments**:
  1. Baseline Reproduction & Metric Validation: Implement OU@ε metric using both PGD and Gaussian noise perturbations, reproduce baseline scores for validation
  2. Spotter Ablation on Single Baseline: Apply Spotter to UNSC on CIFAR-10, sweep hyperparameters (λ₁, λ₂, ε) to understand effects on Acc_f, OU@ε, and Proto-Acc_f
  3. Prototypical Relearning Attack Implementation: Implement attack as described, test success rate on unlearned models, experiment with interpolation factor α

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can Spotter be extended to instance-level and concept-level unlearning, beyond class-level settings?
- **Basis**: Paper focuses on class-level unlearning throughout, though unlearning can target instances, classes, or concepts
- **Why unresolved**: OU@ε metric and dispersion loss rely on class-level prototypes; instance-level lacks clear prototypes to target for dispersion
- **What evidence would resolve it**: Experiments applying adapted Spotter objectives to instance-level unlearning benchmarks showing similar mitigation and resistance

### Open Question 2
- **Question**: How effective is the Prototypical Relearning Attack under black-box access where attackers cannot directly modify classifier weights?
- **Basis**: PRA assumes white-box access to replace classifier head weights; paper notes "service operator with white-box access" as threat model
- **Why unresolved**: Black-box attackers cannot directly interpolate prototype weights, limiting PRA to insider threats or model weight leaks
- **What evidence would resolve it**: Evaluation of PRA variants using fine-tuning or prompt-based approaches under API-only access

### Open Question 3
- **Question**: Does the intra-class dispersion loss degrade embedding space quality for downstream transfer learning tasks?
- **Basis**: Paper acknowledges Lsim can influence overall embedding space but only evaluates retained-class classification accuracy
- **Why unresolved**: Scattering forget-class embeddings may create irregularities harming representation quality for tasks like few-shot learning
- **What evidence would resolve it**: Transfer learning experiments measuring downstream task performance using Spotter-unlearned feature extractors

## Limitations
- The framework's generalizability across diverse unlearning methods and architectures remains unverified beyond the tested combinations
- The perturbation budget ε is critical but its optimal range is not rigorously defined, with performance degrading at high values
- The claim that intra-class dispersion is the key to neutralizing prototypical attacks is primarily supported by ablation studies rather than deep embedding space analysis

## Confidence

**Major Uncertainties:**
Reproducibility of exact perturbation strategy and hyperparameter settings is uncertain due to unspecified training duration, optimizer settings, and batch sizes.

**Confidence Labels:**
- **High Confidence**: Identification of over-unlearning as blind spot and introduction of OU@ε metric are well-supported
- **Medium Confidence**: Effectiveness of masked knowledge distillation for suppressing over-unlearning is demonstrated but mechanism and ε role are not fully characterized
- **Low Confidence**: Claim that intra-class dispersion neutralizes prototypical attacks is primarily supported by ablation studies, deeper analysis needed

## Next Checks

1. **Hyperparameter Sensitivity Sweep**: Perform systematic ablation study on λ₁, λ₂, ε across multiple baseline unlearning methods to quantify robustness and generalizability
2. **Embedding Space Analysis**: Visualize forget-class embeddings with and without dispersion loss (e.g., using UMAP) to provide qualitative evidence for scattering effect
3. **Cross-Architecture Validation**: Apply Spotter to transformer-based model (e.g., ViT-S) and evaluate on TinyImageNet to test plug-and-play applicability beyond ResNet-18 on CIFAR datasets