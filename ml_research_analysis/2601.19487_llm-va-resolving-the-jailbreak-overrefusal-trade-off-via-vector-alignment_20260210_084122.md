---
ver: rpa2
title: 'LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment'
arxiv_id: '2601.19487'
source_url: https://arxiv.org/abs/2601.19487
tags:
- layer
- vector
- llm-v
- benign
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLM-VA resolves the jailbreak-overrefusal trade-off in safety-aligned
  LLMs by aligning the answer vector with the benign vector through closed-form weight
  updates. This approach makes the model's willingness to answer causally dependent
  on its safety assessment, rather than treating them as independent processes.
---

# LLM-VA: Resolving the Jailbreak-Overrefusal Trade-off via Vector Alignment

## Quick Facts
- arXiv ID: 2601.19487
- Source URL: https://arxiv.org/abs/2601.19487
- Authors: Haonan Zhang; Dongxia Wang; Yi Liu; Kexin Chen; Wenhai Wang
- Reference count: 40
- Primary result: 11.45% higher F1 scores than best baseline while preserving 95.92% utility

## Executive Summary
LLM-VA addresses the fundamental trade-off between jailbreak vulnerability and over-refusal in safety-aligned LLMs by aligning answer vectors (v_a) with benign vectors (v_b) through closed-form weight updates. The method makes the model's willingness to answer causally dependent on its safety assessment rather than treating them as independent processes. Experiments on 12 LLMs demonstrate that LLM-VA achieves significant improvements in the F1 score metric while maintaining high utility preservation.

## Method Summary
LLM-VA works by first extracting control vectors from LLM activations using linear SVMs trained on labeled data. The method identifies two key vectors: v_a (answer/refuse direction) and v_b (benign/toxic direction). It then iteratively updates the down-projection weights in selected layers using a closed-form formula that aligns these vectors, with the alignment strength proportional to the cosine similarity between them. The process is guided by a layer selection score that combines classification accuracy and influence, and continues for up to 30 iterations with validation-based checkpoint selection.

## Key Results
- 11.45% higher F1 scores than the best baseline on safety evaluation
- 95.92% utility preservation across 6 benchmarks
- Automatic adaptation to each model's safety bias without manual tuning
- Consistent performance across 12 different LLM architectures (3B-14B parameters)

## Why This Works (Mechanism)
LLM-VA resolves the jailbreak-overrefusal trade-off by establishing a causal relationship between safety assessment and answer willingness through vector alignment. By making the answer vector direction causally dependent on the benign vector, the model learns to refuse based on genuine safety concerns rather than arbitrary thresholds, reducing both over-refusal of benign requests and vulnerability to jailbreak attacks.

## Foundational Learning
**Vector Steering Methods**: Techniques that manipulate LLM behavior by identifying and modifying control vectors in activation space. *Why needed*: Provides the foundation for LLM-VA's approach to safety alignment. *Quick check*: Can you explain how steering vectors differ from traditional fine-tuning?

**Linear SVM for Vector Extraction**: Using linear support vector machines to extract hyperplane normals from activation data. *Why needed*: Enables identification of answer and benign control vectors from LLM activations. *Quick check*: What advantages do linear SVMs offer over neural classifiers for vector extraction?

**Cosine Similarity as Alignment Metric**: Measuring vector alignment using cosine similarity between answer and benign directions. *Why needed*: Quantifies the degree of causal dependency between safety assessment and answer willingness. *Quick check*: How does cosine similarity capture directional alignment differently than Euclidean distance?

**Closed-Form Weight Updates**: Applying direct matrix updates rather than gradient-based optimization. *Why needed*: Enables efficient, interpretable modifications to LLM behavior. *Quick check*: What computational advantages does closed-form updating provide over iterative optimization?

## Architecture Onboarding

**Component Map**: Data Collection -> SVM Vector Extraction -> Layer Scoring -> Iterative Weight Updates -> Validation Selection

**Critical Path**: The layer scoring and iterative weight update process is critical - selecting the right layers and applying appropriate alignment strength determines success. Early layers often provide the most influence but risk utility degradation.

**Design Tradeoffs**: Layer selection balances influence (early layers) against stability (later layers). More iterations can improve alignment but risk over-modification. The method trades some model-specific optimization for generalizability across architectures.

**Failure Signatures**: Validation F1 dropping after peak iteration indicates over-modification. Low utility preservation suggests too many layers were selected or alignment was too aggressive. Inconsistent vector extraction points to poor SVM training data quality.

**First Experiments**: 1) Extract v_a and v_b vectors from a small model to verify the SVM process works. 2) Apply single-layer alignment to observe directional changes in outputs. 3) Run full iterative alignment on a 7B model with monitoring of F1 and utility curves.

## Open Questions the Paper Calls Out
1. **Multi-class Toxicity Classification**: Can LLM-VA be extended to handle nuanced, multi-dimensional toxicity rather than binary benign/toxic labels? The current SVM-based approach assumes a single benign vector direction, but real-world toxicity may require multiple orthogonal or correlated vector directions.

2. **Scalability to Larger Models**: Does LLM-VA's effectiveness transfer to models with 70B+ parameters? Larger models may encode safety decisions in fundamentally different representational structures or distributed across more layers, potentially changing the orthogonality patterns between answer and benign vectors.

3. **Chain-of-Thought Reasoning Models**: Can vector alignment methods be adapted for models with chain-of-thought reasoning where control vectors must be identified after reasoning steps? Reasoning introduces stochastic intermediate states that may shift or fragment the answer/benign vector directions.

4. **Transferability to Unseen Domains**: How can the transferability of LLM-VA to unseen datasets and domains be improved? Vectors identified from training data may capture dataset-specific patterns rather than generalizable safety concepts, limiting generalization to new threat types.

## Limitations
- Architecture-specific implementation details for identifying down-projection matrices are underspecified
- The exact prompt format for obtaining answer/refuse labels during SVM training is not provided
- Limited evaluation to models between 3B-14B parameters, leaving scalability to larger models untested

## Confidence
**High Confidence**: Core methodology and experimental results are well-defined and clearly presented with appropriate statistical backing.
**Medium Confidence**: Theoretical framework is sound but practical implementation details for different architectures introduce uncertainty.
**Low Confidence**: Exact processes for label generation and parameter mapping across architectures are underspecified, potentially affecting reproducibility.

## Next Checks
1. Implement LLM-VA on at least two different LLM architectures (e.g., Llama and Gemma) to verify consistent performance and identify architecture-specific challenges.
2. Conduct an ablation study on layer selection by systematically varying L_select and testing different strategies to determine optimal settings for different model sizes.
3. Perform convergence analysis by tracking F1 scores across all 30 iterations for multiple models to identify optimal stopping points and characterize typical convergence behavior.