---
ver: rpa2
title: 'Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast
  Asia'
arxiv_id: '2509.09121'
source_url: https://arxiv.org/abs/2509.09121
tags:
- emcommerce
- multilin
- data
- southeast
- domainmspeci
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Compass-v3, a 245B-parameter Mixture-of-Experts
  model with 71B active per token, designed for Southeast Asian e-commerce. The model
  addresses challenges in noisy, heterogeneous, multilingual, and dynamic e-commerce
  data through a combination of architectural innovations (fewer but larger experts),
  training optimizations (intra-node expert parallelism, customized memcpy operators),
  and alignment enhancements (Optimal-Transport Direct Preference Optimization).
---

# Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia

## Quick Facts
- arXiv ID: 2509.09121
- Source URL: https://arxiv.org/abs/2509.09121
- Reference count: 2
- 245B-parameter MoE model with 71B active per token, achieving SOTA on Southeast Asian e-commerce tasks

## Executive Summary
Compass-v3 is a 245B-parameter Mixture-of-Experts model designed specifically for Southeast Asian e-commerce applications. The model combines architectural innovations, training optimizations, and alignment techniques to handle the unique challenges of noisy, heterogeneous, multilingual e-commerce data. Trained on 12T tokens of curated multilingual corpora and synthetic e-commerce instructions, Compass-v3 demonstrates state-of-the-art performance on domain-specific tasks while maintaining strong multilingual capabilities across low-resource Southeast Asian languages.

## Method Summary
The model employs a Mixture-of-Experts architecture with fewer but larger experts, achieving 71B active parameters per token from a total of 245B. Training optimizations include intra-node expert parallelism and customized memcpy operators for improved efficiency. The alignment strategy uses Optimal-Transport Direct Preference Optimization to enhance instruction-following capabilities. The training corpus combines 12T tokens of multilingual data with large-scale synthetic e-commerce instructions, enabling both general and domain-specific capabilities.

## Key Results
- Achieves state-of-the-art performance on e-commerce tasks, surpassing DeepSeek-V3.1, GPT-4 series, and Qwen3-235B
- Demonstrates strong multilingual capability across Southeast Asian languages (Indonesian, Thai, Filipino, Vietnamese, Malay, Tagalog) and Portuguese
- Accounts for over 70% of total LLM usage in Shopee's industrial-scale e-commerce platform

## Why This Works (Mechanism)
The model's success stems from its tailored approach to e-commerce data characteristics, combining architectural efficiency with targeted training and alignment strategies. The MoE configuration with larger experts reduces communication overhead while maintaining computational efficiency. The synthetic instruction generation specifically targets e-commerce scenarios, creating high-quality training data that generalizes well to real-world applications. The Optimal-Transport DPO alignment method effectively bridges the gap between general language understanding and domain-specific task performance.

## Foundational Learning
- **Mixture-of-Experts (MoE)**: Why needed - Reduces computational cost while maintaining model capacity; Quick check - Verify active parameter count matches specification (71B)
- **Optimal-Transport DPO**: Why needed - Improves alignment for domain-specific instructions; Quick check - Compare performance against standard DPO methods
- **Synthetic data generation**: Why needed - Scales instruction-following capabilities for specific domains; Quick check - Assess quality through human evaluation of generated instructions
- **Multilingual training**: Why needed - Supports low-resource Southeast Asian languages; Quick check - Test performance on held-out languages not in training data

## Architecture Onboarding
- **Component map**: Data curation -> Synthetic instruction generation -> MoE training with expert parallelism -> Optimal-Transport DPO alignment -> Industrial deployment
- **Critical path**: Model training depends on quality synthetic data generation, which requires effective data curation and instruction templates
- **Design tradeoffs**: Fewer experts (reduced communication) vs. more experts (potentially better specialization); larger synthetic dataset vs. quality control challenges
- **Failure signatures**: Performance degradation on truly low-resource languages, overfitting to synthetic data patterns, reduced efficiency in non-e-commerce tasks
- **3 first experiments**: 1) Ablation study removing expert parallelism to measure efficiency gains, 2) Comparison of Optimal-Transport DPO vs standard DPO on e-commerce tasks, 3) Zero-shot evaluation on truly unseen low-resource languages

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Comparison against GPT-4 series lacks specification of which variant was used, making claims difficult to verify
- Industrial deployment claims (70% LLM usage) lack independent validation or detailed effectiveness metrics
- Synthetic data quality control processes are not transparent, raising concerns about potential biases
- Multilingual evaluation focuses primarily on target region languages with limited assessment of true zero-shot generalization

## Confidence
- **High confidence**: Architectural innovations, technical training optimizations, Southeast Asian language performance
- **Medium confidence**: General benchmark performance relative to GPT-4 series, e-commerce task superiority over DeepSeek-V3.1
- **Low confidence**: Industrial deployment metrics and real-world impact claims, synthetic data quality control processes

## Next Checks
1. Conduct ablation studies removing specific architectural innovations to quantify individual performance contributions
2. Perform independent verification of industrial deployment claims through third-party audits of usage patterns and user satisfaction
3. Test zero-shot generalization on truly low-resource languages outside Southeast Asia and Portuguese to assess genuine multilingual capabilities