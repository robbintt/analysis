---
ver: rpa2
title: 'HVAdam: A Full-Dimension Adaptive Optimizer'
arxiv_id: '2511.20277'
source_url: https://arxiv.org/abs/2511.20277
tags:
- adam
- learning
- optimizers
- rate
- epoch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HVAdam is a novel adaptive optimizer that addresses the "valley
  dilemma" in deep learning by incorporating a hidden vector that captures the stable
  gradient trend of the loss function. The method uses gradient projection to compute
  this hidden vector and employs a restart strategy to handle changes in optimization
  direction.
---

# HVAdam: A Full-Dimension Adaptive Optimizer

## Quick Facts
- arXiv ID: 2511.20277
- Source URL: https://arxiv.org/abs/2511.20277
- Reference count: 40
- Key outcome: HVAdam is a novel adaptive optimizer that addresses the "valley dilemma" in deep learning by incorporating a hidden vector that captures the stable gradient trend of the loss function.

## Executive Summary
HVAdam introduces a novel adaptive optimization framework that addresses the "valley dilemma" in deep learning by incorporating a hidden vector that captures the stable gradient trend of the loss function. The method uses gradient projection to compute this hidden vector and employs a restart strategy to handle changes in optimization direction. A key innovation is the hidden-vector-based preconditioning matrix that adjusts learning rates based on the difference between current gradients and the hidden vector. Theoretical analysis establishes convergence guarantees for both convex and non-convex settings. Experiments on image classification, natural language processing, and generative adversarial networks show HVAdam consistently outperforms state-of-the-art optimizers, achieving higher accuracy and faster convergence. Notably, it improves FID scores in GAN training and closes the generalization gap between adaptive and non-adaptive methods.

## Method Summary
HVAdam is a novel adaptive optimizer that addresses the "valley dilemma" in deep learning by incorporating a hidden vector that captures the stable gradient trend of the loss function. The method uses gradient projection to compute this hidden vector and employs a restart strategy to handle changes in optimization direction. A key innovation is the hidden-vector-based preconditioning matrix that adjusts learning rates based on the difference between current gradients and the hidden vector. Theoretical analysis establishes convergence guarantees for both convex and non-convex settings. Experiments on image classification, natural language processing, and generative adversarial networks show HVAdam consistently outperforms state-of-the-art optimizers, achieving higher accuracy and faster convergence.

## Key Results
- Consistently outperforms state-of-the-art optimizers on image classification, NLP, and GAN tasks
- Achieves higher accuracy and faster convergence rates
- Improves FID scores in GAN training
- Closes the generalization gap between adaptive and non-adaptive methods

## Why This Works (Mechanism)

### Mechanism 1: Hidden Vector Trend Extraction
HVAdam accelerates convergence in non-axis-aligned "valley" landscapes by isolating the invariant direction of descent, which standard adaptive methods miss due to coordinate-wise scaling. The optimizer maintains a "hidden vector" $v_t$ that approximates the stable gradient trend. It projects the current gradient moving average ($m_t$) onto the previous hidden vector ($v_{t-1}$) using a scaling factor $k_t$. This projection identifies the intersection of gradient planes (the "valley floor") where the true descent direction lies, rather than reacting to the steep walls of the valley. The mechanism fails if the loss landscape is strictly stochastic without a stable directional trend, or if the gradient noise is structured such that the projection leads to a saddle point rather than a valley floor.

### Mechanism 2: Dynamic Restart Strategy
The optimizer maintains relevance in dynamic landscapes by resetting the hidden vector when the local optimization direction shifts, preventing the "trend" from becoming stale. The system calculates the cosine similarity between the current unbiased gradient EMA ($cm_t$) and the current hidden vector ($v_t$). If this similarity drops below a threshold (0.1), indicating a landscape shift, the hidden vector is re-initialized to the current gradient EMA. This allows the optimizer to "restart" its trend hunting in a new region. The mechanism fails if the learning rate or batch size is too small, gradients may be too noisy to trigger a restart when needed, or trigger it too frequently, degrading to simple Adam-like behavior.

### Mechanism 3: Trend-Relative Preconditioning
Adjusting learning rates based on the deviation from the hidden vector (trend) rather than raw gradient magnitude reduces oscillation and accelerates progress along the valley floor. The preconditioning matrix uses $s_t$, derived from $p_t = (g_t - v_{t-1})^2$. If the current gradient $g_t$ deviates significantly from the stable trend $v_{t-1}$, it is treated as noise (large $p_t$), resulting in a smaller step size. If $g_t$ aligns with $v_{t-1}$, the step size increases. This is distinct from Adam, which penalizes dimensions based solely on large gradient magnitudes. The mechanism fails if the estimated hidden vector $v_t$ is inaccurate (points in a suboptimal direction), the preconditioning will erroneously amplify the wrong direction and dampen the correct one.

## Foundational Learning

### Concept: The Valley Dilemma (Narrow Valley Optimization)
Why needed here: This is the central problem HVAdam claims to solve. You must understand that in narrow, non-axis-aligned valleys, traditional methods like SGD or Adam oscillate across the steep walls (high gradient) while making slow progress along the floor (low gradient).
Quick check question: Can you explain why Adam's coordinate-wise adaptation might cause it to "zigzag" in a rotated valley?

### Concept: Gradient Projection & Affine Combination
Why needed here: The core logic for updating the hidden vector ($v_t = k_t m_t + (1-k_t)v_{t-1}$) relies on projecting one vector onto another. Understanding this linear algebra operation is key to seeing how the "valley floor" direction is extracted.
Quick check question: If $v_{t-1}$ and $m_t$ point in similar directions, how would the coefficient $k_t$ behave in the update equation?

### Concept: Exponential Moving Average (EMA) Bias Correction
Why needed here: The algorithm relies heavily on $cm_t$ (corrected first moment) and $c\delta_t$ (corrected similarity metric). Understanding why bias correction (dividing by $1-\beta^t$) is necessary during early training steps is critical for debugging the restart logic.
Quick check question: Why is the raw EMA of cosine similarity ($\delta_t$) insufficient for the restart decision compared to the unbiased estimate ($c\delta_t$)?

## Architecture Onboarding

### Component map:
Input: Stochastic Gradient $g_t$ -> State Vectors ($m_t$, $v_t$, $s_t$, $\delta_t$) -> Calculate Gradient -> Update Momentum -> Update/Restart Hidden Vector -> Update Preconditioner -> Update Weights

### Critical path:
The **Restart Logic** (Algorithm 3, lines 11-26). Specifically, the calculation of $c\delta_t$ and the conditional reset of $t_2$ and $v_0$. If this logic is implemented incorrectly (e.g., missing bias correction), the optimizer will fail to adapt to new landscapes or restart too often.

### Design tradeoffs:
- Memory vs. Smoothness: HVAdam requires maintaining additional state vectors ($v_t$, $s_t$, $\delta_t$) compared to SGD, roughly 2-3x the memory of simple SGD.
- Sensitivity: The restart threshold (0.1) and the `lr` function constants (6, -3) are empirical heuristics found in the paper; tuning them might be necessary for non-CV tasks.
- Complexity: The algorithm requires vector norms, dot products, and cosine similarity per step, adding computational overhead compared to pure element-wise Adam updates.

### Failure signatures:
- Oscillation: If the hidden vector $v_t$ is computed but never used effectively in preconditioning, the model behaves like Adam but with extra memory cost.
- Stagnation: If the restart threshold is too low, the hidden vector may never reset, causing the optimizer to persist in an outdated direction long after the landscape has changed.
- NaN/Instability: The calculation of $\eta_t$ involves division by terms like $(g_t - m_t)^2$. If gradients explode or vanish, numerical stability risks increase.

### First 3 experiments:
1. **Valley Function Test:** Replicate the "Valley Dilemma" (e.g., rotated Rosenbrock) optimization path. Verify that HVAdam descends the valley floor while Adam/SGD zigzag (visual check).
2. **CIFAR-10 Baseline:** Train ResNet34. Compare test accuracy convergence speed against Adam and AdaBelief to validate the claim of closing the generalization gap (target: >94% test accuracy).
3. **GAN Stability:** Train a standard WGAN-GP on CIFAR-10. Monitor FID scores to see if the trend-aware preconditioning stabilizes the adversarial training (HVAdam target FID < 60, per Page 7, Fig 6).

## Open Questions the Paper Calls Out

### Open Question 1
Can the computational complexity and memory overhead of HVAdam be reduced to support training in memory-constrained environments or extremely large-scale models? The authors state in the Conclusion: "In future work, we plan to optimize the algorithm's computational efficiency and reduce its memory requirements to enhance its applicability." The current algorithm requires maintaining additional state vectors (hidden vector $v_t$, intermediate variables) which the authors acknowledge limits practicality. A modified implementation demonstrating reduced memory footprint without loss of convergence speed, tested on large-scale models, would resolve this.

### Open Question 2
How sensitive is the optimizer to the empirical restart threshold ($c\delta_{t2} \ge 0.1$) and the functional form of $lr(\cdot)$ across diverse non-convex landscapes? The paper notes the threshold of 0.1 is "selected empirically" and the learning rate function "can be set as other more suitable choices," suggesting the current choices are heuristics rather than theoretically optimal settings. The robustness of the restart strategy relies on these specific hand-picked values, which may not generalize to all tasks without tuning. An ablation study showing performance variance when altering the restart threshold or the $lr$ function on a variety of complex tasks would resolve this.

### Open Question 3
Does HVAdam outperform AdamW on Large Language Model (LLM) pretraining tasks where the "valley dilemma" dynamics might differ from image tasks? While the abstract mentions LLMs as motivation, the experiments are limited to CNNs, GANs, ViT, and small-scale language modeling (LSTM/IWSLT14), leaving LLM-specific performance unverified. The "hidden vector" mechanism assumes a stable gradient trend exists; it is unclear if this assumption holds or provides benefit in the massive, noisy parameter space of LLMs. Benchmark results on standard LLM pretraining datasets (e.g., The Pile or C4) comparing perplexity and convergence speed against AdamW would resolve this.

## Limitations

- The algorithm requires maintaining additional state vectors ($v_t$, $s_t$, $\delta_t$) compared to SGD, roughly 2-3x the memory of simple SGD
- The restart threshold (0.1) and the `lr` function constants (6, -3) are empirical heuristics found in the paper; tuning them might be necessary for non-CV tasks
- The algorithm requires vector norms, dot products, and cosine similarity per step, adding computational overhead compared to pure element-wise Adam updates

## Confidence

High: The theoretical analysis establishes convergence guarantees for both convex and non-convex settings, and the experimental results show consistent improvements across multiple domains.

Medium: The implementation requires careful attention to the restart logic and the calculation of the hidden vector, with potential failure modes related to numerical stability and sensitivity to hyperparameters.

Low: The paper does not provide specific guidance on batch sizes for the experiments, and the restart threshold and `lr` function constants are empirical heuristics that may require tuning for different tasks.

## Next Checks

1. **Implementation Verification:** Implement HVAdam and verify the hidden vector update and restart logic match the paper's specifications, paying particular attention to bias correction and the cosine similarity threshold.

2. **Valley Function Test:** Reproduce the "Valley Dilemma" optimization path using a rotated Rosenbrock function to visually confirm that HVAdam descends the valley floor while Adam/SGD zigzag.

3. **CIFAR-10 Baseline:** Train ResNet34 on CIFAR-10 using HVAdam and compare test accuracy convergence speed against Adam and AdaBelief baselines to validate the claim of closing the generalization gap.