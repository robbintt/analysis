---
ver: rpa2
title: Evaluation of LLMs on Long-tail Entity Linking in Historical Documents
arxiv_id: '2505.03473'
source_url: https://arxiv.org/abs/2505.03473
tags:
- entity
- entities
- linking
- llms
- long-tail
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) for long-tail
  entity linking in historical documents using the MHERCL v0.1 benchmark. Traditional
  methods struggle with rare entities, so the researchers compared GPT 3.5 and Llama
  3 (8B and 70B) against ReLiK, a state-of-the-art system.
---

# Evaluation of LLMs on Long-tail Entity Linking in Historical Documents

## Quick Facts
- **arXiv ID**: 2505.03473
- **Source URL**: https://arxiv.org/abs/2505.03473
- **Reference count**: 23
- **Primary result**: LLMs significantly outperform traditional systems on recall for rare entities in historical documents, though precision suffers from over-generation.

## Executive Summary
This study evaluates large language models for long-tail entity linking in historical documents using the MHERCL v0.1 benchmark. Traditional methods struggle with rare entities, so researchers compared GPT 3.5 and Llama 3 (8B and 70B) against ReLiK, a state-of-the-art system. LLMs were prompted to jointly detect and link entities to Wikipedia pages using JSON output. Results show LLMs significantly outperform ReLiK in recall (up to 60.3% vs. 45.7%), especially for rare entities, though precision is lower due to over-generation. Llama 3-70B achieved competitive F1 scores with ReLiK on very rare entities (threshold = 20). Qualitative analysis revealed OCR errors and insufficient context hinder performance. The study suggests LLMs can complement traditional methods for long-tail entity linking, though precision improvements are needed through better prompting or hybrid approaches.

## Method Summary
The study treats entity linking as a sequence-to-sequence generation task, jointly performing mention detection and linking. Models are prompted with one-shot examples to output entity spans and Wikipedia titles in JSON format. Three LLMs (GPT-3.5-turbo-instruct, Llama-3-8B-Instruct, Llama-3-70B-Instruct) are evaluated against ReLiK-base on the MHERCL v0.1.2 benchmark containing historical music texts (1823-1900) with OCR noise. Performance is measured using precision, recall, and F1-score with exact match on Wikipedia titles, with secondary analysis by entity popularity based on Wikidata triple counts.

## Key Results
- LLama3-70B achieved 60.3% recall, exceeding ReLiK by approximately 15% on rare entities
- GPT-3.5 precision dropped below 1% when prompted for QIDs due to hallucination
- Llama-3-70B reached competitive F1 scores with ReLiK on very rare entities (threshold = 20)
- LLMs showed strong recall advantages but suffered from over-generation causing low precision

## Why This Works (Mechanism)

### Mechanism 1: Joint Mention Detection and Linking via Seq2Seq
Treating entity linking as a single sequence-to-sequence task enables LLMs to leverage shared context for both detection and disambiguation simultaneously. Rather than separating entity recognition from linking, the model generates entity-identifier pairs autoregressively. Contextual cues used to identify a mention also inform its disambiguation, reducing error propagation between stages. Core assumption: LLMs can reliably map textual spans to canonical identifiers using parametric knowledge without intermediate retrieval.

### Mechanism 2: Parametric Knowledge Activation for Long-tail Entities
LLMs outperform retrieval-based systems on long-tail entities because their parametric knowledge encodes patterns from diverse pre-training corpora that include niche domain knowledge. Traditional systems like ReLiK rely on explicit KB lookups; if an entity has sparse representation in the KB, retrieval fails. LLMs instead activate latent knowledge from pre-training, allowing them to recognize and link entities with minimal or no KB presence. Core assumption: The pre-training corpora contained sufficient exposure to the domain-specific entities (here, historical music figures) for the model to form useful representations.

### Mechanism 3: Generative Over-production as Recall-Precision Trade-off
LLMs' high recall comes at the cost of precision due to their generative tendency to produce plausible-but-non-existent entities. Autoregressive generation optimizes for fluency and plausibility, not factual grounding. The model generates entity candidates that "fit" the context linguistically but may lack KB validation, inflating false positives. Core assumption: The one-shot prompt provides insufficient grounding constraints to prevent hallucination.

## Foundational Learning

- **Concept: Entity Linking pipeline (Mention Detection → Candidate Retrieval → Disambiguation)**
  - Why needed here: The paper implicitly critiques traditional multi-stage pipelines by proposing a unified seq2seq approach. Understanding the standard pipeline reveals why joint detection-linking matters.
  - Quick check question: Can you explain why separating mention detection from disambiguation might cause error propagation in low-context scenarios?

- **Concept: Long-tail distribution in knowledge bases**
  - Why needed here: The core problem being solved—rare entities have sparse KB entries and limited training signal, causing traditional systems to underperform.
  - Quick check question: If an entity has only 5 Wikidata triples associated with it, would you expect ReLiK or Llama3-70B to link it more reliably? Why?

- **Concept: Autoregressive generation and hallucination**
  - Why needed here: Understanding why LLMs generate fictional QIDs or entities requires grasping how autoregressive models optimize for next-token probability rather than factual correctness.
  - Quick check question: Why would an LLM generate a plausible-sounding but fake QID like "Q7891234" instead of admitting it doesn't know?

## Architecture Onboarding

- **Component map**: Raw sentence → Prompt template → LLM backbone → Output parser → KB mapper → Evaluation

- **Critical path**: 
  1. Sentence → Prompt construction (one-shot example + target sentence)
  2. LLM inference → JSON output with entity spans and Wikipedia titles
  3. Output parsing → Extract valid JSON, normalize titles
  4. Title-to-QID lookup → Map predictions to Wikidata for evaluation
  5. Precision/Recall/F1 computation against ground truth

- **Design tradeoffs**:
  - Wikipedia titles vs. QIDs: Titles are linguistically grounded; QIDs cause near-total hallucination (Assumption: QIDs lack the lexical patterns LLMs rely on)
  - One-shot vs. zero-shot: The one-shot example provides format grounding but may bias entity types
  - LLM size (8B vs. 70B): 70B substantially outperforms 8B on recall (60.3% vs. 40.1%), suggesting scale matters for long-tail knowledge

- **Failure signatures**:
  - OCR-induced errors: "Mocre" not linked to Thomas Moore; models fail when surface form is corrupted and context is sparse
  - Popularity bias: "Teatro Santo Augustino" incorrectly linked to more famous "Teatro Carlo Felice" despite sufficient context
  - Over-generation: Fictional entities inflate false positives; exact-match evaluation penalizes even minor title variations

- **First 3 experiments**:
  1. **Baseline replication**: Run the same one-shot JSON prompt on MHERCL v0.1.2 with Llama3-70B-Instruct; measure precision, recall, F1. Confirm you can reproduce ~47% precision / ~60% recall.
  2. **Prompt ablation**: Test zero-shot vs. one-shot vs. few-shot (3 examples) prompting. Hypothesis: More examples may improve precision by constraining output format and reducing over-generation.
  3. **Retrieval-augmented hybrid**: Add a KB retrieval step (e.g., ReLiK retriever) before LLM generation; provide retrieved candidates in the prompt. Measure if recall is maintained while precision improves due to grounding.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can In-Context Learning (ICL) or Knowledge Injection techniques mitigate the over-generation of entities and improve precision in long-tail Entity Linking?
  - Basis in paper: [explicit] The conclusion states, "Possible investigations include In-Context Learning (ICL) techniques... or Knowledge Injection... Such methods could potentially mitigate the over-generation issue."
  - Why unresolved: The current study utilized "vanilla prompt-based approaches," and the authors explicitly identify the need for these advanced methods to correct the low precision caused by fictional entities.
  - What evidence would resolve it: Experiments comparing baseline zero-shot performance against few-shot ICL or RAG-enhanced prompting on the MHERCL dataset, specifically analyzing changes in precision scores.

- **Open Question 2**: Are LLMs more effective when used as entity retrievers to augment traditional EL systems rather than functioning as end-to-end linkers?
  - Basis in paper: [explicit] The authors hypothesize in the results that "they could serve as entity retrievers or augment the retrieval of existing EL retrievers" due to their high recall.
  - Why unresolved: The experiments evaluated LLMs as standalone sequence-to-sequence solutions, not as components within a retrieval pipeline.
  - What evidence would resolve it: A hybrid system architecture where LLMs generate candidate mentions for a downstream disambiguator (like ReLiK), measuring the retrieval recall and final disambiguation accuracy.

- **Open Question 3**: To what extent does the provision of semantic context mitigate the impact of OCR noise on LLM performance in historical documents?
  - Basis in paper: [inferred] The qualitative evaluation notes that while LLMs failed to link entities with limited context (e.g., "Mr. Mocre"), they succeeded when "provided with sufficient contextual information" despite OCR errors.
  - Why unresolved: The paper observes this phenomenon anecdotally but does not quantitatively measure the correlation between context window size, noise levels, and linking accuracy.
  - What evidence would resolve it: An ablation study varying the amount of context provided for noisy sentences to identify the threshold required for accurate disambiguation.

## Limitations

- **Generation hyperparameters unspecified**: Temperature, top_p, and max_tokens settings for LLM inference are not provided, affecting reproducibility and hallucination rates.
- **KB mapping reliability concerns**: Exact procedures for Wikipedia-to-QID mapping via KILT are not fully specified, potentially introducing evaluation mismatches.
- **Prompt brittleness**: Handling of malformed or empty JSON outputs is not described, suggesting potential fragility in the experimental setup.

## Confidence

- **High confidence**: The general finding that LLMs outperform ReLiK on recall for rare entities is well-supported by quantitative results and aligns with established knowledge about parametric knowledge advantages.
- **Medium confidence**: The mechanism explanation for why LLMs generate fictional entities (hallucination tendency) is plausible but not thoroughly validated against alternative explanations.
- **Low confidence**: The exact magnitude of precision-recall tradeoffs across different prompting strategies cannot be verified without generation hyperparameters and more detailed false positive analysis.

## Next Checks

1. **Generation hyperparameter ablation**: Test temperature values (0.0, 0.7, 1.0) with the exact prompt to quantify hallucination sensitivity and determine optimal settings for balancing precision and recall.

2. **Prompt format robustness**: Evaluate zero-shot and few-shot variations of the JSON prompt to determine if the one-shot format is optimal or if it contributes to over-generation artifacts.

3. **Retrieval-augmented baseline**: Implement a hybrid approach combining ReLiK's candidate retrieval with LLM disambiguation to test whether traditional retrieval strengths can complement LLM generation advantages.