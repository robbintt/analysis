---
ver: rpa2
title: 'Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba
  as Self-Attention''s Alternative'
arxiv_id: '2508.09294'
source_url: https://arxiv.org/abs/2508.09294
tags:
- speech
- mamba
- fake-mamba
- ieee
- pn-bimamba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Fake-Mamba, a real-time speech deepfake detection
  framework that addresses the limitations of existing self-attention-based methods.
  The core innovation replaces multi-head self-attention with bidirectional Mamba
  blocks to achieve near-linear time complexity and improved artifact localization.
---

# Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative

## Quick Facts
- arXiv ID: 2508.09294
- Source URL: https://arxiv.org/abs/2508.09294
- Authors: Xi Xuan; Zimo Zhu; Wenxin Zhang; Yi-Cheng Lin; Tomi Kinnunen
- Reference count: 40
- Primary result: Achieves EERs of 0.97%, 1.74%, and 5.85% on ASVspoof 2021 LA, DF, and In-The-Wild benchmarks respectively

## Executive Summary
This paper introduces Fake-Mamba, a real-time speech deepfake detection framework that replaces multi-head self-attention with bidirectional Mamba blocks. The core innovation addresses the quadratic complexity and limited temporal-channel interaction of self-attention by using input-dependent selective state-space modeling. Evaluated on three public benchmarks, Fake-Mamba achieves state-of-the-art performance while maintaining real-time inference across varying utterance lengths, demonstrating practical viability for anti-spoofing applications.

## Method Summary
Fake-Mamba processes audio through an XLSR pre-trained front-end, linear projection, and PN-BiMamba backbone consisting of 4-7 bidirectional Mamba blocks with parallel SSM paths and Pre-LayerNorm stabilization. The framework uses weighted cross-entropy loss, Adam optimization, and model averaging of top 5 epochs. Training employs RawBoost augmentation with different configurations for LA and DF targets. The architecture achieves near-linear time complexity while maintaining global receptive field for artifact detection.

## Key Results
- Achieves 0.97% EER on ASVspoof 2021 LA, outperforming XLSR-Conformer by 0.43%
- Maintains real-time inference with consistent RTF across utterance durations (0.37-0.47)
- Shows superior artifact localization through clearer real/fake separation in t-SNE embeddings
- Demonstrates strong cross-dataset generalization with 5.85% EER on In-The-Wild data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing MHSA with bidirectional Mamba enables near-linear time complexity while maintaining global receptive field for artifact detection
- Mechanism: Mamba uses selective state-space modeling where parameters (Δt, Aₜ, Bₜ, Cₜ) update dynamically based on input at each timestep, enabling input-dependent feature contribution control across temporal and channel dimensions
- Core assumption: Synthetic speech artifacts exhibit temporal-channel dependencies benefiting from sequential selective filtering rather than pairwise attention computation
- Evidence anchors: Abstract claims near-linear complexity and artifact localization; Section I states Mamba offers two compelling advantages over Conformer-based approaches

### Mechanism 2
- Claim: PN-BiMamba's parallel SSM paths with Pre-LayerNorm stabilization enable better temporal-channel artifact fusion
- Mechanism: Three Pre-LayerNorm layers stabilize training gradients. Parallel forward/backward SSM paths process input independently before fusion (hforward + hbackward), capturing bidirectional dependencies without constraining cross-dimensional interaction
- Core assumption: Critical artifacts require simultaneous modeling of temporal and channel dimensions, which pure stacked blocks bottleneck
- Evidence anchors: Section IV.B identifies representation bottlenecks in pure block stacking; Table III shows 62.6% performance drop without all three LayerNorm layers

### Mechanism 3
- Claim: XLSR pre-trained representations provide essential linguistic priors that amplify Mamba's ability to detect subtle synthetic cues
- Mechanism: XLSR (wav2vec 2.0-based, trained on 436k hours across 128 languages) encodes natural speech characteristics. Fine-tuning XLSR jointly with BiMamba backbone allows leveraging learned distinctions between human and synthetic speech patterns
- Core assumption: Synthetic speech detection benefits more from rich pre-trained linguistic representations than from raw acoustic feature learning
- Evidence anchors: Section IV.A reports XLSR outperforms other foundation models in SDD; Figure 3 shows clearer real/fake separation for Fake-Mamba vs XLSR-Conformer

## Foundational Learning

- **State-Space Models and Discretization**: Why needed - Mamba's selective SSM builds on continuous-to-discrete transformation via zero-order hold (ZOH). Quick check - Can you explain why discretization parameter Δ affects the convolution kernel Kd computation?

- **Bidirectional Sequence Modeling**: Why needed - PN-BiMamba processes sequences in both directions (hforward + hbackward). Quick check - Why would right-to-left processing help detect artifacts in synthetic speech?

- **Self-Supervised Speech Representations (wav2vec 2.0/XLSR)**: Why needed - Front-end is not trained from scratch. Quick check - What type of information does wav2vec 2.0 learn during pre-training that might transfer to deepfake detection?

## Architecture Onboarding

- **Component map**: Input audio → XLSR → Linear Projection (Dim=144) → [PN-BiMamba × N] → Linear Attention Pooling → MLP → logits

- **Critical path**: Audio input flows through frozen/fine-tuned XLSR front-end, linear projection, PN-BiMamba backbone blocks, linear attention pooling, and binary MLP classifier

- **Design tradeoffs**: 4 blocks (Fake-Mamba-S) vs 7 blocks (Fake-Mamba-L) - L gives better accuracy but S slightly outperforms on 21DF; L has 0.93M more parameters. Pre-LayerNorm critical - removing drops EER significantly. Bidirectional overhead - forward and backward passes required per block, but inference remains real-time per RTF benchmarks

- **Failure signatures**: EER spikes on very short utterances (<3s) where ConBiMamba may outperform; training instability without Pre-LayerNorm (62.6% performance drop); poor generalization to unseen attacks if RawBoost augmentation doesn't match target domain

- **First 3 experiments**:
  1. Reproduce baseline comparison on ASVspoof 2021 LA with XLSR-Conformer to validate setup (expected EER ~1.40%)
  2. Ablate bidirectional processing: Replace PN-BiMamba with unidirectional Mamba (expect ~35% relative degradation per Table III)
  3. Test duration sensitivity: Evaluate on In-The-Wild subsets bucketed by duration (<3s, 3-4s, 4-5s, 5-6s, >6s) to reproduce Table IV patterns

## Open Questions the Paper Calls Out

- **Source Tracing**: Can Fake-Mamba be adapted to identify specific generative models used to synthesize speech deepfakes? The conclusion explicitly states future work will explore this potential, but current binary classification doesn't evaluate attribution to specific TTS or VC algorithms.

- **Edge Device Viability**: Does Fake-Mamba retain efficiency advantages on memory-limited edge devices given the large XLSR front-end? All benchmarks used high-performance server GPU (Tesla V100 32GB), leaving viability on embedded hardware unverified.

- **Artifact Localization**: Does Mamba selection mechanism offer superior temporal-channel artifact localization compared to attention maps of Conformers? Results only visualize utterance-level embeddings (t-SNE) without quantitative localization metrics or specific artifact location analysis.

## Limitations

- **XLSR Model Specification**: Paper references "XLSR" without specifying exact checkpoint (xls-r-300m vs xls-r-1b), preventing precise reproduction despite 319M parameter count suggesting xls-r-300m

- **Mamba Kernel Dependency**: Framework relies on CUDA-specific selective scan kernels - without proper GPU compatibility, near-linear time complexity advantage disappears

- **RawBoost Augmentation Specificity**: Different RawBoost configurations used for LA vs DF targets without providing exact parameter ranges or distributions, making faithful reproduction difficult

## Confidence

- **High Confidence**: ASVspoof 2021 LA benchmark results (EER 0.97%) and comparative performance against XLSR-Conformer - directly measurable with clearly specified experimental setup

- **Medium Confidence**: Near-linear time complexity and real-time inference claims - RTF metrics provided but actual deployment environment not specified and Mamba kernel availability could affect practical runtime

- **Low Confidence**: XLSR representation quality and artifact localization improvements - rely on qualitative t-SNE visualizations without quantitative localization metrics or ablation studies isolating XLSR's contribution

## Next Checks

1. **Replicate XLSR Pre-Training Transfer**: Train identical PN-BiMamba architecture from scratch (without XLSR) on ASVspoof 2019 LA, then evaluate on 2021 LA. Compare EER to full Fake-Mamba to isolate XLSR's contribution.

2. **Test Mamba Kernel Robustness**: Run model on multiple GPU configurations (different CUDA compute capabilities) and measure actual runtime vs reported RTF. Document performance degradation when selective scan kernels are unavailable.

3. **Duration-Based Failure Analysis**: Replicate Table IV's duration sensitivity analysis but add error analysis: for each duration bucket, compute confusion matrices to identify which attack types cause most errors, particularly for utterances under 3 seconds where ConBiMamba reportedly outperforms.