---
ver: rpa2
title: 'ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff
  Code Classification'
arxiv_id: '2509.18400'
source_url: https://arxiv.org/abs/2509.18400
tags:
- zhang
- wang
- code
- classification
- digit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of classifying products under
  the Harmonized Tariff Schedule (HTS), a critical bottleneck in global trade that
  can halt shipments due to misclassification. The authors introduce the first benchmark
  dataset for HTS classification, derived from U.S.
---

# ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification

## Quick Facts
- arXiv ID: 2509.18400
- Source URL: https://arxiv.org/abs/2509.18400
- Authors: Pritish Yuvraj; Siva Devarakonda
- Reference count: 10
- Primary result: 40% 10-digit and 57.5% 6-digit accuracy on HTS classification, outperforming GPT-5-Thinking and Gemini-2.5-Pro-Thinking

## Executive Summary
This paper addresses the challenge of classifying products under the Harmonized Tariff Schedule (HTS), a critical bottleneck in global trade that can halt shipments due to misclassification. The authors introduce the first benchmark dataset for HTS classification, derived from U.S. Customs Rulings Online Search System (CROSS), and present ATLAS, a fine-tuned LLaMA-3.3-70B model specialized for this task. Through supervised fine-tuning on the CROSS dataset, ATLAS achieves 40% fully correct 10-digit classifications and 57.5% correct 6-digit classifications—improvements of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking. Beyond accuracy, ATLAS is 5× cheaper than GPT-5-Thinking and 8× cheaper than Gemini-2.5-Pro-Thinking, and supports privacy-preserving self-hosting. While ATLAS sets a strong baseline, the benchmark remains highly challenging with only 40% 10-digit accuracy, highlighting the need for future work in retrieval, reasoning, and alignment methods for this high-impact global trade problem.

## Method Summary
The authors fine-tune LLaMA-3.3-70B on the CROSS dataset of U.S. Customs rulings using supervised fine-tuning with negative log-likelihood loss. The training pipeline uses AdamW optimizer with cosine learning rate schedule, bf16 precision, and gradient accumulation across 16×A100-80GB GPUs. Each training sample is transformed into a structured prompt containing product description, reasoning trace requirement, and ground-truth HTS code. The model is evaluated on a held-out test set of 200 samples using exact 10-digit match, 6-digit match, and average digit accuracy metrics.

## Key Results
- ATLAS achieves 40% fully correct 10-digit classifications and 57.5% correct 6-digit classifications
- Outperforms GPT-5-Thinking by +15 points and Gemini-2.5-Pro-Thinking by +27.5 points on 10-digit accuracy
- Self-hosted deployment is 5× cheaper than GPT-5-Thinking and 8× cheaper than Gemini-2.5-Pro-Thinking
- Models consistently perform better on 6-digit codes (57.5% accuracy) than 10-digit codes (40% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised fine-tuning on domain-specific legal rulings substantially improves HTS classification over general-purpose LLMs.
- Mechanism: SFT aligns model behavior to the hierarchical HTS task by training on structured prompt-response pairs that include product descriptions, reasoning traces, and ground-truth codes. This grounds predictions in legal precedent rather than generic semantic similarity.
- Core assumption: The CROSS rulings are representative of the broader HTS classification distribution and that standard NLL optimization transfers to this multi-way hierarchical task.
- Evidence anchors:
  - [abstract] "our fine-tuned ATLAS model (LLaMA-3.3-70B) achieves 40% fully correct 10-digit classifications...improvements of +15 points over GPT-5-Thinking"
  - [section 4.1, Table 2] LLaMA-3.3-70B baseline: 2.1% → ATLAS: 40% (same architecture, different training)
  - [corpus] Limited direct corpus support; one related paper (HSCodeComp) addresses hierarchical rule application for HTS but focuses on search agents rather than SFT.
- Break condition: If test products come from HTS codes absent or rare in CROSS rulings, SFT benefits may not generalize (only 2,992 of 17,000+ codes represented).

### Mechanism 2
- Claim: Classification accuracy follows a hierarchical difficulty gradient—models perform better on globally harmonized 6-digit codes than on country-specific 10-digit extensions.
- Mechanism: The first 6 digits are standardized across WTO members and correspond to stable product categories, while digits 7–10 are U.S.-specific extensions requiring finer distinctions and more localized knowledge.
- Core assumption: The 6-digit subset captures a qualitatively different (easier) subtask than the full 10-digit task.
- Evidence anchors:
  - [abstract] "40% fully correct 10-digit classifications and 57.5% correct 6-digit classifications"
  - [section 1] "six-digit accuracy captures worldwide consistency, and ten-digit accuracy reflects the U.S.-specific extension"
  - [section 4.2, Table 3] All models show higher 6-digit than 10-digit accuracy (e.g., GPT-5-Thinking: 55.5% vs 25%)
  - [corpus] HSCodeComp paper notes similar hierarchical rule complexity but does not provide comparative validation of this mechanism.
- Break condition: If a country's tariff extensions are structured differently (e.g., not hierarchical), this pattern may not hold.

### Mechanism 3
- Claim: Requiring reasoning traces alongside code predictions improves classification quality by enforcing explicit justification.
- Mechanism: Chain-of-thought-style prompting forces the model to generate intermediate reasoning before outputting the HTS code, potentially reducing hallucinations and aligning predictions with legal logic.
- Core assumption: The quality of generated reasoning correlates with classification accuracy, and models trained to produce reasoning will apply similar reasoning at inference.
- Evidence anchors:
  - [section 2.2] Prompt template explicitly requires: "Create a reasoning path justifying why the HTS US code is correct"
  - [section 2.2] "This design forces models to both predict the code and provide a reasoning path, aligning with recent work on chain-of-thought reasoning"
  - [corpus] No direct corpus evidence on reasoning-to-accuracy correlation for HTS.
- Break condition: If reasoning traces are hallucinated post-hoc rather than causally contributing to code selection, this mechanism may not hold.

## Foundational Learning

- Concept: **Hierarchical Multi-label Classification**
  - Why needed here: HTS codes are structured as a taxonomy (sections → chapters → subheadings), and errors are not equally costly at each level.
  - Quick check question: Can you explain why predicting "Chapter 84 (machinery)" correctly but the wrong 10-digit subheading is more useful than predicting "Chapter 90 (instruments)" entirely?

- Concept: **Supervised Fine-Tuning Objective (NLL)**
  - Why needed here: ATLAS is trained by minimizing negative log-likelihood over token sequences—understanding this clarifies what the model is optimized to do.
  - Quick check question: Given the SFT loss function, why might the model still struggle with rare HTS codes that appear few times in training?

- Concept: **Domain Shift and Dataset Coverage**
  - Why needed here: CROSS contains only disputed/clarified codes, not all possible HTS codes—this biases what the model learns.
  - Quick check question: If a new product has no similar precedent in CROSS, what failure mode should you expect from ATLAS?

## Architecture Onboarding

- Component map:
  CROSS rulings (HTML) -> GPT-4o-mini extraction -> structured prompts with {product_description, reasoning, hts_code} -> LLaMA-3.3-70B SFT -> HTS code + reasoning trace output

- Critical path:
  1. Data quality depends on extraction fidelity from raw rulings—errors here propagate to training.
  2. SFT convergence (5 epochs, ~1,400 steps) must avoid overfitting given small dataset (18K examples for 70B model).
  3. Evaluation requires hierarchical metrics (6-digit vs 10-digit) to capture partial credit.

- Design tradeoffs:
  - Dense vs. MoE: Authors chose LLaMA-3.3-70B (dense) over MoE architectures (DeepSeek-R1, GPT-OSS-120B) for easier fine-tuning and deployment, trading potential accuracy for reproducibility.
  - Dataset size vs. coverage: 18K rulings cover only ~3K of 17K+ HTS codes; broader coverage would require additional data sources.
  - Cost vs. accuracy: Self-hosted ATLAS is ~5–8× cheaper than proprietary APIs but requires GPU infrastructure and operational overhead.

- Failure signatures:
  - Low 10-digit accuracy with high 6-digit accuracy → model struggles with U.S.-specific extensions (expected; 40% vs 57.5%).
  - Hallucinated reasoning that doesn't match HTS logic → training data may contain inconsistent justifications.
  - Poor performance on codes absent from CROSS → domain shift; model has no precedent.

- First 3 experiments:
  1. **Retrieval augmentation**: Index the 17,000-page HTS PDF corpus and retrieve relevant sections at inference; measure improvement on long-tail codes not well-represented in CROSS.
  2. **Ablate reasoning traces**: Train a variant without requiring reasoning paths to test whether chain-of-thought format causally improves accuracy or just adds overhead.
  3. **Scale down test**: Fine-tune LLaMA-3.3-8B on the same data to quantify accuracy-vs-cost tradeoffs for resource-constrained deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would retrieval-augmented generation (RAG) over the full 17,000-page HTS document corpus significantly improve classification accuracy, particularly for long-tail codes underrepresented in CROSS rulings?
- Basis in paper: [explicit] Section 3.3 states: "Integrating retrieval over the 17,000-page HTS documents may reduce hallucinations and improve long-tail classification accuracy, complementing SFT."
- Why unresolved: ATLAS uses only supervised fine-tuning on CROSS rulings without access to the full HTS documentation during inference, potentially missing important context.
- What evidence would resolve it: A comparative study evaluating ATLAS with and without RAG on a stratified test set including rare HTS codes.

### Open Question 2
- Question: Can ATLAS be distilled to smaller models (e.g., 8B or 3B parameters) while maintaining at least 90% of the 70B model's 10-digit classification accuracy?
- Basis in paper: [explicit] Section 5 lists model distillation as a future direction: "distilling ATLAS into smaller variants (e.g., 8B or 3B) for efficient deployment in resource-constrained settings."
- Why unresolved: Only the 70B dense model was fine-tuned; smaller variants were not evaluated due to budget constraints.
- What evidence would resolve it: Fine-tune LLaMA-3.3-8B/3B on the same CROSS dataset and compare accuracy-cost tradeoffs on the held-out test set.

### Open Question 3
- Question: Would Direct Preference Optimization (DPO) or contrastive learning improve discrimination between semantically similar HTS codes (e.g., semiconductor wafers vs. finished chips)?
- Basis in paper: [explicit] Section 3.3 proposes that DPO "could leverage structured preferences over HTS classifications" and contrastive learning "may sharpen decision boundaries."
- Why unresolved: Only standard negative log-likelihood (NLL) supervised fine-tuning was evaluated.
- What evidence would resolve it: Train variants using DPO with preference pairs from near-miss classifications and compare against the NLL baseline on ambiguous product categories.

## Limitations
- The CROSS dataset covers only ~17% of the 17,000+ HTS codes, potentially limiting generalization to codes outside the 2,992 represented
- The evaluation uses a held-out test set from the same CROSS distribution, which may overestimate real-world generalization where novel products require reasoning from first principles
- 40% 10-digit accuracy still indicates that 60% of classifications require human intervention or alternative approaches

## Confidence
- **High Confidence**: The cost comparison (5× cheaper than GPT-5-Thinking, 8× cheaper than Gemini-2.5-Pro-Thinking) and the hierarchical difficulty pattern (6-digit > 10-digit accuracy) are directly supported by the presented metrics and align with the structural properties of HTS classification.
- **Medium Confidence**: The mechanism that SFT on CROSS rulings improves performance over general LLMs is well-supported by the 15-27.5 point gains, but the extent to which this generalizes to unseen HTS codes remains uncertain given the limited coverage. The reasoning trace requirement's contribution to accuracy is plausible but lacks direct ablation evidence.
- **Low Confidence**: The claim that ATLAS represents a "first-of-its-kind" benchmark is difficult to verify given the proprietary nature of many trade classification systems and the lack of published alternatives, though no public benchmarks were identified in the corpus review.

## Next Checks
1. **Retrieval Augmentation Test**: Index the complete 17,000-page HTS legal text and implement retrieval-augmented generation at inference time. Measure whether retrieving relevant tariff sections improves accuracy on the 200 test samples, particularly for codes poorly represented in CROSS.

2. **Cross-Country Generalization**: Apply ATLAS to classify products using Canada's Customs Tariff (similar 10-digit hierarchical structure but different U.S.-specific extensions). This tests whether the model learns transferable HTS principles or overfits to U.S.-specific rulings.

3. **Reasoning Ablation Study**: Train two variants: one with the full reasoning-trace requirement and one without. Compare 10-digit accuracy and classification latency to determine if the chain-of-thought format causally improves performance or merely adds computational overhead.