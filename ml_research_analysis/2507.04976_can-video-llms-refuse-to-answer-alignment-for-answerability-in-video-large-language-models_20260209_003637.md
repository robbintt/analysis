---
ver: rpa2
title: Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large
  Language Models
arxiv_id: '2507.04976'
source_url: https://arxiv.org/abs/2507.04976
tags:
- video
- alignment
- unanswerable
- questions
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces alignment for answerability in Video Large
  Language Models (Video-LLMs), addressing their inability to handle questions beyond
  the video's informational scope. The authors propose a framework to equip Video-LLMs
  with the ability to evaluate question relevance and decline answering when necessary.
---

# Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models

## Quick Facts
- arXiv ID: 2507.04976
- Source URL: https://arxiv.org/abs/2507.04976
- Reference count: 40
- Primary result: Video-LLMs can be aligned to refuse answering unanswerable questions, with DPO outperforming SFT on trade-off metrics.

## Executive Summary
This paper addresses a fundamental limitation of Video Large Language Models (Video-LLMs): their inability to refuse answering questions that exceed the video's informational scope. The authors introduce a framework for aligning Video-LLMs to evaluate question relevance and decline answering when necessary. They propose a novel UVQA dataset with systematically generated unanswerable questions spanning object, relation, and attribute categories, and introduce evaluation metrics (Excessive Refusal, Permissiveness, Discretion scores) to measure alignment quality. Experiments demonstrate that aligned models significantly outperform unaligned baselines on both alignment metrics and accuracy for unanswerable questions, with DPO achieving better trade-offs between answerability and original performance.

## Method Summary
The method involves three main components: (1) Dataset construction using scene graph alterations on MOMA-LRG and DiDeMo to create unanswerable questions across object, relation, and attribute categories; (2) Alignment training using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) on a balanced mix of answerable (Video-ChatGPT) and unanswerable (UVQA) samples; (3) Evaluation using GPT-4 to classify response types and compute alignment metrics. The training uses AdamW optimizer with learning rate 1e-6, batch size 128, and DeepSpeed-Zero Stage 2 optimization on 2x NVIDIA A100 80GB GPUs.

## Key Results
- Aligned models (SFT/DPO) significantly outperform unaligned baselines on alignment metrics and accuracy for unanswerable questions
- DPO achieves lower excessive refusal scores while maintaining comparable accuracy, suggesting better trade-off than SFT
- Attribute-related unanswerability is consistently harder than object or relation categories across all models
- Scaling model size (7B→72B) does not improve unanswerable QA performance without alignment

## Why This Works (Mechanism)

### Mechanism 1: Alignment Training Teaches Rejection Behavior Through Supervised Preference Modeling
- Claim: Video-LLMs fail on unanswerable questions due to training distribution gaps, not capability deficits; alignment training (SFT/DPO) on mixed answerable/unanswerable data induces learned refusal behaviors.
- Mechanism: The scoring function s(v, x, y) = 1 when k(v, x) · t(y) = 1 creates a training signal where the model learns to map video-question pairs to response types (correct, wrong, unanswerable_c, unanswerable_w). DPO implicitly learns a reward model comparing preferred vs. dispreferred responses, while SFT directly imitate optimal responses.
- Core assumption: The model already has sufficient video understanding to recognize informational boundaries; the bottleneck is behavioral, not perceptual.
- Evidence anchors:
  - [abstract]: "We demonstrate that even the best-performing Video-LLMs fail to reject unfit questions—not necessarily due to a lack of video understanding, but because they have not been trained to identify and refuse such questions."
  - [section 4.1]: Eq. 2-3 define the alignment objective as learning to prefer s(v, x, y) = 1 using alignment algorithms like SFT or DPO.
  - [corpus]: Weak corpus signal; related Video-LLM papers focus on temporal reasoning and efficiency, not answerability alignment specifically.
- Break condition: If the model lacks visual grounding for specific concepts (e.g., fine-grained attributes), alignment alone cannot induce correct refusal—the model would need additional pretraining data.

### Mechanism 2: Scene Graph Alterations Generate Systematic Unanswerable Questions
- Claim: Structured modifications to scene graph triplets (object, relation, attribute) produce unanswerable questions with verifiable ground truth reasoning.
- Mechanism: The pipeline (Eq. 8-10) takes video-description pairs, applies category-constrained alterations (e.g., replacing objects within the same class), then uses an LLM to generate questions that assume the altered description is true. This creates questions with k(v, x) = -1 where the unanswerability stems from specific, traceable elements.
- Core assumption: Category-constrained replacements maintain semantic plausibility (e.g., "food" → "clothing" within object classes) while ensuring the altered element is definitively absent from the video.
- Evidence anchors:
  - [section 4.3]: "We replace one of the elements in the triplet with another from the same category to maintain naturalness in the altered description."
  - [section 4.3]: Shows categorization into Object (O), Relation (R), Attribute (A) modifications with structured replacement rules.
  - [corpus]: No corpus evidence for scene graph-based unanswerability generation; this appears novel.
- Break condition: If the original video descriptions are incomplete or inaccurate, the generated "unanswerable" questions may actually be answerable, creating label noise.

### Mechanism 3: DPO Achieves Softer Alignment Trade-offs Than SFT
- Claim: DPO produces lower excessive refusal scores while maintaining comparable accuracy, suggesting implicit regularization that preserves original capabilities better than direct behavioral cloning.
- Mechanism: DPO optimizes a preference ranking loss that penalizes unwanted behaviors relative to wanted ones, rather than forcing exact imitation. This allows the model to interpolate between pre-training and alignment objectives rather than fully overriding learned representations.
- Core assumption: The preference data captures the desired trade-off between refusal and helpfulness.
- Evidence anchors:
  - [section 5.3]: "DPO generally achieves lower Sex-ref., higher Spermis., and slightly lower Sdisc.. This suggests that DPO facilitates a softer alignment."
  - [figure 19]: Pareto front visualization shows DPO closer to optimal trade-off region than SFT.
  - [corpus]: Related work (RLAIF-V, Silkie) mentions DPO for hallucination mitigation but not specifically for answerability trade-offs.
- Break condition: If preference pairs are poorly constructed (e.g., preferred responses that are overly permissive), DPO may amplify undesirable behaviors rather than correct them.

## Foundational Learning

- Concept: **Video-LLM Architecture (Visual Encoder + Projector + LLM)**
  - Why needed here: Understanding that the projector P bridges visual features to language space is critical—alignment happens in the LLM's response distribution, not in the visual encoder.
  - Quick check question: If you froze the visual encoder V and only trained L via DPO, would the model's ability to detect unanswerable questions change? (Expected: Yes, alignment affects the response policy, not perception.)

- Concept: **Direct Preference Optimization (DPO) vs. Supervised Fine-Tuning (SFT)**
  - Why needed here: The paper shows DPO achieves better trade-offs; understanding why requires knowing that DPO learns a relative preference model while SFT does maximum likelihood.
  - Quick check question: Why might DPO lead to lower excessive refusal than SFT even with the same training data? (Expected: DPO preserves more of the pre-training distribution by learning preferences rather than exact behaviors.)

- Concept: **Scene Graph Representation (Objects, Attributes, Relations)**
  - Why needed here: The UVQA dataset construction relies on modifying scene graph elements; understanding this taxonomy is essential for extending the approach.
  - Quick check question: If a video shows a "red car driving fast," what are three ways to construct an unanswerable question? (Expected: Modify object [car→bus], attribute [red→blue], or relation [driving→parking].)

## Architecture Onboarding

- Component map:
  Input Video → Visual Encoder (V) → Vision-Language Projector (P) → LLM Backbone (L)
                                          ↓
                               Alignment Layer (SFT/DPO training)
                                          ↓
                               Response y ∈ {correct, wrong, unanswerable_c, unanswerable_w}

- Critical path:
  1. **Dataset construction**: MOMA-LRG (for O/R) or DiDeMo (for A) → Apply T(d, c) → LLM generates (x, y_gt)
  2. **Training**: Mix answerable (Video-ChatGPT) + unanswerable (UVQA) at 1:1 ratio
  3. **Evaluation**: Use GPT-4 to classify type(y) for metric computation

- Design tradeoffs:
  - SFT: Faster convergence, higher discretion score, but higher excessive refusal (over-cautious)
  - DPO: Requires preference pairs, better preserves original capabilities, softer alignment
  - Prompting-only (baseline): No training cost, but near-zero improvement (Table 3 shows F1 ≤ 0.12)

- Failure signatures:
  - High Sex-ref. + low Sdisc.: Model became over-cautious without learning proper refusal (alignment collapsed)
  - High accuracy on answerable + zero on unanswerable: Model ignored alignment data (likely learning rate too low or data imbalance)
  - Attribute-category consistently worse than object/relation (Table 2): Expected behavior—attribute detection is inherently harder

- First 3 experiments:
  1. **Baseline check**: Run unaligned Video-LLaVA on UVQA eval set to confirm near-zero performance; verify that scaling model size (7B→72B) doesn't improve unanswerable QA (Figure 1b).
  2. **Alignment ablation**: Train with SFT-only vs. DPO-only on identical data; measure Sex-ref., Spermis., Sdisc., and S_acc. to reproduce the trade-off pattern in Table 1.
  3. **Category analysis**: Evaluate separately on Object, Relation, and Attribute subsets; expect ~15-20% lower accuracy on Attribute category (Table 2). If Object is also low, check scene graph replacement logic for semantic validity.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset generation concerns: UVQA relies on automated scene graph alterations and LLM-generated questions, which may introduce label noise without sufficient human validation.
- Limited model scope: Experiments focus primarily on Video-LLaVA and VLM-RLAIF, with limited ablation on other architectures or scaling laws.
- Real-world generalization gaps: The paper doesn't address temporal dynamics or streaming scenarios where answerability might need incremental evaluation.

## Confidence
- High Confidence: The core observation that video-LLMs lack explicit training to refuse unanswerable questions, and that alignment training (SFT/DPO) improves this capability with measurable metrics.
- Medium Confidence: The claim that DPO achieves better trade-offs than SFT due to implicit regularization. While supported by Table 1 and Figure 19, the exact mechanism and generalizability across different preference datasets remains unclear.
- Low Confidence: The systematic generation of unanswerable questions through scene graph alterations produces semantically valid but definitively unanswerable questions. The assumption that category-constrained replacements maintain naturalness may not hold for all object/attribute classes.

## Next Checks
1. **Human Evaluation of UVQA Dataset Quality**: Conduct human evaluation (at least 100 samples) to assess whether the LLM-generated unanswerable questions are truly unanswerable and whether the scene graph alterations maintain semantic plausibility. Compare human judgment with GPT-4 classifications to validate the evaluation pipeline.

2. **Temporal Streaming Evaluation**: Test the aligned models on streaming video scenarios where questions must be answered with partial context. Measure how answerability judgments change as video frames accumulate, and whether the alignment generalizes to temporal reasoning tasks.

3. **Cross-Dataset Generalization**: Evaluate the aligned models on held-out video sources not used in training (beyond the MSR-VTT comparison in Table 6). Specifically test on videos with different domains (e.g., surveillance, educational, entertainment) to assess domain transfer of the answerability alignment.