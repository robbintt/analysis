---
ver: rpa2
title: 'An analysis of AI Decision under Risk: Prospect theory emerges in Large Language
  Models'
arxiv_id: '2508.00902'
source_url: https://arxiv.org/abs/2508.00902
tags:
- chance
- risk
- points
- scenarios
- framing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study tested prospect theory\u2014which predicts humans take\
  \ more risk when feeling losses versus gains\u2014in large language models (LLMs)\
  \ by presenting them with scenarios having identical expected values but framed\
  \ as losses or gains. Across civilian and military contexts, LLMs exhibited classic\
  \ prospect theory effects, especially in military scenarios, but also showed context-dependent\
  \ variations (e.g., reverse framing in career decisions)."
---

# An analysis of AI Decision under Risk: Prospect theory emerges in Large Language Models

## Quick Facts
- arXiv ID: 2508.00902
- Source URL: https://arxiv.org/abs/2508.00902
- Reference count: 26
- This study demonstrates that LLMs exhibit classic prospect theory effects (risk-seeking in losses, risk-averse in gains) when decision scenarios are framed semantically, but these effects vanish in pure mathematical notation.

## Executive Summary
This study tested whether large language models (LLMs) exhibit human-like framing effects predicted by prospect theory, where identical options presented as losses versus gains produce different risk preferences. Across seven decision scenarios (three civilian, four military), LLMs consistently showed framing effects, particularly in military contexts where loss frames led to more risk-seeking choices. Crucially, these effects disappeared when scenarios were presented as abstract mathematical problems, demonstrating that the biases arise from semantic language context rather than pure mathematical reasoning. Different LLMs displayed distinct "cognitive personalities," suggesting varied internal reasoning styles rather than uniform pattern matching.

## Method Summary
The study tested five LLMs (GPT-4o, o3, Claude Sonnet 4, Claude Sonnet 4 Thinking, Gemini 1.5 Pro) across seven decision scenarios with identical expected values (E[V]=0.5) but framed as losses or gains. Each scenario offered three options: safe, moderate-risk, and high-risk. Models generated both decisions and rationales across 25 iterations per condition. Military scenarios included maritime crisis, border dispute, trade route, and French border dispute; civilian scenarios included business merger, career transition, and championship strategy. The study compared standard LLMs against chain-of-thought models (o3, Claude-4-Thinking) and included control conditions using pure mathematical notation.

## Key Results
- LLMs exhibited classic prospect theory effects across most scenarios, especially military contexts where loss frames increased risk-seeking behavior
- Framing effects completely vanished when scenarios were presented as abstract mathematical problems
- Career transition scenario uniquely showed reverse framing effects (risk-seeking in gains, risk-averse in losses)
- Different LLMs displayed distinct "cognitive personalities" with varying risk preferences
- Chain-of-thought reasoning did not systematically reduce framing susceptibility

## Why This Works (Mechanism)

### Mechanism 1: Semantic Embedding of Human Heuristics
LLMs acquire human-like decision heuristics through language training rather than explicit memorization. Statistical regularities in human-generated text encode culturally-transmitted decision patterns (e.g., "desperate times call for desperate measures") that models learn during pretraining and apply when similar semantic contexts appear in prompts.

### Mechanism 2: Contextual Activation via "Language Games"
Risk preferences are activated locally by semantic domain rather than through universal cognitive bias. Different semantic contexts trigger different embedded scripts—military language activates "defend at all costs" heuristics; business language activates "prudent fiduciary" heuristics; career language activates "opportunity-seeking" heuristics.

### Mechanism 3: Mathematical-Semantic Mode Switching
LLMs operate in two distinct reasoning modes—semantic (biased, context-driven) and mathematical (unbiased, formal)—with semantic mode as default. When problems are presented in natural language, semantic associations override formal calculation even when models explicitly compute expected values.

## Foundational Learning

- **Concept: Prospect Theory (Kahneman-Tversky)**
  - Why needed: The entire experimental design tests whether LLMs replicate the core human finding: risk-seeking in loss frames, risk-aversion in gain frames, for equal expected values
  - Quick check: Given a choice between [100% chance of keeping 500 people] vs. [50% chance of saving 1000, 50% chance of saving none], which would a prospect-theory-consistent agent choose in a "loss frame" (200 will die without action)?

- **Concept: Expected Value Equivalence**
  - Why needed: All scenario options are mathematically identical (E[V] = 0.5). Understanding this is essential to recognize that any preference differences arise from framing, not rational calculation
  - Quick check: Calculate the expected value of: 25% chance of gaining 2.0 points, 75% chance of 0. Is this equivalent to 100% chance of 0.5?

- **Concept: Chain-of-Thought (CoT) Reasoning**
  - Why needed: The study compares standard LLMs against CoT models (o3, Claude-4-Thinking) to test whether explicit reasoning reduces framing effects
  - Quick check: If CoT models show similar framing susceptibility to non-CoT models, what does this suggest about where the bias originates in the inference process?

## Architecture Onboarding

- **Component map:** [Scenario Prompt] → [Model Selection: 5 LLMs] → [Semantic Frame: Loss/Gain] → [Decision + Rationale Generation] → [Risk Score Coding: 0/1/2] → [Statistical Analysis: Mann-Whitney U] → [Control: Mathematical Notation] → [Framing Effect Comparison]

- **Critical path:** The experimental validity depends on (1) genuinely equivalent EVs across options, (2) novel scenarios not in training data, and (3) clean separation between semantic and mathematical conditions. The three-option design (vs. classic two-option) is key to ensuring novelty.

- **Design tradeoffs:** Three options (vs. two) increases novelty but complicates direct comparison to Kahneman-Tversky literature. Using LLMs to code rationales (Table 7) introduces potential circularity. Single-author study limits inter-rater reliability on scenario construction.

- **Failure signatures:** If models had memorized prospect theory, all scenarios would show consistent framing direction (they don't—career reverses). If mathematical reasoning controlled behavior, semantic framing wouldn't persist even with explicit EV calculation (it does persist).

- **First 3 experiments:**
  1. Test a model released after paper submission (e.g., GPT-4.1, Claude Opus 4) on the same scenarios to verify this isn't transient behavior tied to specific training runs
  2. Add "First, calculate the expected value of each option" as a mandatory first step before decision to test whether forced calculation reduces framing effects
  3. Systematically test the same scenarios in 5+ languages with professional translation validation to map language-specific risk cultures

## Open Questions the Paper Calls Out

- **Open Question 1:** Why did the "Career Transition" scenario uniquely trigger a reverse framing effect across all models? The paper identifies this "intriguing exception" but does not determine the specific semantic features causing this reversal.

- **Open Question 2:** Why does explicit chain-of-thought reasoning fail to mitigate susceptibility to framing effects? The paper found no systematic difference between CoT and non-CoT models, suggesting the bias is "embedded within the language of the scenarios."

- **Open Question 3:** What mechanism drives the "variance aversion" observed when models are forced to choose in purely abstract mathematical scenarios? Models chose risk-free options 100% of the time in forced-choice mathematical scenarios, exhibiting a new heuristic distinct from semantic biases.

## Limitations
- Single-author design introduces potential experimenter bias in scenario construction and decision parsing
- Three-option design prevents direct comparison with canonical Kahneman-Tversky experiments using binary choices
- Safety refusals from models like Gemini 2.5-Pro may systematically bias the tested model population

## Confidence

- **High confidence**: Framing effects vanish in pure mathematical notation while persisting in semantic scenarios
- **Medium confidence**: These effects arise from semantic embedding rather than memorization, though alternative explanations cannot be ruled out
- **Low confidence**: The Wittgenstein "language games" framing lacks direct empirical support within this study

## Next Checks
1. Systematically test all scenarios in 5+ languages with professional translation validation to determine whether the French effect generalizes
2. Add mandatory expected value calculation as a first step in all prompts to test whether forcing mathematical reasoning reduces framing effects
3. Test the same scenarios on models released after the study (e.g., GPT-4.1, Claude Opus 4) to determine whether these effects are stable across training runs