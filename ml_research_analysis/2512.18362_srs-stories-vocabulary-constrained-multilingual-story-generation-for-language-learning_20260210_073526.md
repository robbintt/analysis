---
ver: rpa2
title: 'SRS-Stories: Vocabulary-constrained multilingual story generation for language
  learning'
arxiv_id: '2512.18362'
source_url: https://arxiv.org/abs/2512.18362
tags:
- words
- story
- rewrite
- language
- stories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SRS-Stories, a system that uses large language
  models to generate personalized stories for language learners, using only vocabulary
  they know. The approach combines spaced repetition systems with story generation,
  teaching new words in context while reviewing previously learned vocabulary.
---

# SRS-Stories: Vocabulary-constrained multilingual story generation for language learning

## Quick Facts
- arXiv ID: 2512.18362
- Source URL: https://arxiv.org/abs/2512.18362
- Authors: Wiktor Kamzela; Mateusz Lango; Ondrej Dusek
- Reference count: 19
- Primary result: Prompt-based story generation with iterative rewriting produces more grammatical, coherent, and vocabulary-integrated stories than constrained beam search for language learning

## Executive Summary
SRS-Stories is a system that uses large language models to generate personalized stories for language learners, constrained to vocabulary they know plus new words to learn. The approach combines spaced repetition systems with story generation, teaching new words in context while reviewing previously learned vocabulary. Three prompting strategies (Simple, Planning, Examples First) and three rewriting methods (Rewrite, Rewrite Highlighted, Get Synonyms) were evaluated across English, Chinese, and Polish at different proficiency levels. Results show that the proposed methods produce more grammatical, coherent, and interesting stories than constrained beam search baselines, with better integration of target vocabulary. Human evaluation confirmed superior grammatical correctness and word usage compared to the baseline.

## Method Summary
The system uses an SRS engine to select vocabulary (known words V plus new words L to learn), then generates stories constrained to use these words. Three prompting strategies decompose the generation task: Simple Prompting (direct story generation), Planning (title → outline → story), and Examples First (generate example sentences before story). A non-neural verifier identifies out-of-vocabulary (OOV) words, which are iteratively rewritten with simpler alternatives up to 5 times. Stories are 500-750 words generated by Llama 3.1 70B Instruct, evaluated by Qwen 2.5 72B for grammaticality, coherence, and interestingness on 1-5 scales.

## Key Results
- Planning strategy achieved highest target word integration (#L=2.68 occurrences per word) vs CBS baseline (1.46)
- Simple Rewrite achieved near-zero OOV rates (0.01% Planning) vs 6.71% for CBS baseline
- Human evaluation showed significantly better word usage illustration (3.88 vs 1.24, p<0.0001)
- English B1 stories achieved 90.15% target word usage vs 19.37% for Chinese with Simple Prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-step prompting strategies improve vocabulary integration and story quality compared to constrained decoding.
- Mechanism: Decomposing story generation into planning stages (title ideas → outline → story) or example sentence generation helps the LLM contextualize target words before narrative construction, leading to more natural word usage.
- Core assumption: LLMs benefit from explicit reasoning steps to satisfy lexical constraints.
- Evidence anchors:
  - [abstract] "Three prompting strategies... produce more grammatical, coherent, and interesting stories than constrained beam search baselines"
  - [section 4.1] Planning strategy achieved highest #L (2.68 occurrences per target word) vs CBS (1.46)
  - [corpus] Weak direct validation; TaleFrame (arxiv:2512.02402) similarly uses multi-stage control for story generation but doesn't cite this work.
- Break condition: If target words are semantically incompatible or too numerous (>10 per story), planning may fail to produce coherent narratives.

### Mechanism 2
- Claim: Iterative post-hoc rewriting more effectively enforces vocabulary constraints than token-level masking during generation.
- Mechanism: A non-neural verifier identifies OOV words; the LLM rewrites with simpler alternatives over 5 iterations. This decouples generation quality from constraint satisfaction.
- Core assumption: LLMs can reliably substitute synonyms without degrading narrative coherence.
- Evidence anchors:
  - [section 2.2.2] "The number of replaced words flattens out after 5 iterations"
  - [section 4.1] Simple Rewrite achieved 0.01% OOV (Planning) vs 6.71% for CBS
  - [corpus] No direct corpus evidence for this specific rewriting approach.
- Break condition: If unknown words are function words or have no simpler synonyms in the permitted vocabulary, rewriting fails and OOV words persist.

### Mechanism 3
- Claim: SRS-driven word selection creates personalized learning material that optimizes spaced review.
- Mechanism: SRS algorithm selects words approaching forgetting threshold; these become constraints for story generation, embedding review into reading context.
- Core assumption: Contextual exposure through stories can substitute for flashcard-style review without reducing retention.
- Evidence anchors:
  - [section 2] "The story contains the words selected by the SRS, allowing the user to review them seamlessly"
  - [section 4.4] Human evaluation showed significantly better word usage illustration (3.88 vs 1.24, p<0.0001)
  - [corpus] No corpus evidence on learning outcomes; retention improvements are not empirically demonstrated.
- Break condition: If learning retention is not actually improved by reading vs. flashcards, the core value proposition fails. This is not tested in the paper.

## Foundational Learning

- Concept: Lexically constrained decoding
  - Why needed here: Understanding why CBS (hard token masking during beam search) produces lower-quality output than prompt-based approaches.
  - Quick check question: Why might forcing a model to use specific tokens during generation harm fluency?

- Concept: Spaced Repetition Systems (SRS)
  - Why needed here: The system architecture assumes familiarity with how SRS algorithms schedule word review based on predicted forgetting curves.
  - Quick check question: What information does an SRS need to decide which words to review today?

- Concept: LLM-based evaluation
  - Why needed here: The paper relies on LLM-as-judge for quality metrics; understanding biases and limitations is critical.
  - Quick check question: Why might using Qwen 2.5 as both generator and evaluator inflate quality scores?

## Architecture Onboarding

- Component map: SRS Engine -> Prompting Module -> LLM Generator -> Constraint Verifier -> Rewriting Module -> Evaluator
- Critical path: SRS word selection → Prompt construction → Generation → Verification → Rewriting (up to 5 iterations) → Output
- Design tradeoffs:
  - Simple Prompting vs Planning: Simple is faster (1 LLM call), Planning produces more target word repetitions but costs 3x inference
  - Rewrite vs Rewrite Highlighted: Basic rewrite achieves lower OOV; Highlighted preserves more narrative complexity but allows more OOV
  - Temperature=0: Ensures reproducibility but reduces story diversity
- Failure signatures:
  - Low #|L|≥1 (target words missing): Prompt failed to communicate constraints; try Planning strategy
  - High OOV despite rewriting: Vocabulary list too restrictive for language level; expand permitted vocabulary
  - Incoherent plot with CBS baseline: Expected; use prompt-based generation instead
- First 3 experiments:
  1. Replicate English B1 Simple Prompting + Rewrite baseline; verify OOV <1% and #|L|≥1 >95%
  2. Ablate rewriting iterations: Compare 1, 3, 5 iterations on OOV rate to validate the 5-iteration design choice
  3. Test cross-lingual transfer: Generate Chinese stories with same prompts; expect higher OOV due to tokenizer differences (paper shows 1.72% vs 0.52% for English)

## Open Questions the Paper Calls Out
- How can bias mitigation techniques be effectively integrated into the story generation pipeline to address social biases inherent in the pretraining data?
- Can lexical constraint enforcement strategies be refined to eliminate out-of-vocabulary (OOV) words entirely without degrading text fluency?
- Does reading SRS-Stories result in measurably better vocabulary acquisition and retention compared to traditional flashcard-based Spaced Repetition Systems?
- Does generating stories based on topic-related word sets improve the coherence or learning efficiency compared to the random word selection used in the experiments?

## Limitations
- No empirical evidence that story-based learning improves vocabulary acquisition compared to flashcards
- Chinese results show significantly lower target word usage than English, suggesting language-specific limitations
- All automated quality metrics rely on LLM-as-judge, creating potential for evaluator bias
- Vocabulary coverage gaps persist at lower proficiency levels despite iterative rewriting

## Confidence
- **High Confidence**: Basic architecture works - multi-step prompting and iterative rewriting reduce OOV rates vs constrained beam search; human evaluation confirms grammatical correctness
- **Medium Confidence**: Three prompting strategies and three rewriting methods produce measurable quality differences, but performance ordering needs validation across more languages
- **Low Confidence**: Claims about personalized learning optimization and spaced repetition benefits lack empirical support; SRS component not validated as effective learning mechanism

## Next Checks
1. Conduct controlled study comparing SRS-Stories-generated stories against flashcard review for vocabulary retention over 2-4 weeks, measuring immediate comprehension and long-term recall
2. Generate stories for morphologically rich languages (Finnish, Turkish, Arabic) using same prompting strategies to test cross-lingual robustness of multi-step planning
3. Implement cross-evaluation where Qwen 2.5 judges Llama-generated stories and vice versa to identify potential evaluator bias and establish more reliable automated metrics