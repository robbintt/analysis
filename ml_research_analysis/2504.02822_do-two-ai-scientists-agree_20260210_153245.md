---
ver: rpa2
title: Do Two AI Scientists Agree?
arxiv_id: '2504.02822'
source_url: https://arxiv.org/abs/2504.02822
tags:
- mass
- systems
- terms
- scientists
- theories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether independently trained AI models
  learn the same physical theories when given the same data. To address this, the
  authors introduce MASS (Multiple AI Scalar Scientists), a generalized neural network
  framework that can learn scalar functions analogous to Hamiltonians or Lagrangians
  across multiple physical systems.
---

# Do Two AI Scientists Agree?

## Quick Facts
- arXiv ID: 2504.02822
- Source URL: https://arxiv.org/abs/2504.02822
- Reference count: 0
- Primary result: AI scientists learn correlated rather than identical physical theories, with Lagrangian dynamics emerging as the dominant accurate formulation in complex systems

## Executive Summary
This paper investigates whether independently trained AI models learn identical physical theories when given the same data, introducing MASS (Multiple AI Scalar Scientists) as a generalized neural network framework for learning scalar functions analogous to Hamiltonians or Lagrangians. The authors train MASS on various classical mechanics problems and analyze both single and multiple AI scientist setups to understand theory convergence and diversity. Key findings reveal that AI scientists learn correlated theories with strong similarities in internal activations, and that as system complexity increases, the number of distinct terms decreases while diversity increases, with Lagrangian dynamics emerging as the singular accurate family of descriptions.

## Method Summary
The authors develop MASS (Multiple AI Scalar Scientists), a neural network framework designed to learn scalar functions analogous to Hamiltonians or Lagrangians across multiple physical systems. They train MASS on classical mechanics problems including simple harmonic oscillators, pendulums, gravitational systems, and synthetic potentials, analyzing both single and multiple AI scientist configurations. The framework is tested across varying system complexities, from simple oscillators to higher-dimensional problems like the double pendulum, to examine theory convergence, diversity, and the emergence of Lagrangian-like formulations in richer theory spaces.

## Key Results
- AI scientists learn correlated rather than identical theories, with strong similarities in internal activations across different training seeds
- As system complexity increases, the number of distinct terms in learned theories decreases while diversity increases
- MASS transitions from Hamiltonian-like to Lagrangian-like formulations with increasing complexity, with Lagrangian dynamics emerging as the singular accurate family in richer theory spaces
- The framework successfully extends to higher-dimensional problems like the double pendulum, demonstrating general applicability

## Why This Works (Mechanism)
The convergence of AI scientists toward correlated theories stems from the underlying physical constraints embedded in the data, which act as attractors in the learning landscape. When multiple AI models are trained on the same physical system, they are all optimizing toward the same underlying symmetries and conservation laws, leading to similar activation patterns despite different initializations. The emergence of Lagrangian dynamics in complex systems likely reflects the mathematical convenience and generality of the Lagrangian formalism for handling constraints and generalized coordinates, making it a natural convergence point for diverse learning approaches trying to capture the same physical reality.

## Foundational Learning
1. Hamiltonian mechanics - why needed: Provides the mathematical framework for energy-based descriptions of physical systems; quick check: Can the model learn energy conservation from trajectories?
2. Lagrangian mechanics - why needed: Offers a more general framework for systems with constraints; quick check: Does the model naturally transition to Lagrangian form for complex systems?
3. Neural network activation patterns - why needed: Understanding how similar internal representations emerge across different training runs; quick check: Are activation similarities correlated with theory similarity?
4. Theory space exploration - why needed: Characterizing the landscape of possible physical descriptions AI can learn; quick check: How many distinct but valid theories exist for a given system?
5. Generalization across complexity - why needed: Testing whether learned principles scale from simple to complex systems; quick check: Does the model maintain accuracy when complexity increases?

## Architecture Onboarding
Component map: Input trajectories -> MASS neural network -> Scalar function output -> Physical theory extraction
Critical path: Data preprocessing → MASS training → Activation pattern analysis → Theory comparison
Design tradeoffs: Single vs. multiple AI scientists (computational cost vs. diversity understanding), Hamiltonian vs. Lagrangian formulation (mathematical simplicity vs. generality)
Failure signatures: Activation patterns diverging significantly across seeds suggests either insufficient data or overly complex theory space
First experiments: 1) Train two MASS models on simple harmonic oscillator with different seeds, compare activation patterns; 2) Test MASS on double pendulum, verify Lagrangian emergence; 3) Introduce noise to training data, assess robustness of learned theories

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but implicit questions include whether the observed convergence patterns hold for chaotic or high-dimensional systems, whether Lagrangian dominance persists in non-classical or quantum domains, and how the framework performs with noisy or incomplete data.

## Limitations
- Generalizability to truly chaotic or high-dimensional systems remains untested
- Claims about Lagrangian dynamics being the singular accurate family are limited to classical mechanics contexts
- The metric for measuring "distinctness" or "diversity" in learned theories lacks clear definition and validation
- The assumption that activation similarity correlates with theory similarity is plausible but unproven

## Confidence
- Confidence in correlated (not identical) theories: High
- Confidence in Lagrangian dominance for complex systems: Medium
- Confidence in general framework applicability: Low

## Next Checks
1. Test MASS on chaotic or high-dimensional systems (e.g., n-body problems with n>2) to evaluate scalability and robustness
2. Apply MASS to non-classical systems (e.g., quantum or relativistic mechanics) to determine if Lagrangian dominance persists
3. Introduce noisy, incomplete, or adversarial training data to assess the framework's resilience and whether learned theories remain interpretable