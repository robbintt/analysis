---
ver: rpa2
title: 'Knowledge-Driven Hallucination in Large Language Models: An Empirical Study
  on Process Modeling'
arxiv_id: '2509.15336'
source_url: https://arxiv.org/abs/2509.15336
tags:
- process
- language
- standard
- llms
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical reliability issue in large language
  models (LLMs) called knowledge-driven hallucination, where models prioritize their
  internal knowledge over explicit evidence in prompts. The study investigates this
  phenomenon in business process modeling, where processes often follow standard patterns,
  making LLMs likely to have strong pre-trained schemas.
---

# Knowledge-Driven Hallucination in Large Language Models: An Empirical Study on Process Modeling

## Quick Facts
- arXiv ID: 2509.15336
- Source URL: https://arxiv.org/abs/2509.15336
- Reference count: 30
- Primary result: LLMs prioritize pre-trained knowledge over explicit evidence, generating standard process models even when given conflicting input

## Executive Summary
This paper identifies a critical reliability issue in large language models (LLMs) called knowledge-driven hallucination, where models prioritize their internal knowledge over explicit evidence in prompts. The study investigates this phenomenon in business process modeling, where processes often follow standard patterns, making LLMs likely to have strong pre-trained schemas. A controlled experiment was conducted using standard, reversed, and shuffled process models to create deliberate conflicts between provided evidence and the model's background knowledge. The results show that LLMs consistently revert to standard process models even when given conflicting evidence, demonstrating a significant tendency for knowledge-driven hallucination. While prompt engineering can mitigate this issue to some extent, it does not eliminate it, highlighting the need for rigorous validation of AI-generated artifacts in evidence-based domains.

## Method Summary
The study evaluates knowledge-driven hallucination in LLMs by generating process models from textual descriptions and event logs that deliberately contradict standard business process schemas. Four business processes (Sales Order, Booking System, Complaint Handling, Internal Audit) are used, each with three variants: Standard (M+), Reversed (M-, causally inverted), and Shuffled (M*, random label mapping). Ten LLMs generate models using the ProMoAI framework under two prompt conditions: Standard and Strict Adherence. Behavioral-footprint similarity (via PM4Py) compares generated models against all three ground truth variants to identify hallucination instances where output similarity to M+ exceeds the matching ground truth.

## Key Results
- LLMs consistently revert to standard process models (M+) even when given reversed (M-) or shuffled (M*) inputs, demonstrating knowledge-driven hallucination
- Structured event log inputs reduce hallucination frequency compared to natural language descriptions (30 vs 40 instances)
- Strict adherence prompts mitigate but don't eliminate hallucination, reducing cases from 27 to 13 for text and 20 to 10 for logs
- No direct relationship between model size and hallucination resistance; some reasoning models (gemini-2.5-pro) perform poorly while smaller models (o4-mini) excel

## Why This Works (Mechanism)

### Mechanism 1: Pre-trained Schema Override
When LLMs possess strong internal representations of domain patterns, these schemas can systematically override contradictory evidence provided in prompts. The paper demonstrates that LLMs generate outputs more similar to standard process models (M+) even when given reversed (M-) or shuffled (M*) inputs. This occurs because the model's learned statistical patterns from pre-training dominate over the contextual signal from the prompt, particularly for well-documented domains like business processes.

### Mechanism 2: Structured Input Attenuation
Structured, unambiguous input formats (like event logs) reduce but do not eliminate knowledge-driven hallucination compared to natural language descriptions. Event logs provide stronger behavioral evidence through explicit sequence representation, reducing interpretive ambiguity. However, the persistence of hallucination in log-based generation (20 vs. 40 instances across both prompt types) indicates that structure alone cannot fully suppress schema override.

### Mechanism 3: Prompt-Based Constraint as Partial Mitigation
Explicit instructions to disregard background knowledge reduce hallucination frequency but cannot eliminate it, as the conflict is architectural rather than purely behavioral. Strict adherence prompts reduced hallucination cases from 27 to 13 for text and 20 to 10 for logs. However, the paper notes variation across models—some (like o3) showed marked improvement while others (like grok-3-fast) continued hallucinating frequently, suggesting the effectiveness depends on model-specific alignment characteristics.

## Foundational Learning

- **Hallucination Taxonomy**: Understanding the distinction between data-driven, training-driven, and inference-driven hallucination is necessary to accurately diagnose knowledge-driven hallucination specifically arising from knowledge-evidence conflict.
- **Behavioral Footprint Similarity**: The paper uses footprint-based similarity to compare generated models against ground truth variants. Understanding this metric's limitations is necessary for interpreting results, as it sacrifices absolute correctness assessment for relative comparison.
- **Schema vs. Instance Knowledge**: The paper's hypothesis depends on LLMs having "schemas" (generalized patterns) that conflict with specific instances (provided evidence). This distinction is not formally defined in transformer architectures, making it crucial to understand how models might store and access such information.

## Architecture Onboarding

- **Component map**: Input Layer (textual descriptions or event log abstractions) -> Processing Layer (ProMoAI framework converting to POWL language) -> Evaluation Layer (PM4Py behavioral-footprint similarity) -> Intervention Point (Standard vs. Strict Adherence prompts)
- **Critical path**: 1) Select process and generate standard artifacts (M+, D+, L+) 2) Create conflicting variants (reversed M-, shuffled M*) 3) Prompt LLM with each artifact variant under both prompt conditions 4) Compute similarity scores against all three ground truths 5) Identify hallucination cases (output most similar to M+ despite being generated from D-/D*/L-/L*)
- **Design tradeoffs**: Footprint similarity vs. formal conformance checking (chose similarity for relative comparison focus), standardized activity labels vs. open generation (fixed labels to isolate control-flow evaluation), four processes vs. broader coverage (limited process diversity)
- **Failure signatures**: High similarity to M+ when input was D- or D* (red cells in tables)—indicates schema override; low absolute scores even when "correct" variant is matched—indicates degraded performance from knowledge-evidence conflict; high LiveBench score but poor atypical-process performance—indicates benchmark scores don't predict evidence fidelity
- **First 3 experiments**: 1) Replicate with a domain that has no standardized patterns (e.g., fictional processes with novel activity names) 2) Test whether Chain-of-Thought prompting before generation reduces hallucination by forcing explicit evidence tracing 3) Evaluate whether fine-tuning on "unusual but correct" process examples improves fidelity to atypical inputs

## Open Questions the Paper Calls Out

- **Fine-tuning effectiveness**: Can fine-tuning or architectural modifications reduce knowledge-driven hallucination more effectively than prompting alone? The study only tested prompt engineering, which reduced but did not eliminate hallucination. Experiments comparing prompting against techniques like LoRA fine-tuning or retrieval-augmented generation would resolve this.
- **Domain generalizability**: Does knowledge-driven hallucination manifest in other structured generation tasks beyond process modeling? The study was confined to BPM; replication in domains like code synthesis, database schema design, or legal contract generation would test the phenomenon's generality.
- **Model-specific resistance factors**: What architectural or training factors explain why certain models (e.g., o4-mini) resist knowledge-driven hallucination better than larger or reasoning-capable models? The study evaluates outcomes but doesn't investigate mechanisms; controlled experiments across model families would address this.

## Limitations

- The study focuses on business process modeling, which may have stronger standardized patterns than other domains, potentially limiting generalizability
- The experimental methodology uses footprint similarity rather than formal conformance checking, which may miss certain types of modeling errors
- The study only tests prompt-based mitigation strategies, not exploring architectural or fine-tuning approaches that might be more effective

## Confidence

- **High Confidence**: The empirical observation that LLMs revert to standard process models when given conflicting evidence (Section 4 results)
- **Medium Confidence**: The interpretation that this represents a fundamental architectural conflict between pre-trained knowledge and prompt evidence
- **Medium Confidence**: The claim that structured inputs (event logs) reduce but don't eliminate hallucination

## Next Checks

1. **Domain generalization test**: Replicate the experiment with domains lacking standardized patterns (e.g., fictional processes with novel activity names) to test whether schema override risk decreases as hypothesized
2. **Reasoning pathway investigation**: Implement Chain-of-Thought prompting before generation to force explicit evidence tracing, determining whether intermediate reasoning affects hallucination rates
3. **Fine-tuning intervention**: Evaluate whether fine-tuning on "unusual but correct" process examples improves fidelity to atypical inputs, distinguishing between architectural limitations and training data gaps