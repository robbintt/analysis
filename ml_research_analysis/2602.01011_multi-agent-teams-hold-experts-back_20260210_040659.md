---
ver: rpa2
title: Multi-Agent Teams Hold Experts Back
arxiv_id: '2602.01011'
source_url: https://arxiv.org/abs/2602.01011
tags:
- expert
- expertise
- teams
- team
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether multi-agent LLM teams can achieve\
  \ \"strong synergy\"\u2014matching or exceeding the performance of their best individual\
  \ member\u2014when coordination is unconstrained. Across human-inspired psychology\
  \ tasks and frontier ML benchmarks, teams consistently fail to match expert performance,\
  \ with synergy gaps ranging from 8.1% to 37.6%."
---

# Multi-Agent Teams Hold Experts Back

## Quick Facts
- **arXiv ID:** 2602.01011
- **Source URL:** https://arxiv.org/abs/2602.01011
- **Reference count:** 40
- **Primary result:** Multi-agent LLM teams consistently fail to match their best member's performance, with synergy gaps of 8.1-37.6% due to integrative compromise

## Executive Summary
Multi-agent teams of LLMs fail to achieve "strong synergy"—matching or exceeding their best member's performance—when coordination is unconstrained. Across psychology tasks and ML benchmarks, teams systematically underperform expert individuals by 8.1-37.6%. The primary bottleneck is not identifying expertise but leveraging it: teams engage in "integrative compromise," averaging expert and non-expert views rather than appropriately weighting superior knowledge. This consensus-seeking behavior correlates negatively with performance and worsens with team size, though it provides robustness to adversarial agents. The findings reveal a fundamental trade-off between expertise leveraging and manipulation resistance, suggesting current alignment procedures may need to enable contextual expertise utilization without sacrificing robustness.

## Method Summary
The study tests self-organizing multi-agent teams on intellective tasks with demonstrable correct answers. Teams consist of heterogeneous models with documented performance differences, using a four-phase protocol: individual opinion collection, discussion initialization, 4 rounds of collaborative discussion, and final answer elicitation. Three information conditions test whether expertise identification is the bottleneck. The "Reveal Expert" condition uses aggressively-tuned deference prompts. Synergy gaps measure performance relative to the best individual, while conversational analysis codes behaviors as epistemic deference (ED), integrative compromise (IC), supporting peer (SP), or epistemic flexibility (EF).

## Key Results
- Teams consistently fail to match expert performance, with synergy gaps ranging from 8.1% to 37.6% across all tasks
- Consensus-seeking behavior increases with team size and correlates negatively with performance
- When explicitly told who the expert is, teams still engage in integrative compromise rather than epistemic deference
- Integrative compromise correlates positively with synergy gaps (NASA: r=0.55, p<0.001)
- Teams demonstrate robustness to adversarial input, with minimal performance degradation across configurations

## Why This Works (Mechanism)

### Mechanism 1
Self-organizing LLM teams default to integrative compromise rather than epistemic deference. Non-expert agents treat expert opinions as additional evidence to average rather than authority to adopt wholesale, producing middle-ground positions that dilute superior knowledge. This behavior stems from post-training alignment procedures that optimize for agreeableness. Evidence shows integrative compromise correlates positively with synergy gaps (NASA: r=0.55, p<0.001). The mechanism breaks down when tasks require value-based negotiation rather than objective expertise.

### Mechanism 2
Expertise dilution scales with team size. Larger teams increase "compromise pressure"—each additional voice creates more pull away from expert positions toward consensus middle ground, systematically degrading performance. The effect stems from conversational dynamics, not model capability limits. Team size shows statistically significant positive correlations with synergy gap across all tasks (all p<0.05). This dilution applies to unconstrained group discussion but may differ with explicit voting or aggregation rules.

### Mechanism 3
Consensus-seeking provides adversarial robustness as a side effect. The same integrative compromise that dilutes expert signal also dilutes adversarial input—when minority positions deviate substantially from majority, averaging filters extreme inputs. Adversarial success requires high-leverage influence on final output, which consensus mechanisms reduce. Teams demonstrate robustness to single adversarial agents, though coordinated attacks or exploitation of deliberation order may degrade protection.

## Foundational Learning

- **Concept: Strong vs. Weak Synergy**
  - Why needed here: Understanding the evaluation criterion—strong synergy (matching best member) vs. weak synergy (matching average)—is essential to interpret why LLM teams "fail" despite often achieving weak synergy.
  - Quick check question: If a team of 4 agents averages 60% accuracy individually but their best member gets 80%, what accuracy must the team achieve to demonstrate strong synergy?

- **Concept: Epistemic Deference (Preemption Thesis)**
  - Why needed here: The paper distinguishes deferring to expertise (adopting expert view directly) from integrating expertise (treating it as weighted evidence). This frames the conversational analysis.
  - Quick check question: When an expert says "X is correct because of domain knowledge," does evidence integration or epistemic deference produce better outcomes?

- **Concept: Intellective Tasks**
  - Why needed here: The paper's claims apply specifically to tasks with demonstrably correct answers, not value-based or creative tasks where compromise may be appropriate.
  - Quick check question: Is "design a marketing slogan" an intellective task? Why or why not?

## Architecture Onboarding

- **Component map:** Heterogeneous models with expertise differences → Four-phase protocol (individual collection → discussion initialization → 4 rounds deliberation → random agent final answer) → Information conditions (No Information, Expert Not Mentioned, Reveal Expert) → Synergy gap evaluation

- **Critical path:**
  1. Define task type (intellective vs. creative) to determine if expertise leveraging is appropriate
  2. Select heterogeneous model team with documented performance variance
  3. Run individual baselines before any team deliberation
  4. If using Reveal Expert prompts, validate they don't induce over-deference on control tasks
  5. Track conversation behaviors (ED, IC, SP, EF codes) to diagnose failure modes

- **Design tradeoffs:**
  - **Self-organization vs. fixed roles:** Unconstrained deliberation reveals expertise leveraging failures but offers flexibility; fixed workflows avoid the problem but require upfront design
  - **Team size vs. performance:** Larger teams provide robustness but dilute expertise; optimal size depends on adversarial risk vs. expertise asymmetry
  - **Alignment objectives:** Current agreeableness-focused alignment enables adversarial robustness at the cost of expertise utilization

- **Failure signatures:**
  - Team output converges to member average despite clear expert identification
  - Non-experts propose "compromise" or "middle ground" positions in conversations
  - Experts modify positions to accommodate group feedback rather than persisting
  - Performance degrades when team size increases from 2→4→8 agents

- **First 3 experiments:**
  1. **Baseline diagnostic:** Run 4-agent team on intellective task with individual baselines; compute synergy gap and analyze conversation for IC vs. ED patterns
  2. **Reveal Expert ablation:** Compare "Expert Not Mentioned" vs. "Reveal Expert" conditions; a large gap indicates identification failure, small gap indicates leveraging failure
  3. **Team size scaling:** Test 2, 4, 8 agent teams; confirm expertise dilution correlation; if correlation is absent, the self-organization protocol differs from paper's setup

## Open Questions the Paper Calls Out

### Open Question 1
Can alignment procedures be modified to enable contextual epistemic deference while preserving adversarial robustness? The paper states "developing training procedures that enable contextual expertise leveraging without sacrificing robustness remains an open challenge." This trade-off between consensus-seeking (which provides robustness) and expertise leveraging needs resolution through experiments comparing base models against RLHF-aligned variants on both metrics.

### Open Question 2
Is the integrative compromise behavior caused by alignment procedures, or does it emerge from pretraining? The paper notes "our attribution of consensus-seeking behavior to alignment is correlational—we do not compare aligned models to base counterparts." Without comparing aligned vs. base models, the root cause remains speculative. Controlled comparison of expertise leveraging in base vs. aligned versions would resolve this.

### Open Question 3
Do alternative interaction protocols (e.g., structured debate, confidence-weighted voting) mitigate the expertise dilution effect? The paper studies only unconstrained deliberation and notes prior work enforces coordination "through fixed roles, workflows, or aggregation rules," leaving open whether hybrid approaches could preserve self-organization benefits while enabling expertise leveraging. Systematic comparison across interaction protocols varying in structure would provide answers.

## Limitations

- Results apply specifically to intellective tasks with demonstrable correct answers, not extending to value-based or creative tasks where compromise is appropriate
- The study uses proprietary models (GPT-5, Claude Opus 4.5) with unknown architecture details, making generalization to open-source models uncertain
- Adversarial robustness is demonstrated only for single bad actors, not coordinated or sophisticated attacks that could exploit deliberation order

## Confidence

- **High confidence:** The existence of synergy gaps (8.1-37.6%) and the negative correlation between team size and performance are well-established across multiple tasks and conditions
- **Medium confidence:** The mechanism of integrative compromise is supported by conversational analysis, but the exact contribution of alignment procedures versus architectural factors remains unclear
- **Medium confidence:** The adversarial robustness claim is demonstrated for single bad actors, but the scope of protection against coordinated or sophisticated attacks is unknown

## Next Checks

1. **Model transfer validation:** Replicate the core finding (expertise dilution in teams) using open-source models (e.g., Llama-3.1, Qwen2.5) with comparable capabilities to test whether the effect is model-specific or a general phenomenon

2. **Adversarial robustness stress test:** Test team performance under coordinated adversarial attacks where multiple agents collude to manipulate consensus, examining whether the current robustness extends to more sophisticated threat models

3. **Structured protocol comparison:** Implement and test explicit expertise-weighting protocols (e.g., voting, weighted aggregation) against the self-organizing baseline to quantify the performance cost of unconstrained deliberation versus structured coordination