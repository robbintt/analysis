---
ver: rpa2
title: 'Sharing Knowledge without Sharing Data: Stitches can improve ensembles of
  disjointly trained models'
arxiv_id: '2512.17592'
source_url: https://arxiv.org/abs/2512.17592
tags:
- performance
- data
- network
- networks
- stitches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates asynchronous collaboration in deep learning,\
  \ where parties share trained models without sharing raw data or coordinating training.\
  \ It addresses limitations of federated learning, which requires synchronized, online\
  \ model updates, by proposing stitching\u2014a method to combine intermediate representations\
  \ from independently trained models."
---

# Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models

## Quick Facts
- **arXiv ID:** 2512.17592
- **Source URL:** https://arxiv.org/abs/2512.17592
- **Reference count:** 40
- **Primary result:** Stitched ensembles of disjointly trained models match or exceed federated learning performance while improving generalization

## Executive Summary
This work investigates asynchronous collaboration in deep learning, where parties share trained models without sharing raw data or coordinating training. It addresses limitations of federated learning, which requires synchronized, online model updates, by proposing stitching—a method to combine intermediate representations from independently trained models. A new training approach, double-batched training, improves stitch robustness by accounting for downstream effects during training. Experiments on medical image segmentation datasets (cervical cancer and polyp detection) show that stitched ensembles match or exceed performance of data-sharing methods (merging datasets, federated learning) while maintaining better generalization. Stitching also outperforms fine-tuning and basic ensembles, which suffer from performance degradation on out-of-distribution data. Results demonstrate that asynchronous model sharing via stitching is a viable alternative for collaborative learning when data privacy or coordination constraints exist.

## Method Summary
The method trains two parent networks independently on disjoint datasets, then combines them through stitching layers at matched intermediate positions. A modified Hirschberg's algorithm matches layers between networks based on positional similarity and connectivity. Stitching layers (1×1 convolutions or linear layers) translate between feature representations while parent networks remain frozen. Double-batched training improves robustness by maintaining both original and stitched feature maps through the network, allowing stitches to adapt to downstream effects. The final network averages original and stitched paths at switch points, combining knowledge from both networks at the feature level rather than the output level.

## Key Results
- Stitched ensembles match or exceed federated learning performance while maintaining better generalization on out-of-distribution data
- Double-batched training substantially mitigates performance degradation when training multiple stitches simultaneously
- Intermediate feature averaging recovers performance on own data while maintaining improved generalization, addressing the ensemble trade-off
- Stitching outperforms fine-tuning and basic ensembles, which degrade on the other party's data

## Why This Works (Mechanism)

### Mechanism 1: Feature Map Translation via Stitching Layers
Stitching layers translate between intermediate representations of independently trained networks, even when architectures differ, enabling knowledge transfer without raw data exchange. Small trainable layers (1×1 convolutions or linear layers) are inserted at matched positions between networks, learning to transform activations from one representation space to another while parent networks remain frozen. Core assumption: Intermediate representations encode semantically meaningful features that can be mapped between networks despite different initializations and training data.

### Mechanism 2: Double-batched Training for Co-adaptation
Double-batched training improves stitch robustness when multiple stitches are trained simultaneously by accounting for downstream activation impacts. The batch dimension stores both the original reference feature map and the stitched variant. Losses are computed at subsequent switch points, allowing gradients to flow through downstream layers and enabling stitches to adapt to each other's corrections within a single forward/backward pass. Core assumption: Not all reconstruction errors are equally important—errors that survive through activation functions matter more than those nullified by them.

### Mechanism 3: Intermediate Feature Averaging
Averaging original and stitched intermediate representations recovers performance on own data while maintaining improved generalization, addressing the ensemble trade-off. At matched switch positions, both representations are averaged, combining knowledge from both networks at the feature level, correcting errors before they propagate. Core assumption: Intermediate features encoding similar semantic concepts can be meaningfully combined when representation-space symmetries are corrected by the stitching layer.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) for Neural Networks**
  - Why needed here: Architectures like U-Net have parallel branches and cannot be represented as simple sequential layer stacks. DAG representation with vertex-edge formalism enables proper matching across architectures with different depths and branching structures.
  - Quick check question: Given a U-Net architecture, can you identify which layers operate at the same spatial resolution and trace how information flows from encoder through skip connections to decoder?

- **Feature Map Symmetries and Permutation Invariance**
  - Why needed here: Weight averaging fails because identical functions can be represented by different parameter configurations. Understanding this explains why the paper learns transformation layers rather than directly averaging weights.
  - Quick check question: If two MLPs have identical architecture and performance but were initialized differently, why would averaging their weights likely produce a non-functional network?

- **Catastrophic Forgetting in Fine-tuning**
  - Why needed here: The paper contrasts stitching with fine-tuning, which risks destroying previously learned features when adapting to new data. This failure mode motivates keeping parent networks frozen during stitch training.
  - Quick check question: What happens to a pretrained model's original capabilities if you continue training it on a new dataset with a high learning rate without regularization?

## Architecture Onboarding

- **Component map:** Two parent networks (Network A, Network B) represented as DAGs with vertices (layers) and edges (data flow), connected by stitching layers at matched positions, with switch layers that average original and stitched paths

- **Critical path:**
  1. Train parent networks independently using nnUNet on separate datasets
  2. Run acyclic matching to identify compatible layer pairs at same visual scale
  3. Construct combined graph with bidirectional stitches and switches at matched positions
  4. Train all stitches simultaneously using double-batched procedure with AdamW, parent networks frozen and in eval mode
  5. Select stitch position using Rule 2 (lowest mean rank across folds on validation set)

- **Design tradeoffs:**
  - Stitch position selection: Early positions (<20) show minimal effect; middle-to-late positions (50-65) perform best; final position degrades
  - Training methodology: Direct matching is simpler but degrades sharply with multiple active stitches; double-batched requires 2× batch memory but enables co-adaptation
  - Inference cost: Stitched ensemble has computational cost similar to full ensemble plus stitch overhead

- **Failure signatures:**
  - Basic ensemble on own data: Performance degrades on larger party's data because weaker model drags down averaged predictions
  - Direct matching with many stitches: Dice drops from ~0.85 to ~0.55 as stitch count increases
  - Fine-tuning with default learning rate: Catastrophic forgetting—model reverts to scratch-training equivalent performance
  - Poor stitch position: Last stitch index consistently worst across all datasets

- **First 3 experiments:**
  1. Train separate nnUNet models on each party's data; evaluate on both own and other party's test sets to quantify the generalization gap
  2. For one fold pair, train stitches using both direct matching and double-batched procedures; evaluate networks with 0, 25, 50, 75, 100 randomly selected active stitches
  3. Train stitches at all valid positions; plot validation Dice and HD95 vs. stitch index to identify optimal region and confirm positional trends

## Open Questions the Paper Calls Out
- Can knowledge distillation effectively compress the stitched ensemble into a smaller network without losing the generalization benefits?
- How robust is the proposed stitching method when combining models trained on datasets with differing label definitions or delineation protocols?
- How does the matching and stitching performance scale when combining models with significantly divergent architectures?

## Limitations
- The stitching mechanism relies heavily on architectural similarity between parent networks, with performance degradation observed on certain datasets
- Double-batched training requires significant memory overhead, limiting practical applicability on standard GPU configurations
- Claims about stitching's robustness to architectural differences are contradicted by results showing dataset-specific limitations

## Confidence
- **High Confidence:** The core claim that intermediate feature averaging via stitching outperforms basic ensemble methods on generalization metrics is well-supported by ablation studies
- **Medium Confidence:** The double-batched training methodology's superiority over direct matching is demonstrated within the paper but lacks external validation
- **Low Confidence:** Claims about stitching's robustness to architectural differences are theoretically supported but contradicted by the CVC-ClinicDB results

## Next Checks
1. Train parent networks with substantially different architectures and measure stitching performance degradation compared to architectural matches
2. Implement double-batched training with varying batch sizes to quantify the memory overhead and determine practical limits
3. Test stitching across additional medical imaging datasets with varying characteristics to establish robustness bounds and identify failure conditions systematically