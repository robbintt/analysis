---
ver: rpa2
title: 'Structure is Supervision: Multiview Masked Autoencoders for Radiology'
arxiv_id: '2511.22294'
source_url: https://arxiv.org/abs/2511.22294
tags:
- mvmae
- across
- chexpert
- label
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of learning robust medical imaging
  representations when labeled data is scarce and clinical data is structured. It
  proposes MVMAE, a self-supervised framework that leverages the natural multi-view
  organization of radiology studies by combining masked image reconstruction with
  cross-view alignment, learning view-invariant and disease-relevant features.
---

# Structure is Supervision: Multiview Masked Autoencoders for Radiology

## Quick Facts
- arXiv ID: 2511.22294
- Source URL: https://arxiv.org/abs/2511.22294
- Reference count: 30
- The paper addresses the challenge of learning robust medical imaging representations when labeled data is scarce and clinical data is structured. It proposes MVMAE, a self-supervised framework that leverages the natural multi-view organization of radiology studies by combining masked image reconstruction with cross-view alignment, learning view-invariant and disease-relevant features. An extension, MVMAE-V2T, incorporates radiology reports as auxiliary supervision during pretraining while preserving vision-only inference. Evaluated on MIMIC-CXR, CheXpert, and PadChest for disease classification, MVMAE consistently outperforms supervised and vision-language baselines. MVMAE-V2T provides additional gains, especially in low-label regimes, achieving up to 80% AUROC with only 5k labeled samples versus 10–50k for baselines. MVMAE also yields better-calibrated predictions and maintains performance gains when only a single view is available at inference. These results demonstrate that exploiting study-level structure and optional text supervision improves representation quality, label efficiency, and generalization in medical imaging.

## Executive Summary
The paper addresses the challenge of learning robust medical imaging representations when labeled data is scarce and clinical data is structured. It proposes MVMAE, a self-supervised framework that leverages the natural multi-view organization of radiology studies by combining masked image reconstruction with cross-view alignment, learning view-invariant and disease-relevant features. An extension, MVMAE-V2T, incorporates radiology reports as auxiliary supervision during pretraining while preserving vision-only inference. Evaluated on MIMIC-CXR, CheXpert, and PadChest for disease classification, MVMAE consistently outperforms supervised and vision-language baselines. MVMAE-V2T provides additional gains, especially in low-label regimes, achieving up to 80% AUROC with only 5k labeled samples versus 10–50k for baselines. MVMAE also yields better-calibrated predictions and maintains performance gains when only a single view is available at inference. These results demonstrate that exploiting study-level structure and optional text supervision improves representation quality, label efficiency, and generalization in medical imaging.

## Method Summary
MVMAE is a self-supervised pretraining framework for chest X-ray classification that leverages multi-view study structure. It uses a ViT-Base encoder with a lightweight decoder to reconstruct 90% masked patches per view, while aligning encoder outputs across views within the same study using MSE loss. An extension, MVMAE-V2T, adds an autoregressive text decoder trained to predict radiology report tokens from visual embeddings, providing semantic grounding without requiring text at inference. Downstream finetuning uses late-fusion ensemble averaging across views. The model is pretrained on MIMIC-CXR, CheXpert, PadChest, and ChestX-ray datasets, then evaluated on multi-label disease classification across 14 CheXpert labels.

## Key Results
- MVMAE consistently outperforms supervised and vision-language baselines on MIMIC-CXR, CheXpert, and PadChest for 14-label disease classification.
- MVMAE-V2T with report supervision provides additional gains, especially in low-label regimes, achieving up to 80% AUROC with only 5k labeled samples versus 10–50k for baselines.
- MVMAE yields better-calibrated predictions and maintains performance gains when only a single view is available at inference.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-view alignment converts clinical redundancy into self-supervision for view-invariant representations.
- Mechanism: Multiple projections (frontal/lateral/unknown) from the same study share anatomical structure. By minimizing pairwise distance between encoder outputs across views via L_Align, the model learns features that persist despite projection angle differences.
- Core assumption: Views within a study share semantically meaningful correspondence beyond superficial pixel similarity.
- Evidence anchors:
  - [abstract] "MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal."
  - [section 4.1] "L(i)_Align = 1/(1−α) * 1/|P(i)| * Σ d_MSE(z_u, z_v)" defines the alignment loss over all view pairs.
  - [corpus] Weak direct evidence; related work "Alignment Unlocks Complementarity" supports multi-view alignment in other domains but not radiology specifically.
- Break condition: If studies contain uncorrelated or mislabeled views (e.g., wrong patient data mixed in), alignment degrades representations.

### Mechanism 2
- Claim: High masking ratio (90%) with per-view reconstruction preserves fine-grained anatomy while learning global context.
- Mechanism: Masking forces the encoder to reason about missing regions using visible patches and cross-view information, learning both local textures and global anatomical structure.
- Core assumption: Masked patches are reconstructable from unmasked patches and cross-view context.
- Evidence anchors:
  - [abstract] "masked image reconstruction with cross-view alignment"
  - [section 4.1] "During pretraining, we apply a high masking ratio of 90%."
  - [corpus] "HiMAE" and "Mamba-3D as Masked Autoencoders" support hierarchical/resolution-aware masking in medical time-series and ultrasound, suggesting domain-specific masking matters.
- Break condition: If pathology is entirely confined to masked regions with no cross-view or contextual cues, reconstruction fails to encode disease-relevant features.

### Mechanism 3
- Claim: Text-as-auxiliary-supervision (MVMAE-V2T) enriches semantic grounding without inference-time text dependency.
- Mechanism: A captioning-style objective trains a decoder to predict report tokens from visual embeddings, pushing the encoder toward disease-relevant semantics while keeping the vision encoder standalone at inference.
- Core assumption: Radiology reports contain accurate, disease-relevant descriptions that correlate with visual features.
- Evidence anchors:
  - [abstract] "MVMAE-V2T...incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference."
  - [section 4.2] "L(i)_V2T = L_Rec + β·L_Align + L_CE" shows text loss added but not replacing image objectives.
  - [corpus] "MRG-R1" and "ChestX-Reasoner" emphasize structured reasoning from reports, supporting text as supervision—but these focus on generation, not encoder-only pretraining.
- Break condition: If reports are noisy, incomplete, or mismatched to images, the cross-entropy loss adds noise rather than signal.

## Foundational Learning

### Self-supervised learning (SSL)
- Why needed here: MVMAE avoids expensive expert labels by deriving supervision from data structure (multi-view pairs, text reports).
- Quick check question: Can you explain why contrastive learning differs from reconstruction-based SSL?

### Vision Transformers (ViT) and patch-based processing
- Why needed here: The encoder/decoder operate on 16×16 patches; masking and reconstruction occur at the patch level.
- Quick check question: How does a ViT tokenize an image, and where does positional information enter?

### Multi-label classification with imbalanced data
- Why needed here: Downstream evaluation uses 14 CheXpert labels with heavy class imbalance; collapse of "negative/uncertain/not mentioned" into 0-class affects training.
- Quick check question: Why might AUROC be preferred over accuracy for imbalanced medical labels?

## Architecture Onboarding

### Component map
Encoder (ViT-Base) -> Lightweight Decoder (masked patch reconstruction) -> Modality Embeddings (frontal/lateral/unknown) -> Cross-view Alignment Loss (MSE between views) -> (V2T only) Text Decoder (autoregressive captioning)

### Critical path
1. Input study → group views → apply random 90% mask per view.
2. Encoder processes visible patches per view with modality + positional embeddings.
3. Compute L_Rec (masked reconstruction) and L_Align (pairwise MSE across view embeddings).
4. (V2T only) Add L_CE for report token prediction.
5. Backpropagate combined loss; finetune downstream with late-fusion view ensemble.

### Design tradeoffs
- **High masking (90%) vs. lower ratios**: More context reasoning but harder reconstruction; tuned for radiology textures.
- **Beta annealing for alignment weight**: Starts at 0 to stabilize early training; increases later.
- **Single vs. multi-view at inference**: Multi-view pretraining helps even single-view inference, but gains larger with two views.

### Failure signatures
- Alignment loss dominates early → collapse to trivial embeddings; fix with beta annealing.
- Per-label AUROC variance across datasets → check label distribution and harmonization; PadChest required extensive mapping.
- CheXagent appears stronger on specific labels → may reflect validation split imbalance (single positive study), not model failure.

### First 3 experiments
1. Reproduce the Independent ablation (β=0) vs. MVMAE on a small subset to verify alignment benefit.
2. Run linear probing on 5K labeled samples comparing MVMAE, MVMAE-V2T, and Supervised baseline to confirm label efficiency claims.
3. Ablate number of views at inference (1 vs. 2) on studies with exactly two views to verify single-view robustness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the cross-view alignment strategy generalize to 3D volumetric modalities like multi-sequence MRI or CT?
- Basis in paper: [explicit] The authors explicitly state the strategy "could extend to any life-science domain that offers repeated or complementary scans, such as longitudinal magnetic resonance imaging (MRI) or multi-sequence computed tomography (CT)."
- Why unresolved: The current validation is restricted to 2D chest radiographs (frontal/lateral). It is unclear if patch-based alignment scales effectively to the complexity of 3D volumetric relationships or different sequence types.
- What evidence would resolve it: Pretraining MVMAE on a multi-sequence MRI dataset (e.g., BraTS) and evaluating downstream segmentation or classification performance against 3D-specific baselines.

### Open Question 2
- Question: Can the framework be adapted to improve performance on rare or textually ambiguous conditions where semantic grounding is noisy?
- Basis in paper: [explicit] The results section notes that "labels for rare or textually ambiguous conditions remain challenging," even though the model outperforms baselines on frequent conditions.
- Why unresolved: The current improvement is driven primarily by visually localized, high-prevalence pathologies. The reliance on study-level consistency may not provide sufficient signal for rare diseases with subtle or inconsistent cross-view presentations.
- What evidence would resolve it: A targeted ablation study analyzing per-label AUROC specifically for low-prevalence classes, or the introduction of a re-weighting mechanism for rare findings during the alignment loss calculation.

### Open Question 3
- Question: Does the choice of MSE for the alignment objective restrict the representation space compared to contrastive losses?
- Basis in paper: [inferred] The paper utilizes Mean Squared Error (MSE) for cross-view alignment ($d_{MSE}$) but does not ablate this choice against other distance metrics or contrastive objectives used in related work.
- Why unresolved: MSE enforces strict point-wise correspondence which might be overly rigid for anatomical variations between views, potentially failing to capture useful non-linear relationships that contrastive methods preserve.
- What evidence would resolve it: Comparing the downstream performance of MSE alignment against a contrastive alignment head (e.g., InfoNCE) using identical pretraining data and backbone.

## Limitations
- Dataset and label quality: Relies on harmonized labels across datasets with potential noise and mapping inconsistencies.
- Hyperparameter and training details: Key parameters like alignment loss weight (β), annealing schedule, optimizer settings, and pretraining duration are not fully specified.
- Single-view generalization: Claims about 5k-label efficiency depend on dataset splits and may not be robust across all conditions.

## Confidence
- Multi-view structure as supervision: High confidence
- Label efficiency and AUROC gains: Medium confidence
- MVMAE-V2T semantic grounding: Medium confidence
- Single-view robustness: Medium confidence

## Next Checks
1. Ablate alignment loss weight (β) across a range (0.0, 0.5, 1.0, 2.0) on MIMIC-CXR to determine optimal and robust settings, and whether gains persist under different weights.
2. Test single-view inference robustness by evaluating MVMAE and baselines on studies with exactly one view (not just one available at inference), to confirm that multi-view pretraining genuinely helps when only single views are present in training and test data.
3. Replicate label efficiency results on a held-out, stratified split of CheXpert with exactly 5k labeled samples, ensuring the validation set contains more than one positive case per label to avoid split-induced bias.