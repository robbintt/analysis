---
ver: rpa2
title: 'An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating
  the Impact of Noisy Annotations'
arxiv_id: '2505.16705'
source_url: https://arxiv.org/abs/2505.16705
tags:
- noise
- concept
- uni00000013
- concepts
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations

## Quick Facts
- **arXiv ID**: 2505.16705
- **Source URL**: https://arxiv.org/abs/2505.16705
- **Authors**: Seonghwan Park, Jueun Mun, Donghyun Oh, Namhoon Lee
- **Reference count**: 40
- **Primary result**: Sharpness-Aware Minimization (SAM) and uncertainty-guided intervention significantly mitigate the performance drop of Concept Bottleneck Models (CBMs) under noisy concept annotations.

## Executive Summary
This paper investigates how noisy concept annotations degrade Concept Bottleneck Models (CBMs) and proposes mitigation strategies. The authors identify that degradation is not uniform but concentrated in a "susceptible set" of concepts whose accuracy declines disproportionately. They show that SAM training stabilizes concept prediction, while uncertainty-guided intervention at inference time corrects corrupted concepts, effectively restoring performance. Empirical results on CUB and AwA2 datasets demonstrate significant improvements in task accuracy under varying noise levels.

## Method Summary
The study analyzes CBMs under concept annotation noise by first identifying a "susceptible set" of concepts whose accuracy degrades disproportionately. The authors propose Sharpness-Aware Minimization (SAM) for robust training and uncertainty-guided intervention for inference-time correction. The CBM architecture uses InceptionV3 to predict concepts and a linear layer to predict targets. Noise is injected by flipping concept labels with probability γ. SAM is applied to the concept predictor with ρ ∈ {0.01, 0.05, 0.1}. At inference, concepts are corrected based on predictive entropy ranking. Evaluation uses task accuracy, concept accuracy, and Concept Alignment Score (CAS).

## Key Results
- Accuracy degradation in CBMs under noise is non-uniform, with a small "susceptible set" of concepts accounting for most performance loss.
- SAM training significantly improves concept accuracy in the susceptible set (up to 20% absolute improvement) and overall task accuracy.
- Uncertainty-guided intervention (UCP) at inference achieves comparable results to oracle intervention (correcting all corrupted concepts), with effectiveness increasing as more uncertain concepts are corrected.

## Why This Works (Mechanism)

### Mechanism 1: Susceptible Set Concentration
The overall degradation of CBMs under noise is disproportionately driven by a small subset of "susceptible" concepts rather than uniform error. When noise is introduced, concept accuracy degrades non-uniformly, with distinct minority concepts suffering disproportionately large declines. This susceptibility correlates with "representation misalignment," where noise causes the target predictor to assign high weights to irrelevant concepts while diminishing the weights of key informative concepts. Core assumption: Concept annotations have underlying semantic ambiguity or frequency imbalances that make specific concepts more vulnerable to label flipping.

### Mechanism 2: Sharpness-Aware Robustness
Sharpness-Aware Minimization (SAM) improves robustness specifically by stabilizing the learning of the "susceptible" concepts. Standard training may overfit to noisy labels by converging to sharp minima, while SAM seeks flat loss landscape regions. Theoretical analysis suggests SAM acts as an adaptive ℓ₂ regularizer on intermediate activations and final weights, dampening the gradient contribution of noisy samples and preventing aggressive fitting to corrupted labels of susceptible concepts. Core assumption: The standard assumption in SAM literature that flat minima generalize better and are less sensitive to label noise perturbations holds true for the concept prediction bottleneck.

### Mechanism 3: Uncertainty as a Proxy for Susceptibility
Predictive entropy (uncertainty) can reliably approximate the "susceptibility" of a concept, allowing for efficient inference-time intervention without access to ground truth. Concepts that are susceptible to noise tend to exhibit higher predictive uncertainty (entropy) at inference time. By ranking concepts by entropy and intervening on the most uncertain ones, the model approximates the oracle strategy of correcting the actual "susceptible set," restoring the representation alignment needed for the target predictor. Core assumption: Noise induces uncertainty in the model's output distribution (high entropy) and this correlation is monotonic for susceptible concepts.

## Foundational Learning

**Concept: Concept Bottleneck Models (CBMs)**
*Why needed here:* This is the base architecture. You must understand that the model splits into two distinct stages: g(x) → ĉ (concept predictor) and f(ĉ) → ŷ (target predictor). The paper analyzes how noise specifically attacks the interface between these two stages.
*Quick check question:* Can you explain why a CBM is considered "interpretable" compared to a standard ResNet, and where the "bottleneck" occurs?

**Concept: Label Noise Types (Symmetric vs. Asymmetric)**
*Why needed here:* The paper distinguishes between general random flipping (symmetric) and structured flipping (asymmetric/grouped). Understanding the noise injection protocol is vital for replicating the "susceptible set" identification.
*Quick check question:* What is the difference between symmetric noise (flipping to any other class) and asymmetric noise (flipping to a specific adjacent class)?

**Concept: Sharpness-Aware Minimization (SAM)**
*Why needed here:* This is the primary mitigation tool. You need to grasp that SAM doesn't just minimize loss value, but the *neighborhood* of the loss (sharpness), effectively seeking flat minima.
*Quick check question:* In SAM, why do we perturb the weights by ε before calculating the gradient update?

## Architecture Onboarding

**Component map:**
Input Backbone (g) -> Concept Bottleneck (ĉ) -> Target Predictor (f) -> Output (ŷ)

**Critical path:** The most critical path identified in the paper is **Concept Frequency → Susceptibility → Representation Misalignment**. Low-frequency or ambiguous concepts become "susceptible," causing the linear target predictor (f) to shift weights to spurious concepts. Mitigation requires stabilizing g (via SAM) and correcting ĉ (via Uncertainty).

**Design tradeoffs:**
- **Linear vs. Non-linear Target Predictor:** The paper (Appendix D.7) suggests using a *linear* target predictor. Non-linear predictors tend to overfit the noisy concept representations more severely.
- **Independent vs. Joint Training:** Joint training preserves task performance better but causes concept accuracy to plummet (destroying interpretability). Independent training is preferred for interpretability analysis.

**Failure signatures:**
- **Representation Misalignment:** Inspect the weights of the target predictor (f). If semantically irrelevant concepts have high weights (e.g., "orange tail" weighted negatively for a bird that should have it), the bottleneck has collapsed.
- **Concept Entanglement:** Visualizing concept embeddings (t-SNE) shows active/inactive classes becoming inseparable as noise rises.

**First 3 experiments:**
1. **Noise Sensitivity Profile:** Train a baseline CBM on the CUB dataset with 20% symmetric concept noise. Plot the accuracy drop per concept to verify the existence of the "susceptible set" (the long tail of high-error concepts).
2. **SAM Ablation:** Replace the SGD optimizer with SAM (tuning ρ) on the concept predictor g. Compare the "concept accuracy" specifically for the identified susceptible set against the baseline.
3. **Intervention Strategy Comparison:** At inference time, compare "Random" intervention vs. "Uncertainty-guided" intervention (correcting top 5 highest entropy concepts). Measure the recovery in Task Accuracy.

## Open Questions the Paper Calls Out

**Open Question 1**
Can the proposed mitigation framework be effectively extended to handle hierarchical, multi-class, or continuous concept spaces rather than just binary labels? Basis in paper: The authors state in the Limitations and Future Work sections that the current study is confined to binary concept labels and that extending the framework to other spaces remains a promising avenue. Why unresolved: The current theoretical analysis and entropy-based uncertainty metric rely on binary classification properties (e.g., sigmoid outputs, binary cross-entropy), which may not directly translate to continuous or ordinal regression tasks. What evidence would resolve it: A theoretical derivation of SAM's regularization effect on continuous concept losses (e.g., MSE) and empirical validation on tasks like clinical severity grading or fine-grained scene understanding.

**Open Question 2**
Does the uncertainty-guided intervention strategy remain robust when concept noise stems from Large Language Model (LLM) hallucinations rather than human annotation errors? Basis in paper: The Future Work section calls for a systematic study of LLM noise properties, noting that LLM-induced noise arises from "limited domain grounding" unlike human inconsistency, and Section D.6 shows LLMs have low agreement with ground truth. Why unresolved: The paper establishes that LLMs generate noisy concepts (21.8% similarity to ground truth), but it does not test if SAM and uncertainty-guided interventions specifically recover performance when trained on this distinct type of semantic noise. What evidence would resolve it: Empirical results showing task accuracy recovery when training CBMs with SAM on LLM-generated concept labels, compared against the ground-truth human annotation baseline.

**Open Question 3**
How does the complexity of the target predictor interact with noise robustness, and can non-linear predictors be regularized to avoid overfitting noisy concepts? Basis in paper: The Limitations section notes that the reliance on a linear target predictor may hinder generalization to complex boundaries, and Section D.7 shows non-linear variants consistently underperform and overfit noise more severely. Why unresolved: The paper demonstrates that standard non-linear predictors fail under noise but does not explore if architectural modifications or specific regularization techniques could make non-linear predictors viable for complex tasks under noise. What evidence would resolve it: Comparative experiments using regularized non-linear target predictors (e.g., with dropout or spectral normalization) combined with SAM, demonstrating superior performance on complex decision boundaries without succumbing to noise overfitting.

## Limitations
- The theoretical analysis of SAM's regularization effect on concept predictors is promising but not extensively validated across diverse noise distributions.
- The intervention strategy relies on ground-truth concept labels during evaluation, which is unrealistic in practical settings where only noisy labels are available.
- The "susceptible set" identification is dataset-dependent and its generalizability to other domains (e.g., medical imaging with rare pathologies) is not explored.

## Confidence
- **High Confidence:** The core empirical findings regarding SAM's effectiveness in improving concept prediction accuracy under noise, and the positive correlation between predictive entropy and concept susceptibility.
- **Medium Confidence:** The theoretical justification for SAM's adaptive regularization mechanism and its specific impact on the "susceptible set" concepts.
- **Low Confidence:** The robustness of the uncertainty-based intervention strategy (UCP) when applied to models trained on truly noisy labels (without ground-truth access).

## Next Checks
1. **Generalization of Susceptible Set:** Apply the analysis pipeline to a third dataset (e.g., a medical imaging dataset with concept annotations) to verify if the "susceptible set" phenomenon and its correlation with concept frequency/ambiguity hold across domains.
2. **Asymmetric Noise Robustness:** Extend the SAM and UCP experiments to asymmetric/grouped noise patterns (e.g., flipping "wing" to "no wing" more frequently than "wing" to "tail") to test if the proposed mechanisms are noise-distribution agnostic.
3. **Noisy-Label Intervention:** Implement and evaluate the CCTP (Clean-Label Imitation) intervention method on a dataset where only noisy concept labels are available, comparing its performance against UCP (which uses ground-truth).