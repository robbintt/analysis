---
ver: rpa2
title: 'MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented
  Generation Data Extraction'
arxiv_id: '2502.04360'
source_url: https://arxiv.org/abs/2502.04360
tags:
- data
- attack
- adversarial
- marage
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARAGE, an optimization-based attack framework
  for extracting verbatim data from Retrieval-Augmented Generation (RAG) systems.
  MARAGE addresses the discrete optimization challenge by optimizing continuous embeddings
  of adversarial tokens, which are then projected to discrete tokens via nearest neighbor
  search.
---

# MARAGE: Transferable Multi-Model Adversarial Attack for Retrieval-Augmented Generation Data Extraction

## Quick Facts
- arXiv ID: 2502.04360
- Source URL: https://arxiv.org/abs/2502.04360
- Reference count: 40
- Key result: MARAGE achieves exact match accuracy above 80% for extracting verbatim data from RAG systems across diverse models

## Executive Summary
This paper introduces MARAGE, an optimization-based attack framework for extracting verbatim data from Retrieval-Augmented Generation (RAG) systems. MARAGE addresses the discrete optimization challenge by optimizing continuous embeddings of adversarial tokens, which are then projected to discrete tokens via nearest neighbor search. It further enhances transferability by jointly optimizing over multiple models with diverse architectures and employs a primacy weighting mechanism that emphasizes losses on initial tokens in the target sequence. Evaluations show MARAGE consistently outperforms manual and optimization-based baselines, achieving exact match (EM) accuracy above 80% across diverse models and RAG datasets. Probing tasks reveal that MARAGE sustains influence on model internal states throughout generation, unlike baseline methods. The attack demonstrates robust transferability to unseen models, including non-instruction-aligned and multi-billion parameter variants. Defenses relying on simple system prompt instructions are ineffective against MARAGE, though more intrinsic defenses like adversarial training or input filtering may offer stronger protection.

## Method Summary
MARAGE is an optimization-based attack framework designed to extract verbatim data from RAG systems by targeting the generation stage. The key innovation is transforming the discrete optimization problem into a continuous one by optimizing embeddings of adversarial tokens, then projecting these back to discrete tokens via nearest-neighbor search. To enhance transferability across models, MARAGE jointly optimizes adversarial examples over multiple models with diverse architectures during training. Additionally, it employs a primacy weighting mechanism that assigns higher importance to losses on initial tokens in the target sequence. The attack operates by inserting adversarial tokens into the retrieval context, which influences the generation process to produce target sequences verbatim. MARAGE is evaluated against both manual and optimization-based baselines, demonstrating superior performance in terms of exact match accuracy across multiple RAG datasets and model architectures.

## Key Results
- MARAGE achieves exact match accuracy above 80% for data extraction from RAG systems across diverse models
- Transferability to unseen models (including GPT-4) is demonstrated, with consistent performance degradation on out-of-distribution architectures
- MARAGE sustains influence on model internal states throughout generation, unlike baseline methods
- Simple system prompt defenses are ineffective against MARAGE

## Why This Works (Mechanism)
MARAGE works by addressing the fundamental challenge of discrete optimization in adversarial attacks on RAG systems. By optimizing continuous embeddings rather than discrete tokens directly, it enables gradient-based optimization techniques. The joint optimization across multiple models with diverse architectures creates adversarial examples that are robust to architectural differences, enhancing transferability. The primacy weighting mechanism exploits the observation that early tokens in generation have disproportionate influence on the final output sequence, making attacks more effective by focusing optimization on these critical positions.

## Foundational Learning

**RAG Architecture**
Why needed: Understanding the retrieval-augmented generation pipeline is essential to identify attack surfaces
Quick check: Can you trace data flow from retrieval through generation?

**Discrete vs Continuous Optimization**
Why needed: MARAGE's core innovation transforms a discrete problem into continuous optimization
Quick check: Explain why direct gradient descent on discrete tokens is impossible

**Transferability in Adversarial ML**
Why needed: MARAGE's multi-model optimization aims to create transferable attacks
Quick check: What makes adversarial examples transferable across different model architectures?

**Token Embeddings and Nearest Neighbor Search**
Why needed: MARAGE projects optimized continuous embeddings back to discrete tokens
Quick check: How does nearest neighbor search bridge the continuous-discrete gap?

## Architecture Onboarding

**Component Map**
Retriever -> Context Processor -> MARAGE Attack Injector -> Language Model Generator -> Output

**Critical Path**
1. Retrieve relevant documents from database
2. Process and format retrieval context
3. Inject adversarial tokens into context (MARAGE)
4. Generate output sequence through language model
5. Extract target data from generated output

**Design Tradeoffs**
MARAGE trades computational complexity (joint optimization over multiple models) for enhanced transferability. The continuous embedding optimization approach requires access to model embedding spaces but enables gradient-based optimization. Primacy weighting sacrifices uniform influence across the sequence for focused impact on generation-critical early tokens.

**Failure Signatures**
- Reduced effectiveness when adversarial examples are optimized for only one model architecture
- Diminished performance when primacy weighting is replaced with uniform weighting
- Complete failure when continuous optimization is replaced with direct discrete optimization

**3 First Experiments**
1. Compare MARAGE's exact match accuracy against manual baseline on a simple RAG dataset
2. Test transferability by attacking 3 models during optimization, then evaluating on 5 unseen models
3. Ablation study removing primacy weighting to measure its contribution to attack success

## Open Questions the Paper Calls Out
None

## Limitations
- Transferability claims rely on attacking 3-5 models but validating on up to 30 models, relationship between training-time diversity and robustness unclear
- Attack assumes white-box access to target model tokenizers and embedding spaces during optimization
- Primacy weighting heuristic lacks rigorous ablation to confirm optimality versus other sequence-position strategies

## Confidence

**High confidence**: MARAGE's superior performance against optimization and manual baselines on benchmark datasets; the discrete-to-continuous embedding optimization approach; primacy weighting mechanism's effectiveness over uniform weighting

**Medium confidence**: Transferability claims to unseen proprietary models; defense ineffectiveness results; sustained influence on internal states versus baseline methods

**Low confidence**: Generalizability to arbitrary RAG systems beyond tested configurations; optimal number of models for joint optimization; real-world deployment constraints

## Next Checks

1. Test MARAGE's transferability when the attacker has only black-box access to the target model's embedding space, requiring embedding-space queries or API approximations

2. Evaluate MARAGE's performance when the retriever itself is subject to adversarial attacks, examining whether cascading vulnerabilities exist in the full RAG pipeline

3. Conduct ablation studies comparing primacy weighting against alternative position-based weighting schemes (e.g., geometric decay, U-shaped weighting) to establish optimality