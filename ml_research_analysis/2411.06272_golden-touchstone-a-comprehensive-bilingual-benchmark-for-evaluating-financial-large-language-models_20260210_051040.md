---
ver: rpa2
title: 'Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial
  Large Language Models'
arxiv_id: '2411.06272'
source_url: https://arxiv.org/abs/2411.06272
tags:
- financial
- arxiv
- language
- tasks
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Golden Touchstone, a comprehensive bilingual
  benchmark for evaluating financial large language models (LLMs), addressing limitations
  in existing benchmarks such as limited language and task coverage, low-quality datasets,
  and poor adaptability for LLM evaluation. Golden Touchstone encompasses eight core
  financial NLP tasks in both Chinese and English, integrating high-quality datasets
  and aligning tasks across languages.
---

# Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models

## Quick Facts
- arXiv ID: 2411.06272
- Source URL: https://arxiv.org/abs/2411.06272
- Reference count: 24
- The paper introduces Golden Touchstone, a comprehensive bilingual benchmark for evaluating financial large language models, addressing limitations in existing benchmarks such as limited language and task coverage, low-quality datasets, and poor adaptability for LLM evaluation.

## Executive Summary
This paper presents Golden Touchstone, a comprehensive bilingual benchmark designed to evaluate financial large language models across eight core financial NLP tasks in both Chinese and English. The benchmark addresses critical limitations in existing evaluation frameworks by providing high-quality, aligned datasets and consistent evaluation standards. The authors also introduce Touchstone-GPT, a financial LLM trained through domain-specific continual pre-training and instruction tuning, which demonstrates strong bilingual performance while highlighting areas for future improvement.

## Method Summary
Golden Touchstone encompasses eight core financial NLP tasks across both Chinese and English languages, with task alignment ensuring comparable difficulty levels and evaluation metrics. The benchmark integrates high-quality datasets from diverse financial domains and establishes consistent treatment of ambiguous labels as incorrect predictions. For model training, Touchstone-GPT employs a two-stage approach: continual pre-training on 100B financial tokens followed by instruction tuning on 300K curated pairs, with mixed-domain data incorporated throughout to prevent catastrophic forgetting.

## Key Results
- Touchstone-GPT demonstrates strong bilingual performance across most financial tasks, particularly in sentiment analysis and entity extraction
- Stock movement prediction based solely on news information shows "practically unusable" performance (~50% accuracy) across all evaluated models
- Cross-lingual comparison reveals that financial LLMs maintain consistent capabilities between English and Chinese tasks when properly aligned
- Instruction tuning significantly improves financial task performance compared to base models, with Touchstone-GPT achieving 0.70 F1 in entity extraction versus GPT-4o's 0.18 F1

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bilingual task alignment with identical task structures enables robust cross-lingual model comparison.
- Mechanism: The benchmark maintains parallel task definitions across English and Chinese (8 tasks each), using comparable evaluation metrics and datasets with similar difficulty levels. This controls for task-specific variance when isolating language effects.
- Core assumption: Financial concepts are sufficiently portable across English and Chinese contexts that task difficulty is primarily language-dependent, not concept-dependent.
- Evidence anchors:
  - [abstract] "encompassing eight core financial NLP tasks in both Chinese and English"
  - [section] "maintaining identical task structures across English and Chinese benchmarks, using comparable evaluation metrics for equivalent tasks"
  - [corpus] Weak direct evidence; neighbor papers focus on regional benchmarks (KFinEval-Pilot for Korean) rather than cross-lingual alignment mechanisms.
- Break condition: If financial terminology or regulatory concepts differ fundamentally between regions (e.g., US GAAP vs. Chinese accounting standards), task alignment may mask domain-specific challenges.

### Mechanism 2
- Claim: Two-stage training (continual pre-training + instruction tuning) with mixed-domain data produces better financial task performance while preserving general capabilities.
- Mechanism: Touchstone-GPT first performs continual pre-training on 100B financial tokens (textbooks, reports, news), then instruction tuning on 300K curated pairs. Crucially, general-domain corpora are mixed in both stages to mitigate catastrophic forgetting.
- Core assumption: The model's parameter space can accommodate both domain-specific financial knowledge and general reasoning without interference, given sufficient data mixing.
- Evidence anchors:
  - [abstract] "trained through domain-specific continual pre-training and instruction tuning"
  - [section] "To avoid catastrophic forgetting in general tasks, we also incorporated general-domain pre-training corpora and instruction-tuning corpora into continuous pre-training and post-training"
  - [corpus] FinAuditing and FinMaster papers similarly employ multi-stage training but don't isolate the mixing mechanism.
- Break condition: If the mixing ratio is suboptimal, the model may either lose financial specificity (too much general data) or lose general instruction-following (too little general data). The paper does not provide ablation on mixing ratios.

### Mechanism 3
- Claim: Consistent treatment of ambiguous labels (e.g., [unknown]) as incorrect maintains evaluation fairness across models.
- Mechanism: The benchmark treats all [unknown] labels as incorrect predictions rather than omitting samples or random assignment. This prevents models from benefiting from uncertainty avoidance and enforces definitive predictions.
- Core assumption: Real-world financial applications require definitive outputs; models that refuse to predict should be penalized.
- Evidence anchors:
  - [section] "consistently treats [unknown] labels as incorrect predictions, which maintains uniform evaluation standards across all tasks"
  - [corpus] No direct comparison; neighbor benchmarks don't discuss label handling explicitly.
- Break condition: If some tasks genuinely have ambiguous ground truth (e.g., subjective sentiment), penalizing uncertainty may discourage calibrated confidence.

## Foundational Learning

- Concept: **Continual Pre-training vs. Fine-tuning**
  - Why needed here: Touchstone-GPT uses continual pre-training on domain corpus before instruction tuning, distinct from direct fine-tuning. Understanding this distinction is critical for replicating the training pipeline.
  - Quick check question: If you have a base LLM and want to adapt it for finance, should you start with full-parameter continual pre-training or LoRA-based fine-tuning? What determines the choice?

- Concept: **Instruction Template Formatting**
  - Why needed here: The paper shows different models require different inference templates (GPT-4o vs. Llama-3 vs. FinGPT). Using wrong templates degrades performance.
  - Quick check question: Given a model's chat format (`<|im_start|>system...` vs. `Instruction:...`), how would you construct the input for a financial sentiment task?

- Concept: **Evaluation Metrics for Financial NLP**
  - Why needed here: Tasks use different metrics (Weighted-F1 for sentiment, RMACC for QA, ROUGE for summarization, MCC for imbalanced classification like LendingClub).
  - Quick check question: For a credit scoring task with 90% "good" loans and 10% "bad" loans, which metric would you prioritize: accuracy or MCC? Why?

## Architecture Onboarding

- Component map:
  - **Benchmark Layer**: 22 datasets → 8 task categories × 2 languages (English: FPB, FiQA-SA, FinRE, etc.; Chinese: FinFE-CN, FinNL-CN, etc.)
  - **Evaluation Layer**: Task-specific metrics (Weighted-F1, ACC, RMACC, ROUGE, BLEU, MCC) + inference template handlers per model
  - **Training Layer (Touchstone-GPT)**: Qwen-2.5 base → Megatron for continual pre-training → LlamaFactory for instruction tuning

- Critical path:
  1. Dataset curation and cleaning (Appendix A shows sample sizes)
  2. Task alignment across languages (ensuring comparable difficulty)
  3. Instruction template formatting per model
  4. Inference with greedy decoding (seed fixed for reproducibility)
  5. Metric computation with consistent [unknown] handling

- Design tradeoffs:
  - News-based stock prediction chosen over time-series/tabular data; paper admits this limits practical utility (~50% accuracy, "practically unusable")
  - Single-modality focus (text only); paper notes multimodal (charts, time-series) as future work
  - Greedy decoding for reproducibility; sacrifices potential quality gains from sampling

- Failure signatures:
  - Low sentiment analysis scores: likely template mismatch or insufficient instruction tuning
  - Near-random stock prediction (~50%): expected behavior; news-only signal is insufficient
  - Low summarization BLEU/ROUGE: common across all models; indicates task difficulty, not model failure
  - Entity extraction failure on GPT-4o (0.18 F1) vs. Touchstone-GPT (0.70 F1): instruction tuning matters significantly

- First 3 experiments:
  1. Reproduce baseline evaluation on a single task (e.g., FPB sentiment) using the open-source benchmark code to validate inference pipeline and template handling.
  2. Ablate instruction tuning data size: train a smaller Touchstone-GPT variant with 30K vs. 300K instruction pairs to measure sensitivity.
  3. Test template mismatch: evaluate a model with correct vs. incorrect inference template to quantify degradation (paper hints this explains some prior benchmark underperformance).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can incorporating multimodal data (volume-price data, factor analysis, visual charts, and time-series data) substantially improve LLM performance on stock movement prediction beyond news-only approaches?
- **Basis in paper:** [explicit] The authors state that "Market prediction relying solely on news information is likely insufficient; volume-price data and factor analysis can provide more comprehensive information. However, current large language models are unable to process these inputs, which is a significant area of future research" and that stock prediction shows "low performance across most models... still practically unusable."
- **Why unresolved:** Current LLMs cannot process quantitative inputs; the benchmark only uses news-based prediction.
- **What evidence would resolve it:** Comparative evaluation of LLMs augmented with multimodal inputs versus text-only baselines on stock movement tasks.

### Open Question 2
- **Question:** Can agent-based methods and retrieval-augmented generation (RAG) effectively augment financial LLMs' capabilities in numerical computation and real-time news analysis?
- **Basis in paper:** [explicit] The authors state: "Our subsequent research will explore the incorporation of agent-based and retrieval-augmented generation (RAG) methods to augment the model's capabilities in numerical computation and real-time news analysis."
- **Why unresolved:** Current models (including Touchstone-GPT) underperform on numerical reasoning tasks like FinQA; the approach remains untested.
- **What evidence would resolve it:** Performance comparisons of agent-based/RAG-enhanced models versus baseline LLMs on numerical QA tasks within the benchmark.

### Open Question 3
- **Question:** To what extent do evaluation results generalize to other financial sectors (e.g., insurance, futures trading) not covered in the current benchmark?
- **Basis in paper:** [explicit] The authors note: "we plan to expand the benchmark to cover other financial sectors such as insurance and futures trading, thus broadening the scope and applicability of financial LLM assessments across diverse scenarios."
- **Why unresolved:** Current datasets focus on equity markets, banking, and general finance; sector-specific terminology and tasks may reveal different model capabilities.
- **What evidence would resolve it:** Evaluation of existing FinLLMs on new datasets covering insurance and futures trading tasks.

## Limitations

- **Task Alignment Validity**: The claim that cross-lingual task alignment enables fair model comparison rests on the assumption that financial concepts transfer equivalently between English and Chinese contexts. However, the paper provides no empirical validation that aligned tasks (e.g., sentiment analysis on similar news articles) present truly comparable difficulty levels across languages.
- **News-Only Stock Prediction**: The benchmark's stock movement prediction task uses only news-based features, explicitly acknowledged as "practically unusable" with ~50% accuracy. This severely limits the benchmark's practical relevance for real-world financial applications where tabular time-series data, order book information, and technical indicators are essential.
- **Template Sensitivity Without Ablation**: The paper demonstrates that different models require specific inference templates but provides no systematic study of template sensitivity. Without controlled experiments varying template formats while holding other factors constant, it's unclear whether observed performance differences reflect true model capabilities or artifacts of formatting choices.

## Confidence

**High Confidence** (4-5/5):
- The two-stage training methodology (continual pre-training + instruction tuning) is technically sound and follows established LLM adaptation practices
- Task-specific metric selection (Weighted-F1 for sentiment, MCC for imbalanced credit scoring) is appropriate for the problem domains
- The bilingual dataset integration approach is methodologically rigorous, with clear alignment procedures documented

**Medium Confidence** (2-3/5):
- Cross-lingual comparison claims, due to unverified assumption of equivalent task difficulty across languages
- Instruction tuning effectiveness, lacking ablation studies on data volume and mixing ratios
- Real-world applicability of benchmark results, particularly for stock prediction task

**Low Confidence** (0-1/5):
- Stock movement prediction task validity, given explicit acknowledgment of practical limitations
- Claims about catastrophic forgetting mitigation without quantitative analysis of general task retention

## Next Checks

1. **Cross-Lingual Difficulty Validation**: Conduct human evaluation studies where financial experts rate task difficulty for aligned English and Chinese tasks. Compare average ratings to ensure the benchmark's foundational assumption of equivalent difficulty is empirically supported.

2. **