---
ver: rpa2
title: 'HierPromptLM: A Pure PLM-based Framework for Representation Learning on Heterogeneous
  Text-rich Networks'
arxiv_id: '2501.12857'
source_url: https://arxiv.org/abs/2501.12857
tags:
- information
- node
- htrns
- heterogeneous
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HierPromptLM, a pure PLM-based framework
  for representation learning on heterogeneous text-rich networks (HTRNs). Unlike
  existing methods that separate textual and structural information processing, HierPromptLM
  integrates both types of information within a unified textual space using a Hierarchical
  Prompt module.
---

# HierPromptLM: A Pure PLM-based Framework for Representation Learning on Heterogeneous Text-rich Networks

## Quick Facts
- arXiv ID: 2501.12857
- Source URL: https://arxiv.org/abs/2501.12857
- Reference count: 40
- Key outcome: HierPromptLM achieves up to 6.08% improvement in node classification and 10.84% improvement in link prediction on the DBLP dataset compared to state-of-the-art methods.

## Executive Summary
This paper introduces HierPromptLM, a pure pre-trained language model (PLM)-based framework for representation learning on heterogeneous text-rich networks (HTRNs). Unlike existing methods that separate textual and structural information processing, HierPromptLM integrates both types of information within a unified textual space using a Hierarchical Prompt module. This module employs meta-path-based subgraph contexts and learnable relation tokens to capture heterogeneous node-level and edge-level structures. The framework is further enhanced with two novel HTRN-tailored pretraining tasks, HGA-NSP and HGA-MLM, designed to emphasize the interactions between textual and structural information. Experiments on two real-world HTRN datasets show that HierPromptLM significantly outperforms state-of-the-art methods.

## Method Summary
HierPromptLM is a pure PLM-based framework that learns node and edge representations on heterogeneous text-rich networks. It uses a Hierarchical Prompt module to integrate textual and structural information within a unified textual space. The method involves extracting meta-path-based subgraphs, textualizing them, and distilling them into graph tokens using a frozen PLM. These graph tokens, along with node text and learnable relation tokens, are assembled into relation-aware prompts that are fed into a tunable PLM. The model is pretrained using two HTRN-tailored tasks, HGA-NSP and HGA-MLM, which emphasize the interactions between textual and structural information. For downstream tasks, embeddings are generated from the [CLS] token and used for node classification or link prediction.

## Key Results
- HierPromptLM achieves up to 6.08% improvement in node classification Micro-F1 on the DBLP dataset compared to state-of-the-art methods.
- HierPromptLM achieves up to 10.84% improvement in link prediction ROC-AUC on the DBLP dataset compared to state-of-the-art methods.
- HierPromptLM significantly outperforms state-of-the-art methods on both DBLP and OAG datasets, demonstrating the effectiveness of the unified textual-space approach.

## Why This Works (Mechanism)

### Mechanism 1: Unified Textual-Space Integration via Hierarchical Prompting
The Hierarchical Prompt module enables seamless integration of heterogeneous graph structures and textual information within a single representation space, eliminating the need for cross-space alignment. The framework converts graph structures into textual representations. First, node-level heterogeneity is captured by decomposing a node's subgraph into meta-path-based subgraphs (e.g., `Paper-Author-Paper`). These are textualized and distilled into concise "graph tokens" using a frozen PLM. Second, edge-level heterogeneity is captured via learnable "relation tokens." The final input sequence concatenates `[Node1 Text + Graph Tokens] [Relation Token] [Node2 Text + Graph Tokens]`, feeding it into a tunable PLM to process structure and text jointly.

### Mechanism 2: Heterogeneous Graph-aware Pretraining Tasks
Standard pretraining tasks (MLM, NSP) can be adapted to force the PLM to learn the interplay between text content and graph structure. HGA-NSP (Next Sentence Prediction) is modified to predict if a specific relation `r` connects two nodes `u` and `v`, with negatives constrained to same-type nodes. HGA-MLM (Masked Language Modeling) is applied to the entire prompt (text + graph tokens). This forces the model to use structural context to predict textual content and vice versa, creating a bidirectional dependency.

### Mechanism 3: Distillation of Structural Context into Graph Tokens
A frozen PLM can effectively distill verbose subgraph summaries into compact "graph tokens," mitigating context length limits. Direct textualization of subgraphs can exceed token limits. A separate, frozen PLM processes these long summaries into a condensed graph token. This token is prepended to the node's text. The tunable PLM then uses this condensed token as structural context without paying the quadratic cost of full-length attention over raw subgraph text.

## Foundational Learning

- **Meta-paths in Heterogeneous Networks**: Why needed here: The prompt module is built on extracting subgraphs based on meta-paths (e.g., `P-A-P`). Understanding that a meta-path defines a semantic relation (e.g., co-authorship) is crucial for interpreting the model's inputs. Quick check question: Given a graph with `User` and `Item` nodes, what does the meta-path `User-Item-User` represent semantically?

- **Soft Prompts in Pre-trained Models**: Why needed here: The framework relies on learnable "relation tokens" and distilled "graph tokens" acting as soft prompts. These are continuous vectors prepended to the input to guide the PLM without modifying its architecture. Quick check question: How does a soft prompt differ from a hard text prompt, and why is it more suitable for abstract structural information?

- **Negative Sampling for Link Prediction**: Why needed here: The HGA-NSP task relies on creating negative samples (non-existent edges) to teach the model to distinguish valid relations. The paper specifies sampling nodes of the same type, a critical detail for heterogeneous graphs. Quick check question: In a social network, why might purely random sampling for negative edges be problematic, and how does the paper's approach mitigate this?

## Architecture Onboarding

- **Component map**: Frozen PLM (Distiller) -> Generates Graph Tokens; Prompt Constructor -> Assembles Relation-Aware Prompt from node text, Graph Tokens, and Relation Tokens; Tunable PLM (Student) -> Processes full prompt -> Generates final embeddings; Pretraining Heads -> Lightweight prediction heads for HGA-NSP and HGA-MLM losses.

- **Critical path**: 1. Offline Processing: Define meta-paths -> Extract subgraphs -> Textualize -> Feed to frozen PLM -> Save Graph Tokens. 2. Prompt Assembly: For a node pair `(u, r, v)`, construct `H(u, r, v) = [F(u)] [RelToken_r] [F(v)]`. 3. Pretraining: Feed prompt into tunable PLM. Optimize joint loss of HGA-MLM and HGA-NSP. 4. Downstream Application: Generate embeddings for classification/link prediction.

- **Design tradeoffs**: Meta-path Complexity: Longer paths provide richer semantics but increase distillation compute and token sequence length. Frozen vs. Tunable Distiller: Frozen distiller chosen for efficiency; tunable could yield better tokens but increases complexity. Pure PLM vs. Hybrid: Trades GNN inductive bias for PLM's unified space and language understanding.

- **Failure signatures**: Token Limit Exhaustion: If combined text (node + graph tokens) consistently exceeds 512 tokens, truncation loses context. Poor Cold-Start: If the frozen distiller hasn't seen vocabulary of new nodes, graph tokens may be generic. Oversimplified Relations: Learnable relation tokens may fail to capture complex edge types.

- **First 3 experiments**: 1. Reproduce DBLP Node Classification: Train on DBLP. Compare Micro-F1 against baselines to validate setup. 2. Ablation on Graph Tokens: Run `w/o GraphToken` (Table 4) to quantify their contribution over raw node text. 3. Meta-path Sensitivity: Train variants with simple (`P-A`) vs. complex (`P-A-P`) meta-paths to find optimal complexity for a specific task.

## Open Questions the Paper Calls Out

- **Question**: How does HierPromptLM scale when employing significantly larger language models (e.g., LLaMA, T5-large) as the tunable backbone? Basis in paper: The authors state in Section 5.6 and the Conclusion that they "opt not to replace the tunable PLM" because "fine-tuning large models is time-intensive and can be considered for future work." Why unresolved: Current experiments only scale the frozen PLM used for distillation while retaining BERT-base for the tunable component; the performance-computation trade-off of fine-tuning larger backbones remains unknown. What evidence would resolve it: Experiments fine-tuning large-scale encoder-decoder or decoder-only models using the HGA-NSP and HGA-MLM objectives.

- **Question**: Can an automated, learnable meta-path selection mechanism mitigate the noise introduced by complex meta-paths? Basis in paper: Appendix C.4 reveals that while complex meta-paths aid node classification, they degrade link prediction performance on DBLP, suggesting that they "might introduce unnecessary noise" for certain tasks. Why unresolved: The current reliance on static, pre-defined meta-paths creates a task-dependent performance variance that manual selection cannot fully optimize. What evidence would resolve it: A study comparing static pre-defined paths against a dynamic mechanism that weights meta-paths based on the specific downstream task context.

- **Question**: How can the text-graph alignment mechanism be enhanced to improve performance on high-order semantic relations like citation links? Basis in paper: Section 5.3 and C.1 note that all models, including HierPromptLM, struggle with "Paper-Paper" relations, attributing this to the difficulty of capturing "contextual connections" in citations. Why unresolved: Even with graph-aware prompts, the complex reasoning required to predict citation links remains a bottleneck, as evidenced by lower ROC-AUC scores relative to other relation types. What evidence would resolve it: Architectural modifications or specialized prompt designs that specifically target high-order reasoning tasks in link prediction.

## Limitations

- **Conceptual Uncertainty**: The paper assumes a PLM can distill arbitrary subgraph structures into fixed-size graph tokens without losing critical topological information. The theoretical bounds on this compression, or empirical sensitivity analysis of token size vs. performance, are not provided.

- **Evaluation Uncertainty**: While results show strong performance, the paper does not provide extensive ablation studies on the distillation step itself. We do not know how much performance is attributable to the frozen distiller versus the tunable PLM.

- **Implementation Uncertainty**: The exact prompt templates for the program function P(·) and the construction of the node representation F(u) are not fully specified in the paper. The critical "token descriptions" mentioned in the example are also undefined.

## Confidence

- **High Confidence**: The claim that HierPromptLM achieves state-of-the-art performance on the DBLP and OAG datasets.
- **Medium Confidence**: The claim that the Hierarchical Prompt module effectively integrates heterogeneous graph structures and textual information.
- **Medium Confidence**: The claim that the HTRN-tailored pretraining tasks (HGA-NSP and HGA-MLM) are effective.

## Next Checks

1. **Prompt Template Fidelity Check**: Obtain the exact prompt templates used for the program function P(·) and the node representation F(u) from the authors. Reproduce the preprocessing step on a small subset of the DBLP dataset and verify that the generated graph tokens and prompts match the expected format described in the paper.

2. **Distillation Ablation Study**: Implement a variant of HierPromptLM where the graph token distillation step is replaced with the raw subgraph summary (as a baseline). Train both versions on a smaller HTRN dataset and compare their performance on a downstream task (e.g., node classification) to quantify the contribution of the distillation step.

3. **Meta-path Sensitivity Analysis**: On the DBLP dataset, systematically vary the complexity of the meta-paths used for subgraph extraction (e.g., from simple P-A to complex P-A-P-A). Train HierPromptLM variants with each meta-path configuration and analyze the impact on link prediction performance, as suggested by the negative result noted in Appendix C.4.