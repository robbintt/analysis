---
ver: rpa2
title: 'Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling
  Framework for Deep Brain Stimulation'
arxiv_id: '2510.25023'
source_url: https://arxiv.org/abs/2510.25023
tags:
- shared
- private
- latents
- spire
- latent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SPIRE, a deep multi-encoder autoencoder designed
  to disentangle shared and private neural dynamics in multi-region recordings. SPIRE
  uses novel alignment and disentanglement losses to factorize neural activity into
  cross-regional shared latents and region-specific private latents, trained only
  on baseline data.
---

# Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation

## Quick Facts
- **arXiv ID:** 2510.25023
- **Source URL:** https://arxiv.org/abs/2510.25023
- **Reference count:** 40
- **Primary result:** SPIRE successfully disentangles shared and private neural dynamics in multi-region recordings, showing superior performance over classical models on synthetic benchmarks and demonstrating generalization of stimulation-specific signatures across brain regions in human DBS data.

## Executive Summary
SPIRE introduces a deep multi-encoder autoencoder framework designed to disentangle shared and private neural dynamics in multi-region recordings. The method uses novel alignment and disentanglement losses to factorize neural activity into cross-regional shared latents and region-specific private latents, trained only on baseline data. Applied to human DBS recordings, SPIRE demonstrates that shared latents reliably encode stimulation-specific signatures that generalize across brain regions and stimulation frequencies, establishing it as a practical tool for analyzing multi-region neural dynamics under perturbation.

## Method Summary
SPIRE employs a deep multi-encoder autoencoder architecture with GRU encoders and decoders to disentangle multi-region neural recordings into shared (cross-regional) and private (region-specific) latent subspaces. The framework uses a weighted loss function combining reconstruction, cross-reconstruction, self-reconstruction, alignment (using VICReg), orthogonality, and regularization terms. Training follows a 3-phase schedule with a "private gate" mechanism that gradually integrates private latents during training. The method is validated on synthetic benchmarks with known ground truth structure and applied to human DBS recordings from GPi and STN regions, showing that shared latents capture stimulation-specific signatures that generalize across brain regions.

## Key Results
- SPIRE outperforms classical models in recovering shared/private structure under nonlinear distortions and temporal misalignments on synthetic benchmarks
- Shared latents in real DBS data reliably encode stimulation-specific signatures that generalize across brain regions and stimulation frequencies
- The framework successfully disentangles neural dynamics while maintaining low reconstruction error and high interpretability of shared components

## Why This Works (Mechanism)
SPIRE works by using a carefully designed loss function that explicitly encourages separation of shared and private information through multiple mechanisms: alignment losses that force shared latents to capture cross-regional correlation, orthogonality constraints that prevent information leakage between subspaces, and a phased training schedule that gradually introduces private latents. The "private gate" mechanism allows the model to first establish stable shared representations before incorporating private components, preventing collapse to either extreme.

## Foundational Learning
- **Cross-reconstruction loss:** Forces shared latents to capture information that can reconstruct data from both regions - needed to ensure shared components contain cross-regional information; quick check: shared latents should enable good reconstruction of both regions
- **Alignment loss (VICReg):** Maintains variance and decorrelates features in shared space - needed to prevent trivial solutions and ensure meaningful shared representations; quick check: shared latents should have unit variance and be decorrelated
- **Orthogonality loss:** Prevents information leakage between shared and private subspaces - needed to ensure true disentanglement; quick check: CCA correlation between shared and private latents should be near zero
- **Private gate mechanism:** Gradually increases influence of private latents during training - needed to stabilize training and prevent variance collapse; quick check: private gate value should increase smoothly from 0 to 1 during specified epochs
- **Lag augmentation:** Incorporates temporal context into input features - needed to capture dynamic dependencies; quick check: model performance should degrade without temporal context
- **3-phase training schedule:** Controls relative importance of different loss components over time - needed to balance competing objectives; quick check: training should follow the specified weight schedule

## Architecture Onboarding
**Component map:** Input -> GRU Encoder -> Shared/Private latents -> GRU Decoder -> Reconstruction
                      |                           |
                      v                           v
                 Alignment loss              Cross-reconstruction loss
                      |                           |
                      v                           v
                 Orthogonality loss            Self-reconstruction loss

**Critical path:** The critical training path involves the alignment loss forcing shared latents to capture cross-regional correlation, while orthogonality loss prevents information from leaking between shared and private subspaces. The private gate mechanism controls when private latents are fully integrated.

**Design tradeoffs:** The framework trades computational complexity (multiple encoders, decoders, and specialized losses) for better disentanglement performance compared to simpler autoencoders or classical methods like DLAG.

**Failure signatures:** Variance collapse occurs when all variance migrates to one subspace (usually shared), causing training instability. This manifests as near-zero variance in private latents or linear dependence between shared and private components.

**First experiments:** 1) Train on synthetic data with known ground truth to verify recovery of shared/private structure using CCA correlation; 2) Test reconstruction fidelity on real data to ensure information is not lost; 3) Verify shared latents show high CCA correlation between regions while private latents show low correlation.

## Open Questions the Paper Calls Out
- **Question:** Can SPIRE be extended to integrate spiking activity with field potentials and incorporate probabilistic objectives to quantify uncertainty in the latent estimates?
- **Question:** Does the SPIRE framework generalize effectively to neural circuits involving more than two regions, such as cortex and thalamus, and to chronic stimulation timescales?
- **Question:** Can specific biophysical meanings be assigned to the individual dimensions of the shared and private latent factors identified by SPIRE?

## Limitations
- The framework's performance depends heavily on precise implementation of the phased training schedule, particularly the "private gate" mechanism
- Current validation is limited to two-region recordings, with generalization to more regions remaining to be tested
- The latent dimensions are statistical abstractions without established biophysical interpretations

## Confidence
- **High confidence:** Claims about SPIRE's ability to recover ground truth shared/private latents in synthetic benchmarks
- **Medium confidence:** Claims about shared latents capturing stimulation-specific signatures in DBS data
- **Medium confidence:** Claims about generalization across brain regions and stimulation frequencies

## Next Checks
1. Verify the exact mathematical implementation of the "private gate" mechanism by comparing gradient flow and latent variance distributions during epochs 80-140
2. Reproduce the variance collapse diagnostics by intentionally training with reduced orthogonality loss weight
3. Test SPIRE's robustness to training data size by training on progressively smaller subsets of baseline data and measuring degradation in CCA correlation