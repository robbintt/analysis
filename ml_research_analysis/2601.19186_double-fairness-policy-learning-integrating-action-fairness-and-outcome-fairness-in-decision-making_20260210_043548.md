---
ver: rpa2
title: 'Double Fairness Policy Learning: Integrating Action Fairness and Outcome Fairness
  in Decision-making'
arxiv_id: '2601.19186'
source_url: https://arxiv.org/abs/2601.19186
tags:
- fairness
- policy
- action
- outcome
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses fairness in policy learning by distinguishing
  between action fairness (equitable action assignments) and outcome fairness (equitable
  downstream consequences). The authors propose a double fairness learning (DFL) framework
  that manages the trade-off among three objectives: action fairness, outcome fairness,
  and value maximization through a lexicographic weighted Tchebyshev method.'
---

# Double Fairness Policy Learning: Integrating Action Fairness and Outcome Fairness in Decision-making

## Quick Facts
- **arXiv ID:** 2601.19186
- **Source URL:** https://arxiv.org/abs/2601.19186
- **Reference count:** 40
- **Primary result:** DFL framework improves both action and outcome fairness while incurring only modest reductions in overall value compared to competing methods.

## Executive Summary
This paper addresses fairness in policy learning by distinguishing between action fairness (equitable action assignments) and outcome fairness (equitable downstream consequences). The authors propose a double fairness learning (DFL) framework that manages the trade-off among three objectives: action fairness, outcome fairness, and value maximization through a lexicographic weighted Tchebyshev method. They establish theoretical guarantees on regret bounds and demonstrate that their approach can recover Pareto solutions beyond convex settings. Experiments show that DFL substantially improves both action and outcome fairness while incurring only modest reductions in overall value compared to competing methods. The framework is applied to real-world datasets including motor third-party liability insurance and entrepreneurship training data, validating its effectiveness across different fairness notions (equal opportunity and counterfactual fairness).

## Method Summary
The DFL framework operates by first estimating reward functions and fairness-relevant outcome functions through regression. It then constructs fairness metrics (Δ₁ for action fairness, Δ₂ for outcome fairness) and uses a lexicographic weighted Tchebyshev scalarization approach to identify Pareto-optimal fairness policies. The method solves a nested optimization: for each discretization point αk, it computes the optimal Tchebyshev value M̂*αk, then minimizes the sum of fairness gaps Δ̂ subject to M̂ ≤ M̂*αk + κ. Finally, it selects the policy that maximizes value within this fairness-constrained set. The framework accommodates both equal opportunity fairness (conditioning on observed covariates) and counterfactual fairness (requiring estimation of counterfactual covariates under a linear S→X relationship assumption).

## Key Results
- DFL substantially improves both action and outcome fairness metrics compared to unconstrained optimization and linear scalarization baselines
- The framework achieves these fairness gains while incurring only modest reductions in overall value (approximately 5-15% reduction depending on dataset)
- DFL successfully recovers Pareto-optimal policies even in non-convex settings where linear scalarization fails to identify the full Pareto front
- Application to motor third-party liability insurance data shows 20-30% reduction in fairness gaps while maintaining comparable loss ratios
- Entrepreneurship training data experiments demonstrate the framework's ability to balance fairness across different sensitive attributes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Equalizing actions does not generally equalize outcomes when groups face different constraints or respond differently to the same action.
- **Mechanism:** The policy's action A affects outcomes R through an environment response function that differs across sensitive groups S. Action fairness constrains π(a|s,x), but outcome fairness depends on the conditional expectation Eπ[R(2)|s,x] = Σa f(s,x,a)π(a,s,x), where f(s,x,a) varies with S. If treatment effects differ by group (f(1,x,a) ≠ f(0,x,a)), identical action assignments yield different expected outcomes.
- **Core assumption:** The data-generating process follows the DAG: S→X→A→R, with S influencing X and X influencing both A and R; treatment effects on R(2) may vary by group.
- **Evidence anchors:**
  - [abstract] "Crucially, equalizing actions does not generally equalize outcomes when groups face different constraints or respond differently to the same action."
  - [Section 1, page 3] "A government job training program might admit participants equally (action fair), unequal downstream constraints such as transportation access...can still prevent some participants from completing the program"
  - [corpus] Limited direct corpus support for this specific action-outcome gap; related work focuses on single-fairness metrics.
- **Break condition:** If treatment effects are homogeneous across groups (f(1,x,a) = f(0,x,a) for all x,a), then action fairness implies outcome fairness and the gap disappears.

### Mechanism 2
- **Claim:** The lexicographic weighted Tchebyshev scalarization recovers Pareto-optimal fairness policies even when the policy class is non-convex.
- **Mechanism:** Rather than linear scalarization (αΔ₁ + (1-α)Δ₂), which can miss non-convex regions of the Pareto front, DFL uses Mα(π) = max{αΔ₁(π), (1-α)Δ₂(π)}. For each α∈(0,1), define Λ*α = {π: arg inf Mα(π)}, then select π minimizing Δ(π)=Δ₁(π)+Δ₂(π) within Λ*α. This two-stage lexicographic approach ensures coverage of the full Pareto set.
- **Core assumption:** The Pareto fairness set is well-separated (Assumption 4.2): policies δ away from Πp incur objective gap of order δ under either Δ(·) or Mα(·).
- **Evidence anchors:**
  - [abstract] "employ a lexicographic weighted Tchebyshev method that recovers Pareto solutions beyond convex settings, with theoretical guarantees on the regret bounds"
  - [Section 3.2, page 16] Proposition 3 characterizes the Pareto set via this scalarization; Figure 7 in appendix demonstrates linear scalarization fails for non-convex regions.
  - [corpus] No direct corpus validation of Tchebyshev vs. linear scalarization for fairness.
- **Break condition:** If the policy class is convex and objectives are convex, linear scalarization suffices; Tchebyshev adds no benefit but incurs computational overhead.

### Mechanism 3
- **Claim:** Double fairness policies (simultaneous action and outcome fairness) exist if and only if the sensitive variable's effects have opposite signs across treatments.
- **Mechanism:** Assumption 3.2(i) requires [f(1,x,1)−f(0,x,1)][f(1,x,0)−f(0,x,0)] ≤ 0. This "crossing" condition means one treatment benefits group S=1 more, while the other benefits group S=0 more. If satisfied, there exists a stochastic policy π(x) that equalizes both action probabilities and expected outcomes across groups. Proposition 2 proves uniqueness when such a policy exists.
- **Core assumption:** Binary action space; continuous policy π(x)∈[0,1] is allowed (stochastic policies).
- **Evidence anchors:**
  - [Section 3.1, page 13] Proposition 2: "A double fairness policy is either almost surely unique or does not exist. Furthermore, a double fairness policy exists if and only if Assumption 3.2 (i) holds."
  - [Section 9.2, page 44] Proof derives π(x) = [f(0,x,0)−f(1,x,0)]⁻¹·[f(0,x,0)−f(1,x,0)+f(1,x,1)−f(0,x,1)]⁻¹
  - [corpus] No corpus papers address existence conditions for double fairness.
- **Break condition:** If one group dominates under both treatments (f(1,x,a) > f(0,x,a) for all a), no double fairness policy exists; the problem reduces to Pareto optimization with inherent tradeoffs.

## Foundational Learning

- **Concept: Policy Learning & Value Functions**
  - **Why needed here:** DFL extends standard policy learning by adding fairness constraints. You must understand V(π)=Eπ[R(1)] as the optimization target before understanding how fairness objectives modify it.
  - **Quick check question:** Given historical data {(Si,Xi,Ai,Ri)}, can you derive the regression-based value estimator V̂(π)?

- **Concept: Pareto Optimality & Multi-Objective Optimization**
  - **Why needed here:** The core innovation is treating (Δ₁, Δ₂, V) as competing objectives. Understanding that Pareto-optimal policies cannot improve one objective without degrading another is essential.
  - **Quick check question:** For two policies π₁ with (Δ₁=0.2, Δ₂=0.5) and π₂ with (Δ₁=0.3, Δ₂=0.3), which is Pareto-dominant?

- **Concept: Equal Opportunity vs. Counterfactual Fairness**
  - **Why needed here:** DFL accommodates both notions. Equal opportunity conditions on observed X; counterfactual fairness requires estimating Xs,x(s′)—the covariates had S been different.
  - **Quick check question:** If S=gender and X=test score, what is Xs,x(s′) for a female applicant with score 59 under Assumption 4.5?

## Architecture Onboarding

- **Component map:**
```
Data {(Si,Xi,Ai,Ri)} 
    ↓
[Outcome Regression] → r̂(s,x,a), f̂(s,x,a)
    ↓
[Fairness Metrics] → Δ̂₁(π), Δ̂₂(π)
    ↓
[Tchebyshev Optimization] → For each αk∈Ω: compute Λ̂*αk, then Π̂p
    ↓
[Value Maximization] → π̂ = argmax_{π∈Π̂p} V̂(π)
```

- **Critical path:**
  1. Estimate reward r(s,x,a) and fairness-relevant outcome f(s,x,a) via regression (parametric or sieve)
  2. For counterfactual fairness: estimate θ(s) for counterfactual covariate generation
  3. Choose slack parameter κ ≈ c₀·√(log n/n) and discretization K=O(√(n/log n))
  4. Solve nested optimization: M̂*αk = inf_π max{αkΔ̂₁, (1-αk)Δ̂₂}; then minimize Δ̂ subject to M̂ ≤ M̂*+κ

- **Design tradeoffs:**
  - **κ (slack parameter):** Larger κ covers more of the true Pareto set but increases estimation variance. Theorem 2 bounds Hausdorff distance by κ.
  - **K (discretization):** Finer grid (larger K) improves Pareto coverage but increases computation O(K×|Π| evaluations).
  - **Fairness notion choice:** Equal opportunity is simpler (no counterfactual estimation); counterfactual fairness requires Assumption 4.5 (linear S→X relationship) and adds O(√(d/n)) to regret bound.

- **Failure signatures:**
  - **Empty Pareto set:** If κ too small or estimation error high, Π̂p may be empty → increase κ
  - **Degenerate solution:** If "treat everyone equally" (π(a|s,x)=constant) is optimal, problem lacks actionable policy learning → add resource/budget constraints
  - **High regret:** Check Assumption 4.4 (nuisance estimation rates); if γ₁,γ₂ < 1, regret is dominated by n^(-γ/2) terms

- **First 3 experiments:**
  1. **Synthetic validation:** Replicate Section 5 DGP with known ground truth; verify Pareto set recovery and compare to linear scalarization baseline
  2. **Sensitivity to κ:** Sweep c₀∈{0.2,0.5,1,2} on held-out test data; observe tradeoff between fairness metric stability and value function
  3. **Ablation on fairness notions:** On the insurance dataset (Section 6.1), compare equal opportunity vs. counterfactual fairness; quantify how the counterfactual estimation error (√(d/n)) affects final policy performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the DFL framework be extended to handle multiple (>2) sensitive attributes simultaneously, where intersections of protected groups may face compounding disparities?
- Basis in paper: [inferred] The paper explicitly states in Section 2: "we consider a binary sensitive attribute and a binary action." No discussion of multi-attribute fairness is provided.
- Why unresolved: Intersectionality introduces complex trade-offs; optimizing for fairness across multiple attributes simultaneously may lead to infeasible or degenerate solutions.
- What evidence would resolve it: A theoretical extension with modified feasibility conditions (Propositions 1-2 generalized) and empirical validation on datasets with multiple protected attributes.

### Open Question 2
- Question: What are the necessary conditions for outcome fairness policy existence under violations of Assumption 3.2's "opposite signs" requirement?
- Basis in paper: [explicit] Assumption 3.2 requires that sensitive variable effects have opposite signs across treatments (condition i) and conditional treatment effects have opposite signs across sensitive variables (condition ii). The authors note these imply "moderate" sensitivity effects but do not address when they fail.
- Why unresolved: When one group systematically dominates under all treatments, the current theory provides no guidance on whether approximate outcome fairness is achievable.
- What evidence would resolve it: Derivation of relaxed feasibility conditions or impossibility results when Assumption 3.2 is violated, supported by simulation studies.

### Open Question 3
- Question: How does the DFL framework perform under unobserved confounding, where the ignorability assumption required for causal identification of f(s,x,a) fails?
- Basis in paper: [inferred] The methodology relies on estimating f(s,x,a) = E[R(2)|S=s,X=x,A=a] from observational data, implicitly assuming unconfoundedness. No sensitivity analysis is provided.
- Why unresolved: Real-world policy data often contains unmeasured confounders; the regret bounds in Theorems 3-4 may not hold under confounding.
- What evidence would resolve it: Sensitivity analysis quantifying robustness to confounding strength, or development of instrumental variable extensions to the DFL framework.

### Open Question 4
- Question: Can the lexicographic weighted Tchebyshev approach be extended to online or sequential decision-making settings where policies must adapt to streaming data?
- Basis in paper: [inferred] The paper addresses offline policy learning with fixed historical datasets. The related literature section mentions Wang et al. (2025) on sequential settings, but DFL itself is not extended to reinforcement learning.
- Why unresolved: Online fairness requires balancing exploration-exploitation with fairness constraints that evolve as the underlying distribution changes.
- What evidence would resolve it: An online variant of DFL with regret bounds that account for both fairness and cumulative reward in non-stationary environments.

## Limitations

- **Non-convexity assumptions:** The paper claims lexicographic Tchebyshev scalarization is necessary for non-convex Pareto sets, but provides limited empirical evidence that competitors fail in these settings.
- **Counterfactual fairness requirements:** The method relies on strong assumptions (4.5: linear S→X relationship) that may not hold in practice, with the √(d/n) error term potentially being prohibitive for high-dimensional X.
- **Existence conditions:** Double fairness policies exist only under specific "crossing" conditions (Assumption 3.2(i)), with no practical guidance on how often these conditions hold or what happens when they fail.

## Confidence

- **High confidence:** The regret bound derivations (Theorem 1 and 2) are mathematically rigorous and the theoretical framework is sound. The distinction between action fairness and outcome fairness is well-motivated.
- **Medium confidence:** The experimental results show DFL improves fairness metrics while maintaining reasonable value, but comparisons are limited to simple baselines and real-world applicability beyond the two case studies is uncertain.
- **Low confidence:** The practical significance of recovering non-convex Pareto regions is unclear without more extensive empirical validation across diverse domains.

## Next Checks

1. **Non-convexity stress test:** Systematically vary the data-generating process to create both convex and non-convex Pareto sets. Compare DFL's Pareto set recovery to linear scalarization and other multi-objective methods across this spectrum.

2. **Counterfactual robustness:** Evaluate DFL's performance when Assumption 4.5 is violated. Introduce non-linear relationships between S and X and measure how estimation errors propagate to policy performance and fairness metrics.

3. **Generalizability assessment:** Apply DFL to additional domains beyond insurance and entrepreneurship (e.g., healthcare allocation, loan decisions). Compare results across domains to identify where the "crossing" conditions for double fairness are most likely to hold.