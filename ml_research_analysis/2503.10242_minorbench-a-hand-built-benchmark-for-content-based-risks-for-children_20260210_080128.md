---
ver: rpa2
title: 'MinorBench: A hand-built benchmark for content-based risks for children'
arxiv_id: '2503.10242'
source_url: https://arxiv.org/abs/2503.10242
tags:
- children
- content
- llms
- risks
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces MinorBench, a benchmark for evaluating how
  well Large Language Models (LLMs) refuse unsafe or inappropriate content from children.
  The authors conducted a real-world case study of middle-school students using LLM
  chatbots, identifying six key content-based risks specific to minors: Danger, Sexual,
  Profanities, Hateful, Self-Harm, and Substance Use.'
---

# MinorBench: A hand-built benchmark for content-based risks for children

## Quick Facts
- arXiv ID: 2503.10242
- Source URL: https://arxiv.org/abs/2503.10242
- Reference count: 40
- Introduces MinorBench, a benchmark evaluating LLM refusal of unsafe content to children

## Executive Summary
MinorBench is a benchmark designed to evaluate how well Large Language Models refuse unsafe or inappropriate content when interacting with children. The benchmark was developed based on a real-world case study of middle-school students using LLM chatbots, identifying six key content-based risks: Danger, Sexual, Profanities, Hateful, Self-Harm, and Substance Use. The benchmark includes 299 manually curated prompts reflecting these risks. The authors evaluated six LLMs under four different system prompts and found that refusal rates increased significantly with stronger safety instructions, but varied widely across models and risk categories.

## Method Summary
The authors conducted a case study of middle-school students using LLM chatbots to identify content-based risks specific to minors. They developed MinorBench with 299 manually curated prompts across six risk categories: Danger, Sexual, Profanities, Hateful, Self-Harm, and Substance Use. Six LLMs were evaluated under four different system prompts: no instructions, basic refusal instructions, clear safety guidelines, and explicit harm prevention directives. The evaluation measured refusal rates for each model, risk category, and prompt type combination.

## Key Results
- GPT-4o-mini achieved 97% refusal rate under strict safety prompts, while reasoning models like o3-mini and R1 Distilled performed poorly
- Refusal rates increased significantly with stronger safety instructions across all models tested
- Performance varied widely across risk categories, with some categories showing much lower refusal rates than others
- No single model consistently outperformed others across all risk categories and prompt types

## Why This Works (Mechanism)
The benchmark works by providing a standardized set of prompts that represent realistic scenarios where minors might encounter inappropriate content. The evaluation mechanism measures model behavior through explicit refusal rates, allowing for quantitative comparison across different models and safety configurations. The multi-prompt approach captures how system instructions influence safety behavior.

## Foundational Learning

### Risk Categorization Framework
Why needed: Provides structured approach to identifying and classifying potential harms to minors
Quick check: Verify that all six categories capture the full spectrum of observed student interactions

### Safety Instruction Design
Why needed: Enables systematic testing of how different safety prompts affect model behavior
Quick check: Confirm that prompt variations produce measurable differences in refusal rates

### Benchmark Construction Methodology
Why needed: Ensures prompts are realistic and representative of actual use cases
Quick check: Validate that prompts reflect genuine scenarios observed in middle-school environments

## Architecture Onboarding

### Component Map
Prompt Generation -> Risk Classification -> LLM Evaluation -> Refusal Rate Analysis

### Critical Path
Prompt curation → Risk categorization → System prompt design → Model evaluation → Results analysis

### Design Tradeoffs
- Manual prompt curation ensures quality but limits scalability
- Binary refusal measurement simplifies analysis but may miss nuanced responses
- Six risk categories balance comprehensiveness with manageability

### Failure Signatures
- Low refusal rates despite strong safety prompts indicate inadequate safety training
- Inconsistent refusal across similar prompts suggests prompt sensitivity issues
- High refusal rates with false positives may indicate overly conservative safety measures

### First Experiments
1. Test additional models beyond the initial six to expand comparative analysis
2. Evaluate prompts with varied linguistic complexity to assess robustness
3. Test alternative safety instruction formulations to optimize refusal rates

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on model-generated refusals rather than actual harm prevention outcomes
- Six risk categories may not comprehensively capture all content-based risks minors face
- Study only evaluates English-language prompts, limiting generalizability to multilingual contexts

## Confidence
- High confidence: Refusal rates vary significantly across models and risk categories
- Medium confidence: Stronger safety instructions consistently increase refusal rates
- Low confidence: Six identified risk categories generalize to all educational contexts

## Next Checks
1. Test the benchmark with additional languages to assess cross-lingual safety performance
2. Conduct user studies with actual minors to validate risk categories and effectiveness
3. Expand system prompt testing to include prompts balancing safety with educational utility