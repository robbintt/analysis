---
ver: rpa2
title: Power-scaled Bayesian Inference with Score-based Generative Models
arxiv_id: '2504.10807'
source_url: https://arxiv.org/abs/2504.10807
tags:
- prior
- likelihood
- posterior
- power
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a power-scaling method for score-based generative
  models in Bayesian inference to control the influence of prior and likelihood terms.
  The method enables flexible modulation of prior-likelihood trade-off without retraining
  by scaling their contributions at inference time.
---

# Power-scaled Bayesian Inference with Score-based Generative Models

## Quick Facts
- arXiv ID: 2504.10807
- Source URL: https://arxiv.org/abs/2504.10807
- Reference count: 0
- Primary result: Power-scaling score-based models enables flexible modulation of prior-likelihood trade-off without retraining by scaling their contributions at inference time.

## Executive Summary
This paper introduces a power-scaling method for score-based generative models in Bayesian inference, allowing flexible modulation of the trade-off between prior and likelihood terms without retraining. The approach reformulates the power-scaled posterior by decomposing the likelihood score into posterior and prior score terms via Bayes' rule identity, requiring only learned score functions for both distributions. Applied to seismic velocity model generation conditioned on RTM images, experiments demonstrate that increasing likelihood power improves data fidelity with optimal performance at power 2.0, while lower prior power increases structural diversity among samples. The method reduces shot data residuals and provides a principled way to explore different posterior distributions for uncertainty quantification.

## Method Summary
The method uses classifier-free guidance (CFG) to train a single conditional network that can estimate both posterior and prior scores by randomly masking the conditioning input during training. At inference, the posterior and prior scores are combined with power coefficients using the identity ∇x log p(y|x) = ∇x log p(x|y) − ∇x log p(x), reformulated as ∇x log pλ,α(x|y) = λ∇x log p(x|y) + (α − λ)∇x log p(x). Sampling is performed via Langevin-based predictor-corrector steps within an SDE solver framework. The approach requires no retraining when adjusting power coefficients, enabling sensitivity analysis and exploration of intermediate posterior distributions.

## Key Results
- Optimal likelihood power λ=2.0 minimizes shot data residuals compared to standard Bayesian λ=1.0
- Lower prior power α increases structural diversity among generated samples
- Power-scaling reduces shot data residuals while maintaining geological realism
- Method enables uncertainty quantification by sampling from intermediate power posteriors

## Why This Works (Mechanism)

### Mechanism 1: Score Decomposition via Bayes' Rule Identity
The method reformulates power-scaled posteriors by expressing likelihood gradients in terms of posterior and prior scores using Bayes' rule identity. This allows sampling without explicit likelihood evaluations by combining learned score functions with power coefficients.

### Mechanism 2: Classifier-Free Guidance for Amortized Score Estimation
A single conditional network estimates both posterior and prior scores by randomly masking conditioning input during training. This approach reduces training cost while maintaining the ability to generate both conditional and unconditional samples.

### Mechanism 3: Langevin Correction for Valid Sampling
Predictor-corrector steps using Langevin dynamics refine intermediate samples during SDE integration, ensuring samples remain on the manifold defined by the power-scaled distribution and improving sampling stability.

## Foundational Learning

- **Bayes' rule and posterior decomposition**: Essential for understanding how likelihood scores can be expressed in terms of posterior and prior scores. Quick check: Given posterior and prior score functions, how would you compute the likelihood score?

- **Score-based generative models and diffusion SDEs**: Core to understanding how denoising score matching and SDE integration generate samples. Quick check: What does the score function ∇x log p(x) represent geometrically, and how does it guide sampling?

- **Classifier-free guidance (CFG)**: Critical for understanding how a single network can estimate both conditional and unconditional distributions. Quick check: If you increase the conditioning dropout probability from 0.2 to 0.5, what trade-offs do you expect in posterior vs. prior score quality?

## Architecture Onboarding

- **Component map**: Dataset preparation (velocity-RTM pairs) -> CFG training with dropout (p=0.2) -> Denoising score matching -> Score extraction from denoiser -> Power-scaled sampler (SDE + Langevin corrector) -> Generated velocity models

- **Critical path**: 1) Prepare dataset from simulation 2) Train conditional U-Net with CFG dropout 3) Extract scores from denoiser 4) Implement power-scaled sampling with Langevin correction 5) Sweep power coefficients

- **Design tradeoffs**: Single network with CFG reduces training cost but ties prior/posterior quality to shared capacity; Langevin correction improves fidelity but increases latency (~4s per sample); optimal power coefficients are dataset-specific

- **Failure signatures**: Samples ignore RTM conditioning (check CFG implementation); overfitting artifacts (reduce likelihood power); incoherent structures (increase prior power); slow convergence (increase corrector steps)

- **First 3 experiments**: 1) Prior-only sweep (α ∈ {0.25, 0.5, 1.0, 1.5}, λ=0) to verify structural changes; 2) Likelihood power calibration (λ ∈ {0, 0.4, 1.0, 2.0, 4.0, 16.0}) to find optimal residual; 3) Power-scaling compass (α, λ ∈ {0.5, 1.0, 2.0}²) to visualize trade-offs

## Open Questions the Paper Calls Out
1. How does power-scaling impact the calibration and reliability of uncertainty quantification in seismic inversion?
2. Why does the optimal likelihood power (λ=2.0) exceed the standard Bayesian setting (λ=1.0), and is this value task-dependent?
3. Can sampling efficiency be improved to mitigate the computational cost of Langevin-based correction steps?

## Limitations
- Computational overhead from Langevin correction steps (~4s per sample) limits real-time application
- Optimal power coefficients (λ=2.0, α=0.5) are dataset-specific and may not generalize to other domains
- CFG dropout rate (0.2) may be suboptimal for other datasets, potentially degrading unconditional score estimates

## Confidence
- **High**: Bayes' rule decomposition identity and mathematical reformulation are sound and well-established
- **Medium**: Experimental results convincing for specific seismic dataset but lack statistical significance tests
- **Low**: Claims about Langevin correction's stabilizing role weakly supported with no quantitative analysis

## Next Checks
1. Systematically vary CFG dropout probability (0.1, 0.2, 0.5) and measure impact on prior score quality and sample fidelity
2. Extend power sweep beyond λ=16.0 to test for divergence in Langevin correction and quantify computational cost vs. quality trade-off
3. Apply method to synthetic 3D velocity model or non-seismic imaging task to assess generalization of optimal power coefficients