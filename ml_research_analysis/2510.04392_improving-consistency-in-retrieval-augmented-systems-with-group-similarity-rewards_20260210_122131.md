---
ver: rpa2
title: Improving Consistency in Retrieval-Augmented Systems with Group Similarity
  Rewards
arxiv_id: '2510.04392'
source_url: https://arxiv.org/abs/2510.04392
tags:
- consistency
- accuracy
- across
- arxiv
- con-rag
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles information consistency in retrieval-augmented
  generation (RAG) systems, addressing the problem that semantically equivalent queries
  often yield inconsistent outputs due to variability in both retrieval and generation
  components. To improve consistency, the authors introduce a principled evaluation
  framework that decomposes RAG consistency into retriever-level, generator-level,
  and end-to-end components, enabling clear identification of inconsistency sources.
---

# Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards

## Quick Facts
- arXiv ID: 2510.04392
- Source URL: https://arxiv.org/abs/2510.04392
- Reference count: 19
- Key outcome: Introduces Con-RAG, a method using group similarity rewards across paraphrased queries to significantly improve consistency in RAG systems while maintaining accuracy.

## Executive Summary
This paper addresses a critical challenge in retrieval-augmented generation (RAG) systems: inconsistent outputs for semantically equivalent queries. The authors introduce a principled framework that decomposes consistency into retriever-level, generator-level, and end-to-end components, enabling clear diagnosis of inconsistency sources. Their proposed method, Paraphrased Set Group Relative Policy Optimization (PS-GRPO), trains generators to produce consistent outputs across paraphrased queries using group similarity rewards. By leveraging a scalable approximation technique, the approach achieves significant consistency improvements across multiple QA benchmarks while maintaining or improving accuracy.

## Method Summary
The authors propose PS-GRPO, which uses group similarity rewards computed across paraphrased query sets to train the generator for consistency. For each canonical query with paraphrases, they generate multiple rollouts per paraphrase and compute rewards based on BLEU similarity between outputs from different paraphrases. The method uses GRPO's group-relative advantage estimation to normalize rewards within paraphrase groups, eliminating the need for a critic model. To address computational complexity, they develop a scalable approximation that reduces comparisons from quadratic to linear in the number of paraphrases. The approach combines consistency rewards with optional accuracy rewards for short-form QA and uses consistency-only training with KL regularization for long-form tasks.

## Key Results
- Con-RAG achieves 15-20% absolute improvement in generator consistency over strong baselines on TriviaQA and HotpotQA
- End-to-end consistency improves by 10-15% across short-form QA benchmarks while maintaining accuracy
- On long-form ELI5, Con-RAG achieves 30% consistency improvement using only consistency rewards without explicit accuracy supervision
- The scalable approximation maintains effectiveness while reducing computational cost by 80%

## Why This Works (Mechanism)

### Mechanism 1: Group Similarity Rewards Across Paraphrase Sets
Rewarding outputs for similarity to other outputs generated from semantically equivalent queries encourages the model to converge on consistent information content regardless of phrasing. For each canonical query with paraphrases, the method generates multiple rollouts per paraphrase and computes rewards based on BLEU similarity between outputs from different paraphrases, creating pressure toward outputs that convey the same core content across the paraphrase set.

### Mechanism 2: GRPO's Group-Relative Advantage Estimation
The method uses GRPO's group-relative advantage estimation to normalize rewards within paraphrase groups, removing the need for a critic model while providing stable policy gradients. By normalizing rewards per-paraphrase group using group-level statistics, the approach ensures stable training without requiring separate critic networks.

### Mechanism 3: Scalable Approximation via Subsampling
The quadratic computational cost of computing rewards across all paraphrase pairs is addressed through subsampling. By sampling a subset of paraphrases and rollouts, the method reduces comparisons from O(n²g²) to O(ngκs) while maintaining unbiased reward estimates, making the approach practical for large-scale deployment.

## Foundational Learning

- **Concept: Retriever vs. Generator Consistency Decomposition**
  - Why needed: The paper isolates inconsistency sources—Table 1 shows retriever consistency (Jaccard) is often 30-50%, meaning paraphrases retrieve different documents. Without this decomposition, you cannot diagnose whether inconsistency stems from retrieval drift or generator sensitivity.
  - Quick check: Given a RAG system with 90% generator consistency but 40% retriever consistency, where should optimization focus?

- **Concept: BLEU as Proxy for Information Consistency**
  - Why needed: The paper uses BLEU-1/2 as the similarity function. Understanding what BLEU captures (n-gram overlap, surface form) versus what it misses (synonyms, structural paraphrase) is critical for interpreting results and choosing alternatives.
  - Quick check: Why might BLEU-4 be worse than BLEU-2 for long-form QA consistency (per Table 4)?

- **Concept: RL without Ground Truth (Self-Supervised Consistency)**
  - Why needed: For long-form QA (ELI5), Con-RAG uses only consistency rewards—no accuracy supervision. This is a departure from typical RLHF and requires understanding how the reward signal alone shapes behavior.
  - Quick check: What failure mode might arise if you optimize only for consistency without any accuracy signal or KL regularization?

## Architecture Onboarding

- **Component map:** Retriever (e5-base-v2, FAISS index) -> Paraphrase Generator (LLaMA-3.1-70B) -> Generator (LLaMA-3.1-8B/Qwen-2.5-3B) -> Reward Module (BLEU similarity) -> PS-GRPO Optimizer
- **Critical path:** 1) Generate paraphrases for training queries (offline, using ground truth to ensure answerability) 2) For each batch: retrieve documents per paraphrase, generate g rollouts each 3) Compute group similarity rewards (with subsampling approximation) 4) Compute group-normalized advantages, update policy via clipped GRPO objective 5) Evaluate on held-out paraphrase sets using lexical + LLM-judge consistency
- **Design tradeoffs:** BLEU vs. semantic similarity (BLEU is cheap but penalizes valid paraphrases; LLM-judge is more accurate but slow), KL penalty (β=0.0 for short-form with accuracy reward, β=0.05 for long-form consistency-only), paraphrase count (n=6, g=4 with subsampling)
- **Failure signatures:** Reward hacking (model produces identical generic outputs), retrieval bottleneck (generator consistency improves but end-to-end stalls), over-constraining (BLEU-4 or Exact Match rewards hurt both consistency and accuracy)
- **First 3 experiments:** 1) Baseline diagnostic—measure retriever, generator, and end-to-end consistency to identify your bottleneck before training 2) Reward ablation—compare BLEU-1 vs. BLEU-2 vs. ROUGE-L to select similarity metric for your domain 3) Paraphrase quality check—manually inspect generated paraphrases for semantic preservation

## Open Questions the Paper Calls Out
- **Open Question 1:** Can jointly optimizing the retriever and generator for consistency yield greater end-to-end improvements than generator-only optimization?
- **Open Question 2:** What reward signals beyond BLEU can directly optimize information-level consistency without penalizing valid lexical variations?
- **Open Question 3:** Does Con-RAG generalize to naturally occurring user paraphrases, or does it overfit to synthetic LLM-generated paraphrase distributions?
- **Open Question 4:** At what point does the subsampling approximation for group similarity rewards degrade training effectiveness?

## Limitations
- The method's effectiveness depends heavily on paraphrase quality—if generated paraphrases alter answer semantics, the reward signal becomes invalid
- BLEU similarity may not capture semantic equivalence across all domains and could penalize valid paraphrases
- Training configurations (duration, batch sizes, data splits) are unspecified, making hyperparameter tuning uncertain
- The approach assumes consistent retrieval is possible given semantically equivalent queries, but if retrieval drift is inherent to the corpus, generator consistency gains may not translate to end-to-end improvements

## Confidence
- **High confidence:** The decomposition of consistency into retriever/generator/end-to-end components and the PS-GRPO method's mathematical formulation are well-specified and theoretically sound
- **Medium confidence:** Empirical improvements on benchmarks are demonstrated, but the reliance on BLEU similarity and the absence of detailed training configurations introduce uncertainty about generalizability
- **Low confidence:** The long-form QA results (ELI5) depend solely on consistency rewards without accuracy supervision, which may not generalize to domains where factual accuracy is paramount

## Next Checks
1. **Paraphrase Semantic Preservation Test:** Manually annotate a subset of generated paraphrases for semantic equivalence to ensure the reward signal isn't corrupted by semantic drift
2. **Reward Ablation with Semantic Similarity:** Replace BLEU with a semantic similarity metric (e.g., sentence-BERT) on a held-out set to quantify the impact of surface-form vs. semantic consistency rewards
3. **Training Dynamics Monitoring:** Track reward distribution statistics, output diversity metrics, and consistency/accuracy curves throughout training to detect reward hacking or mode collapse