---
ver: rpa2
title: ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions
  Challenge
arxiv_id: '2505.18217'
source_url: https://arxiv.org/abs/2505.18217
tags:
- speech
- emotion
- challenge
- loss
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The Abhinaya system addresses the challenge of speech emotion
  recognition (SER) in naturalistic conditions by leveraging large language models
  (LLMs) and speech large language models (SLLMs). The approach combines five models:
  two speech-based (using WavLM-Large and SALMONN SLLMs), two text-based (using LLaMA
  models), and one multimodal model (SALMONN with speech and text).'
---

# ABHINAYA -- A System for Speech Emotion Recognition In Naturalistic Conditions Challenge

## Quick Facts
- arXiv ID: 2505.18217
- Source URL: https://arxiv.org/abs/2505.18217
- Reference count: 0
- Ranked 4th out of 166 submissions with macro-F1 score of 44.02%

## Executive Summary
The Abhinaya system addresses the challenge of speech emotion recognition (SER) in naturalistic conditions by leveraging large language models (LLMs) and speech large language models (SLLMs). The approach combines five models: two speech-based, two text-based, and one multimodal model. To handle class imbalance, the system explores various loss functions including weighted cross-entropy, weighted focal loss, and vector scaling loss. The models are fine-tuned using self-supervised learning representations and large language models, with attentive pooling for utterance-level aggregation.

## Method Summary
The Abhinaya system employs an ensemble approach combining five distinct models: WavLM-Large and SALMONN for speech-based processing, LLaMA models for text-based analysis, and a multimodal SALMONN model that processes both speech and text inputs. The system addresses class imbalance in naturalistic SER datasets through specialized loss functions including weighted cross-entropy, weighted focal loss, and vector scaling loss. Self-supervised learning representations are used for fine-tuning, with attentive pooling aggregating frame-level predictions to the utterance level. The final predictions are generated through majority voting across all ensemble components.

## Key Results
- Ranked 4th out of 166 submissions in Interspeech 2025 Naturalistic SER Challenge
- Achieved macro-F1 score of 44.02%, representing 33.68% relative improvement over baseline
- Demonstrated effectiveness of combining speech-only, text-only, and speech-text models with specialized loss functions

## Why This Works (Mechanism)
The system leverages the complementary strengths of multiple model architectures to capture different aspects of emotion expression in speech. Speech-based models capture prosodic and acoustic features, text-based models analyze linguistic content, and the multimodal model integrates both modalities. The use of specialized loss functions addresses the inherent class imbalance in naturalistic datasets, where certain emotions occur more frequently than others. The ensemble approach through majority voting provides robustness against individual model weaknesses.

## Foundational Learning
- **Speech Large Language Models (SLLMs)**: Why needed - to capture complex acoustic patterns in naturalistic speech; Quick check - verify model can distinguish between similar emotion categories
- **Attentive Pooling**: Why needed - to aggregate frame-level predictions into meaningful utterance-level representations; Quick check - ensure pooling mechanism preserves emotion-relevant information
- **Weighted Loss Functions**: Why needed - to handle class imbalance in naturalistic SER datasets; Quick check - verify minority classes receive appropriate attention during training
- **Multimodal Integration**: Why needed - to combine acoustic and linguistic cues for more robust emotion recognition; Quick check - ensure both modalities contribute meaningfully to final predictions

## Architecture Onboarding

**Component Map**: Speech input -> WavLM-Large/SALMONN -> Frame-level predictions -> Attentive pooling -> Utterance-level predictions
Text input -> LLaMA models -> Text representations -> Emotion predictions
Speech+Text input -> SALMONN multimodal -> Joint representations -> Emotion predictions
All outputs -> Majority voting -> Final prediction

**Critical Path**: Raw audio/text → Feature extraction → Frame-level predictions → Attentive pooling → Loss computation → Backpropagation → Model updates

**Design Tradeoffs**: Ensemble approach provides robustness but increases computational complexity; multimodal model captures more information but requires synchronized audio-text data; specialized loss functions improve minority class performance but may affect overall accuracy

**Failure Signatures**: Poor performance on underrepresented emotion categories; degradation when audio quality is low; confusion between similar emotion classes; sensitivity to domain mismatch

**First Experiments**:
1. Validate individual model performance on held-out validation set
2. Test ensemble performance with different voting strategies
3. Analyze class-wise performance to identify weaknesses

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on single challenge dataset limits generalizability to other naturalistic conditions
- Equal weighting in majority voting assumes all models contribute equally without evidence
- Limited analysis of individual emotion category performance and model-specific strengths

## Confidence
- High confidence: 4th place ranking out of 166 submissions is objectively verifiable
- Medium confidence: Technical approach using LLM/SLLM fine-tuning is well-established but implementation details may vary
- Medium confidence: Macro-F1 score of 44.02% is internally consistent but requires external validation

## Next Checks
1. Test the system on independent naturalistic SER datasets (e.g., MSP-Podcast, IEMOCAP) to verify cross-dataset generalization
2. Perform ablation studies to quantify the individual contribution of each model type and loss function
3. Conduct error analysis to identify which emotion categories benefit most from the ensemble approach and whether specific model combinations show complementary strengths