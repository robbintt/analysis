---
ver: rpa2
title: 'GridMind: LLMs-Powered Agents for Power System Analysis and Operations'
arxiv_id: '2509.02494'
source_url: https://arxiv.org/abs/2509.02494
tags:
- power
- system
- agent
- agents
- acopf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GridMind demonstrates that multi-agent AI systems can effectively
  orchestrate complex power system analyses through conversational interfaces while
  maintaining numerical precision. The system achieves 100% success rates across all
  tested IEEE cases for both ACOPF and contingency analysis tasks, with smaller language
  models delivering comparable analytical accuracy at significantly reduced computational
  latency.
---

# GridMind: LLMs-Powered Agents for Power System Analysis and Operations

## Quick Facts
- arXiv ID: 2509.02494
- Source URL: https://arxiv.org/abs/2509.02494
- Authors: Hongwei Jin; Kibaek Kim; Jonghwan Kwon
- Reference count: 40
- Multi-agent AI system achieves 100% success rates for power system analysis via conversational interfaces

## Executive Summary
GridMind introduces a multi-agent AI system that enables domain experts to perform complex power system analyses through natural language interfaces while maintaining the numerical precision required for critical infrastructure applications. The system orchestrates specialized agents for AC Optimal Power Flow (ACOPF) and N-1 contingency analysis, using structured tool delegation to prevent hallucination of numerical results. By integrating validated engineering solvers with typed context schemas and conversational planning, GridMind successfully handles all tested IEEE cases from 14 to 300 buses while demonstrating that smaller language models can deliver comparable accuracy with significantly reduced computational latency.

## Method Summary
GridMind employs a multi-agent architecture using PydanticAI framework with specialized agents for ACOPF optimization and contingency analysis, backed by PandaPower deterministic solvers. The system uses structured schemas (Pydantic data models) to enforce type safety and prevent numerical hallucination by decoupling LLM reasoning from solver computation. User queries are processed through a planner agent that routes intent to specialized agents, which then invoke validated tool calls and update a structured context state. The approach was tested across IEEE test cases (14, 30, 57, 118, 300-bus systems) using various LLM models via API, with validation against power balance tolerance (10^-4 p.u.) and constraint satisfaction checks.

## Key Results
- 100% success rate across all tested IEEE cases for both ACOPF and contingency analysis tasks
- Smaller language models (GPT-5-mini) deliver comparable analytical accuracy to larger models with significantly reduced latency
- System maintains numerical precision through function calling rather than hallucination, with validated solver outputs
- Cross-agent coordination successfully preserves state consistency through typed context schemas

## Why This Works (Mechanism)

### Mechanism 1
Structured tool delegation appears to decouple natural language reasoning from numerical computation, reducing hallucination risks in engineering contexts. The LLM parses user intent, maps it to structured JSON function calls, and invokes deterministic solvers rather than calculating power flow directly. This constrains the LLM to a "reason-act-reflect" loop where numerical claims are grounded in external tool outputs.

### Mechanism 2
Strongly-typed context schemas likely enable coherent multi-turn analysis by enforcing state consistency across agent handoffs. The system uses Pydantic data models to serialize session state, including validated numerical artifacts like dispatch and voltage. Agents must consume this structured state, preventing them from inventing prior system conditions.

### Mechanism 3
Smaller language models may offer a better latency-accuracy trade-off for well-defined agentic workflows than larger models. In constrained agentic loops (Plan -> Tool Call -> Interpret), the heavy lifting is done by deterministic tools, requiring only reliable function-calling capabilities and basic reasoning from the LLM.

## Foundational Learning

- **Concept: AC Optimal Power Flow (ACOPF)**
  - Why needed here: Primary domain task requiring non-linear optimization minimizing cost subject to physical grid limits
  - Quick check question: Can you distinguish between the LLM's role (intent parsing) and the interior-point solver's role (converging on cost minimization)?

- **Concept: Schema Validation (Pydantic)**
  - Why needed here: Pydantic enforces "type safety" between agents by validating solver outputs against defined models
  - Quick check question: If the solver returns a voltage of 1.5 p.u. (invalid), does the system crash, or does schema validation catch it?

- **Concept: Agentic Tool Use (Function Calling)**
  - Why needed here: Core mechanism where LLM outputs JSON to trigger functions rather than text
  - Quick check question: Does the LLM generate power flow calculation code on the fly, or select a pre-registered method?

## Architecture Onboarding

- **Component map:**
  - User Interface -> Orchestrator (Planner Agent) -> Specialized Agents (ACOPF, CA) -> Tools (PandaPower solvers) -> Memory (Structured AgentContext)

- **Critical path:**
  1. User inputs natural language query
  2. Planner classifies domain and selects Agent
  3. Agent updates/generates Tool Call (JSON)
  4. Tool executes deterministic solver
  5. Result validated against Schema
  6. Validated result updates Context
  7. Agent synthesizes natural language response

- **Design tradeoffs:**
  - Smaller vs. Larger Models: Smaller models offer speed for routine tasks, trading deep reasoning for latency
  - Rigidity vs. Safety: Strict schemas prevent hallucination but require explicit definition of all data fields

- **Failure signatures:**
  - Schema Mismatch: Tool returns field not in Pydantic model; system raises validation error
  - Solver Divergence: ACOPF fails to converge; agent detects via convergence_flag rather than hallucinating solution
  - Context Truncation: Long "what-if" chains lose early modifications, leading to inconsistent state

- **First 3 experiments:**
  1. **Baseline ACOPF:** Run "Solve IEEE 14" and verify agent calls `solve_acopf_case` and returns objective cost from solver
  2. **Context Persistence:** Ask "Increase load at Bus 2 by 10%", then "What is load at Bus 2?" Verify agent retrieves modified value from Context
  3. **Cross-Agent Handoff:** Run "Solve IEEE 118" then "Check reliability". Confirm CA agent receives dispatch results from ACOPF context rather than re-solving

## Open Questions the Paper Calls Out

### Open Question 1
How can systems effectively detect and mitigate reasoning hallucinations in LLM agents when numerical fabrication is suppressed but logical planning remains fallible? The paper acknowledges agents may hallucinate in reasoning steps, but current validation mechanisms verify numerical outputs without validating semantic logic or strategic soundness of the agent's internal plan.

### Open Question 2
Do smaller language models fail to identify standard critical contingencies, or are they capable of discovering valid, non-obvious stress conditions that larger models miss? GPT-5 Mini identified a unique set of critical lines and higher max overload compared to other models, but the paper doesn't determine if this represents failure or superior detection.

### Open Question 3
How does structured context management and cross-agent coordination overhead scale when applied to large-scale, real-world transmission networks? Evaluation is limited to IEEE test cases (14-300 buses); real-world operations involve significantly larger topologies where context window limits may become bottlenecks.

## Limitations
- Tool Coverage Gap: Success rate assumes all user intents can be mapped to predefined tool schemas
- Performance under Scale: System achieves 100% success on IEEE cases up to 300 buses but scaling behavior for real-world networks is untested
- Cost and Deployment Feasibility: Operational costs and potential rate limiting concerns for maintaining multi-agent system in production are not discussed

## Confidence
- High Confidence: Core mechanism of structured tool delegation and 100% success rate on IEEE test cases are directly demonstrated
- Medium Confidence: Latency comparison between model sizes is supported, but generalizability to complex workflows is assumed
- Low Confidence: Claims about handling ambiguous or novel user requests are not substantiated

## Next Checks
1. **Schema Coverage Analysis**: Measure current tool schema coverage against comprehensive set of real-world power system operational queries
2. **Context Window Stress Test**: Perform 20+ consecutive "what-if" modifications on 300-bus case to verify diff log maintains consistency
3. **Scaling Performance Benchmark**: Benchmark system on synthetic networks scaled from 300 to 5000 buses to measure success rate, latency, and context memory usage