---
ver: rpa2
title: 'Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn
  Interaction'
arxiv_id: '2511.23476'
source_url: https://arxiv.org/abs/2511.23476
tags:
- reasoning
- interaction
- arxiv
- move
- right
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building efficient world
  model reasoning in large language model (LLM) agents for complex, interactive environments.
  Current approaches often rely on rigid reasoning processes that limit active learning
  and hinder efficiency.
---

# Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction

## Quick Facts
- arXiv ID: 2511.23476
- Source URL: https://arxiv.org/abs/2511.23476
- Reference count: 40
- Key outcome: WMAct achieves single-turn performance on complex tasks that previously required multiple interactions through multi-turn interaction training with reward rescaling and frequency annealing.

## Executive Summary
This paper addresses the challenge of building efficient world model reasoning in LLM agents for complex interactive environments. Current approaches rely on rigid reasoning processes that limit active learning and hinder efficiency. WMAct proposes that models should shape thinking through direct interaction rather than pre-defined cognitive patterns. The approach employs reward rescaling based on action efficacy and interaction frequency annealing to progressively reduce allowed interaction turns, encouraging internalization of environmental dynamics. Evaluated across Sokoban, Maze, and Taxi environments, WMAct achieves strong performance, enabling tasks to be solved in a single turn that previously required multiple interactions, while also generalizing to more challenging tasks and improving performance on general reasoning benchmarks.

## Method Summary
WMAct trains LLM agents through multi-turn interaction with environment feedback, using two key mechanisms: reward rescaling that scales outcome rewards by the proportion of effective (state-changing) actions to incentivize purposeful exploration, and interaction frequency annealing that progressively reduces the maximum allowed interaction turns by averaging recent episode statistics. The training uses strict on-policy PPO optimization with GAE advantage estimation, where the model generates thinking traces and actions in structured tags, receives observations after each turn, and learns to internalize environmental dynamics that transfer to single-turn reasoning capability.

## Key Results
- WMAct achieves single-turn performance approaching multi-turn training performance across Sokoban, Maze, and Taxi environments
- The approach successfully generalizes to more complex tasks beyond training environments
- Performance improvements transfer to general reasoning benchmarks
- Qwen2.5-7B-Instruct shows limited internalization, indicating dependency on base model reasoning capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling outcome rewards by action efficacy incentivizes purposeful, low-redundancy exploration and improves generalization.
- Mechanism: The reward is rescaled as `R_scaled = R_outcome × (N_effective / N_total)`, where an action is "effective" if it leads to a different state. This penalizes brute-force enumeration and favors strategies that yield meaningful state progress.
- Core assumption: Effective actions (state-changing actions) correlate with higher-quality environmental knowledge acquisition and better generalization.
- Evidence anchors:
  - [abstract] "...a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction"
  - [section 3.3] "To encourage more effective interaction strategies, we propose scaling the outcome reward by the proportion of effective actions within an episode."
  - [corpus] Weak direct evidence in neighbors; related work on RL reward shaping exists but not this specific formula.
- Break condition: If the environment has sparse state changes where many necessary actions don't change state (e.g., opening a door with no immediate effect), the reward signal may be misleading.

### Mechanism 2
- Claim: Progressive reduction of allowed interaction turns compels the model to internalize environmental dynamics rather than over-relying on environmental feedback.
- Mechanism: Every τ iterations, compute average turns L̄ and maximum turns L′_max from recent episodes, then set new limit: `L_max = (L̄ + L′_max) / 2`. This creates curriculum-style pressure—early exploration permitted, later internalization required.
- Core assumption: Constraining interaction forces the model to develop internal simulation capabilities that substitute for real feedback.
- Evidence anchors:
  - [abstract] "...an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics"
  - [section 3.3] "The annealing process ensures the model gradually internalizes environmental dynamics."
  - [corpus] No direct corpus evidence for this specific annealing mechanism in neighbors.
- Break condition: If τ is too short for complex tasks, the model may fail to acquire necessary skills before the limit tightens; if too long, over-reliance on interaction persists.

### Mechanism 3
- Claim: Multi-turn interaction with feedback enables world model learning that transfers to single-turn reasoning.
- Mechanism: The model generates sequential thinking steps T_t and action sets A_t, receiving observations o_t after each turn. Training optimizes the full trajectory. Over time, the single-turn performance (without interaction) converges toward multi-turn performance, indicating internalization.
- Core assumption: Feedback-driven error correction during training enables the model to construct compressed internal representations of environmental dynamics.
- Evidence anchors:
  - [abstract] "WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions"
  - [section 4.2, Figure 4] "The single-turn performance of our method...gradually approaches its own multi-turn training performance across all environments."
  - [corpus] Thinking vs. Doing (50509) discusses test-time interaction scaling, supporting the general premise that interaction aids reasoning.
- Break condition: If the base model lacks reflection/self-correction patterns (e.g., Qwen2.5-7B-Instruct per Section 5), internalization fails regardless of interaction.

## Foundational Learning

- Concept: **Policy Gradient & PPO Basics**
  - Why needed here: WMAct uses strict on-policy PPO for trajectory optimization. Understanding advantage estimation (GAE), clipping, and baseline subtraction is required to modify the reward structure.
  - Quick check question: Can you explain why subtracting a baseline reduces variance without introducing bias in policy gradient estimation?

- Concept: **World Model Reasoning**
  - Why needed here: The paper's core goal is internalizing environment dynamics (state transitions, spatial constraints) into the model's reasoning, not just completing tasks reactively.
  - Quick check question: What's the difference between an agent that "knows" pushing a box into a corner creates a deadlock vs. one that discovers this only through trial-and-error?

- Concept: **Multi-turn RL & Trajectory-level Optimization**
  - Why needed here: Unlike single-shot RLHF, WMAct optimizes over full multi-turn trajectories with delayed rewards and sequential dependencies.
  - Quick check question: In a multi-turn setting, how should credit assignment work when the final reward depends on actions taken 10 turns earlier?

## Architecture Onboarding

- Component map:
  - LLM Agent -> Environment Interface -> Reward Calculator -> Turn Counter & Annealer -> PPO Trainer

- Critical path:
  1. Environment generates initial state (ASCII prompt)
  2. LLM outputs thinking + action(s) → Environment validates/executes
  3. Repeat until task done or `L_max` reached
  4. Compute rescaled reward, calculate advantages via GAE
  5. PPO policy update on collected trajectories
  6. Every τ steps: recalculate `L_max` from recent episode statistics

- Design tradeoffs:
  - **Annealing interval τ**: Smaller τ (50) risks disrupting complex skill acquisition; larger τ (150) delays internalization. Paper finds τ=100 optimal.
  - **Initial turn limit**: Too low blocks exploration; too high enables over-reliance on interaction. Paper starts at 30.
  - **Reward scaling vs. step penalty**: Scaling is harder to game (based on actual state changes) vs. fixed penalty which encourages myopic behavior (Table 4).

- Failure signatures:
  - **No internalization gap**: Single-turn accuracy never approaches multi-turn accuracy (see Qwen2.5-7B in Figure 5) → base model lacks necessary reasoning patterns
  - **Brute-force persistence**: Model continues enumerating actions without strategic planning → reward rescaling may be insufficient or τ too large
  - **Premature convergence**: Performance plateaus below baseline → `L_max` may be shrinking too fast (τ too small)

- First 3 experiments:
  1. **Baseline validation**: Train PPO-Interactive without rescaling or annealing on a simple environment (e.g., Maze 11×11). Confirm multi-turn success but poor single-turn generalization.
  2. **Ablation sequence**: Add reward rescaling alone → measure Hard-task improvement. Then add frequency annealing → measure convergence speed and final single-turn accuracy.
  3. **Annealing sensitivity**: Sweep τ ∈ {50, 100, 150} on Sokoban Hard-2 (complex planning). Track both final accuracy and training stability to validate τ=100 robustness for your setup.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the WMAct framework succeed on base models lacking pre-existing "cognitive behaviors" (like reflection), or are these priors a strict requirement for world model internalization?
- Basis in paper: [explicit] The Discussion states these behaviors are "crucial" and their absence hinders environmental dynamics modeling.
- Why unresolved: Results show Qwen2.5-7B failed to internalize, but no method is proposed to overcome this lack of prior.
- What evidence would resolve it: Successful training of a weaker base model using a modified WMAct that explicitly teaches reflection.

### Open Question 2
- Question: How should the "reward rescaling" metric be defined for complex environments (e.g., web agents) where "effective action" is not as binary as grid-world state changes?
- Basis in paper: [inferred] The mechanism relies on $N_{eff}/N$ (Eq. 4), which depends on discrete, observable state changes specific to the Sokoban/Maze/Taxi domains.
- Why unresolved: The paper does not extend the definition of "efficacy" to continuous or semantic state spaces.
- What evidence would resolve it: A formulation of "action efficacy" based on semantic similarity or goal-progress embedding distances applied to open-domain tasks.

### Open Question 3
- Question: Does the interaction frequency annealing strategy require environment-specific hyperparameter tuning (e.g., $\tau$), or can a universal adaptive schedule be derived?
- Basis in paper: [inferred] The ablation on $\tau$ (Table 4) shows "performance degradation" with sub-optimal intervals, indicating sensitivity to the task complexity.
- Why unresolved: The paper demonstrates the need for tuning but does not propose an adaptive mechanism to set $\tau$ automatically.
- What evidence would resolve it: An adaptive algorithm that adjusts the turn limit based on real-time convergence metrics across diverse tasks.

## Limitations
- Architecture dependency: Strong performance variation across base models indicates WMAct may not be universally applicable
- Environment specificity: All experiments use grid-based environments with ASCII representations
- Reward engineering sensitivity: The reward rescaling formula may be misleading in environments where necessary actions don't immediately change state

## Confidence
- WMAct's superiority over PPO-Interactive is primarily due to reward rescaling: Medium
- The annealing strategy effectively forces internalization without degrading performance: Medium
- WMAct generalizes effectively to more complex tasks: Medium

## Next Checks
1. **Architecture Transfer Test**: Apply WMAct to a different LLM family (e.g., Llama or Mistral) with varying reasoning capabilities to quantify the dependency on base model architecture.

2. **Break Condition Stress Test**: Systematically vary the annealing interval τ across multiple orders of magnitude (e.g., τ ∈ {25, 50, 100, 200, 400}) on Sokoban Hard-2 to identify the precise failure modes when the annealing is too aggressive or too conservative.

3. **Generalization Benchmark**: Evaluate WMAct on a reasoning benchmark that requires world model reasoning but uses different input modalities (e.g., text-based adventure games or visual reasoning tasks) to test true cross-domain transfer capability.