---
ver: rpa2
title: Imitation from Observations with Trajectory-Level Generative Embeddings
arxiv_id: '2601.00452'
source_url: https://arxiv.org/abs/2601.00452
tags:
- expert
- learning
- offline
- reward
- suboptimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses offline imitation learning from observations
  (LfO), where expert demonstrations are scarce and only state observations are available,
  while the offline dataset consists of suboptimal trajectories. The key challenge
  is extracting a reliable learning signal when expert and suboptimal data have disjoint
  support.
---

# Imitation from Observations with Trajectory-Level Generative Embeddings

## Quick Facts
- arXiv ID: 2601.00452
- Source URL: https://arxiv.org/abs/2601.00452
- Reference count: 40
- One-line primary result: Trajectory-level generative embeddings enable robust offline imitation learning from observations when expert data is scarce and has disjoint support from the offline dataset

## Executive Summary
This paper addresses the challenge of offline imitation learning from observations (LfO), where expert demonstrations are scarce and only state observations are available, while the offline dataset consists of suboptimal trajectories. The key insight is that training a diffusion model on suboptimal trajectories yields embeddings that naturally separate expert-like from suboptimal behaviors without explicit supervision. By constructing a kernel-based surrogate reward using distances in this embedding space, the method enables robust policy learning even when expert and suboptimal data have disjoint support.

## Method Summary
The method trains a trajectory diffusion model on suboptimal state-action data, then uses the encoder to extract trajectory embeddings. A logarithmic distance kernel over nearest-neighbor embeddings approximates particle-based cross-entropy minimization between learner and expert distributions. This reward signal is combined with an offline RL algorithm (IQL or ReBRAC) to train a policy that imitates expert behavior without requiring direct expert-action pairs. The approach is specifically designed for settings where expert demonstrations are limited and the offline dataset has disjoint support from expert states.

## Key Results
- TGE consistently matches or outperforms state-of-the-art offline LfO baselines across D4RL locomotion and manipulation benchmarks
- The method shows particular strength in settings with limited expert data (0, 30, or 200 expert trajectories)
- Ablation studies confirm the importance of temporal context, with longer horizons (H=32) significantly outperforming single-step embeddings (H=1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training a diffusion model on suboptimal trajectories yields embeddings that naturally separate expert-like from suboptimal behaviors without explicit discriminative supervision
- Mechanism: The denoising score-matching objective forces the encoder to preserve temporally coherent features necessary for trajectory reconstruction. Since the model must support recovery of clean trajectories across noise levels, the latent space organizes around underlying dynamic structure rather than superficial state features
- Core assumption: The diffusion training objective induces semantically meaningful trajectory representations that correlate with behavioral quality
- Evidence anchors:
  - [abstract] "TGE... constructs a smooth, dense surrogate reward using the latent space of a temporal diffusion model"
  - [Section 4.2] "the encoder-induced embedding z = φθ(τ0, 0) naturally separates expert-like samples from suboptimal ones in the mixed dataset"
  - [corpus] Related work on "Density-Ratio Weighted Behavioral Cloning" confirms handling corrupted datasets via learned representations, but TGE's unsupervised separation property is not directly validated in corpus literature
- Break condition: If the suboptimal dataset lacks any expert-like transitions, the embedding may not develop a meaningful expert cluster, causing reward signal collapse

### Mechanism 2
- Claim: A logarithmic distance kernel over nearest-neighbor embeddings approximates particle-based cross-entropy minimization between learner and expert distributions
- Mechanism: Given n samples from the policy's latent distribution, cross-entropy H(ρπ, ρE) can be estimated via k-nearest neighbor distances. The reward r(z) = -log(1 + (1/m)Σ||z - zE||) provides a heavy-tailed signal that avoids the sparse gradients of Gaussian kernels while maintaining theoretical grounding in entropy estimation
- Core assumption: The latent embedding space preserves meaningful distance relationships that reflect behavioral similarity, not just superficial state proximity
- Evidence anchors:
  - [Section 4.1] Equation (1)-(3) deriving the connection between cross-entropy and log-distance rewards
  - [Section 4.3] "We adopt a logarithmic kernel f(d) = -log(1 + d). This serves as a proxy for density estimation"
  - [corpus] "Self-Distilled Rewards for Offline RL" proposes related reward synthesis but from novelty rather than expert density estimation; corpus provides indirect support
- Break condition: If the embedding space suffers from state aliasing (functionally distinct states map close together), the distance-based reward provides misleading gradients

### Mechanism 3
- Claim: Trajectory-level embeddings capture long-horizon dynamics that enable reward signal extrapolation across disjoint support regions
- Mechanism: Unlike single-step occupancy matching that requires explicit support overlap, the smooth latent geometry provides graded signals even for states far from expert trajectories. The diffusion encoder's temporal context (horizon H) captures joint state transitions, making distances reflect functional similarity rather than local state similarity
- Core assumption: The generative planner's inductive bias toward temporally coherent features generalizes to unseen state regions in a semantically meaningful way
- Evidence anchors:
  - [Section 4.5] "the logarithmic entropy-based reward induces a smoothly decaying, graded signal that persists even for states far from the expert support"
  - [Figure 3] Ablation showing performance degrades severely with H=1 (single-step) but improves with longer horizons
  - [corpus] Related work on offline imitation from observations (SMODICE, DILO comparisons in Table 1) confirms support mismatch is a known failure mode for density-ratio methods
- Break condition: If the temporal horizon H is too short to capture task-relevant dynamics, or too long causing over-smoothing, the reward signal degrades

## Foundational Learning

- Concept: **Denoising Diffusion Probabilistic Models (DDPMs)**
  - Why needed here: TGE uses the encoder from a trajectory diffusion model; understanding the forward/reverse process and denoising objective is essential
  - Quick check question: Can you explain why the denoising score-matching objective L = E[||ε - εθ(xk, k)||²] encourages the latent space to preserve trajectory structure?

- Concept: **Particle-based entropy estimation**
  - Why needed here: The reward formulation derives from k-nearest neighbor entropy estimators; the log-distance kernel is not ad-hoc but theoretically grounded
  - Quick check question: Given samples from two distributions, how does average distance to nearest neighbors approximate cross-entropy?

- Concept: **Offline RL constraints (support coverage, distribution shift)**
  - Why needed here: TGE specifically addresses the failure mode where expert states are poorly covered by the offline buffer
  - Quick check question: Why do density-ratio methods like SMODICE fail when dπE(s) → 0 for most states in the offline dataset?

## Architecture Onboarding

- Component map:
  - **Diffusion Encoder (φθ)** -> **Embedding Extraction** -> **Reward Computation** -> **Offline RL (IQL/ReBRAC)** -> **Policy**

- Critical path:
  1. Train diffusion model on Dμ (4-6 hours on L40S)
  2. Encode all trajectories → compute Zμ and ZE
  3. For each transition in Dμ, compute rTGE via k-NN search
  4. Train offline RL policy on augmented dataset

- Design tradeoffs:
  - **Horizon H**: Longer H captures more temporal context but increases computation; H=32 is a stable default (Figure 3)
  - **Kernel choice**: Logarithmic vs Gaussian—log preserves heavy-tailed signal and improves separability (Appendix C.1)
  - **Temperature σ**: Robust across 0.1-5.0; default σ=1.0 balances discriminability and smoothness
  - **Backbone selection**: IQL is more stable; ReBRAC with adaptive BC weighting performs better on some tasks

- Failure signatures:
  - Reward collapse: All rewards cluster near a single value → check kernel distribution (Appendix C.1 visualizations)
  - No expert cluster in embedding → t-SNE shows no separation → suboptimal data may lack expert-like transitions entirely
  - Slow convergence: Training curves plateau early → horizon H may be too short

- First 3 experiments:
  1. **Sanity check**: Train diffusion encoder on Dμ; visualize embeddings via t-SNE colored by (known) trajectory quality. Verify separation exists before proceeding
  2. **Kernel ablation**: Compare log vs Gaussian kernel on a single environment (e.g., Walker2d-medium). Plot reward distributions and training curves
  3. **Horizon sensitivity**: Sweep H ∈ {1, 4, 16, 32} on two environments (one locomotion, one manipulation) to confirm task-appropriate context length

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the trajectory-level latent reward framework be effectively extended to online fine-tuning settings where the agent can interact with the environment after offline pre-training?
  - Basis in paper: [explicit] "Future work could explore extending this trajectory-level latent reward framework to online fine-tuning settings..."
  - Why unresolved: The current TGE framework is designed purely for offline LfO; the reward signal is computed once and fixed. Online fine-tuning would require dynamic reward updates as the agent explores new states outside the offline distribution
  - What evidence would resolve it: Experiments showing TGE-derived rewards can be updated or augmented during online interaction, with comparisons to pure offline performance and sample efficiency metrics

- **Open Question 2**: Can larger-scale video diffusion models enable imitation from cross-embodiment observations (e.g., human video to robot policy)?
  - Basis in paper: [explicit] "...leveraging larger-scale video diffusion models to enable imitation from cross-embodiment observations."
  - Why unresolved: Current TGE operates on low-dimensional state vectors from the same embodiment. Cross-embodiment requires handling visual observations and bridging embodiment-specific dynamics gaps
  - What evidence would resolve it: Demonstrations of TGE-style embedding rewards derived from video diffusion models successfully transferring behaviors across different robot morphologies or from human to robot

- **Open Question 3**: Why do trajectory-level diffusion embeddings naturally separate expert-like from suboptimal trajectories without any explicit discriminative supervision?
  - Basis in paper: [inferred] The paper empirically shows separability in Figure 2 but provides no theoretical explanation: "the encoder-induced embedding z = ϕθ(τ0, 0) naturally separates expert-like samples from suboptimal ones... even without any explicit discriminative supervision."
  - Why unresolved: The mechanism underlying this emergent separability is unclear—it could stem from trajectory smoothness, temporal coherence, or implicit density estimation properties of diffusion models, but no ablation or theoretical analysis confirms the cause
  - What evidence would resolve it: Systematic ablations probing which aspects of diffusion training (denoising objective, temporal context, reconstruction pressure) drive separability, or theoretical analysis connecting diffusion score matching to implicit discriminability

## Limitations
- The method relies on the implicit assumption that diffusion encoder embeddings will separate expert-like from suboptimal behaviors without explicit supervision, which lacks theoretical justification
- Performance depends on the quality and diversity of the suboptimal offline dataset—if it lacks any expert-like transitions, the embedding may not develop meaningful expert clusters
- The approach requires significant computational resources for training diffusion models and computing trajectory embeddings, particularly for longer temporal horizons

## Confidence
- **High Confidence**: The empirical evaluation showing consistent improvements over baselines across D4RL benchmarks, particularly in low-expert-data regimes. The ablation studies on temporal horizon H and kernel choice are methodologically sound
- **Medium Confidence**: The theoretical connection between log-distance rewards and particle-based cross-entropy estimation, though the practical approximation quality under finite samples warrants more rigorous analysis
- **Medium Confidence**: The claim that trajectory-level embeddings enable extrapolation beyond support mismatch, as this depends heavily on the quality of the generative model's inductive biases rather than direct optimization for the downstream task

## Next Checks
1. **Embedding quality validation**: Before proceeding with reward computation, verify that t-SNE visualizations of trajectory embeddings show clear separation between expert-like and suboptimal clusters. This sanity check should be standard practice for any new dataset
2. **Kernel sensitivity analysis**: Conduct a more thorough sweep of kernel parameters (m nearest neighbors, σ temperature) across multiple environments to establish the true robustness of the method to hyperparameter choices
3. **Temporal horizon analysis**: Systematically evaluate performance across different horizon lengths H for both locomotion and manipulation tasks to identify whether a single default value is truly appropriate or if task-specific tuning is necessary