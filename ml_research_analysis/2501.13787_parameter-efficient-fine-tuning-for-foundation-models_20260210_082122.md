---
ver: rpa2
title: Parameter-Efficient Fine-Tuning for Foundation Models
arxiv_id: '2501.13787'
source_url: https://arxiv.org/abs/2501.13787
tags:
- arxiv
- peft
- tuning
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of Parameter-Efficient
  Fine-Tuning (PEFT) techniques for Foundation Models (FMs), addressing the critical
  challenge of adapting large-scale pre-trained models to downstream tasks while minimizing
  computational costs. The core method idea revolves around selective parameter tuning,
  where only a small subset of model parameters is updated during fine-tuning, rather
  than the entire model.
---

# Parameter-Efficient Fine-Tuning for Foundation Models

## Quick Facts
- arXiv ID: 2501.13787
- Source URL: https://arxiv.org/abs/2501.13787
- Reference count: 40
- This survey comprehensively reviews PEFT techniques that achieve performance comparable to full fine-tuning while reducing trainable parameters by over 99%

## Executive Summary
This survey provides a systematic overview of Parameter-Efficient Fine-Tuning (PEFT) techniques for adapting large Foundation Models (FMs) to downstream tasks with minimal computational cost. The core insight is that foundation models possess low-rank structure and sparse parameter importance distributions, enabling effective adaptation through selective parameter tuning rather than full model updates. The survey categorizes PEFT methods into five families—Selective, Additive, Prompt, Reparameterization, and Hybrid—and evaluates their effectiveness across different FM types including LLMs, VFMs, VLMs, MFMs, and VGMs.

## Method Summary
The survey systematically analyzes PEFT techniques by categorizing them based on how they modify the foundation model during fine-tuning. It identifies five main approaches: selective methods that mask or freeze parameters, additive methods that insert bottleneck adapters, prompt methods that prepend learnable tokens, reparameterization methods that decompose weight updates into low-rank matrices (like LoRA), and hybrid approaches that combine multiple strategies. The analysis spans 40+ papers and evaluates performance metrics including parameter efficiency, task performance, and computational overhead across different FM architectures.

## Key Results
- PEFT methods can achieve performance comparable to full fine-tuning while reducing trainable parameters by over 99%
- LoRA reduces GPT-3's fine-tuning parameters from 175 billion to just 4.7 million while improving performance by 0.1% to 0.5%
- LLMs and VFMs dominate current PEFT research, with Multi-Modal Foundation Models remaining underexplored
- The survey identifies a critical lack of comprehensive benchmarks for standardized PEFT evaluation across FM types

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Structure Exploitation
- **Claim**: Foundation models possess low-rank internal structure that can be exploited for efficient adaptation.
- **Mechanism**: Reparameterization methods like LoRA assume pre-trained weights exist in a high-dimensional space where task-specific adaptations occupy a lower-dimensional subspace. By decomposing weight updates into low-rank matrices (B and A, where ΔW = BA), the model captures essential task knowledge while drastically reducing trainable parameters.
- **Core assumption**: The intrinsic dimensionality of downstream tasks is significantly lower than the full parameter space of the foundation model.
- **Evidence anchors**:
  - [section D.1]: "LoRA capitalizes on the low-rank structure inherent in many machine learning problems... Aghajanyan et al. [147] delve into the intrinsic dimensionality and demonstrate that natural language tasks can be tackled with a surprisingly small number of parameters."
  - [abstract]: "PEFT, a cost-effective fine-tuning technique, minimizes parameters and computational complexity while striving for optimal downstream task performance."
  - [corpus]: Limited direct corpus evidence on low-rank mechanisms; related work focuses on application benchmarks.
- **Break condition**: When task adaptation requires modifying high-rank features or when the pre-trained model lacks sufficient foundational capability for the target domain.

### Mechanism 2: Selective Parameter Importance
- **Claim**: Only a small subset of pre-trained parameters are critical for any specific downstream task.
- **Mechanism**: Selective PEFT methods identify and update only task-critical parameters while freezing the rest. This works because pre-trained models develop specialized neurons and layers that respond differently to different tasks—some parameters encode general features, others task-specific patterns.
- **Core assumption**: Pre-training creates a sparse distribution of task-relevant parameters that can be identified through gradient-based importance metrics (Fisher information) or architectural heuristics (layer position, bias terms).
- **Evidence anchors**:
  - [section A]: "The fundamental assumption here is that certain parameters are particularly important for specific tasks in large pre-trained models, and adjusting these key parameters can yield satisfactory results."
  - [section A.1]: "FISH-based methods [70, 73] contend that the parameters impacting the final model output constitute only a subset of all parameters."
  - [corpus]: Parameter-Efficient Continual Fine-Tuning survey (arXiv:2504.13822) supports selective updating for sequential task adaptation.
- **Break condition**: When tasks require distributed changes across many parameters or when importance metrics fail to identify the correct parameter subset.

### Mechanism 3: Modular Knowledge Injection via Adapters
- **Claim**: Task-specific knowledge can be injected through lightweight, modular bypass networks without disrupting pre-trained representations.
- **Mechanism**: Additive PEFT inserts small bottleneck networks (down-project → non-linear activation → up-project) between frozen layers. The adapter learns to transform layer outputs in task-specific ways while residual connections preserve original information flow. This creates a modular architecture where different adapters can be swapped for different tasks.
- **Core assumption**: Foundation model layers produce sufficiently rich intermediate representations that can be efficiently transformed by low-dimensional projections.
- **Evidence anchors**:
  - [section B]: "Adapters are small parameter sets that can be inserted between the layers of FMs. They allow the network to be fine-tuned for a new task without modifying its original parameters."
  - [section B.3]: "This category integrates task-specific parameters into the model by adding lightweight adapter layers... thus preserving the integrity of the pre-trained knowledge."
  - [corpus]: Vision4PPG study demonstrates adapter-based fine-tuning enables vision foundation models to perform physiological signal analysis.
- **Break condition**: When adapter bottleneck dimensions are too small to capture necessary transformations, or when tasks require modifying the core representation space itself.

## Foundational Learning

- **Concept: Foundation Models and Pre-training Objectives**
  - **Why needed here**: Understanding what knowledge is already encoded helps determine which PEFT strategy is appropriate. Different FMs (LLMs vs. VFMs vs. MFMs) have different architectures and pre-training objectives that constrain adaptation approaches.
  - **Quick check question**: Can you explain why a model pre-trained on text-to-image generation (DALL-E) might require different PEFT strategies than one pre-trained on image classification (SAM)?

- **Concept: Transformer Architecture Components**
  - **Why needed here**: PEFT methods target specific transformer components (attention layers, FFN, LayerNorm). Knowing where each PEFT type inserts modifications is essential for debugging and optimization.
  - **Quick check question**: Which transformer components does LoRA typically modify, and which does Prefix Tuning modify?

- **Concept: Transfer Learning and Catastrophic Forgetting**
  - **Why needed here**: PEFT explicitly addresses the trade-off between adapting to new tasks and retaining pre-trained knowledge. Understanding this tension explains why methods freeze most parameters.
  - **Quick check question**: Why might full fine-tuning cause performance degradation on tasks the model was originally capable of?

## Architecture Onboarding

- **Component map**:
  - Selective PEFT: Directly masks or freezes subsets of existing weights (bias terms, specific layers, high-importance parameters via Fisher information)
  - Additive PEFT: Inserts bottleneck adapters in parallel or series with FFN/attention layers; includes residual connections
  - Prompt PEFT: Prepends learnable tokens to input embeddings (shallow) or all layer inputs (deep); can be text-based or visual
  - Reparameterization PEFT: Adds low-rank bypass branches to weight matrices (typically attention); merges at inference
  - Hybrid PEFT: Combines above methods with gating mechanisms to dynamically weight contributions

- **Critical path**:
  1. Identify foundation model type and architecture (LLM/VFM/VLM/MFM/VGM)
  2. Determine inference constraints (memory, latency, multi-task serving)
  3. Select PEFT category based on trade-offs (see Design Tradeoffs)
  4. Choose target modules (attention vs. FFN vs. both)
  5. Configure hyperparameters (rank for LoRA, bottleneck size for adapters, prompt length)
  6. Train with higher learning rates than full fine-tuning (typically 10-100x)

- **Design tradeoffs**:
  - **Inference overhead**: Additive PEFT increases latency; Reparameterization PEFT (LoRA) has zero overhead after weight merging; Selective PEFT has no overhead
  - **Parameter efficiency vs. performance**: Prompt tuning is most parameter-efficient (0.01-0.1%) but may underperform on complex tasks; adapters (0.1-3.5%) offer better performance; LoRA (0.02-0.5%) balances both
  - **Multi-task capability**: Adapters excel at task switching (swap adapter weights); LoRA requires separate copies or merging strategies
  - **Memory during training**: QLoRA enables 65B model fine-tuning on single 48GB GPU via 4-bit quantization; LoRA-FA reduces activation memory

- **Failure signatures**:
  - **Performance plateaus below full fine-tuning**: Rank/bottleneck too small; increase capacity or try hybrid approach
  - **Training instability with prompt tuning**: Initialize with class labels or vocabulary tokens; use longer prompts (20+ tokens)
  - **Slow convergence**: Learning rate too low (PEFT typically needs higher LR than full fine-tuning)
  - **Catastrophic forgetting in multi-task scenarios**: Using Selective PEFT that modifies shared parameters; switch to modular Additive approach
  - **Memory overflow despite PEFT**: Storing full activation graphs; switch to LoRA-FA or gradient checkpointing

- **First 3 experiments**:
  1. **Baseline comparison**: Apply LoRA (rank=8) to a single target module (attention) on your task; compare against full fine-tuning to establish performance gap
  2. **Capacity sweep**: Test LoRA with ranks [4, 8, 16, 32, 64] or adapter bottleneck sizes [16, 32, 64, 128] to find minimal sufficient capacity
  3. **Target module ablation**: Compare LoRA applied to (a) attention only, (b) FFN only, (c) both; document which components your task depends on most

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a unified, comprehensive benchmark be established to standardize the evaluation of PEFT methods across diverse Foundation Models?
- **Basis in paper:** [explicit] The authors explicitly state in Section VII-A that "there remains a notable lack of comprehensive benchmarks for PEFT," leading to inconsistent performance assessments due to varied datasets and task setups.
- **Why unresolved:** Current evaluation standards are fragmented across studies, making it difficult for users to fairly compare the strengths and weaknesses of different PEFT methods.
- **What evidence would resolve it:** The development of a standardized baseline framework with consistent datasets and evaluation metrics for LLMs, VFMs, and MFMs.

### Open Question 2
- **Question:** What are the internal mechanisms and functional roles of learned parameters within different PEFT categories (e.g., Adapters vs. Prompts)?
- **Basis in paper:** [explicit] Section VII-A highlights "Interpretability" as a major challenge, noting that prompts are often "unordered token-based" and the "relationship between learned parameters and layers in adapters" is not well understood.
- **Why unresolved:** Translating learned soft prompts into understandable formats and explaining how small adapter networks modify the model's function remains difficult.
- **What evidence would resolve it:** Mechanistic interpretability studies that successfully map learned PEFT parameters to specific functional changes or neuron activations within the Foundation Model.

### Open Question 3
- **Question:** How does PEFT performance scale relative to the number of trainable parameters, and does a universal optimal parameter range exist?
- **Basis in paper:** [explicit] The authors identify "Scaling Laws of PEFT" as a future direction in Section VII-B, asking, "how does performance scale when increasing or decreasing the number of trainable parameters in PEFT methods?"
- **Why unresolved:** While current efforts show diminishing returns beyond certain thresholds, the specific scaling behaviors for methods like LoRA or Adapters across different model sizes are not fully defined.
- **What evidence would resolve it:** Empirical data establishing scaling curves that correlate performance with parameter counts for various PEFT techniques across different model architectures.

## Limitations

- Domain Generalization Gap: PEFT effectiveness is primarily validated on in-distribution tasks with limited evidence for cross-domain robustness
- Multi-Modal MFM Coverage: The survey identifies MFMs as underexplored but provides minimal empirical validation of PEFT methods on this category
- Efficiency Claims Without Resource Context: Parameter reduction metrics are presented without consistent baseline comparisons or context about inference hardware constraints

## Confidence

**High Confidence (9/10)**: The categorization framework for PEFT methods (Selective, Additive, Prompt, Reparameterization, Hybrid) is well-supported by the literature and provides a useful taxonomy for understanding the field. The core observation that PEFT achieves comparable performance to full fine-tuning while using dramatically fewer parameters is consistently validated across multiple studies.

**Medium Confidence (6/10)**: Claims about the dominance of LLMs and VFMs in PEFT research are supported by citation analysis but may reflect publication bias rather than actual deployment patterns. The effectiveness of specific rank values and bottleneck sizes shows consistent patterns but lacks universal guidelines across all model types and tasks.

**Low Confidence (3/10)**: Predictions about future research directions, particularly regarding MFM opportunities, are speculative. The survey identifies gaps but cannot predict which specific approaches will prove most effective for emerging multi-modal architectures.

## Next Checks

1. **Cross-Domain Robustness Test**: Conduct systematic experiments evaluating LoRA and adapter performance when fine-tuned on one domain (e.g., natural images) and tested on structurally different domains (medical imaging, satellite imagery, synthetic data). Measure performance degradation and compare against full fine-tuning baselines.

2. **MFM-Specific Benchmarking Suite**: Create a standardized evaluation framework for PEFT methods on Multi-Modal Foundation Models using tasks that require genuine multi-modal reasoning (visual question answering with complex reasoning, cross-modal retrieval with semantic understanding). Document which PEFT categories perform best for different MFM architectures.

3. **Long-Tail Task Analysis**: Evaluate PEFT performance on tasks with limited training data (100-1000 examples) versus tasks with abundant data (10K-100K examples). Test whether the efficiency advantages of PEFT persist when data scarcity limits the ability to learn complex low-rank adaptations, and whether hybrid approaches outperform pure PEFT in low-data regimes.