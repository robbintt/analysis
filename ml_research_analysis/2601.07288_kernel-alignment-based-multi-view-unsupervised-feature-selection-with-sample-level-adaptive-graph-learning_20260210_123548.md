---
ver: rpa2
title: Kernel Alignment-based Multi-view Unsupervised Feature Selection with Sample-level
  Adaptive Graph Learning
arxiv_id: '2601.07288'
source_url: https://arxiv.org/abs/2601.07288
tags:
- feature
- selection
- multi-view
- methods
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses multi-view unsupervised feature selection
  (MUFS), which aims to select informative features from unlabeled multi-view data
  while reducing both linear and nonlinear redundancy. The proposed KAFUSE method
  tackles two key limitations of existing approaches: (1) inadequate handling of nonlinear
  feature redundancy, and (2) suboptimal cross-view similarity graph fusion using
  sample-invariant weights.'
---

# Kernel Alignment-based Multi-view Unsupervised Feature Selection with Sample-level Adaptive Graph Learning

## Quick Facts
- arXiv ID: 2601.07288
- Source URL: https://arxiv.org/abs/2601.07288
- Reference count: 40
- Primary result: KAFUSE achieves average ACC improvements of 11% over state-of-the-art methods on multi-view feature selection tasks

## Executive Summary
This paper addresses multi-view unsupervised feature selection (MUFS) by proposing KAFUSE, a method that tackles two key limitations of existing approaches: inadequate handling of nonlinear feature redundancy and suboptimal cross-view similarity graph fusion. The method introduces kernel alignment with orthogonal constraints to reduce both linear and nonlinear dependencies among features, while employing sample-level adaptive view weighting during cross-view fusion to produce more reliable similarity graphs. Extensive experiments on eight real-world datasets demonstrate that KAFUSE significantly outperforms state-of-the-art methods, with average accuracy improvements of 11% on several datasets and superior performance in preserving local data structure.

## Method Summary
KAFUSE integrates feature selection and consistent graph learning into a unified optimization framework. The method employs kernel alignment with orthogonal constraints to reduce both linear and nonlinear feature redundancy, while using sample-level adaptive view weighting to produce reliable cross-view consistent similarity graphs. The algorithm alternates between updating feature selection indicators, projection matrices, clustering indicators, consistent graph, view-specific graphs, sample-level weights, and view importance weights. After convergence, the top-l features are selected based on the diagonal values of the feature selection indicators across all views.

## Key Results
- KAFUSE achieves average ACC improvements of 11% over state-of-the-art methods on several datasets
- The method demonstrates superior performance in preserving local data structure compared to baselines
- Extensive experiments on eight real-world datasets validate the effectiveness of the proposed approach

## Why This Works (Mechanism)

### Mechanism 1
Kernel alignment with orthogonal constraints reduces both linear and nonlinear feature redundancy more effectively than orthogonal constraints alone. The method computes two Gaussian kernel matrices—one over selected features and one over unselected features—in a reproducing kernel Hilbert space (RKHS). By maximizing alignment between these centered kernels, highly redundant features are partitioned into separate subsets, while the orthogonal constraint handles linear independence.

### Mechanism 2
Sample-level adaptive view weighting during graph fusion produces more reliable cross-view consistent similarity graphs than sample-invariant weighting. Instead of assigning a single weight to all samples in a view, the method learns per-sample weight vectors for each sample. Views with clearer local neighborhoods for a given sample receive higher weights for that sample, adapting to varying neighborhood clarity across samples within each view.

### Mechanism 3
Joint optimization of feature selection and consistent graph learning enables mutual reinforcement between redundancy reduction and structure preservation. The unified objective couples feature selection indicators with both the redundancy term and the graph learning term, ensuring similarity graphs are learned in the selected feature space while graph quality feedback influences feature selection.

## Foundational Learning

- **Reproducing Kernel Hilbert Space (RKHS) and Kernel Methods**: The nonlinear redundancy mechanism relies on mapping features to RKHS via Gaussian kernels. Quick check: Can you explain why kernel methods can capture nonlinear relationships without explicitly computing nonlinear feature transformations?

- **Graph Laplacian and Manifold Learning**: The consistent graph learning uses Laplacian regularization to preserve local structure. Quick check: Given a similarity matrix S, how do you compute its graph Laplacian, and what does minimizing Tr(XLX^T) enforce?

- **Alternating Optimization and KKT Conditions**: The algorithm solves non-convex problems by alternating updates, with each subproblem using KKT conditions or specialized methods. Quick check: Why does alternating optimization guarantee convergence for each subproblem but not necessarily to a global optimum?

## Architecture Onboarding

- Component map: Multi-view Data {X^(v)} → [Feature Selection Module] ←→ [Graph Learning Module] → Selected Features
- Critical path: Initialize all variables → Iterate: Update W^(v) → F → Z → S^(v) → q_·j → θ_v → ω_v → Λ^(v) → Sort diagonal entries of Λ^(v) and select top-l features
- Design tradeoffs: Discrete vs. relaxed Λ^(v) constraint; parameter count (three manual parameters vs. auto-determined ones); per-sample weights (finer adaptation vs. additional variables)
- Failure signatures: Objective oscillation (check learning rate), poor performance (parameter mismatch), weights collapse to uniform (neighborhood structures too ambiguous)
- First 3 experiments: 1) Convergence validation on COIL20 dataset, 2) Ablation study comparing KAFUSE variants, 3) Parameter sensitivity analysis sweeping α, β, and r

## Open Questions the Paper Calls Out

- **Automatic feature selection ratio**: Can the optimal feature selection ratio be determined automatically to mitigate sensitivity to this parameter? The current methodology requires varying the ratio from 10% to 50% via grid search rather than learning the optimal dimensionality within the optimization framework.

- **Scalability to extreme-scale data**: Can the computational complexity of O(d^2n + n^2d) be reduced to handle extreme-scale multi-view data? The current approach may become prohibitive for datasets with n > 100,000 samples.

- **Handling incomplete multi-view data**: How does the sample-level adaptive weighting mechanism perform when specific views are completely missing for subsets of samples? The current formulation assumes complete datasets and doesn't account for undefined slices where data is missing.

## Limitations

- Kernel bandwidth parameter σ is never specified, making faithful reproduction difficult
- Sample-level adaptive weighting introduces O(n·V) additional variables with unclear sensitivity to initialization
- Empirical validation limited to clustering accuracy rather than feature selection quality metrics

## Confidence

- **High confidence**: Kernel alignment with orthogonal constraints reduces both linear and nonlinear redundancy (supported by objective formulation and ablation results)
- **Medium confidence**: Sample-level adaptive view weighting improves cross-view fusion quality (motivated by example but limited ablation comparison)
- **Medium confidence**: Joint optimization enables mutual reinforcement between feature selection and graph learning (consistent with multi-view learning literature but not explicitly validated)

## Next Checks

1. **Convergence validation**: Run Algorithm 1 on a single dataset, plot objective value vs. iteration, confirm stabilization within ~20 iterations as reported in Figure 4

2. **Ablation study**: Compare KAFUSE vs. KAFUSE-I (no kernel alignment) vs. KAFUSE-II (no sample-level fusion) to isolate component contributions per Table IV

3. **Parameter sensitivity**: Sweep α∈{10^-3,...,10^3}, β∈{10^-3,...,10^3}, r∈{2,...,9} on a validation set; identify stable operating regions per Figure 5-6