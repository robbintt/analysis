---
ver: rpa2
title: The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning
arxiv_id: '2502.00345'
source_url: https://arxiv.org/abs/2502.00345
tags:
- tasks
- subtask
- task
- methods
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Composite Task Challenge (CTC), a novel
  testbed for evaluating cooperative multi-agent reinforcement learning (MARL) methods
  in scenarios requiring division of labor (DOL) and cooperation. The CTC tasks are
  designed by composing atomic subtasks in ways that make DOL and cooperation necessary
  for task completion, unlike existing benchmarks where DOL is optional.
---

# The Composite Task Challenge for Cooperative Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.00345
- Source URL: https://arxiv.org/abs/2502.00345
- Reference count: 24
- Key outcome: CTC tasks require mandatory DOL and cooperation, with all 10 evaluated MARL methods achieving 0% success rate

## Executive Summary
This paper introduces the Composite Task Challenge (CTC), a novel testbed designed to evaluate cooperative multi-agent reinforcement learning methods in scenarios requiring mandatory division of labor and cooperation. Unlike existing benchmarks where DOL is optional, CTC tasks are constructed by composing atomic subtasks in ways that make DOL and cooperation necessary for task completion. The authors systematically evaluate 10 representative MARL methods across policy diversity, agent grouping, and hierarchical paradigms, finding that all baselines fail to solve CTC tasks despite their theoretical incorporation of DOL principles.

## Method Summary
The CTC tasks are designed by composing atomic subtasks (such as coverage, defense, and survival) in ways that require division of labor and cooperation for successful completion. The tasks incorporate three key diversity factors: information interference (cross-agent observational noise), subtask dissimilarity (heterogeneous subtask structures), and subtask quantity (2-4 subtasks per task). The authors evaluate 10 MARL baselines including VDN, QMIX, QPLEX, MADDPG, MAAC, IPPO, MAPPO, ADDC, DAGGER, and HIER. Simplified variants with homogeneous agents or symmetrical enemy distributions are also tested to validate task solvability while confirming the original CTC tasks' difficulty.

## Key Results
- All 10 evaluated MARL methods achieve 0% success rate on CTC tasks
- Simplified variants with homogeneous agents or symmetrical enemy distributions show significantly improved performance
- Stability coefficient analysis reveals poor performance consistency across seeds for most methods
- Task diversity factors (information interference, subtask dissimilarity, and quantity) contribute to difficulty

## Why This Works (Mechanism)
The CTC tasks are designed to create scenarios where successful completion requires mandatory division of labor and cooperation between agents. By composing atomic subtasks in ways that create observational noise, heterogeneous structures, and multiple simultaneous objectives, the tasks force agents to specialize and coordinate rather than adopting homogeneous strategies. The three diversity factors work synergistically to create complex environments where traditional MARL methods that don't explicitly handle DOL fail.

## Foundational Learning
- **Division of Labor (DOL)**: Specialization of agents to different subtasks for efficient task completion
  - Why needed: CTC tasks require agents to specialize rather than adopt homogeneous strategies
  - Quick check: Success rate comparison between homogeneous and heterogeneous agent settings

- **Cooperative Multi-Agent RL**: Learning algorithms where multiple agents collaborate to maximize shared returns
  - Why needed: CTC focuses on fully cooperative scenarios requiring coordination
  - Quick check: Baseline MARL method performance on simplified variants

- **Task Composition**: Building complex tasks from atomic subtasks with varying structures
  - Why needed: CTC creates diversity by combining different subtask types
  - Quick check: Impact of varying subtask quantity (2-4 subtasks) on difficulty

## Architecture Onboarding
**Component Map**: CTC Task Design -> MARL Method Evaluation -> Performance Analysis
**Critical Path**: Task generation (atomic subtasks) -> Agent training (MARL algorithms) -> Success rate measurement
**Design Tradeoffs**: 
- Complex task composition vs. task solvability
- Diversity factors vs. computational complexity
- Simplified variants vs. real-world applicability

**Failure Signatures**: 
- Homogeneous agent policies despite heterogeneous task requirements
- Poor coordination leading to task failure
- Inconsistent performance across random seeds

**First 3 Experiments**:
1. Evaluate VDN on a 2-subtask CTC variant with symmetrical enemy distribution
2. Test QMIX with heterogeneous agents on a 3-subtask composition
3. Compare stability coefficients across different random seeds for MADDPG

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to fully cooperative settings without mixed or competitive scenarios
- Unclear whether failures stem from algorithmic limitations versus implementation details
- Task designs may be overly synthetic rather than representing real-world DOL scenarios

## Confidence
- High confidence: Existing MARL methods struggle with CTC tasks requiring mandatory DOL and cooperation (0% success rate)
- Medium confidence: Task diversity factors contribute to difficulty (supported by simplified variant comparisons)
- High confidence: Poor performance consistency across seeds (stability coefficient analysis)

## Next Checks
1. Systematic ablation studies isolating each diversity factor's contribution to task difficulty
2. Benchmarking with alternative evaluation metrics specifically designed for DOL assessment
3. Extending experiments to heterogeneous agent settings where DOL is theoretically more beneficial