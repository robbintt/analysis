---
ver: rpa2
title: Models That Are Interpretable But Not Transparent
arxiv_id: '2502.19502'
source_url: https://arxiv.org/abs/2502.19502
tags:
- explanations
- explanation
- faithful
- decision
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of providing faithful explanations
  for interpretable models while preventing attackers from reverse-engineering the
  model through explanation queries. The proposed method, FaithfulDefense, formulates
  explanation generation as a maximum set cover problem, where explanations are made
  faithful by revealing parts of the model logic but as little as possible about the
  decision boundary.
---

# Models That Are Interpretable But Not Transparent

## Quick Facts
- arXiv ID: 2502.19502
- Source URL: https://arxiv.org/abs/2502.19502
- Reference count: 33
- Primary result: FaithfulDefense reduces information leakage in explanation queries while maintaining faithful explanations for interpretable models

## Executive Summary
This paper addresses the problem of providing faithful explanations for interpretable models while preventing attackers from reverse-engineering the model through explanation queries. The proposed method, FaithfulDefense, formulates explanation generation as a maximum set cover problem, where explanations are made faithful by revealing parts of the model logic but as little as possible about the decision boundary. Multiple solution methods are provided, including greedy and integer programming approaches. Experiments on credit datasets show FaithfulDefense reduces information leakage compared to baselines, generates explanations within seconds, and requires more queries for attackers to achieve similar performance to the original model.

## Method Summary
The method generates faithful explanations by finding the minimal support explanation that contains the satisfied rule for a given query. It uses a maximum set cover formulation where the goal is to minimize the support of the explanation while maintaining faithfulness. The approach includes a greedy algorithm that achieves a (1-1/e) approximation factor and an integer programming method for optimal solutions. The explanation generation process involves identifying the satisfied rule for the query and then selecting additional conditions to minimize the number of training samples covered by the explanation.

## Key Results
- FaithfulDefense reduces information leakage compared to baselines across three credit datasets
- Explanation generation takes less than one second for the greedy method
- Attackers require significantly more queries to achieve the same performance on surrogate models
- The method consistently produces completely faithful explanations with zero false positive rate

## Why This Works (Mechanism)

### Mechanism 1: Maximum Set Cover Reformulation
- Claim: Minimizing the support of a faithful explanation is equivalent to a maximum coverage problem on the complement set
- Core assumption: The model is a decision set with binary conditions and training data distribution is available
- Break condition: If the model is not a decision set or training data is unavailable

### Mechanism 2: Submodular Greedy Approximation with Guarantees
- Claim: A greedy selection of conditions achieves a (1-1/e) approximation factor to the optimal explanation
- Core assumption: Problem instances remain tractable with modest length budget
- Break condition: If length budget is very large or condition space is not efficiently enumerable

### Mechanism 3: Partial Rule Revelation Maintains Faithfulness
- Claim: An explanation is faithful if it contains a rule that triggered the prediction
- Core assumption: Queries typically satisfy more conditions than just the triggering rule
- Break condition: If a query only satisfies the exact rule conditions

## Foundational Learning

- Concept: Maximum Coverage Problem and Submodularity
  - Why needed here: Core optimization formulation; understanding why greedy works requires grasping submodular function properties
  - Quick check question: Given universe U = {1,2,3,4,5} and sets S1={1,2}, S2={2,3}, S3={3,4}, which two sets maximize coverage if you can select only two?

- Concept: Decision Sets / Disjunctive Normal Form (DNF)
  - Why needed here: The defense targets logical models; you must understand how rules combine to form predictions
  - Quick check question: How would you convert a binary decision tree with 3 leaf nodes predicting positive into an equivalent decision set?

- Concept: Model Extraction Attacks
  - Why needed here: The threat model; explanations reveal subspace predictions, accelerating surrogate model training
  - Quick check question: If an attacker receives "if income<5K AND age≤20 → deny", how might they generate the next informative query?

## Architecture Onboarding

- Component map: Input layer -> Prediction module -> Explanation generator -> History cache -> Output
- Critical path: Query arrives → model prediction → if positive: retrieve/create explanation → solve optimization problem → cache explanation → return to user
- Design tradeoffs:
  - Greedy vs IP: Greedy (~0.1s) offers speed; IP offers optimality but may timeout
  - Max length l: Larger l → less information leakage but longer explanations
  - Random appending (IP-RA): Uses full budget when optimal solution is shorter
  - Explanation reuse: Reduces computation but may create exploitable patterns
- Failure signatures:
  - IP solver timeout → fall back to greedy method
  - Query satisfies only rule conditions → must reveal full rule
  - Low FPR on test set confirms faithfulness
- First 3 experiments:
  1. Support coverage vs queries (Figure 1): Run FaithfulDefense and baselines on FICO/German/Loan datasets with 2000 queries using three attacker strategies; plot proportion of positive samples covered
  2. Timing comparison (Figure 2): Measure explanation generation time for Greedy, IP, IP-RA, LIME across datasets
  3. Surrogate model accuracy gap (Figure 3): Train CART (depth=5) surrogate on collected query-label-explanation triples; compute test agreement with original model

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several arise from the analysis:

### Open Question 1
- Question: How robust is FaithfulDefense against adaptive attackers who are explicitly aware of the defense mechanism and design queries to force revelation of larger subspaces?
- Basis in paper: [inferred] Experiments use general knowledge attackers but not ones optimizing specifically against the set cover minimization logic
- Why unresolved: Security defenses often create an arms race; an attacker knowing the defender minimizes support could craft queries to maximize intersection of possible rules
- What evidence would resolve it: Theoretical analysis or empirical results from a simulated "white-box" attacker using the gradient of the set cover objective

### Open Question 2
- Question: Does minimization of support in explanations negatively impact human utility, trust, or actionability compared to original model rules?
- Basis in paper: [inferred] Paper defines interpretability through faithfulness and brevity but evaluates only attacker performance
- Why unresolved: While FaithfulDefense ensures explanations are technically true, highly specific rules might confuse end-users or appear arbitrary
- What evidence would resolve it: A user study measuring perceived actionability and trust when subjects are presented with minimal-support explanations versus standard rule explanations

### Open Question 3
- Question: What are the theoretical limits on the trade-off between explanation length budget and information leakage for logical models?
- Basis in paper: [inferred] Paper demonstrates trade-off empirically but doesn't provide theoretical bounds
- Why unresolved: Without theoretical characterization of this frontier, setting the budget remains a heuristic choice
- What evidence would resolve it: Derivation of a theoretical lower bound for support coverage as a function of explanation length for a given decision set complexity

## Limitations
- Model extraction threat model assumes attackers use only query-explanation-label triples
- FastSRS rule learning algorithm is anonymized and unavailable, requiring proxy implementations
- Exact binarization scheme (binning thresholds and encoding) is unspecified

## Confidence
- High confidence: Maximum coverage formulation and greedy approximation guarantee (1-1/e) are well-established
- Medium confidence: Empirical results showing reduced information leakage are convincing but dataset-specific
- Low confidence: Practical runtime performance claims depend on unavailable FastSRS implementation

## Next Checks
1. **Alternative rule learner validation**: Reproduce experiments using publicly available rule learning algorithms to verify defense effectiveness is not specific to the anonymized FastSRS method
2. **Cross-dataset generalization**: Test FaithfulDefense on additional tabular datasets beyond credit applications to assess robustness across domains
3. **Real-world attack scenario testing**: Implement attacks that combine explanation information with other model properties (confidence scores, response patterns) to evaluate defense against more sophisticated extraction strategies