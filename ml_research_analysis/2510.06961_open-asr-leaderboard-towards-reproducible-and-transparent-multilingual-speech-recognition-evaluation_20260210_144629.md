---
ver: rpa2
title: 'Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual Speech
  Recognition Evaluation'
arxiv_id: '2510.06961'
source_url: https://arxiv.org/abs/2510.06961
tags:
- leaderboard
- speech
- multilingual
- datasets
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the Open ASR Leaderboard, a fully reproducible
  benchmark for automatic speech recognition (ASR) that evaluates over 60 open-source
  and proprietary systems across 10 datasets in English and 5 multilingual languages.
  It standardizes text normalization and reports both word error rate (WER) and inverse
  real-time factor (RTFx) to enable fair accuracy-efficiency comparisons.
---

# Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual Speech Recognition Evaluation

## Quick Facts
- arXiv ID: 2510.06961
- Source URL: https://arxiv.org/abs/2510.06961
- Reference count: 0
- Benchmark evaluates 60+ ASR systems on 10 datasets across English and 5 multilingual languages

## Executive Summary
The Open ASR Leaderboard presents a fully reproducible benchmark for evaluating automatic speech recognition systems across 10 datasets in English and 5 multilingual languages. The benchmark standardizes text normalization and reports both word error rate (WER) and inverse real-time factor (RTFx) to enable fair comparisons of accuracy and efficiency. The evaluation reveals that Conformer encoders paired with LLM decoders achieve the best English WER (e.g., 5.63 for NVIDIA Canary Qwen 2.5B) but are slower, while CTC and TDT decoders deliver much better RTFx (e.g., 3386.02 for NVIDIA Parakeet TDT 0.6B v2), making them attractive for long-form transcription. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.

## Method Summary
The benchmark evaluates 60+ open-source and proprietary ASR systems on 10 datasets via Hugging Face Hub, including AMI, LibriSpeech, GigaSpeech, Earnings22, SPGISpeech, TED-LIUM v3, VoxPopuli, CoVoST-2, FLEURS, and MLS. Models are run on NVIDIA A100-SXM4-80GB with batch size 64 (adaptively reduced for memory). WER is computed after standardized text normalization (punctuation/casing removal, number normalization, filler word removal), and RTFx is measured as total audio duration divided by transcription time. The evaluation pipeline loads datasets, runs inference, normalizes outputs, computes WER, and measures transcription time.

## Key Results
- Conformer+LLM architectures achieve best English WER (5.63 for NVIDIA Canary Qwen 2.5B) but worst RTFx
- FastConformer+CTC/TDT architectures deliver highest RTFx (3386.02 for NVIDIA Parakeet TDT 0.6B v2) with moderate WER
- Fine-tuned Whisper encoders improve English accuracy but reduce multilingual coverage across 5 tested languages
- SSL encoders with CTC decoders underperform, ranking only 52nd, suggesting need for better decoder pairing

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Conformer encoders paired with LLM-based decoders achieve superior WER for English transcription.
- **Mechanism:** Conformer combines CNNs for local feature extraction with Transformer attention for global context, while LLM decoders leverage large-scale language modeling for better linguistic reasoning.
- **Core assumption:** Robust acoustic representations from Conformer enable LLM decoders to effectively apply linguistic knowledge.
- **Evidence anchors:** Abstract shows 5.63 WER for NVIDIA Canary Qwen 2.5B; section 3 confirms top four models use Conformer+LLM.

### Mechanism 2
- **Claim:** CTC and TDT decoders provide significantly higher inference efficiency than LLM-based decoders.
- **Mechanism:** CTC and TDT have simpler inference paths without autoregressive token generation or LLM overhead.
- **Core assumption:** Efficiency gains come from reduced computational complexity in decoder, not encoder differences.
- **Evidence anchors:** Abstract shows 3386.02 RTFx for NVIDIA Parakeet TDT 0.6B v2; section 3 shows best CTC model ranks only 23rd in WER.

### Mechanism 3
- **Claim:** Fine-tuning Whisper encoders for English improves accuracy but reduces multilingual coverage.
- **Mechanism:** Fine-tuning shifts model capacity toward English acoustic patterns, degrading performance on underrepresented languages.
- **Core assumption:** Finite model capacity means optimization for fine-tuning data distribution comes at expense of original training distribution.
- **Evidence anchors:** Abstract states fine-tuned Whisper encoders improve English accuracy but trade off multilingual coverage; section 3 notes Whisper-derived models train on fewer languages.

## Foundational Learning

- **Concept:** Word Error Rate (WER) and text normalization
  - **Why needed here:** WER is primary accuracy metric, but raw outputs include punctuation, casing, and number formats that vary across models. Standardization ensures fair comparison.
  - **Quick check question:** If Model A outputs "$1,234.56" and Model B outputs "one thousand two hundred thirty four dollars and fifty six cents," how should they be compared against reference text "1234.56"?

- **Concept:** Inverse Real-Time Factor (RTFx)
  - **Why needed here:** RTFx quantifies inference efficiency (higher = faster). Critical for selecting models for long-form or resource-constrained deployments.
  - **Quick check question:** If RTFx = Total audio duration / Transcription time, what does RTFx = 1000 mean in practical terms?

- **Concept:** Encoder-decoder ASR architectures (CTC, RNN-T, TDT, LLM-based)
  - **Why needed here:** Paper explicitly compares these architectures showing different accuracy-efficiency trade-offs. Understanding inference characteristics is essential for model selection.
  - **Quick check question:** Why would an LLM-based decoder likely be slower than a CTC decoder for the same encoder?

## Architecture Onboarding

- **Component map:** Dataset loaders -> Text normalization -> WER computation + RTFx measurement -> Model evaluation
- **Critical path:** 1) Load dataset via Hugging Face `datasets` package, 2) Run inference with target model, 3) Apply standardized text normalization, 4) Compute WER against normalized reference, 5) Measure transcription time for RTFx calculation
- **Design tradeoffs:** Conformer+LLM → best WER, worst RTFx; FastConformer+CTC/TDT → moderate WER, best RTFx; English fine-tuning → better English WER, fewer supported languages; Open models allow RTFx measurement, closed models cannot be fairly compared on efficiency
- **Failure signatures:** SSL+CTC models underperform (52nd rank); OOM on long audio requiring adaptive batch size reduction; WER discrepancies due to normalization mismatch
- **First 3 experiments:** 1) Run your model on LibriSpeech (clean + other) and AMI with standardized normalization; compare WER and RTFx against leaderboard, 2) If using Conformer encoder, benchmark CTC vs. TDT vs. Transformer decoders on same test set to quantify accuracy-efficiency trade-off, 3) Evaluate your model on FLEURS and CoVST-2 subsets to determine if English fine-tuning degraded multilingual performance

## Open Questions the Paper Calls Out
- Can self-supervised learning (SSL) encoders achieve competitive English WER rankings if paired with non-CTC decoders? The top SSL-based system ranks only 52nd using CTC decoders, leaving potential of hybrid architectures untested.
- How does the observed accuracy-efficiency trade-off change when evaluating models on far-field or noisy domain data? Current benchmark may not adequately represent far-field acoustic challenges.
- Does Token Error Rate (TER) provide significantly different ranking of multilingual models compared to Word Error Rate (WER)? WER is sole standardized metric but may not capture sub-word nuances for morphologically rich languages.

## Limitations
- Benchmark focuses on English and 5 additional languages, may not generalize to full diversity of global speech patterns
- Does not address robustness to noise, accents, or domain shifts critical for real-world deployment
- Efficiency metrics only comparable for open models; closed models cannot be fairly benchmarked on speed

## Confidence
- **High confidence:** Empirical results showing Conformer+LLM architectures achieving best English WER and FastConformer+CTC/TDT achieving highest RTFx
- **Medium confidence:** Mechanism explaining why CTC and TDT decoders are more efficient (simpler inference paths) is plausible but not rigorously proven
- **Medium confidence:** Trade-off between English specialization and multilingual coverage for fine-tuned Whisper models is stated but not empirically validated across broad language set

## Next Checks
1. Run ablation study with same Conformer encoder and multiple decoder types (CTC, TDT, Transformer, LLM) on fixed English dataset to isolate decoder contribution to RTFx and WER
2. Evaluate fine-tuned Whisper model on subset of original 99 languages (e.g., Arabic, Hindi, Mandarin) to quantify WER drop relative to base Whisper model
3. Add noisy or accented speech dataset to benchmark and compare WER of Conformer+LLM vs. FastConformer+CTC models to assess accuracy advantage under realistic conditions