---
ver: rpa2
title: 'HAFix: History-Augmented Large Language Models for Bug Fixing'
arxiv_id: '2501.09135'
source_url: https://arxiv.org/abs/2501.09135
tags:
- bugs
- baseline
- code
- bugsinpy
- defects4j
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving bug-fixing performance
  in Large Language Models (LLMs) by incorporating historical context from software
  repositories. The authors propose HAFix, a novel approach that leverages historical
  heuristics derived from blame commits to enhance LLM bug-fixing capabilities.
---

# HAFix: History-Augmented Large Language Models for Bug Fixing

## Quick Facts
- arXiv ID: 2501.09135
- Source URL: https://arxiv.org/abs/2501.09135
- Authors: Yu Shi; Abdul Ali Bangash; Emad Fallahzadeh; Bram Adams; Ahmed E. Hassan
- Reference count: 12
- Key outcome: HAFix achieves 45.05% and 49.92% average improvements in bug-fixing rates on BugsInPy and Defects4J respectively by leveraging historical heuristics from blame commits

## Executive Summary
HAFix addresses the challenge of improving LLM-based bug fixing by incorporating historical context from software repositories. The approach leverages seven historical heuristics derived from blame commits to enhance bug-fixing performance. Evaluated on 51 Python and 116 Java bugs, HAFix demonstrates significant improvements over baseline methods. The aggregated HAFix variant (HAFix-Agg) combines complementary strengths of individual heuristics, achieving substantial performance gains while providing insights into cost-effective deployment strategies.

## Method Summary
HAFix extracts seven historical heuristics from blame commits and their predecessors, including co-evolved function names, function code pairs, and file diffs. These heuristics are appended to baseline prompts and evaluated using Pass@k metrics. The method uses three prompt styles (Instruction, InstructionLabel, InstructionMask) and aggregates outputs from multiple heuristics to capture complementary fix patterns. Statistical significance is assessed using Friedman and Wilcoxon signed-rank tests with Rank-Biserial Correlation for effect size measurement.

## Key Results
- HAFix-Agg achieves 45.05% improvement on BugsInPy and 49.92% on Defects4J compared to baseline
- Instruction prompt style significantly outperforms other styles in 5/6 configurations
- Each heuristic fixes 3-14 unique bugs beyond baseline capabilities
- Early stopping strategies reduce computational cost by 69-73% with minimal performance impact

## Why This Works (Mechanism)

### Mechanism 1
Incorporating blame commit history into LLM prompts improves bug-fixing performance by providing temporal context about code evolution. HAFix extracts seven historical heuristics from blame commits and appends them to the baseline prompt, giving the LLM visibility into how the buggy code was last modified. This narrows the space of plausible fixes toward edits consistent with recent code evolution.

### Mechanism 2
Aggregating outputs from multiple historical heuristics (HAFix-Agg) captures complementary fix patterns no single heuristic can achieve alone. Each heuristic generates 10 samples independently, and HAFix-Agg pools all 70 samples, considering a bug fixed if any sample passes tests. This leverages the observation that different heuristics uniquely fix different bugs.

### Mechanism 3
The Instruction prompt style (explicitly mentioning buggy line in instruction text) enables more effective use of historical context than masked or labeled variants. Three prompt styles were tested, with Instruction winning in 5/6 configurations with large effect sizes, likely because it clearly separates the buggy line from surrounding context without disrupting code structure.

## Foundational Learning

**Concept: Blame commits and SZZ algorithm**
- Why needed here: HAFix's temporal analysis relies on identifying the commit that last modified the buggy line (blame commit). Understanding git blame/annotate and the SZZ algorithm for bug-introducing commit identification is essential.
- Quick check question: Given a buggy line in commit V3 that was fixed in V4, can you trace which commit last modified that line before V3?

**Concept: Pass@k metric for probabilistic code generation**
- Why needed here: HAFix evaluates success using Pass@k (likelihood of correct fix within k samples), not binary fix/no-fix. This accounts for LLM stochasticity.
- Quick check question: If an LLM generates 10 samples and 3 pass tests, what is Pass@1? (Formula: 1 − C(n−c,k)/C(n,k))

**Concept: Friedman test with Wilcoxon post-hoc for paired non-parametric data**
- Why needed here: HAFix compares multiple heuristics on the same bug set, requiring paired statistical tests. Friedman detects differences across groups; Wilcoxon with Bonferroni correction handles pairwise comparisons.
- Quick check question: Why use non-parametric tests for Pass@k distributions rather than ANOVA?

## Architecture Onboarding

**Component map:** Data Collection -> Heuristic Extraction -> Prompt Constructor -> LLM Inference -> Output Parser -> Test Runner -> Aggregator

**Critical path:** Blame commit identification (V2) → Heuristic extraction → Prompt construction → LLM inference → Test validation → Aggregation/early-stop decision

**Design tradeoffs:** Heuristic breadth vs. inference cost (HAFix-Agg fixes more bugs but costs 7×); FN-all/FL-diff provide richest context but have worst time-performance ratios; single-line bug focus limits complexity but enables controlled evaluation

**Failure signatures:** Pass@k drops when historical context is irrelevant; model output contains extraneous text requiring robust regex parsing; FL-diff and FN-all heuristics are time/token-intensive with moderate gains; prompt style sensitivity varies by model

**First 3 experiments:**
1. Baseline validation: Reproduce baseline Pass@k on 10 bugs from BugsInPy using Instruction style, confirming baseline matches reported ~23-29% Pass@1
2. Single-heuristic A/B test: Apply FN-modified heuristic to same 10 bugs, compute Pass@k delta and statistical significance
3. Cost profiling: Measure inference time and tokens for ES vs. Exhaustive on 5 bugs, verify ~70% reduction claim

## Open Questions the Paper Calls Out

**Open Question 1:** Can HAFix be effectively generalized to multi-line and multi-hunk bugs, or integrated into agentic workflows for complex repair tasks? The study is limited to single-line bugs, and future work should extend to more complex cases and agentic workflows.

**Open Question 2:** Does the utility of historical heuristics persist or diminish when applied to significantly larger models (e.g., 70B+ parameters or closed-source models)? The study was computationally restricted to models ranging from 6.7B to 16B parameters.

**Open Question 3:** How can historical heuristics be dynamically filtered or weighted to prevent performance regressions caused by irrelevant historical context? HAFix-Agg currently aggregates all heuristics without filtering for relevance, which can occasionally dilute the signal with noise.

## Limitations
- Single-line bug restriction limits generalizability to more complex multi-line or logical bugs common in production systems
- Effectiveness may degrade when blame commits contain refactoring rather than bug-related changes
- Study does not explore alternative historical sources beyond blame commits, potentially missing richer context available in issue discussions or code reviews

## Confidence

**High Confidence:** The core finding that historical heuristics improve bug-fixing rates compared to baseline (45.05% and 49.92% improvements) is well-supported by statistical tests and multiple independent evaluations.

**Medium Confidence:** The identification of Instruction as the optimal prompt style is based on strong statistical evidence but shows model-specific variation.

**Medium Confidence:** The cost-effectiveness analysis is methodologically sound but relies on specific computational resources that may not generalize to all deployment scenarios.

## Next Checks

1. **Multi-line Bug Extension:** Test HAFix on multi-line bugs from Defects4J-Mini to assess scalability beyond single-line fixes and identify heuristics that generalize across bug complexity levels.

2. **Alternative Historical Sources:** Compare blame-commit heuristics against heuristics derived from issue discussions or pull request reviews to determine whether blame commits provide optimal historical context.

3. **Cross-Repository Generalization:** Apply HAFix to bugs from repositories with different commit patterns (e.g., high churn vs. stable) to evaluate robustness to varying code evolution dynamics.