---
ver: rpa2
title: Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text
  Translation
arxiv_id: '2510.03115'
source_url: https://arxiv.org/abs/2510.03115
tags:
- speech
- translation
- training
- s2tt
- transcription
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether Chain-of-Thought (CoT) speech-to-text
  translation (S2TT) models can effectively leverage speech information beyond transcripts,
  overcoming limitations of cascade systems like error propagation and ignoring prosody.
  The authors evaluate CoT models using interpretability methods (Value Zeroing) to
  quantify speech contribution, robustness tests with corrupted transcripts, and prosody-awareness
  benchmarks (CONTRAPROST).
---

# Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation

## Quick Facts
- arXiv ID: 2510.03115
- Source URL: https://arxiv.org/abs/2510.03115
- Reference count: 0
- Primary result: CoT S2TT models largely ignore speech input unless explicitly trained to use it, showing limited prosody awareness

## Executive Summary
This paper investigates whether Chain-of-Thought (CoT) speech-to-text translation (S2TT) models can effectively leverage speech information beyond transcripts, overcoming limitations of cascade systems like error propagation and ignoring prosody. The authors evaluate CoT models using interpretability methods (Value Zeroing) to quantify speech contribution, robustness tests with corrupted transcripts, and prosody-awareness benchmarks (CONTRAPROST). They find that standard CoT models largely mimic cascade behavior, relying mostly on transcripts and minimally on speech. Simple training interventions—adding Direct S2TT data (DUAL) or injecting noisy transcripts (NOISY)—improve speech attribution and robustness, with NOISY showing the largest gains. However, prosody-awareness remains low across models, suggesting that acoustic information is not well integrated.

## Method Summary
The study uses SALAMANDRATA-7B-instruct LLM with frozen mHuBERT speech encoder (25Hz) to extract discrete speech units (DSUs) via k-means clustering (500 centroids on layer 11). Three training variants are compared: BASE (pure CoT data), DUAL (25% CoT / 75% Direct data), and NOISY (25% CoT with corrupted transcripts). Models are trained on ASR + S2TT + 50k T2TT pairs per language, evaluated on FLEURS (xCOMET, BLASER 2.0) and CONTRAPROST (prosody), with speech attribution measured via Value Zeroing. Generation uses beam-5 decoding with temperature 0.2, top-p 0.95, top-k 50.

## Key Results
- Standard CoT models show minimal speech attribution (0.0228±0.0005) and behave like cascade systems
- DUAL training increases speech attribution to 0.035±0.001 (1.54× BASE) while maintaining translation quality
- NOISY model shows 2.24× more speech reliance and flat performance under transcript corruption (0-30%)
- Prosody awareness remains consistently low across all models (<5% Global CONTRAPROST scores)

## Why This Works (Mechanism)

### Mechanism 1: Direct S2TT Data Mixing Increases Speech Attribution
Training on mixed CoT and Direct formats increases the model's reliance on speech tokens during translation. Direct S2TT samples require translation directly from discrete speech units (DSUs) without intermediate transcripts, forcing the model to develop speech-to-translation mappings. When mixed with CoT training, this learned capability transfers, causing the model to attend more to speech even when transcripts are available.

### Mechanism 2: Noisy Transcript Injection Builds Robustness Through Forced Fallback
Training with corrupted transcripts forces the model to develop fallback strategies using speech when transcripts are unreliable. By replacing transcript portions with semantically divergent but grammatically correct text during training (25% of samples), the model learns that transcripts can be misleading. This creates a learned behavior to consult the speech signal when transcript-translation coherence seems doubtful.

### Mechanism 3: Discrete Speech Unit Quantization Enables LLM Processing
Quantizing continuous speech representations into discrete tokens allows standard LLM architectures to process speech without architectural modifications. A frozen self-supervised encoder (mHuBERT) extracts continuous representations, which are clustered via k-means (500 centroids, 11th layer). These discrete indices become vocabulary tokens via embedding layer expansion, enabling the LLM to process speech identically to text.

## Foundational Learning

- Concept: **Chain-of-Thought vs Cascade Inference**
  - Why needed here: The paper's central finding is that CoT behaves like cascade; understanding the distinction is essential.
  - Quick check question: In CoT inference during translation generation, which inputs remain in context—speech only, transcript only, or both?

- Concept: **Value Zeroing for Attribution**
  - Why needed here: This interpretability method quantifies how much each input modality contributes to outputs; it's the paper's primary diagnostic tool.
  - Quick check question: Why might Value Zeroing be more reliable than inspecting raw attention weights for measuring token contributions?

- Concept: **Discrete Speech Units (DSUs)**
  - Why needed here: DSUs are the technical bridge enabling text-only LLMs to process speech; understanding this pipeline is prerequisite to modifying the architecture.
  - Quick check question: What are the two transformations applied to raw audio before it becomes DSU tokens the LLM can process?

## Architecture Onboarding

- Component map: Raw Speech → mHuBERT Encoder (frozen) → 11th Layer Features → k-means (500 centroids) → DSU Token IDs → Expanded Embedding Layer → SALAMANDRATA-7B LLM → Output: Transcription, then Translation (CoT format)

- Critical path:
  1. DSU embedding adaptation (train embedding layer only with frozen LLM on speech data)
  2. Main training on ASR + T2TT (50k/lang) + S2TT mixture in chatml format
  3. For interventions: modify data mixture (add Direct samples or inject noisy transcripts to 25% of CoT samples)

- Design tradeoffs:
  - **Cluster count (500)**: Higher preserves more acoustic detail but expands vocabulary; risks sparsity.
  - **Direct data ratio (75% in DUAL)**: Higher increases speech attribution but may weaken CoT format adherence.
  - **Noise ratio (25% in NOISY)**: Higher improves robustness but requires omitting transcription loss on noisy samples to preserve ASR quality.
  - **Encoder layer selection (11th)**: Earlier layers capture more acoustic detail; later layers are more linguistic—choice affects what information survives discretization.

- Failure signatures:
  - Speech attribution near zero across all layers (BASE behavior): Model ignores DSUs entirely.
  - Sharp xCOMET drop under transcript corruption (slope matching CASCADE): Indicates no speech fallback.
  - CONTRAPROST Global score <5%: Model fails to leverage prosody for disambiguation.
  - Peak attribution only at early layers with no mid-late speech contribution: Suggests superficial speech processing that doesn't propagate to translation decisions.

- First 3 experiments:
  1. **Baseline measurement**: Train BASE (pure CoT data), run Value Zeroing attribution and corruption robustness tests to confirm cascade-like behavior and establish comparison benchmarks.
  2. **Direct data ratio sweep**: Train DUAL variants at 25%, 50%, 75% Direct data; measure speech attribution vs translation quality (xCOMET) to find operating point where speech awareness improves without quality degradation.
  3. **Noise injection validation**: Train NOISY at 25% corruption ratio; evaluate on controlled corruption test (2.5–30% noise) to verify flat degradation curve and confirm error propagation is mitigated through speech consultation rather than noise-pattern learning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural modifications can enable meaningful prosody awareness in CoT S2TT systems?
- Basis in paper: Authors state "the need for architectures that explicitly integrate acoustic information into translation" and note prosody awareness "remains consistently lower" across all model variants.
- Why unresolved: Simple training interventions (DUAL, NOISY) yielded only modest gains; fundamental architectural limitations persist.
- What evidence would resolve it: Development of novel architectures that achieve substantially higher scores on CONTRAPROST, demonstrating genuine acoustic cue integration.

### Open Question 2
- Question: What causes the consistent attribution peak at layer 4 observed across all model variants?
- Basis in paper: Authors report "we consistently observe a peak at layer 4 across models, leaving detailed analyses to future work."
- Why unresolved: The phenomenon was observed but not investigated; its mechanistic basis and functional significance remain unknown.
- What evidence would resolve it: Layer-wise ablation studies and analysis of attention patterns at layer 4 to determine whether this represents a critical processing stage or architectural artifact.

### Open Question 3
- Question: Do findings generalize to other source languages beyond English?
- Basis in paper: Authors deliberately "restrict source speech to English, thereby minimizing confounding factors," limiting conclusions to English-source translation.
- Why unresolved: Prosodic features vary across languages; it is unclear whether speech awareness findings transfer to typologically different source languages.
- What evidence would resolve it: Replication of attribution, robustness, and prosody-awareness experiments using diverse source languages with varying prosodic systems.

### Open Question 4
- Question: Would unfreezing the speech encoder during training improve speech attribution and prosody awareness?
- Basis in paper: The speech encoder (mHuBERT) remains frozen during training; only LLM parameters are updated.
- Why unresolved: Frozen encoder may limit model's ability to adapt speech representations for translation-specific acoustic features.
- What evidence would resolve it: Comparative experiments with fine-tuned speech encoders measuring speech attribution scores and CONTRAPROST performance.

## Limitations
- DSU quantization with 500 clusters may discard fine-grained acoustic information critical for prosody
- Noise injection process lacks detailed specification of corruption generation and prompt details
- Value Zeroing attribution cannot reveal which specific acoustic features drive speech contributions

## Confidence
- **High confidence**: Standard CoT models largely ignore speech input, behaving similarly to cascade systems (supported by multiple independent evaluations)
- **Medium confidence**: Training interventions (DUAL, NOISY) effectively improve speech attribution and robustness (measured improvements but generalizability needs validation)
- **Low confidence**: Prosody-awareness results (CONTRAPROST scores remain low across all models, suggesting evaluation or architectural limitations)

## Next Checks
1. **Cluster count sensitivity analysis**: Systematically vary DSU cluster count (100, 500, 1000, 2000) and measure impact on speech attribution, translation quality, and prosody awareness
2. **Cross-linguistic generalization**: Extend evaluation to non-European languages with different prosodic systems (tone languages, lexical stress languages)
3. **Fine-grained attribution analysis**: Apply layer-wise relevance propagation or integrated gradients to identify specific acoustic features the model uses beyond aggregate speech token contributions