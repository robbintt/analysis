---
ver: rpa2
title: Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated
  Learning
arxiv_id: '2506.04071'
source_url: https://arxiv.org/abs/2506.04071
tags:
- learning
- local
- data
- which
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an Optimal Transport-based preprocessing
  method for federated learning to address dataset imbalance across edge devices.
  The method uses Wasserstein barycenters to align local datasets into a shared representation
  space, reducing distributional discrepancies.
---

# Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning

## Quick Facts
- **arXiv ID:** 2506.04071
- **Source URL:** https://arxiv.org/abs/2506.04071
- **Reference count:** 0
- **Primary result:** Proposed Optimal Transport-based preprocessing improves FedAvg accuracy on CIFAR-10 from 71.22% to 99.62% with fewer communication rounds.

## Executive Summary
This paper addresses dataset imbalance in federated learning by introducing an Optimal Transport-based preprocessing method that aligns local datasets into a shared representation space. The approach uses Wasserstein barycenters to compute a global target RGB space, which local clients then use to project their images before standard FedAvg training. By minimizing distributional discrepancies in color channels, the method significantly improves model generalization and reduces communication rounds needed for convergence.

## Method Summary
The method operates in two phases: preprocessing and federated learning. First, each client computes channel-wise (R, G, B) Wasserstein barycenters from their local images using entropic regularization. The central server aggregates these local barycenters to compute a global Wasserstein barycenter representing the target RGB space. Each client then projects its entire local dataset to this target space using optimal transport plans. Standard FedAvg (or another FL algorithm) is subsequently executed on these aligned datasets, resulting in improved accuracy and faster convergence compared to training on raw, non-aligned data.

## Key Results
- **Accuracy improvement:** FedAvg with preprocessing achieves 99.62% accuracy versus 71.22% for standard FedAvg on CIFAR-10
- **Communication efficiency:** Fewer communication rounds needed for convergence due to better-initialized local models
- **Model-agnostic:** Method works with various FL frameworks and model architectures without modification

## Why This Works (Mechanism)
The method addresses federated learning's core challenge of non-IID data by aligning the low-level statistical properties of images across clients. By projecting all local datasets to a common target RGB space derived from Wasserstein barycenters, the approach reduces distributional discrepancies that cause model divergence during federated training. This alignment creates a shared representation space where local updates are more consistent, enabling faster convergence and better generalization without requiring architectural changes to the underlying FL framework.

## Foundational Learning

- **Concept: Optimal Transport (OT) and Wasserstein Distance**
  - **Why needed here:** Core mathematical tool to quantify the "cost" of transforming one distribution (client's image color channel) into another (global target)
  - **Quick check question:** If you have two pixel intensity histograms, can you explain in simple terms what the Wasserstein distance measures compared to, say, the Euclidean distance between their means?

- **Concept: Wasserstein Barycenters**
  - **Why needed here:** Computes the "average" of multiple color channel distributions from different clients to create the target space for alignment
  - **Quick check question:** Given three different probability distributions (e.g., histograms A, B, and C), what does a Wasserstein barycenter represent? What is the key property that makes it a suitable "central" target for alignment?

- **Concept: Non-IID Data in Federated Learning**
  - **Why needed here:** Central problem the paper aims to solve - the "dataset imbalance" and "distributional discrepancy" are manifestations of non-IID data
  - **Quick check question:** Why does the standard FedAvg algorithm struggle when client datasets are non-IID? How does the variance in local model updates relate to the variance in local data distributions?

## Architecture Onboarding

- **Component map:** Client Preprocessor (local barycenter computation → projection) -> Server Coordinator (global barycenter aggregation → broadcast) -> Standard FL Framework (training on aligned datasets)

- **Critical path:**
  1. Initialization: All clients compute local channel-wise Wasserstein barycenters
  2. Aggregation: Server collects all local barycenters and computes global barycenter
  3. Broadcast: Server sends global barycenter to all clients
  4. Projection (One-Time Preprocessing): Each client projects its entire local dataset to target space
  5. FL Training: Execute standard FedAvg on aligned datasets

- **Design tradeoffs:**
  - One-time overhead vs. iterative cost: Significant upfront computational cost to compute barycenters and project datasets, favorable if training acceleration outweighs preprocessing time
  - Information loss: Projection to target space could distort semantic information if optimal transport maps are not carefully regularized
  - Static vs. Dynamic Assumption: Assumes data distributions are static, may not suit highly dynamic environments

- **Failure signatures:**
  - Preprocessing Failure: Errors or extreme computational load during initial barycenter computation or projection phase
  - No Improvement in FL Performance: Marginal or non-existent accuracy gains compared to standard FedAvg
  - Model Divergence: Global model fails to converge or diverges despite alignment

- **First 3 experiments:**
  1. Baseline Reproduction: Implement preprocessing pipeline on small CIFAR-10 subset; manually verify projected images are visually aligned to target
  2. Ablation on Target Space: Compare (a) no preprocessing, (b) preprocessing with arithmetic mean target, (c) preprocessing with Wasserstein barycenter target
  3. Varying Heterogeneity: Create client splits with increasing non-IID levels and measure performance gap between proposed method and standard FedAvg

## Open Questions the Paper Calls Out
- **Open Question 1:** Does the preprocessing method maintain computational efficiency and accuracy gains when applied to large-scale datasets (e.g., ImageNet) or non-image modalities?
- **Open Question 2:** How can the optimal transport barycenter approach be adapted for transforming temporal or sequential data in federated learning?
- **Open Question 3:** What is the privacy cost associated with sharing local Wasserstein barycenters, and does this require integration with differential privacy?
- **Open Question 4:** To what extent does aligning RGB color distributions address label distribution skew versus feature distribution skew?

## Limitations
- **Computational overhead:** One-time preprocessing incurs significant computational cost that may not be feasible on resource-constrained edge devices
- **Implementation specificity:** Method relies on channel-wise RGB computations specific to images, limiting applicability to other data modalities
- **Privacy considerations:** Sharing local barycenters with central server raises privacy concerns not addressed by standard federated learning privacy guarantees

## Confidence
- **High confidence** in theoretical foundation of using Wasserstein barycenters for distributional alignment and overall pipeline structure
- **Medium confidence** in reported accuracy improvements, contingent on faithful reproduction of non-IID data split and projection implementation
- **Low confidence** in exact numerical results without clarification on unknown implementation details, particularly projection mechanism and OT solver parameters

## Next Checks
1. **Projection Implementation Validation:** Implement and visualize projection of sample images from different clients to target RGB space; confirm color distributions are aligned as intended
2. **Sensitivity Analysis on Non-IID Split:** Systematically vary Dirichlet concentration parameter to create different non-IID levels; measure baseline FedAvg accuracy and subsequent improvement
3. **Ablation on OT Regularization:** Run experiments with varying entropic regularization levels for both Sinkhorn transport and Bregman barycenter computations; analyze tradeoff between stability, preprocessing time, and final accuracy