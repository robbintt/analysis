---
ver: rpa2
title: Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit and
  Explicit Grapheme Conditioning
arxiv_id: '2506.04527'
source_url: https://arxiv.org/abs/2506.04527
tags:
- speech
- data
- grapheme
- labels
- graphemes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of generating speech annotations
  (phonemic and prosodic labels) that are coherent with corresponding graphemes, which
  is particularly challenging for languages like Japanese where one grapheme sequence
  can have multiple readings. Previous methods often produced mismatches between predicted
  labels and graphemes.
---

# Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit and Explicit Grapheme Conditioning

## Quick Facts
- arXiv ID: 2506.04527
- Source URL: https://arxiv.org/abs/2506.04527
- Reference count: 0
- Primary result: Significant improvements in grapheme-to-phoneme match rates for Japanese speech annotation using dual conditioning (BERT prompt encoder + dictionary pruning).

## Executive Summary
This paper addresses the challenge of generating speech annotations (phonemic and prosodic labels) that are coherent with corresponding graphemes, particularly for Japanese where one grapheme sequence can have multiple readings. The authors propose two complementary conditioning mechanisms: implicit conditioning using a prompt encoder with pre-trained BERT features to incorporate grapheme context during label prediction, and explicit conditioning through a decoding strategy that prunes label hypotheses inconsistent with graphemes using an external dictionary. Experiments demonstrate significant improvements in grapheme-to-phoneme match rates compared to baseline methods without conditioning, while maintaining high accuracy in phonemic and prosodic label prediction.

## Method Summary
The method involves fine-tuning OWSM-CTC v3.1 (1B, 27-layer encoder) with implicit conditioning by connecting a line-distil-bert-base-japanese module to a prompt encoder, whose output conditions the speech encoder's intermediate layers via self-conditioned CTC. An external g2p dictionary is built using mpaligner from 2,188,937 grapheme-phoneme pairs. During inference, CTC outputs are processed with greedy decoding and explicit hypothesis pruning using dynamic programming to enforce grapheme-phoneme consistency. The model is trained on JSUT basic5000 and a larger proprietary corpus, with synthetic augmentation via Period VITS.

## Key Results
- Implicit and explicit conditioning both significantly improved G2P match rates over baseline (non-conditioned) methods.
- Using both methods together provided additive improvements, suggesting they address different error types.
- On the ReazonSpeech corpus for textual accent estimation, the proposed method with data augmentation improved estimation accuracy, particularly when ground-truth graphemes were available.
- Match rates above 90% were achieved when ground-truth graphemes were available.

## Why This Works (Mechanism)

### Mechanism 1: Implicit Grapheme Conditioning via Prompt Encoder with BERT
- Pre-trained BERT features combined with a prompt encoder enable the model to incorporate grapheme context during TTS label prediction, improving generalization to diverse grapheme sequences.
- The BERT module encodes grapheme embeddings, which are passed through a linear layer to the prompt encoder. The prompt encoder injects this conditioning into the speech encoder's intermediate layers via self-conditioned CTC.
- Core assumption: BERT's pre-training on large text corpora provides transferable knowledge that helps the model adapt to varied grapheme domains even with limited paired speech-label data.
- Evidence anchors: Abstract, section 3.2, corpus neighbor papers on dictionary-enhanced ASR decoding.

### Mechanism 2: Explicit Grapheme-to-Phoneme Dictionary Pruning During Decoding
- An external g2p dictionary combined with hypothesis pruning during inference enforces consistency between predicted phonemes and graphemes, correcting minor errors that implicit conditioning alone cannot address.
- During greedy decoding, partial matches between graphemes and phonemic sequences are checked at each step using the g2p dictionary and dynamic programming. Tokens inconsistent with valid grapheme-phoneme mappings are pruned.
- Core assumption: The dictionary has high recall (covering valid pronunciations) even if precision is low, since CTC scores can filter clearly different phonemes.
- Evidence anchors: Abstract, section 3.3, neighbor paper on dictionary-enhanced decoding for Japanese annotation.

### Mechanism 3: Complementary Error Correction Through Dual Conditioning
- Implicit and explicit conditioning address different error types (domain adaptation vs. local consonant confusions), and their combination yields additive improvements in grapheme-phoneme consistency.
- Implicit conditioning provides broad contextual conditioning through learned representations, while explicit conditioning provides hard constraints at the phoneme level during decoding.
- Core assumption: The two methods do not compete and can operate independently without interference.
- Evidence anchors: Section 4.1.2 results showing additive improvements, though no direct corpus evidence for this specific dual-conditioning interaction.

## Foundational Learning

- **CTC (Connectionist Temporal Classification) and Self-Conditioned CTC**
  - Why needed here: OWSM-CTC uses CTC loss with intermediate layer conditioning; understanding how CTC handles alignment and blank tokens is essential for interpreting explicit conditioning's pruning logic.
  - Quick check question: Can you explain why CTC's blank tokens enable the explicit conditioning strategy to filter hypotheses at specific time indices?

- **Prompt Encoding and Cross-Modal Conditioning**
  - Why needed here: The prompt encoder bridges text (grapheme) and speech modalities; understanding how prefix embeddings condition autoregressive or non-autoregressive models clarifies implicit conditioning mechanics.
  - Quick check question: How does pre-training the prompt encoder for long-form ASR prefix tasks transfer to grapheme-conditioned TTS label annotation?

- **Grapheme-to-Phoneme (G2P) Alignment and Dictionary Construction**
  - Why needed here: Explicit conditioning relies on a g2p dictionary built from many-to-many alignment; understanding alignment algorithms (e.g., mpaligner) clarifies dictionary limitations and recall requirements.
  - Quick check question: Why does the paper emphasize high recall over high precision for the g2p dictionary, and what role does dynamic programming play in partial matching?

## Architecture Onboarding

- Component map: Speech input → Speech embedding layer → 27-layer Speech encoder (first 5 frozen) with self-conditioned CTC at intermediate layers → CTC output → Blank filtering → G2P dictionary + DP matching → Hypothesis pruning → Final TTS labels
- Grapheme input → BERT (line-distil-bert-base-japanese) → Linear projection → Prompt encoder → Conditioned speech encoder output
- Critical path: 1. Grapheme embedding via BERT, 2. Prompt encoder transformation, 3. Injection into speech encoder intermediate layers, 4. CTC-based label prediction, 5. Explicit pruning via dictionary lookup
- Design tradeoffs: Implicit conditioning provides better domain generalization but may miss local errors; explicit conditioning enforces consistency but constrained by dictionary coverage and adds inference overhead; base model choice affects robustness vs flexibility
- Failure signatures: Low G2P match with high prosody F1 indicates implicit conditioning insufficient; high PER degradation suggests ASR transcription errors; inference speed bottleneck from explicit decoding
- First 3 experiments: 1. Ablate implicit conditioning to isolate its contribution, 2. Ablate explicit conditioning to quantify its role in correcting consonant confusions, 3. Test dictionary coverage limits on out-of-domain vocabulary

## Open Questions the Paper Calls Out

1. How does the performance of the proposed annotation model compare when applied to other downstream tasks, such as text-to-speech, versus the textual accent estimation task evaluated in this study? (Future work: applying to tasks beyond textual accent estimation)

2. Does utilizing a cleaner, TTS-specific dataset for data augmentation yield significantly higher textual accent estimation accuracy compared to the noisy ReazonSpeech corpus? (Future work: evaluating cleaner datasets for better results)

3. How effectively does the proposed method generalize to other languages with complex grapheme-to-phoneme relationships, such as Chinese or English, given its specific tuning for Japanese? (No validation on non-Japanese datasets)

4. How robust is the explicit conditioning decoding strategy when encountering out-of-vocabulary (OOV) grapheme sequences or words not present in the external g2p dictionary? (No analysis of dictionary coverage gaps or OOV performance)

## Limitations

- The paper lacks direct experimental evidence for the claim that implicit and explicit conditioning address different error types; this is inferred from additive improvements rather than error analysis.
- Heavy dependence on external dictionary quality for explicit conditioning without quantifying coverage gaps or false positive pruning rates.
- Experimental validation constrained by single-speaker JSUT corpus with only 6.78 hours of training data; larger proprietary data not publicly accessible.
- No analysis of computational costs of dual-conditioning approach during inference.

## Confidence

**High Confidence**: Explicit grapheme conditioning via dictionary pruning improves grapheme-phoneme consistency. Mechanism is straightforward and results show consistent improvements.

**Medium Confidence**: Implicit conditioning through BERT + prompt encoder provides domain adaptation benefits. Demonstrates improved G2P match rates but lacks isolation of BERT vs prompt encoder contributions.

**Low Confidence**: Implicit and explicit conditioning are complementary and address different error types. Based on additive performance improvements rather than direct error analysis or ablation studies.

## Next Checks

1. **Error Type Decomposition Analysis**: Conduct detailed error analysis categorizing mismatches (consonant confusions, vowel errors, prosodic misalignments, domain-specific vocabulary issues) and measure which conditioning method corrects each type to validate complementary nature claims.

2. **Dictionary Coverage Stress Test**: Evaluate on out-of-vocabulary terms from Japanese Wikipedia or technical documents, measuring G2P match rate degradation as dictionary coverage decreases to quantify robustness to vocabulary shifts.

3. **Cross-Speaker Generalization Test**: Train and evaluate on full JSUT corpus (multiple speakers, varied styles) rather than single-speaker subset to validate whether implicit conditioning's domain adaptation generalizes beyond training speaker characteristics.