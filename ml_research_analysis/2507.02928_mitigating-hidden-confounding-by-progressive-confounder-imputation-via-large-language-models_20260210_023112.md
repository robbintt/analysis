---
ver: rpa2
title: Mitigating Hidden Confounding by Progressive Confounder Imputation via Large
  Language Models
arxiv_id: '2507.02928'
source_url: https://arxiv.org/abs/2507.02928
tags:
- treatment
- confounders
- confounder
- llms
- proci
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProCI, the first framework to leverage large
  language models (LLMs) for mitigating hidden confounding in treatment effect estimation.
  ProCI uses LLMs to iteratively generate, impute, and validate hidden confounders
  by combining semantic reasoning with world knowledge embedded in the models.
---

# Mitigating Hidden Confounding by Progressive Confounder Imputation via Large Language Models

## Quick Facts
- arXiv ID: 2507.02928
- Source URL: https://arxiv.org/abs/2507.02928
- Reference count: 40
- Primary result: ProCI uses LLMs to iteratively generate and impute hidden confounders, improving treatment effect estimation with up to 70% error reduction across multiple datasets

## Executive Summary
This paper introduces ProCI, the first framework to leverage large language models (LLMs) for mitigating hidden confounding in treatment effect estimation. ProCI uses LLMs to iteratively generate, impute, and validate hidden confounders by combining semantic reasoning with world knowledge embedded in the models. Instead of direct value imputation, it adopts a distributional reasoning strategy to generate diverse, realistic samples. Experiments across multiple datasets (Jobs, Twins) and LLMs (GPT-4o, DeepSeek-R1, LLaMA 3-8B, Qwen2.5-7B) show that ProCI consistently improves treatment effect estimation, with up to 70% error reduction in some cases. The method is robust to varying levels of hidden confounding and demonstrates the potential of LLMs as tools for causal inference under unmeasured confounding.

## Method Summary
ProCI operates through an iterative LLM-driven process that progressively uncovers hidden confounders. The framework first generates candidate hidden confounders through semantic reasoning based on treatment and outcome descriptions, then validates their relevance before imputation. Instead of direct value imputation, ProCI employs distributional reasoning to generate diverse, realistic samples of each confounder. This approach leverages the world knowledge embedded in LLMs to create plausible values that reflect real-world variability. The framework iterates through multiple rounds of generation, validation, and imputation, progressively refining the confounder set and improving treatment effect estimation accuracy.

## Key Results
- ProCI achieves up to 70% error reduction in treatment effect estimation compared to baseline methods
- The framework demonstrates consistent performance improvements across multiple datasets (Jobs, Twins)
- ProCI maintains robustness across varying levels of hidden confounding and different LLM model sizes (GPT-4o, DeepSeek-R1, LLaMA 3-8B, Qwen2.5-7B)

## Why This Works (Mechanism)
ProCI works by leveraging LLMs' semantic reasoning capabilities and embedded world knowledge to systematically uncover and impute hidden confounders. The iterative approach allows the framework to progressively refine its understanding of latent causal structures, while distributional reasoning generates realistic confounder values that capture real-world variability. By combining generation, validation, and imputation in a progressive loop, ProCI can adapt to different confounding scenarios and improve treatment effect estimation accuracy.

## Foundational Learning
- **Causal inference under unmeasured confounding**: Understanding how hidden variables can bias treatment effect estimates - needed to recognize why standard methods fail and why LLM-based approaches might help
- **Distributional reasoning**: Generating realistic value distributions rather than point estimates - needed to capture the uncertainty and variability inherent in real-world confounders
- **Iterative refinement**: Progressive improvement through multiple rounds of generation and validation - needed to gradually uncover complex latent structures
- **Semantic reasoning with LLMs**: Using natural language understanding to identify potential confounders - needed to leverage LLMs' world knowledge for causal discovery
- **Confounder validation**: Systematic checking of generated confounders for relevance - needed to ensure only meaningful variables are imputed
- **Treatment effect estimation metrics**: Understanding how to measure and evaluate causal inference performance - needed to quantify ProCI's improvements

## Architecture Onboarding

**Component Map**: Treatment/Outcome description -> LLM semantic reasoning -> Confounder generation -> Validation check -> Distributional imputation -> Treatment effect estimation

**Critical Path**: The core workflow follows: (1) input treatment and outcome descriptions, (2) LLM generates candidate confounders through semantic reasoning, (3) generated confounders are validated for relevance, (4) distributional reasoning creates realistic value samples, (5) imputed confounders improve treatment effect estimation.

**Design Tradeoffs**: ProCI trades computational efficiency for accuracy by using multiple LLM iterations, but this enables more thorough confounder discovery. The distributional reasoning approach sacrifices direct value imputation precision for generating more realistic and diverse confounder samples that better capture real-world uncertainty.

**Failure Signatures**: The framework may struggle when LLMs lack relevant world knowledge for specific domains, when confounding structures are too complex for semantic reasoning to capture, or when distributional reasoning generates implausible confounder values that introduce bias rather than reduce it.

**First Experiments**: 
1. Test ProCI on synthetic datasets with known hidden confounders to measure accuracy improvements
2. Compare performance across different LLM model sizes to establish scalability bounds
3. Evaluate robustness by varying the degree of hidden confounding in controlled experiments

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments are limited to synthetic datasets (Jobs, Twins), raising questions about real-world applicability
- The method's reliance on LLM-generated samples introduces uncertainty about whether synthetic confounder values reflect true latent structures
- Computational overhead of iterative LLM calls may limit scalability in high-dimensional settings

## Confidence
**High** confidence in methodological innovation and observed performance improvements across multiple datasets and LLM models
**Medium** confidence in generalizability claim due to limited testing on synthetic rather than real-world data
**Medium** confidence in distributional reasoning quality without explicit validation against known latent structures

## Next Checks
1. Test ProCI on real-world observational datasets with partial ground truth to assess robustness beyond synthetic settings
2. Compare distributional reasoning outputs against known latent confounder structures in semi-synthetic data to validate generation quality
3. Evaluate the impact of LLM model size and reasoning depth on performance to establish practical scalability bounds