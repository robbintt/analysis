---
ver: rpa2
title: 'CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning
  in LLMs'
arxiv_id: '2507.03254'
source_url: https://arxiv.org/abs/2507.03254
tags:
- language
- reasoning
- arxiv
- natural
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CodeAgents, a framework for multi-agent\
  \ reasoning that represents all agent interactions\u2014tasks, plans, feedback,\
  \ roles, and tool calls\u2014as modular, typed pseudocode enriched with control\
  \ structures. This approach reduces verbosity, improves modularity, and enhances\
  \ token efficiency compared to natural language prompts."
---

# CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs

## Quick Facts
- arXiv ID: 2507.03254
- Source URL: https://arxiv.org/abs/2507.03254
- Reference count: 29
- Key outcome: 3-36 percentage point accuracy improvements over baselines, with 55-87% token reduction in input and 41-70% in output

## Executive Summary
CodeAgents introduces a token-efficient framework for multi-agent reasoning in large language models by representing all agent interactions—tasks, plans, feedback, roles, and tool calls—as modular, typed pseudocode enriched with control structures. This approach significantly reduces verbosity compared to natural language prompts while improving modularity and reasoning efficiency. The framework demonstrates consistent performance gains across three benchmarks: GAIA, HotpotQA, and VirtualHome, achieving up to 36 percentage points improvement in accuracy and establishing a new state-of-the-art on VirtualHome at 56% success rate.

## Method Summary
CodeAgents encodes multi-agent interactions as Pseudocode+, a structured representation that combines pseudocode with semantic type annotations and control structures. The framework breaks down complex tasks into modular subtasks assigned to specialized agents, with all communications formalized as typed code elements. This structured approach replaces verbose natural language exchanges with concise, semantically rich pseudocode that preserves intent while dramatically reducing token consumption. The modular design enables efficient task decomposition and agent coordination through formalized plans and feedback mechanisms.

## Key Results
- 3-36 percentage point accuracy improvements over GPT-3.5 and GPT-4 baselines on GAIA, HotpotQA, and VirtualHome benchmarks
- 55-87% reduction in input tokens and 41-70% reduction in output tokens compared to natural language approaches
- Achieved new state-of-the-art 56% success rate on VirtualHome benchmark

## Why This Works (Mechanism)
The framework leverages the inherent structure of programming languages to represent complex agent interactions more efficiently than natural language. By encoding agent communications as typed pseudocode with control structures, CodeAgents captures the semantic relationships between tasks, plans, and feedback while minimizing redundancy. This structured representation allows LLMs to process agent interactions more efficiently, reducing the cognitive load associated with parsing verbose natural language exchanges. The modular decomposition of tasks into specialized subtasks enables more focused reasoning and better utilization of agent expertise.

## Foundational Learning

**Pseudocode+ representation** - Structured code format with type annotations and control structures that encodes agent interactions. Needed to replace verbose natural language with semantically rich, compact representations. Quick check: Verify that all agent communications follow the Pseudocode+ syntax rules.

**Modular task decomposition** - Breaking complex problems into specialized subtasks assigned to different agents. Needed to enable focused reasoning and better agent utilization. Quick check: Ensure task dependencies are properly encoded in the pseudocode representation.

**Typed agent interactions** - Semantic typing of agent communications (tasks, plans, feedback, roles). Needed to preserve meaning while reducing token count. Quick check: Validate that type annotations correctly capture the semantic relationships between interactions.

**Control structure encoding** - Representation of conditional logic and iteration in agent workflows. Needed to express complex reasoning patterns concisely. Quick check: Confirm that all control structures in natural language workflows are accurately translated to pseudocode.

## Architecture Onboarding

**Component map**: Problem -> Task Splitter -> Agent Coordinator -> Specialized Agents -> Pseudocode+ Encoder -> LLM -> Pseudocode+ Decoder -> Results

**Critical path**: Task decomposition → Agent assignment → Pseudocode+ encoding → LLM processing → Result decoding

**Design tradeoffs**: The framework trades some readability for significant token efficiency gains. While Pseudocode+ is less immediately interpretable than natural language, it enables substantially better performance through reduced verbosity and improved semantic clarity.

**Failure signatures**: Common failures include incomplete task decomposition leading to agent confusion, incorrect type annotations causing semantic ambiguity, and overly complex pseudocode that exceeds LLM context limits. Performance degradation typically manifests as reduced accuracy or increased token consumption.

**First experiments**: 1) Implement a simple two-agent system on a basic reasoning task to validate the Pseudocode+ encoding. 2) Compare token consumption between natural language and Pseudocode+ representations on a fixed problem set. 3) Test agent coordination through a multi-step planning task to verify modular decomposition effectiveness.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three specific benchmarks (GAIA, HotpotQA, and VirtualHome), with unclear generalization to other domains
- Token efficiency gains measured relative to specific baselines without comparison to other established multi-agent frameworks
- Impact of Pseudocode+ representation on task complexity beyond tested benchmarks is not established
- Baseline comparisons and dataset details for VirtualHome state-of-the-art claim lack full transparency

## Confidence
- High: The modular pseudocode representation for agent interactions is technically sound and aligns with established software engineering principles
- Medium: The reported token efficiency gains (55-87% input, 41-70% output) are based on internal calculations, but the exact methodology for tokenization and comparison is not fully disclosed
- Medium: The performance improvements (3-36 percentage points) are demonstrated on the selected benchmarks, but the robustness across diverse tasks and domains remains untested

## Next Checks
1. Evaluate CodeAgents on additional benchmarks beyond GAIA, HotpotQA, and VirtualHome to assess generalization and robustness across varied multi-agent reasoning tasks
2. Conduct ablation studies to quantify the individual contributions of the Pseudocode+ representation versus other framework components (e.g., task splitting, agent coordination)
3. Perform a detailed comparison with other established multi-agent frameworks (e.g., ReAct, Reflexion) on the same benchmarks to contextualize the claimed improvements