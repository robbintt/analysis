---
ver: rpa2
title: 'Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation'
arxiv_id: '2508.19966'
source_url: https://arxiv.org/abs/2508.19966
tags:
- arabic
- sentiment
- dataset
- language
- subjectivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a transformer-based approach for Arabic subjectivity
  evaluation. To address the lack of specialized annotated datasets, the authors developed
  AraDhati+, a comprehensive dataset combining existing Arabic resources.
---

# Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation

## Quick Facts
- arXiv ID: 2508.19966
- Source URL: https://arxiv.org/abs/2508.19966
- Reference count: 14
- Proposed approach achieved a remarkable accuracy of 97.79% for Arabic subjectivity classification

## Executive Summary
This paper presents Dhati+, a transformer-based approach for Arabic subjectivity evaluation. The authors address the challenge of limited specialized Arabic subjectivity datasets by creating AraDhati+, a comprehensive dataset combining existing Arabic resources. They fine-tune three state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and ArabianGPT) on this augmented dataset for effective subjectivity classification. Additionally, an ensemble decision approach was explored to combine the strengths of individual models, achieving a remarkable accuracy of 97.79%.

## Method Summary
The authors developed AraDhati+ by merging ASTD (tweets), LABR (book reviews), HARD (hotel reviews), and SANAD (news) datasets. They converted sentiment labels (positive/negative/neutral) to binary subjectivity labels (subjective/objective), treating sentiment-bearing texts as subjective and factual news articles as objective. Three Arabic language models (XLM-RoBERTa, AraBERT, ArabianGPT) were fine-tuned on this dataset using grid search over learning rates and epochs. An ensemble voting mechanism combined the predictions of all three models to improve robustness and accuracy.

## Key Results
- Achieved 97.79% accuracy for Arabic subjectivity classification using ensemble model Decision 2
- Demonstrated domain shift effect: augmented training improves performance on external domains but slightly degrades performance on the original ASTD domain (1.28-1.78% drop)
- Showed that ensemble voting improves robustness by averaging architectural biases of different transformer models

## Why This Works (Mechanism)

### Mechanism 1: Sentiment-to-Subjectivity Conversion
The approach converts sentiment-labeled data into binary subjectivity labels, treating all sentiment-bearing texts as subjective and factual news as objective. This effectively mitigates dataset scarcity by leveraging abundant sentiment resources rather than scarce subjectivity-specific annotations.

### Mechanism 2: Ensemble Voting for Robustness
An ensemble voting mechanism improves robustness by averaging the architectural biases of Transformer encoders (BERT-based) and decoders (GPT-based). Different pre-training objectives yield distinct feature representations, and voting allows the system to confirm predictions across these diverse representations.

### Mechanism 3: Augmented Data for Generalization
Fine-tuning on augmented data (AraDhati+) improves generalization to external domains but induces a performance trade-off on the original source domain. The model updates its weights to accommodate a broader distribution of linguistic patterns, shifting representation away from the specific distribution of the initial ASTD dataset.

## Foundational Learning

- **Subjectivity vs. Sentiment Analysis**
  - Why needed here: The paper conflates existing sentiment datasets into a subjectivity task
  - Quick check question: If a text says "The movie is three hours long," is it subjective or objective? (Answer: Objective). If it says "The movie was boring," is it subjective or objective? (Answer: Subjective, negative sentiment).

- **Transformers (Encoder vs. Decoder)**
  - Why needed here: The ensemble combines distinct architectures (AraBERT/XLM-R are Encoders; ArabianGPT is a Decoder)
  - Quick check question: Which architecture is likely better for understanding the relationship between the start and end of a sentence simultaneously? (Answer: Encoder/BERT-like models).

- **Domain Shift / Distribution Drift**
  - Why needed here: The paper explicitly notes a performance drop on the original dataset after augmentation
  - Quick check question: Why would a model trained on news articles and hotel reviews perform worse on Twitter data than a model trained only on Twitter data? (Answer: The vocabulary, syntax, and slang differ between the domains).

## Architecture Onboarding

- **Component map:** Data Pipeline (ASTD, LABR, HARD, SANAD) -> Preprocessing (cleaning, normalization) -> Model Layer (XLM-RoBERTa, AraBERT, ArabianGPT) -> Aggregation (Hard Voting Ensemble)

- **Critical path:** The data labeling logic is the most fragile component. The system depends entirely on the assumption that all "sentiment" data is purely subjective and all "news" data is purely objective. Failure to clean editorial content from the News (SANAD) set would poison the "Objective" class.

- **Design tradeoffs:** Purity vs. Volume (sacrificed pure subjectivity annotations to gain volume), Specificity vs. Generalization (training on AraDhati+ sacrificed peak performance on Twitter-specific data to gain +10-20% accuracy on news/reviews)

- **Failure signatures:** Mixed Tweets (factual statements mixed with opinion), Short Text (lack of context), dialect diversity issues

- **First 3 experiments:**
  1. Validate Baseline vs. Augmented: Train AraSubjBERT_1 (ASTD only) and AraSubjBERT_2 (AraDhati+) to confirm domain shift
  2. Ablate the Ensemble: Isolate the contribution of the GPT model vs. the BERT models to see if voting is necessary
  3. Error Analysis on "Mixed" Class: Replicate "Mixed Tweets" error analysis on SANAD (News) subset

## Open Questions the Paper Calls Out
- How can model drift be effectively mitigated to maintain subjectivity classification performance over time?
- How can classification accuracy be improved for "Mixed Tweets" containing both subjective and objective fragments?
- What techniques can resolve the lack of context in very short texts, such as the "Short Tweets" error category?

## Limitations
- Dataset construction methodology assumes perfect correlation between sentiment and subjectivity, which is not empirically validated
- Lack of explicit handling of Arabic dialectal variation, known to significantly impact Arabic NLP performance
- Reported accuracy is based on internally constructed dataset; generalization to truly unseen domains remains unproven

## Confidence
- **High Confidence:** Domain shift mechanism is directly observed and quantified in results
- **Medium Confidence:** Ensemble voting mechanism provides robustness is plausible and supported by marginal improvement
- **Low Confidence:** Assumption that sentiment data can be perfectly converted to subjectivity labels is untested

## Next Checks
1. Validate Dataset Purity: Manually sample and annotate a subset of "Objective" SANAD news articles to confirm they contain no subjective content
2. Dialectal Robustness Test: Evaluate the best model on a held-out test set of Arabic text from a different dialect region or social media source
3. Ablation Study on Data Sources: Train and evaluate models on each individual dataset separately to reveal which sources contribute most to performance