---
ver: rpa2
title: 'CrochetBench: Can Vision-Language Models Move from Describing to Doing in
  Crochet Domain?'
arxiv_id: '2511.09483'
source_url: https://arxiv.org/abs/2511.09483
tags:
- next
- join
- crochet
- each
- around
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CrochetBench is a new benchmark designed to evaluate whether multimodal\
  \ large language models can generate executable crochet procedures from images.\
  \ It introduces four tasks of increasing difficulty\u2014stitch recognition, instruction\
  \ selection, natural-language instruction generation, and translation into an executable\
  \ crochet DSL\u2014with 6,085 patterns integrated with the CrochetPARADE DSL for\
  \ automated validation."
---

# CrochetBench: Can Vision-Language Models Move from Describing to Doing in Crochet Domain?

## Quick Facts
- arXiv ID: 2511.09483
- Source URL: https://arxiv.org/abs/2511.09483
- Reference count: 40
- Models can recognize crochet stitches but struggle to generate executable procedural patterns.

## Executive Summary
CrochetBench evaluates whether multimodal large language models can translate images of crochet artifacts into executable procedural patterns. The benchmark introduces four tasks of increasing difficulty—stitch recognition, instruction selection, natural-language instruction generation, and translation into an executable crochet DSL—with 6,085 patterns integrated with the CrochetPARADE DSL for automated validation. Across all tasks, models show a consistent gap: they can recognize stitches and retrieve plausible instructions but struggle to synthesize structurally valid procedures or produce executable programs. Larger models often perform worse due to uncontrolled symbolic invention, and supervised finetuning improves fluency but not procedural correctness. This highlights a fundamental gap between visual perception and executable procedural reasoning.

## Method Summary
CrochetBench evaluates 9 VLMs on four crochet-related tasks using 6,085 patterns from Yarnspirations. Task A tests multi-label stitch recognition from images, Task B tests 4-way MCQ instruction selection, Task C tests natural-language instruction generation from images, and Task D tests NL-to-DSL translation using CrochetPARADE DSL with execution-based validation. The benchmark uses F1, accuracy, BLEU/ROUGE, and a custom Valid Pattern Rate metric (DSL compilation success) plus DINO similarity for project-level visual fidelity. Models include BLIP-2, Gemma 3, DeepSeek-VL, Qwen2-VL, GPT-4o, Gemini 2.5, and Claude Sonnet 4, with finetuning on Qwen2-VL 7B using an 80/5/15 split.

## Key Results
- Stitch recognition and instruction selection achieve moderate success (F1 ~50%, accuracy ~45%), but NL-to-DSL translation shows near-zero valid pattern rates (under 15% for steps 1-2).
- Larger models exhibit higher undefined-stitch error rates (Qwen2-VL-72B: 72.0% vs. 7B: 13.6%), suggesting uncontrolled symbolic invention.
- Supervised finetuning improves BLEU scores by 238% but does not improve executable correctness, highlighting the gap between fluency and procedural validity.

## Why This Works (Mechanism)

### Mechanism 1: Execution-Grounded Validation via DSL Compilation
The CrochetBench enables automated procedural correctness evaluation by requiring models to generate DSL programs that must compile and execute, not merely match text patterns. The CrochetPARADE DSL formalizes crochet patterns into structured primitives with explicit control flow (loops, groups, labels). Generated programs are passed through a validator that checks syntactic validity, structural consistency (stitch counts, balanced brackets), and executability. Valid programs are rendered to compare against ground-truth images using DINO similarity.

### Mechanism 2: Task-Progression Diagnostic Isolates Perception-Synthesis Gap
The four-task hierarchy (A→B→C→D) isolates where models transition from successful perception to failed synthesis. Tasks A-B test perception/grounding (multi-label classification, multiple-choice retrieval). Tasks C-D require generation/formalization with long-range dependencies. By measuring performance drop across tasks, the benchmark identifies whether failures stem from visual grounding (A-B) or procedural reasoning (C-D).

### Mechanism 3: Scaling Exposes Symbolic Invention Tendency
Larger models produce lower valid pattern rates because they invent novel stitch types not defined in the DSL, a form of uncontrolled symbolic hallucination. Increased model capacity correlates with fluency but also with creativity beyond constraints. Larger models (e.g., Qwen2-VL-72B) generate more undefined stitches (72.0% error rate vs. 13.6% for 7B), suggesting they override learned DSL vocabulary with plausible-but-invalid constructs.

## Foundational Learning

- **Concept: Domain-Specific Languages (DSLs) for Executable Evaluation**
  - Why needed: Understanding how DSLs enable automated correctness checking is prerequisite to interpreting Task D results and why text metrics (BLEU/ROUGE) are insufficient.
  - Quick check: Given a crochet instruction "sc2tog," what DSL operation does it map to, and why would "sc together" fail compilation despite semantic similarity?

- **Concept: Stateful Sequence Generation**
  - Why needed: Crochet patterns require maintaining state across rounds (stitch counts, loop positions). Models fail not on local tokens but on long-range consistency.
  - Quick check: If Round 1 ends with 12 stitches and Round 2 specifies "2 sc in each st," how many stitches should Round 2 produce? What happens if the model loses count at Round 3?

- **Concept: Fluency vs. Correctness Trade-off in Finetuning**
  - Why needed: Supervised finetuning improved BLEU by 238% but not executable correctness—a counterintuitive result requiring understanding of what finetuning optimizes.
  - Quick check: Why would a finetuned model produce better-formatted instructions with more correct abbreviations while still having incorrect stitch counts?

## Architecture Onboarding

- **Component map:** Product image → Stitch recognition → Instruction selection → Natural language generation → DSL translation → CrochetPARADE validator → (if valid) renderer → DINO similarity vs. ground truth

- **Critical path:** Task D (DSL Translation) is the most informative failure point. Early-step errors (steps 1-2) propagate irreversibly. Focus debugging on: (1) undefined stitch tokens, (2) unbalanced brackets, (3) multiple reference errors.

- **Design tradeoffs:**
  - Strict DSL grammar → unambiguous validation but may reject valid human patterns that use non-canonical notation
  - DINO similarity for semantic fidelity → coarse-grained proxy; low scores (0.10-0.17 across all models) suggest either rendering limitations or genuine structural divergence
  - Multiple-choice for Task B → scalable evaluation but may overestimate grounding via elimination heuristics

- **Failure signatures:**
  - High recognition (A), low generation (C): Perception intact, synthesis broken—likely state-tracking or symbolic reasoning deficit
  - Valid DSL but low DINO similarity: Surface compliance without structural fidelity
  - Larger model = more undefined stitches: Constraint-following failure; consider constrained decoding or vocabulary masking

- **First 3 experiments:**
  1. Baseline Task A-B performance: Run stitch recognition and instruction selection on your VLM to establish perception quality. If F1 < 50% or accuracy < 45%, perception is the bottleneck.
  2. Error taxonomy on Task D step-level: Generate DSL for steps 1-2, 1-4, 1-6 and classify failures into the paper's taxonomy (undefined stitch, unbalanced brackets, etc.). Identify if errors concentrate in early state initialization or later state maintenance.
  3. Constrained decoding ablation: Restrict model output vocabulary to valid DSL tokens only. Compare valid pattern rate against unconstrained baseline to quantify how much of the scaling penalty stems from symbolic invention vs. structural reasoning failure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What training paradigms beyond supervised finetuning—such as program-guided learning or execution-feedback reinforcement—could improve executable procedural correctness rather than just surface fluency?
- Basis: The limitations section states: "Exploring alternative training paradigms, such as program guided learning or learning with execution feedback, remains an important direction for future work."
- Why unresolved: Supervised finetuning improved BLEU (+238%) but did not fix stitch-count errors or malformed repeats, suggesting gradient descent on text similarity alone cannot enforce structural constraints.
- What evidence would resolve it: Comparing supervised, reinforcement-learning, and execution-guided training regimes on Task D's Valid Pattern Rate and DINO similarity metrics.

### Open Question 2
- Question: How can models be constrained to avoid uncontrolled symbolic invention (e.g., hallucinating undefined stitches) while preserving expressive generation?
- Basis: The paper reports that larger models exhibit higher rates of undefined-stitch errors (Qwen2-VL-72B: 72.0% vs. 7B: 13.6%; Gemma-27B: 42.6% vs. 4B: 27.4%), described as "uncontrolled symbolic invention."
- Why unresolved: Increased capacity improves descriptive fluency but appears to increase vocabulary drift; no current mechanism grounds generation strictly within the DSL's defined symbols.
- What evidence would resolve it: Interventions like constrained decoding, grammar-based sampling, or vocabulary masking, evaluated on the undefined-stitch error rate and Valid Pattern Rate.

### Open Question 3
- Question: Do procedural reasoning capabilities learned on CrochetBench transfer to other structured procedural domains such as knitting patterns, mechanical assembly instructions, or robotic task plans?
- Basis: The limitations section cautions: "generalization to other domains should be interpreted with caution" because "the benchmark focuses on a single creative domain."
- Why unresolved: Cross-domain transfer of procedural competence has not been tested; it is unknown whether the gap between perception and execution is domain-specific or fundamental.
- What evidence would resolve it: Zero-shot or few-shot evaluation of CrochetBench-trained models on analogous executable benchmarks (e.g., knitting DSLs, LEGO assembly programs) with executable validation.

### Open Question 4
- Question: What architectural or prompting mechanisms could improve early-step stability in stateful procedural synthesis, given that errors in initial steps propagate irreversibly?
- Basis: Figure 3 shows most models achieve under 15% validity in steps 1–2, improving only when given more context; the paper notes "errors made early propagate irreversibly" and "later correctness often occurs only when the initial state is accidentally valid."
- Why unresolved: Current models lack explicit state-tracking mechanisms; they rely on continuation heuristics rather than principled state initialization.
- What evidence would resolve it: Ablations comparing standard LLMs against models augmented with explicit state variables, memory modules, or chain-of-state prompting, measuring early-step Valid Pattern Rates.

## Limitations
- DSL vocabulary coverage may limit whether models fail on procedural reasoning versus symbol availability
- DINO similarity (0.10-0.17) is a coarse-grained proxy, making it unclear whether low scores reflect structural divergence or rendering limitations
- Cross-dataset generalization is untested, so findings may not transfer beyond Yarnspirations patterns

## Confidence
- **High**: Task A-B perception/grounding diagnostic; execution-grounded validation mechanism
- **Medium**: Scaling penalty mechanism (symbolic invention); finetuning fluency vs. correctness tradeoff
- **Low**: DINO similarity as fidelity measure; DSL vocabulary completeness

## Next Checks
1. Run stitch recognition and instruction selection on your VLM to establish baseline perception quality (F1/accuracy).
2. Generate DSL for steps 1-2, 1-4, 1-6 and classify errors using the paper's taxonomy to isolate early-step instability.
3. Test constrained decoding (DSL vocabulary only) against unconstrained baseline to quantify symbolic invention vs. structural reasoning deficits.