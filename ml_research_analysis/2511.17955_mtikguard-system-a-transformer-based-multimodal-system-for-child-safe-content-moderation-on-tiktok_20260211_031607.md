---
ver: rpa2
title: 'MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content
  Moderation on TikTok'
arxiv_id: '2511.17955'
source_url: https://arxiv.org/abs/2511.17955
tags:
- content
- dataset
- harmful
- video
- videos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MTikGuard, a real-time multimodal harmful content
  detection system for TikTok targeting child safety. The authors extended the TikHarm
  dataset from 3,948 to 4,723 videos by collecting additional diverse real-world samples.
---

# MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok

## Quick Facts
- arXiv ID: 2511.17955
- Source URL: https://arxiv.org/abs/2511.17955
- Authors: Dat Thanh Nguyen; Nguyen Hung Lam; Anh Hoang-Thi Nguyen; Trong-Hop Do
- Reference count: 2
- Primary result: 89.37% accuracy and 89.45% F1-score on TikTok harmful content detection

## Executive Summary
MTikGuard is a real-time multimodal system for detecting harmful content on TikTok, specifically designed for child safety. The system extends the TikHarm dataset from 3,948 to 4,723 videos and uses a transformer-based architecture integrating visual, audio, and textual features through late fusion. Built on Apache Kafka, Spark, Docker, and Airflow, it enables scalable streaming deployment. The ablation study confirms multimodal fusion's critical role, with single-modality models performing significantly worse than the combined approach.

## Method Summary
The system uses TimeSFormer for video representation, Whisper large-v3 for ASR, and EasyOCR for on-screen text extraction. Audio is capped at 60 seconds, and frames are sampled at 30% and 70% duration positions. Text modalities are encoded using Multilingual DistilBERT with the format "Audio: {transcript} | OCR: {text}". Features are concatenated and passed through multi-head self-attention layers before classification. Training uses 6 epochs with batch size 8, learning rate 1e-4, and gradient accumulation every 2 steps. The system processes videos through a Kafka-based pipeline with MongoDB storage and Airflow orchestration.

## Key Results
- Achieved 89.37% accuracy and 89.45% F1-score on test set
- Multimodal fusion outperformed single-modality models significantly
- OCR-only model reached 35.44% accuracy, ASR-only reached 49.11% accuracy
- TimeSFormer-only model achieved 86.7% accuracy as baseline

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Information Complementarity
Video captures visual cues, ASR provides spoken content, and OCR extracts on-screen text. When one modality is ambiguous, others provide disambiguating context. Harmful content often spans multiple modalities, and no single modality consistently captures all risk signals.

### Mechanism 2: Attention-Based Fusion for Noisy Real-World Data
Multi-head self-attention layers after concatenation allow the model to dynamically weight modality contributions based on contextual relevance, rather than treating all features equally. This is particularly valuable for diverse, imbalanced, or noisy real-world data.

### Mechanism 3: Noise Compensation Through Modality Redundancy
The system maintains robustness when individual modalities are corrupted. ASR errors from background noise are compensated by OCR-extracted text, and vice versa. This redundancy ensures correct classification when at least one modality provides clean signal.

## Foundational Learning

- **Transformer Attention (Space-Time)**: TimeSFormer uses Divided Space-Time Attention to separately model spatial and temporal dependencies. Quick check: Can you explain why separating spatial and temporal attention might improve video classification over joint attention?

- **Late Fusion vs. Early Fusion**: MTikGuard uses late fusion to preserve modality-specific representations. Quick check: What is the trade-off between late fusion (modality-specific encoders + concatenation) and early fusion (joint encoding from raw inputs)?

- **Multilingual Text Encoding**: TikTok content mixes Vietnamese and English, requiring cross-lingual representations. Quick check: How does a multilingual model like DistilBERT handle code-switched input (e.g., Vietnamese speech with English captions)?

## Architecture Onboarding

- **Component map**: Selenium crawler → Kafka producer → Kafka consumer (download, extract, classify) → MongoDB Atlas storage → Airflow orchestration
- **Critical path**: 1) Crawler fetches trending TikTok videos by hashtag 2) Kafka producer publishes metadata 3) Consumer downloads video, extracts audio → Whisper → transcript 4) Consumer extracts frames → EasyOCR → on-screen text 5) Fusion network → 4-way classification 6) Result → MongoDB
- **Design tradeoffs**: Late fusion vs. attention fusion (concat is simpler; attention adds ~0.1-0.2 F1 improvement), frame sampling (2 frames balances cost vs. coverage), 60-second audio limit (reduces processing time), imbalanced extended dataset (reflects real-world distribution)
- **Failure signatures**: Low ASR quality from background music, OCR misses from stylized fonts, misclassification between Harmful Content and Suicide, pipeline stalls from Kafka consumer backlog
- **First 3 experiments**: 1) Run TimeSFormer-only to confirm ~86.7% accuracy baseline 2) Compare concat vs. attention fusion on validation set 3) Test noise robustness by manually corrupting one modality on 50 samples

## Open Questions the Paper Calls Out
- Would attention-driven or adaptive multimodal fusion mechanisms significantly improve detection performance and interpretability compared to current late-fusion strategy?
- Would multi-label classification outperform single-label assignment for TikTok videos containing overlapping harmful content categories?
- Would incorporating non-speech audio features (tone, music, sound effects) improve detection of harmful content lacking explicit verbal cues?

## Limitations
- Several critical implementation details unspecified (dropout rates, optimizer configuration, exact network dimensions)
- Dataset expansion may introduce class imbalance, particularly affecting minority classes like Suicide
- Real-world deployment assumptions (Kafka throughput, inference latency) lack performance metrics under production load
- Code-switching handling effectiveness across diverse linguistic patterns remains untested

## Confidence
- **High confidence**: Multimodal fusion consistently outperforms single-modality approaches
- **Medium confidence**: Attention-based fusion provides meaningful advantage over simple concatenation
- **Medium confidence**: Real-time deployment architecture is scalable and robust

## Next Checks
1. Implement the full training pipeline using specified hyperparameters to verify baseline performance matches reported 89.37% accuracy
2. Conduct systematic noise injection tests by degrading individual modalities to validate the noise compensation mechanism
3. Run class-specific performance analysis to quantify per-category recall, particularly for underrepresented classes like Suicide