---
ver: rpa2
title: Mitigating the Participation Bias by Balancing Extreme Ratings
arxiv_id: '2502.03737'
source_url: https://arxiv.org/abs/2502.03737
tags:
- ratings
- rating
- aggregator
- participation
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of rating aggregation under participation
  bias, where some raters do not report their ratings with probabilities depending
  on the rating values. The authors propose two novel aggregators: the Balanced Extremes
  Aggregator (BEA) for known sample sizes, which estimates unobserved ratings using
  a convex combination of extreme ratings, and the Polarizing-Averaging Aggregator
  (PAA) for unknown sample sizes, which averages two polarized histograms.'
---

# Mitigating the Participation Bias by Balancing Extreme Ratings

## Quick Facts
- **arXiv ID:** 2502.03737
- **Source URL:** https://arxiv.org/abs/2502.03737
- **Reference count:** 40
- **One-line primary result:** BEA nearly matches theoretical lower bounds across participation probabilities, while PAA achieves near-optimal performance as sample size grows.

## Executive Summary
This paper addresses the problem of rating aggregation under participation bias, where some raters do not report their ratings with probabilities depending on the rating values. The authors propose two novel aggregators: the Balanced Extremes Aggregator (BEA) for known sample sizes, which estimates unobserved ratings using a convex combination of extreme ratings, and the Polarizing-Averaging Aggregator (PAA) for unknown sample sizes, which averages two polarized histograms. Numerical results show that BEA nearly matches theoretical lower bounds across a wide range of participation probabilities, while PAA achieves near-optimal performance as sample size grows. Experiments on real-world hotel rating data demonstrate that PAA outperforms simple averaging and the spectral method in mitigating participation bias.

## Method Summary
The paper introduces two aggregators for robust rating aggregation under participation bias. BEA (Balanced Extremes Aggregator) is used when the total sample size is known and estimates unobserved ratings using a weighted combination of the lowest and highest possible ratings, where the weight depends on the difference between counts of extreme ratings. PAA (Polarizing-Averaging Aggregator) is used when sample size is unknown and works by finding thresholds to create two modified histograms representing polarized scenarios, then averaging their empirical means. Both methods minimize worst-case regret rather than absolute error, providing robustness against adversarial bias conditions.

## Key Results
- BEA nearly matches theoretical lower bounds across a wide range of participation probabilities when sample size is known.
- PAA achieves near-optimal performance as sample size grows when sample size is unknown.
- On real-world hotel rating data, PAA outperforms simple averaging and the spectral method in mitigating participation bias.

## Why This Works (Mechanism)

### Mechanism 1: Extreme Balancing for Unknown Ratings (BEA)
- **Claim:** When the sample size is known, estimating missing ratings using a weighted combination of the lowest (1) and highest ($m$) possible ratings creates a robust aggregate that resists polarization bias.
- **Mechanism:** The Balanced Extremes Aggregator (BEA) calculates a weight $\alpha$ based on the difference between the counts of extreme ratings ($n_1 - n_m$). It assumes unobserved ratings have an expected mean of $\alpha \times 1 + (1-\alpha) \times m$. This effectively "reclaims" the middle ground by balancing the observed extremes against the theoretical extremes.
- **Core assumption:** Assumption: The worst-case bias involves information structures where ratings are polarized (either extremely low or high) and participation rates vary adversarially between these extremes.
- **Evidence anchors:**
  - [Page 4, Definition 3.1]: "BEA estimates the expectation of the unobserved ratings... using a convex combination of extreme ratings... where $\alpha$ depends on the difference between the count of ratings of 1 and the count of ratings of $m$."
  - [Page 3, Section 1.1]: "BEA estimates unrevealed ratings with a balanced combination of extreme ratings."
  - [Corpus]: Weak relevance in corpus; no direct validation of this specific mathematical balancing act found in neighbors.
- **Break condition:** When the participation lower bound $q$ is close to 1 (no bias), BEA may underperform simple averaging because the correction mechanism adds unnecessary noise (Page 4, Figure 5).

### Mechanism 2: Bounding via Polarized Histograms (PAA)
- **Claim:** When the sample size is unknown, the true mean can be optimally estimated by averaging the empirical means of two specifically "polarized" histograms derived from the observed data.
- **Mechanism:** The Polarizing-Averaging Aggregator (PAA) creates bounds for the true mean. It computes a lower bound $l$ by keeping only a $q$ fraction of counts for ratings above a threshold $k_1$ (simulating the case where high raters are under-reporting) and an upper bound $u$ by keeping a $q$ fraction below threshold $k_2$. The average of these two bounds $(u+l)/2$ minimizes the maximum squared distance to the true mean.
- **Core assumption:** Assumption: As sample size $n \to \infty$, the optimal aggregator is the midpoint of the extreme possible values of the true mean given the observed empirical distribution.
- **Evidence anchors:**
  - [Page 5, Theorem 4.2]: "When $n \to \infty$, PAA is optimal."
  - [Page 3, Section 1.2]: "PAA averages two modified histograms with polarized thresholds."
  - [Corpus]: No direct evidence in neighbors for this specific aggregation logic.
- **Break condition:** Finite sample sizes where the empirical distribution deviates significantly from the theoretical limit, although the paper claims PAA is near-optimal (Page 6, Theorem 4.4).

### Mechanism 3: Minimax Regret Optimization
- **Claim:** The system ensures robustness by minimizing the worst-case "regret" (squared error relative to an ideal omniscient aggregator) rather than minimizing absolute error.
- **Mechanism:** The paper models the problem as a game against "nature." Nature chooses a distribution and participation bias to maximize error; the aggregator chooses an output to minimize it. By optimizing for the worst-case scenario (specifically, mixtures of polarized information structures), the aggregator avoids catastrophic failure modes common in simple averaging.
- **Core assumption:** The goal is safety from adversarial bias conditions, not necessarily maximizing accuracy in average/benign scenarios.
- **Evidence anchors:**
  - [Page 3, Section 1.1]: "We adopt a robust approach and instead minimize the regret relative to the ideal aggregator... in the worst-case scenario."
  - [Page 4, Lemma 3.2]: Establishes a lower bound of regret based on a mixture of two specific adversarial information structures.
- **Break condition:** If the actual participation bias is mild or non-adversarial, this conservative approach may result in lower precision than standard methods.

## Foundational Learning

- **Concept: Participation Bias (Self-Selection)**
  - **Why needed here:** This is the core problem. One must understand that observed ratings are conditional on the user's decision to rate, often skewing toward extreme positive or negative experiences (J-shaped distribution), leaving the "silent majority" unobserved.
  - **Quick check question:** Why does a hotel with a 5.0 average from 2 reviews feel riskier than a 4.5 from 500 reviews, and how does this relate to $q$?

- **Concept: Regret vs. Loss**
  - **Why needed here:** The paper optimizes "regret," which is the performance gap relative to the *ideal* scenario where all ratings are known. Standard machine learning usually optimizes "loss" (absolute error).
  - **Quick check question:** If the true mean is 3.0 and the aggregator outputs 4.0, is the regret higher or lower if the observed sample was extremely sparse vs. extremely dense? (Answer: Regret accounts for the difficulty of the specific instance).

- **Concept: Robust Aggregation**
  - **Why needed here:** The solution assumes the data generation process is "hostile" (worst-case). This is a paradigm shift from assuming data is missing at random.
  - **Quick check question:** In a robust model, if you don't know if missing voters love or hate a product, what single value is the "safest" prediction for their aggregate behavior? (Answer: The midpoint of the bounds).

## Architecture Onboarding

- **Component map:** Histogram of counts ($n_1 \dots n_m$) -> Normalize to empirical distribution $\hat{p}$ -> Logic Branch 1 (BEA) or Branch 2 (PAA) -> Scalar aggregated rating
- **Critical path:** Determining the parameters $a^*$ (for BEA) and thresholds $k_1, k_2$ (for PAA). For PAA, finding the thresholds involves maximizing a condition over the histogram indices, which is the computational bottleneck.
- **Design tradeoffs:**
  - **Known vs. Unknown $n$:** Use BEA if you know the total population (e.g., students in a class). Use PAA if you only see volunteers (e.g., online reviews). BEA is theoretically tighter; PAA is more versatile.
  - **Conservatism:** Both methods "shrink" estimates toward the center. If preserving high variance is desired (e.g., ranking top-tier items), this architecture might artificially compress scores.
- **Failure signatures:**
  - **Over-correction:** If $q$ is set too low (pessimistic), the aggregator will flatten all ratings toward the median $(m+1)/2$, losing signal.
  - **Threshold Instability:** In PAA, if the histogram is flat, small changes in data might toggle thresholds $k_1, k_2$, causing the aggregate score to jump discretely.
- **First 3 experiments:**
  1.  **Sanity Check (Synthetic):** Generate a polarized dataset ($n_1=50, n_5=50$). Run simple averaging (output 3.0) vs. PAA with $q=0.1$. Verify PAA corrects toward the mean.
  2.  **Sensitivity to $q$:** On the hotel dataset (Table 1), sweep $q$ from 0.1 to 0.9. Plot the output rating vs. $q$ to visualize the "shrinkage" effect.
  3.  **Comparison to Baselines:** Compare BEA/PAA against the Spectral Method (SPE) and simple averaging on synthetic data with known ground truth to replicate Figure 5/11.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive algorithms be developed to dynamically adjust to varying participation probabilities over time?
- Basis: [explicit] The authors state that future work could explore adaptive algorithms to enhance robustness in rapidly changing environments.
- Why unresolved: The current paper assumes static participation probabilities for a fixed set of raters.
- What evidence would resolve it: Empirical results on temporal datasets demonstrating robustness to shifting user participation rates.

### Open Question 2
- Question: How can the model be extended to incorporate user heterogeneity, such as systematic biases based on demographics or past behavior?
- Basis: [explicit] The conclusion suggests incorporating user behavior analysis to yield more general aggregators, departing from the current assumption of homogeneous raters.
- Why unresolved: The current theoretical framework relies on the assumption that all raters share the same participation bias.
- What evidence would resolve it: An aggregation method that models distinct participation probabilities for different user clusters and validates performance on demographic datasets.

### Open Question 3
- Question: Can the aggregators be generalized to handle multi-dimensional ratings (e.g., rating service, quality, and price simultaneously)?
- Basis: [explicit] The authors identify extending aggregators to handle multi-dimensional ratings as a promising direction for modern applications.
- Why unresolved: The proposed BEA and PAA are currently formulated only for single-attribute scalar ratings.
- What evidence would resolve it: A vector-based formulation of the aggregators that maintains low regret on multi-attribute review datasets.

### Open Question 4
- Question: Can the aggregators be modified to perform better on non-polarized, smooth distributions typical of real-world data?
- Basis: [inferred] Section 6.1 notes that BEA underperformed compared to simple averaging on hotel data, possibly because it is optimized for worst-case polarized scenarios rather than smooth ones.
- Why unresolved: The robust approach focuses on worst-case regret, which may sacrifice accuracy on "average" or smooth distributions.
- What evidence would resolve it: A hybrid aggregator that matches simple averaging on smooth distributions while retaining robustness to polarization.

## Limitations
- The numerical optimization for parameter $a^*$ in BEA is not fully specified, potentially affecting reproducibility.
- The paper's theoretical optimality holds under adversarial assumptions that may not reflect real-world participation bias patterns.
- BEA's performance degradation when participation rates are high ($q > 0.8$) represents a significant limitation for practical deployment.

## Confidence

- **High confidence:** The core mathematical framework and theorems are sound and internally consistent. The problem formulation (participation bias with known lower bounds) is clearly articulated.
- **Medium confidence:** The numerical experiments demonstrate effectiveness, but the lack of implementation details for optimization steps creates uncertainty about exact replication.
- **Low confidence:** The paper's claim that real-world data follows the worst-case adversarial structure assumed in theory is not empirically validated.

## Next Checks

1. **Implementation verification:** Recreate BEA and PAA using synthetic data with controlled participation bias to verify the algorithms match reported performance curves.
2. **Parameter sensitivity analysis:** Systematically vary the participation lower bound $q$ and sample size $n$ to map the boundaries where BEA/PAA outperform simple averaging.
3. **Real-world robustness test:** Apply the methods to diverse rating datasets (not just hotel reviews) to assess performance across different rating scales and bias patterns.