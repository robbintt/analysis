---
ver: rpa2
title: Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation
arxiv_id: '2510.16829'
source_url: https://arxiv.org/abs/2510.16829
tags:
- support
- your
- help
- role
- cravings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CORUS, a framework for simulating role-based
  questions by embedding social context implicitly in queries. Drawing on role theory
  and posts from an online OUD recovery community, the authors construct a taxonomy
  of three information-seeking roles (patients, caregivers, practitioners) and use
  it to simulate 15,321 role-specific questions.
---

# Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation

## Quick Facts
- **arXiv ID**: 2510.16829
- **Source URL**: https://arxiv.org/abs/2510.16829
- **Reference count**: 40
- **Primary result**: Role-based contexts systematically shape LLM responses: patient/caregiver framing increases support (+15–19%) but reduces knowledge (−15–23%), while practitioner framing decreases support (−9%) and readability

## Executive Summary
This paper introduces CORUS, a framework for simulating role-based questions by embedding social context implicitly in queries. Drawing on role theory and posts from an online OUD recovery community, the authors construct a taxonomy of three information-seeking roles (patients, caregivers, practitioners) and use it to simulate 15,321 role-specific questions. Evaluation shows these questions are highly believable (90% human-like, 99% role-faithful) and comparable to real-world data. When used to evaluate five LLMs, role-based contexts systematically shape responses: patient/caregiver framing increases support (+15–19%) but reduces knowledge (−15–23%) and increases readability, while practitioner framing leaves knowledge unchanged, decreases support (−9%), and reduces readability. This reveals that current role-agnostic evaluations miss critical variations in how LLMs respond to the same question across user roles, especially in sensitive domains like OUD recovery.

## Method Summary
The CORUS framework constructs a role taxonomy from 10,017 posts in r/OpiatesRecovery, filtering to English, non-deleted, text-only content. Posts are summarized into goal, behavior, and experience facets using Claude 3 Haiku, then embedded with all-mpnet-base-v2 and clustered via k-means (k=4). The resulting three roles (patient, caregiver, practitioner) are validated against human annotators. Role-based questions are simulated by prompting Claude 3.7 Sonnet with taxonomy attributes and sampled facet summaries, generating 15,321 questions (5,107 role-agnostic base questions × 3 roles). Five LLMs (GPT-4o, Gemini-2.5-Flash, Llama-3.1-8B/70B, OpenBioLLM-70B) are evaluated on these questions at temp=0.6, max_tokens=512, with responses analyzed for knowledge, support, readability, and abstention rates.

## Key Results
- CORUS questions achieve 90% human-like believability, 99% role faithfulness, and match real-world question diversity
- Role-based contexts cause systematic shifts in LLM responses: patient/caregiver framing increases support (+15–19%) but reduces knowledge (−15–23%) and increases readability
- Practitioner framing leaves knowledge unchanged, decreases support (−9%), and reduces readability
- Abstention rates increase with vulnerability cues: 15% for patients, 9% for caregivers, 5% for practitioners

## Why This Works (Mechanism)

### Mechanism 1: Implicit Role Signaling via Behavioral Framing
Embedding role-specific goals, behaviors, and experiences in queries causes LLMs to systematically adjust response content (knowledge vs. support trade-off) and linguistic complexity. LLMs trained on web text associate specific linguistic patterns (emotional disclosure, technical terminology, first-person narratives) with distinct social roles. When queries exhibit these patterns, models activate learned associations between roles and appropriate response styles—more empathetic/supportive for vulnerable roles, more technical for practitioners.

### Mechanism 2: Knowledge-Support Trade-off via Mirroring
Models mirror user linguistic and emotional cues, trading knowledge content for supportive language when queries signal vulnerability or distress. Emotional and vulnerable language in patient/caregiver queries activates higher weights on supportive/empathic response patterns in the model's learned distribution. This crowds out informational content due to output length constraints and priority given to addressing perceived emotional needs.

### Mechanism 3: Safety-Driven Abstention Triggered by Vulnerability Cues
Queries with role-based vulnerability cues (patient, caregiver) trigger higher abstention rates due to conservative safety guardrails for sensitive domains. Safety classifiers or refusal mechanisms trained to detect high-risk queries (e.g., health advice for vulnerable users) are more likely to flag role-based queries with distress language, leading to refusals.

## Foundational Learning

**Role Theory**
- Why needed here: Provides theoretical basis for defining roles as socially structured expectations (goals, behaviors, experiences) that shape question framing
- Quick check question: How does role theory differentiate between a role (e.g., "patient") and a demographic attribute (e.g., "age")?

**Ecological Validity in Evaluation**
- Why needed here: Justifies why simulated questions must be "believable" (human-like, contextually plausible, role-faithful) to reflect real-world LLM use
- Quick check question: What is the risk if evaluation queries lack ecological validity (e.g., are role-agnostic or AI-sounding)?

**Knowledge-Support Trade-off**
- Why needed here: Central finding that models sacrifice informational content when responding to vulnerable user roles
- Quick check question: In the paper's findings, which role framing results in the largest drop in knowledge content?

## Architecture Onboarding

**Component map**: Taxonomy Construction -> Role-based Question Simulation -> Evaluation

**Critical path**: Taxonomy quality → Simulation believability → Response evaluation metrics. Errors in taxonomy (e.g., role mischaracterization) propagate to simulation and evaluation.

**Design tradeoffs**: Using LLMs for taxonomy and simulation scales analysis but risks bias if models misrepresent community norms; validating with human annotators mitigates but does not eliminate this risk.

**Failure signatures**:
- Low role faithfulness in simulated questions (implies taxonomy not reflected in prompts)
- High abstention rates masking response patterns (safety guardrails interfere with evaluation)
- Classifier misalignment (knowledge/support classifiers not validated for OUD domain)

**First 3 experiments**:
1. Replicate taxonomy construction on a different community (e.g., mental health forum) to test generalizability of role definitions
2. Ablate behavioral/experiential summaries in simulation prompts to measure impact on believability metrics
3. Evaluate a new LLM (not in original study) with CORUS questions to test if role-based response shifts persist across models

## Open Questions the Paper Calls Out

**Open Question 1**: Does the CORUS framework effectively transfer to other sensitive domains such as mental health or maternal care, or does it require significant taxonomy restructuring?
- Basis: Discussion section states authors hope methodology extends to other domains
- Why unresolved: Current taxonomy from single community; may not apply to domains with different role structures
- What evidence would resolve it: Successfully constructing role taxonomy and simulating believable questions in distinct domain (e.g., mental health)

**Open Question 2**: Does the observed reduction in knowledge content for vulnerable roles (patients/caregivers) correlate with a decrease in factual accuracy or an increase in hallucinations?
- Basis: Limitations section notes authors analyzed knowledge content but not factual accuracy
- Why unresolved: Unclear if models trade accuracy for support when addressing vulnerable users, or simply provide less information
- What evidence would resolve it: Evaluation measuring hallucination rates or factual consistency against verified ground truth

**Open Question 3**: How do LLM responses shift in multi-turn interactions where social context accumulates and user roles may evolve over time?
- Basis: Limitations section highlights study restricted to single-turn interactions
- Why unresolved: Current analysis treats role as static; real-world usage involves context accumulation and role transitions
- What evidence would resolve it: Longitudinal study or simulation where user's role and context revealed incrementally across multiple dialogue turns

## Limitations

- Role-based simulation relies on LLM-generated summaries, creating potential bias if models mischaracterize role-specific behaviors
- Knowledge-support trade-off depends on classifier performance in OUD domain, validated but not extensively tested across contexts
- Abstention mechanism attribution is indirect—safety guardrails are inferred rather than explicitly measured

## Confidence

**High confidence**: Role-based simulation methodology (proven believability metrics), systematic variation in LLM responses by role (consistent across 5 models), and knowledge-support trade-off direction (statistically significant differences)

**Medium confidence**: Mirroring mechanism explanation (supported by correlation but not causal evidence), safety-driven abstention hypothesis (based on pattern matching rather than ablation studies), classifier validity in OUD domain (validated but domain-specific)

**Low confidence**: Claims about why practitioner framing leaves knowledge unchanged (could reflect corpus limitations), generalizability beyond OUD recovery contexts (single-community study)

## Next Checks

1. **Ablation study**: Remove emotional language from role-based questions while preserving behavioral context to test whether the knowledge-support trade-off persists without vulnerability cues
2. **Cross-domain replication**: Apply CORUS to mental health or chronic disease communities to test if role-based response patterns hold beyond OUD recovery
3. **Safety classifier audit**: Analyze model attention patterns or conduct targeted prompting to determine whether abstention decisions are triggered by vulnerability cues versus medical content per se