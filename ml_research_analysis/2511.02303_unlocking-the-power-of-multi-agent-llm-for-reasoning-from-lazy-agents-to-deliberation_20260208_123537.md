---
ver: rpa2
title: 'Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to
  Deliberation'
arxiv_id: '2511.02303'
source_url: https://arxiv.org/abs/2511.02303
tags:
- reasoning
- arxiv
- agent
- preprint
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical issue of lazy agent behavior in
  multi-agent LLM reasoning, where one agent dominates while the other contributes
  minimally, undermining collaboration. The authors provide theoretical analysis showing
  that multi-turn GRPO's normalization term biases optimization toward shorter trajectories,
  encouraging lazy behavior.
---

# Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation

## Quick Facts
- arXiv ID: 2511.02303
- Source URL: https://arxiv.org/abs/2511.02303
- Reference count: 40
- One-line primary result: Dr. MAMR mitigates lazy agent behavior in multi-agent LLM reasoning, achieving up to 8% gains over single-agent GRPO and ReMA on mathematical benchmarks.

## Executive Summary
This paper addresses a critical failure mode in multi-agent LLM reasoning systems: lazy agent behavior, where one agent dominates while the other contributes minimally. Through theoretical analysis, the authors show that multi-turn GRPO's normalization term creates gradient bias toward shorter trajectories, encouraging lazy agents. They propose Dr. MAMR, which removes this normalization, introduces Shapley-inspired causal influence measurement to quantify agent contributions, and adds a verifiable restart reward mechanism. Experiments across seven mathematical reasoning benchmarks demonstrate consistent improvements over existing methods, with gains up to 8% on challenging tasks.

## Method Summary
Dr. MAMR builds on multi-turn GRPO by removing the 1/T normalization term that biases optimization toward shorter trajectories. It introduces a Shapley-inspired causal influence method that measures each agent's contribution by averaging influence scores across semantically similar reasoning steps (using Qwen2.5-0.5B embeddings with cosine similarity ≥0.9). The framework also implements a verifiable restart reward that allows the reasoning agent to discard noisy intermediate outputs and restart when beneficial. The method is trained using the Verl RL framework on the DeepScaleR dataset, with agents sharing weights but operating under role-specific system prompts for meta-thinking and reasoning tasks.

## Key Results
- Dr. MAMR achieves up to 8% improvement over single-agent GRPO and ReMA on challenging mathematical benchmarks
- Causal influence analysis confirms effective mitigation of lazy agent behavior during training
- Verifiable restart reward mechanism enables reasoning agents to recover from noisy intermediate outputs
- Method demonstrates stability across seven benchmarks including MATH500, GSM8K, AIME24/25, and OlympiadBench

## Why This Works (Mechanism)

### Mechanism 1: Normalization Debiasing in Multi-Turn GRPO
Removing the 1/T turn-level normalization reduces gradient bias toward shorter trajectories that exhibit lazy agent behavior. The original multi-turn GRPO objective includes a normalization term that averages turn-level advantages, creating preference for shorter trajectories unless longer ones have proportionally larger aggregated contributions. Lazy behaviors (empty outputs, trivial summarization) naturally produce shorter turn counts, receiving preferential reinforcement.

### Mechanism 2: Shapley-Inspired Causal Influence Measurement
Averaging causal influence scores across semantically similar steps provides stable, phrasing-robust estimates of each agent's contribution. For each anchor step, the method forms groups of semantically similar steps (cosine similarity ≥0.9) and computes one-step causal influence by comparing next-step probability under full history vs. masked history. The Shapley-inspired CI is the average across the group, avoiding expensive resampling while reducing single-trajectory bias.

### Mechanism 3: Verifiable Restart Reward for Deliberation Recovery
A binary verifiable reward signal tied to the causal effect of restart actions on final-answer confidence enables the reasoning agent to learn when to discard noisy intermediate outputs. When <restart> is emitted, the method masks prior reasoning outputs and computes the log-probability difference for the final answer. If masking increases confidence and the answer is correct, the restart is rewarded; if confidence decreases, it is penalized.

## Foundational Learning

- Concept: Group Relative Preference Optimization (GRPO)
  - Why needed here: Dr. MAMR builds directly on multi-turn GRPO; understanding the base objective, importance ratios, and KL regularization is prerequisite to comprehending the modifications.
  - Quick check question: Can you explain how GRPO computes advantages relative to group baselines rather than single-rollout baselines, and why this matters for variance reduction?

- Concept: Causal Influence via Attention Suppression
  - Why needed here: The Shapley-inspired CI mechanism extends attention-suppression methods; understanding how suppressing attention to prior tokens and measuring KL divergence in logits quantifies influence is essential.
  - Quick check question: Given a transformer, how would suppressing attention to tokens in step s_t affect the distribution over the next step s_{t+1}, and what does a large KL divergence indicate?

- Concept: Hierarchical Multi-Agent Credit Assignment
  - Why needed here: The meta-thinking/reasoning agent split is a hierarchical decomposition; classical MARL credit assignment problems manifest differently in sequential LLM settings.
  - Quick check question: Why is credit assignment harder in sparse-reward, multi-agent settings than in dense-reward single-agent RL, and how does sequential (vs. simultaneous) agent action change the problem?

## Architecture Onboarding

- Component map: Meta-thinking agent (π_h) -> Reasoning agent (π_l) -> Multi-turn GRPO trainer -> Causal influence estimator -> Restart reward module
- Critical path:
  1. Initialize both agents from same base model with role-specific prompts (RL-from-base is viable for Qwen2.5-Instruct)
  2. For restart behavior, perform SFT on expert data with adversarially inserted noise + restart demonstrations
  3. During each training step: sample G rollouts, compute CI for all steps via semantic grouping, compute restart rewards where applicable, aggregate advantages, perform gradient update with normalization removed
  4. Monitor causal influence distributions to verify balanced contribution
- Design tradeoffs:
  - Weighted advantage (α, β hyperparameters): Paper uses α=β=0.1 for all experiments; higher weights increase regularization toward balanced contribution but may overconstrain optimization
  - Semantic similarity threshold (0.9): Higher thresholds yield smaller, more precise groups but risk excluding functionally similar steps
  - SFT for restart behavior: Required because base models do not spontaneously emit <restart>; introduces cold-start dependency
- Failure signatures:
  - Causal influence collapse: Reasoning agent influence → 0 as training progresses (indicates lazy agent dominance)
  - Training curve collapse: Mean reward drops sharply (indicates reward hacking or unstable credit assignment)
  - Excessive restarts without improvement: Frequent <restart> tokens but no accuracy gain (may indicate reward is being gamed)
- First 3 experiments:
  1. Reproduce lazy agent diagnosis on Qwen2.5-3B: Train ReMA for 50 steps, track causal influence distributions, confirm reasoning agent influence declines
  2. Ablate normalization only: Train with 1/T normalization removed but without CI or restart rewards; compare to full ReMA on MATH500 subset
  3. Validate restart reward in isolation: Freeze meta-thinking agent, train reasoning agent with restart reward only; evaluate on multi-turn benchmark with deliberately noisy intermediate instructions

## Open Questions the Paper Calls Out
- Can Dr. MAMR effectively generalize to non-mathematical reasoning tasks such as code generation or open-ended planning?
- Does the proposed framework maintain training stability and performance benefits when scaled to systems with more than two agents?
- How sensitive is the Shapley-inspired causal influence estimation to the specific semantic similarity threshold used to group reasoning steps?

## Limitations
- Dataset dependency: Evaluation relies heavily on DeepScaleR dataset and curated benchmarks; preprocessing pipeline not fully specified
- Hyperparameter sensitivity: Results use fixed hyperparameters without ablation studies on their impact
- Lazy agent diagnosis: Relies on qualitative patterns (causal influence trends) rather than quantitative thresholds or controlled ablation studies

## Confidence

- High confidence: Empirical performance improvements (up to 8% gains) are well-documented and reproducible given access to same datasets
- Medium confidence: Shapley-inspired causal influence method is plausible but assumes semantic similarity reliably captures functional equivalence
- Low confidence: Theoretical analysis of gradient bias is correct but simplified; assumes linear approximation that may not hold in practice

## Next Checks

1. Ablate normalization only: Train a model with normalization removed but without causal influence or restart reward components; compare lazy agent behavior and performance to full Dr. MAMR
2. Sensitivity to semantic similarity threshold: Vary the cosine similarity threshold (0.8, 0.85, 0.95) and measure impact on causal influence stability and task performance
3. Cross-dataset generalization: Evaluate Dr. MAMR on a held-out reasoning dataset (MATH subset not in DeepScaleR) to test generalization beyond training distribution