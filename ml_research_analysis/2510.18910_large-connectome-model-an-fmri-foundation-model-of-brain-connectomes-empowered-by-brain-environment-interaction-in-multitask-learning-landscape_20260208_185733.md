---
ver: rpa2
title: 'Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered
  by Brain-Environment Interaction in Multitask Learning Landscape'
arxiv_id: '2510.18910'
source_url: https://arxiv.org/abs/2510.18910
tags:
- brain
- data
- foundation
- learning
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Large Connectome Model (LCM), a scalable
  transformer-based foundation model for brain fMRI that leverages multitask learning
  on brain-environment interactions. Unlike prior approaches relying on masked reconstruction
  of raw BOLD signals, LCM pretrains on functional connectomes using phenotypic and
  environmental variables as supervision, enabling better alignment with downstream
  clinical tasks.
---

# Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape

## Quick Facts
- arXiv ID: 2510.18910
- Source URL: https://arxiv.org/abs/2510.18910
- Authors: Ziquan Wei; Tingting Dan; Guorong Wu
- Reference count: 19
- Primary result: LCM achieves state-of-the-art performance on sex prediction, cognitive state recognition, and early disease diagnosis across eight datasets using multitask pretraining on brain-environment interactions.

## Executive Summary
This paper introduces the Large Connectome Model (LCM), a scalable transformer-based foundation model for brain fMRI that leverages multitask learning on brain-environment interactions. Unlike prior approaches relying on masked reconstruction of raw BOLD signals, LCM pretrains on functional connectomes using phenotypic and environmental variables as supervision, enabling better alignment with downstream clinical tasks. The model employs a decoder-only architecture with cross-attention between brain connectomes and tokenized brain-environment interactions, supporting both multitask pretraining and semi-supervised finetuning. Evaluated across eight datasets, LCM outperforms state-of-the-art models on sex prediction, cognitive state recognition, and early disease diagnosis (Autism, Parkinson's, Alzheimer's, Schizophrenia) with superior scalability—larger model sizes consistently improve performance.

## Method Summary
LCM is a decoder-only transformer that processes functional connectivity (FC) matrices as inputs, where each brain region's time series is correlated with all others to form an N×N matrix. The model uses learnable BEI tokens (V ∈ R^(P×E)) that undergo self-attention to communicate across tasks, followed by cross-attention with the FC matrix for conditioning. Training occurs in two stages: momentum phase (5 epochs) averaging predictions across all layers, then adaptive training selecting the best-performing layer per BEI. The model supports semi-supervised finetuning by concatenating new task tokens to pretrained BEI tokens and assigning pseudo-labels. Three model sizes are evaluated: Small (8L/147M), Mid (20L/735M), and Big (32L/1175M).

## Key Results
- LCM outperforms state-of-the-art models on eight datasets for sex prediction, cognitive state recognition, and disease diagnosis
- Larger model sizes consistently improve performance across all tasks, demonstrating strong scalability
- Adaptive layer selection enables different BEIs to be predicted at optimal depths, accommodating varying feature complexity
- Cross-attention visualizations reveal interpretable disease-relevant patterns in brain connectivity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multitask learning on brain-environment interactions (BEIs) provides supervision that better aligns with downstream clinical outcomes than masked reconstruction of raw BOLD signals.
- **Mechanism:** Instead of learning to reconstruct masked input (which may capture noise), LCM learns to predict phenotypic variables (age, sex, cognitive state, disease labels) from functional connectomes. The cross-attention between connectome features and tokenized BEIs enables the model to learn brain-to-outcome mappings directly. The meta-matching phenomenon—where phenotypes share correlated features in connectome space—allows pretrained BEI tokens to transfer to unseen tasks.
- **Core assumption:** Phenotypic and environmental variables encode meaningful brain-environment relationships that generalize across tasks.
- **Evidence anchors:**
  - [abstract] "LCM pretrains on functional connectomes using phenotypic and environmental variables as supervision, enabling better alignment with downstream clinical tasks"
  - [section 1, page 1] "brain connectomes share similar features across arbitrary phenotypic traits... multitask learning for phenotypic prediction can hence act as the foundation objective"
  - [corpus] Weak/no direct corpus evidence for BEI multitask supervision mechanism
- **Break condition:** If phenotypic labels in pretraining data are noisy, sparse, or systematically biased, supervision signal degrades. If downstream tasks have fundamentally different brain-behavior mappings than pretrained BEIs, transfer fails.

### Mechanism 2
- **Claim:** Decoder-only transformer architecture with cross-attention scales more effectively than MLP-based predictive heads for brain connectome learning.
- **Mechanism:** The model represents BEIs as learnable token embeddings that are updated through self-attention (communicating across BEIs) and cross-attention (conditioning on connectome features). This allows information sharing across tasks during pretraining. Unlike MLPs that suffer training loss explosion at scale (Fig. 2), the decoder architecture maintains stable optimization as parameters increase.
- **Core assumption:** BEI tokens can capture task-specific representations that benefit from inter-task communication through self-attention.
- **Evidence anchors:**
  - [abstract] "decoder-only architecture with cross-attention between brain connectomes and tokenized brain-environment interactions"
  - [section 3, page 3] Architecture equations (2-3) define self-attention on V and cross-attention between V and M
  - [corpus] Weak/no corpus evidence comparing decoder vs. MLP scalability in connectome models
- **Break condition:** If tasks are fundamentally incompatible (negative transfer), self-attention between BEI tokens harms rather than helps. Excessive depth without proper initialization causes optimization difficulties.

### Mechanism 3
- **Claim:** Adaptive layer selection allows different BEIs to be predicted at optimal depths, accommodating varying feature complexity across phenotypes.
- **Mechanism:** Two-stage training: (1) momentum stage averages predictions across all layers for initial direction; (2) adaptive stage selects the best-performing layer per BEI. During inference, each BEI uses its empirically determined optimal layer. This accounts for diseases like Alzheimer's (separable at shallow layers) vs. Autism (requiring multi-level features).
- **Core assumption:** Different phenotypes require different levels of feature abstraction; no single layer is optimal for all tasks.
- **Evidence anchors:**
  - [section 3, page 4] "we propose that LCM predicts each BEI at different layers of the model"
  - [figure 7, page 7] Shows distribution of best-matched readout layers varies by disease
  - [table 5, page 8] Ablation shows two-stage strategy outperforms single-stage variants
  - [corpus] Weak/no corpus evidence for adaptive layer selection in brain models
- **Break condition:** If insufficient training data prevents reliable layer selection, or if optimal layers shift between training and test distributions.

## Foundational Learning

- **Functional Connectivity (FC)**
  - **Why needed here:** LCM operates on FC matrices (Pearson correlations between brain regions), not raw BOLD signals. Understanding that FC summarizes temporal dependencies into a spatial graph is essential.
  - **Quick check question:** Can you explain why FC has higher SNR than raw BOLD, and what the matrix dimensions represent?

- **Transformer Decoder Architecture**
  - **Why needed here:** The paper assumes familiarity with self-attention, cross-attention, and decoder-only designs. The critical distinction from encoders is causal masking and the role of learnable query tokens.
  - **Quick check question:** How does cross-attention differ from self-attention, and where does the conditioning signal (FC matrix) enter in LCM?

- **Multitask Learning & Transfer**
  - **Why needed here:** LCM's core premise is that pretrained BEI tokens transfer to unseen tasks. Understanding positive/negative transfer, task relatedness, and representation sharing is essential.
  - **Quick check question:** What conditions cause multitask learning to help vs. hurt individual task performance?

- **Semi-Supervised Finetuning with Pseudo-Labels**
  - **Why needed here:** For downstream tasks without labels, LCM assigns pseudo-labels (e.g., "resting-state") to pretrained tokens and concatenates new task tokens. Understanding this mechanism is critical for real-world deployment.
  - **Quick check question:** How does LCM handle a new dataset with a previously unseen BEI category?

## Architecture Onboarding

- **Component map:**
  Input: FC matrix M ∈ R^{N×N} (N = # brain regions, typically 116 via AAL atlas)
  BEI Tokens: V ∈ R^{P×E} (P = total classes across all BEIs, E = embedding dim = 2048)

  Per Layer:
    1. Self-Attention on V (Eq. 2): BEI tokens communicate
    2. Cross-Attention V←M (Eq. 3): Condition tokens on connectome
    3. FFN: Nonlinear transformation

  Output: Linear layer per BEI segment → predictions
  Stacking: L layers (8/20/32 for Small/Mid/Big)

- **Critical path:**
  1. **Preprocessing:** fMRI → parcellation → regional time series → FC matrix (Pearson correlation)
  2. **Tokenization:** Each BEI class gets one learnable token; categorical BEIs get N_class tokens each
  3. **Forward pass:** V updated through L decoder layers via self-attention + cross-attention with M
  4. **Layer selection:** Store outputs from all layers; compute loss at each; select best during adaptive training
  5. **Finetuning:** Concatenate new task tokens to V; assign pseudo-labels to pretrained tokens; train with same two-stage strategy

- **Design tradeoffs:**
  - **Depth vs. efficiency:** Deeper models (32 layers, 1.2B params) improve performance but require more GPU memory (Table 3 shows consistent gains)
  - **Token granularity:** More BEI classes increase P (token count), raising memory cost; paper uses categorical BEIs with P = sum of class counts
  - **Two-stage training:** Adds m epochs of overhead (momentum stage) but is critical for stability (ablation in Table 5)
  - **Decoder-only vs. encoder-decoder:** Simpler architecture but limits bidirectional context in FC encoding

- **Failure signatures:**
  - **Exploding loss at scale:** If using MLP head instead of decoder (Fig. 2 orange)—switch to decoder architecture
  - **Poor transfer to unseen tasks:** If pretraining BEIs are too narrow—expand diversity of phenotypic labels
  - **Layer selection instability:** If momentum stage (stage 1) is skipped—always use two-stage training
  - **Attention not meaningful:** Check cross-attention visualization (Fig. 6); if random-looking, may need longer pretraining or more diverse BEIs

- **First 3 experiments:**
  1. **Reproduce scalability curve:** Train LCM-Small/Mid/Big from scratch on HCPA, plot training loss vs. parameters; compare with MLP baseline (replicate Fig. 2)
  2. **Ablation on layer selection:** Finetune pretrained LCM on ABIDE with three variants: (a) last-layer only, (b) stage 1 only, (c) stage 2 only, (d) full two-stage; compare F1 scores (replicate Table 5 pattern)
  3. **Cross-attention visualization:** Extract and plot average cross-attention maps at readout layer for ADNI/ABIDE/PPMI; verify attention to known disease-relevant networks (Fig. 6)—default mode for Alzheimer's, limbic for Autism, sensorimotor for Parkinson's

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the mechanistic explanation for the performance degradation on large-scale datasets (HCPA/HCPYA) after multitask pretraining with smaller disease datasets, and how can this negative transfer be mitigated?
- **Basis in paper:** [explicit] The paper explicitly states in the "Model Scalability" section: "LCM drops on HCPA and HCPYA after pretraining because their sample size is 10 times the rest. However, the disease-related datasets that have small sample sizes consistently benefited from big data."
- **Why unresolved:** The paper identifies the phenomenon—pretraining helps small datasets but hurts performance on the massive source datasets—but does not provide a theoretical or empirical justification for why adding more data/tasks reduces performance on the majority class, nor does it propose a solution to prevent this loss.
- **What evidence would resolve it:** Ablation studies analyzing the gradient interference or feature space distortion between the large healthy cohort and the small disease cohorts, or the demonstration of a regularization technique that preserves performance on the large datasets while still adapting to the small ones.

### Open Question 2
- **Question:** How can the Large Connectome Model be extended to incorporate temporal dynamics (e.g., dynamic functional connectivity) without sacrificing the signal-to-noise ratio (SNR) benefits of static connectomes?
- **Basis in paper:** [inferred] The paper explicitly chooses static Functional Connectivity (FC) as the input modality, citing the low SNR of raw BOLD signals (Preliminaries). However, by collapsing the time series into a correlation matrix, the model discards the temporal trajectories of brain states.
- **Why unresolved:** While the paper motivates the use of static FC for robustness, it leaves unexplored whether the "Brain-Environment Interaction" pretraining objective could also successfully supervise a model that processes dynamic or time-resolved features, potentially capturing transient cognitive states missed by static FC.
- **What evidence would resolve it:** A comparative study where LCM is trained on sliding-window dynamic FC matrices or raw time-series (with the proposed BEI supervision) against the static FC version, specifically evaluating tasks requiring high temporal precision.

### Open Question 3
- **Question:** Can the optimal readout layer for a specific phenotypic task be predicted theoretically based on the complexity of the behavior, rather than empirically selected via adaptive training?
- **Basis in paper:** [inferred] The paper introduces an "adaptive training" strategy (Fig. 5) because BEIs differ in feature complexity. Fig. 7 shows a "diverse distribution" of optimal layers for different diseases (e.g., Alzheimer's uses shallow layers, Autism uses deeper ones), implying a link between the trait and the required depth.
- **Why unresolved:** The current method relies on a two-stage training process to empirically "find the best prediction" layer. The paper does not establish a rule or theoretical framework to determine *a priori* which layer depth corresponds to which type of cognitive or disease complexity.
- **What evidence would resolve it:** Analysis mapping the "semantic complexity" or "neural signal-to-noise ratio" of various phenotypes to their empirically selected layer depths, resulting in a predictive heuristic that eliminates the need for the computationally intensive search across all layers.

## Limitations
- Performance degradation on large-scale datasets after multitask pretraining with smaller disease datasets suggests potential negative transfer
- Reliance on high-quality phenotypic labels for pretraining creates circular dependency on label availability and accuracy
- Two-stage training strategy adds significant computational overhead without clear evidence that simpler alternatives would fail

## Confidence
- **High confidence**: Multitask pretraining improves downstream clinical task performance compared to random initialization; larger model sizes consistently improve results across all datasets; two-stage training strategy outperforms single-stage variants for layer selection.
- **Medium confidence**: Decoder-only transformer architecture is superior to MLP predictive heads at scale; adaptive layer selection meaningfully improves performance by matching task complexity to optimal feature depth; cross-attention visualizations reveal interpretable disease-relevant patterns.
- **Low confidence**: Pretraining on brain-environment interactions provides supervision that better aligns with clinical outcomes than masked reconstruction; pseudo-label assignment protocol enables effective transfer to unseen tasks; self-attention between BEI tokens meaningfully improves multitask learning through inter-task communication.

## Next Checks
1. **Verify pseudo-label assignment protocol**: Implement the semi-supervised finetuning procedure on a held-out dataset with no labels, explicitly documenting how pretrained BEI tokens are assigned pseudo-labels and how new task tokens are initialized. Compare performance with supervised finetuning using 10%, 50%, and 100% labeled data.

2. **Test negative transfer scenarios**: Construct artificial multitask scenarios where BEIs are intentionally incompatible (e.g., opposite phenotypes like "young" vs "old" when predicting age-related diseases). Measure whether the decoder architecture with self-attention between BEI tokens suffers from negative transfer compared to task-specific finetuning.

3. **Evaluate bidirectional context benefits**: Reimplement LCM using an encoder-decoder architecture where the encoder processes FC matrices bidirectionally before cross-attention. Compare performance on all eight downstream tasks to determine whether the decoder-only design meaningfully limits representational capacity for connectome analysis.