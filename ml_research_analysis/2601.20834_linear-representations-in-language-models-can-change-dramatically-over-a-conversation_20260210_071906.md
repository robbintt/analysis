---
ver: rpa2
title: Linear representations in language models can change dramatically over a conversation
arxiv_id: '2601.20834'
source_url: https://arxiv.org/abs/2601.20834
tags:
- representations
- conversation
- questions
- factuality
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores how language model representations of high-level
  concepts, such as factuality and ethics, evolve over the course of a conversation.
  By identifying linear dimensions in model representations that correspond to these
  concepts, the authors analyze how these dimensions change as conversations progress.
---

# Linear representations in language models can change dramatically over a conversation

## Quick Facts
- arXiv ID: 2601.20834
- Source URL: https://arxiv.org/abs/2601.20834
- Authors: Andrew Kyle Lampinen; Yuxuan Li; Eghbal Hosseini; Sangnie Bhardwaj; Murray Shanahan
- Reference count: 39
- Primary result: Linear representations of conversation-relevant concepts like factuality can shift dramatically over time, even flipping from factual to non-factual and vice versa.

## Executive Summary
This work demonstrates that language model representations of high-level concepts such as factuality and ethics evolve dramatically over the course of conversations. By identifying linear dimensions in model representations that correspond to these concepts, the authors show that these representations can flip from one state to another while the model is engaged in conversation, even though representations of generic information remain stable. These shifts occur across different model families and layers, and can be reversed through causal interventions along the representation directions.

The findings challenge interpretability methods that assume consistent meaning of internal representations across contexts, suggesting that static interpretations of features or directions may be misleading. The results point to the need for interpretability methods that account for the dynamic nature of model representations over time, and suggest that representations may reorganize based on the conversational "role" the model is playing rather than reflecting stable beliefs.

## Method Summary
The authors construct balanced yes/no question sets (generic questions that are context-independent and context-relevant questions tied to conversation topics) and extract residual stream activations at each layer while models process "Yes"/"No" tokens. They fit regularized logistic regression probes on 90% train split to predict factuality, including opposite-day-prompted examples for robust directions. The best layer is selected via holdout set performance, and margin scores measuring separation between factual and non-factual answer representations are tracked at different conversation turns to observe representation shifts.

## Key Results
- Representations of conversation-relevant topics can shift dramatically over time, even flipping from factual to non-factual and vice versa
- These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved
- Representation shifts are robust across different model families and layers, and occur even when replaying conversations written by other models
- Causal interventions along these representation directions can have opposite effects at different points in a conversation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representations reorganize based on the conversational "role" the model is playing rather than reflecting stable beliefs.
- Mechanism: The model's internal representation directions shift to align with the behavioral stance implied by the conversation context—similar to how an actor might adopt different perspectives.
- Core assumption: The model treats conversation as implying a role/stance rather than updating a world model.
- Evidence anchors: These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Representations oscillate when models argue both sides of consciousness debate. Related work on persona adaptation shows similar effects.
- Break condition: If representations stopped shifting when the model was told it was being evaluated (critic role), or if on-policy conversations produced fundamentally different shifts than off-policy replays.

### Mechanism 2
- Claim: Representation shifts are content-selective—conversation-relevant concepts flip while generic information remains stable.
- Mechanism: The model maintains a base representation for context-independent knowledge while allowing context-dependent reorganization for topics directly engaged by the conversation.
- Core assumption: The model has learned to distinguish between generic vs. context-specific knowledge domains during training.
- Evidence anchors: These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. Generic factuality margin stays positive; context-relevant margin flips negative.
- Break condition: If generic questions also showed systematic flipping, or if context-relevant questions showed no differential effect.

### Mechanism 3
- Claim: Representation changes emerge from higher-order co-occurrence patterns in the conversation, not requiring the model to actively generate content.
- Mechanism: If statements of a particular type cluster together in conversation, linear representations reorganize to reflect this—off-policy replays work because the statistical structure is preserved.
- Core assumption: The emergence mechanism for linear representations operates dynamically within contexts, not just during training.
- Evidence anchors: Linear representations of high-level concepts like 'truth' can emerge from higher-level co-occurrences. Off-policy conversations produce similar effects to on-policy conversations.
- Break condition: If on-policy conversations showed dramatically stronger effects than off-policy replays, suggesting active generation is required.

## Foundational Learning

- Concept: **Linear Representation Hypothesis**
  - Why needed here: The entire paper assumes concepts like "factuality" correspond to linear directions in activation space that can be extracted via probes.
  - Quick check question: Given activations from an LLM, can you explain how a logistic regression probe identifies a direction corresponding to a concept?

- Concept: **In-Context Learning Dynamics**
  - Why needed here: The paper frames representation changes as a form of in-context learning—understanding how models adapt behaviorally to context is prerequisite.
  - Quick check question: Why would a model that has learned "true statements co-occur with true statements" during pretraining also adapt representations dynamically within a single conversation?

- Concept: **Representation Steering**
  - Why needed here: Appendix B.8 shows steering can have opposite effects at different conversation points—critical for understanding causal intervention failures.
  - Quick check question: If you add a steering vector that biases toward "factual" responses, why might it work in an empty context but fail after a long conversation?

## Architecture Onboarding

- Component map: Construct balanced QA datasets -> Extract activations at answer token position -> Fit regularized logistic regression on generic questions -> Evaluate margin on held-out questions at conversation turns -> Track margin score degradation/flipping

- Critical path:
  1. Construct balanced yes/no question sets (generic + context-relevant)
  2. Extract activations at answer token position across all layers
  3. Fit probe on generic questions in empty context (and opposite-day context for robust probes)
  4. Evaluate probe on held-out questions at each conversation turn
  5. Track margin score degradation/flipping over turns

- Design tradeoffs:
  - Probing at answer token lets you see how model represents both factual/non-factual answers, not just what it would generate
  - Robust probes (including opposite-day) separate factuality from behavioral correctness but still flip in conversations
  - Middle-to-late layers (where factuality becomes decodable) show most consistent effects

- Failure signatures:
  - Probe trained on generic questions misclassifies context-relevant questions after ~3-5 turns
  - Margin score goes negative (factual answers classified as non-factual)
  - Steering interventions have opposite behavioral effects in different conversation contexts
  - CCS unsupervised probing often performs below chance after conversations

- First 3 experiments:
  1. Baseline verification: Fit factuality probe on generic questions in empty context; verify >90% holdout accuracy at optimal layer
  2. Opposite day replication: Run the opposite-day prompt experiment; confirm margin flips within 3 turns (this validates your probe extraction pipeline)
  3. Conversation replay test: Replay a pre-written conversation to the model; probe factuality for both generic and context-relevant questions at each turn to observe selective flipping

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanistic circuits or components responsible for driving the dynamic reorganization of linear representations over a conversation?
- Basis in paper: Finally, we have not established the mechanisms by which these representational changes occur. We hope that future works will explore these issues in more depth.
- Why unresolved: The paper characterizes the phenomenon (representation flipping) but focuses on probing existing states rather than tracing the computational causality that rewires the model's "role" mid-conversation.
- What evidence would resolve it: Mechanistic interpretability studies identifying the specific parameters and tokens responsible for rotating the representation space.

### Open Question 2
- Question: Can interpretability methods like Sparse Autoencoders (SAEs) be modified to handle non-static features that change meaning based on context?
- Basis in paper: The discussion notes that SAEs rely on the "fundamental assumption... that the sparse 'features'... have consistent meanings throughout the sequence." The authors suggest these results "should motivate a search for interpretability methods that can more robustly analyze model representations over context."
- Why unresolved: Standard SAEs treat features as static concepts; if "factuality" flips based on dialogue history, a single static feature vector is insufficient, requiring a new class of dynamic decomposition methods.
- What evidence would resolve it: The development and validation of a context-aware SAE or probing technique that successfully disentangles stable semantic content from transient "role-play" variables.

### Open Question 3
- Question: To what extent do these representation dynamics generalize to other high-level concepts beyond factuality and ethics?
- Basis in paper: We have focused on a relatively narrow subset of concepts for which there are linear representations, but many others could be explored.
- Why unresolved: The experiments primarily target "truth" and "ethics" directions; it remains unclear if this is a universal property of all linear dimensions or specific to social/conceptual features.
- What evidence would resolve it: Applying the same turn-by-turn probing methodology to a diverse set of known linear representation directions across long contexts.

## Limitations

- Representation direction stability remains unclear—even robust probes trained with opposite-day examples still flip in conversations
- The exact causal mechanism cannot be definitively distinguished between the three proposed explanations
- Results depend on carefully constructed yes/no question sets that are not released, making exact reproduction difficult

## Confidence

**High Confidence**: Representation shifts occur in conversation contexts, effects are content-selective, off-policy conversations produce similar effects, steering interventions can have opposite effects

**Medium Confidence**: Representations flip rather than simply degrading, robust probes still flip, effects are consistent across model families and layers

**Low Confidence**: Exact causal mechanism, generalization to non-binary question formats, stability of linear directions across different training regimes

## Next Checks

1. **Mechanism Dissection Experiment**: Design a conversation where the model is explicitly told it's being evaluated (critic role) versus playing a role. Compare representation shifts between conditions to test whether the role-playing mechanism is necessary.

2. **Cross-Domain Generalization**: Test the same methodology with a different concept (e.g., sentiment or topic classification) to determine whether selective representation changes are specific to factuality or represent a more general phenomenon.

3. **Temporal Resolution Analysis**: Track representation changes at finer temporal granularity (every 2-3 tokens rather than per turn) to determine whether shifts are abrupt transitions or gradual drifts, which would inform the underlying mechanism.