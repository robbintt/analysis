---
ver: rpa2
title: 'VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning'
arxiv_id: '2601.15724'
source_url: https://arxiv.org/abs/2601.15724
tags:
- video
- tool
- reasoning
- arxiv
- interval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-form video understanding,
  where existing models struggle with temporal localization and information loss due
  to static reasoning over uniformly sampled frames. The authors propose VideoThinker,
  an agentic VideoLLM trained on synthetic tool-interaction trajectories generated
  by converting videos into captions and using a powerful LLM to produce multi-step
  reasoning sequences.
---

# VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning

## Quick Facts
- arXiv ID: 2601.15724
- Source URL: https://arxiv.org/abs/2601.15724
- Reference count: 40
- Primary result: +6.8% on MLVU and +10.6% on LVBench over vanilla VideoLLMs

## Executive Summary
VideoThinker addresses the challenge of long-form video understanding by introducing an agentic VideoLLM that learns to reason over videos using tool-augmented synthetic training data. The core innovation is converting videos into captions, using a powerful LLM to generate multi-step tool-use trajectories in caption space, then grounding these trajectories back to actual video frames for training. This approach enables the model to learn adaptive temporal exploration strategies without requiring pre-existing long-video understanding capabilities. The system integrates Temporal Retrieval and Temporal Zoom tools to implement a coarse-to-fine search strategy that addresses the sparse-relevant-evidence problem in long videos.

## Method Summary
The method involves three key stages: first, generate video captions using a base VideoLLM; second, synthesize tool-reasoning trajectories by running an agentic LLM over these captions with a tool suite including Clip Retrieval, Subtitle Retrieval, and Zoom tools; third, ground the trajectories back to video by replacing caption outputs with corresponding video frames. The resulting interleaved video-tool reasoning dataset is used to fine-tune the VideoLLM with LoRA adapters. During inference, a confidence-gated mechanism determines whether to answer directly or invoke tool-based reasoning, enabling adaptive computation based on question difficulty.

## Key Results
- Achieves +6.8% accuracy improvement on MLVU benchmark over vanilla VideoLLMs
- Achieves +10.6% accuracy improvement on LVBench benchmark over vanilla VideoLLMs
- Demonstrates effectiveness of tool-augmented synthetic data for long-form video understanding
- Shows confidence-based gating successfully balances efficiency and accuracy

## Why This Works (Mechanism)

### Mechanism 1: Caption-Space Transfer
The approach transfers tool-reasoning patterns from text-based LLMs to VideoLLMs by generating synthetic training data in caption space. A powerful LLM creates multi-step reasoning sequences over video captions, which are then grounded back to actual frames. This works because reasoning patterns learned in caption space transfer effectively to visual frame space.

### Mechanism 2: Confidence-Based Gating
A two-stage inference strategy uses confidence scores to balance efficiency (direct reasoning) with accuracy (tool-based reasoning). The model first produces an answer with confidence score; if below threshold (default 0.7), tool-augmented reasoning is triggered. This adapts computation to difficulty.

### Mechanism 3: Coarse-to-Fine Temporal Search
The Temporal Retrieval + Temporal Zoom tool combination implements a coarse-to-fine search strategy. Retrieval tools identify candidate intervals using semantic similarity, then Zoom tools extract frames from narrowed intervals. This addresses the sparse-relevant-evidence problem in long videos.

## Foundational Learning

- **Concept: Interleaved multimodal reasoning** - Why needed: VideoThinker's training data contains video frames embedded within reasoning chains, not appended after text. Understanding token alternation is essential for debugging tokenization issues. Quick check: Given [text][video token][text], which loss terms receive gradients from the video token position?

- **Concept: Tool-augmented LLM agents** - Why needed: The core innovation is training VideoLLM to call tools rather than using external LLM orchestration. Understanding tool-call tokenization and training is required. Quick check: How does the model learn when to emit a tool call versus a direct answer?

- **Concept: Temporal grounding in videos** - Why needed: Subtitle Retrieval and Frame Zoom tools operate on timestamp intervals. Understanding timestamp representation (text vs special tokens) is critical for reproducing the pipeline. Quick check: If subtitles at [357.11, 357.12], how does the model learn to map text queries to this precise interval?

## Architecture Onboarding

- **Component map**: Video → VideoLLM caption → LLM tool-reasoning → CaptionZoom outputs → Replace with <video> tokens → Training data
- **Critical path**: Generate captions → Run agentic LLM with tool suite → Filter correct trajectories → Replace CaptionZoom with FrameZoom + video frames → Fine-tune VideoLLM → Deploy with confidence-gated inference
- **Design tradeoffs**: Caption-space synthesis vs direct video synthesis (cheaper but may lose nuances); LoRA fine-tuning vs full fine-tuning (preserves base capabilities but may limit tool-reasoning internalization); fixed frame budget vs adaptive (efficiency gains but may miss ultra-fine details)
- **Failure signatures**: Retrieval returns irrelevant intervals → hallucinated answers; CaptionZoom descriptions too generic → weak visual supervision; Confidence threshold miscalibrated → latency blowup or never triggering tools
- **First 3 experiments**: 1) Baseline comparison on LongVideoBench subset with uniform sampling; 2) Ablation on synthesis quality (1 vs 5 trajectories with filtering); 3) Confidence calibration check by plotting accuracy vs confidence bins

## Open Questions the Paper Calls Out

- Would VideoThinker benefit from larger-scale synthetic training data beyond the 10k samples from CG-Bench?
- How does VideoThinker perform on videos without subtitles or audio transcripts?
- Does textual reasoning in caption space accurately transfer when grounded back to video frames?
- Is the confidence threshold τ = 0.7 universally optimal, or does the ideal value vary across video types?

## Limitations

- The approach critically depends on quality of synthetic trajectories generated by agentic LLM, with no specified yield rate or filtering criteria
- Temporal localization precision is limited by 10-second chunk retrieval and 8-frame resampling within Zoom tools
- Confidence-based gating assumes token probability products correlate with correctness, which may not hold for temporally complex outputs

## Confidence

- **High**: Architectural framework combining tool-augmented reasoning with confidence-based gating is technically sound
- **Medium**: Empirical improvements are statistically significant but lack context on absolute performance and variance
- **Low**: Transferability claim from caption-space to visual frame-space lacks direct validation

## Next Checks

1. **Temporal Precision Analysis**: Evaluate VideoThinker's ability to localize events at different granularities (1s, 5s, 10s) on a controlled dataset with known timestamps, measuring precision-recall of temporal localization separately from answer correctness.

2. **Confidence Calibration Testing**: Systematically generate test cases where the model should exhibit low confidence (temporally inconsistent or ambiguous videos) and verify that confidence scores appropriately reflect uncertainty, comparing calibration curves across different video types.

3. **Transferability Validation**: Train VideoThinker using two different synthesis approaches: caption-space reasoning as described, and direct video-space reasoning (if feasible), comparing performance to isolate contribution of caption-to-video transfer versus tool reasoning per se.