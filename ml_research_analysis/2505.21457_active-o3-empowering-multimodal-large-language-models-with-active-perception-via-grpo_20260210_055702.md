---
ver: rpa2
title: 'Active-O3: Empowering Multimodal Large Language Models with Active Perception
  via GRPO'
arxiv_id: '2505.21457'
source_url: https://arxiv.org/abs/2505.21457
tags:
- active
- perception
- object
- reward
- sensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of enabling Multimodal Large\
  \ Language Models (MLLMs) to perform active perception\u2014the ability to actively\
  \ select where and how to look in order to gather task-relevant visual information.\
  \ The authors propose ACTIVE-O3, a reinforcement learning-based training framework\
  \ built on GRPO, which equips MLLMs with a two-stage sensing and task execution\
  \ policy."
---

# Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO

## Quick Facts
- **arXiv ID**: 2505.21457
- **Source URL**: https://arxiv.org/abs/2505.21457
- **Reference count**: 40
- **Primary result**: ACTIVE-O3 achieves +2.7/+3.5 AP/AR on LVISdense and +8.5 APs on SODA-A through reinforcement learning-based active perception

## Executive Summary
This paper introduces ACTIVE-O3, a reinforcement learning framework that enables Multimodal Large Language Models (MLLMs) to perform active perception by learning to select informative visual regions for task execution. Built on GRPO, the method trains a sensing model to output K bounding boxes, which are then processed by a task model to execute detection or segmentation. The dual-form reward design combines heuristic feedback (format, overlap, area, coverage) with task-aware metrics (AP/AR or mIoU). Evaluated across open-world small/dense object grounding, domain-specific detection, and fine-grained interactive segmentation, ACTIVE-O3 significantly outperforms passive baselines like Qwen2.5-VL-CoT, demonstrating the value of learned active perception policies.

## Method Summary
ACTIVE-O3 is a reinforcement learning framework for active perception in MLLMs. It consists of a sensing model MO (Qwen2.5-VL-7B) that outputs K bounding boxes with reasoning traces, and a task model MA that executes detection/segmentation on each crop. The framework uses GRPO with group size 8 and dual-form rewards: heuristic rewards (format validity, non-overlap τ=0.3, area ratio 0.01-0.5, coverage) and task-aware rewards (AP/AR for detection, mIoU for segmentation). Training runs on 8 GPUs with bf16 DeepSpeed ZeRO-3 for ~24h, using AdamW with learning rate 1e-6 and KL regularization β=0.04. The sensing model is optimized to generate informative regions while the task model is kept frozen (or replaced with specialized detectors like Grounding DINO in decoupled variants).

## Key Results
- +2.7/+3.5 AP/AR improvement on LVISdense compared to Qwen2.5-VL-CoT
- +8.5 APs improvement on SODA-A (aerial object detection) over Qwen2.5-VL baseline
- Strong zero-shot reasoning performance on V* benchmark without explicit reasoning data
- Dual-form reward design shows synergy: combined reward (4.4 APs) outperforms task-only (3.6) or heuristic-only (3.0) on LVISsmall

## Why This Works (Mechanism)

### Mechanism 1: Group-Relative Advantage Estimation
GRPO enables stable RL training without a separate critic model by normalizing rewards within sampled groups. For each query, N responses are sampled and advantage An = (rn - mean) / std across the group. This relative scoring identifies better responses without learning a value function. KL regularization (β=0.04) prevents policy drift from the frozen reference model. The core assumption is that response diversity within groups provides sufficient information to distinguish good from bad responses.

### Mechanism 2: Dual-Form Reward Decomposition
Combining heuristic and task-aware rewards balances exploration with exploitation. Heuristic reward Rheuristic = λ1Rformat + λ2Rno-overlap + λ3Rarea + λ4Rcoverage provides dense feedback on region proposal quality. Task-aware reward Rdetect = AP@IoU=0.5 + AR@IoU=0.5 provides sparse but task-aligned feedback. Ablation shows combined reward outperforms individual components (4.4 vs 3.6 vs 3.0 APs on LVISsmall).

### Mechanism 3: Parallelized Sensing as Single-Step Active Perception
Reformulating 2D active perception as parallel region selection improves sample efficiency. In 2D static images, T=1 formulation allows proposing K regions simultaneously, enabling batched inference. Each region crop is processed independently by task model MA. This contrasts with sequential zoom approaches, which are inefficient due to near-exhaustive search.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: Core RL algorithm; must understand how advantage estimation works without a critic
  - Quick check: Given rewards [0.2, 0.5, 0.8] for 3 sampled responses, compute the normalized advantage for the second response.

- **Concept: Active Perception vs. Passive Inference**
  - Why needed: Motivates the two-module (sensing + task) architecture; understanding that MLLMs typically consume fixed views
  - Quick check: Why does the paper argue that GPT-o3's zoom-in strategy is a "special case" but inefficient?

- **Concept: Detection Metrics (AP/AR at IoU thresholds)**
  - Why needed: Task-aware rewards use AP@IoU=0.5 and AR@IoU=0.5; understanding these is critical for interpreting results
  - Quick check: What does it mean when APs improves from 1.2 to 2.2 on LVISsmall?

## Architecture Onboarding

- **Component map**: Global image → resize to shorter side 1024 → sensing prompt IO → MLLM generates N=8 responses → parse K=3 boxes → crop regions → resize to 840×840 → task model MA → predictions → compute rewards → GRPO update

- **Critical path**: 1) Global image resize to 1024 shorter side → 2) Sensing prompt to Qwen2.5-VL-7B → 3) Generate 8 responses with K=3 boxes each → 4) Parse JSON bbox_2d from <answer> tags → 5) Crop and resize regions to 840×840 → 6) Task model processes each crop → 7) Compute combined heuristic and task-aware rewards → 8) Normalize rewards across group → 9) GRPO update

- **Design tradeoffs**: Unified vs. decoupled MA (unified simpler but limited; decoupled achieves 7.0 vs 4.3 APs on LVISdense but adds dependency); K=3 regions (not ablated; higher K increases compute); single-step T=1 (simplifies training but limits refinement)

- **Failure signatures**: Domain gap on SODA-A (aerial) with category confusion (windmills, storage tanks); dense object confusion when objects overlap heavily; reward hacking if coverage dominates without precision

- **First 3 experiments**: 1) Reproduce baseline gap: Qwen2.5-VL-CoT vs Active-O3 on 100 LVISdense images; 2) Ablate reward components: train with only heuristic, only task, and combined rewards on LVISsmall subset; 3) Test decoupled variant: replace MA with Grounding DINO on SODA-D

## Open Questions the Paper Calls Out

### Open Question 1
Can joint optimization of the sensing model MO and task model MA outperform the staged optimization approach used in ACTIVE-O3? The paper assumes a fixed, reasonably strong MA and focuses solely on optimizing MO, leaving joint optimization unexplored.

### Open Question 2
Would incorporating a memory mechanism to store past actions and observations significantly improve sensing decisions in sequential active perception tasks? Current framework only uses immediate observation, limiting temporal reasoning.

### Open Question 3
How does ACTIVE-O3's sensing policy generalize when paired with task models MA that have different architectures or were trained on different data distributions? Only two task models are tested, leaving broader compatibility unknown.

### Open Question 4
Can the action space be extended beyond axis-aligned rectangular regions to include rotated boxes or arbitrary polygons without destabilizing GRPO training? Rotation or non-rectangular regions were not implemented or evaluated.

## Limitations
- Lacks detailed ablations on critical hyperparameters like K (number of regions) and T (time steps)
- Unified model variant shows significant performance gaps compared to decoupled approach without systematic exploration
- Domain generalization remains challenging, with aerial imagery (SODA-A) showing persistent errors due to category confusion

## Confidence

- **High confidence**: GRPO as the training algorithm (well-established method, clearly implemented)
- **Medium confidence**: Dual-form reward design effectiveness (supported by ablation but limited cross-domain validation)
- **Medium confidence**: Active perception formulation (conceptually sound but untested for truly sequential tasks)
- **Low confidence**: Zero-shot reasoning claims (V* benchmark results mentioned but methodology unclear)

## Next Checks

1. Ablate K regions (1, 3, 5, 10) on LVISdense to quantify coverage vs. computational cost trade-offs
2. Test on truly sequential perception tasks (video or requiring multiple fixations) to validate the T=1 formulation's limitations
3. Implement domain adaptation techniques for SODA-A and measure whether reward engineering alone can bridge the aerial-ground imagery gap