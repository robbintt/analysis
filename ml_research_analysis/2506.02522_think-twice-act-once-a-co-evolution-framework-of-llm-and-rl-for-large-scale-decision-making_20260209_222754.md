---
ver: rpa2
title: 'Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale
  Decision Making'
arxiv_id: '2506.02522'
source_url: https://arxiv.org/abs/2506.02522
tags:
- learning
- usage
- llms
- policy
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Agents Co-Evolution (ACE), a framework that
  combines large language models (LLMs) and reinforcement learning (RL) for large-scale
  decision-making in industrial settings like power grid operations. The key innovation
  is a dual-role trajectory refinement mechanism where LLMs act as both Policy Actor
  (refining suboptimal actions via multi-step reasoning) and Value Critic (performing
  temporal credit assignment through trajectory-level reward shaping) during RL training.
---

# Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making

## Quick Facts
- arXiv ID: 2506.02522
- Source URL: https://arxiv.org/abs/2506.02522
- Authors: Xu Wan; Wenyue Xu; Chao Yang; Mingyang Sun
- Reference count: 40
- Key outcome: Achieves up to 145% improvement over expert-guided RL baselines in L2RPN power grid optimization with 60K+ discrete actions, requiring only 287-682 LLM refinements vs. 100K+ samples for baselines

## Executive Summary
ACE (Agents Co-Evolution) is a novel framework that integrates large language models with reinforcement learning to solve large-scale decision-making problems in industrial settings, specifically power grid topology optimization. The key innovation is a dual-role trajectory refinement mechanism where LLMs act as both Policy Actor (refining suboptimal actions via multi-step reasoning) and Value Critic (performing temporal credit assignment through trajectory-level reward shaping) during RL training. This approach enables effective guidance while maintaining real-time performance during deployment, achieving state-of-the-art results across three L2RPN competition environments.

## Method Summary
ACE combines Soft Actor-Critic (SAC) with a dual-role LLM system that refines both actions and rewards during training. The LLM Actor (fLLM) identifies low-reward transitions, generates refined actions through multi-step reasoning, and validates them via environment simulation before storing in a separate LLM buffer. The LLM Critic (gLLM) processes complete episode trajectories, identifies key decision points through counterfactual reasoning, and assigns discretized reward adjustments. A mixed buffer combines RL and LLM-refined transitions with reward-based prioritization, enabling bidirectional improvement between LLM and RL. The framework uses Qwen2-7B or GPT-4 with LoRA fine-tuning every 100 refined samples, requiring only 287-682 LLM refinements to achieve state-of-the-art performance.

## Key Results
- Achieves 145% improvement over expert-guided RL baselines on L2RPN challenges
- Requires only 287-682 LLM refinements vs. 100K+ samples for traditional approaches
- Maintains competitive real-time performance (38.7s test time vs. 46.1s for expert-guided RL)
- Improves survival rate from 39.5% to 77% in WCCI 2020 environment

## Why This Works (Mechanism)

### Mechanism 1
LLMs improve RL sample efficiency in large action spaces through selective action refinement on low-reward transitions. The LLM Actor identifies transitions where reward r < r̄, converts state-action pairs to natural language, generates refined actions via multi-step reasoning, and validates these through environment simulation before storing in a separate LLM buffer. This targets RL's early-stage exploration weakness where Q-function estimates and policy exploration are unreliable in vast action spaces (>60K discrete actions). Core assumption: LLMs possess sufficient domain reasoning to propose better actions than early-stage RL policies; text-based state representations preserve decision-relevant information.

### Mechanism 2
LLMs perform trajectory-level credit assignment that approximates non-parametric TD(λ) with better handling of long-term dependencies. The LLM Critic processes complete episode trajectories, identifies key decision points through counterfactual reasoning, and assigns discretized reward adjustments {-2K, -K, +K, +2K}. This differs from exponential decay assumptions in standard eligibility traces by using semantic reasoning to identify causally important states. Core assumption: LLMs can infer causal relationships between actions and delayed outcomes from trajectory text descriptions; discretized adjustments prevent reward instability.

### Mechanism 3
Reward-weighted experience mixing enables bidirectional improvement between LLM and RL with minimal LLM calls. A mixed buffer Dmix combines RL and LLM-refined transitions with sampling distribution weighted by reward improvement. This buffer serves dual purposes: (1) RL trains with prioritized high-quality experiences, (2) LLM fine-tunes on task-specific successful trajectories via LoRA. The co-evolution is sample-efficient because LLM is only queried 287-682 times vs. 100K+ RL samples. Core assumption: LLM-refined experiences that yield higher rewards than RL actions contain learnable patterns transferable to future refinement; RL policies can generalize from limited high-quality demonstrations.

## Foundational Learning

- **Soft Actor-Critic (SAC) with entropy regularization**: ACE uses SAC as its RL backbone; understanding off-policy learning, Q-function updates, and entropy-exploration trade-offs is essential for debugging why early-stage policies need LLM guidance. *Quick check*: Can you explain why SAC's entropy term helps in large action spaces but still struggles with 60K+ discrete actions?

- **Experience replay and prioritization**: ACE's mixed buffer mechanism builds on standard replay; reward-based importance weights require understanding how sampling distributions affect policy gradient estimates. *Quick check*: What happens to policy learning if high-reward LLM refinements are always sampled but represent a narrow distribution of states?

- **LLM fine-tuning with LoRA (Low-Rank Adaptation)**: ACE performs online LLM fine-tuning every 100 samples; understanding parameter-efficient fine-tuning helps assess memory/compute tradeoffs and potential catastrophic forgetting. *Quick check*: Why might frequent LoRA updates on small batches (200 samples) fail to improve LLM guidance quality?

## Architecture Onboarding

- **Component map**: RL Module (SAC with DRL buffer) -> LLM Actor (fLLM) -> LLM Buffer (DLLM) -> Mixed Buffer (Dmix) -> Training Loop -> Fine-tuning Module (LoRA) -> LLM Critic (gLLM) -> Mixed Buffer (Dmix)

- **Critical path**: 1. RL interacts with environment → stores transitions in DRL; 2. fLLM samples low-reward transitions → generates refined actions → validates via simulation → stores in DLLM; 3. gLLM samples high-impact trajectories → adjusts rewards → updates DLLM; 4. Training loop samples from Dmix with reward-weighted priorities → updates Q-function and policy; 5. Periodic LLM fine-tuning on Dmix

- **Design tradeoffs**: Inference cost vs. guidance quality (GPT-4 requires no SFT but has API costs; Qwen2-7B requires SFT infrastructure but improves over time); query frequency vs. compute overhead (higher frequency improves early convergence but increases training time); bad-case threshold (r=0.3 includes 510 samples for more compute but potential noise; r=-0.3 includes 83 samples for faster but slower convergence)

- **Failure signatures**: Survival rate plateaus at ~77% (indicates LLM SFT is not occurring or ineffective); episode rewards decrease after gLLM activation (K values too large, causing Q-value instability); test time explodes (>1000s) (LLM being used during deployment instead of offline-only); refined rewards consistently below RL actions (LLM lacks domain knowledge; requires better prompts or SFT)

- **First 3 experiments**: 1. Ablate fLLM only: Remove Policy Actor, keep Value Critic. Expected: ~30% reward drop (48.3 vs 69.8 on WCCI 2020), confirming action refinement is primary driver; 2. Vary bad-case threshold: Test r ∈ {-0.3, 0, 0.3} with fixed query interval. Expected: r=0 balances sample count and quality; extremes show 6% degradation; 3. Test generalization to unseen scenarios: Train on 288 scenarios, evaluate on 576. Expected: Performance gap between high/low query frequencies narrows, suggesting reduced activation frequency is viable with diverse training data

## Open Questions the Paper Calls Out

### Open Question 1
Can improved semantic encoding of actions bridge the performance gap for action-based trajectory selection in the LLM Value Critic? The authors observe that action-based trajectory selection yields "significantly limited performance" compared to state or reward-based selection, attributing it to LLMs struggling with "abstract topological changes." It is undetermined whether this failure is inherent to LLMs' inability to reason over topology or simply a failure of the specific action-to-text conversion function used in the experiment. Ablation studies using descriptive action encodings (e.g., natural language descriptions of switches) versus raw indices would resolve this.

### Open Question 2
How robust is the ACE framework in scenarios where the "multi-round reasoning" validation step is computationally infeasible? Section 4.3 describes a "multi-round reasoning" trick that validates LLM proposals via Grid2Op simulation, retrying up to five times if rewards are inferior. The framework's success appears partially dependent on this expensive validation loop. In industrial settings where high-fidelity simulation is slow, this "Think Twice" phase could become a training bottleneck. Evaluating the convergence speed and final performance when the simulation budget for multi-round validation is restricted or removed entirely would resolve this.

### Open Question 3
Does the co-evolution mechanism remain stable in environments with sparse rewards where the RL agent cannot generate high-quality fine-tuning data for the LLM early on? Section 3.3 relies on the RL agent generating "high-quality fine-tuning datasets" via prioritized experience replay to update the LLM. If the RL agent explores poorly in early stages (a known issue in sparse reward settings), the LLM may be fine-tuned on low-quality or noisy data, potentially causing the co-evolution to collapse. Testing ACE on a sparse reward variant of the power grid or a different industrial control task would analyze if the LLM's guidance degrades without initial high-quality RL trajectories.

## Limitations
- Neural network architectures for policy and Q-function are underspecified beyond embedding dimensions
- LLM reward shaping mechanism lacks theoretical grounding for discretized credit assignment
- Selection criteria for "BAD actions" in trajectory processing are vaguely defined
- Claim that LLMs can perform meaningful temporal credit assignment through text-based trajectory analysis remains theoretically unproven

## Confidence

**High Confidence**: The core sample efficiency claims (287-682 LLM refinements vs. 100K+ RL samples) are well-supported by ablation studies and comparison baselines. The survival rate improvements (77% vs 39.5%) are directly measurable and consistent across experiments.

**Medium Confidence**: The dual-role LLM mechanism (Actor and Critic) shows clear performance benefits, but the specific contributions of action refinement versus reward shaping are confounded. The claim that LLMs approximate TD(λ) through trajectory reasoning is plausible but lacks rigorous theoretical justification.

**Low Confidence**: The assertion that LLMs can reliably perform causal credit assignment through natural language processing of trajectories is the weakest claim. The discretized reward adjustments and selection of "key decision points" are described qualitatively but not validated for correctness or consistency.

## Next Checks

1. **Mechanism Isolation Test**: Run ACE with fLLM disabled (only gLLM active) and gLLM disabled (only fLLM active) to quantify the independent contribution of action refinement versus reward shaping. This will determine whether the trajectory-level credit assignment provides meaningful value beyond selective action refinement.

2. **Reward Shaping Stability Analysis**: Systematically vary K values (0.1, 0.2, 0.4, 0.8) and measure Q-value variance, policy convergence speed, and final performance. Include a control where reward shaping is applied randomly to non-causal transitions to test whether observed improvements are due to genuine credit assignment or noise injection.

3. **Generalization Robustness Test**: Train ACE on a subset of scenarios (e.g., 50% of available data) and evaluate on completely unseen grid configurations and fault patterns. Compare performance degradation against traditional RL baselines to validate whether the co-evolution approach truly learns transferable decision-making principles rather than overfitting to specific scenarios.