---
ver: rpa2
title: Empirical Evaluation of Embedding Models in the Context of Text Classification
  in Document Review in Construction Delay Disputes
arxiv_id: '2501.09859'
source_url: https://arxiv.org/abs/2501.09859
tags:
- text
- embedding
- dataset
- delay
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates four text embedding models\u2014Bag-of-Words,\
  \ Sentence Transformers (MiniLM-L6-v2, nli-mpnet-base-v2), and NVIDIA NeMo\u2019\
  s LLM-based embeddings\u2014for classifying construction delay-related text snippets\
  \ in legal document review. Using KNN and Logistic Regression on datasets from two\
  \ real-world construction delay disputes, the experiments show that embedding models\
  \ outperform Bag-of-Words in cross-matter scenarios, capturing semantic similarities\
  \ across datasets."
---

# Empirical Evaluation of Embedding Models in the Context of Text Classification in Document Review in Construction Delay Disputes

## Quick Facts
- **arXiv ID**: 2501.09859
- **Source URL**: https://arxiv.org/abs/2501.09859
- **Reference count**: 6
- **Primary result**: Embedding models outperform Bag-of-Words in cross-matter legal text classification, with NVIDIA embeddings achieving highest accuracy

## Executive Summary
This study evaluates four text embedding models—Bag-of-Words, Sentence Transformers (MiniLM-L6-v2, nli-mpnet-base-v2), and NVIDIA NeMo's LLM-based embeddings—for classifying construction delay-related text snippets in legal document review. Using KNN and Logistic Regression on datasets from two real-world construction delay disputes, the experiments show that embedding models outperform Bag-of-Words in cross-matter scenarios, capturing semantic similarities across datasets. NVIDIA embeddings consistently achieved the best performance across all settings. Logistic Regression performed well with sufficient training data, while Bag-of-Words lagged in cross-matter tests. The study highlights the potential of embedding models to enhance accuracy in legal text classification, though computational costs remain a challenge.

## Method Summary
The research employed a comparative experimental approach using two real-world construction delay dispute datasets. Four embedding models were evaluated: traditional Bag-of-Words, two Sentence Transformer variants (MiniLM-L6-v2 and nli-mpnet-base-v2), and NVIDIA NeMo's LLM-based embeddings. For each embedding model, two classification algorithms were applied: K-Nearest Neighbors and Logistic Regression. The evaluation framework tested both within-matter (same dataset for training and testing) and cross-matter (training on one dispute, testing on another) scenarios. Performance metrics included accuracy, precision, recall, and F1-score, with particular attention to the ability of embeddings to capture semantic similarities across different construction delay disputes.

## Key Results
- Embedding models consistently outperformed Bag-of-Words in cross-matter classification scenarios
- NVIDIA NeMo embeddings achieved the highest accuracy across all experimental settings
- Logistic Regression classifier showed strong performance when provided with sufficient training data
- Cross-matter classification revealed Bag-of-Words limitations in capturing semantic similarities between disputes

## Why This Works (Mechanism)
The superior performance of embedding models stems from their ability to capture semantic relationships and contextual similarities in text, rather than relying solely on exact word matches like Bag-of-Words. Modern embedding architectures like Sentence Transformers and LLM-based models learn distributed representations that encode meaning and context, enabling better generalization across different but semantically related documents. The cross-matter success demonstrates that these embeddings can identify common patterns and concepts across different construction delay disputes, even when specific terminology varies.

## Foundational Learning
- **Semantic embeddings**: Why needed - capture meaning beyond exact word matches; Quick check - measure cosine similarity between semantically related phrases
- **Cross-domain classification**: Why needed - real-world legal review involves documents from multiple similar cases; Quick check - train on one dispute, test on another
- **Embedding dimensionality**: Why needed - higher dimensions can capture more nuance but increase computational cost; Quick check - compare performance across different embedding sizes
- **Legal document structure**: Why needed - understanding typical patterns in construction dispute documents; Quick check - analyze document length and content distribution
- **Classification algorithm selection**: Why needed - different algorithms perform differently with embedding representations; Quick check - compare KNN vs Logistic Regression performance curves

## Architecture Onboarding

**Component Map**: Text data -> Preprocessing -> Embedding Model -> Classification Algorithm -> Performance Metrics

**Critical Path**: Embedding Model → Classification Algorithm → Performance Evaluation (most sensitive to embedding quality)

**Design Tradeoffs**: Embedding models provide superior semantic understanding but at higher computational cost versus Bag-of-Words' simplicity and speed

**Failure Signatures**: Poor cross-matter performance indicates insufficient semantic generalization; low accuracy with abundant data suggests embedding model limitations

**First 3 Experiments**:
1. Train Bag-of-Words classifier within the same dispute dataset to establish baseline performance
2. Apply cross-matter classification using embedding models to test semantic generalization
3. Compare computational requirements (time/memory) between embedding models and Bag-of-Words

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability due to focus on only two construction delay disputes
- KNN and Logistic Regression may not fully exploit embedding model capabilities compared to more advanced architectures
- Computational cost analysis lacks quantification, making practical trade-off assessment difficult
- Potential embedding biases and fairness implications in legal applications not addressed

## Confidence

**High confidence**: Superior performance of embedding models over Bag-of-Words in cross-matter scenarios; NVIDIA embeddings consistently achieving best results

**Medium confidence**: Effectiveness of Logistic Regression with sufficient training data; computational cost challenges

**Low confidence**: Generalizability to other legal domains; fairness and bias implications

## Next Checks

1. Test the same embedding models across multiple legal domains (contracts, patents, criminal cases) to assess generalizability
2. Quantify the computational costs (time, memory, hardware requirements) for each embedding model to enable informed trade-off decisions
3. Evaluate potential biases in embeddings by testing classification performance across different demographic indicators or case types to ensure fairness in legal applications