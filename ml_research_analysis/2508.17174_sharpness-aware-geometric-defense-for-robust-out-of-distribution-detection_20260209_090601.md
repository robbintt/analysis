---
ver: rpa2
title: Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection
arxiv_id: '2508.17174'
source_url: https://arxiv.org/abs/2508.17174
tags:
- adversarial
- detection
- training
- fpr95
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles robust out-of-distribution (OOD) detection under
  adversarial attacks, a critical issue for safe model deployment. The authors identify
  that adversarial training creates sharp loss landscapes that degrade OOD detection
  accuracy by impairing latent embedding quality.
---

# Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection

## Quick Facts
- arXiv ID: 2508.17174
- Source URL: https://arxiv.org/abs/2508.17174
- Authors: Jeng-Lin Li; Ming-Ching Chang; Wei-Chao Chen
- Reference count: 40
- Primary result: SaGD achieves 17.71% average FPR95 reduction and 10.18% AUC improvement on CIFAR-100 under adversarial attacks

## Executive Summary
This paper addresses the critical problem of robust out-of-distribution (OOD) detection under adversarial attacks. The authors identify that adversarial training creates sharp loss landscapes that impair latent embedding quality and degrade OOD detection accuracy. They propose the Sharpness-aware Geometric Defense (SaGD) framework combining Multi-Geometry Projection (MGP) backbone with Riemannian Sharpness-aware Minimization (RSAM). MGP jointly learns hypersphere and hyperbolic embeddings to better characterize ID data variability, while RSAM smooths the adversarial loss landscape during training. Using Jitter-based adversarial training, the method achieves significant improvements over state-of-the-art approaches on CIFAR-10 and CIFAR-100 benchmarks.

## Method Summary
SaGD is a three-component framework for robust OOD detection. The Multi-Geometry Projection (MGP) backbone learns dual embeddings: hypersphere embeddings with compactness and disparity losses, and hyperbolic embeddings with Poincaré ball geometry and contrastive loss. Jitter-based adversarial training generates perturbations using an adaptive scaling rule that maintains attack success rate while minimizing perturbation magnitude. Riemannian Sharpness-aware Minimization (RSAM) finds flat minima in the tangent space of Riemannian manifolds by computing optimal perturbations and applying retraction operations. The framework uses ResNet-18/34 backbones, L2-normalized embeddings, and KNN-based scoring for OOD detection.

## Key Results
- SaGD achieves 17.71% average FPR95 reduction and 10.18% AUC improvement on CIFAR-100 under adversarial attacks
- Outperforms state-of-the-art defenses by significant margins: 27.68% vs 36.61% FPR95 on CIFAR-100 (average across attacks)
- Demonstrates strong generalization across attack types, maintaining consistent performance on CIFAR-100 (FPR95: 22.46-37.19%)
- Ablation studies show RSAM optimization improves FPR95 from 32.35% to 27.68% when combined with Jitter training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial training creates sharp loss landscapes that impair latent embedding quality, degrading OOD detection accuracy.
- **Mechanism:** Adversarial samples introduce gradient perturbations Δg_W that interact with Hessian curvature H_z, causing non-convergent updates when Hz > 0 for unconfident samples. The paper derives that the expected gradient condition g^T_x Δg̃_x < 0 may fail, leading to scattered gradients and increased loss curvature.
- **Core assumption:** The relationship between Hessian eigenvalues and adversarial gradient direction determines convergence quality (Assumption: holds primarily for ReLU networks with large margin differences).
- **Evidence anchors:** [Section III-C] Equations 5-9 formally characterize the Hessian-gradient interaction and convergence failure conditions; [Page 5] "The phenomenon leads to difficulties in model convergence during adversarial training"; [Corpus] HALO (2502.19755) confirms OOD detection vulnerability to adversarial attacks.
- **Break condition:** If the model architecture uses skip connections that smooth gradients differently, or if adversarial perturbation magnitude ε is small relative to the Hessian eigenvalue scale, the sharp loss effect may be negligible.

### Mechanism 2
- **Claim:** Jointly learning hypersphere and hyperbolic embeddings improves ID data characterization under adversarial conditions compared to single-geometry approaches.
- **Mechanism:** Hypersphere geometry (positive curvature) enforces intra-class compactness via vMF distribution and inter-class disparity via prototype separation. Hyperbolic geometry (negative curvature) captures hierarchical structures through Poincaré ball embeddings with geodesic distances. The combination covers complementary data variability patterns.
- **Core assumption:** ID data exhibits both angular clustering (suitable for hypersphere) and hierarchical/latent tree structure (suitable for hyperbolic space).
- **Evidence anchors:** [Section III-B] Defines L_com, L_dis for hypersphere; geodesic distance D(u,v) for hyperbolic space; [Table III] MGP-Maha achieves 47.41% average FPR95 vs. CIDER-Maha's 55.52% (adversarial conditions); [Corpus] No direct corpus validation for multi-geometry in OOD; related work focuses on single geometry.
- **Break condition:** If ID data lacks hierarchical structure (e.g., random embeddings), the hyperbolic branch adds noise without benefit. The curvature parameter c=0.01 requires tuning per dataset.

### Mechanism 3
- **Claim:** Riemannian Sharpness-aware Minimization (RSAM) smooths the adversarial loss landscape by finding flat minima in the tangent space of Riemannian manifolds.
- **Mechanism:** RSAM computes optimal perturbation δ* = ρ·∇_θL(θ)/||∇_θL(θ)||_θ in the tangent space, applies retraction R_θ to project onto manifold, then updates parameters using the perturbed loss gradient. This finds parameters where loss is uniformly low in a neighborhood.
- **Core assumption:** Flat minima on Riemannian manifolds generalize better to adversarial perturbations than sharp minima (Assumption: inherited from Euclidean SAM, extended to manifolds).
- **Evidence anchors:** [Section III-C] "LS = max_{||δ||_θ≤ρ} L(R_θ(δ)) - L(θ)" defines manifold sharpness; [Table III] Removing RSAM from SaGD (MGP+Jitter) increases FPR95 from 27.68% to 32.35%; [Corpus] LSAM (2509.03110) validates landscape-smoothed SAM improves distributed training; S2AP (2510.18381) shows sharpness minimization aids adversarial robustness.
- **Break condition:** If the retraction map R_θ is poorly approximated (e.g., using first-order Taylor when curvature is high), RSAM may fail to find meaningful flat regions. Computational cost doubles due to two forward passes per step.

### Mechanism 4
- **Claim:** Jitter-based adversarial training generalizes better to unseen attacks than PGD or FGSM-based training.
- **Mechanism:** Jitter attack rescales softmax as ĥ = softmax(α·h/||h||_∞) with L2 loss plus Gaussian noise N(0,σ). An adaptive rule downscale perturbation by factor β upon attack success, preventing over-optimized adversarial samples far from ID characteristics.
- **Core assumption:** Maintaining fixed attack success rate with minimized perturbation norm produces diverse adversarial samples that better represent the threat space.
- **Evidence anchors:** [Section III-A] Equation 1 defines L_Jitter with adaptive β scaling; [Table III, lower] PGD-trained model achieves 20.25% FPR95 against FGSM but 86.64% against clean data; Jitter-trained SaGD maintains 22.46-37.19% across all attack types; [Corpus] No corpus validation for Jitter specifically in OOD context.
- **Break condition:** If the attack space during deployment differs fundamentally from the Jitter perturbation distribution (e.g., semantic adversarial examples), generalization may fail.

## Foundational Learning

- **Riemannian Manifolds and Retraction**
  - Why needed here: MGP operates on hypersphere (S^d) and hyperbolic space (Poincaré ball), requiring tangent space operations and exponential/log maps for gradient computation.
  - Quick check question: Can you explain why standard Euclidean gradient descent fails on the Poincaré ball and how retraction R_θ addresses this?

- **Sharpness-Aware Minimization (SAM)**
  - Why needed here: RSAM extends SAM to Riemannian manifolds; understanding the flat minima hypothesis is essential to interpret why this improves adversarial robustness.
  - Quick check question: Why does SAM require two forward-backward passes per training step, and what does ρ control?

- **Out-of-Distribution Scoring Functions**
  - Why needed here: The paper uses KNN distance S(z) = ||z - z_k||² as the OOD score; understanding Mahalanobis distance alternatives helps interpret Table I/II results.
  - Quick check question: Why might KNN distance outperform Mahalanobis distance under adversarial conditions?

## Architecture Onboarding

- **Component map:**
  Input x → ResNet backbone → [Hypersphere branch (compactness+disparity loss)]
                                  ↓
                            [Hyperbolic branch (contrastive loss)]
                                  ↓
                            Joint loss: L_sph + L_hypb + L_ce
                                  ↓
                            RSAM optimizer (perturb → retract → update)

- **Critical path:**
  1. Feature clipping before hyperbolic projection: x' = min{1, r/||x||} · x prevents gradient vanishing near Poincaré ball boundary
  2. Dual-branch gradients must be combined correctly; Riemannian gradients differ from Euclidean
  3. Jitter perturbation generation during training; batch size 512 balances memory and gradient stability

- **Design tradeoffs:**
  - Curvature c=0.01 for hyperbolic space: smaller c expands effective capacity but increases numerical instability
  - Intermediate dimension 128: higher dimensions improve separability but increase overfitting risk
  - RSAM perturbation radius ρ: larger ρ flattens more aggressively but may slow convergence
  - KNN k-value for scoring: small k captures local structure but is sensitive to noise

- **Failure signatures:**
  - Embedding collapse: hypersphere compactness loss drives all samples to identical points (check via inter-prototype distances)
  - Hyperbolic overflow: embeddings approach Poincaré ball boundary causing numerical errors (monitor ||u|| < 1/c)
  - RSAM non-convergence: perturbation δ* consistently large indicates sharp local minima (track ||∇L|| over training)
  - Attack-specific overfitting: excellent performance on trained attack type, poor on others (Table III lower rows demonstrate this)

- **First 3 experiments:**
  1. **Baseline sanity check:** Train MGP without adversarial training or RSAM on CIFAR-10; verify FPR95 ≈ 65-70% average under attacks (Table I, MGP-Maha row). This confirms backbone implementation.
  2. **Ablation: RSAM contribution:** Compare MGP+Jitter vs. MGP+Jitter+RSAM. Expected: FPR95 drops from ~32% to ~28% (Table III). This isolates RSAM's effect from Jitter.
  3. **Attack generalization test:** Train with PGD adversarial samples, evaluate on FGSM/FAB/CW. Expected: FPR95 varies wildly (20-95% in Table III). This demonstrates why Jitter is preferred for generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the specific loss convergence conditions required during adversarial geometry learning to ensure robust OOD detection?
- **Basis in paper:** [explicit] The conclusion states, "Future work includes the exploration of loss convergence conditions during adversarial geometry learning."
- **Why unresolved:** The paper identifies that adversarial samples increase gradient norms and create sharp loss landscapes, hindering convergence. While it proposes RSAM to mitigate this, it does not theoretically define the specific conditions under which the multi-geometry manifold converges optimally in an adversarial setting.
- **What evidence would resolve it:** A theoretical analysis or derivation of convergence bounds for the Multi-Geometry Projection (MGP) network when trained with Riemannian Sharpness-aware Minimization (RSAM).

### Open Question 2
- **Question:** How can the generalization of OOD detection capabilities be systematically improved against a wider variety of adversarial conditions?
- **Basis in paper:** [explicit] The conclusion lists "improving the generalization of OOD detection capability under various adversarial conditions" as a primary direction for future work.
- **Why unresolved:** The authors demonstrate that Jitter-based training generalizes well against unseen attacks like PGD and CW, whereas training directly on PGD fails to generalize. The underlying mechanism for why Jitter provides superior generalization in this geometric context remains empirical rather than theoretically fully mapped.
- **What evidence would resolve it:** A comparative study identifying specific perturbation properties (e.g., spectral characteristics) that correlate with cross-attack generalization in geometric embeddings, or a new training objective that matches Jitter's performance without relying on a specific attack heuristic.

### Open Question 3
- **Question:** What is the fundamental theoretical relationship between geometric loss optimization (specifically flatness of the loss landscape) and the accuracy of robust OOD detection?
- **Basis in paper:** [explicit] The conclusion anticipates future research that "investigate[s] an in-depth understanding of the relation between geometric loss optimization and robust OOD detection."
- **Why unresolved:** The paper empirically shows that smoothing the loss landscape with RSAM improves detection, but it does not formalize the link between the flatness of the Riemannian manifold and the separability of ID/OOD data under distribution shifts.
- **What evidence would resolve it:** A theoretical framework proving that minimizing the sharpness of the adversarial loss on the hypersphere and hyperbolic spaces directly tightens the bounds of OOD detection error.

### Open Question 4
- **Question:** Does the SaGD framework scale effectively to high-resolution datasets (e.g., ImageNet) and more complex backbone architectures?
- **Basis in paper:** [inferred] The paper validates the method strictly on CIFAR-10 and CIFAR-100 using ResNet backbones. The introduction mentions "underwater footage" and "urban scenes," implying a need for real-world application, yet the experiments are limited to low-resolution benchmarks.
- **Why unresolved:** The RSAM optimization requires computing Riemannian gradients and perturbations within the tangent space, which introduces computational overhead. It is unstated if this overhead or the multi-geometry projection is stable and efficient for the deeper networks required for high-resolution inputs.
- **What evidence would resolve it:** Experimental results applying SaGD to the ImageNet dataset, reporting both detection metrics (FPR95/AUC) and computational training cost/throughput compared to standard adversarial training baselines.

## Limitations

- Lack of ablation studies separating Jitter adversarial training from RSAM optimization, making it difficult to quantify individual contributions to performance gains
- RSAM implementation details (perturbation radius ρ, retraction mapping specifics) are referenced externally without full specification
- Limited generalizability testing beyond L∞ perturbations, with no validation against semantic adversarial examples
- Computational overhead of RSAM (two forward passes per step) and multi-geometry projection not quantified for larger-scale applications

## Confidence

- Mechanism 1 (Adversarial training creates sharp landscapes): High - well-supported by mathematical derivation and established literature
- Mechanism 2 (Multi-geometry improves characterization): Medium - empirical results strong but lacks theoretical grounding and corpus validation
- Mechanism 3 (RSAM smooths loss landscape): Medium - theoretical basis sound but implementation details unclear
- Mechanism 4 (Jitter generalizes better): Low - empirical evidence present but no comparative ablation with other generalization techniques

## Next Checks

1. Conduct ablation study: Train MGP with only Jitter adversarial training (no RSAM) and with only RSAM (no adversarial training) to quantify individual contributions to the 17.71% FPR95 reduction
2. Test Jitter generalization to semantic adversarial examples (beyond L∞ perturbations) to validate the claim of broad attack robustness
3. Perform sensitivity analysis on the curvature parameter c in hyperbolic space and the KNN k-value to establish robustness to hyperparameter choice