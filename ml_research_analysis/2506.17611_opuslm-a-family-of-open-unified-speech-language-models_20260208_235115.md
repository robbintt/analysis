---
ver: rpa2
title: 'OpusLM: A Family of Open Unified Speech Language Models'
arxiv_id: '2506.17611'
source_url: https://arxiv.org/abs/2506.17611
tags:
- speech
- text
- arxiv
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents OpusLM, a family of open foundational speech
  language models (SpeechLMs) up to 7B parameters. Initialized from decoder-only text
  language models, OpusLMs are continuously pre-trained on 213K hours of speech-text
  pairs and 292B text-only tokens.
---

# OpusLM: A Family of Open Unified Speech Language Models

## Quick Facts
- arXiv ID: 2506.17611
- Source URL: https://arxiv.org/abs/2506.17611
- Reference count: 0
- Key outcome: OpusLM achieves competitive speech recognition (2.3% WER on LibriSpeech Test-Clean), speech synthesis (4.0% TTS WER), and text-only tasks (MMLU 59.0) with fully open code, data, and checkpoints.

## Executive Summary
OpusLM presents a family of open foundational speech language models up to 7B parameters that achieve unified modeling across speech recognition, synthesis, and text-only tasks. The models are initialized from pre-trained decoder-only text language models and continuously pre-trained on 213K hours of speech-text pairs and 292B text tokens. Using a multi-stream architecture with semantic and acoustic tokens, OpusLMs demonstrate that speech and text can be jointly modeled effectively, achieving state-of-the-art performance for open SpeechLM models while maintaining full transparency with released training code, data, checkpoints, and logs.

## Method Summary
OpusLM uses decoder-only Transformers initialized from pre-trained text language models (SmolLM2 or OLMo-2) and extends them with a delay interleave architecture for multi-stream speech-text sequences. The models use 50Hz audio tokenization with 1 semantic token and 8 acoustic tokens per frame, combined with text tokens and N-1 padding tokens for text frames. Pre-training follows a warmup-decay schedule (500k steps, 1M frame batch) with task-specific sequence mixing, followed by annealing on high-quality curated data. The approach demonstrates effective unified modeling while maintaining causal autoregression and inference efficiency.

## Key Results
- Speech recognition: 2.3% WER on LibriSpeech Test-Clean (outperforms prior open SpeechLMs)
- Speech synthesis: 4.0% TTS WER, demonstrating strong generation capabilities
- Text-only tasks: 59.0 MMLU score on 7B model, preserving text capabilities while learning speech
- Model scaling: Clear performance improvements from 135M to 7B parameters, with 1.7B+ required for viable multi-task performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The delay interleave architecture enables unified modeling of multi-stream speech-text sequences while maintaining causal autoregression and inference efficiency.
- **Mechanism**: Each token stream n is delayed by (n-1) frames before embedding summation, transforming a 2D sequence X ∈ Z^(T×N) into a flattened representation. This preserves both temporal autoregression along the T-axis and intra-frame dependencies (predicting x_t,n depends on x_t,<n). The architecture maintains O(T) inference complexity independent of N.
- **Core assumption**: Speech tokens within the same frame have strong sequential dependencies that must be preserved during parallel prediction.
- **Evidence anchors**:
  - [section 2.2]: "it preserves intra-frame auto-regression... for the frame x_t, the prediction of x_t,n depends on all x_t,<n when predicting X̂ frame-by-frame along T-axis"
  - [section 2.2]: "inference time complexity of O(T), which is independent of the number of tokens N, and then can tolerate a larger N"
  - [corpus]: Limited direct evidence; ESPnet-SpeechLM toolkit paper mentions sequential modeling standardization but not delay interleave specifically
- **Break condition**: If intra-frame dependencies are weak for the target speech domain, or if delay offsets don't match actual causal structure of acoustic features.

### Mechanism 2
- **Claim**: Combining semantic tokens (1 per frame) with acoustic tokens (N-1 per frame) provides complementary capabilities for both understanding and generation tasks.
- **Mechanism**: Semantic tokens capture linguistic content favorable for ASR; acoustic tokens capture speaker characteristics and prosody favorable for TTS. Loss reweighting (text:semantic:acoustic = 1:1/2:1/(N-1)) balances their relative importance, treating one text token as equivalent to one complete speech frame.
- **Core assumption**: Semantic tokens excel at content understanding while acoustic tokens are necessary for high-fidelity speech generation; neither alone suffices for both tasks.
- **Evidence anchors**:
  - [section 2.1]: "semantic tokens and acoustic tokens are strong in speech understanding and generation, respectively"
  - [Table 1]: OpusLM-1.7B achieves ASR WER 2.5% and TTS WER 4.0%, outperforming single-task baselines
  - [corpus]: Weak corpus evidence on this specific dual-token strategy in SpeechLMs
- **Break condition**: If semantic tokens lack sufficient linguistic information or if acoustic tokens cannot faithfully reconstruct speech waveforms.

### Mechanism 3
- **Claim**: Annealing with carefully curated high-quality data provides measurable performance gains but is highly sensitive to data composition.
- **Mechanism**: After standard warmup-decay pre-training, linearly decay learning rate to near-zero over 85k updates on smaller, higher-quality datasets. Annealing improves general performance rather than just domain adaptation, but different data compositions yield substantially different results.
- **Core assumption**: Final training phase with decaying learning rate stabilizes and refines learned representations on high-quality examples.
- **Evidence anchors**:
  - [section 2.3]: "annealing is also effective in SpeechLM training... we find the ultimate model performance is sensitive to the annealing data selection"
  - [Table 3]: Opt-A achieves out-domain TTS WER 15.5% vs Opt-B 12.7%; both outperform baseline 21.7%
  - [corpus]: No direct corpus evidence on annealing strategies in SpeechLMs
- **Break condition**: If annealing data is poorly matched to target capabilities or learning rate schedule is misconfigured.

## Foundational Learning

- **Concept: Decoder-only Transformer with Causal Masking**
  - Why needed here: OpusLM inherits from pre-trained text LLMs (SmolLM2, OLMo-2) and extends them to multi-modal sequences while preserving autoregressive generation.
  - Quick check question: Why does causal masking prevent information leakage during training, and how does it enable autoregressive generation at inference?

- **Concept: Neural Audio Codecs and Discrete Speech Tokens**
  - Why needed here: OpusLM uses discrete tokens (semantic + acoustic) at 50Hz rather than continuous features; understanding token properties is essential for debugging.
  - Quick check question: What information does a semantic tokenizer (e.g., from self-supervised speech models) preserve versus an acoustic tokenizer (e.g., neural codec)?

- **Concept: Learning Rate Schedules (Warmup, Decay, Annealing)**
  - Why needed here: Paper uses warmup-decay for pre-training followed by annealing phase; incorrect scheduling will fail to reproduce results.
  - Quick check question: What is the purpose of learning rate warmup at training start, and why might annealing (decaying to zero at end) improve final performance?

## Architecture Onboarding

- **Component map**:
Speech Input → Audio Tokenizer (50Hz, N=9: 1 semantic + 8 acoustic)
↓
Text Input → Text Tokenizer (inherited from base LLM) + N-1 padding tokens
↓
Task Sequence Construction (speech-only | text-only | ASR splice | TTS splice)
↓
Delay Interleave (offset stream n by n-1 frames)
↓
Embedding Sum: h_pre_t = Σ Embedding(x̂_t,n)
↓
Causal Transformer Body (inherited from base text LLM)
↓
Parallel Prediction Heads: p(x̂_t,n | X̂_{1:t-1}) per stream n
↓
Detokenizer → Waveform/Text Output

- **Critical path**:
  1. **Tokenizer compatibility**: Semantic and acoustic tokenizers must operate at identical frame rates (50Hz); vocabulary must include all token types
  2. **Base LLM selection**: Must have access to original training corpus for in-domain text data; paper uses SmolLM2 (1.7B) and OLMo-2 (7B)
  3. **Delay interleave correctness**: Incorrect offset calculation breaks intra-frame causality
  4. **Loss reweighting**: Apply weights 1:1/2:1/(N-1) for text:semantic:acoustic tokens

- **Design tradeoffs**:
  - Model scale: 135M/360M severely underperform; 1.7B+ necessary for viable multi-task performance
  - Batch size: 1M frames outperformed 4M frames despite text LLM practice favoring larger batches
  - Annealing data composition: Opt-A (LibriSpeech+FLEURS) vs Opt-B (LibriTTS+VCTK) show different ASR/TTS tradeoffs
  - Text corpus selection: Out-of-domain text causes severe MMLU degradation (46.0 → 30.5)

- **Failure signatures**:
  - Text capability collapse: MMLU drops ~15+ points when text corpus differs from base LLM training data
  - TTS slow convergence: TTS-WER remains high while ASR converges faster (7B intermediate checkpoint)
  - Small model failure: 135M/360M models show TTS-WER 19.8-38.7% and MMLU near random (~25)
  - Long-form infeasibility: Pre-trained model cannot handle long sequences without annealing phase containing spliced data

- **First 3 experiments**:
  1. Reproduce scaling curve (Figure 2): Train 135M, 360M, 1.7B variants and verify ASR-WER (T-clean: 6.9→4.2→2.5) and MMLU (25→26→46) trends.
  2. Ablate annealing data: Using same pre-trained checkpoint, run annealing with Opt-A vs Opt-B compositions; confirm TTS-WER divergence (Table 3: 5.1% vs 4.0% in-domain).
  3. Text domain mismatch test: Train with in-domain vs out-of-domain text corpus; verify MMLU degradation gap (Table 2c: 46.0 vs 30.5).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the optimal data composition for the annealing stage be systematically determined to balance performance across ASR, TTS, and text capabilities?
- Basis in paper: [explicit] The authors note that while annealing is beneficial, "ultimate model performance is sensitive to the annealing data selection," observing significant performance disparities between Option A and Option B configurations.
- Why unresolved: The paper relies on manual curation and intuition ("we believe the other four datasets are of high quality") rather than a principled method for selecting annealing data, resulting in mixed outcomes where no single option dominates all metrics.
- Evidence: A study evaluating automated data selection algorithms (e.g., based on perplexity or diversity metrics) for the annealing phase that correlates specific data mixtures with consistent gains across all modalities.

### Open Question 2
- Question: Is the sub-optimality of large batch sizes (4M frames) in SpeechLM pre-training caused primarily by insufficient training updates relative to the dataset size?
- Basis in paper: [explicit] The authors attempted large batch sizes (4M frames) but found them sub-optimal, hypothesizing that this approach resulted in "125k updates... expected insufficient to help the model converge."
- Why unresolved: The comparison was confounded by the number of updates; the large batch experiment may have simply been under-trained rather than the batch size itself being detrimental to the optimization landscape.
- Evidence: A training run comparing 1M vs. 4M batch sizes matched on total training tokens/FLOPs (rather than updates) to isolate the effect of batch size from convergence status.

### Open Question 3
- Question: What is the precise model scale threshold required to prevent catastrophic forgetting of text capabilities during joint speech-text pre-training?
- Basis in paper: [inferred] The paper highlights that while the 1.7B model retains text capabilities (MMLU 46.2), smaller models (135M, 360M) degrade to near-random performance (MMLU ~25.0), suggesting a critical capacity requirement.
- Why unresolved: The experiments only provide coarse data points (135M, 360M, 1.7B) without identifying the exact inflection point or the architectural reasons why smaller models fail to handle the multi-stream interleaving without losing text proficiency.
- Evidence: A fine-grained scaling study interpolating model sizes between 360M and 1.7B to identify the minimum parameter count required to maintain stable text-only MMLU scores.

## Limitations
- Architecture generalization may be domain-specific, showing strong results primarily with English speech data at 50Hz sampling
- Data transparency issues with incomplete specification of in-domain text corpus composition and annealing data splicing procedures
- Strong dependence on model scale with 135M/360M models severely underperforming, suggesting limited architectural efficiency

## Confidence

**High Confidence**: The multi-stream architecture's ability to handle unified speech-text modeling is well-demonstrated through consistent performance across ASR (2.3% WER), TTS (4.0% WER), and text-only tasks (MMLU 59.0). The scaling benefits (135M→360M→1.7B→7B showing clear performance improvements) are empirically solid and reproducible given access to the same data.

**Medium Confidence**: The claim that semantic tokens excel at understanding while acoustic tokens excel at generation is supported by task-specific performance but lacks ablation studies isolating each token type's contribution. The effectiveness of annealing is demonstrated but highly sensitive to data selection, suggesting the mechanism may be more about data curation than the annealing procedure itself.

**Low Confidence**: The assertion that OpusLM achieves "unified" modeling without task-specific fine-tuning beyond pre-training is overstated, as the training data includes task-specific sequences (ASR splice, TTS splice) that effectively pre-condition the model for these tasks. The paper doesn't adequately address whether truly unified inference without any task conditioning is feasible.

## Next Checks

1. **Ablation of Token Types**: Train variants using only semantic tokens, only acoustic tokens, and the full multi-stream setup on identical data. Measure task-specific performance to quantify each token type's contribution to ASR vs TTS performance, validating the understanding vs generation claims.

2. **Cross-Domain Robustness Test**: Evaluate OpusLM on non-English speech data or different sampling rates (16kHz vs 8kHz) to test architectural generalization. Measure performance degradation to establish domain transfer limits.

3. **Text Corpus Sensitivity Analysis**: Systematically vary the text corpus composition while holding speech data constant, measuring MMLU performance across the full range observed (46.0 → 30.5). Quantify the exact relationship between text data distribution and text capability preservation.