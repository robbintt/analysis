---
ver: rpa2
title: Improved High-probability Convergence Guarantees of Decentralized SGD
arxiv_id: '2510.06141'
source_url: https://arxiv.org/abs/2510.06141
tags:
- lemma
- follows
- linear
- costs
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the convergence in high-probability (HP) of a
  variant of decentralized stochastic gradient descent (DSGD) under light-tailed noise.
  The authors show that DSGD converges in HP under the same conditions on the cost
  function as in the mean-squared error (MSE) sense, achieving order-optimal rates
  and linear speed-up for both non-convex and strongly convex costs.
---

# Improved High-probability Convergence Guarantees of Decentralized SGD

## Quick Facts
- **arXiv ID:** 2510.06141
- **Source URL:** https://arxiv.org/abs/2510.06141
- **Reference count:** 40
- **Primary result:** DSGD achieves high-probability convergence with linear speed-up under light-tailed noise without requiring uniformly bounded gradients.

## Executive Summary
This paper establishes high-probability convergence guarantees for decentralized stochastic gradient descent (DSGD) under light-tailed noise, removing strong assumptions like uniformly bounded gradients and asymptotically vanishing noise. The key innovation is carefully bounding the moment generating function (MGF) of both the optimality gap and the consensus gap, enabling linear speed-up in the number of users for both non-convex and strongly convex costs. The analysis shows that DSGD achieves order-optimal rates comparable to centralized SGD while maintaining the robustness benefits of decentralization.

## Method Summary
The method employs a variant of DSGD where each user updates their local model by averaging neighbor models and taking a stochastic gradient step with fixed step-size. The analysis decouples the MGF bounds for the optimality gap and consensus gap, using an "offset trick" to handle inner products between gradients and noise without requiring bounded gradients. For strongly convex costs, a time-varying step-size schedule is used, while fixed step-size is required for non-convex cases to achieve linear speed-up. The network topology is assumed to be static with connected graphs satisfying standard mixing conditions.

## Key Results
- DSGD achieves linear speed-up (convergence rate improves by factor √n) in high-probability sense for both non-convex and strongly convex costs
- Removes requirement for uniformly bounded gradients, extending applicability to settings where gradients may be unbounded
- Shows average network noise is O(σ√d/√n)-sub-Gaussian, enabling tighter convergence bounds
- Provides improved transient times compared to mean-squared error results

## Why This Works (Mechanism)

### Mechanism 1: Linear Speed-up via Sub-Gaussian Averaging
The paper establishes that the average network noise is O(σ√d/√n)-sub-Gaussian. By bounding the MGF of this averaged noise, the effective variance contribution to convergence scales down by factor n, allowing the leading error term to decay as O(1/√(nT)) rather than O(1/√T). This enables linear speed-up when using fixed step-size.

### Mechanism 2: Removing Bounded Gradients with Offset Trick
The proof employs an "offset trick" that algebraically manipulates the descent inequality to absorb the impact of the inner product between gradients and noise. This cancels out terms that would otherwise explode if gradients were unbounded, preventing the need for artificial uniform gradient bounds.

### Mechanism 3: Decoupling Optimality and Consensus Gap MGFs
The analysis separately bounds the MGF of the optimality gap and the consensus gap. By deriving tight bounds on the consensus gap's MGF, the disagreement term contributes only to higher-order transient effects rather than the leading convergence term, maintaining high-probability guarantees despite decentralization.

## Foundational Learning

- **Concept: Sub-Gaussian Random Variables**
  - **Why needed here:** The entire proof strategy relies on the existence of the MGF for noise. Understanding sub-Gaussian tails is essential for grasping why HP bounds are possible.
  - **Quick check question:** Can you explain why assuming noise is sub-Gaussian allows bounding P(X > ε) by exp(-cε²)?

- **Concept: Linear Speed-up (Scaling)**
  - **Why needed here:** The paper claims DSGD is efficient because adding users reduces convergence time linearly. Understanding this requires grasping how the 1/√n factor emerges from variance of averaged estimators.
  - **Quick check question:** If you double the number of users n, approximately how much faster does the algorithm converge to a fixed accuracy ε according to the leading term?

- **Concept: Consensus Gap**
  - **Why needed here:** This is the specific "cost" of decentralization. You must distinguish between error due to optimization landscape (optimality gap) and error due to users disagreeing (consensus gap).
  - **Quick check question:** In a fully connected network (ideal consensus), what is the value of the consensus gap?

## Architecture Onboarding

- **Component map:** Users (nodes) -> Stochastic gradient updates -> Weighted neighbor averaging (consensus) -> Model parameter updates
- **Critical path:** The interplay between step-size α and network connectivity λ. Step-size must be small enough to control consensus gap but large enough to allow linear speed-up (specifically α ∝ √n/T).
- **Design tradeoffs:**
  - Fixed vs. time-varying step-size: Fixed required for non-convex to achieve linear speed-up in HP sense
  - Heterogeneity vs. convergence: Bounded heterogeneity assumed for non-convex; significant divergence may break convergence
- **Failure signatures:**
  - Tail blow-up: If empirical failure rates don't decay exponentially, check noise distribution (might be heavy-tailed)
  - Stagnation: If error plateaus high, check network connectivity λ (poor connectivity implies large transient time)
- **First 3 experiments:**
  1. Run DSGD on non-convex task (e.g., ResNet on CIFAR) with n ∈ {1, 2, 4, 8, 16}, plot optimality gap vs iterations to verify √n speed-up
  2. Fix n and T, run DSGD 5000 times, plot histogram of final gradient norms to verify exponential tail decay
  3. Compare DSGD on ring vs fully connected topology, measure transient time before linear speed-up kicks in

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the established HP convergence guarantees and linear speed-up be extended to settings with heavy-tailed noise without uniformly bounded gradient assumptions?
- **Open Question 2:** Do the HP convergence results and linear speed-up hold for costs satisfying the Polyak-Łojasiewicz (PL) condition?
- **Open Question 3:** Can the bounded heterogeneity assumption (A5) required for non-convex costs be removed by incorporating bias-correction mechanisms like gradient tracking in the HP setting?

## Limitations
- Analysis assumes light-tailed (sub-Gaussian) noise; behavior under heavy-tailed noise is not covered
- Step-size conditions involve problem-dependent constants that are not always explicitly specified
- Relies on fixed graph topologies; dynamic or time-varying networks are not considered

## Confidence

- **High confidence:** Core theoretical claims about linear speed-up and removal of uniformly bounded gradients for light-tailed noise
- **Medium confidence:** Extension to strongly convex costs and treatment of consensus gap MGF bounds
- **Low confidence:** Practical implications for very large-scale or dynamic networks, and robustness to violations of sub-Gaussianity

## Next Checks

1. **Verify linear speed-up empirically:** Run DSGD on a non-convex task (e.g., ResNet on CIFAR) with varying numbers of users (n ∈ {1, 2, 4, 8, 16}). Plot the optimality gap vs. iterations to confirm convergence speed increases by factor ≈ √n.

2. **Measure tail decay:** For a fixed n and T, run DSGD 5000 times and plot a histogram of final gradient norms. Check if the empirical tail matches the theoretical sub-Gaussian bound (few outliers, exponential decay).

3. **Test connectivity impact:** Compare DSGD on a ring topology vs. a fully connected topology. Measure the "transient time" before linear speed-up emerges; confirm the ring requires significantly longer due to poor spectral gap λ.