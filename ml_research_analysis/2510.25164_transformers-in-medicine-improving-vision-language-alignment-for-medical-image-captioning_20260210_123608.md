---
ver: rpa2
title: 'Transformers in Medicine: Improving Vision-Language Alignment for Medical
  Image Captioning'
arxiv_id: '2510.25164'
source_url: https://arxiv.org/abs/2510.25164
tags:
- image
- medical
- caption
- visual
- deit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a transformer-based multimodal framework for
  generating clinically relevant captions for MRI scans. The system combines a DEiT-Small
  vision transformer as an image encoder, MediCareBERT for caption embedding, and
  a custom LSTM-based decoder.
---

# Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning

## Quick Facts
- arXiv ID: 2510.25164
- Source URL: https://arxiv.org/abs/2510.25164
- Reference count: 17
- Key result: BLEU-4 scores of 0.30 on brain-only MRI data and 0.23 on all MRI data using hybrid cosine-MSE loss and domain-specific text encoding

## Executive Summary
This paper presents a transformer-based multimodal framework for generating clinically relevant captions for MRI scans. The system combines a DEiT-Small vision transformer as an image encoder, MediCareBERT for caption embedding, and a custom LSTM-based decoder. The architecture is designed to semantically align image and textual embeddings, using hybrid cosine-MSE loss and contrastive inference via vector similarity. The method is benchmarked on the MultiCaRe dataset, comparing performance on filtered brain-only MRIs versus general MRI images against state-of-the-art medical image captioning methods including BLIP, R2GenGPT, and recent transformer-based approaches. Results show that focusing on domain-specific data improves caption accuracy and semantic alignment, with the proposed method achieving BLEU-4 scores of 0.30 on brain-only data and 0.23 on all MRI data. The work proposes a scalable, interpretable solution for automated medical image reporting.

## Method Summary
The method processes MRI images through a DEiT-Small vision transformer (22M parameters, pretrained on ImageNet) that encodes 224×224 images as 16×16 patches (196 tokens) into 768-dimensional embeddings. MediCareBERT, a BERT-base model fine-tuned on MultiCaRe captions via masked language modeling, provides domain-specific text embeddings. A 2-layer LSTM decoder (768 hidden dimensions) is initialized from the image embedding and generates captions token-by-token using greedy decoding. The model is trained end-to-end with a hybrid loss function combining cosine similarity (weighted 0.7) and MSE (weighted 0.3) to optimize both directional alignment and magnitude consistency between visual and textual embeddings. Training uses progressive unfreezing of the DEiT encoder, Adam optimizer with warmup and cosine decay, batch sizes of 6-8, and gradient clipping at 1.0.

## Key Results
- The proposed method achieves BLEU-4 scores of 0.30 on brain-only MRI data and 0.23 on all MRI data
- Domain-specific fine-tuning of BERT (MediCareBERT) improves caption quality over generic BERT-base (0.30 vs 0.25 BLEU-4)
- DEiT-Small outperforms ResNet18 baseline (0.30 vs 0.22 BLEU-4) for vision encoding
- Hybrid cosine-MSE loss (α=0.7) shows superior performance over single-loss variants (0.30 vs 0.28-0.29 BLEU-4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid cosine-MSE loss improves semantic alignment between visual and textual embeddings compared to either loss alone.
- Mechanism: The loss function (Equation 1) weights cosine similarity at α=0.7 to emphasize directional alignment while MSE (weighted 0.3) constrains embedding magnitude, jointly optimizing both aspects of the vector space.
- Core assumption: Optimal embeddings require both directional similarity (semantic relatedness) and magnitude consistency (embedding scale stability).
- Evidence anchors:
  - [abstract] "hybrid cosine-MSE loss" cited as key to semantic alignment
  - [section 5] Equation 1 defines L = α·(1−cos(ŷ,y)) + (1−α)·||ŷ−y||² with α=0.7
  - [section 7.4, Table 2c] Ablation shows hybrid loss (0.30 BLEU-4) outperforms cosine-only (0.29) and MSE-only (0.28)
  - [corpus] Weak direct corpus support; neighboring papers do not evaluate hybrid loss combinations

### Mechanism 2
- Claim: Fine-tuning BERT on medical captions (MediCareBERT) improves clinical language representation over generic BERT-base.
- Mechanism: Masked Language Modeling on MultiCaRe captions adapts wordpiece embeddings and contextual representations to radiology terminology, abbreviations, and report structure.
- Core assumption: Domain-specific vocabulary and syntactic patterns in radiology captions differ meaningfully from general corpora.
- Evidence anchors:
  - [abstract] "MediCareBERT for caption embedding" specified as text encoder
  - [section 4.2] Describes fine-tuning on MultiCaRe; notes BERT captures modifiers, qualifiers, spatial terms
  - [section 6.2] MLM pre-training for 50 epochs with domain WordPiece tokenizer
  - [section 7.4, Table 2c] DEiT + BERT(ft) + hybrid loss (0.30) vs DEiT + BERT-base + hybrid loss (0.25)
  - [corpus] No corpus papers specifically validate medical BERT fine-tuning for captioning

### Mechanism 3
- Claim: DEiT-Small's attention-based encoding captures global spatial dependencies better than CNN local features for MRI captioning.
- Mechanism: Multi-head self-attention across 14×14 patch tokens (196 total) enables pairwise relationships between all spatial regions, supporting lesion localization and anatomical comparisons.
- Core assumption: Medical image understanding requires modeling long-range spatial relationships (e.g., contralateral comparisons) that CNNs capture only through deep stacking.
- Evidence anchors:
  - [abstract] "DEiT-Small vision transformer as an image encoder"
  - [section 4.1] States DEiT "directly encodes pairwise relationships among all patches for holistic anatomical understanding"
  - [section 7.4, Table 2c] DEiT + BERT(ft) + hybrid loss (0.30 BLEU-4) vs ResNet18 + BERT(ft) + hybrid loss (0.22)
  - [corpus] Corpus papers generally support transformer attention for global context in image captioning, though none specifically validate DEiT for MRI

## Foundational Learning

- Concept: **Vision Transformer patch embedding**
  - Why needed here: DEiT processes MRI scans as 16×16 patches (196 tokens), fundamentally different from CNN feature maps.
  - Quick check question: Can you explain how a 224×224 image becomes 196 patch tokens, and what the [CLS] token represents?

- Concept: **BERT [CLS] token as sentence embedding**
  - Why needed here: MediCareBERT's [CLS] token initializes the LSTM decoder and serves as the target for loss computation.
  - Quick check question: Why is the [CLS] token used instead of mean-pooling all token embeddings for sentence representation?

- Concept: **Cosine similarity vs. Euclidean distance in embedding spaces**
  - Why needed here: The hybrid loss explicitly trades off directional alignment (cosine) against magnitude alignment (MSE).
  - Quick check question: If two embeddings have cosine similarity 0.95 but different magnitudes, what does each loss component penalize?

## Architecture Onboarding

- Component map:
  Input preprocessing: 224×224 resize → 16×16 patches → 196 tokens
  Vision encoder: DEiT-Small (22M params) → 768-dim image embedding
  Text encoder: MediCareBERT → [CLS] token (768-dim) + token embeddings
  Decoder: 2-layer LSTM (768 hidden) initialized from DEiT output
  Output: Projected embedding → cosine similarity lookup over BERT vocabulary

- Critical path:
  1. Image patch tokenization (Section 3)
  2. DEiT forward pass → image embedding
  3. LSTM hidden/cell state initialization from image embedding (Section 4.3)
  4. Token-by-token decoding with MediCareBERT embeddings as input
  5. Final embedding compared to ground-truth [CLS] via hybrid loss (Section 5)

- Design tradeoffs:
  - **LSTM vs. Transformer decoder**: Paper claims LSTM is more controllable and interpretable with limited data (Section 4.3), but sacrifices parallel decoding and long-range dependency modeling.
  - **Greedy vs. sampling decoding**: Greedy adopted for reproducibility (Section 7.1), but may produce generic captions; Top-p showed better fluency-diversity trade-off.
  - **Brain-only vs. All-MRI filtering**: Brain-only yields higher BLEU-4 (0.30 vs. 0.23), supporting domain specificity, but reduces training data by ~77%.

- Failure signatures:
  - **Low BLEU-4 (<0.15)**: Check if DEiT layers are frozen, BERT not fine-tuned, or loss function misconfigured.
  - **Generic captions ("MRI scan shows abnormality")**: May indicate insufficient domain pre-training or over-regularization.
  - **Anatomical hallucinations**: LSTM may lack grounding; verify image embedding properly initializes decoder.
  - **Training instability**: Gradient clipping at 1.0 is required; check learning rate schedule (warmup + cosine decay).

- First 3 experiments:
  1. **Reproduce ablation**: Run DEiT + BERT(ft) + hybrid loss vs. ResNet18 + BERT(ft) + hybrid loss on brain-only subset to validate the 0.30 vs. 0.22 BLEU-4 gap.
  2. **Loss sensitivity**: Sweep α ∈ {0.5, 0.6, 0.7, 0.8, 0.9} to confirm 0.7 is optimal or identify dataset-specific tuning needs.
  3. **Decoder comparison**: Replace LSTM with a small transformer decoder (2 layers) to test the paper's claim that LSTM is preferable for limited-data regimes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the current 2D patch-based architecture be effectively extended to handle 3D volumetric (multi-slice) MRI data?
- Basis in paper: [explicit] The authors state, "In the future, we plan to... extend the framework to 3D volumetric (multi-slice) image captioning."
- Why unresolved: The current DEiT encoder processes single 2D slices (224×224), potentially losing the inter-slice spatial continuity critical for diagnosing conditions in MRI volumes.
- What evidence would resolve it: Successful implementation and benchmarking on 3D datasets (e.g., BraTS) demonstrating that inter-slice context improves diagnostic accuracy.

### Open Question 2
- Question: Does high performance on n-gram metrics (BLEU, METEOR) correlate with clinical utility and factual accuracy as assessed by domain experts?
- Basis in paper: [explicit] The authors propose to "perform expert-based clinical validation to assess the model’s readiness for real-world deployment."
- Why unresolved: Automated metrics often fail to capture "hallucinations" or omissions of critical findings, which are unacceptable in clinical settings but may not lower BLEU scores significantly.
- What evidence would resolve it: A user study where radiologists rate the factual consistency and diagnostic value of generated captions against ground-truth reports.

### Open Question 3
- Question: To what extent does incorporating patient metadata (e.g., age, clinical history) improve the specificity and accuracy of generated captions?
- Basis in paper: [explicit] The paper notes future work includes plans to "integrate patient metadata for more context-aware captioning."
- Why unresolved: The current model relies solely on visual features, which may be insufficient to distinguish between differential diagnoses that look similar but depend on patient history.
- What evidence would resolve it: Ablation studies showing improved ROUGE-L or CIDEr scores when non-imaging patient data is fused with the visual embeddings.

### Open Question 4
- Question: Does the choice of an LSTM decoder become a bottleneck for modeling long-range dependencies if the dataset scales to the size of MIMIC-CXR?
- Basis in paper: [inferred] The authors justify the LSTM decoder for "limited-data regimes" (Section 4.3) to avoid overfitting, yet explicitly plan to "scale our approach to larger datasets."
- Why unresolved: While LSTMs prevent overfitting on small data, they generally struggle with long-sequence modeling compared to Transformer decoders, potentially limiting performance on larger, more complex datasets.
- What evidence would resolve it: Comparative loss curves and caption coherence analysis against Transformer-based decoders when trained on datasets exceeding 100k image-report pairs.

## Limitations

- The study relies on a single dataset (MultiCaRe) with limited size, particularly the brain-only subset at 2,718 pairs, raising concerns about model robustness across diverse clinical scenarios.
- The evaluation focuses primarily on automatic metrics (BLEU-4, METEOR, ROUGE-L, CIDEr) without human clinical validation to assess whether generated captions meet radiologist standards for accuracy and clinical utility.
- The architectural choice of LSTM over transformer decoder, while justified for data efficiency, may limit the model's ability to capture long-range dependencies in complex medical narratives.

## Confidence

- **High Confidence**: The hybrid cosine-MSE loss mechanism and its demonstrated superiority over single-loss variants (0.30 vs 0.28-0.29 BLEU-4) is well-supported by ablation studies with clear quantitative evidence.
- **Medium Confidence**: The domain-specific advantages of MediCareBERT fine-tuning are supported by comparison to BERT-base, but lack external validation on other medical captioning datasets or clinical tasks.
- **Medium Confidence**: The architectural benefits of DEiT over CNN baselines are demonstrated within this specific framework, but the comparison uses different parameter scales and may not isolate the transformer architecture's contribution.
- **Low Confidence**: Claims about LSTM providing better interpretability and control with limited data are primarily theoretical, with no empirical comparison to transformer decoder alternatives.

## Next Checks

1. **Cross-dataset validation**: Evaluate the trained model on independent medical captioning datasets (e.g., IU X-Ray, MIMIC-CXR) to assess generalization beyond MultiCaRe and identify potential overfitting to dataset-specific patterns.

2. **Clinical expert evaluation**: Conduct blinded assessment by radiologists to validate whether BLEU-4 improvements translate to clinically meaningful caption quality, accuracy, and report completeness compared to baseline methods.

3. **Decoder architecture ablation**: Systematically compare the proposed LSTM decoder against transformer decoder variants (2-4 layers) with matched parameter counts to empirically test the claimed advantages of LSTM for limited medical data scenarios.