---
ver: rpa2
title: 'ImageNot: A contrast with ImageNet preserves model rankings'
arxiv_id: '2404.02112'
source_url: https://arxiv.org/abs/2404.02112
tags:
- imagenet
- imagenot
- learning
- image
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ImageNot, a large-scale dataset designed
  to be drastically different from ImageNet while maintaining the same scale of 1000
  classes with 1000 examples each. Unlike ImageNet, ImageNot is created without human
  annotation, using web-crawled images and automated labeling.
---

# ImageNot: A contrast with ImageNet preserves model rankings
## Quick Facts
- arXiv ID: 2404.02112
- Source URL: https://arxiv.org/abs/2404.02112
- Reference count: 25
- Key outcome: Model rankings and relative improvements from ImageNet transfer to a drastically different dataset (ImageNot) despite no human annotation

## Executive Summary
This paper introduces ImageNot, a large-scale dataset designed to be drastically different from ImageNet while maintaining the same scale of 1000 classes with 1000 examples each. Unlike ImageNet, ImageNot is created without human annotation, using web-crawled images and automated labeling. The study aims to test the external validity of deep learning progress on ImageNet by training and evaluating key model architectures on ImageNot. Remarkably, the model rankings and relative improvements observed on ImageNet are preserved on ImageNot, suggesting that the relative progress in model development is robust across significantly different datasets.

## Method Summary
The authors created ImageNot by web-crawling images from the internet and using automated labeling techniques to generate a dataset with 1000 classes and 1000 examples per class. The key innovation is that ImageNot is constructed to be drastically different from ImageNet while maintaining the same scale, using no human annotation in the labeling process. The study then trains and evaluates the same set of model architectures on both ImageNet and ImageNot to compare their relative rankings and improvements. By preserving the class structure but changing the data distribution and labeling method, the authors create a controlled experiment to test whether ImageNet progress generalizes to fundamentally different data.

## Key Results
- Model rankings on ImageNet (e.g., ConvNeXt > ResNet > ViT) are preserved on ImageNot
- Relative improvements between models (e.g., +2% accuracy gains) transfer between datasets
- The preservation of rankings holds despite ImageNot being created without human annotation and using drastically different data sources

## Why This Works (Mechanism)
The preservation of model rankings across datasets occurs because relative performance differences between architectures capture fundamental properties of the models rather than dataset-specific features. When models consistently outperform each other across different data distributions, it suggests that architectural choices (depth, attention mechanisms, training procedures) have intrinsic effects that persist regardless of specific image content. This mechanism indicates that ImageNet has been measuring genuine architectural progress rather than overfitting to its particular data distribution, as the same hierarchy of model capabilities emerges even when trained on completely different images with automated labels.

## Foundational Learning
- **Dataset construction methodology**: Understanding how to create large-scale datasets with controlled properties is essential for benchmark design. Quick check: Can you explain the difference between human-annotated and automatically-labeled datasets?
- **Transfer learning evaluation**: The concept of testing whether progress on one dataset transfers to another is fundamental for validating benchmark utility. Quick check: Why is it important to test model rankings across different datasets?
- **Model ranking analysis**: Understanding how to compare models beyond absolute accuracy metrics is crucial for meaningful evaluation. Quick check: What does it mean if model rankings are preserved across datasets?

## Architecture Onboarding
- **Component map**: Web crawler -> Image filtering -> Automated labeling -> Dataset storage -> Model training pipeline
- **Critical path**: Data collection → Automated annotation → Dataset validation → Model training → Performance comparison
- **Design tradeoffs**: Automated labeling trades accuracy for scale and eliminates human bias, but may introduce label noise
- **Failure signatures**: If rankings don't transfer between datasets, it suggests benchmark overfitting or architecture-specific dataset dependencies
- **3 first experiments**: 1) Train baseline models on both datasets to establish ranking preservation, 2) Test with noisy labels to understand annotation quality impact, 3) Evaluate transfer learning from one dataset to the other

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond the scope of their investigation.

## Limitations
- Automated labeling without human verification raises concerns about label accuracy and potential ImageNet contamination
- Does not address whether rankings transfer to real-world applications with different data distributions or task requirements
- Analysis focuses on supervised classification without exploring transfer learning or few-shot learning scenarios

## Confidence
- High confidence in claims about preservation of relative model rankings and improvements across datasets
- Medium confidence in the interpretation that this validates ImageNet as a benchmark for relative progress measurement
- Low confidence in broader generalizability claims to other domains or tasks

## Next Checks
1. Conduct human verification of a sample of ImageNot labels to quantify annotation accuracy and potential ImageNet contamination
2. Test model performance on intermediate datasets that gradually transition between ImageNet and ImageNot distributions to identify where ranking divergence begins
3. Evaluate transfer learning performance from ImageNet to ImageNot and vice versa to understand practical implications for real-world applications