---
ver: rpa2
title: End-to-end workflow for machine learning-based qubit readout with QICK and
  hls4ml
arxiv_id: '2501.14663'
source_url: https://arxiv.org/abs/2501.14663
tags:
- readout
- qubit
- quantum
- qick
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an end-to-end workflow for machine learning-based
  superconducting qubit readout by integrating neural networks into the Quantum Instrumentation
  Control Kit (QICK) platform. The approach leverages hls4ml to convert trained neural
  network models into hardware-efficient FPGA implementations using quantization-aware
  training and high-level synthesis.
---

# End-to-end workflow for machine learning-based qubit readout with QICK and hls4ml

## Quick Facts
- arXiv ID: 2501.14663
- Source URL: https://arxiv.org/abs/2501.14663
- Reference count: 0
- Primary result: Achieves 96% single-shot fidelity with 32ns latency using <16% FPGA resources for ML-based qubit readout

## Executive Summary
This work presents an end-to-end workflow for machine learning-based superconducting qubit readout by integrating neural networks into the Quantum Instrumentation Control Kit (QICK) platform. The approach leverages hls4ml to convert trained neural network models into hardware-efficient FPGA implementations using quantization-aware training and high-level synthesis. For single transmon qubit readout, the method achieves 96% single-shot fidelity with a latency of 32ns while utilizing less than 16% of FPGA look-up table resources. The workflow provides open-source tools for designing, optimizing, and deploying ML-based readout systems, addressing the need for scalable, high-fidelity quantum control in growing qubit arrays.

## Method Summary
The workflow integrates machine learning with FPGA-based readout hardware through a complete pipeline from data collection to hardware deployment. Raw I/Q quadrature values from dispersive qubit readout are captured by the QICK's ADC, processed through a 2-layer neural network synthesized with hls4ml, and output as binary qubit state predictions. The approach employs quantization-aware training to optimize models for low-precision FPGA implementation, and uses window selection to maximize distinguishability while minimizing latency. The entire system operates in real-time, with predictions available within 32 nanoseconds of data capture.

## Key Results
- Achieves 96% single-shot fidelity for transmon qubit readout
- Delivers predictions with 32ns total latency (10 clock cycles)
- Uses less than 16% of FPGA look-up table resources
- Successfully implements ternary (2-bit) quantization without fidelity loss

## Why This Works (Mechanism)

### Mechanism 1: Temporal Feature Selection via Window Optimization
Limiting the readout window to a specific time segment (starting late, ending early) maximizes classification fidelity compared to using the full pulse duration. The readout resonator requires time to populate (ring-up), causing early data to lack clear separation. Conversely, qubit relaxation ($T_1$ decay) degrades the signal over longer durations. By truncating the input window (e.g., to 400 clock cycles starting at cycle 100), the classifier focuses on the steady-state period where I/Q separation is maximized and relaxation noise is minimized.

### Mechanism 2: Hardware-Aware Quantization via QAT
Reducing the numerical precision of the neural network to fixed-point (e.g., ternary weights) drastically reduces FPGA resource usage without significantly degrading readout fidelity. Standard floating-point math is resource-intensive on FPGAs. Quantization-Aware Training (QAT) simulates the quantization error (noise) during the training process. This forces the model weights to converge to a configuration that is robust to low-resolution arithmetic, allowing the hardware implementation to use efficient integer logic (DSPs/LUTs) rather than expensive floating-point cores.

### Mechanism 3: Latency Minimization via Parallelization
Unrolling the neural network layers in the FPGA firmware creates a fully parallel dataflow architecture, minimizing inference latency. Instead of reusing a single arithmetic unit for different weights (sequential processing), "unrolling" instantiates dedicated hardware for every multiplication and addition. This transforms the computation into a pure spatial flow, where the total latency is determined primarily by the propagation delay through the layers (approx. 8 clock cycles) rather than the volume of parameters.

## Foundational Learning

- **Concept:** Dispersive Readout (I/Q Modulation)
  - Why needed: The NN inputs are raw I/Q quadrature values. One must understand that the qubit state shifts the resonator frequency, causing the reflected signal to trace different paths in the I/Q plane. Without this, the input data is just meaningless noise.
  - Quick check: If the readout resonator frequency drifts, how would the I/Q clouds move, and would the pre-trained NN still work?

- **Concept:** Quantization-Aware Training (QAT)
  - Why needed: This is the bridge between software models and FPGA reality. Standard ML uses 32-bit floats; FPGAs prefer integers. QAT ensures the model learns to be "numb" to the loss of precision.
  - Quick check: Why is QAT generally superior to "Post-Training Quantization" for extremely low bit-widths (like ternary weights)?

- **Concept:** High-Level Synthesis (HLS)
  - Why needed: The workflow relies on hls4ml to convert Python code into Verilog/VHDL. Understanding HLS is required to configure the "unroll" pragmas that define the speed/resource tradeoff.
  - Quick check: What is the tradeoff if you set the "reuse factor" to a high number in hls4ml?

## Architecture Onboarding

- **Component map:** ADC -> Demod & Bin (QICK) -> AXI-Stream (Input) -> NN Classifier (hls4ml IP) -> BRAM (Output Buffer)
- **Critical path:** The ADC-to-NN Streaming Interface. The paper notes `in_TREADY` is not used (no backpressure). This implies the NN *must* compute faster than the data rate or the input buffer must handle the burst. The NN takes 8 cycles to compute while the window is 400 cycles, so this constraint is easily met, but timing closure here is critical.
- **Design tradeoffs:** 
  - Accuracy vs. Resources: The paper chose a tiny network (4 hidden neurons) and ternary weights to keep LUT usage low (~16%).
  - Latency vs. Window Size: A larger window provides more data (potentially higher fidelity) but increases the input loading time. The paper found a plateau at 400 cycles.
- **Failure signatures:**
  - Frozen Fidelity: If fidelity caps at ~50%, the Batch Normalization layer may be failing due to mismatched inference statistics vs. training statistics.
  - Resource Exhaustion: If unrolling is applied to a larger model (e.g., 64 neurons), LUT usage may exceed 100%, causing synthesis failure.
  - Data Misalignment: If the "readout offset" (trigger delay) is misconfigured, the NN receives the ring-up phase instead of the steady state, causing predictions to drop to noise levels.
- **First 3 experiments:**
  1. Baseline Thresholding: Implement standard I/Q integration and thresholding on the ZCU216 to establish a baseline fidelity (target ~95-96%).
  2. Quantization Sweep: Train the NN in software using QKeras with varying precisions (6-bit, 3-bit, ternary) to verify that accuracy holds at low bit-widths before touching hardware.
  3. Latency Measurement: Synthesize the hls4ml IP and use the logic analyzer (or timer in Python) to measure the exact clock cycles from `trigger` rising edge to `BRAM` write valid. Target: 10 cycles (32 ns).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does normalizing the ADC input data improve the neural network's robustness against signal drifts?
- Basis in paper: The authors state: "In future work, normalizing the ADC data should be studied for robustness against drifts in the readout signal."
- Why unresolved: The current implementation feeds raw 14-bit unsigned integers directly from the ADC to the network without normalization.
- What evidence would resolve it: A comparison of classification fidelity over extended periods or under intentionally induced signal drifts, contrasting normalized versus unnormalized inputs.

### Open Question 2
- Question: Can the workflow successfully execute real-time feedback control by routing the NN output to conditional logic?
- Basis in paper: The authors identify a "near-term upcoming" goal to "integrate output of the NN block into the conditional logic of QICK to run readout experiments including real-time ML feedback control."
- Why unresolved: The current firmware stores predictions in BRAM but does not yet forward them to the tProc for active feedback.
- What evidence would resolve it: Demonstration of a closed-loop experiment (e.g., active reset) where the NN prediction triggers a subsequent qubit operation within a specified latency.

### Open Question 3
- Question: How does the resource utilization and classification fidelity scale when applied to multi-qubit systems with crosstalk?
- Basis in paper: The paper highlights ML's ability to address "multi-qubit correlations and crosstalk" in the introduction, but the experimental demonstration is restricted to a single transmon qubit.
- Why unresolved: The study explicitly focused on a single qubit with others detuned, leaving the multi-qubit performance and FPGA resource costs unverified.
- What evidence would resolve it: FPGA resource reports and single-shot fidelity metrics derived from simultaneously reading out multiple coupled qubits using the same ML workflow.

## Limitations
- Input Data Range Assumptions: The workflow assumes raw 14-bit ADC values are directly compatible with the ternary-quantized neural network, without explicit documentation of pre-processing normalization.
- Generalization Across Architectures: Results are demonstrated for a single transmon qubit; scaling to multi-qubit arrays or different qubit modalities may require architectural changes and re-validation.
- Precision Sensitivity: While ternary quantization achieves comparable fidelity, performance is highly sensitive to input data distribution and could degrade with qubit coherence drifts.

## Confidence
- **High Confidence:** The core claim that hls4ml can synthesize a hardware-efficient, low-latency neural network implementation for qubit readout is well-supported by the reported 32 ns inference time and <16% FPGA resource usage.
- **Medium Confidence:** The claim of achieving >96% fidelity with a minimal 2-layer MLP is credible given the data and methodology, but relies on assumptions about window selection generalizing across experimental runs.
- **Medium Confidence:** The assertion that quantization-aware training (QAT) is critical for maintaining fidelity at low precision is plausible, but the exact configuration of QKeras layers is underspecified.

## Next Checks
1. **Input Range Verification:** Measure the statistical distribution (mean, variance, min/max) of the I/Q ADC values from the ZCU216 during readout. Compare this to the assumed input range of the trained ternary model to ensure no clipping or misrepresentation occurs.
2. **BatchNorm Layer Implementation Audit:** Examine the hls4ml-generated C++ code for the BatchNormalization layer. Verify that the scale, shift, mean, and variance parameters are correctly implemented in fixed-point arithmetic and match the QKeras model's inference behavior.
3. **T1 Drift Sensitivity Test:** Simulate or experimentally introduce a gradual decrease in qubit T1 time (e.g., from 100 µs to 30 µs). Measure how the classification fidelity degrades and whether the pre-configured 400-cycle readout window remains optimal, or if a new window must be calibrated.