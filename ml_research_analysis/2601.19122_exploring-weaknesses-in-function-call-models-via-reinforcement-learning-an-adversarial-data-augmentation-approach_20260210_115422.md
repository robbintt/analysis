---
ver: rpa2
title: 'Exploring Weaknesses in Function Call Models via Reinforcement Learning: An
  Adversarial Data Augmentation Approach'
arxiv_id: '2601.19122'
source_url: https://arxiv.org/abs/2601.19122
tags:
- training
- query
- data
- function
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an adversarial data augmentation method for
  improving function calling capabilities in large language models (LLMs) using reinforcement
  learning. The key idea is to train a query model that generates adversarial queries
  designed to expose weaknesses in the function call model, then use these queries
  to fine-tune the function call model.
---

# Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach

## Quick Facts
- arXiv ID: 2601.19122
- Source URL: https://arxiv.org/abs/2601.19122
- Reference count: 10
- Primary result: 4.94% higher accuracy than base model on Qwen3-0.6B using adversarial data augmentation for function calling

## Executive Summary
This paper introduces an adversarial data augmentation method to improve function calling capabilities in large language models through reinforcement learning. The approach trains a query model to generate challenging adversarial queries that expose weaknesses in the function call model, then uses these queries to fine-tune the function call model. The two models are trained iteratively in a zero-sum game formulation, with curriculum learning to gradually increase difficulty. The method demonstrates improved robustness and generalization compared to baseline approaches, particularly for smaller models, validated using the Berkeley Function-Calling Leaderboard evaluation framework.

## Method Summary
The proposed method employs a novel adversarial data augmentation approach for function calling tasks in LLMs. A query model is trained to generate adversarial queries that challenge the function call model's capabilities, while the function call model is simultaneously trained to handle these challenging inputs. The training process follows a zero-sum game formulation where the query model aims to minimize function call accuracy while the function call model aims to maximize it. Curriculum learning is incorporated to gradually increase query difficulty during training. The iterative alternating training between the two models allows both to improve progressively, with the query model becoming more sophisticated at generating challenging cases and the function call model becoming more robust at handling them.

## Key Results
- Achieved 4.94% higher accuracy than base model on Qwen3-0.6B
- Demonstrated improved robustness and generalization compared to baseline approaches
- Validated effectiveness using the Berkeley Function-Calling Leaderboard evaluation framework

## Why This Works (Mechanism)
The adversarial training approach works by exposing the function call model to challenging scenarios during training that it might encounter in real-world deployment. By iteratively generating and responding to adversarial queries, both models improve: the query model becomes better at identifying weaknesses, and the function call model becomes more robust at handling diverse and difficult inputs. The zero-sum game formulation creates a natural incentive structure where both models must continuously adapt to each other's improvements, preventing overfitting to static training data and promoting generalization to unseen scenarios.

## Foundational Learning
**Zero-sum game theory** - A mathematical framework where two players have opposite interests, with one player's gain being the other's loss. Why needed: Provides the theoretical foundation for the adversarial training between query and function call models. Quick check: Verify that the reward structure correctly implements opposing objectives for both models.

**Curriculum learning** - A training strategy that starts with simpler examples and gradually increases difficulty. Why needed: Prevents the query model from generating overly difficult adversarial examples too early, which could hinder training stability. Quick check: Monitor training progress to ensure gradual difficulty progression.

**Reinforcement learning** - A machine learning paradigm where agents learn through trial and error by receiving rewards or penalties. Why needed: Enables both the query model and function call model to improve based on performance feedback. Quick check: Verify that reward signals are properly shaping behavior in both models.

## Architecture Onboarding

**Component Map:** Query Model -> Adversarial Query Generator -> Function Call Model -> Performance Evaluator -> Reward Signal

**Critical Path:** The adversarial training loop forms the critical path, where the query model generates challenging inputs, the function call model processes them, performance is evaluated, and rewards are calculated to update both models iteratively.

**Design Tradeoffs:** The paper balances between generating sufficiently challenging adversarial examples versus maintaining training stability. Too difficult queries early in training could cause the function call model to fail completely, while too easy queries won't improve robustness. The curriculum learning approach addresses this by gradually increasing difficulty.

**Failure Signatures:** Potential failure modes include the query model generating unrealistic or nonsensical adversarial examples that don't represent genuine weaknesses, or the function call model overfitting to specific adversarial patterns rather than improving general robustness.

**3 First Experiments:**
1. Verify the adversarial query generation produces meaningful challenges to the function call model
2. Test the zero-sum game training dynamics to ensure both models are learning as intended
3. Evaluate the curriculum learning progression to confirm appropriate difficulty scaling

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope focused exclusively on function calling tasks and one benchmark (Berkeley Function-Calling Leaderboard)
- Limited analysis of computational overhead and inference latency implications
- Potential bias toward specific failure modes rather than overall robustness improvement
- Performance gains on larger models are less pronounced, raising scalability questions

## Confidence
- **High** confidence in core methodology and experimental results
- **Medium** confidence in generalizability claims beyond function calling
- **Low** confidence in long-term robustness assertions

## Next Checks
1. Evaluate the approach on additional LLM benchmarks beyond function calling, including general reasoning, code generation, and multi-task scenarios
2. Conduct ablation studies to isolate the contribution of individual components (curriculum learning, zero-sum game formulation, reinforcement learning)
3. Test the method's effectiveness on larger model sizes (7B, 13B, 70B parameters) to determine scalability