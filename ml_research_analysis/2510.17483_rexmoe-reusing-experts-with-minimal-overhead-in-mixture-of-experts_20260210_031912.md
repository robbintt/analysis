---
ver: rpa2
title: 'ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts'
arxiv_id: '2510.17483'
source_url: https://arxiv.org/abs/2510.17483
tags:
- expert
- experts
- rexmoe
- training
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces REXMOE, a novel approach to mixture-of-experts
  (MoE) models that overcomes the limitation of layer-local routing by allowing routers
  to reuse experts across adjacent layers. This design decouples expert dimensionality
  from per-layer budgets, enabling richer expert combinations without sacrificing
  individual expert capacity or increasing overall parameters.
---

# ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts

## Quick Facts
- arXiv ID: 2510.17483
- Source URL: https://arxiv.org/abs/2510.17483
- Reference count: 40
- Primary result: ReXMoE consistently improves language modeling perplexity and downstream task accuracy across models from 0.5B to 7B parameters compared to vanilla MoE baselines

## Executive Summary
ReXMoE introduces a novel approach to mixture-of-experts (MoE) models that overcomes the limitation of layer-local routing by allowing routers to reuse experts across adjacent layers. This design decouples expert dimensionality from per-layer budgets, enabling richer expert combinations without sacrificing individual expert capacity or increasing overall parameters. The authors also propose a progressive scaling routing (PSR) strategy that gradually expands the candidate expert pool during training, improving training stability and performance. Extensive experiments on models ranging from 0.5B to 7B parameters across different architectures demonstrate that ReXMoE consistently improves both language modeling perplexity and downstream task accuracy compared to vanilla MoE baselines.

## Method Summary
ReXMoE enables cross-layer expert reuse in MoE models, allowing routers to access experts from adjacent layers rather than being constrained to layer-local expert pools. This architectural change decouples expert dimensionality from per-layer capacity budgets, enabling more diverse expert combinations without increasing total parameters. The method introduces progressive scaling routing (PSR), which gradually expands the candidate expert pool during training to improve stability. The approach is evaluated across multiple model sizes (0.5B to 7B parameters) and architectures, demonstrating consistent improvements in both perplexity and downstream task performance.

## Key Results
- Consistent improvements in language modeling perplexity across all tested model sizes
- R4 configuration often delivers the highest average accuracy on downstream tasks
- Performance scales effectively with increasing model size and training data
- Stronger task-specific specialization compared to vanilla MoE baselines

## Why This Works (Mechanism)
ReXMoE works by fundamentally changing how expert routing operates in MoE models. Traditional MoE architectures restrict each router to a local pool of experts within the same layer, creating a trade-off between per-layer capacity and overall model expressiveness. By enabling cross-layer expert reuse, ReXMoE breaks this constraint, allowing each router to access a richer set of experts while maintaining the same per-layer capacity budget. This increases the diversity of expert combinations available for different inputs without requiring additional parameters. The progressive scaling routing strategy further enhances training stability by gradually increasing routing complexity as the model learns, preventing early-stage routing collapse and enabling more effective expert utilization throughout training.

## Foundational Learning
- **MoE Routing Mechanisms**: Understanding how routers select experts for different inputs is crucial for grasping ReXMoE's innovations. Quick check: Can you explain the difference between top-1 and top-k routing?
- **Expert Capacity vs. Model Expressiveness Trade-off**: Traditional MoE faces a fundamental constraint where increasing per-layer expert capacity reduces the total number of available experts. Quick check: How does ReXMoE break this trade-off?
- **Cross-layer Dependencies**: Knowledge of how information flows through transformer layers helps understand why reusing experts across layers can be beneficial. Quick check: What are the potential benefits and risks of allowing cross-layer expert access?
- **Progressive Training Strategies**: Understanding why gradual complexity increase during training can improve stability. Quick check: How does PSR differ from static routing strategies?

## Architecture Onboarding

**Component Map:**
Input tokens → Router layers → Shared expert pool (across layers) → Output tokens

**Critical Path:**
1. Token embedding and initial processing
2. Router determines expert assignments across layers
3. Tokens routed to selected experts from shared pool
4. Expert outputs combined and processed by subsequent layers

**Design Tradeoffs:**
- Increased routing complexity vs. improved expert utilization
- Cross-layer memory overhead vs. richer expert combinations
- Gradual routing expansion vs. training stability

**Failure Signatures:**
- Routing collapse (all tokens going to same experts)
- Increased memory usage due to cross-layer expert storage
- Potential for routing noise in early training stages

**First Experiments:**
1. Compare perplexity on language modeling tasks between ReXMoE and vanilla MoE with identical parameter counts
2. Measure expert utilization diversity across different routing configurations
3. Evaluate downstream task performance on benchmark datasets like SuperGLUE or SQuAD

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Computational overhead of cross-layer expert reuse not thoroughly quantified, particularly regarding activation memory costs and inference latency
- Scaling behavior beyond 7B parameters and with larger expert pools (>32 experts) remains untested
- Routing mechanism's robustness to noisy or adversarial inputs not evaluated

## Confidence

**High confidence** in the architectural innovation and its theoretical soundness
**Medium confidence** in the empirical improvements, as results are limited to specific model sizes and datasets
**Medium confidence** in the practical utility, given the lack of comprehensive efficiency analysis

## Next Checks
1. Measure activation memory usage and inference latency for ReXMoE compared to vanilla MoE across different batch sizes
2. Test scalability to larger models (10B+ parameters) and expert pools (64+ experts) to verify parameter scaling claims
3. Evaluate performance on noisy or adversarial input distributions to assess routing mechanism robustness