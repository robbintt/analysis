---
ver: rpa2
title: A Multifaceted Analysis of Negative Bias in Large Language Models through the
  Lens of Parametric Knowledge
arxiv_id: '2511.10881'
source_url: https://arxiv.org/abs/2511.10881
tags:
- negative
- bias
- knowledge
- parametric
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes negative bias in large language models (LLMs),\
  \ defined as the tendency to produce excessive negative responses in binary decision\
  \ tasks like yes-no question answering. The authors introduce a pipeline to categorize\
  \ datasets based on the model's parametric knowledge\u2014correct, incorrect, or\
  \ absent\u2014enabling fine-grained analysis of negative bias."
---

# A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge

## Quick Facts
- arXiv ID: 2511.10881
- Source URL: https://arxiv.org/abs/2511.10881
- Reference count: 40
- Key outcome: Negative bias in LLMs is most pronounced when models lack sufficient knowledge, and can be mitigated by changing response format from generation to selection

## Executive Summary
This paper investigates negative bias in large language models, defined as the tendency to produce excessive negative responses in binary decision tasks. The authors introduce a novel pipeline that categorizes datasets based on the model's parametric knowledge—correct, incorrect, or absent—enabling fine-grained analysis of when and why negative bias occurs. Through systematic experiments across different prompting strategies and response formats, they demonstrate that negative bias is not semantic-level but rather format-level, and that structural interventions can significantly reduce this bias while improving overall performance metrics.

## Method Summary
The authors develop a categorization pipeline to assess LLM parametric knowledge for binary question answering tasks, dividing responses into three knowledge states: correct knowledge (answers align with ground truth), incorrect knowledge (answers contradict ground truth), and absent knowledge (insufficient information to answer). They then systematically test various prompting strategies including chain-of-thought prompting, context provision, and different response formats (generation vs. option selection). The methodology involves controlled experiments measuring negative bias across these conditions while tracking weighted F1 scores to assess overall performance impact.

## Key Results
- Negative bias is most pronounced when LLMs lack sufficient knowledge to answer questions, representing a "shortcut" behavior
- Providing relevant context and including an "I don't know" option reduces negative bias
- Chain-of-thought prompting amplifies negative bias, while prompting models to select from options rather than generate explicit negative words significantly mitigates negative bias and improves weighted F1 scores

## Why This Works (Mechanism)
The mechanism underlying negative bias appears to be format-dependent rather than rooted in semantic understanding. When models are asked to generate explicit negative words, they default to these responses when uncertain, particularly when lacking knowledge. This "shortcut" behavior suggests that the generation process itself creates pressure toward negative responses in ambiguous situations. By changing the response format to selection from predefined options, this generation pressure is removed, allowing the model to make more balanced decisions based on available knowledge rather than defaulting to negative outputs when uncertain.

## Foundational Learning
- **Parametric Knowledge Assessment**: Understanding how to categorize LLM responses based on their internal knowledge state (correct/incorrect/absent) is crucial for analyzing bias patterns and requires familiarity with ground truth labeling and response evaluation.
- **Prompt Engineering Strategies**: Knowledge of different prompting techniques (chain-of-thought, context provision, format specification) and their effects on model behavior is essential for designing effective bias mitigation approaches.
- **Binary Decision Task Analysis**: Understanding the unique challenges of yes-no question answering and how model uncertainty manifests in binary outputs is necessary for interpreting negative bias phenomena.
- **Format-Level vs. Semantic-Level Bias**: Distinguishing between biases that arise from response format constraints versus those stemming from semantic understanding is critical for developing appropriate interventions.
- **Performance Metrics**: Familiarity with weighted F1 scores and other evaluation metrics for binary classification tasks is needed to assess the impact of bias mitigation strategies on overall model performance.

## Architecture Onboarding
**Component Map**: Input Questions -> Parametric Knowledge Categorizer -> Prompting Strategy Module -> Response Generator -> Output Evaluator
**Critical Path**: Question → Knowledge Assessment → Prompt Design → Response Generation → Bias Measurement
**Design Tradeoffs**: Generation formats allow more natural language but introduce bias pressure; selection formats reduce bias but may constrain expressiveness
**Failure Signatures**: Excessive negative responses in knowledge-absent scenarios, amplified bias with chain-of-thought prompting, reduced performance when context is missing
**First 3 Experiments**:
1. Test parametric knowledge categorization accuracy across different question types
2. Compare negative bias levels across models with varying parametric knowledge
3. Evaluate format-level bias interventions on a held-out dataset

## Open Questions the Paper Calls Out
None

## Limitations
- The categorization of parametric knowledge relies on subjective assessment that could introduce measurement bias
- The interpretation of negative bias as "shortcut" behavior assumes models should default to neutral responses when uncertain
- Format-level intervention effectiveness lacks theoretical grounding explaining why specific formats reduce bias
- Findings may not generalize across different model architectures as experiments focus on limited model sets

## Confidence
- **High confidence**: Core observation that LLMs exhibit format-level negative bias that can be mitigated through structural interventions
- **Medium confidence**: Claim that negative bias represents a "shortcut" behavior specifically linked to knowledge absence
- **Medium confidence**: Generalizability of findings across different LLM architectures

## Next Checks
1. Conduct ablation studies testing whether negative bias reduction from option-selection formats persists when controlling for token length, syntactic complexity, and semantic content
2. Implement cross-model validation by testing the same intervention strategies across diverse LLM architectures to assess generalizability
3. Design controlled experiments where models are explicitly prompted to express uncertainty to determine whether negative bias might represent rational uncertainty expression