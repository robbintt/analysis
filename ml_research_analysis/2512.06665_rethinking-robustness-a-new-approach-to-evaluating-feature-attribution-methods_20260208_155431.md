---
ver: rpa2
title: 'Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods'
arxiv_id: '2512.06665'
source_url: https://arxiv.org/abs/2512.06665
tags:
- robustness
- images
- attribution
- methods
- inputs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new metric for evaluating the robustness
  of feature attribution methods that is independent of the adversarial robustness
  of the underlying neural network model. The authors propose defining similar inputs
  based on both small input differences and small differences in the prediction logits,
  rather than just input proximity.
---

# Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods

## Quick Facts
- arXiv ID: 2512.06665
- Source URL: https://arxiv.org/abs/2512.06665
- Reference count: 38
- Key outcome: Introduces Output Similarity-based Robustness (OSR) metric that decouples attribution evaluation from model accuracy by defining similar inputs via prediction logit proximity rather than just input proximity

## Executive Summary
This paper challenges the current notion of attributional robustness that largely ignores differences in the model's outputs. The authors propose defining similar inputs based on both small input differences and small differences in prediction logits, rather than just input-space proximity. They introduce OSR, which measures the expected ℓ2 distance between attributions on such similar inputs, and develop a GAN-based method to generate these inputs. Experimental results show that OSR provides a more objective evaluation of attribution methods compared to existing metrics, consistently ranking methods regardless of whether the underlying model is well-trained or poorly-trained.

## Method Summary
The paper introduces Output Similarity-based Robustness (OSR) as a new metric for evaluating feature attribution methods. OSR defines similar inputs not just by small ℓ2 distances in input space, but also by small differences in prediction logits (bounded by δ). To generate these similar inputs, the authors develop a GAN-based method that produces inputs with prediction logits within a specified bound δ of the original. The GAN generator is trained with a loss function that includes both traditional adversarial loss and a logit-constraining term. The metric is evaluated across three datasets (MNIST, CIFAR10, chest X-rays) and compared against existing metrics like sensitivity and robustness-Sr.

## Key Results
- OSR provides consistent attribution method rankings across models with varying accuracy, unlike existing metrics which show significant variation based on model accuracy
- The proposed GAN-based method effectively generates diverse similar inputs while maintaining class consistency (50-85% misclassification rate for noise-based methods vs. GAN)
- Sensitivity and robustness-Sr rankings vary significantly between well-trained and poorly-trained models, while OSR rankings remain stable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Defining input similarity via prediction logit proximity rather than only input-space distance decouples attribution evaluation from model training quality.
- **Mechanism:** Traditional metrics use ℓp-norm balls around inputs, implicitly assuming the model treats nearby inputs similarly. This conveys bias: poorly-trained models have irregular decision boundaries, causing existing metrics to rank methods inconsistently. By constraining |Fy(x) - Fy(e_x)| ≤ δ, the metric evaluates attributions only on inputs the model itself considers functionally equivalent, making robustness assessment independent of how well the model learned the data distribution.
- **Core assumption:** The prediction logit for the predicted class is a sufficient proxy for the model's internal notion of input equivalence; other logit dimensions or internal representations do not define similarity meaningfully for this purpose.

### Mechanism 2
- **Claim:** GAN-based generation with a logit-constraining loss approximates the distribution of valid similar inputs more completely than noise injection or adversarial perturbation methods.
- **Mechanism:** Standard perturbation (uniform/Gaussian noise) produces images with concentrated ℓp distances but high misclassification rates (50-85% in experiments). Adversarial methods search within ℓp balls, inheriting adversarial robustness bias. The proposed GAN generator G(z) is trained with loss term Ly that penalizes |Fy(x) - Fy(G(z)) - δ| when the logit gap is below δ, plus cross-entropy to preserve class. This enforces output-space constraints directly, allowing input-space distances to emerge naturally rather than being pre-specified.
- **Core assumption:** A GAN trained on ~1500 copies of a single image can converge to a meaningful approximation of the set S in reasonable time without mode collapse; the discriminator signal plus Ly term is sufficient to explore diverse similar inputs.

### Mechanism 3
- **Claim:** OSR provides consistent attribution method rankings across models with varying accuracy because it evaluates explanation stability within the model's own functional equivalence class.
- **Mechanism:** Metrics like sensitivity (µ_M) and robustness-Sr inherit model accuracy bias: they rank methods differently on well-trained vs. badly-trained models because their "similar" inputs are defined by dataset/visual proximity, not model behavior. OSR computes E[||g(x) - g(e_x)||_2] over GAN-generated inputs with δ-close logits. Since these inputs are, by construction, functionally equivalent from the model's perspective, attribution differences reflect method properties rather than model pathologies.
- **Core assumption:** Attribution methods that are robust on well-trained models should also be robust on poorly-trained models when evaluated within each model's own functional equivalence regions; cross-model ranking consistency is a valid desideratum.

## Foundational Learning

- **Concept: Feature attribution methods (Saliency, Integrated Gradients, DeepLIFT, LRP)**
  - **Why needed here:** The paper evaluates eight attribution methods; understanding what each computes (gradients, layer-wise relevance, integrated paths) is essential to interpret robustness differences.
  - **Quick check question:** Given input x and model f, what does Saliency compute vs. Integrated Gradients?

- **Concept: Adversarial robustness vs. attributional robustness**
  - **Why needed here:** The central argument is that existing metrics conflate these; distinguishing model vulnerability to input perturbations from explanation vulnerability is prerequisite.
  - **Quick check question:** If a model is adversarially robust, does that guarantee its attributions are robust? Why or why not?

- **Concept: GAN training dynamics (generator-discriminator equilibrium, mode collapse)**
  - **Why needed here:** The method trains per-image GANs; understanding convergence, loss landscapes, and failure modes is needed to assess scalability and reliability.
  - **Quick check question:** What happens to generated sample diversity if the discriminator becomes too strong too quickly?

## Architecture Onboarding

- **Component map:**
  ```
  Input Image x → Pre-trained Classifier f (frozen) → Prediction logit Fy(x)
                              ↓
  Noise z → Generator G(z) → Generated image e_x → Classifier f → Fy(e_x)
                              ↓                          ↓
                         Discriminator D           Logit comparison |Fy(x) - Fy(e_x)|
                              ↓                          ↓
                    Real/Fake loss LD            Bounded loss Ly (Eq. 4) + Cross-entropy
                              ↓
                    Combined generator loss (Eq. 5)
                              ↓
  Generated e_x samples → Attribution method g → g(e_x)
                              ↓
                    Compare ||g(x) - g(e_x)||_2
                              ↓
                    OSR = E[||g(x) - g(e_x)||_2]
  ```

- **Critical path:**
  1. Train per-image GAN with Ly constraint until generated images have δ-close logits
  2. Generate N samples from converged GAN
  3. Compute attributions g(x) and g(e_x) for each sample
  4. Average ℓ2 distances to obtain OSR(x); aggregate across dataset

- **Design tradeoffs:**
  - δ selection: Higher δ explores broader similar-input space but risks crossing decision boundaries; paper uses δ=5.0 and notes cross-entropy loss prevents misclassification even with large δ
  - ρ constraint: Not explicitly enforced in GAN training; implicit through training on copies of x
  - Per-image GAN vs. shared GAN: Per-image ensures quality but doesn't scale; trade-off not analyzed
  - GAN architecture depth: 6 linear layers with LeakyReLU and dropout—deeper architectures may improve sample quality but increase training time

- **Failure signatures:**
  - GAN mode collapse: Generated images are near-identical; OSR underestimates true variance
  - Discriminator dominance: Generator fails to produce diverse samples; Ly term ignored
  - Ly term too weak: Generated images drift in logit space; class flips occur
  - Ly term too strong: Generator produces near-exact copies of x; OSR artificially low

- **First 3 experiments:**
  1. **Sanity check on single image:** Train GAN for one MNIST digit with δ=5.0, visualize 50 generated samples, verify all maintain predicted class and show logit distances within bound; compute OSR for Saliency and Integrated Gradients to verify pipeline.
  2. **Noise baseline comparison:** For same image, generate 1000 samples via uniform noise U(-0.05, 0.05) and GAN; plot ℓ2 input distances and logit distances; confirm GAN produces wider input distribution with constrained outputs (replicate Fig. 1, 2).
  3. **Cross-accuracy ranking test:** Train two CIFAR10 models (high/low accuracy); compute OSR, sensitivity, and fidelity for three attribution methods; verify OSR rankings are consistent while other metrics diverge (replicate Fig. 9 pattern).

## Open Questions the Paper Calls Out

- **Open Question 1:** Is there an inherent trade-off between attributional robustness (as measured by OSR) and fidelity?
  - **Basis in paper:** Section 5.3 notes that SmoothGrad exhibits high robustness but low fidelity, while Integrated Gradients shows the opposite, "rais[ing] the question of whether there is a trade-off between robustness and fidelity."
  - **Why unresolved:** The paper observes this inverse relationship empirically but does not provide a theoretical explanation or proof of necessity.
  - **What evidence would resolve it:** A theoretical analysis establishing whether maximizing one property inevitably degrades the other, or empirical results showing a method can maximize both simultaneously.

- **Open Question 2:** How should the bounds on input differences ($\rho$) and output logits ($\delta$) be theoretically related to define optimal similarity?
  - **Basis in paper:** The Conclusion encourages future work to "explore the relationship between bounded inputs and bounded outputs concerning similarity for attributional robustness."
  - **Why unresolved:** The current approach selects bounds (e.g., $\delta=5.0$) empirically to ensure image quality, lacking a formal theoretical framework linking the input and output spaces.
  - **What evidence would resolve it:** A mathematical derivation defining the optimal ratio or functional relationship between input $\ell_2$ distance and logit difference for various model architectures.

- **Open Question 3:** Can the GAN-based generation of similar inputs be scaled efficiently or adapted for non-image modalities like text or tabular data?
  - **Basis in paper:** The method relies on training a "distinct GAN for each input," which is computationally expensive, and the experiments are restricted to image datasets (MNIST, CIFAR, X-ray).
  - **Why unresolved:** Training a generative model per sample is resource-intensive, and the specific loss functions (pixel-space $\ell_2$ norm) may not translate to discrete or sparse data structures.
  - **What evidence would resolve it:** Demonstrating a modified OSR framework that uses a single global generator or a model-agnostic perturbation method effective on non-image data.

## Limitations
- Per-image GAN training approach raises scalability concerns for real-world applications with high-dimensional inputs or large datasets
- The choice of δ=5.0 as a logit threshold is not rigorously justified and could significantly impact OSR values and attribution method rankings
- The method assumes the prediction logit for the predicted class is a sufficient proxy for model similarity, potentially missing important information in other logit dimensions

## Confidence
- **High confidence**: The core mechanism of decoupling attribution evaluation from model accuracy through output-space constraints is well-supported by experimental evidence
- **Medium confidence**: The GAN-based generation method effectively produces diverse similar inputs while maintaining class consistency, though scalability remains an open question
- **Medium confidence**: The claim that OSR provides more objective evaluation than existing metrics is supported by experiments but would benefit from additional datasets and attribution methods

## Next Checks
1. **Scalability test**: Evaluate OSR computation time on a larger dataset (e.g., ImageNet subset) and analyze how per-image GAN training time scales with input resolution and model complexity
2. **δ sensitivity analysis**: Systematically vary δ across a range of values (e.g., 1.0 to 10.0) and measure the impact on OSR values and attribution method rankings to establish sensitivity to this hyperparameter
3. **Cross-dataset generalization**: Apply OSR to a domain significantly different from the three tested datasets (e.g., medical imaging beyond chest X-rays, or time series data) to validate the metric's general applicability across diverse input modalities