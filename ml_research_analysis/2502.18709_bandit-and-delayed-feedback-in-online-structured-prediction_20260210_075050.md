---
ver: rpa2
title: Bandit and Delayed Feedback in Online Structured Prediction
arxiv_id: '2502.18709'
source_url: https://arxiv.org/abs/2502.18709
tags:
- loss
- theorem
- bound
- feedback
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops algorithms for online structured prediction\
  \ under bandit and delayed feedback, extending prior work on full-information settings.\
  \ The authors propose gradient estimators tailored to weaker feedback: an inverse-weighted\
  \ estimator for bandit feedback yielding O(\u221AKT) surrogate regret, and a pseudo-inverse\
  \ matrix estimator for O(T^{2/3}) regret independent of K."
---

# Bandit and Delayed Feedback in Online Structured Prediction

## Quick Facts
- arXiv ID: 2502.18709
- Source URL: https://arxiv.org/abs/2502.18709
- Reference count: 40
- Key outcome: Achieves O(√KT) surrogate regret with inverse-weighted estimator and O(T^{2/3}) regret with pseudo-inverse matrix estimator for bandit feedback, plus O(D+1) and O(√((K+D)T)) bounds for delayed feedback settings.

## Executive Summary
This paper addresses online structured prediction under bandit and delayed feedback, extending prior work on full-information settings. The authors propose gradient estimators tailored to weaker feedback: an inverse-weighted estimator for bandit feedback yielding O(√KT) surrogate regret, and a pseudo-inverse matrix estimator for O(T^{2/3}) regret independent of K. They also analyze delayed feedback settings, achieving bounds of O(min{D^2+1,(D+1)^{2/3}T^{1/3}}) and O(D+1) under full information, and O(√((K+D)T)) under bandit feedback. Theoretical results are supported by experiments on multiclass and multilabel classification, showing empirical alignment with theoretical predictions, particularly the benefit of the K-independent bound for large output spaces.

## Method Summary
The paper develops algorithms for online structured prediction under bandit and delayed feedback by extending full-information online learning to weaker feedback scenarios. For bandit feedback, it proposes two gradient estimators: an inverse-weighted estimator achieving O(√KT) surrogate regret, and a pseudo-inverse matrix estimator achieving O(T^{2/3}) regret independent of K. For delayed feedback, it introduces ODAFTRL (optimistic FTRL) and BOLD (parallel instances) algorithms that handle feedback lags. The method uses Online Gradient Descent with adaptive learning rates and decoding via Randomized Decoding with Uniform Exploration (RDUE), processing structured outputs through Fenchel-Young losses.

## Key Results
- Achieves O(√KT) surrogate regret with inverse-weighted estimator for bandit feedback
- Achieves O(T^{2/3}) surrogate regret with pseudo-inverse matrix estimator (K-independent)
- Achieves O(D+1) regret for delayed full-information with fixed delay D
- Achieves O(√((K+D)T)) surrogate regret for delayed bandit feedback
- Experiments validate theoretical bounds on MNIST and synthetic multilabel data

## Why This Works (Mechanism)

### Mechanism 1: Variance-Controlled Gradient Estimation in Bandit Settings
The paper achieves efficient online structured prediction under bandit feedback by constructing unbiased gradient estimators whose variance can be controlled to balance exploration and exploitation. It uses inverse-propensity weighting with uniform exploration (RDUE) to ensure all outcomes are observed, estimating gradients as $\hat{G}_t = \frac{1[\hat{y}_t = y_t]}{p_t(y_t)} G_t$. To avoid O(√KT) dependence for large K, it proposes a pseudo-inverse estimator based on the Moore-Penrose pseudo-inverse of the second-moment matrix P_t, exploiting the algebraic structure of SELF losses.

### Mechanism 2: Bridging Delayed Feedback and Regret via Adaptivity
Regret under delayed feedback can be bounded independently of T if delay D is fixed and the algorithm processes feedback optimistically or maintains parallel instances. ODAFTRL computes updates using currently available gradients while regularizing for missing gradients, while BOLD runs D+1 independent online learners such that each instance always has fresh data. This achieves O(D+1) regret bounds for fixed delays.

### Mechanism 3: Surrogate Regret as a Performance Proxy
Minimizing surrogate regret (gap between target loss and best surrogate loss) allows finite bounds even when standard regret would be unbounded over time. Instead of competing with best predictor in 0-1 error, the learner competes with best predictor in convex surrogate loss (e.g., logistic loss). Because surrogate loss is convex, online gradient descent can efficiently track the best predictor, bounding the gap between actual target loss and idealized surrogate performance.

## Foundational Learning

**Concept: Online Convex Optimization (OCO) with Bandit Feedback**
- Why needed: The paper extends OCO to structured prediction where loss function is not fully observed, requiring gradient estimation from point feedback.
- Quick check: Can you explain why a standard gradient descent step requires an unbiased estimate of the gradient, and how inverse propensity scoring provides this?

**Concept: Fenchel-Young Losses & Structured Prediction**
- Why needed: The paper treats structured outputs (multiclass, ranking) as vectors in convex hull, using Fenchel-Young losses to compute gradients on these structures.
- Quick check: How does the regularized prediction function $\hat{y}_\Omega(\theta)$ relate the score vector to the output space Y?

**Concept: Delayed Feedback in Online Learning**
- Why needed: The paper fundamentally alters update rules to account for D-step lags in feedback.
- Quick check: In a delayed feedback loop, if delay D=5, when does the algorithm receive the gradient for the decision made at t=10?

## Architecture Onboarding

**Component map:**
Input x_t -> Linear Estimator W_t -> Decoder (RDUE) -> Prediction $\hat{y}_t$ -> Feedback Module -> Gradient Estimator -> Optimizer -> Output W_{t+1}

**Critical path:** The correct implementation of the Gradient Estimator, particularly the construction of the pseudo-inverse matrix P_t^+ from the decoding distribution for the K-independent bound. If P_t is singular or estimated poorly, the update fails.

**Design tradeoffs:**
1. **Inverse-Weighted vs. Pseudo-Inverse:**
   - Inverse-Weighted: Use if K is small; better scaling with T (O(√T))
   - Pseudo-Inverse: Use if K is massive (e.g., ranking); worse scaling with T (O(T^{2/3})) but removes K dependency
2. **BOLD vs. ODAFTRL (for Delays):**
   - BOLD: Better for fixed delays, conceptually simpler (parallel instances)
   - ODAFTRL: Better for variable delays or optimistic scenarios

**Failure signatures:**
- **Exploding Gradients:** If RDUE exploration probability q is too small, inverse-weighted estimator involves dividing by near-zero probabilities
- **Stagnation:** If pseudo-inverse estimator used on loss that doesn't satisfy SELF* properties (V not invertible), gradient estimate is biased, causing convergence to poor solutions
- **Memory Blowup:** BOLD algorithm requires O(D) memory copies of model state; for massive delays, this is costly

**First 3 experiments:**
1. **Sanity Check (Bandit Multiclass):** Run inverse-weighted estimator on MNIST against existing baselines (Gaptron); expect comparable or better error rates
2. **Scaling Test (Multilabel/Ranking):** Generate synthetic data with increasing label dimension d; compare Inverse-Weighted vs. Pseudo-Inverse; expect Pseudo-Inverse to win as d increases
3. **Stress Test (Delayed Bandit):** Introduce fixed delay D into bandit feedback loop; verify regret increases by roughly O(√D) or O(D^{1/3}) rather than linearly with D

## Open Questions the Paper Calls Out

**Open Question 1:** What are the corresponding lower bounds for the bandit feedback setting, and are the proposed O(√KT) and O(T^{2/3}) surrogate regret upper bounds tight?
- Basis: Section 6 explicitly states investigating corresponding lower bounds in bandit feedback setting is important direction for future work
- Why unresolved: While Ω(√T) lower bound exists for graph-feedback settings, it has not been proven for specific bandit feedback settings analyzed
- What evidence would resolve: A formal proof of surrogate regret lower bound (e.g., Ω(√T)) specifically for online structured prediction under bandit feedback

**Open Question 2:** Is the O(√((K+D)T)) surrogate regret bound for delayed bandit feedback optimal, particularly regarding dependence on delay D?
- Basis: Section 5.1 states "Whether this surrogate regret upper bound is optimal remains open"
- Why unresolved: Unclear if additive O(√DT) cost incurred by delay is necessary theoretical requirement or artifact of specific algorithmic analysis
- What evidence would resolve: A matching lower bound of Ω(√((K+D)T)) or improved algorithm achieving regret bound without √D factor

**Open Question 3:** Can the surrogate regret for K-independent algorithms (using pseudo-inverse matrix estimator) be improved from O(T^{2/3}) to O(√T)?
- Basis: Section 3.4 achieves O(T^{2/3}) to eliminate dependence on K, whereas Section 3.3 achieves O(√T) with K dependence
- Why unresolved: The paper presents trade-off where removing K dependence degrades time dependence to T^{2/3}; unclear if this rate is inherent to problem class or just proposed estimator
- What evidence would resolve: An algorithm achieving O(√T) surrogate regret with no dependence on K, or proof that any K-independent algorithm must suffer at least T^{2/3} regret

## Limitations

- The analysis critically depends on SELF (or SELF*) structure of target loss and invertibility of matrix V for K-independent bound
- Experimental validation is limited to specific synthetic and benchmark datasets (MNIST, synthetic multilabel)
- Performance under real-world structured prediction tasks with complex dependency structures remains unclear
- Practical robustness of estimators to violations (near-singular V, non-SELF losses) is not empirically tested

## Confidence

**High Confidence:** The fundamental mechanisms of inverse-propensity weighting for bandit gradient estimation and use of parallel instances for fixed-delay learning are well-established in broader literature. Theoretical regret bounds derived under stated assumptions are internally consistent.

**Medium Confidence:** The novel K-independent pseudo-inverse estimator and associated regret bound are theoretically sound within SELF* framework. However, practical sensitivity to precise algebraic structure of V and numerical stability of pseudo-inverse computation introduce uncertainty.

**Low Confidence:** The efficacy of surrogate regret framework as true performance proxy for final structured prediction task when gap between surrogate and target loss is non-trivial.

## Next Checks

1. **Structure Robustness Test:** Apply K-independent estimator to structured prediction problem where SELF* conditions are only approximately satisfied (loss function close to SELF but with near-singular V). Measure degradation in regret bounds and prediction accuracy compared to theoretically clean case.

2. **Real-World Dataset Validation:** Evaluate delayed bandit algorithm (ODAFTRL) on real-world dataset with naturally occurring delays (e.g., user feedback on online content with variable response times). Compare empirical regret growth against theoretical O(√((K+D)T)) bound.

3. **Numerical Stability Audit:** Implement stress test where pseudo-inverse matrix P_t^+ is computed for range of decoding distributions. Identify threshold at which numerical instability (very large eigenvalues in V^{-1}) leads to exploding gradients and quantify impact on convergence.