---
ver: rpa2
title: 'Working with AI: Measuring the Applicability of Generative AI to Occupations'
arxiv_id: '2507.07935'
source_url: https://arxiv.org/abs/2507.07935
tags:
- user
- work
- occupations
- applicability
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study measures the applicability of generative AI to occupations
  using real-world usage data from Microsoft Bing Copilot. The researchers analyzed
  200,000 anonymized conversations to identify work activities assisted or performed
  by AI, mapping them to ONET's hierarchical occupation-task framework.
---

# Working with AI: Measuring the Applicability of Generative AI to Occupations

## Quick Facts
- arXiv ID: 2507.07935
- Source URL: https://arxiv.org/abs/2507.07935
- Reference count: 40
- This study measures AI applicability to occupations using real Bing Copilot conversations mapped to O*NET tasks

## Executive Summary
This research measures how generative AI applies to different occupations by analyzing 200,000 anonymized Bing Copilot conversations. The study maps AI usage patterns to standardized occupational frameworks (O*NET), finding that AI is most commonly used for information work activities like learning, communicating, teaching, and writing. The analysis reveals that AI performs best at communication and teaching tasks but struggles with image generation and data analysis. Most occupations show some AI applicability due to their information work components, though the impact varies significantly. The study introduces an AI applicability score distinguishing between occupations where AI assists existing workflows versus those where tasks might be delegated to AI.

## Method Summary
The researchers analyzed 200,000 anonymized US Bing Copilot conversations from January to September 2024, using two samples: 100,000 uniform conversations and 100,000 with thumbs feedback. They employed a two-stage GPT-4o pipeline to classify user goals and AI actions into O*NET Intermediate Work Activities (IWAs), then aggregated these to occupation-level AI applicability scores. The methodology maps conversations to 332 IWAs, computes activity share, task completion rates, and scope metrics, then weights these by occupation-IWA importance to produce final scores. Human validation on 195 conversations showed moderate inter-annotator agreement (κ≈0.4-0.5) for IWA matching.

## Key Results
- AI is most commonly used for information work activities: creating, processing, and communicating information
- AI performs best at communication and teaching tasks but struggles with image generation and data analysis
- The study introduces an AI applicability score distinguishing between AI assistance versus task delegation across occupations

## Why This Works (Mechanism)
The study's mechanism relies on mapping real-world AI usage patterns to standardized occupational frameworks. By analyzing actual user-AI interactions rather than theoretical capabilities, the research captures genuine workplace applications. The two-stage classification pipeline first summarizes user goals and AI actions into IWA-style descriptions, then systematically classifies all 332 IWAs using similarity ranking. This approach grounds AI applicability in observable behavior rather than speculation, providing empirical evidence about where AI is most useful in practice.

## Foundational Learning
- **O*NET Intermediate Work Activities**: Standardized hierarchical task framework mapping occupations to specific work activities; needed for consistent occupational classification across all analysis
- **Activity share metric**: Fraction of conversation volume attributed to each IWA; quick check: should sum to ~1 across all IWAs per conversation
- **AI applicability score**: Weighted sum combining coverage, completion, and scope metrics; quick check: values should range from 0-1 for individual occupations
- **GPT-4o classification pipeline**: Two-stage process using similarity ranking and batch classification; quick check: prompt structure must match Appendix C exactly
- **Physical vs. information work distinction**: Critical for understanding AI's limitations; quick check: physical task classifier should show low applicability for manual labor occupations
- **Scope metric**: 6-point ordinal scale measuring how completely AI handles tasks; quick check: expect low correlation with activity share (r≈0.5 per paper)

## Architecture Onboarding

**Component map:**
O*NET database -> Classification pipeline -> IWA metrics -> Occupation scores -> Validation

**Critical path:**
Raw conversations → GPT-4o classification → IWA mapping → Activity share computation → Final applicability scores

**Design tradeoffs:**
- Uses real usage data vs. theoretical capabilities (tradeoff: sampling bias for representativeness)
- GPT-4o classification vs. manual coding (tradeoff: scalability vs. potential classification errors)
- Binary IWA classification vs. multi-label (tradeoff: simplicity vs. potential loss of nuance)

**Failure signatures:**
- Low correlation with published results (expected r=0.73 with Eloundou et al.)
- IWA classification disagreement with expected patterns
- Occupation scores that don't align with known occupational characteristics

**First 3 experiments:**
1. Validate classification pipeline on 10 sample conversations, checking IWA mapping accuracy
2. Compute IWA-level metrics for a small occupation subset and compare with expected patterns
3. Aggregate to occupation scores and verify basic ranking aligns with occupational knowledge

## Open Questions the Paper Calls Out
None

## Limitations
- Bing Copilot usage data may not represent all generative AI tools or workplace environments
- GPT-4o-based classification depends heavily on prompt engineering and may not generalize
- Physical task classification reportedly used GPT-5, unavailable for replication as of paper date

## Confidence
- **High confidence**: AI commonly used for information work activities; distinction between assistance and delegation is observable
- **Medium confidence**: Specific applicability scores for individual occupations; ranking of most affected occupational categories
- **Low confidence**: Generalization to other AI systems beyond Bing Copilot; temporal stability of findings

## Next Checks
1. Validate classification pipeline on independent AI conversations from different platforms (ChatGPT, Claude)
2. Replicate analysis with GPT-4o replacing unavailable GPT-5 for physical task classification
3. Test stability of AI applicability scores over time using subsequent conversation data