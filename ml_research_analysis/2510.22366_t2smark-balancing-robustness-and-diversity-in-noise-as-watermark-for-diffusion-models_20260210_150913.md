---
ver: rpa2
title: 'T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion
  Models'
arxiv_id: '2510.22366'
source_url: https://arxiv.org/abs/2510.22366
tags:
- uni00000013
- uni00000011
- t2smark
- uni00000003
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T2SMark addresses the challenge of balancing watermark robustness
  and generation diversity in diffusion models. The proposed method employs a two-stage
  framework built on Tail-Truncated Sampling (TTS), which embeds watermark bits in
  the reliable tail regions of the Gaussian noise distribution while sampling the
  central region randomly to maintain diversity.
---

# T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models

## Quick Facts
- arXiv ID: 2510.22366
- Source URL: https://arxiv.org/abs/2510.22366
- Reference count: 40
- Primary result: T2SMark achieves near-perfect watermark detection and traceability while maintaining generation diversity comparable to unwatermarked models

## Executive Summary
T2SMark addresses the fundamental trade-off between watermark robustness and generation diversity in diffusion models by introducing a two-stage framework built on Tail-Truncated Sampling (TTS). The method embeds watermark bits exclusively in the reliable tail regions of Gaussian noise distributions while sampling the central region randomly, creating a signal-to-noise ratio enhancement that improves robustness. The two-stage hierarchical keying introduces random session keys to prevent fixed codeword generation patterns, thereby preserving the statistical diversity of model outputs without sacrificing detection performance.

## Method Summary
T2SMark employs a two-stage Noise-as-Watermark framework that embeds watermark bits using Tail-Truncated Sampling (TTS). The first stage encrypts a random session key under a master key, and the second stage uses this session key to encrypt the actual watermark bits. TTS creates a "dead zone" in the central region of the Gaussian distribution where no bits are encoded, while bits are embedded only in high-energy tail regions. This increases the signal-to-noise ratio by concentrating energy in robust regions that resist noise-induced flipping. The method relies on deterministic ODE samplers (DDIM) for inversion-based extraction, using L1 norm of projection vectors as a detection statistic.

## Key Results
- Near-perfect detection rates: TPR 1.000 (U-Net) and 0.998 (DiT) at FPR 10⁻⁶
- High traceability: bit accuracy 1.0000 (U-Net) and 0.9754 (DiT)
- Maintained diversity: LPIPS 0.7069 vs 0.7072 (unwatermarked), preserving generation variety
- No visual quality degradation across multiple image quality metrics

## Why This Works (Mechanism)

### Mechanism 1: Tail-Truncated Sampling (TTS) for SNR Enhancement
Embedding watermark bits exclusively in reliable tail regions of Gaussian distributions increases SNR by avoiding low-energy values near zero that are easily flipped by noise. This creates a robust watermark-encoding subspace of larger-magnitude vectors.

### Mechanism 2: Two-Stage Hierarchical Keying for Diversity Preservation
A random session key prevents fixed codeword generation patterns by ensuring the final noise vector structure varies significantly even for the same user/message, maintaining statistical diversity of outputs.

### Mechanism 3: L1 Norm Detection Statistic
The L1 norm of projection vectors serves as a robust confidence metric for detection, separating watermarked from unwatermarked images more effectively than L2 distance due to concentrated energy in tails.

## Foundational Learning

- **Concept: Diffusion Inversion (DDIM)**
  - Why needed: T2SMark is inversion-based; you cannot extract the watermark without reversing the image back to latent noise
  - Quick check: Can you explain why a deterministic ODE sampler allows exact inversion while a stochastic Markov chain does not?

- **Concept: Orthogonal Subspaces**
  - Why needed: The method splits the noise vector into subspaces for different bits; orthogonality ensures decoding one bit doesn't interfere with another
  - Quick check: If two bit-vectors shared support, how would that affect decoding if both bits were set?

- **Concept: LPIPS (Learned Perceptual Image Patch Similarity)**
  - Why needed: The paper's main selling point is maintaining "diversity"; LPIPS measures perceptual difference between generated images
  - Quick check: If a watermarking method forces initial noise to be nearly identical, would LPIPS between output images increase or decrease?

## Architecture Onboarding

- **Component map:**
  - Input: Random Session Key, Master Key, Watermark Message
  - Embedding Path: PRNG (orthogonal hyperplanes) → TTS Sampler (noise with tails/center) → Diffusion (forward/reverse pass)
  - Extraction Path: Inversion (DDIM) → Projection → Stage 1 Decode (Session Key) → Stage 2 Decode (Message)

- **Critical path:**
  - Threshold τ = 0.674 (empirically optimized); higher τ = more robust but potentially less diversity
  - 10-step DDIM inversion baseline; fewer steps may break robustness

- **Design tradeoffs:**
  - Robustness vs. Capacity: Bit accuracy drops from 97.5% (256 bits) to 87.9% (1024 bits)
  - Diversity vs. Complexity: Two-stage framework required for diversity but introduces failure mode if Stage 1 fails

- **Failure signatures:**
  - Gaussian Noise Attacks: Vulnerability at σ ≥ 0.1 with bit accuracy dropping to ~0.89
  - Forgery Attacks: Proxy model access enables noise vector theft and watermark reuse

- **First 3 experiments:**
  1. Reproduce truncation threshold sweep (τ ∈ [0, 1.0]) plotting bit accuracy vs. LPIPS diversity
  2. Run GauNoise attack with σ=0.05 and σ=0.1 to verify breaking point
  3. Generate 10 images for same prompt using T2SMark vs. Gaussian Shading to confirm diversity differences

## Open Questions the Paper Calls Out

### Open Question 1
Can T2SMark be defended against forgery attacks where adversaries use proxy models to invert and reuse the watermarked noise? Section 5 states high robustness enables adversaries to recover initial noise vectors and generate forged images with valid watermarks.

### Open Question 2
How can the "subtle distributional anomalies" caused by session key embedding strategy be minimized? Section 5 identifies that embedding the key in truncated tails introduces detectable energy concentrations.

### Open Question 3
Can T2SMark be extended to support non-invertible or stochastic samplers? Section 5 notes dependency on ODE-based sampling limits applicability to models supporting this inversion.

### Open Question 4
Can the framework's significant vulnerability to Gaussian noise attacks be mitigated? Table 10 and Section 4.5 show performance degrades dramatically under Gaussian noise due to inversion sensitivity.

## Limitations
- Significant vulnerability to Gaussian noise attacks (σ ≥ 0.1) represents a practical limitation without full resolution
- Forgery attack vector remains a theoretical concern without proposed mitigation strategies
- Exact PRNG algorithm for generating orthogonal vectors from keys is not specified

## Confidence
- **High Confidence**: Detection performance metrics (TPR 1.000/0.998, FPR 1e-6) and generation diversity results (LPIPS 0.7069 vs 0.7072)
- **Medium Confidence**: Two-stage framework's contribution to diversity (Table 17 ablation) needs further validation
- **Medium Confidence**: Claim that L1 norm detection is superior to L2 distance lacks comparative analysis

## Next Checks
1. Reproduce the truncation threshold sweep: Generate images across τ ∈ [0, 1.0] and plot bit accuracy vs. LPIPS diversity to verify the Pareto frontier
2. Gaussian noise attack validation: Implement GauNoise attacks with σ = 0.05 and σ = 0.1 to empirically determine breaking point
3. Visual diversity confirmation: Generate 10 images for identical prompts using T2SMark vs. Gaussian Shading to visually verify claimed diversity differences