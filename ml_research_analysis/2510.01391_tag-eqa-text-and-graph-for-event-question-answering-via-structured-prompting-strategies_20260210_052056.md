---
ver: rpa2
title: 'TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting
  Strategies'
arxiv_id: '2510.01391'
source_url: https://arxiv.org/abs/2510.01391
tags:
- event
- text
- causal
- graphs
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models often struggle with event-based questions
  requiring causal or temporal reasoning. TAG-EQA introduces a prompting framework
  that embeds structured causal event graphs into LLM inputs by converting relations
  into natural language statements.
---

# TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies

## Quick Facts
- arXiv ID: 2510.01391
- Source URL: https://arxiv.org/abs/2510.01391
- Reference count: 13
- Primary result: Large language models improve event-based QA accuracy by 5% average with structured causal graph prompts, up to 18% with graph-augmented chain-of-thought

## Executive Summary
TAG-EQA addresses the challenge of event-based question answering by embedding structured causal event graphs into LLM prompts through natural language serialization. The framework evaluates nine configurations combining zero-shot, few-shot, and chain-of-thought strategies with text-only, graph-only, and combined inputs. Results show consistent accuracy improvements over text-only baselines, with gains reaching 12% in zero-shot settings and 18% with graph-augmented chain-of-thought prompting. The approach is particularly effective for causal and temporal question types, though performance varies significantly across model architectures.

## Method Summary
The framework serializes causal event graphs (ENABLES/BLOCKS relations) into natural language statements and integrates them into LLM prompts across three strategies (Zero-shot, Few-shot, Chain-of-Thought) and three modalities (Text-only, Graphs-only, TAG-combined). Evaluation uses the TORQUESTRA benchmark with 477,549 QA instances, testing three models (T5-XXL, Qwen-32B, GPT-3.5/4o) with greedy decoding. The approach requires no fine-tuning, instead leveraging in-context learning and structured prompt engineering to surface causal dependencies that text-only inputs miss.

## Key Results
- Average 5% accuracy improvement over text-only baselines across all models and configurations
- Up to 12% gains in zero-shot settings and 18% with graph-augmented chain-of-thought prompting
- Qwen-32B shows strongest performance, achieving 79.4% accuracy with few-shot graph prompting
- Graphs-only inputs consistently outperform text-only across models (QwQ: 66.8% → 78.8%)

## Why This Works (Mechanism)

### Mechanism 1
Serializing causal event graphs into natural language statements improves LLM event reasoning without fine-tuning. Graph edges (ENABLES/BLOCKS) are converted into topologically ordered sentences (e.g., "Event A enables Event B"), which LLMs can process as structured context. This surfaces implicit dependencies that text-only inputs often miss. Core assumption: LLMs can leverage structured relations when presented in natural language form, even without explicit graph encoders. Evidence: 5% average accuracy improvement over text-only baselines, with 12% gains in zero-shot settings.

### Mechanism 2
Chain-of-thought prompting combined with causal graphs amplifies reasoning gains for models capable of multi-step inference. CoT prompts include step-by-step reasoning traces that reference graph edges explicitly (e.g., "The event X blocks Y, therefore..."), aligning intermediate reasoning with causal structure. Core assumption: Models must be capable of integrating reasoning traces with structured context; not all architectures handle this well. Evidence: Up to 18% gains with graph-augmented chain-of-thought prompting, particularly effective for Qwen-32B.

### Mechanism 3
Few-shot demonstrations amplify graph utility when aligned with input modality, but model architecture determines success. In-context examples show how to map causal structure to answers. When examples match input type (e.g., Graph-only demos for Graph-only prompts), models learn the mapping more effectively. Core assumption: Demonstration format must match input modality; mismatched examples reduce effectiveness. Evidence: Qwen-32B achieves highest score (79.4%) with Few+TAG, confirming demonstrations and graph input are complementary.

## Foundational Learning

- **Causal Event Graphs (ENABLE/BLOCK relations)**: The framework relies on understanding that ENABLES edges denote prerequisite conditions while BLOCKS edges denote preventive/inhibitory relations. Without this, graph serialization becomes meaningless. Quick check: Given events A, B, C where A→B and C⊣D, what does the ⊣ relation imply about C and D?

- **Prompt Engineering Strategies (Zero/Few/CoT)**: The 3×3 configuration matrix requires distinguishing between no examples (Zero), in-context demonstrations (Few), and explicit reasoning traces (CoT). Quick check: What additional component does CoT add beyond Few-shot prompting?

- **Topological Ordering for Causal Flow**: Graph edges are serialized in topological order to "preserve causal flow and reduce reference distance." Random ordering would break dependency chains. Quick check: Why might topologically sorting edges before serialization improve LLM comprehension?

## Architecture Onboarding

- **Component map**: Input (Passage P + Question Q + Causal Graph G) → Graph Serializer (converts edges to sentences, topologically ordered) → Prompt Assembler (combines instruction + modality + strategy) → LLM (T5-XXL, Qwen-32B, GPT-3.5/4o) → Answer Extractor (regex for "Therefore, the final answer is: <yes/no>")

- **Critical path**: 1) Load TORQUESTRA instance (passage, question, gold graph) 2) Serialize graph edges to natural language (one sentence per edge, topological order) 3) Assemble prompt based on 3×3 configuration (strategy × modality) 4) Call LLM with temperature=0 5) Extract answer via regex, fallback to first yes/no token

- **Design tradeoffs**: Text vs. Graph vs. TAG: TAG provides most information but increases prompt length (336.8 avg tokens vs. 95.2 for Zero-Text) and can introduce conflicts for some models. Zero vs. Few vs. CoT: Few most reliable overall; CoT best for Qwen but harmful for T5. Gold vs. auto-extracted graphs: Current results use gold annotations (upper bound); robustness to noise unknown.

- **Failure signatures**: T5 + TAG + CoT: Accuracy drops to 50.4% (lowest configuration)—model cannot integrate multiple modalities with reasoning traces. GPT-4o + TAG: Shows minimal benefit over Text-only (70.6% vs. 72.3%)—insensitivity to structured input in CoT setting. All models on "possible"/"unknown" categories: Consistently weak performance across configurations—ambiguous questions remain unsolved.

- **First 3 experiments**: 1) Replicate Zero-Text vs. Zero-Graph on 100-instance subset with Qwen-32B to validate ~12% gain claim before full evaluation. 2) Test T5 on Few+Text vs. Few+TAG to confirm modality integration failure pattern (expect ~7% drop per Table 8). 3) Evaluate Qwen with corrupted graphs (random edge permutation, 20% edge removal) to assess robustness beyond gold annotations—critical for real-world deployment.

## Open Questions the Paper Calls Out

- **Graph quality impact**: How does TAG-EQA performance degrade when using automatically induced or noisy causal graphs instead of gold-standard annotations? The authors state that current results rely on expert-annotated graphs and "robustness to automatically induced or noisy graphs remains future work."

- **Adaptive graph selection**: Can adaptive graph selection improve efficiency by filtering edges irrelevant to the specific query? The authors list "adaptive graph selection to filter only edges relevant to a query" as a specific future direction.

- **Scaling effects**: What are the scaling effects and context budget limitations when applying structured prompting to longer narratives or denser graphs? The authors note that while they report average prompt lengths, "a systematic study of context budget and scaling effects remains open."

## Limitations
- Performance relies on gold-standard causal graphs rather than automatically extracted ones, creating an upper-bound scenario
- Significant performance degradation on ambiguous question categories ("possible"/"unknown") comprising meaningful portion of TORQUESTRA
- Architecture-specific results reveal fundamental integration limitations - Qwen benefits substantially while T5 shows consistent accuracy drops with same configuration

## Confidence
- **High confidence** in the average 5% improvement claim across all models and configurations, supported by systematic 3×3 evaluation matrix
- **Medium confidence** in architecture-specific findings (QwQ vs T5), as the paper provides clear examples of integration failures but limited diagnostic analysis of underlying causes
- **Low confidence** in real-world applicability, given exclusive use of gold annotations and absence of error analysis on automatically extracted graphs

## Next Checks
1. Evaluate model performance on automatically extracted (noisy) causal graphs to assess robustness beyond gold annotations
2. Conduct ablation studies removing topological ordering to quantify its contribution to performance gains
3. Test cross-model transfer by applying Qwen-optimized TAG-EQA configurations to T5 and other architectures to identify universal vs model-specific benefits