---
ver: rpa2
title: 'RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical
  Model Merging'
arxiv_id: '2510.20479'
source_url: https://arxiv.org/abs/2510.20479
tags:
- merging
- task
- recall
- knowledge
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RECALL, a data-free model merging framework
  that leverages layer-wise hidden representations to mitigate catastrophic forgetting
  during continual learning. By extracting and aligning intermediate representations
  over clustered typical samples, RECALL computes adaptive, layer-specific merging
  weights that preserve domain-general features in shallow layers while enabling task-specific
  adaptation in deeper layers.
---

# RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging

## Quick Facts
- arXiv ID: 2510.20479
- Source URL: https://arxiv.org/abs/2510.20479
- Reference count: 22
- Primary result: Data-free model merging using representation alignment achieves up to 7.86% average performance gains on unseen tasks

## Executive Summary
RECALL introduces a data-free model merging framework that mitigates catastrophic forgetting during continual learning by leveraging layer-wise hidden representations. The method extracts intermediate representations from clustered typical samples and computes adaptive, layer-specific merging weights based on representation similarity. By preserving domain-general features in shallow layers while enabling task-specific adaptation in deeper layers, RECALL consistently outperforms state-of-the-art baselines across five NLP tasks without requiring historical data or task labels.

## Method Summary
RECALL extracts layer-wise hidden states from clustered "typical samples" and computes RBF kernel similarity between model pairs to determine per-layer merging weights. The method fine-tunes expert models using LoRA, then extracts representations from typical samples to compute similarity matrices. Softmax-normalized similarities become per-layer weights, with shallow layers preserving general features and deeper layers enabling task-specific adaptation. The framework operates without historical data, using only the fine-tuned expert models and a small set of representative samples.

## Key Results
- Achieves average performance gains of up to 7.86% on unseen tasks compared to baselines
- Outperforms state-of-the-art model merging approaches across all five evaluated NLP tasks
- Demonstrates effective knowledge retention and generalization without requiring task labels or historical data
- Shows robust performance in multi-domain settings and sequential fine-tuning scenarios

## Why This Works (Mechanism)

### Mechanism 1
Internal representations serve as reliable proxies for learned knowledge, enabling cross-model alignment without raw data. The method extracts layer-wise hidden states from clustered typical samples and computes RBF kernel similarity between model pairs. Higher similarity at a layer indicates shared knowledge; lower similarity signals task-specific specialization. Softmax-normalized similarities become per-layer merging weights. Core assumption: representation similarity correlates with functional compatibility of parameters at that layer.

### Mechanism 2
Layer-wise adaptive merging preserves domain-general features