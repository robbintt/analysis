---
ver: rpa2
title: Susceptibility of Large Language Models to User-Driven Factors in Medical Queries
arxiv_id: '2503.22746'
source_url: https://arxiv.org/abs/2503.22746
tags:
- medical
- clinical
- tone
- page
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study examined how user-driven factors affect the reliability
  of large language models (LLMs) in medical queries. Researchers conducted two tests:
  perturbation (varying tone, authority, and persona of misinformation) and ablation
  (removing clinical data categories) across proprietary and open-source models using
  MedQA and Medbullets datasets.'
---

# Susceptibility of Large Language Models to User-Driven Factors in Medical Queries

## Quick Facts
- **arXiv ID**: 2503.22746
- **Source URL**: https://arxiv.org/abs/2503.22746
- **Reference count**: 40
- **Primary result**: Large language models show measurable vulnerability to authoritative and assertive misinformation across all tested conditions

## Executive Summary
This study investigates how user-driven factors affect the reliability of large language models in medical queries. Through systematic perturbation and ablation experiments, researchers demonstrate that both proprietary and open-source models are susceptible to misinformation when it is presented with authoritative tone or assertive language. The research reveals that omitting physical exam and lab results causes the most significant accuracy declines, while proprietary models show higher baseline accuracy but greater susceptibility to misinformation framing.

The findings highlight critical vulnerabilities in medical AI applications and emphasize the need for structured prompts, complete clinical context, and careful framing of external information. The study provides actionable insights for improving LLM reliability in healthcare settings, particularly regarding the presentation of clinical data and the potential risks of authoritative misinformation in medical decision support systems.

## Method Summary
The study employed two experimental approaches to evaluate LLM vulnerability: perturbation tests that varied the tone, authority, and persona of misinformation across different models, and ablation tests that systematically removed clinical data categories from medical queries. Using MedQA and Medbullets datasets, researchers tested both proprietary and open-source models to compare their responses to different types of user-driven factors. The experiments measured accuracy changes when misinformation was presented authoritatively versus casually, and assessed the impact of omitting specific clinical information categories on model performance.

## Key Results
- All tested models showed vulnerability to misinformation, with accuracy declining significantly when false information was presented with authoritative tone
- Proprietary models exhibited higher baseline accuracy but dropped more sharply than open-source models when exposed to misinformation
- Omitting physical exam and lab results caused the largest accuracy declines across all model types
- Authoritative and assertive language framing of misinformation had the strongest negative impact on model responses

## Why This Works (Mechanism)
The study suggests that large language models' vulnerability to authoritative misinformation may stem from their training on human-generated text where confident, assertive language often correlates with accurate information. Models appear to assign higher weight to information presented with authoritative tone, potentially due to training data biases where medical professionals typically express diagnostic confidence. This mechanism could explain why models are more susceptible to misinformation when it mimics the confident language patterns of authoritative sources, though the exact underlying mechanisms require further investigation.

## Foundational Learning
The findings indicate that LLMs learn patterns associating confident language with reliability during training, which creates vulnerabilities when users intentionally or unintentionally exploit these patterns. The models appear to have learned statistical associations between authoritative presentation styles and factual accuracy from their training corpus, suggesting that medical applications may need additional safeguards against manipulation through linguistic framing. This foundational insight highlights how surface-level presentation features can override substantive clinical content in model responses.

## Architecture Onboarding
The study's results suggest that current LLM architectures may lack sufficient mechanisms to critically evaluate the credibility of presented information beyond surface-level linguistic cues. The models' susceptibility to authoritative misinformation indicates they may not effectively distinguish between reliable and unreliable sources when information is presented with similar confidence levels. This vulnerability appears consistent across different model architectures, suggesting a fundamental limitation in how current LLMs process and weigh user-provided clinical information.

## Open Questions the Paper Calls Out
The research identifies several important areas requiring further investigation, including how different model architectures handle authoritative misinformation and whether more recent LLM versions show improved resistance to linguistic manipulation. The study also raises questions about how socioeconomic and demographic factors might influence model responses to misinformation, as well as the potential for developing training approaches that reduce susceptibility to confident but incorrect information. Additionally, the authors suggest exploring whether structured prompt engineering can effectively mitigate these vulnerabilities.

## Limitations
- The analysis focused on specific question types from MedQA and Medbullets datasets, potentially limiting generalizability to real-world medical queries
- Proprietary model testing was limited to GPT-3.5, excluding more recent and capable models that may show different susceptibility patterns
- Socioeconomic and demographic factors of hypothetical patients were not explored, which could influence model responses in real-world applications
- The study did not investigate the long-term effects of repeated exposure to authoritative misinformation on model performance
- Testing scenarios were controlled laboratory conditions that may not fully capture the complexity of real-world medical consultations

## Confidence
- **High confidence**: Models show measurable vulnerability to authoritative and assertive misinformation across all tested conditions
- **Medium confidence**: Proprietary models demonstrate greater susceptibility than open-source models to misinformation framing
- **Medium confidence**: Physical exam and lab result omissions cause the most significant accuracy declines

## Next Checks
1. Replicate the study using more recent LLM versions and additional proprietary models to verify if susceptibility patterns persist across updated architectures
2. Conduct field testing with real clinical queries and actual physician-provided misinformation to validate laboratory findings
3. Expand the ablation study to test partial information removal scenarios that better reflect real-world clinical documentation gaps
4. Investigate the effectiveness of structured prompt engineering and additional context in mitigating susceptibility to authoritative misinformation
5. Explore how different model architectures and training approaches affect vulnerability to user-driven factors in medical queries