---
ver: rpa2
title: 'Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning
  Platform for Individual Behavioral Learning'
arxiv_id: '2511.18000'
source_url: https://arxiv.org/abs/2511.18000
tags:
- reward
- agent
- trained
- learning
- movement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ContagionRL, a novel reinforcement learning
  platform for studying epidemic control through reward engineering. The platform
  integrates a spatial SIRS+D epidemiological model with configurable environmental
  parameters, enabling systematic evaluation of how different reward function designs
  affect learned survival strategies.
---

# Reward Engineering for Spatial Epidemic Simulations: A Reinforcement Learning Platform for Individual Behavioral Learning

## Quick Facts
- arXiv ID: 2511.18000
- Source URL: https://arxiv.org/abs/2511.18000
- Reference count: 40
- Single RL agent learns to survive in spatial epidemic simulation through reward-engineered behavior

## Executive Summary
This paper introduces ContagionRL, a novel reinforcement learning platform for studying epidemic control through reward engineering. The platform integrates a spatial SIRS+D epidemiological model with configurable environmental parameters, enabling systematic evaluation of how different reward function designs affect learned survival strategies. The authors evaluate five distinct reward designs across multiple RL algorithms (PPO, SAC, A2C) and conduct comprehensive ablation studies. Results show that reward function choice dramatically impacts agent behavior and survival outcomes, with potential field-based rewards consistently achieving superior performance. Agents learn maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies. The platform's modular design enables systematic exploration of reward-behavior relationships in epidemic simulations, addressing a knowledge gap in models where reward engineering has received limited attention.

## Method Summary
ContagionRL implements a single-agent reinforcement learning framework within a spatial SIRS+D epidemic model on a toroidal 50×50 grid containing 40 non-learning humans. The agent observes its adherence level, relative positions/distances, and infection status of all humans within visibility radius, then selects continuous actions for movement and adherence level. Five reward designs are evaluated: Constant, Constrained Movement, Potential Field, Social Welfare, and Imitation. Training uses PPO/SAC/A2C via stable-baselines3 for 8M timesteps with specific hyperparameters. Performance is evaluated across 3 seeds with 100 episodes each, using Mann-Whitney U tests with Bonferroni correction to compare against Stationary, Random, and Greedy baselines.

## Key Results
- Potential field rewards achieve superior performance by combining directional guidance (cosine similarity to repulsion force) with magnitude matching
- Agents learn maximal adherence to non-pharmaceutical interventions while developing sophisticated spatial avoidance strategies
- Reward function choice dramatically impacts agent behavior and survival outcomes across all RL algorithms tested
- Partial visibility (POMDP) agents paradoxically outperform full visibility (MDP) agents, suggesting observation noise may improve learning

## Why This Works (Mechanism)
The platform succeeds by providing a controlled environment where reward engineering can be systematically studied in isolation. The spatial epidemic model creates meaningful dynamics where agent decisions have cascading effects on disease spread. The modular reward design allows precise attribution of behavioral changes to specific reward components. The integration of epidemiological modeling with reinforcement learning creates a realistic yet tractable domain for studying how reward signals shape strategic behavior in complex, multi-agent systems.

## Foundational Learning
- **Spatial epidemic modeling**: Understanding SIRS+D disease dynamics with reinfection is crucial for interpreting agent-environment interactions and evaluating survival strategies.
- **Reward engineering principles**: Knowledge of how different reward components (health, adherence, movement) influence policy learning is essential for interpreting behavioral outcomes.
- **Reinforcement learning algorithms**: Familiarity with PPO, SAC, and A2C implementations helps understand training dynamics and performance differences across algorithms.
- **Statistical evaluation methods**: Understanding Mann-Whitney U tests and Bonferroni correction is necessary for interpreting the significance of performance comparisons.

## Architecture Onboarding

**Component Map**
ContagionEnv -> RewardEngine -> RLAlgorithm -> EvaluationMetrics

**Critical Path**
Environment simulation → Reward calculation → Policy update → Action selection → Next state

**Design Tradeoffs**
Single-agent focus enables systematic reward study but limits multi-agent interaction insights; partial observability improves learning but reduces interpretability; continuous actions provide smooth control but increase exploration complexity.

**Failure Signatures**
Constant rewards fail to provide learning signal; missing reinfection logic causes trivial extinction episodes; incorrect distance calculations break spatial avoidance strategies.

**First Experiments**
1. Verify toroidal grid implementation with correct distance calculations and boundary wrapping
2. Test potential field reward calculation with force weights (WI=1.0, WS=0.5) on simple agent configurations
3. Train PPO with minimal timesteps to confirm basic learning before scaling to full experiments

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do reward design insights from single-agent ContagionRL transfer to multi-agent reinforcement learning settings with co-adapting populations?
- **Basis in paper**: [explicit] Authors state the single-agent approach is "a first step for understanding the relationship between reward and behavior before applying insights to a complex domain like multi-agent reinforcement learning" and that multi-agent settings introduce complexities that could "complicate and potentially obfuscate how specific reward components influence the learned policy."
- **Why unresolved**: The paper deliberately isolates a single learning agent to enable systematic reward evaluation, leaving multi-agent interactions unexplored.
- **What evidence would resolve it**: Extending ContagionRL to multiple concurrent learning agents and comparing whether the potential field reward's superior performance and the criticality of directional guidance/adherence incentives persist under non-stationarity and emergent social behaviors.

### Open Question 2
- **Question**: What mechanisms explain the superior performance of constrained visibility (POMDP) agents over fully observable (MDP) agents?
- **Basis in paper**: [explicit] The paper notes "trained partial visibility models (r=10, r=15, r=20) paradoxically outperform the baselines and the full visibility trained model" and suggests this could be due to "observation noise and the increased dimensionality in high-dimensional observation spaces" but does not definitively establish the cause.
- **Why unresolved**: The result is described as "counter-intuitive" with hypotheses offered but no ablation or analysis confirming which mechanism(s) are responsible.
- **What evidence would resolve it**: Systematic experiments varying observation noise independently from dimensionality, analyzing attention patterns or learned representations across visibility conditions, and testing whether information bottlenecks consistently improve learning in other epidemic scenarios.

### Open Question 3
- **Question**: Can learned RL policies match or exceed the greedy distance-maximizer heuristic's median survival performance?
- **Basis in paper**: [explicit] Statistical analysis shows "the greedy heuristic baseline demonstrates superior performance in pairwise comparisons against all three RL agents, achieving statistically significantly longer episode durations" though mean performance is comparable with overlapping confidence intervals.
- **Why unresolved**: While RL agents develop sophisticated strategies competitive with the heuristic on average, the distributional differences indicate the greedy policy's median superiority remains unexplained and unmatched.
- **What evidence would resolve it**: Investigating whether longer training, different algorithm architectures (e.g., model-based RL), or incorporating look-ahead mechanisms can close the median performance gap with the myopic greedy baseline.

## Limitations
- No public code repository requires reverse-engineering implementation details from mathematical descriptions
- Single-agent focus limits insights about multi-agent dynamics and social behavior emergence
- Specific performance metrics may vary with implementation choices not fully specified in the paper

## Confidence

**High Confidence Claims:**
- Reward engineering critically shapes learned epidemic control strategies
- Potential field rewards consistently outperform other designs across algorithms
- Agents develop maximal NPI adherence and spatial avoidance behaviors

**Medium Confidence Claims:**
- Specific performance rankings of reward designs across RL algorithms
- Detailed behavioral characterizations (exact thresholds, movement patterns)
- Statistical significance of pairwise comparisons across all conditions

## Next Checks
1. Implement the Gymnasium environment with exact toroidal grid logic and observation space as specified, verifying against the mathematical formulas in the appendix.
2. Replicate the potential field reward implementation with the specified weights (WI=1.0, WS=0.5) and force calculation details, checking the directional alignment and magnitude components.
3. Train PPO with the exact hyperparameters from Table 5 across 3 seeds and compare episode durations and agent trajectories against the published results using Mann-Whitney U tests.