---
ver: rpa2
title: Evolving Prompts for Toxicity Search in Large Language Models
arxiv_id: '2511.12487'
source_url: https://arxiv.org/abs/2511.12487
tags:
- prompts
- toxicity
- prompt
- language
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "ToxSearch introduces a black-box evolutionary framework that red-teams\
  \ LLM safety by evolving prompts via a steady-state (\xB5+\u03BB) loop. Using lexical,\
  \ semantic, negation, back-translation, paraphrasing, and two crossover operators,\
  \ the method generates diverse adversarial prompts."
---

# Evolving Prompts for Toxicity Search in Large Language Models

## Quick Facts
- arXiv ID: 2511.12487
- Source URL: https://arxiv.org/abs/2511.12487
- Reference count: 23
- Key outcome: Black-box evolutionary red-teaming of LLMs with adaptive population tiering and operator analysis shows lexical perturbations give best yield-variance trade-off while cross-model prompt transfer retains ~50% toxicity on average

## Executive Summary
ToxSearch introduces a black-box evolutionary framework that red-teams LLM safety by evolving prompts via a steady-state (µ+λ) loop. Using lexical, semantic, negation, back-translation, paraphrasing, and two crossover operators, the method generates diverse adversarial prompts. Operator analysis shows lexical substitutions give the best yield-variance trade-off, while informed evolution offers high yield but incurs high invalid rates. Elite prompts evolved on LLaMA 3.1 8B transferred to other models, with toxicity dropping roughly 50% on most targets; smaller LLaMA 3.2 variants showed the strongest resistance. Some cross-architecture models retained higher toxicity. These results highlight that subtle, controllable perturbations are reliable for red-teaming and that defenses must plan for cross-model prompt reuse rather than single-model hardening.

## Method Summary
ToxSearch uses a steady-state evolutionary strategy where harmful questions evolve through mutation and crossover operators applied by a Prompt Generator LLM. The framework maintains a dynamic population tiered by score ratios rather than fixed elites, preventing mode collapse. Operators include lexical substitutions, back-translation, negation, MLM, paraphrasing, and two crossover methods. A Response Generator LLM produces responses, evaluated by Google Perspective API for toxicity. The system uses ratio-based thresholds (α=30%, β=3%) for selection and removal, with 50 generations and trend-gated parent selection based on recent fitness slope.

## Key Results
- Lexical operators (synonym, antonym, negation, back-translation, MLM) achieve highest yield-variance trade-off with low invalid rates
- Informed evolution and stylistic transfer show high toxicity gains but incur 40-55% invalid-generation rates
- Cross-model transfer shows ~50% toxicity attenuation, with smaller LLaMA 3.2 variants showing strongest resistance (43.9% invalid rate)
- Cross-architecture models like Qwen 2.5 7B retained higher toxicity (0.240 mean) despite 0.0% invalid rate
- Adaptive population tiering prevents premature convergence to single-topic modes observed in fixed-population versions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Small lexical perturbations achieve higher yield-variance trade-offs than global semantic rewrites for toxicity elicitation under fixed query budgets.
- **Mechanism:** Localized word-level changes (synonym/antonym replacement, negation, back-translation) alter surface patterns while preserving grammatical structure and malicious intent, avoiding safety classifier triggers that global rewrites activate. The paper explicitly states these operators "have high success rates in producing effective mutations and relatively low variance in toxicity scores."
- **Core assumption:** Safety alignment in instruction-tuned LLMs is more sensitive to global semantic shifts than to local lexical substitutions that maintain sentence structure.
- **Evidence anchors:**
  - [abstract] "lexical substitutions offer the best yield–variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs"
  - [section] Table 4 shows Antonym Replacement with NE=83.78%, IR=4.79%, Δσ=0.12 vs. Informed Evolution with IR=43.98% and Δμ=-0.18
  - [corpus] BeamAttack paper (FMR=0.62) similarly finds word-level modifications effective for adversarial text, supporting the local-perturbation hypothesis
- **Break condition:** If target model has lexical-level safety filters (not tested in this paper), the advantage would likely disappear.

### Mechanism 2
- **Claim:** Adaptive population tiering with ratio-based thresholds prevents premature convergence to single-topic modes in adversarial prompt search.
- **Mechanism:** Rather than fixed k-elites, score-ratio tiering (τe = (1-α/100)·Smax, τr = β/100·Smax) preserves secondary topical peaks even when global peak saturates, maintaining crossover diversity. The paper states this "provides scale-invariant selection pressure" where "secondary peaks remain represented."
- **Core assumption:** The toxicity landscape is multi-peaked with topic-specific basins; allowing all elites from a dominant basin starves other regions of search budget.
- **Evidence anchors:**
  - [section] "Early versions of the algorithm utilized a fixed-size population which would quickly converge to a single topic and were not able to find toxic responses"
  - [section] "In rugged or multi-peaked landscapes, ratio thresholds provide scale-invariant selection pressure, secondary peaks remain represented even when the global peak saturates"
  - [corpus] No direct corpus support for this specific population management claim; related EA literature on niching exists but was not retrieved
- **Break condition:** If fitness landscape is unimodal or if all high-toxicity prompts cluster in one semantic region, adaptive tiering provides no benefit over fixed elites.

### Mechanism 3
- **Claim:** Cross-model prompt transfer occurs with ~50% toxicity attenuation, with resistance varying by alignment methodology more than architectural similarity.
- **Mechanism:** Prompts evolved on one model exploit shared vulnerabilities in training data representations and tokenizer behaviors, but alignment differences (refusal training, safety fine-tuning) create model-specific defensive barriers. Smaller LLaMA 3.2 variants' higher refusal rates (25.6%-43.9%) indicate alignment as the primary resistance factor.
- **Core assumption:** Adversarial prompts capture exploitable patterns in model behavior that partially generalize across models sharing training corpora or architectural choices.
- **Evidence anchors:**
  - [abstract] "Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets"
  - [section] Figure 3 shows LLaMA 3.2 3B with 43.9% invalid responses vs. source model's 5.7%, while Qwen 2.5 7B shows 0.0% invalid but higher toxicity (0.240 mean)
  - [corpus] "How Toxic Can You Get?" paper (EvoTox) demonstrated cross-model toxicity elicitation but did not systematically characterize transfer attenuation rates
- **Break condition:** If models are trained on fundamentally different corpora with unrelated tokenizers, transfer would likely drop to near-zero.

## Foundational Learning

- **Concept: Steady-state (µ+λ) evolutionary strategy**
  - **Why needed here:** ToxSearch uses a dynamic-population steady-state loop where parents and children compete for survival each generation, unlike generational EAs that replace entire populations.
  - **Quick check question:** In a (µ+λ) ES with µ=100 and λ=50, if all 50 children have lower fitness than the worst parent, what happens to the population next generation?

- **Concept: Black-box optimization with fitness oracles**
  - **Why needed here:** The framework treats the target LLM as a black box, using only toxicity scores from the Perspective API as fitness feedback without gradient access.
  - **Quick check question:** Why would gradient-based attacks (like Zou et al.'s GCG) potentially find different adversarial prompts than ToxSearch's evolutionary approach on the same model?

- **Concept: Safety alignment and refusal behavior in LLMs**
  - **Why needed here:** The paper's operator analysis hinges on understanding why certain mutations trigger refusals (high IR) versus toxic completions, which relates directly to RLHF/safety training effects.
  - **Quick check question:** If an operator produces 43.98% invalid generations (like Informed Evolution), is this a failure of the operator or evidence of stronger alignment in the prompt generator model?

## Architecture Onboarding

- **Component map:**
  Initial Population (harmful questions dataset) -> [Evaluation] -> Response Generator (target LLM) -> Moderation Oracle (Perspective API) -> [Population Tiering] -> Elite (top α%) / Non-elite / Underperforming (bottom β%) -> [Parent Selection] -> Default/Explore/Exploit modes based on recent fitness slope -> [Operator Suite] -> 12 mutation + 2 crossover operators via Prompt Generator LLM -> [Steady-state Update] -> Merge children, re-tier, remove underperformers -> Elite Prompts (output for transfer testing)

- **Critical path:** The toxicity evaluation loop (prompt -> target model response -> Perspective API score) is the bottleneck; each child prompt requires one full inference call. With 3 parents × ~12 operators + crossover combinations, per-generation query costs scale quickly.

- **Design tradeoffs:**
  - **Lexical vs. global operators:** Lexical (synonym/antonym/negation) gives stable gains with low refusals; global (informed evolution, stylistic transfer) explores more but wastes budget on invalids
  - **Fixed vs. dynamic population:** Dynamic prevents mode collapse but requires tuning α/β thresholds; paper uses α=30%, β=3%
  - **Temperature settings:** PG at 0.9 for creativity vs. RG at 0.7 for consistency—higher PG temperature increases exploration but also invalid generation risk

- **Failure signatures:**
  - **Mode collapse:** Population converges to single topic within <10 generations (fixed-population symptom)
  - **High invalid rate (>40%):** Operator-architecture mismatch; informed evolution on strongly-aligned PG models
  - **Stagnant best score for W=5 generations:** Triggers Explore mode; if still stuck, operator suite may be insufficient for target model's defenses
  - **Transfer toxicity near-zero:** Source and target models share no exploitable vulnerabilities; likely different tokenizer or alignment paradigm

- **First 3 experiments:**
  1. **Operator ablation on single model:** Run ToxSearch with only lexical operators (synonym, antonym, negation, back-translation, MLM) vs. only global operators (informed evolution, paraphrasing, stylistic transfer) for 50 generations on LLaMA 3.1 8B. Compare elite count, mean toxicity, and IR to validate the paper's yield-variance trade-off claims.
  2. **Population tiering sensitivity:** Test α ∈ {15%, 30%, 50%} and β ∈ {1%, 3%, 10%} with fixed operators and budget. Measure topic diversity in final elite set (e.g., embedding clustering) and best-toxicity convergence rate to identify robust settings.
  3. **Minimal transfer probe:** Take top-10 elites from source model, evaluate on 3 targets: same-family smaller (LLaMA 3.2 3B), same-family larger (LLaMA 3.1 70B if available), and cross-architecture (Qwen or Mistral). Confirm the ~50% attenuation pattern and identify which operator's outputs transfer best.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific features of adversarial prompts enable successful cross-model transfer, and can these be formally characterized?
- Basis in paper: [explicit] "Future work should deepen our understanding of why some prompts transfer so well by analyzing the features of successful attacks."
- Why unresolved: The paper observes heterogeneous transfer (toxicity drops ~50% on average but some prompts retain high toxicity across architectures), but does not identify the linguistic or semantic properties that distinguish high-transfer from low-transfer prompts.
- What evidence would resolve it: Feature attribution analysis on transferred vs. non-transferred prompts, controlled experiments isolating syntactic vs. semantic prompt characteristics, and correlation of transfer success with measurable prompt properties.

### Open Question 2
- Question: How can adaptive operator allocation policies be learned to optimize evolutionary red-teaming under varying budget constraints?
- Basis in paper: [explicit] "An adaptive operator allocation policy can be learned that routes budget where marginal return is highest, for example using lightweight reinforcement learning with explicit cost regularization."
- Why unresolved: The paper demonstrates operator heterogeneity but uses fixed operator application; optimal budget allocation across operators remains unexplored.
- What evidence would resolve it: Experiments comparing fixed vs. learned operator scheduling, measuring convergence speed and elite yield under different allocation strategies, with ablations on RL-based vs. heuristic approaches.

### Open Question 3
- Question: What alignment mechanisms specifically confer transfer resistance, and do they generalize across model families?
- Basis in paper: [inferred] The paper notes LLaMA 3.2 variants showed strongest resistance despite architectural similarity to source, suggesting "alignment choices and training methodology contribute at least as much as model size or architectural similarity," but does not isolate which mechanisms matter.
- Why unresolved: Multiple factors (size, alignment method, refusal training) vary across tested models, making causal attribution difficult.
- What evidence would resolve it: Controlled experiments varying single alignment components while holding architecture constant, and analysis of refusal patterns across alignment strategies.

### Open Question 4
- Question: Does cross-lingual transfer of adversarial prompts exhibit similar attenuation patterns as cross-model transfer?
- Basis in paper: [explicit] "We also want to generalize the setup to multilingual settings by adding more translation pivots and multilingual evaluators, so that we can study cross-lingual transfer in a more systematic way."
- Why unresolved: Back-translation operator used Hindi as pivot, but systematic cross-lingual transfer remains uncharacterized.
- What evidence would resolve it: Multilingual experiments with evolved prompts transferred across languages, measuring toxicity retention and comparing attenuation rates to cross-model transfer.

## Limitations

- Operator prompt templates and system instructions are only partially specified, requiring reconstruction for faithful reproduction
- Cross-model transfer results depend heavily on Perspective API's consistency across model responses, which is not validated in the paper
- The adaptive population tiering mechanism's benefits over fixed-elite approaches lack direct comparative validation within the paper
- Invalid-generation handling rules are described as pattern-based but specific patterns are not detailed

## Confidence

- **High confidence:** Operator yield-variance trade-offs (lexical vs global) - supported by detailed per-operator metrics and consistent with BeamAttack findings
- **Medium confidence:** Cross-model transfer attenuation (~50%) - based on limited target model sample and Perspective API consistency assumptions
- **Medium confidence:** Adaptive population tiering benefits - mechanism described but not directly validated against fixed-elite baseline
- **Low confidence:** Exact implementation details for prompt generator templates and invalid response detection patterns

## Next Checks

1. **Operator ablation validation:** Implement controlled ablation testing comparing only lexical operators vs only global operators on LLaMA 3.1 8B. Verify the claimed yield-variance trade-off by measuring elite count, mean toxicity, and invalid generation rates across 10 repeated runs.

2. **Population management comparison:** Run ToxSearch with fixed k-elite selection (k=30) vs dynamic ratio-based tiering (α=30%, β=3%) on same model and budget. Track topic diversity in elite sets via embedding clustering and measure convergence speed to validate adaptive tiering's mode-collapse prevention claims.

3. **Transfer consistency check:** Using top-10 elites from source model, evaluate on three distinct targets: same-family smaller variant (LLaMA 3.2 3B), same-family larger variant (LLaMA 3.1 70B if available), and cross-architecture model (Qwen or Mistral). Confirm the ~50% toxicity attenuation pattern and test whether Perspective API scores remain consistent across model responses for the same toxic content.