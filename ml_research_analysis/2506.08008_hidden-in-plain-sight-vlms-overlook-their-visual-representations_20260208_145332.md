---
ver: rpa2
title: 'Hidden in plain sight: VLMs overlook their visual representations'
arxiv_id: '2506.08008'
source_url: https://arxiv.org/abs/2506.08008
tags:
- vision
- visual
- vlms
- image
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VLMs perform significantly worse than their visual encoders on
  vision-centric tasks, often dropping to near-chance performance. The paper systematically
  evaluates VLMs against direct visual encoder readouts across multiple vision tasks
  including depth estimation, correspondence, and 3D object awareness.
---

# Hidden in plain sight: VLMs overlook their visual representations

## Quick Facts
- arXiv ID: 2506.08008
- Source URL: https://arxiv.org/abs/2506.08008
- Authors: Stephanie Fu; Tyler Bonnen; Devin Guillory; Trevor Darrell
- Reference count: 28
- VLMs perform significantly worse than their visual encoders on vision-centric tasks, often dropping to near-chance performance.

## Executive Summary
This paper reveals a fundamental limitation in visual-language models (VLMs): despite having access to high-quality visual representations from their encoders, VLMs often fail to utilize this information for vision-centric tasks. Through systematic evaluation across depth estimation, correspondence, 3D object awareness, and art style classification tasks, the authors demonstrate that VLMs output near-chance performance while direct visual encoder probes achieve high accuracy. The key finding is that the bottleneck lies in the language model's ability to integrate visual information rather than degradation of visual features or prompt sensitivity.

## Method Summary
The paper evaluates VLMs against direct visual encoder readouts across multiple vision tasks. Visual readouts use DPT decoder for depth (trained on NYUv2), cosine similarity on patch features for correspondence, Gram matrix MSE for art style, and CLS-token similarity for 3D. VLM evaluation uses letterbox preprocessing with 224-1024px images and fuzzy matching for answer extraction. The study employs LoRA fine-tuning (16.7M parameters) to ablate which component (ViT, projector, or LLM) drives performance improvements, comparing accuracy gains and total variation distance to ground truth distributions.

## Key Results
- VLMs achieve near-chance performance (25-33%) on vision-centric tasks while visual encoder probes reach 70-90% accuracy
- Answer distributions from VLMs with visual input closely match blind (no-image) baselines, indicating visual neglect
- Fine-tuning the LLM produces larger accuracy gains and lower total variation distance to ground truth than fine-tuning the vision encoder or projector
- Vision-language pretrained encoders (CLIP/SigLIP) outperform pure vision encoders (DINOv2) in VLM rankings despite weaker standalone visual performance

## Why This Works (Mechanism)

### Mechanism 1: Visual Feature Preservation Through the VLM Pipeline
- Claim: Task-relevant visual information survives intact through projector and LLM layers but remains inaccessible to the language decoder.
- Mechanism: The vision encoder produces discriminative patch embeddings; the projector and early LLM layers propagate these without significant degradation. Probing intermediate layers with direct readout methods recovers high accuracy, indicating the bottleneck is not information loss but information access.
- Core assumption: Probing methods approximate what the LLM could extract if properly aligned.
- Evidence anchors:
  - [abstract] "Analysis reveals that the bottleneck lies in the language model's ability to use visual representations rather than degradation of visual features."
  - [Section 4.1] Figure 5 shows visual evaluation accuracy remains high across projector (gray) and most LLM layers (white), dropping only at the final generation layer.
  - [corpus] Weak direct support; "Line of Sight" (arXiv:2506.04706) confirms linear representations of visual concepts exist in VLLM hidden states but does not address extraction difficulty.

### Mechanism 2: Language Prior Domination Over Visual Evidence
- Claim: VLMs default to LLM-internal answer biases when visual integration is underspecified, producing outputs similar to blind (no-image) baselines.
- Mechanism: The LLM learns strong priors over answer distributions during pretraining. Without robust cross-modal attention mechanisms, the model relies on these priors rather than visual evidence. Fine-tuning the LLM reduces total variation distance to ground-truth distributions more than fine-tuning other components.
- Core assumption: Answer distribution similarity between "with vision" and "no vision" conditions indicates visual neglect rather than systematic visual-to-language mapping.
- Evidence anchors:
  - [Section 3.4] Figure 4 visualizes answer distributions; "VLMs output answer distributions are uncannily similar to their 'blinded' counterparts."
  - [Section 4.3.1] Table 2 shows LLM fine-tuning produces lowest TV distance to ground truth across 5 of 6 tasks.
  - [corpus] No direct corpus support for this specific mechanism.

### Mechanism 3: LLM Attention Allocation as the Integration Bottleneck
- Claim: The LLM's attention heads fail to consistently attend to task-relevant visual regions; fine-tuning the LLM (not the projector or vision encoder) most effectively redirects attention.
- Mechanism: Pretrained LLMs lack learned attention patterns for spatial correspondence or depth-related visual queries. LoRA fine-tuning on the LLM component increases attention weights on reference points and multiple-choice labels, enabling better visual grounding.
- Core assumption: Attention visualization on correspondence tasks generalizes to other vision-centric tasks.
- Evidence anchors:
  - [Section 4.3] Figure 7 shows LLM fine-tuning outperforms projector or ViT fine-tuning across all tasks except 3D Object Awareness.
  - [Section 4.3] Figure 8 visualizes attention differences: "fine-tuning enhances attention over multiple-choice labels, reference points, and other text."
  - [corpus] Weak support; "Don't Blind Your VLA" (arXiv:2510.25616) discusses visual representation alignment for generalization but focuses on action models.

## Foundational Learning

- Concept: **Vision encoder probing (linear readout)**
  - Why needed here: The paper's core methodology compares direct visual encoder probes against VLM outputs to isolate integration failures from representation failures.
  - Quick check question: Can you explain why cosine similarity on patch features is a valid probe for semantic correspondence?

- Concept: **LLM language priors and answer distribution bias**
  - Why needed here: Understanding why VLMs produce near-chance performance requires recognizing that LLMs have strong pre-existing biases over token sequences independent of visual input.
  - Quick check question: How would you detect whether a VLM is ignoring its visual input?

- Concept: **LoRA fine-tuning with controlled parameter counts**
  - Why needed here: The paper attributes performance gains to LLM improvements specifically by matching tunable parameter counts across components (16.7M).
  - Quick check question: Why is parameter count control critical for attributing improvement to a specific component?

## Architecture Onboarding

- Component map:
Vision Encoder (ViT: DINOv2/CLIP/SigLIP/IN-1k) -> patch embeddings [B, N, D] -> Projector (2-layer MLP) -> adapted visual tokens [B, N, LLM_dim] -> LLM (Vicuna v1.5 / Qwen / Phi-3 / InternVL) interleaved with text tokens -> Output: Text response

- Critical path: The LLM's cross-modal attention layers (observed most active at layers 4-6) determine whether visual information is utilized. The final generation layer often shows performance drops as priority shifts from feature preservation to token generation.

- Design tradeoffs:
  - Vision-only pretraining (DINOv2) yields strongest visual representations but underperforms in VLM context.
  - Vision-language pretraining (CLIP/SigLIP) yields better VLM rankings despite weaker standalone visual performance.
  - This rank-order inversion suggests VLM training dynamics favor aligned encoders over discriminative ones.

- Failure signatures:
  - Near-chance accuracy on vision-centric tasks while visual encoder probes show 70-90% accuracy.
  - Answer distributions matching blind (no-image) baselines (high TV distance from ground truth).
  - No consistent attention on task-relevant visual regions (reference points, bounding boxes).

- First 3 experiments:
  1. **Probe your VLM's vision encoder directly** on a vision-centric task (e.g., depth ordering via patch feature averaging) and compare to VLM VQA accuracy on the same task.
  2. **Run blind baseline**: Feed blank images with the same prompts and measure answer distribution similarity (TV distance) to visual-input condition.
  3. **Ablate fine-tuning location**: Apply LoRA with identical parameter counts to (a) ViT, (b) projector, (c) LLM on 5,000 task examples; compare accuracy gains to identify the bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can VLM pre-training objectives be redesigned to compel the LLM to utilize accessible visual representations without relying on task-specific fine-tuning?
- Basis in paper: [explicit] The authors note that while fine-tuning the LLM improves performance, they "do not propose training directly on the tasks as an overarching solution," identifying the LLM's integration capability as the primary bottleneck.
- Why unresolved: The paper diagnoses the failure (LLM inattention) but does not propose a fundamental architectural or training fix that solves the integration issue at scale.
- What evidence would resolve it: A training methodology that achieves visual encoder-level performance on vision-centric tasks using a frozen LLM without task-specific LoRA adaptation.

### Open Question 2
- Question: Do the observed visual integration failures persist in proprietary or larger-scale VLMs (e.g., GPT-4o), or is this failure mode specific to the open-source late-fusion architectures studied?
- Basis in paper: [inferred] The study evaluates open-weight models (Vicuna, LLaVA) but explicitly mentions that different architectures and training strategies contribute to "vastly different performance patterns," leaving the upper bounds of closed models uncertain.
- Why unresolved: The paper's analysis is restricted to specific open-source VLMs, and the authors cannot verify if the same "blind" biases exist in closed systems.
- What evidence would resolve it: Evaluation of closed-source VLMs on the same vision-centric benchmarks (e.g., MOCHI, BLINK) comparing VQA performance to direct readouts.

### Open Question 3
- Question: Why does the standard VLM projector alignment fail to preserve the spatial locality required for the LLM to perform low-level visual matching tasks?
- Basis in paper: [inferred] The paper finds that while features are preserved in a "probing" sense, the LLM fails to use them for low-level matching (e.g., correspondence), suggesting the alignment process destroys necessary spatial cues for the LLM.
- Why unresolved: The analysis confirms the projector maintains "task-relevant information" for probes but does not explain why this information becomes inaccessible for token-generation tasks in the LLM.
- What evidence would resolve it: An ablation study varying projector architectures (e.g., convolutional vs. MLP) to see which, if any, preserve spatial fidelity for the LLM.

## Limitations

- Task Design Sensitivity: The paper's conclusions rest heavily on specific task formulations that may not generalize to broader VLM use cases.
- Probing Method Validity: Probing methods assume linear extractability approximates what the LLM could achieve, which may not hold for complex nonlinear reasoning.
- Pretraining Data Distribution: The analysis doesn't account for how pretraining data distributions might bias VLMs toward language priors.

## Confidence

**High Confidence**: The empirical observation that VLMs consistently underperform their visual encoders across multiple tasks and benchmarks is robust.

**Medium Confidence**: The attribution of the bottleneck specifically to LLM integration is well-supported by the ablation experiments.

**Low Confidence**: The proposed mechanism that language priors dominate over visual evidence relies on distributional analysis but lacks direct causal evidence.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the same VLM models on natural vision-language tasks (VQA, image captioning) where visual information is explicitly required, comparing performance drops to the artificial vision-centric tasks.

2. **Attention Pattern Validation**: Use attention visualization tools to track whether task-relevant visual regions receive consistent attention across multiple examples within each task. Correlate attention strength with accuracy gains from LLM fine-tuning.

3. **Pretraining Data Ablation**: Train VLMs with controlled pretraining data that either includes or excludes examples requiring visual integration for the specific tasks tested. Compare performance to determine whether the bottleneck stems from architectural limitations or training data distribution.