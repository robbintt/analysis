---
ver: rpa2
title: 'AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate
  Time Series Forecasting'
arxiv_id: '2502.10235'
source_url: https://arxiv.org/abs/2502.10235
tags:
- time
- series
- forecasting
- adapter
- adapters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of applying pre-trained univariate
  foundation models to multivariate probabilistic time series forecasting. It introduces
  AdaPTS, a framework that uses feature-space adapters to project multivariate inputs
  into a latent space where a frozen foundation model makes predictions independently
  per dimension, then transforms predictions back.
---

# AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2502.10235
- Source URL: https://arxiv.org/abs/2502.10235
- Reference count: 40
- One-line primary result: AdaPTS achieves state-of-the-art results in multivariate probabilistic forecasting by using feature-space adapters to enable univariate foundation models to process multivariate data while maintaining uncertainty quantification.

## Executive Summary
This paper addresses the challenge of applying pre-trained univariate foundation models to multivariate probabilistic time series forecasting. AdaPTS introduces a framework that uses feature-space adapters to project multivariate inputs into a latent space where a frozen foundation model makes predictions independently per dimension, then transforms predictions back. The adapters are designed to be invertible and can be probabilistic, enabling uncertainty quantification. The method demonstrates significant improvements in forecasting accuracy and uncertainty calibration across multiple real-world datasets, while also enabling dimensionality reduction.

## Method Summary
AdaPTS operates in two stages: first, a linear forecasting head is trained on a pre-trained univariate foundation model (Moment), then the foundation model is frozen and adapters are trained to handle multivariate inputs. The adapters consist of an encoder φ that transforms the multivariate input X into a latent representation Z, a frozen foundation model that processes each latent dimension independently, and a decoder φ^{-1} that reconstructs predictions back to the original feature space. The framework supports both deterministic adapters (LinearAE, DropoutLinearAE) and probabilistic adapters (VAE, β-VAE) for uncertainty quantification. Training uses MSE loss for deterministic variants and ELBO for probabilistic variants, with hyperparameter optimization via Ray Tune and HEBO.

## Key Results
- AdaPTS with DropoutLinearAE achieves ~8% MSE improvement over the identity baseline on ETTh1 with 96-step horizon
- Probabilistic adapters (VAE, β-VAE) significantly improve calibration (lower ECE) compared to deterministic variants
- Dimensionality reduction is effective: optimal performance achieved with D'=7 components (matching original feature count) for ETTh1, with D'=5 maintaining baseline performance
- Decoder contribution dominates adapter performance, with LinearDecoder achieving similar results to full LinearAE

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Feature-space adapters enable univariate foundation models to process multivariate data by decoupling cross-channel dependencies into independently processable latent dimensions.
- **Mechanism**: The adapter (encoder φ) transforms the multivariate input X ∈ R^{L×D} into a latent representation Z = φ(X) where each dimension can be processed independently by the frozen univariate FM. The decoder φ^{-1} reconstructs predictions back to the original feature space. The invertibility constraint ensures the transformation preserves information needed for reconstruction.
- **Core assumption**: There exists a linear or non-linear transformation that maps correlated multivariate features into a latent space where dimensions are sufficiently decoupled for independent univariate processing while remaining invertible.
- **Evidence anchors**:
  - [abstract] "Adapters operate by projecting multivariate inputs into a suitable latent space and applying the FM independently to each dimension."
  - [Section 3.1] Definition 3.1 formalizes the adapter as φ: R^D → R^{D'} with the forecast obtained via Ŷ(X; φ) = φ^{-1}(f_{FM}(φ(X))).
  - [corpus] Related work on PCA adapters (Feofanov et al., 2024) supports feature-space transformations for channel adaptation in classification tasks.
- **Break condition**: If the underlying multivariate dependencies are irreducibly complex (e.g., highly non-linear interactions that cannot be linearly decoupled), and the adapter capacity is insufficient, the reconstruction loss will dominate and forecasting may degrade below the identity baseline.

### Mechanism 2
- **Claim**: Probabilistic adapters (VAE-style) propagate uncertainty from the latent representation through the frozen FM, enabling calibrated predictive distributions without modifying FM weights.
- **Mechanism**: By treating the latent representation Z as stochastic (with prior p(Z) ~ N(0, I)), the encoder outputs parameters of a variational posterior q_φ(Z|X). Multiple samples from this posterior, passed through the frozen FM and decoder, yield an ensemble of predictions. The variance across samples quantifies epistemic uncertainty from the adapter's representation learning.
- **Core assumption**: The frozen FM, even if deterministic (e.g., Moment), produces sufficiently diverse outputs across different latent samples; and the KL regularization in VAE training prevents posterior collapse while maintaining meaningful latent structure.
- **Evidence anchors**:
  - [Section 4.2] "For deterministic FMs such as Moment, a Bayesian treatment of adapters yields an ensemble of such predictions, which is key for accounting for the predictive uncertainty."
  - [Section 4.2, Proposition 4.1] ELBO objective: log p_θ(Y|X, f_{FM}) ≥ E_{q_φ(Z|X)}[log p_θ(Y|X, f_{FM}(Z))] - KL(q_φ(Z|X) || p(Z))
  - [corpus] No direct corpus evidence for probabilistic adapters in time series FMs; this appears novel to this work.
- **Break condition**: If β (KL weight) is too high, the posterior collapses to the prior, losing task-relevant information. If σ (likelihood noise) is misspecified, calibration degrades (Figure 6 shows sensitivity). Long horizons show systematic underconfidence (Figure 5).

### Mechanism 3
- **Claim**: Dimensionality reduction via adapters can preserve or improve forecasting performance when the underlying data manifold has lower intrinsic dimensionality than the observed feature space.
- **Mechanism**: The adapter learns to compress X into a lower-dimensional latent space (D' < D), forcing the model to capture only the most predictive components. The decoder reconstructs from these compressed representations. This is validated on synthetic data where the ground-truth manifold is known to be lower-dimensional.
- **Core assumption**: The multivariate time series has redundant or noisy dimensions that do not contribute to forecasting accuracy, and the adapter can learn to discard these while preserving signal.
- **Evidence anchors**:
  - [Section 5.2, Figure 3] "For the ETTh1 dataset with a 96-step horizon, all adapter architectures achieve optimal performance at 7 components (matching the original feature count)... at just 5 components, all adapters (except the PCA baseline) match the baseline score."
  - [Section 3.3] Synthetic data construction: "five (uncorrelated) base signals... and derives eight additional channels through linear combinations" with known 5-dimensional ground truth.
  - [corpus] PCA-based dimensionality reduction for foundation model adaptation noted in Feofanov et al. (2024) for classification, but AdaPTS extends this with learnable and probabilistic adapters.
- **Break condition**: If D' is set too low relative to the intrinsic dimensionality, information loss degrades forecasting. The paper does not provide a principled method for selecting D'; it relies on empirical sweep (Figure 3).

## Foundational Learning

- **Variational AutoEncoders (VAEs)**
  - Why needed here: The VAE adapter is a core contribution; understanding the ELBO objective, the role of the KL term, and the reparameterization trick is essential for implementing and debugging probabilistic adapters.
  - Quick check question: Can you explain why maximizing the ELBO is equivalent to minimizing KL(q_φ(Z|X) || p(Z|Y,X)) approximately?

- **Bayesian Neural Networks and Partially Stochastic Networks**
  - Why needed here: The paper frames adapters as partially stochastic BNNs (Section 4.2), drawing on theoretical guarantees from Sharma et al. (2023) for universal conditional density estimation.
  - Quick check question: What is the key condition under which a partially stochastic network (with stochasticity only in early layers) can approximate any conditional density?

- **Foundation Models for Time Series**
  - Why needed here: The method assumes familiarity with existing time series FMs (Moment, Chronos, Moirai) and their typical univariate design constraints. Understanding what "freezing" the FM means operationally is critical.
  - Quick check question: Why are most existing time series foundation models trained univariately, and what computational challenge does this create for multivariate forecasting?

## Architecture Onboarding

- **Component map**: StandardScaler/MinMaxScaler → RevIn → Adapter Encoder (φ) → Frozen FM → Adapter Decoder (φ^{-1}) → Output

- **Critical path**:
  1. Freeze FM weights after linear probing (train the forecasting head first)
  2. Initialize adapter (LinearAE, VAE, or dropout variant) with configurable D'
  3. Train adapter on training set with validation-based hyperparameter selection (use Ray Tune + HEBO as in paper)
  4. For probabilistic adapters: tune β and σ carefully (Figure 6 ablation)
  5. Evaluate on test set with MSE and calibration metrics (ECE, reliability diagrams)

- **Design tradeoffs**:
  - **Linear vs. Deep adapters**: Linear is faster and easier to optimize; deep may capture non-linear dependencies but risks overfitting on small datasets.
  - **Deterministic vs. Probabilistic**: Probabilistic enables uncertainty quantification but introduces hyperparameters (β, σ) and potential posterior collapse.
  - **Dimensionality D'**: Lower D' reduces computation but risks information loss. The paper shows D'=2 works for Illness (7 features) but not universally.
  - **PCA pre-processing vs. learned adapters**: PCA is fixed and cheap; learned adapters can adapt to the FM's behavior but require training data.

- **Failure signatures**:
  - **Degraded performance vs. identity baseline**: Indicates adapter is learning a suboptimal transformation; check initialization, learning rate, or reduce D'.
  - **Posterior collapse (VAE)**: KL term near zero; increase β or reduce encoder capacity.
  - **Poor calibration at long horizons**: Predictive distribution too narrow (Figure 5); increase σ or apply explicit recalibration.
  - **Gradient conflicts (Normalizing Flows)**: The paper notes this in Appendix B; avoid shared encoder/decoder parameters in flow-based adapters.

- **First 3 experiments**:
  1. **LinearAE baseline on ETTh1 (H=96)**: Replicate Table 1 result showing dropoutLinearAE achieves ~8% MSE improvement. Use 7 components, batch size 32, Adam with lr=0.001, reduce-on-plateau scheduler.
  2. **Ablate decoder vs. encoder contribution**: Implement LinearEncoder-only and LinearDecoder-only variants on ETTh1, Weather, and ExchangeRate to confirm Figure 7 finding that decoder dominates performance in deterministic adapters.
  3. **VAE hyperparameter sweep on Illness (H=24)**: Sweep β ∈ {0.5, 1.0, 2.0, 4.0} and log(σ²) ∈ {0.5, 1.0, 1.5, 2.0, 3.0} to replicate Figure 6 heatmaps for MSE and ECE; verify β=4.0, log(σ²)=3.0 yields best calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the AdaPTS framework maintain its forecasting accuracy and uncertainty quantification efficacy when applied to other pre-trained univariate foundation models besides Moment?
- **Basis in paper**: [Explicit] The Conclusion states, "While we focus on Moment, our approach can be applied on other univariate deterministic FMs; we leave this direction to future work."
- **Why unresolved**: The experiments were restricted to the Moment foundation model, so the generalizability of the adapters across different architectural backbones (e.g., Chronos or TimesFM) remains unverified.
- **What evidence would resolve it**: Empirical results benchmarking AdaPTS performance when paired with alternative foundation models on the same multivariate datasets.

### Open Question 2
- **Question**: Can theoretical guarantees or specific recalibration techniques be developed to address the systematic underestimation of uncertainty observed at longer prediction horizons?
- **Basis in paper**: [Explicit] The Conclusion suggests "investigating theoretical guarantees and recalibration techniques could further enhance reliability." [Inferred] Section 5.4 notes that "longer-horizon forecasts systematically underestimate uncertainty," indicating a specific failure mode.
- **Why unresolved**: The paper identifies calibration degradation over time but does not propose or test a solution to correct the overconfident predictive distributions.
- **What evidence would resolve it**: A theoretical analysis proving bounds on calibration error, or empirical demonstrations showing improved reliability diagrams at long horizons using a modified AdaPTS variant.

### Open Question 3
- **Question**: Can Normalizing Flows be effectively utilized as adapters within this framework despite the optimization challenges caused by conflicting gradient directions?
- **Basis in paper**: [Explicit] Appendix B states: "We discovered that this adapter construction was challenging to optimize in practice, and we defer the exploration of this direction to future research endeavors."
- **Why unresolved**: The authors attempted to use Normalizing Flows (RealNVP) but found that shared parameters between the encoder and decoder led to conflicting gradients, leaving this architecture unproven.
- **What evidence would resolve it**: A training strategy or architectural modification that stabilizes the optimization of Flow-based adapters, yielding performance competitive with the proposed VAE or AutoEncoder adapters.

## Limitations
- Theoretical analysis is limited to linear adapters; deep non-linear adapters lack theoretical guarantees for invertibility or universal approximation
- No principled method for selecting latent dimension D'; current approach relies on grid search which doesn't scale to high-dimensional problems
- Systematic underestimation of uncertainty at longer horizons identified but no solution proposed

## Confidence

**High Confidence**: The core mechanism of feature-space adapters for decoupling multivariate inputs (Mechanism 1) and the empirical improvements on benchmark datasets are well-supported by ablation studies and quantitative results. The theoretical analysis for linear adapters is rigorous and complete.

**Medium Confidence**: The probabilistic adapter framework (Mechanism 2) shows promising calibration improvements, but the sensitivity to hyperparameters β and σ (Figure 6) indicates the method may require careful tuning for different datasets and forecasting horizons. The lack of corpus evidence for probabilistic adapters in time series FMs suggests this contribution is novel but untested in broader contexts.

**Low Confidence**: The dimensionality reduction claims (Mechanism 3) are demonstrated on synthetic data with known ground-truth dimensionality, but the practical benefits on real-world datasets are less clear. The paper shows D'=2 works for Illness but provides no guidance on how to select D' when the intrinsic dimensionality is unknown.

## Next Checks

1. **Ablation on adapter architecture contribution**: Implement and evaluate LinearEncoder-only and LinearDecoder-only variants on ETTh1, Weather, and ExchangeRate datasets to definitively confirm Figure 7's finding that decoder dominates performance in deterministic adapters.

2. **Theoretical extension for deep adapters**: Prove or disprove whether deep non-linear adapters with ReLU activations satisfy the universal approximation conditions stated in Proposition 4.1, or identify the specific conditions under which the theoretical guarantees break down.

3. **Robustness to distributional shift**: Train the probabilistic adapter on a subset of ETTh1 features, then evaluate on the full feature set to test whether the learned latent representation generalizes to unseen channels, addressing the latent space alignment concerns raised in Figure 4.