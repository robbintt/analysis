---
ver: rpa2
title: Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language
  Models on Reasoning Traditional Bangla Tricky Riddles
arxiv_id: '2512.20324'
source_url: https://arxiv.org/abs/2512.20324
tags:
- riddle
- reasoning
- answer
- semantic
- ambiguity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models show promise on NLP benchmarks but struggle
  with figurative, culturally grounded reasoning in low-resource languages like Bangla.
  To address this, the authors introduce BANGLARIDDLEEVAL, a new benchmark of 1,244
  traditional Bangla riddles instantiated across four tasks (4,976 artifacts total),
  designed to test direct QA, reasoning, MCQ robustness, and semantic ambiguity resolution.
---

# Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles

## Quick Facts
- arXiv ID: 2512.20324
- Source URL: https://arxiv.org/abs/2512.20324
- Reference count: 15
- Large Language Models show promise on NLP benchmarks but struggle with figurative, culturally grounded reasoning in low-resource languages like Bangla

## Executive Summary
Large Language Models demonstrate strong performance on many NLP benchmarks but struggle with figurative, culturally grounded reasoning in low-resource languages like Bangla. To address this gap, the authors introduce BANGLARIDDLEEVAL, a new benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 artifacts total), designed to test direct QA, reasoning, MCQ robustness, and semantic ambiguity resolution. The benchmark reveals that while current models capture some cues needed for Bangla riddle reasoning, they remain far from human-level performance, establishing BANGLARIDDLEEVAL as a challenging benchmark for figurative reasoning in low-resource languages.

## Method Summary
The authors create BANGLARIDDLEEVAL by collecting 1,244 Bangla riddles from printed books and using GPT-4o to generate synthetic annotations including step-by-step explanations, plausible distractors, and ambiguity annotations. They evaluate diverse open-source and closed-source models under zero-shot, few-shot, and chain-of-thought prompting using BERTScore for semantic similarity and LLM-as-a-Judge (Gemini-2.5-Flash) with human validation. The benchmark includes four tasks: Generative QA, MCQ, Reasoning, and Ambiguity Resolution, each targeting different aspects of riddle-solving capability.

## Key Results
- Models achieve moderate semantic overlap on generative QA (BERTScore ~0.74–0.81) but low correctness (2–29% accuracy)
- MCQ accuracy peaks at ~56% versus 83% human baseline, showing models struggle with semantically coherent distractors
- Ambiguity resolution ranges from ~26% to 68%, indicating partial but incomplete cultural knowledge
- Chain-of-thought prompting shows mixed results, sometimes improving and sometimes degrading performance

## Why This Works (Mechanism)

### Mechanism 1
Multi-task framing isolates distinct reasoning failures in figurative language understanding. By decomposing riddle-solving into four tasks (Generative QA, MCQ, Reasoning, Ambiguity Resolution), the benchmark separates retrieval deficits from reasoning deficits from lexical disambiguation failures. Low generative accuracy (2–29%) with moderate BERTScore (0.74–0.81) indicates models produce surface-plausible but semantically incorrect answers.

### Mechanism 2
Semantically coherent distractors expose shallow pattern-matching vs. genuine reasoning. MCQ distractors are generated to be domain-related (e.g., other fruits if answer is a fruit), preventing models from exploiting simple lexical cues. The 27-point gap between best model (56%) and human baseline (83%) suggests models cannot reliably distinguish correct answers from plausible alternatives.

### Mechanism 3
Ambiguity resolution requires cultural-linguistic knowledge beyond statistical patterns. Bangla riddles use homophones and metaphorical triggers (e.g., "শায়া" meaning both "lying" and "1.25"). Models must select the intended sense from semantically close options. Performance range (26–68%) above random (25%) but below ceiling indicates partial but incomplete cultural knowledge.

## Foundational Learning

- **Figurative vs. literal language comprehension**: Why needed here - Riddles require interpreting metaphors and wordplay, not surface-level matching. Models trained predominantly on literal text may lack this capability. Quick check - Can you explain why "dark room" in a riddle might metaphorically refer to a mouth cavity rather than a literal room?

- **Low-resource language challenges in LLMs**: Why needed here - Bangla has limited representation in training corpora compared to English/Chinese, affecting both vocabulary coverage and cultural knowledge encoding. Quick check - What percentage of common crawl data is in Bangla versus English, and how might this affect model performance?

- **Evaluation bias in LLM-as-a-Judge**: Why needed here - The benchmark uses Gemini-2.5-Flash as an evaluator, which may share biases with evaluated models or systematically favor certain answer styles. Quick check - If the judge model was trained on similar data to the evaluated models, what systematic errors might go undetected?

## Architecture Onboarding

- **Component map**: OCR from riddle books -> JSON storage -> synthetic annotation (GPT-4o) -> quality filtering -> Task instantiators (4 parallel processors) -> Model inference -> LLM-as-a-Judge evaluation with human validation

- **Critical path**: 1. Raw riddle collection -> OCR extraction (Gemini-2.5-Flash) 2. Annotation generation (GPT-4o): explanations, distractors, ambiguity options 3. Model inference across three prompting regimes 4. LLM-as-a-Judge evaluation with SME spot-check validation

- **Design tradeoffs**: Synthetic vs. human annotations - Chose GPT-4o for scale (1,244 × 4 = 4,976 artifacts) but risk annotation artifacts; mitigated by manual review of samples. Proprietary vs. open evaluation - Uses Gemini-2.5-Flash as judge for consistency but introduces dependency; human validation provides partial check. Single vs. multi-language - Focused on Bangla for depth; limits cross-lingual generalization claims.

- **Failure signatures**: High BERTScore with low LLM-judge accuracy -> model produces lexically similar but semantically wrong answers. CoT underperforming zero-shot -> reasoning traces introduce confusion rather than clarity (observed in Gemma3-12B for ambiguity task). Near-random MCQ accuracy (25%) -> model fails to engage with task or relies purely on position bias.

- **First 3 experiments**: 1. Baseline diagnostic: Run all four tasks with zero-shot prompting on 3 model sizes (small/medium/large) to establish performance bands and identify which task is hardest for each tier. 2. Distractor robustness test: Replace semantically coherent distractors with random options; if accuracy jumps significantly, the original distractors were successfully adversarial. 3. Cross-prompt consistency check: For the same riddle, compare model answers across zero-shot, few-shot, and CoT; high variance suggests unstable reasoning rather than principled understanding.

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic annotation pipeline using GPT-4o without full human validation for all 4,976 artifacts introduces potential systematic biases
- Gemini-2.5-Flash used for both OCR extraction and LLM-as-a-Judge evaluation creates potential contamination
- Cultural-linguistic knowledge deficit explanation for ambiguity resolution relies heavily on correctness of annotations, which are not externally validated

## Confidence
- **High Confidence**: Benchmark construction methodology (four distinct tasks, 1,244 riddles, 4,976 artifacts) and observation that models achieve moderate BERTScore (0.74-0.81) but low accuracy (2-29%) on generative QA
- **Medium Confidence**: MCQ distractors are "semantically coherent" and effective at exposing shallow reasoning (27-point gap between best model and human baseline), though lacks external validation of distractor quality
- **Low Confidence**: Cultural-linguistic knowledge deficit explanation for ambiguity resolution performance (26-68%) relies heavily on correctness of ambiguity annotations without external validation

## Next Checks
1. **Distractor Quality Validation**: Replace current MCQ distractors with random options and measure accuracy change. If accuracy increases substantially (e.g., >20%), this confirms original distractors were effectively adversarial and the 27-point model-human gap reflects genuine reasoning limitations.

2. **Annotation Error Rate Estimation**: Perform systematic human validation on a stratified random sample of 100 riddles across all four tasks to estimate false positive rate in synthetic annotations, particularly for ambiguity triggers and meanings.

3. **Cross-Linguistic Generalization Test**: Evaluate a subset of BANGLARIDDLEEVAL tasks (e.g., only MCQ component) on an equivalent English riddle benchmark using the same models and prompting strategies. If performance patterns differ significantly, this supports the claim that Bangla-specific cultural knowledge is a key limiting factor.