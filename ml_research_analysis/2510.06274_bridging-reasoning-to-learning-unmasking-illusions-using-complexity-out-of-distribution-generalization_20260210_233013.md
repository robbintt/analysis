---
ver: rpa2
title: 'Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of
  Distribution Generalization'
arxiv_id: '2510.06274'
source_url: https://arxiv.org/abs/2510.06274
tags:
- complexity
- reasoning
- learning
- arxiv
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Complexity Out-of-Distribution (Complexity\
  \ OoD) generalization as a framework to formalize and measure reasoning ability\
  \ in AI models. It defines Complexity OoD as a model\u2019s capacity to generalize\
  \ to test instances whose minimal solution complexity\u2014either representational\
  \ (richer structure) or computational (more reasoning steps)\u2014exceeds that of\
  \ all training examples."
---

# Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization

## Quick Facts
- **arXiv ID:** 2510.06274
- **Source URL:** https://arxiv.org/abs/2510.06274
- **Reference count:** 38
- **Key outcome:** Introduces Complexity Out-of-Distribution (Complexity OoD) generalization as a framework to formalize and measure reasoning ability in AI models.

## Executive Summary
This paper proposes Complexity Out-of-Distribution (Complexity OoD) generalization as a framework to formalize and measure reasoning ability in AI models. It defines Complexity OoD as a model's capacity to generalize to test instances whose minimal solution complexity—either representational (richer structure) or computational (more reasoning steps)—exceeds that of all training examples. The authors formalize complexity via Kolmogorov complexity and practical proxies like object counts or reasoning step counts, distinguishing it from compositional and length OoD. Empirical analysis on datasets like GSM8K, AIME, and Omni-MATH reveals that model accuracy degrades significantly as problem complexity increases, with reasoning-oriented models showing gentler degradation. The framework unifies learning and reasoning by showing that System-1 tasks become System-2 under complexity pressure, while System-2 reasoning is a form of learning to generalize over solution structures. The authors recommend incorporating complexity-aware evaluation, supervision, and inductive biases for adaptive computation to build robust reasoning systems.

## Method Summary
The paper introduces Complexity Out-of-Distribution (Complexity OoD) generalization as a diagnostic framework to separate pattern matching from reasoning in AI models. It formalizes complexity using Kolmogorov complexity and operational proxies like reasoning step counts or object counts, then evaluates models on stratified test sets where problem complexity exceeds training distribution limits. The method distinguishes between representational complexity (richer structure) and computational complexity (more reasoning steps), and applies this framework to analyze model performance across multiple mathematical reasoning benchmarks including GSM8K, AIME, and Omni-MATH.

## Key Results
- Model accuracy degrades significantly as problem complexity increases beyond training distribution, with reasoning-oriented models showing gentler degradation.
- Large Language and Reasoning Models are shown to be "shallow disjunctive reasoners" that rely on memorized shortcuts rather than genuine reasoning.
- The framework unifies learning and reasoning by demonstrating that System-1 tasks become System-2 under complexity pressure, while System-2 reasoning is a form of learning to generalize over solution structures.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Complexity Out-of-Distribution (Complexity OoD) generalization serves as a diagnostic separator between pattern matching (System-1) and reasoning (System-2).
- **Mechanism:** By evaluating test instances whose *minimal required solution complexity* (representational or computational) exceeds the training maximum, the framework forces the model to generate novel solution paths rather than retrieving memorized patterns. Performance degradation on these instances reveals the limits of statistical learning.
- **Core assumption:** Solution complexity can be reliably proxied by operational metrics like reasoning step counts or object counts, approximating Kolmogorov complexity.
- **Evidence anchors:**
  - [Abstract] Defines Complexity OoD as maintaining performance when minimal solution complexity exceeds all training examples.
  - [Section 2.2] Formalizes complexity via Kolmogorov complexity $K(y|x)$ for computational complexity.
  - [Corpus] "Large Language and Reasoning Models are Shallow Disjunctive Reasoners" supports the fragility of current models under OoD pressure, noting reliance on shortcuts.
- **Break condition:** If the complexity proxy (e.g., step count) correlates poorly with the actual computational effort required (e.g., a long solution is trivial to memorize), the diagnostic power of the framework degrades.

### Mechanism 2
- **Claim:** Reasoning capabilities emerge from learning to generalize over solution structures, specifically by learning a heuristic function to navigate the search space of valid programs.
- **Mechanism:** The model does not merely map input to output; it learns a "System-1-like intuition" (a heuristic) that guides the construction of variable-length solution traces (programs). This allows the model to dynamically allocate compute based on problem difficulty, bridging the gap between learning and reasoning.
- **Core assumption:** A model capable of reasoning must possess an inductive bias for unbounded capacity (variable-depth computation) to execute these heuristics beyond training distribution limits.
- **Evidence anchors:**
  - [Section 3.2] Frames System-2 reasoning as "generalization over solution structures" and the solution as a "learned heuristic."
  - [Section 4.3] Discusses how Chain-of-Thought (CoT) and Search (ToT) enable variable-length computation.
  - [Corpus] "Unlocking Out-of-Distribution Generalization... via Recursive Latent Space Reasoning" aligns with the need for recursive/variable structures to solve OoD tasks.
- **Break condition:** If the architecture imposes a fixed computational depth (e.g., standard Transformer without recurrence or external tools), the heuristic cannot scale to required complexity levels.

### Mechanism 3
- **Claim:** Reinforcement Learning (RL) enhances reasoning generalization by training models to optimize for correctness and structure rather than just surface form.
- **Mechanism:** Unlike Supervised Fine-Tuning (SFT), which may overfit to training distributions, RL (specifically with outcome-based or process-based rewards) encourages the model to explore the solution space and allocate computational resources dynamically. This results in "gentler degradation" as complexity increases.
- **Core assumption:** The reward signal provides sufficient guidance to learn robust "verification" or "heuristic" capabilities without requiring exhaustive expert traces.
- **Evidence anchors:**
  - [Abstract] Notes that "reasoning-oriented models" (implying RL-training like DeepSeek-R1) show gentler degradation.
  - [Section 4.3] Highlights that RL methods generalize more effectively to unseen situations by optimizing policies against adaptive reward signals.
  - [Corpus] "Retrieval-Augmented Process Reward Model..." supports the efficacy of process supervision in addressing OoD challenges.
- **Break condition:** If the RL reward is hackable or the search space is too vast without adequate inductive biases, the model may learn sub-optimal strategies that fail to generalize.

## Foundational Learning

- **Concept:** Kolmogorov Complexity
  - **Why needed here:** The paper uses this as the theoretical formalization of "minimal description length" to define what makes a problem instance complex.
  - **Quick check question:** Can you explain why the Kolmogorov complexity of a string is generally uncomputable, necessitating the use of "proxies" in this framework?

- **Concept:** Inductive Biases for Variable-Length Computation
  - **Why needed here:** The paper argues that standard architectures fail Complexity OoD because they lack inductive biases for adaptive depth (e.g., recursion, halting mechanisms).
  - **Quick check question:** How does a "Universal Transformer" or "Adaptive Computation Time" differ from a standard Transformer in terms of handling variable reasoning depth?

- **Concept:** Meta-Learning & Heuristic Search
  - **Why needed here:** The paper reframes reasoning as "learning to search" using learned heuristics. Understanding how a model learns to learn (meta-learning) is key to grasping how it generalizes to unseen solution lengths.
  - **Quick check question:** In the context of this paper, how does "learning a heuristic" differ from "learning a policy" in standard reinforcement learning?

## Architecture Onboarding

- **Component map:** Complexity Estimator -> Generative Reasoner -> Verifier/Reward Model -> Adaptive Compute Controller

- **Critical path:**
  1. **Proxy Definition:** Select a proxy for complexity (e.g., step count) relevant to the domain.
  2. **Stratification:** Bin dataset by complexity to create "Complexity OoD" test splits (Test complexity > Max Train complexity).
  3. **Evaluation:** Plot accuracy vs. complexity to observe the "degradation curve" rather than a single aggregate score.

- **Design tradeoffs:**
  - **Strong vs. Weak Supervision:** Strong supervision (full traces) provides precise learning signal but is expensive and limits scalability. Weak supervision (outcomes + RL) is scalable but requires careful reward design to avoid spurious correlations.
  - **Fixed vs. Adaptive Depth:** Fixed-depth models are faster but fail Complexity OoD. Adaptive depth handles OoD but increases inference latency and architectural complexity.

- **Failure signatures:**
  - **Sharp Accuracy Drop:** Non-reasoning models (e.g., GPT-4o in the paper) show a precipitous drop in accuracy as complexity increases (Fig 3b).
  - **State Tracking Loss:** Models "forget" intermediate states in long-horizon tasks (Section 5.3), proposing invalid moves.
  - **Memorization Illusion:** High performance on low-complexity training distributions that vanishes on Complexity OoD splits, indicating data contamination or rote memorization.

- **First 3 experiments:**
  1. **Benchmark Re-evaluation:** Take a standard benchmark (like GSM8K), sort samples by the number of reasoning steps (proxy for complexity), and plot the accuracy curve of your model vs. a reasoning-specific model (like DeepSeek-R1) across bins.
  2. **Architecture Ablation:** Implement a simple "Adaptive Computation Time" (ACT) mechanism on a baseline model and measure if the accuracy degradation slope improves on high-complexity bins compared to the static baseline.
  3. **Supervision Shift:** Train two small models—one with SFT on full traces and one with RL using only final-answer rewards. Compare their generalization gap on problems requiring 2x the training max complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design and standardize complexity-aware evaluation protocols and metrics for System-2 reasoning that reliably measure Complexity OoD generalization, beyond aggregate accuracy scores?
- Basis in paper: [explicit] The authors state: "Rethinking Benchmarks: We must move towards complexity-aware evaluation, designing benchmarks that explicitly test for Complexity OoD and analyzing model performance across stratified levels of difficulty."
- Why unresolved: Current benchmarks rely on average accuracy, which obscures failure modes under complexity pressure. There is no consensus on complexity proxies or reporting standards.
- What evidence would resolve it: Creation of a benchmark suite with established complexity proxies (e.g., reasoning steps, representational richness) and demonstrated ability to discriminate between models' generalization capability, as shown by consistent, informative performance degradation curves.

### Open Question 2
- Question: What supervision paradigms can efficiently teach models to generate and refine solution traces for Complexity OoD problems, particularly when step-by-step human annotations are scarce?
- Basis in paper: [explicit] The paper argues for "Exploring New Supervision Paradigms" and "rethinking supervision to target solution traces, from final outcomes to process-level feedback and RL/search".
- Why unresolved: Strong supervision with solution traces is costly. Weak supervision from final answers alone leads to difficult credit assignment. The optimal balance and method for scalable process supervision remain unknown.
- What evidence would resolve it: A training method that uses limited process annotations or weak supervision to achieve significant improvements on Complexity OoD test instances, demonstrating superior sample efficiency and generalization compared to standard supervised fine-tuning.

### Open Question 3
- Question: What novel architectural inductive biases are required to endow models with the unbounded representational capacity and adaptive computational depth necessary to overcome Complexity OoD?
- Basis in paper: [explicit] The conclusion states that achieving System-2 AI "demands a fundamental shift in how we evaluate, build, and train models, a shift that equips them with the right inductive biases for generalizing across complexity."
- Why unresolved: Standard architectures (e.g., Transformers) have fixed computational depth and struggle with long-horizon state tracking. Principles for adaptive computation and external memory integration are not well-established.
- What evidence would resolve it: An architecture with a proposed inductive bias (e.g., for recursion, dynamic halting, or external scratchpad) that maintains or improves performance on complexity-stratified benchmarks as the complexity scale increases, outperforming fixed-depth baselines.

## Limitations

- The framework's reliance on proxy metrics for Kolmogorov complexity (like step counts or object counts) represents a fundamental limitation, as these may not capture the true computational complexity of solutions.
- The empirical analysis is limited to mathematical reasoning domains where solution traces are more readily interpretable than in open-ended reasoning tasks.
- The distinction between representational and computational complexity, while theoretically sound, may blur in practice when both dimensions interact in real reasoning tasks.

## Confidence

- **High Confidence:** The observation that model performance degrades significantly as problem complexity increases beyond training distribution is empirically well-supported across multiple datasets (GSM8K, AIME, Omni-MATH). The distinction between learning and reasoning as complementary capabilities rather than opposing mechanisms is theoretically coherent.
- **Medium Confidence:** The claim that RL-trained models show "gentler degradation" on complexity OoD splits is supported but could benefit from more systematic ablation studies comparing different RL approaches and reward structures. The assertion that reasoning is fundamentally "learning to search" over solution structures is compelling but remains at a conceptual level without detailed algorithmic implementation.
- **Low Confidence:** The formalization of complexity OoD using Kolmogorov complexity as the theoretical foundation, while elegant, has limited practical applicability given the uncomputability of Kolmogorov complexity. The proposed solution of using operational proxies lacks rigorous validation for capturing true solution complexity.

## Next Checks

1. **Cross-Domain Complexity Proxy Validation:** Systematically test whether the same complexity proxies (e.g., step counts, object counts) maintain their correlation with actual solution difficulty across diverse reasoning domains (mathematical, logical, commonsense reasoning) to establish proxy generalizability.

2. **Architecture-Independent Complexity Analysis:** Evaluate whether the observed degradation patterns persist across fundamentally different model architectures (RNNs, Transformers, Neural-Symbolic hybrids) to determine if the complexity OoD phenomenon is architecture-agnostic or specific to current LLM designs.

3. **Reward Design Impact Study:** Conduct controlled experiments varying reward structures in RL training (process-based vs. outcome-based, sparse vs. dense) to quantify their impact on complexity OoD generalization and identify optimal reward designs for building robust reasoning systems.