---
ver: rpa2
title: Is Temporal Difference Learning the Gold Standard for Stitching in RL?
arxiv_id: '2510.21995'
source_url: https://arxiv.org/abs/2510.21995
tags:
- stitching
- learning
- boxes
- methods
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper empirically studies whether temporal difference (TD)
  learning is necessary for experience stitching in reinforcement learning. It introduces
  a controlled benchmark environment to test stitching capabilities across three regimes:
  no stitching, exact stitching, and generalized stitching.'
---

# Is Temporal Difference Learning the Gold Standard for Stitching in RL?

## Quick Facts
- arXiv ID: 2510.21995
- Source URL: https://arxiv.org/abs/2510.21995
- Reference count: 17
- One-line primary result: Scaling critic networks is a more powerful lever for experience stitching than the choice between temporal difference and Monte Carlo learning paradigms

## Executive Summary
This paper challenges the conventional wisdom that temporal difference (TD) learning is essential for experience stitching in reinforcement learning. Through controlled experiments in goal-conditioned grid-world environments, the authors demonstrate that Monte Carlo methods can achieve stitching capabilities comparable to TD methods, particularly in generalized stitching regimes. The study systematically compares stitching performance across three evaluation regimes and finds that increasing critic network scale effectively reduces the generalization gap for both TD and MC methods, suggesting that model capacity rather than learning paradigm is the primary determinant of stitching success.

## Method Summary
The authors introduce a custom "Sokoban-style" grid environment to study experience stitching capabilities in goal-conditioned RL. They evaluate stitching across three regimes: no stitching (training and test distributions overlap), exact stitching (composing training trajectories for test solutions), and generalized stitching (transferring skills to novel state-goal pairs). The study compares TD and Monte Carlo variants of DQN, C-Learning, and CRL methods, using both small (MLP) and large (ResNet) critic architectures. Agents are trained for 500M transitions and evaluated on their ability to solve held-out test distributions, with success rate and generalization gap as key metrics.

## Key Results
- Monte Carlo methods achieve stitching comparable to TD methods in generalized stitching regimes
- TD methods provide only slight advantages in exact stitching that degrade with task complexity
- Increasing critic network scale significantly improves stitching for both TD and MC methods, narrowing the performance gap
- The generalization gap can be reduced more effectively by scaling models than by choosing between TD and MC paradigms

## Why This Works (Mechanism)

### Mechanism 1: Implicit Stitching via Representation Generalization
- **Claim:** Monte Carlo (MC) methods can achieve experience stitching comparable to Temporal Difference (TD) methods in generalized stitching regimes, even without explicit bootstrapping.
- **Mechanism:** In function approximation settings, MC methods learn value functions that assign similar values to similar states. This representation learning allows the agent to generalize across disconnected trajectories, effectively "stitching" them by inferring values for unseen state-goal pairs based on their similarity to training data, provided the test states remain on the training support (closed setups).
- **Core assumption:** The test-time queries involve state-goal pairs where the individual states are within the distribution of training data, even if the specific pairings are novel.
- **Evidence anchors:** [abstract] "We empirically demonstrate that Monte Carlo (MC) methods can also achieve experience stitching." [section 5.3] "This result might come as a surprise... however, they are still able to work well in this setup, most likely due to implicit stitching on the representation level." [corpus] Related work (e.g., *Closing the Gap between TD Learning and Supervised Learning...*) supports the convergence of supervised-learning-style (SL) methods and TD capabilities, suggesting the gap is bridgeable via representation.
- **Break condition:** Performance degrades if the evaluation requires "open" stitching where intermediate states fall outside the training support, or if exploration is insufficient (as seen in standard DQN-MC).

### Mechanism 2: Scaling as the Primary Driver of Stitching
- **Claim:** Increasing the scale (capacity) of the critic network significantly improves stitching capabilities for both TD and MC methods, acting as a more powerful lever than the choice of learning paradigm.
- **Mechanism:** Larger networks (e.g., ResNet blocks with higher layer counts) possess greater representational capacity. This allows them to model the complex Q-functions required for stitching with higher fidelity, reducing the generalization gap by mitigating underfitting and improving the smoothness of the value landscape across state-goal pairings.
- **Core assumption:** The benefits of scale apply generally across learning paradigms (TD vs. MC) and are not architecture-specific in this context.
- **Evidence anchors:** [abstract] "We find that increasing critic capacity effectively reduces the generalization gap for both the MC and TD methods." [section 5.4] "Strikingly, the generalization gap might be reduced by simply increasing the scale of the critic for both TD and MC methods." [corpus] *Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning* aligns with the focus on scalability and representation depth for long-horizon tasks.
- **Break condition:** Returns diminish if the bottleneck is data coverage or exploration rather than representational capacity; also, scale did not salvage performance in "open" evaluation settings where test states were strictly OOD.

### Mechanism 3: The "Open" Evaluation Failure Mode
- **Claim:** Stitching capabilities (even for TD) degrade rapidly in "exact stitching" regimes if the composed solution path traverses intermediate states that were never visited during training (Open Setup).
- **Mechanism:** In complex tasks (e.g., many boxes), composing sub-trajectories often requires passing through "waypoint" states that are valid solutions but lie outside the empirical state support $M$ of the training data. When the agent encounters these off-support states during execution, the value function extrapolates poorly, leading to failure.
- **Core assumption:** Effective stitching requires not just connecting $s \to g$, but ensuring the path $s \to \dots \to g$ remains on the support of the training distribution.
- **Evidence anchors:** [section 4] "We label a setup open if test solutions are likely to leave the training support... This evaluation confuses assessing methodsâ€™ stitching capabilities with their perceptual robustness to out-of-distribution data." [section 5.2] "Visual inspection confirms more failures caused by off-support observations with an increased number of boxes (Figure 5)." [corpus] *ASTRO: Adaptive Stitching...* addresses trajectory fragmentation, highlighting the difficulty of stitching when dynamics or data support is insufficient.
- **Break condition:** If the training data coverage is comprehensive enough to cover the waypoint states (converting Open to Closed), this failure mode is avoided.

## Foundational Learning
- **Concept: Temporal Difference (TD) vs. Monte Carlo (MC) Learning**
  - **Why needed here:** The paper core is a comparative study. You must understand that TD updates bootstrap (estimate value from subsequent estimates) while MC updates rely on full episode returns.
  - **Quick check question:** Does the target update for $Q(s,a)$ look at $r + \gamma Q(s', a')$ (TD) or $G_t$ (MC)?
- **Concept: Experience Stitching (Compositional Generalization)**
  - **Why needed here:** This is the capability being tested. It is the ability to combine fragments of training trajectories (e.g., A->B and B->C) to solve a novel test case (A->C).
  - **Quick check question:** If an agent sees paths from $s \to w$ and $w \to g$ during training, can it infer a policy for $s \to g$ at test time?
- **Concept: Goal-Conditioned RL (GCRL)**
  - **Why needed here:** The benchmark tasks are goal-conditioned (moving boxes to targets). Understanding that the policy/value function is conditioned on both state $s$ and goal $g$ is required to interpret the "generalization gap" metric.
  - **Quick check question:** Does the critic take only the state $s$ as input, or the pair $(s, g)$?

## Architecture Onboarding
- **Component map:** Environment (Block Moving grid world) -> Critic ($Q(s, g)$ network) -> Policy (Boltzmann derived from $Q$)
- **Critical path:**
  1. Instantiate the "Quarters" or "Few-to-Many" environment
  2. Train agent on the restricted training distribution (e.g., adjacent quarters only)
  3. Evaluate on the held-out test distribution (e.g., diagonal quarters)
  4. Compute **Generalization Gap** (Train Success - Test Success)
- **Design tradeoffs:**
  - **Small vs. Large Critic:** Small is faster but fails to stitch in complex scenarios; Large narrows the TD/MC gap but requires more compute
  - **Quasimetric Networks:** The paper found they decreased performance because the specific environment dynamics were symmetric, making the asymmetric inductive bias unnecessary
- **Failure signatures:**
  - **Exact Stitching Collapse:** Success rate drops to 0% as box count increases (e.g., Figure 4) due to OOD "open" states
  - **Exploration Starvation:** DQN-MC fails to learn effectively compared to CRL/C-Learning MC due to poor data collection (Section 5.3)
- **First 3 experiments:**
  1. **Verify MC Stitching:** Run CRL (MC) and DQN (TD) on the "Few-to-Many" task (Figure 6) to confirm MC methods generalize despite the lack of bootstrapping
  2. **Scale Ablation:** Train DQN and CRL with the "Small" (MLP) vs. "Big" (ResNet) critic on the generalized stitching task to reproduce the gap narrowing shown in Figure 8
  3. **Open vs. Closed Test:** Run evaluation on "Quarters" (Exact Stitching) with 4 boxes. Observe the failure mode where the agent creates off-support states by dropping boxes in diagonal transit (Figure 5)

## Open Questions the Paper Calls Out
- **Question:** Does stitching capability effectively transfer to separately-parameterized actor policies?
  - **Basis in paper:** [explicit] The authors state in the Limitations section: "We also did not investigate stitching or generalization produced by a separately-parameterized actor policy."
  - **Why unresolved:** The study focused exclusively on critic-based stitching capabilities; it remains unknown if an actor can successfully distill these capabilities without degradation.
  - **What evidence would resolve it:** Experiments where actors are trained via distillation from the critics analyzed in this paper, evaluated on the same stitching benchmarks.

- **Question:** Do the benefits of model scale and Monte Carlo stitching persist in high-dimensional continuous control domains?
  - **Basis in paper:** [explicit] The authors note the "reliance on a relatively simple grid-world with a small action space" and suggest future work should study "scaling to richer, continuous environments."
  - **Why unresolved:** The findings are based on a discrete grid-world; it is unclear if the reduced generalization gap from scaling critics applies to complex perception tasks or continuous dynamics.
  - **What evidence would resolve it:** Replicating the scaling experiments (TD vs. MC with varying critic widths) on standard continuous control benchmarks like MuJoCo robotics tasks.

- **Question:** Can explicit regularization techniques prevent the "open" stitching failures observed in exact stitching regimes?
  - **Basis in paper:** [explicit] The paper observes that TD methods fail when trajectories leave the training support and argues that results "pattern... for methods that regularize agent behavior in online RL so agents remain closer to the training observation distribution."
  - **Why unresolved:** The paper identifies the failure mode (off-support states) but does not experimentally validate regularization as a solution.
  - **What evidence would resolve it:** Incorporating behavior regularization losses (e.g., KL constraints) during training to test if they maintain on-support behavior and improve exact stitching success rates.

## Limitations
- Results are tightly coupled to deterministic grid-world environment and may not generalize to stochastic or continuous domains
- Study focuses exclusively on value-based methods without actor-critic architectures, leaving open questions about stitching in policy gradient frameworks
- Does not investigate computational trade-offs of scaling, including training time and sample efficiency

## Confidence
- **High Confidence:** Network scale is a more powerful lever for stitching than the choice between TD and MC paradigms (supported by clear empirical evidence)
- **Medium Confidence:** MC methods can achieve stitching comparable to TD methods in generalized stitching regimes (demonstrated but may not hold in more complex domains)
- **Medium Confidence:** TD methods provide only slight advantages in exact stitching that degrade with task complexity (well-supported but limited to tested complexity range)

## Next Checks
1. Test whether the MC-TD stitching parity holds in stochastic environments where bootstrapping provides variance reduction benefits beyond pure generalization
2. Evaluate stitching performance across different architecture families (CNNs, Transformers) to determine if ResNet-specific properties drive the scaling benefits
3. Investigate the computational trade-offs of scaling by measuring training time and sample efficiency across small and large architectures to determine if the generalization gains justify the increased resource requirements