---
ver: rpa2
title: 'Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations'
arxiv_id: '2505.05056'
source_url: https://arxiv.org/abs/2505.05056
tags:
- speech
- teochew
- data
- characters
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Teochew-Wild, the first publicly available
  Teochew speech corpus annotated with both standard Chinese characters and Teochew
  pinyin. The dataset comprises 18.9 hours of in-the-wild speech data from 20 speakers,
  covering diverse topics and expressions.
---

# Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations

## Quick Facts
- arXiv ID: 2505.05056
- Source URL: https://arxiv.org/abs/2505.05056
- Authors: Linrong Pan; Chenglong Jiang; Gaoze Hou; Ying Gao
- Reference count: 31
- Primary result: First publicly available Teochew speech corpus with dual character and pinyin annotations, achieving MOS scores of 3.52/3.22 for TTS and CER/WER scores of 10.01/15.03 for ASR

## Executive Summary
This paper introduces Teochew-Wild, the first publicly available Teochew speech corpus annotated with both standard Chinese characters and Teochew pinyin. The dataset comprises 18.9 hours of in-the-wild speech data from 20 speakers, covering diverse topics and expressions. The corpus includes orthographic and pinyin annotations, along with supplementary text processing tools for tasks like polyphone disambiguation and Mandarin-to-Teochew mapping. Experimental results demonstrate the dataset's effectiveness in speech tasks: MOS scores of 3.52 and 3.22 for Tacotron2 and FastSpeech2 in TTS, and CER/WER scores of 10.01/15.03 for fine-tuned Whisper-medium in ASR. The dataset supports future research in low-resource Teochew speech recognition and synthesis.

## Method Summary
The authors collected 18.9 hours of Teochew speech from internet multimedia sources, processed through a multi-stage pipeline involving source separation, denoising, and SNR-based filtering. Annotation proceeded in two stages: first transcribing Mandarin with OCR support, then mapping to Teochew orthography using a 1,500+ entry vocabulary dictionary and resolving polyphones through a 10,000+ word-group lexicon. The resulting dataset includes 12,500 utterances split into train (11,100), validation (700), and test (700) sets with dual annotations. TTS models (Tacotron2, FastSpeech2) were trained for 30,000 steps, while ASR models (Fairseq S2T Transformer XS and Whisper-medium) were fine-tuned for 10 epochs.

## Key Results
- Teochew-Wild achieves MOS scores of 3.52 for Tacotron2 and 3.22 for FastSpeech2 in TTS tasks
- Fine-tuned Whisper-medium achieves CER of 10.01% and WER of 15.03% on Teochew speech
- Rule-based polyphone disambiguation and Mandarin-to-Teochew mapping tools successfully annotate the corpus without requiring expert annotators

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-the-wild speech data can achieve quality comparable to studio recordings when processed through a multi-stage pipeline combining source separation, denoising, and SNR-based filtering
- Mechanism: Raw audio undergoes: (1) UVR model for vocal extraction from background music, (2) Resemble-Enhance for secondary denoising, (3) Silero-VAD for segmentation into 1-20 second utterances, and (4) SNR-based filtering to remove low-quality clips
- Core assumption: The UVR and Resemble-Enhance models generalize across acoustic conditions present in Teochew internet multimedia content
- Evidence anchors: DNSMOS P.835 OVRL score of 3.12 indicates speech quality comparable to other datasets sourced from in-the-wild data, studio recordings, or audiobooks
- Break condition: If input audio contains speaker overlap or heavy reverberation beyond UVR's separation capability, the pipeline may produce artifacts unsuitable for training

### Mechanism 2
- Claim: A two-stage annotation process with rule-based text frontend tools reduces annotation burden while maintaining accuracy for low-resource languages with complex polyphony
- Mechanism: Stage 1 uses native speakers for coarse Mandarin transcription (with OCR support for subtitles). Stage 2 maps Mandarin to Teochew orthography using a 1,500+ entry vocabulary dictionary and resolves polyphones through a 10,000+ word-group lexicon
- Core assumption: Polyphone disambiguation can be sufficiently handled by word-group rules without requiring full context-aware models
- Evidence anchors: 2,256 polyphonic characters, 364 of which have more than three pronunciations, are handled through a lexicon containing over 10,000 word groups
- Break condition: If Teochew vocabulary contains significant code-switching or neologisms outside the mapping dictionary, annotation accuracy degrades

### Mechanism 3
- Claim: Pre-trained multilingual speech models (Whisper) can adapt to unseen low-resource languages with minimal fine-tuning data when provided with consistent orthographic annotations
- Mechanism: Whisper-medium, pre-trained on large-scale multilingual data, is fine-tuned for 10 epochs on Teochew-Wild
- Core assumption: Whisper's acoustic encoder captures language-universal phonetic features that transfer to Teochew's 8-tone system
- Evidence anchors: Whisper model, after fine-tuning for 10 epochs on Teochew it had not previously encountered, achieved CERs of 9.61% and 10.01%
- Break condition: If the target language has phonemic distinctions absent in Whisper's pre-training distribution, fine-tuning may converge to suboptimal local minima

## Foundational Learning

- Concept: **Tone Sandhi in Sinitic Languages**
  - Why needed here: Teochew has 8 fundamental tones with context-dependent changes during continuous speech, more complex than Mandarin's 4 tones
  - Quick check question: Can you explain why Tacotron2 (autoregressive) outperformed FastSpeech2 (non-autoregressive) on tone sandhi in the paper's experiments?

- Concept: **Grapheme-to-Phoneme (G2P) Conversion**
  - Why needed here: Teochew lacks standardized orthography; the paper constructs a lexicon mapping Chinese characters to Teochew pinyin (Pengim) and IPA
  - Quick check question: How would you handle a character with 4 distinct pronunciations (literary vs. vernacular) in a rule-based G2P system?

- Concept: **In-the-Wild Data Processing**
  - Why needed here: The dataset source (internet multimedia) introduces noise, background music, and variable recording conditions
  - Quick check question: What is the failure mode if you skip SNR-based filtering before TTS training?

## Architecture Onboarding

- Component map: Raw Audio/Video → Speaker Selection → Standardization (22050Hz mono) → Source Separation (UVR) → Denoising (Resemble-Enhance) → VAD (Silero) → Segmentation → SNR Filtering → Annotation Stage 1: OCR/Manual Mandarin Transcription → Text Frontend: G2P + Polyphone Disambiguation + Word Mapping → Annotation Stage 2: Teochew Orthography + Pinyin → Training Sets: 11,100 train / 700 val / 700 test

- Critical path: Audio preprocessing quality determines TTS naturalness (DNSMOS correlation); Text frontend accuracy gates ASR CER (homophone disambiguation affects character-level errors); Fine-tuning convergence depends on annotation consistency across speakers

- Design tradeoffs: In-the-wild vs. Studio (wild data provides authentic colloquial expressions but requires heavier preprocessing; studio would be cleaner but lacks diversity); Rule-based vs. Neural G2P (rules are interpretable and require no training data, but fail on out-of-vocabulary words); Character vs. Pinyin Annotation (characters enable direct text output but suffer from homophones; pinyin is phonetically precise but loses semantics)

- Failure signatures: Tone flattening in synthesis (FastSpeech2's non-autoregressive nature fails to model tone sandhi context → lower MOS); High CER with character annotations (homophone confusion in Fairseq S2T); Rare character underrepresentation (1,800+ characters appear <10 times → potential ASR errors on low-frequency vocabulary)

- First 3 experiments: 1) Train Fairseq S2T Transformer XS on Teochew-Wild with both character and pinyin annotations; verify CER ≈ 39/17 on validation set; 2) Fine-tune Whisper-small/medium/large on Teochew-Wild; plot CER vs. model size to identify compute-quality tradeoff; 3) Train TTS on (a) raw audio without denoising, (b) UVR-only, (c) full pipeline; compare MOS to isolate source separation contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can self-supervised learning (SSL) methods effectively leverage the large amount of remaining unlabeled raw data to improve ASR/TTS performance in this low-resource setting?
- Basis in paper: The authors collected over 200 hours of raw data but only annotated 20 hours, explicitly stating the "remaining data is utilized as unsupervised data in future research"
- Why unresolved: The current experimental results are based solely on the supervised 18.9-hour dataset, leaving the potential of the 180+ hours of untranscribed data untapped
- What evidence would resolve it: Experiments integrating the raw data via wav2vec 2.0 or similar SSL pre-training, showing reduced CER/WER compared to the current supervised-only baseline

### Open Question 2
- Question: How can non-autoregressive (NAR) TTS models be adapted to handle the complex tone sandhi of Teochew, which currently causes them to underperform compared to autoregressive models?
- Basis in paper: The authors note that existing sandhi rules are "insufficient" and annotations use original tones; consequently, the NAR model (FastSpeech2) achieved a significantly lower MOS (3.22) than the AR model (Tacotron2, 3.52)
- Why unresolved: The paper identifies the performance gap but does not propose a solution for capturing context-dependent tone variations in NAR architectures
- What evidence would resolve it: A modified FastSpeech2 architecture or training objective that achieves statistical parity with Tacotron2 in MOS scores for naturalness

### Open Question 3
- Question: Does the ad-hoc supplementation of the writing system (assigning new meanings to rarely used characters) introduce tokenization or embedding sparsity issues in pre-trained models?
- Basis in paper: The authors "select rarely used characters from ancient Chinese texts and assign them new pronunciations" to cover missing words, creating a non-standard orthography
- Why unresolved: While this ensures every pronunciation has a character, it is unclear if these low-frequency "revived" characters negatively impact the latent space of models pre-trained on standard Chinese text
- What evidence would resolve it: An ablation study comparing model convergence speed and performance using the ad-hoc characters versus a purely phonemic (Pinyin) representation

## Limitations
- The preprocessing pipeline's effectiveness on Teochew-specific acoustic characteristics is not independently validated
- The rule-based polyphone disambiguation system's coverage and error rates on unseen vocabulary are not quantified
- The Whisper fine-tuning results are limited to a single model size (medium) without exploring the full model family

## Confidence

**High confidence**: The dataset creation methodology is well-documented and reproducible; the claim that Teochew-Wild is the first publicly available Teochew corpus with dual annotations is verifiable

**Medium confidence**: The ASR and TTS performance metrics (CER/WER/MOS scores) are likely accurate for the reported experimental conditions, but their generalization to other models or larger datasets is uncertain

**Low confidence**: The assertion that large pre-trained models exhibit "robust generalization and adaptability" for low-resource languages is supported only by Whisper fine-tuning results on a single language, without ablation studies or comparisons to other foundation models

## Next Checks

1. **Preprocessing ablation study**: Evaluate the full audio pipeline by measuring DNSMOS scores on raw vs. processed audio samples, and test ASR/TTS performance degradation when skipping individual preprocessing steps (source separation, denoising, SNR filtering)

2. **Polyphone disambiguation error analysis**: Sample 100 randomly selected annotated utterances and manually verify the accuracy of polyphone disambiguation, particularly for characters with 3+ pronunciations and low-frequency vocabulary

3. **Cross-lingual transfer validation**: Fine-tune Whisper-medium on Teochew-Wild, then test on another Sinitic language (e.g., Cantonese or Taiwanese Hokkien) to assess whether the claimed acoustic feature transfer generalizes beyond Teochew