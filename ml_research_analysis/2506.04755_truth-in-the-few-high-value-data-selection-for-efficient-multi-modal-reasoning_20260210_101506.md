---
ver: rpa2
title: 'Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning'
arxiv_id: '2506.04755'
source_url: https://arxiv.org/abs/2506.04755
tags:
- reasoning
- samples
- multi-modal
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of multi-modal large language
  models (MLLMs) in complex reasoning tasks due to data redundancy and high computational
  costs. The authors observe that meaningful multi-modal reasoning is triggered by
  only a sparse subset of training samples, termed cognitive samples, while most samples
  contribute minimally.
---

# Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning

## Quick Facts
- **arXiv ID:** 2506.04755
- **Source URL:** https://arxiv.org/abs/2506.04755
- **Reference count:** 40
- **Primary result:** RAP achieves superior performance using only 9.3% of training data, reducing computational costs by over 43%

## Executive Summary
This paper addresses the inefficiency of multi-modal large language models (MLLMs) in complex reasoning tasks by proposing a data selection framework that identifies high-value "cognitive samples" from training data. The authors observe that only a sparse subset of samples actually triggers meaningful multi-modal reasoning, while most contribute minimally to performance. Their proposed Reasoning Activation Potential (RAP) framework uses two complementary estimators to identify these cognitive samples and a difficulty-aware replacement mechanism to ensure sufficient complexity. Experiments across six datasets demonstrate that RAP achieves superior performance while using only 9.3% of the training data and reducing computational costs by over 43%.

## Method Summary
The paper introduces RAP (Reasoning Activation Potential), a novel data selection paradigm that identifies cognitive samples through two complementary estimators: the Causal Discrepancy Estimator (CDE) eliminates samples overly relying on language priors by comparing multi-modal and text-only outputs, while the Attention Confidence Estimator (ACE) filters out samples dominated by irrelevant tokens based on token-level self-attention analysis. Additionally, a Difficulty-aware Replacement Module (DRM) substitutes trivial instances with cognitively challenging ones to ensure robust reasoning. The framework operates on the observation that meaningful multi-modal reasoning is triggered by only a sparse subset of training samples, enabling dramatic reductions in computational costs while maintaining or improving performance.

## Key Results
- RAP achieves superior performance using only 9.3% of training data compared to full-data training
- Computational costs reduced by over 43% while maintaining or improving accuracy
- The framework demonstrates effectiveness across six diverse multi-modal reasoning datasets
- Performance gains are attributed to focusing on high-value cognitive samples that trigger meaningful reasoning

## Why This Works (Mechanism)
The framework exploits the observation that multi-modal reasoning is triggered by only a sparse subset of training samples. By identifying and prioritizing these "cognitive samples" through CDE and ACE estimators, the model learns more efficiently from high-value data while avoiding computational waste on redundant or trivial samples. The CDE ensures samples require actual multi-modal reasoning rather than language priors alone, while ACE filters out samples dominated by irrelevant attention patterns. The DRM component maintains sufficient complexity in the training data to prevent underfitting.

## Foundational Learning
- **Causal Discrepancy Analysis:** Comparing multi-modal vs text-only model outputs to identify samples requiring actual visual reasoning (needed to filter language-prior dependent samples; quick check: measure output divergence between models)
- **Attention Pattern Analysis:** Examining token-level self-attention to identify samples dominated by irrelevant tokens (needed to filter noisy samples; quick check: visualize attention heatmaps for sample classification)
- **Difficulty-aware Sampling:** Dynamically replacing trivial samples with more challenging ones (needed to maintain learning complexity; quick check: track sample difficulty distribution over training)
- **Cognitive Sample Identification:** Recognizing samples that trigger meaningful multi-modal reasoning (needed for efficient data selection; quick check: measure reasoning activation frequency)
- **Multi-modal Reasoning Activation:** Understanding when and why multi-modal models engage visual reasoning (needed to validate cognitive sample concept; quick check: analyze reasoning trace activation patterns)

## Architecture Onboarding

**Component Map:** Input Data -> CDE + ACE Estimators -> Cognitive Sample Filter -> DRM Module -> Selected Training Subset

**Critical Path:** Data → CDE/ACE → Filter → DRM → Model Training (main loop: compute CDE and ACE scores → filter samples → apply DRM → train on selected subset)

**Design Tradeoffs:** The framework trades potential loss of implicit knowledge in filtered samples against computational efficiency gains. CDE and ACE may incorrectly filter samples that encode subtle reasoning patterns. The 9.3% threshold is heuristic and may not generalize across tasks.

**Failure Signatures:** 
- Performance degradation when cognitive samples are incorrectly identified
- Underfitting if DRM fails to maintain sufficient complexity
- Overfitting to specific reasoning patterns if sample diversity is too low
- Computational savings diminish if filtering threshold is too permissive

**First Experiments:**
1. Ablation study: Train with only CDE, only ACE, and both estimators to quantify individual contributions
2. Internal analysis: Compare attention patterns and reasoning traces between RAP-trained and full-data models
3. Generalization test: Apply RAP to additional multi-modal reasoning tasks beyond the six evaluated datasets

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The 9.3% cognitive sample threshold is dataset-dependent and may not generalize to other reasoning tasks
- Lacks ablation studies on the individual contributions of CDE and ACE estimators
- Does not analyze whether RAP induces different internal reasoning patterns compared to full-data training
- Computational savings based on FLOPs reduction, but practical runtime and memory usage comparisons are absent
- Model architecture specification (Qwen2.5-VL-7B) is not clearly detailed in the main text

## Confidence

**Major claim clusters:**
- **RAP identifies high-value cognitive samples:** Medium confidence - While downstream results support this, internal reasoning analysis is missing
- **9.3% data sufficiency claim:** Low confidence - This percentage is task-specific and lacks generalizability analysis
- **Computational efficiency gains:** Medium confidence - FLOPs reduction is shown but practical runtime metrics are absent

## Next Checks
1. Conduct ablation studies to quantify the individual contributions of CDE and ACE estimators to final performance
2. Perform analysis of internal attention patterns and reasoning traces to verify that RAP-trained models actually use different reasoning strategies
3. Test the method across diverse multi-modal reasoning tasks beyond the six evaluated datasets to assess generalizability of the cognitive sample identification