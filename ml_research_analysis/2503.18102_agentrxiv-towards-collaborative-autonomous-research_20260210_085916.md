---
ver: rpa2
title: 'AgentRxiv: Towards Collaborative Autonomous Research'
arxiv_id: '2503.18102'
source_url: https://arxiv.org/abs/2503.18102
tags:
- research
- arxiv
- agents
- preprint
- autonomous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AgentRxiv, a framework enabling collaborative
  autonomous research among LLM agents through a shared preprint server. The key innovation
  is allowing agents to upload and retrieve research papers, building upon each other's
  discoveries.
---

# AgentRxiv: Towards Collaborative Autonomous Research

## Quick Facts
- arXiv ID: 2503.18102
- Source URL: https://arxiv.org/abs/2503.18102
- Authors: Samuel Schmidgall; Michael Moor
- Reference count: 33
- Key outcome: Agents with access to prior research achieve 11.4% relative improvement on MATH-500 compared to isolated agents

## Executive Summary
This paper introduces AgentRxiv, a framework enabling collaborative autonomous research among LLM agents through a shared preprint server. The key innovation is allowing agents to upload and retrieve research papers, building upon each other's discoveries. Experiments show that agents with access to prior research achieve 11.4% relative improvement on MATH-500 compared to isolated agents, with the best discovered reasoning technique improving performance by 3.3% across multiple benchmarks and language models. Parallel execution with 3 labs further accelerates progress, achieving 13.7% relative improvement. The framework demonstrates that autonomous agents can iteratively improve and generalize reasoning techniques, suggesting potential for collaborative AI-driven scientific discovery.

## Method Summary
The framework uses the Agent Laboratory framework with o3-mini backend, running in three phases: Literature Review (pulling N=5 papers from AgentRxiv and N=5 from arXiv), Experimentation (using mle-solver for code generation and testing), and Report Writing (paper-solver for uploading findings). Agents pursue the research direction "Improve accuracy on MATH-500 using reasoning and prompt engineering" for N=40 paper generations. AgentRxiv uses SentenceTransformer embeddings with cosine similarity for retrieval. Key hyper-parameters include mle-solver steps=3, code repair attempts=2, timeout=6000s, and temperature=0.8.

## Key Results
- Sequential agents with AgentRxiv access improve from 70.2% to 78.2% accuracy over 40 generations
- Parallel execution with 3 labs achieves 13.7% relative improvement over baseline
- Discovered Simultaneous Divergence Averaging technique improves performance by 3.3% across GPQA, MMLU-Pro, and MedQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents with access to prior research outperform isolated agents.
- Mechanism: A shared preprint server (AgentRxiv) lets agents retrieve and iteratively build upon prior papers, producing cumulative refinements rather than independent restarts.
- Core assumption: Agents can correctly interpret, adapt, and improve on earlier methods without amplifying errors.
- Evidence anchors:
  - [abstract] "agents with access to their prior research achieve higher performance improvements compared to agents operating in isolation (11.4% relative improvement over baseline on MATH-500)."
  - [section 3.1] Accuracy rises from 70.2% to 78.2% over 40 generations; an ablation without AgentRxiv access plateaus near 73.4â€“73.8%.
  - [corpus] Related systems (AutoLabs, AgenticSciML) demonstrate multi-agent self-correction and collaborative discovery, supporting the principle broadly, but not this exact system.
- Break condition: Retrieved papers are noisy, hallucinated, or misinterpreted, leading to error propagation rather than progress.

### Mechanism 2
- Claim: Parallel labs sharing findings via AgentRxiv accelerate discovery in wall-clock time.
- Mechanism: Multiple laboratories run concurrently, asynchronously uploading and retrieving papers, enabling cross-pollination without sequential bottlenecks.
- Core assumption: Labs explore sufficiently diverse methods; redundancy does not overwhelm the system.
- Evidence anchors:
  - [abstract] "Parallel execution with 3 labs further accelerates progress, achieving 13.7% relative improvement."
  - [section 3.2] Parallel setup reaches 76.2% after 7 papers vs 23 papers sequentially; best accuracy 79.8% vs 78.2%.
  - [corpus] Conceptually aligned proposals (SCP) outline global agent networks, but direct empirical confirmation for this architecture is limited.
- Break condition: Labs converge on similar ideas or produce low-quality papers, increasing cost without proportional gain.

### Mechanism 3
- Claim: Discovered reasoning techniques generalize across benchmarks and models.
- Mechanism: Methods like Simultaneous Divergence Averaging encode general reasoning patterns (multi-path consistency, confidence aggregation) that transfer beyond the discovery benchmark.
- Core assumption: The discovery benchmark (MATH-500) exercises reasoning skills relevant to other domains.
- Evidence anchors:
  - [abstract] "the best discovered reasoning technique improving performance by 3.3% across multiple benchmarks and language models."
  - [section 3.1] SDA improves GPQA, MMLU-Pro, MedQA and shows gains across Gemini, DeepSeek, and GPT models.
  - [corpus] Generalization of agent-discovered reasoning techniques remains under-explored; corpus provides no direct replication.
- Break condition: Techniques overfit to the discovery benchmark and fail on qualitatively different tasks.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: AgentRxiv uses embedding-based similarity search to retrieve relevant papers, grounding agent reasoning in prior outputs.
  - Quick check question: Can you explain how cosine similarity over SentenceTransformer embeddings selects relevant papers?

- Concept: Multi-Agent Coordination
  - Why needed here: Parallel labs operate independently but share a common knowledge base, requiring understanding of asynchronous communication and potential conflicts.
  - Quick check question: What happens if two labs upload contradictory findings simultaneously?

- Concept: Chain-of-Thought Prompting & Reasoning
  - Why needed here: The target domain is reasoning technique discovery; SDA builds on CoT, self-consistency, and multi-path reasoning.
  - Quick check question: How does sampling at different temperatures in SDA encourage diverse reasoning paths?

## Architecture Onboarding

- Component map: AgentRxiv Server -> Embedding Index -> Agent Laboratory Instances -> Orchestrator
- Critical path:
  1. Configure AgentRxiv server and seed with initial papers (if any).
  2. For each lab, set research direction (e.g., "Improve accuracy on MATH-500 using reasoning and prompt engineering").
  3. Labs run literature review (arXiv + AgentRxiv), pulling N=5 papers from AgentRxiv and N=5 from arXiv.
  4. Labs generate, test, and refine code via mle-solver; upload final paper to AgentRxiv.
  5. New labs immediately retrieve recent papers; cycle repeats.
- Design tradeoffs:
  - Parallel vs Sequential: Faster wall-clock time but higher cost and redundancy.
  - Paper Review Count (N): More context vs potential noise.
  - mle-solver steps: Better code quality vs longer runtime and cost.
- Failure signatures:
  - Hallucinated results: Code runs but reports fabricated metrics.
  - Reward hacking: Paper-solver optimizes scores over truthful reporting.
  - Temperature incompatibility: Proposing methods requiring temperature sampling with models that disable it.
- First 3 experiments:
  1. Run a single lab for 10 papers with AgentRxiv access; track accuracy per generation and manually inspect code for hallucinations.
  2. Repeat with N=0 AgentRxiv access to quantify the value of cumulative knowledge.
  3. Launch 3 parallel labs for 10 papers each; compare wall-clock time, total cost, and best accuracy against the sequential baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automated verification modules effectively replace human screening to mitigate hallucinations and reward hacking in agent-generated research?
- Basis in paper: [explicit] The Discussion section proposes "developing a verification module that combines automated validation with selective human oversight... to minimize instances of hallucinated outputs."
- Why unresolved: The current framework relies on manual human verification to ensure code validity because agents frequently hallucinate experiment results or delete core functional code during automated repair attempts.
- What evidence would resolve it: A modified system that automatically detects and discards hallucinated or invalid papers while maintaining discovery rates comparable to human-screened runs.

### Open Question 2
- Question: How does AgentRxiv performance change when agents pursue open-ended research objectives rather than benchmark optimization?
- Basis in paper: [explicit] The Discussion states, "Future work should explore more open-ended objectives for agents to pursue using AgentRxiv."
- Why unresolved: All reported experiments were constrained to the specific goal of improving accuracy on the MATH-500 benchmark, which the authors note is not fully reflective of general scientific discovery.
- What evidence would resolve it: Results from experiments where agents are tasked with broader discovery goals, evaluated through qualitative expert review rather than quantitative benchmark accuracy.

### Open Question 3
- Question: What specific coordination mechanisms can reduce redundant experimentation in parallel laboratory setups without sacrificing discovery speed?
- Basis in paper: [explicit] The Discussion suggests, "increasing communication between parallel laboratory setups may help reduce redundant experimentation."
- Why unresolved: While parallel execution accelerated wall-clock progress, it proved computationally inefficient compared to sequential runs because laboratories independently explored similar hypotheses without awareness of intermediate peer results.
- What evidence would resolve it: A coordination strategy where parallel labs achieve the same performance milestones as the baseline but with significantly reduced total paper generation and computational cost.

## Limitations
- Source Code Availability: The AgentRxiv preprint server implementation is not provided, preventing direct reproduction of the core knowledge-sharing infrastructure.
- Methodological Verification: Manual verification was required for all results due to hallucinations, introducing potential human bias and scalability concerns.
- Generalization Evidence: The generalization claim is based on a single discovered method without establishing whether this is typical or an outlier.

## Confidence
- High Confidence: Sequential improvement mechanism (11.4% relative gain) with clear ablation study evidence.
- Medium Confidence: Parallel execution acceleration (13.7% relative improvement) with empirical wall-clock comparisons.
- Low Confidence: Specific implementation details and exact prompts cannot be verified without source code.

## Next Checks
1. **Source Code Recovery**: Attempt to locate complete AgentRxiv implementation through author correspondence or web archives.
2. **Ablation Replication**: Run sequential experiments with N=0 AgentRxiv access for 40 generations to independently verify the 11.4% improvement claim.
3. **Generalization Testing**: Test the SDA technique on completely novel reasoning domains (e.g., logical puzzles, programming problems) not mentioned in the original paper.