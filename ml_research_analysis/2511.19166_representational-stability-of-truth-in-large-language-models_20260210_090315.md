---
ver: rpa2
title: Representational Stability of Truth in Large Language Models
arxiv_id: '2511.19166'
source_url: https://arxiv.org/abs/2511.19166
tags:
- fictional
- 'true'
- synthetic
- noise
- statements
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models are widely used as factual sources, yet\
  \ small semantic changes can destabilize their truth judgments. We introduce representational\
  \ stability as the robustness of an LLM\u2019s truth representations to controlled\
  \ semantic perturbations."
---

# Representational Stability of Truth in Large Language Models

## Quick Facts
- arXiv ID: 2511.19166
- Source URL: https://arxiv.org/abs/2511.19166
- Authors: Samantha Dies; Courtney Maynard; Germans Savcisens; Tina Eliassi-Rad
- Reference count: 40
- One-line primary result: Epistemic familiarity, not linguistic form, governs LLM truth representation stability

## Executive Summary
Large language models exhibit varying stability in truth judgments when semantic assumptions are systematically varied. This work introduces representational stability as the robustness of LLM truth representations to controlled semantic perturbations. The key finding is that epistemically unfamiliar content (synthetic entities never seen in training) induces significantly larger epistemic retractions than familiar fictional content, revealing that epistemic familiarity governs stability as much as linguistic form.

## Method Summary
The study implements P-StaT (Perturbation Stability of Truth) by training a sparse-aware multiple-instance learning (sAwMIL) linear probe on LLM activations to separate true from not-true statements. The framework measures how the learned decision boundary shifts when labelings change to reclassify certain "neither" statements as true. Sixteen open-source models (3B-15B parameters) across three domains (City Locations, Medical Indications, Word Definitions) are evaluated using both representational probing and behavioral zero-shot prompting methods.

## Key Results
- Synthetic statements induce up to 40% epistemic retractions in fragile domains like word definitions
- Familiar fictional statements remain more stable with ≤8.2% retractions
- Representational distance (Wasserstein) correlates with stability: synthetic statements cluster near factual representations while fictional statements are representationally distinct
- Findings consistent across both probing and behavioral evaluation methods

## Why This Works (Mechanism)

### Mechanism 1
LLM truth representations are destabilized specifically by epistemically unfamiliar content rather than linguistically distinct content. When a linear probe is retrained to classify "Neither" statements as "True," the decision boundary must rotate. Synthetic statements lie close to the True/False cluster in activation space, forcing aggressive boundary shifts that reclassify previously "True" statements as "Not True." Fictional statements are already distant in activation space, requiring smaller adjustments.

### Mechanism 2
Epistemic familiarity acts as a protective buffer for truth representations. The model encodes familiar fictional entities with distinct "genre" or "context" signatures, allowing it to quarantine these entities. When prompted to treat them as true, the model processes them within that distinct context without rewriting the definition of general "truth." Synthetic entities lack this familiarity signal, causing them to interfere directly with the factual "truth" manifold.

### Mechanism 3
Behavioral instability (zero-shot flips) is predicted by representational instability. The P-StaT framework applies the same perturbation via two interfaces - retraining a probe and modifying the prompt context - with consistent findings that synthetic perturbations cause the highest retractions in both settings, implying the activation space geometry limits behavioral output flexibility.

## Foundational Learning

- **Linear Probing (sAwMIL)**: Needed to extract the model's internal "truth direction" from token activations. Quick check: Why does the paper prefer sAwMIL over the "Mean Difference" probe used in prior work?

- **Epistemic Retractions vs. Expansions**: The core metric - retractions (losing a previous True belief) are stronger instability signals than expansions (gaining new True beliefs). Quick check: If a model changes its mind about a "Neither" statement from False to True, is that a retraction or an expansion?

- **Wasserstein Distance in Activation Space**: Quantifies overlap between activation distributions for different statement types. Quick check: According to Figure 2, does the linguistic similarity of "Fictional" statements correlate with their representational distance from "True" statements?

## Architecture Onboarding

- **Component map**: Dataset -> Activation Extraction -> sAwMIL Probe -> Perturbation Engine -> Evaluator
- **Critical path**: Layer Selection → Baseline Probe Training → Perturbation Retraining → Retraction Calculation
- **Design tradeoffs**: sAwMIL chosen over Mean Difference for stability despite computational cost; Noise baselines included to avoid false positives
- **Failure signatures**: Wrong layer selection, probe sensitivity issues, or averaging base/chat model results without checking splits
- **First 3 experiments**: 1) Sanity check with Noise vectors, 2) Domain contrast (City vs Word Definitions), 3) Behavioral correlation between probing and zero-shot methods

## Open Questions the Paper Calls Out

1. How does P-StaT stability generalize to other forms of epistemic ambiguity like disputed claims, probabilistic beliefs, or temporally evolving facts?

2. How does belief stability change when LLM representations evolve through fine-tuning or continual learning?

3. What mechanisms during pretraining cause unfamiliar Synthetic content to cluster near factual representations while familiar Fictional content remains representationally distinct?

4. Can the P-StaT framework inform training interventions that improve epistemic robustness without sacrificing output accuracy?

## Limitations

- Causal attribution of representational distance to epistemic familiarity lacks direct experimental verification
- Probe reliance on linear boundaries raises questions about capturing complex truth concepts
- Study examines only static, pretrained models without addressing evolving representations

## Confidence

- **High Confidence**: Empirical finding that Synthetic statements induce more retractions than Fictional statements across models and domains
- **Medium Confidence**: Mechanism linking representational distance to stability
- **Low Confidence**: Interpretation that epistemic familiarity "protects" truth representations

## Next Checks

1. Rewrite Synthetic statements to match Fictional statements' linguistic patterns while preserving unfamiliar content; test if stability remains low

2. Train probe on City Locations, then apply to Medical Indications and Word Definitions without retraining; measure retraction pattern transfer

3. Fine-tune models on Synthetic statements as true facts; re-run perturbation analysis to test if epistemic familiarity reduces retractions