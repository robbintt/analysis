---
ver: rpa2
title: 'CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion
  in Text'
arxiv_id: '2507.07539'
source_url: https://arxiv.org/abs/2507.07539
tags:
- subjective
- subjectivity
- objective
- sentence
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a competitive approach to multilingual subjectivity
  detection using large language models (LLMs) with few-shot prompting. We participated
  in Task 1: Subjectivity of the CheckThat!'
---

# CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text

## Quick Facts
- arXiv ID: 2507.07539
- Source URL: https://arxiv.org/abs/2507.07539
- Reference count: 36
- Primary result: LLM-based few-shot prompting achieves state-of-the-art subjectivity detection across multiple languages

## Executive Summary
This paper presents a competitive approach to multilingual subjectivity detection using large language models (LLMs) with few-shot prompting. We participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation campaign. We show that LLMs, when paired with carefully designed prompts, can match or outperform fine-tuned smaller language models (SLMs), particularly in noisy or low-quality data settings. Despite experimenting with advanced prompt engineering techniques—such as debating LLMs and various example selection strategies—we found limited benefit beyond well-crafted standard few-shot prompts. Our system achieved top rankings across multiple languages in the CheckThat! 2025 subjectivity detection task, including first place in Arabic and Polish, and top-four finishes in Italian, English, German, and multilingual tracks. Notably, our method proved especially robust on the Arabic dataset, likely due to its resilience to annotation inconsistencies. These findings highlight the effectiveness and adaptability of LLM-based few-shot learning for multilingual sentiment tasks, offering a strong alternative to traditional fine-tuning, particularly when labeled data is scarce or inconsistent.

## Method Summary
The CEA-LIST team approached the CheckThat! 2025 Task 1: Subjectivity as a binary classification problem using LLM-based few-shot prompting. The primary method involved GPT-4o-mini with an "Extended Prompt" derived from annotation guidelines plus 6 randomly selected examples (3 objective, 3 subjective) from the training set. This was compared against a fine-tuned RoBERTa-Base baseline and an ensemble of heterogeneous models. The key innovation was using LLMs without weight updates (In-Context Learning) rather than fine-tuning, which proved particularly effective for noisy datasets like Arabic.

## Key Results
- First place in Arabic and Polish subjectivity detection tracks
- Top-four finishes in Italian, English, German, and multilingual tracks
- LLM few-shot method achieved Macro F1 scores of ~0.76 on English and ~0.80 on Arabic
- Random sampling of few-shot examples outperformed semantic similarity-based selection strategies
- The approach demonstrated strong resilience to annotation noise, particularly in the Arabic dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Few-shot Large Language Models (LLMs) exhibit greater resilience to annotation noise compared to fully fine-tuned Small Language Models (SLMs).
- **Mechanism:** SLMs update weights based on training data, causing them to overfit to incorrect or inconsistent labels (noise). In contrast, LLMs using In-Context Learning (ICL) do not update parameters; they rely on priors and reasoning. If the prompt guidelines are precise, the model can override noisy label signals in the few-shot examples, whereas an SLM is forced to minimize loss against those noisy labels.
- **Core assumption:** The LLM's pre-training provides a sufficiently robust prior understanding of "subjectivity" to distinguish valid examples from noise when guided by explicit rules.
- **Evidence anchors:**
  - [abstract] "...LLMs... can match or outperform fine-tuned smaller language models (SLMs), particularly in noisy or low-quality data settings."
  - [section 8] "Unlike fine-tuned models that heavily depend on high-quality labeled training data, our few-shot LLM approach is less affected by such noise."
  - [corpus] Corpus neighbors (e.g., "Bias in, Bias out") discuss annotation bias in multilingual LLMs but do not specifically confirm the noise-resilience mechanism for few-shot vs. fine-tuning; this is a specific finding of the current paper.
- **Break condition:** This mechanism likely breaks if the few-shot examples provided are predominantly noisy *and* the prompt guidelines are ambiguous, removing the "grounding" for the LLM.

### Mechanism 2
- **Claim:** Encoding explicit annotation guidelines into the system prompt constrains the LLM's decision boundary, significantly reducing false positives on the subjective class.
- **Mechanism:** A basic prompt leaves the definition of "subjective" open to the model's pre-training distribution, which often over-predicts subjectivity (high recall, low precision). By explicitly defining edge cases (e.g., "Quotes are objective," "Emotions are objective if unverifiable"), the prompt forces the model to inhibit subjective classifications for specific linguistic structures.
- **Core assumption:** The LLM has sufficient instruction-following capability to adhere to logical constraints that contradict its probabilistic prior.
- **Evidence anchors:**
  - [section 5.1.2] "...a more detailed prompt derived from annotation guidelines improves both macro F1 (+0.12) and subjective precision (+0.14)."
  - [section 4.1] "...probe how linguistic framing affects the model's interpretability and consistency."
  - [corpus] Weak corpus support for this specific mechanism in this context; neighbors focus on model architecture or data augmentation rather than prompt-guideline ablation.
- **Break condition:** Breaks if the guidelines provided in the prompt conflict with the few-shot examples provided, leading to conflicting signals.

### Mechanism 3
- **Claim:** Random sampling of few-shot examples outperforms semantic similarity selection for this specific classification task.
- **Mechanism:** Subjectivity detection relies on subtle linguistic markers (intensifiers, modality) rather than topic similarity. Selecting examples by semantic similarity may bias the prompt toward the specific *topic* of the test sentence rather than the *subjectivity features*, potentially causing the model to conflate topic relevance with classification logic.
- **Core assumption:** The training set contains sufficient diversity of subjectivity markers such that random sampling captures the necessary feature variance.
- **Evidence anchors:**
  - [section 5.1.3] "Interestingly, random selection outperforms similarity-based strategies across all models."
  - [section 5.1.3] "These results contrast with earlier findings highlighting the benefits of semantically similar examplars..."
  - [corpus] No supporting corpus evidence found; neighbor papers do not explicitly compare random vs. similarity selection for subjectivity.
- **Break condition:** Breaks if the random sample accidentally selects low-quality or out-of-distribution examples that mislead the model.

## Foundational Learning

- **Concept:** **Macro F1-Score vs. Accuracy**
  - **Why needed here:** The datasets are heavily imbalanced (e.g., English Dev-Test has 362 OBJ vs 122 SUBJ). Standard accuracy would reward a model that simply guesses the majority class (Objective).
  - **Quick check question:** If a model predicts "Objective" for every sentence in the English Dev-Test set, what is the accuracy? (Answer: ~75%, but F1 would be poor).

- **Concept:** **In-Context Learning (ICL)**
  - **Why needed here:** This is the core method used. Unlike fine-tuning (changing weights), ICL learns from examples placed in the prompt context.
  - **Quick check question:** Does adding a 6-shot prompt to GPT-4o-mini change the model's internal weights? (Answer: No).

- **Concept:** **Prompt Engineering (Instruction vs. Demonstration)**
  - **Why needed here:** The paper shows that "Instruction" (Extended Prompt) and "Demonstration" (Few-shot) are distinct levers. The best performance came from optimizing *both*.
  - **Quick check question:** Did the "Extended Prompt" improve performance more than adding "Random 6-shot" examples? (Answer: No, 6-shot provided a larger gain over basic prompt than extended prompt alone, but both were required for SOTA results).

## Architecture Onboarding

- **Component map:**
  Data Loader -> Prompt Constructor -> LLM Engine -> Parser -> Ensemble Voter

- **Critical path:**
  1.  **Dataset Inspection:** Check class balance (OBJ vs SUBJ).
  2.  **Prompt Template:** Use the "Extended Prompt" (Appendix A.2) which defines rules for Quotes and Emotions.
  3.  **Example Selection:** Implement a sampler that pulls 3 OBJ and 3 SUBJ examples *randomly* from the training set.
  4.  **Inference:** Send to LLM.

- **Design tradeoffs:**
  - **Complexity vs. Gain:** The paper experimented with "Debating LLMs" (multi-agent) and "Semantic Selection." These add significant latency and complexity. The finding is that **Random Selection + Standard Prompt** offers 95% of the performance with 20% of the complexity.
  - **Cost:** LLMs (GPT-4) are more expensive per inference than running a fine-tuned RoBERTa, but require zero training time.

- **Failure signatures:**
  - **High Recall / Low Precision on SUBJ:** Model is over-predicting subjectivity. *Fix:* Strengthen the "Quotes are Objective" rule in the prompt (see Section 5.1.2).
  - **Low Performance on Arabic (SLM):** Model failing to generalize. *Fix:* Switch to LLM approach or check data for annotation noise (Section 8).
  - **Reproducibility Gap:** If you cannot reproduce the Arabic F1=0.80 from prior work, suspect dataset versioning or annotation noise (Section 6).

- **First 3 experiments:**
  1.  **Baseline Check:** Run RoBERTa-Base fine-tuning vs. GPT-4o-mini (Zero-shot) on the English Dev set. Verify the paper's claim that LLMs match SLMs.
  2.  **Ablation Study:** Remove the "Quotes are Objective" instruction from the Extended Prompt. Measure the drop in Macro F1 to quantify the value of guideline-specific engineering.
  3.  **Selection Strategy Test:** On the Arabic set, compare 6-shot Random vs. 6-shot Semantic Similarity. Verify if Random truly outperforms Similarity (as per Section 5.1.3).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does random example selection outperform semantic similarity-based selection in few-shot subjectivity detection?
- Basis in paper: [explicit] Section 5.1.3 notes random sampling yielded better Macro F1 than similarity-based strategies, contrasting with prior work cited in the paper.
- Why unresolved: The authors observe the phenomenon and suggest dissimilar examples may aid generalization, but they do not validate this hypothesis against the conflicting literature.
- What evidence would resolve it: Ablation studies on multiple datasets that specifically control for the linguistic variance of the few-shot examples (e.g., comparing random vs. clustered vs. out-of-domain selection).

### Open Question 2
- Question: Is the superior performance of LLMs on noisy datasets strictly due to resilience against annotation inconsistencies?
- Basis in paper: [inferred] Section 5.2.2 attributes the team's dominance in Arabic to LLM resilience to noise, while Section 6 highlights potential reproducibility issues and label noise in that specific dataset.
- Why unresolved: The conclusion is based on a single dataset with suspected noise, making it unclear if the performance gap is due to noise resilience or specific linguistic properties of Arabic.
- What evidence would resolve it: A controlled experiment comparing LLMs and SLMs on the same data with artificially injected label noise at varying intensities.

### Open Question 3
- Question: What specific conditions justify the computational cost of multi-agent debating over standard few-shot prompting?
- Basis in paper: [explicit] The Abstract and Conclusion state that advanced techniques like debating LLMs provided "limited benefit" compared to well-crafted standard prompts.
- Why unresolved: The paper establishes that complex methods are not always better for this task but does not define the specific scenarios (e.g., highly ambiguous text) where such complexity might be necessary.
- What evidence would resolve it: Evaluating debate-based prompting on a dataset specifically curated for high ambiguity or irony, where standard prompting is known to fail.

## Limitations

- The paper relies on proprietary LLM access and does not specify exact decoding parameters (temperature, top_p) which introduces performance variance
- The "Debating LLMs" experiments lack procedural detail regarding number of turns and voting logic
- The ensemble voting mechanism is underspecified, making exact reproduction impossible
- The Arabic dataset's annotation inconsistencies are noted but not quantified, leaving uncertainty about whether the LLM advantage stems from noise resilience or linguistic differences
- Access to specific model versions (e.g., "GPT-4.1-mini") may be restricted, blocking faithful reproduction

## Confidence

- **High Confidence**: LLM few-shot matching or outperforming SLMs on multiple languages; random sampling outperforming semantic selection; Arabic robustness to annotation noise
- **Medium Confidence**: Prompt-guideline engineering reducing false positives; debating LLM experiments showing limited benefit
- **Low Confidence**: Exact numerical reproducibility across all languages and tracks due to unreported hyperparameters and proprietary model access

## Next Checks

1. **Reproduce the English vs. Arabic noise-resilience gap**: Run RoBERTa fine-tuning and GPT-4o-mini few-shot on both languages, measure performance drop to quantify annotation noise impact

2. **Test the Extended Prompt's effect on false positives**: Run ablation by removing the "Quotes are Objective" instruction, measure change in subjective precision and macro F1 to isolate guideline engineering value

3. **Verify random vs. semantic selection on Arabic**: Conduct head-to-head comparison of 6-shot random vs. 6-shot similarity selection on Arabic Dev-Test, measuring macro F1 and standard deviation across multiple random seeds