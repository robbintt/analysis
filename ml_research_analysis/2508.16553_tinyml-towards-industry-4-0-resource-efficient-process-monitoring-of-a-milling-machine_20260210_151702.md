---
ver: rpa2
title: 'TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling
  Machine'
arxiv_id: '2508.16553'
source_url: https://arxiv.org/abs/2508.16553
tags:
- process
- machine
- milling
- tinyml
- monitoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work demonstrates the feasibility of deploying TinyML for
  structure-integrated process quality monitoring in industrial milling machines.
  A novel dataset, MillingVibes, was created by recording 3-axial vibration data from
  an industrial machine tool under varying milling conditions, yielding labeled data
  for process quality assessment.
---

# TinyML Towards Industry 4.0: Resource-Efficient Process Monitoring of a Milling Machine

## Quick Facts
- **arXiv ID:** 2508.16553
- **Source URL:** https://arxiv.org/abs/2508.16553
- **Reference count:** 17
- **Primary result:** Achieved 100.0% test accuracy for binary milling process quality classification on ARM Cortex M4F with 12.59 kiB storage and 15.4 ms inference

## Executive Summary
This work demonstrates the feasibility of deploying TinyML for structure-integrated process quality monitoring in industrial milling machines. The authors created a novel MillingVibes dataset by recording 3-axial vibration data from an industrial machine tool under varying milling conditions. A complete TinyML pipeline was developed, including preprocessing and an 8-bit-quantized convolutional neural network (CNN) model requiring only 12.59 kiB of parameter storage. The approach enables real-time, resource-efficient process monitoring, offering a cost-effective solution for retrofitting existing industrial machines with smart sensing capabilities.

## Method Summary
The method converts 3-axial vibration time-series to log-scaled spectrograms, enabling a small CNN to distinguish milling process quality with 100% test accuracy. Key innovations include instance normalization per sample (rather than dataset-wide), aggressive average pooling as preprocessing (reducing feature map size by 16×), and post-training INT8 quantization. The CNN operates on a 3×4×65 feature map across three vibration axes, achieving 15.4 ms inference on an ARM Cortex M4F microcontroller. The preprocessing pipeline (69.1 ms) dominates over CNN inference (15.4 ms), with energy consumption of 1.462 mJ per inference.

## Key Results
- 100.0% test accuracy on binary milling process quality classification
- 15.4 ms inference time on ARM Cortex M4F microcontroller
- 12.59 kiB parameter storage with INT8 quantization
- 1.462 mJ energy consumption per inference

## Why This Works (Mechanism)

### Mechanism 1: Frequency Domain Separation of Process Quality Signatures
Converting 3-axial vibration time-series to log-scaled spectrograms enables a small CNN to distinguish milling process quality with 100% test accuracy. The STFT-based spectrogram transforms temporal vibration patterns into time-frequency representations where good vs. bad milling quality exhibit distinguishable spectral line patterns. Log-transformation (log₁₀) emphasizes magnitude differences between spectral lines and background noise, making classification boundaries clearer.

### Mechanism 2: Instance Normalization Preserves Intra-Sample Spectral Relationships
Normalizing each 1-second sample independently (rather than across the full dataset) improves both quantization robustness and classification accuracy. Instance normalization maps each vibration segment to [-1, 1] using its own min/max values, equalizing absolute amplitude differences between samples while preserving relative magnitude relationships of spectral lines within each sample.

### Mechanism 3: Preprocessing-Model Co-Optimization via Feature Map Compression
Applying average pooling as a preprocessing step (before log-scaling) rather than as a CNN layer reduces both preprocessing and inference time significantly while maintaining accuracy. By computing (8×2) average pooling immediately after spectrogram generation, the feature map shrinks from 3×32×129 to 3×4×65 (16× reduction).

## Foundational Learning

- **Concept: Short-Time Fourier Transform (STFT) for Non-Stationary Signals**
  - **Why needed here:** Industrial milling vibrations are non-stationary—frequency content changes as the tool moves across the workpiece. STFT with overlapping windows captures spectral evolution.
  - **Quick check question:** Given 8000 samples at 8 kHz, 256-point FFT, 8-sample overlap—approximately how many STFT frames are computed before the paper's internal downsampling to 32 timesteps?

- **Concept: INT8 Post-Training Quantization**
  - **Why needed here:** Converting float32 weights/activations to 8-bit integers reduces storage by 4× and enables integer-only inference on microcontrollers.
  - **Quick check question:** Why does quantization require representative calibration data, and what happens if calibration samples don't cover the full input distribution?

- **Concept: Energy per Inference (EPI) Measurement via Shunt Resistor**
  - **Why needed here:** The paper measures actual energy consumption using a 10Ω shunt resistor and voltage sampling at 10 kHz. This is critical for estimating battery life in wireless retrofitted sensors.
  - **Quick check question:** If average supply voltage is 2.987V, average current is 31.807mA, and total inference+preprocessing time is 84.5 ms, calculate EPI and compare to the paper's reported 8.022 mJ.

## Architecture Onboarding

- **Component map:**
Accelerometer -> Raw Buffer -> Instance Normalization -> STFT Spectrogram -> Average Pooling -> Log₁₀ Scaling -> INT8 Quantization -> CNN -> Binary Output

- **Critical path:** Preprocessing (69.1 ms total) dominates over CNN inference (15.4 ms). Spectrogram generation (30.6 ms) and log-scaling (26.9 ms) together consume 68% of total latency.

- **Design tradeoffs:**
  - Sensor placement at 1.4m distance enables retrofitting without machine modification but assumes vibration transmission through machine structure preserves quality signatures
  - 1-second segmentation simplifies pipeline but may split contiguous patterns
  - Average pooling before log-scaling reduces log computation by 16× but applies pooling to linear-scaled spectrogram
  - STM32Cube.AI runtime enables rapid deployment but adds 39.75 kiB flash overhead (3× model size)

- **Failure signatures:**
  - Accuracy drops on different machine/tool indicate spectral signatures depend on machine dynamics
  - Quantized model accuracy < float model suggests poor activation range estimation during calibration
  - RAM overflow on smaller MCU indicates 112.58 kiB requirement may be prohibitive

- **First 3 experiments:**
  1. Validate preprocessing pipeline in Python with exact STFT parameters, verify spectrogram shapes (3×4×65) and achieve similar accuracy
  2. Profile CMSIS-DSP vs. naive implementations for FFT, min-max normalization, and log computation
  3. Stress-test instance normalization with amplitude variations (0.1×, 1×, 10×) to verify amplitude-invariance

## Open Questions the Paper Calls Out

- **Can the classification task be expanded from binary quality assessment to multi-class fault categorization or regression tasks?**
  - The authors state that future research includes extending the task "from a binary process quality decision ('good' vs. 'bad') to more complex analyses," specifically mentioning fault type determination and detection of machine tool configurations.

- **To what extent can hardware-aware Neural Architecture Search (NAS) co-optimize the preprocessing pipeline and the neural network?**
  - The conclusion suggests that "another path to even more efficient algorithm implementations can be to investigate the co-optimization of neural classifiers and preprocessing algorithms by hardware-aware neural architecture search."

- **What specific reductions in storage footprint and latency can be achieved by implementing the model as bare metal C code?**
  - The Discussion notes that the STM32Cube.AI runtime library requires "more than 3x the parameter storage" and recommends the "implementation as bare metal C code... for a minimal storage footprint in the future."

## Limitations

- Generalizability of 100% accuracy claims is limited to single machine tool with controlled conditions
- Preprocessing dependency on instance normalization and log-scaled spectrograms may limit adaptability to other scenarios
- STM32Cube.AI runtime adds 39.75 kiB flash overhead (3× model size), and 112.58 kiB RAM requirement includes 93.75 kiB raw input buffer

## Confidence

**High Confidence:** Preprocessing-model co-optimization mechanism with clear performance metrics; energy measurement methodology is standard and reproducible

**Medium Confidence:** Instance normalization benefits for quantization robustness supported by experiments but underlying theory not fully explained; novel dataset has limited sample diversity

**Low Confidence:** Absolute accuracy claim (100.0%) lacks statistical validation and cross-machine verification; assumption about vibration transmission through 1.4m structure is not empirically verified

## Next Checks

1. **Cross-Machine Generalization Test:** Deploy the quantized model on vibration data from a different milling machine to quantify accuracy degradation and identify domain shift effects

2. **Temporal Boundary Analysis:** Evaluate classification performance on overlapping 1-second windows with 50% overlap to assess whether fixed segmentation misses transient quality changes

3. **Quantization Sensitivity Analysis:** Systematically vary the number of calibration samples and their distribution across the training set to determine minimum calibration data required for maintaining accuracy after INT8 quantization