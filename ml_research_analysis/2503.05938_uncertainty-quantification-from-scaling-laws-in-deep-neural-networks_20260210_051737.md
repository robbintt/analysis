---
ver: rpa2
title: Uncertainty Quantification From Scaling Laws in Deep Neural Networks
arxiv_id: '2503.05938'
source_url: https://arxiv.org/abs/2503.05938
tags:
- scaling
- neural
- infinite-width
- networks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses uncertainty quantification (UQ) in deep neural
  networks by examining the scaling behavior of mean test loss and its variance as
  a function of training set size. The authors compute these quantities analytically
  in the infinite-width limit using neural tangent kernel (NTK) initialization, and
  compare with finite-width network experiments on three tasks: MNIST classification,
  CIFAR classification, and calorimeter energy regression.'
---

# Uncertainty Quantification From Scaling Laws in Deep Neural Networks

## Quick Facts
- arXiv ID: 2503.05938
- Source URL: https://arxiv.org/abs/2503.05938
- Reference count: 0
- Key outcome: The coefficient of variation of test loss becomes independent of training set size for large datasets, both in infinite and finite-width networks

## Executive Summary
This work investigates uncertainty quantification in deep neural networks by analyzing how test loss statistics scale with training set size. The authors compute mean test loss and its variance analytically in the infinite-width limit using neural tangent kernel (NTK) initialization, and validate these predictions against finite-width network experiments. They discover that while both mean loss and standard deviation follow power-law scaling with dataset size, their ratio (coefficient of variation) becomes approximately constant for large datasets. This invariance holds across three different tasks and appears to extend to finite-width networks, suggesting a predictable uncertainty metric that could be useful for practical applications.

## Method Summary
The authors compute mean and variance of test loss analytically for infinite-width MLPs with NTK initialization using kernel matrices rather than ensemble training. They use critically initialized networks with erf activation to enable closed-form Gaussian integrals, and derive scaling laws by examining how NTK inverse elements depend on training set size. For finite-width networks, they train ensembles of 150 MLPs per dataset size using full-batch gradient descent with early stopping. The analysis is validated on MNIST and CIFAR classification (as regression to one-hot labels) and calorimeter energy regression, comparing infinite-width predictions to finite-width network behavior across various training set sizes.

## Key Results
- Mean test loss μ_L and standard deviation σ_L both follow power-law scaling with training set size N_D
- The coefficient of variation ε_L = σ_L/μ_L becomes approximately independent of N_D for large datasets
- This scaling behavior is observed in both infinite-width theoretical predictions and finite-width network experiments
- The invariance of ε_L suggests it may be predictable even in practical finite-width networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** In infinite-width MLPs with NTK parameterization, mean and variance of test loss can be computed analytically using kernel matrices
- **Mechanism:** Network output distribution remains Gaussian throughout training because each GD step is a linear transformation. Final mean and variance depend only on NTK and kernel matrices, avoiding ensemble training
- **Core assumption:** NTK submatrix on training set is invertible, or early stopping can handle ill-conditioned cases
- **Evidence anchors:** Abstract states mean and variance are computed in infinite-width limit; section III details the analytical approach using NTK inversion

### Mechanism 2
- **Claim:** Both μ_L and σ_L follow power-law scaling with N_D, with related exponents
- **Mechanism:** Inverse NTK elements scale with N_D (empirically k < 0), and since μ_L involves two powers of Θ⁻¹ while σ²_L involves four powers, this drives the scaling behavior
- **Core assumption:** Scaling laws persist across architectures and matrix structure can be heuristically simplified
- **Evidence anchors:** Abstract mentions observing scaling laws for both quantities; section IV C explains how inverse NTK scaling drives the observed power laws

### Mechanism 3
- **Claim:** Coefficient of variation ε_L becomes independent of N_D for large datasets
- **Mechanism:** Since σ²_L contains twice as many powers of Θ⁻¹ as μ_L, taking the square root yields σ_L ∝ N^(2k+4)_D, making the ratio ε_L ∝ N^0_D through mathematical cancellation
- **Core assumption:** Dominant terms in variance expression follow this scaling; other terms matter only at small N_D
- **Evidence anchors:** Abstract states ε_L becomes independent of N_D at both infinite and finite width; section IV C derives the mathematical cancellation

## Foundational Learning

- **Concept: Neural Tangent Kernel (NTK)**
  - **Why needed here:** The entire infinite-width analysis depends on understanding how NTK characterizes training dynamics and enables closed-form predictions
  - **Quick check question:** Can you explain why the NTK remains constant during training in the infinite-width limit but changes at finite width?

- **Concept: Critical initialization**
  - **Why needed here:** Critically initialized NTK parameterization (variance 1/n_ℓ−1 for weights) ensures preactivations don't explode or vanish with depth
  - **Quick check question:** What happens to signal propagation in a deep network if weights are initialized with variance 1/n instead of 1/n_ℓ−1?

- **Concept: Gaussian Process interpretation of infinite-width networks**
  - **Why needed here:** Analytic computation of μ_L and σ²_L relies on outputs being Gaussian distributed, enabling Wick's theorem for computing expectations
  - **Quick check question:** Why does the Gaussian property persist through gradient descent training at infinite width but not necessarily at finite width?

## Architecture Onboarding

- **Component map:** Network weights → erf activation → kernel matrices K and Θ → NTK inversion → mean/variance predictions

- **Critical path:**
  1. Initialize network with NTK parameterization (zero biases, Gaussian weights with variance 1/n_ℓ−1)
  2. Compute kernel K^(ℓ) and NTK Θ^(ℓ) matrices recursively using erf activation formulas
  3. Invert training NTK Θ_A or use early stopping if ill-conditioned
  4. Compute mean predictions m^∞_β and covariance Σ_β1β2
  5. Evaluate μ_L and σ²_L using Gaussian expectations from Appendix A

- **Design tradeoffs:**
  - erf activation vs ReLU: erf enables closed-form integrals but may differ from practical architectures
  - Full-batch GD vs Adam: Infinite-width theory strictly applies to GD; Adam shows similar ε_L but different μ_L scaling
  - MLP vs specialized architectures: Calorimeter regression would benefit from CNNs but MLP enables theoretical analysis

- **Failure signatures:**
  - NTK matrix not invertible: Θ_A becomes singular or ill-conditioned at large N_D → use early stopping
  - Incorrect scaling: Verify weights have variance 1/n_ℓ−1 per layer; standard initialization gives different results
  - Training instability: Monitor for divergence on calorimeter data; small learning rate is critical

- **First 3 experiments:**
  1. Reproduce infinite-width scaling on MNIST: Compute kernel/NTK matrices for N_D ∈ {100, 500, 1000, 2000, 5000}, verify μ_L ∝ N^−α_D and ε_L ≈ constant
  2. Test finite-width agreement: Train ensemble of 150 MLPs (n=30) for each N_D, compare coefficient of variation to infinite-width prediction
  3. Validate generalization to new task: Apply same methodology to different regression dataset and check if ε_L invariance persists

## Open Questions the Paper Calls Out

- **Open Question 1:** To what extent is feature learning responsible for observed finite-width effects on the coefficient of variation, particularly regarding different scaling behaviors between datasets like MNIST and calorimeter data?
  - **Basis:** Authors intend to study how feature learning affects finite-width effects on ε_L and note different behaviors between datasets as a consequence of feature learning
  - **Why unresolved:** Infinite-width NTK lacks feature learning yet finite-width networks show similar scaling, but deviations with Adam optimizer suggest feature learning plays a role
  - **What evidence would resolve it:** Comparative analysis of scaling exponents in regimes where feature learning is explicitly enabled vs disabled

- **Open Question 2:** Can finite-width perturbation theory be practically utilized to compute ε_L despite N_D⁴ computational scaling of required tensors?
  - **Basis:** Authors suggest applying finite-width perturbation theory to compute ε_L at finite width
  - **Why unresolved:** While perturbation theory exists, computational cost grows rapidly with dataset size, making direct calculation infeasible for large datasets
  - **What evidence would resolve it:** Deriving a method to compute corrections using smaller tensors or demonstrating feasibility for small N_D

- **Open Question 3:** Do scaling laws for coefficient of variation hold when applying finite-width perturbation theory to Bayesian neural networks?
  - **Basis:** Authors propose applying finite-width perturbation theory to Bayesian neural networks to see if same scaling laws appear
  - **Why unresolved:** Paper focuses on gradient descent in NTK parameterization; unknown if ε_L invariance translates to Bayesian posterior inference setting
  - **What evidence would resolve it:** Replicating scaling analysis on Bayesian neural networks using relevant perturbative expansions

## Limitations
- Analysis strictly valid only in infinite-width limit and relies on NTK staying constant during training; finite-width effects may deviate from predictions
- Only MLPs with erf activation are analyzed; scaling behavior may differ for other architectures (CNNs, transformers) or activations (ReLU, Swish)
- Coefficient of variation becomes invariant only for sufficiently large N_D; behavior at small N_D remains architecture-dependent and less predictable

## Confidence

- **High confidence:** The infinite-width analytical computation of μ_L and σ²_L via NTK; the power-law scaling of both quantities with N_D
- **Medium confidence:** The mathematical derivation that ε_L ∝ N^0_D due to cancellation of scaling exponents; the claim that ε_L invariance extends to finite-width networks
- **Low confidence:** The general applicability of ε_L invariance across all architectures and tasks; the precise value of the invariant coefficient for practical applications

## Next Checks

1. Test whether ε_L invariance holds for CNNs on image classification tasks where spatial locality matters
2. Investigate whether different activation functions (ReLU, Swish) preserve the coefficient of variation invariance
3. Analyze how ε_L behaves at very small N_D (N_D < 100) where scaling laws break and finite-width effects dominate