---
ver: rpa2
title: Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial
  Games
arxiv_id: '2510.16761'
source_url: https://arxiv.org/abs/2510.16761
tags:
- game
- sco-pal
- actions
- games
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SCO-PAL, a step-level policy optimization method
  through play-and-learn to improve strategic reasoning in language agents for adversarial
  games. By analyzing opponent selection, the authors find that self-play is most
  effective for strategy refinement.
---

# Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games

## Quick Facts
- arXiv ID: 2510.16761
- Source URL: https://arxiv.org/abs/2510.16761
- Reference count: 34
- Key outcome: SCO-PAL increases average win rate by ~30% against four opponents and achieves 54.76% win rate against GPT-4

## Executive Summary
This paper introduces SCO-PAL, a step-level policy optimization method that improves strategic reasoning in language agents through play-and-learn in adversarial games. The authors systematically analyze opponent selection and find that self-play generates the most balanced and diverse training data, enabling effective strategy refinement. SCO-PAL combines Monte Carlo win-rate estimation for step-level rewards with a two-stage optimization approach: behavioral cloning on advantageous actions followed by Kahneman-Tversky optimization. Experiments on six games from GTBench demonstrate significant improvements in strategic reasoning, with average win rate gains of ~30% against multiple opponents.

## Method Summary
SCO-PAL operates in three stages: (I) Game interaction through 1000 episodes of self-play at temperature 0.7, collecting trajectories; (II) Step-wise reward estimation using Monte Carlo win rates to label actions as advantageous (r>0.5) or disadvantageous; (III) Two-stage strategy refinement with behavioral cloning on advantageous actions followed by Kahneman-Tversky optimization with balanced loss weights. The method uses Qwen2-7B-Instruct as base model and evaluates against four opponents (Random, MCTS-1000, GPT-3.5, GPT-4) across six GTBench games.

## Key Results
- Average win rate increases by ~30% against four opponents compared to baseline
- 54.76% win rate achieved against GPT-4, demonstrating strong strategic reasoning
- Self-play identified as most effective opponent selection strategy, generating balanced advantageous/disadvantageous action ratios
- Two-stage BC→KTO optimization outperforms joint training by 4.5% average win rate

## Why This Works (Mechanism)

### Mechanism 1: Self-Play Generates Balanced, High-Quality Training Data
- Claim: Self-play produces the most diverse and balanced distribution of advantageous vs. disadvantageous actions
- Mechanism: Minimized skill gap between players yields ~50% win rates, creating largest total action count with balanced advantageous/disadvantageous ratios
- Core assumption: Balanced training data with diverse strategies leads to better generalization than imbalanced data
- Evidence anchors: Self-play yields richest data (Figure 2b); win rates against strong MCTS(1000) drop to 5.40%, reducing advantageous action diversity

### Mechanism 2: Monte Carlo Win-Rate Estimation Enables Step-Level Credit Assignment
- Claim: Empirical win rate across trajectories provides effective signal for labeling actions without hand-crafted reward shaping
- Mechanism: Compute r(s,a) = N_win / N_all for each (state, action) pair; actions with r > δ (threshold = 0.5) are labeled advantageous
- Core assumption: Actions correlating with wins across multiple trajectories are causally beneficial
- Evidence anchors: Monte Carlo Estimation applied over interaction trajectories; no direct corpus comparison to alternative credit assignment methods

### Mechanism 3: Two-Stage BC→KTO Optimization Separates Adaptation from Refinement
- Claim: Behavioral cloning provides stable initialization; Kahneman-Tversky Optimization refines preferences by reinforcing advantageous actions
- Mechanism: Stage 1 - BC trains on actions with r > δ; Stage 2 - KTO optimizes using binary desirable/undesirable labels without pairwise comparisons
- Core assumption: Sequential BC→KTO is more stable than joint optimization or KTO alone
- Evidence anchors: Two-stage achieves 50.08% win rate vs. 44.89% for joint loss; KTO outperforms DPO by 9.56%

## Foundational Learning

- Concept: **Monte Carlo Tree Search (MCTS)**
  - Why needed here: MCTS with varying max_simulations (5–1000) creates opponents of controlled strength for analysis
  - Quick check question: Can you explain how increasing max_simulations in MCTS improves decision quality and why this creates stronger opponents?

- Concept: **Kahneman-Tversky Optimization (KTO)**
  - Why needed here: SCO-PAL uses KTO instead of DPO because it requires only binary desirable/undesirable labels, not pairwise preferences
  - Quick check question: What is the key difference between KTO and DPO in terms of data requirements, and why would KTO be preferable for game trajectories?

- Concept: **Behavioral Cloning vs. Reinforcement Learning**
  - Why needed here: The paper uses BC for fast adaptation and KTO (offline RL) for refinement
  - Quick check question: Why might BC alone be insufficient for strategic improvement, and what does KTO add?

## Architecture Onboarding

- Component map: Game Interaction (Stage I) -> Step-Wise Reward Estimation (Stage II) -> Strategy Refinement (Stage III)
- Critical path: 1) Choose opponent strategy (self-play → best results), 2) Run game interaction at temperature=0.7, 3) Filter actions by win-rate threshold (δ=0.5), 4) Apply BC then KTO sequentially, 5) Evaluate against held-out opponents
- Design tradeoffs:
  - Temperature 0.7 balances diversity vs. action validity; higher temperatures increase invalid actions
  - Threshold δ=0.5 is default; stricter thresholds reduce training data
  - Scaling data: Upsampling imbalanced data degrades performance—balance must be intrinsic to gameplay
  - Iteration: Performance peaks at Iter2 then declines slightly at Iter3, suggesting RL overfitting risk
- Failure signatures:
  - Win rate against strong opponents stays near 0% → insufficient advantageous actions for learning
  - KTO loss diverges → check λ_D, λ_U balance; ensure n_D/n_U ratio is handled
  - Low post-training win rate → verify BC is trained only on r > δ actions
  - High invalid-action rate → reduce temperature below 0.7
- First 3 experiments:
  1. Baseline check: Run base model against Random, MCTS(1000), GPT-3.5, GPT-4 on all 6 games
  2. Self-play data collection: Run 1000 episodes of self-play per game at temperature=0.7
  3. Ablation of training strategy: Compare Direct KTO, Joint Loss, and Two-Stage (BC→KTO) on one game

## Open Questions the Paper Calls Out

- Question: How effectively does SCO-PAL transfer to open-ended or partially observable adversarial environments (e.g., negotiation, web-based competitive tasks) beyond well-defined turn-based games?
  - Basis in paper: "our study is limited to well-defined, turn-based games and does not explore more open-ended or partially observable environments"
  - Why unresolved: SCO-PAL relies on episodic MDP formulation with clear state-action pairs and terminal rewards
  - What evidence would resolve it: Evaluation on negotiation benchmarks or web-based adversarial tasks

- Question: Does incorporating human or style-diverse LLM opponents during training improve SCO-PAL's strategic diversity and robustness compared to self-play alone?
  - Basis in paper: "the opponent pool is restricted to scripted agents and LLM variants; incorporating human or style-diverse LLM opponents could further enrich strategic diversity"
  - Why unresolved: Human opponents may exhibit diverse, unpredictable strategies not captured by scripted MCTS or self-play
  - What evidence would resolve it: Experiments training SCO-PAL with mixed opponent pools

- Question: What mechanisms can prevent performance degradation during iterative SCO-PAL training cycles beyond round 2?
  - Basis in paper: Win rates improve from Iter1 (50.08%) to Iter2 (50.77%) but decline at Iter3 (48.93%), attributed to overfitting
  - Why unresolved: Paper identifies the problem but does not propose or test solutions
  - What evidence would resolve it: Ablation studies testing regularization techniques

## Limitations
- Limited to well-defined, turn-based games without exploration of open-ended or partially observable environments
- Does not investigate transfer to non-adversarial cooperative games or real-world strategic reasoning tasks
- Lacks evidence that performance gains generalize beyond the GTBench game suite

## Confidence
- High confidence: Self-play generates more balanced training data than weak/strong opponents
- Medium confidence: Monte Carlo step-level reward estimation effectively captures advantageous actions
- Medium confidence: Two-stage BC→KTO optimization outperforms joint training
- Low confidence: Gains will generalize to non-adversarial or real-world strategic reasoning tasks

## Next Checks
1. Test SCO-PAL on non-adversarial cooperative games to verify the self-play mechanism works beyond zero-sum environments
2. Conduct cross-game transfer experiments where models trained on subset of GTBench games are evaluated on held-out games
3. Compare Monte Carlo reward estimation against alternative credit assignment methods (e.g., discounted returns, advantage estimation) in the same experimental framework