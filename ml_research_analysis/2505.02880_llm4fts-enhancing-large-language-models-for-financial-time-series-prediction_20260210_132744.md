---
ver: rpa2
title: 'LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction'
arxiv_id: '2505.02880'
source_url: https://arxiv.org/abs/2505.02880
tags:
- financial
- time
- series
- data
- wavelet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# LLM4FTS: Enhancing Large Language Models for Financial Time Series Prediction

## Quick Facts
- arXiv ID: 2505.02880
- Source URL: https://arxiv.org/abs/2505.02880
- Reference count: 9
- Primary result: Outperforms state-of-the-art models on portfolio performance metrics and prediction accuracy

## Executive Summary
This paper proposes LLM4FTS, a framework that reprograms large language models (LLMs) for financial time series prediction. The approach maps numerical financial data to LLM token space using adaptive patch segmentation and learnable wavelet transforms, enabling GPT-2 to process and predict stock returns. The model demonstrates superior performance on cross-sectional stock return prediction tasks compared to existing transformer-based methods.

## Method Summary
The method processes multivariate financial time series (OHLCV data) through a pipeline that includes learnable Stationary Wavelet Transform (SWT) for multi-scale feature extraction, adaptive patch segmentation via K-means++ clustering with Dynamic Time Warping (DTW) distance for pattern recognition, and next-patch prediction pre-training for LLM alignment. The system uses a 6-layer GPT-2 backbone to process variable-length patches mapped to embedding space, ultimately predicting stock return scores for portfolio construction.

## Key Results
- Achieves higher Annualized Return Rate (ARR) and Sharpe Ratio (SR) than benchmark models on CSI 300, CSI 500, S&P 500, and NASDAQ 100 datasets
- Demonstrates superior prediction accuracy with lower MSE and MAE across all tested market indices
- Shows improved Maximum Drawdown (MDD) and Cumulative Return (CR) metrics in portfolio backtesting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adaptive patch segmentation preserves semantic pattern integrity better than fixed-length partitioning.
- **Mechanism:** The model utilizes K-means++ clustering with Dynamic Time Warping (DTW) distance on historical data to identify scale-invariant patterns (e.g., specific trend shapes). It then partitions new sequences into variable-length patches such that segment boundaries align with these identified pattern shifts, rather than arbitrary time steps.
- **Core assumption:** Financial predictive signals are embedded in specific morphological shapes (scale-invariant patterns) that cross-lingual LLMs can process as discrete semantic units if segmented correctly.
- **Evidence anchors:**
  - [abstract] "introduce adaptive patch segmentation that partitions temporal sequences while preserving maximal pattern integrity."
  - [section 3.2] "we partition the entire financial time-series into variable-length segments... using K-means algorithm with DTW."
  - [corpus] Related works like *Multi-period Learning for Financial Time Series Forecasting* suggest multi-scale inputs are beneficial, supporting the variable-length approach.
- **Break condition:** If market regimes shift such that the offline-clustered pattern centroids no longer represent the dominant morphological structures in the live data stream.

### Mechanism 2
- **Claim:** Learnable wavelet transforms adaptively isolate time-frequency features to handle non-stationarity.
- **Mechanism:** Instead of fixed frequency filters, the model employs a trainable Stationary Wavelet Transform (SWT) with learnable high-pass ($g$) and low-pass ($h$) filters. This allows the network to learn optimal decompositions for the specific noise profile and trend characteristics of the target financial asset, separating high-frequency noise from low-frequency trends dynamically.
- **Core assumption:** The predictive signal and the noise reside in different frequency bands that can be separated by convolution, and the optimal separation boundary is data-dependent (learnable).
- **Evidence anchors:**
  - [abstract] "devise a dynamic wavelet convolution module that emulates discrete wavelet transformation with enhanced flexibility."
  - [section 4.4] Figure 5 and accompanying text show "learnable wavelet framework offers significantly enhanced adaptability... compared to traditional fixed wavelet transforms."
  - [corpus] *A FEDformer-Based Hybrid Framework* validates the utility of frequency domain analysis in financial anomalies, though GPT4FTS extends this with learnable filters.
- **Break condition:** If the signal and noise spectra overlap significantly in the Fourier domain, linear filtering (even if learnable) may fail to disentangle them.

### Mechanism 3
- **Claim:** Next-patch prediction pre-training aligns the LLM's attention mechanism with the temporal causality of the new token space.
- **Mechanism:** The model is fine-tuned using a "next-patch prediction" objective. By forcing the model to predict the representation of the immediate next variable-length patch, it conditions the Transformer's self-attention to focus on the temporal dependencies between the newly defined "semantic" patches rather than raw time steps.
- **Core assumption:** The temporal evolution of these pattern-based patches follows a learnable autoregressive structure similar to language syntax.
- **Evidence anchors:**
  - [section 3.3] "We propose using next-patch prediction as a continual pre-training task... enabling it to understand time-series patches."
  - [abstract] "...enhances LLM capabilities for temporal sequence modeling through... modules."
  - [corpus] *Transformer Encoder and Multi-features Time2Vec* highlights the general efficacy of attention in financial temporal dependencies, which this mechanism exploits at the patch level.
- **Break condition:** If the sequence of patches becomes effectively random or lacks sufficient history-to-target correlation (efficient market hypothesis strong form).

## Foundational Learning

- **Concept: Dynamic Time Warping (DTW)**
  - **Why needed here:** This is the distance metric used to cluster similar market shapes of different lengths. You must understand that DTW allows non-linear alignment (stretching/compressing time) to say two price curves are "the same pattern" even if one took 5 days and the other 8.
  - **Quick check question:** If two price trends are identical but one is compressed in time, would Euclidean distance fail to cluster them while DTW succeeds?

- **Concept: Stationary Wavelet Transform (SWT)**
  - **Why needed here:** The model uses SWT (not DWT) for tokenization. You need to know that SWT avoids downsampling, keeping the time-series length ($L$) constant across scales. This is crucial for maintaining temporal alignment with the patching mechanism.
  - **Quick check question:** Why does SWT preserve the temporal resolution ($L \times L$) of the signal compared to standard Discrete Wavelet Transform (DWT)?

- **Concept: LLM Reprogramming / Patching**
  - **Why needed here:** The core innovation is mapping non-text data to the LLM input space. You need to understand that the LLM isn't reading numbers; it is reading "tokens" that represent aggregated numerical patches, projected into the LLM's embedding dimension.
  - **Quick check question:** How does mapping time-series patches to LLM embeddings allow a frozen or lightly tuned language model to process numerical financial data?

## Architecture Onboarding

- **Component map:**
  Raw OHLCV data ($B \times M \times L$) -> Learnable SWT Layer -> Multi-scale Time-Frequency tokens ($B \times M \times L \times (S+1)$) -> SIPR segmentation indices -> Dynamic patch tokenizer -> Linear projection to GPT-2 embedding -> 6-layer GPT-2 backbone -> Return prediction head

- **Critical path:** The synchronization between the **SIPR segmentation indices** and the **Wavelet frontend output**. If the wavelet output dimensions do not align with the patch slicing logic, the "semantic pattern" is destroyed before entering the Transformer.

- **Design tradeoffs:**
  - **Fixed vs. Learnable Wavelets:** Fixed wavelets (Haar, DB4) offer stability but less flexibility; Learnable wavelets adapt to specific market noise but risk overfitting or failing to converge to valid wavelet properties (orthogonality).
  - **Fixed vs. Adaptive Patches:** Adaptive patches capture semantic units but introduce computational overhead and complexity in batching compared to fixed sliding windows.

- **Failure signatures:**
  - **Cluster Drift:** If the model relies on outdated K-means centroids from training data, valid new market patterns might be forced into incorrect segments, leading to high DTW distances during preprocessing.
  - **Filter Divergence:** If learnable wavelet filters diverge into unstable states, the decomposition might amplify noise rather than trends (check gradient norms on wavelet parameters).
  - **Loss Spikes:** Standard in financial transformers, but look for spikes specifically when transitioning between regimes (e.g., low vol to high vol), indicating the patching strategy failed to capture the regime shift scale.

- **First 3 experiments:**
  1. **Centroid Validation:** Visualize the K-means centroids from the SIPR module. Do they represent distinct, recognizable financial shapes (e.g., "cup and handle," "head and shoulders")? If they look like random noise, the clustering hyperparameters ($K$, DTW window) are wrong.
  2. **Filter Frequency Response:** Plot the frequency response of the learned wavelet filters ($h$ and $g$) against the initialization (DB4). If they are identical, the learnable component isn't activating; if they are distorted beyond recognition, they may be overfitting.
  3. **Patch Integrity Test:** Run a forward pass with *fixed* patching vs. *adaptive* patching on a held-out set. Isolate the contribution of the segmentation logic to ensure the performance gain isn't solely from the GPT backbone or Wavelets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of clustering algorithm affect the performance of the scale-invariant pattern recognition module?
- Basis in paper: [explicit] Section 3.2 states, "investigating the impact of different clustering methods on subsequent predictions remains an important research direction," noting the paper focuses only on K-means to establish a baseline.
- Why unresolved: The study explicitly scoped its methodology to a single clustering approach (K-means++ with DTW) to maintain focus.
- What evidence would resolve it: A comparative ablation study substituting K-means with density-based (e.g., DBSCAN) or hierarchical clustering methods within the GPT4FTS pipeline.

### Open Question 2
- Question: Can the offline pattern recognition module be adapted to operate online to better handle sudden market structural breaks?
- Basis in paper: [inferred] The methodology relies on "offline scale-invariant pattern recognition" (Figure 2a) using historical data, which assumes past patterns hold for future segmentation.
- Why unresolved: Offline clustering centroids may become obsolete during distinct market regime changes (e.g., financial crises), potentially degrading segmentation quality.
- What evidence would resolve it: Performance analysis of the model during high-volatility regime shifts compared to a version with continuously updating clustering centroids.

### Open Question 3
- Question: Do the learnable wavelet filters retain the mathematical properties (e.g., orthogonality, vanishing moments) of the initializing wavelet basis?
- Basis in paper: [inferred] Section 3.4 notes filters are initialized with DB4 but are "learnable parameters," merging theory with flexibility.
- Why unresolved: Unconstrained optimization might cause the learned filters to diverge from wavelet properties, effectively becoming generic convolutional kernels rather than wavelet transforms.
- What evidence would resolve it: A spectral analysis of the trained filter coefficients to verify if they maintain the time-frequency localization characteristics of the original Daubechies wavelets.

## Limitations
- Limited geographic scope with evaluation only on Chinese and US market indices
- Critical hyperparameters not fully specified, creating potential reproducibility challenges
- Does not benchmark against traditional statistical methods or established financial models

## Confidence
- **High Confidence:** Technical feasibility of DTW clustering and learnable wavelets; general methodology of numerical patch-to-embedding mapping
- **Medium Confidence:** Claims about adaptive segmentation preserving pattern integrity; assertions about learnable wavelets providing enhanced adaptability
- **Low Confidence:** Claims of state-of-the-art performance across all metrics; scalability for large-scale financial datasets

## Next Checks
1. **Cross-Market Robustness Test:** Evaluate the trained model on a completely different market (e.g., European stocks, cryptocurrency, or commodities) without retraining. Measure performance degradation to assess true generalization beyond the training domain.

2. **Ablation Study with Fixed Alternatives:** Implement and compare against fixed-length sliding window segmentation, fixed wavelet transforms (Haar, Daubechies), and standard numerical encoding to isolate the contribution of each proposed innovation.

3. **Statistical Significance and Economic Validation:** Conduct paired t-tests or bootstrap analysis to confirm performance differences are statistically significant, perform out-of-sample walk-forward analysis with realistic transaction costs, and compare against simple buy-and-hold and traditional factor-based strategies.