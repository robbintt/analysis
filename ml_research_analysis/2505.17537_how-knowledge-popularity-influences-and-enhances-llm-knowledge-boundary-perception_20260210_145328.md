---
ver: rpa2
title: How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception
arxiv_id: '2505.17537'
source_url: https://arxiv.org/abs/2505.17537
tags:
- popularity
- confidence
- answer
- knowledge
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates how knowledge popularity influences LLMs\u2019\
  \ ability to perceive their knowledge boundaries. The authors measure knowledge\
  \ popularity through three perspectives: the popularity of entities in the question,\
  \ the popularity of entities in the answer, and the relation popularity defined\
  \ as their co-occurrence frequency."
---

# How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception

## Quick Facts
- arXiv ID: 2505.17537
- Source URL: https://arxiv.org/abs/2505.17537
- Reference count: 40
- Primary result: LLMs exhibit better QA performance, higher confidence, and more accurate perception on more popular knowledge, with relation popularity showing the strongest correlation

## Executive Summary
This paper investigates how knowledge popularity influences LLMs' ability to perceive their knowledge boundaries. The authors measure knowledge popularity through three perspectives: the popularity of entities in the question, the popularity of entities in the answer, and relation popularity defined as their co-occurrence frequency. Experiments on three datasets show that LLMs exhibit better QA performance, higher confidence, and more accurate perception on more popular knowledge, with relation popularity showing the strongest correlation. The authors propose leveraging these popularity signals for confidence calibration, improving answer correctness prediction accuracy by an average of 5.24% across all models and datasets. They also demonstrate that prompting LLMs to estimate popularity without external corpora provides a viable alternative.

## Method Summary
The study measures knowledge popularity using Wikidata sitelink counts for individual entities (PopQ for question entities, PopGe for answer entities) and Wikipedia co-occurrence counts for relation popularity (RPopGe). For calibration, they train a 3-layer MLP (64→32→2 neurons) that takes generation probability, entity popularity, and relation popularity as inputs to predict answer correctness. They evaluate on three datasets (Movies, Songs, Basketball) filtered to contain valid Wikidata entities, with accuracy computed after filtering out empty generations and entities not in Wikidata.

## Key Results
- LLMs exhibit better QA performance, higher confidence, and more accurate perception on more popular knowledge
- Relation popularity shows the strongest correlation with QA performance among the three popularity measures
- Confidence calibration using popularity signals improves answer correctness prediction accuracy by an average of 5.24% across all models and datasets
- Self-estimation of popularity via prompting is viable but weaker than corpus-based computation (3.4% degradation)

## Why This Works (Mechanism)

### Mechanism 1
Knowledge popularity serves as a proxy for training data exposure, influencing both model performance and confidence. More frequently encountered entities and relations during pre-training develop stronger internal representations, enabling more accurate recall and appropriately calibrated confidence. The model's generation probability (confidence) aligns better with actual correctness when knowledge is more popular because QA performance improves faster than confidence as popularity increases.

### Mechanism 2
Relation popularity (co-occurrence frequency) is the strongest predictor of QA performance and confidence calibration among the three popularity measures. Co-occurrence frequency captures associative learning—the model learns entity relationships through joint appearances in training documents. This signal outperforms individual entity popularity because it directly reflects the strength of the specific relationship being queried, not just general familiarity with entities.

### Mechanism 3
Popularity signals can calibrate confidence by identifying cases where confidence and correctness are misaligned. Low popularity (especially low relation popularity despite high entity popularity) indicates unfamiliarity that should reduce confidence. High popularity with low confidence indicates conservatism that can be corrected upward. An MLP combines probabilistic confidence with popularity features to produce better correctness predictions.

## Foundational Learning

- **Knowledge boundary perception**
  - Why needed here: The core problem—LLMs cannot reliably distinguish what they know from what they don't, producing confident hallucinations. Understanding this concept is essential for appreciating why calibration signals matter.
  - Quick check question: Can you explain why high generation probability doesn't guarantee factual correctness?

- **Confidence calibration**
  - Why needed here: The paper's intervention—adjusting model confidence to better reflect actual correctness likelihood. Without this concept, the 5.24% improvement claim lacks context.
  - Quick check question: What does it mean for a model to be "well-calibrated" vs. "overconfident"?

- **Entity-centric factual QA**
  - Why needed here: The experimental domain—questions with identifiable subject-relation-object structures enable popularity quantification through entities. This constrains generalizability.
  - Quick check question: Why might popularity-based calibration work less well for non-entity reasoning tasks?

## Architecture Onboarding

- **Component map**: Wikidata API -> Entity extraction and sitelink counting -> PopQ/PopGe; Wikipedia dump -> Co-occurrence counting -> RPopGe; Model inference -> Generation probability -> PC; MLP -> Correctness prediction

- **Critical path**: 1) Generate answer and extract confidence from token probabilities; 2) Extract entities from question and generated answer (Wikidata lookup required); 3) Compute PopQ, PopGe via sitelinks; compute RPopGe via Wikipedia co-occurrence search; 4) Feed features to trained MLP for correctness prediction; 5) If corpus unavailable, prompt model to self-rate familiarity (1-10 scale) for each signal

- **Design tradeoffs**: Corpus-based vs. self-generated popularity (corpus-based yields stronger calibration +3.4% avg over self-generated but requires external data); Single vs. combined signals (RPopGe alone outperforms PC in 6/9 cases; combining all three is best but requires all signals); Threshold-based vs. MLP (single features use threshold classification; multi-feature requires MLP training with labeled data)

- **Failure signatures**: Over-calibration (~1.2% of cases where PC+ALL misclassifies correct predictions); Domain mismatch (Basketball dataset shows weak/negative correlations for some popularity measures); Class imbalance (MLP fails on highly imbalanced datasets without rebalancing)

- **First 3 experiments**:
  1. Reproduce correlation analysis: On a held-out slice of one dataset, compute Spearman correlations between RPopGe and QA accuracy. Verify the paper's finding that RPopGe > PopGe > PopQ for accuracy correlation.
  2. Ablate calibration features: Train calibration MLPs with (a) PC only, (b) PC + RPopGe, (c) PC + ALL. Compare accuracy deltas to isolate each signal's contribution.
  3. Test self-estimation viability: For 100 samples, compare corpus-based RPopGe vs. model-self-rated familiarity (0-shot prompt). Compute correlation between the two to assess whether self-estimation is a viable proxy.

## Open Questions the Paper Calls Out

- How can knowledge popularity be effectively represented and measured for non-entity-centric tasks, such as abstract reasoning or open-ended generation?
- Does computing knowledge popularity directly from an LLM's actual pre-training corpus yield significantly better confidence calibration than using external proxy corpora like Wikipedia?
- Can specific fine-tuning or prompting strategies improve the accuracy of LLM self-estimated popularity to match the effectiveness of corpus-based metrics?

## Limitations
- The study focuses exclusively on entity-centric factual QA, limiting generalizability to other task types where popularity signals may not be computable or meaningful
- The assumption that Wikidata sitelink counts and Wikipedia co-occurrence frequencies are valid proxies for training corpus distribution is unverified
- Self-estimation shows significant degradation (3.4%) compared to corpus-based signals, raising questions about its practical viability when external resources are unavailable

## Confidence
- **High confidence**: Correlation between relation popularity and QA performance; calibration effectiveness of combined popularity signals; qualitative evidence of over-generalization behavior
- **Medium confidence**: Relative ordering of correlation strengths holds consistently but with variation across datasets; 3.4% degradation of self-estimation is measured but limited to 1k samples
- **Low confidence**: Mechanism explaining why relation popularity outperforms individual entity popularity; whether the specific MLP architecture is optimal; whether calibration improvements would persist under domain shifts

## Next Checks
1. Test whether popularity-based calibration trained on Wikipedia-derived signals maintains effectiveness when applied to models trained on different corpora
2. Systematically compare zero-shot, few-shot, and chain-of-thought prompting strategies for self-estimation to identify whether prompt engineering can close the 3.4% gap
3. Apply the same popularity-signal framework to non-entity tasks (e.g., mathematical reasoning or code generation) to determine whether the approach generalizes beyond entity-centric factual QA