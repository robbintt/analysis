---
ver: rpa2
title: Scaling Embeddings Outperforms Scaling Experts in Language Models
arxiv_id: '2601.21204'
source_url: https://arxiv.org/abs/2601.21204
tags:
- embedding
- scaling
- n-gram
- parameters
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores embedding scaling as an orthogonal dimension
  to Mixture-of-Experts (MoE) architectures for scaling large language models. Through
  comprehensive analysis and experiments, it identifies specific regimes where embedding
  scaling achieves superior Pareto efficiency compared to expert scaling.
---

# Scaling Embeddings Outperforms Scaling Experts in Language Models

## Quick Facts
- arXiv ID: 2601.21204
- Source URL: https://arxiv.org/abs/2601.21204
- Reference count: 9
- Primary result: N-gram Embedding scaling achieves superior Pareto efficiency compared to MoE expert scaling in specific sparsity regimes, demonstrated through LongCat-Flash-Lite achieving SOTA results on SWE-Bench (54.4), τ2-Bench Telecom (72.8), MMLU (85.52), and MATH500 (96.80)

## Executive Summary
This work explores embedding scaling as an orthogonal dimension to Mixture-of-Experts (MoE) architectures for scaling large language models. Through comprehensive analysis and experiments, it identifies specific regimes where embedding scaling achieves superior Pareto efficiency compared to expert scaling. The study systematically characterizes architectural factors governing embedding scaling efficacy, including parameter budgeting, vocabulary sizing, hyperparameter sensitivity, and model width/depth interactions. By integrating system optimizations and speculative decoding, the authors convert this sparsity into tangible inference speedups. Guided by these insights, they introduce LongCat-Flash-Lite, a 68.5B parameter model with 2.9B-4.5B activated parameters trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite surpasses parameter-equivalent MoE baselines and exhibits competitive performance against existing models of similar scale, particularly excelling in agentic and coding tasks.

## Method Summary
The method introduces N-gram Embedding scaling as an alternative to MoE expert scaling, using polynomial rolling hash tables to capture local context with O(1) lookup complexity. The approach employs K sub-tables per n-gram order (N=3-5) with linear projections, combined with Embedding Amplification to mitigate signal suppression in residual streams. The architecture integrates with MoE layers (256 FFN experts + 128 zero-experts, top-12 routing) and incorporates system optimizations including N-gram Cache, Eagle3 speculative decoding, and kernel fusion. The total parameter budget allocates approximately 50% to N-gram embeddings, with careful vocabulary sizing to avoid hash collisions at integer multiples of base vocabulary.

## Key Results
- N-gram Embedding scaling achieves superior Pareto efficiency compared to MoE expert scaling when base model sparsity exceeds ratio of 20
- LongCat-Flash-Lite (68.5B total, 2.9B-4.5B active) achieves SOTA results: SWE-Bench 54.4, τ2-Bench Telecom 72.8, MMLU 85.52, MATH500 96.80
- Embedding scaling shows stronger scaling advantages in wider models and diminishes in deeper models (>20 layers)
- Inference speedups achieved through N-gram Cache and speculative decoding convert sparsity into practical performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** N-gram Embedding scaling yields a superior Pareto frontier compared to MoE expert scaling when the base model has a high parameter-to-activation ratio (high sparsity), but this advantage diminishes if the embedding parameter budget exceeds ~50% of total parameters.
- **Mechanism:** MoE scaling follows a log-linear curve where adding experts at low counts yields large loss reductions, but at high counts returns diminish. N-gram Embeddings capture local context via hashed n-gram tables without routing overhead, exploiting this saturation. Over-allocating to embeddings (>50%) reverses the advantage, making the model worse than a parameter-equivalent MoE baseline.
- **Core assumption:** The optimal ~50% allocation point generalizes across scales, though evidence is at 280M–1.3B activated parameters and 300B tokens.
- **Evidence anchors:** [abstract], [section 3.1], [section 3.2.1]
- **Break condition:** Advantage breaks if (a) base model is too dense, (b) embedding proportion exceeds 50%, or (c) data/task domain differs significantly.

### Mechanism 2
- **Claim:** N-gram Embedding efficacy is sensitive to hash collisions, mitigated by choosing vocabulary sizes that deviate from integer multiples of the base vocabulary size.
- **Mechanism:** A polynomial rolling hash maps n-grams to indices. When the vocabulary size (modulus) nears an integer multiple of the base vocabulary, 2-gram collisions spike non-linearly, forcing semantic ambiguity and degrading learning.
- **Core assumption:** Collision patterns hold across tokenizers and datasets; evidence is at 128k base vocab and a specific corpus.
- **Evidence anchors:** [section 3.2.2], [figure 3b]
- **Break condition:** May not apply with different hashing schemes, base vocabularies, or higher-order n-grams (3+).

### Mechanism 3
- **Claim:** "Embedding Amplification" (scaling factor or LayerNorm on embedding output) mitigates signal suppression, improving N-gram Embedding effectiveness.
- **Mechanism:** The first attention module's output can have ~10× larger L2 norm than the embedding output, drowning it in the residual stream. Amplifying the embedding ensures sufficient contribution.
- **Core assumption:** Signal suppression persists during training, not just at initialization; observed after 300B tokens.
- **Evidence anchors:** [section 3.2.4]
- **Break condition:** Over-amplification may cause instability; effect may diminish in very deep models.

## Foundational Learning

- **Concept: Residual Stream Dynamics & Signal Suppression**
  - **Why needed here:** Explains why embedding signal gets drowned out and how Amplification fixes it.
  - **Quick check question:** What is the relative L2 norm of a typical attention output vs. the residual branch at layer 1, and why does this matter for embeddings?

- **Concept: Hashing, Modulo Arithmetic, and Collision Analysis**
  - **Why needed here:** Underpins the vocabulary-size selection rule to avoid collision spikes.
  - **Quick check question:** For `H = (sum(t_i * V^j)) % V_n`, why does setting `V_n` to an integer multiple of base vocab increase 2-gram collisions?

- **Concept: MoE Sparsity, Expert Routing, and Pareto Efficiency**
  - **Why needed here:** Frames the trade-off between expert and embedding scaling.
  - **Quick check question:** As you add more experts with a fixed activation budget, why do loss reductions become harder, and what are "diminishing returns"?

## Architecture Onboarding

- **Component Map:** Tokenization -> Base embedding lookup -> Rolling hash for 2-N-grams -> Sub-table lookups + projections -> Average with base embedding -> Embedding Amplification -> Residual stream -> MoE layers -> Inference: Cache + speculative decoding

- **Critical Path:**
  1. Tokenization
  2. Base embedding lookup
  3. Rolling hash for 2-N-grams
  4. Sub-table lookups + projections
  5. Average with base embedding
  6. Embedding Amplification
  7. Entry into residual stream
  8. MoE layers (reduced active parameters)
  9. Inference: Cache + speculative decoding

- **Design Tradeoffs:**
  - N=3-5, K≥2 (robustness vs. compute)
  - Embedding budget ≤50% total
  - Vocab size avoids integer multiples of base
  - Wider models amplify advantage; deeper (>20 layers) diminish it

- **Failure Signatures:**
  - Collision spike: loss plateaus at specific vocab sizes
  - Signal suppression: L2 norm analysis shows attention >> embedding
  - Over-allocation: scaling curve intersects MoE baseline
  - Inference bottleneck: lookup overhead negates MoE gains without cache/batching

- **First 3 Experiments:**
  1. Pareto Frontier Check: Vary total/active ratios in a 280M activated MoE; add N-gram at different sparsities; compare to parameter-equivalent MoE baselines to find intersection.
  2. Hash Collision Analysis: Sweep vocab sizes (30-33× base); compute collisions on sample corpus; correlate with loss; confirm spike near multiples.
  3. Embedding Amplification Ablation: Train with/without √D scaling; monitor layer-wise L2 norms and final loss; confirm ~0.02 loss reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can N-gram Embedding outputs be repurposed as an ultra-fast draft model for speculative decoding by attaching a lightweight linear projection?
- **Basis in paper:** [explicit] Section 4.3 states this is an "unexplored" direction where "we are currently exploring architectures to repurpose the N-gram embedding as an ultra-fast draft model."
- **Why unresolved:** The semantic richness of N-gram Embeddings captures local context, but optimal architectural designs to exploit this for token prediction remain undefined.
- **What evidence would resolve it:** Comparative latency and acceptance rate measurements between N-gram-based drafting and conventional draft models across standard benchmarks.

### Open Question 2
- **Question:** Can N-gram Embedding representations serve as an efficient "early rejection" mechanism for draft tokens before expensive target-model verification?
- **Basis in paper:** [explicit] Section 4.3 proposes using N-gram Embedding as "a semantic consistency check (or confidence estimator)" for early rejection, noting it "would reduce the workload of the verification step."
- **Why unresolved:** The theoretical pruning pathway is identified but the actual rejection threshold calibration and end-to-end latency impact are untested.
- **What evidence would resolve it:** Ablation studies measuring verification workload reduction and net latency gains with varying rejection thresholds.

### Open Question 3
- **Question:** What is the optimal allocation strategy for embedding parameters across layers in Per-Layer N-gram Embedding (PLNE)—concentrated or uniform distribution?
- **Basis in paper:** [explicit] Section 5.3 states PLNE "merits further investigation—specifically regarding the optimal allocation of embedding parameters across layers, such as determining whether to concentrate them in a few specific layers or distribute them uniformly."
- **Why unresolved:** PLNE showed inconsistent advantages over baseline NE in wider/deeper models, suggesting allocation strategy may be critical but unexplored.
- **What evidence would resolve it:** Systematic comparison of concentrated vs. distributed PLNE configurations at matched parameter budgets, measuring convergence speed and final performance.

## Limitations
- The ~50% embedding parameter allocation sweet spot is demonstrated only within a narrow parameter and sparsity range (280M-1.3B activated parameters) and may not generalize across scales.
- Hash collision analysis is validated only for 2-grams with a specific tokenizer and corpus, raising questions about applicability to different hashing schemes or higher-order n-grams.
- Inference speedup claims depend heavily on specific system optimizations (N-gram Cache, Eagle3 speculative decoding) that may not transfer to different hardware or deployment scenarios.

## Confidence

- **High Confidence:** The fundamental claim that N-gram Embeddings can achieve superior Pareto efficiency compared to MoE scaling in specific sparsity regimes is well-supported by systematic experiments and clear mathematical reasoning about diminishing returns in expert scaling.
- **Medium Confidence:** The specific claim about the 50% parameter allocation threshold is well-characterized within the tested regime but may not generalize across scales or different model architectures.
- **Low Confidence:** The inference speedup claims are heavily dependent on specific system optimizations that are not fully detailed in the paper, and strong performance claims on downstream benchmarks lack comprehensive ablation studies.

## Next Checks
1. **Cross-Scale Validation:** Replicate the Pareto frontier analysis at 10B+ parameter scales with varying sparsity ratios (10-70% embedding allocation) to test whether the ~50% sweet spot holds across orders of magnitude in model size and training compute.

2. **Collision Robustness Analysis:** Systematically vary base vocabulary sizes (64k, 128k, 256k), hashing functions (CRC, MurmurHash, custom polynomial), and n-gram orders (2-5) to validate whether the integer-multiple avoidance rule generalizes or is specific to the tested configuration.

3. **Inference Stack Isolation:** Implement N-gram Embedding in a baseline MoE model without the N-gram Cache, speculative decoding, or kernel fusion optimizations to measure the pure computational overhead and determine whether claimed speedups come from embeddings themselves or the surrounding system optimizations.