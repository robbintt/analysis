---
ver: rpa2
title: 'A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining
  Vision-Language Models and Machine Learning for Real-Time Processing'
arxiv_id: '2601.01897'
source_url: https://arxiv.org/abs/2601.01897
tags:
- document
- claim
- extraction
- accuracy
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of automated parsing and structured
  information extraction from heterogeneous claims documents in healthcare and insurance
  operations. The proposed solution integrates a multilingual OCR engine (PaddleOCR),
  a traditional Logistic Regression classifier, and a compact Vision-Language Model
  (Qwen 2.5-VL-7B) into a four-stage pipeline: pre-processing, hybrid classification,
  adaptive extraction, and post-processing.'
---

# A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining Vision-Language Models and Machine Learning for Real-Time Processing

## Quick Facts
- arXiv ID: 2601.01897
- Source URL: https://arxiv.org/abs/2601.01897
- Reference count: 5
- Primary result: >95% document-type classification accuracy and ~87% field extraction accuracy with <2s processing latency

## Executive Summary
This paper presents a hybrid architecture for automated parsing and structured information extraction from heterogeneous claims documents in healthcare and insurance. The solution integrates PaddleOCR, Logistic Regression, and a compact Vision-Language Model (Qwen 2.5-VL-7B) into a four-stage pipeline. The system achieves high accuracy while maintaining real-time performance, successfully deployed in production processing tens of thousands of claims weekly from Vietnam and Singapore.

## Method Summary
The approach uses a four-stage pipeline: pre-processing (PDF splitting, resizing to 1024px), hybrid classification (VLM title extraction with rule-based mapping, fallback to Logistic Regression with TF-IDF), adaptive extraction (schema-specific prompts for the VLM), and post-processing (Elasticsearch normalization). The system handles multilingual documents (50% English, 50% Vietnamese) and various formats (70% printed, 30% handwritten). Classification accuracy exceeds 95% and field-level extraction accuracy reaches approximately 87%, with average latency under 2 seconds per document.

## Key Results
- Document-type classification accuracy exceeds 95% (93-97% across regions)
- Field-level extraction accuracy achieves approximately 87%
- Processing latency maintained under 2 seconds per document
- 300× efficiency improvement over manual processing (10 minutes per claim)

## Why This Works (Mechanism)

### Mechanism 1: Task Decomposition for Compact VLMs
The VLM performs localized text-image reasoning on header regions rather than full-page semantic classification. Title extraction leverages the model's stronger visual grounding on short, well-defined text segments, avoiding the broader contextual reasoning that exceeds compact model capacity. This approach achieves >95% accuracy compared to ~70% when directly classifying document types.

### Mechanism 2: Hybrid Fallback with Traditional ML
When title-to-type mapping fails (particularly for visually similar invoices and bills), the system routes to a TF-IDF + Logistic Regression classifier trained on 2,000 labeled samples. This captures layout-based and lexical cues that the VLM misses, achieving 87% accuracy on unmapped cases.

### Mechanism 3: Schema-Conditioned Extraction with Structured Prompts
Four-component structured prompts (role, fields, format, example) improve field-level extraction accuracy by constraining the VLM's output space. Each document type triggers a schema-specific prompt that enables the model to jointly process visual layouts and OCR tokens while outputting bounding boxes and confidence scores.

## Foundational Learning

- **Vision-Language Model (VLM) Capacity Constraints**: The paper explicitly chooses a compact 7B-parameter model for privacy and latency, limiting full-document reasoning. This explains why task decomposition is necessary. Quick check: Can you explain why extracting a title is computationally easier for a VLM than classifying an entire document type?

- **TF-IDF Vectorization**: The fallback classifier uses TF-IDF features to create sparse lexical representations. Understanding this helps diagnose why certain document types map well to this approach. Quick check: Why might TF-IDF outperform a VLM on distinguishing visually similar invoices?

- **OCR Confidence Scoring and Spatial Coordinates**: PaddleOCR outputs include confidence scores and bounding boxes used for cross-validation and downstream filtering. These metadata enable the post-processing normalization step. Quick check: How would you use OCR confidence scores to flag potentially erroneous VLM extractions?

## Architecture Onboarding

- **Component map**: Image input → OCR (required for cross-validation) → Classification (determines schema) → Extraction (core value) → Normalization
- **Critical path**: Image input → OCR → Classification → Extraction → Normalization. Latency budget is <2 seconds per document.
- **Design tradeoffs**: Compact VLM (7B) vs. large VLM: Privacy/latency wins over raw accuracy. VLM title extraction vs. direct classification: Localized task reduces error but adds fallback complexity. 1024px resolution: Balances detail preservation against token budget and latency.
- **Failure signatures**: Low OCR confidence on handwritten/dotted-line fields (e.g., claim amounts on Vietnamese forms), date field confusion when multiple dates present, hospital name normalization failures when extracted name has no close match in reference database, classification accuracy drops for Singapore (93%) vs. Vietnam (97%), suggesting regional variation.
- **First 3 experiments**:
  1. **Classification ablation**: Measure accuracy with VLM-only, LR-only, and hybrid approaches on a held-out set to validate the fallback design.
  2. **Prompt component analysis**: Remove each of the four prompt components systematically to quantify their individual contributions to extraction accuracy.
  3. **OCR-VLM disagreement logging**: Flag cases where VLM-extracted values conflict with OCR-detected text; manually review to identify systematic error patterns.

## Open Questions the Paper Calls Out
1. How can the pipeline be adapted to automatically handle merged claim files containing multiple document types without manual splitting?
2. How can date ambiguity be resolved when multiple date fields (e.g., visit date vs. treatment date) appear in a single document?
3. What is the optimal architecture for a multi-agent claim adjudication framework that utilizes the outputs of this extraction pipeline?

## Limitations
- Performance degrades with higher proportions of handwritten text where OCR confidence typically drops below 0.5
- Regional performance gap (93% accuracy in Singapore vs 97% in Vietnam) suggests document-type variability not fully characterized
- Effectiveness depends on completeness of title-to-type mapping rules that aren't specified
- Elasticsearch-based normalization assumes comprehensive provider databases that may not exist for all regions

## Confidence
- **High Confidence**: Core architectural decomposition (OCR → title extraction → fallback classification → schema-conditioned extraction) is well-supported by results showing >95% classification accuracy and ~87% field extraction accuracy.
- **Medium Confidence**: 300× efficiency improvement assumes linear scaling that may not hold for edge cases requiring human review. 2-second processing latency achievable with described hardware but would degrade with smaller models or CPU-only inference.
- **Low Confidence**: Treatment of error propagation between stages is superficial. Paper doesn't quantify how OCR errors impact VLM extraction accuracy or how often fallback classifier is invoked in production.

## Next Checks
1. **Regional Generalization Test**: Run the pipeline on a balanced dataset containing only Singapore documents to measure whether the 93% accuracy holds or if performance degrades further.
2. **Handwriting Stress Test**: Create a synthetic test set by overlaying handwritten text on printed document templates, then measure OCR confidence scores and downstream extraction accuracy.
3. **Fallback Frequency Audit**: Instrument the production system to log every instance where title extraction fails and the Logistic Regression classifier is invoked. Measure the accuracy of these fallback predictions and the overall impact on system reliability.