---
ver: rpa2
title: 'DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality
  Assessment'
arxiv_id: '2509.17012'
source_url: https://arxiv.org/abs/2509.17012
tags:
- document
- quality
- image
- images
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DocIQ, a specialized no-reference Document
  Image Quality Assessment (DIQA) model, and DIQA-5000, a novel dataset containing
  5,000 document images rated across three dimensions (overall quality, sharpness,
  color fidelity) by 15 subjects each. The DocIQ model integrates document layout
  features with multi-level image representations through a feature fusion module,
  employing independent quality heads for each dimension to predict score distributions.
---

# DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment

## Quick Facts
- **arXiv ID**: 2509.17012
- **Source URL**: https://arxiv.org/abs/2509.17012
- **Reference count**: 27
- **Primary result**: DocIQ achieves SRCC/PLCC scores of 0.9083/0.9218 for overall quality on DIQA-5000 dataset

## Executive Summary
This paper addresses the challenge of no-reference Document Image Quality Assessment (DIQA) by introducing DocIQ, a specialized model that combines document layout features with multi-level image representations. The authors also present DIQA-5000, a novel dataset of 5,000 document images rated across three quality dimensions (overall quality, sharpness, and color fidelity) by 15 subjects each. The DocIQ model employs a feature fusion module that integrates layout information with progressive multi-scale image features, using independent quality heads for each dimension to predict score distributions. Experiments demonstrate that DocIQ significantly outperforms state-of-the-art IQA models on both the DIQA-5000 dataset and an additional OCR-focused dataset, validating its effectiveness for practical document quality assessment applications.

## Method Summary
DocIQ is a no-reference DIQA model that predicts document image quality across three dimensions using a feature fusion approach. The model integrates document layout features extracted from pretrained detectors with multi-level image representations from a ResNet50 backbone. A feature fusion module progressively combines these features at multiple scales, and three parallel regression heads independently predict quality scores for overall quality, sharpness, and color fidelity. The model is trained on the newly introduced DIQA-5000 dataset, which contains 5,000 document images rated by 15 subjects on each of the three quality dimensions. The training uses Adam optimizer with learning rate 2×10⁻⁴, step decay, and batch size 20 on NVIDIA A10 GPUs.

## Key Results
- DocIQ achieves SRCC/PLCC scores of 0.9083/0.9218 for overall quality on DIQA-5000 dataset
- Performance on sharpness: 0.9006/0.8615 SRCC/PLCC
- Performance on color fidelity: 0.8907/0.8666 SRCC/PLCC
- Strong performance on SmartDoc-QA dataset with OCR accuracy metrics

## Why This Works (Mechanism)
DocIQ works by leveraging both image content and layout structure to assess document quality. The model recognizes that document images have unique characteristics (text, tables, figures) that require specialized assessment beyond general image quality. By integrating layout-aware features with multi-scale image representations, the model can better distinguish between quality degradations that affect readability versus those that don't. The three independent quality heads allow the model to capture different aspects of document quality that may not be correlated, while the multi-rater training strategy enables learning from subjective human judgments.

## Foundational Learning
- **DIQA vs IQA**: Document Image Quality Assessment differs from general Image Quality Assessment because documents have structured layouts (text, tables, figures) where quality affects readability differently than natural images
- **SRCC/PLCC metrics**: Spearman's Rank Correlation Coefficient (SRCC) measures prediction monotonicity while Pearson's Linear Correlation Coefficient (PLCC) measures linear correlation with ground truth scores
- **Feature fusion**: The progressive combination of multi-scale features from different modalities (image content and layout) allows the model to capture both local and global quality aspects
- **Multi-rater training**: Training with multiple subjective scores per image enables the model to learn the distribution of human judgments rather than a single ground truth
- **Layout-aware features**: Document layouts provide important context for quality assessment since degradation in different regions (text vs figures) affects overall quality differently

## Architecture Onboarding

**Component map**: Image → ResNet50 → Feature Fusion → 3 Quality Heads → MOS

**Critical path**: Input image → Layout mask generation → Feature fusion module → Parallel quality heads → Score aggregation

**Design tradeoffs**: The paper trades model complexity for accuracy by using separate quality heads and incorporating layout information, which improves performance but increases computational requirements compared to single-head models.

**Failure signatures**: Poor performance on DIQA tasks when layout masks are inaccurate or when the model is trained without layout information, indicating the importance of the layout-aware component.

**First experiments to run**:
1. Train DocIQ without layout information to verify the importance of layout features
2. Evaluate each quality head independently to understand their contribution to overall performance
3. Test model performance on distorted document images to validate its effectiveness in real-world scenarios

## Open Questions the Paper Calls Out
None identified in the paper.

## Limitations
- The DIQA-5000 dataset is newly introduced and may not be publicly available, limiting reproducibility
- The exact architectural details of the feature fusion module are not fully specified, making exact replication difficult
- The model's performance on extremely degraded document images or those with complex layouts is not extensively evaluated

## Confidence

**High**: The general framework and methodology are well-described, including the three-dimensional quality assessment approach and the use of layout-aware features

**Medium**: The training procedure and evaluation metrics are specified, but dataset access is uncertain

**Low**: Critical architectural details for the feature fusion module and exact implementation specifics are missing

## Next Checks
1. Verify availability of the DIQA-5000 dataset or attempt to replicate the dataset construction pipeline using the referenced enhancement tools
2. Contact authors for clarification on the feature fusion module architecture and loss function formulation
3. Test layout mask generation using DocLayout-YOLO [17] or MinerU [18] to ensure proper integration with the document images