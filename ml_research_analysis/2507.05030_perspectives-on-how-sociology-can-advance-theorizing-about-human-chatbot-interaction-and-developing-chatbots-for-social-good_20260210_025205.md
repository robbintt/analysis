---
ver: rpa2
title: Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction
  and Developing Chatbots for Social Good
arxiv_id: '2507.05030'
source_url: https://arxiv.org/abs/2507.05030
tags:
- https
- chatbot
- theory
- chatbots
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Sociology is underrepresented in human-chatbot interaction research\
  \ despite chatbots' rapid growth. This paper proposes four sociological theories\u2014\
  resource substitution, power-dependence, affect control, and fundamental cause of\
  \ disease theory\u2014to enhance understanding of chatbot use and guide development\
  \ of chatbots for social good."
---

# Perspectives on How Sociology Can Advance Theorizing about Human-Chatbot Interaction and Developing Chatbots for Social Good

## Quick Facts
- arXiv ID: 2507.05030
- Source URL: https://arxiv.org/abs/2507.05030
- Reference count: 29
- Primary result: Sociology is underrepresented in human-chatbot interaction research despite chatbots' rapid growth.

## Executive Summary
This paper argues that sociology is underrepresented in human-chatbot interaction research despite chatbots' rapid growth. The authors propose four sociological theories—resource substitution, power-dependence, affect control, and fundamental cause of disease theory—to enhance understanding of chatbot use and guide development of chatbots for social good. These frameworks offer novel insights into demographic patterns in chatbot use, risks of emotional over-reliance, and how chatbots can promote equity and wellbeing. The work aims to spur sociological engagement in this emerging field.

## Method Summary
This is a conceptual paper proposing theoretical frameworks rather than an empirical ML study. The authors synthesize existing sociological theories and apply them to human-chatbot interaction contexts. They reference publicly available ACT equations (Heise, 2007, 2010) and EPA sentiment profiles via the actdata R package, along with survey data from Madden et al. (2024) and Hopelab (2024) for demographic patterns. The approach involves mapping sociological concepts to chatbot interaction phenomena and proposing how these theories could guide chatbot design and evaluation.

## Key Results
- Resource substitution theory explains why marginalized groups use chatbots more intensively to compensate for systemic resource deficits
- Power-dependence theory reveals how emotional dependency emerges from perceived lack of alternative social connections
- Affect Control Theory's deflection scores can function as pre-deployment guardrails to identify socially inappropriate chatbot responses
- Fundamental cause theory suggests chatbots should target upstream determinants of health inequities, not just individual symptoms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Marginalized groups use chatbots more intensively to compensate for resource deficits created by systemic discrimination
- Mechanism: Resource substitution theory posits that individuals benefit more from any single resource when they have fewer alternatives. When structural barriers constrain access to traditional support networks, chatbots become relatively more valuable as substitutes.
- Core assumption: Chatbots can meaningfully substitute for human-provided resources like companionship, academic support, or emotional validation.
- Evidence anchors: Survey data showing Black adolescents more likely than White adolescents to use generative AI for schoolwork and companionship; transgender/nonbinary youth more likely to engage in extended chatbot conversations.

### Mechanism 2
- Claim: Emotional dependency on chatbots emerges from perceived lack of alternative social connections, not individual pathology
- Mechanism: Power-dependence theory frames dependency as a network property. Users become emotionally dependent when they perceive few alternative sources for companionship, making the chatbot's relative power high.
- Core assumption: Human-chatbot interactions function as exchange relations where users perceive reciprocal value and the chatbot simulates having needs.
- Evidence anchors: Replika users valued support more when "they had no human upon which to rely, making Replika their sole source for support"; users report difficulty stopping usage despite harms.

### Mechanism 3
- Claim: Affect Control Theory's "deflection score" can function as a pre-deployment guardrail to identify socially inappropriate chatbot responses before harm occurs
- Mechanism: ACT quantifies cultural sentiments along EPA (evaluation, potency, activity) dimensions. Equations compute "deflection" scores—lower values indicate situations aligning with shared expectations, higher values indicate norm violations.
- Core assumption: Culturally shared sentiments can be adequately measured and encoded; deflection thresholds generalize across contexts.
- Evidence anchors: Scholars have compared ACT-informed chatbot responses to ChatGPT and "found the former to provide more situationally appropriate responses"; publicly available EPA profiles and equations enable transparent implementation.

## Foundational Learning

- Concept: **Exchange Relations**
  - Why needed here: Power-dependence theory requires conceptualizing human-chatbot interaction as an exchange where both parties provide valued resources. Without this framing, dependency analysis defaults to individual psychology.
  - Quick check question: Can you articulate three resources a user might exchange with a chatbot, and what the chatbot might be perceived as receiving in return?

- Concept: **EPA Profiles (Evaluation-Potency-Activity)**
  - Why needed here: ACT's computational approach requires understanding that identity labels carry quantified sentiment values along three dimensions that vary by culture.
  - Quick check question: How might the EPA profile for "girlfriend" differ from "AI system," and why does this matter for designing identity-transition reminders?

- Concept: **Upstream vs. Downstream Interventions**
  - Why needed here: Fundamental cause theory distinguishes interventions targeting root causes from symptomatic treatments. Chatbot design should consider both.
  - Quick check question: A mental health chatbot refers users to therapy (micro-level). What would meso- and macro-level intervention equivalents look like?

## Architecture Onboarding

- Component map: User need detection layer -> Alternative source mapper -> EPA sentiment database -> Deflection calculator -> Intervention level router

- Critical path:
  1. User input -> need classification
  2. Need classification -> dependency risk assessment (using alternative source mapper)
  3. Candidate response generation -> deflection scoring
  4. If deflection > threshold -> response blocked or modified
  5. If dependency risk high -> meso-level referral优先 over micro-level support

- Design tradeoffs:
  - Cultural specificity vs. generalization: EPA profiles from college samples lack external validity; collecting representative data is expensive but necessary for safe deployment
  - Guardrail strictness vs. engagement: High deflection thresholds may over-block benign responses; low thresholds miss harms
  - Dependency prevention vs. user autonomy: Actively referring users to human alternatives may feel intrusive or unwanted

- Failure signatures:
  - Cold-start cultural mismatch: Using EPA profiles from wrong population produces inappropriate deflection thresholds
  - Dependency spiral: Chatbot successfully meets companionship needs but doesn't build pathways to human alternatives, entrenching reliance
  - Identity-transition distress: Abrupt AI identity reminders after intimate exchanges cause high deflection -> user distress -> potential harm escalation

- First 3 experiments:
  1. EPA validation study: Collect sentiment ratings from target population (e.g., adolescents, specific cultural groups); compare against existing databases to quantify calibration error
  2. Deflection threshold calibration: Generate synthetic chatbot-user conversations with known appropriateness ratings; fit deflection threshold that maximizes classification accuracy
  3. Alternative source intervention pilot: A/B test chatbots that (A) provide direct support vs. (B) provide support + referral to human alternatives; measure dependency outcomes at 30/60/90 days

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do demographic groups predicted to use chatbots for resource deficits (e.g., Black adolescents) actually derive differential benefits consistent with resource substitution theory?
- Basis in paper: The authors state that "whether demographic differences in rates of using chatbots for meeting resource deficits yield differential benefits that are consistent with the predictions of resource substitution theory remains unknown."
- Why unresolved: While resource substitution theory predicts that marginalized groups benefit more from single resources, safety concerns regarding overreliance or excessive dependence make it unclear if chatbots actually function as beneficial substitutions in practice.
- What evidence would resolve it: Comparative survey or experimental data measuring specific outcomes (e.g., academic performance, well-being) across demographic groups who use chatbots to address resource deficits.

### Open Question 2
- Question: What specific network conditions (number of alternatives) mark the threshold where emotional dependency on a chatbot becomes "too much" or dysfunctional?
- Basis in paper: The paper notes that "additional research [is] needed to elaborate power-dependence theory to identify when network conditions (the number of alternatives) reach a level of 'too much' dependency."
- Why unresolved: Current literature defines emotional dependency by observable dysfunction but lacks a process model that identifies the specific structural tipping points in a user's social network that precipitate this state.
- What evidence would resolve it: Longitudinal studies mapping users' social network structures (specifically the number of alternative sources for companionship) against clinical markers of emotional dependency.

### Open Question 3
- Question: Should chatbots use widely shared sentiment estimates or personalized self-sentiments when transitioning from user-assigned identities (e.g., girlfriend) to the AI identity to minimize distress?
- Basis in paper: The authors ask, "Whether this should be accomplished by using widely shared estimates of sentiments... or personalized self-sentiments... will need to be determined."
- Why unresolved: Using shared sentiments privileges societal norms, while self-sentiments privilege user preferences; it is unclear which method reduces safety risks (like self-harm ideation) when the chatbot must break the illusion of a human-like relationship.
- What evidence would resolve it: A/B testing of identity transition strategies using general versus personalized EPA (evaluation, potency, activity) profiles and measuring user distress or deflection scores.

### Open Question 4
- Question: How must Affect Control Theory data collection methods be refined to validly estimate sentiments for minors and minoritized groups?
- Basis in paper: The authors state that "more work is needed to refine ACT's data collection methods and estimates for a broader range of populations, including minors and minoritized groups."
- Why unresolved: Current sentiment dictionaries rely heavily on college samples, which may not accurately reflect the affective norms of the specific populations (e.g., LGBTQ+ youth) most likely to use chatbots for social support.
- What evidence would resolve it: Development and validation of new EPA profiles for specific minoritized and age-based demographics to compare against existing general population databases.

## Limitations

- Theoretical propositions rely heavily on surveys from specific populations without clear external validity assessments
- Mechanisms assume chatbots can meaningfully substitute for human-provided resources, which requires empirical validation
- ACT's deflection-based guardrails depend on EPA profiles primarily collected from college samples, raising cultural and demographic generalization concerns
- Fundamental cause theory's application to chatbot interventions at meso/macro levels is largely speculative with no concrete implementation examples

## Confidence

- **High confidence**: The underrepresentation of sociology in human-chatbot interaction research is real and documented; sociological theories can offer valuable new perspectives
- **Medium confidence**: Resource substitution and power-dependence theories plausibly explain observed demographic patterns in chatbot use; ACT can identify inappropriate responses through deflection scoring
- **Low confidence**: Fundamental cause theory's application to chatbot interventions at meso/macro levels is largely speculative; no concrete examples demonstrate how chatbots can effectively target upstream determinants of health inequities

## Next Checks

1. **Population-specific EPA profile validation**: Collect sentiment ratings for key identities, behaviors, and emotions from target user populations (adolescents, minoritized groups) and compare deflection score predictions against human judgments of appropriateness to quantify cultural generalization errors.

2. **Dependency risk assessment field study**: Conduct longitudinal tracking of chatbot users from populations identified as high-risk for dependency (based on perceived network constraints) to measure whether dependency develops over time and whether alternative-source interventions reduce this risk.

3. **Deflection threshold calibration experiment**: Generate diverse synthetic chatbot responses across different cultural contexts and user populations; use human raters to label appropriateness and fit deflection thresholds that optimize classification accuracy while minimizing false positives/negatives across groups.