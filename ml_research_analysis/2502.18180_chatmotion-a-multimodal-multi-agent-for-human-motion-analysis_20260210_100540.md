---
ver: rpa2
title: 'ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis'
arxiv_id: '2502.18180'
source_url: https://arxiv.org/abs/2502.18180
tags:
- motion
- chatmotion
- human
- arxiv
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ChatMotion is a multi-agent framework for human motion analysis
  that overcomes the limitations of single-model approaches by dynamically decomposing
  tasks, integrating multiple motion LLMs, and aggregating results through specialized
  modules. It addresses the challenge of adapting to complex, multi-faceted user queries
  in human motion understanding by leveraging a planner-Executor-verifier architecture
  with a MotionCore toolbox containing MotionAnalyzer, Aggregator, Generator, and
  Auxiliary Tools.
---

# ChatMotion: A Multimodal Multi-Agent for Human Motion Analysis

## Quick Facts
- arXiv ID: 2502.18180
- Source URL: https://arxiv.org/abs/2502.18180
- Reference count: 16
- ChatMotion achieves 58.79% accuracy on MoVid-Bench, outperforming LLaMo's 55.32%

## Executive Summary
ChatMotion introduces a multi-agent framework for human motion analysis that overcomes the limitations of single-model approaches by dynamically decomposing tasks, integrating multiple motion LLMs, and aggregating results through specialized modules. The system addresses the challenge of adapting to complex, multi-faceted user queries in human motion understanding through a planner-Executor-verifier architecture with a MotionCore toolbox. Extensive experiments demonstrate ChatMotion's superior performance across multiple benchmarks, establishing it as a new benchmark for comprehensive, adaptable human motion analysis.

## Method Summary
ChatMotion employs a three-agent system (Planner→Executor→Verifier) using LLaMA-70B for all agents, with a MotionCore toolkit containing MotionAnalyzer, Aggregator, Generator, and Auxiliary Tools. The system dynamically decomposes user queries into meta-tasks, maps them to specialized models (MotionLLM, MotionGPT, LLaMo, VideoChat2, GPT-4v, Video-LLaVA), and aggregates outputs using confidence-based or motion-aware mechanisms. The approach is fully inference-based with predefined confidence scores for each model based on prior evaluation, requiring no additional training.

## Key Results
- Achieves 58.79% accuracy on MoVid-Bench (vs LLaMo's 55.32%)
- Scores 0.473 BABEL-QA score (vs LLaMo's 0.458)
- Achieves 53.2 average score on MVBench
- Reaches 0.410 OBO score on Mo-Repcount (vs LLaMo's 0.389)

## Why This Works (Mechanism)
The system's effectiveness stems from its ability to decompose complex motion analysis tasks into manageable sub-tasks and leverage the strengths of multiple specialized models. By using a confidence-based aggregation mechanism and motion-aware refinement, ChatMotion can handle diverse query types (action recognition, reasoning, video analysis) more effectively than single-model approaches. The planner-Executor-verifier architecture ensures task decomposition is accurate and outputs are validated.

## Foundational Learning
- Multi-agent coordination: Needed to handle complex, multi-faceted queries; Quick check: Can the system decompose a compound query into appropriate sub-tasks
- Motion-specific model integration: Required for specialized motion understanding; Quick check: Do individual models perform well on their specialized tasks
- Confidence-based aggregation: Essential for combining model outputs effectively; Quick check: Does the aggregation mechanism improve over simple averaging
- Task decomposition: Critical for managing complex analysis; Quick check: Are decomposed meta-tasks appropriate for the original query
- Motion-aware reasoning: Needed for temporal and spatial motion understanding; Quick check: Can the system handle sequential motion analysis
- Cross-modal fusion: Required for combining video and motion capture data; Quick check: Does the system perform well on mixed-modality queries

## Architecture Onboarding
- Component map: User Query → Planner → Executor → MotionCore (MotionAnalyzer + Aggregator + Generator) → Verifier → Final Output
- Critical path: Planner decomposition → Executor tool selection → MotionAnalyzer processing → Aggregator synthesis → Verifier validation
- Design tradeoffs: Uses multiple specialized models vs. single unified model; adds orchestration complexity for improved accuracy
- Failure signatures: Agent coordination loops, aggregation incoherence when model outputs diverge significantly
- First experiments: 1) Test Planner decomposition on compound queries, 2) Validate Executor tool selection accuracy, 3) Evaluate Aggregator performance on model disagreement

## Open Questions the Paper Calls Out
None

## Limitations
- Missing implementation details for predefined confidence scores and specific prompt templates
- No ablation studies showing individual contribution of MotionCore components
- System relies on external model APIs (GPT-4v) which may limit reproducibility

## Confidence
- High confidence in benchmark results and comparative performance metrics
- Medium confidence in generalizability of the multi-agent approach
- Medium confidence in stated improvements over baselines

## Next Checks
1. Request clarification on the predefined confidence scores (ci) for each model across different modalities (motion, video, motion-video) to enable exact reproduction of the aggregation mechanism
2. Obtain or reconstruct the specific prompt templates used by the Planner, Executor, and Verifier agents to validate the decomposition and coordination logic
3. Conduct ablation experiments by systematically removing components from the MotionCore toolkit to quantify the contribution of each model to the final performance