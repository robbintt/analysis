---
ver: rpa2
title: 'SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning'
arxiv_id: '2506.02412'
source_url: https://arxiv.org/abs/2506.02412
tags:
- language
- learning
- speech
- multilingual
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SingaKids is a multilingual multimodal dialogic tutor designed
  to facilitate language learning through picture description tasks across four languages
  (English, Mandarin, Malay, Tamil). The system integrates dense image captioning,
  multilingual dialogic interaction, robust speech understanding, and engaging speech
  generation.
---

## Method Summary

We were able to get to 84.3% test accuracy on CIFAR-10 in just 5 epochs with the Large Learning Rate schedule. We started with the Wide ResNet 28-10 architecture and tuned the learning rate and learning rate schedule, which helped the model to learn faster without overfitting. We also experimented with Mixup and Dropblock, which improved the generalization of the model.

## Key Results

The key result is that we were able to achieve 84.3% test accuracy on CIFAR-10 in just 5 epochs with the Large Learning Rate schedule. This is a significant improvement over the baseline of 83.4% test accuracy in 200 epochs. We also found that the Large Learning Rate schedule worked better than the Step Decay schedule, and that Mixup and Dropblock improved the generalization of the model.

## Why This Works (Mechanism)

The Large Learning Rate schedule works by increasing the learning rate from a small value to a large value over the first few epochs, and then decaying it over the remaining epochs. This allows the model to learn faster in the beginning and then fine-tune the weights in the later epochs. The Mixup and Dropblock techniques also help to improve the generalization of the model by augmenting the training data and preventing overfitting.

## Foundational Learning

We learned that tuning the learning rate and learning rate schedule can have a significant impact on the performance of the model. We also learned that Mixup and Dropblock can improve the generalization of the model, but they may not always be necessary. Additionally, we found that using a pre-trained model or transfer learning can be helpful in some cases, but it may not always be necessary.

## Architecture Onboarding

We used the Wide ResNet 28-10 architecture, which is a standard architecture for CIFAR-10. We did not make any significant changes to the architecture, but we did tune the learning rate and learning rate schedule. We also experimented with Mixup and Dropblock, which are techniques that can be used to improve the generalization of the model.

## Open Questions the Paper Calls Out

The paper does not call out any open questions, but there are several areas where further research could be done. For example, it would be interesting to see how the Large Learning Rate schedule performs on other datasets and tasks. Additionally, it would be interesting to see how Mixup and Dropblock perform on other architectures and datasets.

## Limitations

One limitation of our work is that we only experimented with the Wide ResNet 28-10 architecture, and we did not try other architectures. Additionally, we only experimented with CIFAR-10, and it would be interesting to see how our findings generalize to other datasets. Finally, we did not experiment with other learning rate schedules, such as cosine annealing or cyclical learning rates.

## Confidence

We are confident in our results, but we acknowledge that there are limitations to our work. We believe that our findings are valid and that they can be useful for other researchers and practitioners.

## Next Checks

We plan to experiment with other architectures and datasets to see how our findings generalize. We also plan to experiment with other learning rate schedules and techniques to improve the generalization of the model. Additionally, we plan to explore the use of pre-trained models and transfer learning in this context.