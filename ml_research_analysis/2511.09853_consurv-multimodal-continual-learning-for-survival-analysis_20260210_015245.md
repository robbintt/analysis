---
ver: rpa2
title: 'ConSurv: Multimodal Continual Learning for Survival Analysis'
arxiv_id: '2511.09853'
source_url: https://arxiv.org/abs/2511.09853
tags:
- learning
- survival
- datasets
- data
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in multimodal continual
  learning for cancer survival prediction, where models struggle to retain knowledge
  from prior cancer datasets while learning new ones. The authors propose ConSurv,
  the first method to integrate WSIs and genomic data across multiple cancer types.
---

# ConSurv: Multimodal Continual Learning for Survival Analysis

## Quick Facts
- arXiv ID: 2511.09853
- Source URL: https://arxiv.org/abs/2511.09853
- Reference count: 40
- Key outcome: First method to integrate WSIs and genomic data across multiple cancer types, achieving C-index 0.601 and C-index IPCW 0.597 while mitigating catastrophic forgetting

## Executive Summary
This paper addresses catastrophic forgetting in multimodal continual learning for cancer survival prediction, where models struggle to retain knowledge from prior cancer datasets while learning new ones. The authors propose ConSurv, which employs Multi-staged Mixture of Experts (MS-MoE) to capture shared and task-specific knowledge at different network stages, and Feature Constrained Replay (FCR) to preserve learned features through distillation. A new benchmark, MSAIL, integrating four TCGA cancer datasets, is introduced for evaluation. Experiments show ConSurv outperforms state-of-the-art methods, demonstrating effective mitigation of catastrophic forgetting while balancing stability-plasticity trade-offs.

## Method Summary
ConSurv tackles multimodal continual learning for cancer survival prediction by integrating whole-slide images (WSIs) and genomic data across multiple cancer types. The method uses a MoME backbone with MS-MoE modules inserted at three stages: end of WSI encoder, end of genomic encoder, and end of fusion component. Each MS-MoE employs 8 experts with sparse-shared routing (1 shared + Top-2 task-specific). FCR stores features from previous tasks and constrains current model features through distillation. The model is trained sequentially on BLCA→UCEC→LUAD→BRCA datasets with 20 epochs each, using a loss combining survival loss, feature constraint loss, and replay loss.

## Key Results
- Average C-index of 0.601 and C-index IPCW of 0.597 on MSAIL benchmark
- Superior backward transfer (BWT) and forward transfer (FWT) compared to baselines
- Effective mitigation of catastrophic forgetting across all four cancer types
- MS-MoE routing shows clear differentiation in expert activation between cancer types

## Why This Works (Mechanism)

### Mechanism 1: Sparse-Separated Inter-Modal Routing
The Multi-staged Mixture of Experts (MS-MoE) uses a "Sparse-shared" strategy where a new linear router is added for each new dataset. For an input, the router selects a single "shared" expert (for cross-task knowledge) and top-k task-specific experts. This prevents gradient updates from a new task from modifying the weights of experts dedicated to previous tasks, thereby isolating the "complex inter-modal interactions."

### Mechanism 2: Hierarchical Feature Distillation
Feature Constrained Replay (FCR) stores features from the WSI patch encoder, genomic encoder, and fusion component in a buffer. During training on a new task, it minimizes the L2 distance between the current model's features and the buffered features. This forces the encoders to maintain the "view" of the data they learned on previous tasks, preserving the inter-modal correlations.

### Mechanism 3: Task-Identity Conditioned Inference
The model uses task-specific routers and prediction heads. At inference, the system must know which dataset the sample belongs to to activate the correct router and head. Without this, the wrong experts would be mixed, and the wrong survival bins would be predicted.

## Foundational Learning

- **Mixture of Experts (MoE) & Sparse Gating**: ConSurv relies on routing inputs to specific sub-networks (experts). You must understand how a soft-gating mechanism selects experts and how "sparsity" (only activating a subset) affects computational cost. Quick check: In a Sparse MoE layer with 8 experts and Top-2 gating, how many experts process a single input instance?

- **Catastrophic Forgetting & Stability-Plasticity**: This is the core problem being solved. You need to distinguish between "stability" (retaining old knowledge) and "plasticity" (learning new knowledge) to understand the trade-offs in the loss function. Quick check: Why does simply fine-tuning a model on a new dataset cause a performance drop on the previous dataset?

- **Discrete-Time Survival Analysis (NLL Loss)**: The output isn't a simple class label but a hazard function over time bins. Understanding the Negative Log-Likelihood loss for censored data is required to interpret the training dynamics. Quick check: In the loss function, how is the term for a censored patient different from an uncensored patient?

## Architecture Onboarding

- **Component map**: Input (WSI Patches + Genomic Vector) -> Backbone (MoME) -> MS-MoE (Router selects experts -> Weighted sum) -> Encoding (Pass through ResNet/SNN backbone, then through MS-MoE) -> Fusion (Concatenate/Attend modalities, pass through MS-MoE in fusion component) -> Loss Calculation (Survival Loss on current data + Replay Loss on buffer data + Feature Constraint Loss comparing current vs. buffer features)

- **Critical path**: 1. Input: WSI Patches + Genomic Vector. 2. Encoding: Pass through ResNet/SNN backbone, then through MS-MoE (Router selects experts -> Weighted sum). 3. Fusion: Concatenate/Attend modalities, pass through MS-MoE in fusion component. 4. Loss Calculation: Survival Loss on current data + Replay Loss on buffer data + Feature Constraint Loss comparing current vs. buffer features.

- **Design tradeoffs**: Replace vs. Append (MS-MoE can replace existing FFNs or be appended residually. Replace saves memory; Append preserves original backbone capacity more strictly). Buffer Size vs. Privacy (FCR stores features not images to save space, but storing patient features may still raise privacy concerns). Shared Expert (Always activating one shared expert ensures positive transfer but may force incompatible features to mix if the domains are too distinct).

- **Failure signatures**: Router Collapse (Validation shows the same expert is selected for all tasks - likely the shared expert). Forgetting Spike (Sudden drop in validation C-index on previous tasks - check if FCR loss weight is too low). Feature Drift (High FC loss that doesn't decrease - the new task might be forcing a representation shift that is incompatible with the buffer).

- **First 3 experiments**: 1. Sequential Finetuning Baseline (Run MoME backbone on MSAIL benchmark without MS-MoE or FCR to quantify the exact "forgetting" magnitude). 2. Ablation on FCR Levels (Test FCR using only the final fusion features vs. the full proposed setup to verify contribution of encoder-level distillation). 3. Expert Selection Visualization (Train ConSurv and generate the heatmap to ensure different experts are actually being selected for different cancer types).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can replay-free multimodal continual learning methods be developed to replace Feature Constrained Replay (FCR) while maintaining performance and mitigating privacy risks? The paper identifies this as future work to address privacy concerns associated with storing representations.

- **Open Question 2**: How can the ConSurv framework be effectively integrated with federated learning to enable distributed training across clinical sites without sharing raw data? The paper identifies this as a promising direction for practical deployment where patient privacy is critical.

- **Open Question 3**: Can active learning strategies be incorporated to improve data efficiency and reduce the reliance on expensive manual annotations in the multimodal continual learning setting? The paper suggests exploring active learning to efficiently leverage unlabeled data to address high annotation costs.

## Limitations
- Reliance on task identities at inference limits real-world deployment where dataset provenance may be unknown
- FCR approach requires maintaining a buffer of previous task features, raising potential privacy concerns for sensitive medical data
- Fixed number of experts (8) may be insufficient to capture the diversity across cancer types, potentially leading to negative transfer

## Confidence
- **High Confidence**: C-index improvement metrics and benchmark results, MS-MoE routing mechanism, MSAIL benchmark construction
- **Medium Confidence**: Catastrophic forgetting mitigation effectiveness, backward/forward transfer claims
- **Low Confidence**: Generalization to unseen cancer types beyond the four TCGA datasets, scalability to datasets with significantly different feature distributions

## Next Checks
1. Test ConSurv's performance when task identities are withheld at inference to assess practical deployment limitations
2. Evaluate model performance with varying numbers of experts (e.g., 4 vs 8 vs 16) to determine optimal capacity for this domain
3. Conduct ablation studies removing the shared expert to verify its contribution to positive transfer between cancer types