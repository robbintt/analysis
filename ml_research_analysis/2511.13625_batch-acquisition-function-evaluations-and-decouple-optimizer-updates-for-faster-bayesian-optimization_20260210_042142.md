---
ver: rpa2
title: Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster
  Bayesian Optimization
arxiv_id: '2511.13625'
source_url: https://arxiv.org/abs/2511.13625
tags:
- c-be
- optimization
- function
- d-be
- acquisition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency in multi-start optimization
  (MSO) for Bayesian optimization when using coupled updates with batched evaluations
  (C-BE). The core issue is that C-BE introduces off-diagonal artifacts in the inverse
  Hessian approximation, slowing convergence.
---

# Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization

## Quick Facts
- arXiv ID: 2511.13625
- Source URL: https://arxiv.org/abs/2511.13625
- Authors: Kaichi Irie; Shuhei Watanabe; Masaki Onishi
- Reference count: 32
- Primary result: D-BE achieves 1.1-1.5× wall-clock speedup over SEQ. OPT. while maintaining solution quality

## Executive Summary
This paper addresses a fundamental inefficiency in Bayesian optimization when using multi-start optimization with batched acquisition function evaluations. The core problem is that coupled updates in quasi-Newton methods introduce off-diagonal artifacts in the inverse Hessian approximation when batch evaluations are used, which slows convergence. The proposed solution, Decoupling QN updates per restart while Batching acquisition function Evaluations (D-BE), uses a coroutine-based approach to independently update each restart's quasi-Newton state while still batching acquisition function evaluations. This preserves the computational benefits of batching while avoiding the convergence degradation caused by coupled updates.

## Method Summary
The D-BE approach decouples quasi-Newton (QN) optimizer updates for each restart in multi-start optimization while maintaining batched acquisition function evaluations. The method uses a coroutine to coordinate independent QN state updates for each restart, where each restart only uses its own evaluated points for updates, avoiding off-diagonal artifacts in the inverse Hessian approximation. This requires no new solvers and can be implemented as a wrapper around existing optimizers. The key insight is that while acquisition function evaluations can be batched for efficiency, the optimizer state updates should remain independent per restart to maintain convergence properties.

## Key Results
- D-BE matches SEQ. OPT. in solution quality and iteration counts
- Achieves 1.1-1.5× wall-clock speedup compared to SEQ. OPT. across benchmarks
- Up to 1.76× faster than SEQ. OPT. and 1.29× faster than C-BE in specific cases
- Maintains consistency across different acquisition functions and problem dimensions

## Why This Works (Mechanism)
The mechanism works by separating two aspects of optimization: acquisition function evaluation (which benefits from batching) and optimizer state updates (which suffer from coupling artifacts). By decoupling the quasi-Newton updates per restart, each restart only uses its own evaluated points, avoiding the off-diagonal terms that cause convergence issues in coupled approaches. The coroutine coordination ensures efficient batching of acquisition evaluations while maintaining independent optimizer state management.

## Foundational Learning
- **Multi-start optimization (MSO)**: Needed for global optimization in non-convex spaces; quick check: understand how multiple restarts help escape local minima
- **Quasi-Newton methods**: Approximate second-order information for faster convergence; quick check: know the difference between BFGS and L-BFGS
- **Coupled vs decoupled updates**: Understanding how parameter updates interact in batch settings; quick check: can you explain why off-diagonal terms hurt convergence?
- **Coroutine-based coordination**: Programming pattern for managing concurrent operations; quick check: understand how coroutines differ from threads for this use case
- **Acquisition function batching**: Evaluating multiple candidate points simultaneously; quick check: know why batching acquisition functions is computationally beneficial
- **Inverse Hessian approximation artifacts**: How coupling introduces errors in second-order approximations; quick check: can you derive the off-diagonal term impact?

## Architecture Onboarding

**Component map:**
Optimizer restarts (coroutines) -> Acquisition function batcher -> Individual QN state managers -> Solution aggregator

**Critical path:**
1. Generate multiple restart points
2. Coroutine coordination for batched acquisition evaluation
3. Independent QN updates per restart using only local evaluations
4. Solution quality comparison across restarts

**Design tradeoffs:**
- Batching acquisition functions saves computational cost but risks coupling artifacts
- Independent QN updates preserve convergence but require coordination overhead
- Coroutine approach balances efficiency with implementation complexity
- No new solver development needed but requires careful state management

**Failure signatures:**
- Convergence to suboptimal solutions (indicates coupling artifacts)
- Increased iteration counts (suggests inefficient coordination)
- Memory overhead from maintaining multiple QN states
- Synchronization delays in coroutine management

**First 3 experiments to run:**
1. Simple 2D test function with known global optimum to verify solution quality
2. Timing comparison on synthetic benchmark with varying restart counts
3. Wall-clock performance on acquisition function with expensive evaluations

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes quasi-Newton methods can be safely decoupled without instability
- Limited validation to synthetic benchmarks and single real-world case
- Modest speedups (1.1-1.5×) may not justify implementation complexity
- Doesn't explore high-dimensional or highly non-convex acquisition functions
- No analysis of memory overhead from maintaining multiple QN states

## Confidence
- Technical correctness of D-BE formulation: **High** - Rigorous mathematical framework
- Empirical speed improvements: **Medium** - Consistent but modest gains across limited problem sets
- General applicability to BO workflows: **Low** - Based on limited problem types and dimensions

## Next Checks
1. Test D-BE on high-dimensional acquisition functions (d > 20) to assess scalability and stability of quasi-Newton decoupling
2. Evaluate performance on acquisition functions with multiple local maxima and narrow basins to stress-test the restart mechanism
3. Compare wall-clock performance when using different acquisition functions (EI, PI, UCB) to verify approach isn't specific to one acquisition type