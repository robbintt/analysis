---
ver: rpa2
title: 'IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by
  Efficient Human Preference Alignment'
arxiv_id: '2501.02869'
source_url: https://arxiv.org/abs/2501.02869
tags:
- medical
- arxiv
- dataset
- language
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces IIMedGPT, a Chinese medical large language
  model trained using supervised fine-tuning (SFT) and direct preference optimization
  (DPO). The authors construct a medical instruction dataset (CMedINS) from real medical
  records and employ DPO for efficient human preference alignment.
---

# IIMedGPT: Promoting Large Language Model Capabilities of Medical Tasks by Efficient Human Preference Alignment

## Quick Facts
- arXiv ID: 2501.02869
- Source URL: https://arxiv.org/abs/2501.02869
- Authors: Yiming Zhang; Zheng Chang; Wentao Cai; MengXing Ren; Kang Yuan; Yining Sun; Zenghui Ding
- Reference count: 40
- The paper introduces IIMedGPT, a Chinese medical large language model trained using supervised fine-tuning (SFT) and direct preference optimization (DPO).

## Executive Summary
This paper presents IIMedGPT, a Chinese medical large language model designed for professional medical dialogue and instruction following. The authors construct a medical instruction dataset (CMedINS) from real medical records and employ DPO for efficient human preference alignment. IIMedGPT outperforms existing open-source medical models on medical dialogue tasks while maintaining general language capabilities, achieving state-of-the-art results with significantly less training data compared to previous models.

## Method Summary
The method involves two-stage training: first, supervised fine-tuning (SFT) on a mixed dataset including CMedINS (220K medical instructions), multi-turn and single-turn medical dialogues, and general instructions to prevent catastrophic forgetting; second, direct preference optimization (DPO) using 15,000 annotated preference pairs to align with human preferences on Safety, Professionalism, and Fluency. The model uses Qwen-14B-base as the foundation and is trained with LoRA on 4×A100-80G GPUs.

## Key Results
- IIMedGPT outperforms existing open-source medical models (Zhongjing, HuatuoGPT) on Safety, Professionalism, and Fluency metrics.
- The model achieves state-of-the-art results with significantly less training data compared to previous models.
- DPO effectively improves all three capabilities (Safety, Professionalism, Fluency) in both single-turn and multi-turn dialogue tasks.

## Why This Works (Mechanism)

### Mechanism 1: Direct Preference Optimization Bypasses Reward Model Instability
- DPO achieves human preference alignment more efficiently than PPO-based RLHF by eliminating the reward model training stage.
- DPO transforms the reinforcement learning objective into a classification problem, directly optimizing the policy by maximizing the log-likelihood margin between preferred and dispreferred responses.
- Core assumption: The preference dataset accurately captures the relative quality ranking of responses; the reference model provides a stable baseline for divergence control.

### Mechanism 2: Multi-Task Instruction Dataset Enhances Zero-Shot Generalization
- Diversifying instruction types beyond single-turn dialogue improves the model's ability to handle unseen medical tasks.
- The CMedINS dataset constructs six distinct medical instruction types from real hospital records, creating broader pattern recognition capabilities that transfer to novel queries.
- Core assumption: Tasks derived from real medical scenarios are representative of the distribution of queries the model will encounter; de-identification preserves semantic content.

### Mechanism 3: Hierarchical Preference Annotation Captures Medical Priorities
- Structured annotation with priority-ordered dimensions (Safety > Professionalism > Fluency) enables domain-appropriate alignment.
- Annotators evaluate responses along three dimensions with explicit priority ordering, providing both in-domain refinement and OOD robustness signals.
- Core assumption: Medical professionals' judgments on these dimensions correlate with actual clinical utility and safety; cross-annotation with expert arbitration reduces individual bias.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) on Instruction Data**
  - Why needed here: SFT transforms a base LLM from a general text completer into an instruction-following medical assistant.
  - Quick check question: Given a pre-trained model that predicts next tokens, how would you format medical Q&A pairs to train it to follow instructions rather than continue text?

- **Preference Alignment vs. Instruction Following**
  - Why needed here: SFT teaches the model what to say; preference alignment teaches it which of multiple valid responses is better according to human judgment.
  - Quick check question: If two responses to a medical query are both factually correct, what criteria would determine which is "preferred"?

- **Catastrophic Forgetting in Domain Adaptation**
  - Why needed here: Fine-tuning on medical data risks degrading general capabilities.
  - Quick check question: Why might a model fine-tuned exclusively on medical dialogues struggle to answer non-medical questions it previously handled?

## Architecture Onboarding

- **Component map:** Qwen-14B-base (pre-trained) → SFT on mixed data → DPO on preference data → IIMedGPT (final model)

- **Critical path:**
  1. Data pipeline: Raw medical records → de-identification → instruction formatting → CMedINS
  2. Annotation pipeline: Sample model outputs → medical annotators → SPF scoring → preference pairs
  3. Training: SFT (LoRA, bf16, 4×A100-80G) → DPO (preference optimization)

- **Design tradeoffs:**
  - DPO vs. RLHF: DPO is computationally simpler but may be less flexible for complex reward shaping.
  - Dataset size vs. quality: 220K instruction pairs is smaller than competing models but uses real clinical data.
  - General data mixing: Adding non-medical data preserves general capabilities but dilutes medical specialization.

- **Failure signatures:**
  - Hallucination in medical advice
  - Over-confident generalization causing memorization without reasoning
  - OOD query degradation if preference data over-represents certain query types

- **First 3 experiments:**
  1. Reproduce the preference annotation process with 100 medical dialogues
  2. Ablate the general instruction data to quantify catastrophic forgetting
  3. Compare DPO vs. no alignment using the paper's evaluation protocol

## Open Questions the Paper Calls Out

- **Open Question 1:** How can IIMedGPT be extended to effectively process and integrate medical multimodal information, such as medical images or physiological signals, alongside textual data?
  - Basis: IIMedGPT currently processes only textual information and cannot process medical multimodal information.

- **Open Question 2:** What specific mechanisms can be implemented to effectively mitigate the occurrence of hallucinations in IIMedGPT's responses to ensure higher reliability in medical contexts?
  - Basis: IIMedGPT does not guarantee the accuracy of all responses due to the occurrence of hallucinations.

- **Open Question 3:** How does IIMedGPT perform when evaluated against a unified, standardized medical benchmark that extends beyond the specific datasets used in this study?
  - Basis: Lack of a unified medical benchmark evaluation standard at present.

## Limitations
- Dataset privacy barrier: The core CMedINS dataset remains private, making full methodological reproduction impossible.
- Generalization scope: Model evaluation focuses on Chinese medical dialogue; performance on English medical tasks is unknown.
- Hallucination persistence: Despite DPO safety improvements, hallucination risk in medical advice remains unmitigated.

## Confidence
- **High confidence:** DPO's computational efficiency advantage over PPO-based RLHF
- **Medium confidence:** Multi-task instruction dataset's contribution to zero-shot generalization
- **Low confidence:** SPF annotation framework's domain appropriateness

## Next Checks
1. Evaluate IIMedGPT on non-Chinese medical datasets to assess cross-lingual medical reasoning capabilities.
2. Systematically vary the proportion of general instruction data in SFT training and measure performance decay on both medical and general benchmarks.
3. Compare model behavior before/after DPO using adversarial medical prompts to quantify safety improvements versus potential over-conservatism.