---
ver: rpa2
title: 'The American Ghost in the Machine: How language models align culturally and
  the effects of cultural prompting'
arxiv_id: '2512.12488'
source_url: https://arxiv.org/abs/2512.12488
tags:
- cultural
- importance
- please
- gpt-4
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the cultural alignment of popular large
  language models (LLMs) and tests whether cultural prompting can effectively shift
  their alignment to match different countries. Using Hofstede's cultural dimensions
  and the VSM13 International Survey, the authors evaluated eight models (DeepSeek-V3/V3.1,
  GPT-4/4.1/5, Claude Opus 4, Llama 3.1, Mistral Large) across six countries (China,
  France, India, Iran, Japan, United States).
---

# The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting

## Quick Facts
- arXiv ID: 2512.12488
- Source URL: https://arxiv.org/abs/2512.12488
- Authors: James Luther; Donald Brown
- Reference count: 40
- Key outcome: Cultural prompting successfully shifts LLM alignment to match different countries, with DeepSeek-V3.1 showing 30.87% improvement in cultural dimension distance

## Executive Summary
This study investigates how popular large language models align with different cultural values and whether cultural prompting can effectively shift their alignment to match various countries. Using Hofstede's cultural dimensions framework and the VSM13 International Survey, the researchers evaluated eight models across six countries (China, France, India, Iran, Japan, United States). The study reveals that most models show strong alignment with US cultural values by default, while two DeepSeek models surprisingly align furthest from Chinese culture despite originating there. Cultural prompting proved highly effective, improving alignment for seven of eight models tested, with GPT-4 achieving the lowest overall distance after prompting.

## Method Summary
The study evaluated eight large language models (DeepSeek-V3/V3.1, GPT-4/4.1/5, Claude Opus 4, Llama 3.1, Mistral Large) using Hofstede's cultural dimensions and the VSM13 International Survey framework across six countries. Models were assessed without cultural prompting first, then with cultural prompting techniques applied to measure alignment shifts. The VSM13 survey, a widely-used cultural assessment tool, was used to evaluate how well each model's responses aligned with cultural values in each country. The researchers calculated dimension distances to quantify alignment differences and measured the effectiveness of cultural prompting by comparing pre- and post-prompting alignment scores.

## Key Results
- Most LLMs showed strong default alignment with United States cultural values across six tested countries
- DeepSeek-V3.1 achieved the largest improvement from cultural prompting (30.87% decrease in dimension distance)
- GPT-4 achieved the lowest overall cultural dimension distance after prompting was applied
- Two DeepSeek models showed the furthest alignment from Chinese culture despite originating in China

## Why This Works (Mechanism)
The effectiveness of cultural prompting appears to work by providing context and explicit instructions that guide language models toward generating responses that reflect specific cultural values and norms. When models receive cultural prompts, they can leverage their training data to produce outputs that better match the target cultural framework, suggesting that the underlying cultural knowledge exists within the models but requires activation through appropriate prompting techniques.

## Foundational Learning
- Hofstede's cultural dimensions: Framework measuring cultural values across six dimensions (why needed: provides standardized metric for cultural alignment; quick check: verify all six dimensions are properly operationalized)
- VSM13 International Survey: Established tool for assessing cultural values (why needed: validated measurement instrument; quick check: confirm survey reliability across target countries)
- Cultural prompting techniques: Methods for guiding model outputs toward specific cultural contexts (why needed: enables adaptive cultural alignment; quick check: document specific prompt structures used)
- Dimension distance calculation: Metric for quantifying cultural alignment differences (why needed: provides objective measurement of alignment; quick check: verify calculation methodology is consistent)

## Architecture Onboarding
- Component map: Language model -> Cultural prompting layer -> Cultural alignment measurement -> Dimension distance calculation
- Critical path: Model input → Prompt application → Response generation → Cultural value extraction → Alignment scoring
- Design tradeoffs: Using prompting instead of fine-tuning enables scalability but may be less stable; measuring alignment through text generation rather than direct cultural understanding introduces uncertainty
- Failure signatures: Models showing resistance to cultural prompting, inconsistent alignment across dimensions, or unexpected alignment patterns (e.g., DeepSeek models distant from Chinese culture)
- 3 first experiments: 1) Test prompting effectiveness across additional cultural dimensions not in original study, 2) Compare prompting vs. fine-tuning approaches for cultural alignment, 3) Evaluate cultural alignment stability across multiple prompting sessions

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on Hofstede's cultural dimensions and VSM13 survey may not capture full complexity of cultural values and differences
- Relatively small sample size of six countries limits generalizability to other cultural contexts
- Assessment depends on how well models can simulate cultural values through text generation rather than direct measurement of cultural understanding

## Confidence
- High: Most models align with US culture by default, given systematic evaluation across multiple cultural dimensions
- Medium: Cultural prompting effectiveness demonstrated but mechanism unclear
- Medium: DeepSeek models' unexpected alignment patterns may reflect specific training effects requiring further investigation

## Next Checks
1. Test cultural prompting approach with additional countries beyond the six studied to assess generalizability
2. Conduct ablation studies to identify which specific prompting techniques contribute most to cultural alignment improvements
3. Evaluate whether culturally aligned responses actually improve downstream task performance for users from different cultural backgrounds