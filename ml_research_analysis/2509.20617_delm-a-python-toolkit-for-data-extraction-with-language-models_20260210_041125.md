---
ver: rpa2
title: 'DELM: a Python toolkit for Data Extraction with Language Models'
arxiv_id: '2509.20617'
source_url: https://arxiv.org/abs/2509.20617
tags:
- delm
- extraction
- prompt
- data
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DELM addresses the engineering challenges of building reliable,
  reproducible LLM-based data extraction pipelines at scale. The toolkit introduces
  a configuration-driven framework that decouples scientific inquiry from engineering
  overhead through structured workflows, deterministic caching, batch execution, and
  comprehensive provenance tracking.
---

# DELM: a Python toolkit for Data Extraction with Language Models

## Quick Facts
- arXiv ID: 2509.20617
- Source URL: https://arxiv.org/abs/2509.20617
- Authors: Eric Fithian; Kirill Skobelev
- Reference count: 4
- Key outcome: DELM enables systematic LLM-based data extraction with reproducible experiments, cost tracking, and prompt optimization through configuration-driven workflows

## Executive Summary
DELM addresses the engineering challenges of building reliable, reproducible LLM-based data extraction pipelines at scale. The toolkit introduces a configuration-driven framework that decouples scientific inquiry from engineering overhead through structured workflows, deterministic caching, batch execution, and comprehensive provenance tracking. Two case studies demonstrate its utility: first, a cost-recall tradeoff analysis using keyword filtering on commodity extraction from investor call transcripts, revealing how recall degrades as costs are constrained; second, an LLM-in-the-loop prompt optimization experiment showing iterative improvement in extraction precision through automated prompt refinement.

## Method Summary
DELM implements a configuration-driven framework for LLM-based data extraction using structured schemas, deterministic caching, and batch execution. The system uses Pydantic models for schema validation, semantic caching to eliminate redundant API calls, and Instructor library integration for constrained output generation. Two case studies validate the approach: (1) commodity extraction from investor call transcripts with keyword-based cost-recall tradeoff analysis using greedy forward selection, and (2) price_expectation classification using LILPRO algorithm for iterative prompt optimization through error pattern analysis and meta-prompt refinement.

## Key Results
- Demonstrated systematic cost-recall tradeoffs in commodity extraction, showing recall degradation as API costs are constrained through keyword filtering
- Achieved precision improvement from ~0.65 to ~0.85 over 6 LILPRO optimization batches for price_expectation extraction
- Validated reproducibility and auditability through configuration-driven workflows with deterministic caching and comprehensive provenance tracking

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Semantic Caching
- Claim: DELM reduces redundant API calls and associated costs by caching LLM responses based on a complete configuration fingerprint
- Mechanism: The semantic cache creates keys from the fully materialized prompt, system prompt, model identifier, and generation parameters. Before each API call, the executor queries the cache; cache hits return stored results without API interaction.
- Core assumption: LLM outputs are sufficiently deterministic given identical inputs and parameters (temperature=0 or fixed seed).
- Evidence anchors:
  - [abstract]: "result caching" and "detailed cost tracking"
  - [section 4.2]: "A persistent, deterministic semantic cache keys API responses to the fully materialized prompt, system prompt, model identifier, and generation parameters, eliminating redundant computations across experiments"
  - [corpus]: Weak direct evidence; QueryGym mentions reproducibility but not caching specifics
- Break condition: Non-zero temperature introduces stochasticity; cache invalidation required when prompts, schemas, or model versions change; temperature > 0 may produce different outputs for identical cache keys.

### Mechanism 2: Structured Output Validation via Schema Constraints
- Claim: Pydantic-based schema validation enforces consistent output formats, enabling systematic comparison across experiments
- Mechanism: Users define extraction schemas in YAML; Schema Manager constructs Pydantic models dynamically. The Extraction Manager uses Instructor library to constrain LLM outputs to valid JSON matching the schema, then validates all responses before persistence.
- Core assumption: Constrained decoding techniques can reliably force LLMs to generate schema-compliant JSON.
- Evidence anchors:
  - [abstract]: "structured outputs, built-in validation"
  - [section 4.1]: "validates returned objects against the user-defined schema before persisting results"
  - [section 2.1]: Cites Willard & Louf (2023) on constrained generation for valid JSON; Instructor library handles multiple providers
- Break condition: Complex nested schemas may exceed LLM's capacity to generate valid structures; validation failures require retry logic or human review; schema drift between experiments if not version-controlled.

### Mechanism 3: Iterative Prompt Optimization via Error-Driven Refinement
- Claim: LLM-in-the-loop prompt optimization can improve extraction precision by systematically analyzing and addressing error patterns
- Mechanism: The LILPRO algorithm samples mini-batches, identifies misclassified examples, and uses an optimizer LLM with a meta-prompt to extract error patterns and refine the extraction prompt. This creates a feedback loop where prompt improvements target documented failure modes.
- Core assumption: Error patterns are systematic and can be articulated as explicit guidance; optimizer LLM can identify and generalize from failure examples.
- Evidence anchors:
  - [abstract]: "iteratively optimize prompts via LLM-in-the-loop refinement, improving precision across batches"
  - [section 5.2]: Figure 2 shows precision increasing from ~0.65 to ~0.85 over 6 batches for price_expectation extraction
  - [corpus]: Ramnath et al. (2025) survey automatic prompt optimization; DSPy (Khattab et al., 2023) optimizes prompts via compilation
- Break condition: Optimization may overfit to training batches; prompt changes may not generalize to out-of-distribution data; optimizer LLM may introduce biases or fail to identify root causes.

## Foundational Learning

- Concept: **Predictive Task Framing for IE**
  - Why needed here: DELM treats extraction as prediction, enabling use of classification metrics (precision, recall, F1) rather than ad hoc evaluation. This is central to systematic comparison.
  - Quick check question: Can you articulate why extraction accuracy alone might be misleading for imbalanced datasets?

- Concept: **Pydantic Models and Schema Validation**
  - Why needed here: DELM uses Pydantic for schema definition and validation. Understanding type coercion, validators, and JSON schema generation is essential for defining extraction targets.
  - Quick check question: How would you define a Pydantic model for extracting named entities with optional confidence scores?

- Concept: **Cost-Recall Trade-offs in Retrieval Pipelines**
  - Why needed here: Case Study 1 demonstrates how keyword filtering affects both API costs and extraction coverage. Understanding this Pareto frontier is crucial for pipeline optimization.
  - Quick check question: If you increase keyword filter strictness, what happens to cost and recall? What assumptions does this trade-off make about keyword relevance?

## Architecture Onboarding

- Component map:
  core/ -> Data Processor (loading, chunking, filtering), Extraction Manager (batched LLM calls, Instructor integration, caching), Experiment Manager (run directories, checkpointing, state restoration)
  schemas/ -> Schema Manager (YAML → Pydantic, prompt rendering, validation)
  strategies/ -> Pluggable loaders (text, CSV, PDF via marker OCR, HTML), scoring strategies (keyword, fuzzy), splitting strategies
  utils/ -> Retry Handler (exponential backoff), Semantic Cache (SQLite WAL default, LMDB alternative), Cost Tracker (token metering, budget constraints), logging utilities

- Critical path:
  1. Define extraction schema in YAML (fields, types, constraints)
  2. Configure data loader and preprocessing (chunking rules, format handling)
  3. Set up relevance scorer (optional keyword/fuzzy filter)
  4. Configure Extraction Manager with model, temperature, cache settings
  5. Run batched extraction with automatic validation
  6. Evaluate using scoring modules against ground truth
  7. Optionally: Run LILPRO for prompt optimization iterations

- Design tradeoffs:
  - Configuration complexity vs. reproducibility: Declarative YAML configs add upfront overhead but ensure experiments are reproducible and auditable
  - Schema flexibility vs. validation strictness: Complex schemas provide more structure but increase validation failure rates; Pydantic's optional fields offer middle ground
  - Cache granularity vs. storage costs: Fine-grained caching (all parameters in key) prevents false cache hits but increases storage; coarse caching risks incorrect results
  - Batch size vs. latency: Large batches improve throughput but increase memory usage and checkpoint granularity

- Failure signatures:
  - Cache corruption: Identical inputs produce different outputs → check cache key includes all relevant parameters, clear cache
  - Validation loops: High retry rates due to schema violations → simplify schema, add better prompt guidance, check constrained decoding is enabled
  - Cost overruns: Budget exceeded unexpectedly → verify cost tracker is active, check token counting includes both input and output, monitor early in run
  - Configuration drift: Experiments not reproducible → verify all parameters captured in config file, check for implicit dependencies (e.g., system prompts set externally)
  - Optimization plateau: LILPRO precision stalls → check error diversity in batches, verify meta-prompt quality, consider ensemble approaches

- First 3 experiments:
  1. Baseline extraction with cost tracking: Run DELM on a small labeled dataset (50-100 examples) with a simple schema. Measure baseline precision/recall and per-extraction cost. Verify caching works by re-running—costs should drop to near-zero.
  2. Keyword filter sweep: Implement Case Study 1 methodology on your domain. Test 5-10 keyword configurations, plot cost-recall Pareto frontier, identify optimal operating point for your budget constraints.
  3. Single-variable prompt optimization: Run LILPRO on one ambiguous extraction field (like price_expectation in the paper). Track precision across 5-10 batches, document prompt evolution, and validate improvements on held-out test set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dynamic agentic workflows be integrated into DELM's static, configuration-driven architecture without compromising reproducibility?
- Basis in paper: [inferred] The conclusion explicitly lists "agentic AI integrations" as a future development direction, while the design principles emphasize "deterministic caching" and "declarative configuration" to ensure provenance.
- Why unresolved: Agentic systems typically require non-deterministic, multi-step decision-making that conflicts with the toolkit's current reliance on static configuration files and deterministic cache keys.
- What evidence would resolve it: A successful implementation of an agent-based extraction pipeline within DELM that maintains auditability and identical outputs when re-run with the same random seed.

### Open Question 2
- Question: Does the LILPRO prompt optimization algorithm generalize across diverse domains or does it overfit to the specific linguistic patterns of financial transcripts?
- Basis in paper: [inferred] The case study (Section 5.2) validates LILPRO only on the `price_expectation` variable within investor call transcripts, showing precision gains without testing on external datasets.
- Why unresolved: The paper does not compare LILPRO against existing optimization baselines (e.g., DSPy) or validate the transferability of the refined prompts to other text domains.
- What evidence would resolve it: Benchmarks showing LILPRO's performance across distinct datasets (e.g., biomedical vs. financial) or comparative analysis against other automatic prompt optimization techniques.

### Open Question 3
- Question: What is the computational latency overhead imposed by DELM's strict validation and persistent caching mechanisms compared to standard LLM scripting?
- Basis in paper: [inferred] The architecture section (4.3) details robustness features like SQLite WAL semantic caching and schema validation, but the paper provides no throughput or latency benchmarks.
- Why unresolved: While robustness improves reliability, the "boilerplate" reduction claims do not quantify if the overhead of validation and database operations creates a bottleneck relative to ad hoc scripts.
- What evidence would resolve it: Systematic runtime measurements comparing a DELM pipeline against a baseline Python script for identical high-volume batch extraction tasks.

## Limitations
- Empirical evaluation limited to two case studies on single dataset (Fukui et al., 2024 investor call transcripts) without cross-validation or baseline comparisons
- Cost-recall tradeoff analysis assumes commodity keyword relevance correlates with extraction value without validating actual transcript presence
- LILPRO optimization effectiveness depends heavily on quality of error pattern identification and meta-prompt formulation, neither extensively analyzed

## Confidence
- High: Configuration-driven framework architecture, deterministic caching mechanism, Pydantic-based schema validation
- Medium: Cost tracking accuracy, batch execution efficiency, semantic cache effectiveness
- Low: LILPRO optimization generalization, real-world cost-recall Pareto optimality, extraction accuracy on diverse domains

## Next Checks
1. Schema Robustness Test: Run DELM on datasets with progressively complex nested schemas (3+ levels deep) to identify validation failure thresholds and document required prompt engineering adjustments
2. Cache Consistency Audit: Execute identical extraction tasks with varying temperature settings (0.0, 0.3, 0.7) to measure cache hit rates and output consistency, documenting when cache invalidation becomes necessary
3. Cross-Dataset Generalization: Apply the keyword filter methodology to a different text domain (e.g., medical literature or legal contracts) and compare whether cost-recall tradeoffs follow similar patterns or require domain-specific recalibration