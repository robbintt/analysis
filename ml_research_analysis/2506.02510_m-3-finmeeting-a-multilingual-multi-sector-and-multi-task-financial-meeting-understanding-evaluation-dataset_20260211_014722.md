---
ver: rpa2
title: 'M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting
  Understanding Evaluation Dataset'
arxiv_id: '2506.02510'
source_url: https://arxiv.org/abs/2506.02510
tags:
- question
- financial
- summarization
- extraction
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces M3FinMeeting, a multilingual, multi-sector,
  and multi-task dataset designed for evaluating large language models on financial
  meeting understanding. The dataset covers English, Chinese, and Japanese, spans
  all 11 GICS industry sectors, and includes three tasks: summarization, question-answer
  pair extraction, and question answering.'
---

# M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset

## Quick Facts
- arXiv ID: 2506.02510
- Source URL: https://arxiv.org/abs/2506.02510
- Reference count: 29
- Best model achieved only ~70% overall score on GPT-4-based evaluation

## Executive Summary
This paper introduces M3FinMeeting, a multilingual, multi-sector, and multi-task dataset designed for evaluating large language models on financial meeting understanding. The dataset covers English, Chinese, and Japanese, spans all 11 GICS industry sectors, and includes three tasks: summarization, question-answer pair extraction, and question answering. Experiments with seven LLMs reveal significant room for improvement, with the best model achieving only around 70% overall score on GPT-4-based evaluation. The dataset addresses the gap in existing benchmarks that rely on news articles and earnings reports rather than real financial meeting content.

## Method Summary
The M3FinMeeting dataset contains 600 real-world financial meetings (100 English, 400 Chinese, 100 Japanese) transcribed from audio recordings using Whisper with manual correction. Three tasks are defined: summarization (extracting key points from each thematic section), QA pair extraction (identifying questions and answers throughout long transcripts), and question answering (answering questions given full meeting context). Evaluation uses two metrics: alignment scores (precision, recall, F1 based on cosine similarity between generated and gold outputs using text embeddings) and GPT-4-Judge scores (0-100 across five criteria: Coverage, Redundancy, Readability, Accuracy, Consistency). Seven models were evaluated including GPT-4o, GPT-3.5-turbo, GLM4, Llama3.1-8B, and Qwen2/2.5 variants.

## Key Results
- All models show significant performance gaps, with best overall score around 70% on GPT-4-Judge
- QA pair extraction recall drops below 50% for longer documents (>15K tokens)
- Section-level summarization precision and recall scores are below 30% for all models
- GPT-3.5-turbo performance collapses when input exceeds 16K tokens due to context limits
- Single-response prompting outperforms RAG-based methods for documents exceeding 15K tokens

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Domain Coverage Exposes Gaps
Benchmarking across languages, sectors, and tasks reveals performance limitations that monolingual, single-source benchmarks miss. Real financial meetings contain verbal discussions, topic transitions, and participant interactions that differ structurally from news articles or earnings reports. By spanning 11 GICS sectors and 3 languages, the dataset creates diverse contextual patterns that stress-test generalization.

### Mechanism 2: Long-Context Degradation from Token Limits
Models with shorter context windows or weaker long-context handling show performance cliffs at specific length thresholds. QA pair extraction requires scanning 10K-17K tokens to identify question-answer pairs distributed throughout. Models must maintain attention across the full document rather than relying on local patterns.

### Mechanism 3: Implicit Segmentation Challenge in Summarization
Low section-level alignment scores (<30% F1) indicate that models struggle to implicitly segment transcripts into coherent topic boundaries. Meeting transcripts lack explicit section headers. Models must detect topic shifts from verbal cues and discourse structure, then generate summaries per detected section.

## Foundational Learning

- **Long-Context Attention Mechanisms**
  - Why needed: Meetings average 10K-17K tokens; models must maintain coherent attention across this span to extract QA pairs and generate consistent summaries.
  - Quick check: Can you explain why standard transformer attention scales quadratically with sequence length, and how architectures like sparse attention or linear attention address this?

- **Multilingual Transfer in LLMs**
  - Why needed: Dataset covers English, Chinese, and Japanese; performance varies by language, suggesting uneven multilingual capabilities even in "multilingual" models.
  - Quick check: What is the difference between training on multilingual data vs. being evaluated on multilingual benchmarks, and why might a model excel at one but not the other?

- **Topic Segmentation in Discourse**
  - Why needed: Summarization task requires implicit topic boundary detection in spoken transcripts without explicit markers.
  - Quick check: How would you algorithmically detect topic shifts in a meeting transcript—what signals (lexical, prosodic, discourse) would you use?

## Architecture Onboarding

- **Component map:** Audio recordings (100-400 per language) → ASR transcription (Whisper) + manual correction → Annotation (segmentation, QA extraction, summarization) → Evaluation tasks: Summarization | QA Extraction | QA Answering → Metrics: P/R/F1 (embedding alignment) + GPT-4-Judge scores

- **Critical path:** Audio → ASR correction quality is the bottleneck; errors here propagate to all downstream tasks. Annotation quality depends on financial analyst expertise.

- **Design tradeoffs:**
  - RAG vs. full-context: Paper shows RAG(top-5) only helps for documents <10K tokens; for longer meetings, full-context outperforms. RAG sacrifices global context for retrieval precision.
  - Single vs. batched QA prompts: Batched questions reduce API calls but require maintaining multiple reasoning threads; models like LLaMA3.1-8B struggle with instruction following in batched mode.
  - GPT-4-Judge vs. human evaluation: Judge scores correlate with humans (Fleiss' Kappa 0.701), but Judge is faster/cheaper; humans still needed for validation.

- **Failure signatures:**
  - QA pair extraction recall <50%: Model identifies some but not all questions in long documents
  - Summarization precision/recall <30%: Generated sections don't align with gold sections
  - Japanese performance variability: Higher consistency but some models (GPT-3.5-turbo) struggle more in Japanese
  - Context overflow: GPT-3.5-turbo performance drops to near-zero for >16K tokens

- **First 3 experiments:**
  1. Run all 7 models on the full dataset; confirm reported GPT-4-Judge scores. Check if your model ranking matches (Qwen2.5-72B > GPT-4o > Qwen2-72B).
  2. Stratify evaluation by the 5 length sets (0-5K, 5-10K, 10-15K, 15-20K, >20K). Plot performance decay curves to identify your model's context ceiling.
  3. Sample 50 meetings where recall is lowest. Manually inspect which questions are missed—are they embedded in dense paragraphs, at document boundaries, or phrased informally? This reveals whether failures are positional, syntactic, or semantic.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can structured modeling techniques be applied to mitigate the performance decline in QA pair extraction observed in long-context settings?
- Basis in paper: Page 8 suggests exploring structured modeling to handle long input contexts better.
- Why unresolved: Current models (e.g., Qwen2-72B) show declining QA extraction performance as input length increases.

### Open Question 2
- Question: What specific architectures or training methods can improve LLMs' ability to implicitly segment financial meetings into thematic sections for summarization?
- Basis in paper: Page 7 notes that low alignment scores indicate LLMs struggle with both semantic accuracy and document segmentation.
- Why unresolved: Section-level summarization precision and recall scores were consistently below 30% for all models.

### Open Question 3
- Question: Why does prompting LLMs to answer all questions in a single response outperform RAG-based methods for documents exceeding 15K tokens?
- Basis in paper: Page 8 results show single-response prompting outperforms RAG variants for documents >15K tokens.
- Why unresolved: This contradicts the intuition that RAG aids long-context tasks; standard RAG failed to surpass non-RAG for long inputs.

## Limitations
- Evaluation relies entirely on GPT-4-based metrics, potentially introducing bias toward GPT-style outputs
- Corpus size (600 meetings) is relatively small for robust cross-language comparisons
- Manual ASR correction process introduces potential annotation inconsistencies not quantified

## Confidence

**High Confidence** (Strong empirical support):
- Dataset successfully covers all 11 GICS sectors and three languages as claimed
- Performance ordering across models (Qwen2.5-72B > GPT-4o > Qwen2-72B) is consistent across tasks
- Context length limitations are clearly demonstrated for GPT-3.5-turbo

**Medium Confidence** (Moderate empirical support):
- Claim that financial meeting content is structurally different from news articles is supported but not directly tested
- 70% overall score threshold for "significant room for improvement" is relative rather than absolute

**Low Confidence** (Weak or indirect support):
- Assertion that long-context understanding specifically drives performance differences (vs. retrieval sufficiency)
- Claim that GPT-4-Judge scores correlate well with human evaluation (based on single Kappa measurement)

## Next Checks

1. Conduct blind human evaluation on a stratified sample (10 meetings × 3 tasks) to verify GPT-4-Judge score correlations across different model types, particularly focusing on cases where Judge scores diverge from observed quality.

2. Systematically compare RAG (top-5 chunks) against full-context inference across all 600 meetings, controlling for document length and content type, to determine whether retrieval failures or context window limitations drive the performance gap.

3. Perform inter-annotator agreement analysis on a subset (20 meetings) to quantify annotation reliability, particularly for the implicit segmentation task in summarization, and assess whether performance differences reflect model limitations or annotation artifacts.