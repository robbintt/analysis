---
ver: rpa2
title: 'Redefining CX with Agentic AI: Minerva CQ Case Study'
arxiv_id: '2509.12589'
source_url: https://arxiv.org/abs/2509.12589
tags:
- minerva
- customer
- agent
- agentic
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a case study of Minerva CQ, a real-time Agent\
  \ Assist product deployed in voice-based customer support. Minerva CQ integrates\
  \ real-time transcription, intent and sentiment detection, entity recognition, contextual\
  \ retrieval, dynamic customer profiling, and partial conversational summaries\u2014\
  enabling proactive workflows and continuous context-building."
---

# Redefining CX with Agentic AI: Minerva CQ Case Study

## Quick Facts
- arXiv ID: 2509.12589
- Source URL: https://arxiv.org/abs/2509.12589
- Authors: Garima Agrawal; Riccardo De Maria; Kiran Davuluri; Daniele Spera; Charlie Read; Cosimo Spera; Jack Garrett; Don Miller
- Reference count: 4
- One-line primary result: 38% AHT reduction, 33% Lead-to-Enquiry uplift, 4.8% booking conversion uplift in live A/B test

## Executive Summary
This paper presents a case study of Minerva CQ, a real-time Agent Assist product deployed in voice-based customer support. Minerva CQ integrates real-time transcription, intent and sentiment detection, entity recognition, contextual retrieval, dynamic customer profiling, and partial conversational summaries—enabling proactive workflows and continuous context-building. The study demonstrates that Minerva CQ, as an agentic AI system, can significantly improve agent efficiency and customer experience. In a live production A/B test with 100 agents handling approximately 40,000 calls, Minerva CQ achieved a 38% reduction in Average Handling Time (AHT), a 33% uplift in Lead-to-Enquiry conversion, and a 4.8% uplift in booking conversion. The system's ability to proactively generate context-grounded queries, maintain evolving conversation context through partial summaries, and trigger intent-aware workflows are key drivers of these improvements.

## Method Summary
The study deployed Minerva CQ in a live A/B test with 100 agents handling approximately 40,000 calls. The system uses an agentic AI pipeline combining real-time multilingual ASR, intent detection, proactive query generation, validated FAQ caching, partial summarization, and workflow triggering. The dual-path retrieval system uses a validated FAQ cache (<0.5s) for common queries and RAG fallback (5-9s) for novel questions. The system continuously maintains conversation context through partial summaries and triggers modular workflows based on detected intent.

## Key Results
- 38% reduction in Average Handling Time (AHT) from 4m43s to 2m55s
- 33% uplift in Lead-to-Enquiry conversion rates
- 4.8% uplift in booking conversion rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Proactive, context-grounded query generation with validated FAQ caching reduces agent search latency and cognitive load compared to manual RAG.
- **Mechanism:** The system monitors the live transcript, identifies an implicit or explicit customer ask, reformulates it into a structured query, and checks a validated FAQ cache. If a match exists, it returns an answer in <0.5s; if not, it falls back to slower RAG retrieval.
- **Core assumption:** Agents will trust and utilize AI-suggested prompts rather than ignoring them, and the query reformulation accurately reflects customer intent.
- **Evidence anchors:**
  - [abstract] Mentions "proactive workflows" and "context-grounded queries" as key drivers.
  - [section 5.2] Explicitly cites "Proactive, KB-compatible query generation with validated FAQ caching" as a primary contributor to KPI gains, noting ~7,000 queries served from cache in 10,000 calls.
  - [corpus] Neighbor paper *47681 (Enhancing RAG...)* supports the efficacy of query rewriting in customer support contexts, though specific caching metrics are unique to this paper.
- **Break condition:** If the query reformulation logic fails to map conversational language to KB terminology, or if the cache serves stale answers, agent trust degrades and latency benefits are negated by correction time.

### Mechanism 2
- **Claim:** Incremental partial summaries preserve conversation state, enabling consistent reasoning and reducing the need for agents to manually track details.
- **Mechanism:** Instead of processing each utterance in isolation, the system generates a running "partial summary" that evolves turn-by-turn. This compact state is fed to downstream modules (intent detection, workflows) to maintain continuity.
- **Core assumption:** The summarization model effectively filters noise and retains salient facts without hallucinating details.
- **Evidence anchors:**
  - [abstract] Lists "partial conversational summaries" as a core integration for "continuous context-building."
  - [section 2.5] Describes how these summaries capture facts for downstream reasoning and workflow triggers.
  - [corpus] General support for context-aware NLU is found in neighbor *36232*, but specific partial summary mechanisms are not detailed in the provided corpus.
- **Break condition:** If the summary lags significantly behind real-time speech or omits critical entities (e.g., a specific product model), subsequent workflow triggers will be based on incomplete context.

### Mechanism 3
- **Claim:** Real-time intent detection triggers modular workflows and next-best actions, reducing agent decision latency.
- **Mechanism:** The system continuously classifies customer intent from the transcript. Once confidence crosses a threshold, it automatically triggers a relevant workflow (e.g., "billing correction") and surfaces a step-by-step guide to the agent.
- **Core assumption:** The intent classification model is robust enough to avoid "flip-flopping" as the conversation progresses, and standard workflows cover the majority of customer issues.
- **Evidence anchors:**
  - [abstract] States the system "identifies customer intent, triggers modular workflows... and adapts dynamically."
  - [section 2.2] Details how high-confidence intent triggers workflows to remove manual search.
  - [corpus] Neighbor *36232* supports the value of context-aware intent classification in chatbots.
- **Break condition:** If intent is misclassified early (e.g., sales query misread as support), the triggered workflow guides the agent down the wrong path, requiring manual intervention to reset.

## Foundational Learning

- **Concept:** **Latency Budgeting in Real-Time Voice**
  - **Why needed here:** Voice interactions demand sub-second responses to feel natural. The paper highlights a critical difference: RAG retrieval (5-9s) vs. FAQ cache (<0.5s). Understanding this budget is essential for architecting the fallback logic.
  - **Quick check question:** If a customer asks a novel question not in the FAQ cache, will the 5-9s RAG retrieval delay disrupt the agent's conversational flow?

- **Concept:** **Context State Management (Streaming vs. Static)**
  - **Why needed here:** Unlike chatbots where context is the full chat history, voice agents need "partial summaries" to handle token limits and latency. You must understand how to evolve a state object incrementally rather than reprocessing the full transcript every turn.
  - **Quick check question:** How does the system determine which new information from the latest utterance gets appended to the partial summary versus what is discarded as noise?

- **Concept:** **Agentic vs. Reactive AI**
  - **Why needed here:** The paper defines "Agentic AI" as goal-driven and tool-using, contrasting it with reactive RAG. Engineers need to distinguish between a system that waits for a prompt (search bar) and one that acts autonomously (auto-triggering workflows).
  - **Quick check question:** Does the system require an agent to click "search" to receive help (reactive), or does it push a "next-best action" based on the conversation flow (agentic)?

## Architecture Onboarding

- **Component map:** ASR -> Entity Recognition & Sentiment/Intent Detection -> Partial Summary Generator -> Workflow Orchestrator -> Hybrid Knowledge Layer (FAQ Cache -> RAG Fallback) -> Agent Dashboard
- **Critical path:** The `ASR -> Intent -> Workflow` loop must operate within tight latency constraints (aiming for <1s end-to-end) to provide value during a live call. The FAQ cache is the "fast path" ensuring this budget is met for common queries.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Using a validated FAQ cache is faster but covers fewer queries than full RAG. The system trades breadth for speed on common queries.
  - **Automation vs. Control:** Auto-triggering workflows reduces agent load but risks derailing the call if intent detection is wrong. The design assumes agents can override, but frequent errors degrade trust.
- **Failure signatures:**
  - **Context Drift:** The partial summary accumulating irrelevant details, causing the AI to suggest off-topic workflows.
  - **Cache Staleness:** Policy changes not reflected in the FAQ cache leading to incorrect automated responses.
  - **Intent Flip-flopping:** The system rapidly switching between suggested workflows as the customer speaks, confusing the agent.
- **First 3 experiments:**
  1. **Latency Benchmark:** Measure the P95 latency of the FAQ cache path vs. the RAG path to validate the "6 seconds saved" claim in your specific environment.
  2. **Context Retention Test:** Feed transcripts with specific key details (names, dates) and verify if the partial summary retains these facts after 5+ conversational turns.
  3. **Intent Accuracy Audit:** Sample 50 calls where a workflow was triggered and measure the precision rate (did the workflow match the actual customer need?).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the validated FAQ caching lifecycle be extended with automatic drift detection to maintain answer correctness without sacrificing the low-latency advantages required for real-time voice support?
- **Basis in paper:** [explicit] The authors explicitly identify "FAQ governance and drift" as a future direction, calling for mechanisms to extend the mining-validation-cache-expiry lifecycle with "automatic drift detection and policy/version control."
- **Why unresolved:** While caching FAQs reduces latency (from 5–9s to <0.5s), static caches risk becoming stale as business policies change, creating a trade-off between speed and accuracy that current heuristics may not fully solve.
- **What evidence would resolve it:** Algorithms capable of detecting semantic shifts in knowledge bases and automatically flagging or updating cached FAQs, evaluated on their ability to maintain high accuracy rates over time without manual intervention.

### Open Question 2
- **Question:** Do the efficiency gains from partial summarization and intent-triggered workflows observed in voice interactions transfer effectively to asynchronous channels like chat and email?
- **Basis in paper:** [explicit] Section 5.8 lists "Cross-channel generalization" as a future direction, specifically asking researchers to "investigate how partial summarization... transfer[s] from voice to chat/email."
- **Why unresolved:** The system is currently optimized for the high-pressure, ephemeral nature of voice calls; it is unclear if the same context-building mechanisms provide equal value in text-based channels with inherently different latency expectations.
- **What evidence would resolve it:** Comparative A/B testing in chat/email environments showing that partial summaries and proactive workflows reduce response times and improve resolution rates similarly to the voice pilot.

### Open Question 3
- **Question:** How do the frequency, timing, and UI presentation of proactive "AI co-pilot" suggestions affect agent trust, cognitive load, and the development of effective human-agent handoff strategies?
- **Basis in paper:** [explicit] The paper explicitly calls for "Human factors and training" research to "Quantify how guidance frequency, timing, and UI presentation affect agent trust, handoff smoothness, and learning curves."
- **Why unresolved:** While the paper demonstrates aggregate efficiency gains (lower AHT), it does not isolate the impact of the user interface or interaction design on the agent's psychological state or potential over-reliance on the tool.
- **What evidence would resolve it:** User studies or longitudinal interaction logs that correlate specific UI designs and suggestion timings with qualitative agent feedback (trust scores) and performance consistency across different experience levels.

## Limitations
- **Production environment specifics:** Critical operational details like call duration distribution, agent experience levels, and deployment infrastructure remain unspecified.
- **Model specification gaps:** Exact ASR, LLM, and retrieval models used, along with their configuration parameters, are not provided.
- **Causal attribution ambiguity:** The study shows correlation between system usage and KPI improvements but lacks granular breakdown of individual component contributions.

## Confidence
- **High confidence:** The measured KPI improvements (38% AHT reduction, 33% Lead-to-Enquiry uplift, 4.8% booking conversion uplift) are directly supported by the A/B test results.
- **Medium confidence:** The claim that proactive query generation and partial summaries are primary drivers of improvement is supported by stated mechanisms but lacks individual component contribution data.
- **Low confidence:** The assertion that this represents a fundamental shift to "Agentic AI" versus "reactive AI" is more definitional than empirical, lacking comparative data against a reactive baseline.

## Next Checks
1. **Component isolation test:** Deploy the system with only FAQ caching enabled (no RAG fallback, no workflow triggers) to measure baseline latency improvement and assess whether the cache alone drives the AHT reduction.
2. **Confidence threshold sensitivity:** Systematically vary the intent detection confidence threshold (e.g., 70%, 80%, 90%) and measure corresponding changes in workflow trigger accuracy and agent override rates to identify optimal operating points.
3. **Longitudinal performance tracking:** Extend the A/B test duration to 6 months with continuous monitoring of KPI trends, agent adaptation curves, and any degradation in system performance as conversation patterns evolve.