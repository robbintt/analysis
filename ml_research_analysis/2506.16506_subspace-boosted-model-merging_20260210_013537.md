---
ver: rpa2
title: Subspace-Boosted Model Merging
arxiv_id: '2506.16506'
source_url: https://arxiv.org/abs/2506.16506
tags:
- task
- merging
- singular
- subspace
- rank
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work identifies rank collapse as a fundamental limitation
  in Task Arithmetic-based model merging methods. As more expert models are merged,
  the common information dominates the task-specific information, leading to inevitable
  rank collapse that harms model performance.
---

# Subspace-Boosted Model Merging

## Quick Facts
- arXiv ID: 2506.16506
- Source URL: https://arxiv.org/abs/2506.16506
- Authors: Ronald Skorobogat; Karsten Roth; Mariana-Iuliana Georgescu
- Reference count: 40
- Key outcome: Subspace Boosting improves model merging efficacy by 10%+ for up to 20 experts by addressing rank collapse

## Executive Summary
This work identifies rank collapse as a fundamental limitation in Task Arithmetic-based model merging methods. As more expert models are merged, the common information dominates the task-specific information, leading to inevitable rank collapse that harms model performance. The authors propose Subspace Boosting, a method that operates on the singular value decomposed task vector space to maintain task vector ranks. Subspace Boosting raises merging efficacy for up to 20 experts by large margins of more than 10% on both vision and language benchmarks. Additionally, the paper introduces Higher-Order Generalized Singular Value Decomposition to quantify task similarity, offering a new interpretable perspective on model merging.

## Method Summary
Subspace Boosting addresses rank collapse in model merging by operating in the singular value decomposed (SVD) task vector space. The method first applies SVD to decompose the task vectors, then performs boosting operations within this subspace to preserve task-specific information while preventing rank collapse. Unlike Task Arithmetic which suffers from inevitable rank collapse as more experts are merged, Subspace Boosting maintains the rank of task vectors throughout the merging process. The method also incorporates Higher-Order Generalized Singular Value Decomposition to quantify task similarity, providing interpretability for the merging process.

## Key Results
- Subspace Boosting improves merging efficacy by more than 10% on both vision and language benchmarks
- Method effectively handles up to 20 expert models without rank collapse
- Higher-Order GSVD provides interpretable task similarity quantification

## Why This Works (Mechanism)
Rank collapse occurs in Task Arithmetic when common information dominates task-specific information during merging. This happens because Task Arithmetic operates directly on parameter spaces without preserving the intrinsic structure of task-specific subspaces. Subspace Boosting works by first decomposing task vectors via SVD, which separates the common and task-specific components into distinct subspaces. By performing merging operations within this decomposed space, the method can selectively preserve task-specific information while controlling rank collapse. The Higher-Order GSVD further enhances this by providing a principled way to quantify and compare task similarities across multiple experts.

## Foundational Learning
- Singular Value Decomposition (SVD): Essential for decomposing task vectors into orthogonal components. Quick check: Verify SVD decomposition maintains orthogonality through reconstruction error testing.
- Task Arithmetic fundamentals: Understanding how parameter averaging leads to rank collapse. Quick check: Demonstrate rank collapse empirically when merging multiple experts.
- Generalized Singular Value Decomposition: Extends SVD to multiple matrices for multi-task comparison. Quick check: Validate Higher-Order GSVD produces meaningful similarity metrics.
- Rank preservation theory: Critical for understanding why rank collapse harms model performance. Quick check: Measure rank before and after merging operations.
- Subspace projection: Needed to understand how Subspace Boosting operates in decomposed spaces. Quick check: Verify projection maintains task-specific information content.

## Architecture Onboarding

**Component Map:** Expert Models -> SVD Decomposition -> Subspace Boosting -> Merged Model

**Critical Path:** The core workflow involves decomposing expert models via SVD, applying boosting operations in the decomposed subspace to preserve task-specific information, then reconstructing the merged model while maintaining rank.

**Design Tradeoffs:** Subspace Boosting trades computational overhead (SVD operations) for significant performance gains. The method requires additional memory for storing decomposed components but achieves better preservation of task-specific information compared to Task Arithmetic.

**Failure Signatures:** Rank collapse manifests as performance degradation when merging multiple experts. Common failure modes include: 1) Insufficient rank preservation during decomposition, 2) Poor subspace selection leading to information loss, 3) Numerical instability in SVD computations for very large models.

**First Experiments:**
1. Baseline comparison: Merge 2-5 experts using Task Arithmetic vs Subspace Boosting to demonstrate rank collapse
2. Scalability test: Merge 10-20 experts to verify Subspace Boosting maintains performance gains
3. Interpretability validation: Use Higher-Order GSVD to analyze task similarities across different expert combinations

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis lacks rigorous mathematical proofs for rank collapse mechanisms
- Additional computational overhead from SVD operations may limit scalability
- Performance gains untested for ensembles larger than 20 experts

## Confidence

**High confidence:** The empirical demonstration of rank collapse in Task Arithmetic and the effectiveness of Subspace Boosting in mitigating it

**Medium confidence:** The theoretical explanation of rank collapse mechanisms and the mathematical properties of the proposed method

**Medium confidence:** The interpretability claims of Higher-Order GSVD for task similarity

## Next Checks

1. Test Subspace Boosting scalability beyond 20 experts on larger model architectures to verify performance holds at scale
2. Conduct ablation studies comparing Subspace Boosting with alternative rank-preserving methods to isolate which components drive improvements
3. Perform theoretical analysis proving the mathematical relationship between SVD rank preservation and task-specific information retention during merging