---
ver: rpa2
title: 'PubMed-OCR: PMC Open Access OCR Annotations'
arxiv_id: '2601.11425'
source_url: https://arxiv.org/abs/2601.11425
tags:
- document
- pages
- page
- text
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PubMed-OCR introduces a large-scale, OCR-first corpus derived from
  PubMed Central Open Access PDFs, providing paragraph-, line-, and word-level bounding
  boxes directly from page images using Google Cloud Vision OCR. This approach avoids
  alignment errors inherent in PDF/XML matching and supports non-digital pages.
---

# PubMed-OCR: PMC Open Access OCR Annotations

## Quick Facts
- arXiv ID: 2601.11425
- Source URL: https://arxiv.org/abs/2601.11425
- Authors: Hunter Heidenreich; Yosheb Getachew; Olivia Dinica; Ben Elliott
- Reference count: 40
- Primary result: 209.5K articles (1.5M pages, 1.3B words) with paragraph-, line-, and word-level bounding boxes from PubMed Central Open Access PDFs

## Executive Summary
PubMed-OCR introduces a large-scale corpus of OCR annotations derived directly from PubMed Central Open Access PDFs, providing word-, line-, and paragraph-level bounding boxes at scale. The dataset avoids alignment errors inherent in traditional PDF/XML matching approaches by running Google Cloud Vision OCR directly on rendered page images, supporting both digital and scanned documents. With 1.5M pages and 1.3B words, PubMed-OCR offers 4× more line and 10× more word annotations than comparable datasets, filling a critical gap for layout-aware scientific document understanding.

## Method Summary
The dataset was created by sampling 209.5K documents from ~1.2M PMCOA PDFs with redistributable licenses, rendering each page to 150 DPI images, and running Google Cloud Vision `document_text_detection` to extract word and paragraph polygons. Lines were reconstructed heuristically by clustering words with similar vertical bounds (±5 pixels) within paragraphs. Per-page JSON outputs contain words/lines/paragraphs with axis-aligned bounding boxes `[X1, Y1, X3, Y3]`, along with metadata tracking license type and commercial-use flags.

## Key Results
- 209.5K articles (1.5M pages, 1.3B words) with word-, line-, and paragraph-level bounding boxes
- 4× more line annotations and 10× more word annotations than OCR-IDL
- Qualitative analysis shows high prevalence of formulas (~25%), tables (~18%), and charts using PP-DocLayout
- Compact JSON schema with 40% size reduction compared to raw Google Vision output

## Why This Works (Mechanism)

### Mechanism 1: OCR-First Annotation Avoids Alignment Errors
Direct OCR on page images produces more reliable bounding boxes than approaches that align PDF-extracted text to JATS XML. Traditional PMCOA datasets extract text regions from digital PDFs and heuristically match them to JATS XML, introducing parser noise, missed text from scanned pages, and alignment errors. PubMed-OCR bypasses this by running Google Vision OCR directly on rendered page images, producing word/paragraph boxes that correspond to actual visual positions. Core assumption: Google Vision OCR achieves sufficient accuracy on scientific documents with dense formulas, tables, and multi-column layouts. Evidence: "avoids alignment errors inherent in PDF/XML matching and supports non-digital pages." Break condition: If OCR consistently fails on mathematical notation or unusual fonts, the "avoid alignment errors" advantage is negated by OCR errors.

### Mechanism 2: Heuristic Line Reconstruction from Word Clustering
Lines can be reliably reconstructed by clustering words with similar vertical bounds (±5 pixels) within paragraphs. The system: (1) computes y-min/y-max for each word, (2) groups words whose vertical bounds differ by ≤5 pixels, (3) splits groups spanning multiple paragraphs, (4) sorts words left-to-right. This preserves reading order while respecting paragraph boundaries. Core assumption: Scientific documents have consistent baseline spacing where words on the same line have nearly identical y-coordinates at 150 DPI. Evidence: Full algorithm: "|ymin(w) − ȳmin| ≤ 5 and |ymax(w) − ȳmax| ≤ 5 pixels." Break condition: Subscripts, superscripts in formulas, or tightly-packed tables cause incorrect clustering.

### Mechanism 3: Scale Enables Layout-Aware Model Training
Providing 1.3B words with bounding boxes (10× OCR-IDL) enables training layout-aware document understanding models for scientific literature. Layout-aware models (LayoutLM, DocLLM, LayTextLLM) require paired text and spatial coordinates. PubMed-OCR provides word/line/paragraph granularity at scale, supporting: (1) pre-training models to associate text with layout, (2) coordinate-grounded QA with verifiable attribution, (3) benchmarking OCR pipelines on scientific documents specifically. Core assumption: Annotation volume compensates for noise from single-engine OCR and heuristic reconstruction. Evidence: "supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines." Break condition: Systematic OCR errors at scale propagate bias into downstream models.

## Foundational Learning

- **Concept: Grounded vs. Linearized OCR**
  - Why needed here: The paper distinguishes text recognition (serialized output) from grounded OCR (bounding boxes preserved). This distinction determines whether you can use the data for layout-aware modeling or only text-only tasks.
  - Quick check question: If training a text-only LLM, do you need PubMed-OCR's bounding boxes or just the extracted text?

- **Concept: PDF Parsing Failure Modes**
  - Why needed here: Understanding why PDF/XML alignment fails (parser noise, non-digital pages, heuristic reading order) explains PubMed-OCR's motivation.
  - Quick check question: Why would a PDF parser miss text from a scanned document embedded in a PDF file?

- **Concept: Bounding Box Coordinate Systems**
  - Why needed here: The dataset outputs `[X1, Y1, X3, Y3]` axis-aligned boxes in pixel coordinates at 150 DPI. Using this data requires understanding coordinate normalization for model input.
  - Quick check question: For a 150 DPI render of an 8.5" × 11" page, what are the maximum X and Y pixel values?

## Architecture Onboarding

- **Component map:** License Filter -> Sampler -> Page Renderer (150 DPI) -> OCR Engine (Google Vision) -> Line Reconstructor (5-pixel clustering) -> JSON Serializer -> Metadata CSV

- **Critical path:** PDF -> Render (150 DPI) -> Google Vision API -> Word polygon extraction -> Line clustering (heuristic) -> JSON output. The line reconstruction step is the most fragile.

- **Design tradeoffs:**
  - Single OCR engine: Simpler pipeline, but no ensemble error correction (DeepSeek-OCR uses 4)
  - 150 DPI vs. 300 DPI: Lower cost (~$2.3K vs. estimated $12K full), but may reduce small-text accuracy
  - Axis-aligned boxes: Simpler schema, but loses precision for rotated/diagonal elements
  - No character-level boxes: Compact output, but limits fine-grained grounding tasks

- **Failure signatures:**
  - Cross-column line merging: Lines with >100 words or bounding box width exceeding 80% of page width
  - Formula corruption: High symbol-to-letter ratios in "lines" that aren't valid expressions
  - Missed scanned content: Pages where JSON word count is 0 but visual inspection shows text

- **First 3 experiments:**
  1. Line reconstruction validation: Manually annotate 100 pages with ground-truth lines, compute clustering precision/recall
  2. Multi-engine comparison: Run Tesseract, Azure OCR, Amazon Textract on 1,000-page sample, measure text agreement rate
  3. Downstream task baseline: Fine-tune LayoutLMv3 on scientific document classification using PubMed-OCR vs. PubLayNet coordinates, compare accuracy

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the accuracy of the heuristic line reconstruction algorithm compare to ground-truth line segmentation on pages with complex multi-column layouts?
  - Basis: The paper lists "heuristic line reconstruction" as a limitation, noting that clustering words based on vertical alignment with a 5-pixel threshold may "introduce biases in reading order and grouping."
  - Why unresolved: No quantitative evaluation of the line reconstruction quality, leaving the error rate for dense or irregular scientific layouts unknown.
  - What evidence would resolve it: A comparative evaluation of the generated line clusters against manually annotated ground-truth lines across a stratified sample of the dataset.

- **Open Question 2:** To what extent does the exclusive reliance on Google Cloud Vision OCR bias downstream model performance on documents processed by alternative engines?
  - Basis: The authors identify "reliance on a single OCR engine" as a primary limitation and suggest that open models often combine ensembles (e.g., PaddleOCR, GOT-OCR) to improve quality.
  - Why unresolved: The dataset reflects the specific error modes and tokenization behaviors of the Google Vision API; it is unclear if models trained on this data generalize to documents annotated by Amazon Textract or open-source alternatives.
  - What evidence would resolve it: Benchmarking downstream tasks (e.g., layout analysis or extraction) on models trained on PubMed-OCR but evaluated on documents annotated with different OCR engines.

- **Open Question 3:** Can the current PubMed-OCR annotations effectively serve as weak supervision for training models to detect complex structures like tables and mathematical formulas?
  - Basis: The paper detects a high prevalence of formulas (~25%) and tables (~18%) via PP-DocLayout, but explicitly states the dataset lacks "explicit representations of mathematical expressions or figure/table structure."
  - Why unresolved: While the OCR captures the text within these regions, the lack of structural labels means the dataset's utility for training parsers to understand the internal geometry of tables or formulas is unproven.
  - What evidence would resolve it: Experiments using the word-level bounding boxes as features for training structure recognition models, evaluated against established scientific table benchmarks like PubTables-1M.

## Limitations
- Exclusive reliance on Google Cloud Vision OCR without ensemble validation or comparison to alternatives
- Heuristic line reconstruction algorithm accuracy unvalidated on complex multi-column layouts
- Commercial-use filtering appears inconsistent (only 45% marked as commercially usable despite 60% eligibility)

## Confidence

- **High confidence:** Scale claims (209.5K articles, 1.5M pages, 1.3B words) and basic schema structure
- **Medium confidence:** Layout diversity analysis via PP-DocLayout, relative scale advantage over OCR-IDL (4× lines, 10× words)
- **Low confidence:** OCR accuracy, line reconstruction reliability, downstream utility without empirical validation

## Next Checks
1. Line reconstruction accuracy: Manually annotate 100 randomly selected pages with ground-truth lines and compute precision/recall of the 5-pixel clustering algorithm, particularly for multi-column layouts and formula-heavy pages
2. Multi-engine OCR comparison: Run Tesseract, Azure OCR, and Amazon Textract on a 1,000-page stratified sample; measure text agreement rates and bounding box consistency against Google Vision outputs
3. Downstream task baseline: Fine-tune LayoutLMv3 on scientific document classification using PubMed-OCR vs. PubLayNet coordinates; compare accuracy and assess whether the larger scale compensates for potential OCR noise