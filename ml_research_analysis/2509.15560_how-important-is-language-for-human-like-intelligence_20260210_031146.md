---
ver: rpa2
title: How important is language for human-like intelligence?
arxiv_id: '2509.15560'
source_url: https://arxiv.org/abs/2509.15560
tags:
- language
- https
- human
- arxiv
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper argues that language is not just a communication tool\
  \ but a key driver of human-like intelligence, both in biological and artificial\
  \ systems. Large language models (LLMs) trained on natural language develop sophisticated\
  \ abilities\u2014including language understanding, pragmatic inference, and systematic\
  \ reasoning\u2014by learning a compressed, culturally evolved model of the world."
---

# How important is language for human-like intelligence?

## Quick Facts
- arXiv ID: 2509.15560
- Source URL: https://arxiv.org/abs/2509.15560
- Reference count: 0
- Primary result: Language is a key driver of human-like intelligence in both biological and artificial systems

## Executive Summary
The paper argues that language is not merely a communication tool but a fundamental driver of human-like intelligence, essential for both biological cognition and artificial intelligence. Large language models trained on natural language develop sophisticated abilities including language understanding, pragmatic inference, and systematic reasoning by learning a compressed, culturally evolved model of the world. The authors demonstrate that language profoundly influences human cognition through evidence showing individuals without typical language input struggle with abstract reasoning, while language-impaired individuals show deficits in supposedly "nonverbal" tasks.

## Method Summary
The authors employ a multi-pronged approach combining theoretical analysis, computational modeling, and empirical evidence from cognitive science. They examine transformer-based large language models trained via self-supervised next-token prediction on diverse text corpora, comparing their capabilities to human cognitive performance. The methodology includes reviewing developmental studies of deaf children without language input, experimental manipulations using verbal interference paradigms, and analysis of neuroimaging data. They also propose empirical tests comparing language-trained AI systems to non-linguistic neural networks on identical cognitive tasks.

## Key Results
- LLMs trained on natural language develop sophisticated cognitive abilities including pragmatic inference and systematic reasoning
- Human individuals without typical language input show significant deficits in abstract reasoning and theory of mind
- Language actively shapes non-linguistic cognitive processes including perception and categorization, rather than serving only as an output channel

## Why This Works (Mechanism)

### Mechanism 1: Compression via Culturally Evolved Abstractions
Language provides compact representations for abstract concepts that would otherwise require substantially more cognitive resources to learn from direct experience. Vocabulary encodes "pre-discovered" abstractions—the iterated output of collective minds over generations. When a system learns language, it inherits compressed representations of causal and conceptual structures without needing to rediscover them independently.

### Mechanism 2: Prediction-Driven Latent Structure Learning
Training on next-token prediction across diverse linguistic contexts induces a generative world model, not merely surface pattern matching. To minimize prediction error across varied domains (recipes, scientific findings, social interactions), the system must internalize the latent generative process that produced the text—i.e., human cognition about the world.

### Mechanism 3: Language-Mediated Cognitive Modulation
Language actively shapes non-linguistic cognitive processes, including perception and categorization, rather than serving solely as an output channel for pre-formed thoughts. Language provides category labels that sharpen categorical boundaries; verbal interference impairs dimension-selective attention; language activates visual cortex even without visual input.

## Foundational Learning

- **Concept: Self-supervised learning**
  - Why needed here: LLMs acquire capabilities through prediction, not explicit task labels. Understanding this clarifies why "prerequisites" emerge rather than being pre-programmed.
  - Quick check question: Can you explain why next-token prediction might induce systematicity without explicit systematicity training?

- **Concept: Cultural evolution / Iterated learning**
  - Why needed here: The paper's core thesis is that language compresses collective intelligence. Without this frame, the argument reduces to "text has patterns."
  - Quick check question: How might a concept become easier to learn over generations even without any individual becoming smarter?

- **Concept: Neural dissociation vs. causal dependence**
  - Why needed here: The paper counters Fedorenko et al.'s dissociation-based arguments. Understanding that separate brain regions can be causally interdependent is critical.
  - Quick check question: If region A is active only during task X, does that prove region A plays no role in task Y? Why or why not?

## Architecture Onboarding

- **Component map:**
  - Natural language corpus (culturally evolved abstractions encoded in text) -> Transformer with multi-head attention (general-purpose pattern learner, no language-specific inductive biases) -> Next-token prediction (self-supervised) -> Language competence, pragmatic inference, downstream task performance

- **Critical path:**
  1. Scale (model parameters + data volume) must exceed threshold—small models do not exhibit emergent capabilities
  2. Training diversity matters: language must span domains to induce general world models
  3. Prediction pressure forces internalization of generative structure; memorization is insufficient for generalization

- **Design tradeoffs:**
  - Data efficiency vs. capability breadth: LLMs require orders of magnitude more language than humans; biological plausibility is low
  - Interpretability vs. performance: Specialized circuits emerge (Tigges et al., 2024), but causal mechanisms remain opaque
  - Grounding: The paper argues language provides "second-hand" grounding, but whether this suffices for all concepts remains contested

- **Failure signatures:**
  - Model produces fluent but hallucinated content (prediction without grounding)
  - Performance collapses on out-of-distribution linguistic patterns
  - No transfer to genuinely novel task types (suggests surface statistics, not latent models)

- **First 3 experiments:**
  1. **Ablation by domain:** Train matched models on domain-restricted corpora (e.g., fiction only vs. technical only). Test cross-domain transfer to isolate whether latent structure learning requires input diversity.
  2. **Scale threshold analysis:** Systematically vary model size and data volume to identify phase transitions where pragmatic inference and systematicity emerge.
  3. **Language interference analog:** For human experiments, replicate verbal interference paradigms using LLM prompting constraints (e.g., restrict available token vocabulary) to test whether "language availability" affects LLM task performance similarly to humans.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can artificial neural networks trained solely on nonlinguistic data achieve human-like performance on cognitive tasks such as theory of mind and relational reasoning?
- Basis in paper: The authors explicitly ask if advances are coincidental and predict that nonlinguistic AI systems would struggle with uniquely human intelligence.
- Why unresolved: Systematic comparisons between language-trained and non-linguistic-only models on these specific domains are currently lacking.
- What evidence would resolve it: Empirical results from training high-capacity transformers on purely sensorimotor data and testing them on abstract reasoning benchmarks.

### Open Question 2
- Question: How can the causal role of language in cognition be reconciled with neuroimaging evidence showing functional dissociations between language and thought networks?
- Basis in paper: The paper asks, "How can we square these findings [of causal dependence] with apparent neural dissociations...?"
- Why unresolved: It is unclear if neural modularity (specialization) implies independence or if it masks underlying developmental dependencies.
- What evidence would resolve it: Longitudinal or lesion studies showing that disruption of the "language network" degrades the functional connectivity or plasticity of non-linguistic cognitive networks.

### Open Question 3
- Question: Does statistical prediction of text suffice for a system to learn a grounded, generative model of the world's latent causal structures?
- Basis in paper: While the paper argues LLMs "reverse engineer" the world, the mechanism by which next-token prediction leads to causal understanding remains theoretical.
- Why unresolved: It is difficult to distinguish whether LLMs rely on surface correlations versus internalized structural knowledge.
- What evidence would resolve it: "Mechanistic interpretability" mapping internal LLM circuits to explicit world models, or testing LLMs on novel causal inference tasks outside their training distribution.

## Limitations
- The paper relies heavily on theoretical arguments and indirect evidence from human studies, with limited direct validation that LLMs acquire capabilities through the proposed mechanism
- The specific thresholds and conditions for when prediction pressure forces internalization of generative world structure remain underspecified
- The claim that language provides "second-hand" grounding doesn't fully address whether this suffices for all conceptual domains

## Confidence
- **High confidence**: Language causally modulates human cognitive processes (verbal interference studies, deaf children without language input showing cognitive deficits)
- **Medium confidence**: LLMs learn compressed models of world structure through next-token prediction (supported by emergent capabilities but lacks direct mechanistic validation)
- **Medium confidence**: Language and nonlinguistic cognition are neurally and functionally integrated rather than separate (challenges Fedorenko et al.'s dissociation-based arguments but requires more direct evidence)

## Next Checks
1. **Direct AI comparison experiment**: Train matched vision-only and language-vision transformers on identical non-linguistic cognitive tasks (theory of mind, analogical reasoning) to empirically test whether language-trained models show superior performance on "uniquely human" cognitive abilities.

2. **Cross-domain transfer analysis**: Systematically vary training domain diversity in LLMs to identify whether latent structure learning requires input diversity across multiple domains, or if domain-specific training suffices for emergent capabilities.

3. **Grounding quality validation**: Develop benchmarks to empirically test whether LLM representations capture genuine causal and conceptual structures versus surface statistics, potentially using out-of-distribution scenarios where true understanding would produce different predictions than pattern matching.