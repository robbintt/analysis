---
ver: rpa2
title: 'Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated
  Recommendation'
arxiv_id: '2507.01285'
source_url: https://arxiv.org/abs/2507.01285
tags:
- uni00000013
- user
- uni00000048
- uni00000011
- uni00000014
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of aggregating user embeddings
  in graph federated recommendation systems, where existing methods fail to account
  for user similarity and the importance of anchor users. The authors propose Dist-FedAvg,
  a distance-based aggregation method that assigns higher weights to users with similar
  embeddings while preserving the influence of anchor users.
---

# Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation

## Quick Facts
- **arXiv ID**: 2507.01285
- **Source URL**: https://arxiv.org/abs/2507.01285
- **Reference count**: 40
- **Primary result**: Dist-FedAvg consistently outperforms baseline aggregation methods, achieving higher NDCG@10 and HR@10 scores (e.g., NDCG@10 improvements of 0.4027±0.018 on FilmTrust and HR@10 improvements of 0.89±0.0238).

## Executive Summary
This paper addresses the challenge of aggregating user embeddings in graph federated recommendation systems, where existing methods fail to account for user similarity and the importance of anchor users. The authors propose Dist-FedAvg, a distance-based aggregation method that assigns higher weights to users with similar embeddings while preserving the influence of anchor users. Dist-FedAvg computes a distance matrix using Minkowski distance, normalizes inverse distances to create averaging weights, and applies linear interpolation with anchor user embeddings. The method includes decay strategies for the interpolation parameter α to handle early-round embedding irrelevance. Empirical evaluations on five datasets (MovieLens-100k, MovieLens-1M, LastFM-2k, Amazon Music, FilmTrust) demonstrate that Dist-FedAvg consistently outperforms baseline aggregation methods, achieving higher NDCG@10 and HR@10 scores.

## Method Summary
Dist-FedAvg addresses user embedding aggregation in graph federated recommendation by computing a Minkowski distance matrix between user embeddings, normalizing inverse distances to create aggregation weights, and applying linear interpolation with anchor user embeddings using parameter α. The method uses LightGCN with BPR loss for local training, runs 100 FL rounds with 20 local epochs per round, and employs decay strategies for α to handle early-round embedding irrelevance. The approach shows particular effectiveness when paired with simpler item aggregation methods like FedAvg or SimpleAvg rather than using Dist-FedAvg for both user and item parameters.

## Key Results
- Dist-FedAvg achieves NDCG@10 improvements of 0.4027±0.018 on FilmTrust compared to baseline methods
- HR@10 improvements of 0.89±0.0238 are observed across datasets when using Dist-FedAvg
- Performance plateaus around 20-50 selected clients per round, with more clients not necessarily improving results
- Using Dist-FedAvg for both user and item aggregation underperforms compared to pairing with simpler item methods

## Why This Works (Mechanism)

### Mechanism 1: Inverse Distance Weighting for Similarity-Based Aggregation
- Claim: Weighting user embeddings by inverse distance improves aggregation quality compared to uniform averaging.
- Mechanism: Computes a distance matrix D using Minkowski distance between user embeddings, then normalizes inverse distances to create aggregation weights. Users with closer embeddings contribute more to each other's updates.
- Core assumption: Embedding distance correlates with behavioral similarity relevant to recommendation quality.
- Evidence anchors: Abstract mentions distance matrix computation and weight normalization; section 4 provides Equation 4 for weight calculation; corpus indicates FedCIA uses uniform weighting which Dist-FedAvg addresses.

### Mechanism 2: Anchor User Interpolation for Personalization Preservation
- Claim: Linear interpolation between distance-weighted average and anchor user embedding preserves personalization while benefiting from similar users.
- Mechanism: After computing weighted average, applies linear interpolation with the original user's embedding to maintain personalization signal.
- Core assumption: The anchor user's own embedding contains signal that should not be fully diluted by aggregation.
- Evidence anchors: Abstract emphasizes anchor user influence retention; section 3.2 discusses anchor user centrality and greater contribution to local training.

### Mechanism 3: Alpha Decay for Early-Round Irrelevance Handling
- Claim: Decaying α from high initial values mitigates noise from irrelevant early-round embeddings.
- Mechanism: Uses arithmetic or geometric decay starting with high anchor weight and gradually reducing as training progresses.
- Core assumption: Early-round embeddings are less reliable; similarity signals improve as training progresses.
- Evidence anchors: Abstract mentions decay strategies for handling early-round irrelevance; section 4 notes distance between randomly initialized embeddings is not representative.

## Foundational Learning

- **Federated Averaging (FedAvg)**
  - Why needed here: Dist-FedAvg modifies FedAvg for user embeddings; understanding baseline aggregation is prerequisite.
  - Quick check question: Can you explain why FedAvg weights clients by local data size?

- **Graph Neural Networks for Recommendation (LightGCN)**
  - Why needed here: The paper uses LightGCN for local training; understanding message passing and embedding propagation is assumed.
  - Quick check question: How does LightGCN differ from traditional GCNs in recommendation contexts?

- **User-Item Interaction Graphs and Expansion**
  - Why needed here: The "anchor user" and "expanded users" concepts rely on graph expansion creating local subgraphs with higher-order relations.
  - Quick check question: What is the difference between first-order and higher-order user-item relations?

## Architecture Onboarding

- **Component map**: Client side (LightGCN training on expanded subgraph) -> Server side (Distance matrix computation -> Weight matrix -> Normalization -> Anchor interpolation -> Global embedding update)

- **Critical path**:
  1. Client selection
  2. Local training (LightGCN + BPR loss)
  3. Upload user/item embeddings
  4. Compute distance matrix D
  5. Compute weight matrix W as inverse distance
  6. Normalize and aggregate
  7. Apply anchor interpolation with current α

- **Design tradeoffs**:
  - α value: Fixed hyperparameter vs. decay strategy; paper shows fixed α can work if local epochs are sufficient
  - Number of selected clients: Figure 2 shows plateau around 20-50 clients; more clients don't always help
  - Item aggregation method: Figure 5 shows pairing Dist-FedAvg (users) with FedAvg/SimpleAvg (items) works best—not the same method for both

- **Failure signatures**:
  - Performance plateaus or degrades with too few clients (<10)
  - Early-round instability if embeddings are not warmed up and α is too low
  - Mismatch: using Dist-FedAvg for both user and item aggregation underperforms

- **First 3 experiments**:
  1. Baseline comparison: Replicate Table 3/4 comparing Dist-FedAvg vs. FedAvg, SimpleAvg, FedMedian, FedAtt on ML-100k
  2. Alpha ablation: Test fixed α ∈ {0.1, 0.3, 0.5, 0.7, 0.9} vs. arithmetic decay vs. geometric decay; measure NDCG@10 impact
  3. Client selection sensitivity: Vary number of selected clients (10, 20, 50, 100) and plot NDCG@10 trend as in Figure 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can Dist-FedAvg be adapted to aggregate item embeddings using user-derived weights?
- Basis in paper: The conclusion states that "exploring the aggregation of item embeddings based on user-derived weights represents a promising direction for further research."
- Why unresolved: The current study focuses exclusively on user parameters because items are "relatively uniform," and experiments showed simpler methods worked better for items.
- What evidence would resolve it: A modified version of the algorithm that successfully applies inverse distance weighting to item embeddings, demonstrating improved accuracy over the current best-performing item aggregators (FedAvg/SimpleAvg).

### Open Question 2
- Question: How does Dist-FedAvg perform when fully integrated with strict privacy-preserving mechanisms like differential privacy?
- Basis in paper: The authors note they ignored privacy components like pseudo-item sampling to focus on aggregation, suggesting future work should "integrate and evaluate Dist-FedAvg within existing graph frameworks (e.g., FedPerGNN)."
- Why unresolved: The current evaluation uses a simplified framework without the noise injection typically required for privacy, which could degrade the quality of the distance metrics Dist-FedAvg relies upon.
- What evidence would resolve it: Empirical results showing Dist-FedAvg's stability and accuracy when subjected to the noise and perturbations of differential privacy or secure aggregation protocols.

### Open Question 3
- Question: Can this distance-based aggregation method be effectively extended to non-graph-based federated recommendation frameworks?
- Basis in paper: Section 6 explicitly proposes to "extend it to non-graph-based federated recommendation frameworks" as a primary avenue for future work.
- Why unresolved: The method is designed around "anchor users" and graph expansion concepts specific to GNNs, which may not translate directly to models like Matrix Factorization or NCF.
- What evidence would resolve it: A successful adaptation of the inverse distance weighting strategy to non-graph models that maintains the personalization benefits observed in the GNN setting.

## Limitations

- Alpha hyperparameter sensitivity creates ambiguity about when and how to use decay strategies, as the ablation study reveals decay doesn't always help
- User expansion implementation lacks specifics on how the third-party matching actually works in practice, despite referencing FedPerGNN
- The recommendation to use different aggregation methods for users vs. items is stated but not thoroughly validated through ablation studies

## Confidence

- **High confidence**: The core mechanism of inverse distance weighting for aggregation is well-specified with clear mathematical formulation (Equations 3-6). The empirical improvements over baselines are statistically significant across multiple datasets.
- **Medium confidence**: The anchor interpolation mechanism (Equation 7) is theoretically sound, but the optimal α values and decay strategy selection remain dataset-dependent with limited guidance.
- **Low confidence**: The paper's recommendation to use different aggregation methods for users vs. items (Dist-FedAvg for users, FedAvg/SimpleAvg for items) is stated but not thoroughly validated through ablation studies.

## Next Checks

1. **Alpha hyperparameter sweep**: Systematically test α values {0.1, 0.3, 0.5, 0.7, 0.9} and decay strategies on ML-100k to determine optimal configuration patterns across datasets.
2. **User-item aggregation consistency**: Compare Dist-FedAvg for both users and items vs. mixed methods (Dist-FedAvg users + FedAvg items) to validate the paper's recommendation.
3. **Early-round performance analysis**: Implement warmup rounds with α=1, then decay to evaluate the claim that early-round embeddings are unreliable and benefit from high anchor weight.