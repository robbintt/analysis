---
ver: rpa2
title: 'Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement
  Learning'
arxiv_id: '2502.00802'
source_url: https://arxiv.org/abs/2502.00802
tags:
- learning
- fgsf
- trace
- network
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fisher-Guided Selective Forgetting (FGSF) addresses the primacy
  bias in deep reinforcement learning by leveraging the Fisher Information Matrix
  to selectively modify network weights during training. The method identifies characteristic
  memorization and reorganization phases in the FIM trace evolution and applies periodic
  weight scrubbing to prevent early experiences from dominating the learning process.
---

# Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.00802
- Source URL: https://arxiv.org/abs/2502.00802
- Reference count: 40
- Achieves up to 50% improvement in mean return compared to baseline SAC in complex tasks like Humanoid

## Executive Summary
Fisher-Guided Selective Forgetting (FGSF) is a novel method that addresses the primacy bias in deep reinforcement learning by leveraging the Fisher Information Matrix (FIM) to selectively modify network weights during training. The approach identifies characteristic memorization and reorganization phases in the FIM trace evolution and applies periodic weight scrubbing to prevent early experiences from dominating the learning process. FGSF demonstrates superior performance and stability across DeepMind Control Suite environments while maintaining efficient learning dynamics.

## Method Summary
FGSF builds upon Soft Actor-Critic (SAC) by incorporating Fisher Information Matrix analysis to guide selective forgetting. The method computes FIM traces during training to detect memorization versus reorganization phases, then applies weight scrubbing based on a forgetting rate parameter. When FIM traces indicate stabilization, the system restarts with modified weights to prevent primacy bias. This selective modification of network weights based on their importance allows the agent to maintain performance while avoiding over-reliance on early experiences.

## Key Results
- Achieves up to 50% improvement in mean return compared to baseline SAC in complex tasks
- Humanoid environment: 136.6 ± 14.4 vs 68.5 ± 21.9 for baseline SAC
- Demonstrates superior performance and stability across DeepMind Control Suite environments

## Why This Works (Mechanism)
FGSF works by leveraging the Fisher Information Matrix to identify which weights have become overly reliant on early experiences and need to be modified. The FIM traces reveal patterns of memorization (when weights become fixed) versus reorganization (when the network adapts to new experiences). By detecting these phases and applying selective forgetting during reorganization periods, the method prevents the network from becoming locked into suboptimal early behaviors while maintaining the benefits of successful early learning.

## Foundational Learning

- Fisher Information Matrix: Measures the sensitivity of model parameters to data changes; needed to identify which weights are most affected by early experiences; quick check: compute FIM for a simple neural network on sample data
- Primacy bias in RL: Tendency for early experiences to disproportionately influence learning; needed to understand the problem being solved; quick check: train RL agent on sequential tasks and observe performance degradation
- Weight importance ranking: Prioritizing parameter modifications based on their contribution to performance; needed to selectively modify weights; quick check: perturb different weights and measure impact on loss
- Experience replay buffers: Storing and sampling past experiences; needed as context for understanding memory-based approaches; quick check: implement basic replay buffer and observe learning stability
- Soft Actor-Critic architecture: Baseline RL algorithm; needed to understand FGSF's improvements; quick check: implement SAC and verify performance on simple control task

## Architecture Onboarding

Component map: SAC agent -> FIM computation module -> Weight importance ranking -> Selective forgetting mechanism -> Updated weights -> Improved learning stability

Critical path: Experience collection → SAC learning → FIM trace computation → Phase detection → Weight scrubbing → Network restart → Improved policy

Design tradeoffs: FGSF trades increased computational overhead for better long-term learning performance and reduced primacy bias. The method requires periodic FIM computations but provides more stable learning across diverse environments.

Failure signatures: If FGSF fails, potential causes include: incorrect FIM trace interpretation, forgetting rate set too high/low, phase detection threshold misconfigured, or computational overhead overwhelming training efficiency.

First experiments:
1. Implement FGSF on a simple control task (CartPole) and verify FIM traces show expected memorization/reorganization patterns
2. Compare FGSF performance against SAC on a medium-difficulty task (HalfCheetah) with different forgetting rates
3. Test FGSF's robustness to noisy observations by adding varying levels of observation noise to the Walker environment

## Open Questions the Paper Calls Out
None

## Limitations
- Method relies on specific forgetting rate (0.5) and restart threshold (2.0) that may not generalize across all environments
- Assumes FIM traces reliably indicate memorization versus reorganization phases across different architectures
- Computational overhead of periodic FIM computation not fully characterized for scalability

## Confidence
- High confidence in mathematical formulation of FGSF and its ability to modify network weights based on Fisher Information
- Medium confidence in empirical results showing improved performance on tested environments
- Low confidence in generality of FIM trace-based phase identification across diverse RL scenarios

## Next Checks
1. Test FGSF across a broader range of RL domains including discrete action spaces, sparse reward environments, and Atari benchmark tasks to assess generalizability
2. Conduct ablation studies varying the forgetting rate and restart threshold parameters to understand their impact on different task complexities and network architectures
3. Measure and report the computational overhead introduced by FIM trace computation and compare learning efficiency against other memory-based RL methods like ER and MERLIN