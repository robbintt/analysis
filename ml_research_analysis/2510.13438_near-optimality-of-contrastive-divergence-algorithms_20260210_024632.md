---
ver: rpa2
title: Near-Optimality of Contrastive Divergence Algorithms
arxiv_id: '2510.13438'
source_url: https://arxiv.org/abs/2510.13438
tags:
- sgdw
- init
- logz
- have
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the statistical efficiency of the Contrastive\
  \ Divergence (CD) algorithm for training unnormalized models. While prior work established\
  \ an O(n\u207B\xB9/\xB3) asymptotic convergence rate, this paper proves that CD\
  \ can achieve the parametric O(n\u207B\xB9/\xB2) rate under certain regularity conditions."
---

# Near-Optimality of Contrastive Divergence Algorithms

## Quick Facts
- arXiv ID: 2510.13438
- Source URL: https://arxiv.org/abs/2510.13438
- Reference count: 40
- This paper proves Contrastive Divergence can achieve parametric O(n⁻¹/²) convergence rates under certain regularity conditions.

## Executive Summary
This paper establishes near-optimal statistical efficiency for Contrastive Divergence (CD) algorithms in training unnormalized models. The authors prove that CD can achieve the parametric O(n⁻¹/²) convergence rate in online settings and a near-parametric rate in offline settings, significantly improving upon the previously established O(n⁻¹/³) rate. The analysis shows that averaging CD iterates yields estimators with asymptotic variance within a factor of 4 of the Cramér-Rao lower bound. The work provides non-asymptotic convergence guarantees and relaxes assumptions about the Markov kernels used in the algorithm.

## Method Summary
The paper analyzes two variants of Contrastive Divergence: online CD using fresh data samples and offline CD reusing the same dataset. Both algorithms use finite-step Markov Chain Monte Carlo (MCMC) to approximate gradients of the log-likelihood. The key insight is that strong convexity of the negative log-likelihood in exponential families creates a drift that compensates for the bias introduced by finite MCMC steps. For online settings, this enables O(n⁻¹/²) convergence. For offline settings, the authors bound the correlation between model parameters and training data to achieve a near-parametric rate. Polyak-Ruppert averaging of iterates is shown to reduce variance to near-optimal levels.

## Key Results
- Online CD achieves parametric O(n⁻¹/²) convergence rate under regularity conditions
- Offline CD achieves near-parametric rate with log factors
- Averaged CD iterates have asymptotic variance within 4× of Cramér-Rao lower bound
- Non-asymptotic convergence guarantees provided for both online and offline settings

## Why This Works (Mechanism)

### Mechanism 1: Bias Compensation via Strong Convexity
Online Contrastive Divergence can achieve O(n⁻¹/²) rate when mixing time is sufficient. The CD gradient introduces bias scaling as αᵐ, but strong convexity creates drift toward true parameter that overpowers this bias. Core assumption: convex/compact parameter space and restricted spectral gap condition. Evidence: Theorem 3.2 requires μ̃ₘ > 0; Abstract states parametric rate under regularity assumptions.

### Mechanism 2: Data-Iterate Correlation Control in Offline Settings
Offline CD achieves near-parametric rate by controlling correlation between parameters and training data. The proof bounds L² deviation between actual offline update and ideal fresh-sample path using tail probability decomposition. Core assumption: sub-exponential tails or specific mixing properties. Evidence: Section 4.2.2 Step 1 controls Δ(ψₜ,₁ˢᵍᵈʷ); Theorem 4.2 bounds error using √log n factors.

### Mechanism 3: Variance Reduction via Polyak-Ruppert Averaging
Averaging CD iterates yields near-optimal asymptotic variance within factor of 4 of Cramér-Rao bound. Standard CD suffers from MCMC sampling noise, which averaging smooths out. Core assumption: learning rate schedule ηₜ = Ct⁻ᵝ with β ∈ (1/2, 1]. Evidence: Theorem 3.3 shows variance matches CRLB up to factor 4; Section 3.2.2 describes averaging as "surprisingly effective."

## Foundational Learning

- Concept: **Exponential Families & Convexity**
  - Why needed here: Strong convexity of negative log-likelihood is essential for the convergence analysis
  - Quick check question: Can you explain why the log-partition function log Z(ψ) guarantees convexity in the natural parameter ψ?

- Concept: **Markov Chain Mixing Time & Spectral Gap**
  - Why needed here: Assumption A3 dictates MCMC convergence speed and directly impacts CD rate through αᵐ relationship
  - Quick check question: If spectral gap α is close to 1 (slow mixing), how does that affect required number of steps m to ensure μ̃ₘ > 0?

- Concept: **Biased Stochastic Approximation**
  - Why needed here: CD is SGD with biased gradients due to finite MCMC; distinguishing bias from variance is crucial
  - Quick check question: Why does the "bias" term shrink as the iterate approaches the data distribution, as noted in prior work?

## Architecture Onboarding

- Component map: Data samples → MCMC Kernel → Gradient Estimator → Optimizer → Parameter Update → (Optional) Averager
- Critical path: The MCMC Kernel Application is the computational bottleneck and theoretical pivot point
- Design tradeoffs:
  - Online vs. Offline: Online gives cleaner O(n⁻¹/²) convergence but needs fresh data streams; Offline has correlations but allows data reuse
  - MCMC Steps (m): Increasing m reduces bias but increases compute cost linearly
  - Learning Rate (β): β < 1/2 may fail convergence; β = 1 is robust but potentially slower initially
- Failure signatures:
  - Divergence or Plateauing: Likely insufficient MCMC steps m violating μ̃ₘ > 0
  - High Variance in Final Estimate: Occurs without averaging or with poor spectral gap (model misspecification)
- First 3 experiments:
  1. Mixer Validation: Run CD on synthetic Gaussian model with varying m, plot distance to ψ* vs. n to confirm phase transition from O(n⁻¹/³) to O(n⁻¹/²)
  2. Averaging Check: Compare variance of final iterate ψₙ against averaged iterate ψ̄ₙ, verify tracking near theoretical Fisher Information bounds
  3. Offline Correlation Test: Run Offline CD with replacement vs. without replacement, plot tail error bounds to check alignment with √log n/n predictions

## Open Questions the Paper Calls Out

### Open Question 1
Can O(n⁻¹/²) rates and near-optimal variance results extend to general unnormalized energy-based models beyond exponential families? The analysis relies on properties specific to exponential families like analyticity of log-partition function. Evidence would be convergence proof for CD on general energy-based models or counter-example showing non-linearity prevents these rates.

### Open Question 2
Can non-asymptotic convergence guarantees be established for CD on distributions lacking uniform spectral gap, such as heavy-tailed or highly multimodal distributions? The bounds depend on mixing rate α < 1, which may vanish for heavy tails or high barriers between modes. Evidence would be bounds depending on weaker mixing properties or consistent convergence on excluded heavy-tailed distributions.

### Open Question 3
Can offline CD with reshuffling (SGDo) be proven to converge faster than with-replacement variant (SGDw), similar to standard SGD improvements? Analyzing SGDo requires jointly controlling correlations across minibatches and data reuse. Evidence would be theoretical bound for SGDo demonstrating faster dependency on epochs T compared to SGDw bounds.

### Open Question 4
Is the multiplicative factor of 4 in asymptotic variance bound tight, or can analysis be refined to achieve exact Cramér-Rao lower bound? The factor arises from specific inequalities used to bound bias and variance terms. Evidence would be refined analysis reducing constant to 1 or simulation showing empirical variance consistently at 4× Fisher information.

## Limitations
- Analysis relies heavily on strong convexity and restricted spectral gap assumptions that may not hold for all unnormalized models
- Computational tradeoff for MCMC steps isn't fully characterized - gap between theoretical conditions and practical hyperparameter selection
- Sub-exponential tail assumption for offline settings may not cover heavy-tailed data distributions common in real applications

## Confidence
- Online CD achieving O(n⁻¹/²) rate: High confidence - well-established through stochastic approximation theory
- Offline CD achieving near-parametric rate: Medium confidence - theoretical framework sound but correlation control requires careful verification
- Near-optimal variance via averaging: High confidence - Polyak-Ruppert averaging is well-understood technique

## Next Checks
1. Hyperparameter Phase Transition: Systematically vary MCMC steps m in Gaussian exponential family example to empirically identify threshold where convergence rate shifts from O(n⁻¹/³) to O(n⁻¹/²)

2. Spectral Gap Verification: For restricted Boltzmann machine, compute or estimate spectral gap α of Gibbs sampler to verify restricted spectral gap assumption holds in practice

3. Offline Correlation Analysis: Implement both with-replacement and without-replacement sampling for offline CD, measuring empirical correlation between model parameters and data as predicted by tail probability bounds