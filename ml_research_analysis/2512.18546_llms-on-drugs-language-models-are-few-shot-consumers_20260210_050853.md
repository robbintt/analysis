---
ver: rpa2
title: 'LLMs on Drugs: Language Models Are Few-Shot Consumers'
arxiv_id: '2512.18546'
source_url: https://arxiv.org/abs/2512.18546
tags:
- arxiv
- language
- prompt
- preprint
- persona
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "A controlled experiment tested how persona prompts (\u201Con LSD\u201D\
  , \u201Con cocaine\u201D, \u201Con alcohol\u201D, \u201Con cannabis\u201D) affect\
  \ LLM accuracy on 100 ARC-Challenge items. With a sober control at 45 % accuracy,\
  \ alcohol collapsed performance to 10 % (p = 3.2e-8), cocaine to 21 % (p = 4.9e-4),\
  \ LSD to 19 % (p = 1.3e-4), and cannabis to 30 % (p = 0.041)."
---

# LLMs on Drugs: Language Models Are Few-Shot Consumers

## Quick Facts
- arXiv ID: 2512.18546
- Source URL: https://arxiv.org/abs/2512.18546
- Authors: Alexander Doudkin
- Reference count: 34
- Primary result: Persona prompts ("on LSD", "on cocaine", etc.) reduced GPT-5-mini accuracy on ARC-Challenge from 45% (control) to as low as 10% (alcohol) by breaking required output formatting.

## Executive Summary
This study demonstrates that single-sentence persona prompts simulating psychoactive states can override instruction-following learned during model training, functioning as a "few-shot consumable" that degrades reliability without touching model weights. The key finding is that accuracy collapse is predominantly caused by template omission—models trail off mid-response or skip the final "Answer: <LETTER>" line—rather than reasoning degradation. This reveals that instruction compliance is context-dependent rather than rigidly enforced.

## Method Summary
The experiment tested GPT-5-mini on 100 ARC-Challenge validation items per condition, cycling through five persona prompts: sober, LSD, cocaine, alcohol, and cannabis. Each prompt was prepended to a neutral system instruction enforcing the "Answer: <LETTER>" format, with temperature=0 and 300-token cap. Outputs were parsed for the answer letter, with missing predictions counted as incorrect. Wilson confidence intervals and Fisher exact tests compared each persona condition to the sober control. All code, data, and analysis are open-sourced.

## Key Results
- Alcohol persona collapsed accuracy to 10% (p = 3.2e-8), cocaine to 21% (p = 4.9e-4), LSD to 19% (p = 1.3e-4), and cannabis to 30% (p = 0.041)
- Missing predictions: Alcohol=60/100, Cocaine=55/100, LSD=53/100, Cannabis=38/100, Control=21/100
- Failures were primarily due to format non-compliance, not reasoning errors
- The paper demonstrates that persona text can override instruction-following without modifying model weights

## Why This Works (Mechanism)

### Mechanism 1: Persona Context Override of Instruction-Following
Single-sentence persona cues can suppress instruction-tuned formatting behaviors without modifying model weights. Psychoactive persona prompts inject context that signals relaxed compliance norms, which the model interprets as overriding the system-level formatting instruction. The model's instruction-following is context-dependent rather than rigidly enforced.

### Mechanism 2: Format Compliance Failure as Primary Accuracy Driver
Accuracy collapse is predominantly caused by template omission, not reasoning degradation. The persona framings cause the model to trail off mid-response or skip the final "Answer: <LETTER>" line. The automatic grading pipeline treats missing letters as incorrect regardless of intermediate reasoning quality.

### Mechanism 3: Token Budget Reallocation via Style Modulation
Different personas shift response style and length, reallocating computational budget away from structured output. Expansive personas (cannabis, LSD) trigger longer, metaphorical reasoning that consumes tokens without proportional accuracy gains. Alcohol triggers conversational drift that causes mid-thought truncation.

## Foundational Learning

- **Concept: Instruction Tuning and Compliance**
  - Why needed here: The paper's central claim is that persona prompts override instruction-following behaviors. Understanding what instruction tuning establishes (and its limitations) is essential to interpret why a single sentence can disrupt learned formatting.
  - Quick check question: Can you explain why an instruction-tuned model might still fail to follow a formatting constraint when given conflicting context?

- **Concept: Prompt Sensitivity and Brittleness**
  - Why needed here: The study design relies on the established phenomenon that small prompt changes cause large behavioral shifts. This frames why persona interventions merit systematic evaluation.
  - Quick check question: What does it imply for deployment reliability if a one-sentence persona change can reduce accuracy from 0.45 to 0.10?

- **Concept: Template-Based Evaluation and Its Failure Modes**
  - Why needed here: The accuracy metric conflates reasoning correctness with format compliance. Understanding this distinction is critical for interpreting whether persona effects reflect capability loss or output formatting drift.
  - Quick check question: If a model produces correct reasoning but omits "Answer: X", should this be scored as incorrect? What tradeoffs does this create for benchmark interpretation?

## Architecture Onboarding

- **Component map:**
  - src/run_benchmark.py -> src/analyze_results.py -> src/make_figures.py
  - OpenAI Responses API with credentials loaded via python-dotenv

- **Critical path:**
  1. Shuffle ARC-Challenge validation items with seed 13 → downsample to 100 per condition
  2. Prepend neutral system instruction enforcing "Answer: <LETTER>" contract
  3. Attach persona prefix (sober/LSD/cocaine/alcohol/cannabis)
  4. Call API sequentially per condition (avoids interleaving randomness)
  5. Parse output for letter extraction; treat missing as incorrect
  6. Compute Wilson CIs and Fisher exact tests vs. control

- **Design tradeoffs:**
  - 100 samples per condition bounds API costs but yields wider confidence intervals; authors note future work should scale to hundreds
  - Sequential condition cycling avoids transport-layer randomness but increases total runtime
  - Deterministic decoding (temp=0) ensures reproducibility but may underrepresent persona-induced variance
  - Missing-letter-as-incorrect is standard for multiple-choice evaluation but conflates format failure with reasoning failure

- **Failure signatures:**
  - High missing-prediction count (>50% of trials) indicates severe template disruption
  - Alcohol persona shows 60/100 missing predictions with mid-thought trailing—signature of conversational drift
  - Latency spikes + long responses + missing answer = style modulation without compliance (cannabis/LSD pattern)

- **First 3 experiments:**
  1. **Format reinforcement ablation**: Repeat all conditions with an additional post-persona reminder ("Remember to output your final answer as 'Answer: <LETTER>'"). Test whether explicit reinforcement recovers compliance.
  2. **Semantic grading comparison**: Use a secondary LLM to extract answers from intermediate reasoning when template is missing. Compare semantic accuracy vs. template-based accuracy to quantify the format-vs-reasoning contribution.
  3. **Token budget extension**: Increase token cap from 300 to 500 for alcohol/cannabis conditions to test whether trailing-off is a budget artifact or a persona-induced generation pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised finetuning inoculate models against context-level persona intoxication effects?
- Basis in paper: Authors explicitly call for testing "whether supervised finetuning can inoculate models against context-level intoxication" in Section 7.
- Why unresolved: The study only tests inference-time interventions on a pretrained model without any targeted training to resist persona-induced degradation.
- What evidence would resolve it: Fine-tune GPT-5-mini or similar models on data emphasizing format compliance under adversarial persona conditions, then re-run the benchmark harness.

### Open Question 2
- Question: How do psychoactive persona prompts interact with chain-of-thought prompting and few-shot examples?
- Basis in paper: Section 7 states that "investigating the interaction between persona prompts and other prompt engineering techniques (e.g., chain-of-thought, few-shot examples) could yield insights into prompt composability."
- Why unresolved: The current study uses single-sentence persona prefixes in isolation, without combining them with other established prompting strategies.
- What evidence would resolve it: A factorial design crossing persona conditions with CoT/few-shot manipulations, measuring both accuracy and format compliance.

### Open Question 3
- Question: Would explicit formatting reminders mitigate the template compliance failures observed under psychoactive personas?
- Basis in paper: The paper notes "the alcohol effect might diminish with larger samples or with explicit reminders to comply with formatting," but this was not tested.
- Why unresolved: The current design prepends a neutral system instruction once; no reinforcement of formatting requirements occurs after the persona prompt.
- What evidence would resolve it: Add post-persona reminders such as "Remember to end with Answer: <LETTER>" and compare missing prediction rates against the baseline conditions.

### Open Question 4
- Question: Do these persona degradation effects generalize across different model architectures, sizes, and languages?
- Basis in paper: The study acknowledges being "intentionally narrow: one model, 100 ARC items per condition, English prompts, and no human raters," and calls for multilingual/cultural extensions.
- Why unresolved: Only GPT-5-mini was tested; it remains unknown whether larger models or different architectures exhibit similar vulnerability to persona-induced format collapse.
- What evidence would resolve it: Replicate the benchmark across multiple model families (e.g., Claude, Gemini, open-source LLMs) and multilingual prompt variants.

## Limitations
- 100-sample per condition size yields wide confidence intervals and may miss subtle persona effects
- Exact persona prompt wording not specified, only described thematically
- Analysis conflates reasoning correctness with format adherence, potentially overestimating persona-induced capability loss
- Mechanism claims rely heavily on format compliance failure with minimal direct evidence on token-budget reallocation

## Confidence

- **High confidence**: Format compliance as the dominant failure mode (supported by missing-prediction counts scaling with persona intensity)
- **Medium confidence**: Persona context override of instruction-following (mechanistic, needs direct behavioral testing)
- **Low confidence**: Token budget reallocation via style modulation (minimal direct evidence, speculative)

## Next Checks

1. **Format Enforcement Ablation**: Repeat all conditions with explicit post-persona reminders to output "Answer: <LETTER>". If accuracy recovers, this validates persona-induced compliance override.
2. **Semantic Grading Comparison**: Apply secondary LLM extraction of answers from intermediate reasoning when template is missing. This quantifies how much accuracy loss is format vs. reasoning failure.
3. **Token Budget Extension**: Increase token caps for alcohol/cannabis conditions to test whether trailing-off is a budget artifact or a persona-induced generation pattern.