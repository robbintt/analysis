---
ver: rpa2
title: 'ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting'
arxiv_id: '2509.01997'
source_url: https://arxiv.org/abs/2509.01997
tags:
- graph
- future
- learning
- forecasting
- order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ACA-Net is proposed to address logistical demand-supply forecasting
  in on-demand food delivery platforms. The method introduces ongoing and global graphs
  to replace traditional long time series graphs, effectively capturing future order
  distribution information.
---

# ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting

## Quick Facts
- arXiv ID: 2509.01997
- Source URL: https://arxiv.org/abs/2509.01997
- Reference count: 16
- Primary result: Achieves MAE of 126.3 on demand-supply forecasting using only two graphs (ongoing and global) instead of long time series

## Executive Summary
ACA-Net addresses logistical demand-supply forecasting in on-demand food delivery platforms by replacing traditional long time series inputs with two graphs: an ongoing graph representing real-time order conditions and a global graph containing historical statistics. This dual-graph approach effectively captures future order distribution information while significantly reducing input complexity. The method employs adaptive future graph learning and cross-attention mechanisms to extract relationships between ongoing and global graphs, and to learn the influence of supply and environment factors.

## Method Summary
ACA-Net predicts logistical demand-supply pressure (average delivery time in next 5 minutes) using a four-component architecture: (1) GNN-based graph embedding layers that process both ongoing and global graphs, (2) a cross-attention encoder with inter-graph CAT to fuse ongoing and global information and influence-learning CAT to inject supply/environment features, (3) adaptive graph learning that generates a future adjacency matrix with supervised loss against ground truth, and (4) a pre-trained simulation model that takes the learned future graph and features to predict pressure. The model is trained end-to-end with a combined loss function balancing pressure prediction and graph structure learning.

## Key Results
- Achieves state-of-the-art MAE of 126.3, outperforming existing approaches
- Reduces input bytes by ~90% (from 1.5×10^7 to 1.5×10^6) compared to STGNN methods
- Successfully deployed in production environments with improved forecasting accuracy and convergence speed
- Uses only two input time slices versus the multiple slices required by traditional methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing long historical time sequences with dual-graph input improves efficiency and captures time-series-insensitive nature of food delivery demand
- Mechanism: The architecture uses Ongoing Graph (real-time order conditions) and Global Graph (historical statistics) instead of T time slices, reducing input complexity while maintaining coverage of possible order distributions
- Core assumption: Future order distribution is a random process weakly correlated with immediate long-term history
- Evidence anchors: Abstract states "utilizes only two graphs... superior performance compared to traditional spatial-temporal long-series methods"; Page 2 states "learning future order distribution... is a time-series-insensitive problem with strong randomness"; Table 1 shows input bytes reduced from 1.5×10^7 to 1.5×10^6 while MAE improves
- Break condition: If demand patterns shift rapidly in a way that depends strictly on the sequence of the last 30-60 minutes, this snapshot approach may fail to capture trend information

### Mechanism 2
- Claim: Cross-attention mechanisms effectively fuse static global knowledge with dynamic ongoing states
- Mechanism: Inter-graph Cross Attention Transformer uses ongoing graph embeddings as Query and global graph embeddings as Key/Value to retrieve relevant historical patterns based on current context
- Core assumption: The relationship between current conditions and historical patterns requires dynamic weighting rather than simple concatenation
- Evidence anchors: Page 5 states "inter-graph cross attention to derive the future order information from ongoing and global graph"; Page 6 states "inter-graph cross attention computes the corresponding weights from the ongoing graph to the global graph"
- Break condition: If Global graph is sparse or unrepresentative of Ongoing situation, attention mechanism will retrieve irrelevant or noisy patterns

### Mechanism 3
- Claim: Supervised adaptive graph learning ensures generated future graph is reliable by explicitly penalizing structural deviations from ground truth
- Mechanism: Explicitly generates Future Adjacency Matrix from node features and calculates graph loss comparing learned matrix to actual future order graph, acting as auxiliary task to regularize the model
- Core assumption: Forcing model to predict graph structure before predicting pressure improves feature robustness
- Evidence anchors: Page 3 states "A supervised adaptive graph learning component ensures the reliability of the learned future graph"; Pages 7-8 detail the loss function L_graph used to constrain difference between learned adjacency matrix and ground truth
- Break condition: If Ground Truth future graph is unavailable during training, mechanism cannot enforce reliability, potentially leading to hallucinated graph structures

## Foundational Learning

- Concept: **Cross-Attention Mechanism**
  - Why needed here: This is the engine of the CAT block, allowing the model to "ask" the global graph for information based on the "query" of the ongoing graph
  - Quick check question: If you swap the Query and Key inputs in the Inter-graph CAT, what semantic relationship would the model then be learning?

- Concept: **Adaptive Graph Learning (Adjacency Matrix Generation)**
  - Why needed here: The model constructs its own future graph rather than receiving a pre-defined one, deriving normalized adjacency matrix from inner product of node features
  - Quick check question: Why is the ReLU activation applied before SoftMax when generating the adjacency matrix (Eq. 7)?

- Concept: **Auxiliary Supervision / Multi-Task Learning**
  - Why needed here: The model optimizes two losses (Pressure prediction and Graph structure), with λ balancing these for training stability
  - Quick check question: If λ is set to 0, removing the graph loss, how might the model's prediction of pressure change compared to the proposed method?

## Architecture Onboarding

- Component map: Input Graphs (Ongoing + Global) → GNN Embedding → Inter-graph CAT (Ongoing as Query, Global as Key/Value) → Influence-learning CAT (Supply/Env Features as Query, Inter-graph Output as Key/Value) → Adaptive Graph Learning (Future Adjacency Matrix) → Pressure Inferencing (Pre-trained Simulation Model)
- Critical path: The transformation from raw Ongoing/Global embeddings → Inter-graph CAT output → Influence-learning CAT output → Future Adjacency Matrix (A_future). This path determines the quality of the final prediction.
- Design tradeoffs:
  - Efficiency vs. Temporal Nuance: By using only 2 graphs, input size drops ~90% (Table 1), but the model discards the fine-grained temporal sequences used in STGNNs. This works only if demand is truly "time-series-insensitive."
  - Latent vs. Explicit Structure: The model forces an explicit graph structure (A_future) rather than keeping relations purely latent. This adds interpretability but requires ground truth graph data for training.
- Failure signatures:
  - Missing Ground Truth: If A_truth (future order connections) cannot be constructed for a specific region/time, the L_graph component cannot be computed.
  - Distribution Shift: If Ongoing graph contains nodes (AOIs) not present in Global graph (Definition 2 says V' ⊆ V, but new merchant clusters might form), the Inter-graph CAT will fail to find matching Keys.
- First 3 experiments:
  1. Overfit Single District: Take data for one business district, train the model and verify it can reach near-zero L_graph and L_P to validate data pipeline and capacity.
  2. Ablate Input Type: Run the model with only the Ongoing graph and only the Global graph to quantify the contribution of each (referencing Table 2 logic).
  3. Visualize Adjacency: For a test sample, visualize the learned A_future vs. the Ground Truth A_truth (similar to Fig 4e). Check if the model is capturing the topology (edges) correctly or just the magnitude.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content and context, several important unresolved issues emerge regarding the approach's generalizability, sensitivity to hyperparameters, and limitations in different operational contexts.

## Limitations
- The "time-series-insensitive" assumption may not generalize to platforms with different order dynamics or geographic constraints
- Effectiveness depends on availability and quality of ground truth future graph structures, which may not be available in all operational contexts
- Specific GNN architecture and simulation model details are not fully specified, creating potential reproducibility gaps

## Confidence
- High confidence: The dual-graph approach reduces input complexity while maintaining accuracy (supported by Table 1 showing 90% input reduction with performance gains)
- Medium confidence: Cross-attention mechanisms effectively fuse ongoing and global information (supported by ablation studies but requires deeper architectural verification)
- Medium confidence: Supervised graph learning improves reliability (supported by the design but depends on ground truth availability)

## Next Checks
1. Test the model's robustness to distribution shifts by training on one city/region and evaluating on another with different demand patterns
2. Conduct ablation studies removing the supervised graph learning component to quantify its contribution to overall performance
3. Implement sensitivity analysis for the λ parameter balancing pressure and graph losses to determine optimal values across different operational scenarios