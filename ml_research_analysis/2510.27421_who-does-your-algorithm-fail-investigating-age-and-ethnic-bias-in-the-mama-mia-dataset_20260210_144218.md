---
ver: rpa2
title: Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA
  Dataset
arxiv_id: '2510.27421'
source_url: https://arxiv.org/abs/2510.27421
tags:
- segmentation
- bias
- data
- performance
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study audits fairness in breast tumor segmentation models
  using the MAMA-MIA dataset, revealing significant age-related bias against younger
  patients and ethnicity-based disparities that are masked when data is aggregated
  across sources. The authors evaluate automated segmentation quality using Dice score
  and Hausdorff Distance, finding that segmentation quality improves with age (baseline
  OLS regression shows R2 = 0.0104 for Dice score, p=0.0001).
---

# Who Does Your Algorithm Fail? Investigating Age and Ethnic Bias in the MAMA-MIA Dataset

## Quick Facts
- **arXiv ID:** 2510.27421
- **Source URL:** https://arxiv.org/abs/2510.27421
- **Reference count:** 7
- **Primary result:** Automated breast tumor segmentation shows significant age-related bias against younger patients that persists even after balanced training, while ethnicity-based disparities are masked by data aggregation but reveal 10% performance gaps when disaggregated by source.

## Executive Summary
This study audits fairness in breast tumor segmentation models using the MAMA-MIA dataset, revealing significant age-related bias against younger patients and ethnicity-based disparities that are masked when data is aggregated across sources. The authors evaluate automated segmentation quality using Dice score and Hausdorff Distance, finding that segmentation quality improves with age (baseline OLS regression shows R² = 0.0104 for Dice score, p=0.0001). Critically, this age-related bias persists even after controlling for data source and after training on an age-balanced cohort, suggesting it is intrinsic to the segmentation task. The study also shows that ethnicity-based disparities vary significantly by data source - while global analysis suggests minimal disparity, disaggregation reveals site-specific ethnic bias with Dice score performance gaps reaching 10% in specific cohorts.

## Method Summary
The authors evaluate fairness in automated breast tumor segmentation by comparing silver labels (automated model outputs) against gold labels (expert radiologist annotations) from the MAMA-MIA dataset. They compute Dice score and 95th percentile Hausdorff Distance per case, stratify results by age and ethnicity subgroups, and perform OLS regression to quantify performance disparities. The analysis includes global versus source-disaggregated comparisons and a controlled experiment training nnU-Net on an age-balanced cohort to isolate intrinsic bias from representational imbalance.

## Key Results
- Age-related performance disparities persist even after controlling for data source (p=1.6×10⁻⁸) and balanced training, indicating intrinsic bias against younger patients
- Global ethnic disparity analysis shows minimal differences, but disaggregation reveals site-specific gaps reaching 10% in specific cohorts
- Dice score and Hausdorff Distance show fundamentally different bias patterns - Dice disparities are largely intrinsic while HD95 disparities are source-confounded

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Age-related performance disparities in breast tumor segmentation are intrinsic and persist independent of representational imbalance or data source.
- Mechanism: Younger patients (<40) have denser breast tissue, making tumor boundaries harder to distinguish from surrounding parenchyma. This physiological confounder degrades segmentation quality for both human experts and models. The model learns suboptimal boundary representations for this subgroup because the underlying signal is genuinely noisier.
- Core assumption: Breast tissue density correlates inversely with age and directly impacts segmentation difficulty.
- Evidence anchors: [abstract] "intrinsic age-related bias against younger patients that continues to persist even after controlling for confounding factors, such as data source"; [section 3] "This points towards an intrinsic bias" after ANOVA confirmed age effect remained significant (p=1.6×10⁻⁸) even after adjusting for DataSource; [corpus] Weak direct evidence—neighboring papers address bias but not this specific physiological mechanism
- Break condition: If tissue density annotations become available and show no correlation with performance gaps within age groups, the mechanism is confounded.

### Mechanism 2
- Claim: Multi-center data aggregation masks site-specific ethnic bias by averaging across heterogeneous institutional contexts.
- Mechanism: Different clinical sites serve demographically distinct populations with different imaging protocols, scanner characteristics, and annotation practices. When aggregated, these site-specific biases cancel or dilute each other. Disaggregation reveals that ethnic subgroups experience dramatically different model performance depending on where they are treated.
- Core assumption: Site-specific factors (protocol, equipment, annotation quality) interact with ethnicity to produce heterogeneous performance patterns.
- Evidence anchors: [abstract] "ethnicity-based disparities vary significantly by data source - while global analysis suggests minimal disparity, disaggregation reveals site-specific ethnic bias with Dice score performance gaps reaching 10%"; [section 3] "while the global DPD in Dice scores was only 3.0%, it amplified to 10.0% within the ISPY2 cohort"; [corpus] No corpus papers examine this aggregation-masking mechanism directly
- Break condition: If site-stratified analysis shows uniform ethnic performance gaps across all sites, aggregation is not masking but reflecting consistent bias.

### Mechanism 3
- Claim: Choice of evaluation metric determines whether detected bias appears intrinsic or confounded.
- Mechanism: Dice score measures volumetric overlap (region-level accuracy), while Hausdorff Distance measures boundary precision (surface-level accuracy). Volumetric errors for ethnic subgroups persist after controlling for data source (6.2% effect reduction), but boundary errors collapse (64% effect reduction), suggesting models produce systematically different error types for different populations.
- Core assumption: Intrinsic bias manifests in region-level errors while protocol-related confounders primarily affect boundary precision.
- Evidence anchors: [section 3] "For the Dice score... adjusting for the data source only reduced its effect size by 6.2%. In contrast, for the HD95 score... adjusting for the data source reduced its effect size by a substantial 64.0%"; [section 3] "This divergence suggests the model produces different types of segmentation errors for certain ethnic groups"; [corpus] No corpus papers investigate metric-dependent bias characterization
- Break condition: If alternative segmentation models show reversed patterns (intrinsic HD95 bias, confounded Dice), the mechanism is model-specific rather than task-intrinsic.

## Foundational Learning

- Concept: **Dice Score vs. Hausdorff Distance as complementary fairness probes**
  - Why needed here: These metrics capture fundamentally different failure modes; relying on one masks bias detectable by the other.
  - Quick check question: If your model achieves 0.90 Dice but 15mm HD95 on a subgroup, what type of error pattern does this suggest?

- Concept: **Fairness Under Unawareness paradigm**
  - Why needed here: The auditing framework explicitly evaluates models that were trained without access to protected attributes, reflecting realistic deployment where demographic data may be unavailable.
  - Quick check question: Why might a model exhibit bias against a subgroup it was never explicitly told about?

- Concept: **Simpson's Paradox in multi-site medical AI**
  - Why needed here: Aggregated fairness metrics can show equity while every individual site exhibits disparity—this drives the "masked bias" finding.
  - Quick check question: If Site A shows +5% gap favoring Group X and Site B shows -5% gap favoring Group Y, what will aggregated analysis show?

## Architecture Onboarding

- Component map: MAMA-MIA dataset (1,506 DCE-MRI cases from 4 cohorts) -> Silver vs Gold labels comparison -> Dice/HD95 computation -> Demographic stratification -> OLS regression -> Source-disaggregated analysis -> Balanced training experiment

- Critical path:
  1. Load silver labels and gold labels for comparison
  2. Compute Dice and HD95 per case
  3. Stratify by age/ethnicity → compute subgroup statistics
  4. Run OLS regression (Performance ~ AgeGroup + DataSource)
  5. Disaggregate by source → compare global vs. site-specific DPD
  6. Run controlled experiment on balanced cohort to isolate intrinsic bias

- Design tradeoffs:
  - Discretizing age bins improves interpretability but loses within-bin variance
  - Downsampling majority groups for balanced training reduces statistical power
  - Using only gold labels for retraining ensures clean signal but ignores real-world noisy labels

- Failure signatures:
  - Non-significant Kruskal-Wallis globally but significant site-specific gaps → aggregation masking
  - Persistent gap after balanced training → intrinsic bias
  - Large effect reduction after controlling for source → confounded bias

- First 3 experiments:
  1. Replicate baseline OLS (Performance ~ Age) on Dice and HD95 to verify R² values match paper
  2. Run site-disaggregated ethnic analysis: compute DPD per source, identify which sites have >5% gaps
  3. Train nnU-Net on downsampled balanced cohort and compare fairness gap (§) against automated model to confirm intrinsic component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does breast tissue density mediate the observed age-related segmentation performance gap, and can its inclusion as a covariate eliminate the intrinsic bias against younger patients?
- Basis in paper: [explicit] The authors state: "We hypothesize that this bias may be linked to physiological factors, a known challenge for both radiologists and automated systems."
- Why unresolved: The study identifies intrinsic age bias persisting after balanced training but does not directly measure or control for breast density, which correlates with age and is clinically known to affect tumor delineation.
- What evidence would resolve it: Regression analysis with breast density as a covariate; comparison of performance stratified by both age and density categories.

### Open Question 2
- Question: Do expert annotations in MAMA-MIA contain systematic label bias across demographic subgroups that propagates to automated segmentation quality assessments?
- Basis in paper: [explicit] The Outlook section states: "Future work will therefore focus on these origins through... a systematic examination of annotation quality for evidence of label bias."
- Why unresolved: The paper audits silver labels against gold labels but does not investigate whether gold labels themselves vary systematically in quality or annotation protocol across ethnic or age groups.
- What evidence would resolve it: Inter-rater agreement analysis stratified by demographics; comparison of annotation protocols across sites; annotation time or difficulty metrics by subgroup.

### Open Question 3
- Question: What targeted mitigation strategies can effectively reduce the intrinsic age-related bias that persists after representational balancing?
- Basis in paper: [explicit] The Outlook mentions "the ultimate goal of developing targeted mitigation strategies to ensure equitable model performance."
- Why unresolved: The controlled experiment shows a fairness gap of 0.0399 persists even with age-balanced training, indicating standard approaches are insufficient for intrinsic bias.
- What evidence would resolve it: Testing domain adaptation, loss re-weighting, or architecture modifications designed for subpopulation robustness; evaluation on held-out demographic subgroups.

### Open Question 4
- Question: Why do Dice score and Hausdorff Distance exhibit fundamentally different bias patterns—intrinsic versus source-confounded—for ethnic disparities?
- Basis in paper: [inferred] The paper finds "adjusting for the data source reduced its effect size by 6.2%" for Dice versus "64.0%" for HD95, concluding "the model produces different types of segmentation errors for certain ethnic groups."
- Why unresolved: The mechanism driving this metric-dependent divergence is not investigated; it is unclear whether volumetric overlap errors have different etiology than boundary accuracy errors.
- What evidence would resolve it: Qualitative error analysis characterizing failure modes by ethnicity; correlating error types with imaging acquisition parameters or tumor morphology features.

## Limitations
- Age-related bias mechanism relies on untested physiological assumptions about breast density differences
- Controlled experiment uses downsampling that may reduce statistical power and generalizability
- Metric-dependent bias patterns require clinical validation to determine which error types matter most

## Confidence
- **High confidence:** Age-related performance disparities persist after controlling for data source; disaggregation reveals site-specific ethnic bias masked in global analysis; metric choice affects bias interpretation
- **Medium confidence:** The mechanism attributing age bias to tissue density is plausible but not directly validated; the controlled experiment design is sound but downsampling limits generalizability
- **Low confidence:** Direct physiological validation of the age-tissue density mechanism; clinical significance of metric-specific bias patterns

## Next Checks
1. Replicate age-balanced training experiment with alternative stratification strategies (e.g., age-bins as stratification variables rather than downsampling) to verify the intrinsic bias finding is robust to sampling methodology
2. Conduct site-disaggregated ethnic analysis computing DPD per cohort and performing pairwise comparisons to identify specific institutional contexts where ethnic disparities exceed clinical thresholds
3. Cross-validate metric-specific bias patterns by training a secondary segmentation model optimized for boundary precision (not volumetric overlap) and testing whether HD95 disparities persist independent of source confounding