---
ver: rpa2
title: 'MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers'
arxiv_id: '2511.20382'
source_url: https://arxiv.org/abs/2511.20382
tags:
- more
- cell
- single-cell
- across
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoRE (Multi-Omics Representation Embedding) introduces a parameter-efficient
  framework that repurposes frozen pre-trained transformer backbones for robust multi-omics
  integration. By attaching lightweight, modality-specific adapters and a task-adaptive
  fusion layer, MoRE optimizes masked modeling with supervised contrastive and batch-invariant
  alignment losses, producing embeddings that generalize across unseen cell types
  and platforms.
---

# MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers

## Quick Facts
- arXiv ID: 2511.20382
- Source URL: https://arxiv.org/abs/2511.20382
- Reference count: 0
- Parameter-efficient framework repurposes frozen pre-trained transformers for multi-omics integration

## Executive Summary
MoRE introduces a parameter-efficient framework that repurposes frozen pre-trained transformer backbones for robust multi-omics integration. By attaching lightweight, modality-specific adapters and a task-adaptive fusion layer, MoRE optimizes masked modeling with supervised contrastive and batch-invariant alignment losses, producing embeddings that generalize across unseen cell types and platforms. The method achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to full fine-tuning approaches.

## Method Summary
MoRE attaches modality-specific adapter layers to a frozen transformer backbone, learning lightweight projections from each omics modality into a shared latent space. A task-adaptive fusion layer with learned attention weights combines modality embeddings, followed by iterative batch refinement using residual subtraction. The model is trained with a composite loss combining cross-entropy, supervised contrastive, modality alignment, and intra-class variance objectives. The framework operates on preprocessed single-cell data (e.g., scRNA-seq) and produces unified embeddings suitable for downstream clustering, annotation, and transfer learning tasks.

## Key Results
- Achieves competitive batch robustness and biological conservation while using 95-99% fewer trainable parameters than full fine-tuning
- Outperforms scGPT, scVI, Scrublet, and Harmony on integration fidelity and rare population detection
- Demonstrates superior cross-modality transfer and annotation accuracy in complex biological contexts

## Why This Works (Mechanism)

### Mechanism 1: Frozen Backbone with Modality-Specific Adapters
Freezing pre-trained transformer weights while training only lightweight adapters preserves generalizable biological semantics while enabling task-specific alignment. The frozen backbone serves as a strong regularizer that prevents overfitting to technical noise. Core assumption: pre-trained transformers encode transferable biological structure that remains valid across downstream tasks.

### Mechanism 2: Task-Adaptive Fusion via Learned Modality Attention
Element-wise attention weighting enables dynamic prioritization of informative modalities while suppressing noisy or missing signals. The fusion computes weighted combinations of modality embeddings using learnable attention weights. Core assumption: modalities contribute unequally to downstream tasks and some features within each modality are more discriminative than others.

### Mechanism 3: Iterative Batch Refinement with Multi-Objective Optimization
Residual batch correction combined with contrastive and alignment losses progressively harmonizes representations while preserving biological variation. Each refinement step computes batch embeddings and subtracts them from the current representation. Core assumption: batch effects are additive in the latent space and can be progressively removed without destroying semantic content.

## Foundational Learning

- **Parameter-Efficient Fine-Tuning (PEFT)**: Why needed: MoRE's core innovation depends on understanding why freezing most parameters and training only adapters works better than full fine-tuning. Quick check: Can you explain why updating only 5-10% of parameters might yield better out-of-distribution performance than updating all parameters?

- **Contrastive Learning with Supervision**: Why needed: The supervised contrastive loss is central to MoRE's ability to maintain intra-class coherence while separating distinct cell types. Quick check: How does supervised contrastive learning differ from self-supervised contrastive learning, and when would each be preferred?

- **Latent Space Batch Correction**: Why needed: MoRE performs batch correction in the embedding space rather than on raw counts; understanding this distinction is critical for debugging integration failures. Quick check: Why might batch correction in latent space preserve more biological signal than correction on raw gene expression?

## Architecture Onboarding

- **Component map**: Input tokenization -> Frozen transformer backbone -> Modality-specific adapters -> Task-adaptive fusion -> Iterative batch refinement -> Unified embeddings

- **Critical path**: 
  1. Verify backbone weights are correctly frozen (gradient computation disabled)
  2. Initialize adapter weights with small random values; fusion weights near uniform
  3. Train with composite loss, monitoring each term independently
  4. Validate batch mixing and biological conservation on held-out cell types

- **Design tradeoffs**: 
  - Frozen backbone → faster training, better generalization, but fixed representational capacity
  - Lightweight adapters → parameter efficiency, but may underfit on highly novel modalities
  - Iterative refinement → progressive improvement, but increased inference latency

- **Failure signatures**:
  - Embeddings cluster by batch rather than cell type → batch loss weight too low or insufficient refinement iterations
  - All cell types collapse to single cluster → contrastive loss too dominant, over-smoothing
  - Rare cell types disappear → classification loss overpowers structure preservation
  - Training loss plateaus but validation degrades → adapter overfitting, reduce adapter capacity

- **First 3 experiments**:
  1. Ablation on frozen vs. fine-tuned backbone: Run MoRE with backbone frozen and unfrozen on same dataset; measure integration fidelity (NMI, ARI) and parameter count to confirm PEFT advantage.
  2. Loss term sensitivity analysis: Systematically vary λ weights in composite loss; quantify trade-off between batch mixing (LISI) and biological conservation (cLISI) to identify stable operating regime.
  3. Cross-modality transfer validation: Train on scRNA-seq only, test on paired ATAC-seq or protein data; assess whether modality alignment loss enables transfer as claimed.

## Open Questions the Paper Calls Out

### Open Question 1
How does MoRE perform when integrating high-dimensional ATAC-seq or spatial transcriptomics data, beyond the scRNA-seq-focused evaluation presented? The primary evaluation focused heavily on scRNA-seq and widely accessible dual-modality datasets. The full potential of MoRE in integrating high-dimensional ATAC-seq or spatial transcriptomics data requires further extensive validation.

### Open Question 2
Can knowledge distillation effectively compress MoRE's adapted model for faster deployment without degrading batch robustness or biological conservation? The inference speed is still bounded by the transformer's depth. Future work will explore knowledge distillation techniques to compress the adapted model for even faster deployment.

### Open Question 3
How can adapter architectures be extended to explicitly encode hierarchical biological relationships such as cell type ontologies or developmental lineage trees? We plan to extend the adapter architecture to explicitly model hierarchical biological relationships, moving closer to a true general-purpose foundation model for single-cell biology.

### Open Question 4
How sensitive is MoRE's performance to the empirically tuned loss weighting factors, and can these weights be learned adaptively rather than manually specified? Each loss term is scaled by a weighting factor λ, which we tune empirically, but provides no analysis of sensitivity or principled selection method.

## Limitations

- Core claims rest on untested assumptions about pre-trained transformer generalizability across omics modalities without access to specific frozen backbone weights
- Batch correction mechanism assumes linear additivity of batch effects, which may not hold for complex technical artifacts
- Performance sensitivity to empirically tuned loss weighting factors remains unexplored

## Confidence

- **High confidence**: Parameter-efficient fine-tuning benefits (supported by PEFT literature); multi-objective optimization framework (standard practice)
- **Medium confidence**: Task-adaptive fusion effectiveness (similar mechanisms exist but MoRE's specific implementation unvalidated); batch refinement strategy (iterative correction is common but residual subtraction unproven here)
- **Low confidence**: Frozen backbone preservation of biological semantics (requires validation that pre-training data distribution matches target applications)

## Next Checks

1. **Frozen backbone ablation**: Run identical experiments with frozen vs. fine-tuned transformer; measure integration fidelity (NMI, ARI) and parameter efficiency to verify PEFT advantage.

2. **Loss sensitivity mapping**: Systematically vary λ weights (0.1, 1.0, 10.0) for each term; quantify trade-off between batch mixing (LISI) and biological conservation (cLISI) to identify stable operating regime.

3. **Cross-modality transfer test**: Train on scRNA-seq only, evaluate on paired ATAC-seq/protein; assess whether modality alignment enables transfer or adapters fail on unseen modalities.