---
ver: rpa2
title: 'Teaching According to Students'' Aptitude: Personalized Mathematics Tutoring
  via Persona-, Memory-, and Forgetting-Aware LLMs'
arxiv_id: '2511.15163'
source_url: https://arxiv.org/abs/2511.15163
tags:
- learning
- tutoring
- student
- arxiv
- tasa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TASA, a personalized mathematics tutoring framework
  that integrates student persona, event memory, and forgetting-aware dynamics to
  enhance LLM-based tutoring systems. Unlike existing methods that treat student information
  as static snapshots, TASA continuously models temporal knowledge decay using individualized
  forgetting curves grounded in cognitive psychology.
---

# Teaching According to Students' Aptitude: Personalized Mathematics Tutoring via Persona-, Memory-, and Forgetting-Aware LLMs

## Quick Facts
- arXiv ID: 2511.15163
- Source URL: https://arxiv.org/abs/2511.15163
- Authors: Yang Wu; Rujing Yao; Tong Zhang; Yufei Shi; Zhuoren Jiang; Zhushan Li; Xiaozhong Liu
- Reference count: 30
- Primary result: Achieves 11.4% higher normalized learning gain and 19.7% better personalization quality than baselines

## Executive Summary
This paper introduces TASA, a personalized mathematics tutoring framework that integrates student persona, event memory, and forgetting-aware dynamics to enhance LLM-based tutoring systems. Unlike existing methods that treat student information as static snapshots, TASA continuously models temporal knowledge decay using individualized forgetting curves grounded in cognitive psychology. It dynamically adjusts persona and memory representations based on retention estimates before generating instructional content. Experiments on four mathematics benchmarks demonstrate that TASA significantly outperforms state-of-the-art baselines, achieving 11.4% higher normalized learning gain and 19.7% better personalization quality compared to the strongest competitor. The framework shows stable performance across different LLM backbones and benefits from multi-turn interactions, validating that modeling temporal forgetting and learner profiles is crucial for effective adaptive tutoring.

## Method Summary
TASA is a personalized mathematics tutoring system that uses LLMs to generate adaptive questions and explanations based on student's evolving knowledge state. The method extracts student persona (stable proficiency traits) and memory (timestamped interaction logs) from historical trajectories using LLM agents, then encodes them with BGE encoders and stores in separate vector banks. During tutoring, it retrieves relevant persona/memory with hybrid similarity search (semantic + keyword), re-ranks with BGE reranker, computes forgetting scores using DKT-mastery probabilities and time decay, and uses an LLM rewriter to adjust context descriptions before generating the final tutoring response. The system uses simulated students (GPT-4o-mini) in 20-turn tutoring sessions across four math benchmarks (ASSISTments2017, NIPS34, Algebra2005, Bridge2006), measuring performance through normalized learning gain and response personalization win rate.

## Key Results
- TASA achieves 11.4% higher normalized learning gain compared to the strongest baseline
- TASA achieves 19.7% better personalization quality according to GPT-5-as-judge evaluation
- Performance is stable across different LLM backbones (Qwen3-4B-Instruct, Llama3.1-8B-Instruct)
- Multi-turn interactions benefit TASA, with performance saturating around 20 dialogue rounds

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adjusting context based on estimated memory decay aligns the LLM tutor with the student's actual retention state, improving learning gain.
- **Mechanism:** TASA calculates a forgetting score $F_c(t)$ using a continuous decay function based on time elapsed ($\Delta t_c$) and current mastery ($s_{t,c}$). It uses this score to rewrite the retrieved context (persona/memory) before the LLM generates a response. For example, if a concept hasn't been practiced recently, the context is rewritten to reflect "likely forgotten," prompting the LLM to offer review material rather than advanced problems.
- **Core assumption:** The Ebbinghaus-style forgetting curve accurately approximates a student's internal knowledge state, and translating this numerical score into natural language context allows the LLM to adjust difficulty appropriately.
- **Evidence anchors:**
  - [abstract]: "TASA continuously models temporal knowledge decay... It dynamically adjusts persona and memory representations..."
  - [section]: "Forgetting-Aware Rewriting" (Page 5) details the LLM prompt that uses forgetting scores to revise student descriptions.
  - [corpus]: Weak direct evidence. While corpus papers like "One Size doesn't Fit All" emphasize personalization, TASA's specific *forgetting-based rewriting* mechanism is a novel contribution not externally validated in the provided neighbors.
- **Break condition:** If the student's retention does not follow the exponential decay model (e.g., they possess strong long-term memory or used spaced repetition), the rewriting might incorrectly signal "forgotten" and lower the difficulty unnecessarily.

### Mechanism 2
- **Claim:** Separating student state into stable "Persona" traits and temporal "Event Memory" enables distinct retrieval and prioritization strategies.
- **Mechanism:** The system maintains two separate memory banks. $M_{persona}$ stores stable proficiency summaries (e.g., "struggles with multi-step problems"), while $M_{mem}$ stores timestamped interaction logs. During retrieval, both are queried, but the forgetting score is applied specifically to decay the relevance of these records.
- **Core assumption:** Long-term traits (Persona) and short-term episodes (Memory) provide complementary signals that, when combined, offer a complete picture of the learner.
- **Evidence anchors:**
  - [abstract]: "...framework that integrates student persona, event memory, and forgetting-aware dynamics..."
  - [section]: "Student Modeling with Persona and Memory" (Page 3) defines the two separate generation and storage pipelines.
  - [corpus]: "AI-Powered Math Tutoring" and "PAPPL" discuss the need for adaptive profiling, supporting the value of structured student models, though they do not explicitly validate the binary split used here.
- **Break condition:** If the Persona Generator fails to abstract accurate traits from the history, the retrieved persona will misguide the tutor regardless of the memory retrieval.

### Mechanism 3
- **Claim:** Using Deep Knowledge Tracing (DKT) to provide the mastery probability ($s_{t,c}$) grounds the forgetting calculation in sequential learning patterns rather than static correctness rates.
- **Mechanism:** Instead of relying solely on raw correctness rates, TASA uses a DKT model to predict the probability of a student knowing concept $c$ at time $t$. This probability is a direct input into the forgetting score formula $F_c(t)$.
- **Core assumption:** The DKT model's definition of "mastery probability" is a reliable proxy for the student's latent knowledge state and can be mathematically combined with time decay.
- **Evidence anchors:**
  - [abstract]: "...incorporating a continuous forgetting curve with knowledge tracing..."
  - [section]: Table 3 (Page 7) shows that replacing DKT with simple history-based approximation drops average $\Delta$-NLG from 57.6% to 46.6%.
  - [corpus]: Weak. Corpus neighbors focus on LLM agents for simulation/tutoring (e.g., "Investigating Pedagogical Teacher and Student LLM Agents") but do not explicitly validate the integration of DKT into LLM prompting pipelines.
- **Break condition:** If the DKT model is poorly trained on sparse data, the mastery probability $s_{t,c}$ will be inaccurate, leading to erroneous forgetting scores and mis-calibrated instruction.

## Foundational Learning

- **Concept: Knowledge Tracing (KT)**
  - **Why needed here:** The paper relies on KT (specifically DKT) to estimate the student's latent mastery state, which is a critical variable in the forgetting formula.
  - **Quick check question:** Can you distinguish between the *observable* correctness of an answer and the *latent* mastery probability estimated by a KT model?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** TASA is effectively a specialized RAG system where the "Retrieval" phase involves fetching persona/memory, and "Generation" involves the tutoring response.
  - **Quick check question:** What is the standard process for retrieving relevant documents and conditioning an LLM to generate a response based on them?

- **Concept: The Forgetting Curve (Ebbinghaus)**
  - **Why needed here:** The paper models retention decay using an exponential function derived from cognitive psychology.
  - **Quick check question:** How does the "time since last practice" ($\Delta t$) theoretically affect the probability of remembering a concept according to the standard forgetting curve?

## Architecture Onboarding

- **Component map:** Student Query + Historical Trajectory -> DKT Model -> Forgetting Score Calculation -> Persona/Memory Retrieval (BGE Encoder, Hybrid Search, BGE Reranker) -> Forgetting-Aware Rewriter -> Generator LLM -> Tutoring Response

- **Critical path:** The **Rewriter** is the bottleneck. If the rewriting prompt fails to translate the numerical forgetting score into a meaningful natural language context shift, the Generator will not adapt to the student's retention level.

- **Design tradeoffs:**
  - **DKT vs. History:** Using DKT is more accurate (Table 3) but adds significant architectural complexity and training requirements compared to simple correctness averages.
  - **Simulation vs. Real Users:** The paper validates using GPT-4o-mini as a simulated student. Real human evaluation may reveal different forgetting behaviors.

- **Failure signatures:**
  - **Stuck in Review Loop:** Forgetting scores are consistently high, causing the Rewriter to always frame the student as "needing review," preventing advancement.
  - **Hallucinated Decay:** The Rewriter misinterprets the forgetting score prompt, inventing weaknesses not present in the original persona.

- **First 3 experiments:**
  1. **Ablate the Rewriter:** Run the pipeline with the Rewriter disabled (pass raw context) to quantify the specific contribution of the forgetting-aware text adjustment.
  2. **KT Backbone Swap:** Substitute the DKT model with the simple History-based approximation (as in Table 3) to verify sensitivity to mastery estimation accuracy.
  3. **Time-Warp Test:** Artificially modify the timestamp data ($\Delta t_c$) in the input to simulate students returning after 1 day vs. 30 days, and verify the Rewriter output changes accordingly.

## Open Questions the Paper Calls Out
- **Open Question 1:** Does TASA's forgetting-aware tutoring produce measurable learning gains with real human students in live classroom settings, compared to simulated LLM-based student agents?
- **Open Question 2:** How should the memory strength parameter S_c and stability constant τ be individualized per student and per knowledge concept to maximize tutoring effectiveness?
- **Open Question 3:** Does the forgetting-aware framework generalize effectively to domains beyond mathematics where knowledge structures differ in granularity, prerequisite dependencies, and retention patterns?
- **Open Question 4:** How does TASA perform in extended, multi-session learning trajectories spanning weeks or months where forgetting and relearning cycles compound dynamically?

## Limitations
- **Simulation-only validation:** The paper validates using GPT-4o-mini as a simulated student rather than human learners, limiting confidence in real-world applicability.
- **Limited forgetting curve specification:** The stability constant τ is not specified numerically, making it difficult to assess whether decay rates are realistic for mathematics learning.
- **LLM prompt sensitivity:** The system's performance critically depends on the rewriter's ability to translate numerical forgetting scores into meaningful natural language context adjustments, with insufficient ablations on prompt robustness.

## Confidence
- **High confidence:** TASA outperforms baseline tutoring systems on the tested benchmarks (11.4% improvement in normalized learning gain).
- **Medium confidence:** The forgetting-aware rewriting mechanism is the primary driver of performance improvements.
- **Medium confidence:** Separating persona and memory into distinct representations provides complementary benefits.

## Next Checks
1. **Human user study validation:** Deploy TASA with actual students across diverse mathematics topics and measure learning gains through pre/post testing.
2. **Forgetting curve calibration:** Conduct controlled experiments where students practice concepts at different intervals, then measure actual retention to calibrate the τ parameter.
3. **Cross-domain transferability test:** Apply TASA to non-mathematics domains (e.g., language learning, science concepts) while keeping the same forgetting-aware architecture.