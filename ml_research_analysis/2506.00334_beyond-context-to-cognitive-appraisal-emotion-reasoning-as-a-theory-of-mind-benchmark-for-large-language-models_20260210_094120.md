---
ver: rpa2
title: 'Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind
  Benchmark for Large Language Models'
arxiv_id: '2506.00334'
source_url: https://arxiv.org/abs/2506.00334
tags:
- reasoning
- emotion
- appraisal
- llms
- emotions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Large Language Models (LLMs) reason
  about others' emotions using cognitive appraisal theory. The authors create a novel
  evaluation dataset with vignettes systematically manipulated according to cognitive
  appraisal dimensions, focusing on the Prisoner's Dilemma game show scenario.
---

# Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2506.00334
- Source URL: https://arxiv.org/abs/2506.00334
- Authors: Gerard Christopher Yeo; Kokil Jaidka
- Reference count: 9
- Primary result: LLMs struggle to accurately associate situational outcomes and appraisals with specific emotions, achieving only 57.9% accuracy on emotion classification with Gemma 7b

## Executive Summary
This paper investigates how Large Language Models reason about others' emotions using cognitive appraisal theory, moving beyond simple context-based emotion inference to examine how models integrate multiple appraisal dimensions. The authors create a novel evaluation dataset using Prisoner's Dilemma game show scenarios systematically manipulated across outcome, fairness, and accountability appraisal dimensions. Results show that while LLMs can reason about emotions to some extent, they predominantly rely on heuristic associations rather than nuanced integration of contextual factors, with forward reasoning (context→emotion) proving more tractable than backward reasoning (emotion→context).

## Method Summary
The study employs a controlled vignette-based evaluation using Prisoner's Dilemma scenarios with systematic manipulation of outcome (Co/De combinations), fairness appraisal, and accountability appraisal across three scenarios (game show, business deal, relationship). Each vignette is generated in three rephrasings. Two tasks are evaluated: forward reasoning where models infer target agent's emotion from context (6-way classification: anger, disappointment, joy, pride, regret, relief), and backward reasoning where models infer outcomes and appraisals from stated emotions. The evaluation uses zero-shot prompting with default parameters across four models (Mistral-7B-Instruct-v0.3, Llama-3.1-8b-Instruct, Gemma-7b-Instruct, o3-mini), measuring emotion classification accuracy and conducting ANOVA on intensity ratings.

## Key Results
- Gemma 7b achieved the highest accuracy at 57.9% on the emotion classification task
- LLMs predominantly use System 1-like strategies, relying on heuristic associations rather than abstract reasoning
- Forward reasoning (context→emotion) is more tractable than backward reasoning (emotion→context)
- Models consistently conflate similar emotions like anger and disappointment, requiring fine-grained appraisal distinctions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs engage emotion reasoning through heuristic-based associations rather than deliberative context integration.
- Mechanism: Models appear to map surface-level appraisal cues (e.g., "unfairness") directly to prototypical emotions (e.g., anger), bypassing multi-dimensional integration of contextual factors.
- Core assumption: This behavior reflects training data patterns where emotional associations are frequent, while abstract appraisal-to-emotion reasoning is underrepresented.
- Evidence anchors:
  - [abstract] "LLMs predominantly adopt System 1-like strategies, relying on heuristic-based associations rather than the nuanced integration of contextual factors indicative of System 2 engagement."
  - [section 4] "Anger ratings were consistently elevated in scenarios with perceived unfairness, regardless of outcome—a heuristic shortcut that bypasses the nuanced integration of contextual factors indicative of System 2 engagement."
  - [corpus] Related work on mechanistic interpretability (arxiv 2502.05489) suggests emotion representations exist but processing mechanisms remain underexplored.
- Break condition: Models trained with explicit appraisal-reasoning supervision or chain-of-thought prompting may shift toward System-2-like behavior.

### Mechanism 2
- Claim: LLMs weight fairness and outcome cues disproportionately over accountability when inferring emotions.
- Mechanism: Statistical associations in training data between fairness language and anger-related expressions may dominate, while accountability-context-emotion mappings are sparser.
- Core assumption: Attention mechanisms allocate more weight to high-frequency co-occurrence patterns than to logically necessary but lower-frequency relationships.
- Evidence anchors:
  - [section 3.1] "Two-way interactions between outcome and the appraisal of fairness were statistically significant (Fs(6, 384) > 6.37, ps < .05)... the appraisal of accountability appears to play a minimal role."
  - [section 3.1] "LLMs primarily rely on outcome information and fairness appraisals when reasoning about an agent's emotions."
  - [corpus] No direct corpus evidence on appraisal dimension weighting in LLMs.
- Break condition: When accountability is explicitly foregrounded in prompts or training, models may recalibrate weighting.

### Mechanism 3
- Claim: Forward reasoning (context→emotion) is more tractable than backward reasoning (emotion→context) for current LLMs.
- Mechanism: Forward reasoning maps to classification-like patterns present in pretraining; backward reasoning requires generative inference over under-specified causal spaces.
- Core assumption: The training objective (next-token prediction) favors pattern completion over abductive reasoning from sparse cues.
- Evidence anchors:
  - [section 3.2.1] "LLMs associate both anger and disappointment with the 'Co-De' outcome" rather than discriminating based on appraisal dimensions.
  - [section 4] "LLMs frequently conflated distinct but similar emotions, such as anger and disappointment, which require subtle distinctions based on appraisal dimensions like accountability and goal conduciveness."
  - [corpus] Related work (arxiv 2511.15895) decomposes ToM abilities, suggesting emotional processing mediates ToM but mechanisms differ by task direction.
- Break condition: Models with explicit causal reasoning modules or structured generative frameworks may improve backward inference.

## Foundational Learning

- Concept: Cognitive Appraisal Theory
  - Why needed here: This is the theoretical grounding for the entire evaluation—emotions arise from evaluations of events along dimensions (goal conduciveness, fairness, accountability), not from events directly.
  - Quick check question: Given "Person A cooperated, Person B defected," what appraisal dimensions would differentiate anger from disappointment?

- Concept: Dual-Process Theory (System 1 vs System 2)
  - Why needed here: The paper frames LLM limitations as over-reliance on System 1 (fast, heuristic) versus System 2 (deliberate, integrative) reasoning patterns.
  - Quick check question: If an LLM always predicts "anger" when "unfair" appears in text, is this System 1 or System 2 behavior?

- Concept: Theory of Mind (ToM)
  - Why needed here: Emotion reasoning about third parties requires modeling others' beliefs, goals, and evaluations—the paper extends ToM benchmarks to affective inference.
  - Quick check question: What additional information beyond the situation text would an observer need to infer an agent's emotional state?

## Architecture Onboarding

- Component map:
  Vignette Generator -> Forward Reasoning Task -> Backward Reasoning Task -> Evaluation Module

- Critical path:
  1. Vignette construction with systematic manipulation of (outcome × fairness × accountability)
  2. Zero-shot prompting of target LLMs
  3. Emotion classification accuracy + ANOVA on intensity ratings across conditions

- Design tradeoffs:
  - Controlled vignettes enable causal attribution but limit ecological validity (real conversations are messier)
  - Theory-derived ground truth enables systematic evaluation but may not capture individual/cultural variation
  - Zero-shot evaluation isolates reasoning ability but underestimates few-shot or fine-tuned performance

- Failure signatures:
  - High accuracy on prototypical cases but poor discrimination between similar emotions (e.g., anger vs. disappointment both mapped to "Co-De" outcome)
  - Non-significant three-way interactions suggesting models don't integrate multiple appraisal dimensions
  - Uniform high ratings for one emotion regardless of manipulated appraisal (e.g., always-high anger when "unfair" mentioned)

- First 3 experiments:
  1. Replicate forward reasoning with chain-of-thought prompting to test whether explicit appraisal reasoning improves accuracy.
  2. Add few-shot exemplars showing correct appraisal-emotion mappings to measure learning plasticity.
  3. Extend to naturalistic dialogue contexts beyond Prisoner's Dilemma to test generalization of findings.

## Open Questions the Paper Calls Out
None

## Limitations
- The ground-truth emotion labels derived from Cognitive Appraisal Theory may not fully capture individual or cultural variation in emotion experiences
- Zero-shot evaluation may not reveal the full reasoning capabilities of LLMs, as few-shot or fine-tuned models could perform substantially better
- The controlled vignette format limits ecological validity and may not generalize to naturalistic language use

## Confidence
- **High confidence**: The observation that LLMs show higher accuracy on forward reasoning (context→emotion) than backward reasoning (emotion→context), and that fairness appraisals have stronger effects than accountability on emotion predictions
- **Medium confidence**: The claim that LLMs predominantly use System 1-like heuristic strategies rather than System 2 deliberative reasoning, based on pattern analysis of their responses
- **Medium confidence**: The finding that models conflate similar emotions (anger/disappointment) when they require fine-grained appraisal distinctions

## Next Checks
1. Test whether chain-of-thought prompting or few-shot exemplars improve accuracy on backward reasoning tasks, particularly for distinguishing between similar emotions requiring subtle appraisal differences
2. Validate findings with a broader range of naturalistic dialogue contexts beyond Prisoner's Dilemma scenarios to assess ecological generalizability
3. Conduct ablation studies on appraisal dimension importance by systematically removing or emphasizing specific appraisal cues to confirm their relative weighting in model predictions