---
ver: rpa2
title: 'AdeptHEQ-FL: Adaptive Homomorphic Encryption for Federated Learning of Hybrid
  Classical-Quantum Models with Dynamic Layer Sparing'
arxiv_id: '2507.07316'
source_url: https://arxiv.org/abs/2507.07316
tags:
- quantum
- privacy
- learning
- adeptheq-fl
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AdeptHEQ-FL addresses federated learning challenges\u2014non-IID\
  \ data, privacy vulnerabilities, and high communication costs\u2014by integrating\
  \ a hybrid CNN-PQC architecture with adaptive accuracy-weighted aggregation using\
  \ differentially private validation accuracies and selective homomorphic encryption\
  \ (HE) for final layer protection. It also employs dynamic layer-wise freezing to\
  \ reduce communication overhead while preserving quantum adaptability."
---

# AdeptHEQ-FL: Adaptive Homomorphic Encryption for Federated Learning of Hybrid Classical-Quantum Models with Dynamic Layer Sparing

## Quick Facts
- arXiv ID: 2507.07316
- Source URL: https://arxiv.org/abs/2507.07316
- Reference count: 40
- Primary result: Achieves ≈25.43% and ≈14.17% accuracy improvements over Standard-FedQNN and FHE-FedQNN respectively on CIFAR-10 while maintaining privacy and efficiency

## Executive Summary
AdeptHEQ-FL is a federated learning framework designed to address challenges of non-IID data, privacy vulnerabilities, and high communication costs when training hybrid classical-quantum neural networks. It integrates a CNN-PQC hybrid architecture with adaptive accuracy-weighted aggregation using differentially private validation accuracies and selective homomorphic encryption for final layer protection. The framework also employs dynamic layer-wise freezing to reduce communication overhead while preserving quantum adaptability, achieving significant accuracy improvements over baseline approaches while maintaining strong privacy guarantees.

## Method Summary
The framework uses 10 clients, 20 rounds, and 10 local epochs per round with Adam optimizer. The hybrid model consists of a CNN encoder (3 conv blocks → FC layers → 16-dim output) followed by a PQC (4 qubits, 2 StronglyEntanglingLayers with amplitude embedding) and final FC layer. Clients compute privatized validation accuracies using Laplace noise, which are converted to tempered softmax weights for aggregation. Only the final FC layer parameters are encrypted using CKKS homomorphic encryption before transmission. Dynamic layer-wise freezing identifies stable layers based on L2 norm changes and excludes them from communication, with quantum layers explicitly exempt from freezing.

## Key Results
- Achieves ≈25.43% accuracy improvement over Standard-FedQNN on CIFAR-10
- Demonstrates ≈14.17% accuracy improvement over FHE-FedQNN on CIFAR-10
- Maintains (ε_total=10, δ=10⁻⁵) privacy budget over 20 rounds
- Reduces communication overhead through dynamic layer-wise freezing

## Why This Works (Mechanism)

### Mechanism 1
Adaptive accuracy-weighted aggregation improves performance in non-IID settings by prioritizing updates from clients with higher privatized validation accuracy. Clients add Laplace noise to their local validation accuracy, and the server converts these scores into tempered softmax weights for aggregation.

### Mechanism 2
Selective Homomorphic Encryption on the final classifier layer balances privacy and computational tractability. Only FC4 parameters are encrypted client-side using CKKS, while other layers are aggregated in plaintext.

### Mechanism 3
Dynamic layer-wise adaptive freezing reduces communication overhead by identifying stable layers through L2 norm parameter changes. Layers are frozen and excluded from communication when their parameter changes fall below a threshold, with quantum layers exempt.

## Foundational Learning

- **Hybrid Classical-Quantum Neural Networks (HQNN)**: The core model is a CNN-PQC hybrid. Understanding how classical features are encoded (Amplitude Embedding) and processed (Strongly Entangling Layers) is critical for debugging performance. *Quick check: How does the output dimension of the classical CNN (2^n features) relate to the input capacity of the Amplitude Embedding on n qubits?*

- **Differential Privacy (DP) - Laplace Mechanism**: This mechanism protects the validation accuracy used for weighting. Understanding the noise scale is critical for tuning the privacy-utility trade-off. *Quick check: If a client's validation set size (m_i) doubles, how does the scale of the Laplace noise added to their accuracy change?*

- **Federated Learning with Non-IID Data**: The framework is specifically designed to counteract model divergence caused by non-IID data distributions. *Quick check: Why might a simple average of client models (FedAvg) perform poorly when clients have vastly different data distributions?*

## Architecture Onboarding

- **Component map**: Client-Side: Local Dataset → CNN Encoder → Amplitude Embedding → PQC → Measurement → Final FC Layer → (CKKS Encryption of FC Layer) → Transmit Updates & Privatized Accuracy. Server-Side: Receive Encrypted FC & Plaintext Updates → Decrypt FC Layer → Compute Layer Importance → Determine Frozen Layers → Perform Accuracy-Weighted Aggregation → Broadcast Global Model.

- **Critical path**: The gradient flow from the loss function back through the classical FC layer, into the quantum circuit (parameter-shift rule), and through the CNN. The CKKS encryption/decryption is the primary computational bottleneck.

- **Design tradeoffs**: Privacy vs. Utility (lower ε increases privacy but adds noise), Communication vs. Accuracy (aggressive freezing saves bandwidth but risks missing learning opportunities), Quantum Resources vs. Performance (fewer qubits reduce simulation cost but impact performance).

- **Failure signatures**: Accuracy Collapse (check gradient flow in PQC or excessive DP noise), High Communication Cost (freezing logic not triggering), Decryption Errors (scaling factor mismatch in HE).

- **First 3 experiments**: 1) Baseline Reproduction: Implement full AdeptHEQ-FL and Standard-FedQNN to validate environment, 2) Ablation on Aggregation: Run with uniform aggregation to isolate adaptive weighting gains, 3) Freezing Threshold Sensitivity: Sweep freezing threshold to find operational sweet spot.

## Open Questions the Paper Calls Out

- Can AdeptHEQ-FL maintain efficiency advantages if HE is extended to layers beyond the final FC block? The framework currently applies HE selectively to the final layer for tractability, but comprehensive security may require broader encryption.

- How does the hybrid CNN-PQC architecture perform on physical NISQ devices compared to idealized simulations? The paper acknowledges that performance on real-world quantum hardware remains untested, creating a gap between theoretical results and physical deployment.

- Do theoretical convergence guarantees hold in strictly non-convex federated settings without relying on standard smoothness assumptions? The authors note their convergence analysis assumes smoothness and bounded gradient variance, which may not hold in highly non-convex hybrid quantum-classical settings.

## Limitations

- Selective HE application leaves feature extractor gradients exposed, potentially insufficient under stronger privacy threat models
- Adaptive freezing mechanism relies on heuristic threshold and EMA parameters that may be dataset-dependent
- Non-IID data partitioning scheme severity directly affects baseline performance and relative gains

## Confidence

- **High Confidence**: Overall architecture (CNN-PQC hybrid), CKKS HE parameters, core DP-accuracy weighting mechanism
- **Medium Confidence**: Adaptive freezing mechanism effectiveness (depends on heuristic parameters), reported accuracy improvements (methodology sound but depends on unknown details)

## Next Checks

1. Implement AdeptHEQ-FL with uniform aggregation instead of DP-accuracy weighted aggregation to isolate the specific contribution of the adaptive weighting scheme
2. Run full AdeptHEQ-FL pipeline with freezing thresholds [0.0001, 0.001, 0.01] and plot communication cost vs. final test accuracy
3. Attempt gradient inversion or membership inference attacks on plaintext CNN/PQC updates to test if selective HE of only the final layer is a vulnerability under stronger threat models