---
ver: rpa2
title: Multimodal Learning for Scalable Representation of High-Dimensional Medical
  Data
arxiv_id: '2409.13115'
source_url: https://arxiv.org/abs/2409.13115
tags:
- data
- marblix
- multimodal
- page
- binary
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces MarbliX, a self-supervised multimodal framework\
  \ that jointly learns representations from whole slide images (WSIs) and immune\
  \ cell sequencing data in digital pathology. By embedding these heterogeneous modalities\
  \ into a compact, binary latent space\u2014termed \u201Cmonograms\u201D\u2014MarbliX\
  \ enables efficient, scalable retrieval of clinically similar cases."
---

# Multimodal Learning for Scalable Representation of High-Dimensional Medical Data

## Quick Facts
- **arXiv ID:** 2409.13115
- **Source URL:** https://arxiv.org/abs/2409.13115
- **Reference count:** 37
- **Primary result:** MarbliX achieves 85–89% across all evaluation metrics in lung cancer, outperforming histopathology (69–71%) and immunogenomics (73–76%).

## Executive Summary
This paper introduces MarbliX, a self-supervised multimodal framework that jointly learns representations from whole slide images (WSIs) and immune cell sequencing data in digital pathology. By embedding these heterogeneous modalities into a compact, binary latent space—termed “monograms”—MarbliX enables efficient, scalable retrieval of clinically similar cases. The method employs hybrid autoencoders to align modality-specific embeddings and uses triplet contrastive learning to enforce similarity for same-diagnosis patients and dissimilarity for different ones. In lung cancer, MarbliX achieves 85–89% across all evaluation metrics, outperforming histopathology (69–71%) and immunogenomics (73–76%). In kidney cancer, real-valued monograms yield the strongest performance (F1: 80–83%, Accuracy: 87–90%), with binary monograms slightly lower (F1: 78–82%). Results demonstrate that MarbliX generates discriminative, compact multimodal patient representations, enabling efficient and interpretable case-based reasoning in cancer diagnostics.

## Method Summary
MarbliX processes gigapixel WSIs and immune repertoire sequencing data into compact, binary “monograms” for scalable patient retrieval. WSI patches are extracted via SPLICE and embedded using DINO ViT; RNA-seq data is processed through TRUST4 and Seqwash, then embedded using BERT. Two hybrid autoencoders (Architecture: 512-256-128-256-512) are trained to cross-reconstruct each modality, forcing semantic alignment in the 128-dimensional bottleneck. The fused embeddings are combined via outer product and projected through a triplet network into 8x8 binary monograms using a tanh activation and thresholding. During retrieval, Hamming distance between monograms enables constant-time nearest-neighbor search.

## Key Results
- MarbliX achieves 85–89% across all evaluation metrics in lung cancer, outperforming histopathology (69–71%) and immunogenomics (73–76%).
- In kidney cancer, real-valued monograms yield the strongest performance (F1: 80–83%, Accuracy: 87–90%), with binary monograms slightly lower (F1: 78–82%).
- Retrieval performance degrades slightly when binarizing monograms, but enables scalable, constant-time lookup.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid autoencoders enforce semantic alignment between histopathology and immunogenomics by forcing one modality to reconstruct the other.
- **Mechanism:** Two separate autoencoders (AI and AS) are trained. AI takes the image embedding $f$ as input but is trained via MSE loss to reconstruct the sequence embedding $g$. This forces the bottleneck layer to retain only features in the image that correlate with genomic data, filtering out modality-specific noise.
- **Core assumption:** There exist learnable, shared biological signals between tissue morphology and immune receptor sequences that allow for cross-modal prediction.
- **Evidence anchors:**
  - [section]: "Specifically, each autoencoder model is designed to take one modality and reconstruct the other... This cross-reconstruction forces the network to learn aligned representations rather than modality-specific noise." (Page 8)
  - [abstract]: "...hybrid autoencoders to align modality-specific embeddings..."
  - [corpus]: Related work "MIND" and "Scalable and Loosely-Coupled..." supports the general difficulty of aligning medical modalities, though MarbliX's cross-reconstruction method is distinct.
- **Break condition:** If MSE loss converges but the resulting latent vectors $u$ and $v$ fail to separate diagnostic classes in t-SNE plots, the assumption of cross-modal correlation is invalid for the selected features.

### Mechanism 2
- **Claim:** Structuring the latent space using diagnosis-aware triplet loss creates distinct patient clusters without requiring explicit classification labels during inference.
- **Mechanism:** The framework uses a triplet network (Model Q) taking anchor, positive (same diagnosis), and negative (different diagnosis) pairs. It optimizes a margin between intra-class and inter-class distances, projecting the outer product of latent vectors into an $8 \times 8$ matrix.
- **Core assumption:** Patients with the same diagnosis share multimodal “monogram” patterns that are distinguishable from other diagnoses via Euclidean/Hamming distance.
- **Evidence anchors:**
  - [section]: "This is achieved by employing self-supervised training using triplet loss to minimize the distance between patients with the same primary diagnosis and maximize the distance between patients with different primary diagnoses." (Page 9)
  - [results]: "MarbliX improves class separation, especially with binary monograms..." (Page 13)
  - [corpus]: Weak direct support; neighbor papers focus more on fusion/attention than explicit metric learning for binary retrieval.
- **Break condition:** If the margin $\alpha$ in the triplet loss is too small or data variability is high (e.g., limited KICH samples), the model fails to push negative pairs sufficiently far apart, resulting in retrieval errors.

### Mechanism 3
- **Claim:** Binarizing the latent representation preserves discriminative power while enabling scalable, constant-time retrieval using Hamming distance.
- **Mechanism:** The final layer of Model Q uses a $tanh$ activation (range $[-1, 1]$), thresholded to $\{0, 1\}$. This compresses complex patient data into 64-bit codes, where similarity is calculated via bitwise XOR operations.
- **Core assumption:** The continuous topology of the joint multimodal space can be discretized into a binary hypercube without catastrophic loss of semantic information (collision).
- **Evidence anchors:**
  - [results]: "In kidney cancer, real-valued monograms yield the strongest performance... with binary monograms slightly lower (F1: 78–82%)." (Page 15)
  - [results]: Figure 6 shows low intra-subtype XOR dissimilarity vs high inter-subtype dissimilarity. (Page 16)
  - [corpus]: No direct evidence in provided neighbors regarding binary hashing effectiveness in this specific multimodal context.
- **Break condition:** If the number of distinct clinical subtypes vastly exceeds the capacity of the $2^{64}$ bit-space (unlikely here) or if classes are not linearly separable, binarization introduces too much quantization error.

## Foundational Learning

- **Concept: Triplet Loss (Metric Learning)**
  - **Why needed here:** Standard loss functions (like Cross-Entropy) optimize for class probability, whereas MarbliX requires optimizing the *relative distance* between patients for retrieval.
  - **Quick check question:** If you randomly swap a “positive” and “negative” sample in the triplet, does the loss increase or decrease? (It should increase, signaling the optimization direction).

- **Concept: Autoencoder Bottlenecks**
  - **Why needed here:** The paper uses autoencoders not just for compression, but for *filtering*. The bottleneck forces the model to discard any visual feature in the WSI that has no corresponding signal in the genomic data.
  - **Quick check question:** Why is the decoder required during training but discarded during inference? (To compute the reconstruction loss that guides the encoder’s weights).

- **Concept: Cross-Modal Retrieval (Image-Text vs Image-Seq)**
  - **Why needed here:** Unlike standard image-to-text retrieval (CLIP), this deals with unaligned, high-dimensional biological sequences. Understanding that the “query” and “target” are different data types mapped to a shared index is crucial.
  - **Quick check question:** Can you query the database using only the WSI monogram if the patient has no genomic data? (No, the monogram is a fusion of both; you would need a separate unimodal index or a partial input handling mechanism, which is not the primary design here).

## Architecture Onboarding

- **Component map:** WSI (Gigapixel) + RNA-seq (Raw) -> SPLICE/DINO ViT (Image → 768d) + Seqwash/BERT (Seq → 768d) -> Hybrid Autoencoders (768d → 128d) -> Triplet Network Q (Outer Product → Dense → Tanh → 8x8 Binary).
- **Critical path:** The alignment relies on the **Hybrid Autoencoders**. If the reconstruction loss (MSE) does not converge (e.g., > 0.05), the subsequent triplet learning will operate on noise rather than aligned biological signals.
- **Design tradeoffs:**
  - **Binary vs. Real-valued:** Binary monograms allow for $O(1)$ lookup via XOR but lose ~2-5% F1 score compared to real-valued vectors in the kidney dataset.
  - **SPLICE Threshold:** Setting the similarity threshold too high (selecting few patches) speeds up processing but risks missing rare tissue features critical for genomic correlation.
- **Failure signatures:**
  - **High MSE in Autoencoders:** Indicates the modalities are not correlating; check data preprocessing (Seqwash harmonization) or patch selection.
  - **Collapsed Monograms:** If all monograms look identical (high similarity scores for all pairs), the triplet mining is likely too easy or the learning rate is too high.
  - **Class Imbalance Effects:** The kidney dataset shows lower F1 than accuracy; the model is biased toward majority classes (KIRC).
- **First 3 experiments:**
  1. **Sanity Check Reconstruction:** Train the hybrid autoencoders (AI and AS) and plot cosine similarity between input and reconstruction. Verify median similarity > 0.90 (per Fig 2a/2b).
  2. **Ablation on Modality:** Run the retrieval pipeline using only the image-derived latent $u$ vs. only the sequence-derived latent $v$ to confirm that the fused monogram outperforms both.
  3. **Hard Triplet Mining Test:** Train Model Q with random triplets vs. “hard” triplets (farthest positive, closest negative) to validate the paper’s claim on the necessity of hard mining for convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does MarbliX performance compare against standard multimodal fusion baselines like co-attention transformers or concatenation-based classifiers?
- **Basis in paper:** [explicit] The authors state they did not include direct comparisons with these models because they “rely on large paired datasets” and continuous embeddings not compatible with sparse immune-repertoire data.
- **Why unresolved:** Adapting standard architectures (e.g., CLIP, MCAT) to sparse sequence-derived immunogenomic features requires substantial redesign of encoders and fusion modules.
- **What evidence would resolve it:** Benchmarking MarbliX against specifically adapted versions of transformer-based fusion models on the same WSI-repertoire retrieval tasks.

### Open Question 2
- **Question:** How sensitive is the framework to architectural hyperparameters such as monogram dimensionality and autoencoder bottleneck size?
- **Basis in paper:** [explicit] The authors acknowledge they “did not provide full ablation studies on architectural parameters,” selecting pragmatic defaults (128-d bottleneck, 8x8 monogram) rather than optimized settings.
- **Why unresolved:** Systematic analysis of these variables was deemed outside the scope of the initial study, leaving the robustness of the chosen configuration unverified.
- **What evidence would resolve it:** Ablation studies varying the latent space size and matrix dimensions to quantify the impact on retrieval accuracy and collision rates.

### Open Question 3
- **Question:** Can the learned representations generalize to external clinical cohorts with different staining protocols or sequencing platforms?
- **Basis in paper:** [inferred] The paper notes TCGA data has “variable staining” and “scanner differences,” suggesting models may learn “spurious visual cues” rather than true pathology, risking weak generalization.
- **Why unresolved:** TCGA is the only dataset used; the model has not been tested on data from different institutions or acquisition pipelines.
- **What evidence would resolve it:** Validation of monogram retrieval performance on independent, prospectively collected datasets from different clinical sites.

## Limitations
- **Limited Clinical Validation:** Performance is reported only on TCGA cohorts; generalization to other populations, sequencing platforms, or diagnostic centers is untested. No external validation cohort is mentioned.
- **Data Quality Assumptions:** Results depend on successful preprocessing of gigapixel WSIs (via SPLICE patch extraction) and harmonization of immune repertoires (via Seqwash). Failures in either step would silently corrupt downstream embeddings.
- **Binary Compression Trade-offs:** While binarization enables efficient retrieval, the slight drop in F1 (2-5%) for kidney cancer suggests semantic loss that may accumulate in larger, more diverse datasets.

## Confidence

- **High:** Hybrid autoencoder alignment mechanism (cross-reconstruction is standard, well-documented).
- **Medium:** Triplet-based diagnosis clustering (depends on proper triplet mining and margin tuning).
- **Low:** Real-world scalability and clinical deployment readiness (no mention of latency, storage cost, or regulatory considerations).

## Next Checks

1. **Cross-Platform Stability:** Retrain and test on an independent WSI/RNA-seq cohort (e.g., CPTAC) to verify performance consistency.
2. **Ablation on Thresholding:** Evaluate retrieval with real-valued monograms (no binarization) to quantify semantic loss from compression.
3. **Hard Triplet Mining Robustness:** Compare retrieval accuracy using random vs. hard triplet sampling strategies to confirm the claimed benefit of the mining approach.