---
ver: rpa2
title: Estimating the Effects of Sample Training Orders for Large Language Models
  without Retraining
arxiv_id: '2505.22042'
source_url: https://arxiv.org/abs/2505.22042
tags:
- training
- performance
- sample
- framework
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a retraining-free framework to estimate how
  training sample order affects large language model (LLM) performance. The core idea
  is to approximate Adam optimizer updates using first- and second-order Taylor expansions,
  and store these update terms efficiently using random projection.
---

# Estimating the Effects of Sample Training Orders for Large Language Models without Retraining

## Quick Facts
- arXiv ID: 2505.22042
- Source URL: https://arxiv.org/abs/2505.22042
- Authors: Hao Yang; Haoxuan Li; Mengyue Yang; Xu Chen; Mingming Gong
- Reference count: 40
- One-line primary result: Introduces a retraining-free framework to estimate how training sample order affects LLM performance with up to 132.6× speedup and AbsDiff of 0.0085-0.0917

## Executive Summary
This paper presents a retraining-free framework (FUT) to estimate how training sample order affects large language model performance. The core innovation uses first- and second-order Taylor expansions to approximate Adam optimizer updates, combined with random projection to store intermediate terms efficiently. This enables estimation of model parameters under arbitrary training orders without retraining. Experiments on a 636M parameter LLM show the framework accurately estimates true performance and is up to 132.6× faster than retraining. The method is applied to curriculum design and analysis of memorization/generalization effects, providing insights into training dynamics.

## Method Summary
The FUT framework operates in three stages: (1) Train a reference model with reference sample order to obtain checkpoints {θt}, (2) Compute and store Adam update terms Γ(θt, Blt), ∇θΓ(θt, Blt), ∇²θΓ(θt, Blt) for all T² (checkpoint, batch) pairs using random projection compression, (3) Estimate parameters under permuted orders using first/second-order Taylor expansion and recursively compute validation perplexity. The method targets Adam's update rule and compresses high-dimensional terms via Johnson-Lindenstrauss theorem guarantees. Experiments use WikiText-103 with LLaMA architecture (636M parameters) and Adam optimizer.

## Key Results
- Framework accurately estimates true performance with AbsDiff of 0.0085-0.0917 depending on batch size
- Achieves up to 132.6× speedup compared to retraining for different sample orders
- For curriculum design, FUT-guided method outperforms Random Order baseline by 0.04-0.06 perplexity points
- Captures key trends in memorization: later-training samples show better retention
- Improves generalization for test data similar to later-training samples

## Why This Works (Mechanism)

### Mechanism 1: Taylor Expansion for Cross-Order Parameter Mapping
- Claim: Adam optimizer updates at one checkpoint can approximate updates at another checkpoint under different sample orders using Taylor expansion
- Mechanism: Treats the Adam update term Γ(θ, B) = mt/(√vt + ϵ) as a function of parameters θ. For a new trajectory parameter γt, approximates Γ(γt, Blt) ≈ Γ(θt, Blt) + (γt - θt)∇θΓ(θt, Blt). This enables recursive computation of parameters under any permuted order without retraining.
- Core assumption: The optimization landscape is locally smooth enough that low-order Taylor approximations remain valid when parameters deviate modestly from reference checkpoints.
- Evidence anchors: [abstract] mentions "approximating Adam optimizer updates with first- and second-order Taylor expansions"; [Section 3, Equations 3-4] formal derivation; [corpus] notes influence function literature uses similar gradient-based approximations that can be "fragile" per Basu et al.

### Mechanism 2: Random Projection for Efficient High-Dimensional Storage
- Claim: High-dimensional gradient and update terms can be compressed via random projection while preserving reconstruction accuracy
- Mechanism: Projects matrices M ∈ R^(d1×d2) to M' ∈ R^(d1×k) via random Gaussian matrix A ~ N(0, 1/k). Reconstruction uses Moore-Penrose pseudoinverse: M̂ = M'A⁺. Reduces storage from O(d1×d2) to O(d1×k) where k ≪ d2.
- Core assumption: The Johnson-Lindenstrauss theorem guarantee—that distances are approximately preserved under random projection—translates to usable reconstruction for optimization terms.
- Evidence anchors: [abstract] mentions "utilizing random projection methods to store intermediate checkpoints"; [Section 3, A.2] details JL theorem application with k ∈ {300, 200, 160, 80, 20, 8} based on layer size; [corpus] limited direct validation.

### Mechanism 3: Reference Trajectory as Universal Basis
- Claim: A single reference training pass provides sufficient information to estimate any permuted training order
- Mechanism: Stage 1 trains once with reference order producing Θ = {θt}. Stage 2 precomputes and stores Γ(θt, B) and ∇θΓ(θt, B) for all T² (checkpoint, batch) pairs. Stage 3 recursively estimates new trajectories by looking up relevant stored terms.
- Core assumption: All relevant optimizer dynamics are captured in the cross-product of reference checkpoints with all batches—no fundamentally new dynamics emerge under permutation.
- Evidence anchors: [abstract] mentions "efficiently estimate model parameters for arbitrary training sample orders"; [Figure 1] three-stage architecture visualization; [corpus] no direct precedent for this specific approach.

## Foundational Learning

- Concept: **Taylor Series Expansion**
  - Why needed here: Mathematical foundation enabling retraining-free estimation. Understanding how to locally approximate functions is essential for grasping why Γ(γt) ≈ Γ(θt) + (γt-θt)∇Γ(θt).
  - Quick check question: Given f(x) = x³, approximate f(2.1) using a first-order Taylor expansion around x=2. What's the error?

- Concept: **Adam Optimizer Internals**
  - Why needed here: Method explicitly targets Adam's update rule. Understanding how mt (first moment) and vt (second moment) accumulate gradients explains why ∇Γ depends on both ∇L and ∇²L.
  - Quick check question: In Adam, why does the update term Γ = mt/(√vt + ϵ) require both first and second-order loss gradients when computing its derivative w.r.t. parameters?

- Concept: **Johnson-Lindenstrauss Lemma**
  - Why needed here: Justifies random projection compression. Understanding that distances are approximately preserved under random projection enables principled storage reduction.
  - Quick check question: If you randomly project vectors from 1000D to 50D, what does JL guarantee about the ratio of pairwise distances?

## Architecture Onboarding

- Component map: Stage 1 (Reference Training) -> Stage 2 (Storage) -> Stage 3 (Estimation)
- Critical path:
  1. Stage 2 storage quality (all T² pairs) -> determines Stage 3 accuracy ceiling
  2. Random projection dimension k -> controls memory vs. accuracy tradeoff
  3. Number of batches T -> storage scales O(T²), estimation time O(T) per order
- Design tradeoffs:
  - FUT vs FUT++: First-order is simpler; second-order more accurate for larger models but requires ∇²Γ storage
  - Batch granularity: More batches = finer estimation but T² storage explosion
  - Projection dimension k: Per-layer tuning needed; smaller layers can use k=8, larger need k=300
- Failure signatures:
  - AbsDiff > 0.1: Taylor approximation degrading; check if |γt - θt| is growing too large
  - Estimation plateaus (flat perplexity across steps): Small gradients causing negligible updates (observed in Figure 7)
  - Memory overflow on Stage 2: Reduce k or use gradient checkpointing for Hessian approximation
- First 3 experiments:
  1. Validate estimation accuracy: Train 636M param LLaMA on Wikitext with T=32 batches. Compute AbsDiff between FUT-estimated and true perplexity for 10 random permutations. Target: match paper's 0.0290 AbsDiff.
  2. Ablate projection dimension: Test k ∈ {8, 80, 160, 300} on a single layer. Plot reconstruction error ||M̂ - M|| / ||M|| vs. memory saved.
  3. Curriculum search sanity check: Run genetic algorithm (N=16, K=8 generations) to find curriculum. Verify FUT-guided curriculum beats Random Order baseline by >0.04 perplexity points.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does FUT's estimation accuracy generalize to downstream NLU and reasoning tasks, beyond perplexity?
- Basis in paper: [explicit] "We evaluate the effectiveness of our FUT framework solely based on perplexity performance... downstream natural language understanding and reasoning tasks typically require large-scale models, which are infeasible to retrain repeatedly. Nevertheless, further validation is needed to assess the generalizability of our framework in these more complex tasks."
- Why unresolved: The computational cost of repeated retraining for large models on diverse downstream tasks makes systematic validation impractical with current resources.
- What evidence would resolve it: Correlation analysis between FUT-estimated perplexity and actual performance on benchmarks like MMLU, HellaSwag, or GSM8K for models trained under different sample orders.

### Open Question 2
- Question: At what model scale do higher-order nonlinearities cause first- and second-order Taylor approximations to break down?
- Basis in paper: [explicit] "The accuracy of our estimates relies on the validity of Taylor expansions, particularly when higher-order nonlinearities dominate the optimization dynamics—scenarios where our first- and second-order approximations may fall short."
- Why unresolved: The relationship between model scale, optimization landscape complexity, and Taylor approximation validity has not been systematically characterized.
- What evidence would resolve it: Empirical tracking of AbsDiff as model size increases (e.g., 1B → 7B → 70B), with analysis of when higher-order gradient magnitudes become significant relative to first/second-order terms.

### Open Question 3
- Question: How does FUT perform under multi-epoch training regimes compared to the single-epoch setting tested?
- Basis in paper: [inferred] The paper states "Following common practice, we use the Adam optimizer for LLM training and train for a single epoch," but real-world LLM pre-training typically involves multiple epochs. The recursive error accumulation in Equation (4) may compound differently across repeated data exposures.
- Why unresolved: Multi-epoch training introduces repeated sample interactions and potential error accumulation that the current framework does not address.
- What evidence would resolve it: Comparison of FUT estimation accuracy between 1-epoch and 3-epoch training runs, measuring whether AbsDiff grows non-linearly with training duration.

## Limitations
- **Scalability limits**: O(T²) storage complexity creates prohibitive memory requirements for very large T or even larger models, with practical scaling limits unclear beyond 1.4B parameters
- **Approximation validity domain**: Taylor expansion-based approximation assumes smooth optimization landscapes; for highly non-convex regions or when |γt - θt| grows large, approximation error may accumulate beyond reported bounds
- **Random projection calibration**: Selection of projection dimension k per layer remains underspecified with no clear procedural guidance for mapping k values to specific update terms and layer types

## Confidence
- **High Confidence**: The core retraining-free estimation framework (Stage 1-3 architecture), random projection methodology, and measured performance gains (up to 132.6× speedup, AbsDiff within reported bounds) are well-supported by experimental results and mathematical derivation
- **Medium Confidence**: The effectiveness of Taylor expansion for Adam updates across different training orders is plausible but relies on assumptions about optimization landscape smoothness that may break in practice for very long trajectories or highly non-convex loss regions
- **Low Confidence**: The precise calibration of random projection dimensions per layer and the generalization of results beyond the 636M-1.4B parameter range remain inadequately specified

## Next Checks
1. **Scalability stress test**: Apply the framework to a 3B+ parameter model on the same task. Measure whether storage requirements remain manageable and whether AbsDiff stays within the 0.0085-0.0917 range. This will reveal practical scaling limits.
2. **Approximation error accumulation**: For a fixed model and dataset, systematically increase the number of batches T (e.g., T ∈ {32, 64, 128, 256}) and track how AbsDiff between estimated and true perplexity grows. This quantifies the validity range of the Taylor approximation.
3. **Random projection sensitivity**: For a single layer, test k values across the full range {8, 20, 80, 160, 200, 300} and measure reconstruction error ||M̂ - M|| / ||M|| and downstream estimation accuracy. This will clarify the memory-accuracy tradeoff and provide guidance for k selection.