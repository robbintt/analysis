---
ver: rpa2
title: Optimistic Task Inference for Behavior Foundation Models
arxiv_id: '2510.20264'
source_url: https://arxiv.org/abs/2510.20264
tags:
- reward
- opti-bfm
- task
- episode
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses task inference for Behavior Foundation Models
  (BFMs) without requiring large labeled datasets. Standard BFMs need labeled (state,
  reward) pairs for task inference, which can be expensive.
---

# Optimistic Task Inference for Behavior Foundation Models

## Quick Facts
- **arXiv ID**: 2510.20264
- **Source URL**: https://arxiv.org/abs/2510.20264
- **Reference count**: 40
- **Primary result**: OpTI-BFM identifies and optimizes unseen reward functions within a handful of episodes with minimal labeled data, outperforming random exploration on DeepMind Control Suite benchmarks.

## Executive Summary
Standard Behavior Foundation Models (BFMs) require large labeled datasets for task inference, limiting their practical deployment. OpTI-BFM introduces an online framework that actively collects data during deployment to infer tasks with minimal labeling. The method uses a least-squares estimator to track a belief over task embeddings and employs an optimistic acquisition function to guide data collection. By leveraging the linear structure between features and rewards, OpTI-BFM reduces task inference to a bandit problem over behaviors, enabling sample-efficient exploration. On established zero-shot benchmarks in DeepMind Control Suite, OpTI-BFM consistently identifies and optimizes unseen reward functions within a handful of episodes with minimal compute overhead, achieving performance comparable to Oracle methods with significantly less labeled data.

## Method Summary
OpTI-BFM operates on pre-trained Behavior Foundation Models built on Universal Successor Features. During deployment, it maintains a least-squares estimate of the task embedding and a precision matrix over this estimate. At each step, it selects task embeddings by optimizing an upper confidence bound acquisition function that balances exploration and exploitation. The selected embedding conditions the BFM's policy, and the resulting feature-reward pairs are used to update the belief over the true task embedding. This online approach enables task inference with minimal labeled data, reducing the problem to a linear contextual bandit that can be solved with standard algorithms.

## Key Results
- OpTI-BFM identifies unseen reward functions within 5-10 episodes on DeepMind Control Suite tasks
- Achieves Oracle-level performance with 10× less labeled data through label thresholding
- Maintains performance across varying levels of environmental stochasticity and non-stationarity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing policy search to a linear bandit problem enables sample-efficient task inference with provable regret bounds.
- Mechanism: For well-trained Universal Successor Feature models, the expected return of a policy conditioned on embedding z is approximately linear in the successor features via the inner product with the true task embedding. This linear structure allows the control problem to be reformulated as online linear optimization, where standard bandit algorithms apply.
- Core assumption: The BFM is well-trained (perfect USF approximation) and the reward lies in the span of features with sub-Gaussian noise.
- Evidence anchors: Abstract states linear structure reduces task inference to bandit problem; section 3.2 explains policy search reduces to online optimization of linear function.
- Break condition: If successor features are poorly estimated or reward function has significant nonlinear components outside feature span, linearity assumption fails and regret guarantees do not hold.

### Mechanism 2
- Claim: Optimistic data acquisition accelerates belief convergence by prioritizing state-visitation patterns that maximize information gain about the task embedding.
- Mechanism: OpTI-BFM maintains a confidence ellipsoid over task embeddings via online least-squares. The acquisition function selects the embedding that maximizes the upper confidence bound, encouraging exploration of embeddings where uncertainty in expected return is highest, which induces informative state distributions.
- Core assumption: Optimization oracle can approximately solve acquisition function and feature distribution covers relevant state space.
- Evidence anchors: Abstract mentions optimistic acquisition function guides data collection; section 3.2 explains conditioning on most promising task embedding compatible with observed rewards.
- Break condition: If BFM's policy repertoire is insufficient or confidence ellipsoid is poorly calibrated, optimism may not translate to informative exploration.

### Mechanism 3
- Claim: Reward-level feedback provides tighter confidence sets than return-level feedback, improving sample efficiency.
- Mechanism: OpTI-BFM uses per-step (feature, reward) pairs instead of episodic returns. The precision matrix from reward-level feedback grows at least as fast as from return-level feedback, yielding tighter confidence ellipsoids for the same number of episodes.
- Core assumption: Rewards are observed per-step and horizon is finite.
- Evidence anchors: Section A.2 shows confidence sets may be tighter with reward-level feedback; section 4 highlights crucial difference involving features and successor features.
- Break condition: If reward labels are unavailable per-step and must be inferred from episodic returns alone, confidence sets will be looser, requiring more episodes.

## Foundational Learning

- **Successor Features (SFs) and Universal Successor Features (USFs)**: Essential for understanding how ψ^π(s, a) represents expected discounted sum of features and how z^⊤ψ gives Q-values. Quick check: Given policy π and feature map ϕ, can you write ψ^π(s, a) expression and explain Q^π_r(s, a) = z^⊤ψ^π(s, a) when r(s) = z^⊤ϕ(s)?

- **Linear Contextual Bandits and UCB Algorithms (LinUCB/OFUL)**: OpTI-BFM's regret bound directly inherits from OFUL analysis. Understanding confidence ellipsoids and exploration-exploitation tradeoff is prerequisite. Quick check: In linear bandit with unknown parameter θ*, why does selecting argmax_x [x^⊤θ̂ + β‖x‖_{V^{-1}}] balance exploration and exploitation? What happens if β is too small?

- **Behavior Foundation Models and Zero-Shot RL**: OpTI-BFM is test-time inference procedure applied to pre-trained BFM; it doesn't train BFM itself. Understanding offline pre-training phase and (π_z, ψ_z) parameterization is necessary. Quick check: In USF-based BFM, what is role of task embedding z during pre-training vs. during task inference? Why does same z retrieve (approximately) optimal policy for r(s) = z^⊤ϕ(s)?

## Architecture Onboarding

- **Component map**: Pre-trained BFM -> Online Least-Squares Estimator -> Confidence Ellipsoid Calculator -> Acquisition Optimizer -> Action Execution

- **Critical path**: Start with uniform prior over Z. At each step: (a) sample candidate z's from confidence ellipsoid, (b) compute UCB for each via forward pass through ψ network, (c) select maximizing z, (d) execute action and receive (ϕ(s_t), r_t), (e) update estimator via Cholesky update. After convergence, optionally fix z = ẑ_t for deployment.

- **Design tradeoffs**:
  - UCB vs. Thompson Sampling: UCB provides regret guarantees and better empirical performance; TS avoids optimization problem and is computationally lighter but shows performance gap on Cheetah.
  - Step-level vs. Episode-level updates: Step-level converges faster empirically but lacks theoretical guarantees; episode-level has regret bound.
  - Label request thresholding: Setting κ > 0 reduces labeling cost by 10× while maintaining performance on easier tasks, but introduces hyperparameter.
  - Decay rate for non-stationary rewards: ρ < 1 enables adaptation to changing rewards but may slow convergence if too small.

- **Failure signatures**:
  1. Slow/no convergence: Check if feature network has collapsed or β_t is set too low.
  2. Excessive exploration (low return): May indicate β_t too high or poor BFM pre-training.
  3. High variance across seeds: Indicates sensitivity to initialization or hyperparameters; verify λ, β/σ choices.
  4. Non-stationary failure: If task changes, ensure ρ < 1 and decay rate matches change frequency.

- **First 3 experiments**:
  1. Sanity check on Walker-walk: Run OpTI-BFM for 10 episodes with default hyperparameters. Verify relative performance reaches >0.8 within 5 episodes and plot ‖ẑ_t - z_r‖ over time.
  2. Ablation: UCB vs. TS vs. Random on Cheetah-run. Confirm UCB outperforms TS and both outperform Random significantly.
  3. Label efficiency test on Walker-stand: Run with κ ∈ {0, 0.03, 0.1, 0.3} and plot average reward vs. # labels requested. Confirm κ=0.1 achieves comparable performance to κ=0 with ~10× fewer labels.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can OpTI-BFM be extended to jointly update task embeddings and fine-tune the underlying BFM parameters for long-horizon tasks? The conclusion states fine-tuning additional components may provide even better performance than updating task embedding alone. Unresolved because current framework freezes BFM weights to maintain linear bandit structure; adapting parameters simultaneously breaks theoretical guarantees.

- **Open Question 2**: How do geometric properties of feature space Z impact convergence speed and stability? The conclusion highlights understanding properties of feature space elements constitutes important future work. Unresolved because current analysis treats feature space abstractly; unclear how properties like rank, sparsity, or conditioning influence confidence ellipsoid volume.

- **Open Question 3**: How does OpTI-BFM perform in domains where linearity assumption between features and rewards is strongly violated? The paper notes empirical success even when assumptions are violated. Unresolved because paper doesn't quantify robustness limits against high non-linearity or model misspecification.

## Limitations
- Performance heavily depends on quality of pre-trained BFM (Assumption A1); poor USF approximation breaks linear structure
- Requires per-step reward observations for optimal sample efficiency; sparse-reward settings may lose gains
- Theoretical guarantees rely on well-specified linear reward structure; performance in highly nonlinear domains is unexplored

## Confidence
- **High Confidence**: Overall framework of using optimistic acquisition for task inference; empirical demonstration of outperforming random exploration and achieving Oracle-level performance with minimal labeled data
- **Medium Confidence**: Theoretical regret bound and its reliance on linear bandit literature; actual regret on complex tasks may exceed bounds due to approximation errors
- **Low Confidence**: Comparison of reward-level vs. return-level feedback is theoretically justified but lacks empirical validation; claim of tighter confidence sets not directly tested

## Next Checks
1. **Stress Test A1**: Systematically vary quality of pre-trained BFM (limit training data or model capacity) and measure how OpTI-BFM's performance degrades to quantify "well-trained" assumption.

2. **Reward Feedback Ablation**: Implement return-level baseline using G̃_k instead of (ϕ_k,t, r_k,t) pairs and compare convergence rates and final performance to validate practical importance of reward-level feedback.

3. **Cross-Domain Generalization**: Evaluate OpTI-BFM on task outside DMC suite (robotic manipulation or navigation) to test if method generalizes beyond paper's evaluation domains.