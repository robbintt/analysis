---
ver: rpa2
title: 'From Directions to Regions: Decomposing Activations in Language Models via
  Local Geometry'
arxiv_id: '2602.02464'
source_url: https://arxiv.org/abs/2602.02464
tags:
- local
- each
- language
- activation
- directions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a local-geometry approach to decomposing activation
  space in language models using Mixtures of Factor Analyzers (MFA). Instead of modeling
  activations as combinations of isolated global directions, MFA partitions the space
  into regions and learns low-rank subspaces within each region to capture local variation.
---

# From Directions to Regions: Decomposing Activations in Language Models via Local Geometry

## Quick Facts
- arXiv ID: 2602.02464
- Source URL: https://arxiv.org/abs/2602.02464
- Reference count: 40
- This work introduces a local-geometry approach to decomposing activation space in language models using Mixtures of Factor Analyzers (MFA), achieving significantly higher interpretability than SAEs while using fewer components.

## Executive Summary
This work introduces a local-geometry approach to decomposing activation space in language models using Mixtures of Factor Analyzers (MFA). Instead of modeling activations as combinations of isolated global directions, MFA partitions the space into regions and learns low-rank subspaces within each region to capture local variation. This yields a simple, interpretable decomposition: an activation is described by its region assignment and within-region offset. Experiments on Llama-3.1-8B and Gemma-2-2B show that MFA discovers semantically coherent Gaussians—some narrow and token-specific, others broad and thematic—whose local subspaces encode semantic or syntactic variation. Compared to SAEs, MFA achieves significantly higher interpretability (0.96 vs. 0.29 fraction of interpretable features) while using fewer components. On causal localization benchmarks, MFA outperforms unsupervised baselines and matches or exceeds supervised methods like DAS. On steering tasks, MFA centroids promote interpretable concepts and achieve stronger alignment and coherence than SAE features. These results position local subspace structure as a promising unit for scalable concept discovery and model control.

## Method Summary
MFA decomposes activations into two compositional geometric objects: region centroids and local variation from centroids. The method uses K-Means to initialize centroids, then trains a mixture model with K components, each with a low-rank loading matrix (rank R=10) and diagonal noise. The model is trained via gradient descent to minimize negative log-likelihood on 100M activations from The Pile. At inference, each activation is assigned responsibilities over centroids and local coordinates within the chosen component. Steering is performed either by interpolating toward centroids (for broad concepts) or adding offsets in local subspaces (for local variation).

## Key Results
- MFA achieves significantly higher interpretability (0.96 vs. 0.29 fraction of interpretable features) compared to SAEs while using fewer components
- On causal localization benchmarks, MFA outperforms unsupervised baselines and matches or exceeds supervised methods like DAS
- On steering tasks, MFA centroids promote interpretable concepts and achieve stronger alignment and coherence than SAE features

## Why This Works (Mechanism)

### Mechanism 1: Region-Conditioned Decomposition
- **Claim:** Partitioning activation space into local Gaussian regions with low-rank subspaces yields more interpretable decompositions than global dictionaries.
- **Mechanism:** MFA decomposes each activation x into two compositional objects: (1) responsibilities R_k(x) assigning x to region centroids μ_k, and (2) within-region coordinates ẑ_k in that component's low-rank subspace. The reconstruction x ≈ Ab(x) (Eq. 11-13) uses a shared dictionary of centroids and loadings, but the decomposition is region-conditioned rather than a sparse sum over global directions.
- **Core assumption:** Activations cluster semantically, and within-cluster variation is well-approximated by a small number of degrees of freedom.
- **Evidence anchors:**
  - [abstract]: "MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid."
  - [section 3]: "Each activation is described by where it sits in activation space, via responsibilities over centroids... and how it varies locally, via within-component coordinates."
  - [corpus]: Related work (Lee et al., 2025; Saglam et al., 2025) shows LMs exhibit local low-dimensional structure, supporting the manifold assumption. FMR=0.53 avg.

### Mechanism 2: Absolute Positions Encode Semantic Themes
- **Claim:** Centroid locations (μ_k) encode broad semantic themes that can be steered via interpolation.
- **Mechanism:** Steering via f_μ(x) = (1-α)x + αμ (Eq. 14) moves activations toward absolute positions associated with specific themes. The centroid represents "where similar contexts pass through" in activation space, so interpolation reliably induces the broad topic (Table 1: genre centroids promote genre tokens; narrow centroids promote token-level patterns).
- **Core assumption:** Centroids capture semantically meaningful absolute positions, not just statistical artifacts.
- **Evidence anchors:**
  - [section 3]: "We interpolate toward μ because it is an absolute location in activation space."
  - [section 6.2]: "MFA scores significantly higher than SAEs and DiffMeans in concept score, indicating it promotes very interpretable concepts."
  - [corpus]: Weak direct corpus evidence on absolute position encoding—this is a novel claim from this paper.

### Mechanism 3: Multi-Gaussian Concepts Tile Complex Semantics
- **Claim:** Complex concepts are expressed as neighborhoods of nearby Gaussians rather than single components.
- **Mechanism:** MFA components form semantic neighborhoods via kNN graphs over centroids. Broad concepts (e.g., "emotions") span multiple narrower components (happiness, surprise, intimacy) that collectively tile the region (Figure 1). Increasing K splits broad regions into finer-grained Gaussians with more context-dependent loadings.
- **Core assumption:** Concepts have hierarchical or clustered geometric structure in activation space.
- **Evidence anchors:**
  - [section 4]: "Globally nonlinear concepts are often expressed not by a single component but by a cluster of neighboring Gaussians."
  - [section 4, Figure 3]: "Larger K increases the portion of narrow Gaussians... and increases the frequency of semantic loadings, suggesting that within-component variation becomes more context-dependent."
  - [corpus]: Engels et al. (2025) and Park et al. (2025) provide evidence for multi-dimensional concept structure; FMR=0.59 for related work on feature manifolds.

## Foundational Learning

- **Concept: Factor Analysis (FA)**
  - **Why needed here:** MFA extends FA by learning multiple component-specific subspaces. Understanding that FA models data as x = Wz + ε with low-rank W and diagonal noise Ψ is essential for interpreting MFA's covariance structure (C_k = W_k W_k^T + Ψ).
  - **Quick check question:** Given a 4096-dim activation, if local variation has rank R=10, how many parameters does the factor loading matrix W_k require?

- **Concept: Mixture Models and Posterior Responsibilities**
  - **Why needed here:** Component assignment uses Bayes theorem to compute R_k(x) = p(k|x) (Eq. 8). The responsibilities determine which region's geometry explains a given activation and weight the reconstruction.
  - **Quick check question:** If an activation has equal likelihood under two nearby Gaussians with π₁=0.3 and π₂=0.7, which component has higher responsibility?

- **Concept: Low-Rank Subspace Geometry**
  - **Why needed here:** The meaningful object is span(W_k), not individual loading vectors (rotation invariance, §2). Steering with loadings uses additive offsets in this subspace (Eq. 15), while centroids use interpolation (Eq. 14)—different operations for absolute vs. relative positions.
  - **Quick check question:** Why can't we interpret individual loading axes without a rotation convention?

## Architecture Onboarding

- **Component map:** Activations (from residual stream at layer L) -> K-Means initialization (4M sample) -> MFA training (100M activations, gradient descent on NLL) -> Learned parameters: {μ_k, W_k, Ψ, π_k} for k=1..K -> Inference: Compute responsibilities R_k(x), latent coords ẑ_k -> Steering: f_μ(x) = (1-α)x + αμ OR f_w(x) = x + Wv

- **Critical path:** Initialization quality -> K-Means on 4M samples is essential; random init converges to poor solutions with low centroid diversity (Table 3). Training on 100M activations from The Pile.

- **Design tradeoffs:**
  - K (components): 1K→8K yields large MSE reduction; 8K→32K diminishing returns (Table 4). Larger K → more narrow/token-driven Gaussians.
  - R (rank): Set uniformly to R=10 across components. Acts as "conservative approximation to local intrinsic dimension" (§3). Higher R may overfit noise.
  - Layer selection: Layers at ~1/3 and ~2/3 depth (Gemma: 6, 18; Llama: 8, 22) capture different abstraction levels.

- **Failure signatures:**
  - High reconstruction error on OOD activations: MFA commits to a single component; if activation lies in unmodeled region, assigns to nearest Gaussian with poor fit (§D).
  - Steering with additive centroid interventions: Performance drops from 0.24→0.12 (§E). Centroids encode absolute position—use interpolation.
  - SAE-style interpolation on MFA: Loadings are subspaces, not directions; additive offsets are appropriate for loadings, not centroids.

- **First 3 experiments:**
  1. **Reconstruction sanity check:** Train MFA with K=1K on 10M activations; compute MSE vs. K-Means baseline. Expect ~1.3-1.5x improvement over K-Means (Table 4).
  2. **Steering ablation:** Apply centroid interpolation (α=0.3-0.5) vs. additive intervention to 50 Gaussians. Measure concept/fluency scores. Confirm interpolation outperforms addition.
  3. **Localization probe:** On RAVEL Continent task, train DBM over MFA components. Compare centroids-only vs. centroids+loadings. Expect centroids sufficient for entity-level variables (Continent), loadings needed for positional pointers (MCQA).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the number of Gaussian components (K) and latent rank (R) be optimally selected for a given model and layer?
- Basis in paper: [inferred] The authors use fixed values K∈{1K,8K,32K} and R=10 "for simplicity" and as "a conservative approximation," without providing principled selection criteria.
- Why unresolved: Different layers and models may have different intrinsic dimensionalities and region structures; uniform settings may under- or over-parameterize some regions.
- What evidence would resolve it: Systematic ablations varying K and R per layer, or methods to estimate local intrinsic dimensionality before training.

### Open Question 2
- Question: How can MFA be extended to systematically identify and manipulate multi-Gaussian "constellations" that jointly represent complex concepts?
- Basis in paper: [explicit] The authors observe that "concepts may be realized not by a single component, but by constellations of nearby Gaussians that jointly cover a semantic, complex region" (Section 4).
- Why unresolved: Current analysis uses kNN graphs and BFS traversal heuristically; no principled framework for discovering, aggregating, or intervening on concept-level constellations.
- What evidence would resolve it: Benchmarking constellation-level interventions against single-Gaussian interventions on tasks requiring multi-faceted concepts.

### Open Question 3
- Question: How does MFA performance degrade on out-of-distribution activations, and can this be mitigated?
- Basis in paper: [explicit] Appendix D states: "A key limitation of MFA is that it explicitly models the activation distribution it is trained on... when an activation lies in a region that is rare or out of distribution... MFA may assign it to the nearest available component even if none provides a good local fit."
- Why unresolved: No quantitative analysis of OOD failure modes or mitigation strategies was conducted.
- What evidence would resolve it: Evaluation on held-out domains or adversarially constructed activations, comparing reconstruction error and steering fidelity.

### Open Question 4
- Question: Can MFA's local geometry framework be combined with task-specific or concept-specific learning objectives?
- Basis in paper: [inferred] Related work notes that prior subspace methods "are either learned via concept- or task-specific objectives or lack a shared, scalable representation of local geometry." MFA provides the latter but does not explore integration with the former.
- Why unresolved: The paper treats MFA as purely unsupervised; whether supervised fine-tuning of regions improves localization/steering remains untested.
- What evidence would resolve it: Experiments fine-tuning MFA components on labeled concept data and measuring gains on localization benchmarks.

## Limitations
- MFA's reconstruction quality degrades on out-of-distribution activations, suggesting limited extrapolation capability
- The mechanism's generality across architectures, layers, and domains remains untested
- The claim that centroids encode absolute semantic positions lacks extensive ablation on alternative steering targets

## Confidence
- **High confidence**: MFA's superior interpretability (IF=0.96 vs. SAEs=0.29) and reconstruction performance (1.3-1.5× K-Means baseline) are well-supported by Table 4 and steering experiments
- **Medium confidence**: The claim that centroids encode broad semantic themes (Mechanism 2) is plausible but weakly anchored—corpus evidence for absolute position encoding in LMs is sparse, and steering ablations are not exhaustive
- **Low confidence**: The hierarchical concept structure (Mechanism 3) is conceptually compelling (Figure 1, Table 4) but relies on visual interpretation and limited quantitative validation; multi-Gaussian concepts could be artifacts of MFA's parameterization rather than true semantic organization

## Next Checks
1. **Ablation on steering targets**: Compare centroid interpolation vs. additive centroid interventions on a broader set of concepts (beyond Table 1) to confirm that absolute positions, not just proximity, drive steering performance
2. **OOD robustness test**: Evaluate MFA reconstruction and steering on a held-out domain (e.g., biomedical or legal text) to quantify extrapolation limits and compare against SAEs' generalization
3. **Hierarchical concept validation**: Use kNN graphs over centroids to identify multi-Gaussian concepts (e.g., "emotions"), then test whether removing intermediate Gaussians degrades concept coherence—quantifying the necessity of neighborhoods vs. single components