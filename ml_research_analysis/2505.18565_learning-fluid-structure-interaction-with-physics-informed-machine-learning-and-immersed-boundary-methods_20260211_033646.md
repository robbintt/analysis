---
ver: rpa2
title: Learning Fluid-Structure Interaction with Physics-Informed Machine Learning
  and Immersed Boundary Methods
arxiv_id: '2505.18565'
source_url: https://arxiv.org/abs/2505.18565
tags:
- fluid
- boundary
- velocity
- pressure
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fluid-structure interaction (FSI) problems
  with moving boundaries, where traditional PINN architectures struggle to capture
  the distinct physics governing fluid and structural domains simultaneously. The
  authors propose an innovative Eulerian-Lagrangian PINN architecture that integrates
  immersed boundary method (IBM) principles, using domain-specific neural networks
  for fluid dynamics (Eulerian) and structural interfaces (Lagrangian), coupled through
  physics-based constraints.
---

# Learning Fluid-Structure Interaction with Physics-Informed Machine Learning and Immersed Boundary Methods

## Quick Facts
- arXiv ID: 2505.18565
- Source URL: https://arxiv.org/abs/2505.18565
- Reference count: 40
- Primary result: Domain-decomposed PINN architecture with learnable B-spline activations reduces pressure errors from 12.9% to 2.39% in FSI problems

## Executive Summary
This paper addresses the challenge of fluid-structure interaction (FSI) problems with moving boundaries, where traditional physics-informed neural network (PINN) architectures struggle to simultaneously capture the distinct physics governing fluid and structural domains. The authors propose an innovative Eulerian-Lagrangian PINN architecture that integrates immersed boundary method principles, using separate neural networks for fluid dynamics (Eulerian) and structural interfaces (Lagrangian), coupled through physics-based constraints. The approach demonstrates significant improvements in accuracy compared to unified PINN architectures, particularly in pressure predictions within structural regions.

## Method Summary
The proposed method implements a domain-decomposed PINN architecture with two specialized neural networks: an Eulerian network for fluid dynamics on a fixed grid and a Lagrangian network for structural interface dynamics using moving markers. These networks are coupled through physics-based constraints including velocity continuity and pressure gradient matching at the interface. The architecture incorporates learnable B-spline activation functions combined with SiLU to capture both localized high-gradient features near interfaces and global flow patterns. The approach is evaluated on a 2D lid-driven cavity flow problem with a moving elastic disc, demonstrating substantial accuracy improvements over baseline unified PINN architectures.

## Key Results
- EL-L architecture achieves 24.1-91.4% improvement in accuracy across all metrics compared to baseline unified PINNs
- Pressure errors in structural regions reduced from 12.9% (baseline) to 2.39% (EL-L)
- Domain decomposition mitigates spectral bias and parameter interference in heterogeneous physics problems
- Learnable B-spline+SiLU activations capture localized high-frequency features while maintaining smooth global flow patterns

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specialized Parameter Spaces
The paper suggests that decoupling the neural architecture into separate Eulerian (fluid) and Lagrangian (structure) networks mitigates "spectral bias" and parameter interference caused by heterogeneous physics. Instead of forcing a single network to learn both smooth fluid velocity fields and sharp structural pressure gradients, the architecture allocates distinct parameter spaces.

### Mechanism 2: Locality-Aware Learnable Activations
Replacing fixed Tanh activations with learnable B-spline + SiLU functions appears essential for capturing localized high-frequency features at moving interfaces. B-splines provide compact support and learnable control points, allowing the network to dynamically increase resolution specifically near sharp boundaries.

### Mechanism 3: Soft Constraint Coupling over Hard Parameter Sharing
Coupling the two domains via physics-based loss terms (no-slip, pressure continuity) rather than hard parameter sharing allows the model to satisfy interface conditions without forcing incompatible representations into shared weights. The loss function penalizes velocity mismatches and pressure discontinuities at the interface.

## Foundational Learning

- **Concept: Immersed Boundary Method (IBM)**
  - Why needed: The architecture mimics IBM by using a fixed Eulerian grid for fluid and moving Lagrangian markers for the structure.
  - Quick check: Can you explain why IBM avoids "remeshing" compared to body-fitted methods like ALE?

- **Concept: Spectral Bias in Neural Networks**
  - Why needed: The paper justifies its complex architecture by arguing that standard MLPs fail to learn high-frequency (sharp gradient) components simultaneously with low-frequency ones.
  - Quick check: Why might a standard neural network learn the overall flow direction easily but fail to capture a sharp pressure spike at a wall?

- **Concept: Kolmogorov-Arnold Networks (KANs)**
  - Why needed: The proposed learnable activations are implemented via KANs. Understanding that learnable functions reside on edges rather than nodes is key to implementing the B-spline layers.
  - Quick check: In a KAN, what is being "learned" during backpropagation—the weight multiplying the input, or the shape of the activation function itself?

## Architecture Onboarding

- **Component map:**
  1. Eulerian Network: MLP or KAN taking (t, x, y) from fluid grid → outputs (û, v̂, p̂)
  2. Lagrangian Network: Smaller MLP or KAN taking (t, x, y) from structure markers → outputs (ũ, ṽ, p̃)
  3. Interface Constraints: A loss module that queries both networks at the interface boundary ξ to compute coupling losses

- **Critical path:** Defining the interface sampling. The model succeeds or fails based on how well the interface loss Lξ is computed. You must accurately identify which Eulerian grid points correspond to Lagrangian marker locations to enforce the no-slip condition.

- **Design tradeoffs:**
  - Accuracy vs. Convergence Speed: EL-L model achieves best accuracy but converges slower than baseline due to complexity of balancing coupled loss terms
  - Capacity vs. Overfitting: Eulerian network is larger (350 neurons/layer) than Lagrangian one (50 neurons/layer), reflecting relative complexity of fluid domain vs. interface

- **Failure signatures:**
  - High Pressure RMSE in Structure: If using unified Baseline model, expect ~12% error in structure pressure
  - Oscillatory Training Loss: If using Learnable activations (EL-L), expect oscillatory behavior in later epochs due to gradient variability from spline coefficients

- **First 3 experiments:**
  1. Baseline vs. EL Ablation: Train unified Baseline and decoupled EL model on cavity dataset. Verify EL reduces structure velocity error from ~2% to <1%
  2. Activation Sensitivity: Swap Tanh activations in Eulerian network for B-spline+SiLU setup. Monitor for oscillatory training behavior and check if pressure error drops below 5%
  3. Interface Sampling Robustness: Reduce sampling density of Lagrangian interface points. Determine if soft coupling constraints degrade gracefully or if pressure prediction fails completely

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the EL-L architecture maintain its accuracy advantages at higher Reynolds numbers (Re >> 100) and with significantly deformed structures?
- Basis in paper: Authors state current evaluation uses Re=100 and note IBM struggles "particularly at high Reynolds numbers or significant structural deformations"
- Why unresolved: The paper evaluates only a single case study with moderate flow conditions; spectral bias challenges may intensify at higher Reynolds numbers
- What evidence would resolve it: Benchmark EL-L architecture on FSI problems with Re ≥ 1000 and structures undergoing large deformation

### Open Question 2
- Question: Will incorporating explicit force-coupling terms into the loss function improve momentum conservation and interfacial accuracy without destabilizing training?
- Basis in paper: Authors state "Future work will incorporate force-coupling terms to improve momentum conservation and interfacial accuracy"
- Why unresolved: Current formulation relaxes standard IBM force-spreading equation, relying only on velocity continuity and pressure gradient constraints
- What evidence would resolve it: Extend loss function to include Lagrangian force-spreading terms and measure changes in momentum conservation error

### Open Question 3
- Question: What are the minimum collocation point sampling requirements for EL-L architecture to achieve convergence, and how sensitive is performance to sampling density?
- Basis in paper: The paper uses 0.005% fluid and 0.05% interface data but does not systematically vary sampling density
- Why unresolved: Without sampling sensitivity analysis, it is unclear whether architectural improvements reduce data requirements
- What evidence would resolve it: Conduct ablation studies varying fluid and interface collocation point densities, reporting convergence rates and final errors

## Limitations
- Single test case with moderate Reynolds number (Re=100) limits generalizability to turbulent or high-speed flows
- Oscillatory convergence behavior with learnable activations suggests potential training instability
- Current formulation relies on soft constraint coupling that may be insufficient for strongly coupled FSI problems

## Confidence
- **High confidence**: Domain decomposition reduces parameter interference and improves accuracy
- **Medium confidence**: B-spline+SiLU activations provide meaningful accuracy gains
- **Medium confidence**: Soft constraint coupling effectively enforces interface conditions

## Next Checks
1. **Generalization Test**: Apply EL-L architecture to FSI problem with strong momentum coupling (e.g., flow-induced vibration at higher Reynolds numbers) to test if soft constraints remain sufficient when force transfer dominates
2. **Activation Ablation**: Conduct controlled experiments replacing B-spline+SiLU with alternative learnable activations (e.g., Fourier features, SIREN) in EL architecture to isolate contribution of activation flexibility
3. **Convergence Stability Analysis**: Systematically vary learning rates and batch sizes for EL-L model to quantify conditions under which oscillatory convergence emerges and determine if gradient clipping or adaptive optimizers mitigate instability