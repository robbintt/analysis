---
ver: rpa2
title: 'QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation'
arxiv_id: '2601.21049'
source_url: https://arxiv.org/abs/2601.21049
tags:
- retrieval
- query
- quark
- queries
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles retrieval under non-faithful (noisy, incomplete,
  or distorted) queries, where key semantics are missing and standard retrievers fail.
  The authors formalize this as retrieval under recall noise, where the observed query
  is a noisy realization of a latent target item.
---

# QUARK: Robust Retrieval under Non-Faithful Queries via Query-Anchored Aggregation

## Quick Facts
- **arXiv ID:** 2601.21049
- **Source URL:** https://arxiv.org/abs/2601.21049
- **Reference count:** 40
- **Key outcome:** QUARK improves retrieval performance under non-faithful queries by generating multiple recovery hypotheses and combining them via query-anchored aggregation, outperforming base retrievers on Recall, MRR, and nDCG.

## Executive Summary
This paper addresses the challenge of retrieval under non-faithful queries, where key semantics are missing or distorted, causing standard retrievers to fail. The authors formalize this as retrieval under recall noise, where the observed query is a noisy realization of a latent target item. QUARK is introduced as a training-free framework that generates multiple recovery hypotheses to capture alternative interpretations of the latent intent, then combines their signals using query-anchored aggregation. The original query acts as a semantic anchor to stabilize ranking and prevent drift. Experiments on controlled simulations and BEIR benchmarks show consistent improvements over both lexical and dense retrievers, with ablation studies confirming the robustness and necessity of anchored aggregation.

## Method Summary
QUARK tackles retrieval under non-faithful queries by generating multiple recovery hypotheses as alternative interpretations of the latent target item, then aggregating their retrieval signals with the original query as a semantic anchor. This query-anchored aggregation stabilizes ranking and prevents semantic drift. The approach is training-free and works by combining evidence from both the original and hypothesized queries, rather than relying on a single, potentially degraded, input. The method is evaluated on simulated noisy queries and BEIR datasets, showing consistent gains in recall, MRR, and nDCG.

## Key Results
- QUARK consistently improves Recall, MRR, and nDCG over base lexical and dense retrievers on both controlled simulations and BEIR benchmarks (FIQA, SciFact, NFCorpus).
- Ablation studies confirm robustness to hypothesis count and show that anchored aggregation outperforms unanchored pooling.
- Modeling query uncertainty through recovery hypotheses, coupled with principled anchored aggregation, is essential for robust retrieval under non-faithful queries.

## Why This Works (Mechanism)
QUARK works by addressing the fundamental problem of missing or distorted semantics in non-faithful queries. Instead of relying on a single degraded query, it generates multiple recovery hypotheses that represent alternative interpretations of the latent target intent. These hypotheses provide controlled auxiliary evidence, while the original query acts as a semantic anchor, stabilizing the ranking and preventing drift into irrelevant semantics. This dual approach leverages both the known (original query) and the plausible (hypotheses), leading to more robust retrieval under uncertainty.

## Foundational Learning
- **Query-anchored aggregation:** Needed to stabilize ranking and prevent semantic drift; quick check: does the anchor consistently improve performance?
- **Recall noise:** Formalizes how observed queries deviate from latent targets; quick check: are simulations of recall noise realistic?
- **Training-free framework:** Enables adaptation without fine-tuning; quick check: how does this scale to large or diverse query sets?
- **Hypothesis generation:** Provides alternative interpretations of latent intent; quick check: does increasing hypothesis count improve robustness?
- **Non-faithful query modeling:** Addresses missing or distorted semantics; quick check: are the methods robust to severe query degradation?
- **Semantic anchoring:** Uses original query to maintain context; quick check: what happens if the anchor itself is highly noisy?

## Architecture Onboarding
- **Component map:** Original query -> Hypothesis generation -> Multiple retrieval signals -> Query-anchored aggregation -> Final ranking
- **Critical path:** Hypothesis generation and aggregation must be fast and stable to avoid latency issues.
- **Design tradeoffs:** Training-free approach trades adaptability for efficiency; anchored aggregation trades complexity for stability.
- **Failure signatures:** Degradation when anchor is severely noisy or irrelevant; performance drops if hypothesis generation is unreliable.
- **First experiments:** 1) Ablation: Remove anchor, assess drift; 2) Stress test: Degrade anchor, measure impact; 3) Scalability: Vary hypothesis count, evaluate robustness.

## Open Questions the Paper Calls Out
None.

## Limitations
- BEIR benchmark results rely on simulated non-faithful queries, not naturally occurring ones, limiting real-world applicability.
- Query-anchored aggregation assumes the original query is coherent enough to serve as an anchor; this may fail for completely malformed queries.
- The approach's performance relative to fine-tuned models for noisy queries is not thoroughly compared, leaving scalability and efficiency trade-offs unclear.

## Confidence
- **Methodological contributions and controlled experiment results:** High
- **BEIR benchmark results (simulated queries):** Medium
- **Generalization to naturally occurring non-faithful queries:** Low

## Next Checks
1. Evaluate QUARK on production query logs from commercial search engines to assess performance on naturally occurring non-faithful queries across different domains and user populations.
2. Conduct ablation studies comparing QUARK's training-free approach against fine-tuned dense retrievers on noisy query datasets to quantify the trade-off between adaptability and efficiency.
3. Test QUARK's robustness when the anchor query itself is severely degraded (e.g., completely irrelevant or empty) to establish the boundaries of the query-anchored aggregation mechanism's effectiveness.