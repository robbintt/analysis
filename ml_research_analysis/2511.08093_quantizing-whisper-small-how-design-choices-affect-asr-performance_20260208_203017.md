---
ver: rpa2
title: 'Quantizing Whisper-small: How design choices affect ASR performance'
arxiv_id: '2511.08093'
source_url: https://arxiv.org/abs/2511.08093
tags:
- quantization
- accuracy
- compression
- whisper-small
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates post-training quantization of Whisper-small
  across four libraries and multiple bit-widths. The authors assess accuracy, speed,
  and model size trade-offs on LibriSpeech datasets.
---

# Quantizing Whisper-small: How design choices affect ASR performance

## Quick Facts
- arXiv ID: 2511.08093
- Source URL: https://arxiv.org/abs/2511.08093
- Reference count: 0
- Primary result: Dynamic int8 quantization with Quanto provided best accuracy-speed-size balance, reducing model size by 57% while maintaining or improving baseline performance.

## Executive Summary
This paper evaluates post-training quantization of Whisper-small across four libraries and multiple bit-widths, assessing accuracy, speed, and model size trade-offs on LibriSpeech datasets. The authors found that dynamic int8 quantization with Quanto offered the best trade-off, reducing model size by 57% while improving on the baseline's word error rate. Static quantization degraded both speed and accuracy, likely due to Whisper's Transformer architecture. Lower-bit formats offered greater compression but reduced robustness, especially in noisy conditions. The study highlights that quantization choices should align with device constraints and acoustic environments.

## Method Summary
The paper evaluates post-training quantization (PTQ) of Whisper-small (244M parameters) across four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. The authors tested various bit-widths including int8, int4, int3, and nf4, applying both dynamic and static quantization schemes. Evaluation was performed on LibriSpeech test-clean and test-other subsets, measuring Word Error Rate (WER), Character Error Rate (CER), Real-Time Factor (RTF), and model size reduction. The study focused exclusively on PTQ without retraining, comparing performance on both CPU and GPU platforms.

## Key Results
- Dynamic int8 quantization with Quanto reduced model size by 57% while improving baseline WER on noisy test-other data
- Static quantization performed worse than fp32 baseline due to dequantization overhead on Transformer operations
- Per-channel quantization better preserved accuracy than per-tensor approaches for Whisper-small
- Sub-8-bit formats (int3/nf4) offered higher compression but degraded robustness on noisy speech

## Why This Works (Mechanism)

### Mechanism 1
Per-channel quantization with symmetric scaling better preserves ASR accuracy under acoustic variability than per-tensor approaches. Assigning independent scales to each output channel captures local weight distribution statistics, preserving fine-grained variation that per-tensor quantization flattens. Whisper's near-zero-centered weight distributions align well with symmetric quantization.

### Mechanism 2
Dynamic quantization outperforms static quantization for Whisper-small due to architecture-specific constraints. Static quantization fixes scales via calibration, but LayerNorm and Softmax lack efficient low-bit implementations, forcing repeated dequantization that eliminates expected speed gains. Dynamic quantization adapts scales at runtime, preserving accuracy.

### Mechanism 3
Moderate int8 quantization can act as implicit regularization, improving robustness on noisy speech. Quantization introduces controlled precision loss that may stabilize predictions under distribution shift (noise, accents). This effect does not persist below 8-bit precision.

## Foundational Learning

- **Quantization granularity (per-tensor vs. per-channel vs. per-group)**: Why needed here: The paper explicitly attributes performance differences to granularity choices. Per-tensor simplifies computation; per-channel preserves accuracy. Quick check question: Given a weight tensor of shape [4096, 1024], how many scale factors does per-channel quantization require vs. per-tensor?

- **Dynamic vs. static quantization schemes**: Why needed here: The counterintuitive finding that dynamic outperforms static for Whisper requires understanding when each applies. Quick check question: What information does static quantization require that dynamic quantization computes at runtime?

- **WER/CER/RTF as ASR evaluation metrics**: Why needed here: All results are reported in these units; understanding their meaning is essential for interpreting trade-offs. Quick check question: If RTF = 0.008, how long does inference take for 10 seconds of audio?

## Architecture Onboarding

- **Component map**: Input: LibriSpeech audio → Whisper-small encoder-decoder (244M params, Transformer-based) → Quantization targets (Weights: fp32 → int8/int4/nf4) → Libraries: PyTorch (CPU, per-tensor), Quanto (CPU/GPU, per-channel), HQQ (low-bit optimization), BNB (GPU-only nf4) → Outputs: WER/CER (accuracy), RTF (speed), size reduction (compression)

- **Critical path**: 1. Load Whisper-small fp32 baseline 2. Select library based on deployment target (CPU: PyTorch/HQQ; GPU: Quanto/BNB) 3. Choose quantization scheme (dynamic recommended for Whisper) 4. Apply PTQ (no retraining required) 5. Evaluate on clean and noisy test sets

- **Design tradeoffs**: Speed vs. accuracy: Per-tensor (PyTorch) is fastest; per-channel (Quanto) is most accurate. Compression vs. robustness: 57% reduction (int8) preserves robustness; 71% (int3) degrades on noisy speech. CPU vs. GPU: PyTorch int8 optimal for CPU latency; Quanto int8 optimal for GPU accuracy

- **Failure signatures**: Static quantization on Whisper: Both slower AND less accurate (RTF 0.169 vs. 0.077, WER 15.92 vs. 13.67 on CPU). Sub-8-bit on noisy speech: nf4 WER increases 1.61 points on test-other vs. 0.06 on test-clean. Calibration mismatch: Fixed scales fail under distribution shift (accents, background noise)

- **First 3 experiments**: 1. Replicate Quanto dynamic int8 on GPU: Verify 57% size reduction with WER ≤ baseline on test-other 2. Compare PyTorch vs. Quanto on CPU: Measure RTF difference for per-tensor vs. per-channel overhead 3. Test nf4 on your target noise profile: If your deployment has clean audio, nf4 may be viable; if noisy, expect robustness degradation per Figure 1

## Open Questions the Paper Calls Out

### Open Question 1
To what extent do real-world acoustic conditions (e.g., spontaneous speech, diverse accents, and unrestrained noise profiles) alter the robustness of low-bit quantized Whisper models compared to the LibriSpeech findings? The authors acknowledge that LibriSpeech does not capture the full range of noise profiles, accents, and spontaneous speech found in real-world scenarios and explicitly call for evaluation on more diverse datasets.

### Open Question 2
Can quantization-aware training (QAT) recover the accuracy losses observed in aggressive post-training quantization (PTQ) formats like int3 and nf4? The study restricted its scope to PTQ, but the authors note that other approaches such as quantization-aware training could potentially offer better accuracy under more aggressive compression.

### Open Question 3
Can mixed-precision or layer-wise quantization strategies achieve higher compression than uniform int8 without sacrificing robustness in noise-sensitive components? The authors suggest future work should explore mixed-precision and layer-wise strategies that apply aggressive quantization selectively while preserving accuracy in critical components.

### Open Question 4
Do the trade-offs between dynamic and static quantization change significantly for larger Whisper variants (medium/large) or different ASR architectures? Section 7 states that the analysis was limited to Whisper-small and that results may differ for larger or architecturally different ASR models.

## Limitations
- Findings specific to Whisper-small architecture may not generalize to other ASR models
- Study limited to post-training quantization without exploring quantization-aware training alternatives
- Evaluation restricted to LibriSpeech datasets, which may not represent real-world acoustic conditions

## Confidence

**High confidence**: The 57% model size reduction with Quanto dynamic int8 quantization and its accuracy preservation on clean speech is well-supported by direct measurements across multiple libraries. The performance degradation of static quantization on Whisper's Transformer architecture is consistently observed.

**Medium confidence**: The claim that int8 quantization acts as implicit regularization improving robustness on noisy speech is supported by the data but lacks theoretical justification. The mechanism appears coincidental rather than designed.

**Low confidence**: The assertion that per-channel quantization universally outperforms per-tensor for Whisper due to near-zero-centered weight distributions requires more extensive ablation studies. The paper provides limited analysis of how different layer types within Whisper respond to quantization granularity.

## Next Checks

1. **Hardware dependency validation**: Test Quanto dynamic int8 on CPU-only devices to verify that per-channel quantization overhead doesn't negate speed benefits on hardware without native multi-scale support. Measure actual RTF differences between per-tensor and per-channel implementations.

2. **Noise profile generalization**: Apply the same quantization schemes to non-LibriSpeech datasets with different noise characteristics (e.g., TED-LIUM, Common Voice) to determine if the regularization effect at int8 precision is consistent or dataset-specific.

3. **Alternative quantization schemes**: Implement QAT with the same bit-widths to establish whether the accuracy-speed-size trade-offs observed in PTQ are optimal or if training-aware quantization could achieve better performance, particularly for sub-8-bit formats where PTQ shows significant degradation.