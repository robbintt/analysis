---
ver: rpa2
title: Common to Whom? Regional Cultural Commonsense and LLM Bias in India
arxiv_id: '2601.15550'
source_url: https://arxiv.org/abs/2601.15550
tags:
- region
- your
- what
- cultural
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: INDICA reveals that cultural commonsense in India is predominantly
  regional, not national, with only 39.4% of questions showing agreement across all
  five regions. The benchmark includes 1,630 region-specific question-answer pairs
  across 8 domains and 39 topics.
---

# Common to Whom? Regional Cultural Commonsense and LLM Bias in India
## Quick Facts
- **arXiv ID:** 2601.15550
- **Source URL:** https://arxiv.org/abs/2601.15550
- **Reference count:** 40
- **Primary result:** Cultural commonsense in India is predominantly regional, not national

## Executive Summary
This paper introduces INDICA, a benchmark for evaluating regional cultural commonsense in India, revealing that cultural knowledge varies significantly across five geographic zones. The study finds that only 39.4% of cultural questions show agreement across all regions, while 60.6% exhibit regional variation. State-of-the-art LLMs perform poorly on region-specific questions (13.4%-20.9% accuracy), and when geographic context is removed, models systematically over-predict Central and North Indian practices while under-representing East and West India by 30-40%.

## Method Summary
The authors developed INDICA, a benchmark containing 1,630 region-specific question-answer pairs across 8 domains and 39 topics. They collected responses from 1,223 participants across five Indian regions, finding that cultural commonsense varies significantly by geography. The benchmark was used to evaluate six commercial LLMs (GPT-4, GPT-3.5, Claude 3, Gemini 1.0, Llama 3, and Gemini 1.5) on both region-specific and region-general questions. A key experiment involved prompting models to answer questions without geographic context to reveal systematic geographic biases in model outputs.

## Key Results
- Only 39.4% of cultural questions showed agreement across all five Indian regions, with 60.6% exhibiting regional variation
- State-of-the-art LLMs achieved 13.4%-20.9% accuracy on region-specific questions
- Without geographic context, all models exhibited significant geographic bias, over-selecting Central and North Indian practices (30-40% more than expected) while under-representing East and West Indian practices

## Why This Works (Mechanism)
The paper demonstrates that cultural commonsense is not a monolithic concept but varies systematically across geographic regions within a single nation. The methodology works by creating a controlled benchmark that isolates regional variation from national norms, then systematically tests both human participants and LLMs across the same questions. The artificial removal of geographic context reveals how models default to certain regional biases when lacking specific information, exposing systematic gaps in their cultural knowledge.

## Foundational Learning
- **Cultural variation mapping:** Understanding that cultural knowledge varies across regions rather than being uniformly national - needed to design appropriate evaluation benchmarks and avoid false assumptions about cultural homogeneity
- **Benchmark design methodology:** Creating region-specific versus region-general question sets - needed to isolate and measure the extent of cultural variation versus universal knowledge
- **Geographic bias detection:** Measuring systematic over/under-prediction of regional practices - needed to identify where models default to certain cultural assumptions in absence of context

## Architecture Onboarding
- **Component map:** Survey collection -> Benchmark creation -> Model evaluation -> Bias analysis
- **Critical path:** Data collection → Question categorization → Model testing → Geographic bias analysis
- **Design tradeoffs:** Region-specific questions capture variation but reduce statistical power; removing geographic context reveals bias but creates artificial test conditions
- **Failure signatures:** Models systematically over-predicting certain regions, under-predicting others, and failing on region-specific questions while succeeding on general ones
- **3 first experiments:** 1) Test additional Indian states to verify regional variation holds broadly, 2) Evaluate open-source models to compare performance across architectures, 3) Field validate findings with actual regional populations

## Open Questions the Paper Calls Out
None

## Limitations
- Only six commercial LLMs were tested, excluding potentially different-performing open-source alternatives
- The study sampled five regions rather than all Indian states, limiting generalizability across the full national geography
- Artificial removal of geographic context may not perfectly reflect real-world usage patterns where context is often partially available

## Confidence
- **Regional commonsense claim:** High - supported by systematic survey methodology and clear statistical divergence across regions
- **LLM performance results:** Medium - based on six commercial models only, open-source models may differ
- **Geographic bias findings:** Medium - controlled experimental design, but artificial context removal may not reflect real usage

## Next Checks
- Replicate findings with additional Indian states beyond the five sampled regions to test whether the 60.6% regional variation holds across full national geography
- Test the benchmark with open-source models (Llama, Mistral, etc.) to determine if the 13.4-20.9% accuracy gap persists across different model architectures and training approaches
- Conduct field validation by surveying actual regional populations to verify that model's systematic under-prediction of East and West Indian practices reflects genuine cultural differences rather than sampling bias