---
ver: rpa2
title: 'From Evidence to Belief: A Bayesian Epistemology Approach to Language Models'
arxiv_id: '2504.19622'
source_url: https://arxiv.org/abs/2504.19622
tags:
- evidence
- confidence
- accuracy
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how large language models (LLMs) adjust
  their confidence and responses when presented with evidence of varying informativeness
  and reliability, from a Bayesian epistemology perspective. A dataset with diverse
  evidence types was created, and LLM responses were analyzed using verbalized confidence,
  token probability, and sampling methods.
---

# From Evidence to Belief: A Bayesian Epistemology Approach to Language Models

## Quick Facts
- arXiv ID: 2504.19622
- Source URL: https://arxiv.org/abs/2504.19622
- Authors: Minsu Kim; Sangryul Kim; James Thorne
- Reference count: 16
- Key outcome: LLMs follow Bayesian confirmation but fail disconfirmation/irrelevance assumptions; high confidence doesn't guarantee accuracy; models show golden evidence bias

## Executive Summary
This study investigates how large language models update beliefs when presented with evidence of varying informativeness and reliability through a Bayesian epistemology lens. The researchers created a dataset with diverse evidence types and analyzed LLM responses using verbalized confidence, token probability, and sampling methods. Results show that while models appropriately increase confidence with confirming evidence, they fail to decrease confidence with conflicting evidence and are significantly distracted by irrelevant information. The findings reveal important limitations in LLM belief representation and calibration, with models exhibiting a "golden evidence bias" that prioritizes correct information even when mixed with incorrect data.

## Method Summary
The study uses zero-shot inference on three datasets (SciQ, TriviaQA, GSM8K) with two models (GPT-3.5-turbo, GPT-4o). Evidence is generated via few-shot prompting with GPT-4-0613 for perturbation types (negated, contradictory, coincidental) and GPT-4o for strength-of-evidence variations. Confidence is measured through three parallel methods: verbalized prompting asking models to output probabilities, token probability extraction from logits, and sampling-based consistency. The evaluation uses accuracy metrics (Rouge-L ≥0.3 for SciQ/TriviaQA, exact match for GSM8K) and ECE for calibration, with statistical significance tested via paired t-tests across evidence conditions.

## Key Results
- LLMs follow Bayesian confirmation assumption well with true evidence (P(H|E,θ) > P(H|θ))
- Models fail to adhere to disconfirmation and irrelevance assumptions, showing inconsistent confidence reduction with conflicting evidence
- High confidence does not always guarantee high accuracy, particularly with strong/credible evidence
- Models exhibit golden evidence bias, maintaining performance when correct information is mixed with conflicting evidence
- Irrelevant evidence significantly distracts models, lowering accuracy despite no change in hypothesis probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit Bayesian-consistent belief updating when presented with confirming evidence
- Mechanism: When evidence supports hypothesis H and aligns with parametric knowledge, the model increases P(H|E,θ) above baseline P(H|θ), demonstrating appropriate confidence calibration
- Core assumption: The model possesses relevant parametric knowledge about the domain
- Evidence anchors: [abstract] "language models follow the Bayesian confirmation assumption well with true evidence"; [section 4.1] "when E is golden evidence that helps confirm the answer, we observe P(H|E) > P(H)"

### Mechanism 2
- Claim: LLMs fail to appropriately disconfirm beliefs when presented with conflicting evidence
- Mechanism: Conflicting evidence creates confusion between parametric knowledge and context-provided misinformation, but confidence does not reliably decrease per Bayesian norms
- Core assumption: Training data rarely contains unreliable or conflicting scenarios
- Evidence anchors: [abstract] "fail to adhere to disconfirmation and irrelevance assumptions"; [section 4.1] "conflicting evidence does not have a significant effect on confidence"

### Mechanism 3
- Claim: LLMs exhibit a "golden evidence bias"—they selectively attend to correct information even when mixed with incorrect information
- Mechanism: When presented with mixed evidence, models maintain high confidence and accuracy similar to golden-evidence conditions, prioritizing context aligned with parametric knowledge
- Core assumption: Models learned during pretraining to extract signal from noise and preferentially attend to coherent, correct information patterns
- Evidence anchors: [abstract] "language models are biased toward golden evidence"; [section 5] "the language model focuses more on the given golden evidence in the context than inaccurate evidence"

## Foundational Learning

- Concept: **Bayesian Epistemology / Probability Norm**
  - Why needed here: The entire experimental framework assumes belief can be quantified as probability, with P(H|E,θ) representing confidence in hypothesis H given evidence E and background knowledge θ
  - Quick check question: Given hypothesis H="The answer is mitochondria" and evidence E containing the word "mitochondria" in an irrelevant context, should P(H|E) differ from P(H) according to Bayesian irrelevance assumption?

- Concept: **Calibration Norm / Expected Calibration Error (ECE)**
  - Why needed here: The paper uses ECE as a key metric measuring whether model confidence matches actual accuracy
  - Quick check question: If a model reports 90% confidence on 100 predictions but only 60 are correct, what does this indicate about calibration?

- Concept: **Confidence Elicitation Methods (Verbalized vs. Token Probability vs. Sampling)**
  - Why needed here: The paper uses three distinct methods to measure model confidence, and results differ across methods
  - Quick check question: Why might verbalized confidence show higher absolute values than token probability for the same model and input?

## Architecture Onboarding

- Component map: Evidence Generation Pipeline -> Confidence Measurement Layer -> Evaluation Metrics
- Critical path: 1) Filter source datasets for samples with ≥4 sentence explanations 2) Generate evidence perturbations via few-shot prompting 3) Run inference with zero-shot prompting using target models 4) Extract confidence via three methods simultaneously 5) Compute accuracy, ECE, and statistical significance across evidence types 6) Run ablation studies varying golden/conflicting sentence ratios
- Design tradeoffs: Closed-source only for verbalized confidence (smaller open-source LLMs failed to follow output format); sample size imbalance (GPT-4o limited to 200 samples due to cost vs. ~1000 for GPT-3.5); ROUGE-L threshold at 0.3 allows synonym matching but may overcount partial matches
- Failure signatures: High confidence + low accuracy with strong evidence; irrelevant evidence distraction; coincidental evidence acceptance
- First 3 experiments: 1) Replicate confirmation task on your target model using provided dataset 2) Ablation on golden evidence ratio from 30% to 100% 3) Cross-domain irrelevant evidence test comparing within-dataset vs cross-dataset irrelevance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent do specific pre-training data compositions and training algorithms drive LLMs' failure to adhere to Bayesian disconfirmation and irrelevance assumptions?
- **Basis in paper:** [explicit] Section 8 states, "we did not theoretically investigate the causes behind the observed phenomena, such as the training algorithm and model architecture"
- **Why unresolved:** The current study is empirical, measuring model outputs without analyzing internal weight distributions or isolating causal factors in the training pipeline
- **What evidence would resolve it:** Ablation studies training models on datasets with controlled ratios of conflicting vs. confirming evidence

### Open Question 2
- **Question:** How does a finer-grained taxonomy of conflicting evidence affect model belief revision?
- **Basis in paper:** [explicit] Section 8 notes a limitation where conflicting evidence contained varying natures and suggests "a finer classification of conflicting evidence could benefit future research"
- **Why unresolved:** The study aggregates all negative evidence into a single "Conflicting" category, potentially masking distinct model behaviors
- **What evidence would resolve it:** Experiments using a dataset explicitly categorizing conflicts by type (factual negation vs. context shift)

### Open Question 3
- **Question:** How does LLM belief updating compare quantitatively to human cognitive processes when encountering coincidental or contradictory evidence?
- **Basis in paper:** [explicit] Section 8 states, "Future research could incorporate human evaluation to further assess how models’ belief updating and confidence calibration compare to human cognitive processes"
- **Why unresolved:** The current work benchmarks models against idealized Bayesian norms rather than human behavioral baselines
- **What evidence would resolve it:** A parallel study collecting human confidence scores on the same tasks, followed by comparative analysis

## Limitations
- Evidence generation quality uncertainty: The study relies on GPT-4-0613 for perturbation generation, with only manual filtering validation
- Cross-method confidence discrepancies: Different confidence elicitation methods show varying sensitivity to evidence types without definitive explanation
- Domain-specific limitations: Model performance varies significantly with parametric knowledge availability, suggesting findings may not generalize uniformly

## Confidence
- Bayesian confirmation assumption adherence: High confidence
- Disconfirmation and irrelevance assumption violations: High confidence
- Golden evidence bias: Medium confidence

## Next Checks
1. Conduct a human evaluation of automatically generated evidence types to verify they accurately represent their intended epistemic properties
2. Systematically vary the proportion of golden evidence from 10% to 90% in mixed evidence conditions to identify the threshold where performance degrades
3. Test irrelevance assumption by using evidence from completely unrelated domains (e.g., math evidence for science questions) to determine if cross-dataset irrelevance produces different calibration than within-dataset irrelevance