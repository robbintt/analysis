---
ver: rpa2
title: The distribution of calibrated likelihood functions on the probability-likelihood
  Aitchison simplex
arxiv_id: '2509.03365'
source_url: https://arxiv.org/abs/2509.03365
tags:
- likelihood
- space
- where
- data
- discriminant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the concept of calibrated likelihood functions
  from the binary case to multiple hypotheses using the Aitchison geometry of the
  simplex. The authors introduce the isometric-log-ratio transformed likelihood function
  (ILRL) as a multidimensional extension of the log-likelihood-ratio, allowing for
  a vector representation of statistical evidence across multiple hypotheses.
---

# The distribution of calibrated likelihood functions on the probability-likelihood Aitchison simplex

## Quick Facts
- **arXiv ID:** 2509.03365
- **Source URL:** https://arxiv.org/abs/2509.03365
- **Reference count:** 11
- **Primary result:** Extends calibrated likelihood functions to multiple hypotheses using Aitchison geometry and ILR transformations, with application to Compositional Discriminant Analysis (CDA) that improves interpretability and reliability.

## Executive Summary
This paper extends the concept of calibrated likelihood functions from binary classification to the multi-class setting using the Aitchison geometry of the simplex. The authors introduce the Isometric-Log-Ratio transformed likelihood function (ILRL) as a vector representation of statistical evidence across multiple hypotheses, allowing Bayes' rule to operate through vector addition. They propose a non-linear discriminant analysis called Compositional Discriminant Analysis (CDA) that learns a diffeomorphism (via Normalizing Flows) to warp data distributions into a constrained Gaussian reference distribution, producing calibrated likelihood functions that improve interpretability and reliability compared to standard methods like LDA and QDA.

## Method Summary
The method defines a base space where class conditionals are Gaussian distributions with shared covariance matrix Σ, and means that are constrained to be determined by Σ (Theorem 4). A Normalizing Flow (RealNVP) learns a diffeomorphism mapping features to this base space. The target distribution is constructed such that the first D-1 dimensions follow the constrained Gaussian form while remaining dimensions capture residuals. The model is trained to maximize log-likelihood using the change of variables formula. The resulting classifier produces calibrated likelihood functions where the discriminant components form a calibrated likelihood function over classes.

## Key Results
- ILRLs provide a vector representation of statistical evidence that generalizes log-likelihood-ratios to multiple hypotheses
- The idempotence property enforces calibration by constraining class-conditional distributions to share covariance with means determined by this matrix
- CDA outperforms LDA and QDA on cross-entropy and calibration metrics while maintaining comparable accuracy
- Experiments show CDA extracts more useful information from data, particularly on MNIST with PCA-reduced features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The Isometric-Log-Ratio (ILR) transformation extends binary log-likelihood-ratios (LLRs) to multiple hypotheses by mapping the probability simplex to a Euclidean vector space where Bayes' rule becomes vector addition.
- **Mechanism:** The ILR transformation projects a composition (likelihoods or probabilities) onto an orthonormal basis of the simplex (Eq 26). This recovers the additive form of Bayes' rule ($\tilde{P} = \tilde{w} + \tilde{\pi}$), allowing statistical evidence to be represented as a vector in $\mathbb{R}^{D-1}$ rather than a scalar difference.
- **Core assumption:** The system assumes the validity of Aitchison geometry for discrete probability distributions, specifically that relative information (ratios) is the only information content.
- **Evidence anchors:**
  - [abstract] Introduces "isometric-log-ratio transformed likelihood function (ILRL) as a multidimensional extension of the log-likelihood-ratio."
  - [section 3.3] Explicitly derives $\text{ilr}(P) = \text{ilr}(w) + \text{ilr}(\pi)$ as a vector translation.
  - [corpus] The neighbor paper "Simplex-to-Euclidean Bijections for Categorical Flow Matching" confirms the utility of Aitchison geometry for learning distributions on the simplex.
- **Break condition:** Fails if the data contains structural zeros (probabilities exactly 0), as the Aitchison geometry requires strictly positive values (though replacements exist).

### Mechanism 2
- **Claim:** Calibration of likelihood functions is enforced through an "idempotence" property, which mathematically constrains the class-conditional distributions of the ILRL to be Gaussian with specific covariance/mean relationships.
- **Mechanism:** Idempotence requires that "the likelihood function of the likelihood function is the likelihood function" (Eq 34). If we assume ILRLs are normally distributed under one hypothesis ($H_1$), idempotence forces the distributions under other hypotheses ($H_i$) to share the same covariance matrix $\Sigma$, with means $\mu_i$ entirely determined by $\Sigma$ (Theorem 4).
- **Core assumption:** Assumes that the underlying evidence can be effectively modeled by Gaussian distributions in the ILR space (the "logistic-normal" distribution).
- **Evidence anchors:**
  - [abstract] States authors "extend the definition of calibration and the idempotence property to ILRLs."
  - [section 4.1] Theorem 4 proves that $l|H_i \sim \mathcal{N}(\mu_i, \Sigma)$ with means determined by $\mu_1$ and $\Sigma$.
  - [corpus] Corpus signals in "Calibrated Multivariate Distributional Regression" support the difficulty of multivariate calibration, highlighting the novelty of this geometric constraint.
- **Break condition:** Fails if the empirical distribution of evidence is multi-modal or heavily skewed in a way that a Gaussian in ILR space cannot approximate, violating the parametric assumption.

### Mechanism 3
- **Claim:** Compositional Discriminant Analysis (CDA) creates a calibrated classifier by learning a diffeomorphism (via Normalizing Flows) that warps the data distribution to match the constrained Gaussian reference distribution.
- **Mechanism:** A Normalizing Flow learns an invertible map $g^{-1}: \mathcal{X} \to \mathcal{Z}$. The target base space $\mathcal{Z}$ is constructed such that the first $D-1$ dimensions follow the constrained Gaussian defined by Theorem 4 (Eq 36). By ensuring the base distribution matches the "calibrated" form, the inverse map $g$ produces a generative classifier with calibrated likelihoods.
- **Core assumption:** Assumption: The Normalizing Flow (specifically RealNVP in the paper) is flexible enough to transform the complex feature distribution into the constrained Gaussian form (the "manifold hypothesis").
- **Evidence anchors:**
  - [abstract] Proposes "a non-linear discriminant analysis called Compositional Discriminant Analysis (CDA)."
  - [section 5.1.2] Defines the diffeomorphism $g^{-1}$ and the optimization objective (Eq 42).
  - [corpus] Weak corpus signal for CDA specifically, but "Simplex-to-Euclidean Bijections" validates the use of bijections for simplex modeling.
- **Break condition:** Fails if the data manifold is topologically distinct from the target base space (e.g., concentric circles require tearing the manifold, which diffeomorphisms cannot do without "holes," as noted in the "Circles" experiment discussion).

## Foundational Learning

- **Concept: Aitchison Geometry and Compositional Data**
  - **Why needed here:** Standard Euclidean geometry fails on the probability simplex because the components are constrained (sum to 1). Aitchison geometry provides the operations (perturbation, powering) and distances needed to treat probabilities as vectors.
  - **Quick check question:** Why does adding a perturbation vector $\mathbf{w}$ to a prior $\pi$ (Eq 28) result in a valid posterior probability distribution without explicit normalization?

- **Concept: Normalizing Flows (NF)**
  - **Why needed here:** The CDA architecture relies on NFs to learn the complex non-linear mapping from features to the "calibrated" latent space. Without understanding the change-of-variables formula (Eq 42), the training objective is opaque.
  - **Quick check question:** How does the Jacobian determinant $\det(\partial x / \partial z)$ ensure that a density in the base space $f_Z(z)$ transforms into a valid density in feature space?

- **Concept: Proper Scoring Rules ($C_{llr}$)**
  - **Why needed here:** The paper evaluates performance using $C_{llr}$ (log-loss) and its decomposition into calibration and discrimination, rather than just accuracy.
  - **Quick check question:** Why does the paper argue that Expected Calibration Error (ECE) is "suboptimal" compared to the decomposition of the log-likelihood-ratio cost?

## Architecture Onboarding

- **Component map:** Feature vectors -> RealNVP Normalizing Flow -> Base space (constrained Gaussian + residuals) -> Calibrated likelihoods (via inverse ILR) -> Class prediction
- **Critical path:** The definition of the **Base Space Distributions**. You must implement the mean calculation in Eq 37 ($\mu_1 = A^{-1}B \text{vec}(\Sigma)$) correctly. If the means of the base Gaussians are not strictly coupled to the covariance matrix $\Sigma$, the idempotence property (calibration) is mathematically broken before training starts.
- **Design tradeoffs:**
  - **Interpretability vs. Complexity:** CDA forces the latent space to be Gaussian and calibrated. This might lower pure discrimination accuracy (e.g., QDA on Circles dataset) compared to unconstrained models, but guarantees the probability estimates are meaningful.
  - **Diffeomorphism constraint:** The model cannot separate classes if the boundaries in feature space are topologically impossible to flatten (e.g., "Circles" experiment), leading to a "slice" of miss-classified data.
- **Failure signatures:**
  - **Collapse of Variance:** If $\Sigma$ initialization is poor, the means $\mu_i$ might collapse, causing all classes to map to the same point.
  - **High $C_{llr}^{cal}$:** If the flow is insufficiently deep, the base distribution won't be Gaussian, leading to high calibration error (unreliable probabilities).
  - **Mode Collapse:** Residual dimensions might capture class information if the flow "cheats," violating the independence assumption of the residual.
- **First 3 experiments:**
  1. **Visualize the Warping:** Train on 2D "Moons" data. Plot the deformation of a regular grid in the base space to the feature space to verify the flow learns a smooth bijection (similar to Fig 11).
  2. **Sanity Check Gaussians:** Generate synthetic Gaussian data with shared covariance. Verify CDA recovers the linear discriminant and achieves near-zero calibration loss.
  3. **Ablate the Constraint:** Train a CDA where $\mu_i$ are *not* coupled to $\Sigma$ (standard generative classifier). Compare $C_{llr}^{cal}$ against the constrained version to quantify the calibration benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the matrix $M$ relating the covariance matrix $\Sigma$ to the Kullback-Leibler divergences invertible?
- Basis in paper: [explicit] Appendix C states, "Unfortunately, we did not prove that M is invertible."
- Why unresolved: While the authors derive a linear relation, the mathematical property of invertibility for this specific mapping matrix remains unverified.
- What evidence would resolve it: A formal mathematical proof demonstrating that $M$ is invertible for arbitrary dimensions, or a counter-example showing specific conditions where $M$ is singular.

### Open Question 2
- Question: Can a theoretically grounded initialization strategy be derived for the CDA covariance matrix $\Sigma$ to address training sensitivity?
- Basis in paper: [inferred] Appendix E notes training is "very sensitive to the initialization of $\Sigma$" and the current strategy relies on an approximation lacking "strong theoretical foundation."
- Why unresolved: The proposed initialization uses a heuristic based on standard LDA assumptions which may not hold for the non-linear mapping, risking instability.
- What evidence would resolve it: A theoretical analysis of the CDA loss landscape yielding an optimal initialization scheme, or empirical results showing robust convergence across diverse datasets without heuristics.

### Open Question 3
- Question: How can the CDA framework be adapted to handle data topologies that are not diffeomorphic to the Gaussian base space?
- Basis in paper: [inferred] Section 5.2.2 (Circles dataset) highlights a failure case where "invertibility and differentiability constraints... limit the discrimination ability" because no diffeomorphism perfectly separates the classes.
- Why unresolved: The requirement for a bijective mapping (Normalizing Flow) restricts the model from separating topologically complex structures like concentric circles.
- What evidence would resolve it: A modification of the CDA architecture (e.g., relaxing the diffeomorphism constraint or changing the base topology) that successfully separates non-diffeomorphic datasets.

## Limitations
- The theoretical framework relies heavily on the assumption that ILRLs are normally distributed in the base space, which may not hold for real-world data with complex multimodal structures.
- The method is highly sensitive to the initialization of the covariance matrix Σ, as noted in Appendix E, suggesting potential training instability.
- The diffeomorphism constraint limits expressiveness and cannot handle topologically complex decision boundaries like concentric circles without violating invertibility requirements.

## Confidence

- **High Confidence:** The geometric framework using Aitchison simplex and ILR transformations is mathematically sound and well-established in compositional data analysis literature. The extension of calibration/idempotence to ILRLs follows logically from the mathematical framework.
- **Medium Confidence:** The effectiveness of CDA on synthetic datasets appears robust, but real-world performance (particularly on MNIST) depends heavily on the flexibility of the Normalizing Flow architecture and proper initialization.
- **Low Confidence:** The claim that CDA "extracts more useful information" compared to LDA/QDA is primarily supported by quantitative metrics (cross-entropy, calibration) rather than qualitative analysis of what information is actually being captured.

## Next Checks

1. **Robustness to Initialization:** Systematically vary the initialization of Σ using the LDA-approximation described in Appendix E and measure the impact on convergence rate and final calibration error across multiple random seeds.

2. **Topological Sensitivity Analysis:** Test CDA on synthetic datasets with known topological properties (e.g., concentric rings, figure-8 patterns) to quantify the failure rate when decision boundaries violate diffeomorphism constraints.

3. **Information Content Analysis:** Beyond $C_{llr}$ metrics, implement feature attribution methods to determine whether CDA is actually capturing different information than LDA/QDA, or simply achieving better calibration through different means.