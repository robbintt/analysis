---
ver: rpa2
title: 'JoyAgent-JDGenie: Technical Report on the GAIA'
arxiv_id: '2510.00510'
source_url: https://arxiv.org/abs/2510.00510
tags:
- arxiv
- agents
- agent
- wang
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a unified framework for building effective
  generalist agents through the integration of heterogeneous agent paradigms, hierarchical
  memory, and a validated tool suite. The system combines Plan-Execute and ReAct agents
  coordinated through posterior voting to balance reliability and adaptability.
---

# JoyAgent-JDGenie: Technical Report on the GAIA

## Quick Facts
- **arXiv ID:** 2510.00510
- **Source URL:** https://arxiv.org/abs/2510.00510
- **Reference count:** 40
- **Primary result:** 75.2% Pass@1 on GAIA validation, 82.4% Pass@3

## Executive Summary
This paper proposes a unified framework for building effective generalist agents through the integration of heterogeneous agent paradigms, hierarchical memory, and a validated tool suite. The system combines Plan-Execute and ReAct agents coordinated through posterior voting to balance reliability and adaptability. A hierarchical memory system integrates working, semantic, and procedural layers to enable long-horizon continuity and adaptive control. The tool suite focuses on search, code execution, and multimodal parsing with schema-consistent interfaces. Evaluated on the GAIA benchmark, the framework achieves 75.2 Pass@1 and 82.4 Pass@3 on validation, and 67.1 Pass@1 on the test set, surpassing open-source baselines and approaching proprietary system performance.

## Method Summary
The JoyAgent-JDGenie framework implements a fusion architecture combining Plan-Execute and ReAct agents with posterior voting via a Critic model. The system processes GAIA benchmark tasks through parallel execution of both agent types, with the Critic comparing trajectory segments to select the final answer. It employs a hierarchical memory system spanning working, semantic, and procedural layers, and uses a validated tool suite including search, code execution, and multimodal parsing. The framework is evaluated on GAIA validation (165 questions) and test (300 questions) sets using Exact Match Accuracy and Pass@1/Pass@3 metrics.

## Key Results
- 75.2% Pass@1 and 82.4% Pass@3 on GAIA validation set
- 67.1% Pass@1 on GAIA test set
- Performance significantly exceeds open-source baselines, approaching proprietary systems
- Fusion approach yields 30-60% gains over Plan-Execute baselines

## Why This Works (Mechanism)

### Mechanism 1: Bias-Variance Reduction via Heterogeneous Fusion
Combining Plan-Execute (low variance) and ReAct (high adaptability) agents through a Critic model improves reliability beyond what either paradigm achieves alone. The system routes tasks to a "Fusion" ensemble where a Plan-Execute agent follows structured decomposition while ReAct agents explore dynamically. A Critic model compares execution trajectory segments rather than just final answers, selecting the path with the strongest evidentiary chain.

### Mechanism 2: Hierarchical Memory as Context Filter
Decoupling memory into Working, Semantic, and Procedural layers prevents context window saturation while preserving critical historical constraints. "Working Memory" holds immediate execution state; "Semantic Memory" compresses episodic traces into embeddings for retrieval; "Procedural Memory" enforces high-level behavioral guidelines. This acts as a stateful filter, injecting only relevant historical data into the working context.

### Mechanism 3: Schema-Consistent Tooling for Error Suppression
Restricting tools to a validated set (Search, Code, Multimodal) with strict output schemas minimizes cascade error rates common in massive toolkits. Tools like PDF parsers or code sandboxes return structured objects rather than raw text, forcing the LLM to reason over defined fields and reducing hallucination during the Observe phase of ReAct.

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting) Loop**
  - **Why needed here:** This is the atomic execution unit for the "Single Agents" in the ensemble. Understanding the Think → Act → Observe cycle is required to debug why specific trajectories are rejected by the Critic.
  - **Quick check question:** Can you trace a failed step in the logs to a breakdown in the "Observe" phase (tool error) vs. the "Think" phase (reasoning error)?

- **Concept: Pass@N Metric**
  - **Why needed here:** The paper optimizes heavily for Pass@3 (82.4) via ensemble voting. Understanding this probabilistic metric is crucial for evaluating whether the "Fusion" cost is justified.
  - **Quick check question:** Does increasing the ensemble size (N) improve the Pass@1 score (reliability), or does it merely increase the chance of finding one correct answer (Pass@3)?

- **Concept: Semantic Retrieval (RAG)**
  - **Why needed here:** The Semantic Memory layer relies on embedding past trajectories to guide current actions.
  - **Quick check question:** How does the system handle a "negative case" where no relevant semantic memory exists? Does the agent proceed with zero-shot reasoning or fail?

## Architecture Onboarding

- **Component map:** User Query → Task Augmentation → Parallel Execution (Plan + ReAct) → Tool Invocation → Message Object Generation → Critic Comparison → Final Answer

- **Critical path:** User Query → Task Augmentation → Parallel Execution (Plan + ReAct) → Tool Invocation → Message Object Generation → Critic Comparison → Final Answer

- **Design tradeoffs:**
  - **Accuracy vs. Cost:** Fusion approach yields ~4-5% gain over Single Agent but requires 2-3x the inference cost
  - **Rigidity vs. Drift:** Plan-Execute prevents drift but struggles with Level 3 tasks; ReAct handles exploration but is brittle on Level 1 tasks
  - **Search Engine Lock-in:** Highly dependent on Google Search (75.2 vs. 58.8 for Bing)

- **Failure signatures:**
  - "Browser Agent" Degradation: Adding Browser Agent causes significant performance deterioration
  - Voter Deadlock: Critic may flip-flop when Plan-Execute and ReAct produce contradictory but equally plausible trajectories
  - Search Result Noise: Multi-Source search aggregation performs worse (68.5) than pure Google (75.2) due to noise dilution

- **First 3 experiments:**
  1. Ablate the Critic: Run validation set using only Plan-Execute vs. only ReAct vs. Fusion to quantify Critic contribution
  2. Search Sensitivity: Force system to use DuckDuckGo exclusively on Level 2/3 tasks to measure performance drop
  3. Procedural Memory Injection: Modify system prompt to enforce "verify source dates" rule on temporal reasoning tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does adding a specialized Browser Agent significantly degrade system performance compared to static retrieval agents, and can this interaction complexity be mitigated?
- **Basis:** Section 4.2.1 states that "after introducing the browser agent, the system performance exhibits significant deterioration."
- **Why unresolved:** Paper identifies performance drop but doesn't analyze specific failure modes or propose solutions
- **What evidence would resolve it:** Ablation studies isolating Browser Agent's error modes and success rates on web interaction tasks

### Open Question 2
- **Question:** To what extent is the framework's high performance dependent on the fine-grained filtering capabilities of the Google/SerpAPI rather than the agent's reasoning architecture?
- **Basis:** Table 4 shows substantial performance gap (75.2 vs. 58.8) between Google and Bing, attributed to "fine-grained conditional filtering"
- **Why unresolved:** Unclear if agent can compensate for lower-quality search metadata or if architecture is inherently brittle without specific API features
- **What evidence would resolve it:** Benchmarks using stripped-down search APIs or synthetic noise injection

### Open Question 3
- **Question:** Can dynamic self-improvement via reinforcement learning enable the ensemble to evolve coordination strategies beyond static posterior voting?
- **Basis:** Conclusion identifies "dynamic self-improvement through reinforcement learning... beyond static voting mechanisms" as key future direction
- **Why unresolved:** Current system relies on fixed posterior voting; unknown if ensemble can autonomously optimize coordination
- **What evidence would resolve it:** Comparisons of Pass@1 scores between static voting and RL-tuned dynamic coordination strategy

## Limitations
- Performance highly dependent on Google Search quality (75.2% vs 58.8% with Bing)
- Browser Agent tools cause significant performance degradation
- Performance gap between Pass@1 (67.1%) and Pass@3 (82.4%) suggests ensemble doesn't consistently produce optimal answer on first attempt

## Confidence
- **Medium-High:** Architecture well-specified but key implementation details underspecified
- **Medium:** Performance claims verifiable but dependent on search engine quality
- **Medium:** Generalization beyond GAIA benchmark uncertain

## Next Checks
1. **Search Engine Sensitivity Test**: Run validation set using DuckDuckGo exclusively to quantify performance variance when deviating from Google Search

2. **Ablation Study on Critic Voting**: Execute validation set with only Plan-Execute agent, only ReAct agent, and full fusion system to isolate Critic's exact contribution

3. **Browser Agent Performance Isolation**: Create subset of GAIA tasks requiring browser interaction and measure performance drop when Browser Agent tools are enabled