---
ver: rpa2
title: Evaluating Adversarial Robustness of Concept Representations in Sparse Autoencoders
arxiv_id: '2505.16004'
source_url: https://arxiv.org/abs/2505.16004
tags:
- individual
- concept
- population
- targeted
- untargeted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces robustness as a critical yet overlooked evaluation
  dimension for sparse autoencoders (SAEs) in mechanistic interpretability. The authors
  define SAE robustness through input-space optimization problems and propose four
  structured adversarial scenarios combining semantic and activation-level goals.
---

# Evaluating Adversarial Robustness of Concept Representations in Sparse Autoencoders

## Quick Facts
- arXiv ID: 2505.16004
- Source URL: https://arxiv.org/abs/2505.16004
- Reference count: 40
- Authors: Aaron J. Li; Suraj Srinivas; Usha Bhalla; Himabindu Lakkaraju
- Primary result: Tiny adversarial token changes can nearly double concept overlap between unrelated inputs and reduce overlap with original inputs to 20-30%, all while preserving base LLM semantic integrity

## Executive Summary
This paper introduces robustness as a critical yet overlooked evaluation dimension for sparse autoencoders (SAEs) in mechanistic interpretability. The authors define SAE robustness through input-space optimization problems and propose four structured adversarial scenarios combining semantic and activation-level goals. They adapt the Greedy Coordinate Gradient (GCG) attack algorithm to craft minimal input perturbations that manipulate SAE concept representations. Experiments on multiple LLMs (Gemma2, Llama3) and datasets (AdvBench, AG News, SST2, Art & Science) show that current SAEs are highly vulnerable to input-level adversarial attacks, raising concerns about their reliability for model monitoring and downstream applications.

## Method Summary
The paper proposes a framework to evaluate SAE robustness by adapting the Greedy Coordinate Gradient (GCG) attack algorithm to craft minimal input perturbations that manipulate concept representations. Four adversarial scenarios are defined: targeted/untargeted attacks at population/individual levels. The attack searches for single-token substitutions that maximize changes in concept overlap (population) or flip activation status (individual). The method uses differentiable surrogate losses (cosine similarity, log-likelihood) for gradient computation while evaluating candidates on discrete metrics (concept overlap ratio, activation rank). Experiments test on multiple LLMs (Gemma2-9B, Llama3-8B) and datasets with initial concept overlap < 0.30, using hyperparameters T=10-30 iterations, m=300 candidate tokens, and B=100-200 batch size.

## Key Results
- Single-token adversarial perturbations can nearly double concept overlap between unrelated inputs while reducing overlap with original inputs to 20-30%
- Untargeted population-level attacks are more effective than targeted ones; individual-level activation succeeds for deactivation but fails for dead latents
- Transfer attacks across model families show reduced effectiveness but maintain meaningful manipulation capability
- Base LLM semantic integrity remains largely intact (linear probe ΔFNR < 5%) while SAE representations are substantially altered

## Why This Works (Mechanism)

### Mechanism 1: Input-Level Perturbation Propagates to Sparse Latent Space
- The mapping from input to SAE latent space amplifies small input perturbations because SAEs lack robustness-aware training objectives
- Adversarial tokens optimize for maximal change in continuous SAE representation space, then are evaluated on discrete activation patterns
- Relies on bi-Lipschitz assumption where token-level edit distance proportionally reflects semantic distance
- Break condition: If LLM hidden states become highly robust to token substitutions, the gradient signal for SAE manipulation weakens

### Mechanism 2: Gradient-Guided Token Search via GCG Adaptation
- Extending GCG to SAE setting enables efficient discovery of adversarial tokens that maximize concept representation change
- Gradients computed on differentiable surrogate losses while candidates evaluated on non-differentiable discrete metrics
- Assumes gradient direction in continuous embedding space provides useful signal for discrete token substitution
- Break condition: If sparsity-inducing activation functions create gradient dead zones, attack success degrades

### Mechanism 3: Population vs. Individual-Level Vulnerability Asymmetry
- Untargeted attacks more effective than targeted; suppression of dominant features easier than constructing specific new activations
- Dead latents (permanently inactive neurons) cannot be activated by any input perturbation, creating structural failure mode
- Assumes meaningful SAE latents exhibit dynamic activation behavior rather than constant low activation
- Break condition: If dead latent prevalence is reduced through improved SAE training, individual activation attacks may become viable

## Foundational Learning

- **Sparse Autoencoder Architecture**: Understanding the encoder-decoder structure with sparsity-inducing activation is essential to grasp how concept representations are extracted and why they're vulnerable
  - Quick check: Given an LLM hidden state h, write the SAE encoding equation that produces sparse latents z

- **Concept Overlap Metric**: The primary evaluation metric quantifies similarity between SAE representations as 1 - |I_k(z_i) ∩ I_k(z_j)| / k
  - Quick check: Two inputs have top-10 activated latent sets with 3 indices in common. What is their concept overlap ratio?

- **Greedy Coordinate Gradient (GCG)**: The adapted attack algorithm iteratively optimizes token substitutions using gradient-based candidate generation and loss-based selection
  - Quick check: In GCG, what is computed at each token position to identify promising substitution candidates?

## Architecture Onboarding

- **Component map**: Input X → LLM f_LLM → Hidden state h → SAE encoder (W_enc, b_enc, ϕ) → Sparse latents z → SAE decoder (W_dec, b_dec) → Reconstruction ĥ
  - GCG attack module: Gradient computation → Top-m token candidates → Batch evaluation → Best candidate selection

- **Critical path**: 
  1. Select (x₁, x₂) pairs with initial concept overlap < 0.30
  2. Apply short instruction prompt ("The previous sentence is about") to extract semantic content
  3. Run GCG for T iterations (10–30 depending on attack type) with batch size B (100–200)
  4. Evaluate on discrete metrics (concept overlap ratio for population, rank change for individual)

- **Design tradeoffs**: 
  - Targeted vs. untargeted: Targeted tests steerability toward specific misleading representations; untargeted tests general fragility
  - Population vs. individual: Population-level assesses overall representation stability; individual-level isolates specific feature vulnerability
  - Layer depth: Mid-to-late layers balance abstract representations vs. localization; deeper layers show lower targeted attack upper bounds but higher untargeted effectiveness

- **Failure signatures**: 
  - Dead latents: Zero activation success on individual activation tasks
  - High initial overlap: Pairs with >0.30 overlap are excluded as unsuitable for evaluation
  - Semantic degradation: Linear probe ΔFNR > 5% would indicate attacks alter LLM representations

- **First 3 experiments**: 
  1. Replicate population-level targeted attack on Gemma2-9B layer 30 with AdvBench pairs: verify concept overlap increase from ~27% to ~54%
  2. Run constrained synonym attack (non-adversarial baseline): confirm ~30% overlap decrease with single-word synonym replacement
  3. Transfer attack across model families (Gemma → Llama): assess performance degradation and verify transferred attacks retain meaningful effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified vulnerabilities extend to other concept-extraction architectures, specifically transcoders and crosscoders?
- Basis: The Discussion section states the vulnerability "could generalize to other SAE-inspired approaches" and explicitly notes "We leave the exploration of such extensions to future work."
- Why unresolved: While these variants share structural similarities with standard SAEs, they were not evaluated in current experiments
- What evidence would resolve it: Applying the proposed GCG-based evaluation framework to transcoders and crosscoders to measure their concept overlap fragility

### Open Question 2
- Question: How can SAE activations be effectively denoised or post-processed to isolate reliable signals against adversarial manipulation?
- Basis: The authors argue that without "further denoising or postprocessing they [SAEs] might be ill-suited for applications," identifying this as an "important future direction"
- Why unresolved: The paper establishes the problem of fragility but focuses on evaluation rather than proposing defense mechanisms
- What evidence would resolve it: A post-processing technique that minimizes the shift in concept overlap ratios when inputs are subjected to the GCG attack

### Open Question 3
- Question: What is the theoretical upper bound of SAE vulnerability given unlimited computational resources for attack optimization?
- Basis: The Limitations section notes that "effectiveness... is fundamentally bounded by compute constraints" and that "stronger attack performance is likely achievable" with more memory
- Why unresolved: It is unclear if current success rates (60-90%) represent intrinsic robustness of SAEs or limits of search algorithm on single GPU
- What evidence would resolve it: Scaling the number of GCG iterations and batch sizes to observe if attack success rate saturates or continues to climb toward 100%

## Limitations
- Dataset generalization may not extend to all domains or longer text sequences beyond the four tested datasets
- Results are specific to particular SAE configurations and may not uniformly extend across different architectures
- Computational constraints limit evaluation to single-token perturbations, leaving multi-token attack effectiveness uncertain

## Confidence

**High Confidence**: The fundamental finding that SAE representations are vulnerable to input-level adversarial attacks is well-supported with consistent results across multiple datasets and model families

**Medium Confidence**: Mechanism explanations linking gradient-guided token search to sparse latent space amplification are theoretically sound but rely on assumptions about bi-Lipschitz properties and gradient signal propagation

**Low Confidence**: Transfer attack results across model families show reduced effectiveness, but the magnitude and reliability of cross-model transferability remain uncertain due to limited model-family diversity

## Next Checks
1. **Cross-Dataset Robustness**: Replicate experiments on additional datasets including longer text sequences, code snippets, and domain-specific corpora to validate generalization beyond the four tested domains

2. **SAE Architecture Ablation**: Systematically vary SAE hyperparameters (width, sparsity levels, activation functions) and training objectives to determine which architectural choices most influence adversarial vulnerability profiles

3. **Extended Perturbation Patterns**: Implement multi-token substitution attacks and syntactic-level perturbations to assess whether vulnerability increases with attack sophistication, and identify potential thresholds where base LLM semantic degradation becomes significant