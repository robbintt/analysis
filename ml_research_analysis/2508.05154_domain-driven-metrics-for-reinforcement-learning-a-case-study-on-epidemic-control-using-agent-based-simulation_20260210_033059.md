---
ver: rpa2
title: 'Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic
  Control using Agent-based Simulation'
arxiv_id: '2508.05154'
source_url: https://arxiv.org/abs/2508.05154
tags:
- metrics
- algorithm
- state
- algorithms
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of evaluating reinforcement
  learning algorithms for optimizing agent-based models, particularly in epidemiological
  simulations. The authors propose domain-driven metrics that combine traditional
  reward-based measures with domain-specific knowledge.
---

# Domain-driven Metrics for Reinforcement Learning: A Case Study on Epidemic Control using Agent-based Simulation

## Quick Facts
- arXiv ID: 2508.05154
- Source URL: https://arxiv.org/abs/2508.05154
- Reference count: 1
- This study addresses the challenge of evaluating reinforcement learning algorithms for optimizing agent-based models, particularly in epidemiological simulations.

## Executive Summary
This study addresses the challenge of evaluating reinforcement learning algorithms for optimizing agent-based models, particularly in epidemiological simulations. The authors propose domain-driven metrics that combine traditional reward-based measures with domain-specific knowledge. They introduce a framework using InterestingnessXRL to extract interaction data and develop five key metrics: sequence comparison, median of mean-rewards, state-space coverage, unified coverage, and mean-reward comparison. Tested on a rational ABM disease modeling case study with 1000 agents, the approach evaluates eight RL algorithm variants across different mask availability scenarios. Results show that domain-driven metrics provide more robust algorithm ranking than traditional mean-reward comparisons alone. For instance, in the high-mask experiment, vanilla TD3 achieved the highest aggregate rank of 9, while NR_DDPG led in the low-mask scenario with an aggregate rank of 5. The metrics also aligned closely with Google's reliability metrics, enhancing confidence in the evaluation. This work offers a comprehensive and reliable framework for comparing RL algorithms in complex ABM applications.

## Method Summary
The method involves simulating an epidemiological agent-based model with 1000 rational agents over 100 days, where policy actions (lockdown/vaccination timing and duration) are optimized using eight variants of DDPG and TD3 algorithms. State and action spaces are continuous and discretized using domain-informed bins. The framework extracts interaction data via InterestingnessXRL, computes five domain-driven metrics (state-space coverage, unified coverage, sequence comparison, median mean-reward, mean-reward), and aggregates rankings across these metrics. The approach is validated across three mask-availability scenarios and compared against Google's RL reliability metrics.

## Key Results
- Domain-driven metrics provide more robust algorithm rankings than traditional reward-based metrics alone.
- Vanilla TD3 achieved the highest aggregate rank of 9 in the high-mask experiment.
- NR_DDPG led in the low-mask scenario with an aggregate rank of 5.
- Domain-driven metrics aligned closely with Google's reliability metrics (IQR, CVaR variants).

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating rankings from five domain-informed metrics produces more stable algorithm comparisons than single-metric reward evaluation.
- Mechanism: Each algorithm receives per-metric ranks for state-space coverage, unified coverage, sequence comparison, median reward, and mean reward. Aggregate rank is computed as the sum, with lower aggregate values indicating better overall performance. This diversifies evaluation across exploration quality, exploit performance, and domain-relevant outcomes.
- Core assumption: No single metric captures algorithm quality; aggregation smooths scenario-specific noise and stochasticity.
- Evidence anchors:
  - [abstract] "Results show that domain-driven metrics provide more robust algorithm rankings than traditional reward-based metrics alone."
  - [section 4.6] Algorithm rankings based on Domain-driven metrics remain consistent when Google's Reliability metrics are incorporated, whereas mean-reward rankings are not robust.
  - [corpus] ADAGE (arXiv:2501.09429) highlights adaptation and evaluation challenges in ABMs, supporting the need for multi-faceted metrics, but does not validate these specific metrics.
- Break condition: If metrics are highly correlated (provide redundant information), aggregation may not improve robustness; correlation analysis would be needed.

### Mechanism 2
- Claim: Discretizing continuous state/action spaces via domain-informed binning enables explainability tools designed for discrete spaces to operate on continuous environments.
- Mechanism: Continuous state components (InfectedMild, Hospitalized, minStockHousePercentage) are binned using domain-chosen thresholds ([0.0, 0.05, 0.10, 0.15, 0.20, 1]). Unique combinations of bin indices form discrete "state indices" for frequency analysis, sequence extraction, and coverage calculation.
- Core assumption: Domain knowledge yields bins that preserve task-relevant distinctions; coarse binning does not erase critical state differences.
- Evidence anchors:
  - [section 4.4.2] "Bins used to perform binning of state components are [0.0, 0.05, 0.10, 0.15, 0.20, 1] which is decided on the basis of domain knowledge."
  - [section 4.4.2] Invalid state combinations are pruned using domain constraints (e.g., hospitalization cannot exceed 10%), refining coverage denominators.
  - [corpus] No direct validation in corpus; SSRCA (arXiv:2506.00168) addresses sensitivity analysis in ABMs but not discretization strategies.
- Break condition: If optimal policies depend on fine-grained distinctions within bins, discretization will obscure algorithm differences.

### Mechanism 3
- Claim: Sequence comparison—measuring how often exploit runs end in domain-defined "best" states—captures policy quality beyond reward summaries.
- Mechanism: Transition value analysis identifies local minima (start) and local maxima (end) states in trajectories. Sequences from minima to maxima are extracted. Domain knowledge defines the "best" end state (state index 0: <5% infected, hospitalized, BPL). Best sequence percentage ranks algorithms by how reliably they reach desirable outcomes.
- Core assumption: Exploit runs represent learned policy quality; sequences ending in state 0 reflect meaningful policy success per domain criteria.
- Evidence anchors:
  - [section 4.5.1] "State index 0 is the best end state because at this state infected, hospitalized, and below the poverty line (BPL) population is the least (< 5%)."
  - [table 2, table 3] Best sequence percentages vary across algorithms (0–100%), providing discriminative signal in rankings.
  - [corpus] No corpus evidence directly validates sequence-based RL evaluation; this appears novel.
- Break condition: If stochastic environment dynamics prevent reliable convergence to best states despite good policies, sequence percentage may reflect environment luck rather than algorithm quality.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs) with continuous action spaces**
  - Why needed here: The paper formulates policy optimization as an MDP (S, A, P, R, γ) with continuous actions (lockdown/vaccination timing, durations). Understanding this formalism is prerequisite to interpreting DDPG/TD3 behavior and the role of uncertainty variants.
  - Quick check question: Given state s and action a, can you explain how P(s'|s,a) and R(s,a) define the learning problem?

- Concept: **Actor-Critic algorithms (DDPG, TD3)**
  - Why needed here: Eight algorithm variants (vanilla and uncertainty-aware DDPG/TD3) are compared. TD3's twin critics and delayed policy updates affect stability; understanding this clarifies why TD3 ranks higher in certain experiments.
  - Quick check question: What problem does TD3's twin Q-network design address compared to vanilla DDPG?

- Concept: **Agent-Based Models (ABMs) and emergent behavior**
  - Why needed here: The simulation involves 1000 rational agents making masking, vaccination, and mobility decisions. Policy actions influence but do not deterministically control outcomes; stochasticity and agent heterogeneity complicate evaluation.
  - Quick check question: Why might the same policy action yield different outcomes in two simulation runs of an ABM?

## Architecture Onboarding

- Component map:
  - Simulation Module -> Policy Discovery Module -> Analysis Module -> Evaluation Metric Module -> Algorithm Ranking Module

- Critical path:
  1. Run simulation with policy actions from RL algorithm under evaluation.
  2. Log state/action/reward trajectories during training and exploit runs.
  3. Bin continuous states/actions to indices using domain thresholds.
  4. Extract sequences from local minima to local maxima states.
  5. Compute five metrics; aggregate ranks for final comparison.

- Design tradeoffs:
  - Bin granularity vs. computational tractability: Finer bins capture more detail but increase state-space cardinality and sparsity.
  - Single aggregate rank vs. per-metric analysis: Aggregation provides simplicity but may obscure specific strengths/weaknesses.
  - Domain-only bins vs. data-driven bins: Domain bins encode expert knowledge but may miss empirically important thresholds.

- Failure signatures:
  - All algorithms achieving similar mean rewards with high variance (Figure 2) indicates environment stochasticity dominates; metrics must discriminate beyond reward.
  - Best sequence percentage = 0% for multiple algorithms suggests either environment constraints or binning issues preventing reachability of target states.
  - State-space coverage near 0% indicates binning may be too fine or training insufficient.

- First 3 experiments:
  1. Replicate the binning scheme on a simple environment (e.g., CartPole with discretized states) to validate that sequence extraction and coverage metrics produce sensible rankings against known baselines.
  2. Run ablation on bin granularity (coarse vs. fine bins for one state component) to quantify sensitivity of coverage metrics to binning choices.
  3. Integrate a third RL algorithm family (e.g., PPO, SAC) into the comparison pipeline to test whether metric rankings generalize beyond DDPG/TD3 variants.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do domain-driven metrics perform when applied to larger-scale agent populations with more complex behavioral models?
- Basis in paper: [explicit] The authors state, "Shortly, we will expand the model to include a larger scale of rational agents with more complex human and economic behavior."
- Why unresolved: The current framework was validated on a "small community" of 1000 agents with simplified decision-making rules (office/school/shop).
- What evidence would resolve it: Successful application of the metrics to simulations with orders of magnitude more agents and heterogeneous behavioral rules without computational bottlenecks or loss of ranking stability.

### Open Question 2
- Question: How can the proposed metrics be integrated with explainability techniques to clarify why specific algorithms are ranked higher?
- Basis in paper: [explicit] The authors note, "In addition, we do not employ explainability or techniques to explain each model's decision. Shortly, we aim to extend our work to account for explainability."
- Why unresolved: While the metrics rank algorithms, they currently function as a "black box" assessment without detailing the specific policy features driving the performance differences.
- What evidence would resolve it: A framework extension that correlates specific metric outcomes (e.g., sequence comparison) with interpretable policy attributes or state-action saliency maps.

### Open Question 3
- Question: To what extent does the arbitrary discretization (binning) of the continuous state-action space influence the final algorithm rankings?
- Basis in paper: [inferred] The methodology relies on converting continuous states into discrete "state indices" using domain-knowledge bins (e.g., [0.0, 0.05, 0.10...]) to calculate coverage and sequences, but the robustness of this choice is not tested.
- Why unresolved: Altering the bin granularity could significantly change the "State-space Coverage" percentage or the definition of "best sequences," potentially altering the aggregate ranks.
- What evidence would resolve it: A sensitivity analysis showing that algorithm rankings remain consistent across different valid binning strategies (e.g., comparing 5 bins vs. 10 bins for state components).

## Limitations
- Reward Function Specification: The exact formulation of the reward combining infections, hospitalizations, and economic status is not provided, limiting exact replication.
- Algorithm Hyperparameters: Key DDPG/TD3 hyperparameters (learning rates, network sizes, exploration settings) are unspecified, affecting reproducibility of the reported rankings.
- Binning Sensitivity: The choice of binning thresholds is domain-driven but lacks sensitivity analysis to show robustness to alternative thresholds.

## Confidence

- **High Confidence**: The mechanism by which sequence comparison captures policy quality is well-supported by the state index definition and empirical ranking results.
- **Medium Confidence**: The claim that aggregation of five metrics yields more stable rankings is supported by experimental comparison but would benefit from correlation analysis.
- **Medium Confidence**: The discretization approach is logically sound and supported by domain knowledge, but lacks empirical validation against continuous-space metrics.

## Next Checks

1. **Hyperparameter Sensitivity**: Run the full pipeline with varied DDPG/TD3 hyperparameters (learning rates, network sizes, noise scales) to assess stability of metric rankings across configurations.
2. **Binning Granularity Impact**: Systematically vary state and action bin granularity (e.g., halving/doubling bin counts) and measure impact on state-space coverage and best sequence percentages.
3. **Correlation and Redundancy Analysis**: Compute pairwise correlations between the five domain-driven metrics; if highly correlated, test whether subset selection yields similar ranking stability to full aggregation.