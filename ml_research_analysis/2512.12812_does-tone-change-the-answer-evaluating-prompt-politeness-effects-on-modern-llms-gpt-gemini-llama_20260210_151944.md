---
ver: rpa2
title: 'Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern
  LLMs: GPT, Gemini, LLaMA'
arxiv_id: '2512.12812'
source_url: https://arxiv.org/abs/2512.12812
tags:
- tone
- very
- accuracy
- rude
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated how prompt tone affects the
  accuracy of three state-of-the-art LLMs (GPT-4o mini, Gemini 2.0 Flash, Llama 4
  Scout) across six MMMLU benchmark tasks. We tested three tone variants (Very Friendly,
  Neutral, Very Rude) with ten repeated trials per question and analyzed results using
  mean differences and 95% confidence intervals.
---

# Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA

## Quick Facts
- arXiv ID: 2512.12812
- Source URL: https://arxiv.org/abs/2512.12812
- Reference count: 40
- Key outcome: Tone effects are generally small, statistically insignificant at domain level, with rare significant effects limited to Humanities tasks for GPT and Llama, while Gemini shows no sensitivity

## Executive Summary
This study systematically evaluated how prompt tone affects the accuracy of three state-of-the-art LLMs (GPT-4o mini, Gemini 2.0 Flash, Llama 4 Scout) across six MMMLU benchmark tasks. Using extreme politeness variants (Very Friendly, Very Rude, Neutral) with ten repeated trials per question, the authors found that while neutral and friendly tones generally outperformed rude tones, statistically significant effects were rare and limited to specific Humanities tasks. When performance was aggregated across domains, tone effects diminished and became negligible, suggesting modern LLMs are broadly robust to tonal variation in typical mixed-domain use.

## Method Summary
The researchers tested three tone variants (Very Friendly, Neutral, Very Rude) on three LLMs using MMMLU benchmark tasks. Each question was queried 10 times per tone condition, yielding 540 total condition combinations. Tone prefixes were prepended to questions (e.g., "Would you be so kind as to solve..." for friendly). Accuracy was measured as the proportion of correct single-letter answers across trials. Statistical analysis used pairwise mean differences with 95% confidence intervals, with significance determined when CIs excluded zero. Tasks included STEM domains (Anatomy, Astronomy, College Biology) and Humanities domains (US History, Philosophy, Professional Law).

## Key Results
- No statistically significant tone effects at the domain-aggregated level; most CIs included zero
- GPT and Llama showed rare significant tone effects only in Humanities tasks, where rude tone reduced accuracy
- Gemini demonstrated minimal sensitivity with no significant accuracy differences across tone conditions
- Neutral tone consistently showed small positive effects, but these were rarely statistically significant

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tone sensitivity varies across model families due to differences in training data curation, instruction-tuning objectives, or alignment procedures.
- Mechanism: LLMs trained with different corpora and RLHF strategies may internalize different associations between pragmatic markers (politeness cues) and response quality. Gemini's lack of significant tone sensitivity suggests either more robust alignment or different training data distributions that decouple tone from task performance.
- Core assumption: Architectural or training differences—not just model scale—drive tone sensitivity.
- Evidence anchors: [abstract] "tone sensitivity is both model-dependent and domain-specific... Gemini remains comparatively tone-insensitive"; [section V.B] "the Gemini model demonstrates minimal sensitivity, showing no significant accuracy differences across tone conditions. This difference across model families implies that the architecture or training differences behind each model may modulate how models respond to different prompts phrasing."

### Mechanism 2
- Claim: Humanities tasks exhibit stronger tone effects because they require interpretive reasoning where pragmatic framing influences contextual understanding.
- Mechanism: Tasks involving nuance, ambiguity, or subjective interpretation (Philosophy, Law) may cause models to weight pragmatic cues more heavily in their latent reasoning. STEM tasks, with more objective factual anchors, provide stronger grounding that reduces reliance on conversational context.
- Core assumption: Tone operates as a contextual signal that interacts with task-reasoning type.
- Evidence anchors: [abstract] "statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama"; [section V.C] "Tone effects tend to be more pronounced in Humanities tasks, which involve higher-level reasoning, nuance, and interpretive judgment. In contrast, STEM tasks show consistently positive but statistically weaker effects, with most confidence intervals crossing zero."

### Mechanism 3
- Claim: Aggregating across diverse tasks attenuates tone effects because task-level variability averages out when domains are mixed.
- Mechanism: Individual task comparisons may capture noise or domain-specific artifacts that do not generalize. When performance is aggregated across heterogeneous tasks, random directional differences cancel, revealing that tone has no systematic cross-domain impact.
- Core assumption: Observed task-level effects contain a substantial noise component.
- Evidence anchors: [abstract] "When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance"; [section V.D] "the directional trends observed in mean differences remain consistent with task-level observations; however, most confidence intervals include zero, indicating predominantly NSS outcomes... tone-induced variability observed at the per-task level tends to attenuate when aggregated"

## Foundational Learning

- Concept: Statistical significance vs. directional trends
  - Why needed here: The paper reports many directional differences (e.g., Neutral > Rude) that are not statistically significant; understanding confidence intervals prevents overinterpreting noise.
  - Quick check question: If a mean difference is +2% but the 95% CI includes zero, can you claim the effect is real?

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: Llama 4 Scout uses MoE with 16 experts; tone could theoretically affect expert routing, though the paper does not isolate this mechanism.
  - Quick check question: How might polite vs. rude prompts influence which expert is activated for a given query?

- Concept: Instruction tuning and alignment
  - Why needed here: Differences in RLHF or instruction-tuning across GPT, Gemini, and Llama may explain why Gemini is tone-insensitive while others are not.
  - Quick check question: What alignment objective might produce a model that ignores politeness cues entirely?

## Architecture Onboarding

- Component map: Session reset instruction -> Tone prefix (Very Friendly/Very Rude/Neutral) -> MMMLU question -> Model output (single letter) -> Accuracy calculation
- Critical path:
  1. Preprocess MMMLU questions with tone prefixes (Section III.B.2)
  2. Query each model with identical prompts across 10 trials
  3. Extract single-letter answers, compare to ground truth
  4. Compute mean accuracy differences and confidence intervals per task
  5. Aggregate to domain level and re-test significance
- Design tradeoffs:
  - Using extreme tone variants (Very Friendly/Very Rude) maximizes signal but may not reflect natural user behavior
  - 10 trials reduce stochastic variance but increase API cost
  - MMMLU multiple-choice format provides unambiguous ground truth but limits ecological validity
- Failure signatures:
  - Confidence intervals consistently crossing zero despite large mean differences → high question-level variance
  - Opposite directional effects in different tasks → tone interacts unpredictably with domain
  - One model showing no significant effects while others do → likely training/alignment divergence
- First 3 experiments:
  1. Replicate on a single task (e.g., Philosophy) with 30 trials to test whether significance increases with sample size.
  2. Add a mid-range tone condition ("Slightly Friendly") to test for non-linear effects.
  3. Run the same protocol on an open-ended generation task to test whether tone effects are larger without multiple-choice anchoring.

## Open Questions the Paper Calls Out

- Question: Is tone robustness primarily determined by architectural design, data curation, or instruction-tuning strategies?
  - Basis in paper: [explicit] The conclusion states that evaluating fully open-source architectures with transparent training details is necessary to clarify what shapes tone robustness.
  - Why unresolved: The study used GPT, Gemini, and Llama models, which have undisclosed or proprietary training data and specific tuning details, making it impossible to isolate the cause of sensitivity.
  - What evidence would resolve it: Controlled experiments on open-source models where only the architecture, data composition, or tuning method is varied while holding other factors constant.

- Question: Do tone effects manifest more strongly or differently in multilingual, open-ended, or multimodal interaction settings?
  - Basis in paper: [explicit] The authors explicitly propose moving beyond English multiple-choice benchmarks to these formats to reveal potentially different tone effects.
  - Why unresolved: The current methodology was restricted to the English MMMLU multiple-choice benchmark.
  - What evidence would resolve it: Applying the same tone variants to open-ended generation tasks, multimodal inputs, and non-English languages.

- Question: How does prompt tone impact safety compliance and model calibration?
  - Basis in paper: [explicit] The conclusion lists safety compliance and calibration error as additional metrics that should be incorporated in future work.
  - Why unresolved: This study measured performance solely through accuracy on knowledge-based questions.
  - What evidence would resolve it: Experiments measuring refusal rates (safety) and confidence-accuracy alignment (calibration) across different tone prompts.

## Limitations
- Extreme tone variants used may not reflect natural user behavior in typical interactions
- Single-answer format constraint may suppress tone effects that would manifest in open-ended responses
- Domain sampling bias (STEM vs Humanities split) doesn't capture full diversity of reasoning types

## Confidence
- High Confidence: Tone effects are generally small and statistically insignificant at the domain-aggregated level
- Medium Confidence: Model-specific sensitivity patterns (GPT/Llama showing Humanities effects vs. Gemini's insensitivity)
- Low Confidence: The mechanism that STEM tasks show weaker tone effects due to objective grounding

## Next Checks
1. Natural politeness spectrum test: Replicate using progressively warmer/cooler variants rather than extremes to determine if effects scale linearly
2. Open-ended response validation: Run protocol on explanatory answers to compare effects on response format and confidence calibration
3. Cross-lingual tone transfer: Test whether English politeness effects transfer to equivalent tasks in other languages using translated prompts