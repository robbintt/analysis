---
ver: rpa2
title: 'Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery'
arxiv_id: '2507.23488'
source_url: https://arxiv.org/abs/2507.23488
tags:
- causal
- reasoning
- pipeline
- each
- independence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates whether state-of-the-art reasoning models
  can robustly perform causal discovery, a task where conventional models suffer from
  overfitting and fail under data perturbations. The authors propose a modular in-context
  pipeline inspired by Tree-of-Thoughts and Chain-of-Thoughts methodologies, decomposing
  the Peter-Clark algorithm into four focused prompts: skeleton extraction, v-structure
  identification, Meek-rule orientation, and hypothesis evaluation.'
---

# Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery

## Quick Facts
- arXiv ID: 2507.23488
- Source URL: https://arxiv.org/abs/2507.23488
- Authors: Kacper Kadziolka; Saber Salehkaleybar
- Reference count: 17
- Key outcome: Modular in-context pipeline achieves up to three-fold F1 improvement over conventional baselines for causal discovery

## Executive Summary
This paper evaluates whether state-of-the-art reasoning models can robustly perform causal discovery, a task where conventional models suffer from overfitting and fail under data perturbations. The authors propose a modular in-context pipeline inspired by Tree-of-Thoughts and Chain-of-Thoughts methodologies, decomposing the Peter-Clark algorithm into four focused prompts: skeleton extraction, v-structure identification, Meek-rule orientation, and hypothesis evaluation. Using the Corr2Cause benchmark, the modular pipeline achieves up to a three-fold F1 improvement over conventional baselines, with OpenAI o3-mini reaching 83.83 F1 and DeepSeek-R1 API reaching 79.83 F1. The pipeline consistently improves performance across all tested reasoning models, demonstrating that carefully structured in-context frameworks are essential to maximize the capabilities of reasoning-specialist LLMs for causal discovery tasks.

## Method Summary
The paper proposes a modular in-context learning pipeline that decomposes the Peter-Clark causal discovery algorithm into four sequential stages, each handled by a dedicated prompt and JSON parser. The pipeline consists of: (1) skeleton extraction identifying conditional independencies, (2) v-structure identification, (3) Meek-rule orientation, and (4) hypothesis evaluation. Each stage receives structured output from the previous parser as input, enforcing explicit intermediate commitments. The approach uses zero-shot in-context learning on the CORR2CAUSE benchmark without fine-tuning, comparing a single-prompt baseline against the 4-stage modular pipeline across multiple reasoning-specialist LLMs.

## Key Results
- Modular pipeline achieves up to three-fold F1 improvement over conventional baselines
- OpenAI o3-mini reaches 83.83 F1, DeepSeek-R1 API reaches 79.83 F1 on CORR2CAUSE benchmark
- Pipeline improves recall by 10-15 points and precision even more across all tested reasoning models
- Token usage increases 2-3× with 4 sequential API calls versus 1 single-prompt baseline

## Why This Works (Mechanism)

### Mechanism 1
Decomposing the PC algorithm into sequential prompts amplifies reasoning-model performance by re-activating inference processes at each stage. Each module receives dedicated context, parser-enforced output schemas, and the prior stage's structured artifacts as input—forcing explicit intermediate commitments and enabling error localization. Reasoning-specialist LLMs accumulate reasoning effort across multiple in-context calls rather than exhausting capacity in a single pass.

### Mechanism 2
Reasoning models' iterative self-checks correct errors that conventional models make due to insufficient conditional independence testing. Reasoning traces exhibit micro-step re-evaluation with explicit self-correction markers; conventional models terminate earlier, missing revisits that could catch misclassified edges. Self-correction in reasoning traces causally improves final answers.

### Mechanism 3
Stage-wise decomposition improves precision by preventing collapse to trivial "always-false" strategies common in imbalanced datasets. CORR2CAUSE has ~85% "false" label prevalence; modular prompts balance precision and recall by enforcing explicit graph construction before hypothesis evaluation, reducing pattern-matching shortcuts.

## Foundational Learning

- **Concept**: d-separation and conditional independence in DAGs
  - Why needed here: The PC algorithm's skeleton extraction relies on detecting conditional independencies to remove edges
  - Quick check question: Given A → B → C, are A and C d-separated given B?

- **Concept**: Markov equivalence classes (MECs) and CPDAGs
  - Why needed here: The final hypothesis evaluation must hold across all DAGs in the equivalence class, not just one orientation
  - Quick check question: Can two DAGs with different edge directions imply the same conditional independencies?

- **Concept**: Meek's orientation rules for PDAGs
  - Why needed here: Stage 3 orients edges while avoiding cycles and preserving v-structures; understanding rule preconditions is essential
  - Quick check question: What constraints must hold before Meek Rule 1 can orient A—B as A→B?

## Architecture Onboarding

- **Component map**: Stage 1 Prompt + Parser (skeleton JSON) → Stage 2 Prompt + Parser (separation_sets, v_structures) → Stage 3 Prompt + Parser (directed_edges, undirected_edges) → Stage 4 Prompt + Parser (hypothesis_answer boolean)

- **Critical path**: Stage 1 → Stage 2 → Stage 3 → Stage 4. Errors in skeleton extraction propagate; Stage 2's separation-set extraction depends on correct premise parsing

- **Design tradeoffs**:
  - Token cost vs accuracy: Pipeline increases tokens 2–3× (o3-mini: ~4K → ~11K; DeepSeek-R1 API: ~8K → ~25K per sample)
  - Latency: 4 sequential API calls vs 1; higher total latency for higher F1
  - Robustness: More parsers = more failure points; schema violations trigger re-prompting

- **Failure signatures**:
  - Stage 1 skeleton with spurious edges → Stage 2 false v-structures → Stage 3 over-orientation
  - Parser unable to extract separation_sets → Stage 2 aborts or uses default empty
  - Stage 3 creates directed cycles → violates Meek constraints; downstream hypothesis evaluation unreliable

- **First 3 experiments**:
  1. Replicate single-prompt baseline on a 100-sample CORR2CAUSE subset with o3-mini; log F1, token usage, and error types
  2. Run Stage 1 in isolation; compare extracted skeletons against ground truth to quantify edge-removal precision/recall
  3. Introduce controlled noise into Stage 2 separation_sets (e.g., randomly drop 20%); measure downstream hypothesis evaluation F1 degradation to quantify stage coupling strength

## Open Questions the Paper Calls Out
- Does the modular pipeline maintain its performance improvements when applied to other causal discovery benchmarks or real-world datasets?
- Can the substantial increase in token usage and computational cost be reduced without sacrificing accuracy?
- Can this modular decomposition strategy be effectively adapted to other structured-inference algorithms or reasoning domains?

## Limitations
- Token cost and latency significantly increase with 4 sequential API calls and 2-3× token usage
- Parser dependency creates cascading error risks when schema violations occur
- Evaluation restricted to reasoning-specialist LLMs and synthetic CORR2CAUSE dataset
- Attribution ambiguity between stage-wise decomposition and other confounding factors

## Confidence
- **High Confidence**: The modular pipeline achieves measurable F1 improvements over single-prompt baselines on CORR2CAUSE. Token usage increases and error localization benefits are empirically supported.
- **Medium Confidence**: The mechanism of iterative self-correction in reasoning models causally improves causal discovery. The causal link between reasoning traces and final accuracy is plausible but not definitively proven.
- **Low Confidence**: The pipeline's gains generalize beyond reasoning-specialist LLMs and niche benchmarks. Real-world deployment feasibility under token/latency constraints is uncertain.

## Next Checks
1. **Parser Robustness Test**: Introduce controlled schema violations in intermediate JSON outputs; measure re-prompting frequency and downstream F1 degradation to quantify parser coupling strength.
2. **Non-Reasoning Model Comparison**: Run the modular pipeline on a non-reasoning LLM (e.g., GPT-4o-mini) with identical prompting; compare F1 gains to those of reasoning-specialist models to test mechanism specificity.
3. **Real-World Dataset Transfer**: Apply the pipeline to a domain-shifted causal discovery dataset (e.g., real-world clinical or economic data); evaluate whether precision-recall balance gains persist under distributional shift.