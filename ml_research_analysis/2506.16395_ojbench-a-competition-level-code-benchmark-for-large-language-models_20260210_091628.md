---
ver: rpa2
title: 'OJBench: A Competition Level Code Benchmark For Large Language Models'
arxiv_id: '2506.16395'
source_url: https://arxiv.org/abs/2506.16395
tags:
- code
- arxiv
- ojbench
- problems
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OJBench is a competition-level code reasoning benchmark introduced
  to evaluate the reasoning capabilities of large language models (LLMs) in complex
  algorithmic tasks. The benchmark comprises 232 problems from the National Olympiad
  in Informatics (NOI) and the International Collegiate Programming Contest (ICPC),
  categorized into Easy, Medium, and Hard difficulty levels.
---

# OJBench: A Competition Level Code Benchmark For Large Language Models

## Quick Facts
- arXiv ID: 2506.16395
- Source URL: https://arxiv.org/abs/2506.16395
- Reference count: 40
- 232 NOI/ICPC problems reveal even state-of-the-art reasoning models struggle with competitive-level code reasoning

## Executive Summary
OJBench is a competition-level code reasoning benchmark designed to evaluate large language models' capabilities on complex algorithmic tasks from the National Olympiad in Informatics (NOI) and International Collegiate Programming Contest (ICPC). The benchmark comprises 232 problems categorized into Easy, Medium, and Hard difficulty levels, supporting dual-language assessment in Python and CPP. Experiments on 37 models reveal that even state-of-the-art reasoning-oriented models achieve only modest pass@1 rates on highly challenging problems, with CPP generally outperforming Python. The study demonstrates that while models can improve through refinement based on execution feedback, they struggle particularly with time limit exceeded (TLE) errors.

## Method Summary
OJBench evaluates LLMs on 232 algorithmic problems from NOI and ICPC competitions using full test case suites. Models generate 8 candidate solutions per problem, which are executed in sandboxed environments for both Python and CPP. Solutions must pass all test cases to be considered correct (no partial credit). The benchmark supports optional refinement loops where error feedback (compile errors, wrong answers, TLE) is fed back as prompts for iterative correction. Evaluation uses Pass@1 and Pass@8 metrics, with reasoning-oriented models allocated 64k context windows while non-reasoning models use default settings.

## Key Results
- State-of-the-art reasoning models achieve only 33-39% pass@1 rates on competitive-level problems
- CPP consistently outperforms Python, aligning with human competitive programming experience
- Models can reduce compile and wrong answer errors through refinement but struggle to address TLE errors
- Performance gaps between reasoning-oriented and non-reasoning models confirm specialized training improves competitive code generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning-oriented training (RL and distillation) improves competitive code generation more effectively than scaling alone.
- Mechanism: Post-training with reinforcement learning on reasoning tasks may enhance the model's ability to decompose complex problems, plan multi-step solutions, and self-correct, compared to models trained primarily on large-scale code corpora.
- Core assumption: Improvements on competitive tasks are primarily attributable to reasoning-focused post-training, not other architectural or data differences.
- Evidence anchors:
  - [abstract] "even state-of-the-art reasoning-oriented models, such as o4-mini and Gemini-2.5-pro-exp, struggle with highly challenging competition-level problems."
  - [section 3.2] "models designed for reasoning tasks consistently outperform non-reasoning-oriented models" and "merely relying on large-scale pretraining is insufficient"
  - [corpus] ICPC-Eval confirms existing benchmarks are insufficient for real competition environments, supporting the need for specialized evaluation.
- Break condition: If reasoning-oriented models underperform non-reasoning models of similar or larger scale on algorithmically complex tasks, the mechanism is weakened.

### Mechanism 2
- Claim: CPP language selection yields higher pass rates than Python for top reasoning-oriented models on time-sensitive competitive problems.
- Mechanism: CPP's runtime efficiency may allow borderline-correct algorithms to pass time limits that would fail in Python, revealing algorithmic correctness that Python's slower execution masks.
- Core assumption: The performance gap is primarily due to execution efficiency rather than differences in training data distribution across languages.
- Evidence anchors:
  - [abstract] "models using CPP generally outperform those using Python, aligning with human experience in competitive programming."
  - [section 4.2] "CPP is inherently a high-performance programming language, making it more suitable for solving competition-level programming tasks than Python."
  - [corpus] CodeElo notes similar language preferences in human competition settings, aligning with this finding.
- Break condition: If CPP and Python show comparable pass rates on time-unconstrained problems, but diverge significantly on TLE-prone problems, the efficiency mechanism is supported; if gaps persist uniformly, training data distribution may be dominant.

### Mechanism 3
- Claim: Iterative refinement with execution feedback can improve pass rates, but Time Limit Exceeded (TLE) errors remain particularly resistant to correction.
- Mechanism: Models can interpret explicit error signals (e.g., Compile Error, Wrong Answer) and adjust surface-level code; however, TLE resolution requires deeper algorithmic redesign, which current models struggle to perform.
- Core assumption: TLE correction requires conceptual algorithmic insight rather than syntactic or minor logical fixes.
- Evidence anchors:
  - [abstract] "models can improve their performance through refinement based on execution feedback, though they still face difficulties addressing time limit exceeded (TLE) errors."
  - [section 4.3] "the proportion of CE errors decreased most significantly" while "the model struggled to address TLE errors during the refinement process."
  - [corpus] Related work (ICPC-Eval) discusses pass@k and evaluation metrics but does not specifically analyze refinement dynamics or TLE-specific failure modes.
- Break condition: If future models show rapid TLE correction without algorithmic redesign, the assumption that TLE requires deep insight may need revision.

## Foundational Learning

- Concept: **Pass@k metric**
  - Why needed here: OJBench reports Pass@1 and Pass@8; understanding this metric is essential for interpreting model capabilities and sample diversity.
  - Quick check question: If a model has Pass@8 of 48% but Pass@1 of 33%, what does this imply about solution diversity?

- Concept: **Test case coverage and false positives**
  - Why needed here: The paper demonstrates that performance drops as test case count increases, indicating that small test sets may overestimate capability.
  - Quick check question: Why might a solution pass 5 test cases but fail on 50, and how should this affect benchmark design?

- Concept: **Reasoning-oriented vs. non-reasoning-oriented models**
  - Why needed here: The paper compares these two categories; understanding the training paradigm difference (RL/distillation vs. corpus-only) is key to interpreting results.
  - Quick check question: What training signal distinguishes a reasoning-oriented model from a standard code model?

## Architecture Onboarding

- Component map:
  - Data source -> Problem selection and test case validation
  - Difficulty classification -> Human voting for NOI; formula-based scoring for ICPC
  - Evaluation engine -> Dual-language (Python/CPP) judge using full test case suite
  - Refinement loop -> Error feedback (CE, WA, TLE, etc.) fed back as prompt for iterative correction

- Critical path:
  1. Problem selection and test case validation
  2. Difficulty annotation (voting or formula)
  3. Model inference with 8 candidate solutions per problem
  4. Full test case evaluation (not sample-based)
  5. Optional refinement pass with error-type feedback

- Design tradeoffs:
  - CPP vs. Python: CPP favors runtime efficiency; Python is more common in training data. Evaluate both to separate algorithmic correctness from execution efficiency.
  - Test case volume: More test cases reduce false positives but increase evaluation cost. The paper validates using the full set.
  - Problem scope: Focused on NOI/ICPC algorithmic problems; does not cover applied domains (IoT, blockchain). High difficulty but narrow coverage.

- Failure signatures:
  - High Pass@1 on Easy, near-zero on Hard: Model lacks deep reasoning or algorithmic generalization.
  - Large gap between Pass@1 and Pass@8: Model can explore diverse solutions but lacks consistent reliability.
  - Persistent TLE after refinement: Indicates inability to perform algorithmic redesign.
  - Strong Python / weak CPP performance: May reflect training data language bias rather than reasoning limitation.

- First 3 experiments:
  1. Establish baseline Pass@1 and Pass@8 for your model on OJBench in both Python and CPP; compare gap across difficulty levels to identify where reasoning breaks down.
  2. Run refinement loop with error-type feedback; track reduction in CE, WA, and TLE separately to diagnose which error types your model can correct.
  3. Compare performance on the same problems using reduced vs. full test case sets to estimate false positive rate and validate robustness of your evaluation pipeline.

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow domain focus on NOI/ICPC problems excludes applied programming domains like IoT and blockchain
- Does not analyze specific algorithmic patterns that models fail to capture
- Limited analysis of which error types models can correct beyond TLE resistance

## Confidence
- High: Core finding that reasoning-oriented models struggle with competition-level problems
- High: CPP outperforming Python on algorithmic tasks
- Medium: Effectiveness of refinement mechanism beyond TLE resistance
- Medium: Language-specific performance differences controlling for training data bias

## Next Checks
1. **Algorithmic pattern analysis**: Systematically categorize model failures by algorithmic pattern (e.g., dynamic programming, graph algorithms) to identify specific reasoning gaps rather than treating all failures uniformly.

2. **Training data bias assessment**: Compare OJBench performance against models' training data language distributions to quantify how much CPP superiority reflects training bias versus genuine execution efficiency advantages.

3. **Extended refinement protocol**: Implement a multi-round refinement protocol that explicitly guides models through algorithmic redesign rather than simple error correction, testing whether TLE resistance stems from insight limitations or refinement methodology constraints.