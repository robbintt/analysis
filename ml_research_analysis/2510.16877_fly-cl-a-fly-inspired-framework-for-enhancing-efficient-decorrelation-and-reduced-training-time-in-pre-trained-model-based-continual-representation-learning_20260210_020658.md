---
ver: rpa2
title: 'Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and
  Reduced Training Time in Pre-trained Model-based Continual Representation Learning'
arxiv_id: '2510.16877'
source_url: https://arxiv.org/abs/2510.16877
tags:
- fly-cl
- learning
- accuracy
- performance
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fly-CL addresses catastrophic forgetting in continual learning
  by drawing inspiration from the fly olfactory circuit. It transforms parameter updates
  into an efficient similarity-matching problem, significantly reducing training time
  while maintaining competitive performance with state-of-the-art methods.
---

# Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning

## Quick Facts
- arXiv ID: 2510.16877
- Source URL: https://arxiv.org/abs/2510.16877
- Authors: Heming Zou; Yunliang Zang; Wutong Xu; Xiangyang Ji
- Reference count: 40
- Primary result: Reduces post-extraction training time by 38-93% while improving accuracy by 1.17-2.38% in continual learning

## Executive Summary
Fly-CL introduces a novel continual learning framework inspired by the fly olfactory circuit that addresses catastrophic forgetting through efficient decorrelation of class prototypes. The framework transforms parameter updates into a similarity-matching problem using sparse random projections and top-k operations, significantly reducing training time while maintaining competitive performance with state-of-the-art methods. By progressively decorrelating class prototypes, Fly-CL eliminates multicollinearity that typically degrades similarity-based classification in continual learning scenarios.

## Method Summary
Fly-CL operates by extracting features from pre-trained models and applying a fly-inspired decorrelation mechanism using sparse random projections. The framework maintains class prototypes and updates them through an adaptive streaming ridge classification approach. The key innovation lies in using sparse projections to transform the high-dimensional feature space into a lower-dimensional space where top-k similarity operations efficiently identify the most relevant features for classification. This approach reduces computational complexity while preserving information integrity through moderate sparsity levels.

## Key Results
- Achieves 38-93% reduction in post-extraction training time across multiple datasets
- Improves overall accuracy by 1.17-2.38% compared to baseline methods
- Eliminates the need for prompt-based architectures or model storage overhead
- Maintains competitive performance with state-of-the-art continual learning approaches

## Why This Works (Mechanism)
Fly-CL's effectiveness stems from its fly-inspired decorrelation mechanism that addresses the fundamental challenge of catastrophic forgetting in continual learning. The framework uses sparse random projections to transform parameter updates into an efficient similarity-matching problem, where the top-k operations progressively decorrelate class prototypes. This decorrelation eliminates multicollinearity that typically degrades similarity-based classification, while the adaptive streaming ridge classification enables efficient parameter learning. The moderate sparsity preserves information integrity, allowing the system to maintain discriminative power across multiple tasks.

## Foundational Learning
- Sparse random projections: Why needed - to reduce dimensionality while preserving similarity structure; Quick check - verify that projection sparsity maintains classification accuracy
- Top-k similarity operations: Why needed - to efficiently identify relevant features for classification; Quick check - confirm that top-k selection preserves most discriminative information
- Adaptive streaming ridge classification: Why needed - to enable efficient online parameter updates; Quick check - validate convergence speed and stability
- Prototype decorrelation: Why needed - to prevent multicollinearity in similarity-based classification; Quick check - measure correlation reduction between class prototypes
- Similarity-based classification: Why needed - to leverage pre-trained model representations effectively; Quick check - compare performance against alternative classification approaches

## Architecture Onboarding

Component map: Feature extractor -> Sparse random projection -> Top-k operation -> Prototype update -> Classification

Critical path: The core processing pipeline involves feature extraction from pre-trained models, transformation through sparse random projections, top-k feature selection, prototype updates via adaptive streaming ridge classification, and final similarity-based classification.

Design tradeoffs: Fly-CL prioritizes computational efficiency over model complexity by eliminating the need for storing entire models or using prompt-based architectures. The framework trades some representational flexibility for significant reductions in training time and memory overhead. The choice of sparsity level in random projections represents a key hyperparameter balancing information preservation against computational efficiency.

Failure signatures: Performance degradation may occur when task sequences have high similarity or when sparsity levels are too aggressive, leading to information loss. The framework may struggle with highly interleaved task sequences where clear prototype boundaries are difficult to establish. Excessive top-k values can reduce the efficiency gains while too small values may compromise classification accuracy.

First experiments:
1. Benchmark Fly-CL against standard continual learning methods on Split-CIFAR-10 with varying task orders
2. Evaluate training time reduction on Split-ImageNet across different sparsity configurations
3. Test prototype decorrelation effectiveness by measuring class prototype correlation matrices before and after Fly-CL processing

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance generalizability across diverse real-world datasets and task distributions remains uncertain
- Effectiveness in handling highly interleaved or shuffled task sequences not explicitly validated
- Limited direct comparisons against more recent continual learning approaches beyond baseline methods
- Optimal sparsity level selection and sensitivity to different problem domains requires further investigation

## Confidence

High confidence: The core mechanism of using fly-inspired decorrelation through sparse random projections and top-k operations is technically sound and well-grounded in the theoretical framework. The performance improvements over baseline methods are clearly demonstrated.

Medium confidence: The generalizability of the framework to more complex, real-world continual learning scenarios with diverse task distributions and highly interleaved sequences. The claimed training time reductions may vary significantly based on implementation details and hardware.

Low confidence: The long-term stability of the decorrelation mechanism when applied to extremely long task sequences or in cases with high task similarity. The framework's performance when integrated with more complex backbone architectures beyond those tested.

## Next Checks

1. Evaluate Fly-CL's performance on more challenging continual learning benchmarks with highly interleaved or shuffled task sequences to assess robustness in realistic scenarios.

2. Conduct ablation studies to determine the optimal sparsity level for random projections across different dataset characteristics and task complexities.

3. Perform cross-platform implementation testing to verify the consistency of reported training time reductions across various hardware configurations and deep learning frameworks.