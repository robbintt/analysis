---
ver: rpa2
title: Annotating Training Data for Conditional Semantic Textual Similarity Measurement
  using Large Language Models
arxiv_id: '2509.14399'
source_url: https://arxiv.org/abs/2509.14399
tags:
- condition
- similarity
- sentence
- conditions
- c-sts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses issues in the Conditional Semantic Textual
  Similarity (C-STS) dataset, including ambiguous conditions and inaccurate similarity
  ratings. The authors use Large Language Models (LLMs) to refine condition statements
  and re-annotate similarity scores.
---

# Annotating Training Data for Conditional Semantic Textual Similarity Measurement using Large Language Models

## Quick Facts
- arXiv ID: 2509.14399
- Source URL: https://arxiv.org/abs/2509.14399
- Authors: Gaifan Zhang; Yi Zhou; Danushka Bollegala
- Reference count: 29
- Primary result: 5.4% statistically significant improvement in Spearman correlation using LLM-cleaned C-STS dataset

## Executive Summary
This paper addresses fundamental quality issues in the Conditional Semantic Textual Similarity (C-STS) dataset, including ambiguous conditions and inconsistent similarity ratings. The authors employ Large Language Models (GPT-4o and Claude-3.7-Sonnet) to standardize condition statements and re-annotate similarity scores, creating a cleaned training corpus. The resulting SNPro model trained on this data achieves state-of-the-art performance with a 5.4% improvement in Spearman correlation over previous methods, demonstrating that data quality improvements can significantly impact model performance in conditional semantic similarity tasks.

## Method Summary
The authors systematically improve the C-STS dataset through two parallel LLM-based pipelines: condition modification and similarity rating re-annotation. GPT-4o standardizes condition statements by removing ambiguity and enforcing grammatical consistency, while GPT-4o and Claude-3.7-Sonnet independently predict similarity ratings for each sentence pair-condition triplet. These ratings are aggregated with human annotations through averaging and rounding to maintain the 1-5 ordinal scale. The cleaned dataset is then used to train the SNPro model, which employs a bi-encoder architecture with non-linear projection layers to capture conditional semantic similarity.

## Key Results
- 5.4% statistically significant improvement in Spearman correlation (73.93 vs 68.54) using the cleaned dataset
- Krippendorff's Alpha of 0.865 across 5 repeated LLM annotations indicates high annotation consistency
- Cohen's Kappa of 0.247 between original and re-annotated ratings shows meaningful divergence from baseline
- Performance gains scale with annotator diversity: ensemble methods outperform single LLM approaches

## Why This Works (Mechanism)

### Mechanism 1: Condition Standardization Reduces Semantic Ambiguity
Replacing vague or grammatically inconsistent condition statements with clear, specific formulations improves downstream model training by reducing annotation subjectivity. GPT-4o normalizes condition phrasing, removes stopwords, and enforces uniform granularity, creating consistent comparison criteria across the dataset and reducing noise in the similarity signal that models must learn.

### Mechanism 2: Multi-Source Rating Aggregation Reduces Label Noise
Averaging ratings from two independent LLMs with original human annotations produces more reliable ground truth than any single source alone. Each annotator makes independent errors, and averaging cancels uncorrelated noise while preserving the consensus signal. The arithmetic mean is rounded to the nearest integer to maintain the 1-5 ordinal scale.

### Mechanism 3: Condition-Specific Prompt Engineering Handles Edge Cases
Explicit scoring rules for high-frequency problematic condition types improve annotation consistency where general rubrics fail. The authors identified "number of #" conditions as both frequent (16.7% of dataset) and highly subjective, adding specific instructions: same number = 5, different = 1, approximate otherwise, removing annotator discretion for objective countable attributes.

## Foundational Learning

- **Concept: Conditional Semantic Textual Similarity (C-STS)**
  - Why needed here: Traditional STS produces a single similarity score, but sentence pairs can be similar along one dimension (e.g., room type) and dissimilar along another (e.g., number of people). C-STS explicitly conditions the comparison.
  - Quick check question: Given "A man plays guitar" and "A woman plays piano," would similarity be high or low under condition "type of instrument" vs. "action being performed"?

- **Concept: LLM-as-Annotator with Few-Shot Prompting**
  - Why needed here: The paper uses LLMs not as the final model but as data annotators. Few-shot examples with justifications teach the LLM the scoring rubric while encouraging self-consistency checks.
  - Quick check question: Why include justifications in the LLM output if only the rating score is used for training?

- **Concept: Bi-Encoder Architecture with Non-Linear Projection**
  - Why needed here: The SNPro model independently encodes each sentence-condition pair, then applies a learned non-linear transformation before computing similarity. This allows the model to learn task-specific embedding adjustments.
  - Quick check question: What is the computational advantage of bi-encoders over cross-encoders for large-scale similarity search?

## Architecture Onboarding

- **Component map:** Original C-STS Dataset (11,342 training instances) -> Condition Modification Module (GPT-4o + prompt) -> Modified Conditions Dataset -> Re-annotation Module (GPT-4o || Claude-3.7-Sonnet) -> Rating Aggregation (mean + round) -> Cleaned Dataset (Train-Mod-Reanno) -> SNPro Training (NV-Embed-v2 backbone + 2-layer FFN) -> Evaluation on ReTest-Mod (Spearman correlation)

- **Critical path:**
  1. Condition modification must preserve semantic intent while removing ambiguity (validated via 300-instance manual check)
  2. LLM re-annotation must achieve high consistency (validated via Krippendorff's Alpha â‰¥ 0.86)
  3. Aggregated ratings must show meaningful divergence from original (Cohen's Kappa = 0.247 indicates significant changes)
  4. SNPro training on cleaned data must outperform baseline on held-out test set

- **Design tradeoffs:**
  - Cost vs. coverage: Using only 2 LLMs balances annotation quality against API costs; full ensemble would be prohibitively expensive for 14,176 instances
  - Automation vs. verification: Only 300 instances manually verified (2.1% of dataset); remaining quality assumed from LLM consistency metrics
  - Condition distribution preserved: Authors note imbalanced condition types remain unchanged to maintain comparability with prior work

- **Failure signatures:**
  - High Cohen's Kappa between original and re-annotated ratings (>0.7) would indicate insufficient cleaning
  - Low Krippendorff's Alpha across repeated LLM annotations (<0.7) would indicate unreliable LLM annotators
  - Performance degradation on condition types with heavy modification would suggest over-correction

- **First 3 experiments:**
  1. Ablation on condition modification: Train SNPro on Train-Orig with original conditions, Train-Mod with modified conditions but original ratings, and Train-Mod-Reanno with both modifications. Compare Spearman to isolate which component drives improvement.
  2. Single vs. ensemble LLM annotation: Train separate models using only GPT-4o ratings, only Claude ratings, human+GPT-4o, human+Claude, and full ensemble. Test whether improvement scales with annotator diversity.
  3. Cross-condition generalization: Evaluate model performance separately on frequently-modified condition types (e.g., "number of #") vs. rarely-modified types. If gains concentrate on modified conditions, this validates the condition-cleaning hypothesis; if uniform, suggests broader noise reduction.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach assumes LLM-generated condition modifications and ratings are superior to human annotations without independent verification of semantic preservation
- Manual verification was limited to only 300 instances (2.1% of dataset), potentially missing systematic errors
- The method may not generalize to domains where condition ambiguity is inherent rather than annotator error

## Confidence

- **High Confidence:** Condition modification pipeline and aggregation methodology are sound (documented process with clear error categories)
- **Medium Confidence:** The 5.4% improvement is statistically significant but may not generalize beyond the tested architecture
- **Medium Confidence:** LLM consistency metrics (Krippendorff's Alpha = 0.865) suggest reliability, but manual verification was minimal

## Next Checks
1. **Semantic Preservation Audit:** Independently verify that LLM-modified conditions maintain original semantic intent across 100 randomly sampled instances, focusing on edge cases where conditions were heavily modified
2. **Cross-Architecture Generalization:** Train a different C-STS architecture (e.g., cross-encoder) on the cleaned dataset to test whether improvements transfer beyond the SNPro model
3. **Domain Transfer Test:** Apply the same LLM annotation pipeline to a different STS dataset (e.g., English STS Benchmark) to assess generalizability of the methodology beyond C-STS