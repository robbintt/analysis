---
ver: rpa2
title: 'Playing games with Large language models: Randomness and strategy'
arxiv_id: '2503.02582'
source_url: https://arxiv.org/abs/2503.02582
tags:
- games
- game
- llms
- rock
- repeated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large language models (LLMs) can
  play games requiring randomness and strategic adaptation. Using GPT-4o-Mini-2024-08-17,
  the research tested Rock Paper Scissors (RPS) and Prisoner's Dilemma (PD) through
  one-shot and repeated game formats.
---

# Playing games with Large language models: Randomness and strategy

## Quick Facts
- arXiv ID: 2503.02582
- Source URL: https://arxiv.org/abs/2503.02582
- Authors: Alicia Vidler; Toby Walsh
- Reference count: 10
- Primary result: LLMs struggle with uniform distributions in RPS and exhibit loss-aversion strategies in repeated games

## Executive Summary
This study investigates whether large language models can play games requiring randomness and strategic adaptation. Using GPT-4o-Mini-2024-08-17, the research tests Rock Paper Scissors (RPS) and Prisoner's Dilemma (PD) through one-shot and repeated game formats. Results show LLMs consistently fail to achieve uniform move distributions in RPS despite various prompt designs, and develop loss-aversion strategies in repeated games that converge to stalemate conditions. For PD, LLMs shift from cooperative to competitive play based on explicit equilibrium framing in prompts. The findings demonstrate fundamental limitations in LLM probabilistic decision-making and strategic reasoning, with implications for multi-agent systems requiring sustained strategic interaction.

## Method Summary
The study employs a two-module framework using GPT-4o-Mini-2024-08-17 via LangChain with Temperature=1. The Player module contains game-specific prompts for RPS and PD, while the Evaluation module manages payoffs and game state. Three anti-caching strategies are implemented: regular API key refresh, unique identifiers for one-shot prompts, and complete game history for repeated games. Experiments run 100 simulations per condition with 1000-turn extended games for RPS. Statistical significance is tested at p < 0.05, with chi-square tests used for uniformity assessment.

## Key Results
- RPS: LLMs fail to achieve uniform distributions (Rock dominates at 77-90% in one-shot, improving but still biased in repeated play)
- Repeated games: LLMs develop loss-aversion strategies, converging to stalemate conditions with increasing win milestone intervals
- PD: Basic prompts produce 93%+ cooperation, while Nash equilibrium framing shifts to 99%+ defection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs produce systematically biased outputs when instructed to generate random selections, failing to achieve uniform distributions required for optimal mixed-strategy play.
- Mechanism: Token prediction favors high-frequency patterns from training data (Rock ≈77% in one-shot RPS). The "stochastic parrot" characterization holds for language generation but not for controllable randomization—model weights encode distributional biases that override explicit randomness instructions.
- Core assumption: The bias stems from training data frequencies rather than architectural limitations that could be remedied through temperature tuning alone.
- Evidence anchors: [abstract] "LLMs are often described as stochastic parrots... their outputs - when prompted to be random - are often very biased"; [section 4.1] "regardless of prompts, none of the tests can come close to a uniform distribution... All observed proportions lie outside the 25.4% - 41.3% range"

### Mechanism 2
- Claim: In repeated games, LLMs develop loss-aversion strategies that converge to stalemate conditions rather than optimizing expected payoff.
- Mechanism: Given complete game history, agents appear to minimize opponent wins rather than maximize own score—consistent with minimax reasoning. In 1000-turn RPS, win intervals increase (e.g., Player 1 needs 276 games to advance from 60→80 wins vs. 123 for first 20 wins).
- Core assumption: This reflects emergent strategic reasoning rather than simply mirroring patterns from training data containing loss-averse human gameplay descriptions.
- Evidence anchors: [abstract] "LLMs appear to develop loss aversion strategies in repeated games, with RPS converging to stalemate conditions"; [section 4.3] "Rather than indicating a failure to learn, this behaviour may suggests deliberate loss-aversion strategies... pursue a minimax approach"

### Mechanism 3
- Claim: Prompt design can override default cooperative behavior and shift LLMs toward game-theoretic equilibrium play.
- Mechanism: Explicit equilibrium framing ("The Nash equilibrium for the game is mutual defection") changes the decision context from social cooperation to formal optimization. PD-2 prompts produced 99% (D,D) outcomes in repeated games vs. 0% under PD-1.
- Core assumption: The shift represents reasoning about game structure rather than simple keyword matching.
- Evidence anchors: [abstract] "PD shows systematic shifts between cooperative and competitive outcomes based on prompt design"; [section 5.1] "PD-2, a prompt suggesting the strict game dominance strategy... resulted in 100% dominant game play"

## Foundational Learning

- Concept: **Mixed Strategy Nash Equilibrium**
  - Why needed here: RPS optimal play requires randomizing uniformly (1/3 each action) to remain unpredictable—this is the benchmark against which LLM performance is measured.
  - Quick check question: In a zero-sum game with no pure strategy equilibrium, what probability distribution should a rational player use?

- Concept: **Dominant Strategy vs. Pareto Optimality**
  - Why needed here: PD creates tension between individual rationality (defect is dominant) and collective welfare (cooperate is Pareto optimal)—LLMs default to cooperation but shift under explicit equilibrium framing.
  - Quick check question: Why does individual rationality lead to suboptimal collective outcomes in the Prisoner's Dilemma?

- Concept: **API Caching and Sampling Independence**
  - Why needed here: OpenAI's API caches identical prompts, compromising statistical validity of repeated sampling; requires key rotation and unique identifiers.
  - Quick check question: What three techniques did the authors use to prevent cache retrieval from confounding game results?

## Architecture Onboarding

- Component map: Player Module -> Evaluation Module -> API Interface -> LangChain Agent
- Critical path:
  1. Define prompt with game structure + payoff + output format
  2. Generate unique identifier (prevents caching)
  3. For repeated games: append history JSON to prompt
  4. Call API with fresh key if available
  5. Parse single-letter response, evaluate against payoff matrix
  6. Store result in history for next iteration
- Design tradeoffs:
  - Temperature=1 maintains variability but doesn't fix distributional bias
  - Complete history provides context but increases token costs and may bias toward patterns
  - Single-letter output format reduces parsing errors but limits explainability
- Failure signatures:
  - Non-uniform distributions in RPS (Rock >50%) indicate unfixable randomness limitation
  - Tie rates >>33% in repeated RPS indicate stalemate convergence
  - Unexpected cooperation in PD under equilibrium prompts indicates prompt sensitivity
  - Identical responses across "independent" runs indicate cache contamination
- First 3 experiments:
  1. **Baseline randomness test**: Run 100 one-shot RPS games with P1 Base prompt; verify Rock dominance (~79%) and high tie rate (~68%). Confirms distributional bias exists.
  2. **Prompt sensitivity test**: Run PD with PD-1 vs. PD-2 prompts; verify shift from 93%+ cooperation to 99%+ defection. Confirms equilibrium framing effect.
  3. **Stalemate detection test**: Run 1000-turn RPS with P4 prompt; track win milestone intervals. Verify increasing intervals (loss aversion) and final scores far below theoretical ~333.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs learn optimal mixed strategies without true randomness, and can they develop meta-strategies that compensate for their randomness limitations?
- Basis in paper: [explicit] Authors state: "This raises two important questions - given the concerns around LLM ability to replicate random variables, can LLMs learn optimal mixed strategies without true randomness? Can they develop meta-strategies that compensate for their randomness limitations?"
- Why unresolved: The study found LLMs consistently fail to achieve uniform distribution in RPS regardless of prompt design, suggesting fundamental limitations in probabilistic decision-making, but no meta-strategies were identified that could compensate.
- What evidence would resolve it: Demonstrating that LLMs can achieve statistically uniform play distributions through alternative prompting strategies, architectural modifications, or post-processing techniques.

### Open Question 2
- Question: Is the observed loss-aversion behavior inherent to LLM design (potentially from training on human text) or can it be modified through inter-model game play?
- Basis in paper: [explicit] "Future research will explore whether this tendency toward defensive play is inherent to LLM design (possibly from human text parroting [Bender et al., 2021] or can be modified through inter-model game play."
- Why unresolved: The study observed stalemate emergence and loss-aversion strategies in extended RPS games, but the underlying cause—whether architectural, training-related, or modifiable—remains undetermined.
- What evidence would resolve it: Experiments comparing loss-aversion across different LLM architectures, training corpora, and extended inter-model gameplay scenarios showing whether behavioral modification is possible.

### Open Question 3
- Question: How does OpenAI's API caching mechanism affect prior research conclusions about LLM randomness capabilities?
- Basis in paper: [inferred] Authors note: "The impact of caching on prior research into OpenAI's random number generation capabilities remains unclear and warrants further investigation" after implementing cache-avoidance strategies.
- Why unresolved: The caching mechanism cannot be disabled programmatically, and its influence on previously published results about LLM randomness is unknown, raising questions about reproducibility.
- What evidence would resolve it: Replication of prior LLM randomness studies with and without cache-avoidance controls to quantify the caching effect on distributional outcomes.

### Open Question 4
- Question: Can fine-tuning approaches improve LLM randomization and strategic reasoning capabilities while preserving core strengths?
- Basis in paper: [explicit] "In our future research we plan to explore fine tuning approaches to improve both randomisation and strategic reasoning capabilities while preserving LLMs' core strengths."
- Why unresolved: Current prompting-based approaches failed to achieve uniform distributions or consistent strategic behavior, leaving architectural intervention as an unexplored path.
- What evidence would resolve it: Fine-tuned models demonstrating statistically uniform RPS play and appropriate strategic shifts in PD without degradation in general language capabilities.

## Limitations

- Findings may be specific to GPT-4o-Mini-2024-08-17 rather than generalizable across model families
- Mechanisms driving loss-aversion strategies are inferred rather than directly measured
- Prompt engineering effects could represent keyword sensitivity rather than deeper understanding of game-theoretic concepts

## Confidence

- **High confidence**: RPS distributional bias findings (well-validated through multiple prompt variations and statistical testing)
- **Medium confidence**: Loss-aversion mechanism in repeated games (supported by data but alternative explanations possible)
- **Medium confidence**: Prompt-driven equilibrium shifts in PD (reproducible effect but mechanism unclear)

## Next Checks

1. Test multiple LLM architectures (Claude, LLaMA, Gemini) on identical RPS and PD protocols to determine if randomness limitations and loss-aversion patterns generalize beyond GPT-4o-Mini
2. Conduct ablation studies on prompt components in PD to isolate which elements (Nash equilibrium mention, dominance framing, payoff reminders) drive the strategic shifts
3. Implement alternative randomness mechanisms (cryptographic random number generation, external random sources) to test whether distributional bias stems from token prediction architecture or model training