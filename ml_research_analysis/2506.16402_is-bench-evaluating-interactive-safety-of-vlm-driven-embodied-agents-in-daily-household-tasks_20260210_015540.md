---
ver: rpa2
title: 'IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily
  Household Tasks'
arxiv_id: '2506.16402'
source_url: https://arxiv.org/abs/2506.16402
tags:
- safety
- task
- risks
- evaluation
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces IS-Bench, the first multi-modal benchmark\
  \ for evaluating interactive safety of VLM-driven embodied agents in daily household\
  \ tasks. The key contribution is a process-oriented evaluation framework that assesses\
  \ agents\u2019 ability to perceive emergent risks and execute mitigation steps in\
  \ correct procedural order, contrasting with prior static, termination-oriented\
  \ approaches."
---

# IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks

## Quick Facts
- arXiv ID: 2506.16402
- Source URL: https://arxiv.org/abs/2506.16402
- Reference count: 40
- Introduces first multi-modal benchmark for interactive safety evaluation of VLM-driven embodied agents in household tasks

## Executive Summary
IS-Bench is a pioneering multi-modal benchmark that evaluates the interactive safety of VLM-driven embodied agents performing daily household tasks. Unlike prior benchmarks that focus on static safety assessments or task completion alone, IS-Bench introduces a process-oriented evaluation framework that assesses agents' ability to perceive emergent risks and execute appropriate mitigation steps in the correct procedural order. The benchmark features 161 challenging scenarios with 388 unique safety risks across 10 domestic categories, instantiated in the OmniGibson simulator.

The evaluation reveals that current VLM-driven agents lack sufficient interactive safety awareness, with success rates below 40% in safety-critical scenarios. Safety-aware Chain-of-Thought prompting improves safety performance by 9.3% on average but reduces task completion by 9.4%, highlighting the trade-offs in safety-oriented design. The research identifies proactive risk perception as the primary bottleneck rather than the ability to follow safety constraints once risks are identified.

## Method Summary
IS-Bench introduces a process-oriented evaluation framework that assesses embodied agents through three phases: risk perception (detecting potential hazards), risk reasoning (understanding the consequences and appropriate responses), and safety execution (performing correct mitigation actions in proper sequence). The benchmark covers 161 scenarios across 10 domestic categories including kitchen, bathroom, living room, and bedroom tasks, with 388 unique safety risks such as slippery floors, electrical hazards, and child safety concerns.

The evaluation methodology uses a comprehensive scoring system that rewards agents for correctly identifying risks, reasoning about appropriate responses, and executing mitigation steps in the proper order. Safety-aware Chain-of-Thought prompting is employed as an intervention strategy, where agents are explicitly prompted to consider safety implications before taking actions. Experiments are conducted in the OmniGibson simulator, which provides realistic physics and visual environments for household scenarios.

## Key Results
- Current VLM-driven agents achieve below 40% success rate in safety-critical household scenarios
- Safety-aware Chain-of-Thought prompting improves safety performance by 9.3% on average
- Safety-aware prompting reduces task completion rates by 9.4%, revealing performance trade-offs
- Proactive risk perception identified as the core bottleneck rather than constraint-following capability

## Why This Works (Mechanism)
The benchmark's process-oriented approach works by evaluating agents at each stage of the safety decision-making pipeline rather than only at task completion. This granular assessment reveals where agents fail in the safety reasoning process, allowing for targeted improvements. The multi-modal nature of the evaluation, incorporating visual, textual, and physical interaction data, provides a comprehensive assessment of agent safety awareness.

## Foundational Learning
- **Process-oriented evaluation** (why needed: static benchmarks miss dynamic risk emergence; quick check: can the agent identify and mitigate risks that develop during task execution)
- **Multi-modal safety assessment** (why needed: household risks involve visual, physical, and contextual elements; quick check: does the agent respond appropriately to combined visual and physical safety cues)
- **Chain-of-Thought prompting for safety** (why needed: agents need explicit safety reasoning frameworks; quick check: does CoT prompting improve risk identification rates without catastrophic task failure)
- **Procedural safety execution** (why needed: mitigation steps must be performed in correct order; quick check: can the agent sequence safety actions appropriately)
- **Simulator-based safety validation** (why needed: real-world testing is expensive and potentially dangerous; quick check: does simulator performance correlate with real-world safety outcomes)
- **Risk perception vs. constraint-following distinction** (why needed: different failure modes require different interventions; quick check: which component shows greater improvement with targeted training)

## Architecture Onboarding

Component map: Perception module -> Risk reasoning module -> Safety execution module -> Task completion module

Critical path: Visual perception → Risk identification → Safety reasoning → Action selection → Physical execution

Design tradeoffs: The benchmark prioritizes safety evaluation over task efficiency, accepting lower completion rates (9.4% reduction) in exchange for improved safety awareness. This reflects real-world requirements where safety must take precedence over performance metrics.

Failure signatures: Common failure modes include missing emergent risks during task progression, incorrect risk prioritization, executing safety steps out of order, and failing to balance safety constraints with task completion requirements.

First experiments:
1. Baseline evaluation of 16 VLM-driven agents without safety prompting to establish performance floor
2. Safety-aware CoT prompting intervention across all agent types to measure improvement potential
3. A/B testing of different safety prompt templates to optimize the balance between safety improvement and task completion preservation

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of findings to real-world physical environments, the scalability of the process-oriented evaluation to longer and more complex household tasks, and the potential for integrating safety awareness directly into VLM architectures rather than relying on prompting interventions.

## Limitations
- Evaluation scenarios are simulated in OmniGibson, raising questions about ecological validity and real-world generalizability
- Focus on short-horizon tasks (mean 8 steps) may not capture complex, multi-stage household interactions
- Safety-aware CoT prompting introduces trade-offs with task completion rates that may not generalize across different VLM architectures

## Confidence

High confidence in:
- The novel process-oriented evaluation methodology and its contrast with prior static approaches
- Empirical results showing low baseline success rates (<40%) and measurable safety improvements from CoT prompting (9.3%)

Medium confidence in:
- The claim about proactive risk perception being the "core bottleneck" based on current agent performance rather than systematic ablation studies

## Next Checks
1. Test the benchmark with real-world physical robots or through domain randomization in simulation to assess ecological validity
2. Conduct systematic ablation studies to disentangle whether perception or constraint-following is the true bottleneck
3. Evaluate longer-horizon, multi-stage tasks to determine if safety awareness degrades over extended interactions