---
ver: rpa2
title: 'FIQ: Fundamental Question Generation with the Integration of Question Embeddings
  for Video Question Answering'
arxiv_id: '2507.12816'
source_url: https://arxiv.org/abs/2507.12816
tags:
- video
- question
- information
- answer
- pairs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a limitation in video question answering (VQA)
  where existing methods focus on event-centric annotations, leading to fragmented
  scene understanding and limiting generalization and higher-level reasoning. To overcome
  this, the authors propose FIQ (Fundamental Question Generation with Integration
  of Question Embeddings), a novel approach that generates fundamental Q&A pairs from
  video descriptions to enrich the training data with essential scene information
  such as object types, shapes, and spatial layouts.
---

# FIQ: Fundamental Question Generation with the Integration of Question Embeddings for Video Question Answering

## Quick Facts
- arXiv ID: 2507.12816
- Source URL: https://arxiv.org/abs/2507.12816
- Reference count: 34
- Key outcome: FIQ achieves state-of-the-art performance on SUTD-TrafficQA by enriching event-centric annotations with fundamental Q&A pairs and integrating question embeddings with visual features, significantly improving reasoning tasks.

## Executive Summary
FIQ addresses a key limitation in video question answering (VQA) where existing methods rely on event-centric annotations, leading to fragmented scene understanding and limiting generalization and higher-level reasoning. The proposed method generates fundamental Q&A pairs from video descriptions to enrich training data with essential scene information such as object types, shapes, and spatial layouts. Additionally, FIQ incorporates a VQ-CAlign module to integrate task-specific question embeddings with visual features, preserving domain-specific details and enhancing downstream task adaptability. Evaluated on SUTD-TrafficQA, FIQ achieves state-of-the-art performance, demonstrating significant improvements in reasoning-related tasks like event forecasting, reverse reasoning, introspection, and attribution.

## Method Summary
FIQ enriches video question answering by generating fundamental Q&A pairs from video descriptions and integrating question embeddings with visual features. The method extracts video descriptions using VideoChat2, then generates Q&A pairs via a three-step pipeline: candidate answer extraction, question generation, and answer validation (F1 ≥ 0.54). These pairs cover fundamental scene attributes (objects, shapes, layouts) alongside event information. For the model architecture, FIQ uses frozen CLIP encoders for visual and text features, with a VQ-CAlign module that fuses visual features and question embeddings through self-attention and cross-attention. A Trans-Decoder refines answer candidate embeddings using visual context, and the Ans-Decoder produces final predictions. The model is trained with a combined Hinge and MSE loss, achieving state-of-the-art performance on the SUTD-TrafficQA dataset.

## Key Results
- Achieves state-of-the-art performance on SUTD-TrafficQA, improving average accuracy across six reasoning tasks.
- Significant gains in event forecasting (5.8% absolute), reverse reasoning (4.3% absolute), and introspection (4.0% absolute) tasks.
- Successfully addresses the limitation of fragmented scene understanding by enriching event-centric annotations with fundamental Q&A pairs covering object types, shapes, and spatial layouts.

## Why This Works (Mechanism)

### Mechanism 1: Fundamental Scene Enrichment via Synthetic Q&A
- Claim: Supplementing event-centric annotations with fundamental Q&A pairs (object types, shapes, layouts) reduces scene fragmentation and improves higher-level reasoning.
- Mechanism: A Language Model (LM) generates Q&A pairs from video descriptions (extracted via VideoChat2). These pairs force the model to attend to static and descriptive features across frames, rather than focusing exclusively on high-motion event frames (e.g., collisions).
- Core assumption: The video descriptions extracted by VideoChat2 accurately reflect spatio-temporal reality, and the LM-generated questions are relevant to the downstream reasoning tasks.
- Evidence anchors:
  - [abstract]: "FIQ generates Q&A pairs based on descriptions extracted from videos, enriching the training data with fundamental scene information."
  - [section III-B]: "We employ LMs to generate Q&A pairs... covering both low-level features such as color and objects and high-level features such as motions."
  - [corpus]: Weak direct validation for this specific generation pipeline, though neighbors like *CausalVQA* emphasize the need for physical grounding which fundamental attributes provide.
- Break condition: If generated descriptions contain hallucinations or if fundamental attributes (color, shape) are irrelevant to the specific causal query (e.g., "Why did the car stop?"), the signal-to-noise ratio degrades.

### Mechanism 2: Query-Guided Visual Feature Alignment (VQ-CAlign)
- Claim: Integrating task-specific question embeddings with visual features via a cross-attention mechanism preserves domain-specific details necessary for downstream adaptation.
- Mechanism: The VQ-CAlign module uses self-attention on visual features ($x_{vpe}$) followed by cross-attention where visual features act as the query and question embeddings ($x_q$) act as key/value. This filters visual features through the lens of the specific question, aiming to recover task-relevant details that general pre-training might miss.
- Core assumption: The question embedding contains sufficient semantic information to guide the extraction of relevant visual features before they are fused with answer candidates.
- Evidence anchors:
  - [abstract]: "VQ-CAlign module... integrates task-specific question embeddings with visual features, ensuring that essential domain-specific details are preserved."
  - [section III-C.2]: "The cross-attention connects $x_{vis}$ and $x_q$, allowing the model to focus on visual positions that are relevant to question embeddings."
  - [corpus]: Neighbors like *Commonsense Video Question Answering* utilize entailment trees, suggesting simple embedding alignment may be insufficient for complex logic without external knowledge.
- Break condition: If the visual encoder (CLIP) has already discarded the low-level details required by the question during pre-training, the cross-attention cannot recover them (information bottleneck).

### Mechanism 3: Dual-Stream Textual Refinement
- Claim: Refining answer candidate embeddings using visual context (Trans-Decoder) prevents the model from relying solely on language priors.
- Mechanism: A Trans-Decoder processes answer candidate embeddings ($x_c$) alongside visual features ($x_{vis}$) to produce refined text embeddings ($x_{ctd}$). These are added to the VQ-CAlign output ($x_{fused}$), ensuring the final representation mixes query-guided visual features with visually-grounded text features.
- Core assumption: Simple cosine similarity or standard decoding benefits from this intermediate refinement step more than end-to-end fine-tuning.
- Evidence anchors:
  - [section III-C.1]: "We employ... a Trans-Decoder to focus on extracting meaningful information from the textual input."
  - [section III-C.2]: "$x_{fused}$ is added again with $x_{ctd}$ to fully reflect the textual information for the VQA task."
  - [corpus]: Not explicitly validated in neighbor papers; standard approaches often use projector layers or simpler adapters (e.g., CLIP-Adapter mentioned in related works).
- Break condition: If the visual features are noisy, the Trans-Decoder might degrade the quality of the text embeddings, leading to misalignment.

## Foundational Learning

- Concept: **Event-centric vs. Fundamental Understanding**
  - Why needed here: The paper argues that standard VQA datasets focus on "what happened" (events) rather than "what is present" (fundamentals), leading to models that cannot reason about causality because they lack context.
  - Quick check question: Can you identify the difference between a dataset annotation describing a "car crash" (event) vs. "a red sedan at an intersection" (fundamental)?

- Concept: **Frozen Pre-trained Encoders (CLIP)**
  - Why needed here: The architecture relies on CLIP for both visual and text encoding. Understanding that CLIP aligns images and text in a shared space but may lack temporal resolution is crucial.
  - Quick check question: Why would a static image-text model like CLIP struggle with temporal reasoning tasks like "event forecasting" without adaptation?

- Concept: **Cross-Attention in Multimodal Fusion**
  - Why needed here: The VQ-CAlign module uses cross-attention to fuse modalities. You must understand how Queries, Keys, and Values allow one modality (video) to be filtered by another (question).
  - Quick check question: In the VQ-CAlign cross-attention, which modality serves as the Query and which serves as the Key/Value?

## Architecture Onboarding

- Component map:
  - **Inputs**: Video (Visual Encoder) + Q&A Pairs (Text Encoder).
  - **Data Augmentation**: VideoChat2 (descriptions) -> T5/GPT (Fundamental Q&A Generation).
  - **Encoders**: Frozen CLIP ViT/B-16 (Visual) & Frozen CLIP Text Encoder.
  - **Fusion Core**:
    - **Trans-Decoder**: Refines answer embeddings using visual context.
    - **VQ-CAlign**: Fuses visual features with question embeddings (Self-Attn -> Cross-Attn -> FFN).
  - **Output**: Ans-Decoder (generates final prediction logits).

- Critical path:
  1. Generate descriptions and Fundamental Q&A offline.
  2. Encode Video + Q&A using Frozen CLIP.
  3. Pass Visual + Answer Embeddings through Trans-Decoder ($x_{ctd}$).
  4. Pass Visual + Question Embeddings through VQ-CAlign ($x_{fused}$).
  5. Sum $x_{ctd} + $x_{fused}$ -> Ans-Decoder -> Prediction.

- Design tradeoffs:
  - **Synthetic Data Quality**: Relying on VideoChat2/T5 introduces pipeline complexity and potential hallucination errors, traded off against the cost of human annotation.
  - **Frozen Backbone**: Using frozen CLIP reduces computational cost but limits the model's ability to adapt to novel visual concepts not in the pre-training distribution.
  - **Complexity**: The dual-path refinement (Trans-Decoder + VQ-CAlign) adds parameters compared to simple linear probing.

- Failure signatures:
  - **Fragmented Attention**: If the model fails to generalize, check if it is still overfitting to event-centric frames (the "collision" problem).
  - **Hallucination**: If the model answers fundamental questions incorrectly, verify the LM generation step (filter threshold 0.54 F1 score).
  - **Loss of Task Specificity**: If generalization improves but specific task accuracy drops, the VQ-CAlign may be failing to integrate question embeddings effectively.

- First 3 experiments:
  1. **Ablation on Data Source**: Train with only Original Data vs. Original + T5 vs. Original + GPT (Table II) to measure the delta from fundamental questions.
  2. **Module Ablation**: Run with Tem-Adapter (baseline) vs. VQ-CAlign only vs. Full FIQ to isolate architectural contributions.
  3. **Task-Specific Breakdown**: Evaluate specifically on "Event Forecasting" and "Reverse Reasoning" (Table I, tasks F & R) to validate the causal reasoning hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the answer candidate generation process be refined to better reflect specific question contexts rather than relying on random sampling from other video IDs?
- Basis in paper: [explicit] The authors state in the conclusion: "In future work, we plan to generate a new dataset that reflects the question information inside the dataset as answer candidates."
- Why unresolved: The current method constructs negative answer options by randomly selecting answers from distinct video IDs, which may result in unchallenging or contextually irrelevant distractors that do not sufficiently test the model's discriminative reasoning.
- What evidence would resolve it: A comparative study evaluating model performance when trained on datasets with semantically similar or context-aware negative candidates versus the current random sampling strategy.

### Open Question 2
- Question: Why does the enrichment of fundamental scene information fail to improve performance in counterfactual inference tasks?
- Basis in paper: [inferred] While FIQ improves performance in forecasting, reverse reasoning, introspection, and attribution, Table I shows that the accuracy for Counterfactual Inference (C) slightly decreases (54.5 to 54.0) compared to the baseline, a deviation the paper attributes to the task's need for hypothetical reasoning without further solution.
- Why unresolved: The paper establishes that fundamental attributes help factual reasoning but does not investigate whether these attributes introduce bias or noise that hinders the specific cognitive load required for hypothetical "what-if" scenarios.
- What evidence would resolve it: An ablation study analyzing the correlation between the volume of fundamental Q&A pairs and counterfactual task accuracy, or a qualitative error analysis of counterfactual predictions.

### Open Question 3
- Question: To what extent does the FIQ framework generalize to non-traffic video domains where object attributes are less rigid or defined than vehicles?
- Basis in paper: [inferred] The method is proposed as a general approach for "video question answering" to solve "fragmented scene representation," but the experimental validation is restricted exclusively to the SUTD-TrafficQA dataset, which focuses on specific traffic events and rigid objects.
- Why unresolved: Traffic objects (cars, roads) have distinct and consistent fundamental attributes (color, shape); it is unclear if the reliance on VideoChat2 for description extraction transfers effectively to domains with deformable objects (e.g., human interactions) or complex backgrounds.
- What evidence would resolve it: Benchmark results from FIQ applied to general-domain VQA datasets such as ActivityNet-QA or MSRVTT-QA.

## Limitations
- The paper does not specify exact LM prompts for candidate answer extraction, question generation, and validation, nor does it provide the loss balancing coefficient γ, learning rate, optimizer, or detailed architectural dimensions for key modules.
- The method's generalization to non-traffic domains remains untested, as all experiments are confined to the SUTD-TrafficQA dataset with rigid, well-defined objects.
- Performance on counterfactual inference tasks slightly decreases despite overall improvements, suggesting fundamental attributes may not support hypothetical reasoning scenarios.

## Confidence

- **High**: The claim that synthetic fundamental Q&A pairs improve generalization is well-supported by the ablation showing performance gains when using original + generated data versus original alone.
- **Medium**: The mechanism by which VQ-CAlign preserves domain-specific details is theoretically sound but lacks direct validation; neighbor papers suggest simpler embedding alignment may be insufficient for complex logic without external knowledge.
- **Low**: The assertion that the dual-stream refinement architecture (Trans-Decoder + VQ-CAlign) is necessary for optimal performance is weakly supported; standard approaches often use simpler adapters, and this complexity is not explicitly validated in related works.

## Next Checks

1. Replicate the ablation study on data sources (Original vs. Original + T5 vs. Original + GPT) to measure the contribution of fundamental Q&A pairs to performance gains.
2. Conduct a module-by-module ablation (Tem-Adapter vs. VQ-CAlign vs. Full FIQ) to isolate the architectural contributions and validate the necessity of the dual-path refinement.
3. Perform task-specific analysis on Event Forecasting and Reverse Reasoning to confirm that the improvements are driven by enhanced causal reasoning capabilities rather than general model strength.