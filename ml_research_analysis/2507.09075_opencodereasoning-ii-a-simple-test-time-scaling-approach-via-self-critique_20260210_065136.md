---
ver: rpa2
title: 'OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique'
arxiv_id: '2507.09075'
source_url: https://arxiv.org/abs/2507.09075
tags:
- arxiv
- code
- reasoning
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OpenCodeReasoning-II, a dataset of 2.5 million
  question-solution-critique triples that is nearly twice the size of the previous
  largest code reasoning dataset. The dataset covers 35,000 unique programming problems
  and includes both Python and C++ solutions with reasoning traces and binary judgments
  (right/wrong).
---

# OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique

## Quick Facts
- **arXiv ID:** 2507.09075
- **Source URL:** https://arxiv.org/abs/2507.09075
- **Reference count:** 22
- **Primary result:** 32B model improves pass@1 by ~6 percentage points using self-critique with 10 parallel samples

## Executive Summary
OpenCodeReasoning-II introduces a large-scale dataset of 2.5 million question-solution-critique triples for competitive code generation. The authors employ a two-stage supervised fine-tuning approach, first for code generation and then jointly for code generation and critique, using Qwen2.5-Instruct models of 7B, 14B, and 32B parameters. By generating multiple solutions in parallel and using self-critique to select the best, the models achieve substantial improvements in pass@1 performance, narrowing the gap between single-answer and multi-answer benchmarks. The work also extends LiveCodeBench to support C++ evaluation and demonstrates asymmetric cross-language transfer capabilities.

## Method Summary
The authors created OpenCodeReasoning-II by generating solutions with DeepSeek-R1 and critiques with QwQ-32B, producing 2.5 million triples covering 35,000 unique programming problems in Python and C++. They fine-tuned Qwen2.5-Instruct models using a two-stage approach: Stage I trained on code generation for 3 epochs, followed by Stage II that jointly trained on both code generation and critique data for 1 epoch. At inference, they generated 10 solutions per problem in parallel using vLLM with nucleus sampling, then applied self-critique to select the best solution based on binary "right/wrong" judgments and a heuristic choosing the shortest reasoning trace among correct solutions.

## Key Results
- 32B model achieves ~6 percentage point improvement in pass@1 through test-time scaling with self-critique
- Joint Python/C++ training yields strong performance in both languages, with asymmetric transfer (C++→Python stronger than Python→C++)
- Models trained on C++ achieve reasonable Python scores, but Python-only training severely degrades C++ performance
- Dataset covers 35,000 unique problems with 2.5 million question-solution-critique triples

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Supervised Fine-Tuning for Unified Generation-Critique
Joint training on code generation and critique enables models to self-evaluate solutions at inference time, improving selection accuracy under test-time scaling. Stage I fine-tunes on code generation alone; Stage II adds critique data (solution + reasoning trace + binary judgment) so the model learns to produce both. At inference, the model generates a solution, then critiques it—this critique acts as a verifier for selection among multiple candidates.

### Mechanism 2: Parallel Test-Time Scaling with Self-Critique Selection
Generating multiple solutions in parallel and selecting via self-critique narrows the gap between pass@1 and pass@k. Sample k solutions independently; prompt the model to critique each. Select from solutions labeled "right" using a heuristic (shortest critique reasoning trace). Coverage improves via k samples; selection improves via critique-based filtering.

### Mechanism 3: Cross-Language Transfer via Joint Python/C++ Training
Training on both Python and C++ yields strong performance in both languages, with asymmetric transfer (C++→Python stronger than Python→C++). Shared reasoning traces and problem structures across languages enable transfer; C++ training may enforce more explicit type/memory reasoning that generalizes to Python.

## Foundational Learning

- **Test-Time Scaling (Parallel)**: Why needed: The method relies on generating multiple solutions at inference and selecting the best. Understanding coverage vs. selection trade-offs is essential. Quick check: Given k=10 samples and pass@10=75%, if critique accuracy is 50%, what is the approximate upper bound on pass@1|select@10?

- **Supervised Fine-Tuning with Reasoning Traces**: Why needed: The dataset includes CoT traces for both solutions and critiques. Models must learn to produce structured reasoning. Quick check: How does training on reasoning traces differ from training only on input-output pairs for code generation?

- **Binary vs. Scalar Verification**: Why needed: The paper uses binary "right/wrong" judgments for critiques. Understanding trade-offs vs. scalar rewards is relevant. Quick check: Why might binary judgments be preferred over 1-5 scales for critique training in this setting?

## Architecture Onboarding

- **Component map**: Data pipeline: Question collection → Solution generation (DeepSeek-R1) → Critique generation (QwQ-32B) → Execution verification → Training pipeline: Stage I (code gen SFT) → Stage II (joint gen+critique SFT) → Inference pipeline: Parallel sampling (vLLM) → Self-critique → Selection heuristic (shortest trace among "right")

- **Critical path**: Dataset quality (reasoning trace structure, correct binary labels) → Stage II joint training (enables self-critique) → Inference selection heuristic (directly affects pass@1|select@k)

- **Design tradeoffs**: Binary vs. nuanced critique judgments (chose binary due to model tendency toward extremes); Selection heuristic (shortest trace is simple but potentially suboptimal); Language coverage (joint training improves both but increases data requirements)

- **Failure signatures**: Critique accuracy drops sharply on medium/hard problems, limiting selection gains; Python-only training causes severe C++ degradation; At large k, pass@1|select@k plateaus while pass@k continues rising

- **First 3 experiments**: 1) Ablate selection heuristic: Compare shortest-trace vs. random vs. majority-voting selection on a held-out split; 2) Vary k (samples per problem): Measure where pass@1|select@k saturates for each model size; 3) Cross-language transfer diagnostic: Train on Python-only, C++-only, and joint; evaluate on both to quantify asymmetry

## Open Questions the Paper Calls Out
None

## Limitations
- Critique accuracy is insufficient for medium and hard problems (<47% and <14% respectively), limiting selection gains
- Asymmetric cross-language transfer is observed but not explained, with no corpus evidence addressing the directional difference
- The simple selection heuristic (shortest reasoning trace) is potentially suboptimal and more sophisticated alternatives remain unexplored

## Confidence
- **High confidence**: Dataset scale and construction methodology; Two-stage SFT approach and implementation details
- **Medium confidence**: Test-time scaling improvements, as these depend heavily on selection heuristic effectiveness
- **Low confidence**: Explanation for cross-language transfer asymmetry; Generalizability of the simple selection heuristic

## Next Checks
1. **Ablate the selection heuristic**: Compare shortest-trace selection against random selection and majority-voting on a held-out validation set to quantify the actual contribution of the critique-based selection
2. **Measure selection quality vs. k**: Plot pass@1|select@k against k for each model size to identify where gains saturate, distinguishing coverage limits from selection quality limits
3. **Cross-language transfer diagnostic**: Train separate models on Python-only, C++-only, and joint datasets, then evaluate all on both languages to quantify and better understand the asymmetric transfer patterns