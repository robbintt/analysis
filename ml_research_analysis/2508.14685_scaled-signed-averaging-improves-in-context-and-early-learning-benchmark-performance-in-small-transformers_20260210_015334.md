---
ver: rpa2
title: Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance
  in Small Transformers
arxiv_id: '2508.14685'
source_url: https://arxiv.org/abs/2508.14685
tags:
- softmax
- training
- task
- attention
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Scaled Signed Averaging (SSA) improves in-context and early learning
  benchmark performance in small transformers by replacing softmax with a parameterized
  scoring function that avoids saturation and promotes more balanced attention. SSA
  substantially enhances generalization in simple semantic tasks involving quantifiers
  and linear function prediction, where softmax-based models fail due to boundary
  effects and concentration on single tokens.
---

# Scaled Signed Averaging Improves In-Context and Early Learning Benchmark Performance in Small Transformers

## Quick Facts
- **arXiv ID:** 2508.14685
- **Source URL:** https://arxiv.org/abs/2508.14685
- **Reference count:** 40
- **Primary result:** SSA improves ICL and early learning benchmarks in small transformers by replacing softmax with polynomial scoring function

## Executive Summary
Scaled Signed Averaging (SSA) introduces a parameterized attention scoring function that replaces softmax to address limitations in in-context learning (ICL) and generalization. By using polynomial growth instead of exponential, SSA prevents attention saturation and maintains more balanced weights across multiple tokens. The approach demonstrates substantial improvements on ICL tasks involving quantification and linear function prediction, where softmax-based models fail due to boundary effects and concentration on single tokens. Experiments show SSA achieves lower perplexity and stronger performance on standard NLP benchmarks under zero- and few-shot settings compared to softmax, while also improving grammatical probing accuracy in encoder models trained on child-directed speech.

## Method Summary
SSA replaces softmax normalization in attention with a polynomial scoring function: f(x) = (1 + b|x|)·sgn(x)^n, where b>0 and n≥1 are learnable per-head parameters. The attention weights are computed as α_i = f(z_i) / Σ_k f(z_k) where z are the standard attention scores. This formulation provides polynomial growth that prevents the rapid concentration seen with softmax's exponential e^x. The method adds only 288 parameters for a 12L12H model and maintains the same computational complexity as standard attention. SSA parameters are initialized with b=1 and n=1.5, learned via standard backpropagation.

## Key Results
- SSA achieves 2.4-point perplexity improvement on FineWebText and 5-10% gains on ARC, HellaSwag, LAMBADA, RTE, and WSC tasks
- On ICL tasks, SSA substantially outperforms softmax on quantification (every/some positive) and linear function prediction with lower MSE across distribution shifts
- In encoder models trained on child-directed speech, SSA improves grammatical probing accuracy across syntactic and morphological tests
- Theoretical analysis shows SSA grows polynomially and resists hardmax-like collapse, unlike softmax's exponential growth

## Why This Works (Mechanism)

### Mechanism 1: Polynomial Growth Prevents Attention Saturation
- **Claim:** SSA's polynomial scoring function prevents rapid concentration of attention weights that occurs with softmax's exponential, enabling more distributed attention across tokens
- **Mechanism:** Softmax uses e^x which grows super-polynomially; when logit gaps exceed ~4, the largest value receives weight approaching 1 while others approach 0. SSA uses (1 + b|x|)·sgn(x)^n which grows only polynomially (as x^n for large positive x), maintaining meaningful weights across multiple tokens even with large magnitude differences
- **Core assumption:** Tasks requiring ICL benefit from integrating information across many context tokens rather than focusing narrowly on high-magnitude signals
- **Evidence anchors:** [Section 5.3] shows softmax attention collapsing onto largest values, [Section 7] explains polynomial growth prevents saturation

### Mechanism 2: Self-Regularizing Gradient Suppression
- **Claim:** SSA's gradient structure automatically dampens influence of extreme logits during training, improving numerical stability
- **Mechanism:** The effective slope g(x) = f'(x)/f(x) = nb/(1 + b|x|) decreases toward 0 as |x| → ∞, whereas softmax maintains g(x) ≡ 1. This means SSA naturally suppresses gradient contributions from out-of-distribution large values
- **Core assumption:** Out-of-distribution inputs at inference time produce extreme logits that should not dominate learning signals
- **Evidence anchors:** [Appendix J.2] proves SSA exhibits self-regulating gradient suppression, [Section 5.3] explains training limitations with extreme logits

### Mechanism 3: Bounded Attention Weights Prevent Hardmax Collapse
- **Claim:** SSA mathematically guarantees attention weights remain bounded away from 1 for finite parameters, preventing reduction to single-token attention
- **Mechanism:** For bounded logits |s_i| ≤ M and finite n, α_max ≤ 1 - ε where ε > 0. This entropy preservation prevents collapse unlike softmax where e^(s_max - s_j) → ∞ causes guaranteed convergence to hardmax
- **Core assumption:** Maintaining distributional attention (rather than near-deterministic single-token focus) supports generalization in tasks requiring multi-token reasoning
- **Evidence anchors:** [Appendix J.6] Theorem 1 proves SSA cannot collapse to one-hot distribution for finite n, [Figure 2] shows softmax collapsing entirely onto largest value

## Foundational Learning

- **Concept: Softmax attention mechanism**
  - **Why needed here:** Understanding how standard attention weights are computed via softmax(QK^T/√d_k) is prerequisite to grasping why exponential scoring causes saturation and what SSA changes
  - **Quick check question:** Given attention scores [2.0, 1.5, 1.0, -3.0], what problem arises when softmax is applied and why does the -3.0 value effectively vanish?

- **Concept: In-context learning (ICL)**
  - **Why needed here:** The paper diagnoses ICL generalization failures and positions SSA as a fix; understanding ICL as learning from prompt demonstrations without weight updates is essential
  - **Quick check question:** If a model trained on N(0,1) inputs sees a sequence containing [0.5, -0.3, 47.2], why might its ICL prediction fail on this "deviant" sequence?

- **Concept: Distribution shift and OOD generalization**
  - **Why needed here:** The paper's core contribution addresses performance degradation when test distributions (D_test) differ from training (D_train), particularly for out-of-distribution values
  - **Quick check question:** Why does training on N(0,1) not prepare the Q/K matrices to handle inputs from N(0,10), and how does softmax amplify this problem?

## Architecture Onboarding

- **Component map:** Compute attention scores z = QK^T/√d_k → Apply SSA: f(z_i) = (1 + b|z_i|)·sgn(z_i)^n → Normalize: α = f(z) / Σf(z) → Continue with α·V as standard

- **Critical path:** 1) Compute attention scores z = QK^T/√d_k as usual 2) Apply SSA elementwise: f(z_i) = (1 + b|z_i|)·sgn(z_i)^n 3) Normalize: α = f(z) / Σf(z) 4) Continue with α·V as standard

- **Design tradeoffs:** Higher n → more exponential-like behavior (sharper attention, but closer to softmax's saturation issues); Lower n → more uniform attention (better distribution, but potentially weaker signal on genuinely important tokens); Paper found n=1.5-2.0 effective; n→∞ recovers exponential as limiting case

- **Failure signatures:** If SSA model still shows attention collapse, check whether n has learned to very large values; If performance degrades on tasks requiring sharp focus, n may be too low (try 2.0-2.5); NaN gradients can occur if b becomes ≤ 0; constrain b > 0 with softplus or exp parameterization

- **First 3 experiments:**
  1. **Ablation on n:** Train identical models with fixed n ∈ {1.0, 1.5, 2.0, 3.0} on a simple ICL task (e.g., linear function prediction) and plot MSE vs. distribution shift σ; verify n ≈ 1.5-2 is optimal
  2. **Attention visualization:** Run both softmax and SSA models on a deviant input sequence containing one large outlier; visualize attention heatmaps to confirm SSA maintains distributed weights while softmax collapses
  3. **Benchmark replication:** Train GPT-2 small on FineWebText (or subset) for 10k-50k steps with both softmax and SSA; measure perplexity gap and spot-check 3-4 LM-Eval tasks to confirm reported improvements scale to your setup

## Open Questions the Paper Calls Out

- **Would SSA's improvements persist when scaled to models with billions of parameters?** The experiments only cover small models (up to 124M parameters), leaving open whether benefits are specific to small-scale regime or generalize to larger models where softmax's limitations might be mitigated by scale itself.

- **Can the architectural conflation of token value with token importance be resolved within the attention mechanism itself?** SSA addresses softmax saturation but not the fundamental design issue where large-magnitude embeddings are treated as highly important regardless of actual task relevance.

- **Does SSA provide benefits when both input distribution and task parameters shift simultaneously beyond training distributions?** The paper demonstrates improvements under individual distribution shifts but does not systematically characterize performance under compound shifts where multiple aspects of task distribution change.

## Limitations

- Empirical improvements may be sensitive to implementation details and need independent replication across different model sizes and datasets
- Generalizability of ICL improvements across diverse task types remains uncertain, particularly for complex reasoning tasks
- The paper does not provide comprehensive ablation studies on SSA parameters (b and n) across different model scales and task domains

## Confidence

**High:** The theoretical analysis of SSA's polynomial growth properties is mathematically sound with rigorous proofs provided.

**Medium:** Empirical improvements on downstream NLP benchmarks are demonstrated but may be sensitive to implementation details.

**Low:** Generalizability of ICL improvements across diverse task types remains uncertain, and the specific training setup for ICL tasks may contribute significantly to observed improvements.

## Next Checks

1. **Cross-task ICL validation:** Implement SSA in a decoder model and test on a diverse suite of ICL tasks including arithmetic reasoning, multi-step logical reasoning, and compositional generalization tasks. Compare performance against softmax baselines and document whether polynomial growth consistently prevents attention collapse across these varied domains.

2. **Parameter sensitivity analysis:** Systematically vary n (e.g., n ∈ {1.0, 1.5, 2.0, 2.5, 3.0}) and b initialization strategies across multiple model sizes (124M, 355M, 774M parameters). Measure impact on both training stability and final task performance to establish guidelines for parameter selection.

3. **Scale-up validation:** Train SSA-modified transformers at scale (1B+ parameters) on large-scale pretraining datasets. Measure whether the perplexity improvements and generalization benefits observed in small models persist at scale, and whether any new failure modes emerge that require architectural modifications.