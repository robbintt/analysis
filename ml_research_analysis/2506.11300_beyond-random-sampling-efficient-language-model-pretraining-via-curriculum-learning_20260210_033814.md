---
ver: rpa2
title: 'Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum
  Learning'
arxiv_id: '2506.11300'
source_url: https://arxiv.org/abs/2506.11300
tags:
- training
- learning
- tokens
- data
- curriculum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents the first systematic study of curriculum learning\
  \ for large language model pretraining. It evaluates three curriculum strategies\u2014\
  vanilla ordering, pacing-based sampling, and interleaved curricula\u2014across six\
  \ difficulty metrics computed on a fixed-quality dataset."
---

# Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning

## Quick Facts
- arXiv ID: 2506.11300
- Source URL: https://arxiv.org/abs/2506.11300
- Authors: Yang Zhang; Amr Mohamed; Hadi Abdine; Guokan Shang; Michalis Vazirgiannis
- Reference count: 40
- First systematic study of curriculum learning for large language model pretraining

## Executive Summary
This work introduces a systematic investigation of curriculum learning strategies for large language model pretraining. By ordering training data based on linguistic difficulty metrics, the authors demonstrate that curriculum-based pretraining accelerates early-to-mid-stage convergence by 18-45% and, when used as warmup, yields sustained final performance gains of up to 3.5%. The approach requires negligible computational overhead since difficulty metrics are lightweight and computed only once before training. The findings suggest curriculum learning is a practical, model-independent mechanism to improve pretraining efficiency across different model scales.

## Method Summary
The study evaluates three curriculum learning strategies—vanilla ordering, pacing-based sampling, and interleaved curricula—across six difficulty metrics computed on a fixed-quality dataset. Difficulty metrics include compression ratio, MTLD, Flesch Reading Ease, and others that capture linguistic richness and readability. Models ranging from 0.5B to 3B parameters are trained up to 100B tokens. The curricula are implemented by reordering or sampling training data based on computed difficulty scores, with pacing functions controlling the transition between difficulty levels. The approach operates orthogonally to data selection or pruning methods and requires minimal computational overhead since metrics are computed once before training.

## Key Results
- Early-to-mid-stage convergence accelerated by 18-45% using curriculum learning
- Sustained final performance gains of up to 3.5% when curriculum is used as warmup
- Compression ratio, MTLD, and Flesch Reading Ease identified as most effective difficulty metrics
- Negligible computational overhead from metric computation (computed once, model-independent)

## Why This Works (Mechanism)
Curriculum learning works by presenting increasingly complex linguistic patterns to the model in a structured sequence, allowing it to build foundational representations before tackling more challenging material. This mimics human learning processes where simpler concepts are mastered before advancing to complex ones. By controlling the difficulty progression through pacing functions or interleaving, the model can maintain stable gradient updates and avoid being overwhelmed by complex patterns too early in training. The approach leverages the inherent structure in natural language data, where simpler, more repetitive patterns often precede complex syntactic and semantic constructions.

## Foundational Learning
- Linguistic difficulty metrics: These quantify text complexity using measures like compression ratio (information density), MTLD (vocabulary diversity), and readability scores. Why needed: To objectively rank training data by difficulty without relying on model performance. Quick check: Compare metric rankings across different text samples.
- Pacing functions: Mathematical functions controlling the rate of difficulty progression during training. Why needed: To smooth transitions between difficulty levels and prevent gradient instability. Quick check: Monitor training stability when varying pacing parameters.
- Curriculum interleaving: Alternating between easy and hard samples according to a schedule. Why needed: To maintain model exposure to simpler patterns while learning complex ones. Quick check: Verify sample difficulty distribution matches intended interleaving ratio.

## Architecture Onboarding

Component map: Difficulty metrics -> Data ordering/sampling -> Model training -> Performance evaluation

Critical path: Precompute difficulty metrics → Apply curriculum strategy → Train model with ordered/sampled data → Evaluate convergence and final performance

Design tradeoffs: Static vs. dynamic difficulty assessment (computational cost vs. adaptability), fixed vs. adaptive pacing (simplicity vs. responsiveness), full vs. partial curriculum application (convergence speed vs. final performance)

Failure signatures: Over-curriculum (model plateaus on easy data), under-curriculum (model overwhelmed by complexity), misaligned metrics (difficulty rankings don't reflect true learning progression)

First experiments:
1. Baseline random sampling vs. simplest-first ordering with compression ratio metric
2. Pacing function sensitivity analysis (linear vs. exponential vs. sigmoid progression)
3. Interleaving ratio optimization (1:1 vs. 3:1 easy:hard sampling)

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Experiments use fixed dataset size and distribution, unclear how approach scales to larger/more diverse corpora
- Difficulty metrics computed once before training, not dynamically updated to adapt to evolving model capabilities
- Focus on English-language corpora, multilingual applicability remains untested
- Direct downstream task performance improvements not measured

## Confidence
- Early-to-mid-stage convergence acceleration (18-45%): High
- Sustained final performance gains from warmup curricula (up to 3.5%): Medium
- Best difficulty metrics identified (compression ratio, MTLD, Flesch Reading Ease): High
- Negligible computational overhead: High
- Orthogonality to data selection/pruning: Medium

## Next Checks
1. Evaluate curriculum learning on dynamically updated difficulty metrics that adapt to the current model's performance
2. Test the robustness of observed gains on multilingual and larger-scale datasets
3. Measure direct downstream task performance improvements attributable to curriculum-based pretraining