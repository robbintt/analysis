---
ver: rpa2
title: 'MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware
  Iterative Self-Prompting with LLMs'
arxiv_id: '2509.07622'
source_url: https://arxiv.org/abs/2509.07622
tags:
- clinical
- prompt
- summaries
- summary
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MaLei team applied perspective-aware iterative self-prompting (PA-ISP)
  on GPT-4 and GPT-4o for clinical document summarisation in the MultiClinSUM shared
  task. The method used LLMs to generate task-specific prompts, refine them through
  example-based few-shot learning, and employed ROUGE and BERT-score for guidance.
---

# MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs

## Quick Facts
- arXiv ID: 2509.07622
- Source URL: https://arxiv.org/abs/2509.07622
- Reference count: 40
- MaLei team achieved ROUGE F1 of 30.77 and BERTScore F1 of 85.46 on MultiClinSUM test set using PA-ISP with GPT-4 and GPT-4o.

## Executive Summary
MaLei applied perspective-aware iterative self-prompting (PA-ISP) to the MultiClinSUM clinical document summarization task, using GPT-4 and GPT-4o to generate and refine task-specific prompts through metric-guided feedback loops. The method achieved strong semantic preservation (BERTScore F1: 85.46) but lower lexical overlap (ROUGE-L F1: 30.77), indicating effective content coverage with structural paraphrasing. The approach demonstrates promise for automating clinical report summarization while highlighting the challenge of aligning generated summaries with reference styles.

## Method Summary
The PA-ISP method uses a meta-prompt combining chain-of-thought guidance, five clinical perspectives (Patient Presentation, Clinical Presentation, Diagnosis, Treatment, Outcome), metric guidance, and few-shot examples. GPT-4 generates an initial task-specific prompt, which GPT-4o uses to produce summaries on a held-out batch. ROUGE-L and BERTScore are computed against gold references, with low-scoring cases selected for reflection. GPT-4o analyzes these cases and proposes prompt revisions, creating an iterative refinement loop repeated for up to 5 epochs. The best-performing prompt is then applied to the full test set with length checks and regeneration if summaries exceed source length.

## Key Results
- ROUGE scores: Precision 46.53, Recall 24.68, F1 30.77
- BERT-scores: Precision 87.84, Recall 83.25, F1 85.46
- High BERTScore indicates strong semantic preservation despite lower lexical overlap
- Qualitative analysis confirmed coverage of key clinical aspects with logical paraphrasing

## Why This Works (Mechanism)

### Mechanism 1
Iterative self-prompting enables LLMs to refine their own task instructions through reflective feedback. The system provides a meta-prompt with task description, CoT guidance, clinical perspectives, and few-shot examples. The LLM generates prompt_v1, which is applied to a held-out batch. Summaries are evaluated using ROUGE-L and BERTScore, and the LLM reflects on low-scoring cases to propose prompt revisions. This cycle repeats until performance plateaus.

### Mechanism 2
Perspective-aware structural guidance improves clinical content coverage and semantic alignment. Five clinical perspectives (Patient Presentation, Clinical Presentation, Diagnosis, Treatment, Outcome) act as soft constraints, steering the model to extract and organize information along clinically meaningful dimensions rather than producing unstructured compressions.

### Mechanism 3
Metric-guided prompt optimization can improve lexical overlap but is constrained by the semantic-paraphrase trade-off. ROUGE-L and BERTScore are computed for each summary. Low ROUGE-L cases (0.12-0.52 range) are selected for reflection. BERTScore remained stable (>0.85) across iterations while ROUGE-L showed limited improvement, suggesting a ceiling from fundamental style differences.

## Foundational Learning

- **Concept: Iterative Self-Prompting (ISP)**
  - Why needed here: Understanding how LLMs can generate, evaluate, and refine their own prompts through feedback cycles
  - Quick check question: Given a summary with ROUGE-L 0.20 and BERTScore 0.88, what specific feedback would you provide to a prompt-refining LLM?

- **Concept: Perspective-Aware Structuring**
  - Why needed here: Clinical summarization quality depends on organizing information along clinically relevant dimensions
  - Quick check question: If a source document omits outcome information, should the model generate a placeholder or skip the perspective? What does the paper observe?

- **Concept: ROUGE-L vs BERTScore Trade-offs**
  - Why needed here: The paper shows high semantic overlap (BERTScore >0.85) but low lexical overlap (ROUGE-L ~0.31)
  - Quick check question: A generated summary expands all abbreviations and uses explicit section headers absent from references. Which metric will be most affected and why?

## Architecture Onboarding

- **Component map**: Meta-prompt constructor -> Initial prompt generator -> Summary generator -> Evaluation module -> Reflection engine -> Iterative updater -> Test-time inference with length checks
- **Critical path**: Meta-prompt → prompt_v1 → generate summaries on 50-sample batch → compute metrics → select 15 lowest ROUGE-L cases → feed to reflection engine → update prompt → repeat up to 5 epochs → select best prompt → run on 3,396 test documents
- **Design tradeoffs**: Using GPT-4 for prompt generation vs GPT-4o for summarization; optimizing for ROUGE-L over BERTScore; fixed perspective headers vs flexibility; 50-sample batch with 15-case reflection subset
- **Failure signatures**: ROUGE-L plateau at ~0.30 despite iteration; generated summaries longer than source (12/3,396 cases); hallucinated content when source lacks perspective-relevant info; rigid section headers absent from gold summaries; invalid/parsing failures in early iterations
- **First 3 experiments**: 1) Baseline replication with 3 fixed perspectives vs 5-perspective configuration; 2) Ablation on reflection batch size (5 vs 15 vs 30 cases); 3) Style alignment intervention with explicit abbreviation preservation instruction

## Open Questions the Paper Calls Out

1. Does retrieval-based few-shot augmentation or full-data structure-aware prompt enhancement significantly improve lexical overlap (ROUGE-L) scores? [explicit] Sections 3.4 and 3.5 detail these specific methodologies which were designed but not executed due to time constraints.

2. Can prompt engineering effectively mitigate the model's tendency to hallucinate content when forced to adhere to rigid structural perspectives? [explicit] Section 5.1 notes that structure-guided prompting may induce hallucinations and suggests future work on counterfactual constraints.

3. Do clinically grounded factual consistency metrics correlate better with human judgment than ROUGE-L for this specific task? [explicit] Section 5.2 suggests that low ROUGE-L scores may not indicate poor quality due to stylistic differences, and proposes future work incorporate factual checks.

## Limitations

- Causal link between prompt refinement and lexical score improvement remains unclear due to semantic-paraphrase trade-off
- No ablation of perspective-aware guidance to isolate structural vs iterative effects
- Few-shot examples and exact meta-prompt templates not fully specified, limiting exact replication

## Confidence

- **High**: Semantic preservation claim (BERTScore > 0.85), effectiveness of perspective-aware structure for clinical content coverage, basic ISP framework implementation
- **Medium**: Claims about ROUGE-L improvement via prompt refinement and attribution of performance gains specifically to iterative self-prompting
- **Low**: Claims about optimal perspective set, reflection batch size, and robustness to varying document styles

## Next Checks

1. **Ablation study**: Remove perspective-aware constraints and rerun ISP; compare ROUGE-L and BERTScore to isolate structural vs iterative effects
2. **Style alignment intervention**: Modify meta-prompt to explicitly preserve abbreviations and omit section headers; measure ROUGE-L impact while monitoring BERTScore stability
3. **Reflection batch size test**: Vary the number of low-scoring cases used for prompt refinement (5, 15, 30); measure iteration efficiency and final metric convergence