---
ver: rpa2
title: 'Response-Level Rewards Are All You Need for Online Reinforcement Learning
  in LLMs: A Mathematical Perspective'
arxiv_id: '2506.02553'
source_url: https://arxiv.org/abs/2506.02553
tags:
- reward
- policy
- arxiv
- response-level
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a fundamental challenge in LLM alignment:
  the Zero-Reward Assumption, where only the final token in a response receives a
  reward while intermediate tokens receive zero rewards. The authors introduce the
  Trajectory Policy Gradient Theorem, proving that policy gradients can be unbiasedly
  estimated using only response-level rewards without requiring token-level rewards.'
---

# Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective

## Quick Facts
- **arXiv ID:** 2506.02553
- **Source URL:** https://arxiv.org/abs/2506.02553
- **Reference count:** 40
- **Key outcome:** Response-level rewards are theoretically sufficient for unbiased token-level modeling in LLM alignment, eliminating the need for explicit token-level reward mechanisms.

## Executive Summary
This paper resolves a fundamental tension in LLM alignment by proving that response-level rewards can unbiasedly estimate policy gradients without requiring token-level rewards. The authors introduce the Trajectory Policy Gradient Theorem, which shows that existing RL methods (PPO, GRPO, ReMax, RLOO) inherently possess token-level modeling capability despite using only response-level rewards. This theoretical result unifies the landscape of online RL approaches and suggests that developers should focus on improving response-level reward models rather than implementing complex token-level mechanisms.

## Method Summary
The paper introduces the Trajectory Policy Gradient Theorem, proving that policy gradients based on unknown token-level rewards can be unbiasedly estimated using only response-level rewards through trajectory sampling. This leads to the proposal of Token-Reinforced Policy Optimization (TRePO), which approximates state-dependent baselines via sampled trajectories instead of learned critics, achieving PPO-comparable advantages with GRPO-level memory efficiency.

## Key Results
- The Trajectory Policy Gradient Theorem proves that response-level rewards suffice for unbiased policy gradient estimation regardless of the Zero-Reward Assumption.
- Existing algorithms (PPO, GRPO, ReMax, RLOO) inherently possess token-level modeling capability through their gradient formulations.
- TRePO provides a theoretically grounded alternative to PPO that matches GRPO in memory efficiency while maintaining state-dependent baselines through sampling.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Policy gradients based on unknown token-level rewards can be unbiasedly estimated using only response-level rewards, without requiring the Zero-Reward Assumption.
- **Mechanism:** The Trajectory Policy Gradient Theorem shows that by adding state-dependent constants at each timestep and sampling continuation trajectories, the expectation over sampled trajectories provides an unbiased estimate of the true policy gradient.
- **Core assumption:** The response-level reward decomposes as a discounted sum of token-level contributions; the policy is differentiable w.r.t. its parameters.
- **Break condition:** If the response-level reward cannot be expressed as a discounted sum of token-level contributions, or if sampling continuation trajectories is infeasible, unbiasedness no longer holds.

### Mechanism 2
- **Claim:** Existing algorithms (PPO, GRPO, ReMax, RLOO) already possess token-level modeling capability even when implemented under the Zero-Reward Assumption.
- **Mechanism:** These methods differ primarily in baseline selection for variance reductionâ€”not in fundamental token-level capability. PPO's critic provides state-dependent baselines; GRPO/ReMax/RLOO use response-level baselines equivalent to a specific baseline term.
- **Core assumption:** The policy gradient formulas in these methods match the forms derived from the Trajectory Policy Gradient Theorem.
- **Break condition:** If an algorithm's gradient formula fundamentally differs from the policy gradient theorem form, token-level capability claims don't apply.

### Mechanism 3
- **Claim:** TRePO achieves PPO-comparable theoretical advantages with GRPO-level memory efficiency by replacing the learned critic with sampled trajectory expectations.
- **Mechanism:** TRePO approximates the optimal variance-reducing baseline by sampling trajectories at selected timesteps and using interpolation for intermediate positions, avoiding storing a critic model while maintaining state-dependent baselines.
- **Core assumption:** Sufficient sampling provides reasonable approximations of the true expectations; interpolation between sampled timesteps doesn't introduce critical errors.
- **Break condition:** If sampling budget is insufficient or reward variance is extremely high, baseline estimates become too noisy for stable training.

## Foundational Learning

- **Concept: Policy Gradient Theorem**
  - Why needed here: The entire paper is a specialization of policy gradient theory to LLM autoregressive generation.
  - Quick check question: Can you explain why adding a state-dependent baseline doesn't bias the gradient estimate?

- **Concept: Actor-Critic Architecture**
  - Why needed here: PPO uses a critic (value function) for variance reduction; the paper compares critic-based vs. critic-free approaches.
  - Quick check question: What role does the critic play in computing the advantage function A(s,a)?

- **Concept: Credit Assignment in RL**
  - Why needed here: The core problem is attributing a response-level reward to individual token decisions across potentially hundreds of timesteps.
  - Quick check question: Why is credit assignment particularly challenging when rewards are sparse (only at trajectory end)?

## Architecture Onboarding

- **Component map:** Policy model (LLM) -> Response-level reward model -> Baseline estimator (critic or sampled expectations) -> Reference model for KL penalty
- **Critical path:** Sample trajectories -> Compute response-level rewards -> Estimate advantages -> Compute loss (clipped or REINFORCE) -> Update policy parameters
- **Design tradeoffs:** PPO offers lower variance via learned critic but requires 2x memory; GRPO is memory efficient but uses same baseline for all timesteps; TRePO is memory efficient with state-dependent baselines but requires sampling overhead; DPO needs no online sampling but optimizes ranking rather than expected return.
- **Failure signatures:** Critic initialization issues (PPO) cause early advantage calibration problems; insufficient sampling (TRePO) creates high-variance baseline estimates; poor baseline selection (GRPO/RLOO) leads to suboptimal credit assignment; KL collapse affects all methods.
- **First 3 experiments:** 1) Compare PPO vs. GRPO on synthetic task with known intermediate token importance to verify both learn position-dependent credit assignment. 2) Implement TRePO with varying sampling budgets and compare gradient variance against PPO's critic-based baselines. 3) Test PPO (with/without critic pretraining) vs. TRePO vs. GRPO on binary reward math reasoning to identify performance differences.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does TRePO provide statistically significant performance gains over PPO, GRPO, and DPO in reasoning-heavy or multi-turn environments?
- **Basis in paper:** The authors state in the Future Work section they will evaluate TRePO against other approaches in practical LLM tasks including mathematical reasoning and multi-turn dialogues.
- **Why unresolved:** The paper currently provides only mathematical derivation without experimental results.
- **What evidence would resolve it:** Empirical benchmarks comparing TRePO against PPO, GRPO, and DPO on standard datasets showing win rates or accuracy metrics relative to computational cost.

### Open Question 2
- **Question:** Do performance improvements in "partial-reward" methods (like RTO or ABC) stem from explicit token-level information or simply from stronger aggregated reward signals?
- **Basis in paper:** The authors explicitly question whether improvements come from token-level information or stronger response-level rewards.
- **Why unresolved:** The paper theoretically proves response-level rewards are sufficient, challenging the necessity of partial-reward approaches.
- **What evidence would resolve it:** Ablation studies isolating the "token-level information" component from the "stronger reward model" component in methods like RTO.

### Open Question 3
- **Question:** Can the sampling overhead required by TRePO be reduced enough to make it competitive with critic-free methods like GRPO in terms of wall-clock training time?
- **Basis in paper:** The paper acknowledges TRePO brings computational cost from sufficient sampling and engineering effort.
- **Why unresolved:** While the paper proposes approximation techniques to mitigate cost, it doesn't verify if these maintain theoretical unbiasedness or match the speed of methods avoiding extra sampling.
- **What evidence would resolve it:** Profiling data comparing training throughput and convergence speed of TRePO against GRPO, demonstrating variance reduction benefits outweigh sampling latency.

## Limitations
- The theoretical framework assumes response-level rewards can be decomposed into discounted token-level rewards, which may not hold for all reward modeling approaches.
- The sampling-based baseline estimation in TRePO introduces computational overhead that may become prohibitive for very long trajectories.
- The analysis focuses on single-turn generation; extension to multi-turn conversations or hierarchical generation remains unexplored.

## Confidence

**High Confidence:** The Trajectory Policy Gradient Theorem and its proof are mathematically rigorous, providing solid theoretical foundation for response-level reward sufficiency.

**Medium Confidence:** The analysis of existing algorithms having inherent token-level capability is theoretically sound but may be affected by practical implementation differences.

**Low Confidence:** The practical advantages of TRePO over existing methods require more extensive empirical validation beyond the current theoretical framework.

## Next Checks
1. **Cross-Domain Robustness Test:** Evaluate TRePO and baseline methods across diverse alignment tasks (code generation, instruction following, creative writing) to verify response-level rewards provide sufficient token-level modeling capability beyond mathematical reasoning.

2. **Sampling Budget Sensitivity Analysis:** Systematically vary the sampling parameters in TRePO to identify the Pareto frontier between computational cost and performance, comparing against critic-based approaches under identical compute budgets.

3. **Decomposition Assumption Validation:** Design experiments where response-level rewards include non-additive components (pairwise token dependencies, global coherence scores) to test whether the theoretical decomposition assumption breaks down and identify failure modes.