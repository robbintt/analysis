---
ver: rpa2
title: Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation
arxiv_id: '2505.24479'
source_url: https://arxiv.org/abs/2505.24479
tags:
- fake
- plausibility
- information
- detection
- misinformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel pipeline that leverages knowledge
  graphs (KGs) and large language models (LLMs) to generate structured misinformation.
  The method modifies factual triplets from KGs by replacing the object entity with
  plausible alternatives ranked by structural similarity, then uses LLMs to generate
  coherent fake statements.
---

# Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation

## Quick Facts
- arXiv ID: 2505.24479
- Source URL: https://arxiv.org/abs/2505.24479
- Reference count: 5
- Primary result: High-plausibility misinformation generated from KG triplets significantly reduces LLM detection accuracy across domains

## Executive Summary
This paper introduces a pipeline that leverages knowledge graphs (KGs) and large language models (LLMs) to generate structured misinformation. The method modifies factual triplets from KGs by replacing the object entity with plausible alternatives ranked by structural similarity, then uses LLMs to generate coherent fake statements. Experiments with Phi-4 and Llama-8B for generation, and Falcon-40B, Llama-70B, and Qwen-72B for detection, demonstrate that high-plausibility fake content is challenging to detect, with detection accuracy dropping significantly in certain categories (e.g., travel: 100% to 0%). Overall, LLMs exhibit notable limitations and biases in fact verification, especially for high-plausibility misinformation, highlighting the need for improved detection methods. The work underscores the potential of KGs for scalable misinformation generation and the dual role of LLMs in both generating and detecting deceptive content.

## Method Summary
The pipeline extracts triplets (subject, relation, object) from WikiGraphs, filters candidate object replacements sharing the same relation, and ranks them by Jaccard similarity between subject sets. High and low plausibility candidates are selected and used to construct fake triplets. LLMs (Phi-4, Llama-8B) generate natural language news articles from these triplets via structured prompts. Separate LLMs (Falcon-40B, Llama-70B, Qwen-72B) classify generated text as real or fake using detection prompts. The approach tests whether structurally plausible misinformation evades detection more effectively than random falsehoods.

## Key Results
- LLM detection accuracy drops significantly for high-plausibility misinformation compared to low-plausibility variants
- Detection performance varies dramatically across knowledge domains, with travel category showing 100% to 0% accuracy drop from low to high plausibility
- Current LLMs struggle with nuanced fact verification, often performing near random guessing on fake content
- Falcon-40B achieves high fake detection but poor real fact classification (32.31% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing the object entity in KG triplets with structurally similar alternatives produces fake facts that resist detection proportionally to their relational similarity.
- Mechanism: For a triplet ⟨s, r, o⟩, candidate replacements o′ are drawn from entities sharing the same relation r with different subjects. Plausibility is scored via Jaccard similarity between subject sets: P(o′, ⟨s, r, o⟩) = J[d(r, o), d(r, o′)]. Higher overlap → higher perceived credibility.
- Core assumption: Entities co-occurring with the same predicate share semantic type compatibility, making substitutions less suspicious to both humans and LLMs.
- Evidence anchors:
  - [abstract] "By analyzing the structural properties of KGs, such as the distance between entities and their predicates, we identify plausibly false relationships."
  - [section 3.3] Defines the plausibility scoring function and candidate filtering formally.
  - [corpus] Related work (Di Mauro et al. 2024, cited in paper) supports KG structural properties for credible link identification; corpus papers on KG-LLM integration (KaLM, Wikontic) emphasize relational grounding but do not address misinformation generation directly.
- Break condition: If the KG is sparse or the relation r has few subject-object pairs, candidate sets Or become empty or noisy, collapsing the plausibility ranking.

### Mechanism 2
- Claim: LLMs generate more persuasive misinformation when conditioned on structurally plausible fake triplets with domain-specific prompting.
- Mechanism: Fake triplets are converted to natural language via LLM prompts that include subject descriptions from the KG, a confident journalistic tone, and explicit instructions to avoid revealing falsehood. The LLM's fluency masks factual incoherence.
- Core assumption: LLMs prioritize coherence and stylistic consistency over factual verification when prompted persuasively.
- Evidence anchors:
  - [section 3.4] Details the prompt structure: professional journalist framing, description integration, length constraints.
  - [figure 5] Shows Phi-4 generating plausible fake news where semantic coherence (e.g., "2005 edition" of an 1870 novel) masks factual errors.
  - [corpus] Corpus papers (e.g., "Enhancing LLMs with Reliable KGs") focus on KGs improving factual accuracy; this paper inverts that relationship—no corpus evidence directly addresses KGs for controlled misinformation.
- Break condition: If the fake triplet violates strong world knowledge encoded in the LLM (e.g., replacing "Paris" with "Tokyo" as capital of France), the model may generate hedged or inconsistent text.

### Mechanism 3
- Claim: LLM-based detection accuracy degrades significantly for high-plausibility misinformation and varies across knowledge domains.
- Mechanism: Detection LLMs classify statements as real/fake via prompting. High-plausibility fakes trigger fewer skeptical cues; category-specific knowledge gaps cause inconsistent performance (e.g., travel: 100% → 0% detection from low to high plausibility).
- Core assumption: LLMs lack robust internal fact verification and rely on surface plausibility heuristics.
- Evidence anchors:
  - [section 5.3] Table 1 shows Falcon-40B high fake detection but poor real classification; Qwen-72B/LLaMA-70B near-random (~50-60%) on fakes.
  - [section 5.4] Figure 6 shows category-specific collapse (travel, sports) when plausibility increases.
  - [corpus] Corpus papers do not systematically evaluate LLM detection failures across KG-derived plausibility levels.
- Break condition: If detectors are fine-tuned on domain-specific data or augmented with external fact databases, plausibility-based evasion may weaken.

## Foundational Learning

- Concept: **Knowledge Graph Triplet Structure (⟨subject, relation, object⟩)**
  - Why needed here: The entire pipeline operates on triplet manipulation; understanding how entities connect via predicates is prerequisite to grasping plausibility scoring.
  - Quick check question: Given ⟨Eiffel Tower, locatedIn, Paris⟩, what component would you modify to create a fake triplet, and why might ⟨Eiffel Tower, locatedIn, Berlin⟩ be more plausible than ⟨Eiffel Tower, locatedIn, Mars⟩?

- Concept: **Jaccard Similarity for Set Overlap**
  - Why needed here: Plausibility scoring uses Jaccard similarity on subject sets; readers must understand intersection-over-union to interpret why certain objects rank higher.
  - Quick check question: If d(r, o) = {s1, s2, s3} and d(r, o′) = {s2, s3, s4}, what is J[d(r, o), d(r, o′)]?

- Concept: **LLM Prompt Engineering for Constrained Generation**
  - Why needed here: The generation and detection prompts embed specific framings (journalist persona, binary classification); prompt structure directly affects output quality.
  - Quick check question: Why does the generation prompt explicitly forbid revealing the information is fake, and how might removing this constraint affect output?

## Architecture Onboarding

- Component map:
  1. KG Triplet Extractor → Pulls ⟨s, r, o⟩ from WikiGraphs (25 categories filtered)
  2. Candidate Filter → Builds Or by finding objects sharing relation r with other subjects
  3. Plausibility Scorer → Ranks candidates via Jaccard similarity; selects o′_high (max) and o′_low (min)
  4. Fake Triplet Constructor → Forms tfake = ⟨s, r, o′⟩
  5. LLM Generator (Phi-4 / Llama-8B) → Converts tfake to natural language via journalist prompt
  6. LLM Detector (Falcon-40B / Llama-70B / Qwen-72B) → Classifies generated text as Real/Fake via detection prompt

- Critical path: KG extraction → candidate filtering → plausibility scoring → LLM generation. If any step fails (empty Or, low-quality prompt), the pipeline produces either invalid or trivially detectable outputs.

- Design tradeoffs:
  - Modifying only object vs. subject/relation: Object-only changes simplify analysis but limit misinformation diversity. Subject/relation modifications would increase combinatorial explosion and complicate plausibility grounding.
  - High vs. low plausibility selection: High-plausibility fakes are stealthier but may be less harmful if believed (subtle distortions); low-plausibility fakes are more detectable but potentially more damaging if trusted.
  - Separate generation/detection models: Using independent models (Phi-4 for generation, Falcon/Qwen for detection) increases realism but introduces model-specific biases.

- Failure signatures:
  - Empty candidate set: Or = ∅ when relation r has insufficient co-occurring objects → pipeline stalls.
  - Detector over-conservatism: Falcon-40B labels most inputs as fake (32.31% accuracy on real facts) → unreliable for nuanced verification.
  - Category collapse: Detection accuracy drops to 0% in high-plausibility travel category → indicates domain blind spots.

- First 3 experiments:
  1. Baseline plausibility validation: Select 10 triplets from a single KG category (e.g., geography). Generate high and low plausibility fakes, manually assess whether high-plausibility versions feel more credible.
  2. Cross-model generation quality: Generate fake news from the same triplets using both Phi-4 and Llama-8B. Compare fluency and coherence via human review or automated metrics (e.g., perplexity).
  3. Category-wise detection probing: Test all three detector models on fake/real facts from 3 diverse categories (sports, travel, medicine). Plot accuracy by plausibility level to identify domain-specific weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does modifying the subject or relation in KG triplets, rather than just the object, yield misinformation with higher stealthiness or lower detectability?
- Basis in paper: [explicit] Section 3.3 restricts the pipeline to modifying only the object ($o$), noting that while modifying subject ($s$) or relation ($r$) is theoretically possible, it was omitted for efficiency and control.
- Why unresolved: The paper does not evaluate whether altering other triplet components affects the semantic plausibility or the difficulty of detection for the generated fake news.
- What evidence would resolve it: Comparative experiments where fake triplets are generated by perturbing subjects and relations, bench-marked against the object-modification results using the same LLM judges.

### Open Question 2
- Question: Can targeted fine-tuning or advanced prompt engineering significantly improve LLM detection accuracy for high-plausibility misinformation?
- Basis in paper: [explicit] Section 6 states that "improving LLM-based detection could involve targeted fine-tuning or prompt engineering strategies" to address the low accuracy (often near random guessing) observed in current models.
- Why unresolved: The experiments utilize off-the-shelf LLMs (Falcon, Llama, Qwen) as zero-shot detectors, which show significant limitations and biases without specific adaptation to the KG-generated content.
- What evidence would resolve it: A study measuring detection performance of specifically fine-tuned or specially prompted models on the "high-plausibility" dataset generated by the proposed pipeline.

### Open Question 3
- Question: Can adversarial prompting or category-aware generation strategies systematically exploit the specific domain weaknesses observed in LLM detectors?
- Basis in paper: [explicit] Section 6 suggests future work should "explore category-aware misinformation generation or adversarial prompting to further evade automated detectors."
- Why unresolved: The results show category-specific variance (e.g., detection dropping to 0% in travel), but the study does not test if these specific weaknesses can be actively targeted by an adaptive generation process.
- What evidence would resolve it: Experiments demonstrating that selectively generating fake news for low-accuracy categories further reduces the aggregate detection rate compared to random category selection.

## Limitations

- Unknown exact 25 categories used for KG filtering and incomplete prompt specifications limit faithful reproduction
- Binary real/fake classification framework may not capture nuanced misinformation containing partial truths
- Study focuses exclusively on English-language content from Wikipedia-derived KGs, limiting generalizability

## Confidence

**High Confidence**: The core mechanism of KG-based misinformation generation through object entity replacement is well-supported by the structural definition and plausibility scoring formula. The observation that LLM detection accuracy drops significantly for high-plausibility misinformation is clearly demonstrated across multiple models and categories.

**Medium Confidence**: The claim that detection performance varies dramatically across knowledge domains (e.g., travel accuracy dropping from 100% to 0%) is supported by the data but may reflect specific model limitations rather than universal LLM behavior. The generalizability of these findings to other LLM architectures or larger knowledge graph sources remains uncertain.

**Low Confidence**: The assertion that high-plausibility misinformation poses the greatest threat requires empirical validation beyond detection accuracy metrics. The relationship between structural similarity and human perception of credibility is assumed but not directly tested with human subjects.

## Next Checks

1. **Human Perception Study**: Conduct a blinded study where human evaluators rate the credibility of high-plausibility vs low-plausibility fake statements generated from the pipeline. Compare human detection rates with LLM performance to validate whether structural plausibility correlates with human judgment.

2. **Cross-Domain Generalization**: Apply the same methodology to knowledge graphs from different domains (scientific literature, news archives, social media knowledge bases) and measure whether detection accuracy patterns hold across source types and knowledge domains.

3. **Detector Robustness Testing**: Evaluate the same fake statements using fact-checking systems augmented with external knowledge bases or fine-tuned on misinformation detection. Compare performance against the vanilla LLM classifiers to assess whether detection failures are inherent to LLM architecture or specific to the detection prompting approach.