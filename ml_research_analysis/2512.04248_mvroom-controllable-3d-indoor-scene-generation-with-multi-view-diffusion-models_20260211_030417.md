---
ver: rpa2
title: 'MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion
  Models'
arxiv_id: '2512.04248'
source_url: https://arxiv.org/abs/2512.04248
tags:
- scene
- generation
- layout
- depth
- multi-view
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MVRoom introduces a controllable 3D indoor scene generation pipeline
  using multi-view diffusion models conditioned on coarse 3D layouts. The approach
  employs a two-stage design: the first stage encodes 3D layouts into image-based
  conditions with hybrid layout priors (multi-layer semantics and depth, spatial embeddings),
  while the second stage performs image-conditioned multi-view generation enhanced
  by a novel layout-aware epipolar attention mechanism.'
---

# MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models

## Quick Facts
- arXiv ID: 2512.04248
- Source URL: https://arxiv.org/abs/2512.04248
- Reference count: 40
- Key outcome: 12.5% improvement in SSIM over baseline multi-view diffusion models

## Executive Summary
MVRoom is a controllable 3D indoor scene generation pipeline that uses multi-view diffusion models conditioned on coarse 3D layouts. The method employs a two-stage design: encoding 3D layouts into image-based conditions with hybrid layout priors, followed by image-conditioned multi-view generation enhanced by a novel layout-aware epipolar attention mechanism. Evaluated on 3D-FRONT, MVRoom achieves state-of-the-art performance in novel view synthesis with significant improvements in structural similarity and perceptual quality.

## Method Summary
MVRoom takes coarse 3D layouts as input and generates high-fidelity 3D indoor scenes through a recursive multi-view generation framework. The method first encodes 3D layouts into multi-channel image conditions (semantics, depth, spatial embeddings) using multi-layer ray intersections. These conditions are fed into a Stable Diffusion v2-1 backbone with layout-aware epipolar attention, which constrains cross-view feature aggregation to epipolar segments intersecting with layout bounding boxes. The recursive controller samples camera trajectories, generates multi-view images, and maintains a global point cloud for consistency, integrating new predictions only when depth consistency thresholds are met.

## Key Results
- 12.5% improvement in SSIM (0.8154 vs. 0.7247) over MVDiffusion baseline
- Superior perceptual quality in user studies (PQ, 3DC, LP metrics)
- Enables text-to-scene generation and 3D Gaussian splatting reconstruction
- State-of-the-art novel view synthesis performance on 3D-FRONT dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layout-aware epipolar attention improves cross-view feature alignment compared to standard attention mechanisms.
- Mechanism: The attention module constrains cross-view feature aggregation to only those epipolar line segments that intersect with known 3D layout bounding boxes, filtering out irrelevant features before attention computation. This replaces dense correspondence with layout-guided sparse correspondence.
- Core assumption: The coarse 3D layout bounding boxes accurately approximate scene geometry; epipolar geometry correctly models multi-view relationships for indoor scenes with predominantly planar surfaces.
- Evidence anchors:
  - [abstract] "incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process"
  - [section 3.2] Eq. 5 defines the mask function that selects "only the epipolar line between q_j(d_k^1) and q_j(d_k^2)" where the ray intersects bounding box B_k
  - [table 1] LA-epipolar achieves 0.8154 SSIM vs. 0.8077 (plain epipolar) and 0.7247 (MVDiffusion CAA)
  - [corpus] Related work on epipolar transformers exists (He et al., 2020), but MVRoom's layout-aware masking appears novel
- Break condition: If layout bounding boxes are inaccurate or missing, the epipolar mask may exclude valid correspondences or include invalid ones, degrading consistency.

### Mechanism 2
- Claim: Multi-layer semantic and depth conditions preserve more 3D layout information than single-layer representations.
- Mechanism: Instead of rendering only the first visible surface, the method encodes the first m intersection points along each pixel ray, capturing occluded geometry. Spatial embeddings (local surface coordinates and global 3D positions) provide explicit positional cues across views.
- Core assumption: The layout bounding boxes correctly identify object boundaries and semantics; m=3 layers is sufficient to capture relevant occlusion relationships in typical indoor scenes.
- Evidence anchors:
  - [abstract] "hybrid layout priors (multi-layer semantics and depth, spatial embeddings)"
  - [section 3.1] "our approach considers the first m intersection points along each pixel ray with the 3D scene layout L"
  - [table 2] Multi-layer conditions (m=3) improve SSIM from 0.7863 to 0.8044 over single-layer; full model with spatial embeddings achieves 0.8154
  - [corpus] Corpus papers mention layout conditioning (RoomCraft, CHOrD) but do not report multi-layer encoding details for direct comparison
- Break condition: If scenes have deep occlusion stacks exceeding m layers, or if semantics are mislabeled, the representation loses information.

### Mechanism 3
- Claim: Recursive generation with a global point cloud maintains consistency across sequential multi-view batches.
- Mechanism: After each MVRoom generation, predicted depth is compared against rendered depth from the accumulated global point cloud; only geometrically consistent results are integrated. The point cloud serves as a persistent memory for rendering image conditions for new views.
- Core assumption: Monocular depth estimation with layout-aligned rectification is accurate enough to detect inconsistencies; depth comparison threshold (0.02m) correctly distinguishes errors from acceptable variation.
- Evidence anchors:
  - [abstract] "maintaining a global point cloud for consistency"
  - [section 3.3] Algorithm 1 lines 15-19: "if Check depth consistency(Ď_ki, D_ki) then PC_global ← PC_global ∪ PC_cur"
  - [section 6.3] "we only integrate the newly predicted point cloud if the inconsistency metric is below a predefined threshold (0.02m)"
  - [corpus] ViewCrafter (Yu et al., 2024) uses similar point cloud rendering for consistency but without layout guidance
- Break condition: If depth estimation fails on generated images (e.g., novel textures, lighting), erroneous points may be rejected or, worse, accepted if the threshold is too loose.

## Foundational Learning

- Concept: **Epipolar geometry**
  - Why needed here: The attention mechanism relies on computing epipolar lines to find correspondences across views; understanding how camera poses define epipolar constraints is essential for debugging the mask function.
  - Quick check question: Given two camera poses and a pixel in view 1, can you compute the corresponding epipolar line in view 2?

- Concept: **Diffusion model conditioning (ControlNet/Adapter approaches)**
  - Why needed here: MVRoom conditions the Stable Diffusion backbone on layout-derived signals via separate adapters; understanding how these condition signals are injected helps interpret training requirements.
  - Quick check question: What is the difference between training a full fine-tune vs. training only adapter layers for a diffusion model?

- Concept: **Point cloud fusion and depth consistency checking**
  - Why needed here: The recursive framework depends on integrating new depth predictions into a global point cloud only when they match existing geometry; this is the consistency gate.
  - Quick check question: How would you compute a distance metric between two depth maps (one predicted, one rendered) in a common view?

## Architecture Onboarding

- Component map: Layout encoding (multi-layer conditions) -> Epipolar attention mask computation -> MVRoom denoising -> Depth prediction -> Consistency check -> Point cloud update -> 3D Gaussian splatting
- Critical path: Layout encoding (0.3s) -> Epipolar mask precomputation (2.2s) -> MVRoom denoising -> Depth prediction -> Consistency check -> Point cloud update. The epipolar mask computation and depth consistency check are the bottlenecks for iterative generation.
- Design tradeoffs:
  - **Fixed N=4 views per batch**: Improves consistency within batch but requires more iterations for full scene coverage; increasing N would raise memory costs.
  - **Layout-dependent camera trajectories**: Ensures coverage but constrains exploration; may miss unexpected scene elements not in layout.
  - **Depth threshold (0.02m) for point cloud update**: Tight threshold maintains consistency but may reject valid variations; loose threshold admits drift.
- Failure signatures:
  - **Flickering or geometry shifts across batches**: Likely depth consistency check failing (threshold too loose or depth estimator inaccurate).
  - **Objects appearing in wrong positions**: Layout encoding error (incorrect bounding box projection or spatial embedding).
  - **Blurred or inconsistent textures across views**: Epipolar attention not finding correct correspondences (check epipolar line computation, layout mask).
- First 3 experiments:
  1. **Ablate attention mechanism**: Replace LA-epipolar with standard cross-attention; expect SSIM drop similar to Table 1 (0.8154 → ~0.74-0.79). This validates the core attention contribution.
  2. **Vary depth consistency threshold**: Test thresholds from 0.01m to 0.1m on validation scenes; measure accumulated point cloud size and final 3D-GS reconstruction quality. This calibrates the tradeoff between consistency and coverage.
  3. **Test with noisy layouts**: Add perturbations to bounding box positions (±10-20cm) and rotations (±5-10°) before generation; measure SSIM degradation. This assesses robustness to layout input errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the recursive generation framework maintain geometric consistency and layout alignment when conditioned on real-world images rather than synthetic or text-generated inputs?
- Basis in paper: [explicit] The authors state a limitation is that the framework only supports rendered or text-generated images and suggest extending to real-world scenes as a practical application.
- Why unresolved: The model is trained exclusively on the synthetic 3D-FRONT dataset, creating a domain gap that likely affects the depth alignment and warping mechanisms required for real-world data.
- What evidence would resolve it: Quantitative evaluation on real-world indoor datasets (e.g., ScanNet++), comparing the layout plausibility and texture consistency of generated views against ground truths.

### Open Question 2
- Question: Does replacing the static Stable Diffusion backbone with a video diffusion foundation model improve multi-view consistency when guided by 3D layout priors?
- Basis in paper: [explicit] The authors note their model is constrained by the SD foundation and suggest future work should explore integrating layout priors into video foundation models which exhibit stronger temporal consistency.
- Why unresolved: Integrating explicit geometric constraints (like layout-aware epipolar attention) into the temporal attention mechanisms of video diffusion models is architecturally complex and untested in this work.
- What evidence would resolve it: Ablation studies comparing the current architecture against a version fine-tuned from a video diffusion model (e.g., SVD), measuring 3D consistency metrics like cross-view SSIM.

### Open Question 3
- Question: How robust is the layout-aware epipolar attention mechanism when processing scenes with highly cluttered or occluded layouts that deviate from the clean bounding boxes in 3D-FRONT?
- Basis in paper: [inferred] The method is trained on 3D-FRONT, which contains "professionally designed" rooms, and the pipeline filters for "reasonable layouts," implying potential brittleness with messy or irregular real-world arrangements.
- Why unresolved: The epipolar attention relies on intersecting rays with layout bounding boxes; heavy clutter could obscure these intersections or confuse the spatial embeddings used for feature aggregation.
- What evidence would resolve it: Stress-testing the model on synthetic scenes with randomized, high-density object placement and measuring the degradation in layout plausibility scores.

## Limitations

- Cannot generate novel object placements beyond provided layout boundaries
- Computational cost requires 24 GPUs for training multi-view diffusion
- Evaluation limited to single synthetic dataset (3D-FRONT)
- Recursive framework complexity may not scale to larger or more complex scenes

## Confidence

**High Confidence:**
- 12.5% SSIM improvement over MVDiffusion baseline (well-supported by Table 1)
- Architecture design choices (layout-aware epipolar attention, multi-layer conditions, recursive framework) are clearly specified

**Medium Confidence:**
- User study results showing superior perceptual quality (PQ, 3DC, LP) depend on subjective judgments
- "State-of-the-art" claim relative to tested baselines may not hold against all competing methods

**Low Confidence:**
- Text-to-scene generation capability demonstrated but not quantitatively benchmarked
- 3D Gaussian splatting reconstruction quality depends on external optimization tools

## Next Checks

1. **Ablate attention mechanism**: Replace the layout-aware epipolar attention with standard cross-attention in the MVRoom pipeline and measure the resulting SSIM degradation. This should confirm whether the attention mechanism contributes the claimed 0.0077 SSIM improvement over plain epipolar attention.

2. **Depth consistency threshold calibration**: Systematically vary the depth consistency threshold from 0.01m to 0.1m on validation scenes and measure both the accumulated point cloud size and final 3D-GS reconstruction quality. This will calibrate the tradeoff between consistency and coverage.

3. **Layout robustness testing**: Add controlled perturbations (±10-20cm position, ±5-10° rotation) to bounding box inputs and measure SSIM degradation. This will assess the method's robustness to layout input errors and establish error bounds for practical deployment.