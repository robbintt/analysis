---
ver: rpa2
title: 'Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large
  Language Models'
arxiv_id: '2505.17051'
source_url: https://arxiv.org/abs/2505.17051
tags:
- user
- arxiv
- personalization
- language
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present Embedding-to-Prefix (E2P), a method to inject
  pre-computed user embeddings into frozen LLMs by projecting them into a single soft
  token prefix. This allows parameter-efficient personalization without fine-tuning
  the base model.
---

# Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models

## Quick Facts
- **arXiv ID:** 2505.17051
- **Source URL:** https://arxiv.org/abs/2505.17051
- **Reference count:** 40
- **Primary result:** Embedding-to-Prefix (E2P) achieves +14.3% ROUGE-L on headline generation and +12.9% engagement on music rec by projecting user embeddings into a single soft prefix token.

## Executive Summary
Embedding-to-Prefix (E2P) is a parameter-efficient personalization method that injects pre-computed user embeddings into frozen large language models (LLMs) by projecting them into a single soft token prefix. This approach enables personalized generation without fine-tuning the base model, reducing computational overhead while preserving the LLM's generalization capabilities. Evaluated across dialogue generation, headline generation, and large-scale music/podcast recommendation tasks, E2P consistently outperforms non-personalized baselines and achieves strong gains with minimal modifications.

## Method Summary
E2P uses a two-layer MLP to project user embeddings into a soft token prefix, which is prepended to the LLM's input embeddings at the first layer. The projection module (ϕ(c) = LayerNorm(ReLU(W1·c))W2 + b) maps user embedding c to a prefix p of dimension equal to the LLM's hidden size. During training, only the projection weights are updated while the LLM remains frozen. The method is evaluated on Persona-Chat for dialogue, PENS for headline generation, and proprietary music/podcast datasets with behavioral embeddings, using perplexity, ROUGE scores, and engagement proxy metrics.

## Key Results
- **Dialogue Generation:** +2.3 perplexity reduction on Persona-Chat vs. No Context baseline.
- **Headline Generation:** +14.3% ROUGE-L improvement on PENS dataset.
- **Music Recommendation:** +12.9% engagement proxy improvement on proprietary dataset.

## Why This Works (Mechanism)
E2P works by leveraging the LLM's attention mechanism to integrate user-specific context through a single soft prefix token. The projection module transforms user embeddings into a dense representation that the LLM can attend to throughout the generation process. This approach preserves the frozen model's capabilities while adding personalization with minimal parameters, avoiding catastrophic forgetting and enabling efficient adaptation to new users.

## Foundational Learning

**Soft Token Prefix:** A learnable embedding prepended to the input sequence that the LLM attends to via self-attention. *Why needed:* Enables user-specific conditioning without modifying the backbone. *Quick check:* Verify prefix dimension matches hidden size.

**Parameter-Efficient Fine-Tuning:** Methods that adapt LLMs with minimal parameter updates (e.g., adapters, LoRA). *Why needed:* Reduces computational cost and prevents forgetting. *Quick check:* Count trainable parameters in projection vs. full model.

**User Embedding Projection:** Mapping user representations to model-compatible space via MLP. *Why needed:* Bridges semantic gap between user profiles and model input. *Quick check:* Visualize projected embeddings for different user types.

## Architecture Onboarding

**Component Map:** User Embedding → Projection MLP → Soft Prefix → Frozen LLM Input Layer → Generation

**Critical Path:** User embedding → projection → prefix → LLM attention → output

**Design Tradeoffs:** Single soft prefix vs. multiple tokens (simplicity vs. capacity), frozen LLM vs. partial fine-tuning (efficiency vs. performance), projection depth (expressiveness vs. overfitting).

**Failure Signatures:** E2P-Random outperforms E2P (projection not learning), no improvement over No Context baseline (user embeddings lack signal), training instability (learning rate too high).

**First Experiments:**
1. Train projection on Persona-Chat with lr=5e-6, batch=64, 3 epochs; compare perplexity to No Context baseline.
2. Visualize user embedding clusters to verify meaningful differentiation before projection.
3. Test different projection hidden dimensions (512, 768, 3072) to find optimal capacity.

## Open Questions the Paper Calls Out
None specified.

## Limitations
- **Projection Capacity Unknown:** Optimal hidden dimension for projection MLP not specified, requiring empirical tuning.
- **Embedding Extraction Ambiguity:** "Last hidden state" for public datasets unclear (mean pooling vs. final token).
- **Proprietary Data Dependency:** Music/podcast results rely on datasets not publicly available.
- **Offline Evaluation Gap:** Engagement predictor architecture and KTO loss α weighting not detailed.

## Confidence
- **High:** E2P's conceptual framework (project user embeddings to soft prefix) is clear and implementable.
- **Medium:** Public dataset results (Persona-Chat, PENS) are reproducible with reasonable assumptions.
- **Low:** Proprietary dataset results and offline evaluation metrics depend on unspecified components.

## Next Checks
1. **Projection Capacity:** Test multiple hidden dimensions (e.g., 512, 768, 3072) to identify optimal capacity for user embedding projection.
2. **Embedding Quality:** Compare performance using mean-pooled vs. final-token embeddings to validate extraction method impact.
3. **Generalization:** Apply E2P to a new public dataset (e.g., Wizard of Wikipedia) to test robustness beyond reported tasks.