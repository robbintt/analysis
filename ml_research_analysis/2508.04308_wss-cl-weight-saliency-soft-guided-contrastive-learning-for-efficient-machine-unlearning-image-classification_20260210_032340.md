---
ver: rpa2
title: 'WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine
  Unlearning Image Classification'
arxiv_id: '2508.04308'
source_url: https://arxiv.org/abs/2508.04308
tags:
- unlearning
- forgetting
- learning
- saliency
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of machine unlearning in image
  classification, aiming to efficiently remove the influence of specific data from
  trained models. The authors propose a two-phase method called WSS-CL (Weight Saliency
  Soft-Guided Contrastive Learning) that leverages weight saliency to focus unlearning
  on critical model parameters.
---

# WSS-CL: Weight Saliency Soft-Guided Contrastive Learning for Efficient Machine Unlearning Image Classification

## Quick Facts
- arXiv ID: 2508.04308
- Source URL: https://arxiv.org/abs/2508.04308
- Reference count: 31
- Primary result: Achieves unlearning accuracy of 4.51 with remaining accuracy of 99.39 and test accuracy of 93.32 on CIFAR-10 with 10% random data forgetting, while maintaining negligible performance loss compared to retraining.

## Executive Summary
This paper addresses the problem of efficient machine unlearning in image classification, proposing a two-phase method called WSS-CL (Weight Saliency Soft-Guided Contrastive Learning) that leverages weight saliency to focus unlearning on critical model parameters. The approach consists of a forgetting phase that maximizes KL divergence between output logits and uniform pseudo-labels, followed by an adversarial fine-tuning phase using contrastive learning to maximize the distance between forgotten and retained data samples in feature space. Experiments on CIFAR-10 and CIFAR-100 datasets with ResNet-18 models demonstrate significantly improved unlearning efficacy with negligible performance loss compared to state-of-the-art approaches.

## Method Summary
WSS-CL is a two-phase machine unlearning method for image classification. Phase 1 employs KL divergence maximization between model logits and uniform pseudo-labels on the forgetting set to efficiently remove the model's knowledge of specific data. Phase 2 uses adversarial fine-tuning with contrastive learning where forgetting samples and their augmentations form positive pairs while retained samples serve as negatives, combined with cross-entropy loss on retained data. The method incorporates soft weight saliency masking to focus unlearning on critical parameters. Experiments on CIFAR-10 and CIFAR-100 with ResNet-18 architectures show that WSS-CL achieves significantly improved unlearning efficacy with negligible performance loss compared to state-of-the-art approaches.

## Key Results
- Achieves unlearning accuracy of 4.51, remaining accuracy of 99.39, and test accuracy of 93.32 on CIFAR-10 with 10% random data forgetting
- Maintains average gap of 1.54 compared to retraining baseline for 10% random forgetting scenario
- Demonstrates effectiveness across different unlearning scenarios including class-specific and random data deletion
- Shows improved runtime efficiency compared to traditional retraining approaches

## Why This Works (Mechanism)
The method works by first using KL divergence maximization to efficiently forget specific data through uniform output distribution, then applying contrastive learning to create feature space separation between forgotten and retained samples. The soft saliency masking focuses gradient updates on critical parameters, improving efficiency. The two-phase approach allows for rapid initial forgetting followed by fine-tuning that preserves retained data performance while ensuring complete unlearning.

## Foundational Learning
- **KL Divergence Maximization**: Why needed - to efficiently push model outputs toward uniform distribution for forgetting; Quick check - verify model outputs approach 1/K uniform distribution on forgetting set
- **Contrastive Learning**: Why needed - to maximize feature space separation between forgotten and retained samples; Quick check - measure cosine similarity between forgetting and retention sample embeddings
- **Weight Saliency Masking**: Why needed - to focus computational resources on parameters most influential for forgetting; Quick check - analyze gradient magnitude distribution across layers
- **Adversarial Fine-tuning**: Why needed - to ensure robust forgetting while maintaining retention performance; Quick check - test model robustness to adversarial examples in forgetting set
- **Two-Phase Architecture**: Why needed - to separate efficient forgetting from performance-preserving fine-tuning; Quick check - compare single-phase vs two-phase performance
- **Soft Masking**: Why needed - to enable smooth gradient flow while focusing on critical parameters; Quick check - visualize mask values across training epochs

## Architecture Onboarding
- **Component Map**: Baseline ResNet-18 -> Phase 1 (KL Loss) -> Phase 2 (Contrastive + CE Loss with Saliency Masking) -> Final Model
- **Critical Path**: Forgetting set D_f + retention set D_r -> Phase 1 KL optimization -> saliency mask computation -> Phase 2 contrastive learning + cross-entropy -> validated model
- **Design Tradeoffs**: Speed vs completeness (two phases vs single phase), efficiency vs accuracy (saliency masking vs full gradient), simplicity vs effectiveness (uniform vs class-specific forgetting)
- **Failure Signatures**: High UA (model retains forgetting data), low RA (model forgets retention data), high MIA (model susceptible to membership inference), large Avg. Gap (inefficient compared to retraining)
- **Three First Experiments**: 1) Train baseline ResNet-18 on full CIFAR-10 to convergence; 2) Implement and test Phase 1 KL divergence maximization on 10% forgetting set; 3) Apply Phase 2 contrastive learning with soft saliency masking and measure performance metrics

## Open Questions the Paper Calls Out
- Can WSS-CL architecture be effectively adapted to prevent generation of harmful images in generative models like Stable Diffusion?
- Does performance scale effectively to high-resolution datasets and complex model architectures beyond ResNet-18?
- How robust is soft saliency masking against extreme data imbalance or adversarial manipulation of forgetting set?

## Limitations
- Requires specific hyperparameters (learning rates, epochs, batch sizes) that are not fully specified in the paper
- Limited experimental scope to ResNet-18 and CIFAR datasets, lacking validation on larger architectures and datasets
- Does not analyze robustness against adversarial manipulation of the forgetting set or extreme imbalance scenarios

## Confidence
- High: The two-phase architecture (KL-divergence forgetting + adversarial contrastive fine-tuning) is technically sound and aligns with current machine unlearning literature
- Medium: The soft saliency masking approach is plausible but requires validation that it meaningfully improves efficiency
- Low: The specific quantitative results cannot be fully verified without exact implementation details

## Next Checks
1. Reimplement Phase 1 with multiple learning rate and epoch combinations to find minimum needed to achieve near-random output on forgetting set
2. Test Phase 2 with different augmentations (flips, crops, color jitter) and contrast weighting schemes to isolate their impact on retention accuracy
3. Compare WSS-CL against simpler ablation (Phase 1 only, or Phase 2 without saliency masking) to quantify contribution of each component to reported performance gains