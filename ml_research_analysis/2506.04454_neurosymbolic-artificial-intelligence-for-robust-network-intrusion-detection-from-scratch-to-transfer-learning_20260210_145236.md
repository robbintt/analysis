---
ver: rpa2
title: 'Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection:
  From Scratch to Transfer Learning'
arxiv_id: '2506.04454'
source_url: https://arxiv.org/abs/2506.04454
tags:
- learning
- transfer
- detection
- dataset
- odxu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ODXU, a neurosymbolic AI framework that combines
  deep embedded clustering with XGBoost for network intrusion detection. The system
  achieves state-of-the-art performance on the CIC-IDS-2017 dataset, with 96.9% multiclass
  accuracy and a 50% reduction in false omission rate compared to neural baselines.
---

# Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning

## Quick Facts
- arXiv ID: 2506.04454
- Source URL: https://arxiv.org/abs/2506.04454
- Authors: Huynh T. T. Tran; Jacob Sander; Achraf Cohen; Brian Jalaian; Nathaniel D. Bastian
- Reference count: 35
- One-line primary result: ODXU achieves 96.9% multiclass accuracy with 50% reduction in false omission rate compared to neural baselines.

## Executive Summary
This paper introduces ODXU, a neurosymbolic AI framework that combines deep embedded clustering with XGBoost for network intrusion detection. The system achieves state-of-the-art performance on the CIC-IDS-2017 dataset, with 96.9% multiclass accuracy and a 50% reduction in false omission rate compared to neural baselines. The authors introduce a transfer learning strategy enabling adaptation to new datasets, demonstrated on ACI-IoT-2023 where ODXU outperforms neural models with only 50% of training data. The work integrates five uncertainty quantification methods, finding metamodel-based approaches (particularly MetaUQSHAP) superior for detecting misclassifications and unknown attacks. Early stopping optimization further improves training efficiency. This represents the first systematic transfer learning approach for neurosymbolic NIDS systems, addressing scalability and generalization challenges in cybersecurity applications.

## Method Summary
ODXU uses a neurosymbolic architecture where a Deep Embedded Clustering (DEC) module compresses raw packet payloads into 12-dimensional latent representations, which are then classified by an XGBoost model. The DEC component consists of a stacked denoising autoencoder trained for reconstruction, followed by a clustering layer optimized with a combined loss function. Transfer learning is implemented by freezing the pre-trained autoencoder and retraining the clustering layer and classifier on new datasets. Five uncertainty quantification methods are integrated, with metamodel-based approaches (MetaUQSHAP) showing superior performance for detecting misclassifications and unknown attacks. Early stopping with specific thresholds optimizes training efficiency.

## Key Results
- Achieves 96.9% multiclass accuracy on CIC-IDS-2017 with 50% reduction in false omission rate compared to neural baselines
- Transfer learning strategy enables adaptation to ACI-IoT-2023 with only 50% training data while outperforming neural models
- MetaUQSHAP metamodel-based approach achieves highest AUROC (.808) for misclassification detection
- Early stopping optimization improves training efficiency while maintaining accuracy
- System demonstrates superior open set recognition capabilities for detecting unknown attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling feature extraction from classification via a Neurosymbolic architecture reduces critical security errors (False Omission Rate) compared to end-to-end neural baselines.
- **Mechanism:** The system uses a Deep Embedded Clustering (DEC) module (neural) to compress raw packet payloads into a 12-dimensional latent space. An XGBoost classifier (symbolic) then operates on this compressed representation. This separation appears to regularize the learning, preventing the overfitting that leads to high false omission rates in purely neural models (FcNN/1D-CNN).
- **Core assumption:** The 12-dimensional latent space preserves sufficient class-discriminatory information for the tree-based classifier to separate benign from malicious traffic.
- **Evidence anchors:** [abstract] "...50% reduction in false omission rate compared to neural baselines." [section 5.1] "ODXU achieves a FOR of just .092, a reduction of over 50% compared to FcNN (.253)..."

### Mechanism 2
- **Claim:** Freezing the autoencoder while retraining the clustering layer and fine-tuning the classifier enables efficient transfer learning to new threat landscapes with limited data.
- **Mechanism:** The transfer strategy (Case 6) retains the "general vision" of the pre-trained autoencoder (trained on CIC-IDS-2017) to extract features from a new domain (ACI-IoT-2023). By retraining the clustering module and fine-tuning XGBoost, the system adapts the decision boundaries without destroying the learned feature representations.
- **Core assumption:** The source domain (CIC-IDS-2017) contains foundational packet structures that are transferable to the target domain (ACI-IoT-2023).
- **Evidence anchors:** [abstract] "...demonstrated on ACI-IoT-2023 where ODXU outperforms neural models with only 50% of training data." [section 5.2] "...optimal transfer configuration involves reusing the pre-trained autoencoder..."

### Mechanism 3
- **Claim:** Augmenting uncertainty estimation with Shapley values (MetaUQSHAP) improves the detection of novel ("unknown") attacks compared to simple probability entropy.
- **Mechanism:** Instead of relying solely on classifier confidence (probability scores), a metamodel analyzes the internal reasoning (SHAP values) of the XGBoost classifier. If the features driving a prediction are inconsistent with known class patterns, the metamodel assigns a high uncertainty score, flagging it as a potential unknown attack.
- **Core assumption:** Misclassifications and unknown attacks exhibit distinct, detectable patterns in feature attribution (SHAP) space that differ from correct predictions.
- **Evidence anchors:** [abstract] "...finding metamodel-based approaches (particularly MetaUQSHAP) superior for detecting misclassifications and unknown attacks." [section 5.3] "On ACI-IoT-2023, [MetaUQSHAP] achieves a TP rate of .590..."

## Foundational Learning

- **Concept: Open Set Recognition (OSR)**
  - **Why needed here:** Unlike standard classification, OSR assumes the system will encounter classes (attacks) during inference that were not present during training. You must understand this to interpret the UQ results.
  - **Quick check question:** How does the evaluation metric "TP@TN=.95" differ from standard accuracy in the context of detecting unknown attacks?

- **Concept: Deep Embedded Clustering (DEC)**
  - **Why needed here:** This is the bridge between raw bytes and symbolic logic. You need to understand how the autoencoder learns representations that are simultaneously good for reconstruction and clustering.
  - **Quick check question:** Why does the DEC objective function (Eq. 1) add contrastive loss and cross-entropy loss to the standard KL divergence?

- **Concept: Metamodel-based Uncertainty Quantification**
  - **Why needed here:** The paper argues standard confidence scores are insufficient. Understanding how a secondary model learns to predict the error of the primary model is key to the ODXU framework.
  - **Quick check question:** In the MetaUQ formulation (Eq. 3), what serves as the ground truth label ($y_m$) for training the metamodel?

## Architecture Onboarding

- **Component map:** PCAP -> Payload-Byte (1500-byte vector) -> Autoencoder (Encoder compresses to 12 latent dims) + Clustering Layer (Soft assignments) -> XGBoost (Takes 12 latent dims, outputs Class) -> Metamodel (Takes Latent dims + Class Probs + SHAP, outputs Certainty Score)

- **Critical path:** The `Payload-Byte` preprocessing is the strict dependency. Without converting PCAPs to the fixed 1500-byte format (discarding headers), the pre-trained weights cannot be loaded.

- **Design tradeoffs:**
  - **Efficiency vs. Accuracy (Early Stopping):** Experiment 1 is 2x faster than Experiment 6 but sacrifices ~0.2% accuracy. In 24/7 SOC environments, Experiment 1 may be preferred.
  - **Freeze vs. Fine-tune:** Freezing the AE (Case 6) allows rapid adaptation with 50% data but may cap maximum accuracy compared to a full retrain if sufficient data is available.

- **Failure signatures:**
  - **High False Omission Rate (FOR):** If FOR spikes (>0.20), the DEC latent space is likely collapsing (poor separation of attacks from benign), requiring AE retraining.
  - **Low Transfer Performance:** If Case 6 performs worse than training from scratch (Case 3/4), the source domain features are acting as "negative transfer."

- **First 3 experiments:**
  1. **Baseline Reproduction:** Train FcNN, 1D-CNN, and ODXU from scratch on CIC-IDS-2017. Verify the 50% reduction in False Omission Rate (FOR).
  2. **Transfer Ablation:** Take the pre-trained ODXU and apply to ACI-IoT-2023 using Case 2 (Fine-tune AE) vs Case 6 (Freeze AE). Confirm Case 6 superiority with 50% data.
  3. **UQ Stress Test:** Inject a held-out attack class (e.g., Slowloris) during inference. Compare AUROC of MetaUQSHAP vs. Shannon Entropy to verify the "unknown attack" detection capability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the ODXU transfer learning strategy improve performance on out-of-distribution (OOD) detection tasks compared to models trained from scratch?
- **Basis in paper:** [explicit] The authors state, "In future work... we will also explore out-of-distribution detection tasks to further validate the robustness and generalizability of the proposed transfer learning approach."
- **Why unresolved:** The current study validates transfer learning on the ACI-IoT-2023 dataset for standard classification and Open Set Recognition (OSR), but does not explicitly evaluate OOD detection capabilities in the transfer setting.
- **What evidence would resolve it:** Experimental results showing AUROC and detection rates for OOD samples when the pre-trained ODXU model is transferred to a target domain with distinct distribution shifts.

### Open Question 2
- **Question:** Can the specific characteristics of a dataset predict which Uncertainty Quantification (UQ) method (e.g., MetaUQSHAP vs. MetaUQIG) will yield optimal results?
- **Basis in paper:** [inferred] The authors observe that "the optimal metamodel varies depending on the dataset and evaluation objective," noting that MetaUQIG outperforms MetaUQSHAP for misclassification on ACI-IoT-2023, while MetaUQSHAP leads in OSR tasks.
- **Why unresolved:** The paper identifies the variance in performance but does not provide a theoretical or empirical framework for selecting the best UQ method a priori for a new cybersecurity dataset.
- **What evidence would resolve it:** A comparative analysis identifying statistical properties of datasets (e.g., feature correlation, class imbalance) that correlate with the superior performance of specific metamodel-based UQ techniques.

### Open Question 3
- **Question:** How does the ODXU framework perform when applied to multimodal data structures or datasets with significantly different feature representations?
- **Basis in paper:** [explicit] The authors list the "Unified Multimodal Network Intrusion Detection Systems (UM-NIDS) dataset" as a target for future work to extend the framework.
- **Why unresolved:** The current architecture relies on Payload-Byte processing of raw PCAP files; its ability to generalize to datasets that integrate heterogeneous data sources (multimodal) remains unverified.
- **What evidence would resolve it:** Benchmark results on the UM-NIDS dataset, demonstrating whether the neurosymbolic integration of DEC and XGBoost effectively processes multimodal inputs without architectural modification.

## Limitations
- **Model Architecture:** Critical hyperparameters for both the autoencoder (layer sizes, activations) and XGBoost classifier (tree depth, learning rate) are unspecified, creating ambiguity in exact reproduction.
- **Dataset Preprocessing:** While Payload-Byte conversion is specified, the exact balancing ratios and class distribution after downsampling/upsampling are not fully detailed.
- **UQ Implementation:** The specific features and model architecture of the MetaUQSHAP metamodel are not described, limiting full replication of uncertainty results.

## Confidence
- **High Confidence:** The core claim of a 50% reduction in False Omission Rate (FOR) is strongly supported by direct metric comparisons in the CIC-IDS-2017 experiments.
- **Medium Confidence:** The transfer learning mechanism and superiority of Case 6 (Freeze AE) are well-demonstrated on ACI-IoT-2023, but the exact cause of its success over other transfer strategies remains under-specified.
- **Low Confidence:** The claim that MetaUQSHAP is superior for unknown attack detection is based on AUROC comparisons, but the exact definition and simulation of "unknown attacks" during inference is not explicitly detailed in the paper.

## Next Checks
1. **Architecture Verification:** Reproduce the baseline FcNN and 1D-CNN models on CIC-IDS-2017 to confirm the stated FOR reduction (from ~0.25 to ~0.09) before testing ODXU.
2. **Transfer Ablation Study:** Implement all six transfer learning cases (Cases 1-6) on ACI-IoT-2023 and verify that Case 6 (Freeze AE) consistently outperforms Cases 2-5 when trained on only 50% of the data.
3. **UQ Sensitivity Analysis:** Inject a held-out attack class (e.g., Slowloris from CIC-IDS-2017) during ODXU inference and measure the AUROC of MetaUQSHAP vs. Shannon Entropy to validate its superior "unknown attack" detection capability.