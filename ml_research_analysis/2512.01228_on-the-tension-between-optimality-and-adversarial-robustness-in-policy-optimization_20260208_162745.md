---
ver: rpa2
title: On the Tension Between Optimality and Adversarial Robustness in Policy Optimization
arxiv_id: '2512.01228'
source_url: https://arxiv.org/abs/2512.01228
tags:
- arpo
- robust
- policy
- learning
- barpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the tension between achieving optimality and
  adversarial robustness in deep reinforcement learning. It identifies a key gap between
  theory and practice by comparing standard policy optimization (SPO) and adversarially
  robust policy optimization (ARPO), showing that SPO tends to converge to vulnerable
  first-order stationary policies with high natural performance, while ARPO favors
  more robust policies at the cost of reduced returns.
---

# On the Tension Between Optimality and Adversarial Robustness in Policy Optimization

## Quick Facts
- **arXiv ID:** 2512.01228
- **Source URL:** https://arxiv.org/abs/2512.01228
- **Reference count:** 40
- **Primary result:** BARPO consistently outperforms vanilla ARPO, achieving better natural and robust returns by modulating adversary strength.

## Executive Summary
This paper addresses the tension between achieving optimality and adversarial robustness in deep reinforcement learning. It identifies a key gap between theory and practice by comparing standard policy optimization (SPO) and adversarially robust policy optimization (ARPO), showing that SPO tends to converge to vulnerable first-order stationary policies with high natural performance, while ARPO favors more robust policies at the cost of reduced returns. The authors attribute this trade-off to the reshaping effect of the strongest adversaries in ARPO, which introduces numerous sticky local optima that improve robustness but hinder effective policy improvement. To alleviate this, they propose BARPO, a bilevel framework that modulates adversary strength to create smoother optimization landscapes while preserving global optima. Extensive experiments on MuJoCo benchmarks demonstrate that BARPO consistently outperforms vanilla ARPO, achieving better natural and robust returns, effectively bridging the gap between theoretical alignment and practical performance.

## Method Summary
The paper proposes BARPO (Bilevel Adversarially Robust PPO), which modifies the ARPO objective by replacing the inner maximization of worst-case value with a bilevel optimization that maximizes KL divergence between perturbed and original policies. The method uses Stochastic Gradient Langevin Dynamics (SGLD) for 10 steps in the inner loop to find adversarial perturbations that maximize KL divergence, then updates the policy via PPO combined with SPO guidance weighted by coefficient κ. The approach aims to smooth the optimization landscape while preserving global optima, achieving better balance between natural performance and adversarial robustness.

## Key Results
- BARPO achieves consistently higher natural and robust returns than vanilla ARPO across Hopper, Walker2d, HalfCheetah, and Ant environments
- BARPO outperforms both standard PPO (vulnerable to attacks) and vanilla ARPO (low natural returns) on 6 different attack types
- The KL divergence surrogate effectively serves as a first-order approximation for adversarial perturbations while maintaining landscape navigability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ARPO creates "sticky" local optima by reshaping the loss landscape with the strongest adversary.
- Mechanism: The inner maximization of the strongest adversary introduces deep valleys and disconnected regions, trapping gradient descent at suboptimal FOSPs.
- Core assumption: Policy gradient noise is coercive and landscape geometry allows local stability at suboptimal points.
- Evidence: Abstract and Section 3.2 describe landscape fragmentation; Corpus reference to "Optimal Adversarial Robust RL" provides theoretical existence of ORP.

### Mechanism 2
- Claim: BARPO's bilevel framework smooths the landscape by relaxing the inner adversary objective.
- Mechanism: Modulating adversary strength prevents valleys from becoming barriers, allowing gradients to flow toward better optima.
- Core assumption: Global optimum of relaxed bilevel problem aligns with true optimal robust policy.
- Evidence: Abstract and Section 4.1 describe landscape smoothing; weak support from "Smart Surrogate Losses" corpus.

### Mechanism 3
- Claim: KL divergence serves as valid surrogate for strongest adversary.
- Mechanism: Theorem 4.1 bounds robustness degradation by KL divergence scaled by curvature terms.
- Core assumption: Value function is locally smooth and Fisher information matrix is well-behaved.
- Evidence: Section 4.2 states KL serves as first-order surrogate; Algorithm 3 implements SGLD for KL maximization.

## Foundational Learning

- **Concept:** State-Adversarial MDP (SA-MDP) and ISA-MDP
  - **Why needed here:** Framework where state observations are perturbed by adversary ν, requiring understanding that agent acts on s' = ν(s).
  - **Quick check question:** In SA-MDP, does adversary perturb environment dynamics or agent's observation? (Answer: Observation).

- **Concept:** First-Order Stationary Policies (FOSPs)
  - **Why needed here:** Core tension involves SPO converging to vulnerable FOSPs while ARPO converges to robust but low-return FOSPs.
  - **Quick check question:** Why does a FOSP imply policy has stopped improving even if not optimal? (Answer: Gradient is zero/no signal).

- **Concept:** Bilevel Optimization
  - **Why needed here:** BARPO changes structure from minimax to bilevel where inner objective is constrained maximization/surrogate.
  - **Quick check question:** In BARPO, what is outer vs inner objective? (Answer: Outer maximizes value; inner maximizes KL divergence surrogate).

## Architecture Onboarding

- **Component map:** State → Adversary (SGLD) → Perturbed State → Policy Network → Action → Value Network → PPO Update
- **Critical path:**
  1. Collect trajectories using current policy
  2. Inner Loop: For each state, run SGLD for 10 steps to find perturbation maximizing KL divergence
  3. Outer Loop: Compute PPO loss using perturbed states with SPO guidance
  4. Update policy and value networks

- **Design tradeoffs:**
  - SPO Guidance (κ): Balance between robustness and natural returns
  - Inner Loop Steps: More steps improve adversary but increase compute
  - Surrogate Choice: KL optimizes navigability vs minimax that optimizes robustness

- **Failure signatures:**
  - ARPO Failure: High natural return but collapses under attack
  - SPO Failure: High natural return but zero robustness (-0.9 to -1.5)
  - BARPO Failure: If κ too low, behaves like SPO; if SGLD step too large, perturbation may exceed constraints

- **First 3 experiments:**
  1. Train SPO and vanilla ARPO to confirm tension (SPO high natural/low robust, ARPO low natural/high robust)
  2. Compare BARPO against minimax inner objective to verify KL surrogate effect
  3. Evaluate BARPO against diverse attacks (Critic, MAD, SA-RL) to test generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can convergence and efficiency of BARPO be improved by utilizing fully first-order gradient method or penalty-based minimax formulation instead of ignoring second-order terms?
- **Basis:** Appendix I states solution process can be improved via advanced optimization on bilevel problem
- **Why unresolved:** Current implementation ignores second-order terms for efficiency
- **What evidence would resolve it:** Empirical comparison against version using fully first-order bilevel solver

### Open Question 2
- **Question:** Does ARPO's convergence to FOSPs and BARPO's improvement hold under generalized smoothness conditions rather than strict Lipschitz continuity?
- **Basis:** Appendix H.1 discusses justifications and potential relaxations of assumptions
- **Why unresolved:** Deep neural networks often violate strict Lipschitz gradient assumptions
- **What evidence would resolve it:** Convergence proof relying on generalized smoothness assumptions

### Open Question 3
- **Question:** How does optimality-robustness tension behave in general settings where ORP doesn't strictly align with Bellman optimal policy?
- **Basis:** Appendix I notes analysis can extend to more general settings
- **Why unresolved:** Current paper relies on ISA-MDP framework where ORP alignment is guaranteed
- **What evidence would resolve it:** Analysis of BARPO performance in standard SA-MDPs with non-aligned optimal policies

## Limitations

- Core assertion about ARPO creating sticky local optima lacks formal landscape analysis or visualization
- Theoretical justification for KL divergence as effective surrogate is first-order bound that may not capture higher-order effects
- Claim that BARPO preserves global optima while smoothing landscape is asserted but not rigorously proven beyond KL bound

## Confidence

- **High confidence:** Empirical observation that SPO achieves high natural but low robust returns while ARPO achieves reverse (Tables 1, 2)
- **Medium confidence:** Mechanism explanation that ARPO's strongest adversary creates disconnected high-value regions
- **Low confidence:** Theoretical alignment claim that BARPO's bilevel relaxation preserves global optima while smoothing landscape

## Next Checks

1. Generate and visualize policy value landscapes along random directions for SPO, ARPO, and BARPO agents to empirically verify fragmentation hypothesis
2. Systematically vary inner SGLD step count and ε budget to determine if BARPO's advantage persists across broader hyperparameter range
3. Evaluate BARPO agents against black-box attacks or attacks with different perturbation budgets than training to test generality of learned robustness