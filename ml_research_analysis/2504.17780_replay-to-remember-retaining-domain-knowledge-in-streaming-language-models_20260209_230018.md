---
ver: rpa2
title: 'Replay to Remember: Retaining Domain Knowledge in Streaming Language Models'
arxiv_id: '2504.17780'
source_url: https://arxiv.org/abs/2504.17780
tags:
- domain
- replay
- forgetting
- similarity
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates catastrophic forgetting in large language
  models during streaming domain adaptation using a lightweight combination of LoRA
  fine-tuning and replay buffers. Experiments across medical, genetic, and legal domains
  show that while models inevitably experience performance degradation on prior tasks,
  even minimal replay significantly mitigates forgetting and enables partial recovery.
---

# Replay to Remember: Retaining Domain Knowledge in Streaming Language Models

## Quick Facts
- arXiv ID: 2504.17780
- Source URL: https://arxiv.org/abs/2504.17780
- Reference count: 9
- Key outcome: LoRA fine-tuning with replay buffers significantly mitigates catastrophic forgetting during streaming domain adaptation, though partial degradation remains inevitable

## Executive Summary
This study investigates catastrophic forgetting in large language models during streaming domain adaptation using a lightweight combination of LoRA fine-tuning and replay buffers. Experiments across medical, genetic, and legal domains show that while models inevitably experience performance degradation on prior tasks, even minimal replay significantly mitigates forgetting and enables partial recovery. Perplexity spikes during forgetting but stabilizes with replay; semantic similarity declines modestly in most domains, with more severe drops in genetics. GPT-4-based human evaluations corroborate these trends, revealing consistent quality retention in law, moderate degradation in medicine, and sharp early declines followed by recovery in genetics. Results validate that LoRA-based adaptation with replay offers a practical, resource-efficient approach for real-time, multi-domain LLM deployment under constrained conditions.

## Method Summary
The study employs LoRA adapters applied to attention layers of a transformer-based LLM, combined with a replay buffer that stores samples from prior domain batches. The model sequentially streams through three domains (medical, genetic, legal), with evaluation after each chunk to track forgetting and recovery. Perplexity measures predictive confidence, cosine similarity assesses semantic drift from baseline responses, and GPT-4 evaluations provide human-like quality ratings. The replay buffer size is fixed proportionally to chunk size, and LoRA parameters remain unspecified beyond targeting attention layers.

## Key Results
- LoRA + replay combination effectively mitigates catastrophic forgetting compared to baseline streaming without replay
- Minimal replay (10-20%) provides sufficient stabilization across all domains
- Perplexity spikes to extreme values (326K) in genetics domain without replay, but stabilizes with even minimal replay
- Semantic similarity recovers partially after replay, suggesting suppressed rather than erased knowledge

## Why This Works (Mechanism)

### Mechanism 1: Gradient Interleaving via Replay
Interleaving historical data samples with new streaming data stabilizes the model's predictive confidence (perplexity) and mitigates the rate of knowledge degradation. The replay buffer injects gradient signals from previous tasks during the update step, preventing the optimizer from moving weights solely toward the minima of the new domain.

### Mechanism 2: Parameter Isolation via LoRA
Freezing the pre-trained model weights and training only low-rank adapter matrices allows the model to adapt to new domains without overwriting the general linguistic capabilities required for all tasks. By restricting updates to a low-rank subspace, the bulk of the model's parameters remain fixed, implicitly preserving general knowledge and structural understanding.

### Mechanism 3: Semantic Drift Recovery
Even after semantic drift occurs, the model retains sufficient latent structure to "recover" performance when re-exposed to domain signals via replay. The initial drop in similarity indicates the model prioritizes the active domain, but because LoRA prevents total weight destruction and replay provides a path back, the model can re-align its output distribution to the original domain semantics.

## Foundational Learning

- **Catastrophic Forgetting**: Neural networks aggressively overwrite weights needed for previous tasks when trained sequentially. Quick check: If I fine-tune a model on Legal data after Medical data, what happens to its ability to answer Medical questions? (Answer: It degrades/forgets)

- **Low-Rank Adaptation (LoRA)**: Only updates AÃ—B matrices rather than full weight matrix W, explaining why the base model stays intact. Quick check: Does LoRA modify the pre-trained weights of the attention layers? (Answer: No, it modifies adapter matrices in parallel)

- **Perplexity vs. Semantic Similarity**: Perplexity measures "confidence" (low is good), while Similarity measures "consistency" with the past (high is good). They can tell different stories. Quick check: If perplexity spikes but semantic similarity stays high, is the model forgetting the concept or just uncertain? (Answer: Likely just uncertain/jittery, but the semantic core remains)

## Architecture Onboarding

- **Component map**: Base LLM (frozen) -> LoRA Adapters (trainable) -> Streaming Controller (batch iterator) -> Replay Buffer (sample storage) -> Evaluation Suite (metrics)

- **Critical path**: 1. Initialize LoRA weights for new domain. 2. Load current batch + sample from Replay Buffer. 3. Compute loss on mixed batch; backpropagate updates only to LoRA adapters. 4. Log Perplexity and Semantic Similarity against Chunk 0 baseline. 5. Update Replay Buffer with current batch samples.

- **Design tradeoffs**: Replay Ratio vs. Compute (higher replay = better retention but slower training), LoRA Rank vs. Plasticity (low rank preserves old knowledge but may fail to learn complex new domains), Metric Sensitivity (Perplexity is noisy while GPT-4 evaluation is expensive)

- **Failure signatures**: Runaway Perplexity (spikes >100K indicate insufficient replay or learning rate too high), Semantic Collapse (similarity <0.7 correlates with hallucination), Stability Trap (no fluctuation in Law while Genetics crashes suggests over-reliance on base pre-training)

- **First 3 experiments**: 1. Baseline Streaming (no replay to establish forgetting floor), 2. Ablation on Buffer Size (1%, 5%, 10% replay ratios to visualize stabilization curve), 3. Domain Re-entry Test (re-introduce MedQuAD chunk after Law to measure Recovery Speed)

## Open Questions the Paper Calls Out

1. **Sample-prioritized replay strategies**: Can reservoir sampling, adaptive replay, or domain-aware rehearsal significantly outperform fixed-size replay in mitigating catastrophic forgetting?

2. **Domain-specific mechanisms**: What mechanisms could better preserve knowledge in highly specialized domains like genetics that exhibit severe degradation under standard replay?

3. **Multi-adapter fusion**: Can task-specific LoRA routing enable simultaneous preservation of multiple domain competencies without interference in streaming settings?

4. **Evaluation metric alignment**: How can evaluation metrics be improved to better align semantic similarity with factual correctness and human judgment?

## Limitations

- Base LLM architecture and size remain unspecified, making exact replication difficult
- LoRA hyperparameters and replay buffer size ratio are not detailed
- Three domains chosen may not generalize to domains with different characteristics
- Results show replay mitigates but doesn't eliminate forgetting, particularly in genetics domain

## Confidence

- **High Confidence**: LoRA + replay buffer combination effectively mitigates catastrophic forgetting compared to baseline streaming without replay
- **Medium Confidence**: Minimal replay (10-20%) provides sufficient stabilization across all domains
- **Low Confidence**: Semantic similarity recovery represents genuine knowledge restoration rather than temporary alignment

## Next Checks

1. **Architecture Scaling Test**: Repeat experiments with different base model sizes (7B, 13B, 34B) to assess scalability limits of the LoRA + replay approach

2. **Buffer Size Sensitivity Analysis**: Systematically vary replay buffer ratios (1%, 5%, 10%, 20%) across all domains to identify optimal trade-offs between retention and computational overhead

3. **Domain Generalization Test**: Apply the same methodology to domains with different characteristics (e.g., code generation, conversational dialogue) to assess generalizability beyond Q&A domains