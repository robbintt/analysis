---
ver: rpa2
title: SPLASH! Sample-efficient Preference-based inverse reinforcement learning for
  Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations
arxiv_id: '2507.08707'
source_url: https://arxiv.org/abs/2507.08707
tags:
- learning
- reward
- demonstrations
- splash
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPLASH introduces a preference-based inverse reinforcement learning
  algorithm that enables learning from suboptimal demonstrations in long-horizon and
  adversarial tasks. The method addresses limitations of existing IRL approaches by
  incorporating options-level demonstrations, downsampled full trajectory comparisons,
  success and progress-based learning constraints, and temporal consistency regularization.
---

# SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations

## Quick Facts
- arXiv ID: 2507.08707
- Source URL: https://arxiv.org/abs/2507.08707
- Reference count: 38
- Primary result: Demonstrates significant improvement in reward learning from suboptimal demonstrations in long-horizon adversarial tasks

## Executive Summary
SPLASH addresses the challenge of learning reward functions from suboptimal hierarchical demonstrations in long-horizon adversarial tasks. The method introduces a preference-based inverse reinforcement learning framework that leverages both task success criteria and progress tracking to improve reward learning. By incorporating options-level demonstrations and downsampled full trajectory comparisons, SPLASH can effectively learn from demonstrations that are not optimal but still contain valuable information about task structure and goals.

The algorithm automatically generates ranked demonstrations through behavioral cloning with noise injection, enabling the learning of reward functions that can extrapolate beyond the demonstrator's performance. This approach is particularly valuable for complex tasks like maritime capture-the-flag scenarios, where optimal demonstrations may be difficult or impossible to obtain. SPLASH's ability to transfer learned policies from simulation to real-world autonomous unmanned surface vehicles demonstrates its practical applicability for real-world robotic systems.

## Method Summary
SPLASH employs a preference-based inverse reinforcement learning framework that addresses the limitations of traditional IRL approaches when dealing with suboptimal demonstrations. The method operates by collecting hierarchical demonstrations at the options level and comparing downsampled trajectory pairs to generate preference labels. These preferences are used to train a reward function that incorporates temporal consistency regularization and accounts for both task success criteria and progress tracking.

The algorithm generates ranked demonstrations automatically by applying noise injection to an initial behavioral cloning policy, creating a spectrum of performance levels from which preferences can be learned. This approach allows SPLASH to learn reward functions that capture the underlying task structure even when demonstrations are suboptimal. The learned rewards are then used to train policies through reinforcement learning, with the entire system validated through both simulation and real-world experiments on autonomous unmanned surface vehicles.

## Key Results
- SPLASH significantly outperforms state-of-the-art methods in reward learning from suboptimal demonstrations
- Demonstrated ability to extrapolate beyond demonstrator performance in maritime capture-the-flag tasks
- Successfully transfers learned policies from simulation to real-world autonomous unmanned surface vehicles

## Why This Works (Mechanism)
The effectiveness of SPLASH stems from its ability to extract meaningful reward information from suboptimal demonstrations by leveraging preference-based learning at the options level. By downsampling full trajectories and comparing them pairwise, the algorithm can identify which segments of demonstrations are more successful, even when the overall demonstration is suboptimal. The incorporation of success criteria and progress tracking provides additional signal that helps the reward function capture the true task objectives.

The temporal consistency regularization ensures that the learned reward function maintains coherence over time, preventing erratic behavior that could arise from noisy preference labels. The automatic generation of ranked demonstrations through noise injection allows the system to create a diverse set of examples that span the performance spectrum, enabling the reward function to generalize beyond the quality of the original demonstrations.

## Foundational Learning
- Preference-based learning: needed for extracting reward information from relative comparisons; quick check: verify preference labels align with intuitive task success
- Hierarchical reinforcement learning: needed for handling long-horizon tasks through temporal abstraction; quick check: confirm options capture meaningful sub-tasks
- Behavioral cloning with noise injection: needed for generating diverse demonstration ranks; quick check: ensure injected noise creates distinct performance levels
- Temporal consistency regularization: needed for maintaining reward function coherence; quick check: verify reward smoothness across time steps
- Progress tracking: needed for capturing intermediate task objectives; quick check: confirm progress metrics align with task structure

## Architecture Onboarding

Component map: Behavioral cloning -> Noise injection -> Preference generation -> Reward learning -> Policy optimization -> Real-world deployment

Critical path: The most critical component is the preference generation system, which transforms suboptimal demonstrations into meaningful reward signals. This involves the interaction between the hierarchical demonstration collection, downsampling mechanism, and preference comparison framework.

Design tradeoffs: The primary tradeoff involves the granularity of downsampling versus computational efficiency. Finer downsampling provides more precise preference information but increases computational cost. The noise injection level must balance between generating diverse ranks and maintaining task feasibility.

Failure signatures: Common failure modes include reward function collapse to trivial solutions, preference label noise overwhelming the learning signal, and poor transfer from simulation to real-world due to sim-to-real gaps. These typically manifest as policies that optimize for proxy objectives rather than true task completion.

First experiments: 1) Validate preference generation on simple tasks with known optimal policies; 2) Test reward learning convergence with varying levels of demonstration quality; 3) Evaluate sim-to-real transfer on a simplified version of the target task.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance in highly dynamic environments with frequent state changes remains unverified
- Computational complexity may become prohibitive in tasks with extremely long horizons
- Reliance on behavioral cloning with noise injection assumes access to reasonable initial policy

## Confidence

High confidence: The algorithmic framework and its theoretical foundations are well-established, with clear mathematical formulations for preference-based learning and temporal consistency regularization.

Medium confidence: The experimental results demonstrating improved performance over state-of-the-art methods are convincing within the specific maritime capture-the-flag task, but broader validation across different domains is needed.

Low confidence: The extrapolation capability beyond demonstrator performance, while promising, requires more extensive testing across diverse task types and complexity levels.

## Next Checks

1. Test the algorithm's performance in environments with significantly higher state space dimensionality and more complex hierarchical structures than the maritime scenario.

2. Evaluate the method's robustness when the initial behavioral cloning policy has substantial performance gaps or when demonstrator quality varies widely.

3. Assess the transfer learning capabilities across fundamentally different task domains (e.g., from maritime to aerial or ground robotics) while maintaining performance improvements.