---
ver: rpa2
title: 'Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic
  and Sentiment Classification'
arxiv_id: '2508.03181'
source_url: https://arxiv.org/abs/2508.03181
tags:
- sentiment
- political
- topic
- speeches
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study applies machine learning to analyze political discourse
  in the German Bundestag, using topic and sentiment classification on ~28,000 parliamentary
  speeches from 2019-2024. The authors develop and evaluate two models: a topic classifier
  achieving an AUROC of 0.94 (average across topics) and a sentiment classifier reaching
  0.89 AUROC.'
---

# Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification

## Quick Facts
- arXiv ID: 2508.03181
- Source URL: https://arxiv.org/abs/2508.03181
- Reference count: 7
- Primary result: Topic classifier achieves AUROC 0.94, sentiment classifier reaches 0.89 AUROC on German Bundestag speeches

## Executive Summary
This study applies machine learning to analyze political discourse in the German Bundestag, using topic and sentiment classification on ~28,000 parliamentary speeches from 2019-2024. The authors develop and evaluate two models: a topic classifier achieving an AUROC of 0.94 (average across topics) and a sentiment classifier reaching 0.89 AUROC. Both models are trained on manually labeled data and successfully capture nuanced political language, including shifts in tone when parties move between government and opposition roles. Analysis reveals that opposition parties express significantly more negative sentiment, and parties tend to be most critical on their core policy areas. The study demonstrates the feasibility of large-scale computational analysis of parliamentary discourse, offering insights into political communication patterns and contributing a reproducible framework for future political science research.

## Method Summary
The authors collected ~28,000 parliamentary speeches from German Bundestag plenary protocols (Oct 2019-Oct 2024) via DIP API. They manually labeled 557 speeches for topic classification (6 categories) and 445 for binary sentiment classification (positive/negative, neutral excluded). For topic classification, they trained custom Word2Vec embeddings on the full corpus and used a Bagging ensemble with SVM base estimator. For sentiment, they employed TF-IDF unigrams with Random Forest. Preprocessing included noise removal, tokenization, lemmatization via SpaCy, and custom German parliamentary stopword lists for topic classification only.

## Key Results
- Topic classification achieved AUROC of 0.94 (average across 6 topic categories)
- Sentiment classification reached AUROC of 0.89 for binary positive/negative classification
- Opposition parties express significantly more negative sentiment than governing parties
- Parties are most critical on their core policy areas (e.g., Greens on environmental issues)
- Cross-linguistic validation showed substantial performance drop on Austrian data (sentiment accuracy fell to 0.49)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific embeddings trained on parliamentary corpus improve topic classification over generic pre-trained alternatives
- Mechanism: Political discourse uses specialized vocabulary and semantic relationships that differ from general language; custom Word2Vec captures these domain-specific patterns through co-occurrence statistics
- Core assumption: Parliamentary language patterns are sufficiently distinct and consistent to benefit from domain adaptation
- Evidence anchors: [section 3.3]: "A custom Word2Vec embedding model was trained on the whole corpus of parliamentary speeches, enabling the capture of domain-specific language patterns. These embeddings outperformed generic pre-trained alternatives."

### Mechanism 2
- Claim: Political parties systematically adjust rhetorical tone based on institutional role (government vs. opposition)
- Mechanism: Governing parties face incentives to defend their record and express support; opposition parties face incentives to criticize and highlight failures
- Core assumption: Role-based institutional incentives are stronger drivers of sentiment than fixed ideological positions alone
- Evidence anchors: [section 4.2]: CDU/CSU "after moving into opposition, this share [of negative speeches] increased sharply" while FDP and Grünen saw their "proportion of critical statements fell markedly" upon joining government

### Mechanism 3
- Claim: Task-specific feature engineering optimizes different classification objectives—TF-IDF suits sentiment; embeddings suit topic classification
- Mechanism: Sentiment often expressed through specific emotive vocabulary (captured by frequency), while topic requires understanding conceptual relationships (captured by semantic embeddings)
- Core assumption: The tradeoff between semantic richness and label complexity differs by task type
- Evidence anchors: [section 3.3]: TF-IDF selected "due to its efficiency and strong performance in text classification tasks with limited label complexity" while topic classification required "more semantically rich representation"

## Foundational Learning

- Concept: **AUROC (Area Under ROC Curve)**
  - Why needed here: Primary evaluation metric across both classifiers; measures separability independent of decision thresholds
  - Quick check question: If your classifier achieves AUROC 0.89, what proportion of randomly sampled positive-negative pairs would it correctly rank?

- Concept: **TF-IDF Vectorization**
  - Why needed here: Feature extraction for sentiment classification; weights terms by frequency within documents and rarity across corpus
  - Quick check question: Why would a word appearing in nearly every parliamentary speech receive a low TF-IDF weight?

- Concept: **Word Embeddings (Word2Vec)**
  - Why needed here: Dense vector representations for topic classification; captures semantic similarity through neighborhood context
  - Quick check question: If "Haushalt" (budget) and "Finanzen" (finances) have high cosine similarity in your embedding space, what does that imply for downstream classification?

## Architecture Onboarding

- Component map: DIP API -> Protocol download (393 protocols) -> Regex-based speech extraction -> Preprocessing (tokenization, lemmatization, stopword removal) -> Manual labeling (557 topic samples, 445 sentiment samples) -> Custom Word2Vec embeddings (topic) / TF-IDF unigrams (sentiment) -> Bagging + SVM (topic) / Random Forest (sentiment) -> Full corpus inference (~28,000 speeches) -> Metadata enrichment -> Trend analysis

- Critical path: Manual labeling quality -> Feature representation choice -> Model selection -> Threshold calibration (critical for domain transfer)

- Design tradeoffs:
  1. Binary sentiment vs. multi-class: Authors excluded neutral speeches; binary classification simplified task but may miss nuance
  2. Custom embeddings vs. pre-trained: Custom embeddings outperformed but require full corpus upfront for training
  3. Single-label vs. multi-label topics: Single-topic assignment caused confusion where speeches span multiple themes

- Failure signatures:
  1. Domain transfer degradation: Sentiment accuracy dropped to 0.49 on Austrian data (Section 4.1) with asymmetric errors—positive speeches misclassified as negative, but not vice versa
  2. Topic boundary confusion: Section 5 notes "overlapping themes and ambiguous phrasing contributed to misclassifications"
  3. Label subjectivity: Manual annotation acknowledged as limitation for speeches with "subtle rhetorical forms"

- First 3 experiments:
  1. Baseline validation: Replicate Table 2 results—train on 80% labeled data, report AUROC/accuracy/F1 on 20% held-out test
  2. Domain transfer test: Apply trained models to Austrian National Council data (80 labeled samples); quantify performance gap and analyze error asymmetry patterns
  3. Feature ablation: Compare custom Word2Vec vs. generic German embeddings on topic classification to validate domain-specific embedding contribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a multi-label classification approach significantly improve performance for speeches containing overlapping thematic areas?
- Basis in paper: [explicit] The authors note that "overlapping themes and ambiguous phrasing contributed to misclassifications," explicitly suggesting "potential gains from multi-label classification"
- Why unresolved: The current study utilized a multi-class architecture where speeches are assigned to a single category, failing to capture the complexity of speeches addressing multiple topics
- What evidence would resolve it: A comparative study benchmarking the F1-score and AUROC of a multi-label model against the current single-label approach on the same test set

### Open Question 2
- Question: Can active learning or semi-supervised techniques effectively scale the training data without compromising the domain-specific accuracy achieved through manual labeling?
- Basis in paper: [explicit] The discussion states that the "size of the labeled training data remains limited" and explicitly proposes "crowdsourcing, active learning, or semi-supervised approaches" for future efforts
- Why unresolved: The current methodology relied on a relatively small manually labeled subset (557 speeches), which potentially constrains robustness for underrepresented topics
- What evidence would resolve it: Experiments demonstrating that models trained on expanded datasets via active learning maintain or exceed the current 0.94 (topic) and 0.89 (sentiment) AUROC scores

### Open Question 3
- Question: Can domain adaptation techniques mitigate the performance drop observed when applying these classifiers to other German-speaking parliamentary corpora?
- Basis in paper: [inferred] In Section 4.1, the external validation on Austrian data showed a sharp decline in sentiment accuracy (0.49) due to a "distinct tone," implying the current model struggles with domain shift
- Why unresolved: The models were trained exclusively on German Bundestag data, capturing specific rhetorical styles that do not fully generalize to sister parliaments
- What evidence would resolve it: Successful fine-tuning or transfer learning application that raises Austrian sentiment classification accuracy closer to the original 0.89 AUROC benchmark

## Limitations

- Manual labeling quality acknowledged as limitation but specific inter-annotator agreement scores were not reported
- Binary sentiment classification excluded neutral speeches, potentially missing important rhetorical nuance
- Cross-linguistic validation revealed substantial performance degradation (sentiment accuracy dropped to 0.49 on Austrian data)

## Confidence

- Topic Classification Performance (AUROC 0.94): High confidence - robust evaluation methodology with multiple metrics and clear feature importance analysis
- Sentiment Classification Performance (AUROC 0.89): Medium confidence - asymmetric error patterns in cross-domain validation suggest potential systematic biases
- Institutional Role Effects: Medium confidence - compelling empirical patterns but lacks causal identification strategy to rule out confounding factors
- Domain-Specific Embeddings Advantage: Low-Medium confidence - demonstrated superiority but no systematic comparison with other domain adaptation approaches

## Next Checks

1. **Replication of Cross-Linguistic Transfer**: Apply the trained German models to additional parliamentary corpora (e.g., Swiss, Austrian) with at least 100 labeled samples each to map the boundaries of domain transfer degradation
2. **Neutral Speech Classification Experiment**: Retrain the sentiment classifier including neutral speeches as a third class to assess whether binary classification oversimplifies the rhetorical landscape
3. **Ablation Study on Embedding Approaches**: Systematically compare custom Word2Vec embeddings against multiple pre-trained alternatives (FastText, GloVe, parliamentary-specific embeddings from other languages) to isolate the contribution of domain adaptation