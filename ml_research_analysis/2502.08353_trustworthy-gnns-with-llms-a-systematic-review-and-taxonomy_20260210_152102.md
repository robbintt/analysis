---
ver: rpa2
title: 'Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy'
arxiv_id: '2502.08353'
source_url: https://arxiv.org/abs/2502.08353
tags:
- graph
- llms
- gnns
- zhang
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a systematic survey of methods that integrate
  large language models (LLMs) with graph neural networks (GNNs) to enhance model
  trustworthiness across four dimensions: reliability, robustness, privacy, and reasoning.
  The authors propose a taxonomy that categorizes these approaches and survey representative
  methods within each category.'
---

# Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy

## Quick Facts
- arXiv ID: 2502.08353
- Source URL: https://arxiv.org/abs/2502.08353
- Reference count: 11
- Primary result: Systematic survey of LLM-GNN integration methods for trustworthiness across reliability, robustness, privacy, and reasoning dimensions

## Executive Summary
This paper provides a systematic survey of methods that integrate large language models (LLMs) with graph neural networks (GNNs) to enhance model trustworthiness across four dimensions: reliability, robustness, privacy, and reasoning. The authors propose a taxonomy that categorizes these approaches and survey representative methods within each category. Key findings include that LLMs can improve GNN reliability by addressing inherent noise through semantic augmentation and structural editing, enhance robustness by identifying malicious edges and recovering missing important edges, and improve explainability through reasoning capabilities. However, current research has limitations, such as heavy reliance on textual attributes and insufficient exploration of privacy protection methods.

## Method Summary
The paper surveys LLM-GNN integration methods through a systematic literature review, focusing on how LLMs enhance GNN trustworthiness. Methods are categorized into two integration paradigms: LLM-as-enhancer for generating embeddings, pseudo-labels, and semantic augmentation, and LLM-as-predictor for direct graph task prediction. The survey covers four trustworthiness dimensions: reliability (handling noise and data quality issues), robustness (defending against adversarial attacks), privacy (protecting sensitive information), and reasoning (improving explainability). Key mechanisms include semantic augmentation through text attribute processing, structural editing based on homophily assumptions, and edge filtering for robustness against attacks.

## Key Results
- LLMs improve GNN reliability by addressing noise through semantic augmentation and structural editing
- LLM-enhanced robustness achieved through identifying malicious edges and recovering missing important edges
- Current research heavily relies on textual attributes, limiting applicability to text-free graphs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can enhance GNN reliability by augmenting semantic information to mitigate inherent noise in graph data.
- Mechanism: LLMs generate node embeddings, virtual nodes, and pseudo-labels that supplement sparse or noisy graph data. This semantic augmentation allows GNNs to better handle structural, attribute, and label noise through zero-shot and few-shot generation capabilities.
- Core assumption: Nodes in the graph possess rich textual attributes that LLMs can process and reason over.
- Evidence anchors:
  - [abstract] "LLMs can improve GNN reliability by addressing inherent noise through semantic augmentation"
  - [section 3.1] "LLMs can assist GNNs in mitigating structural noise in graph data... [by] generating node embeddings, virtual nodes, and pseudo-labels, providing supplementary information for graph data"
  - [corpus] Limited direct corpus support; neighboring papers focus on LLM trustworthiness broadly rather than GNN-specific semantic augmentation mechanisms.
- Break condition: Graphs without textual node attributes or with severely corrupted/missing text will not benefit from this mechanism.

### Mechanism 2
- Claim: LLMs can directly edit graph structures to improve model performance by evaluating node relationships.
- Mechanism: LLMs assess semantic similarity between node pairs to determine whether edges should exist. Based on homophily assumptions, LLMs suggest adding edges between same-class nodes and removing irrelevant connections. This structural editing is guided by LLM reasoning rather than purely statistical learning.
- Core assumption: Homophily principle holds—nodes of the same class benefit from stronger connectivity, and LLMs can accurately infer class relationships from text.
- Evidence anchors:
  - [abstract] "structural editing" identified as key approach
  - [section 3.1] "LLMs directly interact with graph structures... [and] generate suggestions for graph edits... LLMs evaluate the labels between node pairs to determine whether they belong to the same class"
  - [corpus] No direct corpus validation; neighboring papers do not address graph structure learning via LLMs.
- Break condition: Text-free graphs (e.g., molecular networks without semantic descriptors) cannot leverage this mechanism; high-proportion topological attacks may overwhelm semantic guidance.

### Mechanism 3
- Claim: LLMs can enhance GNN robustness against adversarial attacks by identifying malicious edges and recovering missing important edges.
- Mechanism: Two-stage process: (1) Instruction tuning fine-tunes a local pre-trained model to classify edges as malicious or legitimate; (2) An LM-based edge predictor computes relevance scores to identify and recover missing important edges while purging detected malicious edges. LLMs' inferential capabilities provide semantic grounding for distinguishing attack patterns.
- Core assumption: Adversarial perturbations create semantically inconsistent patterns that LLMs can detect through their reasoning capabilities.
- Evidence anchors:
  - [abstract] "enhance robustness by identifying malicious edges and recovering missing important edges"
  - [section 3.2] "LLM4RGNN... takes advantage of the inferential capabilities of LLMs to identify malicious edges and recover missing important edges, thus restoring the robustness of the graph structure"
  - [corpus] Assumption: Corpus validation weak; no neighboring papers directly address LLM-enhanced adversarial robustness for graphs.
- Break condition: Attacks that preserve semantic consistency or target purely structural properties without textual signals may evade LLM detection.

## Foundational Learning

- Concept: Message-passing in Graph Neural Networks
  - Why needed here: Understanding how GNNs aggregate neighbor information is essential to grasp where LLM-augmented embeddings and structural edits intervene in the pipeline.
  - Quick check question: Can you explain how a 2-layer GNN updates a node's representation using its 2-hop neighborhood?

- Concept: In-context learning and Chain-of-Thought reasoning in LLMs
  - Why needed here: The surveyed methods leverage LLMs' ability to perform reasoning without fine-tuning and to decompose complex edge-evaluation tasks into intermediate steps.
  - Quick check question: How does in-context learning differ from fine-tuning, and why is CoT prompting relevant for graph edge classification?

- Concept: Trustworthiness dimensions (reliability vs. robustness vs. privacy vs. explainability)
  - Why needed here: The taxonomy organizes methods by which trustworthiness aspect they target; conflating these leads to misapplication.
  - Quick check question: Is defending against label noise a robustness or reliability concern? (Answer: reliability—non-adversarial data quality issues)

## Architecture Onboarding

- Component map: Text preprocessing -> LLM encoder -> GNN backbone -> Edge evaluator -> Integration layer -> Output
- Critical path:
  1. Text preprocessing → LLM embedding/pseudo-label generation
  2. Graph construction/augmentation using LLM edge suggestions
  3. GNN training on enhanced graph
  4. (Optional) Knowledge distillation from LLM to smaller student model for privacy/cost reasons
- Design tradeoffs:
  - LLM-as-enhancer vs. LLM-as-predictor: Enhancer improves representations but requires GNN training; predictor bypasses GNN but loses structural inductive bias.
  - Local vs. API-based LLM: Local deployment preserves privacy but increases compute cost; API-based is cheaper but risks data exposure.
  - Text dependency: Methods assume rich textual attributes; text-free graphs require alternative approaches (future work, not yet mature).
- Failure signatures:
  - Heavy text dependency: Performance degrades sharply on graphs with minimal or no textual attributes.
  - High-proportion attacks: Even LLM-enhanced methods show significant accuracy drops under severe topological perturbations.
  - Privacy leakage: Uploading sensitive graph data to external LLM APIs creates exposure risk.
- First 3 experiments:
  1. Baseline comparison: Run standard GNN (GCN/GAT) vs. LLM-augmented version on a text-attributed node classification dataset with injected label noise; measure accuracy recovery.
  2. Robustness test: Apply edge perturbation attacks (random + targeted) to citation graphs; compare GNN vs. LLM4RGNN-style edge filtering on attack detection rate and classification accuracy.
  3. Ablation on text availability: Systematically reduce available textual attributes (100% → 50% → 10% → 0%) and measure reliability/robustness degradation to identify the break point for semantic augmentation mechanisms.

## Open Questions the Paper Calls Out
None

## Limitations
- The survey relies heavily on the assumption that textual attributes are present and sufficient for LLM reasoning, with no validation for non-text-attributed graphs.
- Specific hyperparameters, prompt templates, and training procedures for the surveyed methods are not disclosed, making exact reproduction challenging.
- Privacy protection claims are underdeveloped; the survey acknowledges this explicitly and provides minimal methodological detail or empirical support for privacy-preserving LLM-GNN approaches.

## Confidence
- **High**: The taxonomy framework (four trustworthiness dimensions) is well-grounded and logically organized; the categorization of integration methods (enhancer vs. predictor) is clearly specified.
- **Medium**: The mechanisms for reliability enhancement (semantic augmentation) and robustness improvement (edge filtering) are plausible given LLM capabilities but lack direct empirical validation in the survey itself.
- **Low**: Privacy protection claims are underdeveloped; the survey acknowledges this explicitly and provides minimal methodological detail or empirical support for privacy-preserving LLM-GNN approaches.

## Next Checks
1. **Text-dependency break point**: Systematically reduce textual attribute availability on benchmark datasets and measure performance degradation to identify the minimum text requirement for semantic augmentation mechanisms.
2. **Adversarial robustness under semantic consistency**: Design attacks that preserve textual coherence while perturbing graph structure to test whether LLM-based detection can still identify malicious edges when semantic patterns are preserved.
3. **Privacy-utility tradeoff quantification**: Implement differential privacy noise injection on LLM outputs and measure the resulting degradation in reliability and robustness metrics to quantify the fundamental tradeoff.