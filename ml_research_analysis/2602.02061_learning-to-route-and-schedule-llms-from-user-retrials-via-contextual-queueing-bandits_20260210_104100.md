---
ver: rpa2
title: Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing
  Bandits
arxiv_id: '2602.02061'
source_url: https://arxiv.org/abs/2602.02061
tags:
- queue
- have
- lemma
- regret
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient routing and scheduling
  for large language model (LLM) services where user dissatisfaction can lead to query
  retrials and increase server backlog. Existing online algorithms often rely on explicit
  user feedback, which is impractical and degrades user experience.
---

# Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits

## Quick Facts
- arXiv ID: 2602.02061
- Source URL: https://arxiv.org/abs/2602.02061
- Reference count: 40
- This paper introduces a novel contextual queueing bandit framework that learns to route and schedule LLM queries using only implicit feedback from user retrials, achieving sublinear regret while maintaining queue stability.

## Executive Summary
This paper addresses the challenge of efficient routing and scheduling for large language model (LLM) services where user dissatisfaction can lead to query retrials and increase server backlog. Existing online algorithms often rely on explicit user feedback, which is impractical and degrades user experience. The authors propose a novel framework called contextual queueing bandits with multinomial logit feedback (CQB-MNL) that leverages implicit feedback inferred from user retrial behaviors.

The key idea is to model user choice probabilities using the multinomial logit framework, where a user either accepts one of the candidate responses (satisfaction) or rejects all and retries the query (dissatisfaction). The proposed algorithm, anytime CQB (ACQB), combines Thompson sampling with forced exploration at a decaying rate to achieve efficient learning while maintaining queue stability. The algorithm achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$.

## Method Summary
The authors propose the contextual queueing bandits with multinomial logit feedback (CQB-MNL) framework. The algorithm ACQB combines Thompson sampling with decaying forced exploration to learn routing and scheduling policies. It uses an MNL model to interpret user retrials as implicit feedback, updating parameters via maximum likelihood estimation. For practical implementation, the authors introduce disjoint parameterization for individual LLM characteristics and utility-aligned query embedding refinement via contrastive learning (ACQB-CL). The theoretical analysis establishes regret bounds for both cumulative regret and queue length regret.

## Key Results
- ACQB achieves cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ and queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$
- Both ACQB and ACQB-CL consistently outperform baseline algorithms in queue length and cumulative regret on synthetic and real-world datasets
- ACQB-CL shows additional improvements by aligning query representations with routing utilities rather than semantic similarity
- Experiments validate the algorithm on SPROUT, EmbedLLM, and RouterBench datasets with up to 112 LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning from implicit user retrial feedback is made tractable by modeling user choice behavior with a Multinomial Logit (MNL) model.
- **Mechanism:** The system treats a user's decision to retry a query as selecting an "outside option" (j=0). The MNL model defines a probability for this option and for selecting any of the provided LLM responses. This probabilistic framework allows a standard maximum likelihood estimator to update user preference parameters ($\theta$) based only on the observed binary outcome (retry or not), without requiring explicit ratings.
- **Core assumption:** User satisfaction and the decision to retry follow the rational choice assumptions of the MNL model (specifically, that choice probabilities have a specific relationship to an underlying utility).
- **Evidence anchors:**
  - [abstract] "CQB-MNL models user choice behavior and retrial dynamics using the multinomial logit model."
  - [section 2] Defines choice probabilities $p_j(x,S,\theta^*)$ and specifically $p_0$ for the outside option, linking departure to avoiding this option.
  - [corpus] Related works in the corpus like "Learning to Route LLMs from Bandit Feedback" also explore learning from bandit feedback, but the specific **MNL formulation for retrials** is the key innovation here.
- **Break condition:** If user retrials are driven by factors outside the model (e.g., network latency, UI issues) or if user preferences do not conform to the MNL structure, the learned $\theta$ will be biased, degrading routing performance.

### Mechanism 2
- **Claim:** A decaying forced exploration strategy balances the dual objectives of learning the optimal routing policy and maintaining queue stability.
- **Mechanism:** The ACQB algorithm runs forced, random exploration with a probability $\eta(t)$ that decays over time (specifically, $\eta(t) \propto t^{-1/2}$). This ensures enough diverse data is collected early to learn the MNL parameters. As the model's confidence grows, exploration decays to minimize suboptimal decisions that cause user retrials, which would otherwise increase queue backlog.
- **Core assumption:** There is a critical tradeoff: some exploration is necessary for learning, but exploration can be harmful as it risks triggering retrials that destabilize the queue.
- **Evidence anchors:**
  - [abstract] "ACQB... achieves efficient learning while maintaining queue stability by combining Thompson sampling with decaying forced exploration."
  - [section 3] "The probability of running uniform exploration decreases as time goes on."
  - [corpus] The corpus discusses "adaptive routing" but the explicit coupling of **exploration probability with queue stability bounds** is the primary theoretical contribution.
- **Break condition:** If the decay rate of $\eta(t)$ is too fast, the system may not gather sufficient data to learn the true parameters. If too slow, the system may fail to stabilize the queue.

### Mechanism 3
- **Claim:** Theoretical guarantees for system stability are established by analyzing "queue length regret" through a novel coupling argument.
- **Mechanism:** The analysis does not just bound the standard cumulative regret for the bandit task. It introduces "queue length regret," defined as the difference between the queue length under the learning algorithm and under an optimal, all-knowing policy. The proof uses a "policy-switching queue" framework to decompose this difference and prove it decays over time ($\tilde{O}(t^{-1/4})$).
- **Core assumption:** The system operates under "traffic slackness" (Assumption 4), meaning the maximum possible departure rate is higher than the arrival rate. This is a standard prerequisite for any stable queueing system.
- **Evidence anchors:**
  - [abstract] "ACQB simultaneously achieves... a queue length regret of $\tilde{O}(t^{-1/4})$ for any large $t$."
  - [section 4.1] "We employ a coupling argument and define policy switching queues... to decompose the queue length regret."
  - [corpus] None of the related works in the corpus mention "queue length regret" or the associated "coupling" analysis technique, marking it as a distinct methodological contribution.
- **Break condition:** If the traffic slackness condition is violated (arrival rate is too high), the queue will grow without bound regardless of the algorithm's performance, and the regret bounds do not apply.

## Foundational Learning

- **Concept: Multinomial Logit (MNL) Bandits**
  - **Why needed here:** This is the core mathematical model for user choice. Without understanding MNL, one cannot implement the likelihood function or the Thompson sampling updates.
  - **Quick check question:** In this system, what user action corresponds to selecting the "outside option" in the MNL model?

- **Concept: Queueing Theory Basics (Stability & Slackness)**
  - **Why needed here:** The paper's primary contribution is ensuring queue *stability* while learning. You must understand that stability requires the average service rate (departure rate) to exceed the average arrival rate.
  - **Quick check question:** What does it mean for a queue to be "stable"? What is the "traffic slackness" parameter $\epsilon$ and why must it be positive?

- **Concept: Thompson Sampling for Exploration**
  - **Why needed here:** This is the primary learning algorithm used during non-exploration steps. It uses Bayesian posterior sampling to make optimistic decisions.
  - **Quick check question:** How does the algorithm use the sampled parameter $\tilde{\theta}$ to make a decision about which query and LLM to select?

## Architecture Onboarding

- **Component map:** Queue System -> ACQB Agent -> User Feedback Interface
- **Critical path:**
  1. A query with context arrives; the agent decides between forced random exploration or Thompson sampling.
  2. Based on the decision, an LLM is assigned, and a response is served.
  3. **Crucially**, the user's follow-up action (retry or accept) is observed.
  4. This observation updates the Maximum Likelihood Estimator ($\hat{\theta}$) and the design matrix ($V$), refining the user preference model.
- **Design tradeoffs:**
  - **Shared vs. Disjoint Parameterization:** The theoretical analysis assumes shared parameters, but experiments use disjoint $\theta_j$ for each LLM for better performance. This increases model dimensionality.
  - **Assortment Size ($K$):** Setting $K=1$ is standard routing; $K=2$ provides pairwise comparison data (like for RLHF) but complicates the MNL model.
- **Failure signatures:**
  - **Exploding Queue:** If the queue length grows linearly, the system is unstable. This indicates the learned policy is not achieving a departure rate above the arrival rate.
  - **Stagnant Learning:** If cumulative regret grows linearly, the algorithm may be stuck in a suboptimal policy, potentially because the exploration rate decayed too quickly.
- **First 3 experiments:**
  1. **Synthetic Stability Test:** Implement the simulation from Section 6.1. Plot the queue length over time to verify it remains bounded and the regret grows sublinearly. This validates your core implementation.
  2. **Ablation on Exploration Decay:** Run experiments with different forced exploration schedules (e.g., no exploration, constant rate, the proposed decaying rate). Observe how this tradeoff affects both learning speed and queue stability.
  3. **Real-World Simulation (SPROUT):** Use a real dataset to simulate user choices based on historical performance scores. Compare ACQB's routing against a simple "Random" or "Round-Robin" policy to quantify the benefit of learning user preferences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the ACQB algorithm maintain queue stability and sublinear regret in non-stationary environments where user preferences drift over time?
- Basis in paper: [inferred] The introduction explicitly motivates the use of online learning to address the "dynamic and non-stationary nature of online environments," but the theoretical formulation (Section 2) assumes a fixed, static preference parameter $\theta^*$.
- Why unresolved: The regret analysis relies on the convergence of the maximum likelihood estimator to a single true parameter, a property that breaks if the underlying preference distribution shifts during the "anytime" execution.
- What evidence would resolve it: An extension of the theoretical analysis to include a dynamic regret term or an algorithmic adaptation (e.g., sliding windows) that bounds queue length regret under specific drift rates.

### Open Question 2
- Question: Is it possible to remove the dependency on the unknown traffic slackness parameter $\epsilon$ from the queue length regret bound while preserving the "anytime" property?
- Basis in paper: [inferred] Theorem 5 presents a regret bound with a strong dependency on $\epsilon$ (e.g., $\tilde{O}(\epsilon^{-5})$), and Assumption 4 requires this slackness to guarantee stability, contrasting with the algorithm's independence from the time horizon $T$.
- Why unresolved: While the algorithm is "anytime" regarding the horizon, the theoretical guarantees for queue stability still rely on a fixed traffic slackness assumption, which may not hold or be known in volatile LLM serving scenarios.
- What evidence would resolve it: An adaptive algorithm that estimates the effective slackness online or a modified analysis proving stability (finite queue length regret) without requiring an explicit slackness parameter in the bound.

### Open Question 3
- Question: Can the theoretical guarantees be extended to choice models that violate the Independence of Irrelevant Alternatives (IIA) property, such as those with position bias?
- Basis in paper: [inferred] Section 2 defines the user choice probability strictly using the Multinomial Logit (MNL) model, which assumes IIA, whereas real-world user behaviors in LLM selection (e.g., pairwise comparisons) often exhibit complex, context-dependent biases.
- Why unresolved: The specific regret bounds rely on the convexity and gradient properties of the MNL log-likelihood, which do not directly apply to models like the Nested Logit or general attraction models.
- What evidence would resolve it: A generalization of the proof sketches (specifically Proposition 10) for a broader class of discrete choice models, or an empirical study showing the algorithm's robustness to IIA violations.

## Limitations

- Theoretical analysis assumes shared parameters across all LLMs while practical experiments use disjoint parameterization
- Forced exploration schedule depends on unspecified constants $c_1$ and $\kappa$
- Requires strict traffic slackness condition that may not hold in real-world scenarios with bursty arrivals
- MNL model assumes IIA property which may not capture real user behavior

## Confidence

- **High confidence**: The MNL modeling framework for implicit feedback from retrials is well-established and the connection to maximum likelihood estimation is straightforward. The overall algorithmic structure combining Thompson sampling with decaying exploration is sound.
- **Medium confidence**: The queue length regret analysis and its connection to queue stability relies on specific coupling arguments that are not fully verifiable without the complete proof details. The practical effectiveness of the contrastive learning extension depends on implementation details not fully specified.
- **Low confidence**: The exact choice of forced exploration decay rate $\eta(t)$ and its impact on the tradeoff between learning and stability is not empirically validated across different traffic patterns.

## Next Checks

1. **Validate Theoretical vs. Practical Parameterization Gap**: Implement both the shared parameter model (used in theory) and the disjoint parameter model (used in experiments) on the same synthetic dataset. Compare the convergence rates and queue stability to quantify the theoretical-practical gap.

2. **Stress Test Exploration Schedule Sensitivity**: Run experiments with three different exploration schedules: (a) no exploration, (b) constant exploration rate, and (c) the proposed decaying exploration. Measure both cumulative regret and queue length over time under varying traffic intensities to identify failure modes.

3. **Test Traffic Slackness Violation**: Modify the synthetic arrival process to violate the traffic slackness condition (e.g., set $\lambda > \max_S R(x,S,\theta^*) - \epsilon$). Observe how the queue length and regret behave under these unstable conditions to validate the assumption's necessity.