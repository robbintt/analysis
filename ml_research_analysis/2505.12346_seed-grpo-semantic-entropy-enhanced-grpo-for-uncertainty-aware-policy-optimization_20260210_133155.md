---
ver: rpa2
title: 'SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization'
arxiv_id: '2505.12346'
source_url: https://arxiv.org/abs/2505.12346
tags:
- semantic
- entropy
- arxiv
- reasoning
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEED-GRPO addresses the limitation in Group Relative Policy Optimization
  (GRPO) where all prompts are treated equally during policy updates, ignoring the
  varying confidence levels LLMs demonstrate across different input prompts. The core
  method introduces semantic entropy as a measure of semantic diversity in generated
  answers, which serves as a proxy for model uncertainty.
---

# SEED-GRPO: Semantic Entropy Enhanced GRPO for Uncertainty-Aware Policy Optimization

## Quick Facts
- arXiv ID: 2505.12346
- Source URL: https://arxiv.org/abs/2505.12346
- Authors: Minghan Chen; Guikun Chen; Wenguan Wang; Yi Yang
- Reference count: 40
- Primary result: New state-of-the-art on five mathematical reasoning benchmarks using 7B model

## Executive Summary
SEED-GRPO addresses a fundamental limitation in Group Relative Policy Optimization (GRPO) where all prompts are treated equally during policy updates, ignoring the varying confidence levels LLMs demonstrate across different input prompts. The core innovation introduces semantic entropy as a measure of semantic diversity in generated answers, which serves as a proxy for model uncertainty. By dynamically adjusting the magnitude of policy updates based on this uncertainty signal, SEED-GRPO implements a question-specific adaptive learning rate mechanism that significantly improves mathematical reasoning performance.

The approach achieves new state-of-the-art performance across five mathematical reasoning benchmarks: AIME24 (56.7%), AMC (68.7%), MATH (83.4%), Minerva (34.2%), and OlympiadBench (48.0%). Using only a 7B parameter model, SEED-GRPO surpasses several 32B models including SRPO, DAPO, DeepSeek-R1-Zero-Qwen, and QwQ-preview, while maintaining significantly lower computational costs. The method demonstrates consistent improvements across different base models, with gains scaling from 10.4 percentage points for 1.5B models to 20.0 percentage points for 7B models.

## Method Summary
SEED-GRPO enhances GRPO by incorporating semantic entropy as an uncertainty measure to modulate policy updates. The method samples multiple responses (G=8 or 16) per prompt, clusters them by semantic equivalence of final answers, and computes semantic entropy to estimate model uncertainty. This entropy value scales the advantage function, reducing policy updates for high-uncertainty prompts while maintaining or increasing updates for confident predictions. The approach creates an implicit curriculum learning effect without manual difficulty labels, where the model learns more aggressively from problems it understands well while being conservative on uncertain ones. The method is implemented as a modification to the Dr.GRPO variant, using a linear weighting function with α=0.02 as the default configuration.

## Key Results
- Achieves 56.7% on AIME24, surpassing 32B models like DeepSeek-R1-Zero-Qwen and QwQ-preview
- Sets new state-of-the-art of 83.4% on MATH500 benchmark
- Outperforms SRPO, DAPO, and other GRPO variants across all five tested benchmarks
- Maintains computational efficiency despite requiring multiple rollouts per prompt

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantic entropy serves as a reliable proxy for model uncertainty at the prompt level.
- Mechanism: When an LLM generates multiple responses to the same prompt, clustering them by semantic meaning reveals consistency or divergence. High semantic entropy (many distinct meaning clusters) indicates the model lacks coherent understanding; low entropy (responses cluster together) indicates confidence. The paper approximates SE(q) ≈ -(1/K) Σ log p(Ck|q) from sampled responses.
- Core assumption: Semantic diversity in generated outputs correlates with the problem exceeding the model's current reasoning capability.
- Evidence anchors:
  - [abstract] "Semantic entropy measures the diversity of meaning in multiple generated answers given a prompt and uses this to modulate the magnitude of policy updates."
  - [section 3.2] "When all G responses convey the same meaning (K=1), the entropy reaches its minimum value of 0, indicating complete certainty."
  - [corpus] Related work "Entropy-guided sequence weighting" (arXiv:2503.22456) supports entropy-based weighting but uses token-level entropy rather than semantic clustering.
- Break condition: If semantic clustering fails (e.g., open-ended problems without unique answers), entropy estimates become unreliable.

### Mechanism 2
- Claim: Scaling advantages by uncertainty creates an implicit curriculum learning effect without manual difficulty labels.
- Mechanism: The modulated advantage Âi = Ai · f(α · SE(q)/SEmax(q)) reduces gradient magnitude for high-uncertainty prompts. From Eq. 9: θ ← θ + η · ∇log πθ(oi|q) · ratioi(θ) · Âi. This effectively creates a per-question adaptive learning rate—larger steps on confident problems, conservative steps on uncertain ones.
- Core assumption: High-uncertainty prompts yield noisier reward signals that could cause unstable learning if updated aggressively.
- Evidence anchors:
  - [section 3.1] "For questions where the model exhibits high semantic entropy (high uncertainty), we adaptively downscale the advantages during policy updates, resulting in more conservative learning steps."
  - [section 3.3] "This uncertainty-aware advantage computation effectively creates a question-specific adaptive learning rate."
  - [corpus] "GTPO: Stabilizing GRPO" (arXiv:2508.03772) addresses GRPO instability through gradient control, suggesting instability is a known failure mode.
- Break condition: If the weighting function f(·) is too aggressive (high α), the model may underfit hard but learnable problems.

### Mechanism 3
- Claim: Increasing rollout count G improves both semantic entropy estimation and final performance, with diminishing returns.
- Mechanism: More samples per prompt yield better Monte Carlo estimates of semantic cluster distribution. The paper shows increasing G from 8 to 16 improves AIME24 from 46.7% to 56.7% (+10pp), though average gain is smaller (+1.6pp).
- Core assumption: The additional computational cost of more rollouts is justified by improved uncertainty estimation.
- Evidence anchors:
  - [section 4.3, Table 4(d)] "Increasing G from 8 to 16 improves the average accuracy from 56.6% to 58.2%, with particularly gains on the challenging AIME benchmark."
  - [section 5] Limitation notes current implementation focuses "solely on final answers for semantic clustering," suggesting room for improved estimation.
  - [corpus] "Hybrid GRPO" (arXiv:2502.01652) explores multi-sample evaluation but without uncertainty weighting.
- Break condition: Beyond G=16, computational costs may exceed marginal gains; optimal G likely task-dependent.

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: SEED-GRPO builds directly on GRPO's advantage calculation (Ai = ri - r̄). Understanding how GRPO eliminates the critic model by using group-averaged rewards as baseline is essential.
  - Quick check question: Can you explain why GRPO uses group mean reward as baseline instead of a learned value function?

- Concept: **Semantic Entropy vs. Shannon Entropy**
  - Why needed here: The paper explicitly argues vanilla token-level entropy overestimates uncertainty when responses differ syntactically but not semantically. Understanding this distinction is critical for the clustering mechanism.
  - Quick check question: Why would "3 is the answer" and "The value is 3" produce high Shannon entropy but low semantic entropy?

- Concept: **Clipped Surrogate Objective (PPO)**
  - Why needed here: SEED-GRPO inherits PPO's clipping mechanism (Eq. 2). The advantage modulation interacts with clipping to bound policy updates.
  - Quick check question: What happens to the effective update magnitude when both the clipping threshold (ε=0.2) and uncertainty weighting (f(·)) are applied?

## Architecture Onboarding

- Component map:
  Rollout Generator -> Reward Verifier -> Semantic Clusterer -> Entropy Computer -> Advantage Modulator -> Policy Updater

- Critical path:
  Rollout sampling → Final answer extraction → Semantic clustering → SE(q) computation → Advantage modulation → Policy update

- Design tradeoffs:
  - **Clustering granularity**: Paper uses final-answer-only clustering for simplicity; intermediate reasoning steps ignored (noted as limitation)
  - **Weight function f(·)**: Linear outperforms exponential/focal in ablations; α=0.02 optimal for average, α=0.03 better for hardest tasks
  - **Rollout count G**: Higher G improves entropy estimates but increases compute; G=8 baseline, G=16 best reported

- Failure signatures:
  1. **All responses identical (SE≈0)**: No uncertainty signal; may over-update on confidently wrong answers
  2. **All responses unique (SE≈max)**: Advantage nearly zeroed; no learning signal
  3. **Cluster collapse on open-ended tasks**: Semantic equivalence undefined; entropy meaningless
  4. **α too high**: Excessive dampening causes underfitting on learnable hard problems

- First 3 experiments:
  1. **Baseline replication**: Implement GRPO (Dr.GRPO variant) on Qwen2.5-Math-7B with MATH L3-L5 training; verify ~51% average accuracy matches Table 2
  2. **Ablation on weight function**: Test Linear vs. Exponential vs. Focal f(·) with α=0.02; expect Linear to match or exceed others per Table 4(c)
  3. **Rollout scaling study**: Compare G∈{4,8,16} on AIME24 subset; expect monotonic improvement with diminishing returns, validate SE estimation quality by comparing Monte Carlo vs. theoretical SE distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can SEED-GRPO effectively scale to domains with non-unique answers or open-ended reasoning paths, such as code generation and multimodal tasks?
- Basis in paper: [explicit] The "Limitation and Future Work" section notes the current reliance on final answers limits applicability to open-ended problems and proposes extending the method to code and multimodal domains.
- Why unresolved: The current semantic entropy calculation depends on clustering final answers with unique correctness criteria, which are absent in open-ended tasks.
- What evidence would resolve it: Successful application of SEED-GRPO on benchmarks like HumanEval (code) or VQA datasets showing performance improvements over standard GRPO.

### Open Question 2
- Question: Does integrating semantic entropy over intermediate reasoning steps, rather than solely final answers, yield more robust uncertainty modeling?
- Basis in paper: [explicit] The authors state in "Limitation and Future Work" that refining semantic entropy estimation by incorporating intermediate reasoning steps is a promising avenue.
- Why unresolved: The current implementation clusters responses based only on final answers, potentially ignoring semantic uncertainty that arises during the reasoning process.
- What evidence would resolve it: A comparative analysis of training dynamics and final benchmark performance between step-level and answer-level uncertainty models.

### Open Question 3
- Question: Can semantic entropy signals be utilized to dynamically optimize test-time compute strategies, such as adjusting rollout counts or triggering fallback mechanisms?
- Basis in paper: [explicit] The "Limitation and Future Work" section suggests exploring test-time compute strategies based on entropy signals to allow uncertainty-aware reasoning at inference time.
- Why unresolved: The current study restricts the use of semantic entropy to the training phase; its utility for dynamically managing computational resources during inference is unexplored.
- What evidence would resolve it: Experiments demonstrating improved inference accuracy or efficiency when using entropy thresholds to trigger specific inference strategies (e.g., self-consistency).

## Limitations

- Semantic entropy calculation relies on final-answer-only clustering, ignoring intermediate reasoning steps that could provide richer uncertainty signals
- Multiple rollouts required (G=8-16) add significant computational overhead despite claims of efficiency relative to larger models
- Method's effectiveness on open-ended or multi-answer problems remains unclear due to reliance on unique ground-truth answers
- Optimal α and G values appear task-dependent, suggesting need for hyperparameter tuning across different domains

## Confidence

- **Performance claims**: High confidence - The reported benchmark results show consistent improvements across multiple datasets with clear statistical superiority over baselines
- **Semantic entropy as uncertainty proxy**: Medium confidence - While theoretically sound and supported by results, the validation relies primarily on final-answer clustering without examining intermediate reasoning
- **Computational efficiency claims**: Low confidence - The paper claims reduced computational costs compared to 32B models, but this comparison is indirect and the multiple rollouts required aren't fully quantified

## Next Checks

1. **Intermediate reasoning analysis**: Evaluate whether incorporating semantic entropy from intermediate reasoning steps (not just final answers) improves uncertainty estimation and performance, particularly for problems requiring multiple solution steps

2. **Open-ended problem validation**: Test SEED-GRPO on mathematical problems with multiple valid solution approaches or approximate answers to assess how well semantic entropy handles cases where strict answer equivalence is undefined

3. **Computational overhead quantification**: Measure total wall-clock training time and GPU-hours for SEED-GRPO versus baseline GRPO with varying G values to provide direct evidence for efficiency claims, including analysis of how G scales with dataset size