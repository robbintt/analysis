---
ver: rpa2
title: NeurIPS 2023 LLM Efficiency Fine-tuning Competition
arxiv_id: '2503.13507'
source_url: https://arxiv.org/abs/2503.13507
tags:
- evaluation
- competition
- open
- tasks
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This competition aimed to democratize access to state-of-the-art
  LLMs by challenging participants to fine-tune a pre-trained language model within
  24 hours using a single GPU. The evaluation revealed that top-performing models
  exhibited significant overfitting to benchmark datasets, with poor generalization
  to unseen tasks.
---

# NeurIPS 2023 LLM Efficiency Fine-tuning Competition

## Quick Facts
- arXiv ID: 2503.13507
- Source URL: https://arxiv.org/abs/2503.13507
- Reference count: 8
- Primary result: Top models overfit benchmarks, poor generalization to unseen tasks

## Executive Summary
This competition challenged participants to fine-tune a pre-trained LLM within 24 hours using a single GPU. The evaluation revealed that top-performing models exhibited significant overfitting to benchmark datasets, with poor generalization to unseen tasks. Winning submissions primarily focused on data curation rather than novel architectures, using standard open-source libraries. Approximately 30% of submissions failed to complete all evaluation tasks within the allotted time, negatively impacting their rankings. The results highlight the limitations of current benchmark-based evaluation schemes and demonstrate the need for more robust evaluation methods that assess real-world task performance and include metrics beyond accuracy, such as bias, robustness, and fairness.

## Method Summary
Participants fine-tuned pre-trained LLMs (Qwen-14B, Mistral-7B, Llama2-7B) within 24 hours using single GPUs (A100 40GB or 4090 24GB tracks). Winners used QLoRA/LoRA parameter-efficient fine-tuning with standard libraries (PEFT, Transformers, LLaMA-Factory). Data curation was critical: teams profiled base models on open evaluation, curated task-aligned dataset mixtures (LIMA, Open-Platypus, Dolly-15k, OASST1), and filtered non-English data and potential contamination. Evaluation used HELM framework with geometric mean aggregation across scenarios. Strict time limits (300 min open, 600 min closed) enforced inference efficiency, with unanswered questions scoring zero.

## Key Results
- Top models showed significant overfitting to benchmark datasets with poor generalization to unseen tasks
- Winning submissions focused on data curation rather than architectural innovation
- ~30% of submissions failed to complete all evaluation tasks within time limits, scoring zero for unanswered questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data curation drives fine-tuning performance gains more than architectural innovation under compute constraints
- Mechanism: Winners profiled base model performance on open evaluation tasks, then curated training data mixtures (LIMA, Open-Platypus, Dolly-15k, OASST1) that reflected open evaluation task composition while filtering non-English data, potential contamination, and banned sources. They validated each mixture by running it on open evaluation before final selection
- Core assumption: Task-aligned training data transfers better than generic instruction data when compute is bounded
- Evidence anchors:
  - "winning submissions utilized standard open-source libraries and focused primarily on data curation"
  - "None of them created custom code for fine-tuning, and all relied on well-established open-source libraries"
  - DCVLR Challenge on data curation for reasoning supports data-centric approaches

### Mechanism 2
- Claim: Two-stage evaluation (open/closed) exposes generalization failures that single-stage benchmarks hide
- Mechanism: Open tasks are published early, allowing optimization. Hidden closed tasks measure true generalization. Low correlation between stages indicates overfittingâ€”winners deliberately did not maximize open scores to avoid this
- Core assumption: High open evaluation scores without corresponding closed scores indicate memorization rather than capability
- Evidence anchors:
  - "top-performing models exhibit significant overfitting on benchmark datasets... poor generalization to unseen tasks"
  - "the winning entries did not achieve the highest scores on the open evaluation tasks... which suggests that they were less overfitted"
  - Figure 7 shows "very weak" correlations (-0.08 for 4090, 0.18 for A100) between open and closed evaluation scores

### Mechanism 3
- Claim: Hard timeout constraints with zero-score penalties enforce inference efficiency without explicit efficiency metrics
- Mechanism: Strict runtime limits (300 min open, 600 min closed) penalize slow generation. Unanswered questions receive 0 points, directly impacting rankings
- Core assumption: Practical deployability requires both accuracy and acceptable inference speed
- Evidence anchors:
  - "approximately 30% of submissions were unable to finish all questions in the allowed time, negatively affecting their ranking"
  - "Any unanswered questions due to time constraints were given a score of 0"

## Foundational Learning

- Concept: **LoRA/QLoRA (Parameter-Efficient Fine-Tuning)**
  - Why needed here: All top submissions used PEFT approaches. Understanding how rank decomposition matrices reduce trainable parameters while freezing base weights is essential for memory-constrained fine-tuning
  - Quick check question: Can you explain why injecting trainable low-rank matrices into each Transformer layer reduces VRAM requirements compared to full fine-tuning?

- Concept: **HELM (Holistic Evaluation of Language Models)**
  - Why needed here: Competition adapted Stanford's HELM framework. Understanding scenarios (bias, robustness, fairness), win rates, and geometric mean aggregation is critical for interpreting results
  - Quick check question: How does HELM's geometric mean of win rates across scenarios differ from averaging accuracy across tasks?

- Concept: **Benchmark Contamination Detection**
  - Why needed here: Winners explicitly filtered "potential evaluation set contamination." Understanding how training data can leak test information prevents false performance signals
  - Quick check question: What methods would you use to detect if your curated training data contains examples too similar to evaluation benchmarks?

## Architecture Onboarding

- Component map: Pre-trained LLM (Qwen-14B/Mistral-7B/Llama2-7B) -> QLoRA/LoRA fine-tuning via LLaMA-Factory/PEFT -> HELM evaluation with Sparse HELM -> Docker deployment

- Critical path:
  1. Profile candidate base models on open evaluation to establish baseline
  2. Select model fitting VRAM constraints (24GB/40GB)
  3. Curate data mixture matching open evaluation task distribution
  4. Fine-tune within 24-hour constraint
  5. Verify inference completes within timeout limits

- Design tradeoffs:
  - Model size vs. inference speed: Larger models may timeout
  - Data alignment vs. overfitting: Task-specific data improves open scores but risks closed-evaluation failure
  - Quantization vs. accuracy: QLoRA enables larger models but may reduce quality
  - Ensemble vs. simplicity: 2nd place 4090 used task-specific ensembles; winners used single models

- Failure signatures:
  - Overfitting: High open scores, chance-level closed scores
  - Timeout: ~30% of submissions incomplete; 0 points for unanswered
  - Build failures: >50% of Dockerfiles failed (unpinned dependencies, breaking changes in PEFT/Transformers)
  - Memory errors: ~10% ran out of memory during inference

- First 3 experiments:
  1. Run open evaluation on 3-5 candidate base models to identify strongest performer within VRAM budget
  2. Train with 3 different data mixture combinations; validate on open evaluation to find best balance
  3. Run full evaluation timing test to ensure completion within 300-600 minute windows before submission

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do performance discrepancies between open and closed evaluation stages result primarily from dataset overfitting or from the tasks assessing fundamentally distinct skills?
- Basis in paper: Section 2.2.1 states that low agreement between stages "indicates one of two possibilities: either the submissions overfitted to the open evaluation tasks, or the open and closed evaluation tasks assessed different skills."
- Why unresolved: The analysis showed conflicting correlations between tracks (strong for 4090, weak for A100), leaving the specific cause of the generalization failure ambiguous
- What evidence would resolve it: A controlled ablation study analyzing the skill overlap between open and hidden tasks, coupled with an analysis of training data contamination

### Open Question 2
- Question: How can evaluation frameworks be redesigned to better assess real-world utility and non-accuracy metrics like bias and robustness?
- Basis in paper: Section 3.1 argues that "accuracy-based score alone is insufficient" and that "evaluations should be more real-world task-driven" including metrics for bias and fairness
- Why unresolved: Current benchmark-based schemes showed significant limitations, with top models failing to generalize, suggesting existing holistic metrics do not fully capture model utility
- What evidence would resolve it: The development and validation of a new benchmark suite that correlates higher with downstream application performance than current static leaderboards

### Open Question 3
- Question: What is the comparative efficacy of fine-tuning versus holistic systems approaches (e.g., RAG, Mixture of Experts) under strict resource constraints?
- Basis in paper: Section 3.4 suggests fine-tuning is "simply one of the many tools" and questions its utility versus "retrieval-augmented generation (RAG), Mixture of Experts (MoE), model merging, and swarming intelligences"
- Why unresolved: The competition focused on fine-tuning in isolation, while real-world performance may rely on these hybrid architectures which were not evaluated in this specific challenge format
- What evidence would resolve it: A comparative study measuring task performance of fine-tuned models against RAG/MoE systems using identical hardware and time constraints

## Limitations

- Unknown exact dataset mixture proportions and filtering criteria used by winning entries
- Limited transparency around validation procedures that distinguished top performers from overfitting models
- Insufficient empirical evidence for the timeout mechanism's effectiveness in enforcing inference efficiency

## Confidence

- Data curation superiority claim: Medium confidence (supported by winner behaviors but lacks controlled ablation studies)
- Two-stage evaluation validity: High confidence (strong empirical support from correlation analysis)
- Timeout enforcement effectiveness: Low confidence (based on descriptive statistics but lacks controlled experiments)

## Next Checks

1. Analyze winning teams' curated datasets against evaluation benchmarks using semantic similarity metrics to verify contamination filtering claims and quantify potential leakage impact

2. Train multiple models with identical compute budgets but varying degrees of open-task optimization, then measure generalization gaps to establish causation between overfitting and poor closed-task performance

3. Systematically vary evaluation time limits while holding model architectures constant to isolate timeout's contribution to ranking differences and identify threshold effects on submission viability