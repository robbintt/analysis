---
ver: rpa2
title: 'Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for Hindi'
arxiv_id: '2504.06011'
source_url: https://arxiv.org/abs/2504.06011
tags:
- hindi
- language
- https
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Nanda, a 10-billion parameter Hindi-English
  bilingual large language model built on top of Llama-3-8B. The model addresses the
  scarcity of high-quality Hindi text data through rigorous data curation and augmentation,
  employing a 1:1 English-to-Hindi token mix during pretraining to maintain cross-linguistic
  capabilities while expanding into Hindi.
---

# Llama-3-Nanda-10B-Chat: An Open Generative Large Language Model for Hindi

## Quick Facts
- arXiv ID: 2504.06011
- Source URL: https://arxiv.org/abs/2504.06011
- Reference count: 40
- Introduces Nanda, a 10-billion parameter Hindi-English bilingual model achieving state-of-the-art performance on Hindi benchmarks

## Executive Summary
Nanda is a 10-billion parameter Hindi-English bilingual large language model built on Llama-3-8B that addresses the scarcity of high-quality Hindi text data through rigorous data curation and augmentation. The model employs a 1:1 English-to-Hindi token mix during pretraining to maintain cross-linguistic capabilities while expanding into Hindi, achieving state-of-the-art performance on Hindi benchmarks while remaining comparable to English models trained on larger datasets. Nanda also introduces a novel Hindi safety evaluation dataset to assess biases and harmful content, and is open-sourced to advance research and practical applications in Hindi NLP.

## Method Summary
Nanda adapts Llama-3-8B through block expansion (adding 8 layers to expand from 32 to 40 layers), tokenizer extension (20% Hindi vocabulary addition), and embedding initialization using WECHSEL with semantic similarity. The model undergoes continued pretraining on 55B Hindi tokens mixed equally with English replay data, followed by instruction tuning on approximately 81K examples with 300% oversampling to ensure Hindi proficiency. The approach uses identity-initialized blocks to preserve English capabilities while learning Hindi-specific patterns, with a custom tokenizer reducing Hindi fertility from 2.61 to 1.19 tokens per word.

## Key Results
- Achieves state-of-the-art performance on Hindi benchmarks, outperforming similarly sized Hindi and multilingual models
- Maintains English capabilities through 1:1 mixing ratio while expanding into Hindi
- Reduces Hindi fertility from 2.61 to 1.19 tokens per word through tokenizer extension
- Introduces novel Hindi safety evaluation dataset for assessing biases and harmful content

## Why This Works (Mechanism)

### Mechanism 1: Block Expansion for Plasticity-Preservation Tradeoff
Adding transformer blocks initialized to identity mappings while freezing the original backbone enables Hindi language acquisition without erasing English capabilities. The Llama-Pro approach adds 8 new decoder blocks (expanding from 32 to 40 layers) inserted every 4 original blocks. These new blocks learn Hindi-specific patterns while frozen original blocks retain pretrained knowledge. Core assumption: Language-specific knowledge can be localized to newly added parameters without disrupting distributed representations in the frozen backbone.

### Mechanism 2: Vocabulary Extension with Semantic Embedding Initialization
Extending the tokenizer with Hindi tokens and initializing their embeddings via semantic similarity to existing tokens reduces fertility and accelerates convergence. Add 20% new Hindi vocabulary; initialize each new token's embedding as weighted average of top-k=5 most similar base vocabulary tokens using OpenAI text-embedding-3-large. Core assumption: Semantic proximity in a pretrained embedding space predicts functional equivalence in the target model's embedding space.

### Mechanism 3: Replay-Based Mitigation of Stability Gap
Maintaining 1:1 English-to-Hindi token ratio during continued pretraining prevents catastrophic forgetting while enabling cross-lingual knowledge transfer. English replay data mixed equally with Hindi data signals that original distribution remains relevant, reducing the stability gap from out-of-distribution language exposure. Core assumption: Forgetting is driven by distribution shift; maintaining original-distribution samples during adaptation preserves learned capabilities.

## Foundational Learning

- **Catastrophic Forgetting / Stability Gap**
  - Why needed here: Nanda builds on Llama-3 via continued pretraining; understanding why replay data prevents forgetting explains the 1:1 mixing requirement.
  - Quick check question: What would happen to English performance if pretraining used 100% Hindi data with no replay?

- **Token Fertility**
  - Why needed here: Motivates vocabulary extension; fertility measures how many tokens represent one word, directly impacting training cost and context efficiency.
  - Quick check question: Why does high fertility (2.61) hurt a model more than low fertility (1.19)?

- **Cross-Lingual Transfer via Embedding Alignment**
  - Why needed here: WECHSEL initialization assumes semantic similarity bridges languages; this enables new tokens to "inherit" useful representations.
  - Quick check question: Why average top-k similar tokens rather than copy the single most similar one?

## Architecture Onboarding

- Component map: Llama-3-8B (frozen backbone, 32 layers) -> Block Expansion (+8 identity-initialized layers → 40 total) -> Tokenizer Extension (+20% Hindi vocab → 153,856 tokens) -> Embedding Initialization (WECHSEL with text-embedding-3-large, k=5) -> Continued Pretraining (55B Hindi tokens + 55B English replay, 8K context) -> Instruction Tuning (~100M tokens, 300% oversampling, 20K safety examples) -> Llama-3-Nanda-10B-Chat

- Critical path: Block expansion → Vocabulary extension → Embedding initialization → Bilingual pretraining (1:1 ratio). Errors in tokenizer/embedding setup propagate irreversibly through pretraining.

- Design tradeoffs:
  - Vocabulary size: 20% extension balances fertility reduction (54.4%) vs. embedding matrix overhead; 30% showed "minimal improvement."
  - Block ratio: 25% expansion (8/32 layers) follows prior work; more blocks increase capacity but also training cost.
  - Replay ratio: 1:1 is conservative; may underfit Hindi but preserves English. Paper does not ablate other ratios.
  - Assumption: These tradeoffs are language-pair specific; different languages may require different configurations.

- Failure signatures:
  - High Hindi fertility (>2.0) after tokenizer extension → vocabulary poorly matched to corpus
  - English benchmark degradation >5% → insufficient replay data or excessive learning rate
  - Hindi generation produces mixed-script gibberish → embedding initialization failed or insufficient Hindi pretraining
  - Safety refusals on benign Hindi queries → overfitting to safety prompts; check oversampling balance

- First 3 experiments:
  1. **Tokenizer validation**: Measure fertility on held-out Hindi corpus; if >1.5, re-evaluate vocabulary extension percentage or source tokenizer.
  2. **Replay ablation**: Train with 1:2 and 2:1 En:Hi ratios; measure tradeoff between English retention (MMLU, TruthfulQA) and Hindi acquisition (HellaSwag-hi, ARC-hi).
  3. **Embedding initialization sanity check**: Compare WECHSEL (k=5) vs. random initialization vs. mean of all embeddings on early-loss convergence; divergence indicates initialization quality matters.

## Open Questions the Paper Calls Out

- Would extending Nanda to Mixture-of-Experts (MoE) architectures or multimodal modalities improve downstream task accuracy?
- Why does Nemotron-4-Mini-Hindi outperform Nanda on log-likelihood benchmarks while Nanda excels in generation quality, and which metric better predicts real-world utility?
- Is the 1:1 English-to-Hindi token ratio optimal for bilingual adaptation, or do task-specific applications require different mixing strategies?
- How does machine-translated instruction-tuning data affect model quality compared to native Hindi instruction data?

## Limitations

- Data quality and representativeness concerns: The paper relies on "65B tokens of curated Hindi text" without detailed corpus composition, domain coverage, or quality metrics.
- Ablation gaps: Critical design choices lack systematic ablation studies for the 1:1 mixing ratio, 20% vocabulary extension, and 25% block expansion.
- Benchmark scope limitations: Performance gains are demonstrated primarily on benchmark tasks without testing real-world deployment scenarios.

## Confidence

**High Confidence (Likelihood >80%)**:
- Nanda outperforms existing Hindi models on reported benchmarks
- Block expansion architecture successfully expands model capacity
- Tokenizer extension reduces Hindi fertility from 2.61 to 1.19
- Instruction tuning improves instruction-following capabilities

**Medium Confidence (Likelihood 50-80%)**:
- 1:1 mixing ratio optimally balances Hindi acquisition with English retention
- WECHSEL embedding initialization meaningfully accelerates convergence
- State-of-the-art claims hold across diverse Hindi usage contexts
- Safety evaluation dataset comprehensively captures harmful content patterns

**Low Confidence (Likelihood <50%)**:
- Performance gains generalize to real-world deployment scenarios
- Architectural choices (block ratio, vocabulary size) are optimal for all Hindi-English applications
- Safety model effectively handles regional Hindi variants and code-mixing
- English retention claims hold under extended Hindi specialization

## Next Checks

**Check 1: Replay Ratio Sensitivity Analysis**
Systematically vary the English-to-Hindi token ratio (1:3, 1:2, 1:1, 2:1, 3:1) during continued pretraining and measure the tradeoff curve between Hindi benchmark performance and English benchmark degradation. This quantifies the stability gap and identifies the optimal ratio for specific application requirements.

**Check 2: Corpus Composition Audit**
Analyze the 65B token Hindi corpus for domain distribution, text quality metrics (perplexity, lexical diversity), and comparison to established Hindi corpora. Validate that the corpus represents diverse Hindi usage patterns and doesn't overfit to specific domains or text styles.

**Check 3: Cross-Lingual Transfer Evaluation**
Test Nanda's zero-shot and few-shot capabilities on tasks requiring cross-lingual reasoning, such as translating between Hindi and English in context, answering questions that require knowledge from both languages, or performing tasks where Hindi prompts require English knowledge. This validates the claimed cross-linguistic capabilities beyond simple retention.