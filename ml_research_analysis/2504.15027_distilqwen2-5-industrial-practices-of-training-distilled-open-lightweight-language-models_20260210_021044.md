---
ver: rpa2
title: 'DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight
  Language Models'
arxiv_id: '2504.15027'
source_url: https://arxiv.org/abs/2504.15027
tags:
- distilqwen2
- llms
- student
- teacher
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DistilQwen2.5, a family of distilled lightweight
  language models derived from Qwen2.5 models. The authors employ multi-agent data
  augmentation using proprietary LLMs to generate and refine instruction-response
  pairs, followed by efficient model fusion to integrate fine-grained hidden knowledge.
---

# DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models

## Quick Facts
- **arXiv ID:** 2504.15027
- **Source URL:** https://arxiv.org/abs/2504.15027
- **Reference count:** 16
- **One-line primary result:** DistilQwen2.5-7B achieves 34.86% on AlpacaEval 2.0, outperforming baseline Qwen2.5-7B at 31.43%

## Executive Summary
This paper presents DistilQwen2.5, a family of distilled lightweight language models derived from Qwen2.5 models. The authors employ multi-agent data augmentation using proprietary LLMs to generate and refine instruction-response pairs, followed by efficient model fusion to integrate fine-grained hidden knowledge. The resulting models demonstrate significantly improved instruction-following capabilities across benchmarks like AlpacaEval 2.0 and IFEval, outperforming original Qwen2.5 checkpoints. For instance, DistilQwen2.5-7B achieves 34.86% on AlpacaEval 2.0 compared to 31.43% for the baseline. The approach balances performance and computational efficiency, with practical industrial applications demonstrated in SQL completion tasks.

## Method Summary
The method combines multi-agent data augmentation (black-box KD) with efficient model fusion (white-box KD). First, proprietary LLMs act as specialized agents—Expansion, Rewriting, Selection, and Verification—to process and enhance seed instruction-response pairs, generating high-quality augmented data. The student models undergo supervised fine-tuning on this data. Next, the approach leverages offline pre-computation of top-K logits from large teacher models to enable computationally feasible white-box knowledge distillation, minimizing divergence between teacher and student probability distributions. This two-phase pipeline enables efficient transfer of fine-grained knowledge while maintaining scalability for industrial applications.

## Key Results
- DistilQwen2.5-7B achieves 34.86% on AlpacaEval 2.0 vs. 31.43% for Qwen2.5-7B baseline
- Models show consistent improvements across MT-Bench and IFEval benchmarks
- DistilQwen2.5-7B outperforms DeepSeek-Coder-V2-6.7B-Instruct on coding tasks
- The framework demonstrates practical industrial utility in SQL completion tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-agent data augmentation using proprietary LLMs produces higher-quality training data than raw instruction-response pairs.
- Mechanism: Four specialized agents—Expansion (generates diverse instruction variations while preserving task category), Rewriting (enhances quality with semantic preservation, including CoT for complex tasks), Selection (filters by informativeness/helpfulness), and Verification (checks factual correctness)—process seed data before student training. This encodes teacher knowledge into the training examples themselves.
- Core assumption: The quality and diversity of instruction-response pairs directly determines how much teacher knowledge transfers to the student, and CoT responses improve reasoning capabilities in smaller models.
- Evidence anchors:
  - [abstract] "proprietary LLMs with varying capacities as multi-agent teachers to select, rewrite, and refine instruction-response pairs that are more suitable for student LLMs to learn"
  - [section 3.1] Describes all four agent functionalities; notes "we encourage them to be Chain-of-Thought (CoT) outputs for complex tasks such as logical reasoning, mathematical problems, and code generation"
  - [corpus] Related work on data augmentation for KD exists (EasyDistill toolkit, AM-DeepSeek-R1-Distilled dataset), but corpus does not provide direct validation of this specific multi-agent design.
- Break condition: If expansion agent causes semantic drift beyond task category, or if verification agent fails to catch factual errors, data quality degrades and distillation effectiveness collapses.

### Mechanism 2
- Claim: Storing only top-K logits (K=10) from teacher models enables computationally feasible white-box KD at industrial scale without meaningful knowledge loss.
- Mechanism: Instead of online forward passes through large teachers during student training (memory-prohibitive), the system pre-computes and stores only the top-10 logits per token position offline. During training, divergence minimization operates only on these top-K elements. Token alignment handles vocabulary mismatches between teacher and student.
- Core assumption: "The sum of the probabilities of the top-10 tokens is almost equal to 1" (Section 3.2)—meaning nearly all teacher knowledge resides in top-10 predictions.
- Evidence anchors:
  - [abstract] "computationally efficient model fusion approach that enables student models to progressively integrate fine-grained hidden knowledge"
  - [section 3.2] Explicitly states the top-K observation and provides the modified divergence formula; Figure 3 shows 3-5× speedup vs. vanilla approach
  - [corpus] EasyDistill mentions supporting both black-box and white-box KD, but does not validate the top-K approximation specifically.
- Break condition: If critical knowledge for downstream tasks resides in tokens beyond top-10 (rare but domain-specific vocabulary, edge cases), student model will fail to learn those patterns.

### Mechanism 3
- Claim: Applying black-box KD before white-box KD yields additive improvements; smaller student models benefit more from distillation than larger ones.
- Mechanism: Black-box KD (data augmentation) first establishes a strong fine-tuned baseline. White-box KD (logits matching) then refines the student by aligning its internal probability distributions with the teacher's. The sequential approach maximizes computational efficiency by not wasting white-box compute on unprepared students.
- Core assumption: The two KD methods capture complementary knowledge—black-box encodes reasoning patterns into training data, white-box transfers fine-grained uncertainty/calibration.
- Evidence anchors:
  - [abstract] "After standard fine-tuning, we further leverage a computationally efficient model fusion approach"
  - [section 4.3/Table 1] Models with `*` (black-box only) consistently underperform full models (black-box + white-box); e.g., DistilQwen2.5-0.5B improves from 4.72 to 4.89 on AlpacaEval 2.0
  - [corpus] No direct corpus validation of sequential ordering; DeepDistill explores difficulty-graded training but not this specific pipeline.
- Break condition: If black-box phase overfits to augmented data distribution, white-box phase may struggle to align with teacher logits; diminishing returns set in as teacher size increases beyond student's capacity to absorb (Figure 5).

## Foundational Learning

- Concept: **Knowledge Distillation (KD) - Black-box vs. White-box**
  - Why needed here: The entire DistilQwen2.5 approach combines both paradigms; understanding the distinction is essential for grasping why two phases are used.
  - Quick check question: Can you explain why black-box KD only needs API access to the teacher while white-box KD requires access to internal logits?

- Concept: **Kullback-Leibler Divergence and Temperature Scaling**
  - Why needed here: White-box KD minimizes divergence between teacher and student probability distributions; temperature T controls distribution softness.
  - Quick check question: What happens to knowledge transfer when temperature T is set very high versus very low?

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The rewriting agent explicitly generates CoT outputs for complex tasks to enhance student reasoning capabilities.
  - Quick check question: Why might CoT responses be more valuable than direct answers for training smaller models?

## Architecture Onboarding

- Component map:
Seed Instructions -> [Expansion Agent] -> Diverse instruction variations
Seed Instructions -> [Rewriting Agent + Teacher LLM] -> CoT-enhanced instruction-response pairs
Seed Instructions -> [Selection Agent] -> Filtered by quality/balance
Seed Instructions -> [Verification Agent] -> Factual correctness check
Augmented Dataset -> [Student SFT Training] -> Fine-tuned checkpoint
Teacher Logits -> [Offline Generation] -> Top-K logits stored (Token alignment if needed)
Fine-tuned checkpoint -> [White-box KD Training] -> Divergence minimization on top-K
Final output: DistilQwen2.5 Model

- Critical path:
  1. Data quality from multi-agent pipeline (garbage in → garbage out dominates all downstream effects)
  2. Correct token alignment when teacher/student vocabularies differ (Section 3.2)
  3. Appropriate teacher size selection (Figure 5 shows diminishing returns; 14B→32B→72B yields marginal gains)

- Design tradeoffs:
  - Teacher size: Larger teachers (72B) provide marginal improvement over 32B but cost significantly more for logits generation (Figure 5)
  - K value: K=10 is default; lower K speeds storage/training but risks losing tail knowledge; higher K increases cost with diminishing returns
  - Dataset size: Figure 6 shows improvements diminish as dataset scales; optimal zone appears 10K-100K for white-box KD

- Failure signatures:
  - Semantic drift: Expansion agent changes task category despite constraints (Section 3.1 example shows intended behavior)
  - Vocabulary mismatch errors: If token alignment fails, logits tensors won't correspond correctly
  - Storage bottleneck: Full logits for large datasets with large teachers overwhelm storage systems (motivation for top-K approach)
  - Overfitting to augmented data: Student performs well on benchmarks but poorly on real distribution shift

- First 3 experiments:
  1. **Ablate K value:** Train with K∈{5, 10, 20, 50} on a held-out validation set; plot performance vs. storage/compute cost to find sweet spot for your domain.
  2. **Teacher size scaling:** For your target student size, benchmark 14B vs. 32B vs. 72B teachers; if 14B→32B improvement < 1-2% on your metrics, use 14B for cost efficiency.
  3. **Black-box only vs. full pipeline:** Compare `*` variant (black-box only) against full model on your specific task distribution; if gap is small, consider skipping white-box phase for faster iteration.

## Open Questions the Paper Calls Out

- **Question:** How can the collaborative aspects of model fusion be enhanced to facilitate more dynamic knowledge transfer?
  - Basis in paper: [explicit] The Conclusion explicitly lists "enhance the collaborative aspects of model fusion" as a goal for future work.
  - Why unresolved: The current fusion method relies on progressive integration of offline-generated logits, lacking mechanisms for dynamic or interactive transfer.
  - What evidence would resolve it: A study proposing a new fusion architecture that adapts based on real-time student feedback or varying task complexity.

- **Question:** Does the DistilQwen2.5 framework generalize effectively across diverse domains and low-resource languages?
  - Basis in paper: [explicit] The Limitations section notes that generalizability across diverse domains and languages "remains to be thoroughly evaluated."
  - Why unresolved: The paper focuses on general instruction-following benchmarks and a specific SQL use case, leaving niche domains unexplored.
  - What evidence would resolve it: Evaluation results on specialized domains (e.g., medical, legal) and a broader set of multilingual benchmarks.

- **Question:** To what extent do biases and hallucinations inherent in proprietary teacher models propagate into the distilled student models?
  - Basis in paper: [explicit] The Limitations section acknowledges that "Biases or errors inherent in the teacher models could propagate... affecting their performance and fairness."
  - Why unresolved: The study prioritizes performance improvements (benchmark scores) over the analysis of negative artifact inheritance.
  - What evidence would resolve it: A comparative analysis measuring specific bias metrics or hallucination rates in the student models versus their teachers.

## Limitations

- Multi-agent data augmentation pipeline lacks detailed specifications for agent prompts and quality control thresholds, creating reproducibility challenges
- Top-K approximation (K=10) for white-box KD may not capture domain-specific vocabulary or edge case knowledge, potentially limiting transfer effectiveness
- Industrial validation is limited to SQL completion tasks, with insufficient evaluation across diverse domains and low-resource languages

## Confidence

- **High Confidence:** Sequential application of black-box KD followed by white-box KD improves performance over either method alone; computational efficiency gains from top-K logits storage are well-supported
- **Medium Confidence:** Multi-agent data augmentation produces higher-quality training data, though lack of ablation studies and quality metrics reduces confidence in specific claims
- **Low Confidence:** Smaller student models benefit more from distillation than larger ones; diminishing returns are dataset-specific rather than a general principle, and CoT responses' impact on reasoning lacks direct validation

## Next Checks

1. **Ablation of Multi-Agent Pipeline:** Conduct controlled experiments ablating each agent individually to quantify their specific contributions to final model performance, measuring both task accuracy and reasoning capabilities on held-out benchmarks.

2. **Top-K Sensitivity Analysis:** Systematically vary K∈{5, 10, 20, 50} and measure the trade-off between knowledge retention and computational efficiency across multiple downstream tasks, including domain-specific evaluations to identify vocabulary or reasoning patterns affected by the K parameter.

3. **Teacher Size Efficiency Benchmark:** For each student size (0.5B, 1.5B, 3B, 7B), conduct comprehensive benchmarking comparing 14B, 32B, and 72B teachers across diverse industrial tasks, including cost-benefit analysis of performance gains against computational overhead for logits generation and storage.