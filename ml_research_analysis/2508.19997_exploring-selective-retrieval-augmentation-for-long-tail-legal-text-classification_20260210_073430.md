---
ver: rpa2
title: Exploring Selective Retrieval-Augmentation for Long-Tail Legal Text Classification
arxiv_id: '2508.19997'
source_url: https://arxiv.org/abs/2508.19997
tags:
- legal
- classes
- ledgar
- augmentation
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores Selective Retrieval-Augmentation (SRA) to address
  long-tail label distributions in legal text classification. SRA targets low-frequency
  classes by retrieving semantically relevant clauses from the training set, avoiding
  noise for well-represented classes.
---

# Exploring Selective Retrieval-Augmentation for Long-Tail Legal Text Classification

## Quick Facts
- **arXiv ID:** 2508.19997
- **Source URL:** https://arxiv.org/abs/2508.19997
- **Reference count:** 12
- **Key outcome:** SRA improves macro-F1 by up to 6.0% over baselines on long-tail legal text classification, outperforming LexGLUE models.

## Executive Summary
This paper addresses the challenge of long-tail label distributions in legal text classification by introducing Selective Retrieval-Augmentation (SRA). The method selectively retrieves semantically relevant clauses from the training set for low-frequency classes, avoiding noise injection into well-represented classes. Experiments on LEDGAR (single-label) and UNFAIR-ToS (multi-label) datasets demonstrate that SRA significantly improves macro-F1 scores compared to baselines and full augmentation approaches, with optimal results achieved at a 65% cutoff ratio for low-frequency classes.

## Method Summary
SRA identifies low-frequency labels in the training set and retrieves semantically relevant clauses exclusively for those samples. The retrieval pipeline uses TF-IDF for candidate generation (top-20) followed by SBERT re-ranking to select the most relevant clause. Retrieved context is concatenated to the original input (truncated to 64 tokens) and fed to a RoBERTa-base classifier. The cutoff ratio (e.g., 65%) determines which classes receive augmentation, with experiments showing selective augmentation outperforms full augmentation by preventing noise in high-frequency classes.

## Key Results
- SRA achieves 6.0% improvement in Macro-F1 over baseline on LEDGAR dataset
- SRA outperforms best LexGLUE models on both LEDGAR and UNFAIR-ToS datasets
- Full Retrieval-Augmentation degrades performance (0.827 to 0.816 Macro-F1), while SRA improves it (0.827 to 0.887)
- Optimal cutoff ratio found at 65% for LEDGAR dataset

## Why This Works (Mechanism)

### Mechanism 1
Selective augmentation prevents performance degradation caused by injecting noise into well-represented classes. The system calculates class frequencies and defines a cutoff, performing retrieval only for low-frequency classes. This preserves the integrity of head classes while boosting tail classes, whereas full augmentation adds context to already sufficient data, acting as noise.

### Mechanism 2
Legal clauses are semantically dense enough to serve as their own knowledge base, making external corpora unnecessary. The retrieval pipeline searches exclusively within the training set, leveraging contextual similarity without risking information leakage or domain shift from external data.

### Mechanism 3
Reported performance gains rely on an "oracle" assumption during inference. The pipeline uses ground-truth labels to determine if a sample should be augmented, creating a dependency on knowing the answer before asking the question. This assumption is unrealistic for deployment.

## Foundational Learning

- **Concept: Macro-F1 vs. Micro-F1**
  - **Why needed here:** Macro-F1 treats all classes equally and is essential for evaluating long-tail performance, while Micro-F1 is dominated by frequent classes.
  - **Quick check question:** If your model improves from 90% to 95% accuracy but errors are now evenly distributed across rare classes instead of concentrated in one common class, did Macro-F1 go up or down? (Answer: It likely went up).

- **Concept: TF-IDF vs. Bi-Encoder Retrieval**
  - **Why needed here:** The paper uses a two-stage retrieval where TF-IDF provides fast lexical matching and SBERT provides semantic understanding through embeddings.
  - **Quick check question:** Why would the authors retrieve top-20 with TF-IDF before re-ranking with SBERT, rather than just using SBERT on the whole corpus? (Answer: Computational efficiency).

- **Concept: Long-tail Distribution**
  - **Why needed here:** Understanding that tail classes are structurally disadvantaged with gradient signals drowned out by common classes during training.
  - **Quick check question:** In a dataset with 100 classes where 1 class has 10,000 samples and 99 classes have 10 samples each, what is the majority class accuracy likely to be without augmentation?

## Architecture Onboarding

- **Component map:** Frequency Scanner -> Retrieval Engine (TF-IDF -> SBERT) -> Augmenter -> Classifier
- **Critical path:** The cutoff ratio (α) is the single most important hyperparameter. If α is too low, you don't help enough tail classes. If α is too high, you introduce noise and performance collapses.
- **Design tradeoffs:** Fixed 64-token truncation for retrieved clauses ensures the original text isn't pushed out of the 512-token window, but assumes legal clause essence fits in 64 tokens.
- **Failure signatures:** Semantic drift when retrieved clauses are similar but legally distinct, and context truncation when original input exceeds 448 tokens.
- **First 3 experiments:** 1) Baseline RoBERTa on LEDGAR to verify Macro-F1 ≈ 0.82, 2) Full Retrieval-Augmentation test to confirm performance drops validating selective targeting, 3) Cutoff sweep with α ∈ {20%, 50%, 65%, 90%} to find optimal sweet spot.

## Open Questions the Paper Calls Out
- How can the selective augmentation trigger be replaced with a label-free alternative for realistic deployment?
- Can the optimal cutoff ratio for augmentation be determined automatically for a given dataset?
- How can the retrieval mechanism be modified to filter out semantically similar but label-different clauses?

## Limitations
- The method depends on an unrealistic oracle assumption of having ground-truth labels at inference time to decide whether to augment.
- Effectiveness is tightly coupled to the semantic coherence and completeness of the training set; if training lacks relevant neighbors for tail classes, retrieval fails.
- The 64-token truncation and 512-token limit may not capture full nuance of complex legal clauses, potentially limiting applicability to highly technical documents.

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| SRA's superior Macro-F1 performance on LEDGAR and UNFAIR-ToS | High |
| Legal text is self-contained and training set sufficient for retrieval | Medium |
| Fixed 64-token truncation and oracle assumption robustness | Low |

## Next Checks

1. **Deployable Inference Test:** Replace oracle label check with lightweight classifier or uncertainty metric to identify tail-class samples during inference. Validate Macro-F1 performance remains close to oracle results.

2. **Retrieval Coverage Analysis:** For each tail class, measure semantic similarity of top-1 retrieved clause against ground truth. Quantify percentage of tail samples where retrieval yields relevant, on-topic clause to assess semantic drift risk.

3. **Truncation Impact Study:** Systematically vary retrieved clause truncation length (32, 64, 128, 256 tokens) and evaluate impact on Macro-F1 for different classes. Identify if longer clauses significantly improve performance for specific legal text types.