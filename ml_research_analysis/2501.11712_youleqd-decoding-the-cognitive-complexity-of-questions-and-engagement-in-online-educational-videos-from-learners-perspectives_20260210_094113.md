---
ver: rpa2
title: 'YouLeQD: Decoding the Cognitive Complexity of Questions and Engagement in
  Online Educational Videos from Learners'' Perspectives'
arxiv_id: '2501.11712'
source_url: https://arxiv.org/abs/2501.11712
tags:
- questions
- question
- cognitive
- education
- youtube
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents YouLeQD, a dataset of 57,242 learner-posed
  questions extracted from YouTube educational video comments, and two RoBERTa-based
  classification models for detecting questions and analyzing their cognitive complexity
  using Bloom's Taxonomy. The authors fine-tuned RoBERTa models using Knowledge Distillation
  and data augmentation to classify questions into six cognitive levels (Knowledge,
  Comprehension, Application, Analysis, Evaluation, Synthesis) plus an "Irrelevant"
  category for out-of-distribution examples.
---

# YouLeQD: Decoding the Cognitive Complexity of Questions and Engagement in Online Educational Videos from Learners' Perspectives

## Quick Facts
- arXiv ID: 2501.11712
- Source URL: https://arxiv.org/abs/2501.11712
- Reference count: 40
- This paper presents YouLeQD, a dataset of 57,242 learner-posed questions extracted from YouTube educational video comments, and two RoBERTa-based classification models for detecting questions and analyzing their cognitive complexity using Bloom's Taxonomy.

## Executive Summary
This paper introduces YouLeQD, a dataset of 57,242 learner-posed questions from YouTube educational videos, along with two RoBERTa-based classification models. The first model detects questions with 99.42% F1-score using Knowledge Distillation from GPT-4o, while the second classifies cognitive complexity using Bloom's Taxonomy with 84.5% weighted average F1-score. The study reveals that most questions fall under the Knowledge level (44-48% across subjects), with slight variations in cognitive complexity across different STEM subjects. The dataset and models are publicly available to support further research in educational AI applications.

## Method Summary
The authors developed two RoBERTa-based classification models: one for question detection and another for Bloom's Taxonomy cognitive complexity classification. For question detection, they fine-tuned RoBERTa using Knowledge Distillation from GPT-4o with a combined loss function (LCE + α·LKL), achieving 99.42% F1-score. For BT classification, they employed a two-stage approach: stage 1 trained on DASQBT dataset, then identified low-confidence samples as "Irrelevant" class, followed by stage 2 retraining with augmented data. The augmentation used GPT-4o to generate synthetic questions, though this showed negative transfer on the DASQBT dataset. The final model achieved 84.5% weighted F1-score on human-labeled samples.

## Key Results
- Question detection model achieved 99.42% F1-score using Knowledge Distillation from GPT-4o
- BT classification model reached 84.5% weighted average F1-score on human-labeled samples
- Most questions fall under the Knowledge level (44-48% across subjects), with slight variations across STEM disciplines
- Inverse relationship between question popularity and cognitive complexity, but positive correlation between higher cognitive levels and interaction rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation from GPT-4o to RoBERTa improves question detection accuracy by softening label boundaries.
- Mechanism: A teacher LLM (GPT-4o) provides probability distributions over labels rather than hard classifications. The student RoBERTa model is trained using a combined loss: L = LCE + α · LKL, where KL divergence penalizes deviation from the teacher's soft labels. This helps the student model handle ambiguous cases (e.g., questions without marks, rhetorical questions) that hard labels would misclassify.
- Core assumption: The teacher model's probability distribution encodes meaningful uncertainty that the student model can learn to replicate.
- Evidence anchors:
  - [section] "We used GPT-4o as the teacher model... utilized the probability distribution of the predictions in the optimization process." (Section III-B)
  - [section] "The model achieved an f1-score of 99.42%... a 1.31% improvement over the initial model trained without KD." (Section IV-A)
  - [corpus] Weak direct corpus support; related work on LLM annotation exists but not specific to KD for this task.
- Break condition: If teacher model is overconfident on incorrect predictions, soft labels may amplify error rather than calibrate uncertainty.

### Mechanism 2
- Claim: Two-stage training with OOD detection improves BT classification by explicitly handling noisy, informal YouTube comments.
- Mechanism: Stage 1 trains on clean educational data (DASQBT). The trained model scores unlabeled YouTube questions via softmax probability; low-confidence samples are flagged as "Irrelevant." Stage 2 retrains with this new class. This distribution shift handling prevents the model from forcing formal taxonomy labels onto informal comments.
- Core assumption: Low softmax probability reliably indicates out-of-distribution or low-quality samples rather than just difficult but valid examples.
- Evidence anchors:
  - [section] "We identified the top 500 examples with low maximum/predicted class probability... and labeled them as belonging to the 'Irrelevant' class." (Section III-C.2)
  - [section] "This method has improved the model's F1-score in predicting BT levels to 0.845, an increase of 2.0%." (Section IV-B, Table V)
  - [corpus] Weak corpus support; Hendrycks & Gimpel cited for softmax confidence but not empirically validated in this domain.
- Break condition: If low-confidence examples include valid higher-order questions from informal contexts, they will be misclassified as irrelevant, reducing recall for complex questions.

### Mechanism 3
- Claim: LLM-based data augmentation expands training data but can introduce distribution shift.
- Mechanism: GPT-4o generates synthetic questions labeled by BT level using few-shot prompting. The augmented dataset increases sample diversity but may shift the training distribution away from real YouTube question characteristics.
- Core assumption: Generated questions maintain sufficient alignment with real learner-posed questions to improve generalization.
- Evidence anchors:
  - [section] "Upon conducting data augmentation, we observe that the effects of data augmentation on this dataset are negative... F1-score from 0.836 to 0.825." (Section IV-B)
  - [section] High GPT-original label agreement (0.989 F1) suggests label quality, not generation quality, is the issue. (Table III)
  - [corpus] Multi-Agent Collaborative Framework (arXiv:2511.03958) notes similar struggles with "precisely control[ling] problem complexity" in generated content.
- Break condition: If generated questions are too formal or structured compared to real comments, augmentation will degrade performance on informal data.

## Foundational Learning

- Concept: **Knowledge Distillation (KD)**
  - Why needed here: Enables smaller RoBERTa model to learn from GPT-4o's soft probability outputs, handling ambiguous question boundaries.
  - Quick check question: What is the role of temperature τ in softening probability distributions, and what happens if τ is set too low?

- Concept: **Bloom's Taxonomy (BT) Cognitive Levels**
  - Why needed here: Provides the six-level classification schema (Knowledge → Synthesis) for labeling question complexity.
  - Quick check question: Why might "how to" questions be classified as Application rather than Knowledge?

- Concept: **Out-of-Distribution (OOD) Detection via Softmax**
  - Why needed here: Identifies informal or nonsensical YouTube comments that don't fit formal BT categories.
  - Quick check question: What is the limitation of using softmax probability as a confidence estimate for OOD detection?

## Architecture Onboarding

- Component map:
  YouTube API → Comments → [RoBERTa + KD Question Detector] → Extracted Questions → [RoBERTa BT Classifier (2-stage)] → Low-confidence samples → "Irrelevant" class → Retrain → Final BT labels (6 levels + Irrelevant)

- Critical path:
  1. Question detection must be high-precision (99.42% F1 achieved) before BT classification—errors cascade.
  2. OOD detection threshold selection directly affects "Irrelevant" class size and downstream recall.
  3. Human annotation sample (n=300) is the ground truth for final evaluation, not training.

- Design tradeoffs:
  - **Data augmentation**: Increases diversity but introduces distribution shift; negative on DASQBT, positive on human-labeled YouTube sample.
  - **Model scale**: RoBERTa chosen for efficiency; teacher GPT-4o not deployable at inference time.
  - **Threshold selection**: 500 low-confidence samples for "Irrelevant" class is arbitrary; no sensitivity analysis reported.

- Failure signatures:
  - High precision (0.970) but low recall on higher BT levels in human-labeled test—model is conservative, over-classifying as "Irrelevant."
  - PMI analysis shows weak verb-BT associations for Knowledge and Application in YouTube data—informal language doesn't match formal taxonomy cues.
  - Confusion between Comprehension and Knowledge (largest off-diagonal in confusion matrix).

- First 3 experiments:
  1. **Reproduce question detection with KD ablation**: Train with and without KL divergence loss on the interrogative classification dataset; measure F1 delta.
  2. **OOD threshold sensitivity**: Vary the low-confidence sample count (e.g., 200, 500, 1000) and measure impact on human-labeled test recall across BT levels.
  3. **Domain-specific augmentation**: Generate synthetic questions using YouTube comment style prompts (colloquial, incomplete) instead of formal educational prompts; compare to GPT-generated formal questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the distribution of cognitive complexity levels in learner-posed questions differ between formal educational platforms (e.g., MOOCs, LMS discussion forums) and informal platforms like YouTube?
- Basis in paper: [inferred] The authors acknowledge that "YouTube is not primarily an educational platform" yet draw conclusions for developing "effective AI models for education" broadly. The dataset comprises only informal, self-directed learners.
- Why unresolved: No comparison data exists from formal educational settings in the study. The characteristics of self-selected YouTube learners may systematically differ from enrolled students in structured courses.
- What evidence would resolve it: Replicate the same Bloom's Taxonomy classification methodology on questions from platforms like Coursera, edX, or university LMS discussion boards and compare distributions.

### Open Question 2
- Question: What factors causally drive the positive correlation between higher cognitive complexity questions and increased interaction rates?
- Basis in paper: [inferred] The paper reports a correlation—"a positive correlation between interaction rate and the higher cognitive levels, specifically from Analysis to Synthesis"—but offers no causal explanation.
- Why unresolved: The observational nature of YouTube comment data precludes causal inference. Multiple confounds (video quality, topic difficulty, commenter expertise) could explain the relationship.
- What evidence would resolve it: Controlled experiments manipulating question complexity in educational video contexts, or qualitative analysis of reply chains to understand why higher-level questions attract more engagement.

### Open Question 3
- Question: Can alternative data augmentation strategies overcome the negative transfer observed when using GPT-generated questions for Bloom's Taxonomy classification?
- Basis in paper: [explicit] The authors note "we observe that the effects of data augmentation on this dataset are negative. This outcome is expected and explainable; given the inherent discrepancies between the original questions and those generated by GPT."
- Why unresolved: The paper does not investigate why GPT-4o's high agreement with original labels (98.9% F1-score) still produces negative transfer, nor tests alternative augmentation approaches.
- What evidence would resolve it: Systematic comparison of augmentation strategies (back-translation, paraphrase models, curriculum-based synthetic data) measuring their impact on downstream classification of real YouTube questions.

## Limitations
- Limited generalization to non-STEM domains: The study focuses exclusively on STEM subjects, leaving unclear how well the models and patterns generalize to humanities or social sciences.
- Sampling bias in YouTube data: The dataset is constrained to 1,762 STEM lecture videos from unspecified channels, potentially skewing question types toward certain teaching styles or difficulty levels.
- Arbitrary OOD detection threshold: The selection of 500 low-confidence samples for the "Irrelevant" class lacks principled justification or sensitivity analysis, significantly impacting final model recall.

## Confidence

**High Confidence (Mechanistic Claims)**: The core mechanisms of knowledge distillation for question detection and two-stage BT classification with OOD handling are well-supported by the reported results.

**Medium Confidence (Domain Patterns)**: The reported cognitive complexity distributions across subjects and the inverse relationship between question popularity and cognitive complexity are based on corpus analysis but lack statistical significance testing.

**Low Confidence (Generalizability)**: Claims about the dataset's utility for educational AI applications and the broader implications for understanding learner engagement are speculative.

## Next Checks
1. **Statistical significance testing**: Perform chi-squared tests on the cognitive complexity distributions across subjects to determine if observed differences are statistically significant rather than sampling artifacts.

2. **Domain generalization experiment**: Test the BT classification model on questions from non-STEM educational videos or different question types to evaluate cross-domain performance and identify domain-specific failure modes.

3. **OOD threshold sensitivity analysis**: Systematically vary the low-confidence sample count (200, 500, 1000, 2000) and measure the impact on human-labeled test recall across all BT levels. Plot precision-recall curves to identify optimal operating points and quantify the tradeoff between recall and precision for higher cognitive levels.