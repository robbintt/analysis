---
ver: rpa2
title: Near-Optimality of Contrastive Divergence Algorithms
arxiv_id: '2510.13438'
source_url: https://arxiv.org/abs/2510.13438
tags:
- sgdw
- init
- logz
- have
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the statistical efficiency of the Contrastive\
  \ Divergence (CD) algorithm for training unnormalized models. While prior work established\
  \ an O(n\u207B\xB9/\xB3) asymptotic convergence rate, this paper proves that CD\
  \ can achieve the parametric O(n\u207B\xB9/\xB2) rate under certain regularity conditions."
---

# Near-Optimality of Contrastive Divergence Algorithms

## Quick Facts
- **arXiv ID:** 2510.13438
- **Source URL:** https://arxiv.org/abs/2510.13438
- **Reference count:** 40
- **Key outcome:** CD can achieve the parametric $O(n^{-1/2})$ convergence rate under regularity conditions, improving upon prior $O(n^{-1/3})$ bounds.

## Executive Summary
This paper analyzes the statistical efficiency of Contrastive Divergence (CD) algorithms for training unnormalized models, specifically exponential family distributions. While previous work established a slower $O(n^{-1/3})$ asymptotic convergence rate for CD, this paper proves that CD can achieve the parametric $O(n^{-1/2})$ rate under certain regularity conditions. The analysis covers both online and offline settings with various batching schemes. The paper shows that averaging CD iterates yields an estimator with asymptotic variance close to the Cramér-Rao lower bound, making it near-optimal. The theoretical results improve upon previous work by providing non-asymptotic convergence guarantees and relaxing some assumptions about the Markov kernels used in the algorithm.

## Method Summary
The paper analyzes Contrastive Divergence (CD) algorithms for parameter estimation in unnormalized exponential family models. The method involves running $m$ steps of a Markov kernel from each data point to approximate the negative phase gradient, then updating parameters via stochastic gradient descent. Two variants are studied: online CD (using data once) and offline CD (reusing data batches). The key insight is that averaging the parameter iterates recovers near-optimal statistical efficiency. The analysis assumes strong convexity of the negative log-likelihood, restricted spectral gap conditions for the Markov kernel, and sub-exponential tail bounds for the gradient noise.

## Key Results
- Proves CD can achieve the parametric $O(n^{-1/2})$ convergence rate under strong convexity and sufficient MCMC mixing
- Shows averaging CD iterates yields asymptotic variance close to the Cramér-Rao lower bound (within factor of 4)
- Establishes $\tilde{O}(n^{-1/2})$ rate for offline CD with a $\sqrt{\log n}$ penalty due to data-iterate correlations
- Provides non-asymptotic convergence guarantees and relaxes assumptions about Markov kernels

## Why This Works (Mechanism)

### Mechanism 1: Strong Convexity Compensation for MCMC Bias
The bias from finite MCMC steps doesn't prevent convergence because strong convexity of the negative log-likelihood acts as a restoring force. The effective strong convexity constant is reduced by a bias term dependent on the spectral gap $\alpha$. As long as $m$ is large enough to keep $\tilde{\mu}_m > 0$, the optimization error contracts, allowing the algorithm to converge despite gradient approximation inaccuracy.

### Mechanism 2: Tail Control in Offline Correlation
Offline CD introduces correlations between iterates and training data, but these can be managed by controlling tail probabilities of gradient error. The paper uses a tail decomposition to bound the probability of large deviations in the gradient estimator, preventing error accumulation across epochs. This requires sub-exponential tail conditions or specific moment bounds.

### Mechanism 3: Averaging for Statistical Efficiency
While standard CD iterates converge at the parametric rate, their asymptotic variance is suboptimal. Averaging iterates (Polyak-Ruppert averaging) reduces the variance term that dominates non-averaged recursions. The averaged estimator achieves minimal possible variance bound provided $m$ scales logarithmically with $n$.

## Foundational Learning

- **Concept: Exponential Families & Strong Convexity**
  - **Why needed here:** The analysis relies on negative log-likelihood being $C^\infty$, convex, and strictly strongly convex. This geometric property pulls biased estimates back toward the true parameter.
  - **Quick check question:** Can you explain why the strong convexity constant $\mu$ appears in the denominator of error bounds, making convergence faster for "more curved" likelihoods?

- **Concept: Spectral Gap of Markov Chains**
  - **Why needed here:** The paper uses a restricted spectral gap ($\alpha < 1$) to quantify MCMC mixing relative to sufficient statistics. The rate at which $\alpha^m$ vanishes determines required MCMC steps to kill bias.
  - **Quick check question:** If a distribution is multi-modal, would you expect the spectral gap $\alpha$ to be closer to 0 or 1, and how would that affect required MCMC steps $m$?

- **Concept: Stochastic Approximation Recursions**
  - **Why needed here:** The proof technique follows standard stochastic optimization by unrolling recursions. Understanding how contraction factor and noise accumulation balance is essential to read the theorems.
  - **Quick check question:** In the recursion $\delta_t \le (1 - 2\eta \tilde{\mu})\delta_{t-1} + \eta^2 \tilde{\sigma}^2$, which term drives convergence to a specific error radius, and which determines speed?

## Architecture Onboarding

- **Component map:** Data Stream -> MCMC Engine -> Gradient Estimator -> Optimizer -> Averager
- **Critical path:** The MCMC Engine is the primary bottleneck. The required length $m > \log(\sigma C_\chi / \mu) / \log|\alpha|$ determines computational cost.
- **Design tradeoffs:**
  - Online CD is statistically more efficient (exact $n^{-1/2}$) but requires data streaming; offline CD is standard for fixed datasets but has $\sqrt{\log n}$ penalty
  - Polyak-Ruppert averaging requires $\beta \in (1/2, 1)$; standard diminishing step-sizes outside this range fail to achieve optimal Cramér-Rao bound
  - Lower $m$ increases bias, potentially overwhelming convexity and preventing convergence
- **Failure signatures:**
  - Stalling/Drift: If model is ill-specified or tails are heavy, bias terms may become unbounded
  - Variance Explosion: If spectral gap $\alpha$ is near 1, required $m$ becomes infeasible or variance dominates
- **First 3 experiments:**
  1. Train RBM or Gaussian Mixture on synthetic data; vary $m$ to find threshold where $\tilde{\mu}_m > 0$ matches theoretical bound
  2. Compare online vs. offline CD on fixed dataset; plot $L_2$ error vs. $n$ to verify $\sqrt{\log n}$ factor overhead
  3. Compare final parameter $\psi_n$ vs. averaged parameter $\bar{\psi}_n$; compute empirical covariance to verify near-optimality

## Open Questions the Paper Calls Out

- Can near-optimal guarantees extend to general unnormalized models where log-density is non-linear in parameters? (Basis: explicit statement about future work for more general unnormalized models)
- Does offline CD with reshuffling (SGDo) strictly improve upon sampling with replacement (SGDw) convergence rates? (Basis: explicit mention of technical hurdles for CD-SGDo analysis)
- Can the $\sqrt{\log n}$ factor be removed from offline convergence rate to achieve exact parametric optimality? (Basis: inferred from discussion of offline stationary term being "asymptotically suboptimal")

## Limitations
- Analysis assumes strict strong convexity, excluding heavy-tailed or multimodal distributions where CD is commonly applied
- Spectral gap requirement demands careful kernel design despite being weaker than mixing time conditions
- Tail bounds may fail for distributions with heavier-than-sub-exponential tails, potentially degrading convergence rate
- Theoretical bounds require $m$ to scale with $n$, creating computational trade-off not fully resolved

## Confidence
- **High Confidence:** $O(n^{-1/2})$ convergence rate for online CD under strong convexity (Theorem 3.2)
- **Medium Confidence:** Near-optimality of averaged estimators (Theorem 3.3)
- **Medium Confidence:** $\sqrt{\log n}$ penalty for offline CD (Theorem 4.2)

## Next Checks
1. For a specific exponential family (e.g., Gaussian), analytically derive strong convexity constant $\mu$ and spectral gap $\alpha$, then verify theoretical condition $m > \log(\sigma C_\chi/\mu) / \log|\alpha|$ matches empirical threshold where convergence rate transitions from $O(n^{-1/3})$ to $O(n^{-1/2})$.

2. Implement CD on a distribution with heavier-than-sub-exponential tails (e.g., Student's t with low degrees of freedom) to empirically test whether tail control fails and convergence rate degrades as predicted.

3. Run multiple independent CD training sessions on same dataset, compute both final iterate $\psi_n$ and averaged estimator $\bar{\psi}_n$, and compare their empirical covariance matrices against inverse Fisher Information to verify claimed near-optimality.