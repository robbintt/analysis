---
ver: rpa2
title: '2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with
  Disentangled Control and Sequence Consistency'
arxiv_id: '2512.05557'
source_url: https://arxiv.org/abs/2512.05557
tags:
- control
- arxiv
- character
- identity
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating coherent, controllable
  visual narratives with consistent character identities across sequential frames.
  The authors introduce 2K-Characters-10K-Stories, a large-scale dataset containing
  2,000 uniquely stylized characters across 10,000 stories (75,000+ illustrations),
  explicitly designed to enable fine-grained control over transient attributes such
  as pose, expression, and composition.
---

# 2K-Characters-10K-Stories: A Quality-Gated Stylized Narrative Dataset with Disentangled Control and Sequence Consistency

## Quick Facts
- arXiv ID: 2512.05557
- Source URL: https://arxiv.org/abs/2512.05557
- Authors: Xingxi Yin; Yicheng Li; Gong Yan; Chenglin Li; Jian Zhao; Cong Huang; Yue Deng; Yin Zhang
- Reference count: 40
- Primary result: Introduces 2K-Characters-10K-Stories dataset enabling fine-grained control over 2,000 stylized characters across 10,000 stories, achieving +32.65 pose alignment improvement over baselines

## Executive Summary
This paper introduces a novel dataset and pipeline for controllable visual storytelling with consistent character identities across sequential frames. The 2K-Characters-10K-Stories dataset contains 75,000+ illustrations of 2,000 uniquely stylized characters, built using a rigorous Human-in-the-Loop pipeline with expert-verified character templates, LLM-guided narrative planning, and a Quality-Gated loop with automated evaluation and correction. The key innovation is a decoupled control scheme that separates persistent identity from transient attributes (pose, expression, composition), enabling precise, quantifiable manipulation. Models fine-tuned on this dataset significantly outperform baseline models in control fidelity and rival closed-source models in narrative coherence.

## Method Summary
The method employs a three-phase Human-in-the-Loop pipeline. Phase I uses Flux to generate character templates with VIEScore ≥7.0 quality gate, followed by 3-expert panel validation. Phase II employs an LLM ensemble (Gemini Pro 2.5 + GPT-4o) to generate structured JSON story scripts with 7-12 frames per story, validated by human reviewers. Phase III implements a two-stage synthesis approach: first generating reference images that fuse character identity with transient attributes using an editing model, then using these references with Gemini-2.5-flash-image for final frame synthesis. A Quality-Gated loop with MMLM evaluation, Auto-Prompt Tuning, and Local Image Editing ensures pixel-level consistency, with human escalation for ~2% of samples. The final model is fine-tuned on 75K+ IQCC-verified triplets.

## Key Results
- Achieves +32.65 improvement in pose alignment over baseline OmniGen2
- Surpasses baseline models in control fidelity across all metrics (pose, expression, composition)
- Rivals closed-source models in narrative coherence while exceeding them in fine-grained control accuracy
- Maintains high identity consistency across sequential frames (CIDS scores)

## Why This Works (Mechanism)

### Mechanism 1: Decoupled Control via Semantic Identifiers
Separating persistent identity from transient attributes using discrete semantic identifiers enables quantifiable, unambiguous control. The system assigns unique text-based identifiers (C_ID, P_ID, E_ID, C_TAG) forming a structured control space (28 poses, 12 expressions, 21 compositions). During training, the model learns to bind each identifier to its corresponding visual feature without cross-interference, because the dataset provides explicit, non-overlapping supervision for each attribute class.

### Mechanism 2: Quality-Gated Iterative Correction Loop
A multi-stage automated evaluation and correction cycle enforces pixel-level consistency. After initial generation, a separate MMLM Arbitrator performs Triple-Check validation across three criteria. Failures route to targeted resolution: global errors trigger Auto-Prompt Tuning with iterative prompt optimization (averaging 2.19 iterations); localized errors trigger Local Image Editing. Hard failures after 3 attempts escalate to human review.

### Mechanism 3: Reference-Guided Two-Stage Synthesis
Generating intermediate reference images that explicitly fuse identity with transient attributes provides stronger structural control than text-only conditioning. Rather than conditioning final synthesis on text alone, the pipeline first generates per-character reference images using dedicated editing models that combine C_ID with P_ID/E_ID. These visual anchors then guide the main frame synthesis.

## Foundational Learning

- **Concept: Disentangled Representations**
  - Why needed: The entire dataset design hinges on the assumption that identity and transient attributes can be learned as separable factors.
  - Quick check: Can you explain why training a model on coupled identity+pose data prevents reliable pose control at inference time?

- **Concept: Human-in-the-Loop Data Curation**
  - Why needed: The pipeline uses three human validation checkpoints with explicit consensus protocols.
  - Quick check: What types of errors does HV3 catch that automated Triple-Check cannot?

- **Concept: MMLM-Based Evaluation**
  - Why needed: The quality-gating depends on MMLMs as arbitrators.
  - Quick check: Why does the pipeline use a separate MMLM for evaluation rather than the generation MMLM self-evaluating?

## Architecture Onboarding

- **Component map:** Phase I: Flux → VIEScore filtering → Human Validation 1 (3-expert panel, 2/3 consensus)
  Phase II: LLM Ensemble → Structured JSON script generation → Human Validation 2
  Phase III Sub-Loop: Editing model → Reference generation → VIEScore gate → Auto-Prompt Tuning
  Phase III Main Loop: Gemini-2.5-flash-image → Triple-Check MMLM Arbitrator → IQCC routing

- **Critical path:** Reference generation (Sub-Loop) → Reference-guided synthesis (Main Loop) → Triple-Check evaluation

- **Design tradeoffs:** Scale vs. Quality (human validation caps throughput), Control Fidelity vs. Creative Flexibility (discrete control space limits expressivity), Training Input Simplicity (excludes auxiliary reference images from training)

- **Failure signatures:** Identity drift across frames (insufficient C_ID binding), Pose/expression misalignment (decoupling failure), Character omission in multi-character scenes (composition failure), High HV3 escalation rate (MMLM evaluation failure)

- **First 3 experiments:**
  1. Train on dataset with shuffled P_ID/E_ID labels to quantify contribution of structured labeling vs. data scale
  2. Compare models trained with vs. without reference images as auxiliary inputs during training
  3. Evaluate on 200 OOD characters using novel pose/expression combinations not seen during training

## Open Questions the Paper Calls Out

### Open Question 1
Can the Human-in-the-Loop quality assurance steps be effectively automated while maintaining the same fidelity standards? The paper acknowledges the pipeline is "inherently resource-intensive" and future work will focus on automation, but hasn't validated whether automated quality assurance can match human expert validation fidelity.

### Open Question 2
Can the decoupled control framework transfer effectively to photorealistic real-world human characters? The current work focuses on stylized/illustration domains with virtual characters, but photorealistic humans may have stricter identity consistency requirements that could expose limitations of the approach.

### Open Question 3
How does the fixed discrete control vocabulary constrain expressiveness and naturalness of generated narratives? The bounded control space (28 poses, 12 expressions, 21 compositions) may limit intermediate expressions and nuanced motions required for natural storytelling.

## Limitations

- The Human-in-the-Loop pipeline is resource-intensive, limiting immediate scalability
- Evaluation relies on automated MMLM arbiters whose reliability remains unreported
- The fixed discrete control vocabulary may constrain narrative expressiveness
- No empirical validation of embedding space disentanglement is provided

## Confidence

- **High confidence**: Dataset construction methodology and quality-gating mechanisms are well-documented and reproducible
- **Medium confidence**: Quantitative performance claims are supported by described evaluation pipeline but MMLM evaluation reliability is not independently verified
- **Low confidence**: Claims about rivaling closed-source models lack public baseline comparisons or independent replication

## Next Checks

1. Run IQCC Triple-Check evaluation on held-out validation set with known correct/incorrect samples to measure false positive/negative rates
2. Test fine-tuned model on novel character templates with out-of-distribution pose/expression combinations to verify generalization
3. Compare MMLM arbitrator scores against human expert ratings on stratified sample to quantify evaluation bias and reliability