---
ver: rpa2
title: 'Readme_AI: Dynamic Context Construction for Large Language Models'
arxiv_id: '2509.19322'
source_url: https://arxiv.org/abs/2509.19322
tags:
- data
- readme
- context
- hedgehog
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReadmeAI, a specification for dynamically
  constructing query-specific context for Large Language Models (LLMs). The approach
  addresses the problem of LLMs providing inaccurate or hallucinated responses due
  to lack of appropriate context.
---

# Readme_AI: Dynamic Context Construction for Large Language Models

## Quick Facts
- arXiv ID: 2509.19322
- Source URL: https://arxiv.org/abs/2509.19322
- Authors: Millie Vyas; Timothy Blattner; Alden Dima
- Reference count: 11
- This paper introduces Readme_AI, a specification for dynamically constructing query-specific context for Large Language Models (LLMs).

## Executive Summary
This paper addresses the problem of LLMs providing inaccurate or hallucinated responses due to lack of appropriate context. Readme_AI is a specification where data source owners create metadata files containing validated information about their resources. A prototype Model Context Protocol (MCP) server implements this specification, fetching and processing metadata to build XML-formatted context for LLMs. Testing with the NIST Hedgehog library demonstrates significant improvements in accuracy and code generation capabilities.

## Method Summary
The method involves data source owners creating Readme_AI.json metadata files at the root of their repositories. An MCP server implements the specification using Python and FastMCP, parsing the JSON and executing type-specific handlers (fetch, crawl, download) to aggregate context. The server constructs XML-formatted context strings that are injected into LLM prompts. The system dynamically constructs context at query time rather than relying on pre-trained weights.

## Key Results
- LLM correctly identifies Hedgehog library as C++ parallel computation framework (vs. hallucinated PDE solver) after context injection
- LLM generates working parallel C++ code for particle simulator using structured API context
- Dynamic context construction significantly reduces hallucinations for niche libraries

## Why This Works (Mechanism)

### Mechanism 1: Owner-Curated Grounding
The system bypasses the LLM's reliance on probabilistic weights by injecting owner-validated metadata as "ground truth" context. This forces the model to reason over provided facts rather than recalling potentially absent or incorrect training data. The core assumption is that data owners accurately populate and validate their Readme_AI.json files.

### Mechanism 2: Dynamic Resource Aggregation
The specification uses extensible types (fetch, crawl, download) to aggregate live data from git repositories, websites, and PDFs at query time. This ensures the context reflects the current state of the library, not a frozen snapshot from training. The MCP server must have network access and permissions to clone repositories and download external files.

### Mechanism 3: Structured XML Context Injection
The tool converts JSON structure into XML format with specific tags like <API> and <PAPERS>. This delineation helps the model distinguish between descriptive text, code examples, and API references, reducing context confusion. The specific LLM used must have a strong bias toward obeying XML tag structures during reasoning.

## Foundational Learning

- **Model Context Protocol (MCP)**
  - Why needed: This is the transport layer used by the tool. Understanding MCP is required to implement or modify the server that fetches and processes the Readme_AI.json.
  - Quick check: How does an MCP server differ from a standard REST API in the context of an LLM client?

- **Context Window Management**
  - Why needed: The paper explicitly notes the risk of generating ~97k tokens for a single library. Understanding token limits and costs is critical for the proposed "agentic" future work or subsetting strategies.
  - Quick check: If a Readme_AI.json instructs a crawl of a massive documentation site, what fails first: the crawler, the context window, or the user's budget?

- **Hallucination vs. Ignorance**
  - Why needed: The paper illustrates that LLMs often invent answers (hallucinate) rather than admitting ignorance for niche topics like the Hedgehog library.
  - Quick check: Why does providing "ground truth" context mitigate hallucination more effectively than prompt engineering alone?

## Architecture Onboarding

- **Component map:** Source -> Orchestrator -> Logic -> Consumer
  Readme_AI.json (Root of target repo) -> Readme_AI MCP Server (Python/FastMCP) -> Handlers for fetch (GitPython), crawl (BeautifulSoup), download (PyPDF) -> LLM Client (Receives XML context)

- **Critical path:**
  1. User queries LLM with a repo URL
  2. MCP Server clones/pulls the repo
  3. Server parses Readme_AI.json
  4. Server executes type-specific handlers
  5. Server constructs XML string
  6. XML is injected into the LLM prompt

- **Design tradeoffs:**
  - Centralization vs. Decentralization: Unlike Context7 (centralized repo), this relies on distributed Readme_AI.json files. This improves freshness but requires buy-in from every repo owner.
  - Completeness vs. Cost: The paper demonstrates high accuracy but with massive token usage (97k+). Subsetting (Future Work) reduces cost but risks omitting critical details.

- **Failure signatures:**
  - Silent Hallucination: If Readme_AI.json is missing, the LLM defaults to training data without the user knowing the tool wasn't triggered
  - Context Explosion: Unbounded crawl operations including entire websites, crashing the context window
  - Security Risks: download types fetching malicious PDFs or fetch types exposing local file structures

- **First 3 experiments:**
  1. Run the exact Hedgehog experiment to verify the divergence between baseline (hallucinated Python code) and tool-enabled response (C++ code)
  2. Create a Readme_AI.json with recursive crawl tag pointing to large documentation site to observe context truncation
  3. Modify tool to output Markdown instead of XML and compare LLM's adherence to API constraints

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can context be efficiently subsetted to prevent token explosion (e.g., 97k tokens) without omitting critical query-relevant information?
- Basis: Section 5.1 identifies context size as a limitation; Section 7 proposes investigating subsetting strategies
- Why unresolved: Current prototype transmits full context; proposed "agentic approach" is planned but not implemented
- What evidence would resolve it: Implementation of context-filtering mechanism with benchmarks showing reduced token usage while maintaining accuracy

### Open Question 2
- Question: What security architectures are necessary to mitigate backdoor attacks when dynamically fetching context from external sources?
- Basis: Section 7 explicitly states analysis on security controls related to this approach will be needed
- Why unresolved: Current system dynamically fetches data without "approved/blocked lists" or validation mechanisms
- What evidence would resolve it: Formal threat model and security framework demonstrating safe handling of untrusted external metadata

### Open Question 3
- Question: Does integrating structured dataset specifications (e.g., Frictionless Data Packages) significantly improve an LLM's ability to perform sophisticated data analysis?
- Basis: Section 7 hypothesizes that including Data Packages will allow for "more sophisticated data analysis"
- Why unresolved: Current specification lacks the proposed "new 'type' for handling Data Packages"
- What evidence would resolve it: Comparative tests showing LLM identifying trends or generating visualizations with vs. without structured data types

## Limitations

- Token Explosion Risk: The system demonstrates high accuracy but with massive context usage (97k+ tokens), creating cost and technical constraints
- Dependency on External Resources: Effectiveness relies on successful fetching of external resources, which can fail due to network issues or authentication requirements
- Accuracy Depends on Data Owner: The core claim assumes Readme_AI.json files contain accurate, validated metadata, but no validation mechanism is described

## Confidence

- High Confidence: The basic mechanism of dynamic context construction through owner-provided metadata is sound and technically implementable
- Medium Confidence: The qualitative improvement in responses is demonstrated but relies on a single example; XML format's advantage over alternatives is asserted but not proven
- Low Confidence: Claims about cost-effectiveness, scalability, and consistent tool invocation behavior lack supporting evidence; unimplemented subsetting strategy's effectiveness remains speculative

## Next Checks

1. **Multi-Library Validation:** Test the system across 10-15 diverse niche libraries to verify the hallucination reduction claim generalizes across different domains and library sizes.

2. **Context Format Comparison:** Implement an alternative Markdown-based context generator and conduct A/B testing to measure whether XML provides measurable advantages in LLM code generation accuracy.

3. **Failure Mode Analysis:** Systematically test scenarios where external resources are unavailable to quantify how often context construction fails and how the system degrades gracefully.