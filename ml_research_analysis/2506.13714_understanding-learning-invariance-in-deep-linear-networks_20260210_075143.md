---
ver: rpa2
title: Understanding Learning Invariance in Deep Linear Networks
arxiv_id: '2506.13714'
source_url: https://arxiv.org/abs/2506.13714
tags:
- linear
- data
- networks
- learning
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares three approaches for achieving invariance
  in deep linear networks: data augmentation, regularization, and hard-wiring. The
  authors analyze the optimization landscapes of these approaches in the context of
  mean squared error regression with rank-bounded linear maps.'
---

# Understanding Learning Invariance in Deep Linear Networks

## Quick Facts
- arXiv ID: 2506.13714
- Source URL: https://arxiv.org/abs/2506.13714
- Reference count: 40
- Primary result: Data augmentation and hard-wiring achieve identical critical points (saddles and global optimum) in rank-constrained linear networks; regularization converges to same solution with continuous path

## Executive Summary
This paper analyzes three approaches for achieving invariance in deep linear networks: data augmentation, explicit regularization, and architectural hard-wiring. The authors prove that data augmentation with unitary group actions produces the same critical points as hard-wiring (consisting only of saddles and the global optimum), while regularization introduces additional critical points but converges to the same solution as λ→∞. The regularization path is continuous and provides a smooth interpolation between unconstrained and invariant solutions. Experiments on MNIST rotation tasks validate these theoretical findings, showing that data augmentation and hard-wiring achieve similar performance in late training stages, while appropriately tuned regularization can also match their results.

## Method Summary
The paper studies mean squared error regression with rank-bounded linear maps invariant to group actions (specifically 90° rotations on MNIST). Three approaches are compared: (1) data augmentation - expanding dataset with all group-transformed samples; (2) hard-wiring - explicit architectural constraints ensuring W·G=0 via basis projection; (3) regularization - unconstrained optimization with penalty λ‖WG‖²F added to loss. The theoretical analysis characterizes critical points in function space, proving equivalence between data augmentation and hard-wiring under unitary representations, while regularization provides a continuous path to invariance. Experiments use a two-layer linear network with 5 hidden units trained on MNIST (excluding digit 9) with Adam optimizer.

## Key Results
- Data augmentation and hard-wiring produce identical critical points consisting solely of saddles and the global optimum
- Regularization introduces more critical points but converges to the same invariant solution as λ→∞
- The regularization path is continuous, providing smooth interpolation between function spaces
- Data augmentation requires 4× more samples than hard-wiring for the same invariance performance
- Cross-entropy loss may produce non-invariant critical points, unlike MSE

## Why This Works (Mechanism)

### Mechanism 1
Data augmentation with unitary group actions implicitly constrains optimization to the invariant subspace, making the effective objective identical to explicit architectural constraints. The group averaging operator projects the target matrix onto the invariant subspace when the representation is unitary.

### Mechanism 2
Regularization modifies the target matrix from Z to Z(λ)reg = ZB(λ)⁻¹, where B(λ)² = Id + nλeGeGᵀ. As λ increases, B(λ)⁻¹ progressively annihilates components along the non-invariant subspace, smoothly interpolating between function spaces.

### Mechanism 3
Hard-wiring and data augmentation restrict optimization to the invariant subspace (dimension d = nullity(G)), yielding (ᵈʳ) critical points. Regularization optimizes over full rank-constrained space (dimension m = min{d₀, dₐ}), yielding (ᵐʳ) critical points—fewer constraints mean more optimization landscape features.

## Foundational Learning

- **Group representations and equivariance**: Understanding homomorphisms ρ: G → GL(X) is essential for formalizing invariance constraints like Wρₓ(g) = W and constructing the constraint matrix G = Id - ρₓ(g).
- **Low-rank matrix approximation**: All three methods reduce to finding optimal rank-r approximations of transformed target matrices; the Eckart-Young-Mirsky theorem explains why truncated SVD gives optimal Frobenius-norm approximation.
- **Critical points in constrained optimization**: The analysis focuses on critical points in function space (rank-constrained matrices) rather than parameter space; understanding this distinction is key to interpreting why overparameterization eliminates spurious local minima in function space but not parameter space.

## Architecture Onboarding

- **Component map**:
  - Hard-wiring: Input → [Basis B: BG=0] → Linear layers → Output
  - Data augmentation: Input → [Apply all ρ(g)·x] → Linear layers → Output  
  - Regularization: Input → Linear layers → Output + λ‖WG‖²F penalty

- **Critical path**:
  1. Identify group structure: Determine G, representation ρₓ, and constraint matrix G = Id - ρₓ(g)
  2. Choose approach: Hard-wire if architecture design is feasible; regularization if rapid prototyping needed; augmentation if no architecture control
  3. For hard-wiring: Compute basis B for null space of Gᵀ; parameterize as f(x) = W₂W₁Bx
  4. For regularization: Add λ‖WG‖²F to loss; tune λ (larger = more invariant but potentially underfit)
  5. For augmentation: Generate {ρₓ(g)xᵢ, yᵢ} for all g ∈ G; train on expanded dataset

- **Design tradeoffs**:
  - Hard-wiring uses ~|G|× fewer parameters and samples than augmentation but requires upfront architecture design
  - Small λ maintains expressiveness but weak invariance; large λ approaches hard-wiring but introduces more critical points
  - Narrower bottleneck (smaller r) reduces model capacity but makes invariance easier to learn

- **Failure signatures**:
  - Non-unitary representations may prevent data augmentation from yielding invariant critical points
  - Insufficient rank (rank(Zinv) ≤ r) makes the low-rank constraint trivially satisfied
  - Cross-entropy loss may produce non-invariant critical points unlike MSE
  - Invariance learned via augmentation may deteriorate under distribution shift with nonlinear networks

- **First 3 experiments**:
  1. Generate synthetic data with known invariant linear function; train all three methods; verify final losses match and W converges to same matrix
  2. Train with varying λ ∈ {0.001, 0.01, 0.1, 1.0, 10.0}; plot ‖Ŵ(λ)reg - Ŵinv‖F vs λ to verify continuous convergence
  3. Train with non-unitary representation using augmentation—verify W does NOT become invariant, contrasting with unitary case

## Open Questions the Paper Calls Out

- **Open Question 1**: What causes the observed "double descent" phenomenon in the non-invariant component ‖W⊥‖F during training with data augmentation and regularization?
- **Open Question 2**: Do nonlinear neural networks with non-convex activation functions and non-MSE losses (e.g., cross-entropy) learn genuine invariance via data augmentation, and under what conditions?
- **Open Question 3**: Are all critical points invariant when training with cross-entropy loss, as they are for MSE loss?
- **Open Question 4**: What are the training dynamics of gradient flow for the three invariance-learning approaches?

## Limitations

- Theoretical guarantees apply only to deep linear networks with MSE loss and unitary group actions
- Regularization with finite λ only achieves approximate invariance and introduces more critical points
- Empirical validation is limited to MNIST rotations and doesn't test generalization to other groups or real-world datasets
- Cross-entropy loss may produce non-invariant critical points unlike MSE

## Confidence

- **High confidence**: Critical point structure for linear networks with MSE loss under unitary representations; equivalence of global optima between data augmentation and hard-wiring; continuity of regularization path
- **Medium confidence**: Equivalence breaking under cross-entropy loss; practical performance differences on MNIST; relationship between rank constraints and invariance strength
- **Low confidence**: Generalization to non-unitary representations, partial augmentation, other group actions, or nonlinear architectures; practical impact of increased critical points in regularization

## Next Checks

1. Test non-unitary representations by applying data augmentation with anisotropic scaling transformations and verify that W does not converge to an invariant solution
2. Compare all three approaches using cross-entropy loss on MNIST rotations to test whether critical points remain invariant
3. Replace 90° rotations with 2D translations or scaling groups to verify whether critical point equivalence holds for non-unitary actions