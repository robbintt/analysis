---
ver: rpa2
title: Debiasing Multilingual LLMs in Cross-lingual Latent Space
arxiv_id: '2508.17948'
source_url: https://arxiv.org/abs/2508.17948
tags:
- debiasing
- space
- debias
- bias
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of cross-lingual bias transfer
  in multilingual large language models (LLMs). Previous debiasing techniques show
  limited effectiveness across languages when applied directly to LLM representations
  due to imperfect cross-lingual alignment.
---

# Debiasing Multilingual LLMs in Cross-lingual Latent Space

## Quick Facts
- arXiv ID: 2508.17948
- Source URL: https://arxiv.org/abs/2508.17948
- Reference count: 40
- This paper proposes performing debiasing in a cross-lingual latent space rather than directly on LLM representations, achieving up to 65% bias reduction and improved cross-lingual transferability.

## Executive Summary
This paper addresses the challenge of cross-lingual bias transfer in multilingual large language models. Previous debiasing techniques show limited effectiveness across languages when applied directly to LLM representations due to imperfect cross-lingual alignment. The authors propose performing debiasing in a joint cross-lingual latent space constructed through an autoencoder trained on parallel TED talk scripts. Experiments with Aya-expanse across four languages demonstrate that autoencoders effectively construct a well-aligned cross-lingual latent space, and applying debiasing techniques in this learned space significantly improves both overall debiasing performance and cross-lingual transferability.

## Method Summary
The method involves training an autoencoder on parallel TED talk scripts to construct a well-aligned cross-lingual latent space. The autoencoder architecture consists of a shared encoder (3-layer MLP) that projects LLM representations into a 128-dimensional latent space, and language-specific decoders that reconstruct the original representations. Once established, debiasing techniques (INLP and SentDebias) are applied within this latent space. The approach uses Aya-expanse-8B as the base LLM, with mean-pooled last hidden states as sentence representations. The autoencoder is trained using a combined reconstruction loss (self and cross-lingual) with AdamW optimizer.

## Key Results
- Autoencoders effectively construct a well-aligned cross-lingual latent space, with t-SNE visualizations showing parallel sentences clustering together regardless of language
- Applying debiasing techniques in the learned latent space significantly improves cross-lingual transferability, achieving bias reductions of up to 65%
- The approach demonstrates consistent performance across four languages (English, French, German, Dutch) using the CrowS-Pairs benchmark

## Why This Works (Mechanism)
The core insight is that multilingual models have imperfect cross-lingual alignment, making direct debiasing ineffective across languages. By first learning a cross-lingual latent space through autoencoder training on parallel data, the method ensures that semantically equivalent content from different languages maps to similar representations. This alignment allows bias subspace identification and removal techniques to work consistently across languages. The 128-dimensional bottleneck forces the model to learn a compressed representation that captures cross-lingual semantic meaning while discarding language-specific idiosyncrasies that hinder bias transfer.

## Foundational Learning

- Concept: **Cross-lingual Representation Alignment**
  - Why needed here: The paper's core argument is that multilingual models have poor alignment. Understanding what constitutes "good" alignment (e.g., parallel sentences mapping to similar vectors) and how to measure it (e.g., using t-SNE, retrieval accuracy) is a prerequisite for grasping the problem and the proposed solution.
  - Quick check question: If you plot the embeddings of the sentence "The cat sat on the mat" in English and its French translation using a well-aligned model, would you expect the points to be close together or far apart?

- Concept: **Autoencoders and Latent Spaces**
  - Why needed here: The proposed method uses a specific autoencoder architecture to create the aligned space. One must understand the roles of the encoder (projecting to latent space), the decoder (reconstructing), and how the bottleneck forces the model to learn a compressed, useful representation.
  - Quick check question: In the autoencoder described, what is the role of the "shared encoder" and why is it critical for achieving cross-lingual alignment?

- Concept: **Debiasing via Subspace Identification and Removal**
  - Why needed here: The paper uses two established debiasing techniques (INLP, SentDebias) as its test cases. It is essential to understand their fundamental premise: that social biases can be captured in a low-dimensional subspace of the representation, and that debiasing involves mathematically identifying and neutralizing this subspace.
  - Quick check question: Both INLP and SentDebias rely on a core assumption about the nature of bias in vector space. What is that assumption? (Hint: It involves PCA and null-space projection).

## Architecture Onboarding

- Component map: Base LLM → Mean-Pooled Representation → Shared Encoder → Latent Space → Debiasing Operation → (Implicitly used for evaluation)

- Critical path: The critical innovation is inserting the debiasing step after the encoder but before any language-specific reconstruction/usage. The path is: Aya-expanse-8B last hidden states → mean-pooling → shared encoder → 128-dim latent space → debiasing operation → (used for evaluation).

- Design tradeoffs:
  - **Latent Dimensionality (128)**: A smaller dimension may force more abstraction and alignment but risks losing important semantic nuance from the original LLM representations. A larger dimension might preserve more information but be harder to align.
  - **Training Data (TED Talks)**: Using parallel TED talks provides high-quality, aligned data but may be domain-specific. The resulting latent space may not transfer perfectly to other domains, although the paper shows it generalizes to the FLORES+ benchmark.
  - **Model Complexity (3-layer MLP)**: The authors tuned this. A simpler model may fail to capture the complex mapping needed for alignment; a more complex one could overfit to the TED talk domain and fail to generalize.

- Failure signatures:
  - **Mode Collapse in Autoencoder**: The decoder ignores the latent code and reconstructs the input from noise or a bias. The t-SNE plot would not show alignment.
  - **Overcompensation/Amplified Bias**: When applying debiasing to a model with low initial bias, the measured bias score increases instead of decreases. This indicates the debiasing operation is removing important semantic information that was not actually bias.
  - **Poor Cross-Lingual Transfer**: Debiasing in language X yields low bias for X but high bias for Y. This suggests the latent space is not as aligned as the t-SNE plot implies.

- First 3 experiments:
  1. **Reconstruction and Alignment Sanity Check**: Train the autoencoder on a subset of the TED data. Visualize the latent space of parallel sentences from an unseen dataset (like FLORES+) using t-SNE. If sentences do not cluster by meaning (all overlapping) but instead by language, the autoencoder has failed to learn a cross-lingual space.
  2. **Baseline Debiasing Comparison**: Apply INLP and SentDebias directly to the Aya-expanse-8B representations (without the autoencoder) and evaluate on CrowS-Pairs for all four languages. Quantify the "limited effectiveness" the paper claims, establishing a clear baseline.
  3. **Latent Space Debiasing Ablation**: Apply INLP and SentDebias in the learned latent space. Compare the bias scores against the baseline from experiment 2. Key metric: does debiasing in language X now reduce bias in languages Y and Z more effectively? This tests the core claim of improved cross-lingual transfer.

## Open Questions the Paper Calls Out
- **Non-European Languages**: The authors explicitly state that their experiments primarily focus on European languages and that expanding this work to non-European languages with different scripts or morphological structures remains an important direction for future research.
- **Alternative Debiasing Methods**: While INLP and SentDebias were tested, the paper notes that other methods like Dropout Regularization could be considered in future studies, leaving open whether the improved cross-lingual transferability is specific to projection-based methods.
- **Domain Influence**: The autoencoder is trained exclusively on TED talk scripts, but the paper does not analyze if this specific domain limits the generalization of the bias mitigation to out-of-domain text.

## Limitations
- The study is restricted to four European languages (English, French, German, Dutch), leaving uncertainty about effectiveness for non-European languages with different scripts or structures.
- The evaluation focuses primarily on gender bias, not addressing whether the method works effectively against other types of social bias.
- The paper does not extensively explore the trade-off between bias reduction and potential degradation in downstream task performance.

## Confidence
- **High Confidence**: The autoencoder architecture effectively constructs a cross-lingually aligned latent space, as evidenced by t-SNE visualizations showing parallel sentences clustering together regardless of language.
- **Medium Confidence**: The claim of "up to 65% reduction in bias" is well-supported within the tested domain, but generalizability to other bias types and languages remains uncertain.
- **Low Confidence**: The assertion that this method causes "minimal impact on downstream tasks" is mentioned but not empirically validated in the paper.

## Next Checks
1. **Latent Space Completeness Test**: Systematically vary the latent dimensionality (e.g., 64, 128, 256) and evaluate both cross-lingual alignment quality and downstream task performance to identify optimal balance.
2. **Domain Transferability Experiment**: Train the autoencoder on one parallel corpus (e.g., TED talks) and evaluate cross-lingual alignment and debiasing effectiveness on a completely different parallel corpus (e.g., Europarl, Wikipedia translations).
3. **Downstream Performance Benchmark**: Conduct comprehensive testing on a suite of GLUE-style tasks across all four languages, comparing performance before and after debiasing in the latent space to validate minimal impact claims.