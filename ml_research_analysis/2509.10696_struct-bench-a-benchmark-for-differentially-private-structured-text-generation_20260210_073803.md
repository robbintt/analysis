---
ver: rpa2
title: 'Struct-Bench: A Benchmark for Differentially Private Structured Text Generation'
arxiv_id: '2509.10696'
source_url: https://arxiv.org/abs/2509.10696
tags:
- data
- synthetic
- dataset
- query
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Struct-Bench, a benchmark for evaluating
  differentially private synthetic data generation for structured text datasets containing
  natural language. The key innovation is using context-free grammars (CFGs) to represent
  dataset structure and extract structural and semantic attributes for evaluation.
---

# Struct-Bench: A Benchmark for Differentially Private Structured Text Generation

## Quick Facts
- arXiv ID: 2509.10696
- Source URL: https://arxiv.org/abs/2509.10696
- Reference count: 40
- Primary result: Introduces Struct-Bench benchmark using CFGs to evaluate DP synthetic data for structured text, showing existing methods struggle with structural properties while maintaining semantic quality.

## Executive Summary
This paper introduces Struct-Bench, a benchmark for evaluating differentially private synthetic data generation on structured text datasets containing natural language. The key innovation is using context-free grammars (CFGs) to represent dataset structure and extract structural and semantic attributes for evaluation. The benchmark includes seven datasets and evaluates four DP synthetic data generation methods. Results show that existing methods struggle to capture structural properties while maintaining semantic quality. The authors propose algorithmic improvements to Private Evolution (PE), including LLM-assisted reformatting and node extraction with auto-generation, achieving up to 94% CFG compliance and improving semantic metrics. The work demonstrates that no single metric fully describes synthetic data quality and highlights the need for structure-aware evaluation frameworks in DP synthetic data generation.

## Method Summary
The benchmark defines datasets via Context-Free Grammars (CFGs) to capture structural properties. Users specify key nodes (e.g., query/response pairs) and attributes for evaluation. The evaluation suite includes structural metrics (CFG Pass Rate, Key Node Dependency using Wasserstein-2 distance, Attribute Match), non-structural metrics (KNN-Precision, KNN-Recall), and downstream evaluation (accuracy via train-synthetic-test-real). Four baselines are evaluated: Private Evolution (PE), DP Fine-Tuning (DP-FT), and variants with instruction-following. PE improvements include LLM-assisted reformatting after voting and node extraction with auto-generation.

## Key Results
- Existing DP synthetic data methods achieve high semantic quality (KNN-Precision) but fail to capture structural properties (low CFG Pass Rate)
- PE with LLM-assisted reformatting achieves up to 94% CFG compliance on ShareGPT
- Node extraction with auto-generation improves KNN-Recall while maintaining CFG compliance
- No single metric fully captures synthetic data quality; a multi-metric evaluation is necessary

## Why This Works (Mechanism)

### Mechanism 1: Context-Free Grammar as Structural Constraint
CFG parsing provides a formal, verifiable mechanism to measure structural compliance of synthetic structured text. Users define production rules specifying valid node types and their sequencing. Synthetic samples that fail CFG parsing receive a CFG Pass Rate of 0, isolating structural failures from semantic ones. CFGs are public knowledge (schemas are known even if data is not), so this imposes no additional privacy burden.

### Mechanism 2: Private Evolution's Noisy Voting Histogram for Distribution Matching
PE generates DP synthetic data by iteratively constructing a noisy histogram of private samples voting for their nearest synthetic counterparts, then resampling and perturbing high-vote candidates. This progressively improves distributional match without accessing private data during model inference.

### Mechanism 3: Node Extraction with Auto-Generation for Semantic Diversity
Extracting specific nodes (e.g., queries) and auto-generating dependent nodes (e.g., responses) via LLM-conditioned generation improves KNN-Recall while maintaining or improving CFG compliance. This relaxes semantic constraints, allowing more diverse outputs.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: The entire benchmark is designed for DP synthetic data; understanding (ε, δ)-DP, privacy budgets, and the privacy-utility tradeoff is essential to interpret results.
  - Quick check question: Can you explain why adding noise to a histogram of votes provides differential privacy, and what happens to utility as ε decreases?

- Concept: Context-Free Grammars (CFGs)
  - Why needed here: CFGs are the core structural representation; you must be able to write and debug production rules for new datasets.
  - Quick check question: Given a multi-turn conversation where speaker alternates and each turn starts with a fixed prefix, can you write a CFG that enforces this structure?

- Concept: Private Evolution (PE) Algorithm
  - Why needed here: PE is the primary baseline and the target of algorithmic improvements in the case study.
  - Quick check question: In PE's iteration loop, what is the purpose of the voting histogram, and how does the Variation API contribute to sample diversity?

## Architecture Onboarding

- Component map:
  Dataset + CFG Definition -> Parser -> Key Node Selector -> Metric Suite -> Synthetic Data Generator Interface -> Leaderboard

- Critical path:
  1. Define CFG for your dataset (manual; can potentially use LLM assistance)
  2. Specify key nodes and attributes (statistical and semantic)
  3. Run synthetic data generator (e.g., PE) with chosen privacy budget (ε, δ)
  4. Parse synthetic samples with CFG; compute CFG-PR
  5. Compute KND (node pair embeddings), AM (attribute distributions), and non-structural metrics
  6. Optionally run downstream evaluation (label generation → model training → held-out test accuracy)

- Design tradeoffs:
  - CFG vs. Context-Sensitive Grammar: CFGs are easier to specify but cannot enforce semantic dependencies; CSGs are more expressive but require more domain expertise and are error-prone. Struct-Bench compensates with KND metric.
  - Reformatting before vs. after voting: Reformatting after voting achieves higher CFG-PR (only reformats winners), but may miss semantically valid samples that were structurally invalid before voting.
  - Node extraction choice: Extracting queries improves diversity more than extracting responses, as query semantics constrain downstream response generation more heavily.
  - Foundation model selection: PE can use API-only models (GPT-4o) for better structure capture; DP-FT requires open-weight models (GPT-2) due to gradient access needs.

- Failure signatures:
  - CFG-PR = 0 but high KNN-Precision: Structure completely wrong but content semantically plausible (common with DP-FT)
  - High CFG-PR but near-zero KNN-Recall: Structure captured but semantic diversity collapsed (common with PE on tabular/synthetic datasets with GPT-4o)
  - KND high but AM low: Node dependencies preserved but attribute distributions (e.g., token lengths) diverge significantly
  - Downstream accuracy ≈ random: Synthetic data fails to transfer signal; check if labels are extractable or if synthetic samples are too noisy

- First 3 experiments:
  1. Baseline replication: Run PE (ε=4, GPT-4o) and DP-FT (ε=4, GPT-2) on ShareGPT. Verify CFG-PR (should be ~0.86 for PE, 0 for DP-FT) and KNN-Recall (should be low for both).
  2. Ablation on reformatting: Implement the "reformat after voting" variant on PE with Llama3-8B on ShareGPT. Measure CFG-PR improvement (target: >20% absolute gain over vanilla PE at ε=4) and check for semantic degradation via KNN-Precision.
  3. Node extraction on a new dataset: Apply "extract query, auto-generate response" to the Water dataset. Compare KNN-Recall vs. vanilla PE. If Recall improves without CFG-PR dropping, the mechanism transfers.

## Open Questions the Paper Calls Out

- Can a unified differentially private synthetic data generation algorithm be developed that simultaneously maximizes structural compliance (CFG-PR) and semantic diversity (KNN-Recall) without sacrificing performance in either area?
- Can Context-Sensitive Grammars (CSGs) be effectively integrated into the Struct-Bench framework to capture semantic dependencies as hard constraints, rather than relying solely on the statistical Key Node Dependency (KND) metric?
- To what extent are the evaluation metrics proposed in Struct-Bench applicable to non-private synthetic data generation tasks, such as data augmentation or conditional generation?

## Limitations

- CFGs cannot capture semantic dependencies, requiring statistical metrics like KND to compensate
- The effectiveness of proposed improvements (reformatting, node extraction) is demonstrated only on tested datasets; generalizability to other domains is unknown
- Absolute performance numbers are subject to random seeds, API variations, and unspecified implementation details

## Confidence

- High: CFG-based structural evaluation; PE algorithmic description; claim that no single metric fully captures quality
- Medium: Relative ranking of methods across datasets; quantitative CFG-PR and KND values; effectiveness of PE improvements
- Low: Absolute performance numbers; claim that the benchmark fully captures "structured text" quality

## Next Checks

1. Run PE baseline (ε=4, GPT-4o) and DP-FT (ε=4, GPT-2) on ShareGPT; verify CFG-PR and KNN-Recall match reported ranges
2. Implement the "reformat after voting" variant on PE with Llama3-8B; measure CFG-PR improvement and semantic degradation
3. Apply "extract query, auto-generate response" to the Water dataset; compare KNN-Recall vs. vanilla PE