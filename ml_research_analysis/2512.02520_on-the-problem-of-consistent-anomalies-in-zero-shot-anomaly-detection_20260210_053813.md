---
ver: rpa2
title: On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection
arxiv_id: '2512.02520'
source_url: https://arxiv.org/abs/2512.02520
tags:
- anomaly
- anomalies
- normal
- similarity
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This dissertation introduces and addresses the problem of consistent\
  \ anomalies in zero-shot anomaly detection, where recurring similar defects systematically\
  \ bias distance-based methods. The work identifies two key phenomena\u2014similarity\
  \ scaling and neighbor-burnout\u2014that describe how relationships among normal\
  \ patches change with and without consistent anomalies in settings characterized\
  \ by highly similar objects."
---

# On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection

## Quick Facts
- arXiv ID: 2512.02520
- Source URL: https://arxiv.org/abs/2512.02520
- Reference count: 0
- Introduces CoDeGraph framework addressing consistent anomalies in zero-shot anomaly detection

## Executive Summary
This dissertation addresses the problem of consistent anomalies in zero-shot anomaly detection, where recurring similar defects systematically bias distance-based methods. The work identifies two key phenomena—similarity scaling and neighbor-burnout—that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects. To address this, CoDeGraph is introduced, a graph-based framework that filters consistent anomalies through multi-stage graph construction, community detection, and structured refinement. The dissertation extends this framework to 3D medical imaging via a training-free volumetric tokenization strategy for MRI data, enabling genuinely zero-shot 3D anomaly segmentation. Finally, it bridges batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models.

## Method Summary
The CoDeGraph framework addresses consistent anomalies through a three-stage graph-based approach. Stage 1 computes weighted endurance ratios to identify suspicious patch-to-patch links. Stage 2 constructs an anomaly similarity graph using coverage-based link selection, where nodes represent image collections and edges capture cross-collection patch relationships. Stage 3 applies community detection (Leiden with CPM) to identify outlier communities, then removes high-dependency patches through dependency ratio filtering. The method operates on patch tokens extracted from ViT layers and uses multi-layer/multi-scale aggregation for final anomaly scoring. The framework is extended to 3D MRI via volumetric tokenization and connects to text-based methods through pseudo-mask supervision.

## Key Results
- Outperforms state-of-the-art methods on both industrial and medical datasets
- Significant improvements in segmentation accuracy and robustness
- Captures 73.9% of consistent-anomaly patches while removing only 6.9% of total patches on MVTec-CA
- Successfully extends to genuinely zero-shot 3D anomaly segmentation in MRI volumes
- Demonstrates effective pseudo-mask supervision for text-based vision-language models

## Why This Works (Mechanism)

### Mechanism 1: Similarity Scaling Phenomenon
- Claim: Normal patch tokens exhibit a power-law decay in the log growth rate of their mutual similarity vectors, providing a baseline statistical signature for normality.
- Mechanism: For a normal element z, the mutual similarity vector D(z) = [d(z)^(1), ..., d(z)^(B-1)] contains distances to other collections sorted by rank. The log growth rate τ_i(z) = ln(d(z)^(i+1)/d(z)^(i)) decays as τ_i ∝ i^(-α) (empirically α ≈ 0.8). This arises from regularly varying similarity tails (Section 4.2) and local manifold geometry (Section 4.3), where the number of similar patches in a ball B(z,r) scales as r^d.
- Core assumption: The Doppelgänger assumption holds—most normal elements find many close matches in other collections (|N(z^n, ε)| ≥ K for some K ≪ B).
- Evidence anchors:
  - [abstract] "similarity scaling and neighbor-burnout—that describe how relationships among normal patches change with and without consistent anomalies"
  - [section 2.2.2] Log-log plots show linear scaling with R² ≥ 0.98 across ViT-L and DINOv2 backbones, validated on cable, screw, breakfast_box, and brain_slices datasets
  - [corpus] Weak/no corpus evidence; related papers (ViP²-CLIP, MetaUAS) address prompting but not this specific scaling law
- Break condition: Fails when normal patches lack sufficient similar counterparts (small batch size, diverse normal variants, or ω set too large relative to variant count).

### Mechanism 2: Neighbor-Burnout as Consistent-Anomaly Signature
- Claim: Consistent anomalies produce a characteristic spike in the log growth rate τ_i at their "burnout point" H, enabling local detection of deceptive matches.
- Mechanism: An H-level ε-consistent anomaly z^a has H nearly identical matches (d(z^a)^(i) < ε for i ≤ H) but a semantic cliff at rank H+1 where d(z^a)^(H+1) ≥ ε_0. This causes τ_H(z^a) = ln(d^(H+1)/d^(H)) to spike sharply, violating the power-law trend. The endurance ratio ζ(z, C^(i)) = d(z)^(i)/d(z)^(ω) quantifies this: small ζ indicates rapid exhaustion of similar neighbors.
- Core assumption: Consistent anomalies exist as a minority subset (Assumption 2.3)—their frequency never exceeds normal samples of the same type.
- Evidence anchors:
  - [section 2.2.3] Figure 2.5 shows consistent anomalies in cable exhibit τ_i spikes after rank H, while normal and inconsistent-anomaly patches maintain power-law decay; capsule (no consistent anomalies) shows no deviation
  - [section 3.2.1] Distributions of endurance ratios show consistent-anomaly patches dominate at the low-ζ tail (Figure 3.2a), while absolute distances overlap (Figure 3.2b)
  - [corpus] No corpus evidence found for this specific neighbor-burnout formulation
- Break condition: Fails when H is very large (consistent anomalies dominate) or when ω < H (reference rank falls within the burnout region).

### Mechanism 3: Graph-Based Community Detection and Targeted Filtering
- Claim: Images sharing consistent anomalies form exceptionally dense communities in an anomaly similarity graph, enabling outlier detection and targeted patch removal.
- Mechanism: Stage 1 identifies suspicious links S_l where ζ'(z, C^j) ≤ λ (weighted endurance ratio). Stage 2 constructs graph G=(V,E) where nodes are collections and edge weights w_ij count cross-links. Consistent-anomaly communities exhibit high internal density ρ(M) = Σw_ij / (|M|(|M|-1)). Stage 3 flags outlier communities via IQR threshold (ρ(M) > Q_3 + k_IQR·IQR) and removes high-dependency patches P_ex where r_M(z) = a_{B\M}(z)/a_B(z) exceeds the 99th percentile of patches outside M.
- Core assumption: Consistent anomalies cluster together more densely than normal variants or inconsistent anomalies.
- Evidence anchors:
  - [section 3.3.2, Table 3.3] On MVTec-CA, P_ex captures 73.9% of consistent-anomaly patches while removing only 6.9% of total patches; on MVTec-IA (no consistent anomalies), exclusion rate is 0.3%
  - [section 3.3.2, Figure 3.3] Metal nut shows one community with density 147.42 containing all 23 flipped samples, exceeding IQR threshold 40.85
  - [corpus] Related work MetaUAS explores one-prompt meta-learning but does not address graph-based community detection for anomalies
- Break condition: Fails when multi-modal normal variants exist with ω ≥ N/C (C = number of variants), causing variant-based clusters to dominate; or when consistent anomalies outnumber some normal variants (violates Minority assumption).

## Foundational Learning

- Concept: **Vision Transformer (ViT) Patch Tokens**
  - Why needed here: The entire framework operates on patch tokens z_i^ℓ ∈ R^D extracted from ViT layers. Understanding patchification (Eq. 1.2), the distinction between [CLS] and patch tokens, and multi-layer aggregation (layers 6, 12, 18, 24) is essential.
  - Quick check question: Given a 518×518 image processed by ViT-L/14, how many patch tokens are produced per layer? (Answer: 1369 = (518/14)²)

- Concept: **Mutual Similarity Vector and Top-K Aggregation**
  - Why needed here: The anomaly score a(z) = (1/K)Σ d(z)^(k) aggregates the K smallest distances. The choice of K (10% vs. 30%) affects sensitivity to consistent anomalies.
  - Quick check question: Why does a small K make the method vulnerable to consistent anomalies? (Answer: If K ≤ H where H is the burnout point, the score only sees the deceptive low-distance matches)

- Concept: **Extreme Value Theory and Regularly Varying Tails**
  - Why needed here: The theoretical justification for similarity scaling relies on P(S > s) ~ s^(-α)L(s) where S = 1/Z is the patch-to-patch similarity. The Fréchet-type tail enables the τ_i ∝ 1/i scaling.
  - Quick check question: What does the Hill plot plateau indicate? (Answer: Stable tail index estimation across order statistics, confirming power-law tail behavior)

## Architecture Onboarding

- Component map:
```
Input Batch B → Feature Extraction (ViT/DINOv2)
    → Mutual Similarity Vectors D_B(z) for all patches
    → [Stage 1] Compute endurance ratios ζ, ζ' → Identify suspicious links S_l
    → [Stage 2] Build anomaly similarity graph G → Coverage-based link selection
    → [Stage 3] Community detection (Leiden + CPM) → IQR outlier detection → Dependency ratio filtering → P_ex
    → Refined base set B_refined → Recompute MSM scores → Multi-layer/multi-scale aggregation → Anomaly maps
```

- Critical path:
  1. Feature extraction at layers [6, 12, 18, 24] with ℓ2-normalization
  2. Mutual similarity computation (dominant O(B²N²) cost)
  3. Graph construction with coverage threshold τ = 0.95
  4. CPM community detection with γ = 25th percentile of edge weights
  5. Targeted patch removal via dependency ratio r_M(z)

- Design tradeoffs:
  - **Backbone choice**: DINOv2-L/14 achieves higher segmentation accuracy (89.9% pixel AUROC vs. 98.2% for CLIP ViT-L/14-336) but lacks text alignment; CLIP enables pseudo-mask training for text-based models
  - **Reference rank ω**: Lower ω (10-30% of B) better isolates consistent anomalies but may miss them if H > ω; higher ω (50-70%) is safer but less discriminative
  - **IQR multiplier k_IQR**: Higher values (4.5) are conservative, only flagging extremely dense communities; lower values (1.5-3) increase sensitivity but risk false positives on variant clusters
  - **Subset division + CLS screening**: Reduces runtime by ~37% with <1.5% F1-seg loss, but η < 30% degrades performance

- Failure signatures:
  - **Dense normal-variant communities**: On multi-modal datasets (e.g., juice bottle with 3 flavors), variant clusters may be flagged as anomalies if ω ≥ N/C (C = number of variants)
  - **Dominant consistent anomalies**: If consistent anomalies outnumber a normal variant, community labels invert (Figure 3.9)
  - **Small tumors in 3D MRI**: 3D-patch tokens at 16³ resolution may fail to capture anomalies smaller than 14³ voxels
  - **Memory overflow**: Index tensors of shape [L, B, N, B-1] for storing ranking indices; offload to CPU if VRAM < 8GB

- First 3 experiments:
  1. **Baseline reproduction**: Run MuSc 10% on MVTec-CA (cable, metal_nut, pill) to confirm vulnerability to consistent anomalies—expect low F1-seg (~50-60%) on flipped metal nuts
  2. **Graph visualization**: Construct anomaly similarity graph on metal_nut, visualize communities with CPM (γ = 25th percentile), verify that the 23 flipped images form a single high-density community
  3. **Ablation on ω and k_IQR**: Sweep ω ∈ {0.1, 0.3, 0.5, 0.7}×B and k_IQR ∈ {1.5, 3.0, 4.5} on MVTec-CA + ConsistAD; plot capture rate vs. F1-seg to find operational sweet spot

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can multi-modal normal variant distributions be systematically separated from consistent-anomaly communities without prior knowledge of the number of normal classes?
- Basis in paper: [explicit] Section 3.3.4 states: "Future extensions could integrate pre-clustering of normal variants or hierarchical community detection to separate variant-level and anomaly-level structures" after identifying failure cases where normal variants form dense communities misidentified as anomalies.
- Why unresolved: CoDeGraph's community detection relies on density-based outlier detection, which cannot distinguish between normal variant clusters and consistent-anomaly clusters when both exhibit high internal connectivity.
- What evidence would resolve it: A modified graph construction or community detection algorithm that explicitly handles multi-modal normals, evaluated on datasets with known multiple normal variants (e.g., MVTec LOCO juice bottle).

### Open Question 2
- Question: What are the theoretical conditions under which the Similarity Scaling phenomenon emerges, and do they generalize beyond datasets with highly redundant normal structures?
- Basis in paper: [explicit] Section 4.3.3 states: "For a more general dataset with highly diverse normal patterns... the extremely similar patches that control the tail of S may be so rare that they are never observed unless we sample an enormous (in the limit, infinite) number of images."
- Why unresolved: The theoretical analysis assumes local manifold geometry leads to heavy-tailed similarity distributions, but this may not manifest in finite datasets lacking structural redundancy.
- What evidence would resolve it: Systematic evaluation across datasets with varying normal pattern diversity, measuring whether power-law decay in τ_i holds and correlating with intrinsic dimensionality estimates.

### Open Question 3
- Question: Can pseudo-masks from batch-based methods serve as reliable supervision for text-based models across diverse domains, and what architectural properties determine robustness to pseudo-label noise?
- Basis in paper: [explicit] Section 6.1 poses: "Can high-quality pseudo-masks, when combined with a robust text-based model, approach the performance achieved using true pixel-level supervision?"
- Why unresolved: Results show AnomalyCLIP benefits from pseudo-masks while APRIL-GAN degrades, but the underlying reasons for this differential robustness remain unclear.
- What evidence would resolve it: Controlled experiments varying pseudo-mask noise levels systematically, coupled with analysis of learned representations in each architecture.

### Open Question 4
- Question: How can 3D volumetric anomaly detection scale to clinically relevant resolutions while maintaining computational tractability?
- Basis in paper: [explicit] Section 5.3.2 states: "Future research aims to improve computational efficiency, enabling higher volumetric resolutions such as 518³" and identifies that "CoDeGraph struggles with small or low-contrast tumors" due to coarse grid resolution (16³).
- Why unresolved: Current 3D-patch token extraction requires pooling along depth to manage memory, limiting spatial resolution for detecting small anomalies.
- What evidence would resolve it: Implementation of memory-efficient volumetric processing (e.g., chunked computation, gradient checkpointing) enabling 32³ or higher token grids with corresponding improvements in small-lesion detection metrics.

## Limitations

- **Variant Cluster Confusion**: Multi-modal normal datasets can cause normal variant clusters to be misidentified as consistent anomalies when ω ≥ N/C
- **Minority Assumption Violation**: When consistent anomalies outnumber some normal variants, community detection can invert labels and treat normal samples as outliers
- **Doppelgänger Requirement**: The method fails when normal patches lack sufficient similar matches across the batch (small batch size or highly diverse normals)

## Confidence

- **High Confidence**: Graph-based community detection and density-based outlier filtering (Stage 3) - directly validated on MVTec-IA (no consistent anomalies) and MVTec-CA (with consistent anomalies) with clear quantitative improvements (73.9% capture rate, 6.9% exclusion on MVTec-CA)
- **Medium Confidence**: Similarity scaling and neighbor-burnout phenomena - supported by log-log plots (R² ≥ 0.98) and spike detection in τ_i curves, but corpus evidence is limited to this dissertation's own datasets
- **Low Confidence**: 3D MRI extension - while the method claims to be training-free, the specific patch resolution (16³) and its ability to capture small tumors remain unproven on broader medical datasets beyond the provided cases

## Next Checks

1. **Variant Robustness Test**: Apply CoDeGraph to a multi-modal normal dataset (e.g., juice bottle with 3 distinct flavors) and sweep ω across {0.1, 0.3, 0.5, 0.7}×B to determine the threshold where variant clusters begin to dominate and cause false positives

2. **Batch Size Sensitivity**: Systematically reduce batch size B on MVTec-CA and measure the degradation in similarity scaling R² and neighbor-burnout detection accuracy, identifying the minimum viable batch size for reliable τ_i spike detection

3. **3D Small Anomaly Detection**: Evaluate the 3D-patch tokenization strategy on a controlled synthetic MRI dataset with tumors of varying sizes (e.g., 8³, 12³, 16³, 20³ voxels) to quantify the resolution threshold below which detection fails