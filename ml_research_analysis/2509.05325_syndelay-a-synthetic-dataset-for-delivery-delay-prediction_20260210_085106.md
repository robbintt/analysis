---
ver: rpa2
title: 'SynDelay: A Synthetic Dataset for Delivery Delay Prediction'
arxiv_id: '2509.05325'
source_url: https://arxiv.org/abs/2509.05325
tags:
- data
- dataset
- supply
- chain
- delivery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SynDelay is a synthetic dataset for delivery delay prediction in
  supply chains, addressing the lack of open, high-quality benchmark data. It was
  generated using a generative model trained on real-world data, preserving realistic
  delivery patterns while ensuring privacy.
---

# SynDelay: A Synthetic Dataset for Delivery Delay Prediction

## Quick Facts
- arXiv ID: 2509.05325
- Source URL: https://arxiv.org/abs/2509.05325
- Reference count: 8
- Primary result: Synthetic dataset with 155,488 rows and 41 variables for delivery delay prediction, achieving best baseline Macro F1 of 0.4833 with Random Forest

## Executive Summary
SynDelay is a synthetic dataset for delivery delay prediction in supply chains, addressing the lack of open, high-quality benchmark data. It was generated using a generative model trained on real-world data, preserving realistic delivery patterns while ensuring privacy. The dataset contains 155,488 rows and 41 variables, including a three-class delivery outcome. Baseline models were evaluated using metrics such as accuracy, macro F1, weighted F1, and per-class precision/recall. Ensemble methods (Random Forest, XGBoost, CatBoost) outperformed trivial baselines, with Random Forest achieving the highest macro F1 (0.4833) and weighted F1 (0.5601). The dataset is publicly available through the Supply Chain Data Hub, supporting reproducible research and benchmarking in supply chain AI.

## Method Summary
SynDelay was generated using an advanced generative model trained on real-world delivery data, employing a score-based diffusion model in latent space combined with LLM-guided relationship extraction. This approach preserves both statistical properties and inter-column logical relationships while ensuring privacy. The dataset contains 155,488 rows with 41 variables (22 numerical, 12 categorical) and a three-class delivery outcome. Five baseline models were evaluated: Random Guess, ZeroRule (majority class), Random Forest, XGBoost, and CatBoost, using a 0.8:0.1:0.1 train/validation/test split with minimal preprocessing and common hyperparameters.

## Key Results
- Random Forest achieved highest Macro F1 (0.4833) and Weighted F1 (0.5601) among ensemble methods
- ZeroRule achieved highest accuracy (0.5770) by predicting majority class but very low Macro F1 (0.2439)
- All ensemble methods substantially outperformed trivial baselines on both aggregate and per-class metrics
- Class distribution: 29,055 early, 36,724 on-time, 89,709 delayed deliveries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Latent diffusion with LLM-guided relationship extraction produces synthetic tabular data that preserves statistical distributions and inter-column dependencies
- Mechanism: A score-based diffusion model operates in latent space after an LLM infers column relationships; the model learns joint distributions rather than marginal approximations, enabling coherent sample reconstruction
- Core assumption: The original dataset contains learnable statistical patterns that survive the preprocessing pipeline
- Evidence anchors:
  - [abstract] "Generated using an advanced generative model trained on real-world data, SynDelay preserves realistic delivery patterns while ensuring privacy."
  - [Dataset Generation and Description] "The framework integrates LLM-based reasoning into a score-based diffusion model operating in the latent space, enabling the generation of synthetic datasets that preserves both statistical properties and inter-column logical relationships."
  - [corpus] Limited direct corpus evidence on this specific generative architecture; related work (SupplyGraph, LaDe) focuses on real data collection rather than synthetic generation
- Break condition: If original data has severe integrity violations or missingness patterns that cannot be tokenized meaningfully, the diffusion model will learn artifacts rather than signal

### Mechanism 2
- Claim: Ensemble tree methods outperform trivial baselines on imbalanced multi-class delivery prediction by handling mixed feature types and nonlinear relationships
- Mechanism: Random Forest reduces variance through bootstrap aggregation; XGBoost and CatBoost iteratively reduce bias by focusing on hard-to-predict samples. All handle categorical/numerical mixes without extensive preprocessing
- Core assumption: Delivery delay signal is distributed across multiple features with nonlinear interactions
- Evidence anchors:
  - [Baselines and Evaluation Metrics] "These ensemble methods are well-suited for mixed feature types, nonlinear relationships, and class imbalance."
  - [Baseline Results] "Ensemble classifiers substantially outperform trivial baselines... Random Forest delivers the most balanced performance, with the highest Macro F1 (0.4833) and Weighted F1 (0.5601)."
  - [corpus] Corpus papers on supply chain prediction (e.g., food delivery time prediction, credit risk with GANs) similarly employ tree ensembles or deep learning for tabular prediction tasks
- Break condition: If feature-target relationships are approximately linear or if categorical encoding introduces severe cardinality issues without calibration, simpler models may be competitive

### Mechanism 3
- Claim: Multi-metric evaluation (aggregate + per-class) exposes precision-recall trade-offs that single-metric reporting hides in imbalanced settings
- Mechanism: ZeroRule achieves highest accuracy (0.5770) by predicting the majority class, but its Macro F1 (0.2439) reveals poor minority-class recognition. Macro F1 weights all classes equally, surfacing minority performance
- Core assumption: Early and on-time deliveries (Classes 0 and 1) matter operationally, not just the majority delayed class
- Evidence anchors:
  - [Baseline Results] "ZeroRule achieves the highest accuracy (0.5770) by always predicting the majority class (Class 2), but its very low Macro F1 (0.2439) highlights poor minority-class recognition."
  - [Baseline Results] "These outcomes illustrate precision–recall trade-offs in imbalanced settings and the necessity of evaluating both aggregate and per-class metrics."
  - [corpus] Limited explicit discussion of multi-metric evaluation frameworks in neighbor papers; most focus on single-task objectives
- Break condition: If downstream use only cares about majority-class detection (e.g., "just tell me if delayed"), Macro F1 becomes less actionable

## Foundational Learning

- **Concept: Synthetic Tabular Data Generation**
  - Why needed here: SynDelay is not raw operational data; understanding how it was generated (diffusion + LLM relationship extraction) informs what biases or artifacts may exist
  - Quick check question: Can you explain why a diffusion model in latent space might preserve column correlations better than independent marginal sampling?

- **Concept: Multi-class Imbalanced Classification**
  - Why needed here: The three-class outcome (early/on-time/delayed) is imbalanced (~60% delayed), requiring evaluation strategies that do not reward majority-only prediction
  - Quick check question: Why does accuracy alone fail as a metric when one class represents 60% of samples?

- **Concept: Ensemble Tree Methods for Tabular Data**
  - Why needed here: Baseline results show Random Forest, XGBoost, and CatBoost as viable starting points; understanding their bias-variance trade-offs guides model selection
  - Quick check question: What is the practical difference between bagging (Random Forest) and boosting (XGBoost/CatBoost) when applied to noisy supply chain data?

## Architecture Onboarding

- **Component map:**
  Input (155,488 rows × 41 features) -> Minimal preprocessing (encoding) -> Train/val/test split (0.8:0.1:0.1) -> Ensemble models (RF, XGBoost, CatBoost) -> Multi-metric evaluation (Accuracy, Macro F1, Weighted F1, per-class Precision/Recall/F1)

- **Critical path:**
  1. Load SynDelay from Supply Chain Data Hub
  2. Split into train/validation/test (0.8:0.1:0.1)
  3. Apply minimal preprocessing (encoding for categorical variables)
  4. Train baseline ensemble with default hyperparameters
  5. Evaluate using both aggregate and per-class metrics

- **Design tradeoffs:**
  - Default hyperparameters vs. tuning: Paper uses common settings for reproducibility; tuning may improve results but reduces comparability
  - Macro F1 vs. Weighted F1: Macro highlights minority-class performance; Weighted reflects overall accuracy skewed by majority class
  - Per-class focus: Prioritizing Class 2 (delayed) maximizes recall for the majority but may miss early/on-time signal

- **Failure signatures:**
  - High accuracy + low Macro F1 → model predicting majority class only
  - High variance across runs → unstable learning due to noise or insufficient data
  - CatBoost macro F1 much lower than Random Forest (as observed: 0.3096 vs. 0.4833) → potential sensitivity to default settings on this specific data distribution

- **First 3 experiments:**
  1. Reproduce baseline table using sklearn/XGBoost/CatBoost with the 0.8:0.1:0.1 split and default hyperparameters to validate environment setup
  2. Introduce class weighting or oversampling (e.g., SMOTE) and measure impact on Macro F1 and minority-class recall
  3. Apply targeted feature engineering (e.g., temporal features, interaction terms) and compare against minimal-preprocessing baseline to quantify potential gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can domain-specific feature engineering and advanced hyperparameter optimization improve prediction performance over the provided baselines?
- Basis in paper: [explicit] The authors state that "targeted feature engineering and hyperparameter tuning would likely lead to further improvements" upon the baseline results
- Why unresolved: The paper intentionally provides only initial benchmarks using minimal transformations and standard hyperparameters to serve as reference points rather than state-of-the-art results
- What evidence would resolve it: A study applying advanced tuning to the SynDelay dataset, demonstrating a statistically significant increase in Macro F1 and per-class metrics compared to the current Random Forest baseline

### Open Question 2
- Question: Can the LLM-guided diffusion generation pipeline be effectively adapted to create high-fidelity benchmark datasets for non-retail supply chain sectors?
- Basis in paper: [explicit] The discussion notes the dataset is "currently restricted to the retail sector" and explicitly calls for "extending future datasets to a wider range of industries and contexts"
- Why unresolved: The current study validates the generative approach only on retail delivery data, leaving the transferability of the framework to other domains unproven
- What evidence would resolve it: Successful generation and validation of synthetic datasets for sectors like manufacturing or healthcare logistics using the proposed methodology

### Open Question 3
- Question: How does the "sim-to-real" transfer performance of models trained on SynDelay compare to models trained on proprietary real-world data?
- Basis in paper: [inferred] The paper acknowledges the dataset is bounded by the "biases of the underlying synthetic data generation model" and contains noise, yet it is intended as a "practical testbed"
- Why unresolved: While the synthetic data preserves statistical properties, the paper does not validate whether models trained on SynDelay generalize effectively to actual operational environments
- What evidence would resolve it: Comparative benchmarking showing the predictive accuracy of SynDelay-trained models when applied to held-out, real-world delivery data streams

## Limitations
- Generative model architecture (diffusion + LLM relationship extraction) is described but not fully specified, limiting reproducibility of synthetic data generation
- No hyperparameter details for baseline ensemble models provided, though default settings are claimed
- Limited external validation beyond internal benchmarking against trivial baselines
- No ablation studies showing impact of individual features or temporal patterns on model performance

## Confidence
- High confidence: Multi-metric evaluation framework correctly identifies limitations of accuracy-only assessment in imbalanced settings
- Medium confidence: Ensemble methods' superiority over trivial baselines is well-supported, though exact performance depends on undisclosed hyperparameters
- Medium confidence: Synthetic data preserves realistic patterns, but limited corpus evidence on diffusion+LLM generative approach in tabular data

## Next Checks
1. Replicate baseline results with publicly available SynDelay dataset and default ensemble hyperparameters to verify performance claims
2. Compare SynDelay performance against a held-out subset of the original real-world data (if available) to assess synthetic-to-real gap
3. Conduct sensitivity analysis by varying train/validation/test splits and random seeds to establish robustness of baseline results