---
ver: rpa2
title: Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification
  and Object Detection
arxiv_id: '2502.07821'
source_url: https://arxiv.org/abs/2502.07821
tags:
- attacks
- attack
- image
- rfpar
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a query-based black-box adversarial attack
  method called Remember and Forget Pixel Attack using Reinforcement Learning (RFPAR),
  which extends pixel attacks from image classification to object detection. The core
  idea is to use a one-step reinforcement learning algorithm with a "Remember" process
  that stores high-reward perturbations and a "Forget" process that resets the agent
  to prevent overfitting.
---

# Amnesia as a Catalyst for Enhancing Black Box Pixel Attacks in Image Classification and Object Detection

## Quick Facts
- **arXiv ID:** 2502.07821
- **Source URL:** https://arxiv.org/abs/2502.07821
- **Reference count:** 40
- **Primary result:** Achieves 64.1% attack success rate with 211 L0 norm and 613 queries for ViT-B on ImageNet-1K

## Executive Summary
This paper introduces Remember and Forget Pixel Attack using Reinforcement Learning (RFPAR), a novel query-based black-box adversarial attack method that extends pixel attacks from image classification to object detection. The core innovation is the "Forget" mechanism, which resets the reinforcement learning agent to prevent overfitting to specific images and improve query efficiency. RFPAR demonstrates significant improvements over state-of-the-art methods, achieving 64.1% success rate on ImageNet-1K with reduced query counts and perturbation magnitude. The method successfully scales to object detection tasks on MS-COCO and Argoverse datasets, achieving high object removal rates with minimal pixel modifications.

## Method Summary
RFPAR employs a one-step reinforcement learning algorithm with a "Remember" process that stores high-reward perturbations and a "Forget" process that resets the agent to prevent overfitting. The method uses Proximal Policy Optimization (PPO) to learn optimal perturbation strategies. The "Forget" mechanism introduces amnesia into the learning process, forcing the agent to generalize across different images rather than memorizing specific attack patterns. This approach enables effective pixel-level attacks that can transfer from classification to object detection tasks, achieving competitive performance with significantly reduced query counts and perturbation sizes.

## Key Results
- Achieves 64.1% attack success rate on ImageNet-1K with 211 L0 norm and 613 queries for ViT-B
- Outperforms state-of-the-art methods by 12.1% in success rate while reducing queries by 26.0% and L0 norm by 41.1%
- For object detection on MS-COCO, achieves comparable mAP reduction to GARSDC while reducing queries by 52.8%
- Demonstrates high object removal rates above 0.9 on Argoverse dataset with minimal pixel modifications (0.1%)

## Why This Works (Mechanism)
The effectiveness of RFPAR stems from its unique approach to preventing overfitting in reinforcement learning-based adversarial attacks. Traditional methods often memorize specific attack patterns for individual images, leading to poor generalization and high query costs. The "Forget" mechanism introduces strategic amnesia, forcing the agent to learn robust perturbation strategies that work across diverse images rather than overfitting to specific targets. This allows the method to achieve high success rates with minimal queries and perturbations. The combination of the "Remember" process (storing successful perturbations) and "Forget" process (resetting the agent) creates a balanced learning dynamic that generalizes well across both classification and detection tasks.

## Foundational Learning
- **Reinforcement Learning with PPO**: Used to optimize the adversarial attack strategy through reward-based learning
  - *Why needed:* Provides a framework for learning optimal perturbation strategies in black-box settings
  - *Quick check:* Verify PPO implementation correctly optimizes for attack success while minimizing queries

- **Pixel-level Adversarial Attacks**: Techniques for modifying individual pixels to fool neural networks
  - *Why needed:* Enables minimal perturbation attacks that are harder to detect and defend against
  - *Quick check:* Confirm pixel modifications stay within valid range and maintain imperceptibility

- **Object Detection Evaluation Metrics**: mAP, removal rates, and other metrics specific to detection tasks
  - *Why needed:* Provides appropriate metrics for evaluating attack effectiveness on detection models
  - *Quick check:* Ensure metrics correctly handle different object sizes and confidence thresholds

## Architecture Onboarding

**Component Map:**
Image -> PPO Agent -> Perturbation Generator -> Modified Image -> Target Model -> Reward Signal -> Agent Update

**Critical Path:**
Image input → PPO agent selects pixel modifications → Modified image generated → Target model processes image → Reward calculated based on attack success → Agent parameters updated

**Design Tradeoffs:**
- **Memory vs. Generalization**: The "Remember" process stores successful perturbations but must be balanced with the "Forget" mechanism to prevent overfitting
- **Query Efficiency vs. Success Rate**: The method prioritizes reducing queries while maintaining high success rates, requiring careful reward function design
- **Perturbation Size vs. Attack Success**: Minimal pixel modifications are preferred for stealth, but may reduce attack effectiveness

**Failure Signatures:**
- Agent overfitting to specific images, resulting in poor generalization across the dataset
- High query counts indicating inefficient exploration of the perturbation space
- Low success rates suggesting inadequate learning of effective attack strategies

**First Experiments to Run:**
1. Validate PPO agent converges to effective policies on a small subset of ImageNet-1K
2. Test "Forget" mechanism effectiveness by comparing with and without forgetting on generalization
3. Evaluate attack transferability from classification to detection on MS-COCO

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance evaluation limited to specific model architectures (ViT-B, ResNet50) and datasets
- "Forget" mechanism may introduce stochasticity affecting attack reproducibility across runs
- Limited analysis of robustness against potential defenses or adaptive countermeasures

## Confidence

| Claim | Confidence |
|-------|------------|
| RFPAR's effectiveness for image classification attacks | High |
| RFPAR's performance on object detection tasks | Medium |
| RFPAR's scalability to larger datasets | Medium |

## Next Checks
1. Evaluate RFPAR's performance against adversarially trained models or models with common defense mechanisms
2. Test the method's transferability across different model architectures beyond ViT and ResNet
3. Conduct ablation studies isolating the contribution of the "Forget" mechanism to determine optimal configuration parameters