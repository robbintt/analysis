---
ver: rpa2
title: Who Reasons in the Large Language Models?
arxiv_id: '2505.20993'
source_url: https://arxiv.org/abs/2505.20993
tags:
- proj
- reasoning
- arxiv
- llms
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper hypothesizes that the output projection (oproj) module
  in the Transformer's multi-head self-attention (MHSA) mechanism is the key component
  responsible for reasoning capabilities in large language models (LLMs). To investigate
  this, the authors introduce Stethoscope for Networks (SfN), a suite of diagnostic
  tools designed to probe and analyze the internal behaviors of LLMs.
---

# Who Reasons in the Large Language Models?

## Quick Facts
- arXiv ID: 2505.20993
- Source URL: https://arxiv.org/abs/2505.20993
- Authors: Jie Shao; Jianxin Wu
- Reference count: 40
- Primary result: Output projection (o_proj) module in MHSA is key to reasoning capabilities in LLMs

## Executive Summary
This paper investigates which Transformer modules are responsible for reasoning capabilities in large language models. Through a suite of diagnostic tools called Stethoscope for Networks (SfN), the authors demonstrate that the output projection (o_proj) module in the multi-head self-attention mechanism undergoes the largest weight changes during reasoning-focused fine-tuning and plays a central role in reasoning performance. The findings suggest that o_proj is the primary locus of reasoning learning, enabling more efficient training strategies that tune only this module and normalization layers while achieving comparable reasoning performance.

## Method Summary
The authors introduce four diagnostic tools (Delta, Merge, Freeze, Destruction Stethoscopes) to probe Transformer module functionality. They compare base models (Qwen2.5-Math, LLaMA3.1) with reasoning models (DeepSeek-R1-Distill-Qwen) across different scales (1.5B-32B parameters). The Delta Stethoscope computes L2 weight differences between model pairs, the Merge Stethoscope transfers modules surgically, the Freeze Stethoscope fine-tunes with selective parameter updates, and the Destruction Stethoscope tests module importance through ablation. Evaluation uses AIME 2024, Math 500, and GPQA Diamond benchmarks.

## Key Results
- o_proj exhibits the largest weight changes when comparing reasoning and non-reasoning models
- Replacing only o_proj in a non-reasoning model with that from a reasoning model enables reasoning task performance
- Tuning only o_proj and layer normalization during fine-tuning achieves strong reasoning performance while being 3x faster and more memory-efficient than full-model fine-tuning
- While o_proj is crucial for reasoning, other modules (MLP) are more important for conversational ability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: o_proj undergoes the most significant weight changes during reasoning-focused fine-tuning
- Core assumption: Weight change magnitude correlates with functional importance for reasoning capability
- Evidence anchors: o_proj L2 norm is at least 2x larger than other components; unique bimodal distribution of relative weight changes
- Break condition: If weight changes were merely overfitting artifacts rather than functional adaptations

### Mechanism 2
- Claim: Transferring only o_proj from reasoning model to non-reasoning model transfers reasoning capability without additional training
- Core assumption: Reasoning capability can be modularly localized and transferred via parameter replacement
- Evidence anchors: Model with only o_proj replaced solves AIME 2024 problems (0.200 accuracy) that base model cannot solve (0.067)
- Break condition: Effect diminishes at larger scales (7B+) due to layernorm parameter mismatch

### Mechanism 3
- Claim: Training efficiency can be dramatically improved by freezing all parameters except o_proj and normalization layers
- Core assumption: O_proj is the primary locus of reasoning learning; other modules contribute minimally to reasoning acquisition
- Evidence anchors: o_proj-only tuning achieves comparable reasoning performance while training 3x faster and using significantly less GPU memory
- Break condition: If reasoning emerges from distributed representations across all modules, freezing would degrade performance

## Foundational Learning

- **Concept**: Transformer MHSA Architecture (q_proj, k_proj, v_proj, o_proj)
  - Why needed here: The paper's central hypothesis depends on distinguishing o_proj from other attention components
  - Quick check question: Can you explain why o_proj is structurally distinct from q/k/v_proj in terms of what it operates on?

- **Concept**: Supervised Fine-Tuning vs. Reinforcement Learning for Reasoning
  - Why needed here: The paper's experimental setup uses SFT to create reasoning models from base models
  - Quick check question: What is the difference between PPO/DPO-based reasoning training and SFT with chain-of-thought traces?

- **Concept**: Weight Difference Analysis for Functional Localization
  - Why needed here: The Delta Stethoscope assumes that ||w(B) - w(A)|| reveals functional importance
  - Quick check question: Why might large weight changes not necessarily indicate functional importance?

## Architecture Onboarding

- **Component map**: MHSA Module: q_proj, k_proj, v_proj, o_proj → o_proj handles reasoning; MLP Module: up_proj, down_proj, gate_proj → handles conversation; Normalization: layernorm (must be tuned alongside o_proj); I/O: embed_tokens, lm_head (must be tuned for convergence)

- **Critical path**: 1) Identify base and reasoning models; 2) Compute ||w_X(B) - w_X(A)||_ℓ2 per module; 3) For transfer: surgically replace o_proj weights only; 4) For efficient training: freeze all params except o_proj + layernorm + embed_tokens + lm_head

- **Design tradeoffs**: Merge Stethoscope works well at 1.5B scale but fails at 7B+ due to layernorm mismatch; Freeze Stethoscope sacrifices some GPQA Diamond performance; Destruction experiments restricted to layers 5-30

- **Failure signatures**: Merged model produces Level I (nonsense) output when MLP is replaced; Training fails to converge if embed_tokens and lm_head are frozen; ReInit destructor causes worse damage than Zero destructor

- **First 3 experiments**: 1) Replicate Delta Stethoscope: Load Qwen2.5-14B and DeepSeek-R1-Distill-Qwen-14B, compute per-module L2 weight distances; 2) Minimal Merge Test: Replace only o_proj in base 1.5B model with reasoning model's o_proj, evaluate on 5 AIME problems; 3) Efficient Fine-tuning: Fine-tune base model freezing all params except o_proj + layernorm + embed_tokens + lm_head on s1K dataset, compare to full-parameter tuning

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the localization of reasoning in o_proj generalize to non-Transformer architectures or Mixture-of-Experts (MoE) models?
  - Basis in paper: Authors acknowledge findings "primarily based on a limited set of model families... and may not generalize to all architectures"
  - Why unresolved: Experiments restricted to standard Transformer decoder models
  - What evidence would resolve it: Applying Stethoscope tools to MoE models or state-space models

- **Open Question 2**: What is the theoretical mechanism by which the output projection specifically facilitates logical inference?
  - Basis in paper: Section 6 states "a theoretical understanding of its function in reasoning remains to be established"
  - Why unresolved: Paper demonstrates correlation and causal capability but not underlying representation dynamics
  - What evidence would resolve it: Mechanistic interpretability studies mapping logical dependencies in o_proj

- **Open Question 3**: Can cross-model parameter merging be stabilized for larger models by correcting normalization mismatches?
  - Basis in paper: Merge Stethoscope worked for 1.5B models but failed for larger ones due to "substantial mismatch in normalization parameters"
  - Why unresolved: Unclear if failure is intrinsic limitation or fixable engineering issue
  - What evidence would resolve it: Experiments aligning or co-tuning LayerNorm parameters during merging

## Limitations
- Merge Stethoscope experiments restricted to 1.5B models due to layernorm parameter mismatches at larger scales
- o_proj-only tuning comes at performance cost for GPQA Diamond, suggesting incomplete reasoning capability capture
- Analysis relies heavily on weight change magnitude as proxy for functional importance without adequate validation

## Confidence

**High Confidence**: Delta Stethoscope observations about o_proj weight changes are well-supported; Merge Stethoscope results robust at 1.5B scale

**Medium Confidence**: Freeze Stethoscope findings about training efficiency are convincing though with performance trade-offs; Destruction Stethoscope's qualitative assessment is supported but subjective

**Low Confidence**: Broader theoretical claims about o_proj being "the key component responsible for reasoning" are not fully substantiated; the reasoning vs conversation dichotomy may be oversimplified

## Next Checks
1. **Scale Generalization Test**: Replicate Merge Stethoscope at 7B+ scales by implementing consistent layer normalization across merged models to test if findings generalize

2. **Ablation of Weight Change Interpretation**: Create synthetic weight change patterns where o_proj weights show large changes without functional reasoning improvements to challenge the correlation assumption

3. **Cross-Domain Reasoning Transfer**: Apply Freeze Stethoscope approach to fine-tune models on multiple reasoning domains to determine if o_proj adaptations are domain-specific or general