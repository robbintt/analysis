---
ver: rpa2
title: Towards Fine-Grained Video Question Answering
arxiv_id: '2503.06820'
source_url: https://arxiv.org/abs/2503.06820
tags:
- video
- scene
- graph
- question
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the MOMA-QA dataset, a VideoQA benchmark
  that addresses gaps in temporal and spatial granularity found in existing datasets.
  MOMA-QA includes 300,791 QA pairs with ground truth scene graphs and temporal intervals,
  enabling fine-grained video understanding.
---

# Towards Fine-Grained Video Question Answering

## Quick Facts
- **arXiv ID:** 2503.06820
- **Source URL:** https://arxiv.org/abs/2503.06820
- **Reference count:** 40
- **One-line primary result:** SGVLM achieves 79.66% accuracy on MOMA-QA, outperforming prior VideoQA models on fine-grained spatial and temporal understanding.

## Executive Summary
This paper introduces MOMA-QA, a new VideoQA benchmark addressing gaps in temporal and spatial granularity found in existing datasets. MOMA-QA includes 300,791 QA pairs with ground truth scene graphs and temporal intervals, enabling fine-grained video understanding. The authors propose SGVLM, a novel video-language model featuring a scene graph predictor, efficient frame retriever, and integration with a pre-trained LLM. SGVLM achieves state-of-the-art results on MOMA-QA with 79.66% accuracy and 0.8688 WUPS@0.9, and also performs well on public datasets like NExT-QA (74.3% accuracy) and QVHighlights. The study demonstrates the effectiveness of scene graphs and temporal grounding in enhancing video QA performance.

## Method Summary
SGVLM processes videos through a pipeline: Frame Encoder (EVA-02) extracts visual features and bounding boxes, Scene Graph Predictor (Neural Motifs) generates structured object relationships, and Frame Localizer (UniVTG-based) selects relevant frames using a dual-stream attention mechanism with masking. Separate Q-Formers align frame and scene graph features to language space before LLM fusion. The model is pre-trained on Visual Genome (SG) and QVHighlights (temporal grounding), then fine-tuned end-to-end on MOMA-QA. The architecture emphasizes spatial reasoning through explicit graph structures and temporal localization through attention-masked frame selection.

## Key Results
- SGVLM achieves 79.66% accuracy on MOMA-QA, outperforming baselines by 3-4% in both accuracy and WUPS@0.9 metrics.
- On relationship questions, SGVLM shows 2.63% higher accuracy than ablations without scene graphs (81.36% vs 78.73%).
- Zero-shot performance on MOMA-QA remains poor (<28% accuracy), highlighting the dataset's fine-grained relational complexity.
- SGVLM generalizes well to public datasets, achieving 74.3% accuracy on NExT-QA and competitive results on QVHighlights.

## Why This Works (Mechanism)

### Mechanism 1: Scene Graph-Guided Spatial Reasoning
Explicitly modeling entity relationships as scene graphs improves performance on spatial reasoning questions. The scene graph predictor extracts objects via biLSTM-encoded contexts, then generates relationship features and probabilities, retaining only top-k relationships. This structured spatial context enables better entity-relationship understanding than implicit frame features alone.

### Mechanism 2: Dual-Stream Frame Localization with Attention Masking
Separating frame and scene graph tokens from direct interaction via attention masking improves temporal localization accuracy. The frame localizer restricts frame and scene graph tokens to interact only with question tokens, preventing the model from attending to inherent frame-SG correlation at the expense of question relevance.

### Mechanism 3: Modality-Specific Q-Former Projection
Using separate Q-Formers for frame and scene graph features before LLM fusion preserves modality-specific information. Two Q-Formers process frame embeddings and scene graph embeddings separately, projecting each to LLM embedding space via linear layers. This recognizes that frame features and scene graph features encode complementary information that benefits from separate alignment to language space.

## Foundational Learning

- **Scene Graph Generation (Visual Genome, Neural Motifs)**: Why needed here: The scene graph predictor is pre-trained on Visual Genome and frozen during VideoQA training. Understanding object detection, relationship classification, and context encoding is essential for debugging SG quality. Quick check question: Can you explain how biLSTM layers encode object context before relationship prediction?

- **Temporal Grounding / Moment Retrieval (QVHighlights, UniVTG)**: Why needed here: The frame localizer is trained on QVHighlights for moment retrieval. Understanding alignment vs. contrastive learning routes and saliency scoring is critical. Quick check question: What is the difference between alignment loss (La) and contrastive loss (Lintra, Linter) in frame selection?

- **Q-Former Architecture (BLIP-2)**: Why needed here: SGVLM directly adopts BLIP-2's Q-Former design for vision-language alignment. Understanding query tokens, attention mechanisms, and projection to LLM space is prerequisite. Quick check question: How do Q-Formers bridge frozen vision encoders to frozen LLMs in BLIP-2?

## Architecture Onboarding

- **Component map:** Video → Frame Encoder → (Scene Graph Predictor + Frame Localizer in parallel) → Q-Formers → LLM → Answer
- **Critical path:** The vision encoder extracts features, scene graph predictor generates relationships, frame localizer selects relevant frames, Q-Formers project to language space, and LLM generates final answer. The Scene Graph Predictor is frozen; Frame Localizer is fine-tuned end-to-end.
- **Design tradeoffs:** Attention masking prevents frame-SG correlation from dominating but requires careful mask design. Two separate Q-Formers preserve modality distinctions but double parameters and complexity. Top-k filtering reduces noise but risks discarding relevant relationships for complex scenes.
- **Failure signatures:** Low relationship accuracy but high action accuracy indicates SG predictor failing. Correct frames selected but wrong answer suggests LLM reasoning or Q-Former projection issue. Incorrect frame selection despite good SG indicates attention mask may be too restrictive.
- **First 3 experiments:** 1) Reproduce ablations: Run SGVLM_NoLoc and SGVLM_NoSG on MOMA-QA validation set to confirm ~2% drop in relationship questions when SG is removed. 2) Visualize attention masks: Plot attention weights from Frame Localizer transformer on 5 sample videos to verify frame-SG token interaction patterns. 3) Error analysis on scene graphs: For failed relationship questions, output predicted scene graphs vs. ground truth to identify systematic object detection or relationship classification gaps.

## Open Questions the Paper Calls Out
The paper identifies three key open questions: How to enhance the vision encoder to accurately identify occluded or small objects within scene graphs to prevent reasoning failures in complex environments; What architectural or training modifications are required to bridge the significant performance gap between fine-tuned and zero-shot settings for fine-grained VideoQA; and To what extent does the reliance on self-generated pseudo-labels limit the performance of the frame localizer when applied to datasets lacking ground truth temporal annotations.

## Limitations
- The reliance on pre-trained frozen components (scene graph predictor and LLM integration) without full disclosure of the LLM architecture may inflate performance gains.
- The QVHighlights pre-training of the frame localizer introduces domain-specific adaptation that may not generalize to other temporal grounding tasks.
- Zero-shot performance on MOMA-QA remains poor (<28% accuracy), highlighting limitations in cross-domain transfer without fine-tuning.

## Confidence
- **High confidence:** Architectural design and ablation results (confirmed by explicit comparison numbers across multiple datasets).
- **Medium confidence:** Temporal grounding mechanism (QVHighlights results show smaller improvements, and attention mask contribution is modest).
- **Low confidence:** Zero-shot generalization claim (paper states all models perform poorly on MOMA-QA without fine-tuning, but doesn't quantify gap between SGVLM and strong baselines).

## Next Checks
1. **Component-level ablation:** Train and evaluate SGVLM with each component removed (NoSG, NoLoc) on a held-out validation set of MOMA-QA to verify the 2-3% accuracy drops reported in the paper, particularly focusing on Relationship vs. Action question categories.

2. **Attention mask visualization:** Generate attention weight heatmaps from the Frame Localizer transformer for 10 sample videos, comparing with and without the attention mask to confirm that frame-SG token interaction is suppressed while maintaining question relevance.

3. **Scene graph error analysis:** For 20 failed relationship questions from MOMA-QA, output the predicted scene graphs alongside ground truth, systematically categorizing errors into object detection failures vs. relationship classification errors.