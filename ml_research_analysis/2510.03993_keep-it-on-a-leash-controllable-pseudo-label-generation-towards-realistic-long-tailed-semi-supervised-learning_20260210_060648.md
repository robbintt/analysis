---
ver: rpa2
title: 'Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic
  Long-Tailed Semi-Supervised Learning'
arxiv_id: '2510.03993'
source_url: https://arxiv.org/abs/2510.03993
tags:
- data
- unlabeled
- distribution
- labeled
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of realistic long-tailed semi-supervised
  learning (LTSSL), where labeled data follow a long-tailed distribution while unlabeled
  data may follow an unknown arbitrary distribution. The authors propose a Controllable
  Pseudo-label Generation (CPG) framework that iteratively expands the labeled dataset
  with reliable pseudo-labels from unlabeled data, training the model on an updated
  labeled dataset with a known distribution to avoid distribution mismatch issues.
---

# Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2510.03993
- Source URL: https://arxiv.org/abs/2510.03993
- Reference count: 40
- Primary result: CPG framework achieves state-of-the-art performance on realistic long-tailed semi-supervised learning, with up to 15.97% accuracy improvement over existing methods.

## Executive Summary
This paper addresses the problem of realistic long-tailed semi-supervised learning (LTSSL), where labeled data follow a long-tailed distribution while unlabeled data may follow an unknown arbitrary distribution. The authors propose a Controllable Pseudo-label Generation (CPG) framework that iteratively expands the labeled dataset with reliable pseudo-labels from unlabeled data, training the model on an updated labeled dataset with a known distribution to avoid distribution mismatch issues. The core method employs a controllable self-reinforcing optimization cycle with dynamic filtering for pseudo-label selection, logit adjustment for Bayes-optimal classifier construction, class-aware adaptive augmentation for minority classes, and an auxiliary branch for data utilization. Theoretical analysis proves the optimization cycle reduces generalization error. Extensive experiments on CIFAR-10-LT, CIFAR-100-LT, Food-101-LT, and SVHN-LT datasets demonstrate CPG achieves state-of-the-art performance across various distribution scenarios.

## Method Summary
CPG operates through a controllable self-reinforcing optimization cycle that iteratively identifies high-confidence pseudo-labels using dynamic filtering, adds them to the labeled set, and updates the known class distribution. The method uses a primary branch with logit-adjusted loss for Bayes-optimal classification on the expanded labeled data, while an auxiliary branch applies standard consistency regularization on all samples to prevent data wastage during early training. The framework includes class-aware adaptive augmentation that synthesizes additional samples for minority classes based on intra-class compactness, and a voting strategy for consistent pseudo-label selection across training steps.

## Key Results
- CPG achieves up to 15.97% accuracy improvement over existing methods on realistic long-tailed semi-supervised learning tasks
- State-of-the-art performance across four datasets (CIFAR-10-LT, CIFAR-100-LT, Food-101-LT, SVHN-LT) with three unlabeled distribution scenarios
- Theoretical analysis proves the optimization cycle reduces generalization error under controlled pseudo-label error rates
- Dynamic filtering with confidence threshold τ=0.95 effectively controls pseudo-label quality while enabling dataset expansion

## Why This Works (Mechanism)

### Mechanism 1: The Controllable Self-Reinforcing Optimization Cycle
The framework iteratively migrates reliable pseudo-labels from unlabeled set to labeled set, ensuring the model trains on a dataset with known distribution. This avoids distribution mismatch errors common in standard SSL. The pseudo-labeling error rate must remain low enough that the benefit of added samples outweighs noise introduction.

### Mechanism 2: Bayes-Optimal Classifier Construction via Logit Adjustment
Training with logit adjustment on the updated labeled set minimizes balanced error and theoretically yields Bayes-optimal predictions. The method calculates class frequencies in the expanded labeled set and applies logit-adjusted softmax loss to counteract long-tailed bias.

### Mechanism 3: Auxiliary Branch for Early-Stage Data Utilization
An auxiliary branch applies standard consistency regularization on all data (labeled and unlabeled) during early training before sufficient pseudo-labels accumulate. This prevents data wastage during the initial 30 epochs when the primary filter is strict.

## Foundational Learning

**Concept: Semi-Supervised Learning (SSL) & Pseudo-Labeling**
- Why needed here: The method builds upon FixMatch but modifies how pseudo-labels are consumed (dataset expansion vs. direct training)
- Quick check question: Can you explain the difference between using a pseudo-label for a single training step (consistency) vs. permanently adding it to the training set?

**Concept: Long-Tailed Distribution & Imbalance Ratio (γ)**
- Why needed here: Realistic LTSSL is defined by the mismatch between labeled tail (γ_l) and unlabeled unknown distribution
- Quick check question: If a dataset has γ=100, how many samples does the tail class have compared to the head class?

**Concept: Logit Adjustment**
- Why needed here: This allows the model to remain Bayes-optimal despite the expanded dataset being potentially imbalanced
- Quick check question: How does adding ln P_y to the logits change the decision boundary for a minority class?

## Architecture Onboarding

**Component map:**
- Encoder (g): Shared backbone
- Primary Branch (f_pri): The "Leash". Uses LA Loss on D_l + reliable pseudo-labels. Strict filtering
- Auxiliary Branch (f_aux): The "Safety Net". Uses Aux Loss on all D_u. Standard consistency
- State Manager: Maintains updated labeled set D_l^(t) and class distribution π^(t)

**Critical path:**
1. Warm-up (Epochs 1-30): Train both branches on D_l only (plus Aux consistency)
2. Selection: Apply Dynamic Controllable Filtering + Voting to select samples
3. Expansion: Update D_l^(t) and recalculate π^(t)
4. Update: Train Primary branch with LA loss based on new π^(t)

**Design tradeoffs:**
- Strictness vs. Growth: Confidence threshold τ controls the "Leash". Lower τ grows dataset faster but risks noise
- Stability: Auxiliary branch prevents model from "starving" for data in early epochs but adds computational overhead

**Failure signatures:**
- Stagnation: D_l^(t) stops growing (0 utilization rate). Fix: Lower τ or check augmentation strength
- Drift: Accuracy drops rapidly after epoch 30. Check: Monitor pseudo-label error rate ε_t; if it rises, filter is admitting noise
- Auxiliary instability: Primary and auxiliary branch accuracies diverge >5% in early training

**First 3 experiments:**
1. Ablation: Run CPG with only Primary Branch (No Aux) to confirm Aux branch's role in early stability
2. Threshold Sweep: Vary τ ∈ [0.75, 0.85, 0.95] on CIFAR-10-LT to understand sensitivity
3. Distribution Visualization: Reproduce Fig 5 to verify predicted distribution (KL divergence) converges to ground truth over time

## Open Questions the Paper Calls Out

**Open Question 1:** How can the CPG framework be effectively adapted to handle scenarios where the initial labeled dataset contains noisy labels?
- Basis: The authors explicitly state in Appendix H that CPG assumes labeled data is free from noise and may encounter challenges with noisy labels
- Why unresolved: The current method relies on reliability of initial labeled distribution to construct Bayes-optimal classifier and guide pseudo-label generation
- What evidence would resolve it: Experiments on standard noisy-label benchmarks (e.g., CIFAR-N) or synthetic noise settings with loss correction mechanism

**Open Question 2:** How does the CPG framework perform when labeled data follows a uniform distribution rather than a long-tailed distribution?
- Basis: Appendix H notes that CPG operates under assumption that labeled data follows long-tailed distribution and may degrade if labeled data is uniform but supervision is extremely limited
- Why unresolved: Logit adjustment depends on estimating class priors from labeled data; uniform distribution with few samples may fail to provide sufficient supervision
- What evidence would resolve it: Experimental results on datasets where labeled subset is uniformly sampled but small in size

**Open Question 3:** Does the Class-Aware Adaptive Augmentation (CAA) module hinder performance for minority classes that naturally exhibit high intra-class diversity?
- Basis: The CAA module assumes minority classes typically exhibit lower intra-class diversity (higher compactness) and sets augmentation radius inversely proportional to compactness
- Why unresolved: If a minority class is inherently diverse, the low compactness score would trigger large augmentation radius, potentially synthesizing representations that invade other classes' feature space
- What evidence would resolve it: Analysis of representation clusters for diverse minority classes, checking if synthesized features overlap with majority classes

**Open Question 4:** What are the computational and memory overheads of iterative dataset expansion strategy on large-scale datasets?
- Basis: The method iteratively expands labeled dataset by moving samples from unlabeled set, effectively growing training set size continuously
- Why unresolved: While accuracy improvements are demonstrated, scalability of continuously appending data to labeled set is not analyzed
- What evidence would resolve it: Profiling of GPU memory usage and training duration per epoch on large-scale datasets as training progresses

## Limitations

- The framework assumes labeled data is free from noise during training and may encounter challenges when adapting to scenarios involving noisy labels
- CPG operates under the assumption that labeled data follows a long-tailed distribution, and performance may degrade if labeled data is uniform but supervision is extremely limited
- Computational overhead increases as the labeled dataset expands iteratively, which may impact scalability on large-scale datasets

## Confidence

- **High confidence:** The core mechanism of controlled pseudo-label expansion with known distribution updates is theoretically sound and empirically validated across multiple datasets
- **Medium confidence:** The effectiveness of logit adjustment depends on accurate class frequency estimation in the expanded labeled set, which may degrade if certain classes are underrepresented
- **Low confidence:** The auxiliary branch's exact contribution to early-stage stability needs more rigorous ablation studies, as weight balancing between primary and auxiliary losses is not fully specified

## Next Checks

1. **Dynamic filtering analysis:** Measure pseudo-label error rate (ε_t) across training iterations to verify the theoretical assumption of controlled error accumulation
2. **Distribution drift monitoring:** Track KL divergence between updated labeled set distribution and original long-tailed distribution to quantify distribution shift over iterations
3. **Ablation on voting strategy:** Compare CPG performance with different pseudo-label selection methods (single step vs. multi-step voting, different temporal windows) to isolate contribution of the voting mechanism