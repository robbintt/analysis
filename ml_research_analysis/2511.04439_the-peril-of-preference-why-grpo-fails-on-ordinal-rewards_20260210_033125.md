---
ver: rpa2
title: 'The Peril of Preference: Why GRPO fails on Ordinal Rewards'
arxiv_id: '2511.04439'
source_url: https://arxiv.org/abs/2511.04439
tags:
- baseline
- grpo
- policy
- training
- advantage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental flaw in GRPO when applied
  to ordinal rewards: its group-average baseline can assign positive advantage to
  failed trajectories, reinforcing incorrect behavior. The authors propose CoRPO,
  which uses an adaptive baseline that enforces a minimum quality threshold, preventing
  failed solutions from receiving positive reinforcement while transitioning to relative
  preference mode as policy performance improves.'
---

# The Peril of Preference: Why GRPO fails on Ordinal Rewards

## Quick Facts
- arXiv ID: 2511.04439
- Source URL: https://arxiv.org/abs/2511.04439
- Reference count: 20
- This paper identifies a fundamental flaw in GRPO when applied to ordinal rewards: its group-average baseline can assign positive advantage to failed trajectories, reinforcing incorrect behavior.

## Executive Summary
This paper identifies a critical flaw in GRPO when applied to ordinal rewards: the group-average baseline can assign positive advantage to failed trajectories, reinforcing incorrect behavior. The authors propose CoRPO, which uses an adaptive baseline that enforces a minimum quality threshold, preventing failed solutions from receiving positive reinforcement while transitioning to relative preference mode as policy performance improves. On a code verification task, CoRPO demonstrates more stable convergence and achieves 95.8% pass@16 on out-of-domain "both correct" evaluation versus 89.6% for GRPO.

## Method Summary
CoRPO modifies GRPO's baseline calculation by clamping it at a minimum correctness threshold. The baseline is computed as $b_{corpo} = max(R_{min\_correct}, b_{mean})$, where $R_{min\_correct}$ is a task-specific threshold separating acceptable from unacceptable outputs. During early training when the policy's average performance is below threshold, the baseline remains clamped, preventing any trajectory with reward below $R_{min\_correct}$ from receiving positive advantage. Once the policy's average performance exceeds the threshold, the baseline transitions to relative preference mode using the group average. The method maintains GRPO's standard advantage calculation and policy update mechanics.

## Key Results
- CoRPO achieves 95.8% pass@16 on out-of-domain "both correct" evaluation versus 89.6% for GRPO
- CoRPO demonstrates more stable convergence with positive-to-negative advantage ratio stabilizing mid-training
- The method shows superior exploration by uniformly reinforcing correct solutions regardless of their initial likelihood

## Why This Works (Mechanism)

### Mechanism 1: Correctness Guarantee via Baseline Clamping
- Claim: Failed trajectories never receive positive advantage under CoRPO, preventing reinforcement of incorrect behavior.
- Mechanism: The baseline is clamped at a minimum correctness threshold: $b_{corpo} = max(R_{min\_correct}, b_{mean})$. Any trajectory with $R(y_f) < R_{min\_correct}$ mathematically must have negative advantage, regardless of how poor the group average is.
- Core assumption: There exists a well-defined threshold $R_{min\_correct}$ that meaningfully separates acceptable from unacceptable outputs for the target task.

### Mechanism 2: Phase Transition from Correctness-Seeking to Preference-Seeking
- Claim: CoRPO automatically shifts learning objective once policy reaches minimum competence.
- Mechanism: When $b_{mean} < R_{min\_correct}$ (early training), baseline is locked to threshold (correctness mode). When $b_{mean} \geq R_{min\_correct}$ (later training), baseline becomes group average (preference mode), pushing model from "acceptable" toward "optimal."
- Core assumption: Policy will eventually reach performance where average reward exceeds the threshold within training budget.

### Mechanism 3: Rank Bias Mitigation via Uniform Reinforcement of Correct Solutions
- Claim: CoRPO enables exploration by reinforcing correct solutions regardless of initial probability, unlike GRPO which favors already-likely trajectories.
- Mechanism: In correctness-seeking phase, all trajectories with $R(y) \geq R_{min\_correct}$ receive positive advantage independent of peer performance, preventing distribution sharpening around high-probability outputs.
- Core assumption: Correctness is more valuable than conformity to current policy preferences; unlikely correct solutions should be valued equally.

## Foundational Learning

- **Concept: Advantage Functions in Policy Gradient Methods**
  - Why needed here: Understanding that $A(y) = R(y) - b$ determines learning signal direction is essential to grasp why a poorly-chosen baseline can reinforce failures.
  - Quick check question: If a trajectory has reward -2 and baseline is -5, what is the advantage and what does this mean for policy updates?

- **Concept: Ordinal vs Binary Rewards**
  - Why needed here: The paper's critique specifically targets ordinal rewards (partial credit) where "less bad" failures can appear better than group average. Binary rewards avoid this by having only two outcome states.
  - Quick check question: Why would GRPO's group-average baseline work reasonably well with binary (0/1) rewards but fail with ordinal (1-5 scale) rewards?

- **Concept: Central Tendency vs Absolute Quality Baselines**
  - Why needed here: CoRPO's innovation is recognizing that baselines encode implicit objectives—group average encodes relative preference, thresholds encode absolute quality standards.
  - Quick check question: If you used only a static threshold baseline throughout training, what learning behavior would you expect in late-stage training compared to CoRPO?

## Architecture Onboarding

- **Component map**: Group sampler -> Reward function -> Baseline estimator (b_corpo = max(R_min_correct, b_mean)) -> Advantage calculator (A = R - b_corpo) -> Policy updater

- **Critical path**:
  1. Define $R_{min\_correct}$ based on task semantics
  2. Sample G rollouts; compute rewards for all
  3. Calculate $b_{mean}$, then clamp to get $b_{corpo}$
  4. Compute advantages; verify failed trajectories have negative advantage
  5. Apply gradient update with standard GRPO loss structure

- **Design tradeoffs**:
  - Coarse (4-point) vs fine (10-point) reward scale: Coarser yields larger advantage magnitudes, faster learning
  - Weight decay presence: Removing WD improves exploration but requires stability monitoring
  - Static vs adaptive baseline: Static gives stronger OOD generalization; adaptive balances in-domain exploitation with correctness guarantee

- **Failure signatures**:
  - **Learning stalls**: Advantage magnitudes near zero—policy predictions clustering tightly. Solution: coarsen reward scale.
  - **No phase transition**: Positive-to-negative ratio never stabilizes—threshold may be too high. Solution: lower $R_{min\_correct}$.
  - **Rank bias resurfaces**: High-probability solutions dominate late training—may have transitioned to preference mode too early. Solution: raise threshold or use static baseline longer.

- **First 3 experiments**:
  1. Reproduce Figure 1 on your task: Plot advantage sign vs trajectory status for GRPO to confirm flaw exists in your domain before implementing fix.
  2. Threshold sensitivity sweep: Test 3 values of $R_{min\_correct}$ (low/medium/high) to find operating range where correctness guarantee holds without stalling learning.
  3. Training dynamics comparison: Plot positive-to-negative advantage ratio over training steps for GRPO vs CoRPO to verify phase transition occurs and timing is appropriate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the CoRPO framework be extended to utilize denser, per-step supervision rather than relying solely on outcome-based rewards?
- Basis in paper: The Abstract and Section 7 explicitly state the authors' goal to progress "onward to denser, per-step supervision" to enable LLMs to learn complex, multi-step reasoning more efficiently.
- Why unresolved: The current work is restricted to outcome-based ordinal rewards applied at the end of a trajectory, which provides a less fine-grained signal than per-step feedback.
- What evidence would resolve it: A modified CoRPO formulation using process-based rewards that demonstrates efficient learning on multi-step reasoning tasks without relying on outcome-level aggregation.

### Open Question 2
- Question: How can training schedules and hyperparameters be adapted to prevent vanishing advantage magnitudes when policy predictions cluster closely on ordinal scales?
- Basis in paper: Section 7 identifies this as a key challenge, noting that "advantage magnitudes are very small when the policy predictions cluster closely together" and that future work must adapt hyperparameters to ensure "consistent, impactful updates."
- Why unresolved: The paper identifies that the current conservative nature of CoRPO leads to smaller weight updates compared to GRPO, and initial solutions (like reward bucketing) are preliminary.
- What evidence would resolve it: The identification of a training configuration or adaptive baseline adjustment that maintains stable gradient magnitudes throughout the clustering phase of training.

### Open Question 3
- Question: What are the underlying training dynamics that allow CoRPO without weight decay to actively encourage the exploration of initially improbable correct solutions?
- Basis in paper: Section 5.3 observes that removing weight decay (WD=0) creates an "interaction with model exploration" that upweights unlikely trajectories, noting that this specific dynamic "warrants deeper analysis."
- Why unresolved: While the empirical result shows improved horizons, the theoretical mechanism for why the absence of weight decay promotes this specific exploration behavior in CoRPO is not yet established.
- What evidence would resolve it: A mechanistic analysis of gradient flow and weight updates in the WD=0 setting, showing how it prevents the dampening of exploration signals for low-probability correct paths.

## Limitations
- The core mechanism depends critically on selecting an appropriate $R_{min\_correct}$ threshold, for which the paper provides no systematic guidance across different task domains.
- The phase transition behavior assumes the policy will reach threshold performance within training budget, but no analysis is provided for cases where this assumption fails.
- The OOD generalization improvements may be partly attributable to the use of static baseline rather than specifically due to the clamping mechanism.

## Confidence

- **High Confidence**: The mathematical proof that GRPO can assign positive advantage to failed trajectories under ordinal rewards, and that CoRPO's clamping mechanism prevents this (Mechanism 1).
- **Medium Confidence**: The phase transition from correctness-seeking to preference-seeking behavior (Mechanism 2) and the rank bias mitigation claims (Mechanism 3), as these are demonstrated primarily through single experimental setup with limited ablation.
- **Medium Confidence**: The OOD generalization improvements, as the comparison includes methodological changes beyond just the baseline clamping.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary $R_{min\_correct}$ across multiple orders of magnitude and measure the proportion of failed trajectories receiving positive advantage, phase transition timing, and final performance to establish robust selection criteria.

2. **Failure Mode Boundary Testing**: Design adversarial scenarios where the policy is intentionally prevented from reaching threshold performance, then verify whether CoRPO degrades gracefully or becomes ineffective.

3. **Cross-Domain Applicability**: Apply CoRPO to at least two additional task domains with different ordinal reward structures (e.g., mathematical reasoning and instruction following) to validate generalizability of the correctness-seeking behavior and phase transition phenomenon.