---
ver: rpa2
title: Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression
  Prediction
arxiv_id: '2509.12234'
source_url: https://arxiv.org/abs/2509.12234
tags:
- modality
- perm-moe
- neuroimaging
- modalities
- flex-moe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Alzheimer's disease progression prediction
  using multimodal neuroimaging data, focusing on improving model performance when
  many modalities are missing during inference - a common clinical scenario. The authors
  introduce PerM-MoE, a novel sparse mixture-of-experts method that uses independent
  routers for each modality instead of a single router, enhancing flexibility when
  modality availability is limited.
---

# Flexible Multimodal Neuroimaging Fusion for Alzheimer's Disease Progression Prediction

## Quick Facts
- arXiv ID: 2509.12234
- Source URL: https://arxiv.org/abs/2509.12234
- Authors: Benjamin Burns; Yuan Xue; Douglas W. Scharre; Xia Ning
- Reference count: 37
- Primary result: PerM-MoE outperforms Flex-MoE in 11 of 15 modality combinations, achieving 8.6%, 4.3%, and 3.6% RMSE improvements with only FLAIR, AB PET, and tau PET respectively

## Executive Summary
This paper addresses Alzheimer's disease progression prediction using multimodal neuroimaging data, focusing on improving model performance when many modalities are missing during inference - a common clinical scenario. The authors introduce PerM-MoE, a novel sparse mixture-of-experts method that uses independent routers for each modality instead of a single router, enhancing flexibility when modality availability is limited. Using T1-weighted MRI, FLAIR, amyloid beta PET, and tau PET data from ADNI, they evaluate PerM-MoE against Flex-MoE and unimodal models for predicting two-year change in CDR-SB scores under varying levels of modality missingness. PerM-MoE outperforms Flex-MoE in 11 of 15 modality combinations, achieving 8.6%, 4.3%, and 3.6% improvements in RMSE with only FLAIR, AB PET, and tau PET respectively, while maintaining competitive performance with greater modality availability.

## Method Summary
The paper proposes PerM-MoE, a sparse mixture-of-experts architecture that addresses multimodal fusion under missing modality conditions. The method uses independent routers for each modality (T1 MRI, FLAIR, AB PET, tau PET) instead of a single shared router, combined with a missing modality bank that imputes learned embeddings for absent modalities. The model processes each available modality through modality-specific CNN encoders, applies self-attention, routes through modality-specific routers to select expert outputs, and predicts two-year change in CDR-SB scores. The architecture includes expert specialization loss to encourage experts to handle specific modality availability patterns, alongside standard SMoE balancing loss. The model is evaluated on ADNI data with systematic modality withholding to simulate clinical missingness scenarios.

## Key Results
- PerM-MoE outperforms Flex-MoE in 11 of 15 modality combinations tested
- Achieves 8.6%, 4.3%, and 3.6% improvements in RMSE with only FLAIR, AB PET, and tau PET respectively
- Shows particular strength in predicting small changes in CDR-SB (0 < ∆ CDR-SB ≤ 1), achieving 21.5% average improvement in RMSE
- Maintains competitive performance when more modalities are available

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Independent per-modality routers improve expert selection when modality availability is severely limited.
- Mechanism: Each modality has its own learned gating network that independently computes routing weights via softmax over expert outputs, decoupling expert selection from modality interactions.
- Core assumption: Modality-specific characteristics differ enough that decoupled routing strategies outperform a unified routing policy.
- Evidence anchors: Abstract states "PerM-MoE...uses independent routers for each modality"; section 2.2 describes "per-modality routing assigns each modality its own router"; related work on multimodal fusion doesn't address per-modality routing.
- Break condition: If modalities are highly correlated (e.g., AB PET and tau PET together), a single generalist router may better capture their interactions.

### Mechanism 2
- Claim: A missing modality bank enables inference by imputing learned placeholder embeddings conditioned on available modalities.
- Mechanism: For missing modality m ∈ M, a learned vector BPm is retrieved based on the set of present modalities P, allowing the model to make predictions even when most modalities are absent.
- Core assumption: The learned placeholders capture enough residual information about missing modalities that downstream experts can still produce useful outputs.
- Evidence anchors: Section 2.1 states "Embeddings for each missing modality m ∈ M are imputed as learned vectors BPm conditioned on P"; references Flex-MoE for detailed descriptions.
- Break condition: If the conditioning set P has very low information, the imputed embeddings may be too generic, limiting prediction quality.

### Mechanism 3
- Claim: Expert specialization loss encourages each expert to handle specific modality availability patterns, improving routing consistency.
- Mechanism: A cross-entropy loss supplements the standard SMoE balancing loss, explicitly training the router to direct inputs with a given modality combination to a designated expert.
- Core assumption: Explicit specialization improves generalization on missing-modality cases better than implicit specialization from standard SMwE training.
- Evidence anchors: Section 2.1 mentions "expert specialization loss...encourages each expert to specialize in a target combination of input modalities"; Figure 3 shows PerM-MoE has more balanced expert activation while retaining some specialization.
- Break condition: If specialization is too rigid, experts may fail to generalize to novel modality combinations not seen during training.

## Foundational Learning

- **Concept**: Sparse Mixture of Experts (SMoE)
  - Why needed: The core architecture routes inputs to a subset of experts via top-k gating; understanding load balancing and expert capacity is essential before modifying routing.
  - Quick check: Can you explain how top-k routing differs from dense routing, and why load balancing loss is typically added?

- **Concept**: Multimodal Fusion under Missingness
  - Why needed: The entire problem setting assumes variable modality availability; you need to understand how fusion strategies degrade when inputs are missing.
  - Quick check: What happens to a concatenation-based fusion model if one modality is missing at inference time?

- **Concept**: Top-k Gating with Softmax
  - Why needed: PerM-MoE extends top-k gating to multiple independent routers; the softmax and sparsity mechanism are foundational.
  - Quick check: Given gating logits v, what does Top-k(v, j) output for an expert j that is not in the top k?

## Architecture Onboarding

- **Component map**: Modality-specific CNN encoders → Self-attention layer → Per-modality routers (4 routers) → Sparse expert layer (E experts) → Concatenation with baseline CDR-SB → MLP prediction head → ∆CDR-SB output
- **Critical path**: Encoder outputs → self-attention → per-modality routing (ensure correct router per modality) → expert outputs → prediction. If routing misassigns or missing modality bank fails, the whole prediction collapses.
- **Design tradeoffs**:
  - More experts increase specialization capacity but raise training complexity and risk underutilization
  - Independent routers improve single-modality performance but may miss cross-modality interactions
  - Missing modality bank enables inference but introduces approximation error when conditioning information is sparse
- **Failure signatures**:
  - Expert collapse: One expert receives almost all inputs (check activation distribution via Figure 3-style analysis)
  - Router confusion: Single-modality inputs routed inconsistently across runs (check router output variance)
  - Imputation drift: Predictions for missing-modality cases degrade sharply compared to full-modality cases
- **First 3 experiments**:
  1. Replicate single-modality RMSE comparison (Table 1, columns with only one filled circle) to validate per-modality routing benefit over Flex-MoE.
  2. Ablate the missing modality bank (replace with zeros or random vectors) to measure its contribution to prediction quality.
  3. Visualize expert activation patterns (replicate Figure 3) to confirm balanced expert utilization and identify any collapsed experts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does per-modality routing underperform compared to a single shared router for highly correlated modalities?
- Basis in paper: The authors note that Flex-MoE outperforms PerM-MoE when only AB PET and tau PET are available, speculating that "a single, generalist router may better capture potential interactions between these two, similar modalities."
- Why unresolved: The paper does not systematically investigate the relationship between modality similarity/correlation and optimal routing architecture.
- What evidence would resolve it: A controlled study varying modality pairwise correlations and comparing PerM-MoE vs. single-router performance across these conditions.

### Open Question 2
- Question: Does PerM-MoE generalize to non-neuroimaging modalities and other disease progression tasks beyond AD CDR-SB prediction?
- Basis in paper: The authors state "our per-modality routing design is modality-agnostic and can be applied in a wide variety of prediction problems" but only evaluate on four neuroimaging modalities for AD.
- Why unresolved: No experiments validate the claimed modality-agnostic nature on other data types (e.g., clinical, genetic, laboratory) or other diseases.
- What evidence would resolve it: Evaluation on additional modalities (blood biomarkers, genetic data, cognitive tests) and other progressive diseases (Parkinson's, ALS).

### Open Question 3
- Question: How does PerM-MoE perform on prediction horizons longer than two years?
- Basis in paper: The study only evaluates two-year CDR-SB change; clinical planning often requires longer-term prognostic estimates.
- Why unresolved: Longer prediction horizons may involve different disease dynamics and modality importance patterns not captured in the 2-year window.
- What evidence would resolve it: Experiments predicting 3-year, 5-year, and 7-year CDR-SB changes under the same missingness conditions.

## Limitations

- Several critical architectural details remain underspecified, particularly the number of experts, router architectures, and exact training hyperparameters
- The paper does not report whether performance improvements persist across different seeds beyond the reported three runs, limiting statistical confidence
- The missing modality bank's conditioning mechanism lacks complete description, making exact reproduction challenging

## Confidence

- **High Confidence**: The core mechanism of per-modality routers and their advantage for single-modality predictions is well-supported by the 8.6%, 4.3%, and 3.6% RMSE improvements in Table 1.
- **Medium Confidence**: The expert specialization loss's contribution remains uncertain without ablation studies. The missing modality bank's effectiveness is plausible but not directly validated.
- **Low Confidence**: Exact hyperparameter choices, particularly expert count and router architecture, significantly impact performance but are not disclosed.

## Next Checks

1. **Ablation of Missing Modality Bank**: Replace the learned missing modality bank with zero vectors or random embeddings to quantify its contribution to single-modality performance improvements.

2. **Expert Activation Analysis**: Replicate the expert activation distribution analysis (Figure 3) across all 15 modality combinations to verify balanced utilization and identify potential expert collapse in PerM-MoE.

3. **Seed Sensitivity Analysis**: Extend evaluation to 10+ random seeds to establish statistical significance of the reported RMSE improvements, particularly for single-modality cases where gains appear most pronounced.