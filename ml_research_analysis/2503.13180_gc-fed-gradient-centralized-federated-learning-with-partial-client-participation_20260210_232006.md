---
ver: rpa2
title: 'GC-Fed: Gradient Centralized Federated Learning with Partial Client Participation'
arxiv_id: '2503.13180'
source_url: https://arxiv.org/abs/2503.13180
tags:
- local
- learning
- gradient
- gc-fed
- global
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Gradient Centralized Federated Learning (GC-Fed),
  a method to mitigate client drift in federated learning (FL) when only a subset
  of clients participate in each training round. Existing approaches use historical
  snapshots as reference points, which can be inaccurate under partial participation,
  leading to unstable training.
---

# GC-Fed: Gradient Centralized Federated Learning with Partial Client Participation

## Quick Facts
- arXiv ID: 2503.13180
- Source URL: https://arxiv.org/abs/2503.13180
- Reference count: 40
- Key outcome: Proposes GC-Fed method achieving up to 20% accuracy improvement under heterogeneous partial participation scenarios

## Executive Summary
This paper addresses client drift in federated learning when only a subset of clients participate in each training round. The authors propose Gradient Centralized Federated Learning (GC-Fed), which uses a hyperplane as a historically independent reference point to guide local training and enhance inter-client alignment. The method consists of Local GC for feature-extraction layers during client training and Global GC for classifier layers during server aggregation. Extensive experiments demonstrate that GC-Fed effectively mitigates client drift and achieves significant performance gains over FedAvg and state-of-the-art baselines under various non-IID data distributions and partial participation settings.

## Method Summary
GC-Fed employs gradient centralization through projection onto a hyperplane defined by P = I - ee^T, where e is a normalized all-ones vector. Local GC is applied during client-side training to feature layers (layers 1 to ⌊λ·L⌋), while Global GC is applied during server aggregation to classifier layers. The method uses SGD with momentum=0.9, weight_decay=1e-5, and local_epochs=5. Participation ratios range from 5/100 to 5/1000 clients per round, with non-IID data partitioned using LDA with heterogeneity parameter α ∈ {0.05, 0.1, 1.0, 1000}. The layer boundary λ is typically set to 0.85-0.95 for ResNet architectures.

## Key Results
- Achieves up to 20% accuracy improvement over FedAvg under heterogeneous and partial participation conditions
- GC-Fed with λ=0.85-0.95 provides optimal balance between performance and stability
- Outperforms state-of-the-art methods like FedSSG and FedDPC in accuracy and convergence speed
- Maintains high performance while requiring no additional communication overhead

## Why This Works (Mechanism)

### Mechanism 1: Historically Independent Reference Point
- **Claim:** Projecting gradients onto a fixed hyperplane provides a historically independent reference point that aligns client updates without relying on potentially stale historical information.
- **Mechanism:** GC projects gradients via $\tilde{\mathbf{G}} = \mathbf{P}\mathbf{G}$ where $\mathbf{P} = \mathbf{I}_m - \mathbf{ee}^\top$ is the projection matrix. This centers gradients to zero-mean across output channels, constraining all clients to optimize within the same subspace regardless of which subset participates.
- **Core assumption:** The optimal solution $\mathbf{w}^*$ lies in (or near) the zero-mean subspace defined by $\mathbf{P}$. The paper notes potential discrepancy bounded by $\frac{L}{2}\|\Delta\|^2$ if this fails.
- **Evidence anchors:**
  - [abstract] "employs a hyperplane as a historically independent reference point to guide local training"
  - [section 3.1] "This hyperplane can serve as a reliable reference for aligning local gradients or updates, potentially offering benefits in FL—even without additional information sharing"
  - [corpus] FedSSG and FedDPC also address partial participation drift, but use history-aware mechanisms; GC-Fed is distinctive in being reference-free
- **Break condition:** If the true global optimum has significant orthogonal components (non-zero column means), GC introduces bias. Extreme class imbalance with single-class clients can cause gradient explosion.

### Mechanism 2: Layer-Specific Centralization Strategy
- **Claim:** Applying Local GC to feature layers and Global GC to classifier layers balances performance and stability better than either alone.
- **Mechanism:** Feature layers capture general representations; centralizing their gradients during local training harmonizes client contributions without communication overhead. Classifier layers are more sensitive to class imbalance; centralizing at aggregation smooths round-to-round variance from partial participation.
- **Core assumption:** Feature and classifier layers have different roles and drift characteristics—feature layers benefit from local alignment, while classifier layers need global coordination.
- **Evidence anchors:**
  - [abstract] "Local GC is applied to feature-extraction layers to harmonize client contributions, while Global GC refines classifier layers to stabilize round-wise performance"
  - [section 3.3] Table 2 shows Local GC has higher mean accuracy (0.065) but higher variance (std 6.18), while Global GC has lower variance (std 3.70) but slightly lower mean (0.062); GC-Fed achieves high mean (0.067) with moderate variance (4.33)
  - [corpus] Classifier variance is a known FL issue; neighbor papers like FedLC address it through logit calibration rather than gradient projection
- **Break condition:** Layer borderline $\lambda$ selection matters. The ablation shows $\lambda \in [0.85, 0.95]$ works best for ResNet18; inappropriate $\lambda$ may misallocate centralization effort.

### Mechanism 3: Variance Reduction for Faster Convergence
- **Claim:** Projection reduces gradient variance, accelerating convergence toward the optimum.
- **Mechanism:** Theoretical analysis shows GC-Fed reduces the gap to optimum by non-negative terms $\eta^2_\tau\|\mathbf{e}^\intercal\mathbf{G}_\tau\|^2$ (gradient mean norm) and $\eta^2_\tau\|\mathbf{e}^\intercal(\mathbf{G}_\tau - \bar{\mathbf{G}}_\tau)\|^2$ (variance of the mean), compared to FedAvg.
- **Core assumption:** Assumption 1 holds—orthogonal component of $\mathbf{w}^*$ is zero or negligible. L-smoothness of the loss function bounds residual error.
- **Evidence anchors:**
  - [section 3.4] Theorem 3 and Lemma 1 formally derive the gap reduction
  - [appendix A.3] Provides bounded discrepancy analysis when assumption fails
  - [corpus] FedAdaVR also uses variance reduction but adaptively; the mathematical approach differs from projection-based GC
- **Break condition:** If gradients are highly sparse or structured such that $\mathbf{e}^\intercal\mathbf{G}$ is already near zero, GC provides minimal benefit.

## Foundational Learning

- **Concept: Client Drift in FL**
  - **Why needed here:** GC-Fed directly addresses drift caused by non-IID data and partial participation. Understanding drift is essential to interpret why a shared hyperplane reference helps.
  - **Quick check question:** In a 100-client FL system where only 5 clients participate per round with highly skewed class distributions, why would local updates diverge from the global optimum even without GC?

- **Concept: Gradient Centralization (GC)**
  - **Why needed here:** The entire method builds on GC, originally a centralized optimization technique. You must understand both formulations (mean-subtraction and projection) to implement correctly.
  - **Quick check question:** For a convolutional layer with shape $[C_{out}, C_{in}, K_w, K_h]$, along which dimension does GC compute the mean, and what is the shape of the resulting $\mu$ vector?

- **Concept: Partial Participation Effects**
  - **Why needed here:** The paper argues that historical references fail under partial participation because sampled clients don't represent the population. Figure 1 visualizes this discrepancy.
  - **Quick check question:** As client participation ratio decreases from 50/100 to 5/100, what happens to the gap between the "true update" and "partial update"?

## Architecture Onboarding

- **Component map:** Client model -> Local GC (layers 1 to ⌊λ·L⌋) -> SGD update -> Server aggregation -> Global GC (classifier layer) -> Global model update

- **Critical path:**
  1. Initialize global model $\mathbf{w}^0$
  2. For each round: select clients $\mathcal{K}$
  3. Clients receive $\mathbf{w}^t$, compute gradients, apply Local GC to layers $\leq L_{local}$, update locally for $E$ epochs
  4. Clients send $\Delta_k = \mathbf{w}^t_k - \mathbf{w}^t$ to server
  5. Server aggregates $\Delta = \frac{1}{|\mathcal{K}|}\sum_{k \in \mathcal{K}} \Delta_k$, applies Global GC, updates $\mathbf{w}^{t+1} = \mathbf{w}^t + \tilde{\Delta}$

- **Design tradeoffs:**
  - **Local GC only:** Higher peak accuracy but unstable training (high std in round-to-round accuracy)
  - **Global GC only:** More stable but slightly lower performance; fails under extreme heterogeneity ($\alpha=0.05$, 5/100 participation)
  - **GC-Fed (hybrid):** Best of both; requires tuning $\lambda$. Paper recommends $\lambda \approx 0.85$–$0.95$ for ResNet18
  - **No extra communication/storage:** Unlike SCAFFOLD, FedVARP, FedDyn which require control variates or history storage

- **Failure signatures:**
  - **Gradient explosion:** When extreme class imbalance (single-class clients) combines with aggressive centralization; manifests as sudden accuracy collapse
  - **Convergence to wrong optimum:** If assumption 1 badly violated, model may converge to biased solution
  - **Global GC + high local momentum:** FedACG-style conflicts observed; local momentum 0.9 with Global GC can degrade performance

- **First 3 experiments:**
  1. **Sanity check:** Implement FedAvg baseline on CIFAR-10 with 5/100 clients, $\alpha=0.1$. Verify accuracy ~53–66% (Table 5). Add Local GC only; expect ~71% but observe variance.
  2. **Layer borderline ablation:** Fix architecture (CNN on CIFAR-10), sweep $\lambda \in \{0.0, 0.5, 0.85, 0.95, 1.0\}$. Plot accuracy vs. $\lambda$. Confirm peak at 0.85–0.95.
  3. **Integration test:** Take FedProx or FedLC baseline, add Global GC only. On CIFAR-100 ResNet18, expect +10–15 point gain (Table 7). Then add full GC-Fed; expect +17–22 point gain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does GC-Fed perform when applied to Transformer-based architectures?
- **Basis in paper:** [explicit] Page 10 states that GC "can be extended to various architectures, including transformer-based models," but the experiments were restricted to MLPs, CNNs, VGG, and ResNets.
- **Why unresolved:** The theoretical projection benefits were only empirically validated on convolutional and fully-connected layers; the interaction with attention mechanisms remains untested.
- **What evidence would resolve it:** Benchmarking GC-Fed on vision transformers (ViT) or BERT models in a federated setting.

### Open Question 2
- **Question:** Can the layer borderline $\lambda$ be determined adaptively rather than manually?
- **Basis in paper:** [inferred] Page 8 introduces $\lambda$ to allow "adaptive control," and Section 4.3.2 analyzes performance across fixed $\lambda$ values, but no method for dynamic tuning is proposed.
- **Why unresolved:** The optimal $\lambda$ range (0.85–0.95) varies by setting, requiring a grid search; an automated mechanism could enhance usability across diverse architectures.
- **What evidence would resolve it:** A proposed algorithm that adjusts $\lambda$ during training and matches or exceeds the performance of the best static $\lambda$ configuration.

### Open Question 3
- **Question:** Can specific regularization techniques stabilize Global GC in extreme non-i.i.d. scenarios?
- **Basis in paper:** [explicit] Page 11 notes that Global GC failed to train under extreme heterogeneity ($\alpha=0.05$) due to gradient explosion, and the authors stated, "We do not explore this direction" regarding gradient clipping or tuning.
- **Why unresolved:** It is unclear if Global GC is fundamentally brittle in these scenarios or if standard optimization tricks used in other FL methods would allow it to converge.
- **What evidence would resolve it:** A study showing stable convergence of Global GC with gradient clipping or adaptive learning rates under $\alpha < 0.1$.

## Limitations

- Global GC fails under extreme non-IID conditions (α=0.05) with single-class clients causing gradient explosion
- Theoretical analysis relies on L-smoothness and assumes negligible orthogonal component of the optimum
- Layer boundary hyperparameter λ requires manual tuning per architecture without principled selection method

## Confidence

- **High confidence** in Local GC for feature layers: well-established technique with clear empirical support
- **Medium confidence** in Global GC effectiveness: theoretical justification exists but performance varies with participation ratio and data heterogeneity
- **Medium confidence** in hybrid GC-Fed approach: ablation studies show benefits, but optimal λ selection remains heuristic
- **Low confidence** in scalability claims: experiments use moderate client counts (100-1000) without massive-scale validation

## Next Checks

1. Test GC-Fed on a 10,000-client synthetic benchmark with extreme α=0.01 and 1% participation to evaluate scalability and robustness
2. Implement a gradient clipping mechanism specifically for Global GC under single-class client scenarios and measure performance impact
3. Conduct a systematic study of λ selection across multiple architectures (CNN, ResNet variants, Transformers) to derive a principled selection criterion based on layer characteristics