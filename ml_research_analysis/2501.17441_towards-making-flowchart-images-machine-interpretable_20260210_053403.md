---
ver: rpa2
title: Towards Making Flowchart Images Machine Interpretable
arxiv_id: '2501.17441'
source_url: https://arxiv.org/abs/2501.17441
tags:
- flowchart
- code
- images
- dataset
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FloCo, the first large-scale dataset for
  the Flow2Code task, containing 11,884 flowchart images and corresponding Python
  codes. The authors propose FloCo-T5, a transformer-based framework that converts
  flowchart images to executable Python code.
---

# Towards Making Flowchart Images Machine Interpretable

## Quick Facts
- arXiv ID: 2501.17441
- Source URL: https://arxiv.org/abs/2501.17441
- Reference count: 35
- Introduces FloCo dataset with 11,884 flowchart-image to Python-code pairs and achieves BLEU 67.4 on Flow2Code task

## Executive Summary
This paper introduces FloCo, the first large-scale dataset for converting flowchart images into executable Python code, and proposes FloCo-T5, a transformer-based framework that achieves state-of-the-art results on this task. The approach involves converting flowchart images into structured sequence encodings using OCR and shape detection, pre-training CodeT5 with logic-preserving augmented code samples, and fine-tuning for code generation. Experiments demonstrate that FloCo-T5 significantly outperforms competitive baselines and generalizes to hand-drawn flowcharts, advancing the goal of making flowchart images machine-interpretable.

## Method Summary
The FloCo-T5 framework processes flowchart images through a multi-stage pipeline: shape detection identifies block types (OVAL, RECTANGLE, DIAMOND, PARALLELOGRAM) via Hough transform, OCR extracts intra-block text using EasyOCR for digitized flowcharts, and coordinates matching pairs text with shapes. The final encoding concatenates text-shape pairs with [SEP] tokens into a linearized text sequence. The model is pre-trained using masked token modeling on 40,408 augmented code samples (4× expansion via random function and variable name substitution), then fine-tuned on the original FloCo dataset using CodeT5's architecture with cross-attention between encoder and decoder.

## Key Results
- FloCo-T5 achieves BLEU 67.4, CodeBLEU 75.7, and exact match 20.0 on the Flow2Code task
- Outperforms competitive baselines including BART (BLEU 45.8), PLBART (BLEU 48.4), and CodeT5 (BLEU 63.8)
- Generalizes to hand-drawn flowcharts with BLEU 21.4, demonstrating practical utility
- Modified string encoding outperforms tuple and string encodings by 50.7 BLEU points

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Converting flowchart images into structured sequence encodings enables transformer models to process visual-logical information as text.
- Mechanism: Shape detection identifies block types via Hough transform, OCR extracts intra-block text, and coordinates matching pairs text with shapes. The final encoding concatenates text-shape pairs with [SEP] tokens.
- Core assumption: The semantic structure of flowcharts can be losslessly represented as linearized text sequences without explicit graph encoding.
- Evidence anchors: [abstract] model can effectively learn semantics, structure, and patterns; [section 4.1] final encoding is text sequence combining recognized text and shapes; [corpus] limited direct corroboration.
- Break condition: If flowcharts contain complex non-sequential control flow, linearization may lose structural information and degrade performance.

### Mechanism 2
- Claim: Logic-preserving data augmentation combined with masked token modeling pre-training improves code generation by exposing the model to equivalent programs with different surface forms.
- Mechanism: Function and variable names are randomly replaced while preserving program logic, expanding training data 4×. The model then reconstructs masked tokens in both encodings and code, learning to infer shapes from context and code semantics from structure.
- Core assumption: Name substitution preserves logical equivalence and the model generalizes from augmented samples to unseen naming patterns.
- Evidence anchors: [abstract] task-specific pre-training objective using logic-preserving augmented code samples; [section 4.2] train dataset increased from 10,102 to 40,408; [corpus] weak corroboration from related work.
- Break condition: If test programs use domain-specific naming conventions absent from augmentation vocabulary, generalization may suffer.

### Mechanism 3
- Claim: Fine-tuning a code-pretrained encoder-decoder transformer (CodeT5) on flowchart-code pairs transfers programming language knowledge to the Flow2Code task.
- Mechanism: CodeT5's pre-training on code corpora provides syntactic and semantic priors. Fine-tuning on (encoding, code) pairs aligns flowchart representations with code generation, using autoregressive decoding with cross-attention to encoder outputs.
- Core assumption: CodeT5's pre-trained representations transfer to the novel flowchart encoding domain without catastrophic forgetting.
- Evidence anchors: [section 5.4] CodeT5 is pre-trained with programming-language-specific, fine-grained identifier-aware denoising tasks; [table 3] FloCo-T5 achieves 67.4 BLEU vs. CodeT5's 63.8; [corpus] consistent with MM-Coder showing multimodal code generation benefits from code-pretrained models.
- Break condition: If flowchart structures diverge significantly from code patterns, pre-trained code knowledge may mislead generation.

## Foundational Learning

- Concept: Transformer encoder-decoder architecture (T5 family)
  - Why needed here: FloCo-T5 inherits T5's architecture; understanding cross-attention, autoregressive decoding, and tokenization is essential for debugging and extension.
  - Quick check question: Can you explain how the decoder attends to encoder outputs during token generation?

- Concept: OCR and shape detection pipelines
  - Why needed here: The first stage converts images to sequence encodings; errors here cascade to code generation.
  - Quick check question: What happens if OCR misreads "if" as "it" inside a decision diamond?

- Concept: Pre-training vs. fine-tuning paradigm
  - Why needed here: The paper's contribution combines both; distinguishing their roles is critical for replication and improvement.
  - Quick check question: Why does pre-training on augmented data help, given that augmentation doesn't add new logic?

## Architecture Onboarding

- Component map:
  1. Input: Flowchart image (cropped)
  2. Shape Detector (Hough transform) → Block types + coordinates
  3. OCR (EasyOCR for digitized, CRAFT+TrOCR for hand-drawn) → Text + coordinates
  4. Encoding Generator → Linearized text with [SEP] tokens
  5. CodeT5 Encoder (12 layers, 222.9M params) → Contextualized representations
  6. CodeT5 Decoder (12 layers) → Autoregressive Python code
  7. Output: Python code string

- Critical path: Encoding representation → pre-training data quality → fine-tuning alignment. Table 4 shows modified string encodings outperform tuple encodings by 50.7 BLEU points, making encoding format the highest-leverage design decision.

- Design tradeoffs:
  - Tuple vs. string vs. modified string encoding (Table 4): [SEP]-based encoding best preserves structure.
  - Augmentation diversity vs. data bias: Random name generation avoids overfitting but may miss real-world naming patterns.
  - Pre-training compute vs. fine-tuning data: 12-hour pre-training on augmented data; diminishing returns possible.

- Failure signatures:
  - Long programs (>15 lines): Performance drops due to dataset bias toward short programs (avg. 4.6 lines; see Figure 8).
  - Hand-drawn flowcharts: OCR errors (e.g., "*" misrecognized) propagate; BLEU drops from 67.4 to 21.4.
  - Uncommon shapes or arrow annotations: Not in vocabulary; may cause encoding failures.

- First 3 experiments:
  1. Replicate encoding ablation (Table 4): Train with tuple, string, and modified string encodings on a small subset; verify [SEP]-token encoding superiority.
  2. Baseline comparison sanity check: Train Vanilla Transformer and BART on FloCo; confirm they underperform CodeT5-based models as reported.
  3. Hand-drawn stress test: Manually create 10 hand-drawn flowcharts with clear text; measure OCR error rate and its correlation with code generation quality to isolate OCR bottleneck.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model's performance be stabilized for flowcharts representing longer programs (>15 lines of code)?
- Basis in paper: [explicit] The authors note that performance drops for longer programs due to the dataset's bias towards shorter samples (average length 4.6 lines).
- Why unresolved: The current dataset distribution lacks sufficient complex examples to teach the model robust handling of extended control flow.
- What evidence would resolve it: Results from a model trained on an expanded dataset specifically curated to include a higher proportion of long, complex flowcharts.

### Open Question 2
- Question: Can an end-to-end visual model outperform the proposed pipeline approach for hand-drawn flowcharts?
- Basis in paper: [inferred] The paper identifies OCR errors (e.g., misinterpreting '*') as the primary failure mode for hand-drawn samples in Figure 7, implying the separation of visual recognition and code generation is a bottleneck.
- Why unresolved: The current framework relies on off-the-shelf OCR tools which propagate errors to the transformer, making it unclear if a unified visual encoder could better handle noisy handwriting.
- What evidence would resolve it: A comparative study between the current OCR-based pipeline and a model utilizing a visual encoder (e.g., ViT) trained directly on raw image pixels.

### Open Question 3
- Question: How do state-of-the-art Large Language Models (LLMs) compare to FloCo-T5 on the Flow2Code task when