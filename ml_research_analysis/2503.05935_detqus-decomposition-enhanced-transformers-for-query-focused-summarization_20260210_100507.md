---
ver: rpa2
title: 'DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization'
arxiv_id: '2503.05935'
source_url: https://arxiv.org/abs/2503.05935
tags:
- table
- data
- summarization
- tabular
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents DETQUS, a novel system for query-focused tabular
  summarization that leverages table decomposition to improve summarization accuracy.
  DETQUS uses a large language model to selectively reduce table size by retaining
  only query-relevant columns while preserving essential information, enabling more
  efficient processing of large tables.
---

# DETQUS: Decomposition-Enhanced Transformers for QUery-focused Summarization

## Quick Facts
- arXiv ID: 2503.05935
- Source URL: https://arxiv.org/abs/2503.05935
- Reference count: 10
- ROUGE-L score: 0.4437 on QTSUMM dataset

## Executive Summary
DETQUS introduces a novel approach to query-focused tabular summarization by leveraging table decomposition to improve summarization accuracy. The system uses a large language model to selectively reduce table size by retaining only query-relevant columns while preserving essential information, enabling more efficient processing of large tables. This addresses the challenge of token limitations when dealing with complex tabular data.

## Method Summary
DETQUS employs a multi-stage approach to query-focused tabular summarization. The system first uses a large language model to perform table decomposition, selectively retaining columns that are most relevant to the query while preserving essential information. This decomposed table is then processed by a fine-tuned encoder-decoder model, Omnitab, which generates the final summary. The decomposition step is critical for handling large tables that would otherwise exceed token limits, allowing the system to maintain focus on query-relevant content while reducing computational overhead.

## Key Results
- Achieved ROUGE-L score of 0.4437 on QTSUMM dataset
- Outperformed previous state-of-the-art REFACTOR model (ROUGE-L: 0.422)
- Demonstrated effective mitigation of token limitation issues through table decomposition

## Why This Works (Mechanism)
DETQUS works by addressing the fundamental challenge of processing large tabular data within token limitations. The table decomposition approach allows the system to maintain semantic relevance to the query while reducing the computational burden. By using a large language model to identify and retain only the most relevant columns, DETQUS preserves the essential information needed for accurate summarization while discarding less relevant data that would otherwise consume valuable tokens.

## Foundational Learning
- Table decomposition - selectively reducing table size by retaining query-relevant columns while preserving essential information; needed to handle token limitations in large tables; quick check: verify decomposition retains all query-relevant data
- Query-focused summarization - generating summaries that specifically address user queries; needed for targeted information extraction; quick check: ensure summaries directly answer posed queries
- ROUGE-L metric - measures longest common subsequence between generated and reference summaries; needed for automated evaluation of summarization quality; quick check: verify ROUGE-L scores align with human judgment

## Architecture Onboarding

Component map: Table -> LLM Decomposition -> Omnitab Encoder-Decoder -> Summary

Critical path: Query + Table → LLM Decomposition → Fine-tuned Omnitab → Summary Generation

Design tradeoffs: The system prioritizes computational efficiency through decomposition over processing entire tables, potentially sacrificing some contextual information for scalability.

Failure signatures: Poor decomposition decisions could lead to missing critical information; overly aggressive column removal might result in incomplete summaries.

First experiments:
1. Test decomposition quality on tables with intentionally distributed relevant information
2. Evaluate performance degradation when processing full tables without decomposition
3. Assess sensitivity to different prompting strategies in the decomposition stage

## Open Questions the Paper Calls Out
The paper highlights several uncertainties around the general applicability of the table decomposition approach. Questions remain about how well the selective column retention strategy generalizes to tables with different structures, missing values, or highly heterogeneous data types. The system's reliance on a large language model for decomposition decisions introduces potential variability that wasn't fully characterized, and it's unclear how sensitive the decomposition quality is to different prompting strategies or model versions.

## Limitations
- Limited validation beyond the primary QTSUMM dataset
- Unclear how the system handles tables where query-relevant information is distributed across multiple columns
- Absence of comprehensive ablation studies showing individual component contributions

## Confidence

High confidence: ROUGE-L score improvement claim (clear numerical comparison provided)

Medium confidence: Decomposition approach effectiveness (limited validation beyond primary dataset)

Low confidence: Scalability claims (theoretical discussion but minimal empirical scaling analysis)

## Next Checks
1. Conduct systematic ablation studies comparing DETQUS performance with and without the decomposition module across multiple datasets
2. Test the system on tables with intentionally distributed relevant information across seemingly peripheral columns to assess decomposition reliability
3. Perform human evaluation studies to verify that the summary quality improvements correspond to meaningful information retention rather than metric gaming