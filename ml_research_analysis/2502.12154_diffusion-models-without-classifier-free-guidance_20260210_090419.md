---
ver: rpa2
title: Diffusion Models without Classifier-free Guidance
arxiv_id: '2502.12154'
source_url: https://arxiv.org/abs/2502.12154
tags:
- diffusion
- guidance
- ours
- training
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Model-guidance (MG), a novel training objective
  for diffusion models that eliminates the need for Classifier-free Guidance (CFG).
  MG incorporates the posterior probability of conditions directly into the diffusion
  model's training process by using the model itself as an implicit classifier, guiding
  the denoising process without requiring separate unconditional models or multiple
  forward passes during inference.
---

# Diffusion Models without Classifier-free Guidance

## Quick Facts
- arXiv ID: 2502.12154
- Source URL: https://arxiv.org/abs/2502.12154
- Authors: Zhicong Tang; Jianmin Bao; Dong Chen; Baining Guo
- Reference count: 18
- Primary result: Eliminates classifier-free guidance while achieving state-of-the-art FID of 1.34 on ImageNet 256

## Executive Summary
This paper introduces Model-guidance (MG), a novel training objective for diffusion models that eliminates the need for Classifier-free Guidance (CFG). MG incorporates the posterior probability of conditions directly into the diffusion model's training process by using the model itself as an implicit classifier, guiding the denoising process without requiring separate unconditional models or multiple forward passes during inference. The proposed method significantly accelerates training (6.5x faster convergence), doubles inference speed, and achieves state-of-the-art performance on ImageNet 256 benchmarks with an FID of 1.34.

## Method Summary
The core innovation is a training objective that integrates conditional and unconditional objectives through an auxiliary task that predicts the posterior probability of conditions given noisy latents. Instead of training separate conditional and unconditional models or using dropout-based mixing during training, MG trains a single model to predict both the noise and the posterior probability simultaneously. During inference, the model uses its own predictions to guide sampling through a single forward pass, eliminating the need for CFG's multiple model evaluations. This is achieved by modifying the denoising objective to include terms that encourage the model to capture conditional dependencies directly in the latent space.

## Key Results
- Achieves state-of-the-art FID of 1.34 on ImageNet 256 without using classifier-free guidance
- Training convergence speed increased by 6.5x compared to standard diffusion models
- Inference speed doubled by eliminating multiple forward passes required for CFG
- Performance maintained across different model sizes and resolutions

## Why This Works (Mechanism)
Model-guidance works by embedding the conditional information directly into the denoising process through posterior probability estimation. The key insight is that the posterior p(y|x) contains rich information about the relationship between conditions and data that can be leveraged for better denoising. By training the model to predict this posterior alongside the noise, the architecture learns to naturally incorporate conditional information without explicit guidance. During sampling, the model uses its own posterior predictions to weight the denoising direction, effectively replacing the need for separate unconditional conditioning that CFG requires. This creates a self-consistent system where the model's understanding of conditional relationships improves both training and inference simultaneously.

## Foundational Learning

**Diffusion Model Basics**: Understanding the forward and reverse processes in diffusion models, where noise is gradually added to data and then removed through learned denoising. Why needed: Essential for grasping how the model-guidance objective modifies the standard denoising process. Quick check: Can you explain the difference between the forward noising process and reverse denoising process?

**Classifier-free Guidance**: The standard technique of using a conditional model and an unconditional model to guide sampling through linear interpolation. Why needed: Understanding what MG eliminates and why this is significant for efficiency. Quick check: Can you describe how CFG works and its computational cost during inference?

**Posterior Probability Estimation**: The mathematical framework for computing p(y|x), the probability of conditions given observed data. Why needed: Core to understanding how MG uses the model's own predictions to guide sampling. Quick check: Can you derive the posterior probability formula from Bayes' theorem?

## Architecture Onboarding

**Component Map**: Diffusion model backbone (U-Net) -> Noise prediction head + Posterior probability head -> Sampling loop with model-guidance

**Critical Path**: Training data -> Noise injection -> Posterior estimation -> Denoising objective -> Model parameters -> Sampling loop -> Generated image

**Design Tradeoffs**: MG trades additional computation in the training objective (predicting posterior) for significant gains in inference efficiency. The approach requires careful balancing of the noise prediction and posterior prediction tasks to avoid interference between objectives.

**Failure Signatures**: Poor posterior probability estimation leads to unstable guidance during sampling. If the model fails to capture conditional dependencies, generated samples will lack coherence with respect to the conditions. Training instability may occur if the posterior prediction task dominates the noise prediction objective.

**First Experiments**: 1) Verify that the model can accurately predict posterior probabilities on simple synthetic datasets. 2) Test the sampling stability with varying guidance weights using the model's own predictions. 3) Compare FID scores with and without the model-guidance objective on a small-scale dataset.

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Scalability beyond 256x256 resolution remains unverified and may face stability challenges
- Performance on non-natural image domains (text, audio) has not been tested
- Computational overhead of posterior conditioning may vary across different hardware configurations

## Confidence

**Major Claim Confidence Labels:**
- Training acceleration (6.5x): High - supported by direct empirical comparison
- Inference speed doubling: High - logically follows from single-pass generation
- State-of-the-art FID of 1.34: Medium - benchmark-specific and may not generalize
- Scalability across model sizes: Medium - limited to tested parameter ranges
- Elimination of classifier-free guidance without quality loss: High - demonstrated across multiple experiments

## Next Checks
1. Evaluate model-guided sampling at 512x512 and 1024x1024 resolutions to verify scalability claims and identify any resolution-dependent instabilities in the posterior conditioning approach.

2. Test the method across diverse data modalities including text-to-image generation and discrete diffusion tasks to assess whether the model-guidance objective generalizes beyond natural images.

3. Conduct ablation studies isolating the contribution of different posterior probability estimation techniques to determine the sensitivity of MG performance to approximation quality.