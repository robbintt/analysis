---
ver: rpa2
title: Automated Meta Prompt Engineering for Alignment with the Theory of Mind
arxiv_id: '2505.09024'
source_url: https://arxiv.org/abs/2505.09024
tags:
- human
- content
- arxiv
- generative
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel method for aligning generative AI content
  with human expectations by using a Theory of Mind (ToM) approach. The core idea
  involves iteratively optimizing the similarity of neural states between human mental
  expectations and Large Language Model (LLM) outputs through agentic reinforcement
  learning.
---

# Automated Meta Prompt Engineering for Alignment with the Theory of Mind

## Quick Facts
- **arXiv ID**: 2505.09024
- **Source URL**: https://arxiv.org/abs/2505.09024
- **Reference count**: 0
- **Primary result**: Novel method using Theory of Mind reasoning to align LLM-generated content with human expectations through iterative prompt optimization

## Executive Summary
This paper introduces a Theory of Mind (ToM)-based approach for aligning generative AI content with human editorial expectations through automated meta-prompt engineering. The method employs an iterative optimization loop where an LLM-as-a-Judge evaluates generated content across dimensions like factualness, novelty, repetitiveness, and relevance, then an LLM-as-an-Editor refines prompts based on dimensional feedback until convergence. Applied to tennis match report generation during the US Open 2024, the system achieved 53.8% full alignment with human expectations while requiring an average of 4.38 iterations per match. The approach significantly reduces manual editing effort while improving content quality through personalized editor profiles that capture individual preference patterns.

## Method Summary
The system uses a multi-agent architecture where IBM Granite 13B generates factual bullets from 238 tennis statistics per match, which Llama 3 70B transforms into fluent paragraphs. An LLM-as-a-Judge (LLMaaJ) scores outputs on four orthogonal dimensions (0-100 each), mapping these scores into a Hilbert vector space to calculate a geometric loss function combining Theory of Mind Area (tma) and Theory of Mind Distance (tmd). An LLM-as-an-Editor (LLMaaE) receives dimensional feedback and rewrites prompts to reduce misalignment. The loop iterates until loss falls below 0.05 or a 2-minute timeout is reached. Editor modifications update personalized preference profiles used as optimization targets in subsequent generations, creating a reinforcement learning from human feedback (RLHF) loop without model fine-tuning.

## Key Results
- 53.8% of generated match reports achieved 100% alignment between AI output and human expectations
- Average convergence required 4.38 iterations per match report
- Non-converged cases showed up to 15.1% factualness degradation while improving novelty
- Editor preference profiles converged after sufficient feedback accumulation, enabling personalized optimization

## Why This Works (Mechanism)

### Mechanism 1: Iterative Prompt-Space Optimization via Geometric Loss
The system converts qualitative content judgments into geometric relationships in Hilbert space, enabling gradient-like optimization of prompts toward human mental models. LLMaaJ scores four dimensions (factualness, novelty, repetitiveness, relevance) which are mapped into a vector space where loss = |tma| + |tmd|. LLMaaE receives dimensional feedback and rewrites prompts, reducing loss across iterations until convergence (loss < 0.05). This assumes human editorial preferences can be captured as stable, quantifiable dimensional profiles transferable across content instances.

### Mechanism 2: In-Context Learning via Theory-of-Mind Chain-of-Thought
Each iteration includes a ToM Chain-of-Thought explanation describing which dimensions are above/below expectations and why. This context conditions the next generation step, allowing the writer LLM to adjust its output distribution. The LLMaaE generates explicit instructions like "Repetitiveness is 10% above expectations. Please improve by decreasing repetitiveness." This assumes LLMs can interpret and act on abstract dimensional feedback consistently enough to move output toward target profiles.

### Mechanism 3: Personalized Editor Profiles via RLHF Signal Extraction
When editors modify AI-generated content, LLMaaJ re-scores the edited version. The delta between raw and edited scores updates the editor's profile (average target values per dimension). Subsequent generations use this profile as the optimization target. This assumes editor preferences are consistent enough across content pieces that a profile model provides meaningful optimization targets, effectively implementing RLHF where the reward is dimensional alignment rather than scalar preference.

## Foundational Learning

- **Concept: Theory of Mind (ToM) in AI Context**
  - Why needed here: The paper frames alignment as ToM—predicting what a human editor expects. Understanding ToM helps clarify why dimensional profiling approximates "mental state" modeling.
  - Quick check question: Can you explain why ToM is an analogy here rather than a claim that LLMs have human-like mental states?

- **Concept: LLM-as-a-Judge Evaluation Paradigm**
  - Why needed here: The entire feedback loop depends on LLMaaJ producing reliable dimensional scores. Understanding judge limitations (bias, inconsistency) is critical for interpreting results.
  - Quick check question: What failure modes would you expect if the judge LLM systematically over-scores novelty?

- **Concept: Vector Space Optimization Without Gradient Descent**
  - Why needed here: The method uses geometric loss but optimizes via prompt modification (not weight updates). This distinction is essential for understanding convergence behavior.
  - Quick check question: How does optimizing via prompt edits differ fundamentally from gradient descent on model weights?

## Architecture Onboarding

- **Component map**: Kafka event ingestion -> Fact extraction (Granite 13B) -> Writer generation (Llama 3 70B) -> LLMaaJ scoring (Llama 3 70B) -> LLMaaE prompt refinement (Granite 13B Chat V2) -> Storage (Cloudant + COS) -> Content Hub for editor review -> CDN delivery

- **Critical path**: 1) Match ends → Kafka message → Fact extraction → Writer generation 2) LLMaaJ scores raw output → Compare to editor profile → Calculate loss 3) If loss > 0.05: LLMaaE generates refined prompt → Regenerate → Re-judge (loop) 4) If loss < 0.05 or timeout: Output to Content Hub for editor review 5) Editor modifications → LLMaaJ re-scores → Update editor profile

- **Design tradeoffs**: 4D dimensions (adding repetitiveness) showed higher convergence (60.9% vs 58.3% without NLG templates) but more iterations (4.71 vs 3.95). Convergence vs non-convergence tradeoff: non-converged outputs sometimes degraded factualness but improved novelty; editors preferred non-converged over initial output. Personalization vs generalization: per-editor profiles improve alignment but require sufficient edit history; cold-start uses generic ideal scores. Timeout threshold: 2-minute limit balances quality vs latency; shorter thresholds reduce convergence rates.

- **Failure signatures**: Oscillation (dimensional scores swing between iterations without convergence, suggesting judge instability or conflicting targets). Factualness degradation (non-convergence cases showed up to 15.1% factualness loss). Profile drift (editor preferences shift mid-event, causing historical profile to misalign with current intent). Judge-editor mismatch (LLMaaJ scores may not reflect editor's true judgment dimensions).

- **First 3 experiments**: 1) Baseline calibration: Run LLMaaJ on 20 manually edited articles to establish inter-rater agreement between judge scores and editor self-assessments; identify systematic biases. 2) Dimension ablation: Compare convergence rates and iteration counts for 3D vs 4D on held-out matches; measure quality tradeoffs. 3) Profile stability test: For a single editor, track profile evolution across 30+ edits; measure how quickly scores stabilize and whether early profile is representative.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does implementing a jury of LLM judges following the Condorcet Jury Theorem improve alignment accuracy over a single LLM-as-a-Judge?
- Basis in paper: Section 7 proposes to "extend our work to follow the Condorcet Jury Theorem, which will add LLM jury members to the judge chamber."
- Why unresolved: The current system relies on a single LLM judge (Llama 3 70B) to interpret dimensions; it is unknown if a majority-rule approach would yield better optimization or mitigate individual model biases.
- What evidence would resolve it: Comparative experiments measuring convergence rates and human satisfaction scores between the current single-judge architecture and a multi-judge ensemble.

### Open Question 2
- Question: Can the framework successfully allow editors to define and optimize for custom dimensions of measurement?
- Basis in paper: Section 7 states a desire to "enable an editor to define their own dimensions, definitions, and measurement criteria to align towards their priorities."
- Why unresolved: The study fixed the evaluation dimensions to factualness, novelty, repetitiveness, and topic alignment; the system's ability to dynamically handle user-generated metrics remains unproven.
- What evidence would resolve it: A user study where editors input custom dimensions (e.g., "tone" or "complexity") followed by an analysis of the system's convergence success on these new axes.

### Open Question 3
- Question: Is the geometric optimization method robust when scaled to a larger and more diverse population of human editors?
- Basis in paper: Section 5.1 identifies that the human feedback and "mental belief" measurements were derived from a very small sample of "4 content adjudicators."
- Why unresolved: With only four editors, the reported 53.8% convergence rate may reflect overfitting to specific individual preferences rather than a generalizable Theory of Mind model.
- What evidence would resolve it: Deployment results from a larger cohort (e.g., n > 30) showing consistent convergence rates and low variance in the "tma" (ToM area) loss function across different users.

## Limitations
- The method's effectiveness depends on LLMaaJ's reliability in scoring content across orthogonal dimensions, with non-converged cases showing up to 15.1% factualness degradation while improving novelty.
- The geometric loss function combining spatial volume (tma) and vertices alignment (tmd) is innovative but mathematically under-specified, making exact reproduction difficult.
- Claims about generalizability beyond tennis match reports are unsupported, as the approach requires domain-specific fact extraction and substantial computational resources (multiple 70B models running in parallel).

## Confidence
- **High Confidence**: The core workflow architecture (fact extraction → writer → judge → editor loop) is clearly specified and technically sound. The 53.8% full alignment rate and 4.38 average iteration count are concrete, measurable outcomes.
- **Medium Confidence**: The theoretical framework linking Theory of Mind to dimensional alignment is conceptually coherent, but the actual implementation of "mental state prediction" through polygon geometry remains more metaphorical than rigorously proven.
- **Low Confidence**: The approach's scalability and generalizability to different content domains remain unproven, as it requires substantial computational resources and domain-specific adaptations.

## Next Checks
1. **Judge Reliability Test**: Run inter-rater reliability analysis comparing LLMaaJ scores against human editors across 100+ content samples to quantify judge bias and consistency before/after the optimization loop.
2. **Profile Stability Analysis**: Track how editor preference profiles evolve over time with 50+ edits per editor; measure convergence of profile scores and identify conditions under which profiles become unstable or drift.
3. **Domain Transfer Experiment**: Apply the exact same pipeline to a different content domain (e.g., financial news summaries or product reviews) using domain-appropriate fact extraction; compare convergence rates and quality metrics to tennis baseline.