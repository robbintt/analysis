---
ver: rpa2
title: 'Bridging Language Barriers in Healthcare: A Study on Arabic LLMs'
arxiv_id: '2501.09825'
source_url: https://arxiv.org/abs/2501.09825
tags:
- medical
- arabic
- language
- english
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the challenge of developing Arabic-language
  medical large language models (LLMs) with performance comparable to English. The
  authors evaluated existing Arabic medical datasets (PubMedQA, MedMCQA, MedQA, MMLU)
  using models like Llama3.1 and Qwen2.5, finding significant performance gaps (e.g.,
  Llama3.1-70B: 78.2% English vs 56.6% Arabic on MedQA).'
---

# Bridging Language Barriers in Healthcare: A Study on Arabic LLMs

## Quick Facts
- arXiv ID: 2501.09825
- Source URL: https://arxiv.org/abs/2501.09825
- Reference count: 4
- Primary result: Arabic-only fine-tuning generally outperforms mixed-language approaches for Arabic medical tasks

## Executive Summary
This study investigates the performance gap between English and Arabic medical large language models (LLMs) and explores strategies to bridge this divide. The authors evaluate existing Arabic medical datasets and translation pipelines, finding that general-purpose LLMs outperform specialized translation models on medical content but still lag behind native English performance. Through systematic fine-tuning experiments with varying Arabic-English data ratios, they discover that optimal language mixtures depend heavily on the specific medical task, with Arabic-only training yielding the best results for Arabic language tasks across different model sizes.

## Method Summary
The authors evaluated existing Arabic medical datasets (PubMedQA, MedMCQA, MedQA, MMLU) using models like Llama3.1 and Qwen2.5, finding significant performance gaps between English and Arabic. They tested translation pipelines using models like Helsinki, Flores-101, LlamaX, and Qwen2.5, discovering that general-purpose LLMs outperformed specialized translation models on medical content. The study then fine-tuned Llama3.1 models with varying ratios of Arabic-English medical data, discovering that optimal language mixtures varied by task. They built fine-tuning datasets including AHQAD (100K Arabic Q&A), translated Med42, CIDAR, and synthetic QA pairs totaling ~480M Arabic + ~470M English tokens.

## Key Results
- Arabic-only training consistently achieved best Arabic performance across all tasks
- Larger models (70B) showed more consistent behavior with Arabic-only training than smaller models
- Optimal language mix varies significantly by task type (Arabic-heavy for PubMedQA, English-heavy for MMLU)
- Fine-tuning instruct models rarely outperformed original instruct baselines, suggesting capacity saturation

## Why This Works (Mechanism)

### Mechanism 1: Task-Dependent Language Mix Optimization
- Claim: Optimal ratio of target-language to source-language training data varies by clinical task type
- Mechanism: Tasks requiring deep contextual reasoning benefit from higher target-language ratios, while structured knowledge tasks perform better with source-language data
- Evidence: PubMedQA achieves 71.2% with Arabic-only data vs. MMLU's 42.4% with English-only data in 8B base model
- Break condition: If tasks exhibit strong cross-lingual transfer regardless of training language mix

### Mechanism 2: Model Scale Enables More Consistent Language-Specific Performance
- Claim: Larger models exhibit more consistent behavior across language-specific tasks with target-language-only training
- Mechanism: Greater parameter capacity allows models to abstract and generalize linguistic features more uniformly
- Evidence: 70B parameter models show more consistent behavior across all Arabic tasks with Arabic-only training data
- Break condition: If smaller models with optimal language ratios match larger models' target-language performance

### Mechanism 3: Fine-Tuning Insufficiency for Language Acquisition
- Claim: Fine-tuning alone on target-language medical data may not achieve optimal performance if the model lacks foundational target-language representations
- Mechanism: Fine-tuning adjusts output distributions but cannot fundamentally alter learned language representations established during pretraining
- Evidence: Fine-tuned version of Llama3.1-70B-Instruct rarely outperforms the original model
- Break condition: If fine-tuning with sufficient target-language data consistently improves performance over baselines

## Foundational Learning

- **Cross-Lingual Transfer in Transformers**
  - Why needed here: Understanding why English-dominant pretraining creates representations that don't transfer cleanly to morphologically distinct languages like Arabic
  - Quick check question: Can you explain why a model trained predominantly on English might struggle with Arabic's root-based morphology and RTL script?

- **Supervised Fine-Tuning vs. Continued Pretraining**
  - Why needed here: The paper's central claim is that fine-tuning may be insufficient; understanding the distinction between adjusting output heads versus learning new token representations is critical
  - Quick check question: What representation-level changes does continued pretraining enable that supervised fine-tuning does not?

- **Medical Domain Evaluation Benchmarks**
  - Why needed here: The study evaluates on PubMedQA, MedMCQA, MedQA, and Medical MMLU—each testing different capabilities (reasoning vs. knowledge recall)
  - Quick check question: Why might PubMedQA (research article QA) require different evaluation handling than MedQA (licensing exam questions)?

## Architecture Onboarding

- **Component map**: Base models (Llama 3.1 8B/70B, Qwen 2.5 3B-72B) -> Translation layer (Qwen2.5-72B-Instruct) -> Evaluation framework (modified Harness with RTL support) -> Fine-tuning pipeline (varying Arabic-English ratios)

- **Critical path**: 1. Baseline evaluation on target-language medical benchmarks -> 2. Test translation pipeline (input → translate → process → back-translate) -> 3. Fine-tune with varying language ratios -> 4. If fine-tuning plateaus, evaluate need for continued pretraining

- **Design tradeoffs**: Translation pipeline improves accuracy (+15-25%) but triples inference calls and introduces error propagation; Arabic-only vs. mixed training optimizes target-language performance vs. preserves English capabilities; 70B models more consistent with less data mixing; 8B models require careful ratio tuning per task

- **Failure signatures**: Base models failing chat-template tasks indicate instruction-following deficit; fine-tuned instruct models underperforming baselines suggest capacity saturation; large performance variance across similar tasks indicates overfitting to specific data distributions

- **First 3 experiments**: 1. Establish baseline: Evaluate existing multilingual models on both English and Arabic medical benchmarks to quantify the language gap; 2. Test translation overhead: Implement Arabic→English→Arabic pipeline using Qwen-72B, measure accuracy gain vs. latency cost; 3. Small-scale ratio sweep: Fine-tune 8B model with 3-5 language ratio configurations on target task to identify optimal mix

## Open Questions the Paper Calls Out

1. **Is continued pretraining on native-language medical corpora necessary to achieve parity with English performance, or can fine-tuning strategies be optimized to close this gap?**
   - Basis: Authors conclude "relying solely on fine-tuning may not be the most effective approach" and suggest "data and computationally intensive pretraining methods may still be necessary"
   - Why unresolved: Study tested various fine-tuning mixtures but found consistent degradation compared to English baselines; efficacy of continued pretraining vs. fine-tuning was not directly compared
   - What evidence would resolve it: Comparative study evaluating model trained with continued pretraining on Arabic medical data against fine-tuned models presented in paper

2. **Does the optimal ratio of target-language to English data in fine-tuning depend on the specific cognitive demand of the task (e.g., reasoning vs. knowledge retrieval)?**
   - Basis: Paper notes "optimal language mix in training data varies significantly across different medical tasks," finding Arabic-only best for PubMedQA (reasoning) while English-only worked best for MMLU (knowledge) in 8B models
   - Why unresolved: Paper observes variance but does not isolate specific task features that drive need for specific language ratios
   - What evidence would resolve it: Ablation study varying data ratios across spectrum of tasks categorized by reasoning depth versus factual density

3. **To what extent do high accuracy scores on translated multiple-choice benchmarks correlate with safety, reduced hallucination, and factual grounding in open-ended clinical generation?**
   - Basis: Authors acknowledge work "primarily focuses on close-ended question benchmarks" which "do not fully capture the generation capabilities, safety, and bias aspects"
   - Why unresolved: Models may achieve high accuracy by exploiting artifacts in translated benchmarks while remaining unsafe or prone to hallucination in real-world generative scenarios
   - What evidence would resolve it: Evaluating fine-tuned models using new benchmarks like MEDIC specifically adapted for multilingual open-ended generation and safety probing

## Limitations
- Translation quality for medical terminology remains unverified beyond subjective assessment
- Task-specific generalization findings may reflect dataset-specific artifacts rather than genuine task characteristics
- Model scale claims based on only two model sizes (8B and 70B) without broader validation

## Confidence
- **High Confidence**: Arabic-only training generally produces best Arabic performance across tasks; performance gaps between English and Arabic benchmarks are clearly documented
- **Medium Confidence**: Translation pipelines can partially bridge language gap; larger models show more consistent behavior with target-language training
- **Low Confidence**: Fine-tuning alone is insufficient for language acquisition and extensive pretraining remains necessary for new languages

## Next Checks
1. Implement blind medical expert review of translated medical QA pairs to quantify translation accuracy and identify systematic errors across different medical subdomains
2. Conduct controlled experiments comparing continued pretraining on Arabic medical corpora versus direct fine-tuning on same medical data
3. Break down each medical benchmark into sub-tasks (factual recall vs reasoning vs clinical inference) and evaluate language ratio effects separately to determine whether task-specific optimal ratios reflect genuine cognitive demands or dataset artifacts