---
ver: rpa2
title: How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes
  for Cardiovascular Risk Prediction?
arxiv_id: '2508.07127'
source_url: https://arxiv.org/abs/2508.07127
tags:
- tier
- data
- risk
- llms
- cardiac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using large language models (LLMs) to integrate
  single nucleotide polymorphism (SNP) and electrocardiogram (ECG) data for cardiovascular
  disease (CVD) prediction. It constructs a three-tier dataset based on certainty
  of cardiac diagnoses, applies Chain-of-Thought prompting with genetic and ECG features,
  and fine-tunes models (GPT-2, Llama 3.2 1B, DeepSeek 1.3B) using parameter-efficient
  LoRA.
---

# How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?

## Quick Facts
- arXiv ID: 2508.07127
- Source URL: https://arxiv.org/abs/2508.07127
- Reference count: 26
- Key outcome: DeepSeek 1.3B achieved 0.910 accuracy and 0.840 F1 score overall for multimodal CVD prediction

## Executive Summary
This paper demonstrates that large language models (LLMs) can effectively integrate single nucleotide polymorphism (SNP) and electrocardiogram (ECG) data for cardiovascular disease (CVD) prediction. The study employs a three-tier dataset stratified by diagnosis certainty, applies Chain-of-Thought prompting to encourage evidence-based reasoning, and fine-tunes models using parameter-efficient LoRA. DeepSeek 1.3B outperformed other models, showing strong generalization across labeled and unlabeled tiers. The work establishes LLMs as a viable tool for interpretable multimodal cardiogenomic modeling while identifying key limitations and future directions.

## Method Summary
The method constructs a three-tier dataset from 1,050 curated participants, stratifying by diagnosis certainty: confirmed diagnoses (Tier 1), indirect indicators (Tier 2), and unlabeled data (Tier 3). SNP variants are filtered using GWAS catalog thresholds (p ≤ 5×10⁻⁸) and TF-IDF weighting. ECG features and curated SNPs are flattened into natural language prompts formatted as Chain-of-Thought reasoning tasks. The models (GPT-2, Llama 3.2 1B, DeepSeek 1.3B) are fine-tuned using LoRA with rank=8 and alpha=16 on attention and MLP layers. Evaluation uses BioBERT semantic similarity (0.7 threshold) rather than exact string matching to account for clinical synonyms.

## Key Results
- DeepSeek 1.3B achieved the best overall performance: 0.910 accuracy and 0.840 F1 score
- Model showed strong generalization across all three tiers, including unlabeled Tier 3
- Semantic evaluation via BioBERT (threshold 0.7) enabled lenient matching of clinically equivalent outputs
- LoRA parameter-efficient fine-tuning enabled effective adaptation of 1B-3B parameter models on limited data

## Why This Works (Mechanism)

### Mechanism 1: Structured Context Injection via Chain-of-Thought (CoT)
If heterogeneous multimodal data (SNPs, ECG metrics) is serialized into natural language "vignettes," decoder-only LLMs can reportedly perform classification by leveraging semantic context rather than requiring specialized architectural fusion layers. The system flattens structured ECG features and filtered SNP lists into a text prompt. By prepending an instructional query and requiring a diagnosis followed by an explanation, the model conditions its generation on the explicit intersection of genetic predisposition and phenotypic expression. Core assumption: The LLM's pre-trained semantic knowledge generalizes to biological token relationships (e.g., "rsID" + "QT interval") without requiring domain-specific pre-training from scratch.

### Mechanism 2: Noise Reduction via Tiered Feature Engineering
If the dataset is stratified by label certainty (Tier 1–3) and features are curated using external databases (GWAS), the signal-to-noise ratio improves compared to naive ingestion of raw genomic data. Instead of feeding all patient SNPs, the pipeline filters variants using GWAS p-value thresholds (p ≤ 5×10⁻⁸) for Tier 1 and TF-IDF weighting for Tier 2. This acts as a "semantic prior," restricting the model's attention to variants with established or statistical disease associations. Core assumption: GWAS catalog associations represent the primary causal mechanism for the CVD phenotypes in this cohort, and rare variants not in these catalogs are less predictive.

### Mechanism 3: Semantic Evaluation for Generative Classification
If LLMs are evaluated via semantic similarity (BioBERT) rather than exact string matching, the measured utility of the model increases by accounting for valid clinical synonyms in generated outputs. The model generates a text diagnosis (e.g., "atrial fibrillation"). Instead of checking for an exact label match, a secondary encoder (BioBERT) computes cosine similarity against ground truth, accepting matches above a 0.7 threshold. Core assumption: The semantic space of the evaluation model aligns with clinical validity; a high similarity score correlates with a correct diagnostic inference.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: The study fine-tunes 1B–3B parameter models on a limited dataset (1,050 samples). Standard full fine-tuning would likely overfit or require prohibitive memory. LoRA allows efficient adaptation by freezing base weights and injecting trainable rank-decomposition matrices.
  - Quick check question: If the paper specifies `rank=8` and `alpha=16`, what is the effective scaling factor applied to the low-rank updates during the forward pass?

- **Concept: TF-IDF (Term Frequency-Inverse Document Frequency)**
  - Why needed here: Used for Tier 2/3 processing to convert sparse SNP profiles into weighted vectors. Understanding this helps explain how the model prioritizes "rare" variants (high IDF) over common genomic background noise.
  - Quick check question: In this context, if a specific SNP appears in 90% of the population (high document frequency), would its TF-IDF weight increase or decrease compared to a SNP found in only 2% of the population?

- **Concept: GWAS (Genome-Wide Association Study) Catalogs**
  - Why needed here: The paper relies on GWAS to filter SNPs for Tier 1. You must understand that GWAS identifies statistical correlations between loci and traits, providing the "ground truth" priors used to construct the prompts.
  - Quick check question: The paper filters SNPs with p ≤ 5×10⁻⁸. Does this threshold ensure biological causality or just statistical significance?

## Architecture Onboarding

- **Component map:** JSON (SNPs/ECG) + CSV (Labels) -> QC Filters -> Tier Stratification -> Prompt Constructor -> LLM (DeepSeek 1.3B) + LoRA -> BioBERT Semantic Similarity Evaluator
- **Critical path:** The **Prompt Constructor** is the most brittle component. It converts structured JSON data into the specific natural language format the model expects. A failure here (e.g., missing an ECG metric or misformatting an rsID) directly breaks the "Chain of Thought" reasoning.
- **Design tradeoffs:**
  - Interpretability vs. Accuracy: The paper trades off the potential raw accuracy of specialized neural networks (e.g., 1D CNNs for ECG) for the interpretability and semantic flexibility of LLMs.
  - Precision vs. Recall: DeepSeek 1.3B showed high precision but lower recall in Tier 1; the semantic evaluation threshold (0.7) is a tuning knob for this tradeoff.
- **Failure signatures:**
  - Hallucination: The model might invent cardiac conditions not present in the prompt's context or supported by the data.
  - Prompt Sensitivity: Changing the instruction phrasing (e.g., "Diagnose" vs "Assess risk") significantly alters output distribution.
  - Tier 3 Drift: Performance on Tier 3 relies on clustering; if clusters are impure, "future-risk" pseudo-labels will misguide the LLM.
- **First 3 experiments:**
  1. Baseline Validation: Reproduce the exact prompt template with GPT-2 on the Tier 1 subset to verify the 0.810 accuracy benchmark using the provided BioBERT evaluation script.
  2. Ablation on Modalities: Run DeepSeek 1.3B with SNPs-only prompts vs. ECG-only prompts to quantify the marginal contribution of each modality to the 0.910 accuracy score.
  3. Semantic Threshold Sensitivity: Vary the cosine similarity threshold (0.5 vs 0.7 vs 0.9) to determine how much of the reported "performance" is driven by the leniency of the semantic evaluation metric.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does scaling model architecture to larger parameter sizes (e.g., 7B or 70B) significantly improve the extraction of latent biological relationships in unlabeled or sparsely labeled data (Tier 3)?
- Basis in paper: The authors state that "scaling to larger architectures such as Llama-3 7B or GPT-4 could substantially enhance performance, particularly in Tier 3 scenarios or zero-shot classification tasks."
- Why unresolved: The current study only tested smaller models (GPT-2, Llama 3.2 1B, DeepSeek 1.3B) due to resource constraints; it is unknown if larger models would show diminishing returns or emergent reasoning capabilities for genotype-phenotype linking.
- What evidence would resolve it: Benchmarking the proposed pipeline using 7B+ parameter models on the same Tier 3 dataset to measure relative gains in F1 score and semantic coherence.

### Open Question 2
- Question: Can the integration of Knowledge Graphs (KGs) outperform the current unsupervised clustering approach in capturing complex gene-disease relationships?
- Basis in paper: The authors note that "utilizing techniques such as knowledge graphs will be a significant focus in the future work to capture significant relationships that clusters may not be able to capture."
- Why unresolved: The current method relies on TF-IDF and clustering for Tier 3 inference, which may miss explicit hierarchical or semantic connections between SNPs and cardiac phenotypes that a structured KG would retain.
- What evidence would resolve it: A comparative study where Tier 3 inference is guided by a KG versus the current clustering method, evaluated by the accuracy of "future-risk" pseudo-labels against longitudinal patient outcomes.

### Open Question 3
- Question: How robust is the semantic similarity evaluation metric against clinical hallucinations compared to expert clinical review?
- Basis in paper: The paper acknowledges that "causal language models... outputs are often sensitive to prompt structure" and admits that standard metrics like BLEU were not designed for this task. They utilized a BioBERT cosine similarity threshold of 0.7 to judge correctness.
- Why unresolved: Semantic similarity does not guarantee factual medical accuracy; a model could generate a plausible-sounding but biologically incorrect explanation that scores highly on semantic similarity.
- What evidence would resolve it: A human-in-the-loop evaluation where clinical experts grade the "reasoning" quality of the Chain-of-Thought outputs alongside the automated semantic scores to check for alignment.

### Open Question 4
- Question: To what extent does the model's performance generalize to external, ethnically diverse cohorts beyond the single geographic population used in this study?
- Basis in paper: The paper notes the data is "curated from the PhenoAI HPP repository" consisting of "individuals residing in the Asia/Jerusalem time zone" and highlights the "relatively small cohort of 1,050 participants" as a limitation.
- Why unresolved: SNP frequencies and ECG morphologies can vary significantly across ethnicities; a model trained on a geographically specific, small cohort may overfit to population-specific noise rather than universal biological signals.
- What evidence would resolve it: Zero-shot or few-shot testing of the fine-tuned DeepSeek model on an external genomic-ECG dataset (e.g., UK Biobank or MIMIC) with different demographic distributions.

## Limitations
- Data accessibility: The PhenoAI HPP repository is not publicly available, preventing independent validation of the exact dataset composition and methodology.
- Evaluation methodology: The semantic similarity threshold of 0.7 is not justified by sensitivity analysis and may inflate performance metrics.
- Generalizability: Results may not extend to broader cardiovascular phenotypes or different genetic backgrounds due to limited sample size and geographic specificity.
- Prompt sensitivity: Performance is sensitive to prompt phrasing without systematic testing of alternative formulations.

## Confidence
- **High confidence:** The technical feasibility of using LLMs for multimodal cardiogenomic modeling is demonstrated through controlled experiments and reproducible code.
- **Medium confidence:** The reported performance metrics (0.910 accuracy, 0.840 F1) are specific to the described methodology and dataset but require independent verification due to data accessibility limitations.
- **Low confidence:** The semantic evaluation approach's validity for clinical decision-making and the model's performance on rare variants or population-specific genetic architectures remain unproven.

## Next Checks
1. **Data access verification:** Obtain the PhenoAI HPP dataset or equivalent publicly available cardiogenomic dataset to reproduce the exact QC filters, tier stratification, and prompt construction methodology.
2. **Semantic threshold analysis:** Systematically vary the BioBERT cosine similarity threshold (0.5, 0.7, 0.9) and document the impact on precision, recall, and F1 scores across all three tiers to validate the choice of 0.7.
3. **Prompt sensitivity testing:** Design and execute a controlled experiment varying the Chain-of-Thought prompt phrasing (e.g., "Diagnose" vs "Assess risk" vs "Identify condition") while keeping all other variables constant to quantify performance sensitivity to instruction wording.