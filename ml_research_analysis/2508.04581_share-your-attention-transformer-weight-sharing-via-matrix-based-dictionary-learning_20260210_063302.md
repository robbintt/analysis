---
ver: rpa2
title: 'Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary
  Learning'
arxiv_id: '2508.04581'
source_url: https://arxiv.org/abs/2508.04581
tags:
- masa
- attention
- matrix
- dictionary
- sharing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MASA, a matrix-based dictionary learning approach
  to reduce parameter redundancy in transformer attention layers. MASA decomposes
  query, key, value, and output projection matrices into shared atoms, enabling each
  layer to reconstruct its weights via linear combinations.
---

# Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning

## Quick Facts
- arXiv ID: 2508.04581
- Source URL: https://arxiv.org/abs/2508.04581
- Authors: Magauiya Zhussip; Dmitriy Shopkhoev; Ammar Ali; Stamatios Lefkimmiatis
- Reference count: 24
- One-line result: Achieves 66.7% reduction in attention parameters while maintaining performance through matrix-based dictionary learning

## Executive Summary
MASA introduces a matrix-based dictionary learning approach to reduce parameter redundancy in transformer attention layers. The method decomposes query, key, value, and output projection matrices into shared atoms, enabling each layer to reconstruct its weights via linear combinations. This achieves 66.7% reduction in attention parameters while maintaining performance. Experiments on language and vision tasks show MASA outperforms existing compression methods, including GQA, low-rank baselines, and sequential/Repeat-all-over sharing, with 34.43% average accuracy and 112.23 WikiText perplexity on a 110M-parameter model. The method is compatible with pretrained models and extends to vision transformers with negligible accuracy loss.

## Method Summary
MASA frames transformer attention compression as dictionary learning: each layer's weight matrix is expressed as a linear combination of shared "matrix atoms." The method maintains separate dictionaries for Q, K, V, and O projections to preserve functional specialization while enabling compression. During training, dictionaries and layer-specific coefficients are learned jointly via backpropagation. For pretrained models, a training-free adaptation uses Matrix PCA and functional grouping via KL divergence to analytically compute shared atoms. The approach achieves 66.7% parameter reduction with S = L/3 atoms per dictionary, where L is the number of layers.

## Key Results
- 66.7% reduction in attention parameters while maintaining performance
- Outperforms GQA, low-rank baselines, and sequential/Repeat-all-over sharing methods
- Achieves 34.43% average accuracy and 112.23 WikiText perplexity on 110M-parameter model
- Extends to vision transformers with negligible accuracy loss
- Compatible with pretrained models via training-free Matrix PCA adaptation

## Why This Works (Mechanism)

### Mechanism 1: Dictionary Learning Decomposition for Cross-Layer Weight Sharing
Attention projection matrices across transformer layers can be accurately reconstructed as linear combinations of a small set of shared "matrix atoms." Each layer's weight matrix W_l is expressed as Ŵ_l = Σ(cl_s × D_s), where D_s are shared dictionary atoms and cl_s are layer-specific scalar coefficients. The model jointly learns D and C via backpropagation on the training loss, rather than enforcing rigid weight tying. Core assumption: Transformer attention weights across layers exhibit statistical regularities that can be captured by a low-dimensional shared basis.

### Mechanism 2: Parameter Reduction Through Projection-Specific Dictionaries
Maintaining separate dictionary pools for Q, K, V, and O projections preserves functional specialization while enabling compression. Independent dictionaries D^Q, D^K, D^V, D^O are learned, each with S atoms. With S = L/3, attention parameters reduce by ~66.7% (compression rate r ≈ 1 - S/L). Core assumption: Q, K, V, O projections serve distinct roles and benefit from specialized subspaces; forcing a common dictionary degrades performance.

### Mechanism 3: Training-Free Adaptation via Matrix PCA and Functional Grouping
Pretrained LLMs can be compressed without fine-tuning by analytically computing shared atoms via Matrix PCA and grouping layers by functional similarity. Compute KL divergence between consecutive layers' output distributions via a semantic probe; group layers with minimal distributional drift; for each group, recover S principal matrix atoms via eigenvectors of W×W^T; apply data-aware local refinement on residuals using Cholesky-whitened low-rank approximation. Core assumption: Layers inducing minimal semantic change are functionally redundant and can share parameters.

## Foundational Learning

- **Dictionary Learning / Sparse Coding**
  - Why needed here: MASA frames attention compression as reconstructing weights from a learned dictionary of matrix atoms; understanding overcomplete representations and coefficient sparsity clarifies why S << L suffices.
  - Quick check question: Can you explain how dictionary learning differs from PCA in its constraints on the basis and coefficients?

- **Matrix Decomposition (SVD, PCA, Low-Rank Approximation)**
  - Why needed here: The training-free path relies on Matrix PCA (SVD of stacked weight matrices) and Eckart-Young-Mirsky theorem for optimal low-rank residuals.
  - Quick check question: Given a matrix W, what does truncating its SVD to rank-r minimize, and under what norm?

- **Transformer Attention Mechanics (Multi-Head, GQA, MQA)**
  - Why needed here: MASA compresses Q, K, V, O projections differently; understanding GQA's grouped key-value heads clarifies why W_k, W_v have reduced dimensionality and rank constraints in modern LLMs.
  - Quick check question: In GQA, why do K and V projections have fewer parameters than Q, and how does this affect MASA's adaptive rank allocation?

## Architecture Onboarding

- **Component map**: Block embeddings -> 3-layer MLP -> Coefficient matrices C^Q, C^K, C^V, C^O -> Dictionary atoms D^Q, D^K, D^V, D^O -> Reconstructed weights Ŵ_l for each layer

- **Critical path**: 1) Choose compression regime (MASA-QKV or MASA-QKVO) 2) Set dictionary size S (typical: S = L/3) 3) Training-from-scratch: Initialize D, C randomly; optimize jointly via AdamW 4) Pretrained models: Run grouping → Matrix PCA per group → local residual refinement

- **Design tradeoffs**: S vs. performance (larger S improves perplexity but reduces compression); QKV-only vs. QKVO (O projection more sensitive); group count (3 groups minimize perplexity, 6 groups maximize accuracy); common vs. separate dictionaries (separate consistently outperforms shared)

- **Failure signatures**: Perplexity spikes → likely O projection over-compressed or S too small; accuracy collapse on reasoning benchmarks → grouping too aggressive or local refinement budget exhausted; training instability → use embedding-based coefficient parameterization

- **First 3 experiments**: 1) Train Transformer-S (110M) with MASA-QKV (S=4) on RefinedWeb; verify perplexity < vanilla and accuracy ≥ baseline 2) Sweep S ∈ {2, 4, 6, 8} for MASA-QKVO on Transformer-M; plot perplexity vs. compression rate 3) Apply Matrix PCA + grouping (4-6 groups) to Llama 3.2 1B at 20% compression; compare WikiText perplexity and avg accuracy against SVD-LLM

## Open Questions the Paper Calls Out

- **Open Question 1**: Can more sophisticated clustering algorithms (e.g., hierarchical or spectral clustering) improve the quality of layer grouping in the training-free MASA adaptation compared to the proposed greedy KL-divergence approach?
  - Basis: The current greedy, sequential grouping is not globally optimal; more sophisticated clustering methods are left for future work.
  - Resolution: Empirical results showing reconstruction error and downstream performance when applying non-greedy clustering algorithms.

- **Open Question 2**: Can the MASA framework be effectively applied to the Feed-Forward Network (FFN) layers to achieve end-to-end model compression without significant performance degradation?
  - Basis: The method is currently applied only to attention projections despite the theoretical formulation being generalizable to any weight matrix.
  - Resolution: Experiments applying MASA to FFN weights of standard benchmarks and comparing accuracy/perplexity retention against attention-only compression.

- **Open Question 3**: Does the training-free MASA adaptation introduce computational overhead that offsets the benefits of parameter reduction during inference?
  - Basis: The local refinement step requires computing weight approximations using matrix operations that may not be optimized for standard inference kernels.
  - Resolution: Latency and throughput benchmarks comparing training-free MASA models against dense baselines on standard hardware.

## Limitations

- Pretrained model compression shows mixed outcomes with modest accuracy gains on reasoning tasks despite strong perplexity improvements
- Scaling behavior beyond 1.4B parameters remains unexplored, particularly for models with more heterogeneous layer behaviors
- Vision transformer applicability is limited to basic ViT variants without thorough validation on architectural variants like Swin or ConvNeXt

## Confidence

- **High Confidence**: Core mechanism of dictionary-based weight sharing and parameter reduction math are well-established through controlled experiments
- **Medium Confidence**: Pretrained model compression via Matrix PCA shows promising but inconsistent results, with accuracy gains on reasoning tasks being modest
- **Low Confidence**: Scaling behavior beyond 1.4B parameters and generalization to diverse vision transformer architectures are not thoroughly validated

## Next Checks

1. **Pretrained model robustness test**: Apply MASA to Llama 2 7B and Llama 3 8B with varying group counts (3, 6, 9) and dictionary sizes (S = L/4, L/3, L/2). Measure both WikiText perplexity and MMLU accuracy to quantify the tradeoff between compression and knowledge preservation across scales.

2. **Cross-architecture generalization**: Compress Swin Transformer variants (Swin-T, Swin-S) on ImageNet-1K using MASA-QKVO with S = L/3. Compare Top-1 accuracy against vanilla Swin and GQA-compressed baselines to validate vision task applicability.

3. **Dynamic dictionary adaptation**: Implement an adaptive S per projection type (e.g., S_Q = L/4, S_K = L/3, S_V = L/3, S_O = L/2) based on sensitivity analysis from ablation studies. Train on RefinedWeb and measure whether projection-specific dictionary sizing improves the perplexity vs. compression tradeoff curve.