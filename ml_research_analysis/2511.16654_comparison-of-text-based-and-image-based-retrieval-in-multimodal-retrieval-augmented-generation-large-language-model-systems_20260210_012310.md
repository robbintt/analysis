---
ver: rpa2
title: Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval
  Augmented Generation Large Language Model Systems
arxiv_id: '2511.16654'
source_url: https://arxiv.org/abs/2511.16654
tags:
- retrieval
- multimodal
- text
- visual
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares two multimodal retrieval approaches for RAG
  systems: text-based chunk retrieval (where images are summarized into text before
  embedding) and direct multimodal embedding retrieval (where images are stored natively
  in vector space). The study evaluates these approaches across 6 LLM models and 2
  embedding models on a newly created financial earnings call benchmark with 40 question-answer
  pairs.'
---

# Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems

## Quick Facts
- arXiv ID: 2511.16654
- Source URL: https://arxiv.org/abs/2511.16654
- Authors: Elias Lumer; Alex Cardenas; Matt Melich; Myles Mason; Sara Dieter; Vamse Kumar Subbiah; Pradeep Honaganahalli Basavaraju; Roberto Hernandez
- Reference count: 2
- Primary result: Direct multimodal embedding retrieval achieves 13% absolute improvement in mAP@5 and 11% in nDCG@5 over text-based approaches

## Executive Summary
This paper compares two multimodal retrieval approaches for RAG systems: text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in vector space). The study evaluates these approaches across 6 LLM models and 2 embedding models on a newly created financial earnings call benchmark with 40 question-answer pairs. Direct multimodal embedding retrieval significantly outperforms text-based approaches, achieving 13% absolute improvement in mean average precision (mAP@5) and 11% improvement in normalized discounted cumulative gain (nDCG@5). These correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5. The multimodal approach also produces more accurate and factually consistent answers, particularly for larger models with stronger multimodal reasoning capabilities. The results demonstrate that preserving visual information in its native form rather than converting to text summaries enables more accurate retrieval and generation in multimodal RAG systems.

## Method Summary
The study compares two multimodal retrieval approaches using a financial earnings call benchmark with 40 question-answer pairs. Approach 1 (text-based) summarizes images into text using LLMs before embedding with ada-002, while Approach 2 (multimodal) embeds images and text directly using Jina v4. Six LLM models (GPT-4o, GPT-4o-mini, GPT-5, GPT-5-mini, Gemini 1.5, Gemini 1.5-Flash) and two embedding models are evaluated. Retrieval quality is measured using mAP@5 and nDCG@5, while generation quality is assessed through pairwise LLM comparisons focusing on accuracy, factual consistency, and numerical fidelity.

## Key Results
- Direct multimodal embedding retrieval achieves mAP@5 of 0.5234 versus 0.3963 for text-based approaches
- nDCG@5 scores show 11% absolute improvement for multimodal retrieval
- Multimodal approach produces answers with 90% "No Unsupported Additions" versus 10% for text-based approaches
- Full models (GPT-5) show 82% win rate for multimodal, while mini-models show near-random performance (50/50 win rate)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct multimodal embeddings preserve semantic information that is lost during text-based summarization.
- **Mechanism:** Text-based approaches convert images to text descriptions using VLMs (e.g., GPT-5) before embedding. This projection from high-dimensional visual data to discrete text creates an information bottleneck, stripping spatial relationships and numerical precision. Direct multimodal embeddings (e.g., Jina v4) map image pixels and text directly into a shared vector space, retaining visual fidelity.
- **Core assumption:** The loss of retrieval performance in text-based systems is primarily caused by the "information bottleneck" of the intermediate summarization step, rather than the capacity of the embedding model.
- **Evidence anchors:**
  - [abstract] "LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context."
  - [section 4.4] "...preserving visual information in its native form enables more accurate semantic matching..."
  - [corpus] MMGraphRAG supports fusing modalities to address missing information in text-only RAG.

### Mechanism 2
- **Claim:** Unified vector spaces improve ranking quality for queries requiring cross-modal reasoning.
- **Mechanism:** By embedding the user's text query and the document's images into the same geometric space, the retrieval engine can compute similarity scores based on shared semantic meaning (e.g., a text query about "revenue growth" matches a line chart visually representing that trend). This avoids the lexical mismatch that occurs when matching a query against a generated summary.
- **Core assumption:** The embedding model has sufficient capacity to align complex visual concepts (like financial chart trends) with natural language queries in a shared manifold.
- **Evidence anchors:**
  - [section 4.2] "Direct multimodal embedding retrieval (IMG) significantly outperforms text-based... achieving mAP@5 of 0.5234 vs 0.3963."
  - [abstract] "...text queries to retrieve both textual passages and visual content based on shared meaning."
  - [corpus] Omni-Embed-Nemotron abstract discusses handling increasing complexity of real-world information needs via unified retrieval.

### Mechanism 3
- **Claim:** Native visual context reduces hallucination rates in generation by providing grounding for numerical data.
- **Mechanism:** When a VLM receives a native image, it can attend to specific pixels to extract precise values. When it receives a text summary of an image, it relies on the summary's fidelity; if the summary omitted a number, the model might hallucinate one to answer the question.
- **Core assumption:** The downstream VLM possesses sufficient vision reasoning capabilities to interpret the raw image accurately.
- **Evidence anchors:**
  - [section 4.3] "The multimodal approach prevents unsupported additions 90% of the time... reducing information loss and fabrication."
  - [figure 2] Shows "No Unsupported Additions" score of 0.90 for IMG vs 0.10 for LLM IMG on GPT-5.
  - [corpus] Corpus evidence specifically linking native image grounding to reduced hallucination is weak; general RAG literature implies grounding reduces hallucination.

## Foundational Learning

- **Concept:** **Dense Retrieval & Bi-Encoders**
  - **Why needed here:** The system relies on "embedding" queries and documents into vectors to calculate similarity. You must understand that this is a semantic match (meaning), not a lexical match (keywords).
  - **Quick check question:** Why does the paper use Jina v4 instead of a keyword search like BM25 for the multimodal approach?

- **Concept:** **Multimodal Alignment (Vision-Language Models)**
  - **Why needed here:** The core innovation is putting text and images in the "same vector space." You need to grasp that a mathematical operation can determine a text query is "close to" an image of a chart.
  - **Quick check question:** How can a text string like "Q3 earnings" be mathematically similar to a PNG file of a bar chart?

- **Concept:** **RAG (Retrieval-Augmented Generation)**
  - **Why needed here:** The paper evaluates not just retrieval, but the quality of the final generated answer. You need to understand that retrieved context is injected into the prompt to ground the model's response.
  - **Quick check question:** What specific pipeline component differs between Approach 1 and Approach 2 that causes the difference in final answer accuracy?

## Architecture Onboarding

- **Component map:** Financial documents (PDFs/Slides) split into Text Chunks and Slide Images -> Embedding Store (Images -> LLM Summarizer -> Text Embedder OR Images + Text -> Multimodal Embedder) -> Vector DB -> User Query -> Embedder -> Vector Search (Top-K) -> Retrieved Context + Query -> VLM -> Answer

- **Critical path:** The **Embedding Store** step. The paper demonstrates that replacing the "LLM Summarizer + Text Embedder" chain with a "Native Multimodal Embedder" is the primary driver of the 32% retrieval improvement.

- **Design tradeoffs:**
  - **Accuracy vs. Complexity:** Direct multimodal embedding (Path B) offers higher accuracy (mAP@5 +32%) but requires complex preprocessing to extract and convert images to compatible formats (see Limitations).
  - **Model Dependency:** Path B requires powerful downstream VLMs (GPT-4o/5) to interpret the images; "mini" models showed significantly reduced gains.

- **Failure signatures:**
  - **Summarization Hallucination:** The text-based approach generates answers with "Unsupported Additions" (90% failure rate vs 10% for multimodal in GPT-5 tests).
  - **Mini-Model Bottleneck:** Using smaller models (e.g., GPT-4o-mini) with native images results in near-random preference (50/50 win rate), indicating the model cannot utilize the preserved visual data.

- **First 3 experiments:**
  1. **Baseline Validation:** Run the text-based pipeline (LLM Summarization) on the financial benchmark to establish the lower bound for mAP@5 and nDCG@5.
  2. **Ablation on Retrieval:** Switch the embedding model to Jina v4 while keeping the generator constant. Measure if retrieval metrics (mAP) improve without changing the generation step.
  3. **Judge Evaluation:** Run pairwise comparisons (LLM-as-a-judge) specifically on "Numerical Fidelity" to verify if native images reduce hallucination compared to text summaries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does direct multimodal embedding retrieval perform across diverse domains such as medical, legal, and scientific documents where visual content serves different purposes?
- Basis in paper: [explicit] Conclusion states: "Future work should extend evaluation to diverse domains including medical, legal, and scientific documents where visual content serves different purposes."
- Why unresolved: Current study only evaluates financial earnings call documents; different domains may have distinct visual conventions, chart types, and information density patterns.
- What evidence would resolve it: Systematic evaluation on benchmarks from medical imaging reports, legal contracts with annotated exhibits, and scientific publications with figures/tables.

### Open Question 2
- Question: Can automated document parsing pipelines reliably segment and classify visual elements across heterogeneous document formats?
- Basis in paper: [explicit] Limitations section states: "Future work should develop robust document parsing pipelines that automatically segment and classify visual elements across document formats."
- Why unresolved: Current preprocessing requires manual intervention; distinguishing tables from images and handling PowerPoint slides versus PDF figures remains challenging.
- What evidence would resolve it: Comparative study of automated parsing tools (Docling, Azure Document Intelligence, Unstructured.io) measuring segmentation accuracy across document types.

### Open Question 3
- Question: What is the minimum model capacity required for effective utilization of native multimodal context in RAG systems?
- Basis in paper: [inferred] Results show mini models (GPT-4o-mini: 0.57, GPT-5-mini: 0.50) derive limited benefit from multimodal retrieval compared to full models (GPT-5: 0.82), suggesting capacity thresholds exist.
- Why unresolved: Paper demonstrates correlation but does not isolate specific architectural or parameter-count requirements for cross-modal reasoning.
- What evidence would resolve it: Ablation studies across model scales measuring multimodal RAG performance while controlling for parameter count and architecture.

## Limitations

- Dataset scope uncertainty: The financial earnings call benchmark represents a narrow domain, and the 32% improvement may not generalize to other domains like medical imaging or legal documents.
- Model dependency concern: The approach requires powerful downstream VLMs (GPT-4o/5) to realize benefits, with mini-models showing near-random performance when using native images.
- Retrieval-only evaluation gap: The paper measures retrieval and generation quality separately without a unified evaluation framework capturing end-to-end pipeline performance.

## Confidence

**High confidence:** The direct comparison showing multimodal embeddings achieve 13% absolute improvement in mAP@5 (32% relative improvement) over text-based approaches. This result is consistently demonstrated across multiple LLM models and embedding configurations.

**Medium confidence:** The claim that preserving visual information reduces hallucination rates. While the paper shows "No Unsupported Additions" scores of 0.90 vs 0.10, this metric is based on pairwise LLM judgments rather than human evaluation, and the mechanism linking visual preservation to hallucination reduction requires further validation.

**Low confidence:** The generalizability of the 32% improvement across different domains and document types. The financial earnings call domain has specific characteristics (charts, tables, numerical data) that may not be representative of other multimodal retrieval scenarios.

## Next Checks

1. **Cross-domain generalization test:** Apply the multimodal embedding approach to non-financial domains such as medical imaging reports, engineering schematics, or social media content to validate whether the 32% retrieval improvement generalizes beyond earnings call documents.

2. **Mini-model capability analysis:** Conduct controlled experiments comparing different sizes of multimodal models (from small to large) on the same benchmark to precisely quantify the threshold at which visual information preservation becomes beneficial versus detrimental.

3. **End-to-end pipeline evaluation:** Implement a unified scoring metric that combines retrieval quality and generation accuracy into a single measure, then validate whether improvements in mAP@5 consistently translate to better user-facing outcomes across different model sizes and document types.