---
ver: rpa2
title: 'TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English
  Speech Translation'
arxiv_id: '2512.07265'
source_url: https://arxiv.org/abs/2512.07265
tags:
- data
- translation
- speech
- hours
- telugu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper develops a 46-hour manually verified Telugu-English speech
  translation benchmark from the CSTD corpus (30h/8h/8h train/dev/test split). Systematic
  evaluation of cascaded versus end-to-end architectures shows IndicWhisper + IndicMT
  achieves highest performance due to extensive Telugu-specific training data, while
  finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using
  significantly less Telugu-specific training data.
---

# TeluguST-46: A Benchmark Corpus and Comprehensive Evaluation for Telugu-English Speech Translation

## Quick Facts
- **arXiv ID**: 2512.07265
- **Source URL**: https://arxiv.org/abs/2512.07265
- **Reference count**: 4
- **Primary result**: IndicWhisper + IndicMT achieves 24.0 BLEU; fine-tuned SeamlessM4T reaches 17.1 BLEU on Telugu-English ST benchmark

## Executive Summary
This paper develops TeluguST-46, a 46-hour manually verified Telugu-English speech translation benchmark from the CSTD corpus with a 30h/8h/8h train/dev/test split. The study systematically evaluates cascaded (ASR→MT) versus end-to-end architectures, finding that IndicWhisper + IndicMT achieves the highest performance (24.0 BLEU) due to extensive Telugu-specific training data, while fine-tuned SeamlessM4T models demonstrate remarkable competitiveness (17.1 BLEU) despite using significantly less Telugu-specific training data. A comprehensive metric reliability study reveals that traditional metrics (ROUGE-L: 0.97 Pearson correlation, ChrF++: 0.96) provide better quality discrimination than BERTScore (0.73) for Telugu-English translation.

## Method Summary
The study created TeluguST-46 by manually verifying a 50-hour subset of the CSTD corpus, retaining 46 hours after filtering noisy or multi-speaker segments. Three training configurations were evaluated: Verified-Only (30h), Balanced (45h), and Unbalanced (73-80h). Cascaded systems used IndicWhisper (800+ hours Telugu ASR) followed by IndicMT translation, while end-to-end systems fine-tuned SeamlessM4T-v2-large with various hyperparameters. Evaluation employed BLEU, ChrF++, TER, ROUGE-L, METEOR, BERTScore, and ChatGPT-style human scoring. The best end-to-end configuration (LR=1e-6, batch=10, 80h data) achieved 17.1 BLEU, while the best cascaded system reached 23.99 BLEU.

## Key Results
- IndicWhisper + IndicMT achieves highest performance (24.0 BLEU) due to extensive Telugu-specific training data
- Fine-tuned SeamlessM4T (Config 4: 80h, LR=1e-6, batch=10) achieves 17.1 BLEU, only 29% below cascaded baseline
- Traditional metrics (ROUGE-L: 0.97, ChrF++: 0.96 Pearson correlation) outperform BERTScore (0.73) for quality discrimination
- Wav2Vec-based cascades underperform (8.3 BLEU) due to insufficient Telugu ASR training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cascaded systems achieve superior performance when component models have extensive language-specific pre-training
- Mechanism: IndicWhisper (800+ hours Telugu ASR) produces accurate transcriptions that propagate to IndicMT (substantial Telugu-English parallel data), yielding 24.0 BLEU versus 8.3 BLEU for Wav2Vec-based cascades with limited Telugu exposure
- Core assumption: ASR quality is the bottleneck in cascaded ST; better transcription directly improves downstream translation
- Evidence anchors: [abstract] "IndicWhisper + IndicMT achieves the highest performance due to extensive Telugu-specific training data"; [section 3.1] "IndicWhisper, an ASR model fine-tuned on over 800 hours of Telugu speech data"

### Mechanism 2
- Claim: Fine-tuned end-to-end models can approach cascaded performance with modest high-quality parallel data (<100 hours) and careful hyperparameter selection
- Mechanism: SeamlessM4T-v2 with conservative learning rate (1×10⁻⁶), small batch size (10), and mixed verified/unverified data (Config 4) achieves 17.1 BLEU—only 29% below cascaded baseline despite using ~73-80 hours total training data
- Core assumption: Pre-trained multilingual representations transfer effectively to Telugu; fine-tuning adapts without catastrophic forgetting
- Evidence anchors: [abstract] "finetuned SeamlessM4T models demonstrate remarkable competitiveness despite using significantly less Telugu-specific training data"; [section 6] "Configuration 4 achieves 17.1 BLEU and 43.82 METEOR—only a 29% BLEU drop from the cascaded baseline"

### Mechanism 3
- Claim: Traditional lexical metrics (ChrF++, ROUGE-L) outperform embedding-based metrics (BERTScore) for quality discrimination in morphologically rich languages
- Mechanism: ChrF++ (character n-grams) and ROUGE-L (longest common subsequence) capture morphological variations and structural similarity; BERTScore's contextual embeddings may conflate semantic similarity with translation accuracy in low-resource settings
- Core assumption: Human judgment reflects lexical and structural adequacy more than semantic embedding proximity for Telugu-English
- Evidence anchors: [abstract] "traditional metrics (ROUGE-L: 0.97 Pearson correlation, ChrF++: 0.96) provide better quality discrimination than BERTScore (0.73)"; [section 6.1] "BERTScore (r=0.73) shows lower alignment across configurations, suggesting limited discriminative power"

## Foundational Learning

- Concept: **Cascaded vs. End-to-End Speech Translation**
  - Why needed here: Understanding the trade-off between modular component optimization (cascaded) and joint optimization (end-to-end) determines architecture selection and data requirements
  - Quick check question: Can you explain why cascaded systems may outperform end-to-end in low-resource settings despite error propagation risk?

- Concept: **Agglutinative Morphology in Evaluation**
  - Why needed here: Telugu's morphological richness (complex word forms from root+affixes) affects metric choice; character-level metrics handle this better than word-level BLEU
  - Quick check question: Why might ChrF++ correlate more strongly with human judgment than BLEU for morphologically rich languages?

- Concept: **Fine-Tuning Hyperparameter Sensitivity**
  - Why needed here: Learning rate and batch size significantly impact end-to-end model performance in low-resource scenarios; conservative settings prevent overfitting
  - Quick check question: What happens to SeamlessM4T performance when learning rate increases from 1×10⁻⁶ to 1×10⁻⁵ in this study?

## Architecture Onboarding

- Component map:
  - Audio → IndicWhisper (ASR, 800h Telugu) → Telugu transcript → IndicMT → English text (Cascaded Path)
  - Audio → SeamlessM4T-v2-large (fine-tuned) → English text (End-to-End Path)
  - Audio → BLEU/ChrF++/METEOR/ROUGE-L/TER/BERTScore + ChatGPT-style human scoring (Evaluation Layer)

- Critical path:
  1. Dataset preparation: 50h subset → manual verification → 46h clean data (30/8/8 split)
  2. Baseline inference: Run pre-trained models without fine-tuning (Config 0)
  3. Fine-tuning: Verified-only (30h) → Balanced (45h) → Extended mixed (80h) with hyperparameter grid
  4. Metric correlation: Pearson correlation between automatic metrics and human judgment

- Design tradeoffs:
  - Data quality vs. quantity: Verified-only (30h) vs. Balanced (45h) vs. Extended (80h with noise)
  - Architecture choice: Cascaded (higher performance, requires component data) vs. End-to-end (competitive, simpler pipeline)
  - Metric selection: Traditional (ChrF++, ROUGE-L: high correlation) vs. Semantic (BERTScore: lower discrimination)

- Failure signatures:
  - Wav2Vec + IndicMT (8.3 BLEU): Insufficient ASR training data cascades errors downstream
  - SONAR (10.9 BLEU): Multilingual encoder lacks Telugu-specific adaptation
  - Aggressive learning rate (Config 3): 14.9 BLEU vs. 17.1 BLEU with conservative rate

- First 3 experiments:
  1. Replicate baseline: Run IndicWhisper + IndicMT and SeamlessM4T Config 0 on test set to establish benchmark scores
  2. Ablate data quality: Fine-tune SeamlessM4T with verified-only vs. balanced data at fixed hyperparameters to isolate quality effects
  3. Metric validation: Compute Pearson correlation between ChrF++, ROUGE-L, and BERTScore against human scores on model outputs to confirm discriminative power ranking

## Open Questions the Paper Calls Out

- **Open Question 1**: What are the optimal training data requirements for cascaded versus end-to-end architectures, and how does performance scale when expanding from 46 hours to 100+ hours of verified parallel data?
  - Basis: Future Work states expanding to 100+ hours would enable more robust conclusions about optimal training data requirements
  - Why unresolved: The constrained 46-hour dataset prevented systematic scaling experiments
  - What evidence would resolve it: Comparative benchmarking at multiple data scales (46h, 75h, 100h, 150h) with controlled quality verification

- **Open Question 2**: How do parameter-efficient fine-tuning methods (LoRA, adapters) compare to full fine-tuning for end-to-end ST models in low-resource settings?
  - Basis: Future Work calls for exploring PEFT methods given SeamlessM4T's promising competitiveness
  - Why unresolved: Only full fine-tuning was evaluated; PEFT methods could enable efficient adaptation with even less data
  - What evidence would resolve it: Comparative experiments evaluating LoRA/adapter-based fine-tuning against full fine-tuning across data configurations

- **Open Question 3**: Can cross-lingual transfer strategies leverage shared linguistic properties among Dravidian languages (Tamil, Kannada, Malayalam) to improve low-resource ST?
  - Basis: Future Work proposes investigating how insights from Telugu-English speech translation can transfer to other Dravidian languages
  - Why unresolved: Only Telugu-English was studied; related languages remain unexplored despite similar morphological complexity
  - What evidence would resolve it: Multilingual models trained on combined Dravidian language data evaluated on each language individually

## Limitations

- The study's findings depend heavily on ASR quality for cascaded systems, but lacks systematic WER analysis showing how translation performance degrades with ASR errors
- Fine-tuning hyperparameters use unspecified epoch counts, making exact reproduction challenging without early stopping criteria
- Metric reliability study uses ChatGPT-style human scoring without detailed protocol documentation, raising questions about consistency and reproducibility
- TeluguST-46 corpus construction relies on manual verification but doesn't report inter-annotator agreement statistics

## Confidence

- **High Confidence**: The cascaded architecture advantage when using extensively trained ASR models (IndicWhisper + IndicMT achieving 24.0 BLEU) is well-supported by systematic evaluation showing 8.3 BLEU for Wav2Vec-based cascades
- **Medium Confidence**: The claim that fine-tuned end-to-end models can approach cascaded performance (17.1 BLEU vs 24.0 BLEU) is supported but depends heavily on hyperparameter selection
- **Medium Confidence**: The metric reliability findings (ROUGE-L and ChrF++ correlating at 0.96-0.97 vs BERTScore at 0.73) are based on the study's human evaluation protocol but lack detailed methodology documentation

## Next Checks

1. **WER-to-BLEU Correlation Analysis**: Measure and report the Word Error Rate of IndicWhisper vs Wav2Vec ASR outputs on the test set, then correlate these WER values with their respective translation BLEU scores to quantify the ASR quality threshold for effective cascaded translation

2. **Hyperparameter Sensitivity Sweep**: Systematically test SeamlessM4T fine-tuning across a broader range of learning rates (1e-7 to 1e-5) and batch sizes while monitoring both training and development set performance to identify optimal configurations and potential overfitting points

3. **Inter-Annotator Reliability Assessment**: Conduct multiple rounds of human evaluation with different annotator groups using the same model outputs to establish Cohen's kappa or Fleiss' kappa scores, validating the consistency of the Likert scoring methodology