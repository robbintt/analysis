---
ver: rpa2
title: A Generative Caching System for Large Language Models
arxiv_id: '2503.17603'
source_url: https://arxiv.org/abs/2503.17603
tags:
- cache
- caching
- queries
- which
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GenerativeCache, a system that enhances LLM
  performance by caching responses and synthesizing new answers from cached data.
  The core innovation is "generative caching," which combines multiple cached responses
  to answer previously unseen queries.
---

# A Generative Caching System for Large Language Models

## Quick Facts
- arXiv ID: 2503.17603
- Source URL: https://arxiv.org/abs/2503.17603
- Reference count: 30
- Primary result: ~9x faster than GPTCache with stable lookup times even as cache grows

## Executive Summary
This paper introduces GenerativeCache, a system that enhances large language model (LLM) performance through semantic caching with a novel "generative caching" capability. The system stores query-answer pairs and, for semantically similar queries, retrieves cached responses. Its key innovation is the ability to synthesize new answers by combining multiple cached responses for previously unseen queries. GenerativeCache also features adaptive semantic similarity thresholds that adjust based on content type, cost, latency, and user feedback. The modular architecture supports pluggable components for embeddings, vector comparison, data storage, and LLM interfaces, demonstrating significant performance improvements over existing caching solutions.

## Method Summary
The system implements a semantic caching mechanism using vector embeddings and cosine similarity to identify cache hits. When a new query arrives, it computes an embedding and searches for similar cached queries using a vector database (Redis Stack or Milvus). The generative caching algorithm identifies multiple cached queries that individually exceed a similarity threshold and, if their combined similarity exceeds a higher threshold, synthesizes a new response by combining their answers. The system features adaptive semantic similarity thresholds that vary based on query content type, estimated LLM cost, latency requirements, and user feedback. An enhanced client enables parallel querying across multiple LLM providers (ChatGPT, Gemini, Llama 2) using asynchronous I/O operations.

## Key Results
- GenerativeCache achieves approximately 9x faster performance than GPTCache (45 vs 5 requests per second)
- Average cache lookup times remain stable at ~25ms even as cached content grows from 1K to 130K pairs
- Embedding computation dominates performance overhead (~22ms), while cache operations take <2ms
- The system supports both local and remote embedding models, allowing trade-offs between speed and quality
- Parallel querying across multiple LLMs improves both performance and user experience

## Why This Works (Mechanism)

### Mechanism 1: Generative Caching for Unseen Queries
- Claim: Synthesizing responses from multiple cached query-answer pairs can satisfy semantically novel queries.
- Mechanism: Given a new query Q', the system finds cached queries {x_i} where S(x_i, Q') > t_single. If Σ S(x_i, Q') > t_combined, a cache hit is declared and answers are combined/summarized.
- Core assumption: Query semantics are compositional—answers to sub-questions can be meaningfully combined.
- Evidence anchors:
  - [abstract] "multiple cached responses can be synthesized to provide answers to queries which have never been seen before"
  - [section 3] Algorithm: X ← { cached queries x_i for which S(x_i, Q') > t_single }; if Σ S(x_i, Q') > t_combined then cache hit
  - [corpus] Weak direct evidence—related work focuses on semantic caching but not generative synthesis
- Break condition: Queries requiring synthesis of contradictory cached answers; domains where answer composition is semantically invalid.

### Mechanism 2: Adaptive Semantic Similarity Thresholds
- Claim: Optimal cache hit/quality trade-offs require context-dependent similarity thresholds.
- Mechanism: Threshold t_s is adjusted based on: (1) content type (code requires higher t_s than natural language), (2) estimated LLM cost (higher cost → lower t_s to increase hit rate), (3) latency expectations, (4) user feedback on response quality.
- Core assumption: Users can provide reliable quality feedback; cost/latency estimates correlate with actual values.
- Evidence anchors:
  - [section 2] "We argue that t_s should vary based on multiple factors, including content of the query, run-time situations, and user preferences"
  - [section 3.1] Quality feedback loop: if quality_rate < t_q, increase t_s; if quality_rate > t_q, decrease t_s
  - [corpus] Related systems (GPTCache, Portkey) use fixed thresholds—comparative validation lacking
- Break condition: Rapid context switches where feedback lag prevents timely threshold adjustment; adversarial inputs.

### Mechanism 3: Embedding-Driven Lookup with Pluggable Backends
- Claim: Cache lookup performance is dominated by embedding computation, not storage/retrieval operations.
- Mechanism: Queries are vectorized via pluggable embedding models; similarity search performed via vector databases (Redis Stack, Milvus) or in-memory structures. Lookup time remains constant as cache grows (approximate nearest neighbor methods).
- Core assumption: Approximate vector similarity correlates sufficiently with semantic equivalence for caching purposes.
- Evidence anchors:
  - [section 6, Figure 6] Embedding computation: ~22ms; cache operations: <2ms
  - [section 6, Figure 5] "Average lookup times did not increase as cached question-answer pairs increased within this range [1K-130K]"
  - [corpus] Consistent with KV-cache findings that memory/bandwidth dominate LLM serving costs
- Break condition: Vector dimensionality explosion; embedding model drift over time; out-of-distribution queries.

## Foundational Learning

- Concept: **Vector Embeddings & Semantic Similarity**
  - Why needed here: Core mechanism for determining cache hits without exact string matching.
  - Quick check question: Can you explain why cosine similarity > threshold is an approximation of semantic equivalence, not a proof?

- Concept: **Cache Hit Rate vs. Quality Trade-off**
  - Why needed here: Lower thresholds increase hits but may return irrelevant responses.
  - Quick check question: If you lower t_s to increase cost savings, what metric must you monitor to detect quality degradation?

- Concept: **Asynchronous I/O for Parallel LLM Queries**
  - Why needed here: Sequential queries to multiple LLMs would negate latency benefits from caching.
  - Quick check question: Why does Python's GIL require multiprocessing (not asyncio) for some LLM clients?

## Architecture Onboarding

- Component map:
  Cache Manager -> LLM Proxy -> Semantic Similarity Calculator -> Embeddings Manager -> Cache Data Store

- Critical path: Query → Embedding computation (bottleneck: ~22ms) → Vector similarity search → Cache decision (hit/miss/generative) → Response synthesis or LLM fallback

- Design tradeoffs:
  - **Local vs. remote embeddings**: Local = faster, free, lower quality; Remote = slower, costly, higher quality
  - **Redis Stack vs. Milvus**: Redis = single DB, simpler; Milvus = better vector scaling, requires second DB for large payloads
  - **L1/L2 hierarchy**: L1 = low-latency, personalized; L2 = shared, higher hit potential, privacy concerns

- Failure signatures:
  - **Silent quality degradation**: t_s too low → cache returns irrelevant but "similar" responses
  - **Embedding bottleneck**: High query rate with local CPU-bound embedding model
  - **Stale cached responses**: Domain knowledge changes but cache lacks invalidation policy

- First 3 experiments:
  1. **Embedding model benchmark**: Measure latency vs. retrieval quality (MTEB scores) for 3-5 models on your query distribution; identify Pareto frontier.
  2. **Threshold calibration**: Run A/B test with different t_s values; plot hit rate vs. user-reported quality rate to find stable operating point.
  3. **Generative cache validation**: Construct test set of compositional queries (Q3 = Q1 + Q2); measure accuracy when Q1, Q2 are cached but Q3 is not.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a caching system automatically classify query sensitivity to differentiate between private L1-only caching and shared L2 caching without relying on explicit user hints?
- Basis in paper: [explicit] The authors state that automatically classifying queries to determine if they should be cached in L2 "gets complicated," leading the current system to rely on manual user control instead.
- Why unresolved: The paper proposes a hierarchical architecture (L1/L2) but leaves the automated policy enforcement for privacy-conscious data sharing as an unsolved complexity.
- What evidence would resolve it: An algorithm capable of detecting personalized or sensitive context in queries with high precision, validated by user privacy preferences in a multi-user study.

### Open Question 2
- Question: What is the optimal strategy for synthesizing multiple cached responses to ensure the generated output is factually consistent and superior to a single cached hit?
- Basis in paper: [inferred] The paper introduces generative caching to combine answers but notes that contacting more caches adds overhead and requires limiting processing.
- Why unresolved: While the mechanism for combining answers is described, the paper does not evaluate the *quality* or factual coherence of the synthesized result compared to a ground-truth LLM response.
- What evidence would resolve it: Benchmarks comparing the factual accuracy and semantic coherence of synthesized "generative" hits against single cache hits and live LLM responses.

### Open Question 3
- Question: Can a closed-loop control system dynamically adjust semantic similarity thresholds ($t_s$) to strictly guarantee monetary cost caps while maintaining acceptable response latency?
- Basis in paper: [inferred] The paper suggests varying $t_s$ based on cost and latency but relies on user feedback or general run-time estimates rather than a proven optimal control algorithm.
- Why unresolved: The system currently reacts to user dissatisfaction or general cost estimates; it lacks a formal model proving it can optimize the trade-off between cache hit rates (cost savings) and similarity strictness (quality) autonomously.
- What evidence would resolve it: Experiments showing the system maintaining a specific cost-per-day target without significant degradation in answer quality or user satisfaction scores.

## Limitations
- Key implementation details (threshold adaptation algorithms, synthesis mechanism, source code) are not provided, preventing full reproduction
- Experimental validation relies heavily on throughput benchmarks without sufficient analysis of quality degradation
- Study focuses on a single dataset (SQuAD) without demonstrating generalizability across different domains
- Privacy classification between L1/L2 caching remains manual rather than automated

## Confidence
- **High confidence**: Cache lookup performance dominated by embedding computation (~22ms vs <2ms for storage operations); approximate nearest neighbor methods maintain constant lookup times as cache grows
- **Medium confidence**: 9x throughput improvement over GPTCache is well-supported by experimental data but may depend on specific implementation details not fully disclosed
- **Low confidence**: Generative caching mechanism effectiveness for truly novel queries; adaptive threshold adjustment prevents quality degradation across diverse contexts

## Next Checks
1. **Quality degradation analysis**: Systematically measure response quality across varying t_s thresholds using human evaluation or automated metrics (e.g., ROUGE, BERTScore) to identify the point where caching compromises answer relevance

2. **Cross-domain generalization**: Test the system on multiple datasets beyond SQuAD (e.g., technical documentation, conversational data) to verify that adaptive thresholds and generative caching work across different semantic spaces

3. **Threshold adaptation robustness**: Simulate rapid context switches and adversarial queries to evaluate whether the feedback-based threshold adjustment can maintain appropriate cache hit/quality trade-offs without requiring extensive retraining or manual intervention