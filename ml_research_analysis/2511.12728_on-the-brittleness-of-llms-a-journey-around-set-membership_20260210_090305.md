---
ver: rpa2
title: 'On the Brittleness of LLMs: A Journey around Set Membership'
arxiv_id: '2511.12728'
source_url: https://arxiv.org/abs/2511.12728
tags:
- related
- unrelated
- above
- prompt
- llama-3
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the brittleness of large language models
  (LLMs) on the fundamental task of set membership queries using extremely simple,
  explicitly listed sets of four elements. The authors conduct large-scale experiments
  across diverse prompt phrasings, semantic structures, element orderings, and multiple
  LLMs (7 models, over 60 million queries).
---

# On the Brittleness of LLMs: A Journey around Set Membership

## Quick Facts
- **arXiv ID**: 2511.12728
- **Source URL**: https://arxiv.org/abs/2511.12728
- **Reference count**: 40
- **Primary result**: LLMs exhibit significant brittleness on set membership queries despite high accuracy, with sensitivity to prompt phrasing, element ordering, and semantic relationships.

## Executive Summary
This study investigates the brittleness of large language models (LLMs) on the fundamental task of set membership queries using extremely simple, explicitly listed sets of four elements. The authors conduct large-scale experiments across diverse prompt phrasings, semantic structures, element orderings, and multiple LLMs (7 models, over 60 million queries). Despite high overall accuracy, they find consistent brittleness: models are highly sensitive to minor prompt variations, element ordering, and semantic relationships among set elements—properties that should not affect set membership decisions. Different LLMs exhibit substantially different error patterns, with no consistent "understanding" of the set concept. The work demonstrates that the simplicity of the task enables comprehensive mapping of failure modes, providing a valuable methodology for LLM evaluation.

## Method Summary
The authors evaluate 7 LLMs on set membership queries using 22 sets of 4 elements each, testing all permutations and 2,880 prompt template variations. For each set, they generate positive queries (element is member), negative-member queries (element is in same semantic category but not in set), and negative-intruder queries (unrelated element). Four prompt template classes (NL1, NL2, CS, CA) with different phrasings, arrangements, and quotation styles are used. The study measures accuracy, false positive rates, permutation consistency, and error pattern correlations across models, finding that high accuracy coexists with substantial brittleness to semantic relationships and element ordering.

## Key Results
- Models achieve high accuracy (~98.6%) but show significant brittleness to prompt variations and element ordering
- Semantic relationships between elements affect membership decisions, causing semantic leakage (higher false positives for related non-members)
- Different LLMs exhibit substantially different error patterns with low cosine similarity between models
- No consistent "understanding" of set membership across models; prompt effectiveness is highly model-specific
- Permutation sensitivity reveals that specific orderings can consistently cause failures across templates

## Why This Works (Mechanism)

### Mechanism 1: Semantic Interference with Formal Reasoning
- Claim: LLMs fail to isolate formal set membership logic from semantic associations between elements, causing both error types (leakage and boosting).
- Mechanism: When elements share semantic category membership (e.g., fruits, cardinal directions), model representations blend conceptual similarity with set membership, overriding explicit syntactic structure. This manifests as: (a) semantic leakage—higher false positives when queried element is semantically related to set members; (b) semantic boosting—improved accuracy for complete/related sets versus unrelated ones.
- Core assumption: The interference occurs at the representation level rather than pure attention failure; semantic embeddings influence the final membership decision despite explicit task framing.
- Evidence anchors:
  - [abstract] "models are highly sensitive to... semantic relationships among set elements—properties that should not affect set membership decisions"
  - [section 4.3] "FPRs are substantially larger for member words than for intruder words... when the correct answer is 'not a member', most LLMs are more likely to say 'member' when the word tested for membership is semantically related"
  - [corpus] Related work "Systematic Diagnosis of Brittle Reasoning in Large Language Models" addresses structured failure diagnosis but focuses on mathematical reasoning; direct corpus support for semantic leakage mechanism is weak.
- Break condition: If models were fine-tuned on set membership with explicit abstraction training, semantic interference would attenuate but not eliminate the effect.

### Mechanism 2: Permutation-Dependent Attention Patterns
- Claim: LLM responses to set membership queries vary based on element presentation order, despite sets being formally order-invariant.
- Mechanism: Sequential token processing creates position-dependent attention weights; elements earlier or later in the sequence receive differential attention, and the queried element's position relative to set members affects recognition. Adversarial orderings exist where specific permutations consistently cause failures.
- Core assumption: The effect stems from positional encoding interactions rather than pure token co-occurrence statistics from training data.
- Evidence anchors:
  - [section 4.2] "Does the set (Hearts, Clubs, Spades) contain the character sequence East?" → correct; "Does the set (Hearts, Spades, Clubs) contain the character sequence East?" → incorrect (same model, different order)
  - [section 4.2] "Inconsistency is much higher than the overall error rate... indicating that many sets have 'adversarial' orderings, in which the query fails"
  - [corpus] No direct corpus evidence for permutation sensitivity in set tasks; related work on permutation invariance (Egressy and Stühmer 2025, Zaheer et al. 2017) cited but not directly tested.
- Break condition: If permutation sensitivity drops significantly with explicit position-agnostic prompting (e.g., "regardless of order..."), attention mechanism involvement is confirmed.

### Mechanism 3: Prompt-Model Misalignment in Concept Instantiation
- Claim: No consistent mapping exists between prompt formulations and set concept activation across different LLMs; each model has idiosyncratic "preferred" prompt structures.
- Mechanism: Instruction-tuning creates model-specific associations between phrasings and task schemas; the abstract "set membership" concept is stored in fragmented, prompt-contingent patterns. Features like "arrangement" (set-first vs. element-first) have opposite effects across models.
- Core assumption: The fragmentation is inherent to how set concepts are encoded during pretraining/instruction-tuning, not merely a prompt engineering limitation.
- Evidence anchors:
  - [section 4.4] "In the NL1 class, there is in fact no perfect template and the highest number of templates are perfect only on a single LLM"
  - [figure 2] Shapley analysis shows different models have completely different top features affecting accuracy
  - [corpus] "Are Humans as Brittle as Large Language Models?" investigates prompt brittleness comparatively; "Systematic Diagnosis of Brittle Reasoning" addresses failure mode mapping but not set membership specifically.
- Break condition: If cross-model prompt transfer were improved via shared instruction-tuning protocols, model-specific preferences would converge.

## Foundational Learning

- Concept: **Invariance properties in formal reasoning**
  - Why needed here: Set membership should be invariant to element order, phrasing, and semantic content; understanding what "should" be invariant reveals where models violate formal logic.
  - Quick check question: Given set {a, b, c}, should "a ∈ {a, b, c}" produce the same answer as "a ∈ {c, b, a}"?

- Concept: **Semantic vs. syntactic processing**
  - Why needed here: The paper's core finding is that semantic relationships interfere with syntactic set operations; distinguishing these is essential for interpreting error patterns.
  - Quick check question: If a model correctly handles {Hearts, Diamonds, Clubs, Spades} but fails on {Hearts, Spades, Clubs}, is the failure syntactic or semantic?

- Concept: **Brittleness vs. accuracy as evaluation metrics**
  - Why needed here: High average accuracy (~98.6%) coexists with concerning brittleness; understanding both metrics is needed for proper model assessment.
  - Quick check question: If a model achieves 99% accuracy but fails on 0.1% of permutations, is it reliable for production use?

## Architecture Onboarding

- Component map:
  - **Prompt template system**: 4 classes (NL1, NL2, CS, CA) with feature combinations (arrangement, quotation, listing style, etc.)
  - **Query generation**: 22 sets × 3 query types (positive, negative-member, negative-intruder) × 24 permutations × multiple templates
  - **Set taxonomy**: Complete sets (exhaust categories), related sets (shared semantics), unrelated sets (no obvious connection)
  - **Analysis pipeline**: Error rate aggregation, Shapley value computation, cosine similarity between prompt categories

- Critical path:
  1. Define sets with clear semantic categories (test for "completeness" via LLM completion)
  2. Generate all query instances systematically
  3. Execute with constrained output ("Answer with a single word")
  4. Parse responses for yes/no/true/false keywords
  5. Aggregate by query type, prompt class, set type, and model

- Design tradeoffs:
  - **Set size = 4**: Small enough for tractable permutation coverage (24 orderings), large enough to reveal patterns
  - **Template diversity vs. comparability**: Many templates enable brittleness mapping but reduce direct comparability
  - **Natural language vs. Python prompts**: Tests whether formal code framing reduces brittleness (spoiler: it doesn't fully)

- Failure signatures:
  - **Permutation inconsistency**: Same query, different ordering → different answers
  - **Semantic leakage**: Higher FPR for negative-member vs. negative-intruder queries
  - **Model-specific preferences**: Templates perfect on one model failing on others
  - **Python prompt brittleness**: Even code prompts with explicit `in` operator show ordering sensitivity

- First 3 experiments:
  1. Replicate permutation sensitivity test on a single set type (e.g., complete word sets) with 2-3 models to confirm the phenomenon before full-scale testing
  2. Test semantic leakage hypothesis: compare FPR for negative-member vs. negative-intruder queries across 10 related word sets
  3. Map model-specific prompt preferences: for one model, identify top-3 best and worst templates via Shapley analysis, then validate on held-out sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanistic or architectural factors cause the observed brittleness in set membership reasoning?
- Basis in paper: [explicit] The authors state in Limitations: "we have not explored in depth the several directions we uncovered... This would involve looking 'under the hood' using, e.g., mechanistic interpretability techniques."
- Why unresolved: The study maps failure modes comprehensively but does not investigate internal model representations or circuitry that produce these errors.
- What evidence would resolve it: Mechanistic interpretability analyses (e.g., probing classifiers, attention pattern analysis, activation patching) identifying which layers or attention heads encode set membership information.

### Open Question 2
- Question: Are semantic leakage and semantic boosting phenomena specific to LLMs, or do humans exhibit similar patterns?
- Basis in paper: [explicit] The authors ask whether "certain brittleness properties (e.g., semantic leakage or boosting) are specific to LLMs" and suggest human experiments as future work.
- Why unresolved: The study only evaluates LLMs; no human baseline was collected to determine if semantic interference on formal reasoning is a uniquely artificial phenomenon.
- What evidence would resolve it: Controlled human experiments using identical set membership queries, measuring response times and error rates across semantically related vs. unrelated sets.

### Open Question 3
- Question: Why do different LLMs exhibit substantially different and sometimes opposite error patterns despite similar overall accuracy?
- Basis in paper: [inferred] Figure 1 shows low cosine similarity between most model pairs' error patterns; Figure 2 shows different LLMs preferring opposite prompt arrangements; Figure 5 shows opposite effects of semantic relatedness on numbers.
- Why unresolved: The paper documents inter-model variability but does not identify whether differences stem from training data, architecture, instruction-tuning procedures, or model scale.
- What evidence would resolve it: Systematic ablation studies across model families controlling for training data, scale, and fine-tuning methods to isolate sources of behavioral divergence.

### Open Question 4
- Question: Does the simplicity-based evaluation methodology generalize to reveal brittleness in other fundamental reasoning tasks?
- Basis in paper: [explicit] The authors state their approach is "a valuable methodology for LLM evaluation in general" and propose it "can be applied using other extremely simple problems as well."
- Why unresolved: Only set membership was tested; it remains unclear whether similar brittleness patterns emerge for other basic operations (e.g., list length, element comparison, logical conjunction).
- What evidence would resolve it: Applying the same large-scale, factorial experimental design to other elementary tasks and comparing whether prompt sensitivity, ordering effects, and semantic interference patterns replicate.

## Limitations

- **Limited to small sets**: The study only tests 4-element sets, which may not generalize to larger or real-world membership queries
- **No mitigation strategies**: The paper identifies brittleness but doesn't explore whether fine-tuning or prompting strategies could reduce it
- **Correlational evidence**: The proposed mechanisms (semantic interference, permutation sensitivity) are supported by correlations rather than causal ablation studies

## Confidence

- **High confidence** in the central claim that LLMs exhibit brittleness on set membership tasks despite high accuracy, based on systematic experimentation across multiple models and prompt variations
- **Medium confidence** in the specific mechanisms proposed (semantic interference and permutation-dependent attention) due to the correlational nature of the evidence and lack of ablation studies confirming causality

## Next Checks

1. Test whether fine-tuning on set membership tasks with explicit order-invariance instructions reduces permutation sensitivity and semantic interference
2. Evaluate models on larger sets (8-12 elements) to determine if brittleness scales with set size or remains consistent
3. Conduct ablation studies removing semantic information from set elements (using abstract tokens) to isolate whether the brittleness stems from semantic processing or general attention mechanisms