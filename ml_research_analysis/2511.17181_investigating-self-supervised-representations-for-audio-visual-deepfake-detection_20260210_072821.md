---
ver: rpa2
title: Investigating self-supervised representations for audio-visual deepfake detection
arxiv_id: '2511.17181'
source_url: https://arxiv.org/abs/2511.17181
tags:
- features
- detection
- audio
- deepfake
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates self-supervised representations
  (audio, visual, multimodal) for audio-visual deepfake detection, addressing three
  research questions: detection effectiveness, interpretability of encoded information,
  and cross-modal complementarity. The authors adapt linear probing to the video domain,
  assess performance across in-domain and out-of-domain datasets, and propose a multi-faceted
  evaluation suite including anomaly detection and explainability techniques.'
---

# Investigating self-supervised representations for audio-visual deepfake detection

## Quick Facts
- arXiv ID: 2511.17181
- Source URL: https://arxiv.org/abs/2511.17181
- Reference count: 40
- Systematic evaluation of self-supervised representations for audio-visual deepfake detection across datasets

## Executive Summary
This paper systematically evaluates self-supervised representations (audio, visual, multimodal) for audio-visual deepfake detection, addressing three research questions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. The authors adapt linear probing to the video domain, assess performance across in-domain and out-of-domain datasets, and propose a multi-faceted evaluation suite including anomaly detection and explainability techniques.

Key findings show that many self-supervised features capture deepfake-relevant information, with audio-informed representations generalizing best across datasets. Temporal and spatial explanations align with manipulated regions and human annotations, indicating the models attend to semantically meaningful cues. Feature combinations improve performance, particularly when combining audio and visual modalities. However, none of the features reliably generalize across datasets, likely due to dataset characteristics rather than superficial pattern learning. Audio features (particularly Wav2Vec2) perform strongly when speech-level manipulations are present, while A V-HuBERT provides the strongest overall performance. The study reveals both the promise of self-supervised features for deepfake detection and the fundamental challenges in achieving robust cross-domain generalization.

## Method Summary
The study evaluates self-supervised representations for audio-visual deepfake detection using linear probing adapted to the video domain. The methodology involves extracting features from pretrained models (Wav2Vec2, VideoMAE, A V-HuBERT, etc.) and training linear classifiers to detect deepfakes. Evaluation spans multiple datasets (DFD, FF++, Celeb-DF, DeeperForensics) for both in-domain and out-of-domain testing. The assessment includes performance metrics, anomaly detection scores, and explainability techniques (Grad-CAM, attention visualization) to understand what features the models use. Feature combinations are tested to explore cross-modal complementarity. The study systematically addresses detection effectiveness, interpretability of encoded information, and cross-modal complementarity through this multi-faceted evaluation approach.

## Key Results
- Audio-informed representations (particularly A V-HuBERT) generalize best across datasets for deepfake detection
- Feature combinations improve performance, especially when combining audio and visual modalities
- Temporal and spatial explanations align with manipulated regions and human annotations, showing models attend to semantically meaningful cues
- None of the features reliably generalize across datasets, likely due to dataset characteristics rather than superficial pattern learning
- Wav2Vec2 performs strongly when speech-level manipulations are present in the data

## Why This Works (Mechanism)
Self-supervised representations capture deepfake-relevant information through pretraining on large-scale unlabelled data. The pretraining objectives (contrastive learning, masked prediction) force models to learn meaningful representations that encode both semantic content and fine-grained details necessary for detecting manipulations. Audio-informed models like A V-HuBERT leverage cross-modal synchronization patterns, making them particularly effective at identifying inconsistencies between audio and visual streams that characterize deepfakes. The linear probing approach reveals that these representations already contain most of the discriminative information needed for detection, requiring minimal task-specific adaptation.

## Foundational Learning
- Self-supervised learning (why needed: pretrain models without labels; quick check: models achieve competitive performance on downstream tasks)
- Linear probing (why needed: evaluate quality of frozen representations; quick check: high accuracy with simple classifier indicates good feature quality)
- Cross-modal learning (why needed: detect audio-visual inconsistencies in deepfakes; quick check: multimodal models outperform single-modality models)
- Explainability techniques (why needed: verify models attend to meaningful regions; quick check: attention maps align with manipulated areas)
- Out-of-domain generalization (why needed: assess real-world robustness; quick check: performance drop across datasets indicates domain shift)

## Architecture Onboarding

**Component Map:**
Raw video -> Feature extractor (pretrained model) -> Linear classifier -> Deepfake detection output

**Critical Path:**
Feature extraction -> Linear classification -> Evaluation (metrics, explainability, anomaly detection)

**Design Tradeoffs:**
- Frozen representations vs. fine-tuning: Linear probing preserves pretraining benefits but may limit task-specific adaptation
- Single modality vs. multimodal: Multimodal models capture cross-modal inconsistencies but require more complex architectures
- Explainability vs. performance: Interpretability techniques may reduce detection accuracy but provide crucial insights

**Failure Signatures:**
- Poor generalization across datasets suggests dataset-specific biases
- Low anomaly detection scores indicate representations not capturing manipulation artifacts
- Explainability maps misaligned with manipulated regions suggest superficial learning

**First Experiments:**
1. Linear probing baseline with Wav2Vec2 on single-modality audio deepfake detection
2. Multimodal feature combination using A V-HuBERT across all available datasets
3. Cross-dataset generalization test comparing in-domain vs. out-of-domain performance

## Open Questions the Paper Calls Out
The paper identifies several key open questions: How to achieve reliable cross-domain generalization for deepfake detection? What are the fundamental limitations of self-supervised representations in capturing manipulation artifacts? How can we design more robust evaluation frameworks that better reflect real-world deployment scenarios? What is the optimal balance between frozen representations and task-specific fine-tuning for different deepfake generation techniques?

## Limitations
- Dataset scope limited to similar generation techniques, potentially not capturing true out-of-domain generalization challenges
- Interpretability analysis relies on specific techniques that may not reveal complete decision-making processes
- Limited exploration of the feature combination space, with many potential multimodal architectures untested
- No longitudinal evaluation of performance degradation over time as deepfake generation techniques evolve

## Confidence

| Claim | Confidence |
|-------|------------|
| Audio-informed representations generalize best | High |
| Feature combinations improve performance | High |
| Models attend to semantically meaningful cues | Medium |
| Dataset characteristics cause generalization issues | Medium |
| None of the features reliably generalize | Medium |

## Next Checks
1. Test the evaluated features on truly out-of-domain datasets with different deepfake generation techniques (e.g., real-time video conferencing manipulations, artistic forgeries) to validate generalization claims
2. Conduct ablation studies systematically varying feature combination strategies to identify optimal multimodal architectures beyond the current tested combinations
3. Implement longitudinal evaluation tracking performance degradation over time as new deepfake generation techniques emerge, addressing the stated need for continual updates