---
ver: rpa2
title: 'FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation'
arxiv_id: '2510.16597'
source_url: https://arxiv.org/abs/2510.16597
tags:
- reviewer
- recommendation
- dataset
- reviewers
- papers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FRONTIER-RevRec is a large-scale dataset for reviewer recommendation
  containing 177,941 reviewers and 478,379 papers across 209 journals from the Frontiers
  publishing platform. The dataset spans multiple disciplines and provides authentic
  reviewer-paper assignment records from 2007-2025.
---

# FRONTIER-RevRec: A Large-scale Dataset for Reviewer Recommendation

## Quick Facts
- arXiv ID: 2510.16597
- Source URL: https://arxiv.org/abs/2510.16597
- Reference count: 34
- Key outcome: Large-scale dataset for reviewer recommendation containing 177,941 reviewers and 478,379 papers across 209 journals from Frontiers publishing platform.

## Executive Summary
FRONTIER-RevRec is a comprehensive dataset for reviewer recommendation research, containing 177,941 reviewers and 478,379 papers across 209 journals from the Frontiers publishing platform spanning 2007-2025. The dataset provides authentic reviewer-paper assignment records and reveals that content-based methods significantly outperform collaborative filtering for reviewer recommendation, contrary to findings in commercial recommendation domains. Language models, particularly LLaMA2, excel at capturing semantic alignment between papers and reviewer expertise. The research identifies optimal aggregation strategies: averaging for word-to-paper embedding and LSTM for paper-to-reviewer aggregation.

## Method Summary
The dataset was constructed from Frontiers' reviewer assignment records, with preprocessing that removed reviewers with fewer than two reviewed papers. The evaluation used a leave-one-out split, holding out one paper per reviewer for testing. For each test paper, 25 candidate reviewers were selected (1 ground truth + 24 negatives). Multiple encoding approaches were compared including BERT-base-uncased and LLaMA2-7B, with aggregation strategies ranging from simple averaging to sequential models like LSTM. The best performance was achieved using LLaMA2-7B with single-tower architecture, averaging pooling for word-to-paper aggregation, and LSTM for paper-to-reviewer aggregation.

## Key Results
- Content-based methods significantly outperform collaborative filtering for reviewer recommendation
- LLaMA2-7B achieves P@1=0.5627, outperforming BERT (P@1=0.4886) and TF-IDF (P@1=0.1743)
- Averaging works best for word-to-paper embedding aggregation, while LSTM is optimal for paper-to-reviewer aggregation
- The Frontiers network exhibits extreme fragmentation with 6,137 connected components and 48-hop diameter

## Why This Works (Mechanism)

### Mechanism 1
Content-based methods outperform collaborative filtering because academic networks lack the dense connectivity patterns that make collaborative signals predictive in commercial domains. Frontiers network has 6,137 connected components vs. 1 in commercial datasets, diameter of 48 hops vs. 7-8 in commercial, and only 5% reachability vs. 100%. This structural fragmentation means graph-based methods cannot identify appropriate assignments.

### Mechanism 2
Large language models excel at reviewer recommendation because they capture nuanced semantic alignment between paper content and reviewer expertise that simpler encoders miss. LLaMA2-7B's larger model capacity and extensive pretraining enable it to encode subtle disciplinary distinctions, evidenced by intra-category embedding similarity (0.529) exceeding inter-category similarity (0.471).

### Mechanism 3
Different aggregation strategies are optimal at different pipeline stages because word-to-paper and paper-to-reviewer transformations have distinct structural properties. Paper titles are concise and information-dense, so averaging word embeddings outperforms sequential models (39.6% gap). However, reviewer expertise evolves over time, so LSTM (capturing temporal dynamics) outperforms averaging for paper-to-reviewer aggregation (12.5% improvement).

## Foundational Learning

- **Collaborative Filtering vs. Content-Based Recommendation**: Commercial recommendation leverages user-item interaction patterns, while content-based methods match item attributes to user profiles. Quick check: If you have a new paper with no prior reviewer assignments, which approach would fail first and why?

- **Bipartite Graph Topology**: Network properties like connected components, diameter, and path lengths explain why collaborative filtering fails. Quick check: A network with 6,137 components and 48-hop diameter suggests what about the density of reviewer-paper relationships?

- **Embedding Aggregation Strategies**: The pipeline requires transforming word embeddings → paper embeddings → reviewer embeddings. Different transformations (averaging, LSTM, attention) have inductive biases suited to different data structures. Quick check: Why would averaging work for static, concise text but LSTM work for temporally-ordered sequences?

- **Single-Tower vs. Dual-Tower Architectures**: Single-tower enables cross-attention during encoding; dual-tower separately encodes then matches. Quick check: Which architecture trades accuracy for inference efficiency, and when would you choose each?

## Architecture Onboarding

- **Component map**: Paper titles and metadata -> Text Encoder (LLaMA2-7B) -> Word-to-Paper Aggregation (Average pooling) -> Paper-to-Reviewer Aggregation (LSTM) -> Matching Layer (Cosine similarity) -> Evaluation (Precision@k, Recall@k, NDCG@k)

- **Critical path**: Load reviewer history and construct paper sequences per reviewer -> Encode all paper titles using frozen or fine-tuned LLM -> Aggregate word embeddings → paper embeddings (average pooling) -> Aggregate paper embeddings → reviewer embeddings (LSTM over temporal sequence) -> For each target paper, compute similarity to all candidate reviewers -> Rank and evaluate against ground-truth assignments

- **Design tradeoffs**: LLaMA2 vs. BERT provides +7.4% P@1 improvement but requires 7B parameters vs. 110M; single-tower achieves better accuracy but requires O(n×m) inference vs. O(n+m) for dual-tower; LSTM captures temporal expertise evolution (+12.5% performance) but requires sequential processing

- **Failure signatures**: Collaborative filtering yields ~0.08 P@1 (expected due to network fragmentation); cold-start reviewers get random predictions (paper history < 2); performance drops with large candidate sets (P@1: 0.56→0.23); imbalanced discipline distribution (Clinical medicine dominates 49.7%)

- **First 3 experiments**: 1) Implement BERT + average pooling + LSTM to validate pipeline matches reported results (target P@1 ≈ 0.48-0.49); 2) Replace LSTM with average pooling for paper-to-reviewer aggregation (expect ~10-12% drop in P@1); 3) Evaluate LLaMA2 with candidate sizes [25, 50, 100, 200] to document precision degradation curve

## Open Questions the Paper Calls Out
- How can cross-domain recommendation models be designed to effectively address the severe disciplinary imbalances and reviewer scarcity in underrepresented fields like the Humanities?
- Can fairness-aware algorithms be implemented to ensure equitable reviewer distribution across demographics and institutions while maintaining quality standards?
- How does the reliance on random negative sampling affect the validity of the conclusion that content-based methods outperform collaborative filtering?
- Does the structural fragmentation observed in the Frontiers dataset generalize to other academic publishing platforms?

## Limitations
- Findings may not generalize beyond Frontiers publishing platform due to specific review management workflow
- Focus on paper titles rather than full abstracts or text may limit semantic richness captured
- LLaMA2-7B requires significant computational resources limiting accessibility
- Dataset heavily skewed toward biomedical and life sciences (83% of data)

## Confidence
- **High Confidence**: Content-based methods outperform collaborative filtering on this dataset
- **Medium Confidence**: LLaMA2-7B provides superior performance
- **Medium Confidence**: Aggregation strategy recommendations

## Next Checks
1. Test the recommended approaches (BERT + average pooling + LSTM) on reviewer recommendation data from other publishers (e.g., Springer, IEEE, or arXiv) to verify generalizability beyond Frontiers
2. Implement the same pipeline using abstracts or full paper text instead of titles to quantify the impact of information richness on matching accuracy
3. Systematically evaluate performance degradation as reviewer history length decreases to establish minimum requirements for practical deployment