---
ver: rpa2
title: Utilising Deep Learning to Elicit Expert Uncertainty
arxiv_id: '2501.11813'
source_url: https://arxiv.org/abs/2501.11813
tags:
- uncertainty
- learning
- expert
- deep
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel method to elicit expert uncertainty
  by using deep learning models to analyze complex data, such as images and reports,
  that experts typically use in their decision-making processes. Traditional methods
  of expert knowledge elicitation rely on interviews and direct questioning, which
  can be time-consuming, introduce bias, and require statistical expertise.
---

# Utilising Deep Learning to Elicit Expert Uncertainty

## Quick Facts
- **arXiv ID**: 2501.11813
- **Source URL**: https://arxiv.org/abs/2501.11813
- **Reference count**: 40
- **Primary result**: Deep learning model trained on expert decisions can generate probability distributions capturing expert uncertainty, with higher entropy for cases where experts disagree.

## Executive Summary
This paper presents a novel method to elicit expert uncertainty by analyzing the data experts use in their decision-making processes, rather than relying on traditional interview-based approaches. The method employs Monte Carlo Dropout in deep learning models to generate probability distributions that capture epistemic uncertainty. Demonstrated on a histopathology dataset for colon cancer risk prediction, the approach shows that model uncertainty correlates with expert disagreement, producing more uncertain distributions when pathologists disagree and more certain distributions when there is full agreement.

## Method Summary
The method leverages expert decision data by training deep learning models to predict outcomes based on complex data like images and reports. Monte Carlo Dropout is applied during inference to approximate Bayesian uncertainty, generating multiple predictions for the same input. These samples are then fitted to a Beta distribution using the Method of Moments, creating a compact representation of uncertainty that can be used as a prior in downstream Bayesian analysis. The approach was validated using a histopathology dataset where models predicted cancer risk from tissue images.

## Key Results
- Model produces more uncertain distributions (higher entropy) when pathologists disagree on diagnoses
- Mean entropy increases systematically as the number of opposing pathologists increases
- Calibration plots show the model's confidence levels align with actual accuracy rates
- The elicited Beta distributions vary meaningfully between individuals, reflecting different patterns of uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Stochastic regularization (Dropout) applied during inference approximates a Bayesian posterior, transforming a point-estimate classifier into a probability distribution generator.
- **Mechanism**: By retaining dropout layers active during prediction, the network produces varying outputs for the same input over multiple forward passes. This variance models the uncertainty in the model parameters, effectively sampling from a posterior distribution without requiring computationally expensive Variational Inference or MCMC.
- **Core assumption**: The dropout approximation holds as a valid proxy for Bayesian inference in deep Gaussian processes.
- **Evidence anchors**: Abstract mentions "employing probabilistic deep learning techniques, specifically Monte Carlo Dropout, to generate probability distributions"; section 3.1 explains dropout mathematically approximates a probabilistic Gaussian process; corpus supports viability of Bayesian DL for medical uncertainty.
- **Break condition**: If the model architecture is too shallow or dropout probability is set too low, the variance across passes may fail to capture sufficient epistemic uncertainty, resulting in overconfident narrow distributions.

### Mechanism 2
- **Claim**: Inter-rater disagreement in training data serves as a latent signal for the model to calibrate its predictive uncertainty (entropy).
- **Mechanism**: The model is trained on majority vote labels. When features overlap significantly or when experts historically disagree, the loss landscape forces the model to output probabilities closer to 0.5. The MC Dropout mechanism then samples around this mean, producing a wider distribution (higher entropy) for contentious cases.
- **Core assumption**: The majority vote accurately reflects the "true" label, while the variance in expert opinion correlates with the difficulty of the specific case.
- **Evidence anchors**: Abstract states "model produces more uncertain distributions when pathologists disagree"; section 4.3 shows "Mean Entropy increases as the number of opposing pathologists increases"; corpus supports link between model uncertainty and difficulty signals.
- **Break condition**: If the training labels are noisy or biased, the model will learn to be "certain" about incorrect diagnoses, failing to align with true epistemic uncertainty.

### Mechanism 3
- **Claim**: Fitting a parametric Beta distribution to sampled probabilities allows for the extraction of a compact prior distribution for downstream Bayesian analysis.
- **Mechanism**: The MC Dropout samples are discrete probability points. The Method of Moments is used to fit these points to a Beta distribution, summarizing the high-dimensional uncertainty of the neural network into two interpretable parameters that can be used as priors.
- **Core assumption**: The true uncertainty distribution of the expert/event can be adequately approximated by a Beta distribution.
- **Evidence anchors**: Section 3.1 mentions "Method of Moments can be used to fit a Beta density function to the samples"; section 4.4 shows individual Beta distributions with different parameters; corpus has weak/missing support for this specific application.
- **Break condition**: If the distribution of samples is multi-modal, a single Beta distribution will fail to capture the bimodality, obscuring the true nature of the disagreement.

## Foundational Learning

- **Concept**: **Monte Carlo (MC) Dropout**
  - **Why needed here**: Standard Deep Learning outputs a single score (deterministic). MC Dropout is the specific technique used here to force the model to output a *range* of possibilities, which is the core product of this research.
  - **Quick check question**: Does dropout remain active during the *training* phase, the *inference* phase, or both in this architecture? (Answer: Both).

- **Concept**: **Aleatoric vs. Epistemic Uncertainty**
  - **Why needed here**: The paper explicitly targets the elicitation of uncertainty (specifically epistemic/knowledge uncertainty in the diagnosis example). Distinguishing these helps in understanding why the model behaves "conservatively."
  - **Quick check question**: If you added more training images, which type of uncertainty would likely decrease? (Answer: Epistemic).

- **Concept**: **Calibration**
  - **Why needed here**: A model can be "uncertain" but wrong. Calibration plots are used in Section 4.3 to prove that when the model says "80% confident," it is actually correct 80% of the time. Without this, the elicited distributions are meaningless.
  - **Quick check question**: If a model predicts "90% chance of cancer" for 100 patients, but only 10 actually have cancer, is the model well-calibrated? (Answer: No, it is overconfident).

## Architecture Onboarding

- **Component map**: Input (Histopathology Images) -> ResNet18 Backbone -> Dropout Layers (between residual blocks) -> Sigmoid Output Head -> $N$ Stochastic Forward Passes -> Sample Set -> Method of Moments -> Beta Distribution

- **Critical path**: The integration of Dropout layers into the ResNet blocks is the critical architectural deviation from standard classification. The training phase uses standard cross-entropy, but the inference phase requires a loop to collect samples.

- **Design tradeoffs**:
  - **BNN vs. MC Dropout**: The paper chooses MC Dropout over Bayesian Neural Networks (BNNs) with Variational Inference. Tradeoff: MC Dropout is much simpler and faster to implement but may provide a less precise approximation of the true posterior than full BNNs.
  - **Tabular vs. Image**: Using images prevents the information loss associated with tabular data but significantly increases computational requirements and model complexity.

- **Failure signatures**:
  - **Overconfidence**: The model produces narrow Beta distributions (e.g., Beta(100, 5)) for cases where experts strongly disagree. (Check calibration plots).
  - **Mode Collapse**: The entropy histogram shows all predictions clustered near 0.5 (the model learns to be "safe" rather than informative).
  - **Bimodality Mismatch**: The Method of Moments fits a unimodal Beta distribution to samples that are clearly bimodal (split decision), masking the conflict.

- **First 3 experiments**:
  1. **Sanity Check**: Train the ResNet18 on the dataset with dropout OFF during inference. Verify standard accuracy matches baseline (approx 78-79%).
  2. **Uncertainty Activation**: Enable MC Dropout during inference (e.g., 100 passes). Plot the **Entropy Histogram** (Figure 3a) to verify the model produces a mix of certain (low entropy) and uncertain (high entropy) predictions.
  3. **Agreement Correlation**: Group test set results by "Number of Opposing Pathologists." Calculate mean entropy for each group. Verify the positive correlation shown in Table 3 (More opposition â†’ Higher entropy).

## Open Questions the Paper Calls Out
None

## Limitations
- The method's efficacy is limited to cases where expert disagreement correlates with intrinsic data ambiguity; systematic biases may mislead the model
- The Beta distribution assumption is a simplification that may fail to capture bimodal expert opinions
- Computational overhead of MC Dropout, while less than full BNNs, still requires multiple forward passes per prediction

## Confidence
- **High Confidence**: The mechanism of using MC Dropout to generate probability distributions is well-established and empirically validated (Mechanism 1). The correlation between model entropy and expert disagreement is strongly supported by the experimental results (Mechanism 2).
- **Medium Confidence**: The Method of Moments for fitting Beta distributions to sampled probabilities is a standard statistical technique, but its adequacy for capturing complex, multi-modal expert disagreement is not fully validated in this context (Mechanism 3).
- **Low Confidence**: The generalizability of the method to domains outside histopathology, particularly those with significantly different data modalities or expert workflows, is an open question.

## Next Checks
1. **Bias Sensitivity Test**: Introduce controlled bias into the training labels and evaluate if the model's uncertainty estimates become misaligned with true epistemic uncertainty.
2. **Multi-Modal Agreement Test**: Create a synthetic dataset where experts are split 50/50 on ambiguous cases. Verify if the Beta distribution fit captures the bimodal nature of disagreement or collapses it into a single mode.
3. **Real-Time Performance Evaluation**: Implement the method in a simulated clinical workflow and measure the latency introduced by multiple forward passes required for MC Dropout. Assess if this latency is acceptable for the target application.