---
ver: rpa2
title: 'Hummus: A Dataset of Humorous Multimodal Metaphor Use'
arxiv_id: '2504.02983'
source_url: https://arxiv.org/abs/2504.02983
tags:
- metaphor
- caption
- metaphors
- image
- humor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HUMMUS, a novel dataset of humorous multimodal
  metaphor use, annotated by experts across 1,000 image-caption pairs from the New
  Yorker Caption Contest. The dataset leverages theories of humor (Incongruity Theory)
  and metaphor (Conceptual Metaphor Theory) to identify and explain humorous metaphors
  that span both visual and textual modes.
---

# Hummus: A Dataset of Humorous Multimodal Metaphor Use

## Quick Facts
- **arXiv ID**: 2504.02983
- **Source URL**: https://arxiv.org/abs/2504.02983
- **Reference count**: 40
- **One-line primary result**: HUMMUS dataset reveals state-of-the-art MLLMs struggle with multimodal metaphor understanding despite expert annotations

## Executive Summary
This paper introduces HUMMUS, a novel dataset of humorous multimodal metaphor use, annotated by experts across 1,000 image-caption pairs from the New Yorker Caption Contest. The dataset leverages theories of humor (Incongruity Theory) and metaphor (Conceptual Metaphor Theory) to identify and explain humorous metaphors that span both visual and textual modes. It includes detailed annotations such as conceptual metaphors, image bounding boxes, caption highlights, and explanations of humor. Experiments show that state-of-the-art multimodal language models (MLLMs) struggle with tasks involving metaphor identification, localization, and explanation, especially in integrating visual and textual information. Human performance remains superior, indicating room for model improvement. The dataset and code are publicly available for future research.

## Method Summary
The HUMMUS dataset contains 1,000 image-caption pairs from the New Yorker Caption Contest with expert annotations including conceptual metaphors (TARGET IS SOURCE), bounding boxes for visual elements, caption highlights (`<i>` tags), and humor explanations. Six tasks evaluate MLLM performance: Classification (metaphor presence), Naming (conceptual metaphor ID), ImageBbox (localization), ImageLabel (object naming), CaptionHL (text highlighting), and Explanation. Models are evaluated using multiclass F1, semantic similarity (LaBSE), IoU, Jaccard index, and ROUGE scores. The evaluation uses zero-shot prompting with specified templates across multiple MLLM architectures including GPT-4o, LLaVA-NeXT, and Qwen2-VL.

## Key Results
- MLLMs achieve high similarity on ImageLabel (0.60-0.95) but poor IoU on ImageBbox (0.13-0.25), indicating they can name objects but not localize them accurately
- Models show positive class bias, with F1 scores near zero for negative classification while achieving 0.61-0.74 on positive cases
- CaptionHL tasks show high recall (0.85-0.95) but low precision, suggesting models over-highlight non-metaphorical context
- Human performance significantly exceeds MLLM performance across all tasks, particularly for complex integration and explanation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Humor arises through incongruity-resolution mediated by cross-domain metaphorical mapping.
- Mechanism: An incongruous image element creates expectation violation → caption triggers metaphorical mapping between domains → incongruity is reinterpreted through source-domain logic → humor emerges from the unexpected but coherent resolution.
- Core assumption: Metaphor identification requires recognizing which conceptual domain each visual/textual element belongs to, then detecting the cross-domain mapping.
- Evidence anchors:
  - [abstract]: "We take inspiration from the Incongruity Theory of humor, the Conceptual Metaphor Theory"
  - [section 2]: "The caption resolves the incongruity by attributing the car crash to the couple's disastrous relationship, and humor arises from this witty, unexpected attribution"
  - [corpus]: Related work on multimodal metaphor (MultiCMET, MultiMET) similarly relies on domain mapping identification but lacks humor-theoretic framing.
- Break condition: If models cannot first identify the two domains separately in each modality, cross-domain integration will fail systematically.

### Mechanism 2
- Claim: MLLMs fail at multimodal metaphor tasks because they under-utilize visual information when processing image-caption pairs.
- Mechanism: Models receive both modalities but process the caption as primary signal → visual features are weakly integrated or treated as context → metaphor identification defaults to linguistic patterns → fails when metaphor is genuinely multimodal (not detectable from text alone).
- Core assumption: Effective multimodal metaphor processing requires explicit binding between visual regions and textual phrases that belong to the same conceptual domain.
- Evidence anchors:
  - [abstract]: "Our experiments show that current MLLMs still struggle with processing humorous multimodal metaphors, particularly with regard to integrating visual and textual information"
  - [section 6.4]: "there is not much difference between the models' performance in the multimodal and monomodal experiments"
  - [corpus]: HUMORCHAIN and related multimodal humor work show similar integration challenges, though focused on generation rather than understanding.
- Break condition: If visual encoding is insufficiently fine-grained (e.g., cannot localize the metaphor-related object), bounding box tasks will fail even when classification succeeds.

### Mechanism 3
- Claim: Metaphorical idioms can be "resurrected" when combined with visual context, creating humor from re-metaphorization.
- Mechanism: An idiom like "go through the roof" is normally processed as non-metaphorical → visual depiction literalizes the idiom's original metaphor → cross-domain mapping becomes active again → incongruity between literal and figurative readings generates humor.
- Core assumption: Idiom "deadness" is context-dependent; multimodal contexts can reactivate dormant metaphoricity.
- Evidence anchors:
  - [section 5]: "When the caption is combined with the image, however, the metaphorical mapping between AMOUNT and HEIGHT is resurrected"
  - [section 5]: "5% of metaphor samples where the humorous metaphor use concerns an idiom"
  - [corpus]: Limited direct coverage; The Medical Metaphors Corpus notes idioms as distinct from metaphors but doesn't address resurrection.
- Break condition: If models process idioms purely as fixed expressions without accessing underlying domain structure, they cannot explain multimodal humor involving idiom resurrection.

## Foundational Learning

- Concept: **Conceptual Metaphor Theory (CMT)**
  - Why needed here: The entire annotation scheme is built on identifying TARGET DOMAIN IS SOURCE DOMAIN mappings; without understanding that metaphor is a cognitive process (not just linguistic decoration), you cannot interpret the dataset labels or evaluation tasks.
  - Quick check question: Given "our marriage has gone off the track," can you identify the target domain, source domain, and explain why the mapping is unidirectional?

- Concept: **Incongruity-Resolution Theory of Humor**
  - Why needed here: The two-stage annotation process (find incongruity → check if caption resolves it via metaphor) directly implements this theory; understanding it clarifies why some non-metaphorical jokes exist and why metaphor is only one humor mechanism among others.
  - Quick check question: For an image showing a hippo answering a phone, what is the incongruity and how might a caption resolve it metaphorically versus non-metaphorically?

- Concept: **Multimodal Fusion in Vision-Language Models**
  - Why needed here: The core finding is that MLLMs fail at cross-modal integration; understanding where fusion happens (early vs. late, attention-based vs. concatenation) helps diagnose whether architectural limitations or training data gaps are responsible.
  - Quick check question: If a model performs identically with real images vs. text-only image descriptions, what does this imply about its fusion mechanism?

## Architecture Onboarding

- Component map: Image encoder (ViT-based) -> Text tokenizer -> Cross-attention fusion -> Task heads (classification, naming, localization, highlighting, explanation)
- Critical path: Image encoder must localize fine-grained objects → Cross-modal attention must bind image region → text phrase → Reasoning layer must infer domain assignment → Generation must verbalize cross-domain mapping
- Design tradeoffs:
  - Bounding box precision vs. label correctness: Models achieve ~0.6 similarity on ImageLabel but only 0.13-0.25 IoU on ImageBbox—they can name the object but not localize it accurately
  - Precision vs. recall in CaptionHL: Models have high recall (0.85-0.95) but low precision—they over-highlight, capturing non-metaphorical context
  - Prompt engineering stability: Smaller models (Qwen-7B) are highly prompt-sensitive; larger models (GPT-4o) are stable but still fail at negative classification
- Failure signatures:
  - False positive classification: Models default to "Yes" (metaphorical); F1 on negative cases is ~0.0-0.47 vs. 0.61-0.74 on positive
  - Speaker attribution errors: LLaVA assumes caption is spoken by visible human, missing ANIMALS ARE HUMANS metaphors
  - Isolated idiom detection: Models recognize idioms as metaphorical but fail to connect them to visual instantiations
- First 3 experiments:
  1. Chain-of-thought prompting: Instruct models to first describe image domains, then caption domains, then identify the cross-mapping—tests whether explicit domain separation improves naming accuracy
  2. Visual grounding ablation: Provide models with oracle bounding boxes (ground truth regions) and measure improvement in naming/explanation—if large improvement, the bottleneck is localization; if small, the bottleneck is reasoning
  3. Negative example augmentation: Fine-tune on augmented data with explicit non-metaphorical examples and measure F1 improvement on negative classification—tests whether the "Yes" bias is a data distribution issue or architectural limitation

## Open Questions the Paper Calls Out
- **Can fine-tuning or few-shot prompting significantly improve MLLM performance on the HUMMUS benchmark?** The authors note these techniques are "beyond the scope of the current study" and relied solely on zero-shot prompting.
- **Do state-of-the-art reasoning models (e.g., OpenAI o1) outperform standard MLLMs in integrating visual and textual metaphor cues?** The authors note that reasoning models released after their experiments (like DeepSeek R1 or o1) were not included.
- **Does chain-of-thought (CoT) prompting improve model performance by better mimicking the human annotation process?** The error analysis suggests performance could improve if models are instructed via CoT to process images before integrating text.

## Limitations
- The dataset is limited to humorous metaphors from New Yorker Caption Contest, potentially biasing toward Western cultural contexts and professional-level humor
- Expert annotations rely on subjective interpretation of humor and metaphor, though inter-rater reliability was controlled
- Evaluation focuses on English-language MLLMs, leaving cross-linguistic generalization unexplored
- The distinction between "humorous" and "non-humorous" metaphors is based on expert judgment rather than measurable affective responses

## Confidence
- **High Confidence**: MLLMs struggle with multimodal metaphor integration tasks, particularly bounding box localization and negative classification
- **Medium Confidence**: The HUMMUS dataset provides comprehensive multimodal metaphor annotations suitable for training future models
- **Low Confidence**: Claims about specific failure modes (e.g., speaker attribution errors, idiom resurrection) are based on qualitative observations from limited model outputs

## Next Checks
1. **Cross-cultural validation**: Test HUMMUS-trained models on multimodal metaphors from different cultural sources (e.g., international meme datasets) to assess cultural bias and generalization
2. **Human evaluation correlation**: Compare model performance metrics with human perception studies measuring actual humor appreciation and metaphor comprehension
3. **Ablation on visual grounding**: Conduct controlled experiments where ground truth bounding boxes are provided to models, measuring improvement in metaphor identification to isolate localization from reasoning bottlenecks