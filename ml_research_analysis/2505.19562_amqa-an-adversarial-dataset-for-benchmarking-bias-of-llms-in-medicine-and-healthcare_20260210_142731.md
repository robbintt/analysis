---
ver: rpa2
title: 'AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and
  Healthcare'
arxiv_id: '2505.19562'
source_url: https://arxiv.org/abs/2505.19562
tags:
- bias
- clinical
- medical
- accuracy
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FairMedQA, an adversarial dataset for benchmarking
  bias in large language models (LLMs) for medical question answering. Built from
  801 USMLE clinical vignettes, the dataset includes 4,806 adversarial variants that
  systematically alter demographic attributes (race, sex, socioeconomic status) while
  preserving clinical content.
---

# AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare

## Quick Facts
- **arXiv ID:** 2505.19562
- **Source URL:** https://arxiv.org/abs/2505.19562
- **Reference count:** 38
- **Key outcome:** Introduces FairMedQA, an adversarial dataset that reveals up to 19 percentage points of demographic bias in LLMs on medical QA, with GPT-5 showing the best fairness.

## Executive Summary
This paper introduces FairMedQA, an adversarial dataset for benchmarking bias in large language models (LLMs) for medical question answering. Built from 801 USMLE clinical vignettes, the dataset includes 4,806 adversarial variants that systematically alter demographic attributes (race, sex, socioeconomic status) while preserving clinical content. Using a multi-agent generation framework and human review, the dataset exposes bias by creating realistic but challenging scenarios. Benchmarking 12 representative LLMs, the study finds substantial accuracy disparities—up to 19 percentage points—between privileged and unprivileged groups, with GPT-5 achieving the best fairness (CFR 94%, AD 3%). Compared to the CPV benchmark, FairMedQA reveals 15% larger accuracy gaps, demonstrating superior sensitivity in bias detection. The findings show that fairness and diagnostic accuracy can improve simultaneously, challenging the "alignment tax" hypothesis. The dataset and code are publicly released to support reproducible research and advance trustworthy, bias-aware medical AI.

## Method Summary
FairMedQA is constructed from 801 USMLE clinical vignettes, expanded into 4,806 adversarial variants targeting Race (White/Black), Sex (Male/Female), and SES (High/Low Income). A multi-agent framework (Generation, Fusion, Validation) using GPT-4o systematically injects demographic context into neutralized vignettes, followed by human review. The dataset is evaluated using counterfactual fairness metrics (CFR and AD) across 12 representative LLMs, revealing substantial demographic accuracy disparities.

## Key Results
- GPT-5 achieves the best fairness (CFR 94%, AD 3%), while other models show up to 19 percentage points disparity between privileged and unprivileged groups.
- FairMedQA detects 15% larger accuracy gaps compared to CPV benchmark, demonstrating superior sensitivity to bias.
- The study challenges the "alignment tax" hypothesis by showing that fairness and diagnostic accuracy can improve simultaneously.

## Why This Works (Mechanism)
FairMedQA exposes LLM bias by generating realistic adversarial scenarios that preserve clinical validity while systematically altering demographic attributes. The multi-agent framework ensures rigorous adversarial generation through iterative refinement, while human review maintains medical accuracy. The counterfactual fairness metrics provide principled quantification of demographic disparities, and the use of USMLE vignettes ensures medical relevance. The dataset's sensitivity to bias is demonstrated through larger accuracy gaps compared to existing benchmarks.

## Foundational Learning
- **Counterfactual Fairness:** Evaluating model performance across demographic groups by comparing predictions on counterfactual scenarios.
  - *Why needed:* To quantify demographic bias in medical AI systems.
  - *Quick check:* Verify that CFR metric calculations are consistent across demographic pairs.

- **Adversarial Generation:** Systematically modifying inputs to expose model weaknesses while preserving core task validity.
  - *Why needed:* To create challenging but realistic scenarios that reveal bias.
  - *Quick check:* Ensure generated vignettes maintain clinical accuracy after demographic modification.

- **Multi-agent Framework:** Using multiple specialized agents for generation, fusion, and validation of adversarial examples.
  - *Why needed:* To ensure systematic and rigorous adversarial example creation.
  - *Quick check:* Monitor pass rates per generation round to detect overly strict validation.

- **USMLE Vignette Structure:** Clinical case presentations with patient demographics, symptoms, and multiple-choice questions.
  - *Why needed:* Provides standardized, medically relevant context for bias evaluation.
  - *Quick check:* Validate that ground truth answers remain consistent across counterfactual variants.

## Architecture Onboarding

**Component Map:** USMLE vignettes -> Generation-Agent -> Fusion-Agent -> Validation-Agent -> Human Review -> FairMedQA dataset

**Critical Path:** Data Preparation -> Adversarial Generation -> Human Review -> Benchmarking

**Design Tradeoffs:** Adversarial generation using GPT-4o enables systematic bias exposure but introduces potential circularity; human review ensures medical validity but adds cost and complexity.

**Failure Signatures:**
- Clinical Drift: Fusion-Agent alters medical reasoning path while adding social context, invalidating ground truth
- Template Rejection: Validation-Agent overly strict, causing generation loops to fail

**3 First Experiments:**
1. Run adversarial generation pipeline on 10 sample vignettes to validate component integration
2. Benchmark current SOTA models (GPT-4o, Claude-3.5) on generated variants to establish baseline performance
3. Conduct human review validation on 50 random counterfactual pairs to verify clinical consistency

## Open Questions the Paper Calls Out
### Open Question 1
- **Question:** What specific debiasing techniques can effectively mitigate the accuracy disparities revealed by FairMedQA without inducing performance regression?
- **Basis in paper:** The conclusion explicitly states the results "underscore an urgent need for targeted debiasing techniques" to ensure safe clinical integration.
- **Why unresolved:** This study focuses exclusively on benchmarking and exposing bias in existing models rather than developing or testing mitigation strategies.
- **What evidence would resolve it:** Empirical results showing that applying specific algorithmic debiasing methods to a model (e.g., GPT-4.1) reduces the Accuracy Disparity (AD) metric on FairMedQA while maintaining or improving diagnostic accuracy.

### Open Question 2
- **Question:** Does the bias performance measured by FairMedQA generalize to clinical contexts outside the USMLE/Western paradigm?
- **Basis in paper:** The limitations section acknowledges that "USMLE questions may not fully capture the diversity of global healthcare systems or non-Western clinical contexts."
- **Why unresolved:** The dataset is derived entirely from US-based licensing exams, potentially limiting the ecological validity of the findings for global healthcare deployment.
- **What evidence would resolve it:** A comparative study constructing adversarial variants from non-Western medical datasets (e.g., clinical guidelines from Asia or Africa) and measuring if similar demographic biases persist.

### Open Question 3
- **Question:** To what extent do the bias patterns identified in FairMedQA's multiple-choice setting persist in open-ended clinical scenarios?
- **Basis in paper:** The limitations section notes that while multiple-choice allows for objective evaluation, it "can limit the external validity," and success in this format does not guarantee success in "more complex and realistic scenarios."
- **Why unresolved:** Real clinical interactions rarely offer multiple-choice options; the paper leaves open whether the lack of constrained options exacerbates or alters the expression of demographic bias.
- **What evidence would resolve it:** An evaluation of the same LLMs using the adversarial vignettes adapted for open-ended generation, assessing bias through semantic equivalence or clinical correctness rather than exact option matching.

## Limitations
- The reliance on GPT-4o for adversarial generation introduces potential circularity in bias evaluation, as the same model family is used for both generation and benchmarking.
- The selection of 801 vignettes from the original 1,273 USMLE corpus is not fully justified, raising concerns about potential selection bias.
- The temporal inconsistency between paper publication (2025) and claimed benchmarking of future model versions (GPT-5, Claude-4) introduces uncertainty about the actual models tested versus reported.

## Confidence
- **High confidence**: Counterfactual fairness methodology and metric definitions
- **Medium confidence**: Benchmark construction and dataset quality
- **Low confidence**: Real-world clinical applicability and temporal consistency of model versions

## Next Checks
1. Verify adversarial vignette generation preserves clinical validity by having independent medical experts validate a random sample of 50 paired counterfactuals
2. Replicate benchmark results using current production models (GPT-4o, Claude-3.5) to establish baseline performance absent future model claims
3. Conduct ablation studies removing demographic context to measure impact on medical reasoning accuracy and identify potential trade-offs between fairness and clinical performance