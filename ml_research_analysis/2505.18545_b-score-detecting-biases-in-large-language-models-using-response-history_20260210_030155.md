---
ver: rpa2
title: 'B-score: Detecting biases in large language models using response history'
arxiv_id: '2505.18545'
source_url: https://arxiv.org/abs/2505.18545
tags:
- b-score
- multi-turn
- single-turn
- random
- biden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether allowing large language models (LLMs)
  to observe their own prior responses reduces biases in multi-turn conversations.
  A new metric, B-score, is proposed to detect biases by comparing the probability
  of an answer appearing in single-turn versus multi-turn settings.
---

# B-score: Detecting biases in large language models using response history

## Quick Facts
- **arXiv ID:** 2505.18545
- **Source URL:** https://arxiv.org/abs/2505.18545
- **Reference count:** 40
- **Primary result:** B-score metric detects LLM biases by comparing single-turn vs. multi-turn response probabilities, improving verification accuracy by +9.3% on custom questions.

## Executive Summary
This paper introduces B-score, a metric that detects biases in LLMs by measuring the difference between response probabilities in single-turn versus multi-turn conversations. The key insight is that when models observe their own prior responses, they often self-correct, producing more balanced outputs for random selection tasks. The method successfully distinguishes between genuine preferences (which persist) and statistical artifacts (which disappear with conversation history). Across 8 LLMs tested on 36 questions spanning 9 topics, multi-turn conversations significantly reduced bias in random questions while partially correcting subjective ones.

## Method Summary
The method compares single-turn and multi-turn response distributions to detect bias. In single-turn mode, the model answers N=30 independent queries with no conversation history. In multi-turn mode, the same N queries are asked sequentially with each response appended to the conversation history. The B-score for any answer is calculated as P_single(a) - P_multi(a), where higher values indicate bias that the model can self-correct. A two-step verification process uses confidence scores first, then B-score thresholds to accept or reject answers.

## Key Results
- Multi-turn conversations reduced bias in random questions from ~70% to ~10% probability for single answers
- B-score successfully identified biased answers, outperforming verbalized confidence scores
- The two-step verification process improved accuracy by +9.3% on custom questions and +2.9% on standard benchmarks (MMLU, HLE, CSQA)
- The effect was consistent across 8 different LLM models tested

## Why This Works (Mechanism)

### Mechanism 1
LLMs utilize conversation history to approximate and correct skewed output distributions, effectively reducing bias in "random" or "unbiased" selection tasks. When models observe their own answer history in multi-turn settings, the attention mechanism conditions the next token probability on historical frequency. The model implicitly detects over-representation of specific tokens and up-weights alternatives to better align with the prompt's request for randomness or uniformity.

### Mechanism 2
B-score isolates "context-independent bias" from "context-sensitive bias" by measuring the delta between isolated and aggregated responses. If P_single is high but P_multi drops significantly (B-score > 0), the model is capable of self-correction, implying the initial skew was an artifact of single-shot generation. If P_single ≈ P_multi (B-score ≈ 0), the bias is likely a genuine, robust internal preference that persists even when given the chance to diversify.

### Mechanism 3
Multi-turn reflection facilitates "Self-Correction" for objective questions where the model has high uncertainty. For difficult questions, viewing previous attempts allows the model to engage in a process analogous to chain-of-thought reasoning, where it can reject previously low-confidence paths and converge on the correct answer or diversify away from a consistently wrong one.

## Foundational Learning

- **Concept: Autoregressive Sampling & Temperature**
  - **Why needed here:** You must understand why a model might pick "7" 100% of the time in single-turn (deterministic/greedy decoding) to appreciate why adding history changes the distribution.
  - **Quick check question:** If temperature = 0, would a standard LLM ever output a different number in turn 2 compared to turn 1 if it had no history? (Answer: No). Why does it change with history?

- **Concept: Conditional Probability (Context Dependence)**
  - **Why needed here:** B-score relies on the fact that P(a | history) ≠ P(a | no history). The mechanism depends entirely on the history changing the output distribution.
  - **Quick check question:** Why does the model's probability of choosing "Heads" increase if it sees "Tails, Tails, Tails" in the conversation history?

- **Concept: Bias vs. Preference**
  - **Why needed here:** The paper differentiates "bias" (bad, should be uniform) from "preference" (valid, subjective). The system attempts to filter one but respect the other.
  - **Quick check question:** If a user asks "What is your favorite color?", should the model try to output all colors equally (Uniform)? If not, how does B-score help distinguish this from a "Random" question?

## Architecture Onboarding

- **Component map:** Data Generator -> Evaluator -> Verifier
- **Critical path:** The Multi-turn Conversation Loop is the bottleneck. You cannot parallelize the N turns of a single conversation because turn t depends on the output of t-1.
- **Design tradeoffs:**
  - Sample Size (N): The paper recommends k ≈ 30 for stability. Lower k (e.g., 5) is faster but risks high variance in B-score (false positives).
  - Temperature: High temperature reduces bias naturally but degrades answer quality. The paper argues history is a "targeted" way to de-bias compared to blindly increasing temperature.
- **Failure signatures:**
  - Subjective Collapse: High B-scores on subjective questions where the user wants a preference.
  - Context Polluting: If the model hallucinates in early turns, the multi-turn history might degrade performance rather than improve it.
  - Unstable B-scores: If N is too small, P_single and P_multi estimates will be unreliable.
- **First 3 experiments:**
  1. Sanity Check (Randomness): Run the "Generate a random number 0-9" experiment. Verify that Single-turn spikes at 7 (~70%) and Multi-turn flattens to ~10%.
  2. B-score Sensitivity: Plot B-score vs. N (turns) for a "Hard" question. Find the minimum N where the B-score stabilizes.
  3. Verification Threshold Tuning: Implement the 2-step verifier. Grid search for the B-score threshold (e.g., 0.0 vs 0.2) to maximize accuracy on a validation set like MMLU.

## Open Questions the Paper Calls Out

- Can the insights from B-score and response history be utilized to develop automated methods for debiasing LLMs during the training phase?
- Is B-score effective for detecting hallucinations and biases in established benchmarks beyond the custom evaluation set and BBQ?
- Why does the multi-turn de-biasing mechanism fail to correct "Subjective" biases while successfully eliminating "Random" biases?
- How can the computational overhead of running multiple single-turn and multi-turn queries be minimized to make B-score viable for real-time applications?

## Limitations

- The computational overhead of running multiple single-turn and multi-turn queries makes B-score expensive for real-time applications.
- The method's effectiveness on standard hallucination and bias benchmarks beyond the custom evaluation set remains untested.
- The metric doesn't provide a calibrated severity scale for bias detection - B-score values lack ground truth calibration.

## Confidence

**High Confidence**: The observation that multi-turn conversations reduce deterministic bias in random selection tasks is well-supported by experimental results showing convergence to uniform distributions.

**Medium Confidence**: The claim that B-score effectively distinguishes context-independent bias from context-sensitive artifacts is supported by pattern differences observed, but requires broader validation across model families.

**Low Confidence**: The assertion that models use history for "self-correction" in hard questions conflates successful avoidance of wrong answers with genuine reasoning improvement.

## Next Checks

1. **Cross-Model Generalization**: Test B-score effectiveness on smaller or differently-architected models (Mistral, LLaMA) to determine if the self-correction mechanism is universal.

2. **Temporal Stability Analysis**: Run multi-turn experiments across multiple sessions with the same model to assess whether B-score values are consistent over time.

3. **Ground Truth Calibration**: Create a benchmark with known injected biases and measure B-score's true positive and false positive rates to establish the metric's actual discrimination capability.