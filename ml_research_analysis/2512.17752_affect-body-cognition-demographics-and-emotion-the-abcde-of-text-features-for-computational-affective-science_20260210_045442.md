---
ver: rpa2
title: 'Affect, Body, Cognition, Demographics, and Emotion: The ABCDE of Text Features
  for Computational Affective Science'
arxiv_id: '2512.17752'
source_url: https://arxiv.org/abs/2512.17752
tags:
- text
- social
- features
- language
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ABCDE, a large-scale dataset of over 400 million
  text utterances annotated with features relevant to computational affective science.
  The dataset includes social media, blog, book, and AI-generated texts annotated
  for affect, emotion, body part mentions, cognition, and demographics.
---

# Affect, Body, Cognition, Demographics, and Emotion: The ABCDE of Text Features for Computational Affective Science

## Quick Facts
- arXiv ID: 2512.17752
- Source URL: https://arxiv.org/abs/2512.17752
- Reference count: 0
- Key outcome: Large-scale dataset of 400M+ text utterances annotated with 136 features across Affect, Body, Cognition, Demographics, and Emotion for computational affective science research

## Executive Summary
This paper introduces ABCDE, a comprehensive dataset of over 400 million text utterances annotated with lexical features relevant to computational affective science. The dataset spans multiple domains including social media (Twitter, Reddit), books (Google Books Ngrams), blogs (Spinn3r), and AI-generated text from 15 different sources. Using lexicon-based matching and regular expressions, the authors annotate features including valence/arousal/dominance scores, emotion intensity, body part mentions, cognitive processes, and demographic information. The resource enables large-scale analysis of affect, embodiment, cognition, and demographics across diverse text sources, with findings showing distinct patterns across platforms and between human and AI-generated content.

## Method Summary
ABCDE is constructed by applying lexicon-based annotation and regex extraction to five source corpora: Twitter TUSC (45.2M tweets), Reddit Pushshift (78.6M posts), Google Books Ngrams (177.1M 5-grams), Spinn3r Blogs (34.2M posts), and 15 AI-generated datasets (68.9M instances). Features are computed using NRC VAD lexicon for affect, NRC Emotion Intensity and related lexicons for emotion, 292 curated body part terms, 98 cognitive verbs from Bloom's Taxonomy, and regex patterns with dictionary lookups for demographics. The pipeline tokenizes text, performs part-of-speech tagging via UniMorph, matches against lexicons, and outputs structured features including average scores, counts, and binary presence flags. The resulting dataset is available on HuggingFace with 136 feature columns per instance.

## Key Results
- Emotion words are most prevalent in Twitter (1.4%) compared to Reddit (0.4%), books (0.2%), blogs (0.4%), and AI text (0.01%)
- Body part mentions are highest in Reddit (1.2%) versus Twitter (0.3%), books (0.4%), blogs (0.2%), and AI text (0.03%)
- Cognition terms are more common in human-authored sources than AI-generated text, which systematically under-expresses cognitive vocabulary
- Reddit users show younger age demographics than Twitter users, with distinct occupation distributions across platforms
- AI-generated text shows significantly different affect and cognition patterns compared to human-authored content

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lexicon-based features function as statistically reliable proxies for aggregate population-level trends.
- **Mechanism:** The architecture substitutes complex model inference with high-coverage word-matching against curated intensity lexicons (e.g., NRC VAD). When analyzing longitudinal shifts in a corpus, the signal-to-noise ratio improves through statistical aggregation.
- **Core assumption:** The frequency of specific lexical items (e.g., "anxiety," "joy") maintains a stable correlation with the underlying psychological construct across the dataset.
- **Evidence anchors:** The authors aim to provide a "standardized, pre-computed resource" for "computational affective and social science"; lexicon-based methods are comparable and sufficient for aggregate-level analysis with correlation scores > 0.9 for temporal arcs.
- **Break condition:** The mechanism fails if applied to short, individual utterances where context and sarcasm are missing.

### Mechanism 2
- **Claim:** Demographic "self-disclosure" extraction via regex creates a high-precision but low-recall metadata layer for stratification.
- **Mechanism:** Instead of inferring demographics, the system uses regular expressions to capture explicit statements (e.g., "I am a [occupation]").
- **Core assumption:** Users who explicitly state demographic information are representative of the studied population, and regex accurately captures self-disclosure syntax.
- **Evidence anchors:** "We extract demographic attributes... using regular expression matching... [e.g.,] 'I am/work at/employed as [occupation]'" and "These reflect rates among disclosed posts... absence of gender mention should be treated as missing, not negative."
- **Break condition:** The mechanism breaks if users treat absence of regex match as "null" rather than "missing data."

### Mechanism 3
- **Claim:** Embodied cognition can be operationalized via "Body Part Mentions" to reveal physical-emotional correlates.
- **Mechanism:** The system tags anatomical unigrams/bigrams using possessive pronouns to distinguish self-reports from general descriptions.
- **Core assumption:** Body part mentions correlate with the author's focus on physical states rather than purely descriptive discourse.
- **Evidence anchors:** "We identify Body Part Mentions (BPMs)... capturing mentions with possessive pronouns (e.g., 'my heart', 'her hand')" and "BPMs can reveal how people linguistically connect to their bodies."
- **Break condition:** The signal degrades if applied to medical subreddits or fitness blogs where body mentions are clinical rather than affective.

## Foundational Learning

- **Concept:** **Lexical Resource (Lexicon) vs. Embedding**
  - **Why needed here:** ABCDE relies entirely on dictionary lookups rather than dense vector representations.
  - **Quick check question:** If "sick" appears in slang meaning "great," will NRC VAD score it as positive or negative? (Answer: Likely negative)

- **Concept:** **Aggregate vs. Instance-Level Analysis**
  - **Why needed here:** The paper explicitly warns against using this for single-sentence prediction.
  - **Quick check question:** Can you use this dataset to detect if a specific user is depressed based on one tweet? (Answer: No)

- **Concept:** **Positive-Only Labels**
  - **Why needed here:** Demographic features rely on self-disclosure; absence doesn't mean negation.
  - **Quick check question:** In 1M users, only 10k disclose religion. How do you label the remaining 990k? (Answer: As "unknown/missing")

## Architecture Onboarding

- **Component map:** Raw text sources (Twitter, Reddit, Books, AI-gen) -> Tokenization -> Lexicon Lookup (NRC VAD, Emotion Intensity) + Regex Extraction (Demographics) -> Structured table with 136 feature columns -> HuggingFace Hub storage

- **Critical path:**
  1. Select domain (e.g., Reddit)
  2. Filter for instances with target demographic regex hits
  3. Aggregate lexicon scores (e.g., average Joy intensity for age > 30 group)

- **Design tradeoffs:** Lexicon-based methods sacrifice instance-level accuracy for computational efficiency and transparency; regex extraction prioritizes precision over recall for demographics.

- **Failure signatures:** Lexicon version mismatches cause feature value discrepancies; tokenization differences affect word matching accuracy.

- **First experiments:**
  1. Compute average valence scores across platforms to verify expected emotional patterns
  2. Compare body part mention frequencies between Reddit and Twitter to identify platform differences
  3. Analyze emotion intensity distributions in AI-generated vs human-authored text

## Open Questions the Paper Calls Out

- **Open Question 1:** How do ABCDE feature patterns generalize across languages and cultures beyond English-speaking, North American populations?
- **Open Question 2:** How do lexicon-based aggregate measurements compare to machine learning-based instance-level annotations for the same constructs?
- **Open Question 3:** How does semantic drift affect the validity of static lexicons when applied to longitudinal data spanning centuries?
- **Open Question 4:** What mechanisms explain the stark divergence between AI-generated and human-authored text in emotion word usage?

## Limitations
- English-only content with North American cultural bias limits generalizability
- Static lexicons may not capture semantic drift across centuries or domains
- Regex-based demographic extraction provides only explicit self-disclosure, creating systematic gaps
- AI-generated texts introduce confounds as their patterns reflect training data rather than genuine human experience

## Confidence

- **High confidence:** Dataset construction methodology is clearly specified and lexicon-based annotation is valid for aggregate analysis
- **Medium confidence:** Demographic extraction via regex is precise but incomplete by design; platform comparisons assume uniform lexicon coverage
- **Low confidence:** Cross-platform affect/emotion pattern comparisons require careful interpretation due to platform-specific demographics

## Next Checks
1. Test lexicon coverage by sampling 1,000 random instances from each source and manually verifying key affect/emotion term capture
2. Validate demographic extraction by comparing regex-captured age distributions against known platform demographics
3. Cross-validate body part mention patterns by examining medical/health subreddits separately to distinguish embodied affect from clinical discourse