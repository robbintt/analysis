---
ver: rpa2
title: Taming Transformer Without Using Learning Rate Warmup
arxiv_id: '2505.21910'
source_url: https://arxiv.org/abs/2505.21910
tags:
- block
- matrix
- training
- attention
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies spectral energy concentration (SEC) of $Wq^\top
  Wk$ in Transformer attention layers as the root cause of model crash. When SEC occurs,
  attention maps become sparse and low-rank, leading to training instability.
---

# Taming Transformer Without Using Learning Rate Warmup

## Quick Facts
- **arXiv ID:** 2505.21910
- **Source URL:** https://arxiv.org/abs/2505.21910
- **Reference count:** 40
- **Primary result:** Identifies spectral energy concentration (SEC) as root cause of Transformer training crashes without warmup; proposes AdamW2 optimizer with learning rate bounding based on singular value ratios to enable stable training without warmup, achieving performance comparable to standard AdamW with warmup.

## Executive Summary
This paper identifies spectral energy concentration (SEC) of $W_q^\top W_k$ in Transformer attention layers as the root cause of training instability when using learning rate warmup. The authors prove through theoretical analysis using Kronecker products and matrix calculus that low-rank inputs and weight matrices propagate to cause SEC, leading to sparse, low-rank attention maps and training collapse. They propose a novel optimization strategy (AdamW2) that bounds learning rate based on the ratio $\frac{\sigma_1(\nabla W_t)}{\sigma_1(W_{t-1})}$, preventing rapid singular value growth. Experiments on ViT, Swin-Transformer, and GPT demonstrate stable training without learning rate warmup, achieving performance comparable to standard AdamW with warmup.

## Method Summary
The paper proposes AdamW2, a modified AdamW optimizer that prevents model crash by bounding learning rate updates using Weyl's Inequality. The method estimates the largest singular values of weight matrices and their gradients via power iteration (2-3 iterations), then truncates the learning rate if $\alpha_t \times \frac{\sigma_1(\nabla W_t)}{\sigma_1(W_{t-1})} > \tau$, where $\tau$ is a hyperparameter (typically 0.003-0.01). This ensures that spectral norm growth remains controlled, preventing the malignant entropy collapse caused by SEC. The optimizer maintains the bias-corrected moments of AdamW but adds the spectral norm estimation and conditional learning rate truncation step.

## Key Results
- Successfully trains ViT-B, Swin-S, and GPT models without learning rate warmup, achieving performance comparable to AdamW with warmup (ViT-B: 80.2% vs 80.5%, Swin-S: 83.0% vs 83.1%)
- Eliminates model crash in ViT training without warmup, where standard AdamW fails after ~2000 iterations
- Scales to large models up to 1B parameters (ViT-g) while maintaining stability
- Ablation study shows method is robust to $\tau$ values in [0.003, 0.01] range

## Why This Works (Mechanism)

### Mechanism 1: Spectral Energy Concentration (SEC) Detection as Training Instability Indicator
The paper claims that Spectral Energy Concentration (SEC) of the $W_q^\top W_k$ matrix in Transformer attention layers is the root cause of model crash during training without learning rate warmup. When SEC occurs, the largest singular value $\sigma_1(W_q^\top W_k)$ grows rapidly, causing attention maps to become sparse and low-rank. This leads to malignant entropy collapse where gradients stop flowing effectively, causing training instability. The method monitors this by tracking the ratio $\frac{\sigma_1(\nabla W_t)}{\sigma_1(W_{t-1})}$ as an indicator of potential SEC.

### Mechanism 2: Weyl's Inequality for Learning Rate Bounding (AdamW2)
The AdamW2 optimizer bounds the effective learning rate based on the singular value ratio $\frac{\sigma_1(\nabla W_t)}{\sigma_1(W_{t-1})}$ to prevent rapid singular value growth, ensuring stable weight updates. Weyl's Inequality states $\sigma_1(W_1 + W_2) \leq \sigma_1(W_1) + \sigma_1(W_2)$. Applied to weight updates $W_t = W_{t-1} - \alpha_t \nabla W_t$, this gives $\sigma_1(W_t) \leq \sigma_1(W_{t-1}) + \alpha_t \sigma_1(\nabla W_t)$. To maintain steady updates, the paper introduces Rule 1: $\|W_t\|_2 \leq (1+\tau)\|W_{t-1}\|_2$, which requires $\alpha_t \leq \tau \frac{\sigma_1(W_{t-1})}{\sigma_1(\nabla W_t)}$. If the preset learning rate violates this, it is truncated.

### Mechanism 3: Low-Rank Jacobian Propagation and Gradient Magnification
The paper theoretically proves that low-rank input matrices and weight matrices propagate through the attention mechanism, creating low-rank Jacobians that concentrate gradient energy and lead to SEC. Using matrix calculus (Proposition 1), the Jacobian $\frac{\partial \text{vec}(P)}{\partial \text{vec}(W_q^\top W_k)} = X^\top \otimes X^\top$. Since $\text{rank}(X^\top \otimes X^\top) = \text{rank}(X)^2$, low-rank inputs produce low-rank Jacobians, concentrating gradient updates. Additionally, $\frac{\partial \text{vec}(P)}{\partial \text{vec}(X)}$ is proportional to $W_q^\top W_k$, so large $\sigma_1(W_q^\top W_k)$ magnifies input gradients, exacerbating instability.

## Foundational Learning

- **Concept:** Singular Value Decomposition (SVD) and Spectral Norm
  - Why needed here: The entire method revolves around monitoring and bounding the largest singular value (spectral norm) of weight matrices and their updates.
  - Quick check question: Given a matrix $A$, what is its spectral norm and how does it relate to its largest singular value?

- **Concept:** Kronecker Product and Vectorization in Matrix Calculus
  - Why needed here: The theoretical analysis (Proposition 1) uses Kronecker products to derive Jacobians for matrix-valued functions, key to understanding gradient propagation in attention.
  - Quick check question: For matrices $A$ and $B$, what is the rank of the Kronecker product $A \otimes B$?

- **Concept:** Learning Rate Warmup and its Role in Training Stability
  - Why needed here: The paper's goal is to eliminate warmup. Understanding why warmup is used provides context for the problem being solved.
  - Quick check question: What is the typical behavior of a learning rate schedule with warmup, and why might omitting it cause instability in Transformers?

## Architecture Onboarding

- **Component map:** Replace AdamW optimizer with AdamW2 -> Add power iteration for spectral norm estimation -> Monitor singular value ratios -> Conditionally truncate learning rate
- **Critical path:** Replace the existing optimizer (e.g., AdamW) with AdamW2, set hyperparameter $\tau$ (default 0.004-0.01), and ensure the training loop accommodates additional power iteration computations (minimal overhead: 2-3 matrix-vector multiplications per layer per step)
- **Design tradeoffs:**
  1. **$\tau$ Value Selection:** Lower $\tau$ ensures stability but may slow convergence; higher $\tau$ allows faster updates but risks instability. Performance is relatively robust to $\tau$ in [0.003, 0.01] for tested models.
  2. **Power Iteration Iterations:** More iterations yield more accurate estimates but increase cost; 2-3 iterations are sufficient.
  3. **Scope of Application:** The method is applied to all weight matrices. Applying it selectively might reduce overhead but is untested.
- **Failure signatures:**
  1. **Loss Divergence:** Sudden, rapid increase in training loss, often accompanied by a spike in $\sigma_1(W_q^\top W_k)$ (e.g., to >200,000 in ViT).
  2. **Attention Map Collapse:** Visualization shows attention maps becoming sparse and low-rank, with entropy approaching 0.
  3. **Normalization Parameter Explosion:** $\|\gamma_1\|_2$ and $\|\beta_1\|_2$ increasing dramatically.
- **First 3 experiments:**
  1. **Baseline Reproduction:** Train a small Transformer (e.g., ViT-B) with standard AdamW and no warmup to reproduce the model crash. Observe failure signatures (loss divergence, SEC).
  2. **AdamW2 Integration and Ablation:** Replace AdamW with AdamW2 (no warmup) on the same model. Vary $\tau$ (e.g., 0.003, 0.004, 0.005) to find a stable value. Monitor actual learning rate curves to confirm truncation.
  3. **Scale-Up Test:** Apply AdamW2 (no warmup) to a larger model (e.g., ViT-L or GPT-S) and compare performance against a baseline using AdamW with warmup. Ensure comparable results as shown in Table 1.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does AdamW2 remain stable and effective when training Transformer models with parameters significantly exceeding 1 billion?
- **Basis in paper:** [explicit] The authors validate the method on ViT-g (1B parameters) and GPT (774M) but acknowledge that scaling to large scales is "increasingly gaining more attention" and usually requires warmup.
- **Why unresolved:** Instability modes (like SEC) might manifest differently or more severely at massive scales (e.g., 7B+ parameters) where numerical precision and hardware constraints differ.
- **What evidence would resolve it:** Successful training runs of Large Language Models (LLMs) >7B parameters using AdamW2 without warmup, matching baseline performance.

### Open Question 2
- **Question:** Is the sensitivity of the threshold $\tau$ dependent on the model dimension ($d_{model}$) or batch size?
- **Basis in paper:** [inferred] The ablation study (Figure 6) tests $\tau$ values for specific models (Swin-S, GPT-S), but the paper does not analyze if the optimal $\tau$ scales with model width or data batch size.
- **Why unresolved:** Spectral norms of weights and gradients change with network width and batch size; a fixed $\tau$ might require retuning for different configurations.
- **What evidence would resolve it:** A scaling law analysis showing the relationship between optimal $\tau$, model width, and batch size.

### Open Question 3
- **Question:** Can AdamW2 eliminate the need for architectural stability interventions like QK-Norm or $\mu$P (maximal update parametrization)?
- **Basis in paper:** [inferred] The introduction contrasts the method with architectural modifications (ReZero, DeepNet), but the paper does not test if AdamW2 renders these structural changes redundant or if they are orthogonal improvements.
- **Why unresolved:** It is unclear if the optimizer-level fix fully addresses the underlying Lipschitz continuity issues that architectural modifications target.
- **What evidence would resolve it:** Ablation studies combining or comparing AdamW2 against QK-Norm and $\mu$P on standard benchmarks.

## Limitations
- The method requires additional computational overhead for power iteration to estimate singular values, though this is relatively small (2-3 matrix-vector multiplications per layer per step).
- The optimal threshold $\tau$ may require tuning for different model architectures and scales, though the method shows robustness within a reasonable range.
- The theoretical analysis focuses on single-head attention, with claims that the generalization to multi-head is straightforward but not explicitly proven.

## Confidence
- **Core mechanism (SEC and AdamW2):** High
- **Experimental results on tested models:** High
- **Theoretical analysis completeness:** Medium (single-head focus, some assumptions about gradient behavior)
- **Scalability to 10B+ parameter models:** Low (not tested)

## Next Checks
1. Implement AdamW2 with power iteration (2-3 iterations) and test on ViT-B with no warmup, comparing against standard AdamW crash.
2. Verify the learning rate truncation mechanism by monitoring actual vs scheduled learning rates during training.
3. Perform ablation study on $\tau$ parameter (0.003, 0.004, 0.005) to find optimal value for stable training.