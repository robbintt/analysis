---
ver: rpa2
title: 'Grokking vs. Learning: Same Features, Different Encodings'
arxiv_id: '2502.01739'
source_url: https://arxiv.org/abs/2502.01739
tags:
- grokking
- learning
- ising
- compressibility
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Grokking and steady learning lead to the same dataset features
  but different model compressions. In modular addition, a compressive training regime
  emerges with a linear trade-off between model loss and compressibility, enabling
  up to 25x compression compared to 5x in grokking.
---

# Grokking vs. Learning: Same Features, Different Encodings

## Quick Facts
- arXiv ID: 2502.01739
- Source URL: https://arxiv.org/abs/2502.01739
- Reference count: 40
- Key outcome: Grokking and steady learning lead to same dataset features but different model compressions, with a compressive training regime emerging with a linear trade-off between model loss and compressibility.

## Executive Summary
This paper investigates whether grokking (delayed generalization) and steady learning converge to the same or different internal representations. Using modular addition and the Ising model as testbeds, the authors find that both regimes learn the same task-relevant features (Fourier modes for addition, energy/magnetization for Ising) but with different encodings and compression properties. Notably, a "compressive regime" emerges in steady learning where models can be compressed up to 25x more than grokked models, enabled by tuning initialization scale and weight decay. Information-geometric measures reveal that grokking dynamics follow straight trajectories in model space, while steady learning shows more varied development patterns.

## Method Summary
The authors train neural networks on modular addition (MLP) and the Ising model (CNN) under different initialization scales to induce either grokking or steady learning. They then analyze learned representations using feature correlation coefficients, inverse participation ratios (IPR) for Fourier mode localization, and Fisher Information Metric (FIM) trajectory measures. Compressibility is quantified via magnitude pruning sweeps. The study compares these metrics across training regimes to understand representational differences.

## Key Results
- Grokking and steady learning converge to the same task-relevant features (r ≈ 0.99 correlation for Ising energy, comparable IPR for Fourier modes)
- Steady learning in a compressive regime achieves up to 25x parameter compression versus 5x for grokking
- Grokking dynamics follow straight trajectories in FIM space, indicating weight-decay dominance
- Modular addition shows feature refinement during the plateau while Ising shows none

## Why This Works (Mechanism)

### Mechanism 1: Feature Convergence Across Training Regimes
The task structure constrains the space of viable features. Both regimes optimize toward representations that minimize loss on the underlying task distribution, not training set idiosyncrasies. The optimization pressure toward generalizable features is structurally determined by the task geometry, not the path taken. The task admits a canonical feature representation (energy/magnetization for Ising, Fourier modes for modular addition) that is discoverable from the data distribution.

### Mechanism 2: Compressive Regime Through Initialization Scale Tuning
Lower initialization scales produce models with sparser, more structured weight distributions. The optimizer finds flatter minima where redundant parameters can be pruned without harming performance. This effect is specific to certain batch sizes and weight decay settings. Large batch sizes (>200) eliminate this regime entirely, while very high weight decay (3×10⁻⁴) pushes models toward breakdown where Fourier representation collapses.

### Mechanism 3: Straight-Line Trajectories via Weight Decay Dominance
In grokking, weight decay dominates the early plateau dynamics, causing the model to move toward the origin in parameter space along a nearly linear trajectory. This is reflected in high FIM-cosine similarity (~1.0) between consecutive update steps and between steps and the origin vector. For modular addition, the trajectory is less straight (lower cosine similarity), indicating task-dependent dynamics and more feature development during plateau.

## Foundational Learning

- **Concept: Grokking as Phase Transition**
  - Why needed here: The paper defines grokking operationally (t_grok > t_train) and uses it as the primary comparison axis. Understanding grokking as a delayed generalization phenomenon after memorization is essential.
  - Quick check question: Can you explain why grokking is induced by larger initialization scales rather than smaller ones?

- **Concept: Fisher Information Metric and Model Space Geometry**
  - Why needed here: The paper's novel contribution uses FIM to define trajectory measures (step magnitude, cosine similarity) in model space. The FIM captures how sensitive the model output distribution is to parameter changes.
  - Quick check question: Why would straight-line motion in FIM space indicate weight decay dominance rather than feature learning?

- **Concept: Pruning as Compression Proxy**
  - Why needed here: Compressibility is operationalized through magnitude pruning. The area under the accuracy-vs-pruning curve quantifies how many parameters are truly necessary.
  - Quick check question: If magnitude pruning achieves 25x compression, what does that imply about the trained model's effective capacity?

## Architecture Onboarding

- **Component map:** Modular Addition MLP (2-layer, 174193 params) -> Ising CNN (2 conv + 1 FC, 6798 params) -> Adam optimizer with weight decay; initialization scale w₀ as primary control knob -> Interpretability measures (correlation coefficients, IPR) + FIM trajectory measures (step magnitude, cosine similarity) + pruning curves

- **Critical path:**
  1. Set weight multiplier w₀ to induce grokking (w₀ > 3 for Ising, w₀ > 0.4 for modular addition)
  2. Train to convergence; record model checkpoints
  3. Compute feature correlations (Ising) or IPR values (modular addition)
  4. Run magnitude pruning sweep; compute compressibility integral
  5. Compute FIM diagonals at checkpoints; calculate trajectory measures

- **Design tradeoffs:**
  - Batch size 64 vs 200: Smaller batch enables compressive regime but noisier dynamics
  - Weight decay 3e-5 vs 3e-4: Higher decay yields extreme compression (25x) but risks Fourier breakdown
  - Magnitude vs Fisher pruning: Magnitude is simpler; Fisher captures non-linear importance but requires FIM computation

- **Failure signatures:**
  - Fourier representation collapse: IPR drops by order of magnitude; model near failure threshold
  - No compressive regime: Check batch size; if >200, regime disappears
  - FIM computation failure: Requires sufficient data samples for expectation estimation; numerical issues with near-zero probabilities

- **First 3 experiments:**
  1. Reproduce the compressive regime: train modular addition MLP at w₀ = 0.1, 0.2, 0.4 with batch size 64; verify linear loss-compressibility trade-off appears
  2. Compare FIM trajectories: compute FIM-cosine similarity for consecutive steps in both regimes; confirm grokking shows higher peak similarity (~1.0) vs steady learning
  3. Test task dependence: apply the same analysis pipeline to Ising CNN; verify no compressive regime emerges and grokking shows weight-decay-dominated trajectory

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are there tasks that can only be solved via grokking, or conversely, only via steady learning?
- Basis in paper: [explicit] The conclusion explicitly asks, "it is unknown if there are tasks which can only be grokked, or, vice versa, can only be steadily learned."
- Why unresolved: The authors demonstrated that steady learning often yields better compression on the tested tasks, but they did not identify tasks where grokking is a necessity rather than just a slower alternative.
- What evidence would resolve it: Identification of a specific dataset or architecture where grokking achieves generalization while steady learning permanently fails.

### Open Question 2
- Question: Does the "compressive regime" of steady learning utilize neural superposition to achieve high compression factors?
- Basis in paper: [explicit] The conclusion suggests it would be "particularly interesting to investigate in further work whether models in this regime display significantly higher superposition than models trained outside of it."
- Why unresolved: The paper observes a novel linear trade-off between loss and compressibility in this regime but does not explain the internal mechanism (e.g., superposition) that enables it.
- What evidence would resolve it: Comparative analysis of feature superposition in models trained within the compressive regime versus those trained with standard hyperparameters.

### Open Question 3
- Question: Why is there no observed relationship between model compressibility and the Local Learning Coefficient (LLC)?
- Basis in paper: [inferred] The authors state in Appendix D: "Surprisingly, we also find no relationship between the compressibility and the model degeneracy, as measured by the local learning coefficient," which contradicts the intuition that degenerate models should be more compressible.
- Why unresolved: This is an unexpected negative result; the paper does not offer a theoretical explanation for why the measure of degeneracy (LLC) fails to predict pruning efficiency.
- What evidence would resolve it: Theoretical analysis or empirical studies linking singularities (measured by LLC) to weight magnitude and sparsity.

## Limitations

- The observed feature convergence may represent superficial similarity rather than true representational equivalence, as IPR values (0.43-0.53) indicate only moderate localization
- The compressive regime appears highly sensitive to hyperparameter choices, vanishing entirely at batch size 200
- FIM-based trajectory analysis assumes the Fisher metric captures meaningful optimization geometry, which remains unproven for deep learning landscapes

## Confidence

- **High confidence**: Feature correlation results for Ising task (r ≈ 0.99), FIM trajectory measures showing weight-decay dominance in grokking, and the basic grokking vs steady learning distinction
- **Medium confidence**: Fourier mode IPR comparisons, the existence of a compressive regime in modular addition, and the 25x vs 5x compression ratio comparison
- **Low confidence**: The claim that grokking and steady learning learn "the same" features in any rigorous mathematical sense, and the generalization of the compressive regime to other tasks beyond modular addition

## Next Checks

1. **Cross-task compressibility testing**: Apply the full analysis pipeline to a third task (e.g., MNIST classification or a different algorithmic task) to determine whether the compressive regime is modular addition-specific or represents a broader phenomenon. Test multiple batch sizes systematically to map the regime boundaries.

2. **Alternative feature representation analysis**: Implement independent feature analysis methods (e.g., centered kernel alignment or mutual information estimation) to verify whether the same task-relevant features are truly learned across regimes, beyond the correlation and IPR measures used in the paper.

3. **FIM trajectory ablation study**: Remove weight decay entirely and compare FIM trajectories between grokking and steady learning. Additionally, test whether the straight-line motion persists when using different optimizers (SGD vs Adam) or when training with label noise to determine if the phenomenon is weight-decay-specific or represents a more general property of delayed generalization.