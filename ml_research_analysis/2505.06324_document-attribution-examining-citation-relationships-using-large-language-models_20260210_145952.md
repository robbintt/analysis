---
ver: rpa2
title: 'Document Attribution: Examining Citation Relationships using Large Language
  Models'
arxiv_id: '2505.06324'
source_url: https://arxiv.org/abs/2505.06324
tags:
- attribution
- zero-shot
- layer
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of attribution in document-based
  tasks, where Large Language Models (LLMs) need to trace their outputs back to source
  documents. The authors propose two techniques: (1) a zero-shot textual entailment
  approach, framing attribution as a binary classification problem using the flan-ul2
  model, and (2) an attention-based classification method using the flan-t5-small
  model to explore the role of attention layers in improving attribution.'
---

# Document Attribution: Examining Citation Relationships using Large Language Models

## Quick Facts
- arXiv ID: 2505.06324
- Source URL: https://arxiv.org/abs/2505.06324
- Authors: Vipula Rawte; Ryan A. Rossi; Franck Dernoncourt; Nedim Lipka
- Reference count: 7
- One-line primary result: Zero-shot textual entailment achieves 73.8% ID and 83.43% OOD F1 for document attribution

## Executive Summary
This paper addresses the challenge of document attribution in LLM outputs, where models must trace claims back to source documents. The authors propose two approaches: a zero-shot textual entailment method and an attention-based classification technique. The zero-shot method achieves strong F1 scores of 73.8% for in-distribution data and 83.43% for out-of-distribution data, outperforming baseline approaches. The attention-based method shows improved performance across most transformer layers, with notable exceptions in middle layers.

## Method Summary
The paper proposes two attribution methods: (1) a zero-shot textual entailment approach that frames attribution as binary classification using flan-ul2, and (2) an attention-based classification method using flan-t5-small that extracts attention weights from each transformer layer for binary classification. The zero-shot method requires no training data but uses computational resources, while the attention method requires training a fully connected layer but offers interpretability through layer-wise analysis.

## Key Results
- Zero-shot textual entailment achieves 73.8% F1 on in-distribution data and 83.43% F1 on out-of-distribution data
- Attention-based method shows improved F1 scores across most layers, with lower false positives and negatives
- Middle attention layers (4, 8-11) show degraded performance with F1 scores dropping to 0
- The flan-ul2 model demonstrates superior generalization capability across both ID and OOD settings

## Why This Works (Mechanism)

### Mechanism 1: Zero-shot Textual Entailment Framing
Framing attribution as a textual entailment problem enables effective zero-shot transfer for binary classification. The model receives a CLAIM and REFERENCE, then determines via YES/NO whether the reference logically supports the claim. This leverages pre-existing entailment reasoning capabilities in instruction-tuned models.

### Mechanism 2: Attention Layer Feature Extraction
Attention weights from specific transformer layers encode attribution-relevant signals that improve classification over baseline. Earlier and final layers (1-3, 12) outperform baseline; middle layers (4, 8-11) underperform.

### Mechanism 3: Scale-Dependent Generalization
Larger instruction-tuned models (flan-ul2, 20B) generalize better to out-of-distribution attribution tasks. Greater model capacity and broader instruction tuning improve entailment reasoning across diverse claim-reference patterns.

## Foundational Learning

- **Textual Entailment**: Binary relation where S1 entails S2 if S1 logically supports S2. Why needed: The entire zero-shot method depends on understanding entailment as a binary relation.
- **Zero-shot Transfer in Instruction-Tuned LLMs**: Models performing attribution without task-specific training, leveraging broad language understanding. Why needed: The approach relies on models generalizing to attribution without fine-tuning.
- **Transformer Attention Layer Semantics**: Understanding what layers capture is essential since layers 1-3 and 12 work while 4 and 8-11 fail. Why needed: The attention-based method extracts features per layer.

## Architecture Onboarding

- **Component map**: Input (CLAIM+REFERENCE) -> Zero-shot path (Prompt template -> flan-ul2 -> YES/NO output) OR Attention path (flan-t5-small -> extract attention weights per layer -> FC layer -> binary classification)
- **Critical path**: Format claim and reference using the exact prompt template, then either query flan-ul2 directly or extract attention weights from target layer and train FC classifier
- **Design tradeoffs**: Zero-shot requires no training data but may underperform fine-tuned approaches; attention method offers interpretability but shows layer-dependent instability
- **Failure signatures**: Very high FN rate in zero-shot baseline (LFQA: FN=86.9%), attention layers 4 and 8-11 produce F1=0, high FP rates on certain datasets
- **First 3 experiments**: (1) Replicate zero-shot entailment on Stanford-GenSearch subset, (2) Train attention-based classifiers on layers 1, 3, 12 vs. layer 4 using LFQA subset, (3) Run zero-shot with flan-t5-small vs. flan-ul2 on ID subset

## Open Questions the Paper Calls Out

1. **Fine-tuning Performance**: Can supervised fine-tuning significantly improve attribution performance over the zero-shot textual entailment baseline? The authors plan to overcome computational limitations for future work by conducting fine-tuning experiments.

2. **Attention Analysis with Larger Models**: Does the efficacy of attention-based attribution classification scale to larger, state-of-the-art LLMs? The authors aim to use more advanced LLMs to perform a deeper analysis of attention layers.

3. **Middle Layer Performance**: What causes the performance degradation in specific attention layers (e.g., layer 4 and layers 8-11), and can these layers be adapted to improve results? The paper reports the empirical failure but does not investigate the internal representations that led to these results.

## Limitations
- Computational constraints prevented fine-tuning approaches, limiting ability to establish whether zero-shot methods are truly optimal
- Extreme variability in attention layer performance (layers 4 and 8-11 fail completely with F1=0) indicates unstable feature extraction
- The strong OOD performance advantage over ID data is counterintuitive and may suggest dataset-specific artifacts

## Confidence
- High confidence in mechanism 1 (Zero-shot Textual Entailment): The approach is straightforward and performance metrics are clearly reported
- Medium confidence in mechanism 2 (Attention Layer Feature Extraction): Layer-wise analysis is detailed but extreme failures in middle layers suggest incomplete understanding
- Medium confidence in scale generalization claims: OOD vs ID performance differences are reported but causal factors are not fully explored

## Next Checks
1. Replicate the zero-shot method with controlled temperature settings (0.0) to test whether output consistency affects the OOD performance advantage
2. Conduct ablation studies on attention layers by training separate classifiers on combined layer outputs (1-3, 12) vs. individual layers to understand interaction effects
3. Perform error analysis on high-FP cases (HAGRID: 42.79%) to identify systematic failure patterns and determine if they stem from dataset characteristics or model limitations