---
ver: rpa2
title: Scaling Algorithm Distillation for Continuous Control with Mamba
arxiv_id: '2506.13892'
source_url: https://arxiv.org/abs/2506.13892
tags:
- learning
- context
- tasks
- in-context
- reinforcement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Algorithm Distillation (AD) enables in-context reinforcement learning
  (ICRL) by modeling across-episodic training histories with a causal transformer,
  but its quadratic complexity limits it to short-horizon tasks. This work leverages
  Mamba, a Selective Structured State Space (S6) model with linear scaling, to scale
  AD to continuous control environments with long time horizons.
---

# Scaling Algorithm Distillation for Continuous Control with Mamba

## Quick Facts
- arXiv ID: 2506.13892
- Source URL: https://arxiv.org/abs/2506.13892
- Authors: Samuel Beaussant; Mehdi Mounsif
- Reference count: 39
- Mamba-based Algorithm Distillation achieves better asymptotic performance than Decision Transformer on continuous control tasks with long horizons

## Executive Summary
Algorithm Distillation (AD) enables in-context reinforcement learning by modeling across-episodic training histories, but its quadratic complexity limits it to short-horizon tasks. This work leverages Mamba, a Selective Structured State Space (S6) model with linear scaling, to scale AD to continuous control environments with long time horizons. Mamba-based AD achieves better asymptotic performance than a similarly sized Decision Transformer across four continuous control tasks (Reacher-Goal, Pusher-Goal, Half-Cheetah-Vel, Ant-Dir), with improvements ranging from slight to significant. Longer context lengths improve performance in more complex tasks, highlighting the importance of modeling long-range dependencies. Mamba-AD is competitive with online meta-RL (MQL) and outperforms offline meta-RL (MACAW) baselines, despite training purely offline.

## Method Summary
The method involves two stages: first, generating learning trajectories using source RL algorithms (PPO/SAC/DroQ depending on environment), then training a Mamba model autoregressively via behavior cloning to predict actions conditioned on downsampled cross-episodic subsequences. Each token represents a concatenated transition tuple (s_t, a_t, r_t, s_{t+1}). The model uses 6-8 Mamba S6 layers with 384-512 dimensional embeddings and is trained with MSE loss. During inference, no gradient updates occur - the model populates its own context by interacting with the environment, demonstrating in-context policy improvement.

## Key Results
- Mamba-AD achieves better asymptotic performance than Decision Transformer across all four tested tasks
- Longer context lengths improve performance in more complex tasks (Half-Cheetah-Vel requires full-history context)
- Mamba-AD is competitive with online meta-RL (MQL) and outperforms offline meta-RL (MACAW) baselines
- Improvements range from slight to significant depending on task complexity

## Why This Works (Mechanism)

### Mechanism 1
Mamba's linear scaling in sequence length enables modeling of cross-episodic training histories that transformers cannot efficiently process. Selective Structured State Space (S6) models use a recurrent formulation (h_k = Ah_{k-1} + Bx_k) that scales O(n) in sequence length rather than O(n²) attention. This allows AD to maintain context across multiple episodes without computational explosion. The core assumption is that longer cross-episodic context is necessary for effective in-context RL in complex tasks with extended time horizons.

### Mechanism 2
Algorithm Distillation induces in-context RL by learning to imitate the policy improvement operator of a source RL algorithm through autoregressive sequence modeling. The model is trained via behavior cloning (MSE loss on continuous actions) to predict actions conditioned on learning histories. Correct prediction requires implicitly approximating credit assignment and exploration-exploitation trade-offs present in source trajectories. The core assumption is that the source algorithm's learning dynamics (exploration patterns, credit assignment) are recoverable from observable state-action-reward sequences.

### Mechanism 3
Optimal context length correlates with task complexity; more complex environments require longer histories for effective ICRL. Complex tasks (higher-dimensional state-action spaces, longer horizons) require longer credit assignment chains. Sub-sampled full learning histories preserve the policy improvement trajectory across episodes better than truncated contexts. The core assumption is that the relationship between context length and task complexity is monotonic but task-specific.

## Foundational Learning

**State Space Models (SSMs) and Selective S6**
Why needed here: Mamba's architecture is built on S6 layers; understanding the recurrent state update (h_k = Ah_{k-1} + Bx_k) is essential for debugging sequence modeling failures.
Quick check question: Can you explain why S6 achieves linear scaling while attention is quadratic?

**Meta-RL vs. In-Context RL**
Why needed here: AD performs meta-RL without gradient updates at test time; distinguishing in-context adaptation from in-weights adaptation clarifies what AD actually learns.
Quick check question: What is the difference between MAML-style meta-learning and in-context learning during inference?

**Cross-Episodic Credit Assignment**
Why needed here: The paper emphasizes that effective ICRL requires context spanning multiple episodes to capture policy improvement signals.
Quick check question: Why would a single-episode context fail to enable in-context policy improvement?

## Architecture Onboarding

**Component map:**
Input: Transition tokens c_i = (s_t, a_t, r_t, s_{t+1})
↓
Embedding layer (concatenate transition → single token)
↓
Mamba S6 layers (6-8 layers, 384-512 dim model)
↓
Output head → continuous action prediction
↓
Loss: MSE between predicted and ground-truth actions

**Critical path:**
1. Generate learning trajectories using source RL algorithm (PPO/SAC/DroQ depending on environment)
2. Downsample trajectories by factor k (4-10 depending on task)
3. Sample cross-episodic subsequences of length c during training
4. Train autoregressively to predict actions given history

**Design tradeoffs:**
- **Sub-sampling rate k**: Higher k reduces compute but may sparse-ify learning signal; paper found k=4-10 optimal depending on task
- **Context length vs. compute**: Longer contexts improve complex tasks but require more memory; transformer baseline fails beyond certain lengths
- **Token representation**: Paper concatenates full transitions into single tokens (vs. per-element tokens) to fit 3× more context

**Failure signatures:**
- ICRL plateaus near zero-shot performance → context too short or sub-sampling too aggressive
- High variance in test performance → source trajectories may be unstable (SAC/DroQ volatility)
- Transformer baseline fails on Ant-Dir → suggests attention mechanism cannot capture long-range dependencies in high-dimensional tasks

**First 3 experiments:**
1. Replicate Reacher-Goal (simplest task) with both Mamba-AD and Decision Transformer to verify implementation; expect small performance edge for Mamba
2. Ablate context length on Half-Cheetah-Vel (where only full-history context works); confirm that short contexts fail to induce ICRL
3. Test inference speed: measure wall-clock time for Mamba vs. Decision Transformer on same sequence length; verify linear vs. quadratic scaling claim

## Open Questions the Paper Calls Out
None

## Limitations
- Limited empirical validation: Only four continuous control tasks tested, limiting generalizability
- Source algorithm dependence: Different source algorithms per environment without ablation studies
- Statistical significance: Performance improvements reported without statistical testing

## Confidence

**High Confidence**: Mamba-AD achieves better asymptotic performance than Decision Transformer on the tested tasks. This is directly observable from the results and the architectural advantage of linear scaling is well-established.

**Medium Confidence**: Mamba-AD enables effective ICRL in complex continuous control tasks. While demonstrated on four tasks, the sample size is limited and the mechanism connecting context length to task complexity needs more empirical validation.

**Low Confidence**: Mamba-AD is competitive with online meta-RL (MQL) and outperforms offline meta-RL (MACAW) baselines. These comparisons depend heavily on the specific implementations and hyperparameters of the baselines, which aren't fully specified.

## Next Checks

1. **Ablation study on source algorithms**: Train Mamba-AD using the same source algorithm (e.g., PPO) across all four environments to isolate architectural effects from source algorithm effects.

2. **Statistical significance analysis**: Perform t-tests or bootstrap confidence intervals on the performance differences between Mamba-AD and Decision Transformer across the 30 test runs to quantify the reliability of observed improvements.

3. **Context length sensitivity**: Systematically vary context length and sub-sampling rate k across all tasks (not just Half-Cheetah-Vel) to establish quantitative relationships between task complexity metrics and optimal context parameters.