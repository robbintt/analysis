---
ver: rpa2
title: 'DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented
  Generation'
arxiv_id: '2505.17058'
source_url: https://arxiv.org/abs/2505.17058
tags:
- knowledge
- graph
- answer
- do-rag
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DO-RAG, a hybrid framework that combines knowledge
  graph construction with retrieval-augmented generation to improve accuracy and reasoning
  in domain-specific QA. The system automates extraction of entities and relationships
  from multimodal documents, integrates graph-based and semantic search for context
  retrieval, and applies a refinement step to reduce hallucinations.
---

# DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2505.17058
- Source URL: https://arxiv.org/abs/2505.17058
- Reference count: 32
- Outperforms baseline RAG frameworks by up to 33.38% in composite score

## Executive Summary
DO-RAG is a hybrid QA framework that combines knowledge graph construction with retrieval-augmented generation to improve accuracy and reasoning in domain-specific technical documents. The system automates extraction of entities and relationships from multimodal documents using a four-agent pipeline, integrates graph-based and semantic search for context retrieval, and applies a refinement step to reduce hallucinations. Evaluated on database and electrical engineering datasets with 245 questions each, DO-RAG achieves near-perfect contextual recall (1.0) and over 94% answer relevancy, outperforming baseline RAG frameworks by up to 33.38% in composite score.

## Method Summary
The framework ingests multimodal technical documents, extracts structured knowledge using a four-agent pipeline (High-Level, Mid-Level, Low-Level, Covariate), constructs a knowledge graph in Neo4j, performs hybrid retrieval combining graph traversal and vector search with α-weighted fusion, and generates answers through a three-stage process (naive prompt → refinement → condensation). The system uses PostgreSQL with pgvector for document storage, Redis for caching, and is evaluated using RAGAS and DeepEval metrics on expert-curated questions across two technical domains.

## Key Results
- Near-perfect contextual recall (1.0) achieved across all tested models
- Over 94% answer relevancy maintained on both database and electrical engineering datasets
- Outperforms baseline RAG frameworks by up to 33.38% in composite score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-level agentic extraction improves knowledge graph completeness for technical documents.
- Mechanism: Four specialized agents operate at different abstraction layers to capture structural elements, domain entities, fine-grained relationships, and attributes respectively. Deduplication via cosine similarity prevents redundancy while synopsis nodes reduce graph complexity.
- Core assumption: Hierarchical agent specialization yields more complete extraction than single-agent approaches; LLM-based extraction reliably captures domain relationships from multimodal inputs.
- Evidence anchors: [abstract] "employs an agentic chain-of-thought architecture to automatically extract structured relationships from multimodal documents"; [section III.B] "the pipeline includes four specialized agents operating at different abstraction levels"; [corpus] Related KG-RAG systems (KG2QA, M³KG-RAG) similarly use structured extraction but lack DO-RAG's multi-level agent hierarchy; limited comparative evidence on agent-based vs. single-stage extraction quality.
- Break condition: If documents lack clear structural hierarchy or contain predominantly unstructured prose, agent specialization may not provide advantages over unified extraction.

### Mechanism 2
- Claim: Hybrid retrieval (graph traversal + vector search) improves contextual recall over pure vector-based approaches.
- Mechanism: Queries are decomposed into sub-queries via intent analysis. KG retrieval identifies relevant entities through embedding similarity, then multi-hop traversal expands context. This graph-derived evidence rewrites and disambiguates the query before vector search. Final score combines both sources: S = α · max_sim(Q, Ci) + (1 − α) · R(GQ).
- Core assumption: Graph structure captures relationships that semantic similarity misses; α weighting is domain-appropriate; multi-hop traversal improves rather than dilutes relevance.
- Evidence anchors: [abstract] "fuses graph and vector retrieval results to generate context-aware responses"; [Table II] Contextual Recall improved from 0.964–0.977 (vector-only) to 1.000 (KG-enhanced) across DeepSeek models; [corpus] GFM-RAG and CDF-RAG similarly demonstrate hybrid retrieval gains; corroborating evidence from DSRAG shows document-derived KGs improve domain-specific retrieval.
- Break condition: If graph quality is poor (missing entities, incorrect relations), traversal amplifies noise rather than signal. α misconfiguration could overweight unreliable graph paths.

### Mechanism 3
- Claim: Staged generation with grounded refinement reduces hallucinations.
- Mechanism: Three-stage generation: (1) naive prompt constrains answer to retrieved evidence, (2) refinement prompt restructures and validates, (3) condensation aligns tone. System returns "I do not know" when evidence is insufficient. Follow-up questions are generated based on refined answers.
- Core assumption: Explicit constraints in prompts reduce model tendency to fabricate; staged processing catches errors that single-pass generation misses; "I do not know" capability is reliably triggered.
- Evidence anchors: [abstract] "hallucination mitigation via grounded refinement"; [section III.D] "The output is passed through a refinement prompt that restructures and validates the answer"; [corpus] Limited external evidence on staged refinement effectiveness; CDF-RAG proposes causal feedback loops for similar goals but empirical comparison unavailable.
- Break condition: If refinement prompts are poorly designed, they may introduce new hallucinations or over-constrain valid responses. Creative models (e.g., DeepSeek-R1) may resist grounding constraints—Table II shows Faithfulness declined 5.6% with KG for DeepSeek-R1 despite gains on other metrics.

## Foundational Learning

- Concept: Knowledge Graph Construction (entities, relations, attributes, confidence weights)
  - Why needed here: Core infrastructure enabling structured retrieval; understanding node-edge representations is prerequisite for debugging extraction quality and traversal behavior.
  - Quick check question: Given a technical manual excerpt, can you identify at least 3 entity types and 2 relation types that should be extracted?

- Concept: Retrieval-Augmented Generation (RAG) architecture
  - Why needed here: DO-RAG extends standard RAG with KG integration; must understand baseline vector retrieval before appreciating hybrid fusion benefits.
  - Quick check question: Explain why pure semantic similarity retrieval might fail on a query requiring multi-hop reasoning across document sections.

- Concept: Multi-agent LLM orchestration
  - Why needed here: Extraction pipeline coordinates four specialized agents; understanding agent roles and handoff patterns is essential for troubleshooting construction failures.
  - Quick check question: What information must the Mid-Level Agent pass to the Low-Level Agent for successful fine-grained relationship extraction?

## Architecture Onboarding

- Component map:
  - Document Ingestion -> Chunking with Metadata -> pgvector Storage
  - 4-Agent Extraction Pipeline -> Neo4j Graph Store -> Deduplication + Synopsis
  - Intent Analyzer -> KG Entity Matching + Multi-hop Traversal -> Query Rewriting -> Vector Search -> Fusion
  - Naive Generation -> Refinement -> Condensation -> Follow-up Question Generator
  - PostgreSQL v16.4 (pgvector) + Redis v7.2.5 + MinIO + ClickHouse v25.4.3 + LangFuse v3.29.0

- Critical path:
  1. Document ingestion quality directly impacts KG completeness—garbage in, garbage out
  2. KG entity extraction accuracy determines hybrid retrieval effectiveness
  3. Retrieval fusion (α parameter) must be tuned per domain; default may not generalize
  4. Refinement prompt design is the last defense against hallucination

- Design tradeoffs:
  - Completeness vs. complexity: Synopsis nodes reduce graph size but may obscure fine-grained distinctions
  - Recall vs. precision: Multi-hop traversal improves recall but risks including tangentially related nodes
  - Speed vs. accuracy: Multi-agent extraction is computationally intensive; real-time updates require batch processing optimization
  - Model creativity vs. grounding: Highly creative models (DeepSeek-R1) show lower faithfulness despite KG constraints

- Failure signatures:
  - Contextual Recall < 0.95: Likely KG construction gap—check entity extraction coverage
  - Faithfulness declining with KG enabled: Model overriding constraints—tighten refinement prompts or switch to less creative model
  - Slow retrieval (>2s latency): Check multi-hop depth limit; consider caching frequent traversal paths
  - "I do not know" over-triggered: Vector search threshold too strict or KG entity matching failing

- First 3 experiments:
  1. Ablation on α parameter: Run retrieval with α ∈ {0.2, 0.5, 0.8} on held-out queries; measure recall/precision tradeoff to identify domain-optimal balance.
  2. Agent-level extraction audit: Sample 20 documents; manually verify entities/relations extracted at each agent level; identify which agent contributes most errors.
  3. Refinement prompt stress test: Construct 50 adversarial queries designed to trigger hallucinations; compare single-pass vs. staged generation error rates; iterate on prompt constraints.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of the multi-agent extraction pipeline be optimized to support real-time knowledge updates in large-scale deployments?
- Basis in paper: [explicit] The Discussion section states that "computational overhead of multi-agent extraction and hybrid retrieval... remains significant for real-time updates in large-scale deployments."
- Why unresolved: The current implementation focuses on accuracy and retrieval quality but acknowledges latency bottlenecks during dynamic updates.
- What evidence would resolve it: Benchmarks showing extraction and indexing latency under load, or the implementation of distributed processing/adaptive caching maintaining accuracy with reduced latency.

### Open Question 2
- Question: Can stricter prompt engineering effectively mitigate the "creative deviations" and faithfulness decline observed in reasoning-focused models like DeepSeek-R1?
- Basis in paper: [explicit] The paper notes DeepSeek-R1 exhibited a faithfulness decline (-5.6%) due to "creative deviations" and lists "stricter prompt engineering" as a future focus.
- Why unresolved: The current refinement step reduced hallucinations for some models but failed to prevent reasoning models from prioritizing internal logic over graph-grounded facts.
- What evidence would resolve it: A comparative analysis of DeepSeek-R1's faithfulness scores using standard versus stricter grounding prompts on the same dataset.

### Open Question 3
- Question: How does DO-RAG performance degrade when handling rare or adversarial edge-case queries compared to the expert-curated benchmarks?
- Basis in paper: [explicit] The Discussion notes the dataset "limited to 245 questions per domain, may not capture rare or edge-case queries, potentially limiting generalizability."
- Why unresolved: The high performance metrics (near-perfect recall) are derived from expert-curated questions which may reflect common rather than boundary-case usage patterns.
- What evidence would resolve it: Evaluation results on a dataset specifically designed to include ambiguous, out-of-distribution, or adversarial queries.

## Limitations
- Computational overhead of multi-agent extraction and hybrid retrieval remains significant for real-time updates in large-scale deployments
- Model creativity dependence creates tension: DeepSeek-R1 showed 5.6% faithfulness decline despite KG constraints
- Dataset limited to 245 expert-curated questions per domain, may not capture rare or adversarial edge cases

## Confidence
- High confidence: Contextual recall improvement (from 0.964-0.977 to 1.000 across models) and overall metric gains (up to 33.38% composite score)
- Medium confidence: Hallucination reduction claims due to limited external validation and the DeepSeek-R1 counterexample
- Medium confidence: Agent-based extraction superiority, as corroborating KG-RAG systems lack direct comparative analysis

## Next Checks
1. **α parameter sensitivity analysis**: Systematically vary α across 0.2, 0.5, 0.8 on held-out queries to quantify recall/precision tradeoffs and identify domain-optimal settings.
2. **Agent-level extraction audit**: Manually verify entities/relationships extracted by each agent across 20 sampled documents to identify bottlenecks and quality gaps.
3. **Adversarial hallucination stress test**: Construct 50 challenging queries designed to trigger fabrication; compare single-pass versus staged generation error rates to empirically validate refinement effectiveness.