---
ver: rpa2
title: 'RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal
  Knowledge Graph Completion'
arxiv_id: '2505.17794'
source_url: https://arxiv.org/abs/2505.17794
tags:
- sampling
- hits
- recipe-tkg
- temporal
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RECIPE-TKG addresses the challenge of improving temporal knowledge
  graph completion when historical evidence is sparse or indirect. It introduces a
  three-stage framework: rule-based multi-hop history sampling to retrieve structurally
  diverse context, contrastive fine-tuning with lightweight adapters to shape relational
  semantics, and test-time semantic filtering to refine predictions based on embedding
  similarity.'
---

# RECIPE-TKG: From Sparse History to Structured Reasoning for LLM-based Temporal Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2505.17794
- Source URL: https://arxiv.org/abs/2505.17794
- Reference count: 40
- Primary result: Up to 30.6% relative improvement in Hits@10 over prior LLM-based methods

## Executive Summary
RECIPE-TKG tackles the challenge of temporal knowledge graph completion when historical evidence is sparse or indirect. The framework introduces a three-stage approach that leverages rule-based multi-hop history sampling, contrastive fine-tuning with lightweight adapters, and test-time semantic filtering. By structuring context retrieval and shaping relational semantics, it significantly improves prediction accuracy and coherence over existing LLM-based methods, particularly in low-context scenarios.

## Method Summary
RECIPE-TKG operates through a three-stage framework. First, it uses rule-based multi-hop history sampling to retrieve structurally diverse and temporally relevant context from the KG. Second, it applies contrastive fine-tuning with lightweight adapters to the LLM, shaping relational semantics using annotated triples. Finally, at test time, it applies semantic filtering to refine predictions based on embedding similarity, improving factual consistency. This approach addresses sparsity by enhancing the LLM's reasoning capacity with structured, context-aware guidance.

## Key Results
- Achieves up to 30.6% relative improvement in Hits@10 over prior LLM-based methods
- Outperforms baselines across four benchmark datasets (ICEWS, GDELT)
- Generates more semantically coherent predictions in low-context settings

## Why This Works (Mechanism)
RECIPE-TKG's strength lies in its structured approach to sparse historical evidence. By using rule-based multi-hop sampling, it retrieves diverse and relevant context paths that standard methods might miss. Contrastive fine-tuning with lightweight adapters allows the LLM to internalize relational patterns without heavy retraining. Semantic filtering at test time ensures predictions align with learned embeddings, reducing noise and improving coherence. Together, these stages compensate for sparse history by enriching context and refining reasoning.

## Foundational Learning
- **Temporal Knowledge Graphs (TKGs)**: Graphs with time-stamped edges; needed to model dynamic relationships; quick check: verify timestamp granularity and schema
- **Multi-hop Reasoning**: Traversal across multiple edges; needed to capture indirect evidence; quick check: confirm hop limits and sampling rules
- **Contrastive Learning**: Learning by comparing positive/negative examples; needed to shape relational semantics; quick check: inspect contrastive pairs and loss function
- **Adapter-based Fine-tuning**: Lightweight parameter updates; needed to adapt LLMs without full retraining; quick check: verify adapter architecture and bottleneck size
- **Embedding Similarity Filtering**: Ranking by vector proximity; needed to refine predictions at test time; quick check: confirm embedding model and similarity metric
- **Rule-based History Sampling**: Handcrafted traversal rules; needed to ensure structural diversity; quick check: validate rule coverage and edge case handling

## Architecture Onboarding
**Component Map**: Multi-hop Sampler -> Contrastive Adapter -> Semantic Filter -> LLM Predictor
**Critical Path**: History retrieval → adapter fine-tuning → prediction → embedding-based filtering
**Design Tradeoffs**: Rule-based sampling offers interpretability but may miss novel patterns; adapters save compute vs full fine-tuning but may limit capacity; embedding filtering adds inference cost but improves precision
**Failure Signatures**: Over-reliance on rules may bias toward frequent patterns; contrastive loss may collapse if negatives are too easy; embedding filtering may drop valid but semantically distant facts
**First Experiments**: 1) Ablation: remove semantic filtering to measure impact; 2) Stress test: evaluate on artificially sparsified KG subsets; 3) Comparison: swap rule-based sampler with learned alternative

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may not generalize to truly sparse or dynamic real-world scenarios beyond benchmark datasets
- Rule-based history sampling is handcrafted and may not scale to larger or more complex graphs
- Contrastive fine-tuning requires annotated triples, limiting deployment in low-resource settings

## Confidence
- **High confidence**: Core framework design and experimental results are technically sound and well-supported
- **Medium confidence**: Relative improvements are substantial but depend on specific protocols and may vary with KG scale
- **Low confidence**: Qualitative claims about "semantic coherence" lack rigorous quantification

## Next Checks
1. Test RECIPE-TKG on datasets with controlled levels of historical sparsity to validate robustness claims
2. Conduct ablation studies isolating the contribution of the rule-based sampler against learned alternatives
3. Compare embedding-based semantic filtering against alternative ranking methods like ComplEx or TransE to assess true source of improvement