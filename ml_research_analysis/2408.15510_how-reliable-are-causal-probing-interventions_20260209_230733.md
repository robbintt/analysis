---
ver: rpa2
title: How Reliable are Causal Probing Interventions?
arxiv_id: '2408.15510'
source_url: https://arxiv.org/abs/2408.15510
tags:
- interventions
- reliability
- completeness
- methods
- selectivity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a general framework for evaluating causal
  probing interventions based on completeness (fully transforming targeted properties)
  and selectivity (minimally impacting non-targeted properties), with reliability
  defined as their harmonic mean. The framework uses validation probes to measure
  intervention effects, enabling systematic comparisons across different intervention
  types (linear/nonlinear, removal/counterfactual).
---

# How Reliable are Causal Probing Interventions?

## Quick Facts
- arXiv ID: 2408.15510
- Source URL: https://arxiv.org/abs/2408.15510
- Authors: Marc Canby; Adam Davies; Chirag Rastogi; Julia Hockenmaier
- Reference count: 36
- Primary result: Introduces framework evaluating causal probing interventions based on completeness and selectivity, finding nonlinear counterfactual methods more reliable than linear methods across most LLM layers

## Executive Summary
This paper introduces a general framework for evaluating causal probing interventions based on completeness (fully transforming targeted properties) and selectivity (minimally impacting non-targeted properties), with reliability defined as their harmonic mean. The framework uses validation probes to measure intervention effects, enabling systematic comparisons across different intervention types (linear/nonlinear, removal/counterfactual). Experiments across multiple LLMs (BERT, GPT-2, Pythia, Llama) show that all methods exhibit a completeness-selectivity tradeoff, with more reliable and complete interventions having greater impact on model task performance. Notably, nonlinear counterfactual methods are substantially more reliable than linear methods across most models and layers, suggesting that linear representation hypotheses may not generalize to intermediate layers of modern LLMs.

## Method Summary
The framework evaluates causal probing interventions by measuring their completeness (ability to transform target property distributions) and selectivity (ability to preserve non-target properties) using separately-trained validation probes. The method involves training interventional probes on 40% of data, validation probes on disjoint 40% with decorrelated target and non-target property labels, and applying interventions with hyperparameters swept across ranges. Completeness is measured as TV distance to target distribution, selectivity as TV distance to original distribution for non-target properties, and reliability as their harmonic mean. The framework is applied to subject-verb agreement task across BERT, GPT-2, Pythia, and Llama models using linear methods (INLP, RLACE, AlterRep) and gradient-based nonlinear counterfactuals (FGSM, PGD, AutoAttack).

## Key Results
- All intervention methods show a clear completeness-selectivity tradeoff, with reliability (harmonic mean) maximized at intermediate hyperparameter values
- Nonlinear counterfactual interventions are substantially more reliable than linear methods across most models and layers, with reliability gaps reaching 0.2-0.3
- Higher reliability interventions correlate with greater impact on model task performance (ΔTask accuracy)
- The completeness-selectivity tradeoff persists across different model architectures and sizes, though the optimal reliability point varies

## Why This Works (Mechanism)

### Mechanism 1: Completeness-Selectivity Tradeoff
- Claim: Causal interventions exhibit an inherent tradeoff where increasing intervention strength improves completeness (target transformation) but degrades selectivity (collateral damage to non-targeted properties).
- Mechanism: Intervention hyperparameters (ε for gradient methods, rank r for removal methods, α for counterfactuals) control perturbation magnitude. Stronger interventions more thoroughly overwrite target representations but increasingly corrupt neighboring dimensions encoding unrelated properties.
- Core assumption: Target and non-target properties share representational dimensions or are correlated in embedding space, making surgical isolation impossible.
- Evidence anchors:
  - [abstract] "all methods show a clear tradeoff between completeness and selectivity"
  - [Section 5.1] "increasing the hyperparameter values yields higher completeness and lower selectivity"
  - [corpus] Related work on improving interventions (arXiv:2506.11673) addresses similar divergence concerns

### Mechanism 2: Nonlinear Interventions Outperform Linear Methods
- Claim: Gradient-based nonlinear counterfactual interventions achieve higher reliability than linear methods across most layers and models.
- Mechanism: Linear methods (INLP, RLACE, AlterRep) operate on subspaces assuming linear separability. Gradient-based interventions (FGSM, PGD, AutoAttack) traverse nonlinear manifolds, following loss gradients to find adversarial perturbations that more effectively shift representations toward target values while staying within bounded ε-balls.
- Core assumption: Intermediate layer representations encode properties nonlinearly, making linear subspaces insufficient for complete transformation.
- Evidence anchors:
  - [abstract] "nonlinear interventions are almost always more reliable than linear interventions"
  - [Section 6] "reliability gap between linear and nonlinear LLMs may be due to LLMs encoding task-relevant representations nonlinearly, particularly in intermediate layers"
  - [corpus] Limited direct corpus evidence; this finding contrasts with linear representation hypothesis literature

### Mechanism 3: Validation Probes Enable Independent Reliability Assessment
- Claim: Separately-trained validation probes provide unbiased measurement of intervention effects by decoding what the model's embeddings encode, independent of intervention probe training.
- Mechanism: Validation probes v are trained on disjoint data from interventional probes, with Z_c and Z_e decorrelated via subsampling. This prevents validation probes from learning spurious correlations that interventional probes might exploit, enabling accurate assessment of whether interventions achieved their intended distributional shift.
- Core assumption: Validation probes accurately decode model representations rather than exploiting statistical artifacts.
- Evidence anchors:
  - [Section 3] "validation probes enable us to estimate how well various intervention methods carry out the target transformation"
  - [Section 4] "Validation probes are trained on data that is completely disjoint from that used to train interventional probes"
  - [corpus] Related work on causal interventions (arXiv:2511.04638) raises concerns about divergent representations from interventions

## Foundational Learning

- Concept: **Probing classifiers**
  - Why needed here: Understanding that probes predict properties from embeddings is prerequisite to grasping why validation probes can measure intervention success.
  - Quick check question: Can you explain why a high-accuracy probe doesn't necessarily mean the model "uses" that property?

- Concept: **Nullspace projection**
  - Why needed here: Linear removal methods (INLP, RLACE) project embeddings into classifier nullspaces to erase information; understanding this clarifies why linear methods may be incomplete.
  - Quick check question: What happens to dimensions orthogonal to the removed subspace during nullspace projection?

- Concept: **Harmonic mean vs arithmetic mean**
  - Why needed here: Reliability uses harmonic mean to penalize interventions that achieve one criterion (e.g., perfect completeness) at the expense of the other (e.g., zero selectivity).
  - Quick check question: If completeness=1.0 and selectivity=0.1, what's the harmonic mean vs arithmetic mean?

## Architecture Onboarding

- Component map:
  Input sentence → LLM → Embedding h_l at layer l → Interventional probe g_Z → Intervention do(Z) → Modified embedding ĥ_l → Validation probes {v_Z_i} ← h_l and ĥ_l → Measure completeness & selectivity → Feed ĥ_l back to LLM → Intervened prediction → ΔTask accuracy

- Critical path:
  1. Train interventional probe on 40% of data (allows spurious correlations)
  2. Train validation probes on separate 40% with decorrelated Z_c/Z_e
  3. Apply intervention with hyperparameter x
  4. Measure completeness via TV distance to target distribution
  5. Measure selectivity via TV distance between original and intervened distributions for non-target properties
  6. Compute reliability = harmonic_mean(completeness, selectivity)
  7. Find x_opt maximizing reliability

- Design tradeoffs:
  - MLP vs linear validation probes: MLP more expressive but could bias toward nonlinear methods; paper finds similar results with both
  - Counterfactual vs removal interventions: Counterfactual easier to evaluate (target distribution is point mass) vs removal (target is uniform)
  - Layer selection: Final layers show more linear structure; intermediate layers may require nonlinear methods

- Failure signatures:
  - Validation probe accuracy <80%: Cannot reliably measure intervention effects; need better probe training
  - Reliability curves don't show clear peak: May indicate hyperparameter range too narrow or intervention fundamentally flawed
  - High completeness but near-zero ΔTask accuracy: Suggests model doesn't actually use that property for the task
  - Selectivity near 1.0 with low completeness: Intervention too weak; increase hyperparameter

- First 3 experiments:
  1. **Baseline validation probe training**: Train linear and MLP probes for Z_c and Z_e on your target LLM layer; verify >90% accuracy before proceeding
  2. **INLP reliability sweep**: Run INLP with rank r ∈ [0, 40], plot completeness vs selectivity curve, identify if Pareto frontier exists
  3. **FGSM reliability calibration**: Run FGSM with ε ∈ [0.005, 5.0], find ε_opt maximizing reliability, compare ΔTask accuracy at ε_opt vs max ε

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the completeness-selectivity tradeoff persist in tasks with multiple causal variables taking an arbitrary number of values?
- **Basis:** [explicit] The authors explicitly state in the Limitations that they intentionally selected simple tasks with binary variables and aim to explore settings with multiple causal variables in future work.
- **Why unresolved:** The current experimental validation is restricted to binary properties (singular/plural) and does not test if the framework scales to high-dimensional or multi-valued properties.
- **What evidence would resolve it:** Applying the reliability framework to complex generative tasks or datasets where properties have many distinct classes (e.g., fine-grained entity types or semantic relations).

### Open Question 2
- **Question:** Do findings that nonlinear interventions outperform linear ones in intermediate layers invalidate the linear representation hypothesis for modern LLMs?
- **Basis:** [explicit] The authors note that their results "provide an important contrast" to the linear representation hypothesis, suggesting findings of linearity in final layers may not generalize to earlier layers.
- **Why unresolved:** While the paper shows nonlinear methods are empirically more reliable in intermediate layers, it does not theoretically prove that representations are inherently nonlinear in those layers.
- **What evidence would resolve it:** A layer-wise theoretical analysis of representational geometry combined with the framework's metrics to determine if linear subspaces are sufficient for interventions in non-final layers.

### Open Question 3
- **Question:** Can novel intervention methods be designed to strictly decouple completeness and selectivity, avoiding the observed reliability tradeoff?
- **Basis:** [inferred] The paper establishes an inherent tradeoff (reliability) for all current methods, but it leaves open whether this is a theoretical limit of the problem or a limitation of existing techniques (INLP, RLACE, GBIs).
- **Why unresolved:** The study benchmarks existing methods but does not propose a new method designed specifically to optimize the harmonic mean without sacrificing one metric for the other.
- **What evidence would resolve it:** The development of an intervention technique that achieves near-perfect validation probe accuracy (completeness) while maintaining zero shift in non-targeted properties (selectivity).

## Limitations

- Dataset Representativeness: Findings based on Wikipedia sentences for subject-verb agreement may not generalize to other linguistic phenomena, model architectures, or domains.
- Model Architecture Assumptions: All tested models are transformer-based; framework's applicability to CNNs, RNNs, or graph neural networks is untested.
- Hyperparameter Sensitivity: Optimal hyperparameter values may not be stable across different training runs, data subsets, or model versions.

## Confidence

**High Confidence** (Mechanistic Evidence + Corpus Support):
- Completeness-Selectivity Tradeoff: Multiple experiments across different models and layers consistently show this relationship.
- Validation Probe Framework: The methodology for using disjoint validation probes is clearly specified and addresses known issues with interventional probe overfitting.

**Medium Confidence** (Direct Evidence + Some Corpus Gaps):
- Nonlinear vs Linear Performance: Strong experimental evidence shows nonlinear methods are more reliable, but limited corpus evidence exists about why linear representation hypotheses fail for intermediate layers.
- Intervention Impact on Task Performance: The correlation between reliability and ΔTask accuracy is demonstrated, but the causal relationship (whether reliable interventions actually identify causally important representations) needs further validation.

**Low Confidence** (Mechanism Not Well Established):
- Property Disentanglement Assumption: The framework assumes properties share representational dimensions, but direct evidence of this correlation in embedding space is limited to the selectivity measurements themselves.

## Next Checks

1. **Cross-Property Validation**: Apply the framework to measure interventions for properties known to be highly correlated (e.g., tense and aspect) versus properties expected to be independent. This would test whether the completeness-selectivity tradeoff strength varies with property correlation, validating the underlying assumption about shared representational dimensions.

2. **Training Stability Analysis**: Run each intervention method with identical hyperparameters across 5 different random seeds. Measure the variance in reliability scores and identify which methods show the most stable behavior. This would quantify the practical reliability of the framework itself.

3. **Ablation on Validation Probe Design**: Train validation probes with varying architectures (linear, small MLP, large MLP) and compare reliability measurements. Additionally, test whether decorrelating Z_c and Z_e via subsampling is necessary by comparing results with correlated validation probe training.