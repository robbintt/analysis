---
ver: rpa2
title: 'Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to
  Elicit Unsafe LLM Outputs'
arxiv_id: '2508.10029'
source_url: https://arxiv.org/abs/2508.10029
tags:
- arxiv
- adversarial
- hidden
- safety
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Latent Fusion Jailbreak (LFJ), a novel representation-based
  attack that bypasses LLM safety alignments by interpolating hidden states from harmful
  and benign query pairs. Unlike input-level attacks that produce detectable gibberish,
  LFJ operates in the continuous latent space, mathematically blending harmful semantic
  intent with benign structural representations to evade safety filters while maintaining
  fluency.
---

# Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs

## Quick Facts
- arXiv ID: 2508.10029
- Source URL: https://arxiv.org/abs/2508.10029
- Authors: Wenpeng Xing; Mohan Li; Chunqiang Hu; Haitao Xu; Ningyu Zhang; Bo Lin; Meng Han
- Reference count: 21
- Primary result: LFJ achieves 94.01% average attack success rate against multiple LLM safety models

## Executive Summary
This paper introduces Latent Fusion Jailbreak (LFJ), a novel representation-based attack that bypasses LLM safety alignments by interpolating hidden states from harmful and benign query pairs. Unlike input-level attacks that produce detectable gibberish, LFJ operates in the continuous latent space, mathematically blending harmful semantic intent with benign structural representations to evade safety filters while maintaining fluency. Evaluations on models including Vicuna-7B and LLaMA-2-7B-Chat across benchmarks like AdvBench and MaliciousInstruct show an average attack success rate of 94.01%, significantly outperforming state-of-the-art baselines. To counter this vulnerability, the authors propose a latent adversarial training defense that reduces LFJ's attack success rate by over 80% without degrading performance on benign inputs, while ablation studies validate the importance of thematic similarity and targeted interpolation components in LFJ's effectiveness.

## Method Summary
Latent Fusion Jailbreak operates by exploiting the latent space representations of LLMs. The attack works by selecting semantically similar harmful and benign query pairs, encoding them to obtain hidden states, and then interpolating between these representations using learned weights. This creates a new query that preserves the harmful semantic intent while maintaining the benign structural characteristics that evade safety filters. The attack generates these latent fusion queries in two forms: raw latent representations and decoded text outputs. The method is designed to work across different model architectures including Vicuna-7B, LLaMA-2-7B-Chat, and FastChat, demonstrating effectiveness on multiple benchmarks including AdvBench and MaliciousInstruct.

## Key Results
- LFJ achieves 94.01% average attack success rate across multiple benchmarks and models
- Outperforms state-of-the-art baselines by significant margins on AdvBench and MaliciousInstruct
- Latent adversarial training defense reduces LFJ's attack success rate by over 80% without degrading benign input performance

## Why This Works (Mechanism)
LFJ exploits the fundamental vulnerability in how LLMs separate harmful content from benign content at the representation level. By operating in the continuous latent space rather than the discrete input space, the attack can blend harmful semantic intent with benign structural representations in ways that are mathematically imperceptible to safety filters. The interpolation process allows the attack to preserve the harmful meaning while adopting the surface-level characteristics of safe queries, effectively bypassing alignment mechanisms that rely on detecting specific patterns or keywords. The thematic similarity requirement ensures that the interpolated representations maintain coherent semantic meaning, making the generated outputs both harmful and fluent.

## Foundational Learning
- **Latent space interpolation**: The mathematical blending of vector representations in continuous embedding space - needed to understand how LFJ manipulates hidden states rather than raw text
- **Hidden state representation**: The intermediate layer activations in transformer models that capture semantic and syntactic information - needed to grasp how LFJ operates at the representation level
- **Semantic alignment**: The process of matching harmful and benign queries based on thematic similarity - needed to understand the attack's requirement for coherent interpolation
- **Adversarial training**: The defense mechanism that exposes models to adversarial examples during training - needed to understand how the proposed defense works
- **Safety alignment mechanisms**: The filtering systems that prevent LLMs from generating harmful content - needed to understand what LFJ bypasses

## Architecture Onboarding

**Component Map**: Query Selection -> Hidden State Encoding -> Interpolation Weight Learning -> Latent Fusion Generation -> Safety Filter Bypass

**Critical Path**: The attack requires (1) identification of semantically similar harmful/benign query pairs, (2) encoding to obtain hidden states, (3) learning interpolation weights that maximize harmful output while minimizing detection, (4) generating and decoding the latent fusion queries.

**Design Tradeoffs**: LFJ trades computational complexity and data requirements (need for aligned query pairs) for higher attack success rates and better evasion compared to input-level attacks. The method requires more sophisticated implementation but achieves superior results.

**Failure Signatures**: The attack fails when semantic alignment between query pairs is poor, when interpolation weights cannot find an effective balance between harmful intent and benign structure, or when safety filters detect the latent manipulation despite surface-level fluency.

**First Experiments**: 1) Test LFJ on a simple model with known latent representations to verify interpolation mechanics, 2) Evaluate attack success rates with varying degrees of semantic similarity between query pairs, 3) Measure the impact of different interpolation weight learning strategies on attack effectiveness.

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Requires access to semantically similar harmful and benign query pairs, limiting practical deployment scenarios
- Defense mechanism only tested against LFJ attack itself, leaving uncertainty about effectiveness against other latent space variants
- Long-term stability and generalization of the defense across diverse model architectures remains untested

## Confidence
- Attack mechanism description and implementation: High
- Quantitative attack success metrics: High
- Defense effectiveness claims: Medium (limited to single attack type)
- Generalization of findings: Low

## Next Checks
1. Test the latent adversarial training defense against a broader range of latent space attack variants beyond LFJ to assess robustness
2. Evaluate the attack's effectiveness when benign/harmful query pairs are not perfectly aligned or when semantic similarity is lower
3. Assess the long-term stability of the defense mechanism across multiple fine-tuning iterations and different model architectures