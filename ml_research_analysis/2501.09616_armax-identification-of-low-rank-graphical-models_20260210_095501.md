---
ver: rpa2
title: ARMAX identification of low rank graphical models
arxiv_id: '2501.09616'
source_url: https://arxiv.org/abs/2501.09616
tags:
- rank
- graphical
- estimation
- noise
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of identifying low-rank vector
  processes when measurement data is corrupted by noise. Traditional methods fail
  in this setting because noise obscures the rank-deficient spectral density, leading
  to non-negligible inaccuracies even under weak noise.
---

# ARMAX identification of low rank graphical models

## Quick Facts
- **arXiv ID**: 2501.09616
- **Source URL**: https://arxiv.org/abs/2501.09616
- **Authors**: Wenqi Cao; Aming Li
- **Reference count**: 0
- **Primary result**: Proposed two-stage ARMAX identification algorithm achieves >90% fit for low-rank graphical models under low noise levels

## Executive Summary
This paper addresses the fundamental problem of identifying low-rank vector processes from noisy measurements, where traditional methods fail due to noise obscuring the rank-deficient spectral density. The authors propose a two-stage algorithm that exploits a sparse plus low-rank structure in the inverse spectral density of noisy measurements. The method first estimates an AR innovation model for the low-rank latent variable using maximum entropy covariance extension, then estimates the deterministic relation between observed and latent variables using maximum likelihood ARMAX estimation. Theoretical guarantees of identifiability and consistency are provided, and simulation results demonstrate superior performance compared to traditional approaches, especially as a filter for denoising low-rank processes.

## Method Summary
The proposed method follows a two-stage approach: (1) Compute covariance lags from noisy data and adjust for noise variance to estimate the latent process using maximum entropy covariance extension with AR graphical constraints, and (2) Use these estimates to initialize a constrained maximum likelihood ARMAX optimization that jointly estimates the autoregressive and exogenous parameters while handling the colored noise introduced by the ARMAX structure. The algorithm requires knowledge of the latent dimension l and topology constraints as inputs, and uses Newton's method with backtracking line search for the non-convex optimization in the second stage.

## Key Results
- The algorithm achieves fit values exceeding 90% for both the latent variable and deterministic relation under low noise levels (σ < 0.6)
- Outperforms traditional low-rank identification methods, particularly when used as a filter for denoising low-rank processes
- Theoretical consistency is proven as data length N approaches infinity, with identifiability conditions specified for the ARMAX parameters
- Performance degrades sharply under high noise levels (σ > 0.6), with fit values dropping below 70%

## Why This Works (Mechanism)

### Mechanism 1: Sparse Plus Low-Rank (S+L) Spectral Decomposition
The method separates measurement noise from the latent low-rank process by exploiting that the inverse spectral density admits a "Diagonal plus Low-rank" structure, where diagonal components correspond to independent measurement noise and low-rank components to internal correlations of the latent process.

### Mechanism 2: Two-Stage Decoupling of Identification
Decomposing the joint identification problem into covariance extension and ARMAX estimation stages improves tractability and consistency by preventing the curse of dimensionality and handling the specific noise structure in the feedback loop.

### Mechanism 3: Maximum Likelihood for Colored Noise Compensation
ML estimation in the second stage handles the colored noise introduced by the ARMAX structure, where standard least squares fails because the error term is correlated rather than white.

## Foundational Learning

- **Concept: Spectral Density & Rank Deficiency**
  - Why needed: The core problem definition relies on noise making rank-deficient spectral density appear full rank
  - Quick check: If a process has spectral density matrix of rank r < n, what does that imply about dimensionality of subspace containing the process?

- **Concept: ARMAX (Autoregressive Moving Average with Exogenous Inputs)**
  - Why needed: Second stage identifies ARMAX model; must distinguish A (autoregressive), B (exogenous), and C (moving average/noise) polynomial matrices
  - Quick check: In ARMAX model A(z)y = B(z)u + C(z)e, how does C(z) term affect correlation of residual errors vs. ARX model?

- **Concept: Maximum Entropy Covariance Extension**
  - Why needed: Stage 1 uses this to estimate missing covariance information; it's a convex optimization technique finding most "random" spectrum consistent with fixed covariance lags
  - Quick check: Why is "maximum entropy" preferred over simple interpolation for extending covariance lags in spectral estimation?

## Architecture Onboarding

- **Component map**: Noisy data ζ(t) -> Covariance lags computation -> Stage 1 (Maximum entropy extension) -> ŷ_l(t) -> Stage 2 (ML ARMAX estimation) -> Parameters A, B -> Filtered process ŷ(t)
- **Critical path**: Estimation of ŷ_l(t) in Stage 1 is the critical dependency; incorrect noise variance σ² causes covariance correction errors that propagate to Stage 2
- **Design tradeoffs**: AR innovation model selected for Stage 1 simplicity but introduces compensation variable; non-convex Newton method accepted for Stage 2 consistency at computational cost
- **Failure signatures**: Divergence of ŷ_m(t) estimates to 10^21 when noise is ignored; sharp fit degradation below 70% when σ > 0.6; non-identifiability when rank condition not met
- **First 3 experiments**:
  1. Noise Floor Test: Sweep σ from 0.1 to 0.8 to verify performance "knee" where algorithm breaks down
  2. Consistency Check: Fix σ at 0.2, vary N (250, 500, 1000) to validate parameter variance decreases
  3. Constraint Validation: Modify topology constraints to test solver stability under incorrect structural assumptions

## Open Questions the Paper Calls Out
- How to estimate the rank (dimension l) of the low-rank latent variable directly from noisy measurement data without prior knowledge
- How to identify the topology (zero entries) of the deterministic relation H(z) automatically during estimation
- How to solve the non-convex ML optimization in the second stage to guarantee global optimum convergence under low signal-to-noise ratios

## Limitations
- Strong assumption of i.i.d. measurement noise across channels; correlated noise breaks the sparse-plus-low-rank structure
- Two-stage approach heavily relies on accurate Stage 1 estimates; errors propagate to Stage 2
- Non-convex ML optimization may converge to local minima, affecting finite-sample performance

## Confidence
- **High confidence**: Theoretical framework (Theorems 1-5) and sparse-plus-low-rank spectral decomposition mechanism; reproducible numerical results with >90% fit under low noise
- **Medium confidence**: Two-stage decoupling approach and ML compensation for colored noise; proofs provided but non-convex optimization sensitivity to initialization is a concern
- **Low confidence**: Scalability to very high-dimensional systems (m >> l) and robustness to non-Gaussian noise distributions; paper focuses on moderate dimensions and Gaussian assumptions

## Next Checks
1. Noise Correlation Test: Introduce correlated noise to verify performance degradation as predicted by sparse-plus-low-rank mechanism
2. Initialization Sensitivity: Systematically vary ML optimization initial conditions to quantify impact on convergence to global vs. local minima
3. Real Data Validation: Apply algorithm to real-world low-rank process dataset to assess performance outside synthetic simulations