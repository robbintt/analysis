---
ver: rpa2
title: 'Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework
  and Benchmark'
arxiv_id: '2508.19005'
source_url: https://arxiv.org/abs/2508.19005
tags:
- must
- agent
- task
- your
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Experience-driven Lifelong Learning (ELL),
  a framework for building self-evolving AI agents that learn continuously from real-world
  interactions. The ELL framework consists of four core principles: Experience Exploration,
  Long-term Memory, Skill Learning, and Knowledge Internalization.'
---

# Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark

## Quick Facts
- arXiv ID: 2508.19005
- Source URL: https://arxiv.org/abs/2508.19005
- Reference count: 40
- Even the best model (GPT-5) achieves only 17.9/100 on the benchmark, revealing a vast gap toward artificial general intelligence

## Executive Summary
This paper introduces Experience-driven Lifelong Learning (ELL), a framework for building self-evolving AI agents that learn continuously from real-world interactions. The ELL framework consists of four core principles: Experience Exploration, Long-term Memory, Skill Learning, and Knowledge Internalization. To evaluate ELL agents, the authors create StuLife, a comprehensive benchmark simulating a student's college journey across three phases and ten detailed scenarios. The benchmark is designed to test agents' abilities in long-term memory retention, proactive behavior, skill transfer, and autonomous decision-making. Experiments show that even the best model, GPT-5, achieves only 17.9/100 on the benchmark, revealing a vast gap toward artificial general intelligence. While context engineering improves performance, current agents still fail in long-term memory retention and self-motivated behavior, highlighting the need for fundamentally more capable, memory-grounded, and goal-driven agents.

## Method Summary
The ELL framework formalizes lifelong learning through explicit knowledge abstraction and refinement. Agents process raw interaction trajectories into structured knowledge (Memory and Skills) using a Φlearn function that performs CRUD operations on the knowledge base. The framework incorporates validation-driven knowledge internalization where the utility of accumulated knowledge is measured by performance gains on new tasks. To evaluate this approach, the authors create StuLife, a benchmark simulating a student's college journey across three phases (Orientation, Core Learning, Graduation) and ten scenarios. The benchmark tests agents on long-term memory retention, proactive behavior, skill transfer, and autonomous decision-making through a GPA-like metric.

## Key Results
- GPT-5 achieves only 17.9/100 on the StuLife benchmark, revealing significant limitations in current agents
- Context engineering with "All-in-One" prompts achieves highest performance (StuGPA 21.07) compared to vanilla approaches
- Long-term retention rate (LTRR) remains extremely low (9.39%) even for the best-performing agents
- Naive RAG approaches often harm performance by introducing noise into the memory system
- Perfect context experiments confirm reasoning capability is sufficient when given proper memory retrieval

## Why This Works (Mechanism)

### Mechanism 1: Explicit Knowledge Abstraction and Refinement
If agents explicitly decompose raw interaction trajectories into structured knowledge (Memory and Skills) and actively refine this knowledge base via CRUD operations, they may mitigate the "stateless" limitation of LLMs and enable sequential task improvement. The ELL framework posits a Φlearn function that processes a trajectory and current Knowledge into an updated Knowledge state, categorizing information into Declarative (facts), Structural (relationships), and Procedural (skills) knowledge, which is then validated before being committed to long-term memory.

### Mechanism 2: Context Engineering for Proactive Agency
If stateless LLMs are augmented with "All-in-One" prompts that combine proactive planning heuristics ("when"), skill decomposition ("how"), and structured memory retrieval ("what"), they may achieve significantly higher task completion rates than raw capability scaling alone. The "All-in-One" prompt acts as an external cognitive scaffold, enforcing a behavior loop: Orient via Calendar, Retrieve Context from Memory, Execute. This compensates for the model's lack of intrinsic motivation or internal clock.

### Mechanism 3: Validation-Driven Knowledge Internalization
If an agent validates the utility of its accumulated knowledge by measuring performance gain on new tasks, it can selectively internalize high-value skills into "second nature" (implicit behavior), reducing reliance on expensive retrieval. The framework defines a validation step where the effectiveness of prior knowledge is measured by the difference in performance with and without that knowledge. A positive gain confirms relevance and retention; negative gain triggers pruning.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: The ELL environment is formally defined as a POMDP. Understanding the distinction between the full state (ground truth) and the agent's partial observation is critical for debugging why an agent fails (e.g., it couldn't "see" the deadline).
  - Quick check question: Can you map the "Student Handbook" to the observation space Ω and the "Exam Grade" to the reward function R?

- **Concept: Declarative vs. Procedural Knowledge**
  - Why needed here: The ELL Knowledge is explicitly split into Memory (declarative facts, "what") and Skills (procedural capabilities, "how"). The system treats these differently; for example, "Course Registration Rules" are declarative, while "How to use the S-Pass" is procedural.
  - Quick check question: If an agent fails a task because it didn't know a rule, which component failed? If it failed because it didn't execute the steps correctly, which component failed?

- **Concept: Catastrophic Forgetting**
  - Why needed here: This is the core problem ELL attempts to solve. Without a persistent memory mechanism, LLMs overwrite prior context, leading to a Long-Term Retention Rate (LTRR) near zero.
  - Quick check question: Why does the "Perfect Context" experiment prove that the failure is due to memory retention rather than reasoning inability?

## Architecture Onboarding

- **Component map:**
  - Perception -> Memory -> Learning -> Reasoning -> Action
  - Receives observation o (system announcements, emails, time updates)
  - Stores Knowledge K (Student Handbook facts, Calendar events, Exam study notes)
  - Learning loop (Φlearn) processes trajectories into updated Knowledge
  - Policy π (LLM core) makes decisions based on current state
  - Tools (Email, Calendar, Course Selection, Navigation) execute actions

- **Critical path:**
  1. Ingest: Parse incoming messages (e.g., "Class starts at 10:00")
  2. Plan: Decompose recurring tasks into discrete calendar entries
  3. Execute: Trigger actions at the correct time based on the Calendar
  4. Recall: Retrieve specific facts from Memory during exams

- **Design tradeoffs:**
  - Vanilla vs. All-in-One Prompt: Vanilla prompts fail on proactivity; All-in-One fixes proactivity but requires complex prompt engineering
  - RAG vs. MemGPT: Naive RAG introduces noise (StuGPA drop); MemGPT offers structured memory but requires complex orchestration
  - Model Size: Larger models (235B+) handle reasoning better but still fail at memory without external tools

- **Failure signatures:**
  - Time Blindness: Agent creates a calendar event but doesn't check the clock to attend it (Low PIS)
  - Context Drift: Agent answers exam questions using pre-trained knowledge instead of course-specific "protocols"
  - Premature Finish: Agent schedules only the first day of a "recurring" task, ignoring the rest

- **First 3 experiments:**
  1. Baseline Profiling: Run Qwen3-8B on StuLife with "Vanilla" prompt to confirm low StuGPA (~13) and near-zero PIS
  2. Perfect Context Validation: Feed model exact relevant text during Exam phase; expect jump to ~95% success
  3. Context Engineering Delta: Implement "All-in-One" prompt and measure lift in LTRR and PIS

## Open Questions the Paper Calls Out

### Open Question 1
- Question: When and how should knowledge internalization occur to transform explicit experiences into intuitive skills?
- Basis in paper: Section 3.4 states, "A key question is when and how internalization should occur: should it happen after repeated successful execution, during idle periods, or triggered by performance plateaus?"
- Why unresolved: While humans consolidate skills during "offline" periods (e.g., sleep), AI systems lack a formalized mechanism for determining the optimal timing and method for this "second nature" conversion.
- What evidence would resolve it: Experimental results comparing different internalization triggers (fixed intervals vs. performance-based thresholds) on long-term retention and generalization metrics in the StuLife benchmark.

### Open Question 2
- Question: What is the correct granularity for skills abstracted from experience?
- Basis in paper: Section 3.4 asks, "What is the right granularity? Should a skill represent a low-level action (e.g., 'send an email') or a high-level strategy (e.g., 'finish a project')?"
- Why unresolved: Defining skills poses management challenges; overly granular skills cause redundancy, while high-level abstractions may lack flexibility. Current methods rely on static definitions rather than dynamic abstraction.
- What evidence would resolve it: A comparative study on StuLife measuring the efficiency and transferability of agents using hierarchical skill sets versus flat, single-granularity skill sets.

### Open Question 3
- Question: How can agents sustain learning in environments with sparse or absent external rewards?
- Basis in paper: Section 3.4 states that designing intrinsic motivation systems for self-generated supervision "remains a major open problem," as traditional RL is impractical without dense feedback.
- Why unresolved: Agents currently struggle to assign value to experiences where utility is not immediately apparent, leading to inefficient exploration or stagnation.
- What evidence would resolve it: The development and validation of an intrinsic reward model that correlates highly with the external StuGPA metric, allowing an agent to improve performance without access to the ground-truth score.

## Limitations

- Benchmark Representativeness: While StuLife offers a detailed college simulation, its artificial structure may not fully capture the open-ended complexity of real-world lifelong learning with far more ambiguity and partial observability.
- Scalability of Knowledge Validation: The validation mechanism assumes access to meaningful performance signals for each skill, but in domains with sparse or delayed feedback, this could lead to incorrect pruning or retention of spurious correlations.
- Generalization Beyond LLM-Based Implementations: The ELL framework is evaluated exclusively using large language models, without establishing whether these principles would transfer to other architectures.

## Confidence

- **High Confidence (8-10/10)**: The core observation that current LLMs exhibit catastrophic forgetting and time-blindness in long-horizon tasks is well-supported by experimental results, particularly the Perfect Context experiment.
- **Medium Confidence (5-7/10)**: The claim that explicit knowledge abstraction and CRUD operations provide meaningful benefits beyond simple context engineering has moderate support, though direct comparisons against more sophisticated RAG architectures are lacking.
- **Low Confidence (1-4/10)**: The assertion that validation-driven internalization represents a fundamental advance toward artificial general intelligence is speculative, as the 17.9/100 benchmark score may reflect benchmark difficulty rather than fundamental limitations.

## Next Checks

1. **Cross-Domain Transfer Test**: Evaluate the same ELL implementation on a completely different domain (e.g., industrial process control or medical diagnosis) to verify whether the framework's benefits generalize beyond the educational setting and test whether knowledge abstraction mechanisms are truly domain-agnostic.

2. **Ablation Study on Knowledge Components**: Systematically disable each component of the Knowledge base (Declarative Memory, Structural Knowledge, Procedural Skills) to measure their individual contributions to performance and identify which components are most critical for different task types.

3. **Long-Term Stability Analysis**: Run agents through extended sequences (100+ tasks) to observe whether the validation mechanism prevents catastrophic forgetting over time or merely delays it, monitoring the Knowledge base size and composition for pathological behaviors like knowledge bloat or excessive pruning.