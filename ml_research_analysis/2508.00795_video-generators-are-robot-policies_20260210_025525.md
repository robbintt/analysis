---
ver: rpa2
title: Video Generators are Robot Policies
arxiv_id: '2508.00795'
source_url: https://arxiv.org/abs/2508.00795
tags:
- video
- policy
- robot
- learning
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that video generation can serve as an effective
  proxy for learning robot policies, substantially improving robustness and generalization
  compared to traditional behavior cloning. The authors propose Video Policy, a modular
  framework that jointly predicts videos of robot behavior and corresponding actions
  using diffusion models.
---

# Video Generators are Robot Policies
## Quick Facts
- arXiv ID: 2508.00795
- Source URL: https://arxiv.org/abs/2508.00795
- Reference count: 40
- Key outcome: Video generation can serve as an effective proxy for learning robot policies, substantially improving robustness and generalization compared to traditional behavior cloning

## Executive Summary
This paper presents Video Policy, a framework that uses video generation as a proxy for learning robot manipulation policies. By jointly predicting videos of robot behavior and corresponding actions using diffusion models, the approach achieves state-of-the-art performance on simulation benchmarks while requiring dramatically fewer demonstrations (50 vs. 300-3000). The method leverages strong video priors from large-scale generative models to improve robustness and generalization across object variations and backgrounds.

## Method Summary
Video Policy is a modular framework that jointly predicts videos of robot behavior and corresponding actions using diffusion models. The approach treats video generation as a proxy for policy learning, where the model learns to generate sequences of robot actions by first learning to predict what those actions look like in video form. This leverages the strong generative priors learned by video diffusion models during pretraining on large-scale video datasets. The framework is trained on relatively few demonstrations (50 per task) and demonstrates superior performance compared to methods requiring hundreds to thousands of demonstrations.

## Key Results
- Achieves 66% average task success rate on RoboCasa validation set, outperforming baselines including Diffusion Policy, GR00T, and Unified Video Action Model
- Requires only 50 demonstrations per task compared to 300-3000 demonstrations used by competing methods
- Demonstrates strong real-world generalization across variations in object locations, appearances, and backgrounds
- Ablation studies show learning to generate videos of robot execution is both necessary and sufficient for learning robust manipulation policies

## Why This Works (Mechanism)
The approach works by leveraging the strong generative priors learned by video diffusion models during pretraining on large-scale video datasets. By treating video generation as a proxy for policy learning, the model can learn robust manipulation strategies from limited demonstrations. The video generation process implicitly captures spatial-temporal relationships and object interactions that are difficult to learn directly from action data alone. Action-free video data provides critical benefits for generalizing to novel tasks by exposing the model to diverse manipulation patterns and scenarios.

## Foundational Learning
- Diffusion models for video generation: Why needed - provide strong generative priors for learning robot behaviors; Quick check - verify video quality and diversity in generated sequences
- Behavior cloning from demonstrations: Why needed - standard approach for learning robot policies from expert data; Quick check - compare success rates with traditional BC methods
- Multimodal learning (video + actions): Why needed - enables joint prediction of visual outcomes and corresponding control signals; Quick check - assess performance with and without action conditioning

## Architecture Onboarding
- Component map: Video diffusion model -> Action prediction head -> Policy output
- Critical path: Demonstration videos → Video generation training → Joint video-action prediction → Policy execution
- Design tradeoffs: Uses pretrained video diffusion models for strong priors vs. fine-tuning challenges; balances video quality with action accuracy
- Failure signatures: Poor video generation quality leading to incorrect action predictions; over-reliance on video priors causing domain shift issues
- First experiments: 1) Validate video generation quality on held-out demonstration data; 2) Test action prediction accuracy on known task sequences; 3) Evaluate policy performance on unseen but similar tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided materials.

## Limitations
- Uncertain whether improvements generalize beyond specific simulation environments (RoboCasa and Libero10) to diverse real-world settings
- Real-world generalization to significantly different object appearances, lighting conditions, and background complexity remains unproven
- Reliance on diffusion models trained on large video datasets raises questions about quality and domain alignment of video priors

## Confidence
- High confidence in the core claim that video generation can serve as a proxy for learning robot policies
- Medium confidence in the assertion that action-free video data is necessary and sufficient for robust policy learning
- Medium confidence in the real-world generalization claims based on limited tasks and controlled conditions

## Next Checks
1. Test the Video Policy framework on a broader set of real-world robotic manipulation tasks, including those with objects and backgrounds not seen during training or simulation
2. Conduct a systematic ablation study varying the amount and type of video data to determine minimum requirements for effective policy learning
3. Compare Video Policy's robustness and sample efficiency against other state-of-the-art approaches in settings with significant domain shift