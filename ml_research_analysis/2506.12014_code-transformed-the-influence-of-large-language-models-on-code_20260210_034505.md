---
ver: rpa2
title: 'code_transformed: The Influence of Large Language Models on Code'
arxiv_id: '2506.12014'
source_url: https://arxiv.org/abs/2506.12014
tags:
- code
- uni00000014
- uni00000015
- python
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how Large Language Models (LLMs) have influenced
  coding style by analyzing over 19,000 GitHub repositories linked to arXiv papers
  from 2020-2025. The study focuses on naming conventions, code complexity, maintainability,
  and similarity between human-written and LLM-generated code.
---

# code_transformed: The Influence of Large Language Models on Code

## Quick Facts
- arXiv ID: 2506.12014
- Source URL: https://arxiv.org/abs/2506.12014
- Reference count: 40
- Key outcome: LLMs are influencing coding style, with Python snake_case usage rising from 47% in Q1 2023 to 51% in Q1 2025, and LLM-generated code showing higher maintainability and lower complexity than human-written code.

## Executive Summary
This paper investigates how Large Language Models (LLMs) have influenced coding style by analyzing over 19,000 GitHub repositories linked to arXiv papers from 2020-2025. The study examines naming conventions, code complexity, maintainability, and similarity between human-written and LLM-generated code. Key findings include a steady increase in snake_case variable names in Python, evidence that LLM-generated code tends to be more maintainable and less complex than human-written solutions, and the observation that LLMs can effectively mimic human coding style when given reference code but diverge significantly without such guidance.

## Method Summary
The study analyzes naming patterns, complexity metrics, and similarity scores across 19,000+ GitHub repositories and Code4Bench submissions from 2020-2025. It employs two generation modes - direct (problem description only) and reference-guided (problem + human AC code) - using 6+ LLMs including GPT-4.1 and DeepSeek-R1. Metrics include naming convention statistics, cyclomatic complexity, Halstead metrics, maintainability index, and similarity scores via cosine/Jaccard measures. The analysis spans 200 problems across 4 difficulty levels and 10 algorithm types, with 32 generations per model per problem for stability.

## Key Results
- Snake_case variable names in Python increased from 47% in Q1 2023 to 51% in Q1 2025
- LLM-generated code demonstrates higher maintainability and lower complexity than human-written code
- Reference-guided generation produces more stylistically similar output to human code than direct generation
- LLMs show stronger algorithmic reasoning in C/C++ compared to Python
- Models tend to rely on a limited set of mainstream algorithms when solving problems

## Why This Works (Mechanism)

### Mechanism 1: Stylistic Homogenization via LLM Defaults
Developers accept LLM suggestions (which default to PEP 8 standards like snake_case) rather than enforcing personal or legacy styles, creating a feedback loop where the "standard" style becomes dominant in public repositories.

### Mechanism 2: Algorithmic Reasoning Constraints
The training data for C/C++ often contains more rigorous algorithmic implementations, whereas Python data may prioritize scripting. Without a reference, LLMs default to the most probable (common) solution path rather than the optimal one.

### Mechanism 3: Reference-Guided Style Mimicry
The model conditions on the provided reference code (formatting, variable names) via in-context learning, effectively "masking" its native preference for verbose or generic patterns.

## Foundational Learning

- **Concept: Cyclomatic Complexity**
  - Why needed here: To quantify the "simplicity" of LLM code vs. human code. The paper uses this to argue LLMs produce less complex, more maintainable solutions.
  - Quick check question: Does a lower cyclomatic complexity score always mean the code is better, or can it hide inefficient logic?

- **Concept: Halstead Metrics**
  - Why needed here: To measure code volume, difficulty, and estimated bugs. The paper uses these to claim LLM code is "easier to maintain."
  - Quick check question: How does "Halstead Difficulty" differ from "Cyclomatic Complexity" in what it measures about code?

- **Concept: Naming Convention Taxonomy**
  - Why needed here: To track the specific stylistic shifts (e.g., snake_case vs. camelCase). The paper categorizes names to detect LLM influence.
  - Quick check question: If a repository enforces a linter (e.g., pylint), how would that confound the detection of "LLM influence" on naming?

## Architecture Onboarding

- **Component map**: Data Ingestion -> Simulation Engine -> Analysis Layer -> Trend Correlator
- **Critical path**: The comparison between Reference-Guided (REF) and Direct Generation (ANS) is the key differentiator. REF reveals how well the model adapts; ANS reveals its native bias.
- **Design tradeoffs**: The study uses style trends as a proxy for LLM usage because direct detection is unreliable. This trades precision for scale.
- **Failure signatures**: Non-CS Repositories show high variance/noise, suggesting LLM influence is harder to isolate in non-expert code. High "Error Rates" in reasoning signal hallucination or shallow understanding.
- **First 3 experiments**:
  1. Replicate Naming Trend: Plot frequency of snake_case names in new repos from 2024-2025 to verify the curve holds.
  2. Reference Ablation: Run simulation with "noisy" reference code to see if LLM still mimics it or corrects to its default.
  3. Language Contrast: Isolate "Difficulty" metric delta between Python and C++ outputs for same algorithmic problem to verify language-dependency of reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
Can precise methodologies be developed to quantify the exact proportion of LLM-generated versus human-written code in public repositories to confirm causal links in stylistic evolution?
Basis in paper: The authors explicitly state in the Limitations section: "We did not examine how much of the code on GitHub was generated by LLMs... there is currently a lack of widely adopted methods and benchmarks."
Why unresolved: The study establishes correlation between LLM characteristics and GitHub trends but lacks ground-truth data to prove specific files were actually AI-generated.
What evidence would resolve it: Development of a robust, widely-adopted detector or "watermarking" system that can accurately attribute code segments to LLMs in the wild.

### Open Question 2
Do the observed shifts toward higher maintainability and lower complexity persist in industrial or enterprise codebases that are not linked to academic research?
Basis in paper: The study relies on "GitHub repositories linked to arXiv papers," which may bias the dataset toward academic or research code. The authors note the dataset "cannot reflect the overall style of the entire human code ecology."
Why unresolved: Trends identified are specific to research-focused subset of GitHub analyzed; it is unknown if these stylistic shifts are universal or confined to academic/open-source software.
What evidence would resolve it: Replication of the study analyzing private enterprise repositories or random sampling of non-academic GitHub projects over same time period.

### Open Question 3
How can LLM training be optimized to improve algorithmic reasoning accuracy and reduce the reliance on a limited set of mainstream algorithms?
Basis in paper: The authors conclude (Finding 4) that "LLMs have low algorithm analysis capabilities" and that high error rates in reasoning are "likely due to their reliance on a limited set of mainstream algorithms."
Why unresolved: While models like GPT-4.1 have higher match rates, they still exhibit significant error rates in reasoning compared to ground-truth labels, suggesting a fundamental gap in algorithmic problem-solving strategies.
What evidence would resolve it: Experiments showing that specific training interventions (e.g., specialized algorithmic fine-tuning) significantly increase the "match rate" while decreasing the "error rate" in the model's reasoning process.

### Open Question 4
To what extent do varying prompt engineering strategies and user parameters influence the stylistic and complexity metrics of the resulting code?
Basis in paper: The authors acknowledge in the Limitations that "there may be many scenarios for user-generated code and prompt parameters. Our simulation cannot exhaust all user usage situations."
Why unresolved: The study tests "Direct Generation" and "Reference-Guided Generation," but the "universality of our research results needs to be further improved" given the infinite variability of real-world user prompts.
What evidence would resolve it: A comprehensive ablation study testing a wide variety of prompt styles (e.g., zero-shot, few-shot, role-playing) to measure the variance in output style and complexity.

## Limitations
- Causality Gap: Temporal alignment could reflect independent trends in coding standards rather than LLM influence
- Sampling Bias: Code4Bench dataset from competitive programming may not reflect general software engineering practices
- Metric Implementation Variability: Different tools may calculate Halstead and cyclomatic complexity slightly differently

## Confidence
- **High Confidence**: LLMs show stronger algorithmic reasoning in C/C++ vs Python; Reference-guided generation produces more stylistically similar output; LLM-generated code demonstrates lower complexity and higher maintainability
- **Medium Confidence**: Observed increase in snake_case naming conventions correlates with LLM adoption; LLMs tend to rely on limited mainstream algorithms; relationship between coding style homogenization and LLM preferences
- **Low Confidence**: Exact magnitude of LLM influence on coding style evolution across all GitHub repositories; whether trends persist in non-academic contexts; long-term sustainability of current coding style patterns

## Next Checks
1. **Temporal Validation**: Replicate the naming convention analysis using a different dataset (e.g., Apache projects or Linux kernel repositories) to verify if the snake_case trend holds across diverse codebases and timeframes.

2. **Reference Robustness Test**: Generate code with intentionally poor reference examples (wrong language, inconsistent style) to determine if LLMs correct to their defaults or faithfully mimic even flawed patterns, clarifying the strength of reference conditioning.

3. **Cross-Domain Generalization**: Apply the same analysis framework to repositories from different domains (web development, embedded systems, data science) to assess whether observed LLM influences are universal or domain-specific.