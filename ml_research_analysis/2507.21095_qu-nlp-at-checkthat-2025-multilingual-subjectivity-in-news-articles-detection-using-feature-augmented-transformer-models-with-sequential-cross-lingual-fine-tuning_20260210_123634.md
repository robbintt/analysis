---
ver: rpa2
title: 'QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection
  using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning'
arxiv_id: '2507.21095'
source_url: https://arxiv.org/abs/2507.21095
tags:
- languages
- language
- subjectivity
- cross-lingual
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses subjectivity detection in news articles across
  multiple languages, distinguishing between sentences that express the author's subjective
  view and those that present objective information. The authors propose a feature-augmented
  transformer architecture that combines contextual embeddings from pre-trained language
  models with statistical and linguistic features.
---

# QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning

## Quick Facts
- arXiv ID: 2507.21095
- Source URL: https://arxiv.org/abs/2507.21095
- Reference count: 35
- Primary result: Competitive performance in multilingual subjectivity detection, ranking 1st in English and Romanian zero-shot, 3rd in German, 4th in Arabic

## Executive Summary
This paper tackles subjectivity detection in news articles across multiple languages by proposing a feature-augmented transformer architecture. The approach combines contextual embeddings from pre-trained language models with statistical and linguistic features through a learnable gating mechanism. The system employs AraELECTRA with POS tags and TF-IDF for Arabic, and DeBERTa V3 with sequential cross-lingual fine-tuning for other languages. Evaluated across monolingual, multilingual, and zero-shot settings, the method achieves competitive results, particularly excelling in English (macro-F1=0.8052) and Romanian zero-shot (macro-F1=0.8126). The study also reveals important insights about the sensitivity of cross-lingual transfer to training language order and linguistic proximity.

## Method Summary
The method employs language-specific transformer architectures: AraELECTRA for Arabic augmented with POS tags and TF-IDF features, and DeBERTa V3 for other languages. Both models use a gating mechanism to dynamically combine TF-IDF lexical features with transformer embeddings. The DeBERTa model undergoes sequential cross-lingual fine-tuning in the order German → Italian → English. Arabic is trained separately due to its distinct linguistic properties. The approach is evaluated in three settings: monolingual, multilingual, and zero-shot on unseen languages including Romanian, Polish, Ukrainian, and Greek.

## Key Results
- English: 1st place with macro-F1=0.8052
- German: 3rd place with macro-F1=0.8013  
- Arabic: 4th place with macro-F1=0.5771
- Romanian (zero-shot): 1st place with macro-F1=0.8126
- Polish (zero-shot): Underperformed baseline with macro-F1=0.5165

## Why This Works (Mechanism)

### Mechanism 1: Gated Feature Augmentation
The gating mechanism combines sparse TF-IDF features with dense transformer embeddings through a learnable scalar gate. This dynamic fusion allows the model to amplify lexical cues when contextual signals are weak and down-weight them when transformer representations are strong, balancing deep semantics with interpretable lexical markers. Evidence shows English improves from 0.7974 to 0.8052 F1 with gating, and German from 0.5866 to 0.8013.

### Mechanism 2: Sequential Cross-Lingual Fine-Tuning
Fine-tuning a multilingual model on languages in a specific sequence improves zero-shot transfer to linguistically similar unseen languages. The sequential training (German → Italian → English) allows the model to accumulate subjectivity detection capabilities while retaining transferable features. Romanian benefits from Romance-language signals accumulated during Italian fine-tuning, achieving 0.8126 F1 in zero-shot.

### Mechanism 3: Language-Specialized Architectures
Using language-specific pre-trained models with tailored feature combinations outperforms a single multilingual model. AraELECTRA captures Arabic morphological richness, while DeBERTa V3's disentangled attention handles cross-lingual transfer for Indo-European languages. This specialization addresses the different manifestations of subjectivity across language families.

## Foundational Learning

- **Concept: Cross-Lingual Transfer Learning** - Why needed here: Core to the multilingual and zero-shot evaluation settings; enables subjectivity detection in languages without training data. Quick check question: Can you explain why fine-tuning on Italian before Romanian might help more than fine-tuning on German?
- **Concept: Feature Fusion with Gating** - Why needed here: Enables dynamic combination of heterogeneous features without manual weighting. Quick check question: What happens to the gate value when the transformer representation is already confident about the class?
- **Concept: Catastrophic Forgetting in Sequential Training** - Why needed here: Explains why language order matters—later languages can overwrite representations learned from earlier ones. Quick check question: If you train [German → Italian → English], why might English performance degrade compared to training English first?

## Architecture Onboarding

- **Component map:** AraELECTRA/DeBERTa V3 → CLS extraction → Gating computation → Modulated TF-IDF → Concatenation with CLS → Classification head
- **Critical path:** Tokenize input → Extract TF-IDF features → Pass through transformer → Compute gate → Concatenate gated features → Classify with softmax
- **Design tradeoffs:** Separate Arabic model vs. unified multilingual: Higher Arabic performance potential vs. maintenance overhead; Sequential vs. joint multilingual training: Better zero-shot to similar languages vs. risk of catastrophic forgetting
- **Failure signatures:** Zero-shot F1 below baseline (Polish: 0.5165) → linguistic distance too large; Arabic F1 significantly lower than others → POS tagger errors or TF-IDF vocabulary mismatch
- **First 3 experiments:** 1) Ablation on target language: backbone only vs. backbone + TF-IDF vs. backbone + TF-IDF + gating; 2) Language order sweep: test at least 3 sequences on target languages; 3) Zero-shot linguistic proximity test: evaluate on held-out languages grouped by family

## Open Questions the Paper Calls Out

- **Open Question 1:** What principles determine the optimal ordering of languages in sequential cross-lingual fine-tuning for subjectivity detection? The paper shows performance varies significantly with different sequences but provides no principled method for selecting optimal orderings.
- **Open Question 2:** Why does sequential cross-lingual fine-tuning transfer successfully to Romanian but fail for Polish, Ukrainian, and Greek? The authors note Romanian's outstanding performance while acknowledging poor performance on Slavic and Hellenic languages, but offer only speculative explanations about linguistic proximity.
- **Open Question 3:** Would adversarial training or language adapters improve cross-lingual transfer to typologically distant languages? The conclusion explicitly lists implementing adversarial training techniques as future work, acknowledging current limitations in multilingual generalization.
- **Open Question 4:** How can the Arabic model's performance gap be closed, and would integrating Arabic into the cross-lingual DeBERTa pipeline improve results? The authors used a separate AraELECTRA model for Arabic but excluded it from the DeBERTa sequential training, noting this as a limitation.

## Limitations

- The approach shows limited generalization to linguistically distant languages, with zero-shot performance on Polish, Ukrainian, and Greek falling below baseline.
- Arabic performance lags significantly behind other languages (0.5771 F1), suggesting the specialized architecture may not be optimal for this language.
- Results appear to be from single runs without multiple random seeds, making it difficult to assess stability and variance.

## Confidence

- **High confidence:** The core methodology of feature-augmented transformers with gating mechanisms and sequential cross-lingual fine-tuning is well-documented and technically sound.
- **Medium confidence:** The reported performance metrics are likely reproducible given the specified hyperparameters, but exact scores may vary due to unspecified details in the classification head architecture.
- **Low confidence:** The generalizability claims for zero-shot transfer to distant languages are not well-supported, given the poor performance on Polish, Ukrainian, and Greek.

## Next Checks

1. **Sequential fine-tuning sensitivity analysis:** Systematically test all possible training sequences on the multilingual test set to quantify the impact of language order on performance across all target languages.

2. **Cross-lingual transfer family clustering:** Evaluate the model's zero-shot performance on languages grouped by linguistic family (Romance: Romanian; Slavic: Polish, Ukrainian; Greek) to empirically validate the hypothesis that linguistic proximity determines transfer success.

3. **Ablation with linguistic features:** Replace the TF-IDF feature extractor with alternative linguistic feature sets (e.g., dependency parse features) in both Arabic and DeBERTa models to determine whether the gating mechanism's effectiveness generalizes beyond lexical features.