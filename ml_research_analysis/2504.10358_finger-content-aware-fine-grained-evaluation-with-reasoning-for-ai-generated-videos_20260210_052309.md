---
ver: rpa2
title: 'FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated
  Videos'
arxiv_id: '2504.10358'
source_url: https://arxiv.org/abs/2504.10358
tags:
- reasoning
- video
- arxiv
- entity-level
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FingER, a novel entity-level reasoning evaluation
  framework for AI-generated videos that addresses the challenge of assessing localized
  defects in increasingly sophisticated video generation models. The core innovation
  lies in decomposing overall video quality assessment into fine-grained entity-level
  questions across five dimensions (visual quality, text-to-video alignment, temporal
  consistency, factual consistency, and dynamic degree), which are then answered by
  a reasoning model.
---

# FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos

## Quick Facts
- arXiv ID: 2504.10358
- Source URL: https://arxiv.org/abs/2504.10358
- Reference count: 40
- Introduces FingER, an entity-level reasoning framework for evaluating AI-generated videos with state-of-the-art performance

## Executive Summary
FingER addresses the challenge of evaluating AI-generated videos by introducing a fine-grained, entity-level reasoning framework that decomposes video quality assessment into localized questions across five dimensions: visual quality, text-to-video alignment, temporal consistency, factual consistency, and dynamic degree. The framework leverages a reasoning model to answer these decomposed questions and provides detailed explanations for its evaluations. By constructing a high-quality dataset (FingER-Instruct-60k) with 3.3k videos and 60k QA annotations, the authors demonstrate that their approach achieves superior performance on public benchmarks while requiring significantly fewer training samples than existing methods.

## Method Summary
The FingER framework introduces a novel approach to video quality assessment by decomposing overall evaluation into fine-grained entity-level questions. The system first extracts entities from video content and questions, then decomposes complex questions into simpler sub-questions using a reasoning model. These questions are answered across five evaluation dimensions, with the final assessment derived from the aggregation of these fine-grained evaluations. The authors employ both supervised fine-tuning and reinforcement learning protocols to train their evaluation model, demonstrating that this approach achieves state-of-the-art performance while being more sample-efficient than existing methods.

## Key Results
- Achieves state-of-the-art performance on GenAI-Bench and MonetBench with relative margins of 11.8% and 5.5% respectively
- Demonstrates superior sample efficiency, requiring only one-tenth of training samples compared to existing methods
- Shows strong generalization capability across different video generation models and content types

## Why This Works (Mechanism)
The framework's effectiveness stems from its ability to break down complex video evaluation into manageable, entity-specific questions that can be answered with high precision. By focusing on localized defects rather than holistic assessment, the system can identify specific quality issues that might be missed by traditional evaluation methods. The multi-dimensional approach ensures comprehensive coverage of different aspects of video quality, from visual fidelity to temporal coherence and factual accuracy.

## Foundational Learning
- **Entity-level evaluation**: Why needed - enables precise localization of defects; Quick check - verify entity extraction accuracy on diverse video content
- **Question decomposition**: Why needed - breaks complex evaluations into answerable sub-questions; Quick check - test decomposition quality across different question types
- **Multi-dimensional assessment**: Why needed - captures different aspects of video quality comprehensively; Quick check - validate coverage of all five dimensions on test videos
- **Reasoning-based evaluation**: Why needed - provides explainable assessments with detailed reasoning; Quick check - verify reasoning quality against human judgments
- **Sample-efficient training**: Why needed - reduces dependency on large labeled datasets; Quick check - measure performance degradation with reduced training data

## Architecture Onboarding

**Component Map**: Video Input -> Entity Extraction -> Question Decomposition -> Sub-question Answering -> Dimension Aggregation -> Final Evaluation

**Critical Path**: The most critical components are the entity extraction and question decomposition modules, as errors in these stages propagate through the entire evaluation pipeline and cannot be recovered downstream.

**Design Tradeoffs**: The framework trades computational complexity for evaluation precision, as the decomposition and multi-dimensional assessment approach requires more processing but provides more detailed and actionable feedback compared to holistic evaluation methods.

**Failure Signatures**: Common failure modes include incorrect entity extraction leading to irrelevant questions, poor question decomposition resulting in unanswerable sub-questions, and dimension misalignment where the evaluation focuses on wrong aspects of video quality.

**First Experiments**:
1. Test entity extraction accuracy on videos with varying complexity and entity density
2. Validate question decomposition quality across different types of evaluation questions
3. Measure dimension-specific performance to identify which evaluation aspects need improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on reasoning model accuracy for question decomposition and entity extraction introduces potential compounding errors
- Dataset represents a relatively narrow domain compared to the vast diversity of real-world video content
- Reinforcement learning approach may lead to evaluation criteria that drift from human judgment over time

## Confidence
- **High confidence**: Technical architecture and dataset construction methodology are well-documented and reproducible
- **Medium confidence**: Efficiency claims require more extensive ablation studies for verification
- **Medium confidence**: Generalizability claims need broader validation beyond tested models

## Next Checks
1. Conduct out-of-distribution testing using videos with significantly different content, styles, or generation methods not represented in the training set
2. Perform ablation studies specifically testing the impact of question decomposition and entity extraction components on overall evaluation accuracy
3. Implement human evaluation studies comparing FingER's assessments against expert human judgments across diverse video types and generation models