---
ver: rpa2
title: Multi-level Mixture of Experts for Multimodal Entity Linking
arxiv_id: '2507.07108'
source_url: https://arxiv.org/abs/2507.07108
tags:
- entity
- mention
- multimodal
- textual
- mmoe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MMoE, a multi-level mixture-of-experts model
  for multimodal entity linking (MEL), addressing two key issues: mention ambiguity
  caused by brief textual contexts, and dynamic selection of modal content importance.
  The model enhances mentions with relevant Wikidata descriptions using LLMs, employs
  a multimodal feature extraction module, and uses switch mixture-of-experts (SMoE)
  mechanisms at both intra-modal and inter-modal levels to dynamically select important
  features.'
---

# Multi-level Mixture of Experts for Multimodal Entity Linking

## Quick Facts
- arXiv ID: 2507.07108
- Source URL: https://arxiv.org/abs/2507.07108
- Reference count: 40
- Outperforms state-of-the-art MEL baselines by up to 1.70% in MRR and 1.70% in Hits@1

## Executive Summary
This paper introduces MMoE, a multi-level mixture-of-experts model for multimodal entity linking (MEL), addressing two key issues: mention ambiguity caused by brief textual contexts, and dynamic selection of modal content importance. The model enhances mentions with relevant Wikidata descriptions using LLMs, employs a multimodal feature extraction module, and uses switch mixture-of-experts (SMoE) mechanisms at both intra-modal and inter-modal levels to dynamically select important features. Extensive experiments on three benchmarks (WikiMEL, RichpediaMEL, WikiDiverse) show MMoE consistently outperforms state-of-the-art baselines, achieving up to 1.70% improvement in MRR and 1.70% in Hits@1, while ablation studies confirm the effectiveness of each component.

## Method Summary
MMoE processes multimodal mentions (word, text context, image) and candidate entities (name, attribute, image) by first optionally enhancing the mention text with a relevant Wikidata description selected by an LLM (DME module). It then encodes text and images using CLIP, applies SMoE mechanisms at both intra-modal (text-to-text, image-to-image) and inter-modal (text-to-image, image-to-text) levels to dynamically select important features, computes matching scores, and aggregates them using a contrastive loss with in-batch negatives. The model is trained with AdamW optimizer at learning rate 1e-5, using 4 experts and top-k selection (typically k=2 or 3).

## Key Results
- MMoE consistently outperforms state-of-the-art baselines on WikiMEL, RichpediaMEL, and WikiDiverse datasets
- Achieves up to 1.70% improvement in MRR and 1.70% in Hits@1 metrics
- Ablation studies confirm effectiveness of DME module, IntraMoE, and InterMoE components
- Parameter sensitivity analysis shows optimal configuration: 4 experts with top-2 or top-3 selection

## Why This Works (Mechanism)

### Mechanism 1: Description-aware Mention Enhancement (DME)
The DME module leverages GPT-3.5 to identify the most relevant Wikidata description for a mention word and its textual context, then concatenates this description with the mention context. This reduces ambiguity by providing additional semantic context that helps disambiguate mentions like "Apple" (fruit vs company). The core assumption is that LLMs can accurately identify contextually relevant entity descriptions, though this depends on the LLM's ability to handle ambiguous cases correctly.

### Mechanism 2: Switch Mixture of Experts (SMoE) for Intra-modal Feature Selection
SMoE applies a router network that sends each token or patch representation to a small subset (top-k) of expert feed-forward networks. This dynamically selects specific features within each modality, focusing on the most relevant information while reducing noise. The core assumption is that different parts of a modality contribute differently to predictions, and sparse expert selection can adaptively learn these contributions. Router collapse, where a single expert dominates, is a potential failure mode.

### Mechanism 3: Multi-level (Intra- and Inter-modal) Feature Matching
The model combines unimodal matching (text-to-text, image-to-image) with cross-modal matching (text-to-image) using adaptive expert selection. An inter-level mixture of experts module uses features from one modality to guide feature selection from another via a gated mechanism, producing a cross-modal context embedding. The core assumption is that effective entity linking requires reasoning within and across modalities, with adaptive interactions between them.

## Foundational Learning

- **Mixture of Experts (MoE) / Switch Transformer**: The core innovation uses a "switch" variant for sparse, dynamic feature selection. Quick check: If a router has 4 experts and selects top-1, how many experts process a single token? (Answer: 1)

- **Contrastive Learning**: The paper uses contrastive loss for training. Quick check: In the loss function, what serves as negative samples for the target entity `e`? (Answer: In-batch negative samples `e'`).

- **Multimodal Entity Linking (MEL)**: The task definition involves linking ambiguous mentions (text + image) to entities in a Knowledge Base (Wikidata). Quick check: What are the two main challenges this paper addresses? (Answer: Mention ambiguity and dynamic selection of modal content).

## Architecture Onboarding

- **Component map**: Input (Mention + Entity) -> CLIP Features -> (DME Augmentation) -> IntraMoE/InterMoE Feature Processing -> Score Calculation -> Contrastive Loss. IntraMoE and InterMoE paths are parallel.

- **Critical path**: Input -> CLIP Features -> (DME Augmentation) -> IntraMoE/InterMoE Feature Processing -> Score Calculation -> Contrastive Loss. The IntraMoE and InterMoE paths are parallel.

- **Design tradeoffs**: 
  - Number of Experts (K) and Top-k: More experts increase model capacity but also computation. The paper found K=4, k=3 optimal on one dataset, but K=4, k=2 or K=4, k=4 on others.
  - DME Module: Adds external LLM dependency and computational overhead but provides significant gains, especially on noisy datasets.
  - Visual Data Availability: Performance can degrade if visual information is sparse or noisy, suggesting visual modality contribution is conditional on data quality.

- **Failure signatures**:
  - Router Collapse: Check router entropy if performance plateaus or experts are not utilized diversely.
  - LLM Hallucination in DME: Monitor performance degradation with DME enabled to detect irrelevant description selection.
  - Convergence Issues: Verify learning rate (1e-5 found optimal) if model fails to converge.

- **First 3 experiments**:
  1. Baseline Reproduction: Implement core MMoE model (without DME) and verify performance on WikiMEL against ablation "MMoE" row in Table 3.
  2. Ablation on MoE: Remove IntraMoE module and then InterMoE module separately to confirm individual contributions as shown in Table 5.
  3. Hyperparameter Scan: Re-run parameter sensitivity experiment for "Number of Experts and Top-experts" on validation set to confirm optimal K and k values reported in Figure 3(a).

## Open Questions the Paper Calls Out

### Open Question 1
Can MMoE effectively transfer to specialized domains like biomedical or drug discovery where entity descriptions have distinct terminological patterns and knowledge structures? The conclusion states "we plan to explore tasks in specific domains, such as the biomedical and drug discovery domains." This remains unresolved as the model was evaluated only on general-purpose datasets. Domain-specific challenges like technical jargon and specialized entity taxonomies remain untested.

### Open Question 2
How can sparse visual knowledge in MEL datasets be effectively completed without introducing noisy or irrelevant information that degrades performance? The conclusion notes "the visual knowledge of three datasets is very sparse, thus how to complete it is an interesting research direction." WikiDiverse has only 44.37% visual coverage and ablation studies show removing visual loss sometimes improves performance, suggesting naive completion may harm results.

### Open Question 3
Is there a minimal LLM capability threshold for effective DME module performance, and can smaller open-source models be adapted to match GPT-3.5? Table 7 shows LLaMA3.1-8B underperforms GPT-3.5 across all datasets (e.g., WikiDiverse Hits@1: 75.79% vs 77.57%), with LLaMA2-7B performing worse. The DME module depends on LLMs ranking entity descriptions, but the relationship between LLM capability and MEL performance remains unclear.

### Open Question 4
Can the SMoE mechanism's computational overhead be reduced while preserving the adaptive feature selection benefits that drive MMoE's performance gains? Complexity analysis shows SMoE accounts for most computational burden (19.443G FLOPs vs 481.333M without), and training times are 15-36% longer than baselines. The paper demonstrates SMoE effectiveness but does not explore efficiency optimizations.

## Limitations

- DME module effectiveness depends on LLM's ability to consistently select relevant Wikidata descriptions, which may be challenging for highly ambiguous mentions
- SMoE performance relies on router's ability to differentiate informative features, with risk of router collapse where single expert dominates
- Visual modality contribution is conditional on data quality, with performance degrading on datasets with sparse or noisy images
- Batch size is critical for contrastive loss using in-batch negatives but not specified in paper

## Confidence

- **High Confidence**: Overall claim that MMoE outperforms state-of-the-art baselines on three benchmarks, supported by experimental results and ablation studies
- **Medium Confidence**: Specific mechanism claims for how DME reduces ambiguity and how SMoE dynamically selects important features, with architectural details but limited validation of causal links
- **Low Confidence**: LLM's accuracy in selecting most relevant Wikidata description for ambiguous mentions, with paper asserting effectiveness but not providing detailed validation

## Next Checks

1. Implement and validate the DME module using GPT-3.5 API with specified prompt to augment mention contexts, then evaluate LLM's description selection accuracy on a subset of mentions
2. Analyze router behavior during training by logging expert selection frequency per batch and computing router entropy to check for uniform distribution and detect potential collapse
3. Test model robustness by intentionally degrading visual data quality (adding noise, using low-resolution images) on one benchmark to assess performance impact and validate conditional contribution of visual modality