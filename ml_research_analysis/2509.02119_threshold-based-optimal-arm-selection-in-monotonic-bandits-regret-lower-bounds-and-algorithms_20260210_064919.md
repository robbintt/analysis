---
ver: rpa2
title: 'Threshold-Based Optimal Arm Selection in Monotonic Bandits: Regret Lower Bounds
  and Algorithms'
arxiv_id: '2509.02119'
source_url: https://arxiv.org/abs/2509.02119
tags:
- optimal
- regret
- threshold
- identification
- lower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies threshold-based identification tasks in stochastic\
  \ multi-armed bandit problems where arm means are monotonically increasing or decreasing.\
  \ Instead of maximizing rewards, the goal is to identify an arm based on its position\
  \ relative to a threshold \u03C4\u2014either the first above \u03C4, the \u2113\
  th above/below \u03C4, or the one closest to \u03C4."
---

# Threshold-Based Optimal Arm Selection in Monotonic Bandits: Regret Lower Bounds and Algorithms

## Quick Facts
- arXiv ID: 2509.02119
- Source URL: https://arxiv.org/abs/2509.02119
- Reference count: 20
- Primary result: Threshold-based arm identification in monotonic bandits with regret lower bounds and KL-divergence algorithms

## Executive Summary
This paper studies threshold-based identification tasks in stochastic multi-armed bandit problems where arm means are monotonically increasing or decreasing. Instead of maximizing rewards, the goal is to identify an arm based on its position relative to a threshold τ—either the first above τ, the ℓth above/below τ, or the one closest to τ. The authors derive asymptotic regret lower bounds showing that optimal performance depends only on arms adjacent to τ.

The paper proposes three algorithms—TOSMB for threshold-crossing, RTOSMB for ranked threshold identification, and POSMB for proximity identification—that use KL-divergence confidence bounds to guide exploration and exploitation. These algorithms are evaluated via Monte Carlo simulations (30 trials over 10⁶ iterations), showing convergence toward theoretical lower bounds. The work extends classical bandit theory to structured, threshold-based objectives relevant to applications like clinical dosing and communication networks.

## Method Summary
The authors establish asymptotic regret lower bounds for threshold-based identification in monotonic bandit settings by analyzing the fundamental limits of information gathering. They develop three distinct algorithms that leverage KL-divergence confidence bounds to balance exploration and exploitation. TOSMB targets the first arm crossing a threshold, RTOSMB identifies the ℓth arm above or below τ, and POSMB finds the arm closest to τ. Each algorithm employs adaptive sampling strategies that focus on arms near the threshold, with confidence bounds guiding when sufficient evidence has been gathered to make a selection. The theoretical analysis shows that optimal performance depends only on the arms immediately adjacent to the threshold, leading to computationally efficient implementations.

## Key Results
- Derived asymptotic regret lower bounds for threshold-based arm identification in monotonic bandits
- Proposed three KL-divergence-based algorithms (TOSMB, RTOSMB, POSMB) with theoretical guarantees
- Demonstrated convergence toward lower bounds through Monte Carlo simulations (30 trials, 10⁶ iterations)
- Showed optimal performance depends only on arms adjacent to the threshold τ

## Why This Works (Mechanism)
The algorithms work by exploiting the monotonic structure to reduce the effective problem size to just the arms near the threshold. KL-divergence confidence bounds provide statistically sound stopping criteria, ensuring correct identification with high probability while minimizing unnecessary exploration. The adaptive sampling focuses resources on the most informative arms—those near τ—rather than uniformly exploring all options. This targeted approach leverages the problem's structure to achieve near-optimal regret bounds while maintaining computational efficiency.

## Foundational Learning

**Monotonic Bandit Structure**: Understanding why ordered arm means enable focused exploration near thresholds.
*Why needed*: Enables reduction of effective problem size and efficient algorithm design.
*Quick check*: Verify monotonicity assumption holds in application domain.

**KL-Divergence Confidence Bounds**: Statistical tools for quantifying uncertainty in bandit settings.
*Why needed*: Provides mathematically rigorous stopping criteria for arm selection.
*Quick check*: Confirm KL-divergence calculations match theoretical expectations.

**Asymptotic Regret Analysis**: Framework for deriving fundamental performance limits.
*Why needed*: Establishes benchmarks for evaluating algorithm performance.
*Quick check*: Validate regret bounds hold under various monotonic configurations.

## Architecture Onboarding

**Component Map**: Threshold τ → Adjacent arms → KL-divergence bounds → Sampling strategy → Arm selection
**Critical Path**: Threshold detection → Adaptive sampling → Confidence bound checking → Decision making
**Design Tradeoffs**: Focused exploration near threshold vs. potential misspecification risks; computational efficiency vs. generality
**Failure Signatures**: Incorrect monotonicity assumption → algorithm divergence; noisy estimates → delayed convergence; threshold ambiguity → suboptimal selection
**First Experiments**:
1. Validate monotonic structure assumption on synthetic data
2. Test algorithm performance with varying threshold positions
3. Evaluate robustness to noise in arm mean estimates

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes strict monotonic structure which may not hold in practice
- Monte Carlo simulations limited to only 30 trials
- Algorithms rely on specific parametric distributions (Bernoulli/Gaussian)
- Computational complexity may grow with large arm counts

## Confidence
- Theoretical claims: High
- Empirical validation: Medium
- Practical applicability: Low to Medium

## Next Checks
1. Test algorithm performance under model misspecification where monotonic assumptions are violated
2. Evaluate scalability with increasing arm counts to assess computational burden
3. Extend empirical validation to non-parametric reward distributions and compare against alternative threshold-based approaches