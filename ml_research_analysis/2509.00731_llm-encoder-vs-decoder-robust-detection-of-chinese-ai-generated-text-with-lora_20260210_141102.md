---
ver: rpa2
title: 'LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with
  LoRA'
arxiv_id: '2509.00731'
source_url: https://arxiv.org/abs/2509.00731
tags:
- text
- chinese
- detection
- ai-generated
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of detecting AI-generated Chinese
  text, which is complicated by linguistic nuances. The research compares encoder-based
  Transformers (BERT and RoBERTa), a decoder-only LLM (Qwen2.5-7B), and a FastText
  baseline using a dataset from the NLPCC 2025 Chinese AI-Generated Text Detection
  Task.
---

# LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA

## Quick Facts
- arXiv ID: 2509.00731
- Source URL: https://arxiv.org/abs/2509.00731
- Reference count: 28
- Primary result: LoRA-adapted Qwen2.5-7B decoder achieves 95.94% test accuracy on Chinese AI-generated text detection, outperforming encoder models (RoBERTa: 76.3%, BERT: 79.3%) and FastText baseline (83.5%)

## Executive Summary
This study addresses the challenge of detecting AI-generated Chinese text by comparing encoder-based Transformers (BERT and RoBERTa), a decoder-only LLM (Qwen2.5-7B), and FastText on the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models were fine-tuned using a novel prompt-based masked language modeling approach, while Qwen2.5-7B was adapted for classification with an instruction-format input and LoRA fine-tuning. Results show that encoder models suffered severe overfitting, with test accuracy dropping from near-perfect training performance to 76-79%. In contrast, the LoRA-adapted Qwen2.5-7B achieved 95.94% test accuracy with balanced precision-recall metrics, demonstrating superior generalization and resilience to dataset-specific artifacts.

## Method Summary
The study compares three approaches for Chinese AI-generated text detection: (1) encoder models (RoBERTa-wwm-ext-large and BERT-large) fine-tuned with a prompt-based masked language modeling objective that predicts "人工" (human) or "算法" (algorithm) tokens, (2) decoder model (Qwen2.5-7B) adapted via LoRA with a classification head on the first-token hidden state and instruction-based input format, and (3) FastText baseline using jieba segmentation and subword n-grams. The NLPCC 2025 Task 1 dataset provided train/dev/test splits for evaluation. Key innovations include the prompt-based MLM formulation for encoders and the LoRA adaptation of the decoder model to prevent overfitting while preserving generalization capabilities.

## Key Results
- LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with balanced precision-recall metrics
- RoBERTa drops from 99.65% training accuracy to 76.31% test accuracy, showing severe overfitting
- BERT shows similar overfitting pattern (99.35% train → 79.28% test)
- FastText achieves 83.5% accuracy through lexical patterns but lacks semantic understanding
- Encoder models overfit to training distribution idiosyncrasies while LoRA prevents this overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA-adapted decoder models achieve superior out-of-domain generalization for Chinese AI-text detection compared to full fine-tuning of encoder models
- Mechanism: LoRA injects trainable low-rank decomposition matrices into projection layers while freezing backbone weights, preventing catastrophic overfitting to training distribution idiosyncrasies. The decoder's generative pretraining on 2.4T tokens provides richer linguistic priors that survive distribution shift when only low-rank updates are applied
- Core assumption: The generalization advantage stems from both parameter-efficient adaptation (preventing overfitting) AND the scale/diversity of decoder pretraining data—not LoRA alone
- Evidence anchors:
  - LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy
  - RoBERTa's accuracy plunges to 76.31% while Qwen's LoRA regularization prevents overfitting
  - Related work shows similar encoder-decoder gaps in other detection tasks

### Mechanism 2
- Claim: Prompt-based masked language modeling for encoder classification provides implicit regularization but insufficient to match decoder robustness
- Mechanism: By constraining classification to masked-token prediction ("人工"/"算法") rather than a randomly initialized [CLS] head, the model leverages pretrained MLM knowledge directly. However, bidirectional encoders still overfit to training distribution patterns because their smaller pretraining corpus (0.4B + 5.4B words) lacks the diversity to capture transferable AI-text signatures
- Core assumption: The MLM objective provides some regularization benefit over random classification heads, but the effect is secondary to pretraining scale
- Evidence anchors:
  - Prompt-based MLM regularizes encoders better than random classification heads
  - RoBERTa still drops from 99.65% (train) to 76.31% (test) despite this approach
  - No direct comparison with standard [CLS] heads provided

### Mechanism 3
- Claim: Surface-level lexical patterns (captured by FastText) remain surprisingly discriminative for current Chinese AI-generated text
- Mechanism: FastText's bag-of-n-grams captures consistent lexical/artifactual patterns in AI-generated Chinese (e.g., specific word frequencies, lack of human-typical disfluencies) that persist across domains
- Core assumption: The 83.5% FastText accuracy reflects genuine lexical artifacts in current AI-generated Chinese, not dataset-specific quirks
- Evidence anchors:
  - FastText demonstrates surprising lexical robustness (83.5% accuracy)
  - FastText exploits lexical regularities even when Transformers falter
  - Surface-level feature effectiveness noted in other Chinese NLP tasks

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: Core technique enabling parameter-efficient fine-tuning of 7B-parameter decoders. Understanding LoRA rank (r=4,8,16) tradeoffs is essential for reproducing results
  - Quick check question: Given a weight matrix W ∈ R^(d×d), can you write the LoRA decomposition W' = W + ΔW where ΔW = BA, B ∈ R^(d×r), A ∈ R^(r×d)?

- Concept: **Distribution Shift / Out-of-Domain Generalization**
  - Why needed here: The paper's central claim hinges on encoder-decoder robustness differences under distribution shift. Train-to-test performance gaps (e.g., RoBERTa: 99.65%→76.31%) are the key diagnostic
  - Quick check question: If a model achieves 99% train accuracy and 75% test accuracy on the same task distribution, what are two possible causes?

- Concept: **Masked Language Modeling (MLM) vs. Causal Language Modeling (CLM)**
  - Why needed here: Encoders use bidirectional MLM; decoders use unidirectional CLM. The paper exploits MLM directly via prompt-based classification for encoders
  - Quick check question: Why can't standard decoder LLMs use [MASK] tokens natively for classification without architectural modification?

## Architecture Onboarding

- Component map:
```
Encoder Pipeline (RoBERTa/BERT):
Input text → Prompt template wrapping → Tokenizer → 
24-layer bidirectional Transformer → Masked position prediction ("人工"/"算法")
→ Cross-entropy loss on 2 tokens

Decoder Pipeline (Qwen2.5-7B):
Input text → Instruction prefix ("判断下面的文本...") → Tokenizer →
28-layer causal Transformer (GQA: 28Q/4KV heads) → 
First-token hidden state h₀^(L) → Linear classification head (768→2) →
LoRA adapters (rank r) in projections → Softmax

FastText Baseline:
Input text → jieba segmentation → Subword n-gram embeddings →
Average pooling → Linear classifier
```

- Critical path:
  1. **Data preparation**: Obtain NLPCC-2025 Task 1 corpus; ensure train/dev/test splits match paper
  2. **For decoders**: Implement LoRA injection into Qwen2.5-7B projections (Q, K, V, output projections); add classification head on first token
  3. **For encoders**: Implement prompt-based template with MLM loss on target mask positions
  4. **Hyperparameter sweep**: LoRA rank (4, 8, 16), learning rate, early stopping patience

- Design tradeoffs:
  - **LoRA rank vs. capacity**: r=16 achieves best accuracy (95.94%) but increases trainable parameters ~4× over r=4; diminishing returns beyond r=16
  - **Encoder vs. decoder inference cost**: RoBERTa (~330M params) is ~20× smaller than Qwen2.5-7B; for latency-sensitive applications, encoder may be preferable if domain shift is limited
  - **FastText as pre-filter**: 83.5% accuracy with minimal compute suggests FastText could filter obvious cases, reserving Transformer inference for uncertain samples

- Failure signatures:
  - **Encoder overfitting**: Near-perfect train accuracy (>98%) but >15% test drop indicates memorization of training artifacts
  - **LoRA rank too low (r=4)**: Qwen achieves 94.31% vs. 95.94% at r=16; diminishing returns suggest r=8-16 as practical range
  - **Class imbalance in errors**: RoBERTa shows AI-recall collapse (0.5925) on test set, indicating distribution-specific bias

- First 3 experiments:
  1. **Reproduce encoder baseline**: Fine-tune Chinese RoBERTa-wwm-ext-large with prompt-based MLM on NLPCC-2025 train split; verify ~76% test accuracy
  2. **LoRA rank ablation**: Train Qwen2.5-7B with r∈{4,8,16}; confirm monotonic improvement and report precision/recall per class
  3. **Domain robustness check**: If additional Chinese AI-text datasets exist, evaluate best checkpoint out-of-box to test generalization claims; if not, create held-out split from training data simulating distribution shift (e.g., different AI generators)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the superior robustness of LoRA-adapted decoder models be replicated across diverse cross-domain Chinese detection datasets?
- Basis in paper: The Conclusion states that "evaluation on a single dataset limits generalizability claims" and explicitly calls for future research to "prioritize cross-domain robustness evaluation using multiple Chinese AI detection datasets."
- Why unresolved: The current study relied exclusively on the NLPCC-2025 Task 1 corpus; it is unknown if the 95.94% accuracy of Qwen2.5-7B is an artifact of that specific dataset or a generalizable property of the architecture
- What evidence would resolve it: Testing the fine-tuned Qwen2.5-7B and encoder baselines on independent Chinese AI-detection benchmarks (e.g., CDT) to measure performance degradation

### Open Question 2
- Question: Does the prompt-based masked language modeling (MLM) approach provide a statistically significant advantage over standard classification heads for encoder models?
- Basis in paper: The Conclusion acknowledges that the "prompt-based masked language modeling approach for encoders lacks systematic comparison with standard classification heads."
- Why unresolved: While the novel prompt-based method was used for BERT/RoBERTa, the authors did not run an ablation study against a standard [CLS]-token classification head, making it unclear if the method or the model capacity caused the observed behaviors
- What evidence would resolve it: A controlled experiment comparing the prompt-based MLM objective against a standard fine-tuning objective on the same encoder architectures using the same data splits

### Open Question 3
- Question: Do non-instruction-tuned base models capture AI-generated text signatures more effectively than reasoning-distilled or instruction-tuned variants?
- Basis in paper: The authors hypothesize in the results section that "models without instruction fine-tuning might better capture the inherent logic," but admit this "remains speculative" due to "multiple confounding factors" (e.g., different distillation procedures)
- Why unresolved: Qwen2.5-7B (base) consistently outperformed DeepSeek-R1-Distill-Qwen-7B, but because the weights and training histories differ significantly, the specific cause (instruction tuning vs. distillation quality) cannot be isolated
- What evidence would resolve it: A study comparing a single base model against its own instruction-tuned and distilled variants while controlling for pretraining corpora and architecture

## Limitations
- Key hyperparameters remain unspecified: learning rates, batch sizes, epochs, and early stopping patience are absent from methodology
- LoRA implementation details missing: alpha values, dropout rates, and specific projection layers receiving adapters are not documented
- FastText baseline lacks training procedure details beyond basic parameters
- Hardware configuration and training duration are not provided

## Confidence

**High Confidence**: LoRA-adapted decoder LLMs (Qwen2.5-7B) achieve superior test performance (95.94%) compared to encoder models (RoBERTa: 76.31%, BERT: 79.3%) on Chinese AI-generated text detection. The test accuracy difference is substantial and the precision-recall balance indicates genuine robustness rather than threshold optimization.

**Medium Confidence**: The performance gap stems from LoRA's parameter-efficient fine-tuning preventing overfitting while preserving decoder pretraining benefits. While the overfitting evidence is clear (RoBERTa train accuracy 99.65% vs test 76.31%), the relative contribution of LoRA versus decoder architecture versus pretraining scale cannot be disentangled without additional experiments.

**Low Confidence**: Prompt-based masked language modeling for encoders provides implicit regularization benefits. The paper asserts this improves out-of-domain generalization but provides no comparative evidence against standard [CLS] head fine-tuning, and the empirical results show encoders still severely overfit despite this approach.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary LoRA rank (r=4,8,16), learning rate, and batch size for Qwen2.5-7B; document train-test performance curves to identify overfitting signatures and establish optimal configuration ranges.

2. **Architecture Ablation Study**: Apply identical LoRA fine-tuning (same rank, learning rate, regularizers) to Chinese RoBERTa-wwm-ext-large and compare performance to Qwen2.5-7B. This isolates whether the encoder-decoder gap is due to LoRA regularization effectiveness or fundamental architectural differences.

3. **Cross-Dataset Generalization Test**: Evaluate the best-performing LoRA-adapted Qwen2.5-7B checkpoint on an independent Chinese AI-generated text dataset (if available) or create a held-out test split from the training data using different AI generators to validate claims of true out-of-domain robustness beyond distribution shift within the same corpus.