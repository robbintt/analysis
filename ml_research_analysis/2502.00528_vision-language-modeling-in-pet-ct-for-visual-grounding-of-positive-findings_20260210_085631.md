---
ver: rpa2
title: Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings
arxiv_id: '2502.00528'
source_url: https://arxiv.org/abs/2502.00528
tags:
- suvmax
- contextual
- visual
- grounding
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of visual grounding in whole-body
  PET/CT imaging, where connecting text descriptions of lesions to their specific
  image locations could enhance radiology reporting and clinical workflows. The authors
  developed an automated weak-labeling pipeline that extracts lesion descriptions
  containing SUVmax values and axial slice numbers from radiology reports, then generates
  corresponding 3D segmentation labels using an iterative thresholding algorithm.
---

# Vision-Language Modeling in PET/CT for Visual Grounding of Positive Findings

## Quick Facts
- arXiv ID: 2502.00528
- Source URL: https://arxiv.org/abs/2502.00528
- Reference count: 31
- Primary result: Vision-language model achieved F1=0.80 for visual grounding of PET/CT lesions, outperforming LLM-based and 2.5D approaches but falling short of human performance

## Executive Summary
This study develops a vision-language model for visual grounding of positive findings in whole-body PET/CT imaging, addressing the challenge of connecting text descriptions to specific image locations. The authors created a large dataset of 11,356 image-text pairs from 25,578 PET/CT exams using an automated weak-labeling pipeline that extracts SUVmax values and axial slice numbers from radiology reports. Their 3D vision-language model, ConTEXTual Net 3D, achieved F1=0.80 performance, significantly outperforming baseline methods but falling short of two nuclear medicine physicians (F1=0.94 and 0.91). The model showed particular success with FDG and DCFPyL radiotracers but struggled with DOTATE and Fluciclovine, suggesting opportunities for improvement with larger, more diverse datasets.

## Method Summary
The authors developed an automated weak-labeling pipeline to create training data by extracting lesion descriptions containing SUVmax values and axial slice numbers from radiology reports, then generating corresponding 3D segmentation labels using an iterative thresholding algorithm. From this pipeline applied to 25,578 PET/CT exams, they created 11,356 labeled image-text pairs spanning multiple radiotracer types. The core model, ConTEXTual Net 3D, integrates text embeddings with a 3D nnU-Net via cross-attention to perform visual grounding. The model was trained and evaluated against two baseline approaches: an LLM-based method (LLMSeg) and a 2.5D approach, with performance measured using F1 scores and compared to human readers.

## Key Results
- ConTEXTual Net 3D achieved F1=0.80 for visual grounding, significantly outperforming LLMSeg (F1=0.22) and 2.5D approach (F1=0.53)
- Model performance varied by radiotracer type: FDG (F1=0.78), DCFPyL (F1=0.75), Fluciclovine (F1=0.66), DOTATE (F1=0.58)
- Two nuclear medicine physicians achieved higher performance (F1=0.94 and 0.91), establishing human performance benchmarks

## Why This Works (Mechanism)
The vision-language approach works by leveraging the structured information in radiology reports (SUVmax values and axial slice numbers) to create weak labels that guide the model's learning. The cross-attention mechanism between text embeddings and 3D nnU-Net allows the model to associate textual descriptions with specific spatial locations in the PET/CT volumes. The automated labeling pipeline enables large-scale dataset creation without manual annotation, while the 3D architecture captures the volumetric nature of PET/CT imaging that 2D or 2.5D approaches miss. The performance variation across radiotracer types suggests the model learns radiotracer-specific features that influence its grounding accuracy.

## Foundational Learning
- **Weak supervision in medical imaging**: Uses readily available report data to generate training labels without manual annotation, enabling large-scale dataset creation
- **Vision-language grounding**: Connects textual descriptions to specific image regions through cross-attention mechanisms, essential for multimodal understanding
- **3D medical image segmentation**: Processes volumetric data end-to-end rather than slice-by-slice, capturing spatial context across adjacent slices
- **Automated label extraction**: Parses structured report information (SUVmax, slice numbers) to create training targets, reducing human annotation burden
- **Iterative thresholding algorithms**: Generates segmentation masks from quantitative metrics, providing consistent but potentially imperfect labels
- **Radiotracer-specific imaging characteristics**: Different tracers produce distinct uptake patterns that affect model performance and generalization

## Architecture Onboarding

**Component map**: Report text -> Text embedding module -> Cross-attention -> 3D nnU-Net -> Segmentation output

**Critical path**: The weak-labeling pipeline (report parsing â†’ threshold-based segmentation) feeds training data to ConTEXTual Net 3D, where text embeddings are combined with 3D features through cross-attention to produce final segmentation masks.

**Design tradeoffs**: The automated labeling approach enables large-scale training data creation but may introduce systematic errors compared to manual annotation. The 3D architecture captures volumetric context but requires more computational resources than 2D/2.5D alternatives. The vision-language integration allows natural language queries but depends on consistent report formatting.

**Failure signatures**: Poor performance on DOTATE and Fluciclovine suggests the model struggles with less common radiotracer types or those with different imaging characteristics. The significant gap between model (F1=0.80) and human performance (F1=0.94-0.91) indicates limitations in the weak-labeling approach or model architecture.

**First experiments**: 1) Test model performance on held-out report text to verify grounding capability without visual input 2) Evaluate label quality by comparing automated segmentations to a small set of manually annotated lesions 3) Assess model robustness by training on subsets of radiotracer types to identify generalization patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Automated weak-labeling pipeline may introduce systematic errors due to inconsistent report formatting or inaccurate extraction of SUVmax values and axial slice numbers
- Performance degradation on DOTATE and Fluciclovine radiotracers suggests limited generalization to less common imaging types
- Comparison to human performance using only two nuclear medicine physicians may not represent typical radiologist performance or account for inter-reader variability

## Confidence
- **High confidence**: Technical feasibility of vision-language modeling for PET/CT visual grounding is well-demonstrated with F1=0.80 and clear improvement over baselines
- **Medium confidence**: Clinical utility and generalizability across institutions remains uncertain due to limited validation on diverse reporting styles
- **Medium confidence**: Weak-labeling pipeline accuracy is reasonable but unverified through extensive manual validation

## Next Checks
1. External validation on PET/CT datasets from multiple institutions with different reporting formats and radiological practices to assess generalizability
2. Detailed error analysis comparing model failures to human reader disagreements, particularly for the lower-performing radiotracer types (DOTATE and Fluciclovine)
3. A prospective clinical study evaluating whether the model's visual grounding outputs actually improve radiology reporting efficiency or diagnostic accuracy in real-world workflows