---
ver: rpa2
title: Automatic Classification of Arabic Literature into Historical Eras
arxiv_id: '2601.16138'
source_url: https://arxiv.org/abs/2601.16138
tags:
- eras
- arabic
- classification
- class
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the first comprehensive analysis of automatic
  temporal classification of Arabic literature beyond poetry. Using neural networks
  and deep learning, the authors classify Arabic texts into predefined historical
  eras and custom time periods.
---

# Automatic Classification of Arabic Literature into Historical Eras

## Quick Facts
- arXiv ID: 2601.16138
- Source URL: https://arxiv.org/abs/2601.16138
- Authors: Zainab Alhathloul; Irfan Ahmad
- Reference count: 15
- Primary result: First comprehensive analysis of automatic temporal classification of Arabic literature beyond poetry using neural networks

## Executive Summary
This study presents the first comprehensive analysis of automatic temporal classification of Arabic literature beyond poetry, using neural networks and deep learning to classify Arabic texts into predefined historical eras and custom time periods. Experiments on two public datasets (OpenITI and APCD) reveal that classification performance ranges from F1-scores of 0.83 (binary classification) to 0.18 (15-class classification), depending on the dataset and setup. Fully connected feedforward networks outperform recurrent models in most cases. Notably, authorial style significantly impacts classification, with author-disjoint splits yielding lower accuracy than author-merged splits. Lemmatization improves performance for poetry but not for non-poetry texts. Confusion matrices show that eras are often misclassified with adjacent periods, suggesting gradual linguistic evolution rather than sharp historical boundaries.

## Method Summary
The authors experimented with multiple neural network architectures (ANNs, RNNs, CNNs) using sparse lexical features (BoW/TF-IDF) and dense embeddings on two public Arabic literature datasets: OpenITI (1.3M texts, 1500-1922 CE) and APCD (8.4K poems, 512 BCE-1925 CE). They tested various classification setups including binary genre classification, 5-era classification, and finer-grained period classification. The experiments systematically varied preprocessing steps including stopword removal, lemmatization using Farasa, and different train-test splits (author-disjoint vs. author-merged). Performance was measured using macro-averaged F1-score to handle class imbalance.

## Key Results
- Fully connected ANNs achieved the highest F1-scores across most classification tasks, outperforming RNNs and CNNs
- Lemmatization improved classification for poetry but not prose, suggesting genre-specific morphological complexity
- Misclassifications predominantly occurred between adjacent historical periods, indicating gradual linguistic evolution
- Author-disjoint splits yielded significantly lower accuracy than author-merged splits, revealing strong author-specific stylistic influence
- Performance degraded substantially as classification granularity increased from binary to 15-class setups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fully connected ANNs outperform sequential models (RNNs/CNNs) on era classification when using sparse lexical features (BoW/TF-IDF).
- Mechanism: Historical eras manifest primarily through vocabulary distribution shifts (word frequency, presence/absence patterns) rather than sequential or positional patterns. Sparse representations coupled with dense layers capture these distributional differences directly, while RNN embeddings add unnecessary abstraction that dilutes discriminative signal.
- Core assumption: Era-defining features are primarily lexical (vocabulary-level) rather than syntactic or sequential.
- Evidence anchors:
  - [abstract] "Fully connected artificial neural networks outperform other models, especially on poetry"
  - [Section 6.1.1, Table 10 vs Table 12] ANN achieves 0.436 F1 on 5-era OpenITI vs RNN's 0.396; similar gaps across all multiclass setups
  - [Section 6.3] Word-frequency analysis shows era classification correlates with aggregate word counts per era, suggesting lexical distribution matters more than sequence

### Mechanism 2
- Claim: Lemmatization improves classification on poetry but not prose because poetry has higher vocabulary richness and morphological variation.
- Mechanism: Poetry employs denser, more varied vocabulary with greater inflectional diversity. Lemmatization reduces sparsity by collapsing morphological variants to lemmas, improving generalization. Prose uses more repetitive, functional vocabulary where inflectional variation is lower, so lemmatization removes potentially discriminative signals without sufficient sparsity reduction benefit.
- Core assumption: The vocabulary-sparsity reduction tradeoff favors lemmatization only when baseline sparsity is high (poetry).
- Evidence anchors:
  - [Section 6.1.2] "Lemmatization has a stronger positive impact on APCD than on OpenITI, likely owing to richer vocabulary and reduced sparsity"
  - [Section 6.2.2, Table 18] APCD 5-era with lemmatization: 0.691 F1; without explicit mention, implies prose (OpenITI) showed no improvement
  - [corpus] No direct corpus evidence on lemmatization effects across genres

### Mechanism 3
- Claim: Misclassifications cluster between adjacent eras because literary change is gradual and period boundaries are porous, not discrete.
- Mechanism: Language evolution is continuous; vocabulary and style shift incrementally across decades. Historians impose discrete boundaries for convenience, but texts near boundaries share lexical overlap with both neighboring periods. Models learn this overlap and correctly predict adjacent eras for boundary-proximal texts—this is not random error but accurate reflection of linguistic continuity.
- Core assumption: Gradual linguistic change produces genuine lexical overlap at period boundaries.
- Evidence anchors:
  - [abstract] "Misclassifications often occur between adjacent eras, reflecting gradual literary evolution"
  - [Section 6.3, Figure 3] >50% of Islamic era samples misclassified as Abbasid or Aldoul; ~50% of Modern misclassified as Aldoul or Ottoman—both adjacent
  - [Section 6.3] "Performance was recomputed after merging adjacent eras. For OpenITI, accuracy increases from 43.6% to 76.6%; for poetry, from 65.4% to 93.1%"

## Foundational Learning

- **Concept: Author-disjoint vs. merged-author data splits**
  - Why needed here: Determines whether models learn era-specific stylometric signals or author-specific writing style. Author-disjoint splits force generalization to temporal features; merged splits allow author memorization to inflate performance.
  - Quick check question: If you achieve 90% accuracy with merged authors but 50% with author-disjoint splits on the same task, what does this imply about what the model learned?

- **Concept: Class imbalance and macro-averaging**
  - Why needed here: Historical corpora often have unequal class distributions (e.g., more Abbasid texts than pre-Islamic). Macro-averaging weights all classes equally, preventing majority-class dominance from masking poor minority-class performance.
  - Quick check question: Why would accuracy alone be misleading if 80% of your corpus is from one era?

- **Concept: Sparse vs. dense text representations**
  - Why needed here: BoW/TF-IDF (sparse) preserve exact vocabulary presence but scale with vocab size; embeddings (dense) compress semantics but may lose discriminative word-specific signals. Choice depends on whether task relies on specific word indicators or semantic similarity.
  - Quick check question: For detecting archaic words that only appear in pre-Islamic texts, which representation preserves this signal better?

## Architecture Onboarding

- **Component map:** Input → Preprocessing (normalization, optional stopword removal, optional lemmatization) → Tokenization (word-level for ANN/RNN; character-level option for RNN) → Representation (BoW/TF-IDF for ANN; embeddings for RNN) → Model (ANN: Dense+Dropout layers; RNN: Embedding→BiGRU→Dense) → Softmax output → Era prediction

- **Critical path:**
  1. Vocabulary capping (15K for ANN, 20-80K for RNN) controls overfitting
  2. Dropout rate (0.7) critical for regularization on limited data
  3. Early stopping on validation accuracy prevents overfitting to training eras

- **Design tradeoffs:**
  - ANN + BoW/TF-IDF: Faster training, better multiclass performance, interpretable features; loses word order
  - RNN + Embeddings: Captures sequence, handles OOV via character-level; slower, underperforms on this task
  - Stopword removal: Expected to help, but experiments show retaining stopwords improves results (stopwords may carry temporal distributional signal)
  - Lemmatization: Helps poetry (high vocab sparsity), neutral/harmful for prose

- **Failure signatures:**
  - Accuracy near random chance (1/N classes): Model not learning temporal features; check preprocessing, vocabulary size, or class overlap
  - Validation >> Test performance: Overfitting to author style; switch to author-disjoint splits
  - Adjacent-era confusion dominates: Not a bug—reflects genuine boundary overlap; consider merging adjacent classes or using hierarchical classification
  - Binary task works (0.83 F1), multiclass collapses (0.20 F1): Signal insufficient for fine-grained distinctions; increase input length, reduce classes, or add features (named entities, metadata)

- **First 3 experiments:**
  1. **Baseline establishment**: Train ANN with BoW on 5-era OpenITI using author-disjoint splits. Record macro F1. If <0.35, increase sample word count from 100 to 200+ words.
  2. **Lemmatization ablation**: Repeat experiment 1 on APCD with and without Farasa lemmatization. Expect +5-10% F1 gain for poetry; minimal gain for prose.
  3. **Boundary analysis**: For 5-era setup, identify samples with prediction probability <0.4 for winning class. Check if these cluster near historical boundary dates. If yes, this confirms gradual-change hypothesis rather than model failure.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can pretrained transformer models (e.g., AraBERT, MARBERT) outperform current ANN baselines for era classification despite being trained predominantly on modern texts?
- Basis in paper: [explicit] The authors explicitly propose to "evaluate pretrained transformers... while accounting for the fact that many Arabic transformers are trained predominantly on modern-era data and may introduce temporal bias."
- Why unresolved: The study utilized ANNs, RNNs, and CNNs but did not test transformer architectures, leaving their efficacy and potential temporal bias on historical corpora unverified.
- What evidence would resolve it: Experiments fine-tuning transformers on the OpenITI and APCD datasets and comparing their performance against the reported ANN F1-scores.

### Open Question 2
- Question: Does removing text reuse (common boilerplate passages) from the OpenITI corpus improve the accuracy of automatic era classification?
- Basis in paper: [explicit] The authors note that roughly 20% of OpenITI is "reused material" and state, "Future work could examine how using this de-reused corpus affects temporal classification performance."
- Why unresolved: The experiments utilized the standard OpenITI corpus; the impact of noise from frequent, non-discriminative religious or literary phrases on model confusion remains unquantified.
- What evidence would resolve it: Re-training the best-performing models on the de-reused version of OpenITI (released by Belinkov et al.) and comparing the results to the original 0.83 F1-score.

### Open Question 3
- Question: Can a specialized lemmatization tool for Classical Arabic improve classification performance more effectively than the general-purpose Farasa tool?
- Basis in paper: [explicit] The authors conclude that "improving lemmatization for Classical Arabic, which proved less effective in the present experiments; such advances could enhance era classification."
- Why unresolved: While lemmatization aided poetry, it was less effective for non-poetry texts, suggesting the current tool's inability to handle historical morphology limited the model's feature extraction.
- What evidence would resolve it: Developing or applying a historical-aware lemmatizer and measuring the resulting change in F1-scores across the different era setups.

## Limitations
- Reliance on binary genre splits (poetry vs. non-poetry) for APCD dataset may oversimplify complex temporal evolution
- Weak corpus evidence for key assumptions, particularly regarding lemmatization effects and gradual linguistic change at era boundaries
- Absence of adjacent-era overlap studies weakens mechanistic claims about boundary confusion
- Focus on lexical features may miss important syntactic or semantic temporal patterns

## Confidence

**High Confidence**: The observation that fully connected ANNs outperform RNNs on sparse lexical features is well-supported by systematic comparisons across multiple experimental setups (Section 6.1.1). The author-disjoint split methodology and its impact on classification performance is also robustly demonstrated.

**Medium Confidence**: The claim that lemmatization benefits poetry more than prose due to vocabulary richness has some support but lacks direct corpus evidence. The adjacent-era confusion reflecting gradual linguistic evolution is plausible but weakly anchored in corpus analysis.

**Low Confidence**: The assertion that stopwords carry temporal distributional signals sufficient to warrant retention requires further validation. The mechanism linking morphological variation to lemmatization effectiveness in different genres needs stronger empirical support.

## Next Checks

1. **Author-effect ablation**: Re-run the 5-era classification on OpenITI using three author-split strategies: author-disjoint, author-merged, and hybrid (half authors disjoint, half merged). Compare performance differences to quantify how much accuracy gain comes from author memorization versus era learning.

2. **Boundary proximity analysis**: For misclassified samples in the 5-era setup, extract publication dates and compute distance to era boundaries. Test whether misclassification probability correlates with temporal proximity to adjacent eras using logistic regression. This would provide statistical evidence for the gradual-change hypothesis.

3. **Feature importance decomposition**: Apply LIME or SHAP to ANN models trained on 5-era data to identify specific words driving era predictions. Compare feature importance distributions across eras to determine whether temporal classification relies on era-specific vocabulary (e.g., archaic terms) or general stylistic patterns.