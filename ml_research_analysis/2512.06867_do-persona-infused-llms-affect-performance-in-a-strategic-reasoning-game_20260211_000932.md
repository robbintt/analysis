---
ver: rpa2
title: Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?
arxiv_id: '2512.06867'
source_url: https://arxiv.org/abs/2512.06867
tags:
- heuristic
- persona
- region
- personality
- player
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether persona-prompting affects strategic
  decision-making in a world-domination board game. Two methods for translating persona
  descriptions into game heuristics were compared: direct heuristic inference and
  a personality inventory approach.'
---

# Do Persona-Infused LLMs Affect Performance in a Strategic Reasoning Game?

## Quick Facts
- **arXiv ID**: 2512.06867
- **Source URL**: https://arxiv.org/abs/2512.06867
- **Reference count**: 40
- **Primary result**: Persona prompting alone doesn't reliably produce distinct behaviors, but structured personality inventories can enhance behavioral alignment with persona traits in strategic settings.

## Executive Summary
This study investigates whether persona-prompting affects strategic decision-making in a world-domination board game. Two methods for translating persona descriptions into game heuristics were compared: direct heuristic inference and a personality inventory approach. The inventory method produced heuristics with higher face validity and stronger correlations with persona traits such as strategic thinking and domain expertise. Certain personas associated with strategic thinking achieved better game performance, but only when using the inventory-based translation. Smaller language models showed weaker consistency in heuristic assignments compared to GPT-4. The results suggest that direct persona prompting alone does not reliably produce distinct behaviors, while structured personality inventories can enhance behavioral alignment with persona traits in strategic settings.

## Method Summary
The study used PERIL, a Risk-inspired board game, with 50 personas selected from Persona Hub using variance-maximizing greedy algorithm. Two methods generated heuristics: Direct Heuristic (DH) prompts asking LLMs to assign weights directly, and Personality Inventory (PI) method where LLMs answered structured behavioral questions that were algorithmically mapped to heuristics. Four LLMs participated (gpt-4o-2024-11-20, Meta-Llama-3-8B-Instruct, Llama-4-Maverick-17B-128E-Instruct-FP8, Mistral-Small-Instruct-2409). Performance was measured through 49-round tournaments (1,225 games) using TrueSkill ratings, with Spearman correlations computed between persona features and performance outcomes.

## Key Results
- PI method heuristics showed significant correlations with strategicThinker (ρ = 0.49***) and domainExpert (ρ = 0.41***) persona traits using GPT-4
- DH method showed weak or non-significant correlations between persona features and performance
- Smaller models (Mistral, LLaMA 3) demonstrated poor opposite-value consistency compared to GPT-4, with some showing positive values where negative were expected
- Strategic personas achieved better performance than non-strategic ones only under the PI method

## Why This Works (Mechanism)

### Mechanism 1: Structured Mediation via Personality Inventories
The personality inventory acts as an intermediate representation layer. Instead of asking LLMs to directly map abstract persona descriptions to heuristic weights, the LLM first responds to concrete behavioral items (e.g., "I prefer to spread my influence to less contested regions"). These responses are then algorithmically mapped to heuristics using predetermined positive/negative associations, reducing variance in interpretation.

### Mechanism 2: Psychometric Mapping with Forced Opposite-Value Consistency
The inventory method implicitly enforces that logically opposing heuristics receive opposing weights. Each inventory item maps positively to one set of heuristics and negatively to opposing ones. For example, "I prefer to spread my influence to less contested regions" adds points to PTL (attack weakest regions) while subtracting from PTM (attack strongest regions).

### Mechanism 3: Separation of Persona Interpretation from Heuristic Assignment
The PI method separates "who the persona is" from "how they play." In the DH method, the LLM simultaneously interprets the persona description AND assigns heuristic weights in one step, creating opportunity for confounds. The PI method first has the LLM answer inventory items as the persona, then uses a deterministic algorithm to map those answers to weights.

## Foundational Learning

- **Persona Prompting**: Why needed - The entire experimental manipulation depends on assigning personality descriptions to LLMs and measuring whether these produce behavioral differences. Quick check - Can you explain why simply telling an LLM to "act like a strategic thinker" might not reliably change its decision-making behavior?

- **Heuristic-Based Game Agents**: Why needed - PERIL doesn't use LLMs to play directly; LLMs generate heuristic weights that guide a separate game-playing agent. This architectural choice is critical to understanding why the PI method works. Quick check - If an LLM assigned equal weights to "attack the strongest player" and "attack the weakest player," what would behavior look like during gameplay?

- **Face Validity vs. Construct Validity**: Why needed - The paper claims PI heuristics have higher face validity but explicitly disclaims deeper psychometric validity. Understanding this distinction prevents over-claiming about what the results prove. Quick check - If "retired military general" personas prioritize attacking weaker opponents, is that face validity, construct validity, both, or neither?

## Architecture Onboarding

- **Component map**: Persona Source -> Heuristic Generation Module (DH/PI) -> PERIL Game Engine -> Tournament Runner -> TrueSkill Evaluator
- **Critical path**: Generate heuristics for each persona using chosen method (DH or PI) -> Run tournament (49 rounds × 25 games = 1,225 games per run) -> Compute TrueSkill ratings and correlate with persona feature annotations
- **Design tradeoffs**: Fixed vs. adaptive heuristics (set at game start, don't adapt to game state); Two-player games (reduces confounds from multi-agent dynamics but makes some heuristics less discriminative); TrueSkill convergence (rating locks in after many games)
- **Failure signatures**: Low correlation between runs for same method (PI runs: ρ = 0.278, p = 0.051); Small models showing positive values in opposite-value consistency table; Non-strategic personas outperforming strategic ones
- **First 3 experiments**: 1) Baseline replication: Run DH and PI methods with GPT-4 on same 50 personas; 2) Model scaling test: Run PI method with LLaMA-3-70B to test whether improved opposite-value consistency scales with model size; 3) Inventory item ablation: Remove half the inventory items randomly and measure correlation degradation

## Open Questions the Paper Calls Out

- **Open Question 1**: How do persona-infused LLMs perform in adversarial multi-agent settings where agents may engage in strategic deception? The current study only examined two-player matches with uniform world-domination objectives; the PERIL platform supports missions enabling deception, but this feature was not tested.

- **Open Question 2**: Why did personality inventory (PI) runs show lower cross-run ranking consistency (ρ = 0.278) than direct heuristic (DH) runs (ρ = 0.524)? The PI method produced heuristics with stronger persona-trait correlations yet less stable player rankings across runs, suggesting a tension between behavioral distinctiveness and performance predictability.

- **Open Question 3**: Do persona-derived heuristics align with actual human behavioral patterns beyond face validity? Current validation relies on LLM-based annotations and intuitive face validity; no human-subject data confirms that personas translate into human-consistent strategic behaviors.

## Limitations

- The study's focus on a single game (PERIL) and relatively small persona set (50 personas) constrains generalizability to other strategic domains
- Opposite-value consistency findings for smaller models suggest the PI method may not scale reliably below GPT-4's capability level
- The behavioral validity of the heuristics—whether they actually capture meaningful strategic dimensions—remains untested beyond face validity assessments

## Confidence

- **High confidence**: The DH method produces weak or non-significant correlations between persona traits and performance
- **Medium confidence**: The PI method achieves higher face validity and moderate-to-strong correlations for strategic traits within this specific game context
- **Low confidence**: Claims about the PI method's superiority for smaller models or its applicability to other strategic domains

## Next Checks

1. **Cross-domain validation**: Apply the PI method to a different strategic game (e.g., chess or Go) and measure whether the same correlation patterns between persona traits and performance emerge

2. **Inventory item sensitivity**: Systematically remove inventory items and measure correlation degradation to establish whether the mapping captures genuine strategic dimensions or exploits artifacts in the current setup

3. **Multi-run stability test**: Run 10 tournament iterations with fixed personas and random seeds to establish the variance and confidence intervals around TrueSkill ratings for each method