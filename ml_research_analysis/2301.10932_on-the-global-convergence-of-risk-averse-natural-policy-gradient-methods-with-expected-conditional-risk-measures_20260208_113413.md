---
ver: rpa2
title: On the Global Convergence of Risk-Averse Natural Policy Gradient Methods with
  Expected Conditional Risk Measures
arxiv_id: '2301.10932'
source_url: https://arxiv.org/abs/2301.10932
tags:
- policy
- where
- have
- risk-averse
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes global convergence guarantees for risk-averse
  natural policy gradient (NPG) methods applied to infinite-horizon Markov decision
  processes (MDPs) with Expected Conditional Risk Measures (ECRMs). The authors derive
  entropy-regularized NPG updates for ECRMs-based RL problems and prove linear convergence
  rates for both exact and inexact policy evaluation settings.
---

# On the Global Convergence of Risk-Averse Natural Policy Gradient Methods with Expected Conditional Risk Measures

## Quick Facts
- arXiv ID: 2301.10932
- Source URL: https://arxiv.org/abs/2301.10932
- Authors: Xian Yu; Lei Ying
- Reference count: 40
- One-line primary result: Establishes global convergence guarantees for risk-averse NPG methods with ECRMs, achieving O(κ₂/βτγ · log(C₁γ/ϵ)) iteration complexity.

## Executive Summary
This paper addresses the challenge of risk-averse reinforcement learning in infinite-horizon MDPs by developing a natural policy gradient method based on Expected Conditional Risk Measures (ECRMs). The authors reformulate the dynamic risk-averse problem as a risk-neutral MDP on an augmented state-action space, enabling the application of standard optimization techniques. They prove global linear convergence rates for both exact and inexact policy evaluation settings, providing the first global convergence guarantees for risk-averse NPG algorithms.

## Method Summary
The method transforms the risk-averse RL problem into a risk-neutral MDP by augmenting the state and action spaces with an auxiliary variable representing the Value-at-Risk threshold. This allows the use of entropy-regularized natural policy gradient updates with softmax parameterization. The algorithm employs separate policy update rules for the initial step and subsequent steps to handle structural differences in the cost function. The Fisher information matrix is used to compute natural gradients, ensuring stable updates invariant to reparameterization.

## Key Results
- Achieves O(κ₂/βτγ · log(C₁γ/ϵ)) iteration complexity for reaching ||Q*τ - Q(t+1)τ||∞ ≤ ϵ under exact policy evaluation
- Proves linear convergence rates for both exact and inexact policy evaluation settings
- Demonstrates improved convergence compared to standard policy gradient methods on stochastic Cliffwalk environment when varying regularization weight τ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper transforms the dynamic risk-averse problem into a risk-neutral Markov Decision Process (MDP) on an augmented state-action space to allow standard optimization techniques.
- Mechanism: By introducing an auxiliary variable $\eta$ (representing the Value-at-Risk threshold) into the state and action spaces, the Expected Conditional Risk Measure (ECRM) is reformulated. The immediate cost function is modified to a "risk-averse cost" $\bar{C}$, which includes a term $\lambda/\alpha [C - \eta]^+$ penalizing tail costs. This allows the use of standard Bellman operators on the extended space $(s, \eta)$.
- Core assumption: The user discretizes the continuous space of $\eta \in [0,1]$ into a finite set $H$ with resolution $I$ sufficient to approximate the Lipschitz continuous cost function.
- Evidence anchors:
  - [Page 4] "We extend the action space $a_t \in A$ to $(a_t, \eta_{t+1}) \in A \times [0, 1]$... and manipulate the immediate costs..."
  - [Page 5] Proposition 1 guarantees an $\epsilon_{opt}$-optimal policy if discretization resolution $I$ is sufficiently high.
  - [corpus] Weak relevance; neighboring papers focus on CVaR gradients or constraints rather than this specific ECRM state-augmentation technique.
- Break condition: If the discretization resolution $I$ is too low (specifically $I < (1 + 1/\alpha) \frac{\lambda \gamma}{1-\gamma} \frac{1}{\epsilon_{opt}}$), the approximation error exceeds the optimality tolerance, breaking the guarantee of finding an $\epsilon$-optimal policy for the original problem.

### Mechanism 2
- Claim: Entropy-regularized Natural Policy Gradient (NPG) updates with softmax parameterization drive global linear convergence to the optimal regularized policy.
- Mechanism: The algorithm uses a KL-divergence constraint (approximated via the Fisher information matrix $F_\rho$) to update policy parameters, rather than standard gradient descent. This "natural" direction prevents the policy from changing too drastically. Entropy regularization ($\tau > 0$) is added to the objective to ensure the policy remains stochastic, smoothing the optimization landscape and preventing premature convergence to deterministic local optima.
- Core assumption: The learning rate $\beta$ must remain within a specific bound relative to the regularization weight $\tau$ and Fisher coefficients.
- Evidence anchors:
  - [Abstract] "We provide global optimality and iteration complexity... with softmax parameterization and entropy regularization..."
  - [Page 9, Theorem 4] Establishes linear convergence rate $\mathcal{O}((1 - \frac{\beta \tau \gamma}{\kappa_2})^t)$ assuming $\beta \le \frac{\kappa_2(1-\gamma)}{\tau \gamma}$.
  - [corpus] "Ordering-based Conditions for Global Convergence..." supports the general premise that representation and update conditions dictate global convergence.
- Break condition: If the learning rate $\beta$ violates the upper bound (e.g., $\beta > \kappa_1/\tau$), the performance improvement theorem no longer guarantees monotonicity, potentially causing oscillation or divergence.

### Mechanism 3
- Claim: Separate policy update rules for the initial step ($\theta_1$) and subsequent steps ($\theta_2$) are required to handle structural differences in the cost function across time.
- Mechanism: The ECRM formulation defines immediate costs differently at $t=1$ versus $t \geq 2$ (due to the timing of the auxiliary $\eta$ decision). Consequently, the policy gradient theorem yields different weighting coefficients ($\kappa_1$ vs $\kappa_2 \gamma / (1-\gamma)$) for the gradients of the initial policy $\pi_1$ and the stationary policy $\pi_2$.
- Core assumption: The ratio of user-defined coefficients $\kappa_1 / \kappa_2$ must be bounded by the discount factor $\gamma$ to ensure convergence of the first-step value function.
- Evidence anchors:
  - [Page 7] "We separate the risk-averse NPG updates for $\theta_1$ and $\theta_2$... note that the different user-defined coefficients $\kappa_1$ and $\kappa_2$... are helpful to adjust the learning rates."
  - [Page 9] Theorem 4(iii) requires $\frac{\kappa_1}{\kappa_2} < \frac{1}{\gamma}$ for convergence bounds on the first-step Q-function.
- Break condition: If the ratio $\frac{\kappa_1}{\kappa_2} \ge \frac{1}{\gamma}$, the convergence proof for the initial step's value function (Assertion iii) fails, potentially leading to instability in the very first decision step despite convergence in later steps.

## Foundational Learning

- Concept: **Conditional Value-at-Risk (CVaR)**
  - Why needed here: The paper's risk measure (ECRM) is built upon a convex combination of Expectation and CVaR. You must understand that CVaR optimizes the expectation of the worst $\alpha\%$ of outcomes to grasp why the auxiliary variable $\eta$ and the cost term $[C - \eta]^+$ are introduced.
  - Quick check question: If the confidence level $\alpha$ is set very low (e.g., 0.01), does the algorithm become more sensitive or less sensitive to rare, high-cost events?

- Concept: **Natural Policy Gradient (NPG)**
  - Why needed here: This is the core optimizer. Unlike vanilla policy gradient, NPG pre-conditions the gradient by the inverse Fisher information matrix. This ensures the update step is invariant to re-parameterization and allows for larger, more stable steps.
  - Quick check question: Why does the Fisher information matrix appear in the denominator of the update step $\theta_{t+1} = \theta_t - \beta F^{-1} \nabla J$?

- Concept: **Entropy Regularization**
  - Why needed here: The theoretical guarantees rely on the policy retaining sufficient randomness (entropy) to ensure the Q-functions are smooth and the optimal policy is unique. This term $\tau \mathcal{H}(\pi)$ is subtracted from the cost objective.
  - Quick check question: If the regularization weight $\tau$ is set to 0, does the paper still guarantee linear convergence for the softmax parameterization?

## Architecture Onboarding

- Component map:
  State Space -> State/Action Augmenter -> Cost Modifier -> Two-Head Policy Network -> NPG Optimizer

- Critical path:
  1. Initialize $\theta_1, \theta_2$ and sample initial state $s_1$.
  2. **First Step:** Sample $(a_1, \eta_2) \sim \pi_1$. Compute modified cost $\bar{C}_1$.
  3. **Recurrent Steps:** For $t \ge 2$, sample $(a_t, \eta_{t+1}) \sim \pi_2(\cdot|s_t, \eta_t)$. Compute modified cost $\bar{C}$.
  4. **Evaluation:** Estimate $Q_\tau$ using the trajectory of modified costs.
  5. **Update:** Calculate Fisher matrices $F_{\rho}^{\theta_1}$ and $F_{\rho}^{\theta_2}$; apply NPG updates to $\theta_1$ and $\theta_2$ separately using coefficients $\kappa_1, \kappa_2$.

- Design tradeoffs:
  - **Discretization Resolution ($I$):** Higher $I$ improves approximation accuracy of $\eta$ but increases the cardinality of the action space $|A \times H|$, slowing computation.
  - **Regularization ($\tau$):** Increasing $\tau$ stabilizes convergence (Fig 2 shows stability at $\tau=0.05$ vs instability at $\tau=0$) but biases the optimal policy away from the true risk-minimizing policy.

- Failure signatures:
  - **Instability at $t=0$:** If test costs spike initially, check the ratio $\kappa_1 / \kappa_2$. If it exceeds $1/\gamma$, the first-step policy may be diverging.
  - **Numerical Noise:** If using $\tau=0$ (no entropy), the paper notes numerical issues inverting the Fisher matrix (Page 16, "instability... caused by numerical issues"). Increase $\tau$ slightly to fix.

- First 3 experiments:
  1. **Cliffwalk Validation:** Replicate the stochastic Cliffwalk experiment (Fig 2) comparing PG vs. NPG with $\tau \in [0, 0.05]$. Verify that NPG with regularization converges in ~200 episodes while PG takes ~500.
  2. **Learning Rate Sensitivity:** Test the bounds of learning rate $\beta$. Verify that performance degrades or oscillates as $\beta$ approaches or exceeds $\frac{\kappa_2(1-\gamma)}{\tau \gamma}$.
  3. **Discretization Impact:** Run ablations on the grid size $I$ for the $\eta$ variable. Confirm that small $I$ leads to suboptimal risk avoidance (higher average cost in tail scenarios) consistent with Proposition 1.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can iteration complexity guarantees be established for ECRMs-based policy gradient algorithms using restricted policy classes, such as log-linear or neural network parameterizations, rather than the softmax parameterization?
- **Basis in paper:** [explicit] The conclusion explicitly identifies this as a direction for future research.
- **Why unresolved:** The current theoretical analysis relies on softmax parameterization to derive specific linear convergence rates, whereas function approximation introduces approximation errors not accounted for in the current proofs.
- **What evidence would resolve it:** A theoretical analysis providing iteration complexity bounds for log-linear policies or providing sample complexity bounds for two-layer neural networks in the ECRM setting.

### Open Question 2
- **Question:** How does the discretization of the auxiliary $\eta$-space affect the algorithm's convergence and sample complexity in high-dimensional or continuous problems?
- **Basis in paper:** [inferred] The paper assumes a discretized space $\mathcal{H}$ (Section 2.3) to handle the continuous VaR variable $\eta$, noting that the discretization resolution $I$ must be large to ensure $\epsilon$-optimality.
- **Why unresolved:** While the paper notes iteration complexity is "almost" dimension-free, the sample complexity in Remark 3 scales with $|\mathcal{H}|$, which grows exponentially with the discretization resolution, potentially creating a bottleneck.
- **What evidence would resolve it:** An analysis quantifying the trade-off between discretization resolution, computational cost, and convergence accuracy, or a proof of convergence using continuous $\eta$-spaces.

### Open Question 3
- **Question:** Do similar global convergence guarantees exist for risk-averse NPG methods using dynamic risk measures other than ECRMs?
- **Basis in paper:** [inferred] The authors utilize the specific decomposable structure of ECRMs to reformulate the problem (Section 2.3), implying the proofs rely heavily on properties unique to this risk measure class.
- **Why unresolved:** It is unclear if the contraction mapping properties and gradient forms required for the NPG convergence proof hold for general coherent risk measures or other dynamic risk metrics.
- **What evidence would resolve it:** Extending the convergence analysis to other time-consistent risk measures or providing a counter-example showing where the NPG update fails to converge for non-ECRM measures.

## Limitations
- Assumes exact policy evaluation, which is impractical in real-world applications
- Discretization of auxiliary risk variable $\eta$ introduces approximation errors
- Empirical validation limited to single grid-world environment (Cliffwalk)
- Convergence guarantees rely on specific conditions on Fisher coefficients and learning rates

## Confidence
- **High Confidence:** The core theoretical framework for transforming risk-averse RL into risk-neutral MDPs via state augmentation, and the derivation of entropy-regularized NPG updates are mathematically sound and well-supported by proofs.
- **Medium Confidence:** The linear convergence rates under exact policy evaluation are rigorously proven, but the practical implications are limited by the unrealistic assumption of exact value function knowledge.
- **Low Confidence:** The numerical validation on the Cliffwalk environment provides limited evidence for the practical utility of the method, particularly regarding its performance on more complex, high-dimensional tasks.

## Next Checks
1. **Policy Evaluation Robustness:** Conduct experiments comparing exact policy evaluation with approximate methods (e.g., TD learning) to quantify the impact of policy evaluation error on convergence rates and final performance.
2. **Scalability Assessment:** Test the algorithm on continuous control benchmarks (e.g., MuJoCo tasks) to evaluate its scalability and performance relative to state-of-the-art risk-averse RL methods.
3. **Hyperparameter Sensitivity:** Systematically explore the sensitivity of convergence to the learning rate $\beta$, regularization weight $\tau$, and discretization resolution $I$ across multiple environments to identify practical guidelines for hyperparameter selection.