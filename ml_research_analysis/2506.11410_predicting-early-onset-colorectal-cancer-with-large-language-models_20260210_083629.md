---
ver: rpa2
title: Predicting Early-Onset Colorectal Cancer with Large Language Models
arxiv_id: '2506.11410'
source_url: https://arxiv.org/abs/2506.11410
tags:
- cancer
- predic
- were
- colorectal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explored the use of large language models (LLMs) to\
  \ predict early-onset colorectal cancer (EoCRC) in individuals aged 18\u201344 using\
  \ EHR data. Compared to 10 traditional ML models, a fine-tuned GPT-4o LLM achieved\
  \ the highest performance with 73% sensitivity and 91% specificity, outperforming\
  \ conventional models."
---

# Predicting Early-Onset Colorectal Cancer with Large Language Models

## Quick Facts
- **arXiv ID**: 2506.11410
- **Source URL**: https://arxiv.org/abs/2506.11410
- **Reference count**: 0
- **Primary result**: Fine-tuned GPT-4o LLM achieved 73% sensitivity and 91% specificity for early-onset colorectal cancer prediction, outperforming 10 traditional ML models

## Executive Summary
This study explored using large language models (LLMs) to predict early-onset colorectal cancer (EoCRC) in individuals aged 18-44 using electronic health record (EHR) data. The researchers compared a fine-tuned GPT-4o LLM against 10 traditional machine learning models, finding that the LLM achieved the highest performance with 73% sensitivity and 91% specificity. The model leveraged clinical codes, lab results, and observations from 6 months prior to diagnosis, with feature importance analysis and natural language explanations enhancing interpretability. The results demonstrate LLMs' potential for improving early cancer detection in younger populations.

## Method Summary
The study used EHR data from multiple US health systems to identify 1,953 CRC patients and an equal number of matched non-CRC controls aged 18-44. The researchers extracted structured data including conditions (ICD/SNOMED-CT), lab results (LOINC), and observations from months 2-7 before diagnosis, intentionally excluding the final month to avoid diagnostic contamination. They trained 10 traditional ML classifiers (including XGBoost, LightGBM, and Random Forest) alongside a fine-tuned GPT-4o LLM. The LLM was prompted with Chain-of-Thought reasoning and processed clinical codes as text names rather than numeric vectors. Performance was evaluated across 10 test runs with balanced sets (100 CRC + 990 non-CRC) reflecting 1% disease prevalence.

## Key Results
- Fine-tuned GPT-4o achieved 73% sensitivity and 91% specificity, outperforming all 10 traditional ML models
- The LLM leveraged natural language explanations and SHAP feature importance analysis for interpretability
- All models struggled with precision (<8%) due to the extreme class imbalance (1% prevalence)
- Gradient boosting models (LightGBM, HGBoost) achieved 100% sensitivity but <10% specificity, indicating overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Fine-tuning transfers general medical knowledge in LLMs to specific EoCRC risk patterns
- Base GPT-4o encodes general clinical knowledge from pretraining; supervised fine-tuning on 1,853 CRC cases with explicit EHR-derived examples specializes the model to recognize early warning patterns
- Core assumption: The base model's pretraining included sufficient clinical text to encode relevant pathophysiological relationships
- Evidence: Fine-tuned GPT-4o achieved 73% sensitivity and 91% specificity, with supervised fine-tuning allowing the LLM to learn specific patterns in the training data

### Mechanism 2
- Temporal exclusion of proximal diagnostic signals isolates pre-symptomatic risk patterns
- By excluding data from the final month before diagnosis and using only months 2-7 of patient history, the model learns subtle early indicators rather than obvious diagnostic workup signals
- Core assumption: Clinically actionable signals exist ≥2 months before formal diagnosis
- Evidence: The study intentionally excluded patient data within the last month prior to diagnoses to avoid signals highly indicative of an impending diagnosis

### Mechanism 3
- Natural language encoding of structured codes enables LLMs to leverage semantic relationships in clinical ontologies
- ICD/SNOMED/LOINC codes are converted to their text names rather than numeric vectors, allowing the LLM to process these as clinical concepts and infer relationships
- Core assumption: Code name text carries sufficient semantic signal; code hierarchy/relationships are implicit in LLM pretraining
- Evidence: Feature importance analysis and natural language explanations enhanced interpretability, with medical conditions represented by the names of their corresponding codes

## Foundational Learning

- **Class imbalance in rare disease prediction**: Why needed here - CRC prevalence in age 18-44 is <1%, making standard accuracy metrics misleading. The study used balanced training but 1% prevalence test sets. Quick check: Given 10,000 patients with 1% disease prevalence and a model with 73% sensitivity, 91% specificity, how many false positives will occur?

- **Chain-of-Thought (CoT) prompting**: Why needed here - The prompt template includes explicit reasoning steps, encouraging multi-step clinical inference rather than direct classification. Quick check: What is the difference between zero-shot classification and CoT-guided classification in terms of intermediate token generation?

- **SHAP values for instance-level explanation**: Why needed here - The paper uses SHAP waterfall plots to show which features pushed individual predictions toward/away from CRC, providing patient-specific rationale for clinical trust. Quick check: If "erythrocyte distribution width ratio" has a positive SHAP value for a patient, does that mean the feature value was high, or that the feature contributed to higher CRC risk?

## Architecture Onboarding

- **Component map**: EHR database -> filtered cohorts (ICD/SNOMED codes reviewed by gastroenterologist) -> temporal split (months 2-7) -> text encoding of structured codes -> GPT-4o fine-tuning (2 epochs) + 10 ML classifiers -> 10 test runs × (100 CRC + 990 non-CRC)

- **Critical path**: Cohort definition (inclusion/exclusion criteria determine signal quality) -> Temporal windowing (excludes diagnostic contamination) -> Feature representation (text vs. vector determines which model classes apply) -> Threshold calibration (Youden's J on imbalanced validation sets)

- **Design tradeoffs**: Balanced training vs. realistic test distribution (training on 50% CRC improves sensitivity but test set reflects real-world 1% prevalence, causing low precision/F1 across all models) vs. missing data handling (treated as "not specified" vs. imputation or deletion) vs. LLM probability calibration (coarse Likert-style scores preventing ROC analysis)

- **Failure signatures**: Gradient boosting collapse (LightGBM and HGBoost achieved 100% sensitivity but <10% specificity - classic overfitting to minority class) vs. Feature space mismatch (training set had 11,509 features; test sets 5,200-5,800) vs. Precision floor (all models had precision <8% due to 1% prevalence)

- **First 3 experiments**: Ablate temporal window (train on months 1-6 vs. 2-7 to quantify diagnostic contamination) vs. Compare LLM providers (replicate fine-tuning with Llama 3 and Gemini) vs. Calibrate LLM probabilities (use structured output format to extract continuous probabilities)

## Open Questions the Paper Calls Out

- **Open Question 1**: Can other leading LLM architectures (e.g., Llama 3, Google Gemini) match or exceed the predictive performance of GPT-4o for early-onset colorectal cancer risk stratification? This study exclusively validated OpenAI's GPT-4o, leaving the generalizability of these findings across different model architectures unknown.

- **Open Question 2**: Can a clinical language model pre-trained on longitudinal medical records improve prediction accuracy for patients with sparse or irregular visit histories? General-purpose LLMs may struggle to interpret irregular time gaps in EHR data, a limitation acknowledged but not yet addressed by domain-specific pre-training.

- **Open Question 3**: How can LLMs be optimized to output continuous probability scores rather than discrete Likert-scale values to enable standard ROC analysis? Current prompting strategies produce coarse confidence estimates, limiting the ability to fine-tune the trade-off between sensitivity and specificity required for clinical guidelines.

## Limitations
- Fine-tuning dataset of 1,853 cases may be insufficient for complex clinical pattern learning
- Absence of external validation raises questions about generalizability across health systems
- Coarse probability outputs from GPT-4o prevent standard ROC analysis, limiting direct model comparison
- High false positive rate implied by <8% precision across all models could limit clinical utility in real-world screening scenarios

## Confidence
- **High confidence**: Sensitivity/specificity metrics from the balanced test design and the observed superiority of fine-tuned GPT-4o over traditional ML models
- **Medium confidence**: Interpretability findings through SHAP analysis and natural language explanations, though external validation is needed
- **Medium confidence**: Temporal exclusion strategy effectiveness, given limited evidence about pre-symptomatic signal timing in young CRC patients

## Next Checks
1. External validation: Test the fine-tuned model on EoCRC cases from different health systems to assess generalizability and identify potential EHR coding biases
2. Calibration study: Implement continuous probability extraction methods for LLM outputs to enable proper ROC/AUC comparison and threshold optimization
3. Clinical impact assessment: Calculate expected false positive burden in a screening population of 10,000 patients to evaluate real-world deployment feasibility