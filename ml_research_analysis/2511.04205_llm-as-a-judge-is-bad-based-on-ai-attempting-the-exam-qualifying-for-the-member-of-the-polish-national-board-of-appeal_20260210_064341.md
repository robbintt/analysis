---
ver: rpa2
title: LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member
  of the Polish National Board of Appeal
arxiv_id: '2511.04205'
source_url: https://arxiv.org/abs/2511.04205
tags:
- legal
- points
- exam
- were
- appeal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluates whether large language models (LLMs) can pass\
  \ Poland\u2019s National Appeal Chamber exam and whether LLM-as-a-judge is reliable.\
  \ Models including GPT-4.1, Claude 4 Sonnet, and Bielik-11B-v2.6 were tested on\
  \ a knowledge test and a written judgment task using retrieval-augmented generation\
  \ pipelines."
---

# LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal

## Quick Facts
- **arXiv ID:** 2511.04205
- **Source URL:** https://arxiv.org/abs/2511.04205
- **Reference count:** 40
- **Primary result:** LLMs scored 70-74% on knowledge tests but failed written judgments (max 37/100), and LLM-as-a-judge systematically overscored outputs, missing critical legal errors.

## Executive Summary
This study evaluates whether large language models can pass Poland's National Appeal Chamber exam and whether LLM-as-a-judge is reliable for legal evaluation. Models including GPT-4.1, Claude 4 Sonnet, and Bielik-11B-v2.6 achieved satisfactory scores (70-74%) on multiple-choice knowledge tests but failed the written judgment task, with the best score being 37/100. The LLM-as-a-judge evaluation significantly overscored submissions, failing to detect hallucinated legal citations and fundamental errors in reasoning. Expert judges found model outputs stylistically fluent but legally unsound, warning of risks if used without oversight. While models performed well on formal procedural checks (97%+ accuracy), they failed substantive legal reasoning, concluding that LLMs cannot replace human judges and that LLM-as-a-judge is unreliable for legal evaluation without rigorous human verification.

## Method Summary
The study tested GPT-4.1, Claude 4 Sonnet, and Bielik-11B-v2.6 on Poland's National Appeal Chamber exam, which includes a 50-question multiple-choice knowledge test and a written judgment task. Models were evaluated using retrieval-augmented generation pipelines with hybrid search (semantic + keyword) across three document collections: judicial decisions, summaries, and PZP commentary. Knowledge tests were administered in closed-book and RAG variants, while formal assessments used zero-shot JSON-enforced extraction. LLM-as-a-judge used GPT-4o with an 8-criterion rubric to evaluate model-generated judgments against human expert assessments.

## Key Results
- LLMs scored 70-74% on multiple-choice knowledge tests but none passed the written judgment task (max 37/100).
- LLM-as-a-judge evaluations systematically overscored submissions, missing critical errors like hallucinated legal citations.
- Models achieved 97%+ accuracy on formal procedural checks but below 40% on substantive legal reasoning.
- Expert judges found model outputs stylistically fluent but legally unsound, warning of risks if used without oversight.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation improves knowledge-test performance but does not transfer to legal reasoning.
- Mechanism: External knowledge retrieval augments model context with statutory text and jurisprudence, improving factual recall on multiple-choice items. However, the paper shows this does not improve substantive judgment writing because retrieval is text-similarity based, not logic-aware.
- Core assumption: The relevant provisions can be identified via semantic similarity to the question.
- Evidence anchors:
  - [abstract] "Several LLMs... were tested in closed-book and various Retrieval-Augmented Generation settings... achieved satisfactory scores in the knowledge test, none met the passing threshold in the practical written part."
  - [section 6.2] "For the previous years, [GPT-4.1] scored 77.9 for the closed-book... and 88.4... for the advanced RAG approach."
  - [corpus] LLMzSzŁ benchmark shows similar pattern: models perform better on structured tests than open-ended tasks requiring reasoning.
- Break condition: When provisions share vocabulary but differ in purpose or jurisdiction, semantic retrieval returns contextually wrong documents, leading to hallucinated citations.

### Mechanism 2
- Claim: Structured information extraction with JSON-enforced outputs achieves near-perfect accuracy on formal procedural checks.
- Mechanism: Zero-shot prompting with explicit JSON schemas forces models to extract discrete fields (dates, amounts, binary flags) from case descriptions. The constrained output space and deterministic post-processing enable high reliability on well-defined procedural questions.
- Core assumption: The information needed is explicitly stated in the case description and can be mapped to predefined fields.
- Evidence anchors:
  - [section 5.6] "We have implemented a formal assessment using the n8n framework... To extract information from the factual state, we have used the AI Agent block, to which the Auto-fixing Output Parser with the gpt-4o model was connected."
  - [section 6.3.2] "For both cases the GPT-4.1 model achieved the best results... 97.3% F1 score and 98.6% accuracy for the binary case."
  - [corpus] Machine-Assisted Grading paper similarly shows LLMs excel at structured evaluation tasks with clear rubrics.
- Break condition: When information is ambiguous, incomplete, or requires legal interpretation rather than extraction, accuracy degrades (e.g., "allegationsDismissed" variable showed lower F1: 84.6–95.2%).

### Mechanism 3
- Claim: LLM-as-a-Judge systematically overscores legal outputs because it evaluates surface-level coherence rather than substantive legal correctness.
- Mechanism: Judge LLMs assess generated text against evaluation criteria but lack the legal reasoning capacity to detect fundamental errors. They conflate stylistic fluency with legal validity, assigning high scores to documents that "look professional" but contain hallucinated provisions and illogical reasoning.
- Core assumption: The judge LLM can understand legal reasoning quality when given a rubric and reference answers.
- Evidence anchors:
  - [section 6.5] "The scores assigned by the LLM evaluators were uniformly higher than the benchmark score... the LLM did not differentiate between much better (GPT-4, Claude) and much worse (Bielik) works."
  - [section 7] "Automatic grading by another LLM produced results substantially diverging from human assessments... consistently overvalued stylistic conformity and verbosity."
  - [corpus] G-Eval paper notes LLM judges exhibit bias toward LLM-generated text, but legal domain amplifies this due to specialized reasoning requirements.
- Break condition: Any domain requiring adversarial correctness verification rather than surface quality assessment.

## Foundational Learning

- Concept: Hybrid search (semantic + keyword)
  - Why needed here: Legal retrieval requires both conceptual understanding (semantic) and exact terminology matching (keyword) for citations and article references.
  - Quick check question: Can you explain why pure semantic search would fail to find a specific article number that uses different vocabulary?

- Concept: Structured output enforcement
  - Why needed here: The formal assessment pipeline relies on extracting 11+ fields reliably; without JSON schema enforcement, models produce free-text that requires fragile parsing.
  - Quick check question: What happens if a model returns "fifteen thousand" instead of "15000" in a numeric field?

- Concept: Evaluation rubric calibration
  - Why needed here: The LLM-as-a-Judge used an 8-criterion rubric but applied it superficially; understanding how rubrics fail is critical for any evaluation system.
  - Quick check question: Why might a judge give high "legal provisions" scores (28/30) to work that human experts score as 0/30?

## Architecture Onboarding

- Component map: Case Description → Information Extraction Pipeline (JSON-enforced) → Formal Assessment (deterministic rules) → Substantive Assessment (allegation extraction + merit evaluation) → Judgment Generation
- Knowledge Base → Chunking (256 tokens) → Vector Store (Typesense) → Hybrid Search (semantic + keyword) → RAG Context

- Critical path: Information extraction accuracy determines downstream judgment quality. If appeal dates, fee amounts, or procedural flags are wrong, the entire reasoning chain fails.

- Design tradeoffs:
  - 256-token chunks fit embedding model limits but may fragment legal context across article boundaries
  - Including possible answers in RAG queries improved accuracy (84% vs 72% for GPT-4.1) but doesn't reflect real-world usage
  - Case law retrieval helped technical performance but introduced irrelevant jurisprudence that misled reasoning

- Failure signatures:
  - "Cardinal errors": hallucinated article numbers (e.g., Article 514(4) PZP which doesn't exist)
  - Panel composition errors (all models defaulted to 3-member panels when case required 1)
  - Logical inconsistency: operative ruling contradicts justification
  - Citation of provisions present in context without analysis of their applicability

- First 3 experiments:
  1. Replicate the formal assessment extraction on a sample of 20 cases, measuring precision/recall per field to identify which extraction tasks are reliable.
  2. Test retrieval quality: manually evaluate whether top-5 retrieved provisions for 10 sample questions are actually relevant, distinguishing semantic matches from legal relevance.
  3. Implement a human-in-the-loop validation step: have a legal expert review the extracted formal assessment before substantive analysis, measuring error propagation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can supervised fine-tuning of LLMs on Polish legal judgments (with case facts, regulations, and precedents as input) enable models to pass the National Appeal Chamber exam?
- Basis in paper: [explicit] "One of the most important areas of research is fine-tuning LLMs to write rulings or judgments (supervised fine-tuning). The training dataset for each training sample should contain the content of the case, fragments of regulations, rulings, and judgments on the basis of which the answer (input) and the content of the judgment (output) should be provided."
- Why unresolved: Current study used general-purpose models without domain-specific fine-tuning; none passed the written judgment task (best score: 37/100).
- What evidence would resolve it: Train models on curated Polish procurement law judgments, then re-administer the exam under identical conditions.

### Open Question 2
- Question: Would hybrid retrieval with law-specific cross-encoder re-ranking and boundary-aligned chunking at legal units reduce hallucinated legal citations?
- Basis in paper: [explicit] "Future work should move beyond purely linguistic similarity toward a structure- and logic-aware stack... hybrid retrieval methods (sparse + dense) with a law-specific cross-encoder re-ranker are worth exploring."
- Why unresolved: Current approach retrieved semantically similar but contextually unrelated provisions, causing hallucinated citations like non-existent Article 514(4).
- What evidence would resolve it: Implement logic-aware retrieval, measure reduction in citation errors compared to current RAG pipeline on held-out cases.

### Open Question 3
- Question: Can reinforcement learning from human feedback (RLHF), with rewards for correct legal decisions and logical argumentation, improve substantive legal reasoning scores?
- Basis in paper: [explicit] "the quality of results could be improved by using reinforcement learning, including RLHF. Rewards should be given for correct decisions, correct construction of answers, logical references to regulations and historical judgments, and alignment with the subjective assessment of the expert."
- Why unresolved: Models achieved high formal assessment accuracy (97%+) but below 40% on substantive legal reasoning.
- What evidence would resolve it: Apply RLHF with expert-designed reward functions, compare judgment quality against baseline using committee evaluation.

### Open Question 4
- Question: Can multi-agent LLM systems outperform single-model approaches on complex legal judgment tasks?
- Basis in paper: [explicit] "Another interesting direction is to conduct the described experiment (exam) using multi-task agent systems, such as GPT-5 pro, or at least implement them at a specific stage of the process. The design of dedicated, more complex multi-agent systems could also significantly improve results."
- Why unresolved: Current study used single models with pipeline architecture; no multi-agent comparison was conducted.
- What evidence would resolve it: Design and evaluate multi-agent architecture with specialized agents for retrieval, reasoning, and drafting against current single-model baseline.

## Limitations

- The evaluation is confined to Polish public procurement law, limiting generalizability to other jurisdictions despite the authors' attempts to draw parallels.
- The sample size for the written judgment task is limited to one exam case and five historical cases, constraining statistical robustness.
- The LLM-as-a-judge evaluation relies on a single GPT-4o model, raising questions about whether results would generalize across different judge LLM architectures.

## Confidence

- **High Confidence**: Models scoring 70-74% on multiple-choice knowledge tests but failing written judgments (37/100 GPT-4.1, 30/100 Claude, 8/100 Bielik). This finding is directly supported by exam results and consistent with expert human evaluation.
- **Medium Confidence**: LLM-as-a-judge systematically overscoring submissions by conflating stylistic fluency with legal validity. While well-supported by the data, this mechanism requires further validation across different legal domains and evaluation rubrics.
- **Medium Confidence**: Structured information extraction achieving near-perfect accuracy on formal procedural checks. The results are robust for well-defined extraction tasks, but performance degradation on ambiguous legal interpretation suggests domain limitations.

## Next Checks

1. **Cross-jurisdictional validation**: Test the same LLM-as-a-judge methodology on legal domains from different jurisdictions (e.g., German administrative law or U.S. contract law) to assess whether overscoring is domain-specific or generalizable.

2. **Multi-judge calibration**: Evaluate the same legal outputs using multiple LLM judges (different models, different prompt variants) to measure inter-judge reliability and identify systematic biases.

3. **Human-in-the-loop workflow validation**: Implement a hybrid system where LLM extraction results are reviewed by legal experts before substantive analysis, measuring error propagation reduction and workflow efficiency gains.