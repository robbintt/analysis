---
ver: rpa2
title: Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation
  in Federated Learning
arxiv_id: '2507.05852'
source_url: https://arxiv.org/abs/2507.05852
tags:
- learning
- federated
- client
- data
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses communication overhead and statistical heterogeneity
  in federated learning by proposing a method that communicates only lightweight adapter
  modules and prototypes instead of full model parameters. The approach combines prototype-based
  learning for inherent interpretability with adapter modules that act as compressed
  surrogates to guide generalization across non-IID client distributions.
---

# Prototype-Guided and Lightweight Adapters for Inherent Interpretation and Generalisation in Federated Learning

## Quick Facts
- arXiv ID: 2507.05852
- Source URL: https://arxiv.org/abs/2507.05852
- Reference count: 28
- Average accuracy 86.82% across clients, matching FedAdapter baseline (87.58%)

## Executive Summary
This paper proposes a federated learning framework that addresses communication overhead and statistical heterogeneity by exchanging only lightweight adapter modules and prototype representations instead of full model parameters. The approach combines prototype-based learning for inherent interpretability with adapter modules that compress and guide generalization across non-IID client distributions. Each client aligns class embeddings toward prototype representations while adjusting lightweight adapters, maintaining frozen backbone weights. Experiments on diabetic retinopathy classification demonstrate performance parity with state-of-the-art methods while providing interpretable disease-relevant visualizations.

## Method Summary
The method employs a ResNet-50 backbone (frozen, ImageNet-pretrained) with lightweight adapter modules inserted at each convolutional block. These adapters use a bottleneck structure with residual connections to compress feature representations. A prototypical network layer with learnable class-specific templates enables prototype-based learning for interpretability. During federated training, clients optimize adapters and prototype parameters locally while maintaining frozen backbone weights. The server aggregates only adapter and prototype parameters across 100 communication rounds. The loss function combines cross-entropy with prototype clustering/separation regularizers, L1 regularization on prototype-to-class weights, L2 adapter regularization, and a proximal alignment term to encourage global parameter consistency.

## Key Results
- Achieved average accuracy of 86.82% across clients, closely matching FedAdapter baseline (87.58%)
- Successfully highlighted disease-relevant retinal regions through prototype activation maps
- Demonstrated strong generalization on external APTOS dataset with accuracy above 93% across clients
- Reduced communication overhead by transmitting only adapter and prototype parameters rather than full model

## Why This Works (Mechanism)
The method works by decoupling feature extraction from adaptation through frozen backbone weights combined with learnable adapters. The prototype layer provides a shared semantic space that regularizes local training, preventing catastrophic forgetting of global patterns. The proximal alignment term explicitly penalizes divergence from global parameters, addressing statistical heterogeneity. Adapter modules act as compressed surrogates that capture client-specific variations while maintaining the general feature space defined by the backbone. This architecture enables efficient communication while preserving both performance and interpretability through the prototype-based explanations.

## Foundational Learning
- **Adapter modules**: Lightweight neural network components that can be inserted into existing architectures to adapt them to new tasks without full fine-tuning. Needed to compress client-specific variations while maintaining backbone integrity. Quick check: Verify adapter parameter count is significantly smaller than backbone parameters.
- **Prototype-based learning**: A few-shot learning approach where class representations are defined by prototype vectors in embedding space. Needed to provide inherent interpretability and regularize local training. Quick check: Ensure prototype clustering loss actively pulls embeddings toward their class prototypes.
- **Proximal regularization**: A penalty term that discourages deviation from a reference parameter set. Needed to address statistical heterogeneity in federated learning. Quick check: Verify performance degradation when proximal term is removed (baseline dropped from 87.58% to 72.02%).
- **Non-IID federated learning**: Federated learning scenarios where data distribution varies significantly across clients. Needed to model realistic clinical deployment scenarios. Quick check: Confirm data split creates meaningful distribution differences across the four client sites.

## Architecture Onboarding

**Component Map**: Image → ResNet-50 Backbone → Adapter Modules → Prototype Layer → Classification

**Critical Path**: Backbone feature extraction → Adapter compression → Prototype alignment → Classification decision

**Design Tradeoffs**: Frozen backbone ensures consistency but limits adaptation capacity; adapters provide flexibility at cost of potential overfitting; prototypes enable interpretability but may constrain representation capacity.

**Failure Signatures**: Performance drops to 72.02% when proximal alignment is removed; communication inefficiency if full parameters are transmitted; poor interpretability if prototype regularization is insufficient.

**First Experiments**:
1. Verify parameter transmission efficiency by comparing total parameter counts before and after aggregation
2. Test sensitivity to adapter bottleneck dimension by training with r=32, 64, 128
3. Evaluate prototype quality through visualization of activation maps for diseased vs. healthy cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework be extended to support model heterogeneity where clients possess different network architectures?
- Basis in paper: [explicit] The conclusion states that future work involves "integrating model heterogeneity to the proposed model could address scenarios where clients have different network structure."
- Why unresolved: The current method relies on a frozen, shared backbone (ResNet-50) across all clients to extract features for the adapters and prototypes.
- Evidence: A successful implementation where clients utilize different backbone architectures (e.g., ResNet vs. DenseNet) while maintaining the prototype alignment mechanism.

### Open Question 2
- Question: Can the consistency of prototype-based explanations be improved across clients without sacrificing local generalization?
- Basis in paper: [explicit] The authors observe in the conclusion that "prototype-based explanations can differ between client models" despite robust global performance.
- Why unresolved: The local adaptation of lightweight adapters allows for personalized feature spaces, which may result in semantic drift where different visual features are learned as prototypes for the same class.
- Evidence: A quantitative metric measuring the semantic similarity of activation regions across clients, coupled with ablation studies on regularization strength.

### Open Question 3
- Question: Does the proposed method maintain interpretability and accuracy when scaled to multi-class or fine-grained classification tasks?
- Basis in paper: [inferred] The experimental setup explicitly limits the task to binary classification (healthy vs. diseased) for Diabetic Retinopathy.
- Why unresolved: Prototype-based learning often suffers from concept overlap or "prototype collision" as the number of classes increases, and the lightweight adapters may lack the capacity to separate complex boundaries.
- Evidence: Evaluation results on a dataset with more than two classes (e.g., 5-class DR grading or CIFAR-100) showing accuracy and visual distinction between class prototypes.

## Limitations
- No specification of adapter bottleneck dimension or exact ResNet insertion points
- Missing prototype layer dimensions and number of prototypes per class
- Unspecified loss coefficient values affecting training dynamics
- Limited external validation (only one dataset reported)

## Confidence
- Performance claims: Medium (missing critical hyperparameters affect reproducibility)
- Interpretability claims: Medium (prototype configuration unknown)
- External validation: Low (single dataset, unknown preprocessing consistency)

## Next Checks
1. Verify only adapter and prototype parameters are transmitted in FL rounds by comparing parameter counts before/after aggregation.
2. Test model performance sensitivity to adapter bottleneck size and prototype regularization coefficients to identify optimal ranges.
3. Apply the trained prototypes to APTOS data with consistent preprocessing to independently verify the >93% external validation accuracy.