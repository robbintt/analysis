---
ver: rpa2
title: 'Control Illusion: The Failure of Instruction Hierarchies in Large Language
  Models'
arxiv_id: '2502.15851'
source_url: https://arxiv.org/abs/2502.15851
tags:
- constraint
- instruction
- user
- constraints
- priority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic framework for evaluating how
  well large language models enforce hierarchical instruction priorities. Through
  controlled experiments with six state-of-the-art models, the authors demonstrate
  that system/user prompt separation fails to reliably establish instruction hierarchies,
  even for simple verifiable formatting conflicts.
---

# Control Illusion: The Failure of Instruction Hierarchies in Large Language Models

## Quick Facts
- arXiv ID: 2502.15851
- Source URL: https://arxiv.org/abs/2502.15851
- Reference count: 26
- Large language models fail to reliably enforce hierarchical instruction priorities despite system/user prompt separation

## Executive Summary
This paper systematically evaluates how well large language models (LLMs) enforce instruction hierarchies, revealing fundamental failures in current approaches. Through controlled experiments with six state-of-the-art models, the authors demonstrate that system/user prompt separation does not reliably establish instruction hierarchies, even for simple formatting conflicts. Models rarely acknowledge conflicting instructions and exhibit strong inherent biases toward certain constraint types regardless of priority designation.

The study reveals that societal hierarchy framings (authority, expertise, consensus) show stronger influence on model behavior than explicit system/user roles, suggesting pretraining-derived social structures function as latent behavioral priors with potentially greater impact than post-training guardrails. This finding points to a fundamental gap in current LLM behavior and highlights the need for architectural and training-level innovations to enable robust instruction prioritization.

## Method Summary
The authors developed a systematic framework for evaluating instruction hierarchy enforcement in LLMs through controlled experiments. They tested six state-of-the-art models using formatting conflict scenarios where explicit instruction hierarchies were established through system/user prompt separation. The experiments measured whether models could identify and resolve conflicting instructions according to designated priorities, with particular attention to whether societal framings (authority, expertise, consensus) influenced behavior differently than explicit system prompts.

## Key Results
- System/user prompt separation fails to reliably establish instruction hierarchies, even for simple verifiable formatting conflicts
- Models rarely acknowledge conflicting instructions and exhibit strong inherent biases toward certain constraint types regardless of priority designation
- Societal hierarchy framings (authority, expertise, consensus) show stronger influence on model behavior than explicit system/user roles, suggesting pretraining-derived social structures function as latent behavioral priors

## Why This Works (Mechanism)
The paper's experimental design directly tests the fundamental assumption that LLMs can be controlled through instruction hierarchies established via system/user prompt separation. By creating verifiable formatting conflicts and measuring model responses, the authors isolate whether models can identify, acknowledge, and resolve conflicts according to explicit priorities. The systematic variation of framing (system/user roles vs. societal hierarchies) reveals how pretraining-derived social structures may override post-training guardrails.

## Foundational Learning
- Instruction hierarchy enforcement: Understanding how LLMs prioritize competing instructions when explicitly directed to do so; why needed to assess control mechanisms, quick check by testing conflict resolution in controlled scenarios
- System/user prompt separation: The conventional approach to establishing instruction priorities in LLMs; why needed to evaluate current best practices, quick check by measuring compliance with explicit hierarchies
- Societal framing effects: How social constructs (authority, expertise, consensus) influence model behavior; why needed to understand pretraining impacts, quick check by comparing responses across different framings

## Architecture Onboarding
Component Map: Input Processing -> Instruction Hierarchy Detection -> Conflict Resolution -> Output Generation

Critical Path: The key pathway involves detecting conflicting instructions, determining hierarchy priorities, and generating compliant outputs. The study reveals failures at the hierarchy detection and conflict resolution stages.

Design Tradeoffs: Current LLM architectures prioritize fluency and coherence over strict instruction compliance, leading to trade-offs between natural language generation and hierarchical control.

Failure Signatures: Models exhibit two primary failure modes: (1) complete failure to acknowledge conflicting instructions, and (2) implicit bias toward certain constraint types regardless of explicit priorities.

3 First Experiments:
1. Test conflict resolution for factual accuracy vs. stylistic preferences to expand beyond formatting conflicts
2. Evaluate models with varying pretraining data sources to assess pretraining influence
3. Conduct ablation studies removing societal framing elements to isolate their contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on formatting conflicts, representing a narrow subset of potential instruction hierarchies
- Six tested models may not capture full diversity of current LLM architectures and training approaches
- Societal hierarchy framings were specifically constructed for this study and may not fully represent pretraining-derived social structures

## Confidence
- High confidence: Core finding that system/user prompt separation fails to establish reliable instruction hierarchies, supported by systematic experimental evidence
- Medium confidence: Claim that societal hierarchy framings show stronger influence than explicit system/user roles, based on indirect behavioral evidence
- Medium confidence: Assertion that pretraining-derived social structures function as latent behavioral priors, with reasonable theoretical grounding

## Next Checks
1. Replicate experiments with task types beyond formatting conflicts (e.g., factual accuracy vs. style preferences, safety vs. utility instructions)
2. Test additional models with varying pretraining data sources and instruction tuning approaches
3. Conduct ablation studies removing societal framing elements to quantify their relative contribution compared to explicit system prompts