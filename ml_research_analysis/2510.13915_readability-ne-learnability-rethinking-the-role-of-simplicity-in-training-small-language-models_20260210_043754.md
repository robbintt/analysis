---
ver: rpa2
title: 'Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training
  Small Language Models'
arxiv_id: '2510.13915'
source_url: https://arxiv.org/abs/2510.13915
tags:
- start
- they
- readability
- lily
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Recent studies suggest that very small language models (SLMs)\
  \ can generate surprisingly coherent text when trained on simplified, child-directed\
  \ corpora such as TinyStories. These findings have been interpreted as evidence\
  \ that readability\u2014characterized by accessible vocabulary, familiar narrative\
  \ structure, and simple syntax\u2014plays a key role in enabling such capabilities\
  \ to emerge."
---

# Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models

## Quick Facts
- **arXiv ID**: 2510.13915
- **Source URL**: https://arxiv.org/abs/2510.13915
- **Reference count**: 40
- **Primary result**: Readability alone does not predict coherence or learning efficiency in small language models; statistical simplicity (low n-gram diversity) is a stronger predictor of learnability.

## Executive Summary
Recent studies have suggested that simplified, child-directed language is key to enabling small language models to generate coherent text. This paper challenges that interpretation by constructing synthetic datasets with matched structure but varied readability. Through controlled experiments, the authors find that readability does not predict coherence or learning efficiency in small models. Instead, statistical simplicity—measured by n-gram diversity—emerges as the primary factor influencing learnability. Models trained on complex, adult-level text achieve comparable performance to those trained on simplified language, and even exhibit faster development of coherence during training. The findings caution against anthropomorphizing language model training and emphasize the need for more precise reasoning about what properties actually support capability emergence in small models.

## Method Summary
The authors construct synthetic datasets using prompt templates with controlled vocabulary (child-friendly vs. GRE-level) and fixed narrative features, generating approximately 1 billion tokens per dataset using Llama-3.1-8B-Instruct. They train transformer language models ranging from 262K to 33M non-embedding parameters for 10 epochs over 10 billion tokens. Evaluation uses Llama-3.1-70B-Instruct as judge for coherence and readability, alongside n-gram diversity statistics and novelty analysis. The study compares child-directed datasets (TinyStories, LlamaTales-Jr) with adult-level GRE vocabulary datasets (LlamaTales-GRE) and various domain variants.

## Key Results
- Readability and coherence are uncorrelated in model outputs (r=0.03), while coherence correlates strongly with model quality rankings
- Statistical simplicity (low n-gram diversity) predicts learnability better than readability, with inverse correlation to unique 3-grams
- Models trained on complex adult-level text achieve high coherence (>85) after the first epoch, surpassing simplified child-directed datasets
- High in-distribution coherence does not imply robust generalization; models fail when tested on prompts from other synthetic corpora or natural text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Statistical simplicity (low n-gram diversity) predicts learnability in SLMs better than human readability.
- Mechanism: Models learn more efficiently when token sequences have higher predictability and structural regularity, regardless of vocabulary complexity or syntactic simplicity. Low n-gram diversity indicates a narrower distribution over sequences, which small-capacity models can capture more completely.
- Core assumption: N-gram diversity adequately captures the statistical properties that matter for small model learning; the relationship generalizes beyond synthetic datasets.
- Evidence anchors:
  - [abstract] "statistical simplicity—measured by n-gram diversity—predicts learnability better than readability"
  - [section 4, Figure 4a] Learnability ratio (output coherence / train data coherence) shows clear inverse correlation with unique 3-grams; synthetic datasets cluster in high-learnability regime.
  - [corpus] Weak external validation—neighbor papers focus on SLM reasoning/efficiency but don't test the readability vs. statistical simplicity distinction directly.
- Break condition: If statistical simplicity were the only factor, models trained on statistically similar datasets should show similar generalization—but they don't (distribution-specific learning observed).

### Mechanism 2
- Claim: Coherence emergence speed is not accelerated by simpler, child-directed language.
- Mechanism: Contrary to developmental analogies, models trained on complex adult-level text (LlamaTales-GRE) achieved high coherence (>85) after the first epoch, while child-directed datasets (TinyStories, LlamaTales-Jr) required more training exposure. The template-based generation process may produce more consistent structural patterns in GRE-level text despite vocabulary complexity.
- Core assumption: Early-training coherence reflects genuine pattern learning rather than overfitting to surface statistics.
- Evidence anchors:
  - [section 4, Figure 4b] "Models trained on the less readable, more complex LlamaTales-GRE dataset (blue) achieve a high level of coherence remarkably quickly, surpassing a score of 85 after just the first epoch"
  - [corpus] No direct corpus evidence on training dynamics—neighbor papers don't examine coherence emergence timing.
- Break condition: If early coherence were due to memorization, n-gram novelty would be low—but Figure 5 shows substantial novelty at 3-5 gram levels.

### Mechanism 3
- Claim: High in-distribution coherence does not imply robust generalization; statistical simplicity creates narrow capability.
- Mechanism: The same low n-gram diversity that enables efficient learning confines models to a narrow distributional regime. Models trained on synthetic datasets fail when tested on prompts from other synthetic corpora or natural text, despite achieving near-large-model coherence in-distribution.
- Core assumption: The brittleness is inherent to statistical simplicity rather than synthetic data artifacts.
- Evidence anchors:
  - [section 4] "When tested on prompts from any other dataset—including other synthetic corpora or real-world web text—their coherence degrades substantially"
  - [section 4, Figure 5] N-gram novelty analysis shows genuine recombination, not memorization—suggesting the limitation is distributional narrowness, not capacity.
  - [corpus] Neighbor paper "MiCoTA" addresses SLM reasoning gaps but via distillation/CoT, not data design—orthogonal approach.
- Break condition: Training on mixed synthetic distributions might broaden capability without sacrificing learnability—this was not tested.

## Foundational Learning

- Concept: **N-gram diversity as statistical complexity proxy**
  - Why needed here: The paper's central claim rests on distinguishing "readability" (human-centric) from "statistical simplicity" (distributional). Without understanding why unique n-gram counts proxy for compressibility/predictability, the mechanism is opaque.
  - Quick check question: If Dataset A has 10M unique 4-grams and Dataset B has 100M unique 4-grams (same token count), which would a 10M parameter model likely learn faster, and why?

- Concept: **LLM-as-a-judge for evaluation**
  - Why needed here: The paper validates LLM-based coherence/readability scoring against human judgments (CLEAR dataset, r=0.74). Understanding this validation is critical for interpreting all results.
  - Quick check question: Why might LLM-judged coherence correlate minimally with readability (r=0.03) but strongly with model quality rankings?

- Concept: **Distribution-specific learning vs. generalization**
  - Why needed here: The paper shows SLMs can match large-model coherence in-distribution but fail out-of-distribution. This distinction is essential for interpreting what "capability emergence" actually means.
  - Quick check question: A 33M parameter model achieves 94 coherence on LlamaTales-GRE prompts but 65 coherence on TinyStories prompts. What does this imply about its "understanding"?

## Architecture Onboarding

- Component map: Synthetic data generation → statistical simplicity validation (n-gram counts) → model training (10B tokens, 10 epochs) → coherence tracking during training → in-distribution and cross-distribution evaluation

- Critical path: Dataset construction → statistical simplicity validation (n-gram counts) → model training (10B tokens, 10 epochs) → coherence tracking during training → in-distribution and cross-distribution evaluation

- Design tradeoffs:
  - **Statistical simplicity ↔ Generalization**: Low n-gram diversity enables learning but creates brittle, distribution-specific models
  - **Synthetic control ↔ Ecological validity**: Template-generated data allows clean comparisons but may not reflect natural language properties
  - **LLM-judge efficiency ↔ Human validation**: Scalable but inherits LLM biases; validated on CLEAR but not on synthetic story coherence specifically

- Failure signatures:
  - High in-distribution coherence + low cross-distribution coherence → narrow distributional learning, not general capability
  - Low n-gram novelty at 3-5 grams → memorization rather than pattern learning (not observed in results)
  - Coherence/readability correlation in model outputs → judge may be conflating dimensions (Figure A27 shows minimal correlation, validating separation)

- First 3 experiments:
  1. **Replicate n-gram diversity measurement**: Sample 100M tokens from TinyStories vs. FineWeb; compute unique 1-gram through 8-gram counts. Verify the two-cluster separation shown in Figure 2.
  2. **Train minimal model on LlamaTales-GRE**: 262K parameter model for 1 epoch; evaluate coherence on held-out GRE prompts. Confirm rapid emergence (>70 coherence after 1B tokens).
  3. **Cross-distribution coherence test**: Take your best LlamaTales-GRE model; evaluate on TinyStories prompts and FineWeb prompts. Quantify the coherence drop to internalize the generalization gap.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can richer measures of statistical complexity beyond n-gram diversity better predict learnability in small language models?
  - Basis: Authors state n-gram diversity "likely captures one facet of a broader set of properties" and call for "developing richer, more comprehensive measures of dataset complexity and learnability beyond simple n-grams."
  - Why unresolved: N-gram statistics are a crude proxy; they may not capture syntactic regularity, semantic coherence patterns, or distributional properties that affect learning.
  - What evidence would resolve it: Comparing alternative complexity metrics (e.g., compression ratios, parse tree depth distributions, entropy estimates) against SLM learnability across diverse corpora.

- **Open Question 2**: Can training on mixed or broader synthetic distributions mitigate the narrow generalization observed in SLMs while preserving statistical learnability?
  - Basis: Authors note: "We did not explore training on mixed or broader synthetic distributions, which might mitigate this" when discussing SLM brittleness.
  - Why unresolved: Experiments only used narrow, single-domain synthetic datasets; the tradeoff between statistical simplicity and distributional diversity remains unexplored.
  - What evidence would resolve it: Training SLMs on combined synthetic datasets spanning multiple domains/styles and evaluating cross-dataset coherence transfer.

- **Open Question 3**: Why does complex, less readable text (LlamaTales-GRE) enable faster initial coherence emergence during training compared to simplified child-directed text?
  - Basis: Authors document this phenomenon—"models trained on the less readable, more complex LlamaTales-GRE dataset achieve a high level of coherence remarkably quickly"—but do not explain the mechanism.
  - Why unresolved: The paper demonstrates the effect but does not investigate whether it stems from vocabulary density, syntactic patterns, or other properties of adult-level text.
  - What evidence would resolve it: Ablation studies varying specific text properties (vocabulary sophistication, sentence structure complexity) while controlling for statistical simplicity to isolate causal factors.

## Limitations
- Synthetic data design creates uncertainty about external validity due to controlled template generation that may not reflect natural language properties
- N-gram diversity metric may not capture all relevant aspects of statistical simplicity affecting SLM learning
- Focus on narrative story generation limits generalizability to other text domains

## Confidence
- **High confidence**: Readability and coherence are uncorrelated (r=0.03) in model outputs; statistical simplicity correlates with faster coherence emergence
- **Medium confidence**: Child-directed language doesn't accelerate SLM learning compared to complex adult text, given synthetic comparison and limited natural validation
- **Medium confidence**: Narrow distributional learning explains SLM brittleness, as alternative explanations weren't fully excluded

## Next Checks
1. **Natural language validation**: Train models on naturally occurring child-directed corpora versus naturally complex adult text with matched n-gram diversity statistics to test whether synthetic findings hold in ecological settings.

2. **Mixed-distribution training**: Train models on combined synthetic distributions (e.g., TinyStories + LlamaTales-GRE) to test whether statistical simplicity's benefits can be preserved while broadening generalization capability.

3. **Extended n-gram analysis**: Conduct detailed n-gram novelty analysis on naturally occurring datasets rather than just synthetic ones to validate that observed recombination patterns hold beyond controlled generation.