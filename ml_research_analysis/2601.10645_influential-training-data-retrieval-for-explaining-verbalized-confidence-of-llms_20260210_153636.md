---
ver: rpa2
title: Influential Training Data Retrieval for Explaining Verbalized Confidence of
  LLMs
arxiv_id: '2601.10645'
source_url: https://arxiv.org/abs/2601.10645
tags:
- confidence
- data
- training
- llms
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of understanding how large language
  models (LLMs) ground their verbalized confidence, particularly given that LLMs are
  often overconfident and their stated confidence does not reliably align with factual
  accuracy. The authors introduce TracVC (Tracing Verbalized Confidence), a method
  that combines information retrieval and influence estimation to trace generated
  confidence expressions back to training data.
---

# Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs

## Quick Facts
- arXiv ID: 2601.10645
- Source URL: https://arxiv.org/abs/2601.10645
- Reference count: 40
- Primary result: TracVC reveals that LLMs often mimic superficial linguistic expressions of certainty rather than grounding confidence in content-relevant information

## Executive Summary
This paper introduces TracVC, a method that combines information retrieval and influence estimation to trace verbalized confidence expressions in LLMs back to their training data. The authors demonstrate that large language models frequently generate confidence statements based on superficial linguistic patterns rather than genuine content understanding, with larger models not necessarily showing better content-grounded confidence. They introduce "content groundness" as a metric to quantify this phenomenon and show that post-training techniques can have varying effects on different models' content groundness.

## Method Summary
TracVC works by retrieving two sets of top-10 relevant training examples for each test instance: one set lexically similar to the question and answer (content-related), and another similar to the confidence prompt and expression (confidence-related). The method then estimates the influence of these retrieved examples on the model's confidence generation using gradient-based attribution. This allows the authors to determine whether the model's verbalized confidence is grounded in content-relevant information or merely mimics confidence-related linguistic patterns from the training data.

## Key Results
- OLMo2-13B is frequently influenced by confidence-related training data that is lexically unrelated to the question, suggesting superficial mimicry of certainty expressions
- Larger models are not necessarily more content-grounded than smaller ones
- Content groundness is higher when models answer questions correctly
- Post-training techniques can impact content groundness in opposite directions on different models

## Why This Works (Mechanism)
TracVC works by leveraging the observation that if a model's confidence expression is truly grounded in content understanding, the most influential training examples should be those relevant to the question content itself. By comparing the influence of content-related versus confidence-related training examples, the method can detect when models are merely mimicking confidence expressions rather than genuinely assessing their answer quality.

## Foundational Learning

**Information Retrieval** - Why needed: To identify training examples similar to test instances for influence analysis. Quick check: Can retrieve relevant examples using lexical similarity measures.

**Influence Estimation** - Why needed: To quantify how much specific training examples affect model outputs. Quick check: Can attribute model predictions to training data using gradient-based methods.

**Content Groundness Metric** - Why needed: To measure whether confidence expressions are based on actual understanding versus linguistic mimicry. Quick check: Can compare influence of content-related vs confidence-related examples.

**Gradient-Based Attribution** - Why needed: To trace model predictions back to training data influences. Quick check: Can compute gradients with respect to training examples to estimate influence.

**Confidence Verbalization** - Why needed: To understand how LLMs express certainty in their outputs. Quick check: Can identify and categorize different types of confidence expressions.

## Architecture Onboarding

Component Map: Input -> Retriever -> Influence Estimator -> Content Groundness Calculator -> Analysis

Critical Path: Test instance → Retriever (content & confidence) → Influence Estimator (gradient attribution) → Content Groundness calculation → Results interpretation

Design Tradeoffs: Using top-10 examples balances computational efficiency with retrieval coverage; gradient-based attribution trades off accuracy for tractability versus more complex influence estimation methods.

Failure Signatures: Low content groundness scores could indicate either superficial confidence mimicry or genuine content understanding with similar confidence language in training data.

First Experiments:
1. Validate influence estimation on synthetic data with known ground truth
2. Test retrieval stability with varying k values and similarity metrics
3. Compare content groundness across different model architectures and sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Influence estimation assumes linear relationships that may not hold for complex neural architectures
- Retrieving only top-10 examples may miss more distant but potentially influential training instances
- Low content groundness could indicate either superficial mimicry or genuine understanding with similar confidence language

## Confidence

| Claim | Confidence |
|-------|------------|
| TracVC reveals superficial confidence mimicry | High |
| Larger models are not necessarily more content-grounded | Medium |
| Post-training techniques affect content groundness oppositely | Medium |
| Correct answers correlate with higher content groundness | Medium |

## Next Checks

1. Test influence estimation methodology using synthetic training data with known ground truth to validate accuracy of gradient-based approach.

2. Expand retrieval analysis to include larger k values and alternative similarity metrics to assess stability of content groundness metric.

3. Conduct human evaluation studies where annotators assess whether retrieved training examples genuinely influenced model's confidence expression.