---
ver: rpa2
title: 'TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive
  Retrieval'
arxiv_id: '2601.09523'
source_url: https://arxiv.org/abs/2601.09523
tags:
- temporal
- query
- retrieval
- reasoning
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TEMPO is the first benchmark combining temporal reasoning with
  reasoning-intensive retrieval across 13 domains. It addresses the gap between temporal
  QA benchmarks (focusing on fact-seeking) and reasoning-intensive retrieval benchmarks
  (lacking temporal grounding).
---

# TEMPO: A Realistic Multi-Domain Benchmark for Temporal Reasoning-Intensive Retrieval

## Quick Facts
- **arXiv ID**: 2601.09523
- **Source URL**: https://arxiv.org/abs/2601.09523
- **Reference count**: 40
- **Primary result**: TEMPO introduces the first temporal reasoning-intensive retrieval benchmark, revealing that even the best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4% Temporal Coverage@10.

## Executive Summary
TEMPO addresses a critical gap in retrieval evaluation by combining temporal reasoning with complex multi-step retrieval tasks across 13 domains. Unlike existing benchmarks that focus on either temporal fact-seeking or reasoning-intensive retrieval without temporal grounding, TEMPO requires systems to retrieve evidence spanning multiple time periods to answer complex questions. The benchmark features 1,730 queries with detailed temporal annotations, step-wise reasoning plans, and novel evaluation metrics that measure temporal completeness. Evaluation of 12 retrieval systems reveals substantial challenges, with the best model achieving only 71.4% Temporal Coverage@10, demonstrating that current systems struggle to retrieve temporally complete evidence for complex queries.

## Method Summary
TEMPO is constructed from 1,654,055 documents sourced from Stack Exchange and external web links, with 1,730 complex queries annotated with temporal intent, 10 reasoning classes, and step-wise retrieval plans. The dataset creation involved human annotation of Stack Exchange posts, GPT-4o mining for hard negatives (topically similar but temporally mismatched documents), and Gemini-assisted search for gold documents. Evaluation covers 12 sparse, dense, and reasoning-enhanced retrievers using NDCG@10 as the primary ranking metric and novel temporal metrics (Temporal Precision@k, Temporal Relevance@k, Temporal Coverage@k, and NDCG|FC@k) computed via LLM-as-judge. The benchmark supports two tasks: direct query-to-documents retrieval and step-wise query-to-step-to-documents retrieval.

## Key Results
- The best model (DiVeR) achieves only 32.0 NDCG@10 and 71.4% Temporal Coverage@10, revealing substantial challenges in temporal reasoning-intensive retrieval.
- Reasoning-enhanced architectures like ReasonIR benefit significantly from concatenated query+step input (+18 points), while sparse models suffer from "keyword dilution."
- Standard retrieval metrics (NDCG) fail to capture temporal completeness, with the gap between NDCG and Temporal Coverage scores across models demonstrating the need for specialized temporal metrics.
- Temporally misaligned retrieved documents actively mislead downstream generators more than having no retrieval at all, with the "No Retrieval" baseline outperforming all retrieval-augmented configurations in answer correctness.

## Why This Works (Mechanism)

### Mechanism 1
Concatenating decomposed reasoning steps with the original query improves retrieval for reasoning-enhanced architectures but degrades lexical matching. Reasoning models appear to utilize the explicit temporal anchors in the steps to bridge semantic gaps, whereas sparse models suffer from "keyword dilution" where the added reasoning text lowers the relative weight of critical temporal keywords. This is evidenced by ReasonIR improving from 17.2 (Step-Only) to 35.0 (Query+Step), while BM25 degrades from 10.8 to 10.3.

### Mechanism 2
Standard retrieval metrics (NDCG) fail to capture "temporal completeness," requiring a specialized Temporal Coverage (TC@k) metric to evaluate cross-period synthesis. NDCG rewards retrieving any relevant document, but for queries requiring comparison (e.g., "changes since 2017"), a system retrieving only 2018 documents gets high NDCG but fails the user intent. TC@k explicitly measures if the top-k results span the required baseline and comparison periods.

### Mechanism 3
Temporally misaligned retrieved documents actively mislead downstream generators more than having no retrieval at all. RAG generators rely on attention over context, and if the retrieved context is topically relevant but temporally outdated, the generator attends to these "facts" and produces confident but incorrect answers. This is supported by the "No Retrieval" baseline (77.3) outperforming "BM25" (73.8) and all other retrievers in answer correctness.

## Foundational Learning

- **Temporal Reasoning Classes** (e.g., Event Analysis & Localization vs. Trends & Cross-Period Comparison)
  - Why needed here: To analyze failure modes, one must distinguish between finding a date (localization) and synthesizing a trend across decades (cross-period).
  - Quick check question: Is the query asking when something happened, or how it changed over time?

- **Hard Negative Mining via Multi-LLM Reformulation**
  - Why needed here: To prevent models from relying on simple keyword matching, the dataset construction uses GPT-4o to generate queries that find topically similar but temporally distinct documents.
  - Quick check question: Can you identify a document that discusses "Bitcoin regulation" but is irrelevant because it only covers laws passed in 2013, while the query asks about 2023?

- **LLM-as-Judge for Temporal Alignment**
  - Why needed here: Determining if a document covers "the pre-GDPR era" is semantic, not just lexical. Evaluating this requires an LLM to interpret the time scope of text.
  - Quick check question: Does the LLM prompt strictly verify if the document covers the baseline period vs. the comparison period?

## Architecture Onboarding

- **Component map**: StackExchange Posts -> Human Selection -> GPT-4o (Annotation & Decomposition) -> Gemini (Web Search for Positives) -> GPT-4o (Hard Negatives) -> Retriever -> LLM Judge -> Temporal Metrics
- **Critical path**: The Temporal Coverage@k calculation is the critical differentiator of this benchmark. Ensuring the LLM judge correctly parses "start_iso" and "end_iso" from unstructured text is essential for metric validity.
- **Design tradeoffs**: Scale vs. Reasoning Depth (dataset is small but dense in annotation quality); Metric Cost (LLM-as-judge is expensive and slow compared to static NDCG calculation).
- **Failure signatures**: High NDCG, Low TC@k (model retrieves relevant topics but misses the timeline); Step-Only Success (steps contain the entire answer verbatim or are too specific).
- **First 3 experiments**:
  1. Run the "Strip" vs. "Temporal-Only" ablation to confirm your system relies on temporal signals rather than just keywords.
  2. Compare "Query+Step" vs. "Query+All" retrieval on the ReasonIR model to reproduce the +18 point gain.
  3. Measure the "No Retrieval" vs. "Oracle" gap to establish the upper bound of how much retrieval could help (or hurt) your specific generator.

## Open Questions the Paper Calls Out

- **Does retrieving temporally incomplete evidence actively harm downstream generation more than retrieving nothing at all?**
  - Basis in paper: [explicit] The authors hypothesize that "temporally incomplete documents in TEMPO actively mislead the generator; for complex temporal queries, retrieving wrong temporal evidence is worse than retrieving nothing."
  - Why unresolved: The RAG evaluation shows the no-retrieval baseline outperforming all retrieval-augmented configurations, but the causal mechanism remains untested.

- **Can the observed performance patterns generalize to domains beyond Stack Exchange (e.g., medicine, legal case law, scientific literature)?**
  - Basis in paper: [explicit] In the Limitations section: "Future work could extend to other languages and domains such as medicine, legal case law, or scientific literature."
  - Why unresolved: TEMPO is constructed exclusively from Stack Exchange posts, which have particular characteristics (expert-authored, technical, English-only).

- **Why do different retrieval architectures respond so differently to query reformulation for temporal reasoning?**
  - Basis in paper: [inferred] Figure 8 shows ReasonIR gains +13.7 NDCG@10 with GPT-4o reformulation while BM25 drops from 10.8 to 4.3â€“6.2.
  - Why unresolved: The asymmetry suggests different models process temporal reasoning signals differently, but the specific representational differences remain unexplored.

## Limitations

- **Reliability of LLM-as-judge**: The methodology lacks explicit inter-annotator agreement statistics or calibration against human judgments for the novel temporal metrics.
- **Limited generalizability**: The small scale (1,730 queries) and exclusive focus on Stack Exchange posts limit generalizability to production settings and other domains.
- **Potential temporal bias**: The hard negative mining process may introduce bias toward certain temporal patterns through GPT-4o's identification of topically relevant but temporally misaligned documents.

## Confidence

- **High Confidence**: The observation that standard retrieval metrics (NDCG) fail to capture temporal completeness is well-supported by the consistent gap between NDCG and Temporal Coverage scores across multiple models.
- **Medium Confidence**: The mechanism by which reasoning-enhanced architectures benefit from concatenated query+step input is supported but could vary significantly with different model architectures or step generation quality.
- **Medium Confidence**: The claim that temporally misaligned documents actively mislead generators is supported by the "No Retrieval" outperforming BM25 in answer correctness, but the effect size may depend on the specific generator architecture.

## Next Checks

1. **Temporal Judge Calibration**: Run a small validation set of TEMPO queries through both the proposed LLM-as-judge and human annotators to measure inter-annotator agreement for Temporal Coverage@k, establishing confidence intervals for the metric.
2. **Temporal Negative Mining Analysis**: Systematically analyze the hard negative set to verify that GPT-4o is correctly identifying topically relevant but temporally misaligned documents, checking for systematic temporal biases.
3. **RAG Impact Control Experiment**: Test the "No Retrieval" vs. "Oracle" gap with a generator that has strong parametric temporal knowledge to determine whether the observed negative impact of misaligned retrieval is specific to the tested generator or a more general phenomenon.