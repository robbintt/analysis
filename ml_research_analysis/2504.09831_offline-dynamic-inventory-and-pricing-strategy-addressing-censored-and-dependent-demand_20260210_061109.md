---
ver: rpa2
title: 'Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent
  Demand'
arxiv_id: '2504.09831'
source_url: https://arxiv.org/abs/2504.09831
tags:
- policy
- censoring
- where
- demand
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the offline sequential pricing and inventory
  control problem where demand is censored (unobservable when exceeding inventory)
  and dependent on past demand levels. The main challenge is learning an optimal policy
  from historical data containing censored sales, prices, ordering quantities, inventory
  levels, covariates, and censored sales, while maximizing long-term profit.
---

# Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand

## Quick Facts
- **arXiv ID:** 2504.09831
- **Source URL:** https://arxiv.org/abs/2504.09831
- **Reference count:** 40
- **Primary result:** Proposes C-FQI and PC-FQI algorithms that handle censored, dependent demand in offline joint pricing/inventory control with theoretical regret bounds and numerical validation

## Executive Summary
This paper addresses the challenge of learning optimal pricing and inventory policies from historical data where demand is both censored (unobservable when exceeding inventory) and dependent on past demand levels. The authors propose two novel algorithms, Censored Fitted Q-Iteration (C-FQI) and Pessimistic Censored Fitted Q-Iteration (PC-FQI), which incorporate survival analysis to impute missing reward information and offline reinforcement learning to estimate optimal policies. The framework transforms the problem into a high-order MDP based on consecutive censoring instances and introduces the concept of "censoring coverage" to measure data adequacy. Numerical experiments demonstrate that the algorithms converge to optimal policies as sample size increases, with C-FQI performing better under uniform behavior policies and PC-FQI under near-optimal behavior policies.

## Method Summary
The method combines survival analysis with offline reinforcement learning to handle censored demand in joint pricing and inventory control. First, the Kaplan-Meier estimator imputes missing rewards for censored demand instances by estimating conditional survival functions. The data is then partitioned based on consecutive censoring counts, transforming the problem into a high-order MDP. The C-FQI algorithm performs standard fitted Q-iteration on this transformed problem, while PC-FQI adds pessimism through uncertainty quantifiers to mitigate distributional shift. Both algorithms operate within the offline setting, learning optimal policies directly from historical data without exploration.

## Key Results
- Proposed C-FQI and PC-FQI algorithms successfully handle censored and dependent demand in offline settings
- Theoretical finite-sample regret bounds established with explicit dependence on censoring coverage
- C-FQI outperforms under uniform behavior policies while PC-FQI excels with near-optimal behavior policies
- Algorithms converge to optimal policies as sample size increases in numerical experiments
- Implementation available at https://github.com/gundemkorel/Inventory_Pricing_Control

## Why This Works (Mechanism)

### Mechanism 1: Surrogate Reward Imputation
When demand is censored ($D_t > Y_t$), the stock-out cost is unobservable, breaking the reward function $R_t$. The authors propose a **Surrogate Reward** $\tilde{R}_t$ that replaces the censored demand component with its conditional expectation, $E[D_t | \text{censored}, \text{history}]$. This expectation is estimated non-parametrically using the **Kaplan-Meier method** (a survival analysis technique), effectively imputing the missing information required for the Q-learning update.

### Mechanism 2: High-Order MDP Transformation
Censoring breaks the Markov property because the current state (sales) doesn't fully reveal the previous state (true demand). The authors solve this by defining the state space based on the **number of consecutive censoring instances** ($n_{K,b}$). Instead of a single Q-function, they learn multiple Q-functions ($Q^{(i)}$) where $i$ represents the number of consecutive censored steps. This restores the Markov property required for Bellman updates by conditioning on the necessary history depth.

### Mechanism 3: Pessimistic Offline RL
Standard offline RL suffers from "extrapolation error" when the learned policy visits state-action pairs not well-covered by the offline dataset. The **PC-FQI** algorithm subtracts an uncertainty quantifier $U_k^{(i)}$ from the Q-estimates during the Bellman backup. This penalizes values associated with low-confidence regions, encouraging the policy to stay within the support of the behavior policy.

## Foundational Learning

- **Fitted Q-Iteration (FQI)**: Understanding batch RL that separates data collection from policy iteration is prerequisite for understanding why "pessimism" and "censoring" modifications are necessary. *Quick check:* Can you explain why standard Q-learning might diverge in an offline setting without exploration?

- **Survival Analysis (Censored Data)**: The core novelty is handling censored demand via survival analysis. A user must understand that "censored" data (e.g., stock-outs) provides a lower bound, not an exact value, and requires specialized estimators like Kaplan-Meier to infer the underlying distribution. *Quick check:* If a store sells out of 100 units, why is treating the demand as "100" biased?

- **High-Order Markov Decision Processes (POMDPs)**: The paper transforms the problem into a high-order MDP. The learner needs to grasp that extending the state space with history (memory) can restore the Markov property in partially observed environments, but at the cost of increased dimensionality. *Quick check:* How does increasing the "order" of the MDP (length of history) affect the sample complexity of the learning algorithm?

## Architecture Onboarding

- **Component map:** Offline Dataset $O_N$ -> Survival Module (Kaplan-Meier) -> Data Partitioning (based on $n_{K,b}$) -> Q-Iteration (C-FQI/PC-FQI) -> Policy Extraction (Greedy + Uncensored Pathway)

- **Critical path:** The most critical step is **Data Partitioning and Imputation**. If the survival analysis module fails to estimate the true demand distribution during stockouts, or if the data partitioning based on consecutive censoring counts is misaligned with the true process, the entire Bellman backup will propagate biased values, rendering the final policy useless.

- **Design tradeoffs:** C-FQI vs. PC-FQI: C-FQI converges faster with uniform/broad behavior policies but risks overestimation with narrow data. PC-FQI is safer for near-optimal behavior policies but may be overly conservative if uncertainty estimates are loose. Complexity vs. History Depth: Increasing consecutive censoring count ($n$) makes the model more expressive but exponentially increases the number of Q-functions to learn, risking data sparsity in higher-order states.

- **Failure signatures:** Oscillating Q-values suggests unstable survival analysis imputation or weak uncertainty quantifier. Policy stuck at "Safety" Action indicates overestimated uncertainty, causing default to extreme price hikes. Censoring Coverage Gap occurs if deployed policy encounters $n > n_{K,b}$ consecutive stockouts, leading to undefined behavior.

- **First 3 experiments:**
  1. **Sanity Check (No Censoring):** Run C-FQI on zero stockout dataset; should reduce to standard FQI matching "Oracle" baseline.
  2. **Imputation Accuracy Analysis:** Compare Kaplan-Meier estimated demand against ground truth for censored time points; plot error distribution.
  3. **Behavior Policy Comparison:** Generate uniform vs. near-optimal behavior policy datasets; compare C-FQI performance on former against PC-FQI on latter.

## Open Questions the Paper Calls Out

1. **Supply Chain Extension:** Investigating competition and cooperation among supply chain echelons under complete and incomplete information presents a valuable area for further exploration.

2. **Responsive Pricing:** Incorporating responsive pricing—adjusting prices after observing demand information—into the current simultaneous decision-making model represents an intriguing direction for future research.

3. **Informative Censoring:** Adapting the algorithm to handle violations of the conditional non-informative censoring assumption where demand depends on inventory levels requires alternative imputation techniques beyond Kaplan-Meier.

## Limitations

- The algorithm's validity rests on multiple strong assumptions (particularly conditional independence and consistent survival function estimation) with limited discussion of practical violations
- Theoretical guarantees explicitly depend on "censoring coverage" condition - the offline data must contain sufficient instances of consecutive censoring to match optimal policy requirements
- Practical implementation of PC-FQI with kernel regression and specific uncertainty quantification method lacks detailed documentation

## Confidence

**High Confidence:** The core mechanism of using survival analysis (Kaplan-Meier) for reward imputation is well-established and mathematically sound. The transformation to high-order MDPs for handling censoring is theoretically rigorous. The regret bounds are properly stated with clear dependencies.

**Medium Confidence:** Numerical experiments demonstrate algorithms work as intended in controlled simulations, but are limited to specific parameter settings and don't explore sensitivity to assumption violations or data quality issues.

**Low Confidence:** The practical implementation of PC-FQI with kernel regression and the specific uncertainty quantification method, as these lack explicit implementation details in the paper.

## Next Checks

1. **Sensitivity Analysis:** Test algorithm performance across varying levels of censoring in training data to empirically validate censoring coverage requirement. Generate datasets with controlled censoring frequencies and measure regret as a function of the gap between $\hat{n}_{K,b}$ and oracle policy's censoring depth.

2. **Assumption Violation Testing:** Systematically relax Assumption 4 (conditional independence) by introducing demand patterns informative of inventory levels. Measure how quickly performance degrades as violation increases to understand practical robustness of Kaplan-Meier imputation.

3. **Uncertainty Quantifier Calibration:** Implement multiple uncertainty quantification methods (bootstrap, concentration bounds) for PC-FQI and compare their performance. Test whether uncertainty estimates correctly identify high-risk state-action pairs by checking if pessimistic penalties correlate with actual performance degradation.