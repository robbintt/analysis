---
ver: rpa2
title: Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation
arxiv_id: '2601.10109'
source_url: https://arxiv.org/abs/2601.10109
tags:
- skill
- skills
- data
- training
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient knowledge distillation
  for reasoning models, where standard fine-tuning methods require large-scale data
  and overlook the latent skill structure within training examples. The authors propose
  a skill-centric distillation framework that leverages hierarchical skill trees to
  select training data based on the student model's per-skill weaknesses and incorporates
  explicit skill chains into the training process.
---

# Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation

## Quick Facts
- arXiv ID: 2601.10109
- Source URL: https://arxiv.org/abs/2601.10109
- Reference count: 14
- Primary result: +1.6% Avg@8 accuracy on Qwen3-4B and +1.4% on Qwen3-8B across five math benchmarks using only 1,000 examples

## Executive Summary
This paper addresses the problem of efficient knowledge distillation for reasoning models, where standard fine-tuning methods require large-scale data and overlook the latent skill structure within training examples. The authors propose a skill-centric distillation framework that leverages hierarchical skill trees to select training data based on the student model's per-skill weaknesses and incorporates explicit skill chains into the training process. Using only 1,000 examples from a 100K corpus, the method improves Avg@8 accuracy by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five math reasoning benchmarks, outperforming random data selection baselines. The approach demonstrates that aligning training with model weaknesses and embedding skill structures enhances both efficiency and interpretability in reasoning model distillation.

## Method Summary
The method operates in three stages: first, a hierarchical skill tree (Instruct-SkillMix or EvalTree) is used to label 100K examples from the OpenMathReasoning corpus via top-down attribution with a Qwen2.5-32B-Instruct model. Second, the student model is evaluated on this corpus to compute per-skill accuracy scores, which are then used to sample 1,000 examples inversely proportional to accuracy (with clipping to prevent numerical instability). Third, the student is fine-tuned with supervised fine-tuning (SFT) where each training example is prepended with its explicit skill chain ("Skills: [root → ... → leaf]"). The SFT uses 5 epochs, batch size 32, learning rate 1e-5, and standard hyperparameters. The framework claims that this skill-aware approach improves data efficiency and reasoning performance by targeting model weaknesses and encouraging hierarchical skill decomposition during training.

## Key Results
- Skill-based sampling with 1K examples achieves Avg@8: 72.9, outperforming random sampling (70.7) and full 100K SFT (69.3)
- Adding explicit skill chains improves average accuracy in nearly all settings, with the largest boost on AIME2024 (+5.0)
- Full skill chain (72.9 avg) outperforms root-only (72.2) and leaf-only (72.7) skill chain variants
- Qwen3-4B improves from 71.2 to 72.9 Avg@8; Qwen3-8B improves from 70.8 to 72.2 Avg@8

## Why This Works (Mechanism)

### Mechanism 1: Inverse-Accuracy Sampling Aligns Training with Model Weaknesses
- Claim: Sampling training examples inversely proportional to per-skill accuracy concentrates learning on underperforming skills without degrading already-mastered skills.
- Mechanism: The student model is evaluated on the full corpus; each leaf skill receives an accuracy score. Training examples are sampled with probability P(skill) ∝ acc⁻¹, capped to prevent numerical instability.
- Core assumption: Per-skill accuracy computed on a single pass reliably reflects model competence; accuracy is a stable proxy for skill mastery.
- Evidence anchors:
  - [abstract] "prioritizes examples targeting the student model's weaker skills"
  - [Section 4.2, Figure 2] "Skill-based oversampling effectively aligns SFT data distribution with model weaknesses. Weaker skills that are sampled more frequently correspond to larger accuracy gains."
  - [corpus] AIR (arXiv:2512.13279) similarly uses attention-head influence for data selection, suggesting model-adaptive selection is an active research direction, but does not directly validate skill-based sampling.
- Break condition: If skill-wise accuracy estimates are noisy (limited evaluation data per skill), sampling becomes unstable; if skills are mislabeled, the accuracy signal is misattributed.

### Mechanism 2: Explicit Skill Chain Exposure Induces Hierarchical Reasoning Patterns
- Claim: Prepending the ordered skill chain to each training example encourages the model to internalize skill decomposition as part of its reasoning process.
- Mechanism: Each problem is augmented with "Skills: [root → ... → leaf]" before the solution. This makes implicit skill relationships explicit during SFT.
- Core assumption: Models can learn hierarchical skill structures from explicit textual prefixes and transfer this pattern to unseen problems.
- Evidence anchors:
  - [abstract] "encourages explicit skill decomposition during problem solving"
  - [Section 4.2] "Adding explicit skill chains improves average accuracy in nearly all settings, with the largest boost on AIME2024 (up to +5.0)"
  - [Section 4.3, Table 5] Full skill chain (72.9 avg) outperforms root-only (72.2) and leaf-only (72.7), suggesting the hierarchical structure matters.
  - [corpus] No direct corpus evidence validates skill-chain prefixes specifically; related work (Didolkar et al., 2024, cited in paper) shows skill-prompting helps, but mechanism remains hypothesized.
- Break condition: If skill chains are incomplete, misordered, or inconsistent with actual solution steps, the model may learn spurious patterns.

### Mechanism 3: Hierarchical Top-Down Attribution Avoids Flat Classification Overload
- Claim: Traversing the skill tree top-down with O(log N) decisions per level yields more accurate and comprehensive skill attribution than flat multi-label classification.
- Mechanism: A Qwen2.5-32B-Instruct model is prompted at each tree level to select relevant child skills, recursing until leaf nodes are reached.
- Core assumption: The predefined skill tree adequately covers the problem space; the teacher LLM makes reliable local decisions at each level.
- Evidence anchors:
  - [Section 3, Step 1] "This recursive process leverages the hierarchical structure (with O(log N) complexity) to avoid overwhelming the model with a flat multi-label decision"
  - [Section 3, Footnote 1] "We manually inspected ~100 random QA pairs and found no evidence of missing or mislabeled skills."
  - [corpus] No direct corpus evidence compares hierarchical vs. flat attribution; this remains an implementation choice with limited external validation.
- Break condition: If the skill tree is misaligned with actual problem-solving skills, or if the LLM attritor makes systematic errors at any level, errors propagate down the chain.

## Foundational Learning

- Concept: **Supervised Fine-Tuning (SFT) for Reasoning Distillation**
  - Why needed here: The entire framework operates within the SFT paradigm; understanding why "less data, better selected" outperforms "more data, randomly sampled" (Table 1: full 100K underperforms base model) is critical.
  - Quick check question: Can you explain why training on 100K examples might degrade performance relative to the base model, and what this implies about data quality vs. quantity?

- Concept: **Hierarchical Skill Taxonomies**
  - Why needed here: The method depends on a pre-defined skill tree (Instruct-SkillMix or EvalTree) to map problems and guide sampling.
  - Quick check question: If you were given a new domain (e.g., legal reasoning), how would you construct or validate a skill tree?

- Concept: **Model-Adaptive Data Selection**
  - Why needed here: The sampling distribution is computed from the student model's own performance profile, making the process student-specific.
  - Quick check question: What would happen if two student models had opposite weakness profiles—would they receive different training sets under this framework?

## Architecture Onboarding

- Component map: Skill Tree -> Attribution Module (Qwen2.5-32B-Instruct) -> Student Evaluator -> Sampler -> Skill-Aware SFT Trainer

- Critical path:
  1. One-time: Label entire 100K corpus with skill chains via attribution module (~200 GPU hours).
  2. Per-student: Evaluate student on corpus → compute per-skill accuracy → sample 1K examples → fine-tune with skill-aware SFT (~40 GPU hours).
  3. The labeling step is amortizable across multiple students.

- Design tradeoffs:
  - **Skill tree choice**: Instruct-SkillMix vs. EvalTree (Table 2 shows comparable results; EvalTree slightly better on some benchmarks).
  - **Sampling aggressiveness (T)**: T=0.5 (70.7 avg) → T=1.0 (72.9 avg) → T=2.0 (72.0 avg); diminishing returns beyond T=1.0 (Table 5).
  - **Skill chain depth**: Full chain > leaf-only > root-only (Table 5), but marginal gains.

- Failure signatures:
  - **Full corpus SFT degrades performance**: Table 1 shows training on 100K samples underperforms the base model—this is a known failure mode in reasoning distillation.
  - **No improvement on already-strong skills**: Figure 3 shows skill-based sampling preserves strong-skill accuracy; if this drops, sampling may be too aggressive.
  - **Skill attribution errors**: Manual inspection of ~100 samples found no errors, but systematic mislabeling would propagate through the pipeline.

- First 3 experiments:
  1. **Validate skill attribution quality**: Manually inspect 50–100 labeled examples across different skill tree depths; verify skill chains match the solution steps.
  2. **Ablate sampling aggressiveness**: Run T ∈ {0.5, 1.0, 2.0, 3.0} on a held-out validation set; confirm the paper's finding that T=1.0 is a stable default.
  3. **Test generalization to a new model family**: Apply the pipeline to a non-Qwen model (e.g., LLaMA-based) and verify skill-based selection + skill-aware SFT still improves over random sampling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can automatically learned or dynamic skill hierarchies improve distillation performance compared to static, pre-defined skill trees?
- Basis in paper: [explicit] Section 6 (Limitations) states that the reliance on existing skill trees may not perfectly align with the student model's decomposition and suggests exploring "automatically learned skill hierarchies" as future work.
- Why unresolved: The current method relies on fixed taxonomies (Instruct-SkillMix, EvalTree) which impose an external structure that may not match the student model's internal representation of skills.
- What evidence would resolve it: Experiments comparing the performance of student models trained on data selected via dynamic, model-derived skill