---
ver: rpa2
title: Optimal Conformal Prediction under Epistemic Uncertainty
arxiv_id: '2505.19033'
source_url: https://arxiv.org/abs/2505.19033
tags:
- prediction
- coverage
- sets
- credal
- second-order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of incorporating second-order
  predictors (e.g., credal sets or Bayesian models) into conformal prediction while
  maintaining conditional coverage guarantees. The authors introduce Bernoulli Prediction
  Sets (BPS), which construct the smallest prediction sets that ensure conditional
  coverage given valid second-order predictions.
---

# Optimal Conformal Prediction under Epistemic Uncertainty

## Quick Facts
- arXiv ID: 2505.19033
- Source URL: https://arxiv.org/abs/2505.19033
- Reference count: 40
- Key result: Introduces Bernoulli Prediction Sets (BPS) that optimally incorporate second-order predictors (credal sets) into conformal prediction while maintaining conditional coverage guarantees

## Executive Summary
This paper addresses a fundamental limitation of conformal prediction: its inability to distinguish between epistemic and aleatoric uncertainty. Standard conformal methods use only first-order predictions (e.g., softmax outputs), which don't capture model uncertainty. The authors introduce Bernoulli Prediction Sets (BPS), a method that constructs optimal prediction sets using second-order predictors (like ensembles or Bayesian models) that provide credal sets. BPS generalizes Adaptive Prediction Sets and maintains conditional coverage guarantees when the second-order predictions are valid, falling back to marginal coverage via conformal risk control when they are not.

## Method Summary
BPS constructs prediction sets by solving a linear program that ensures coverage constraints are met for all vertices of the credal set simultaneously. Given a second-order distribution, the method samples m first-order predictions to form a credal set, then solves an optimization problem to find the smallest prediction set that satisfies 1-α conditional coverage for every distribution in the credal set. When validity assumptions are compromised, conformal risk control is applied to obtain marginal coverage guarantees through threshold calibration on a held-out set.

## Key Results
- On synthetic data with valid credal sets, BPS consistently satisfies conditional coverage while APS often violates it
- On CIFAR-10 with conformalized credal sets, BPS-nom achieves better conditional coverage (0.987 vs 0.868) with manageable set size increase
- Across CIFAR-10 and CIFAR-100 with ensemble, MC dropout, and evidential models, BPS-nom consistently achieves the best conditional coverage
- BPS-based methods show better performance on epistemic uncertainty stratified coverage (EUSC) measures

## Why This Works (Mechanism)

### Mechanism 1
If a second-order predictor provides a credal set containing the true first-order distribution, BPS can construct smaller prediction sets that guarantee conditional coverage better than using a single mean distribution. BPS formulates prediction set construction as a linear program that ensures coverage constraints are met for all vertices of the credal set simultaneously. This optimization ensures coverage for every distribution in the credal set, not just their mean. The core assumption is that the credal set provided as input is valid, meaning the oracle first-order distribution is contained within it.

### Mechanism 2
When second-order predictions are not strictly valid, BPS uses conformal risk control to guarantee marginal coverage. A threshold λ is calibrated on a held-out calibration set to ensure the expected miscoverage risk is bounded. The optimal λ is found via binary search to satisfy the marginal coverage constraint on the calibration data. This mechanism relies on the exchangeability assumption between calibration and test data.

### Mechanism 3
Given a second-order distribution (e.g., from a Bayesian neural network or ensemble), sampling a sufficient number of first-order predictions from it and forming their convex hull yields a valid credal set with high probability. The validity depends on the Tukey depth of the true distribution relative to the second-order distribution. For non-zero depth, the probability of the credal set being invalid decreases exponentially with the number of samples. This mechanism assumes the second-order distribution has non-zero Tukey depth with respect to the true distribution.

## Foundational Learning

- **Concept:** Conformal Prediction (CP)
  - **Why needed here:** This is the base framework. BPS is an extension of CP designed for second-order predictions. Understanding nonconformity scores, calibration sets, and marginal coverage guarantees is a prerequisite.
  - **Quick check question:** Given a calibration set of nonconformity scores, how is the prediction set for a new test point constructed to guarantee 1-α marginal coverage?

- **Concept:** Adaptive Prediction Sets (APS)
  - **Why needed here:** BPS is shown to be a generalization of APS. APS is the optimal solution for conditional coverage when only a single (oracle) first-order prediction is available. BPS reduces to APS in this special case.
  - **Quick check question:** How does APS construct a prediction set to achieve conditional coverage given a single first-order probability distribution?

- **Concept:** Epistemic vs. Aleatoric Uncertainty & Second-Order Predictions
  - **Why needed here:** The core motivation of the paper. Standard CP uses first-order predictions (softmax) which don't distinguish between these. Second-order predictions (credal sets, Bayesian models) do. The paper's entire goal is to optimally incorporate this second-order information into CP.
  - **Quick check question:** What is the key difference between a first-order prediction (e.g., softmax output) and a second-order prediction (e.g., output from a deep ensemble) in terms of the uncertainty they represent?

## Architecture Onboarding

- **Component Map:**
  Second-Order Predictor -> Sampler -> BPS Solver -> Calibrator -> Set Generator

- **Critical Path:**
  1. **Offline:** Train any standard second-order model (Ensemble, MC Dropout, etc.)
  2. **Offline Calibration:**
     a. For each point in calibration set, get second-order prediction
     b. Sample m first-order predictions to form credal set
     c. Run the BPS Solver for different λ values as part of binary search
     d. Find λ* that achieves target marginal coverage on calibration set
  3. **Online Inference:**
     a. For a new test point, get second-order prediction
     b. Sample m first-order predictions to form credal set
     c. Run BPS Solver once using pre-computed λ* to get b*
     d. Generate final prediction set by sampling from Bernoulli distributions

- **Design Tradeoffs:**
  - Conditional vs. Marginal Coverage: Theoretical optimality holds for conditional coverage assuming valid credal sets, but falls back to marginal coverage in practice
  - Threshold Selection (λCP vs. λCons): Using conformally calibrated λCP guarantees marginal coverage but may hurt conditional coverage; conservative λCons is safer for conditional coverage but produces larger sets
  - Number of Samples (m): Larger m increases probability of valid credal set but increases computational cost

- **Failure Signatures:**
  - Persistent Under-Coverage on High-EU Data: If second-order predictor fails to capture epistemic uncertainty, BPS won't provide conditional coverage benefits
  - Excessively Large Prediction Sets: May indicate extremely uncertain second-order predictor leading to very large credal sets
  - Computational Bottleneck in Calibration: Solving many linear programs per iteration can be slow

- **First 3 Experiments:**
  1. Replicate synthetic experiment with artificially generated credal sets of varying radii, comparing BPS vs. APS
  2. On CIFAR-10/100, use ensemble model and vary m samples to form credal set, plotting coverage vs. m
  3. On CIFAR-10 with pre-trained ensemble, compare BPS performance using both λCP and conservative λCons thresholds

## Open Questions the Paper Calls Out
None

## Limitations
- Primary limitation is dependence on validity of second-order predictions, which cannot be verified in practice
- Computational cost of solving linear programs for each calibration point can be prohibitive for large-scale applications
- Effectiveness heavily depends on quality of second-order predictor's ability to capture epistemic uncertainty

## Confidence

- **High Confidence:** Core mechanism of BPS (solving LP to find minimal prediction sets) is well-defined and mathematically sound under stated assumptions
- **Medium Confidence:** Empirical results showing BPS outperforming APS on conditional coverage metrics are convincing but limited to specific datasets and model types
- **Low Confidence:** Claim about exponential decrease in credal set invalidity probability relies on theoretical results that may not hold in practice with finite samples and complex distributions

## Next Checks

1. **Stress Test on Distribution Shift:** Evaluate BPS on dataset with known covariate shift to assess whether conformal risk control threshold calibration remains effective when exchangeability is violated

2. **Ablation on Credal Set Quality:** Compare BPS performance using credal sets from different second-order predictors (ensemble, MC dropout, evidential) on same dataset to quantify how epistemic uncertainty estimation quality impacts conditional coverage

3. **Scalability Analysis:** Benchmark computational cost of BPS calibration against APS and other conformal methods on datasets of varying sizes to assess practical viability