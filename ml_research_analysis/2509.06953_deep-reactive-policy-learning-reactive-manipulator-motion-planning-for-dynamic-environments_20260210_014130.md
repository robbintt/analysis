---
ver: rpa2
title: 'Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic
  Environments'
arxiv_id: '2509.06953'
source_url: https://arxiv.org/abs/2509.06953
tags:
- policy
- dynamic
- motion
- impact
- obstacle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Deep Reactive Policy (DRP), a visuo-motor
  neural motion policy designed for reactive collision-free goal reaching in dynamic,
  partially observable environments. The core innovation is IMPACT, a transformer-based
  policy pretrained on 10M expert trajectories from a GPU-accelerated planner (cuRobo)
  and further refined via iterative student-teacher distillation with a locally reactive
  controller (Geometric Fabrics).
---

# Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments

## Quick Facts
- arXiv ID: 2509.06953
- Source URL: https://arxiv.org/abs/2509.06953
- Reference count: 40
- Primary result: 95.7% success rate in dynamic scenes and 100% in static scenes for reactive collision-free goal reaching with 3.48ms cold-start inference

## Executive Summary
This paper introduces Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive collision-free goal reaching in dynamic, partially observable environments. The core innovation is IMPACT, a transformer-based policy pretrained on 10M expert trajectories from a GPU-accelerated planner (cuRobo) and further refined via iterative student-teacher distillation with a locally reactive controller (Geometric Fabrics). During inference, DRP enhances dynamic obstacle avoidance using DCP-RMP, a point-cloud-based Riemannian Motion Policy. DRP operates directly on point clouds and achieves strong generalization across diverse simulation and real-world tasks, outperforming classical planners and prior neural methods in success rates, especially in dynamic and goal-blocking scenarios.

## Method Summary
DRP is a visuo-motor neural motion policy that generates collision-free manipulator trajectories in dynamic environments. It uses IMPACT, a transformer-based policy, which is first pretrained via behavior cloning on 10M expert trajectories generated by cuRobo. This is followed by iterative student-teacher finetuning in IsaacGym, where a teacher model (IMPACT + Geometric Fabrics) distills knowledge into a point-cloud-only student. At inference, DRP combines IMPACT's reactive motion generation with DCP-RMP for dynamic obstacle avoidance. The method is evaluated on a 7-DOF Franka Panda arm across static, dynamic, and goal-blocking scenarios in both simulation and the real world.

## Key Results
- 95.7% success rate in dynamic scenes (vs 80.3% for AIT* and 84.8% for NeuralMP)
- 100% success rate in static scenes (vs 80.0% for AIT* and 94.7% for NeuralMP)
- 3.48ms cold-start inference time
- Strong generalization across unseen obstacle types and environments

## Why This Works (Mechanism)
DRP combines the strengths of classical planners and learned policies: cuRobo provides high-quality expert trajectories for pretraining, Geometric Fabrics enables reactive local corrections during finetuning, and DCP-RMP provides a fast, reactive dynamic obstacle avoidance layer at inference. The transformer-based IMPACT policy can process rich point cloud information and output reactive actions, while the student-teacher distillation allows the policy to generalize beyond static expert data.

## Foundational Learning
- **Behavior Cloning (BC)**: Used to pretrain IMPACT on expert trajectories; needed to bootstrap the policy with safe, collision-free motions; quick check: loss curves during pretraining should decrease steadily.
- **Student-Teacher Distillation**: Geometric Fabrics (with privileged SDFs) teaches the point-cloud-only student; needed to enable reactive behaviors in partially observable environments; quick check: student SR should approach teacher SR after finetuning.
- **Riemannian Motion Policy (RMP)**: DCP-RMP provides reactive dynamic obstacle avoidance; needed for fast, local corrections during inference; quick check: RMP gradient fields should repel the end-effector from nearby obstacles.

## Architecture Onboarding
- **Component Map**: cuRobo Planner -> IMPACT (Pretrained) -> IMPACT + Geometric Fabrics (Teacher) -> IMPACT (Student) -> IMPACT + DCP-RMP (Inference)
- **Critical Path**: Point cloud input -> IMPACT -> (DCP-RMP for dynamics) -> Robot action
- **Design Tradeoffs**: Use of privileged information (SDFs) during training but not at inference; reliance on static expert data augmented with reactive distillation; external reactive module (DCP-RMP) vs end-to-end dynamic learning.
- **Failure Signatures**: High collision rate with static obstacles suggests incomplete finetuning or SDF issues; poor dynamic avoidance indicates DCP-RMP not active or incorrect KD-tree extraction; real-world degradation points to perception pipeline issues.
- **First Experiments**:
  1. Verify IMPACT pretraining loss and SR on static validation set.
  2. Test student-teacher finetuning: compare teacher vs student SR in static IsaacGym scenes.
  3. Enable DCP-RMP and measure FDO/DGB performance in dynamic simulation.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can a single DRP policy be trained to generalize across diverse robot embodiments, or does the pipeline require separate models for each platform? (Basis: Explicit limitation on page 9)
- **Open Question 2**: Does incorporating raw RGB or RGB-D inputs improve DRP's robustness in unstructured environments compared to the current point-cloud-only approach? (Basis: Explicit suggestion on page 9)
- **Open Question 3**: Can the reactive behaviors provided by the DCP-RMP module be internalized into the core IMPACT policy to enable fully end-to-end dynamic planning? (Basis: Inferred from computational infeasibility of dynamic expert generation)

## Limitations
- Limited to a single robot embodiment (Franka Panda); scaling to multiple embodiments is a challenge.
- Relies on privileged information (SDFs) during training but not at inference; performance gap not quantified.
- Real-world evaluation is limited to static scenes; dynamic robustness not fully validated.

## Confidence
- 95.7% success rate in dynamic scenes: High confidence (well-supported by ablation studies and baseline comparisons)
- 100% success rate in static scenes: High confidence (strong baseline superiority, measurable metric)
- Strong generalization claims: Medium confidence (rely on breadth of 10M trajectory dataset, not fully specified)
- 3.48ms cold-start inference time: High confidence (direct, measurable hardware-dependent metric)

## Next Checks
1. Verify data generation and sampling strategy—reproduce the exact scene and obstacle distributions used in pretraining.
2. Reproduce IMPACT architecture and training details—check hidden dimensions, MHSA heads, and optimization parameters.
3. Reimplement Geometric Fabrics finetuning pipeline—validate SDF precomputation and student-teacher update schedule.