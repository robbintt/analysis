---
ver: rpa2
title: How Benchmark Prediction from Fewer Data Misses the Mark
arxiv_id: '2506.07673'
source_url: https://arxiv.org/abs/2506.07673
tags:
- benchmark
- data
- estimation
- prediction
- random-sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of efficient LLM evaluation
  through benchmark prediction, where the goal is to estimate model performance on
  a full benchmark using only a small subset of data points. The authors evaluate
  11 state-of-the-art benchmark prediction methods across 19 diverse benchmarks, comparing
  them against simple baselines.
---

# How Benchmark Prediction from Fewer Data Misses the Mark

## Quick Facts
- **arXiv ID**: 2506.07673
- **Source URL**: https://arxiv.org/abs/2506.07673
- **Reference count**: 40
- **Primary result**: Simple random sampling with regression learning outperforms most sophisticated benchmark prediction methods in interpolation regimes

## Executive Summary
This paper investigates efficient LLM evaluation through benchmark prediction, where the goal is to estimate model performance on a full benchmark using only a small subset of data points. The authors evaluate 11 state-of-the-art methods across 19 diverse benchmarks and find that a simple random sampling with regression learning baseline outperforms most existing methods. The study reveals that these methods heavily rely on model similarity and struggle when evaluating novel, potentially superior models - precisely when such methods would be most valuable. A new augmented inverse propensity weighting (AIPW) method consistently outperforms random sampling in both interpolation and extrapolation regimes, though gains are modest.

## Method Summary
The paper compares 11 benchmark prediction methods across 19 benchmarks using performance matrices where each entry represents a model's accuracy on a specific data point. The key baseline is Random-Sampling-Learn, which randomly samples n=50 points and fits a Ridge regression to map subset performance to full-benchmark performance. Methods are evaluated under two scenarios: interpolation (random 75/25 source/target split) and extrapolation (bottom 50% accuracy models as sources, top 30% as targets). The primary metric is estimation gap - the absolute difference between true and predicted full-benchmark scores. The study finds that random sampling with learning consistently outperforms complex core-set selection methods, and that all methods struggle when evaluating models that significantly outperform the source models.

## Key Results
- Random-Sampling-Learn (random sampling + Ridge regression) outperforms most existing methods, reducing estimation gap by 37% in interpolation settings
- All methods show dramatically reduced effectiveness when evaluating models that outperform source models (extrapolation regime), with most failing to outperform basic random sampling
- AIPW consistently outperforms Random-Sampling both in interpolation and extrapolation, though gains are modest
- No method outperforms Random-Sampling when given twice the data budget, even in interpolation settings

## Why This Works (Mechanism)

### Mechanism 1: Regression on Random Subsets
- **Claim**: Fitting regression on randomly sampled subset outperforms complex core-set selection algorithms
- **Mechanism**: Random-Sampling-Learn treats benchmark prediction as supervised learning, capturing correlation structure between data points through regression rather than sophisticated selection
- **Core assumption**: Stable linear relationship exists between performance on random subset and full benchmark
- **Evidence**: Random-Sampling-Learn consistently outperforms most methods and challenges the prevailing notion that core-set selection is the primary driver of success

### Mechanism 2: Model Similarity Interpolation
- **Claim**: Methods succeed by interpolating among similar models rather than discovering universal evaluation signals
- **Mechanism**: Methods leverage source model performance matrix; accurate inference depends on target model behaving similarly to source models
- **Core assumption**: Target model lies within convex hull of source models' performance distributions
- **Evidence**: Strong negative correlation between model similarity and estimation gap; methods fail at evaluation frontier with superior models

### Mechanism 3: AIPW for Variance Reduction
- **Claim**: AIPW acts as consistent estimator reducing variance compared to random sampling
- **Mechanism**: Combines observed sample mean with debiasing term from source model predictions, correcting random sample mean
- **Core assumption**: Regression trained on source models provides informative signal for target model
- **Evidence**: AIPW reliably outperforms Random-Sampling in both interpolation and extrapolation thanks to being a consistent estimator

## Foundational Learning

- **Concept: Interpolation vs. Extrapolation in Model Space**
  - **Why needed here**: Central finding is methods work for interpolation but fail for extrapolation; understanding this distinction is critical for knowing when to trust methods
  - **Quick check**: If your new model significantly outperforms all previous source models, will Random-Sampling-Learn likely overestimate or underestimate its true performance?

- **Concept: Core-set Selection vs. Learning to Predict**
  - **Why needed here**: Many prior works focused on which data points to select; this paper shifts focus to how to aggregate those points, rendering complex selection largely redundant
  - **Quick check**: Why does the paper claim that Random-Sampling-Learn challenges the "prevailing notion" about core-set selection?

- **Concept: Consistent Estimators**
  - **Why needed here**: AIPW is favored because it's a "consistent estimator" that converges to true value as data increases, unlike standard regression which may have asymptotic bias
  - **Quick check**: Why is the "debiasing" term in AIPW critical for maintaining performance even when regression model isn't perfect?

## Architecture Onboarding

- **Component map**: Data Store (full performance matrix S(F(s), D)) -> Core-set Generator (selects n=50 indices) -> Predictor (h: estimation logic) -> Evaluator (computes estimation gap)
- **Critical path**: 1) Split models into Source (train) and Target (test) 2) For Target models, mask all data except selected Core-set C 3) Train Predictor on Source models using only C to predict full scores 4) Apply Predictor to Target models' visible C data to estimate full scores
- **Design tradeoffs**:
  - Selection Strategy: Random is faster and robust; Anchor-Points is computationally heavier with no significant edge
  - Estimator Choice: Ridge Regression minimizes error in interpolation but fails catastrophically at frontier; AIPW is more robust with modest gains
  - Core-set Size (n): Increasing n helps all methods; Random-Sampling eventually matches complex methods if given 2Ã— data budget
- **Failure signatures**: Frontier Collapse (estimation gap explodes for top-performing targets), Negative Reduction (complex methods worse than naive mean when distributions diverge)
- **First 3 experiments**:
  1. Implement Random-Sampling and Random-Sampling-Learn; verify Learn reduces estimation gap by ~37% in random split
  2. Implement extrapolation: use bottom 50% as Source, top 30% as Target; observe failure of Random-Sampling-Learn and stability of AIPW
  3. Compute model similarity vs. estimation gap; verify negative correlation for regression methods and neutral/superior profile of AIPW

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can benchmark prediction methods be developed that do not rely on model similarity and maintain effectiveness in the extrapolation regime?
- **Basis**: Authors conclude methods fail "just when it is most needed: at the evaluation frontier, where the goal is to evaluate new models of unknown capabilities"
- **Why unresolved**: AIPW still relies on model similarity for variance reduction, providing only modest improvements
- **Evidence needed**: Method maintaining low estimation gap for target models with low similarity to source models, particularly when target models outperform all source models

### Open Question 2
- **Question**: What is the theoretical trade-off between method sophistication and simply increasing the sampling budget for Random-Sampling?
- **Basis**: Authors find no method outperforms Random-Sampling when given twice as much data, even in interpolation
- **Why unresolved**: Paper empirically demonstrates phenomenon but lacks theoretical characterization of when each approach is more efficient
- **Evidence needed**: Theoretical analysis showing conditions for efficiency, or empirical scaling laws comparing estimation gap reduction per unit computational cost

### Open Question 3
- **Question**: How can core-set selection methods be improved to provide meaningful benefits over random sampling?
- **Basis**: Paper challenges prevailing notion that core of benchmark prediction lies in identifying most informative subset
- **Why unresolved**: Authors demonstrate existing core-set selection is ineffective but don't identify what properties would make selection valuable
- **Evidence needed**: Core-set selection method consistently outperforming random sampling with same sample size, demonstrating what makes certain examples more informative

## Limitations
- Findings based on existing leaderboard performance matrices that may not represent all real-world evaluation scenarios
- Extrapolation regime uses specific accuracy-based split that may not generalize to all evaluation frontiers
- Paper doesn't exhaustively test all possible selection strategies or their effectiveness across different benchmark types
- AIPW effectiveness, while demonstrated, shows only modest improvements over random sampling

## Confidence
- **High confidence**: Core finding that Random-Sampling-Learn outperforms most existing methods in interpolation settings is well-supported by extensive experiments across 19 benchmarks
- **Medium confidence**: Claim that complex core-set selection is largely unnecessary is supported but doesn't exhaustively test all strategies; AIPW effectiveness shows only modest gains
- **Medium confidence**: Interpretation that methods fail because they interpolate rather than discover universal signals is reasonable but relies on indirect evidence

## Next Checks
1. **Model Architecture Sensitivity**: Test whether Random-Sampling-Learn effectiveness varies significantly across different model architectures (encoder-decoder vs. decoder-only) beyond just performance levels
2. **Benchmark Type Dependency**: Evaluate whether certain benchmark characteristics (multiple choice vs. generation, synthetic vs. natural data) influence relative performance of selection strategies
3. **Alternative Extrapolation Scenarios**: Design extrapolation tests beyond simple accuracy ranking (e.g., models with novel capabilities, different training paradigms) to better understand method limits