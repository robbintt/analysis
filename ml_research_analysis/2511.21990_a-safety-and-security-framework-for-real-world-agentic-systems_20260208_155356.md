---
ver: rpa2
title: A Safety and Security Framework for Real-World Agentic Systems
arxiv_id: '2511.21990'
source_url: https://arxiv.org/abs/2511.21990
tags:
- agentic
- risk
- agent
- safety
- security
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a compositional safety and security framework
  for agentic AI systems that treats safety and security as emergent system-level
  properties arising from dynamic interactions among models, orchestrators, tools,
  and data. The framework operationalizes contextual risk management through auxiliary
  AI models and agents with human oversight, enabling automated risk discovery, evaluation,
  and mitigation.
---

# A Safety and Security Framework for Real-World Agentic Systems

## Quick Facts
- arXiv ID: 2511.21990
- Source URL: https://arxiv.org/abs/2511.21990
- Reference count: 0
- Primary result: Agent Red Teaming via Probes (ARP) reduced attack success from 24% to 3.7% in multi-stage agentic workflows

## Executive Summary
This paper introduces a compositional safety and security framework for agentic AI systems that treats safety and security as emergent system-level properties arising from dynamic interactions among models, orchestrators, tools, and data. The framework operationalizes contextual risk management through auxiliary AI models and agents with human oversight, enabling automated risk discovery, evaluation, and mitigation. A novel methodology called Agent Red Teaming via Probes (ARP) is presented, using injection and evaluation probes to test attack propagation through multi-stage agentic workflows. Applied to NVIDIA's AIRA research assistant, ARP discovered that direct attacks propagate and amplify through workflow stages while indirect attacks attenuate, with 24% baseline attack success reduced to 3.7% using layered defenses. The study also releases a dataset of over 10,000 attack/defense traces to advance agentic safety research.

## Method Summary
The framework instruments agentic workflows as graphs with injection probes at external data entry points (user input, RAG retrieval, web search) and evaluation probes at processing nodes. Threat snapshots define test scenarios (injection point, evaluation point, attack objective, metric) without attack payloads, which are added to create threat executables. ARP runs these executables to measure normalized risk scores (0-1) at each stage, revealing propagation patterns. Defenses are then deployed contextually at high-risk edges rather than universally. The approach was validated on NVIDIA's AIRA system using the Nemotron-AIQ-Agentic-Safety-Dataset-1.0 with 9 risk categories across 2,200 attacks.

## Key Results
- Direct attacks propagate and amplify through workflow stages (mean risk scores increasing from 0.42 to 0.44)
- Indirect attacks attenuate through workflow stages (risk scores declining from 0.38 to 0.17)
- 24% baseline attack success reduced to 3.7% with layered defenses (guard model + prompt hardening)
- Dataset released with over 10,000 attack/defense traces for community research

## Why This Works (Mechanism)

### Mechanism 1: Compositional Risk Emergence
Component-level risks compound through workflow execution, creating cascading effects that can amplify localized hazards into system-level harms. System-level risks are composed of granular component-level risks accounting for both compounding effects and second-order interactions.

### Mechanism 2: Probe-Based Attack Propagation Quantification
Injection probes at workflow entry points and evaluation probes at downstream nodes measure how attacks propagate, amplify, or attenuate through multi-stage agentic workflows by computing normalized risk scores at each stage.

### Mechanism 3: Layered Contextual Defense Placement
Defenses deployed strategically at high-risk workflow edges achieve greater risk reduction per unit overhead than blanket coverage at all interfaces, enabling targeted mitigation of the most vulnerable components.

## Foundational Learning

- **Agentic Risk Taxonomy**: Operational categorization (low/medium/high impact) based on autonomy level, blast radius, and detection maturity. Guides prioritization of risk mitigation efforts.
  - Quick check: Given a multi-agent system where agents can modify shared database state, would "memory poisoning" be classified as low, medium, or high impact per Table 3.1?

- **Threat Snapshots vs. Threat Executables**: Threat snapshots define test scenarios but lack attack payloads; combining snapshots with payloads creates threat executables that actually run. Confusing these leads to incomplete test definitions.
  - Quick check: If you have a threat snapshot specifying injection at RAG output and evaluation at final report, what additional component is required to execute the test?

- **LLM-as-Judge with Calibration Constraints**: Framework relies on LLM-based JUDGE metrics with 76.8% exact agreement with human labels (98% when tolerating Â±0.5 error). Understanding judge limitations and calibration constraints is critical for interpreting results.
  - Quick check: What are the four constraints the paper implements to reduce LLM-judge calibration drift?

## Architecture Onboarding

- **Component map**: Global Contextualized Safety Agent -> Local Contextualized Attacker Agent -> Local Contextualized Defender Agent -> Local Evaluator Agent
- **Critical path**:
  1. Instrument workflow with injection probes at external data entry points
  2. Place evaluation probes at processing nodes
  3. Define threat snapshots with attack objectives and metrics
  4. Generate contextual attack payloads to form threat executables
  5. Run ARP in sandboxed environment; collect risk scores
  6. Analyze propagation patterns to identify high-risk edges
  7. Deploy defender agents with layered defenses at those edges
  8. Monitor with evaluator agents; update risk profiles on drift detection
- **Design tradeoffs**: Probe granularity vs. latency overhead; guard model precision vs. false positive rate; static vs. adaptive attack generation; single-judge vs. multi-judge evaluation
- **Failure signatures**: Risk scores consistently at 1.0 despite defenses; high variance between workflow stages; judge-human agreement below 70%; direct attacks amplify while indirect attenuate
- **First 3 experiments**:
  1. Baseline propagation profiling: Run ARP without defenses to establish which attack types amplify vs. attenuate
  2. Single-defense ablation study: Add one defense type at highest-risk node to map which defenses address which risk categories
  3. Layered defense integration: Combine guard model + prompt rules at high-risk node; measure attack success, latency, and false positive rates

## Open Questions the Paper Calls Out

### Open Question 1
How do layered defense mechanisms quantitatively impact agent utility, latency, and task completion rates in complex workflows? The paper demonstrates risk reduction (24% to 3.7% attack success) but does not measure operational costs of these mitigations on system performance or helpfulness.

### Open Question 2
Can agent source code analysis automatically generate optimal injection and evaluation probe placements to replace manual instrumentation? Currently, ARP relies on manual engineering, creating a scalability bottleneck for diverse agentic architectures.

### Open Question 3
How can we develop reliable metrics to detect higher-order system properties like goal misalignment and multi-agent collusion? Current evaluation relies on LLM-judges and string matching, which fail to capture latent intent or complex behavioral patterns in autonomous agents.

## Limitations
- Dependency on closed-source components, particularly Lakera's proprietary attacker agent with RL-trained LLM
- Reliance on a single agentic system (NVIDIA AIRA) and specific dataset, limiting generalizability
- LLM-as-judge methodology introduces potential bias and may not capture all safety-relevant nuances

## Confidence
- **Compositional Risk Emergence**: Medium-High confidence - theoretical foundation is well-established but empirical validation across diverse systems is limited
- **Probe-Based Attack Propagation Quantification**: High confidence - methodology is clearly specified and reproducible with released dataset
- **Layered Contextual Defense Placement**: Medium-High confidence - quantitative results are compelling but exact defense configurations are not fully disclosed

## Next Checks
1. **Cross-System Generalization Test**: Apply ARP methodology to a different agentic system (e.g., customer service bot) and compare propagation patterns to those observed in AIRA to test generalizability.

2. **Defense Transferability Study**: Take defense configurations that achieved 3.7% ASR in AIRA and apply them to the cross-system test to measure portability and adaptation requirements.

3. **Real-World Deployment Simulation**: Implement time-extended simulation where ARP runs continuously in a live agentic system with human-in-the-loop oversight to assess operational viability beyond controlled experiments.