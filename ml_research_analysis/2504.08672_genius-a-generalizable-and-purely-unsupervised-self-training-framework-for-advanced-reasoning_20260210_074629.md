---
ver: rpa2
title: 'Genius: A Generalizable and Purely Unsupervised Self-Training Framework For
  Advanced Reasoning'
arxiv_id: '2504.08672'
source_url: https://arxiv.org/abs/2504.08672
tags:
- arxiv
- genius
- reasoning
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving LLM reasoning without
  external supervision, such as labeled responses or reward models. It introduces
  Genius, a generalizable and purely unsupervised self-training framework that enables
  LLMs to self-improve their reasoning abilities using only general queries.
---

# Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning

## Quick Facts
- arXiv ID: 2504.08672
- Source URL: https://arxiv.org/abs/2504.08672
- Authors: Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, Zhiyong Wu
- Reference count: 19
- Primary result: Unsupervised self-training framework achieving >7% average improvement on reasoning benchmarks without external supervision

## Executive Summary
This paper tackles the challenge of improving LLM reasoning without external supervision, such as labeled responses or reward models. It introduces Genius, a generalizable and purely unsupervised self-training framework that enables LLMs to self-improve their reasoning abilities using only general queries. The core idea is to generate responses stepwise, simulate future outcomes via foresight sampling, and use the resulting foresight scores to explore and exploit optimal response sequences. To handle noise and uncertainty inherent in the unsupervised setting, the method employs an advantage-calibrated optimization (ACO) loss that downweights training examples with inconsistent advantages.

Experiments on diverse reasoning benchmarks (GSM8K, MATH, ReClor, LogiQA, StrategyQA, GPQA, ARC-Challenge) show that Genius significantly improves average performance—by over 7%—compared to strong baselines like SFT, SPIN, STaR, CoH, Self-Rewarding, and ScPO. It consistently outperforms these methods, particularly on challenging tasks such as MATH, where it improves accuracy by more than 4% over Self-Rewarding. The framework also maintains stability on general benchmarks (AlpacaEval, WildBench, ArenaHard, WikiBench, MMLU, MMLU-Pro), avoiding catastrophic forgetting after post-training.

## Method Summary
Genius employs stepwise foresight re-sampling to generate responses, simulating future outcomes to compute foresight scores that guide exploration and exploitation. For each query, the method runs K=4 steps with beam M=2 and N=4 rollouts per beam, generating foresight scores by averaging log-probabilities of future steps. These scores are normalized and used to construct preference pairs (positive=highest foresight, negative=re-sampled), which are then optimized using an advantage-calibrated optimization (ACO) loss. The ACO loss downweights training examples with inconsistent advantages, addressing the noise and uncertainty inherent in unsupervised settings. The framework is trained with DeepSpeed Zero3 on 8×A100, batch size 128, lr=5e-7, and evaluated on reasoning benchmarks using vLLM with specified shots.

## Key Results
- Achieves >7% average improvement across 7 reasoning benchmarks compared to strong baselines
- Outperforms Self-Rewarding by 4%+ on MATH benchmark
- Maintains performance on general benchmarks while improving reasoning capabilities
- Improves AIME 2024 performance by 6.67% on competition-level tasks

## Why This Works (Mechanism)
Genius addresses the fundamental challenge of improving reasoning in LLMs without external supervision by creating an internal reward signal through foresight sampling. The framework generates stepwise responses and simulates future outcomes to compute foresight scores, which serve as a proxy for response quality. By building preference pairs based on these scores and optimizing with ACO loss, the method effectively learns to distinguish high-quality reasoning chains from suboptimal ones. The ACO loss is crucial as it downweights training examples with inconsistent advantages, mitigating the noise inherent in unsupervised learning and preventing the model from being misled by unreliable foresight estimates.

## Foundational Learning
- **Foresight sampling**: Simulates future outcomes to estimate response quality; needed because no external rewards exist; quick check: verify foresight scores correlate with actual response quality on validation set
- **Advantage-calibrated optimization**: Downweights training examples with inconsistent advantages; needed to handle noise in unsupervised setting; quick check: monitor advantage distribution during training
- **Preference pair construction**: Creates positive/negative examples from foresight scores; needed to frame learning as pairwise ranking; quick check: ensure positive pairs genuinely outperform negatives
- **Stepwise generation**: Breaks reasoning into discrete steps; needed for fine-grained foresight evaluation; quick check: test different step granularities (sentence vs token chunk)

## Architecture Onboarding

**Component map**: Query -> Stepwise Generation -> Foresight Sampling -> Score Normalization -> Preference Pair Construction -> ACO Loss -> Model Update

**Critical path**: The foresight sampling and ACO loss optimization form the core pipeline. For each query, stepwise generation produces candidate responses, foresight sampling evaluates their quality through simulated future outcomes, and ACO loss updates the model based on these evaluations while accounting for advantage consistency.

**Design tradeoffs**: The framework balances exploration (trying different response paths) with exploitation (focusing on high-scoring paths) through beam size M and rollout count N. Higher values increase computational cost but may find better solutions. The ACO loss trades off between fitting all data versus focusing on high-confidence examples, with temperature β controlling this balance.

**Failure signatures**: Training instability manifests as performance degradation on general benchmarks, indicating catastrophic forgetting. Poor foresight estimation shows as inconsistent advantage values that don't align with actual response quality. The framework may also suffer from local optima if exploration is insufficient.

**First experiments**:
1. Baseline comparison: Run Genius vs SFT on GSM8K with same data and compute budget
2. Hyperparameter sweep: Test different beam sizes (M=1,2,4) and rollout counts (N=2,4,8) on a single benchmark
3. Advantage analysis: Plot advantage distribution during training to verify ACO loss effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Foresight score temperature τ not specified, affecting reproducibility of exact results
- Step definition ambiguity (sentence vs token chunk vs semantic segment) impacts implementation
- Limited evaluation on very long-context reasoning tasks despite increasing importance
- Reliance on specific Magpie/OpenHermes-2.5 datasets may limit generalizability

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core methodology effectiveness for reasoning improvement | High |
| Stability regarding catastrophic forgetting | Medium |
| Generalizability across different unsupervised datasets | Medium |
| Performance on competition-level tasks | Medium |

## Next Checks

1. **Parameter sensitivity analysis**: Systematically evaluate the impact of foresight temperature τ and step definition on performance across 3-4 key reasoning benchmarks to establish robust hyperparameter ranges.

2. **Generalization stress test**: Apply Genius to reasoning tasks from completely different domains (e.g., legal reasoning, scientific hypothesis generation) using alternative unsupervised datasets to validate broad applicability beyond the tested benchmarks.

3. **Long-context evaluation**: Design and execute experiments specifically targeting long-context reasoning scenarios to assess whether the stepwise foresight mechanism scales effectively to extended reasoning chains.