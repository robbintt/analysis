---
ver: rpa2
title: Simplifying Knowledge Transfer in Pretrained Models
arxiv_id: '2510.22208'
source_url: https://arxiv.org/abs/2510.22208
tags:
- knowledge
- teacher
- transfer
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of bidirectional knowledge transfer
  between pretrained models. The core idea is a dynamic data partitioning strategy
  where models autonomously adopt the role of student or teacher based on prediction
  confidence, enabling bidirectional knowledge transfer within a single training stage.
---

# Simplifying Knowledge Transfer in Pretrained Models

## Quick Facts
- **arXiv ID:** 2510.22208
- **Source URL:** https://arxiv.org/abs/2510.22208
- **Reference count:** 18
- **One-line primary result:** Dynamic confidence-based data partitioning enables bidirectional knowledge transfer, improving ViT-B accuracy by ~1.4% and achieving state-of-the-art video saliency prediction.

## Executive Summary
This paper addresses bidirectional knowledge transfer between pretrained models through a novel dynamic data partitioning strategy. The method allows models to autonomously adopt student or teacher roles based on prediction confidence, enabling both models to improve within a single training stage. Tested across image classification, semantic segmentation, and video saliency prediction tasks, the approach demonstrates consistent performance gains. The framework also extends naturally to multidirectional knowledge transfer among multiple models, showing cumulative improvements as more models participate in collaborative learning.

## Method Summary
The core innovation is a dynamic data partitioning strategy where pretrained models autonomously adopt either the role of a student, seeking knowledge, or that of a teacher. For each sample, the method assigns the teacher role to the model with the highest prediction probability for the ground-truth class, enabling bidirectional knowledge transfer in a single training stage. The approach combines distillation loss with task-specific supervision to stabilize training and preserve prior knowledge. The framework extends naturally to multiple models and three distinct vision tasks: image classification, semantic segmentation, and video saliency prediction, with experimental validation on standard benchmarks.

## Key Results
- Improved ViT-B performance by approximately 1.4% through bidirectional knowledge transfer with ViT-T on ImageNet classification.
- Achieved state-of-the-art video saliency prediction results, improving CC from 0.552 to 0.558 and NSS from 3.188 to 3.216 on the DHF1K dataset.
- Demonstrated consistent performance improvements as more models were added to the collaborative learning environment in multidirectional knowledge transfer settings.

## Why This Works (Mechanism)

### Mechanism 1: Confidence-Based Dynamic Role Assignment
Assigning the teacher role to the model with higher prediction confidence on the ground-truth class enables bidirectional knowledge transfer in a single training stage. For each sample, compute softmax probability for ground-truth class for both models and create binary masks where the higher-confidence model becomes the teacher. Models with higher confidence on the correct class possess more reliable knowledge to transfer for that specific instance. Break condition: If both models have uniformly low confidence on ground-truth, the "teacher" may impart noisy signal, though task-specific loss mitigates this.

### Mechanism 2: Complementary Knowledge from Diverse Training Methodologies
Models trained with different objectives/architectures capture uncorrelated errors, enabling each to transfer unique data-specific insights. Pair models with divergent design choices (e.g., self-supervised DINOv2 + supervised ResNet). Each model acts as teacher on samples where it has higher confidence, transferring complementary patterns the other missed. Core assumption: Models with different pretraining regimes generalize differently and make uncorrelated errors. Break condition: If models are too similar, complementary knowledge is limited, though gains still occur.

### Mechanism 3: Task-Specific Loss Anchoring for Stability
Combining distillation loss with task-specific ground-truth supervision preserves prior knowledge and prevents degradation from imperfect teacher signals. Total loss includes task loss (cross-entropy, mask+dice, or CC) alongside distillation loss to anchor models to ground truth while distillation provides complementary signal. Pure distillation can degrade performance when teacher predictions diverge from ground truth. Break condition: Relies on ground-truth labels; cannot operate in fully unsupervised settings.

## Foundational Learning

- **Concept: KL Divergence for Knowledge Distillation**
  - Why needed here: Core loss function measures discrepancy between softened teacher/student output distributions.
  - Quick check question: Can you explain why temperature T > 1 is used before softmax in distillation?

- **Concept: Softmax Confidence and Ground-Truth Alignment**
  - Why needed here: Dynamic partitioning requires extracting σ(z)₍gt₎—the probability assigned to the correct class—to determine teacher role.
  - Quick check question: For a 3-class problem with logits [2.0, 1.0, 0.5], what is the softmax probability for class 0?

- **Concept: Stop-Gradient for Teacher Stability**
  - Why needed here: Distillation loss backpropagates only to student; teacher receives stop-gradient to prevent collapse.
  - Quick check question: Why would updating both models from the same KL loss lead to mode collapse?

## Architecture Onboarding

- **Component map:** Input (X, Y) → two pretrained models f₁, f₂ → compute logits z₁, z₂ → compute masks m₁, m₂ based on confidence → distillation with stop-gradient → task losses → total loss L = L_T1 + L_T2 + L_dist → update both θ₁, θ₂

- **Critical path:** Mask computation determines per-sample role assignment; incorrect implementation breaks bidirectional flow.

- **Design tradeoffs:**
  - Temperature T=1 (no softening) vs T>1: Paper uses T=1; higher T may help with noisy labels but wasn't tested.
  - With vs without data augmentation: Paper disables augmentation except for ViTs (uses timm defaults)—reduces variance but may limit generalization.
  - Two-way vs multi-way: Extends naturally to K models via Equation 11-13, but compute scales as O(K²) for distillation terms.

- **Failure signatures:**
  - Both models degrade: Likely missing task-specific loss or stop-gradient on teacher.
  - Only one model improves: Check mask computation—may be stuck in fixed teacher mode.
  - Instability/oscillation: Learning rate too high (paper uses 1e-6) or T too low causing hard targets.

- **First 3 experiments:**
  1. **Sanity check:** Reproduce ViT-B + ViT-T result on ImageNet subset (10K images). Expect ~1% gain for ViT-B. Verify masks are non-degenerate (both m₁, m₂ have >10% samples).
  2. **Ablation on task loss:** Run with and without cross-entropy component. Hypothesis: removing it causes degradation on Case 3 samples.
  3. **Architecture diversity test:** Pair two same-architecture models (e.g., ViT-S + ViT-S with different random seeds) vs two different architectures (ViT-S + ResMLP). Compare gains to validate complementary-knowledge hypothesis.

## Open Questions the Paper Calls Out

- **Question:** Does Bi-KD-induced task specialization degrade the general-purpose representation capabilities of foundational backbones like DINOv2?
  - **Basis in paper:** [explicit] The authors state in the conclusion that the method "may affect the generalization capabilities of the underlying models," specifically noting concerns for widely adopted architectures like DINOv2.
  - **Why unresolved:** The experiments focus solely on improving performance for the specific task being trained rather than evaluating retention of transfer learning capabilities on downstream tasks.
  - **Evidence:** Zero-shot or linear probe evaluations on diverse downstream datasets (e.g., VTAB) for models before and after the Bi-KD process.

- **Question:** Can the stability of Bi-KD be preserved in fully unsupervised settings without relying on ground-truth labels for the task-specific loss?
  - **Basis in paper:** [explicit] The conclusion lists the reliance on ground-truth labels as a limitation that "restricts its use in fully unsupervised settings."
  - **Why unresolved:** The method relies on a task-specific loss ($L_T$), such as cross-entropy, to stabilize training and mitigate instances where the "teacher" provides incorrect guidance (Case 3 errors).
  - **Evidence:** Experiments replacing the ground-truth dependent loss with consistency regularization or pseudo-labeling schemes that achieve comparable performance without label access.

- **Question:** Does the feature alignment produced by Bi-KD enhance the efficacy of model merging techniques?
  - **Basis in paper:** [explicit] The authors identify "exploring model merging to consolidate the strengths of multiple models into a single, higher-performing system" as a direction for future work.
  - **Why unresolved:** While the paper demonstrates that Bi-KD aligns feature representations (evidenced by CCA analysis), it does not test if this alignment facilitates merging parameters into one model.
  - **Evidence:** Comparative analysis of weight averaging or other model merging techniques applied to Bi-KD models versus independently trained models.

## Limitations

- Method's applicability to unsupervised settings is explicitly acknowledged as a limitation, as it requires ground-truth labels for confidence-based role assignment.
- Complementary knowledge hypothesis lacks direct corpus validation - the cited Gontijo-Lopes et al. (2022) work is referenced but not independently verified within this study.
- Confidence-based partitioning mechanism may struggle when both models have uniformly low confidence on ground-truth (Case 3), though task-specific loss anchoring provides some mitigation.

## Confidence

- **High Confidence:** ViT-B performance improvement (~1.4% accuracy gain) and video saliency state-of-the-art results (CC 0.552→0.558, NSS 3.188→3.216) are well-supported by experimental results presented in the paper.
- **Medium Confidence:** The complementary knowledge hypothesis across different training methodologies is supported by empirical results but lacks direct theoretical validation or corpus evidence.
- **Medium Confidence:** Task-specific loss anchoring for stability is demonstrated empirically but the specific mechanism of how it prevents degradation in Case 3 scenarios could benefit from deeper analysis.

## Next Checks

1. **Architecture Diversity Test:** Compare gains between same-architecture model pairs (e.g., ViT-S + ViT-S with different random seeds) versus different architecture pairs (e.g., ViT-S + ResMLP) to validate the complementary knowledge hypothesis.

2. **Ablation on Task Loss:** Run experiments with and without the cross-entropy component to quantify the stabilizing effect of task-specific losses, particularly for samples where both models struggle (Case 3).

3. **Confidence Distribution Analysis:** Examine the distribution of confidence scores across samples to determine how frequently models fall into Case 3 scenarios and quantify the impact on overall performance.