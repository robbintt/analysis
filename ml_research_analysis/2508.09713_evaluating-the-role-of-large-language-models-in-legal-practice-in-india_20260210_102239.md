---
ver: rpa2
title: Evaluating the Role of Large Language Models in Legal Practice in India
arxiv_id: '2508.09713'
source_url: https://arxiv.org/abs/2508.09713
tags:
- legal
- human
- llms
- tasks
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the performance of large language models
  (LLMs) in Indian legal tasks. Using a survey experiment with 50 advanced law students,
  outputs from five LLMs (GPT-4, Claude 3, ChatGPT 3.5, Gemini, Llama 2) and a junior
  lawyer were compared across five tasks: issue spotting, legal drafting, advice,
  research, and reasoning.'
---

# Evaluating the Role of Large Language Models in Legal Practice in India

## Quick Facts
- arXiv ID: 2508.09713
- Source URL: https://arxiv.org/abs/2508.09713
- Reference count: 6
- Key outcome: LLMs generally perform as well as or better than a junior lawyer in Indian legal tasks like drafting and reasoning, but struggle significantly with legal research due to hallucinations.

## Executive Summary
This study evaluates large language models (LLMs) in Indian legal practice by comparing outputs from five LLMs and a junior lawyer across five tasks: issue spotting, legal drafting, advice, research, and reasoning. Using 50 advanced law students as evaluators, the research found that LLMs excel in drafting and reasoning tasks but struggle with specialized legal research due to hallucinations. Claude 3 and GPT-4 were top performers, while Llama 2 performed poorly. The study concludes that while LLMs can augment certain legal tasks, human expertise remains essential for nuanced reasoning and accurate legal research.

## Method Summary
The study used a mixed-methods survey experiment comparing LLM outputs to a junior lawyer across five legal tasks using a consumer law scenario. Five LLMs (GPT-4, Claude 3, ChatGPT 3.5, Gemini, Llama 2) and one junior lawyer with one year of experience were evaluated by 50 advanced law students using three 5-point Likert scales for helpfulness, accuracy, and comprehensiveness. The junior lawyer had access to law library resources and 6 hours per task, while LLMs used carefully engineered one-shot prompts. Outputs were anonymized and randomized for evaluation.

## Key Results
- LLMs outperformed the junior lawyer in drafting tasks and were comparable in reasoning and advice tasks
- All LLMs struggled significantly with legal research, generating hallucinations and fabricating case citations
- Claude 3 and GPT-4 were top performers, while Llama 2 performed poorly
- Students occasionally confused verbosity with quality, potentially inflating LLM scores for advice tasks

## Why This Works (Mechanism)

### Mechanism 1
LLMs outperform humans in structured generation tasks (drafting, issue spotting) but fail at precise information retrieval due to probabilistic text generation rather than database lookup. Models like GPT-4 and Claude 3 optimize for semantic coherence and syntactic structure, producing fluent outputs for drafting tasks. However, when tasked with legal research requiring specific case citations, the probabilistic mechanism generates plausible tokens rather than verifying against ground truth, leading to hallucinations.

### Mechanism 2
Performance degradation in the Indian legal context is driven by a lack of localized training data and the dominance of Western (US/EU) legal corpora in pre-training. General-purpose LLMs are trained on vast datasets skewed toward jurisdictions with high digital transparency. When prompted with tasks requiring specific Indian statutes or case law (which are less digitized/public), the model reverts to patterns from dominant jurisdictions or fabricates to fill probability gaps.

### Mechanism 3
High evaluation scores for LLMs in advice and reasoning are partly driven by "verbiage" and structural clarity, which students confuse with depth, whereas humans prioritize conciseness and strategic restraint. LLMs generate exhaustive, well-structured responses that students rated highly, while human experts produce concise, strategic outputs that may be judged more harshly due to a mismatch in expected verbosity.

## Foundational Learning

- **Concept: Legal Hallucinations**
  - Why needed here: The paper's central warning is that LLMs fabricate case law (e.g., "Neelkanth Venkatesh") with high confidence. Understanding why this happens (probabilistic next-token prediction vs. retrieval) is essential before deployment.
  - Quick check question: Can an LLM distinguish between a real citation it "knows" and a plausible citation it "invents"? (Answer: Generally no, without external tools).

- **Concept: Prompt Engineering & Context Window**
  - Why needed here: The study utilized "careful prompt engineering" and specific templates to get usable results. Performance is highly contingent on how the task is framed.
  - Quick check question: If I provide the raw text of a specific Indian statute in the prompt, does that eliminate the training data gap for that specific task?

- **Concept: The "Junior Lawyer" Baseline**
  - Why needed here: The study compares AI to a human with "one year of experience." This sets a specific capability ceiling. The AI is not replacing a senior partner; it is competing with entry-level labor for specific rote tasks.
  - Quick check question: Does the AI perform better than a junior on speed and drafting, but worse on strategic research?

## Architecture Onboarding

- **Component map:** User query + Context -> Router (Drafting vs. Research task) -> General-purpose LLM (e.g., GPT-4/Claude) or Fine-tuned/Open-source (Llama 2 performed poorly) -> Verification Layer (mandatory for Research/Citations) -> Draft / Advice / Summary

- **Critical path:**
  1. Drafting/Issue Spotting: LLM Generation -> Human Review (Low latency)
  2. Legal Research: LLM Generation -> *Hallucination Check* -> Human Review (Do not deploy without this check)

- **Design tradeoffs:**
  - General vs. Specialized: The paper shows general models (GPT-4) outperformed the open-source model (Llama 2) significantly. Tradeoff: Cost/Privacy (Llama) vs. Performance/Fluency (GPT-4)
  - Verbosity vs. Precision: Models optimize for helpfulness (often verbosity), which students rated highly. Tradeoff: Client clarity vs. perceived thoroughness

- **Failure signatures:**
  - Fabricated Citations: "Case names" that sound Indian but do not exist (e.g., hallucinated NCDRC cases)
  - Generic/Western Logic: Applying US/EU consumer protection principles to Indian specific statutes
  - Formulaic Empathy: Overly structured "I understand your concern" blocks in advice tasks

- **First 3 experiments:**
  1. Citation Verification Loop: Run the "Legal Research" task with GPT-4, then automatically feed the generated citations back into a search engine to quantify the hallucination rate
  2. Drafting Template Test: Provide the exact Consumer Protection Act template used in the paper to a local model (e.g., Llama 3) vs. GPT-4 to measure the "structure gap" in drafting tasks
  3. Context Injection: Rerun the "Issue Spotting" task by prepending the full text of the specific Indian statute to the prompt to see if "Accuracy" scores improve for the Human or AI

## Open Questions the Paper Calls Out

### Open Question 1
How does LLM performance compare to human legal experts with varying levels of seniority (e.g., senior associates vs. partners) rather than just junior lawyers? The author notes that relying on a "single junior lawyer" as a baseline is a limitation and suggests that comparing with additional human outputs of different types may help grade the level of human expertise that AI reaches.

### Open Question 2
Do evaluation scores change significantly when outputs are assessed by practicing lawyers rather than advanced law students? The author identifies the "convenience sample of 50 law students" as a limitation and calls for further empirical research with larger and more diverse sample pools.

### Open Question 3
To what extent do algorithmic bias and data privacy concerns impede the deployment of LLMs in high-stakes Indian legal domains like family law or criminal defense? The author states that the study did not touch on the fact that concerns about accountability, data privacy, and algorithmic bias will also have to be addressed.

## Limitations
- Small sample size (50 law students) and narrow focus on one specific consumer law scenario limits generalizability
- Evaluation conducted in April 2024 using specific model versions; performance may change with newer iterations
- "Junior lawyer" baseline represents entry-level expertise rather than experienced practitioners, potentially underestimating human capabilities
- Study doesn't account for potential improvements from retrieval-augmented generation systems or specialized legal AI tools

## Confidence

- **High confidence:** LLMs perform well on drafting and reasoning tasks but struggle significantly with legal research due to hallucinations
- **Medium confidence:** Performance differences are driven by training data bias toward Western legal systems
- **Medium confidence:** Evaluator bias toward verbose AI outputs may inflate helpfulness scores

## Next Checks

1. Conduct hallucination rate quantification by automatically verifying all case citations generated in legal research tasks against Indian legal databases, expecting near-100% hallucination rate for Indian case law
2. Test whether providing full text of Indian statutes in prompts improves "accuracy" scores, distinguishing between reasoning failures and knowledge retrieval limitations
3. Replicate the study with experienced legal practitioners rather than law students to validate whether verbosity bias affects expert evaluations of legal advice quality