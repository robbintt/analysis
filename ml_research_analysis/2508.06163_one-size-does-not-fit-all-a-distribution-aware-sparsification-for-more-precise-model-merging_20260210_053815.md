---
ver: rpa2
title: 'One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise
  Model Merging'
arxiv_id: '2508.06163'
source_url: https://arxiv.org/abs/2508.06163
tags:
- tadrop
- merging
- task
- tasks
- sparsification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TADrop addresses the problem of parameter heterogeneity in model
  merging by introducing a tensor-wise adaptive sparsification strategy. Instead of
  applying a uniform sparsity ratio, TADrop assigns each parameter tensor a tailored
  drop rate based on its distributional properties, using a quantile ratio metric.
---

# One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging

## Quick Facts
- **arXiv ID:** 2508.06163
- **Source URL:** https://arxiv.org/abs/2508.06163
- **Reference count:** 40
- **Primary result:** TADrop improves model merging by 2.0% average accuracy on 8 ViT tasks through tensor-wise adaptive sparsification

## Executive Summary
TADrop addresses the problem of parameter heterogeneity in model merging by introducing a tensor-wise adaptive sparsification strategy. Instead of applying a uniform sparsity ratio, TADrop assigns each parameter tensor a tailored drop rate based on its distributional properties, using a quantile ratio metric. This approach preserves critical sparse parameters while aggressively pruning redundant dense ones. When integrated with state-of-the-art merging methods, TADrop consistently improves performance across vision, language, and multimodal tasks.

## Method Summary
TADrop computes a quantile ratio for each parameter tensor (ratio of 50th to 95th percentile) to determine drop rates - low ratios for sparse distributions (conservative pruning) and high ratios for dense distributions (aggressive pruning). After applying the adaptive mask, the remaining parameters are rescaled to preserve the original L2 norm. This distribution-aware approach operates at the tensor level, capturing intra-task heterogeneity that global methods miss. The method is plug-and-play, requiring no data and integrating seamlessly with existing merging algorithms.

## Key Results
- Achieves 2.0% average accuracy gain across 8 ViT-B/32 tasks when enhancing a leading merging method
- Maintains and widens performance advantages as task count increases from 8 to 30
- Demonstrates consistent improvements across vision, language, and multimodal tasks
- Outperforms uniform sparsification by better preserving critical parameters while removing redundancy

## Why This Works (Mechanism)

### Mechanism 1: Distribution-Aware Sparsity Allocation
The algorithm computes a "Quantile Ratio" ($d_k^i = Q_a / Q_b$) for each parameter tensor. If the distribution is heavy-tailed (indicating sparse, high-magnitude critical parameters), the ratio is small, resulting in a conservative drop rate. If the distribution is dense (redundant), the ratio is higher, triggering aggressive pruning. This addresses the core problem that uniform sparsification inadvertently prunes critical parameters in sparse tensors while retaining redundancy in dense ones.

### Mechanism 2: Norm-Preserving Rescaling
Magnitude-based pruning arbitrarily reduces the L2 norm of task vectors, distorting their contribution during aggregation. TADrop scales the remaining non-zero values by the ratio of the original tensor norm to the new sparse tensor norm, restoring the original "energy" of the vector. This is critical - without rescaling, accuracy drops significantly below baseline.

### Mechanism 3: Intra-Task Heterogeneity Exploitation
By operating at the tensor level rather than model or layer level, TADrop dynamically allocates different sparsity budgets to different modules. It recognizes that fine-tuning modifies different architectural components in statistically distinct ways, creating a "fingerprint" of redundancy. This fine-grained approach captures heterogeneity between Attention vs FFN modules that global methods miss.

## Foundational Learning

- **Concept:** Task Vectors
  - Why needed here: TADrop is a pre-processing module specifically for the "Task Arithmetic" paradigm. You must understand that a task vector $\tau = \theta_{fine} - \theta_{pre}$ represents the "knowledge delta" being sparsified.
  - Quick check question: How does TADrop interact with the pretrained weights $\theta_{pre}$ during the actual merge? (Answer: It doesn't; it only modifies the delta $\tau$ before merging).

- **Concept:** Quantiles & Heavy-Tailed Distributions
  - Why needed here: The core metric (Quantile Ratio) relies on detecting "heavy tails." You need to distinguish between a Gaussian-like distribution (dense, high ratio) and a sparse distribution with outliers (heavy tail, low ratio).
  - Quick check question: If a tensor has a Quantile Ratio close to 1, what does that imply about its sparsity and how TADrop will treat it? (Answer: It implies a dense/redundant distribution, leading to aggressive pruning).

- **Concept:** Parameter Interference
  - Why needed here: The paper frames itself as solving the "suboptimal trade-off" of global sparsification. Understanding that merging conflicting parameters degrades performance explains *why* we need precise sparsification.
  - Quick check question: Why does applying a uniform 80% drop rate to two tensors with different distributions cause interference? (Answer: It may destroy unique, low-magnitude critical signals in the sparse tensor while failing to remove noise in the dense tensor).

## Architecture Onboarding

- **Component map:** Input (Batch of Task Vectors) -> Metric Engine (Compute Quantiles) -> Mask Generator (Calculate Drop Rates) -> Rescaler (Preserve L2 Norm) -> Output (Purified Task Vectors)
- **Critical path:** The mask generation step is the bottleneck for precision. The choice of quantiles ($a=0.50, b=0.95$) is hardcoded in the official implementation and verified by ablation.
- **Design tradeoffs:**
  - Fixed vs. Learnable Quantiles: Uses fixed quantiles for "plug-and-play" simplicity vs. learnable approach that might squeeze more performance
  - Tensor-level vs. Layer-level: TADrop is granular (Tensor-level) vs. computationally cheaper layer-level scaling
- **Failure signatures:**
  - Performance Collapse: If the Rescaler is omitted, accuracy drops significantly below baseline
  - No Gain: If the model is very small or poorly fine-tuned (no heterogeneity), TADrop offers marginal gains
  - NaN values: Missing or too-small epsilon term in quantile ratio or rescaling step
- **First 3 experiments:**
  1. **Sanity Check (Visual):** Plot histograms of parameter distributions for a specific layer from a fine-tuned ViT to visually confirm the "heterogeneity" premise
  2. **Ablation (Rescaling):** Run TADrop on 8-task ViT-B/32 with and without the L2 norm rescaling step to verify the collapse
  3. **Integration (EMR-Merging):** Apply TADrop as a pre-processor to EMR-Merging on the 8-task benchmark and verify if the average accuracy lifts from ~88.7% to ~90.7%

## Open Questions the Paper Calls Out
- Can the tensor-wise adaptive strategy be effectively generalized to Convolutional Neural Networks (CNNs) or diffusion models, which possess distinct spatial inductive biases compared to the Transformers tested?
- Is there a formal theoretical relationship between the quantile ratio metric and established measures of parameter importance, such as the Fisher Information Matrix?
- Can the quantile hyperparameters ($a=0.50$, $b=0.95$) be dynamically optimized per-task or per-layer to yield superior results, rather than using fixed global constants?

## Limitations
- The core assumption that quantile ratios effectively capture parameter redundancy is validated empirically but lacks theoretical grounding
- Performance gain depends on models exhibiting sufficient heterogeneity between and within tensors - homogeneous models may see minimal benefit
- The computational overhead of tensor-wise processing scales with parameter count, though the paper doesn't analyze this trade-off quantitatively

## Confidence
- **High Confidence:** The empirical improvements over baselines (+2.0% average accuracy on ViT-B/32, consistent gains on 30-task experiments) are well-documented and reproducible through ablation studies
- **Medium Confidence:** The distributional assumptions about parameter tensors are reasonable but not rigorously proven; the method works well empirically without strong theoretical guarantees
- **Low Confidence:** Claims about scalability and performance on "hundreds of tasks" extend beyond the tested range (8-30 tasks) and remain speculative

## Next Checks
1. Test TADrop on a homogeneous model (where all layers exhibit similar distributions) to verify if it degrades to baseline performance or offers no advantage
2. Measure computational overhead: time and memory costs of tensor-wise quantile computation versus layer-wise or global approaches across different model sizes
3. Validate transfer capability: apply TADrop to merge models with different architectures (e.g., CLIP ViT with ResNet) to test robustness beyond the controlled experimental setting