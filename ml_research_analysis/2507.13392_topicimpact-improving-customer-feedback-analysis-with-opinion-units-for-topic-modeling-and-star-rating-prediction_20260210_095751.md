---
ver: rpa2
title: 'TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic
  Modeling and Star-Rating Prediction'
arxiv_id: '2507.13392'
source_url: https://arxiv.org/abs/2507.13392
tags:
- topic
- sentiment
- opinion
- units
- topics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TopicImpact improves customer feedback analysis by preprocessing
  reviews into opinion units using LLMs, then clustering these units via topic modeling.
  This approach generates coherent, interpretable topics linked to sentiment scores
  and business metrics like star ratings.
---

# TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction

## Quick Facts
- **arXiv ID:** 2507.13392
- **Source URL:** https://arxiv.org/abs/2507.13392
- **Reference count:** 22
- **Primary result:** TopicImpact achieves high topic coherence (precision 86-92%) and strong star-rating prediction (R² up to 0.726) by preprocessing reviews into opinion units and clustering them via topic modeling.

## Executive Summary
TopicImpact improves customer feedback analysis by first using an LLM to extract "opinion units"—atomic aspect-sentiment pairs—from reviews, then clustering these units via topic modeling. This approach generates coherent, interpretable topics linked to sentiment scores and business metrics like star ratings. Experiments on restaurant reviews show that sentiment-aware embeddings or sentiment-split clustering (Method 3) achieve the best results, enabling businesses to identify key topics, their sentiment, and impact on ratings.

## Method Summary
TopicImpact processes customer reviews through a pipeline: (1) GPT-4 extracts opinion units (label, excerpt, sentiment 1-10) from raw text, filtering out overall experience comments; (2) opinion units are embedded using either general (all-mpnet-base-v2) or sentiment-aware (SentiCSE) models; (3) BERTopic clusters the embeddings with HDBSCAN, producing K=20 topics; (4) a multiple linear regression model predicts star ratings from topic frequencies and sentiment scores. Method 3 splits data by sentiment before clustering to separately model positive and negative topic impacts.

## Key Results
- Topic coherence (precision) reaches 86-92% with general embeddings; sentiment-aware embeddings yield lower precision (82%) but better sentiment capture
- Sentiment-split clustering (Method 3) achieves the highest star-rating prediction accuracy (R²=0.726)
- MLR coefficients quantify each topic's impact on ratings, with "Food safety" showing a large negative coefficient (-1.21)

## Why This Works (Mechanism)

### Mechanism 1: Semantic Isolation via Opinion Unit Segmentation
Restructuring the pipeline to operate on "opinion units" rather than full reviews increases topic coherence and interpretability. Reviews often address multiple aspects; clustering entire reviews dilutes semantic signals. By extracting atomic opinion units, the system isolates specific semantic concepts, aligning vector proximity with specific aspects rather than general document themes.

### Mechanism 2: Sentiment-Conditioned Clustering for Impact Prediction
Separating opinion units into positive and negative datasets before clustering improves star-rating prediction. General embeddings may cluster "delicious" and "disgusting" closely due to shared topical context. Sentiment-split clustering (Method 3) forces distinct clusters for positive and negative sentiments, allowing regression to assign independent weights to each, capturing the asymmetry where negative experiences impact ratings more heavily.

### Mechanism 3: Proxying Business Value via Multiple Linear Regression (MLR)
Mapping topic frequency and sentiment intensity to star ratings via MLR provides a proxy for business impact. The system aggregates opinion units back to review level, applies MLR, and calculates β coefficients. A large negative β for "Food Safety" quantifies how much that topic drags down the rating, converting unsupervised clusters into interpretable business metrics.

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA)**
  - **Why needed here:** TopicImpact is essentially a pipeline to solve ABSA at scale. You cannot understand the "opinion unit" extraction without understanding that the goal is to separate the *target* (the food) from the *expression* (delicious).
  - **Quick check question:** If a review says "The steak was tough but the sauce was divine," how many opinion units should the LLM theoretically extract?

- **Concept: Density-Based Clustering (HDBSCAN)**
  - **Why needed here:** The paper uses BERTopic, which relies on HDBSCAN. Unlike K-Means, HDBSCAN does not force every point into a cluster. This explains why the paper reports "% Outliers" (Table 1) and argues that noise removal is a feature, not a bug.
  - **Quick check question:** Why is allowing "outliers" (noise points) beneficial when clustering messy customer feedback data?

- **Concept: Embedding Spaces & Vector Proximity**
  - **Why needed here:** The core tradeoff explored is between "General Embeddings" (semantic similarity) and "Sentiment-Aware Embeddings." You must understand that embeddings map words to coordinates, and "closer" coordinates imply similarity.
  - **Quick check question:** Why would a general embedding model place "tasty" and "tasteless" close together in vector space?

## Architecture Onboarding

- **Component map:** Preprocessor (LLM) -> Encoder (Embedder) -> Clusterer (BERTopic) -> Aggregator -> Predictor (MLR)
- **Critical path:** The LLM Preprocessing is the bottleneck. If the LLM prompt fails to normalize labels (e.g., outputs "Waiter" vs "Staff"), the clustering step will fragment, and the regression will be noisy.
- **Design tradeoffs:** Method 1 (General Embedding) offers high Topic Precision but poor Star Prediction. Method 3 (Sentiment Splitting) offers high Star Prediction (R² 0.72) but requires running the clustering pipeline twice and managing twice the number of topics. The paper chooses MLR over Neural Networks for interpretable β coefficients.
- **Failure signatures:**
  - Topic Fragmentation: Seeing 5 different clusters for "Service" (e.g., "Waiter," "Staff," "Manager," "Host," "Server"). *Fix:* Adjust `min_topic_size` or improve LLM prompt to normalize labels.
  - Overall Experience Leakage: Clusters filled with generic quotes like "Great place." *Fix:* Ensure the LLM prompt rigorously filters/excludes "overall experience" tags.
- **First 3 experiments:**
  1. **Reproduce the Sentiment Split (Method 3):** Take a dataset of 100 reviews. Run the pipeline once on the whole set, and once on a sentiment-split set. Compare the R² scores to verify the ~0.72 claim.
  2. **Outlier Analysis:** Inspect the documents marked as "outliers" by HDBSCAN. Are they nonsense, or are they niche topics that deserve their own cluster? This validates the density-based assumption.
  3. **Prompt Sensitivity Test:** Change the LLM prompt to forbid specific generic words (e.g., "nice", "good") in the labels. Observe if Topic Precision improves.

## Open Questions the Paper Calls Out

- How does TopicImpact perform when applied to non-restaurant domains such as retail product reviews, employee surveys, or course evaluations?
- Can keyword or example-guided topic seeding be effectively integrated into the framework to control topic granularity without sacrificing the benefits of exploratory analysis?
- Do the trade-offs between general-purpose and sentiment-aware embeddings generalize to a wider variety of models?
- How can the lack of standardized benchmarks for aspect-based sentiment analysis be addressed to better evaluate topic modeling on full-length reviews?

## Limitations
- LLM preprocessing errors and hallucinations can propagate downstream, potentially fragmenting topics or introducing noise
- The methodology requires manual review of extracted opinion units for quality control
- No standardized benchmarks exist for comparing opinion unit-based topic modeling approaches

## Confidence
- Topic coherence improvements from opinion unit segmentation: High
- Sentiment-split clustering improving star-rating prediction: High
- MLR as effective proxy for business impact: Medium
- Generalizability to other domains: Low

## Next Checks
1. Test opinion unit extraction quality by manually reviewing 100 randomly selected units for accuracy and consistency
2. Validate the robustness of Method 3 by running the pipeline with different sentiment thresholds (e.g., ≤4 vs >6)
3. Assess topic stability by measuring coherence scores when varying the number of topics (K=15 vs K=25)