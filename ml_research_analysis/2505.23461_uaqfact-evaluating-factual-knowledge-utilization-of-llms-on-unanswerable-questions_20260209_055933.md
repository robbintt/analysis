---
ver: rpa2
title: 'UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable
  Questions'
arxiv_id: '2505.23461'
source_url: https://arxiv.org/abs/2505.23461
tags:
- knowledge
- llms
- factual
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UAQFact, a bilingual dataset for evaluating
  LLMs on unanswerable questions with factual knowledge support. The dataset includes
  13,970 questions (6,985 unanswerable and 6,985 answerable) across three question
  types, each paired with auxiliary factual knowledge from Wikidata.
---

# UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions

## Quick Facts
- **arXiv ID**: 2505.23461
- **Source URL**: https://arxiv.org/abs/2505.23461
- **Reference count**: 40
- **Primary result**: LLMs struggle to consistently refuse unanswerable questions even when possessing relevant factual knowledge

## Executive Summary
This paper introduces UAQFact, a bilingual dataset for evaluating LLMs on unanswerable questions with factual knowledge support. The dataset includes 13,970 questions (6,985 unanswerable and 6,985 answerable) across three question types, each paired with auxiliary factual knowledge from Wikidata. The authors define three evaluation tasks: discriminating between answerable and unanswerable questions, assessing internal knowledge utilization, and evaluating external knowledge integration. Experiments on multiple LLM series show that UAQFact is challenging—LLMs struggle to consistently refuse unanswerable questions even when they possess relevant factual knowledge. Incorporating external knowledge improves performance but does not fully resolve the difficulty. The results highlight limitations in LLMs' ability to utilize stored and external factual knowledge for handling unanswerable questions.

## Method Summary
UAQFact is constructed by sampling Wikidata triples to generate three types of questions (intersection, time, dilemma) that are either answerable or unanswerable. The evaluation framework includes three tasks: (1) direct discrimination between UAQ and ABQ, (2) internal knowledge probing via multiple-choice questions, and (3) external knowledge integration via chain-of-thought reasoning clues. Metrics include refusal rates (Rua for UAQ, Rab for ABQ), knowledge pass rates (KPR), and knowledge-aware refusal rates (KRR). The dataset is evaluated using lm-evaluation-harness framework with temperature=0 for open-sourced models and API calls for black-box models.

## Key Results
- LLMs show high knowledge pass rates (68-83%) on probing questions but low knowledge-aware refusal rates, revealing a storage-utilization gap
- External knowledge via CoT improves discrimination (RΔ) by up to 53.65% for Llama3, but models still cannot make full use of the knowledge
- Chinese questions pose greater challenges than English, with Qwen2.5-7B-Instruct achieving 46.53% KRR in Chinese vs 51.91% in English
- GPT-4 achieves highest KRR (66.45%) but still fails to refuse 33.55% of unanswerable questions it has knowledge about

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Storage-Utilization Gap
- Claim: LLMs store factual knowledge in parameters but cannot reliably retrieve and apply it for unanswerable question discrimination.
- Mechanism: High knowledge pass rate (KPR 68-83%) on probing questions demonstrates storage; low knowledge-aware refusal rate (KRR) reveals activation failure when the same knowledge is needed for UAQ handling.
- Core assumption: Multiple-choice probing accurately reflects internal knowledge state; refusal requires explicit knowledge retrieval pathways not activated by question-alone prompting.
- Evidence anchors: [abstract] "LLMs do not consistently perform well even when they have factual knowledge stored"; [section 4.3] "the high KPR indicates that LLMs have stored extensive knowledge... the comparatively low KRR reveals limitations in their ability to effectively utilize internal knowledge"

### Mechanism 2: External Knowledge via Chain-of-Thought Scaffolding
- Claim: Providing reasoning clues with factual knowledge improves discrimination by offloading retrieval and supplying explicit inference steps.
- Mechanism: CoT prompts decompose questions into sub-questions, provide answers to sub-components, and specify combination rules. This bypasses internal retrieval failures and makes inference path explicit.
- Core assumption: LLMs can follow provided reasoning steps even when they cannot generate them; the bottleneck is retrieval/generation, not comprehension.
- Evidence anchors: [abstract] "incorporating external knowledge may enhance performance, but LLMs still cannot make full use of the knowledge"; [section 4.4] "With the help of CoT... all LLMs demonstrate improved performance... Llama3 exhibited the most remarkable enhancement... achieving the highest R∆ with an improvement of 53.65"

### Mechanism 3: Cross-Lingual Knowledge Asymmetry
- Claim: Factual knowledge is language-specific in accessibility; Chinese inputs show lower discrimination performance even when knowledge is stored.
- Mechanism: Training data composition and language-specific fine-tuning create uneven knowledge representations. The same entity-relation knowledge may be retrievable in English but not activated by Chinese prompts.
- Core assumption: Knowledge is stored with language-tagged access pathways; cross-lingual transfer is incomplete for specialized factual domains.
- Evidence anchors: [section 4.2] "Chinese questions pose greater challenges for LLMs in discriminating between UAQ and ABQ... black-box LLMs... decline in KRR when processing Chinese inputs"

## Foundational Learning

- Concept: **Unanswerable Questions (UAQ) vs. Hallucinations**
  - Why needed here: Core evaluation target; UAQs have no factual answer (empty intersection, violated constraints, false dilemmas) whereas hallucinations are confidently wrong answers. The dataset distinguishes legitimate refusal from knowledge failure.
  - Quick check question: Given "Who is both editor of Enneads and cast member in The Sixth Sense?", if an LLM answers "Porphyry", is this a UAQ failure or a hallucination?

- Concept: **Knowledge Graph Triple Sampling**
  - Why needed here: Dataset construction relies on Wikidata (entity, property, value) triples to generate questions with provable ground truth. Understanding SPARQL queries and property selection criteria is essential for data validation.
  - Quick check question: If a property appears <5 times in Wikidata, why is it excluded from UAQFact?

- Concept: **Refusal Rate Metrics (Rua, Rab, R∆)**
  - Why needed here: Primary evaluation framework. Rua = refusal rate for unanswerable; Rab = refusal rate for answerable; R∆ = discrimination capability. Ideal: Rua→1, Rab→0, R∆→1.
  - Quick check question: A model refuses 85% of UAQs and 42% of ABQs. What is R∆ and what does it indicate about calibration?

## Architecture Onboarding

- Component map: Question Type Definition (Inter/Time/Dilemma) -> Factual Triple Sampling from Wikidata -> Template Generation (GPT-3.5 + human validation) -> Template Filling

- Critical path:
  1. Verify Wikidata property selection (724 properties meeting comprehension/frequency criteria)
  2. Validate question templates against manual inspection standard (99.2% pass rate achieved)
  3. Confirm lexical refusal detection alignment with human judgment (Cohen's Kappa 94.90)
  4. Run baseline Task 1 on target LLM before proceeding to Task 2/3 comparisons

- Design tradeoffs:
  - **Triple form vs. CoT form for external knowledge**: CoT yields higher R∆; triple form yields higher ABQ accuracy. Paper selects CoT for UAQ focus.
  - **Lexical matching vs. semantic detection for refusal**: Lexical is reproducible but may miss paraphrased refusals; human validation shows strong correlation but not perfect.
  - **Multiple-choice vs. generation for knowledge probing**: Multiple-choice enables automated evaluation but may overestimate knowledge via guessing.

- Failure signatures:
  - **High KPR + Low KRR**: Model has knowledge but cannot activate it for UAQ discrimination (signature: most black-box LLMs in Chinese)
  - **Negative R∆**: Model refuses answerable questions more than unanswerable (signature: Qwen2.5-0.5B in English)
  - **External knowledge backfire**: Model abandons correct refusal when given CoT, attempts wrong answer (signature: GPT-4 case in Appendix G.2.2)

- First 3 experiments:
  1. **Baseline Task 1 evaluation**: Run English and Chinese questions on target model, compute Rua/Rab/R∆/Acc. If R∆ < 20, model lacks basic discrimination; proceed only after investigating refusal calibration.
  2. **Knowledge probing correlation**: On subset where Task 1 failed (wrong refusal/answer), run Task 2 probes. If KPR > 70% on failed cases, diagnose as utilization failure; if KPR < 40%, diagnose as knowledge gap.
  3. **CoT format sensitivity test**: Compare Task 3 performance using paper's CoT format vs. triple-only format vs. paraphrased CoT. If R∆ variance > 15 points across formats, model is prompt-sensitive rather than genuinely utilizing knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can specific training or architectural methods be developed to enhance LLMs' ability to utilize internal factual knowledge for identifying unanswerable questions?
- Basis in paper: [explicit] The Conclusion states future research should prioritize "developing methods to enhance the utilization of LLMs’ internal knowledge" and bridge the gap between storage and utilization.
- Why unresolved: Experiments (Task 2) revealed a disconnect where LLMs possess high Knowledge Pass Rates (KPR) but fail to apply this knowledge to refuse unanswerable questions, resulting in low Knowledge-aware Refusal Rates (KRR).
- What evidence would resolve it: A fine-tuning method or prompting strategy that significantly increases KRR scores without requiring external knowledge input.

### Open Question 2
- Question: What mechanisms can enable LLMs to fully integrate and leverage external, verified factual knowledge (such as CoT reasoning clues) to correctly handle unanswerable questions?
- Basis in paper: [explicit] Section 4.4 notes that while external knowledge helps, "LLMs still cannot make full use of the knowledge," identifying this as an interesting topic for future work.
- Why unresolved: Even when provided with explicit reasoning clues (Task 3), models did not achieve perfect refusal rates, indicating they struggle to process and apply external context to the specific task of refusal.
- What evidence would resolve it: An evaluation on UAQFact Task 3 showing that providing external knowledge results in near-perfect discrimination between answerable and unanswerable questions.

### Open Question 3
- Question: What automated evaluation metric can surpass lexical matching to more accurately measure refusal rates in LLMs handling unanswerable questions?
- Basis in paper: [explicit] The Limitations section states, "it is necessary to develop a more precise automated evaluation method in the future" due to potential discrepancies between lexical matching and human judgment.
- Why unresolved: The current reliance on keyword identification (lexical matching) for refusal metrics, while validated, is noted as potentially imperfect compared to human evaluation.
- What evidence would resolve it: A new evaluation framework that demonstrates higher correlation with human annotators than the current lexical matching baseline on the UAQFact dataset.

## Limitations
- The lexical matching approach for refusal detection may miss semantically equivalent refusals despite high Cohen's Kappa (94.90) with human judgment
- Knowledge probing mechanism assumes multiple-choice accuracy correlates with internal knowledge state, potentially overestimating true knowledge storage
- Cross-lingual asymmetry findings are based on limited model comparisons without systematic ablation of language-specific training factors

## Confidence
- **High**: Dataset construction methodology and basic Task 1 evaluation metrics (Rua, Rab, RΔ, Acc)
- **Medium**: Knowledge probing correlation (KPR → internal knowledge inference) and cross-lingual asymmetry claims
- **Low**: External knowledge utilization improvements (Task 3 RΔ gains) and the specific mechanism of CoT scaffolding effectiveness

## Next Checks
1. **Refusal detection validation**: Sample 100 Task 1 outputs and manually classify refusals; compare against lexical matching accuracy to quantify missed refusals.
2. **Knowledge probing guessing analysis**: On Task 2 probes, compute item-level guessing patterns and correlate with knowledge state claims; adjust KPR if random guessing explains >20% of correct responses.
3. **CoT format sensitivity**: Run Task 3 on 3 models with alternative CoT formats (reordered steps, paraphrased clues, triple-only) and measure RΔ variance; if >15 points, attribute improvements to prompt sensitivity rather than knowledge utilization.