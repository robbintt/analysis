---
ver: rpa2
title: 'AutoMathKG: The automated mathematical knowledge graph based on LLM and vector
  database'
arxiv_id: '2505.13406'
source_url: https://arxiv.org/abs/2505.13406
tags:
- entity
- entities
- knowledge
- definition
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoMathKG addresses the challenge of constructing a comprehensive,
  updatable mathematical knowledge graph from natural language by integrating diverse
  sources and leveraging large language models (LLMs). It structures mathematics as
  a directed graph of definitions, theorems, and problems, using LLMs for entity extraction,
  relationship discovery, and knowledge augmentation.
---

# AutoMathKG: The automated mathematical knowledge graph based on LLM and vector database

## Quick Facts
- **arXiv ID:** 2505.13406
- **Source URL:** https://arxiv.org/abs/2505.13406
- **Reference count:** 40
- **Primary result:** AutoMathKG constructs an automatically updatable mathematical knowledge graph with a vector database (MathVD) and LLM-based reasoning agent, outperforming baselines in reachability queries and mathematical reasoning tasks.

## Executive Summary
AutoMathKG addresses the challenge of constructing a comprehensive, updatable mathematical knowledge graph from natural language by integrating diverse sources and leveraging large language models (LLMs). It structures mathematics as a directed graph of definitions, theorems, and problems, using LLMs for entity extraction, relationship discovery, and knowledge augmentation. To enable efficient similarity searches, it builds MathVD, a vector database with two embedding strategies using SBERT. Two mechanisms ensure automatic updates: knowledge completion via a specialized Math LLM, and knowledge fusion through VD and LLM. Experiments show superior reachability query performance (Hits@q) in MathVD compared to five baselines and robust mathematical reasoning in Math LLM, demonstrating the system’s effectiveness and broad applicability.

## Method Summary
AutoMathKG constructs a mathematical knowledge graph by first extracting entities (Definitions, Theorems, Problems) from diverse mathematical corpora using rule-based parsing. It then employs Llama-2 via In-Context Learning to augment these entities with semantic relationships, tactic labels, and additional context. The system builds MathVD, a vector database using SBERT embeddings, with two strategies: concatenating all text fields (MathVD1) or using a weighted sum of embeddings for title, content, and references (MathVD2). For updates, it uses a knowledge fusion mechanism where MathVD retrieves similar entities and an LLM determines whether to merge them. The system also includes a Math LLM (Gemma-7b-it with Adapters) for knowledge completion and reasoning tasks.

## Key Results
- MathVD achieves superior reachability query performance (Hits@q) compared to five baseline methods (TransE, RotatE, etc.).
- MathVD2, using weighted embeddings, performs slightly better for smaller query depths, while MathVD1 shows higher Hits@q for larger query depths.
- The Math LLM demonstrates robust mathematical reasoning capabilities across multiple domains when evaluated on the GHOSTS dataset.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structuring mathematical natural language into a directed graph with semantic "tactic" edges likely improves the logical traversability of the knowledge base compared to simple keyword indexing.
- **Mechanism:** The system extracts entities and uses an LLM via In-Context Learning to identify reference relationships and assign tactic labels (e.g., "premise," "lemma," "deduction"). This transforms unstructured text into a directed graph where edges carry semantic weight about *how* entities relate (dependency vs. definition), mimicking formal proof structures.
- **Core assumption:** Assumes the LLM can accurately infer logical dependencies and reference links from natural language text that were not explicitly linked in the raw corpus.
- **Evidence anchors:**
  - [abstract] "...enhancing entities and relationships with large language models (LLMs) via in-context learning..."
  - [section 4.2.3] "...we innovatively employ LLM to identify the tactic labels and store them in the directed edges... similar to the use of tactics in the formal language Lean 4."
- **Break condition:** If the input corpus lacks clear citation markers or logical flow, the LLM may hallucinate edges, creating a disconnected or erroneous graph topology.

### Mechanism 2
- **Claim:** Weighted vector embeddings that include topological context (MathVD2) appear to outperform pure content embeddings in retrieving structurally relevant mathematical entities.
- **Mechanism:** The system builds a Vector Database (MathVD) using SBERT. It compares two strategies: MathVD1 (concatenating all text fields) and MathVD2 (weighted sum of vectors for title, content, and references). By encoding "in-refs" and "out-refs" into the vector, the embedding captures not just semantic meaning but also the graph neighborhood, improving "fuzzy search" for entities that serve similar structural roles.
- **Core assumption:** Assumes that mathematical relevance correlates with shared graph neighbors (topology) in addition to textual similarity.
- **Evidence anchors:**
  - [abstract] "...MathVD, a vector database with two embedding strategies... Experiments show superior reachability query performance (Hits@q)..."
  - [section 7.3.1] "...MathVD2 performs slightly better for smaller q, while MathVD1 shows a little higher Hits@q for larger q."
- **Break condition:** If the graph is sparse (few references), the topological weighting in MathVD2 provides little signal, potentially degrading performance compared to pure semantic search (MathVD1).

### Mechanism 3
- **Claim:** Automating knowledge fusion via vector-similarity filtering followed by LLM verification may reduce the manual overhead of integrating diverse mathematical sources.
- **Mechanism:** When new text arrives, the system creates an "Input KG" and searches the "Existing VD" for similar vectors. If candidates exist, an LLM checks for semantic equivalence (Consistency) to decide whether to merge nodes or add new ones. This decouples the retrieval (fast, approximate) from the fusion decision (slow, precise).
- **Core assumption:** Assumes that Vector DB similarity search (cosine similarity) is sufficient to recall the correct "candidate" entities for the LLM to verify; if the retrieval fails, the fusion will create duplicates.
- **Evidence anchors:**
  - [abstract] "...knowledge fusion mechanism, MathVD is used to retrieve similar entities, and LLM is used to determine whether to merge..."
  - [section 6.2] "Step 4: Determination of entity consistency by LLM... employ LLM through ICL to determine the consistency."
  - [corpus] Neighbor papers like *ReCellTy* and *GraphRAG* frameworks similarly utilize retrieval-LLM loops to handle entity alignment or query resolution, supporting the viability of this hybrid architecture.
- **Break condition:** If the definition of a mathematical concept shifts subtly between sources (e.g., differing axioms), the LLM might incorrectly merge distinct entities into a single node.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The architecture relies on Llama-2 not just for text generation, but for *classification* and *extraction* (identifying entity types and tactic labels) without fine-tuning.
  - **Quick check question:** How does the system ensure the LLM outputs valid JSON or specific tactic labels (e.g., "lemma") instead of free-form text?

- **Concept: SBERT (Sentence-BERT) Embeddings**
  - **Why needed here:** Understanding how semantic text is converted to vectors is crucial for debugging why MathVD retrieves certain "similar" theorems.
  - **Quick check question:** What is the trade-off between MathVD1 (concatenation) and MathVD2 (weighted sum) in terms of information loss versus noise reduction?

- **Concept: Knowledge Graph (KG) Topology**
  - **Why needed here:** The system defines specific entity types (Def, Thm, Prob) and edges (refs). Understanding the allowed cycles and directionality is key to querying the data.
  - **Quick check question:** Can a "Problem" entity refer to another "Problem" entity, or only to "Theorems" and "Definitions"?

## Architecture Onboarding

- **Component map:**
  - **Input Layer:** Data ingestion (ProofWiki, arXiv, Textbooks).
  - **Processing Layer:** Rule-based pre-processing $\rightarrow$ LLM-based Augmentation (Llama-2) $\rightarrow$ JSON storage.
  - **Storage Layer:** AutoMathKG (Graph structure) + MathVD (Vector Database).
  - **Agent Layer:** Math LLM (Gemma-7b-it with Adapters) for reasoning/completion.

- **Critical path:** The **Entity Augmentation Pipeline (Section 4.3)**. If the LLM fails to correctly extract "references tactics" or segment "bodylist" content here, the downstream Vector Database (MathVD) will be seeded with poor context, causing retrieval failures in the Update and Reasoning mechanisms.

- **Design tradeoffs:**
  - **LLM Size:** The authors use Llama-2-7b to save resources, acknowledging that larger models might perform better (Limitation). This imposes a ceiling on extraction quality.
  - **Embedding Strategy:** MathVD1 favors broader context (better for large $k$ queries), while MathVD2 weights content/title higher (better for precision at low $k$).

- **Failure signatures:**
  - **Hallucinated Edges:** The graph shows spurious edges between unrelated theorems (check "references tactics" confidence).
  - **Vector Collision:** Distinct mathematical concepts map to the same vector in MathVD (check embedding dimensionality and model choice).
  - **Fusion Loops:** The system attempts to merge and split the same entity repeatedly during updates.

- **First 3 experiments:**
  1.  **Extraction Validity Test:** Feed the pipeline 10 diverse mathematical pages (e.g., from ProofWiki vs. arXiv) and manually verify the accuracy of the "references_tactics" JSON field.
  2.  **Reachability Benchmark:** Implement the Hits@q test (Section 7.3.1) comparing MathVD1 vs. MathVD2 on a sample of 50 queries to validate the paper's claim about structural retrieval.
  3.  **Fusion Stress Test:** Feed the "Knowledge Fusion" mechanism two descriptions of the same theorem (one formal, one informal) and verify if the LLM successfully merges them vs. creating a duplicate.

## Open Questions the Paper Calls Out
- **Question:** How does the performance of AutoMathKG’s extraction and fusion mechanisms improve when utilizing larger-scale foundation models compared to the Llama-2-7b model used in the study?
  - **Basis in paper:** [explicit] Section 8.4 explicitly identifies the use of the small-scale Llama-2-7b model as a limitation and suggests that larger language models may perform better.
  - **Why unresolved:** The authors restricted the scope to Llama-2-7b to save computational resources, leaving the potential performance gains from more advanced models (e.g., GPT-4 or larger open-source variants) untested.
  - **What evidence would resolve it:** Comparative benchmarks of entity extraction accuracy and knowledge fusion success rates using models with significantly higher parameter counts.

- **Question:** Can the current AutoMathKG framework effectively integrate informal mathematical corpora, such as forum discussions, without sacrificing the precision required for formal knowledge representation?
  - **Basis in paper:** [explicit] Section 8.4 highlights the exclusion of informal texts (e.g., discussion dialogues) as a limitation, noting a trade-off between corpus richness and knowledge specialization.
  - **Why unresolved:** The system was validated exclusively on formal texts (textbooks, papers, ProofWiki). The pipeline's ability to handle the ambiguity, unstructured nature, and potential noise of informal mathematical dialogue remains unknown.
  - **What evidence would resolve it:** Performance metrics (such as Hits@q or retrieval precision) of the MathVD and AutoMathKG systems when constructed from a dataset comprising both formal texts and informal forum data.

- **Question:** What is the quantitative rate of LLM hallucination during the "LLM-based augmentation" phase, and how does it impact the veracity of the final knowledge graph?
  - **Basis in paper:** [inferred] Section 4.3 describes using Llama-2 to extract references and segment content (augmentation), but the evaluation focuses on reachability and retrieval rather than the factual correctness of the LLM-generated graph edges and attributes.
  - **Why unresolved:** While the system automates updates, there is no reported metric regarding the "noise" or false positives introduced by the LLM when inferring relationships or tactics that were not present in the original rule-based extraction.
  - **What evidence would resolve it:** A manual or automated audit of the "references tactics" and "bodylist" attributes generated by the LLM against the source text to measure the false discovery rate.

## Limitations
- The system's reliance on a smaller Llama-2-7b model may limit extraction quality compared to larger foundation models.
- The framework is validated only on formal mathematical texts and may not generalize well to informal or ambiguous mathematical discourse.
- There is no quantitative analysis of LLM hallucination rates during the augmentation phase, which could impact the accuracy of the knowledge graph.

## Confidence
- **High Confidence:** The core claim that MathVD (especially MathVD2) improves reachability query performance (Hits@q) compared to baselines like TransE. This is directly supported by the reported experimental results and aligns with the established utility of vector similarity search in KG retrieval tasks.
- **Medium Confidence:** The claim that the automated knowledge fusion mechanism (MathVD + LLM) effectively reduces manual overhead for integrating new sources. While the mechanism is plausible and the fusion loop is well-defined, the actual performance (precision of merge decisions, rate of duplicate creation) is not quantitatively reported in the paper.
- **Low Confidence:** The claim that the entire AutoMathKG system, with its Math LLM reasoning agent, will generalize robustly to arbitrary mathematical domains. This is a broad claim for a system that has only been tested on specific datasets (ProofWiki, TheoremQA, etc.) and evaluated on a curated test set (GHOSTS). The reliance on ICL and the lack of detailed error analysis for the LLM's augmentation tasks introduce significant uncertainty.

## Next Checks
1. **Extraction Validity Test:** Feed the pipeline 10 diverse mathematical pages (e.g., from ProofWiki vs. arXiv) and manually verify the accuracy of the "references_tactics" JSON field to assess the reliability of the LLM's logical dependency extraction.
2. **Reachability Benchmark:** Implement the Hits@q test (Section 7.3.1) comparing MathVD1 vs. MathVD2 on a sample of 50 queries to validate the paper's claim about the superiority of structural retrieval in MathVD2 for smaller query depths.
3. **Fusion Stress Test:** Feed the "Knowledge Fusion" mechanism two descriptions of the same theorem (one formal, one informal) and verify if the LLM successfully merges them vs. creating a duplicate to assess the precision of the auto-merge decision process.