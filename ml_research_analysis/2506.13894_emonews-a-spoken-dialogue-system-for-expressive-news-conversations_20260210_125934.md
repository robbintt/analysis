---
ver: rpa2
title: 'EmoNews: A Spoken Dialogue System for Expressive News Conversations'
arxiv_id: '2506.13894'
source_url: https://arxiv.org/abs/2506.13894
tags:
- emotional
- system
- news
- speech
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study develops an emotional spoken dialogue system for news
  conversations that integrates sentiment analysis with emotional text-to-speech synthesis.
  The system uses a sentiment analyzer to identify appropriate emotions and PromptTTS
  to generate context-appropriate emotional speech, addressing the underexplored area
  of task-oriented emotional dialogue systems.
---

# EmoNews: A Spoken Dialogue System for Expressive News Conversations

## Quick Facts
- **arXiv ID:** 2506.13894
- **Source URL:** https://arxiv.org/abs/2506.13894
- **Reference count:** 8
- **Key outcome:** Emotional spoken dialogue system for news conversations integrates sentiment analysis with emotional text-to-speech synthesis, achieving significant improvements in speech emotion appropriateness (d = 3.070) while maintaining task performance

## Executive Summary
EmoNews introduces an emotional spoken dialogue system for news conversations that addresses the underexplored area of task-oriented emotional dialogue systems. The system combines a sentiment analyzer to identify appropriate emotions with PromptTTS for generating context-appropriate emotional speech. Unlike existing emotional SDS that focus on social chat or rely on user vocal affect, EmoNews predicts emotions from LLM-generated text responses. Human evaluation demonstrated the proposed system significantly outperformed a baseline in speech emotion appropriateness with a large effect size, while also showing trends toward increased user engagement without compromising task performance.

## Method Summary
The system uses a cascade architecture where ASR (Whisper Large) transcribes user speech, RAG-LLM (LLaMA 3.2 1B) generates responses grounded in news articles, a fine-tuned RoBERTa sentiment classifier predicts emotion tags from the response text, and PromptTTS synthesizes emotional speech conditioned on both text and emotion. The emotional components operate post-hoc on LLM outputs, preserving task performance. The system was fine-tuned on the ESD dataset (17,500 utterances across 5 emotions) and evaluated through 10 conversation samples per condition, using a 5-point Likert scale questionnaire measuring RAG evaluation, task achievement, speech emotion appropriateness, and engagement.

## Key Results
- Speech emotion appropriateness significantly improved with large effect size (d = 3.070, p < 0.01)
- No significant differences in RAG performance or task achievement between systems (both scored ~3/5)
- Engagement and number of turns showed large effect sizes (d = 0.824, 0.831) but did not reach statistical significance (p = 0.090, 0.073)

## Why This Works (Mechanism)

### Mechanism 1: Sentiment-Guided Emotion Tag Generation
Fine-tuned sentiment classification of LLM outputs produces more accurate emotion tags than prompt-based LLM approaches for downstream TTS conditioning. The system uses a distilled RoBERTa classifier fine-tuned on mapped emotion categories to output discrete emotion tags that condition PromptTTS synthesis. This approach avoids the over-prediction of sadness and surprise observed with prompt-based methods.

### Mechanism 2: Prompt-Based Emotional TTS Conditioning
PromptTTS fine-tuned on emotional speech data produces perceptually distinct emotional prosody. The system conditions speech synthesis on both text and emotion prompts, learning to modulate acoustic features based on the five emotion categories. This discrete categorical control provides sufficient coverage for news dialogue contexts.

### Mechanism 3: Cascade Architecture Preserving Task Performance
Adding emotional speech regulation as a post-hoc module in cascade SDS architecture improves social perception without degrading task-oriented metrics. The emotional components operate on LLM outputs independently, preventing emotional regulation from interfering with factual accuracy or retrieval relevance.

## Foundational Learning

- **Emotional Text-to-Speech (Emotional TTS):** Understanding how acoustic features (pitch, intensity, cadence) are conditioned on emotion labels to produce expressive speech. *Quick check:* Can you explain how PromptTTS differs from conventional TTS in terms of input requirements and output control?

- **Retrieval Augmented Generation (RAG):** Understanding how retrieval combined with LLM generation grounds responses in news databases. *Quick check:* In a RAG pipeline, how does retrieved context influence the LLM's generation, and where does the sentiment analyzer fit in this flow?

- **Affective Computing Evaluation Metrics:** Understanding subjective scales (Likert items), effect sizes (Cohen's d), and non-parametric tests for small samples (Mann-Whitney U). *Quick check:* Why might the authors use Mann-Whitney U tests rather than t-tests for comparing system ratings?

## Architecture Onboarding

- **Component map:** User Speech → ASR (Whisper Large) → Text Transcript → RAG Retrieval → LLM (LLaMA 3.2 1B) → Response Text → [Proposed: Sentiment Analyzer → Emotion Tag] → TTS (VITS/PromptTTS) → Speech Output

- **Critical path:** The emotion regulation path runs through LLM Response → Sentiment Analyzer → PromptTTS. If the sentiment classifier fails or outputs inappropriate tags, the entire emotional expressiveness collapses.

- **Design tradeoffs:**
  - Discrete vs. continuous emotions: Five categories limit expressiveness but simplify training and evaluation
  - Fine-tuned classifier vs. LLM prompting: RoBERTa avoids over-prediction but requires labeled training data
  - Cascade vs. integrated architecture: Cascade preserves task modularity but prevents emotional context from influencing retrieval or generation

- **Failure signatures:**
  - Emotion tag stuck on one category: Check sentiment analyzer calibration
  - Abrupt emotional transitions between turns: Current architecture lacks dialogue history in emotion prediction
  - Task performance degradation: Monitor RAG relevance scores separately

- **First 3 experiments:**
  1. Sentiment classifier validation: Test fine-tuned RoBERTa on held-out news snippets
  2. Emotional TTS quality check: Generate samples for each emotion tag and conduct A/B listening tests
  3. Ablation on emotion appropriateness: Randomly shuffle emotion tags before TTS to verify metric validity

## Open Questions the Paper Calls Out

**Open Question 1:** Would incorporating conversational history to enable gradual emotional transitions, rather than discrete shifts, significantly improve user engagement scores? The authors note moderate engagement scores were "possibly due to abrupt, discrete emotional shifts without considering prior conversational context."

**Open Question 2:** Does the proposed emotional SDS framework maintain its benefits when adapted to other task-oriented domains beyond news conversations? The authors state the "modular design enables easy adaptation to other domains."

**Open Question 3:** Would studies with larger sample sizes confirm the observed trend toward increased engagement with emotional speech regulation? Engagement metrics showed large effect sizes but lacked statistical significance likely due to small sample size (n=10 per condition).

## Limitations

- **Emotion classifier reliability across domains:** Sentiment analyzer fine-tuned on GoodNewsEveryone and GoEmotions datasets may produce systematic biases on actual news conversations without domain adaptation validation.

- **Effect size interpretation without significance testing:** Large effect sizes reported for engagement improvements lack statistical validation, introducing uncertainty about whether observed trends represent real effects.

- **Five-category emotion constraint:** Discrete emotion model may inadequately capture nuanced emotional states in news contexts, with no quantitative analysis of when categories fail.

## Confidence

- **High confidence** in task performance preservation claims - No significant differences in RAG performance or task achievement between systems (both scored ~3/5), directly measured and validated.
- **Medium confidence** in speech emotion appropriateness improvements - Large effect size (d = 3.070) and statistical significance, but single-turn design may overestimate real-world performance.
- **Low confidence** in engagement improvement claims - Only trends toward increased engagement without significance testing, making it difficult to distinguish genuine improvements from sampling variability.

## Next Checks

1. **Cross-domain emotion classifier validation:** Test the fine-tuned RoBERTa sentiment analyzer on held-out news articles not used in training to verify emotion prediction accuracy and identify systematic biases.

2. **Engagement metric statistical validation:** Re-analyze the engagement questionnaire data with appropriate significance tests (Mann-Whitney U or similar) to determine whether observed trends represent statistically reliable improvements.

3. **Mixed emotion scenario testing:** Create test cases with news content requiring multiple emotions within single utterances or across short dialogue turns to evaluate whether the discrete five-category system produces perceptually acceptable transitions.