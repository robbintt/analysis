---
ver: rpa2
title: Factorizable joint shift revisited
arxiv_id: '2601.15036'
source_url: https://arxiv.org/abs/2601.15036
tags:
- label
- shift
- distribution
- respect
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper extends the concept of factorizable joint shift (FJS)
  to general label spaces, covering both classification and regression problems. It
  provides a theoretical framework analyzing distribution shift under FJS and generalizes
  existing results from the categorical label case.
---

# Factorizable joint shift revisited

## Quick Facts
- arXiv ID: 2601.15036
- Source URL: https://arxiv.org/abs/2601.15036
- Reference count: 23
- Primary result: Extends factorizable joint shift to general label spaces, providing theoretical framework and generalized EM algorithm for estimating target label distributions

## Executive Summary
This paper extends the concept of factorizable joint shift (FJS) from categorical to general label spaces, covering both classification and regression problems. The authors provide a theoretical framework analyzing distribution shift under FJS, showing it can be decomposed into consecutive label and covariate shifts. They develop a generalized EM algorithm for estimating target label distributions and examine the relationship between generalized label shift and FJS, demonstrating that GLS implies FJS when the representation mapping is sufficient.

## Method Summary
The paper presents two main approaches for estimating target label distributions under FJS. For Problem A (known target marginals), an iterative method solves a nonlinear integral equation for ψ(y). For Problem B (known target features only), a generalized EM algorithm alternates between E-steps computing posterior label distributions and M-steps updating the label density. The algorithms operate under assumptions of absolute continuity between source and target distributions, existence of regular conditional distributions, and availability of a density φ for computational simplification.

## Key Results
- FJS can be decomposed into consecutive label shift followed by covariate shift, providing a constructive interpretation of the joint shift assumption
- The classical EM algorithm for label shift extends to arbitrary label spaces, with KL divergence decreasing monotonically
- GLS implies FJS when the representation mapping is statistically sufficient for predicting labels
- Exact feature matching through label shift is often impossible, requiring acceptance of approximate solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorizable joint shift can be decomposed into consecutive label shift followed by covariate shift (or vice versa), making the joint shift assumption more interpretable and tractable.
- Mechanism: The joint density ratio f(x,y) = dQ/dP factorizes as h(x)g(y) under FJS. This enables construction of an intermediate distribution Q_L via label shift (dQ_L/dP ∝ g(Y)), then Q_C via covariate shift (dQ_C/dQ_L ∝ h(X)), yielding Q = Q_C. This sequential decomposition provides a constructive interpretation of what FJS means operationally.
- Core assumption: Target distribution Q is absolutely continuous with respect to source P (Assumption 2.2), ensuring the density ratio f exists.
- Evidence anchors:
  - [abstract]: "Recently, it has been observed that FJS actually arises from consecutive label and covariate (or vice versa) shifts."
  - [section 3.1, Proposition 3.4]: Formalizes the decomposition with Q_L and Q_C constructions.
  - [corpus]: Related work on covariate/label shift exists; this paper provides the first formal decomposition proof for general label spaces.
- Break condition: If Q is not absolutely continuous w.r.t. P (e.g., disjoint supports), the density ratio f does not exist and the decomposition fails.

### Mechanism 2
- Claim: The classical EM algorithm for estimating target class priors under label shift can be generalized to arbitrary label spaces (continuous, mixed, or categorical).
- Mechanism: The iterative algorithm alternates between: (E-step) computing posterior label distributions q_Y|X=x^(n)(y) ∝ g_n(y)P(Y|X=x) normalized over labels, and (M-step) updating the label density g_{n+1}(y) = ∫ q_Y|X=x^(n)(y) h(x) P(X|Y=y) dx. This iteratively refines g to match the observed target feature marginal h while maintaining consistency with source conditionals.
- Core assumption: Regular conditional distributions P(Y|X) and P(X|Y) exist (Assumption 2.4), plus a density φ = dP/d(P_X ⊗ P_Y) exists for computational simplification (Assumption 2.9).
- Evidence anchors:
  - [abstract]: "development of a generalized expectation-maximization (EM) algorithm for estimating target label distributions"
  - [section 3.2.3, Theorem 3.12]: Proves KL divergence KL_PX(h || h_n) decreases monotonically, ensuring non-deteriorating feature distribution approximation.
  - [corpus]: EM algorithms for label shift are well-studied for categorical labels; extensions to general spaces are novel.
- Break condition: If regular conditionals don't exist (e.g., X deterministically determines Y violating support conditions), the E-step integrals become undefined.

### Mechanism 3
- Claim: Generalized Label Shift (GLS) in a representation space implies Factorizable Joint Shift in the original feature space when the representation mapping is statistically sufficient.
- Mechanism: If representation R satisfies P[Y∈G|X] = P[Y∈G|R] for all label sets G (sufficiency), then label shift in the R-space (dQ_Y/dP_Y = g(y)) propagates to FJS in X-space with f(x,y) = (E_P[f(X,Y)|X=x]/E_P[g(Y)|X=x]) · g(y). This provides theoretical justification for representation learning approaches to domain adaptation.
- Core assumption: The representation σ(R) ⊂ σ(X) must be sufficient for predicting Y under both P and Q (equation 4.1).
- Evidence anchors:
  - [abstract]: "demonstrating that GLS implies FJS when the representation mapping is sufficient"
  - [section 4, Corollary 4.2]: Shows GLS implies FJS with specific structure (ψ=1 constraint).
  - [corpus]: Prior work established this for categorical labels (He et al., 2021; Tasche, 2022); this paper extends to general spaces.
- Break condition: If representation R loses predictive information (P[Y|X] ≠ P[Y|R]), the sufficiency condition fails and GLS no longer guarantees FJS.

## Foundational Learning

- Concept: **Radon-Nikodym derivatives (density ratios)**
  - Why needed here: The entire framework expresses distribution shift through density ratios f = dQ/dP. Understanding that Q[F] = E_P[f·1_F] is the foundation for importance weighting and the factorization f(x,y) = h(x)g(y).
  - Quick check question: Explain why f(x,y) = h(x)g(y) implies that h(x) is not simply the marginal feature density ratio, but rather a conditional density of Q_X|Y with respect to P_X|Y after normalization.

- Concept: **Regular conditional distributions**
  - Why needed here: The EM algorithm and integral equations require P(Y|X) and P(X|Y) to be well-defined probability measures for each conditioning value. Without this, the E-step integration and M-step updates are mathematically invalid.
  - Quick check question: Why does Assumption 2.9 (P ≪ P_X ⊗ P_Y) guarantee the existence of both regular conditionals, and what does the density φ represent?

- Concept: **Kullback-Leibler divergence properties**
  - Why needed here: Theorem 3.12 uses KL divergence to prove the EM algorithm makes progress. Understanding that KL ≥ 0 with equality iff distributions match is essential for interpreting convergence diagnostics.
  - Quick check question: If KL_PX(h || h_n) converges to KL* > 0, what does this imply about whether the target distribution Q can be realized through pure label shift from P?

## Architecture Onboarding

- Component map:
  - Source distribution P(X,Y): Fully characterized with known P_X, P_Y, P(Y|X), P(X|Y)
  - Target marginals: Either (A) both Q_X, Q_Y known, or (B) only Q_X observable
  - Density ratio f = dQ/dP: Unknown target quantity; factorizes as h(x)g(y) under FJS
  - Label density g(y): Primary estimation target; ratio dQ_Y/dP_Y
  - Feature density h(x): Observable or estimable from unlabeled target data; ratio dQ_X/dP_X
  - Integral equation system: (3.5b-c) couples h, g, and ψ for Problem (A); EM algorithm (3.11a-b) for Problem (B)

- Critical path:
  1. Verify assumptions: Check Q ≪ P (no new regions in target); verify regular conditionals exist
  2. Estimate source components: Learn P(Y|X), P(X|Y), and φ from labeled source data
  3. Select problem formulation: (A) Known both marginals → solve (3.8) iteratively for ψ; (B) Features only → run EM algorithm
  4. Initialize densities: Set g_0(y) = 1 (uniform) or use domain knowledge
  5. Iterate until convergence: Monitor KL divergence stabilization for EM; track ψ updates for integral approach
  6. Reconstruct target posteriors: Apply equation (3.7) to compute Q(Y|X=x) once g converges

- Design tradeoffs:
  - ψ=1 vs. unconstrained FJS: Setting ψ(y)=1 ∀y (label shift-like constraint) ensures exact feature match but may be infeasible; unconstrained FJS is more flexible but requires solving the nonlinear integral equation (3.5c)
  - Assumption 2.9 simplification: Using φ enables tractable integrals via (3.12) but requires additional density estimation; direct approach is more general but computationally harder
  - Exact fit vs. approximate solutions: Proposition 3.14 shows exact fit (KL*=0) is often impossible for label shift; accepting KL*>0 trades precision for tractability

- Failure signatures:
  - KL divergence plateau at positive value: Indicates label shift assumption violated; consider full FJS with ψ estimation
  - Oscillating density estimates g_n: Suggests numerical instability or Assumption 2.4 violations; check conditional distribution estimation quality
  - Negative/unnormalized posteriors: Q(Y|X) > 1 from equation (3.7) signals g or h estimation errors; re-examine source conditional quality
  - Non-convergence of ψ iteration: Equation (3.9) may not satisfy interchange of limits (3.10); consider discretizing label space or adding regularization

- First 3 experiments:
  1. Synthetic Gaussian mixture validation: Generate source from K-component mixture with known P(Y|X); apply known label shift g*(y); verify EM recovers ||g_n - g*||_2 < ε. This confirms algorithm correctness under ideal conditions where all assumptions hold and ground truth is available.
  2. Assumption violation characterization: Systematically break Assumption 2.2 (introduce Q-regions with P-support zero), Assumption 2.4 (create deterministic X-Y relationships), and Assumption 2.9 (make X functionally dependent on Y). Measure degradation in KL convergence rate and final approximation quality to establish robustness boundaries.
  3. Real domain adaptation with diagnostics: Apply to benchmark with known distribution shift (e.g., MNIST→USPS, sentiment analysis across domains). Compare three conditions: (a) label shift EM, (b) FJS with ψ=1, (c) FJS with ψ estimation via (3.9). Report: target accuracy, KL_PX(h||h_n) trajectory, and posterior calibration metrics. Distinguish cases where pure label shift suffices vs. where joint shift modeling provides measurable benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the convergence rates and computational complexities of the iterative estimation approaches proposed for Factorizable Joint Shift (FJS) in general label spaces?
- Basis in paper: [explicit] The conclusion explicitly lists "the computational properties of the estimation approaches proposed in Sections 3.2.1 and 3.2.3" as a topic for future research.
- Why unresolved: The paper provides the mathematical formulation and proofs of decreasing Kullback-Leibler divergence (Theorem 3.12) but does not analyze the speed of convergence or the computational cost of the iterative algorithms for general (non-categorical) spaces.
- What evidence would resolve it: A theoretical analysis of convergence rates or empirical benchmarks comparing the runtime of the generalized EM algorithm against baseline methods on regression tasks.

### Open Question 2
- Question: Can the framework for general label spaces be extended to characterize other specific types of distribution shift, such as sparse joint shift?
- Basis in paper: [explicit] The conclusion suggests "the study of other specific types of distribution shift (e.g. sparse joint shift, Chen et al., 2022) in the face of general label spaces" as a future direction.
- Why unresolved: The current work focuses on generalizing FJS and Generalized Label Shift (GLS); definitions and theoretical results for sparse joint shift remain confined to categorical label spaces in existing literature.
- What evidence would resolve it: A formal definition of sparse joint shift adapted for general label spaces, accompanied by theorems parallel to those presented for FJS in this paper.

### Open Question 3
- Question: What are the sufficient conditions for the sequence of densities $g_n$ in the generalized EM algorithm to converge to a unique limit in general label spaces?
- Basis in paper: [inferred] Remark 3.13 notes that Theorem 3.12 does not prove the convergence of the sequence $Q(n)$ or the existence of a limit density $h^*$, contrasting this with the categorical case where sufficient conditions (Wu, 1983) exist.
- Why unresolved: The paper demonstrates that the EM algorithm decreases the Kullback-Leibler divergence but leaves the actual convergence of the density sequence $g_n$ for general labels as an open theoretical gap.
- What evidence would resolve it: A formal proof establishing conditions (e.g., constraints on the source distribution $P$ or the feature space) under which the EM recursion converges to a fixed point for continuous labels.

### Open Question 4
- Question: Under what conditions does the iterative solution for $\psi$ (Eq. 3.9) guarantee that the limit satisfies the fixed-point equation (3.8)?
- Basis in paper: [inferred] Section 3.2.1 explicitly highlights that the existence of a limit $\psi^*$ does not necessarily imply it fulfills the equation, noting the potential inability to swap the limit and integral (Eq. 3.10).
- Why unresolved: The author identifies this as a potential problem for the proposed iterative approach but offers no solution or constraints to ensure the validity of the limit for general label spaces.
- What evidence would resolve it: A theorem identifying the necessary regularity conditions (e.g., uniform integrability or dominated convergence criteria) required to ensure the limit of the iterative sequence solves the integral equation.

## Limitations

- The theoretical framework relies heavily on strong continuity assumptions that may not hold in practice, particularly when target distributions assign positive probability to regions with zero source density
- The absence of empirical validation across diverse datasets limits confidence in real-world applicability of the generalized EM algorithm
- The analysis focuses on convergence of density estimates but doesn't establish generalization guarantees for downstream classification tasks

## Confidence

- Factorizable joint shift decomposition mechanism: **High** - supported by formal proof and constructive interpretation
- Generalized EM algorithm convergence: **Medium** - theoretical guarantees exist but no empirical convergence patterns shown
- GLS implies FJS relationship: **Medium** - theoretical derivation is sound but sufficiency conditions may be restrictive in practice

## Next Checks

1. Empirical evaluation of EM algorithm convergence across varying degrees of label shift severity and dimensionalities
2. Systematic study of assumption violations (e.g., Q_X has regions where P_X=0) and resulting algorithm behavior
3. Comparison of FJS-based adaptation versus baseline importance weighting methods on standard domain adaptation benchmarks