---
ver: rpa2
title: Activation Function Design Sustains Plasticity in Continual Learning
arxiv_id: '2509.22562'
source_url: https://arxiv.org/abs/2509.22562
tags:
- activation
- learning
- plasticity
- continual
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies how activation function design impacts plasticity\
  \ in continual learning. It introduces two novel drop-in activation functions\u2014\
  Smooth-Leaky and Randomized Smooth-Leaky\u2014designed to preserve gradient flow\
  \ and maintain adaptability under non-stationary data."
---

# Activation Function Design Sustains Plasticity in Continual Learning

## Quick Facts
- arXiv ID: 2509.22562
- Source URL: https://arxiv.org/abs/2509.22562
- Reference count: 40
- Primary result: Novel activation functions (Smooth-Leaky, Randomized Smooth-Leaky) outperform standard activations in continual learning by maintaining gradient flow and adaptability under non-stationary data.

## Executive Summary
This paper demonstrates that activation function design critically impacts plasticity in continual learning. The authors introduce Smooth-Leaky and Randomized Smooth-Leaky activations that maintain non-zero gradient floors for negative inputs while providing smooth transitions at the origin. Through extensive experiments on supervised continual learning benchmarks and non-stationary reinforcement learning tasks, these functions consistently outperform standard activations like ReLU and Swish. The key insight is that preserving a minimum gradient signal prevents neuron dormancy during distribution shifts, enabling networks to maintain adaptability throughout lifelong learning scenarios.

## Method Summary
The authors propose two drop-in activation functions: Smooth-Leaky (f(x) = αx + (1-α)x·σ(cx^p)) and Randomized Smooth-Leaky (same but with randomized α during training). These are evaluated across multiple continual learning settings including Split-CIFAR-100, Permuted MNIST, and cyclic MuJoCo locomotion tasks. Experience replay with buffer size 10,000 is used for supervised tasks, while PPO with 2×256 MLP backbones handles the RL experiments. Performance is measured using task accuracy, Plasticity Score (IQM-normalized late-cycle return), and Generalization Gap metrics.

## Key Results
- Smooth-Leaky and Randomized Smooth-Leaky activations achieve 5-15% higher average accuracy than ReLU on Split-CIFAR-100 across 20 tasks
- Non-zero derivative floors (α ≥ 0.6) reduce dead neuron fractions from ~45% to under 15% in distribution shift scenarios
- Randomized Smooth-Leaky shows statistically significant improvements in Continual ImageNet and MuJoCo RL benchmarks
- C¹ smooth transitions at the origin provide measurable benefits when combined with moderate negative-side slopes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A non-zero derivative floor for negative inputs prevents permanent gradient flow loss (dormant neurons) during distribution shifts.
- Mechanism: Standard ReLU has zero gradient for all negative pre-activations. If pre-activations shift negative, those neurons receive no gradient signal to correct—becoming permanently inactive. Leaky-style activations maintain φ'(x) ≥ α > 0 on the negative branch, guaranteeing a minimum gradient floor that allows recovery.
- Core assumption: The negative-branch slope α is sufficiently large to provide meaningful gradient signal but not so large as to destabilize optimization (a "Goldilocks zone" of roughly 0.6–0.9 for the effective slope).
- Evidence anchors:
  - [abstract] "activation functions with non-zero derivative floors...significantly mitigate loss of plasticity"
  - [section 3] "As s̄→0, a dead-unit regime dominates (≈45% inactive)...strongly correlates with accuracy loss (Pearson r=−0.51)"
  - [corpus] "Preserving Plasticity...with Adaptive Linearity Injection" shows linearity helps; "Activation by Interval-wise Dropout" addresses dormancy.

### Mechanism 2
- Claim: Smooth C¹ transitions at the origin reduce optimization curvature spikes that impede continual adaptation.
- Mechanism: Kinked activations (ReLU, Leaky-ReLU) have discontinuous first derivatives at x=0, creating sharp curvature changes in the loss landscape. Under distribution shift, this can amplify gradient variance and slow convergence. Smooth activations (Smooth-Leaky, Swish) have continuous first derivatives, smoothing the loss landscape and improving gradient stability during adaptation.
- Core assumption: The benefit of smoothness is realized primarily when a non-zero floor and moderate leak are already maintained.
- Evidence anchors:
  - [abstract] "two drop-in activation functions—Smooth-Leaky and Randomized Smooth-Leaky—designed to preserve gradient flow...with a C¹ transition"
  - [section 5] "prefer a C¹ (smooth) transition at the origin when (i)–(ii) are held fixed"
  - [corpus] "Constrained Rational Activations for RL" shows smoothing/rational forms help; corpus otherwise weak on C¹ benefits directly.

### Mechanism 3
- Claim: Randomizing the negative slope across forward passes injects controlled noise that improves robustness to pre-activation distribution shifts.
- Mechanism: Randomized Smooth-Leaky samples r ~ U(l, u) per forward pass, creating an ensemble-like effect where different units experience different gradient floors. This prevents any single unit from consistently falling into a bad local regime and encourages the network to learn representations robust to input scale variations.
- Core assumption: The randomization bounds [l, u] are centered around a good "Goldilocks" mean value and are not too wide (to avoid excessive variance).
- Evidence anchors:
  - [abstract] "Randomized Smooth-Leaky—designed to preserve gradient flow and maintain adaptability under non-stationary data"
  - [section 3] "RReLU attains the top mean in both settings; the C-IL improvement is statistically significant"
  - [corpus] Corpus does not directly address randomization mechanisms; evidence is paper-internal.

## Foundational Learning

- Concept: **Loss of Plasticity vs. Catastrophic Forgetting**
  - Why needed here: These are distinct failure modes. Forgetting is about losing prior knowledge; plasticity loss is about losing the *ability to learn* new tasks even if old ones are retained. The paper targets plasticity loss specifically.
  - Quick check question: If a network performs perfectly on old tasks but fails to learn a new, different task, is that forgetting or plasticity loss?

- Concept: **Dead (Dormant) Neurons**
  - Why needed here: A key diagnostic for plasticity loss. Neurons that output near-zero consistently and receive negligible gradient become "dead," reducing effective network capacity.
  - Quick check question: In a ReLU network, if pre-activations are consistently negative for a subset of neurons, can those neurons ever recover through gradient descent?

- Concept: **Desaturation Dynamics**
  - Why needed here: After a distribution shift (e.g., scaling shock), networks may have many units pushed into saturation (near-zero gradient regions). The speed and likelihood of "desaturation"—recovering active gradient flow—is a key plasticity metric.
  - Quick check question: Would you expect a Sigmoid network (two-sided saturation) to desaturate faster or slower than a Leaky-ReLU network after a large input scaling shock?

## Architecture Onboarding

- Component map: Input → CNN/MLP backbone with Smooth-Leaky/Rand. Smooth-Leaky activations → Task-specific heads → Output
- Critical path:
  1. Identify activation layers in existing backbone
  2. Replace with Smooth-Leaky (fixed α) or Rand. Smooth-Leaky (bounds [l, u])
  3. Tune hyperparameters: Start with α ∈ [0.6, 0.8], c, p ∈ {1, 2, 3}; for randomized, l, u around same range
  4. Monitor dead-unit fraction and gradient norms during training on first few tasks to confirm plasticity is maintained

- Design tradeoffs:
  - Smooth-Leaky vs. Rand. Smooth-Leaky: Smooth-Leaky is deterministic and simpler; Rand. adds stochastic regularization but requires tuning bounds. Rand. may help more in highly non-stationary RL; Smooth-Leaky may suffice for supervised CL
  - α too small → dead units persist; α too large → optimization instability (high curvature)
  - p, c too large → transition too sharp, approximating kinked activation; too small → overly linear, reducing expressivity

- Failure signatures:
  - Dead-unit fraction rising above ~10–15% early in training → α too small or floor not strict
  - Gradient norms collapsing to near-zero despite non-zero floor → pre-activations pushed deep into tails; check input scaling/normalization
  - High variance in loss/accuracy across runs with Rand. Smooth-Leaky → randomization bounds too wide
  - Performance degrades on later tasks despite good early performance → classic plasticity loss; confirm activation is actually being used (check implementation)

- First 3 experiments:
  1. **Baseline comparison on Split-CIFAR-100 (class-incremental)**: Compare ReLU, Leaky-ReLU (α=0.7), Smooth-Leaky (α=0.7, c=1, p=3), Rand. Smooth-Leaky (bounds [0.6, 0.8]). Report average accuracy and dead-unit fraction. Expect Smooth-Leaky variants to outperform ReLU significantly.
  2. **Shock recovery stress test**: Apply periodic scaling shocks (γ ∈ {0.5, 2.0}) to pre-activations during training. Measure desaturation time (AUSC, τ_95). Expect non-zero-floor activations to recover faster and more reliably.
  3. **Non-stationary RL on MuJoCo sequence**: Train single PPO agent cycling through HalfCheetah/Hopper/Walker2d/Ant. Compare Plasticity Score and Generalization Gap for Sigmoid, Swish, Smooth-Leaky, and Rand. Smooth-Leaky. Expect Rand. Smooth-Leaky to achieve highest plasticity but may struggle on unstable environments (e.g., Humanoid) due to unboundedness.

## Open Questions the Paper Calls Out

- Can adaptive, per-neuron activation slopes be constrained or regularized to reliably self-tune within the 'Goldilocks zone' (s̄ ∈ [0.6, 0.9]) without manual intervention?
- Does the high train-side plasticity achieved by non-zero floor activations reliably improve generalization to perturbed test conditions in reinforcement learning?
- How do Smooth-Leaky activations interact with optimizers other than Adam (e.g., SGD, RMSProp) regarding plasticity retention?
- Can theoretical curvature or desaturation bounds be derived to formally explain the existence of the 'Goldilocks zone' for negative slopes?

## Limitations
- Empirical validation limited to specific architectures (CNNs for supervised CL, PPO for RL) and may not generalize to transformer-based models or other RL algorithms
- "Goldilocks zone" for negative slopes (α ≈ 0.6–0.9) is empirically observed but lacks theoretical bounds
- Sensitivity to hyperparameter choices (c, p, randomization bounds) is not fully characterized
- RL findings rely on specific cyclic MuJoCo setup; results may differ with longer task sequences or more unstable environments

## Confidence
- High: Activation functions with non-zero derivative floors prevent permanent gradient loss and reduce dead neurons
- Medium: Smooth C¹ transitions improve gradient stability during continual adaptation
- Medium: Randomizing slopes provides ensemble-like robustness to pre-activation distribution shifts

## Next Checks
1. **Architecture generalization test**: Apply Smooth-Leaky and Rand. Smooth-Leaky to transformer-based continual learning tasks (e.g., continual language modeling) and compare plasticity metrics against ReLU and Swish
2. **Hyperparameter sensitivity analysis**: Systematically vary α, c, p, and randomization bounds across a broader range to map the stability and performance landscape, identifying robust defaults
3. **Alternative RL algorithm validation**: Evaluate plasticity on non-stationary tasks using SAC or TD3 instead of PPO to confirm the activation benefits are algorithm-agnostic