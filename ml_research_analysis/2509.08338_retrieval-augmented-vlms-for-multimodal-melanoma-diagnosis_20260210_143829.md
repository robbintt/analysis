---
ver: rpa2
title: Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis
arxiv_id: '2509.08338'
source_url: https://arxiv.org/abs/2509.08338
tags:
- bert
- resnext-50
- clinical
- classification
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a retrieval-augmented VLM framework for multimodal
  melanoma diagnosis using dermoscopic images and clinical metadata. The method retrieves
  semantically similar patient cases and incorporates them into VLM prompts, enabling
  accurate classification without fine-tuning.
---

# Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis

## Quick Facts
- arXiv ID: 2509.08338
- Source URL: https://arxiv.org/abs/2509.08338
- Reference count: 24
- Primary result: F1-score of 0.6864 on SIIM-ISIC 2019 dataset using retrieval-augmented VLM without fine-tuning

## Executive Summary
This paper proposes a retrieval-augmented vision-language model (VLM) framework for multimodal melanoma diagnosis that leverages dermoscopic images and clinical metadata. The method retrieves semantically similar patient cases and incorporates them into VLM prompts, achieving accurate classification without fine-tuning. Experiments on the SIIM-ISIC 2019 dataset show significant improvements over conventional baselines, with the framework effectively correcting false positives and false negatives by leveraging retrieved examples for contextual reasoning. The approach demonstrates strong potential as a generalizable strategy for clinical decision support using pretrained multimodal models.

## Method Summary
The framework uses ResNeXt-50 for image encoding and BERT for metadata text encoding, with embeddings concatenated and indexed in FAISS. For inference, it retrieves top-K (K=2 optimal) semantically similar cases and constructs prompts containing retrieved examples plus the query sample for a frozen LLaVA 7B v1.5 model. Clinical metadata is serialized as attribute-value pairs (e.g., "Age: 45, Sex: Female") to improve parsing and reduce prompt length. The approach enables few-shot reasoning through analogical comparison with retrieved cases, avoiding fine-tuning while achieving superior classification performance.

## Key Results
- Achieved F1-score of 0.6864, outperforming early-fusion models by 0.2099 and zero-shot VLMs by 0.3135
- Attribute-value metadata serialization yielded 24.57% FP and 7.07% FN recovery improvements over sentence format
- ResNeXt-50 encoder provided better retrieval alignment than EfficientNet-V2-M (82.12% FP, 42.65% FN recovery rates)
- K=2 retrievals optimal; K>2 degraded performance due to noise

## Why This Works (Mechanism)

### Mechanism 1: Retrieval-Augmented Context Injection
The framework retrieves semantically similar cases and embeds them into VLM prompts to improve melanoma classification accuracy over zero-shot inference. Retrieved image-metadata-label triplets provide domain-specific exemplars that anchor the VLM's reasoning to clinically relevant patterns, enabling analogical comparison similar to physician decision-making. Core assumption: the VLM can effectively exploit few-shot exemplars without parameter updates, and retrieved cases are sufficiently similar to the query to provide useful signal rather than noise. Evidence shows improvement of 0.2099 over early-fusion and 0.3135 over zero-shot VLM. Break condition: if retrieved cases are dissimilar (poor embedding alignment) or K is too large, noise may overwhelm signal.

### Mechanism 2: Structured Metadata Serialization Improves Retrieval and Inference
Attribute-value pair formatting of clinical metadata yields better classification than HTML or sentence formats. Compact key-value pairs reduce prompt length while explicitly encoding semantic roles of each variable, improving both embedding quality during indexing and VLM parsing during inference. Core assumption: text encoder and VLM can more effectively parse and attend to structured attribute-value representations than verbose or tag-heavy formats. Evidence shows attribute-value input encodes clinical variables more explicitly with 24.57% FP and 7.07% FN improvements. Break condition: if metadata fields increase substantially, prompt length may exceed context window.

### Mechanism 3: Visual Encoder Quality Governs Retrieval Alignment
ResNeXt-50 provides better visual features for retrieval-augmented diagnosis than EfficientNet-V2-M in this framework. Higher-quality image embeddings yield more semantically consistent nearest-neighbor retrieval, producing exemplars that align with query lesions in both visual and clinical dimensions. Core assumption: visual similarity in embedding space correlates with clinical relevance; better retrieval directly translates to better VLM context. Evidence shows ResNeXt-50 captures visual features relevant to melanoma classification more effectively. Break condition: if image domain shifts substantially, encoder advantages may not transfer without re-evaluation.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG):** Why needed here: Understanding how external knowledge retrieval augments frozen models without weight updates is essential to grasp why this approach avoids fine-tuning. Quick check question: Can you explain why retrieving exemplars at inference time differs from training on those same exemplars?

- **Vision-Language Models (VLMs):** Why needed here: The framework relies on LLaVA's ability to process interleaved image-text prompts; understanding VLM architecture clarifies why few-shot prompting works. Quick check question: What component in a VLM enables joint reasoning over images and text in a single forward pass?

- **Vector Similarity Search (FAISS):** Why needed here: The framework indexes 16,756 multimodal embeddings and performs approximate nearest neighbor search; understanding FAISS operations is critical for implementation. Quick check question: Why might approximate nearest neighbor search be preferred over exact search in a clinical retrieval system?

## Architecture Onboarding

- **Component map:** Image encoder (ResNeXt-50) -> Text encoder (BERT) -> Concatenation -> FAISS index -> Retrieval (top-K) -> VLM (LLaVA 7B v1.5) -> Prompt construction -> Classification output

- **Critical path:** 1) Offline: Serialize all training metadata to attribute-value format → encode with BERT + image encoder → concatenate → index in FAISS 2) Inference: Encode query (image + metadata) → retrieve top-2 neighbors → construct prompt with retrieved examples → VLM generates classification

- **Design tradeoffs:** K selection: K=2 balances context richness vs. noise; K<2 lacks exemplar diversity, K>2 introduces irrelevant cases. Serialization: Attribute-value is compact but requires template design; HTML preserves structure but obscures features; sentence matches VLM training but increases length. Encoder choice: ResNeXt-50 yields better retrieval alignment; EfficientNet-V2-M may be preferable if computational efficiency is critical

- **Failure signatures:** Low sensitivity despite high precision: Retrieved cases may be visually similar but clinically mismatched (check encoder alignment). Inconsistent predictions across similar queries: FAISS index may be corrupted or K too large. VLM ignoring retrieved examples: Prompt construction may exceed context window or format is misaligned with VLM expectations

- **First 3 experiments:** 1) Replicate baseline comparison: Run zero-shot LLaVA vs. RAG-augmented LLaVA (K=2) on held-out test set; expect F1 improvement of ~0.31 as reported 2) Ablate serialization format: Compare attribute-value vs. HTML vs. sentence on same retrieval configuration; expect attribute-value to outperform 3) Sweep K values: Test K=1,2,3,4 and plot F1-score; expect peak at K=2 with degradation at higher K

## Open Questions the Paper Calls Out
- Can the retrieval-augmented framework maintain performance improvements when applied to multi-class skin lesion classification tasks? The paper notes "Future work may expand this approach to multi-class skin lesion classification and other domains," but current study evaluates only binary classification.
- Does the effectiveness of the retrieval-augmented prompting strategy generalize across different VLM architectures? The authors note "reliance on a single VLM may limit generalizability across diverse diagnostic tasks," but all findings are based on LLaVA 7B v1.5.
- Does the proposed framework improve the diagnostic accuracy or confidence of human clinicians in a real-world setting? The paper claims the framework provides "clinical decision support," but evaluates performance solely through computational metrics without involving human experts.

## Limitations
- Prompt template specifics are unspecified, making exact replication challenging; slight variations in instruction phrasing could affect VLM reasoning and reported F1-score.
- Serialization format choice is empirical with no ablation of attribute-value templates; different key orderings or metadata inclusion may alter retrieval quality.
- Encoder fusion method is undefined, leaving ambiguity about vector normalization and weighting that could impact nearest-neighbor relevance.
- Dataset access is required for SIIM-ISIC 2019; without it, independent validation is impossible.

## Confidence
- High confidence: Retrieval-augmented context improves melanoma classification over zero-shot VLMs (mechanistic plausibility supported by analogical reasoning literature; empirical F1 gain of ~0.31 is substantial)
- Medium confidence: Attribute-value serialization yields better results than HTML/sentence formats (empirical but lacks direct ablation studies; format benefits may not generalize)
- Medium confidence: ResNeXt-50 outperforms EfficientNet-V2-M for visual embeddings (encoder comparison is empirical; no ablation of other backbones)

## Next Checks
1. Replicate baseline comparison: Run zero-shot LLaVA vs. RAG-augmented LLaVA (K=2) on held-out test set; expect F1 improvement of ~0.31 as reported
2. Serialization format ablation: Compare attribute-value vs. HTML vs. sentence on same retrieval configuration; expect attribute-value to outperform
3. K-value sweep: Test K=1,2,3,4 and plot F1-score; expect peak at K=2 with degradation at higher K