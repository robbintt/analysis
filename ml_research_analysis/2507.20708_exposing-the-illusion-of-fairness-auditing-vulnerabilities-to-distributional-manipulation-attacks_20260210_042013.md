---
ver: rpa2
title: 'Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional
  Manipulation Attacks'
arxiv_id: '2507.20708'
source_url: https://arxiv.org/abs/2507.20708
tags:
- fairness
- dataset
- methods
- tests
- sample
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how data samples can be manipulated to
  artificially satisfy fairness criteria, specifically Disparate Impact, while remaining
  statistically indistinguishable from the original distribution. The authors introduce
  methods based on entropic and optimal transport projections to create minimally
  perturbed datasets that meet prescribed fairness constraints.
---

# Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks

## Quick Facts
- **arXiv ID:** 2507.20708
- **Source URL:** https://arxiv.org/abs/2507.20708
- **Reference count:** 40
- **Primary result:** Methods exist to manipulate datasets to satisfy fairness criteria while remaining statistically indistinguishable from original distributions, evading standard fairness audits.

## Executive Summary
This paper reveals critical vulnerabilities in current fairness auditing frameworks by demonstrating how data samples can be manipulated to artificially satisfy fairness criteria, specifically Disparate Impact, while remaining statistically indistinguishable from the original distribution. The authors introduce three main manipulation methods—entropic projection, Wasserstein-based matching, and gradient-based optimization—that can improve fairness metrics without detection by standard statistical tests. Experiments on seven benchmark datasets show that certain methods can significantly improve fairness metrics while evading detection, highlighting the need for more robust fairness auditing approaches that account for potential adversarial manipulation.

## Method Summary
The paper investigates data manipulation attacks against fairness audits using three main approaches: (1) Entropic projection reweights samples via convex optimization to minimize KL divergence while enforcing target fairness constraints, (2) Wasserstein-minimizing matching iteratively transports individuals toward existing data points to improve fairness while maintaining distributional similarity, and (3) Gradient-based methods require access to the classifier's gradients to directly optimize fairness. The experiments use 7 tabular datasets with a multilayer perceptron classifier trained using ScheduleFree optimizer, testing 8 manipulation methods against 7 statistical tests (Wasserstein, MMD, KL, KS) applied at 10% and 20% sampling rates.

## Key Results
- Entropic and MW(X,S,Ŷ) methods can improve Disparate Impact to 0.54/0.42 undetected at 10% sampling for certain datasets (INC, TRA, BAF).
- Grad methods are easily detected via KL tests due to support expansion (KL=∞), while MW achieves the best KL-Wasserstein trade-off.
- Detection rates increase sharply with sample size (10% → 20%), with PUC showing no undetected improvement at 20% sampling.
- Sequential application of 7 statistical tests provides more robust detection than single tests, though MMD tests contribute minimally to overall detection rates.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auditees can construct minimally perturbed datasets that satisfy fairness thresholds by reweighting individual samples to minimize KL divergence while enforcing target Disparate Impact.
- Mechanism: Entropic projection solves a convex optimization (Eq. 2) yielding a reweighted empirical distribution Qt with weights λᵢ = exp(⟨ξ(t), Φ(Zᵢ)⟩ - log Z(ξ(t))) (Eq. 4). The constraint function Φ encodes the joint (S, Ŷ) statistics needed to shift DI by specified δ₀, δ₁.
- Core assumption: The empirical covariance matrix is invertible; target t is a convex combination of observed Φ values (Theorem 4.1 conditions).
- Evidence anchors:
  - [abstract] "creating minimally perturbed datasets that remain statistically indistinguishable from the original distribution while satisfying prescribed fairness constraints"
  - [Section 4.2, Proposition 4.2] Derives balanced vs. proportional parameter choices for DI manipulation.
  - [corpus] Weak direct corpus support for entropic projection in fairness; related work on adversarial attacks in bandits (arXiv:2505.21938) but different threat model.
- Break condition: If target DI is too far from original, KL(Qₜ, Qₙ) → ∞; gradient-based methods creating synthetic individuals also yield KL = ∞ (Appendix B.1), making detection trivial via KL-based tests.

### Mechanism 2
- Claim: Wasserstein-minimizing matching (MW(X,S,Ŷ)) achieves the best trade-off between DI improvement and distributional detectability by transporting individuals toward existing data points rather than creating synthetic ones.
- Mechanism: Iteratively selects τᵢ₀ maximizing (DI(τᵢ₀(Zʲ)) - DI(Zʲ)) / ‖τᵢ₀(Zʲ) - Zʲ‖ (Eq. 15 variant), preferring swaps among the 4 admissible (S,Ŷ) bins that increase numerator (Ŷ=1,S=0) and/or decrease denominator (Ŷ=0,S=1).
- Core assumption: The classifier's predictions on transported points remain consistent; discrete covariates can be matched exactly via 1D-transport variant.
- Evidence anchors:
  - [Section 5.4] "transports individuals towards others...therefore...it can be used in any type of audit"
  - [Figure 3 radar chart] MW(X,S,Ŷ) achieves smallest surface area across distance metrics.
  - [corpus] arXiv:2505.21938 (fake data injection in bandits) shares conceptual similarity but targets different domain.
- Break condition: When sample size increases (10% → 20%), detection rate rises sharply (Table 4, Figure 9); PUC dataset shows no undetected improvement at 20% sampling.

### Mechanism 3
- Claim: Combining multiple statistical tests with different geometric assumptions increases detection robustness, though tests vary in sensitivity and computational cost.
- Mechanism: Seven tests evaluate H₀: sample ≈ original distribution—two Wasserstein-based (full vs. (S,Ŷ) only), two MMD-based, two KL-based, and Kolmogorov–Smirnov. Tests are applied sequentially; rejection on any test fails the sample.
- Core assumption: The supervisory authority has access to the full dataset to estimate ground-truth distribution Qₙ for comparison.
- Evidence anchors:
  - [Section 6.3] Describes two testing categories: hypothesis tests (KS, Wasserstein) and divergence-based confidence intervals.
  - [Table 3] Shows Grad methods detected at both sample sizes (✕), while MW and Entropic methods pass (–) for INC, TRA, BAF.
  - [corpus] arXiv:2601.20269 (Empirical Likelihood-Based Fairness Auditing) proposes distribution-free certification—complementary but not directly evaluated here.
- Break condition: Tests fail to detect when: (1) original DI already near threshold (minimal modification needed), (2) P(Ŷ=1) is very low (e.g., BAF at 1%), or (3) only single test is used rather than combination.

## Foundational Learning

- Concept: Disparate Impact (DI) = P(Ŷ=1|S=0) / P(Ŷ=1|S=1)
  - Why needed here: Central fairness metric being manipulated; understanding its decomposition into λ₀, λ₁, n₀, n₁ is essential for constructing attacks.
  - Quick check question: If n₀=100, n₁=400, λ₀=30, λ₁=200, what is DI and by how much must λ₀ increase to reach DI=0.8?

- Concept: Kullback-Leibler divergence vs. Wasserstein distance
  - Why needed here: These metrics quantify "how much" the distribution shifted—KL captures information-theoretic dissimilarity (undefined if support differs), Wasserstein captures geometric displacement.
  - Quick check question: Why does KL(Qₙ, Qₜ) = ∞ for methods that create synthetic individuals, while W(Qₙ, Qₜ) remains finite?

- Concept: Optimal Transport / Monge-Kantorovich projection
  - Why needed here: Foundation for Theorem 5.1; understanding push-forward T_λ#Qₙ and the role of Lagrange multipliers in constrained optimization.
  - Quick check question: In Eq. 11, how does increasing λ affect the trade-off between minimizing ‖Zᵢ - x‖² and satisfying the fairness constraint?

## Architecture Onboarding

- Component map: Fairness constraint module -> Distribution projection module -> Statistical test suite -> Descriptor embedding layer
- Critical path:
  1. Load original dataset Qₙ, compute baseline DI
  2. Select manipulation method based on audit context (model access → Grad variants; no model access → Replace/MW)
  3. Optimize for target DI while tracking d(Qₙ, Qₜ) across metrics
  4. Draw samples at 10%/20% size; run through all 7 tests
  5. If any test rejects, either: (a) reduce target DI, or (b) increase sample size requirement

- Design tradeoffs:
  - Entropic methods: O(n) memory, fast, but higher Wasserstein shift
  - MW matching: O(n²) distance matrix, best KL-W trade-off, 3–10 min runtime
  - Grad methods: Require gradient access to classifier, easily detected via KL tests (KL=∞)
  - Sample size: Larger samples harder to manipulate undetectably but increase audit burden

- Failure signatures:
  - KL(Qₙ, Qₜ) = ∞ → Method creates out-of-support points (Grad, synthetic methods)
  - H₀ rejected at 10% but accepted at 20% → Insufficient statistical power; need larger audit sample
  - High DI achieved but W(X,S,Ŷ) > 0.3 → Likely detected by Wasserstein test (Table 12 thresholds)

- First 3 experiments:
  1. Replicate Table 4 for Adult dataset: Run all 8 manipulation methods with DI target = 0.8, test at 10%/20% sampling, confirm MW(X,S,Ŷ) achieves highest undetected DI (0.54/0.42).
  2. Ablation on test combinations: Remove MMD tests (Table 16), measure change in detection rates—expect minimal impact per Appendix J.
  3. Sample size sensitivity: Plot Figure 9 curve for Adult using MW and Entropic methods—confirm undetected DI decreases monotonically as sample size increases from 5% to 30%.

## Open Questions the Paper Calls Out
None

## Limitations
- The Entropic and MW(X,S,Ŷ) methods only achieve undetectable fairness improvements in certain datasets (INC, TRA, BAF) while failing on others (Adult, MOB, EMP, PUC) at 20% sampling.
- The paper does not provide clear theoretical explanations for why these methods work in some domains but not others.
- While vulnerabilities are demonstrated, the paper does not propose comprehensive solutions or evaluate alternative fairness certification approaches.

## Confidence
- **High confidence**: The core demonstration that fairness metrics can be manipulated while maintaining statistical indistinguishability from the original distribution. The experimental results showing differential detection rates across methods and datasets are well-supported.
- **Medium confidence**: The characterization of which datasets are vulnerable to undetected manipulation. While results are presented, the paper lacks theoretical grounding for predicting vulnerability based on dataset characteristics.
- **Low confidence**: The practical implications for real-world auditing scenarios, as the paper focuses on controlled experimental conditions without addressing implementation challenges in actual regulatory contexts.

## Next Checks
1. **Dataset vulnerability prediction**: Conduct additional experiments to identify dataset characteristics (e.g., class balance, feature correlation structure, DI baseline distance from target) that predict successful undetected manipulation. This would validate whether the observed patterns in Table 4 are systematic or dataset-specific artifacts.

2. **Alternative certification robustness**: Test whether the Empirical Likelihood-Based Fairness Auditing approach from arXiv:2601.20269 can detect the Entropic and MW(X,S,Ŷ) manipulations that evade current statistical tests. This would validate whether the vulnerabilities are specific to the tested methods or represent a broader class of attacks.

3. **Real-world audit simulation**: Implement a simulation where auditors apply these statistical tests sequentially (as described) to samples drawn from manipulated datasets, measuring false positive rates on genuinely fair data versus false negative rates on manipulated data. This would validate whether the proposed sequential testing framework achieves acceptable trade-offs in practice.