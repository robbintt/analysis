---
ver: rpa2
title: Toward Cybersecurity-Expert Small Language Models
arxiv_id: '2510.14113'
source_url: https://arxiv.org/abs/2510.14113
tags:
- security
- cybersecurity
- evaluation
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CyberPal 2.0 introduces a family of cybersecurity-expert small
  language models (SLMs) ranging from 4B-20B parameters. The approach uses an enriched
  chain-of-thought cybersecurity instruction dataset built with a data enrichment
  and formatting pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering
  of reasoning formats alongside LLM-driven multi-step grounding.
---

# Toward Cybersecurity-Expert Small Language Models

## Quick Facts
- arXiv ID: 2510.14113
- Source URL: https://arxiv.org/abs/2510.14113
- Reference count: 40
- Introduces CyberPal 2.0, a family of 4B-20B parameter cybersecurity-expert SLMs that outperform larger frontier models on core cyber threat intelligence tasks

## Executive Summary
CyberPal 2.0 introduces a family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B parameters. The approach uses an enriched chain-of-thought cybersecurity instruction dataset built with a data enrichment and formatting pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of reasoning formats alongside LLM-driven multi-step grounding. Across diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its baselines and matches or surpasses various open and closed-source frontier models while remaining a fraction of their size.

## Method Summary
The CyberPal 2.0 methodology centers on SecKnowledge 2.0, an enriched chain-of-thought cybersecurity instruction dataset. The pipeline combines expert-in-the-loop steering of reasoning formats with LLM-driven multi-step grounding to create high-quality training data. The models are fine-tuned from various checkpoints (base and post-trained) across parameter sizes from 4B to 20B. The approach emphasizes format consistency and structured reasoning to enhance cybersecurity domain expertise while maintaining computational efficiency.

## Key Results
- On core cyber threat intelligence knowledge tasks, CyberPal 2.0 models outperform almost all tested frontier models, ranking second only to Sec-Gemini v1
- On core threat-investigation tasks, the best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1, ranking first
- The smallest 4B-parameter model ranks second on threat-investigation tasks while being orders of magnitude smaller than competing frontier models

## Why This Works (Mechanism)
The methodology leverages enriched chain-of-thought reasoning through structured format definitions and multi-step grounding. Expert-in-the-loop steering ensures domain-appropriate reasoning patterns while LLM automation scales data generation. The approach systematically breaks down complex cybersecurity reasoning into manageable steps, improving model performance on specialized tasks without requiring massive parameter counts.

## Foundational Learning
- **Chain-of-thought reasoning**: Structured step-by-step problem solving improves complex task performance by breaking down reasoning into manageable components. Needed to handle sophisticated cybersecurity analysis requiring multi-step inference.
- **Domain-specific instruction tuning**: Specialized cybersecurity datasets enhance model expertise in security operations contexts. Quick check: Compare performance on cybersecurity vs. general benchmarks.
- **Expert-in-the-loop training**: Human expertise guides format definition and reasoning schema development. Quick check: Measure performance impact of expert vs. fully automated format generation.
- **Model scaling tradeoffs**: Smaller models with specialized training can match or exceed larger general models on domain-specific tasks. Quick check: Compare performance-per-parameter across different model sizes.

## Architecture Onboarding
- **Component map**: SecKnowledge 2.0 pipeline -> Data enrichment and formatting -> Fine-tuning (4B-20B parameters) -> Benchmark evaluation
- **Critical path**: Dataset creation → Format definition (expert-in-the-loop) → LLM grounding → Model fine-tuning → Performance validation
- **Design tradeoffs**: Small parameter count vs. domain expertise (specialization over scale), automated data generation vs. expert verification (efficiency vs. quality), static benchmarks vs. dynamic operations (evaluation vs. deployment readiness)
- **Failure signatures**: Poor performance on novel threat types, inability to generalize beyond training formats, failure to execute multi-step reasoning in dynamic scenarios
- **First experiments**: 1) Benchmark CyberPal 2.0 against baseline models on CTIBench and SecEval, 2) Test model generalization to unseen cybersecurity scenarios, 3) Evaluate reasoning consistency across different format variations

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do data quality and data scale specifically interact with the choice of starting checkpoint (base vs. post-trained) when fine-tuning cybersecurity models?
- Basis in paper: [explicit] Appendix B notes that while base models learn more effectively, "further work is needed to systematically disentangle how data quality and data scale interact with the choice of starting checkpoint during fine-tuning."
- Why unresolved: The paper empirically observes that base models yield better results but does not determine if this holds true for lower-quality data or smaller dataset scales.
- What evidence would resolve it: Ablation studies comparing base and post-trained checkpoints across varying dataset sizes and quality tiers to map performance boundaries.

### Open Question 2
- Question: Can the static benchmark performance of CyberPal 2.0 translate into effective, autonomous agentic capabilities for live security operations?
- Basis in paper: [inferred] The Introduction claims the goal is a model that encompasses the "entire security operations loop" including response actions, yet the evaluation relies exclusively on static Q&A and classification benchmarks (e.g., CTIBench, SecEval).
- Why unresolved: High accuracy on knowledge retrieval and root-cause mapping does not guarantee the model can successfully execute multi-step actions or use tools in a dynamic environment.
- What evidence would resolve it: Evaluation results from a dynamic cyber range or agent simulation where the model must interact with live systems to detect and remediate threats.

### Open Question 3
- Question: Can the expert-in-the-loop schema definition be fully automated to reduce the reliance on scarce human expertise without degrading reasoning fidelity?
- Basis in paper: [inferred] The methodology describes the "expert-in-the-loop system" as critical for defining formats, but also notes that acquiring domain expertise is "scarce and costly."
- Why unresolved: The paper does not test if the LLM-generated formats could be used without human verification, leaving the trade-off between automation and format precision unexplored.
- What evidence would resolve it: A comparison of model performance when trained on datasets generated with fully automated formats versus expert-verified formats.

## Limitations
- The evaluation methodology relies heavily on static benchmarks that may not capture real-world cybersecurity task complexity or generalization requirements
- The instruction dataset SecKnowledge 2.0 is not publicly available, preventing independent verification of the data enrichment and formatting pipeline
- The paper does not address potential safety concerns or adversarial vulnerabilities specific to cybersecurity applications, which are critical for deployment in sensitive environments

## Confidence
- Model performance claims (High confidence): The benchmark results and comparisons are presented with specific metrics and clear relative rankings. The consistent outperformance of baseline models across multiple parameter sizes provides strong internal validation.
- Methodology transparency (Medium confidence): While the general approach is described, key technical details about the data enrichment pipeline and expert-in-the-loop process are insufficiently detailed for full reproducibility.
- Real-world applicability claims (Low confidence): The paper makes strong claims about cybersecurity expertise but lacks evaluation on actual security operations tasks or integration with existing security tools and workflows.

## Next Checks
1. Independent replication of CyberPal 2.0 training using the described methodology on publicly available cybersecurity datasets to verify the claimed performance improvements.
2. Evaluation of model outputs on realistic cybersecurity incident response scenarios involving time-sensitive decision-making and integration with existing security tools.
3. Security audit of CyberPal 2.0 for potential vulnerabilities including prompt injection attacks, adversarial examples, and unintended information disclosure in cybersecurity contexts.