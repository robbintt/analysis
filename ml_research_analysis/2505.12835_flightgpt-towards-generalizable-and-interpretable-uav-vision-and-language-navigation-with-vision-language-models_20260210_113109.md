---
ver: rpa2
title: 'FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language
  Navigation with Vision-Language Models'
arxiv_id: '2505.12835'
source_url: https://arxiv.org/abs/2505.12835
tags:
- reasoning
- target
- wang
- zhang
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FlightGPT, a UAV vision-and-language navigation
  system that leverages vision-language models to address key challenges in multimodal
  fusion, generalization, and interpretability. The proposed two-stage training pipeline
  combines supervised fine-tuning on high-quality demonstrations with reinforcement
  learning guided by composite rewards for goal accuracy, reasoning quality, and format
  compliance.
---

# FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models

## Quick Facts
- **arXiv ID:** 2505.12835
- **Source URL:** https://arxiv.org/abs/2505.12835
- **Reference count:** 34
- **Primary result:** 9.22% higher success rate than strongest baseline on unseen environments

## Executive Summary
FlightGPT introduces a vision-language model-based approach for UAV vision-and-language navigation (VLN) that addresses key challenges in multimodal fusion, generalization, and interpretability. The system employs a two-stage training pipeline combining supervised fine-tuning with reinforcement learning guided by composite rewards. FlightGPT achieves state-of-the-art performance on the CityNav benchmark, demonstrating strong generalization capabilities with a 9.22% higher success rate than the strongest baseline in unseen environments.

## Method Summary
FlightGPT uses a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) with high-quality Chain-of-Thought demonstrations generated by a larger model (Qwen2.5-VL-32B) to teach a smaller student model (Qwen2.5-VL-7B) the reasoning format; second, Reinforcement Learning with Group Relative Policy Optimization (GRPO) using composite rewards for goal accuracy, reasoning quality, and format compliance. The system outputs target coordinates rather than direct actions, decoupling semantic understanding from control planning. The model operates on semantic maps with current UAV position, heading, and landmarks, using Chain-of-Thought reasoning to identify landmarks before predicting target locations.

## Key Results
- Achieves 9.22% higher success rate than strongest baseline on test-unseen environments
- Outperforms larger models (Qwen2.5-VL-32B) in zero-shot evaluation
- Demonstrates strong generalization with 21.20% success rate on unseen test environments

## Why This Works (Mechanism)

### Mechanism 1: SFT-Driven Policy Warm-up for RL Stability
SFT initializes the VLM with structured reasoning patterns before RL exploration, creating a policy network that understands output syntax and basic task logic. This stabilizes the subsequent RL phase by establishing a baseline reasoning structure. The ablation study shows RL-only suffers from low initial success rates and slower convergence compared to SFT+RL.

### Mechanism 2: Composite Reward for Reasoning Alignment
The GRPO reward function combines Goal Accuracy, Intermediate Reasoning (IoU), and Format compliance to force the model to align its internal reasoning process with physical constraints. This compels the VLM to attend to visual features (landmarks) in the semantic map as a prerequisite to predicting the target location.

### Mechanism 3: Modularity via Target Prediction vs. Action Planning
Decoupling high-level semantic understanding from low-level control allows the VLM to focus on multimodal grounding while a separate mechanism handles trajectory execution. This reduces the VLM's action space complexity and enables the model to output coordinates rather than discrete commands.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed here: Standard RL can be unstable for VLMs; GRPO handles group-based rewards or relative advantages differently than standard PPO
  - Quick check question: How does the variance of the reward signal in GRPO compare to standard RLHF when using batch sizes of 1?

- **Concept: Chain-of-Thought (CoT) Distillation**
  - Why needed here: SFT relies on a larger model teaching a smaller one; understanding distillation is key to evaluating if reasoning quality is genuinely learned or merely mimicked
  - Quick check question: If the 32B teacher fails to identify a landmark visually but guesses the correct location, what reasoning behavior will the 7B student inherit?

- **Concept: Semantic Map Representation**
  - Why needed here: The VLM sees annotated semantic maps, not raw video; understanding how spatial metadata is encoded into image tokens is crucial
  - Quick check question: Does the model ingest the map as a single image token, or does the prompt template explicitly bind text coordinates to visual features?

## Architecture Onboarding

- **Component map:** Qwen2.5-VL-7B (Backbone) -> LLaMA-Factory (SFT) -> VLM-R1 (RL/GRPO) -> Look-ahead Planner (External)

- **Critical path:** Data Curation (Teacher model → JSON filtering → Training Set) → SFT Warm-up (7B training on filtered reasoning chains) → RL Optimization (GRPO with composite reward on 4,758 samples)

- **Design tradeoffs:** 7B vs 32B parameter tradeoff (training compute vs inference efficiency); simulation vs reality dependency (CityNav simulator with privileged semantic map input)

- **Failure signatures:** Format Collapse (outputs plain text instead of JSON); Reasoning Drift (short, fragmented blocks); IoU/Goal Mismatch (correct landmark but wrong specific coordinate)

- **First 3 experiments:**
  1. Validate the data filter by running the Teacher model on held-out set to verify the 20-meter filter removes hallucinations vs hard examples
  2. Ablate the intermediate reward by training without R_IoU to quantify its contribution to the 9.22% generalization gain
  3. Visualize attention maps during inference to confirm the VLM is actually looking at red-masked landmarks mentioned in its CoT output

## Open Questions the Paper Calls Out

### Open Question 1
How does FlightGPT perform under real-world uncertainties such as GPS drift, weather disturbances, and dynamic obstacles compared to the high-fidelity CityNav simulator? The authors acknowledge a significant gap between simulation and reality, noting that performance in real-world settings remains unverified.

### Open Question 2
Can FlightGPT meet the strict inference latency and memory constraints required for deployment on resource-constrained edge devices? The authors acknowledge a lack of systematic evaluation of deployment feasibility, noting that inference latency and computational resource demands have not been quantified.

### Open Question 3
How can the framework be extended to bridge the gap with human navigation intelligence in complex scenarios involving ambiguous expressions or multi-turn instructions? The authors highlight a substantial gap compared to human navigation abilities, specifically noting struggles with ambiguous expressions and commonsense reasoning.

### Open Question 4
Can the training pipeline be adapted to remove the dependency on expensive ground-truth landmark bounding boxes for the Intermediate Reasoning Reward? The current RL stage requires ground-truth bounding boxes that may not be available in new environments, potentially limiting scalability.

## Limitations
- Significant gap between simulation and reality, with unmodeled factors like GPS drift and weather
- Lack of systematic evaluation of deployment feasibility on resource-constrained edge devices
- Substantial gap compared to human navigation abilities in handling ambiguous expressions and implicit goals

## Confidence

- **High Confidence:** The two-stage training methodology (SFT + RL) is well-documented and the ablation study convincingly demonstrates that SFT initialization improves RL stability and convergence speed
- **Medium Confidence:** The Chain-of-Thought reasoning mechanism improves interpretability, but evaluation metrics are self-reported without comparison to established benchmarks
- **Low Confidence:** The claim that the 7B model "outperforms larger models" is qualified by noting it only surpasses a zero-shot 32B model, not a fine-tuned one

## Next Checks

1. **Generalization Test:** Evaluate FlightGPT on a real-world UAV dataset or transfer it to a different simulation environment (e.g., from urban to rural settings) to verify the claimed robustness to environmental changes

2. **Ablation of Reward Components:** Systematically remove R_IoU and R_format from the reward function to quantify their individual contributions to the 9.22% performance gain

3. **Failure Mode Analysis:** Conduct a detailed analysis of failed trajectories to identify whether errors stem from semantic misunderstanding, landmark misidentification, or limitations in the look-ahead action planner