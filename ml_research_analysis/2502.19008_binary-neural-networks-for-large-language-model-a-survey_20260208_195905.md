---
ver: rpa2
title: 'Binary Neural Networks for Large Language Model: A Survey'
arxiv_id: '2502.19008'
source_url: https://arxiv.org/abs/2502.19008
tags:
- bitnet
- quantization
- arxiv
- language
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of binary neural networks
  for large language models (LLMs), focusing on the emerging 1-bit quantization techniques
  that differ from traditional PTQ and QAT methods. The paper reviews the evolution
  of binarization from convolutional neural networks to LLMs, detailing methods such
  as BitNet, BitNet b1.58, FBI-LLM, and Bi-Mamba.
---

# Binary Neural Networks for Large Language Model: A Survey

## Quick Facts
- arXiv ID: 2502.19008
- Source URL: https://arxiv.org/abs/2502.19008
- Reference count: 40
- Primary result: Comprehensive survey of binary neural networks for LLMs, focusing on from-scratch training with 1-bit quantization techniques that outperform traditional PTQ/QAT at extreme compression rates.

## Executive Summary
This paper provides a comprehensive survey of binary neural networks (BNNs) for large language models (LLMs), focusing on emerging 1-bit quantization techniques that differ fundamentally from traditional post-training quantization (PTQ) and quantization-aware training (QAT) methods. The survey traces the evolution from convolutional neural networks to LLMs, detailing methods like BitNet, BitNet b1.58, FBI-LLM, and Bi-Mamba. It covers optimization strategies for weight and activation quantization, KV cache quantization, loss functions, and training techniques including gradient calculation methods and mixed precision training. The paper highlights the energy efficiency and memory savings achieved through binarization while discussing the key differences between BNN-LLM and BNN-CNN approaches.

## Method Summary
The survey describes training LLMs from scratch using 1-bit or 1.58-bit weights through a combination of sign-based binarization, straight-through estimator (STE) for gradient flow, and autoregressive distillation for convergence. The method replaces standard linear layers with BitLinear variants that binarize weights using the Sign function with scaling factors, while activations are quantized using Absmax to 8-bit precision. KV cache quantization is applied at 3-8 bits for memory efficiency. Training employs mixed precision (FP16/BF16) for gradients, optimizer states, and critical components like embeddings and LayerNorm parameters, while weights are stored as binary values (+1, -1) or ternary values (-1, 0, +1).

## Key Results
- 1-bit quantization from scratch achieves better performance than PTQ/QAT at extreme compression rates (PPL < 1000 vs > 1000 at 2-bit)
- BitNet b1.58 offers a tradeoff between accuracy and compression, using ternary weights (-1, 0, +1) for improved expressivity
- FBI-LLM requires autoregressive distillation for stable convergence, with performance collapsing when distillation is removed
- BNN-LLMs achieve significant energy efficiency and memory savings while maintaining competitive accuracy
- KV cache quantization to 3-8 bits reduces memory footprint during long-sequence inference

## Why This Works (Mechanism)

### Mechanism 1: From-Scratch Binarization with Sign Function
Replacing standard linear layers with binarized variants trained from scratch preserves representational capacity better than PTQ at extreme compression rates. During training, full-precision weights are transformed into binary values (+1, -1) using the Sign(W - α) function with scaling factor β to recover magnitude information. This replaces expensive floating-point matrix multiplication with efficient addition operations.

### Mechanism 2: Straight-Through Estimator (STE) for Gradient Flow
Gradients can be effectively backpropagated through non-differentiable discrete sign functions using STE, which passes gradients through the backward pass as if the binarization operation were the identity function. This allows learning even though the Sign function has a derivative of zero almost everywhere.

### Mechanism 3: Distillation-Driven Convergence
Training binary models from scratch is stabilized by using a full-precision "teacher" model to guide the binary "student" through minimizing KL-divergence against the teacher's probability distribution. This transfers "dark knowledge" to help the binary model navigate the loss surface.

## Foundational Learning

- **Concept: Quantization-Aware Training (QAT) vs. From-Scratch**
  - Why needed: The paper explicitly differentiates BNN-LLM from standard QAT, where BNN-LLM initializes randomly and trains with binary weights from day one
  - Quick check: Does the method require a pre-trained FP16 checkpoint to start, or can it start from random initialization?

- **Concept: The Sign Function & Zero-Centering**
  - Why needed: Binarization maps values to +1 or -1, and if weights are not zero-centered, binarization introduces systematic bias
  - Quick check: Before applying Sign(), what transformation must be applied to the weight tensor to ensure the binary values are balanced?

- **Concept: Mixed Precision Training**
  - Why needed: While weights are 1-bit, BNN-LLMs do not binarize everything; gradients, optimizer states, and embeddings usually remain in high precision for stability
  - Quick check: Which three components of an LLM must typically remain in high precision even in a Binary LLM architecture? (Answer: Embeddings, LayerNorm parameters, and the Causal Head)

## Architecture Onboarding

- **Component map:** Input (FP16/BF16 Tokens) -> Embedding (High Precision) -> LayerNorm -> Absmax Quantization -> BitLinear (Binary Weights) -> Accumulate -> Dequantize -> Output -> Loss
- **Critical path:** Forward: Input -> LayerNorm -> Absmax Quantization (Act to Int8) -> Binarized Weights (Int1) -> Accumulate -> Dequantize -> Output. Backward: Loss -> STE (skips quantization logic, treats Sign as identity) -> Latent FP Weights Update -> Re-quantize (Sign) for next step
- **Design tradeoffs:** BitNet (1-bit) offers max compression but lower accuracy; BitNet b1.58 adds zero value (sparsity + expressivity) improving accuracy but slightly increasing compute/storage. Speed vs. Stability: BitNet b1.58 training is 60% slower than FP16 baseline due to RMSNorm and quantization overhead.
- **Failure signatures:** Non-convergence with Hard Tanh derivatives due to high curvature; performance collapse when distillation is removed; semantic destruction when embedding layer is binarized
- **First 3 experiments:** (1) Train small Llama-style model (100M params) in FP16 vs BitNet architecture and compare training loss curves. (2) Implement simple BNN block and compare convergence with Hard Tanh vs STE. (3) Implement KV cache quantization and measure memory footprint reduction during 4096 context length inference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the convergence of fully binarized LLMs be stabilized without relying on autoregressive distillation?
- Basis in paper: [explicit] FBI-LLM suffers significant performance degradation or fails to converge entirely if the distillation component is removed
- Why unresolved: Current dependency on full-precision teacher model restricts training flexibility and efficiency
- What evidence would resolve it: A training methodology for fully binarized LLMs that achieves stable convergence using standard one-hot label autoregressive loss without requiring a teacher model

### Open Question 2
- Question: How can binary or ternary quantization techniques be effectively adapted to non-Transformer architectures like Jamba and Mixture of Experts (MoE)?
- Basis in paper: [explicit] Section 6.2 identifies lack of research on binarization in Jamba domain and suggests applying 1-bit quantization to MoE models as future direction
- Why unresolved: Current BNN-LLM research focuses predominantly on standard Transformer architectures
- What evidence would resolve it: Successful implementation of binarized Jamba/MoE model maintaining competitive accuracy and throughput

### Open Question 3
- Question: What optimization strategies are required to reduce the high training latency currently observed in BitNet b1.58?
- Basis in paper: [explicit] BitNet b1.58 training is 60% slower than baseline, making it relatively costly despite inference efficiency
- Why unresolved: Computational overhead during training phase negates some efficiency benefits
- What evidence would resolve it: Optimized training kernels or algorithms allowing 1.58-bit LLMs to train at speeds equal to or faster than FP16 baselines

## Limitations
- The survey lacks implementation details for critical hyperparameters including specific group sizes for quantization across different model scales
- Precise learning rate schedules beyond "high learning rate" guidance are not detailed
- Exact configurations for distillation loss weighting between teacher and ground truth signals are unspecified
- The paper doesn't systematically compare different distillation strategies across configurations

## Confidence
- **High Confidence:** Core mechanisms of STE for gradient flow and the fundamental tradeoff between 1-bit and 1.58-bit precision are well-supported by experimental evidence
- **Medium Confidence:** Claim that from-scratch binarization outperforms PTQ/QAT at extreme compression rates is supported by comparative results, though performance gaps may vary with architecture and dataset
- **Low Confidence:** Exact impact of different distillation strategies on convergence is mentioned but not systematically compared across configurations

## Next Checks
1. **Parameter Sensitivity Test:** Systematically vary the group size G for quantization across a range (2, 4, 8, 16) and measure impact on perplexity for a 100M parameter model
2. **Distillation Ablation Study:** Train the same BNN-LLM architecture with three configurations: (A) Full distillation, (B) No distillation, (C) Partial distillation (50% weight), measuring convergence speed and final accuracy
3. **Hardware Profiling:** Implement the BitLinear layer and measure actual speed improvements on target hardware versus theoretical FLOPs reduction, accounting for memory bandwidth limitations and quantization overhead