---
ver: rpa2
title: 'MPO: Multilingual Safety Alignment via Reward Gap Optimization'
arxiv_id: '2505.16869'
source_url: https://arxiv.org/abs/2505.16869
tags:
- safety
- multilingual
- language
- alignment
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of multilingual safety alignment
  in large language models (LLMs), where models often perform poorly in low-resource
  languages despite strong safety alignment in English. Existing approaches like DPO
  and RLHF struggle with noisy multilingual data and fail to transfer safety capabilities
  effectively.
---

# MPO: Multilingual Safety Alignment via Reward Gap Optimization

## Quick Facts
- **arXiv ID:** 2505.16869
- **Source URL:** https://arxiv.org/abs/2505.16869
- **Reference count:** 40
- **Primary result:** MPO reduces attack success rate from 98% to 7% in Swahili while preserving English safety alignment

## Executive Summary
MPO addresses the challenge of multilingual safety alignment in large language models, where safety performance degrades significantly in low-resource languages despite strong alignment in English. Traditional approaches like DPO and RLHF struggle with noisy multilingual data and fail to effectively transfer safety capabilities across languages. MPO introduces a novel approach that leverages the well-aligned safety capabilities of English as a high-quality supervision signal by directly minimizing the reward gap difference between English and target languages. Experiments on LLaMA-3.1, Gemma-2, and Qwen2.5 demonstrate consistent improvements across multilingual safety benchmarks, particularly in low-resource languages, while maintaining general multilingual utility.

## Method Summary
MPO is a two-component optimization method that fine-tunes LLMs for multilingual safety alignment by minimizing the difference between reward gaps in a well-aligned dominant language (English) and target languages. The method computes reward gaps using a SimPO-style formula with length normalization, comparing log-likelihood differences between safe and unsafe responses. A frozen reference model provides stable English reward gap targets, while the policy model is trained to match these targets in target languages. An additional L2 loss constrains hidden representations of the dominant language to prevent capability degradation during transfer.

## Key Results
- Reduces attack success rate from 98% to 7% in Swahili while improving across all tested languages
- Outperforms DPO, RLHF, and CAPO baselines on multilingual safety benchmarks (MultiJail, AdvBench-X, CSRT)
- Maintains or improves general multilingual utility (MT-Bench, M-MMLU, MGSM) while enhancing safety
- Shows robustness to varying data quality, with performance degrading less than DPO as translation quality decreases

## Why This Works (Mechanism)

### Mechanism 1: Reward Gap as a Proxy for Safety Performance
MPO observes that the implicit reward gap between safe and unsafe responses strongly correlates with multilingual safety performance (Attack Success Rate). By minimizing the difference β·RGt - RGd, the method forces the model's preference for safe responses in target languages to match the stronger preference established in English. This assumes the model's internal log-likelihood difference is a valid proxy for observable safety behavior across languages.

### Mechanism 2: Cross-Lingual Safety Capability Transfer via Reference Gap Anchoring
Instead of directly optimizing on noisy translated preference pairs, MPO uses the pre-aligned dominant language's reward gap from a fixed reference model as a stable target. This prevents reward hacking and noisy optimization common in standard DPO with translated multilingual data. The gradient update is weighted by the gap difference, providing stable instance-level supervision derived from high-quality English data.

### Mechanism 3: Preservation of Dominant Language via Representation Constraint
Constraining the hidden representations (at the last token) of the dominant language prevents catastrophic forgetting during the transfer process. This direct constraint on activation space is claimed to be more effective than KL-divergence on logits for behavior control, ensuring the model's safety strengths in the dominant language are preserved.

## Foundational Learning

**Direct Preference Optimization (DPO) & Implicit Reward** - Understanding how r(x,y) is re-parameterized from log π(y|x) is essential to grasp what a "Reward Gap" is and how it's manipulated. Quick check: Given the DPO reward formula r(x,y) = β log [πθ(y|x) / πref(y|x)] + β log Z(x), explain why Z(x) cancels out when computing the reward gap RG = r(x, yw) - r(x, yl).

**Cross-Lingual Transfer & Noisy Data Problem** - The core motivation for MPO is the failure of standard methods on "noisy multilingual data." One must understand why translated safety data is unreliable and how this breaks standard alignment. Quick check: Why would using a machine translation API to convert an English safety dataset into Swahili for DPO training potentially harm the model's safety alignment?

**Representation Engineering / Constraints** - The L2 loss component operates directly on the model's internal hidden states. Understanding the intuition behind controlling model behavior via its activation space is key to the architecture. Quick check: What is the proposed advantage of using a representation-based constraint (||hd - hd_ref||^2) over a KL-divergence constraint for preserving dominant language capabilities?

## Architecture Onboarding

**Component map:** Data Preparation -> Reference Model (πref) -> Policy Model (πθ) -> Loss Computation (L = L1 + L2) -> Model Update

**Critical path:** The most critical step is the correct computation of RGt and RGd using the SimPO-style formula with length normalization. An error here will invalidate the entire optimization.

**Design tradeoffs:**
- **Constraint Type:** Representation constraint (L2) vs. KL-divergence (L_KL). The paper argues for L2 based on empirical results showing it's less restrictive on output distribution but more direct on behavior.
- **Gap Computation:** SimPO-style (length-normalized) vs. DPO-style (unnormalized). The paper provides empirical and theoretical arguments for length normalization to counteract length bias of unsafe responses.
- **Reference for RG:** Using πref for RGd vs. πθ. Appendix H.3 shows using πθ leads to instability and worse performance.

**Failure signatures:**
- **Model Collapse / Utility Loss:** If L2 is too weak or omitted, safety alignment may succeed but general multilingual utility could degrade significantly.
- **Over-Optimization / Reward Hacking:** If β is set too high or RGd is unrealistically large, the model might learn to output safe-sounding but irrelevant refusals.
- **No Transfer / Stagnation:** If the backbone model has very weak foundational capabilities in the target language, MPO may show limited gains.

**First 3 experiments:**
1. Verify RG-ASR correlation: Before running full MPO, compute Reward Gap and Attack Success Rate for English and target languages to confirm the inverse relationship.
2. Ablation on constraint: Run MPO with and without the L2 retention term to validate the necessity of the preservation mechanism.
3. Robustness to data noise: Train MPO and a baseline DPO using preference data translated by models of varying quality and plot performance vs. data quality.

## Open Questions the Paper Calls Out
None

## Limitations
- **Language Coverage Gap:** Validation across six languages is heavily skewed toward Asian languages; performance on Romance/Germanic and additional low-resource languages remains unverified.
- **Quality Dependency:** MPO's effectiveness depends on the dominant language having well-aligned safety capabilities; if English safety is weak or biased, these weaknesses propagate to target languages.
- **Evaluation Scope:** Safety evaluation focuses on attack success rate but doesn't assess subtler alignment failures like biased refusals or cultural appropriateness of safety responses.

## Confidence

**High Confidence:** The inverse correlation between reward gap and attack success rate is empirically demonstrated across multiple models and languages. The preservation of dominant language capabilities through representation constraints is validated through ablation studies.

**Medium Confidence:** The cross-lingual transfer mechanism works as described for the specific language set tested, but generalization to other language families requires additional validation.

**Low Confidence:** Claims about universal safety concept transferability across all languages and cultures are theoretical assumptions not empirically validated.

## Next Checks

**Check 1:** Evaluate MPO on a broader set of languages including Romance (Spanish, French), Germanic (German, Dutch), and additional low-resource African languages to test universality of the RG-ASR relationship.

**Check 2:** Design evaluation prompts that test culturally-specific safety scenarios to assess whether MPO's safety alignment respects linguistic-cultural nuances rather than imposing English-centric safety norms.

**Check 3:** Run MPO training for extended periods (10+ epochs) and evaluate both safety performance and utility degradation over time to quantify catastrophic forgetting rates and identify potential reward hacking behaviors.