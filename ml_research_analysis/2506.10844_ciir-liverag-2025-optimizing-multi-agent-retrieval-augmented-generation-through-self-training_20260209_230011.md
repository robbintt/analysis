---
ver: rpa2
title: 'CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through
  Self-Training'
arxiv_id: '2506.10844'
source_url: https://arxiv.org/abs/2506.10844
tags:
- information
- question
- agent
- response
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mRAG, a multi-agent retrieval-augmented generation
  framework that employs specialized agents for planning, searching, reasoning, and
  coordination. The system uses self-training with reward-guided trajectory sampling
  to optimize inter-agent collaboration and enhance response generation.
---

# CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training

## Quick Facts
- arXiv ID: 2506.10844
- Source URL: https://arxiv.org/abs/2506.10844
- Reference count: 40
- Primary result: mRAG achieves 7th place among 20 teams in SIGIR 2025 LiveRAG competition

## Executive Summary
This paper introduces mRAG, a multi-agent retrieval-augmented generation framework that employs specialized agents for planning, searching, reasoning, and coordination. The system uses self-training with reward-guided trajectory sampling to optimize inter-agent collaboration and enhance response generation. Evaluated on DataMorgana-derived datasets during the SIGIR 2025 LiveRAG competition, mRAG outperforms conventional RAG baselines and achieves 7th place among 20 competing teams. Case studies demonstrate the system's ability to decompose complex questions, adaptively search for information, and integrate multi-aspect evidence into coherent responses.

## Method Summary
The mRAG framework implements a multi-agent architecture where specialized agents handle distinct tasks: planning agent decomposes complex questions, searching agent retrieves relevant information, reasoning agent synthesizes evidence, and coordination agent manages inter-agent communication. The system employs self-training with reward-guided trajectory sampling to optimize agent interactions. During training, the system samples successful trajectories based on reward signals and uses these to fine-tune agent policies. This approach enables adaptive learning of optimal collaboration patterns between agents, moving beyond static hand-crafted interaction rules.

## Key Results
- mRAG achieves 7th place among 20 competing teams in SIGIR 2025 LiveRAG competition
- Outperforms conventional RAG baselines on DataMorgana-derived datasets
- Demonstrates effective question decomposition and multi-aspect evidence integration through case studies

## Why This Works (Mechanism)
The multi-agent architecture allows for specialized handling of distinct RAG subtasks, with each agent focusing on its core competency. The self-training mechanism enables the system to learn optimal collaboration patterns through reinforcement learning, rather than relying on fixed interaction rules. Reward-guided trajectory sampling ensures that the system learns from successful interaction patterns while avoiding reinforcement of suboptimal behaviors. The coordination agent serves as a central hub that manages information flow and task allocation, preventing agent conflicts and ensuring coherent response generation.

## Foundational Learning
- **Multi-agent coordination**: Multiple specialized agents work together to solve complex tasks, with each agent handling a specific subtask (planning, searching, reasoning, coordination). Needed to enable modular decomposition of complex RAG tasks and prevent agent interference. Quick check: Verify each agent has clearly defined responsibilities and input/output interfaces.
- **Self-training with reinforcement learning**: The system learns optimal agent interactions through reward-guided trajectory sampling, rather than static rules. Needed to adapt collaboration patterns to specific task domains and improve over time. Quick check: Confirm reward signals are properly shaped and trajectories are correctly sampled.
- **Retrieval-augmented generation**: The system retrieves external information to augment its knowledge base before generating responses. Needed to handle questions requiring up-to-date or domain-specific information beyond model pretraining. Quick check: Verify retrieval quality and relevance to the input questions.

## Architecture Onboarding
**Component Map**: User Query -> Planning Agent -> Coordination Agent -> Searching Agent + Reasoning Agent -> Response Generation
**Critical Path**: User Query → Planning Agent → Coordination Agent → (Searching Agent → Reasoning Agent) → Response Generation
**Design Tradeoffs**: Multi-agent architecture provides specialization benefits but introduces coordination overhead and complexity compared to monolithic approaches. Self-training enables adaptive learning but requires careful reward design and sufficient training data.
**Failure Signatures**: Poor question decomposition leads to irrelevant searches; coordination failures cause agent conflicts; inadequate reasoning produces incoherent responses; self-training instability results in suboptimal collaboration patterns.
**First Experiments**: 1) Test individual agent performance on isolated tasks, 2) Evaluate end-to-end system on simple questions, 3) Conduct ablation study removing coordination agent

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to competition-derived datasets, restricting generalizability to other domains
- Self-training reward mechanisms and robustness across question types remain underspecified
- Multi-agent coordination overhead was not quantitatively analyzed
- Performance claims based on relative ranking rather than absolute quality metrics

## Confidence
- Multi-agent architecture design and implementation: High
- Competition performance ranking: High
- Self-training methodology description: Medium
- Generalization beyond competition datasets: Low
- Quantitative analysis of coordination overhead: Low

## Next Checks
1. Conduct ablation studies removing individual agents (planning, searching, reasoning, coordination) to quantify their independent contributions to performance
2. Test the system on diverse real-world datasets outside the competition scope to assess domain generalization
3. Compare mRAG against hierarchical RAG baselines with equivalent retrieval capabilities to measure the specific benefit of multi-agent coordination