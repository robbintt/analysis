---
ver: rpa2
title: 'No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers'
arxiv_id: '2512.08889'
source_url: https://arxiv.org/abs/2512.08889
tags:
- depth
- answer
- table
- reasoning
- sofa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an annotation-free training framework for
  visual reasoning that improves both reasoning and visual grounding using multimodal
  verifiers. The method uses an LLM verifier to refine reasoning via reinforcement
  learning and a VLM verifier to strengthen visual grounding through automated hard-negative
  mining, eliminating the need for ground truth labels.
---

# No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers

## Quick Facts
- arXiv ID: 2512.08889
- Source URL: https://arxiv.org/abs/2512.08889
- Authors: Damiano Marsili; Georgia Gkioxari
- Reference count: 40
- Primary result: Annotation-free training framework surpassing both open-source and proprietary models on diverse spatial reasoning benchmarks

## Executive Summary
This work introduces an annotation-free training framework for visual reasoning that improves both reasoning and visual grounding using multimodal verifiers. The method uses an LLM verifier to refine reasoning via reinforcement learning and a VLM verifier to strengthen visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This approach combines language-only reasoning models for decomposing spatial queries with vision specialist models improved via VLM critics. The method surpasses both open-source and proprietary models on diverse spatial reasoning benchmarks, including complex 3D understanding tasks. Notably, the verifier-based training signal shows an upward trend in both reasoning and execution accuracy, offering a scalable alternative to collecting expensive ground truth labels.

## Method Summary
The framework consists of two main components: (1) LLM reasoning improvement via RL with verifiers, and (2) visual grounding improvement via VLM verifier pseudo-labeling. The system uses a frozen LLM verifier (Gemini-2.5-Flash) to provide structured rewards for training a smaller LLM policy (Qwen3-8B) via GRPO on 800 spatial reasoning queries. Simultaneously, a VLM verifier (GPT-5-mini) filters over-predicted object detections from a grounding model to generate pseudo-labels for fine-tuning GroundingDINO-T. Vision specialists include GroundingDINO for object detection, MoGe2 for depth estimation, and GPT-5-mini for VQA. The approach demonstrates that high-capacity models can serve as reliable verifiers for training smaller models without ground truth labels.

## Key Results
- VALOR achieves 44.0% accuracy on OMNI3D-BENCH, surpassing both open-source (GPT-4o: 43.8%) and proprietary models
- VLM verifier filtering pipeline achieves 75% precision after three-stage processing (0.45→0.50→0.75)
- Upward trend in reasoning accuracy with training data scaling: 40 queries (40.0%) → 800 queries (43.9%) on OMNI3D-BENCH
- Fine-tuned GroundingDINO improves from 48.4% to 48.7% mAP on COCO val after VLM verifier training

## Why This Works (Mechanism)

### Mechanism 1: LLM Verifier-Guided Reinforcement Learning
A high-capacity frozen LLM can serve as a reliable reward signal for training smaller LLMs on spatial reasoning without ground truth labels. The system decomposes program quality into six binary rewards—format, syntax, logic, attribute, spatial, and adherence—evaluated by Gemini-2.5-Flash. These rewards form a structured signal optimized via GRPO, which maximizes expected advantages while preventing drift from the pretrained policy. High-capacity LLMs are more reliable as verifiers than generators for spatial reasoning; their binary judgments are sufficiently correlated with ground truth to guide learning.

### Mechanism 2: VLM Verifier as Pseudo-Labeler for Visual Grounding
VLMs can effectively filter over-predicted object detections to generate pseudo-labels for fine-tuning grounding models without human annotation. A three-stage verification pipeline: (1) coarse filtering removes semantically incorrect boxes via full-image evaluation; (2) per-crop verification validates each remaining detection; (3) de-duplication removes overlapping predictions. This process transforms low-confidence detector outputs into training data with 75% precision. VLMs excel at verification (binary classification) even when they struggle with generation (producing accurate bounding boxes).

### Mechanism 3: Explicit Tool-Based Reasoning Decomposition
Decomposing visual reasoning into explicit tool invocations with structured outputs improves reliability over end-to-end VLM prediction. The LLM generates Python programs invoking three specialist tools—GD_DETECT (object localization), DEPTH (metric depth estimation), VQA (attribute queries)—producing explicit intermediate representations (bounding boxes, depth values) rather than implicit reasoning chains in text. Explicit, executable intermediate representations reduce error propagation compared to implicit chain-of-thought in VLMs.

## Foundational Learning

- **Reinforcement Learning from Verifiable Rewards**
  - Why needed here: Core optimization method enabling label-free training via structured reward signals rather than dense supervision
  - Quick check question: How does GRPO's group-relative advantage computation differ from standard PPO's advantage estimation?

- **Hard Negative Mining**
  - Why needed here: Historical technique adapted for VLM-based pseudo-labeling; lowering detection thresholds then filtering creates informative training examples
  - Quick check question: Why might over-prediction followed by verification produce better training signal than high-confidence predictions alone?

- **Tool-Augmented Reasoning**
  - Why needed here: Framework for decomposing spatial queries into executable programs with explicit intermediate outputs
  - Quick check question: What failure modes does explicit bounding box output prevent compared to text-only chain-of-thought?

## Architecture Onboarding

- **Component map:**
  - Query input → LLM Policy (Qwen3-8B) → Generates <plan> + <answer> → Vision Specialists (GD_DETECT, DEPTH, VQA) → LLM Verifier (Gemini-2.5-Flash) → 6-component rewards → GRPO Optimizer → Updated LLM Policy
  - Parallel path: Detections → VLM Verifier (GPT-5-mini) → 3-stage filtering → Pseudo-labels → Fine-tuned GroundingDINO-T

- **Critical path:**
  1. Query input → LLM generates plan + executable Python code
  2. Code executes via vision specialist APIs (detections, depth, VQA)
  3. LLM verifier evaluates plan/code → composite reward R(q,p,c)
  4. GRPO updates policy based on reward advantages
  5. Parallel path: Detections → VLM verifier filtering → pseudo-labels → SFT fine-tuning of GroundingDINO

- **Design tradeoffs:**
  - Training scale: 800 queries sufficient for GRPO; sparse rewards work with fewer samples than dense SFT
  - Verifier selection: Gemini-2.5-Flash (87% agreement) vs. Qwen3-8B (15%)—capacity critically matters
  - Reward granularity: 6 components provide more signal than single binary, but increase inference cost
  - Frozen vs. trainable verifier: Simpler implementation but verifier doesn't improve over time

- **Failure signatures:**
  - Reasoning: Ignores spatial constraints ("closest", "below"), conflates depth with distance, computes wrong attribute dimensions
  - Grounding: Under-prediction on cluttered scenes, residual duplicates after filtering, misses small objects
  - Verifier: Fails on overlapping identical objects, small crops unresolvable

- **First 3 experiments:**
  1. Ablate reward components: Measure each component's contribution (spatial and attribute rewards most critical for 3D reasoning per Section A.6.2 analysis)
  2. Scale training data: Test 40 → 160 → 400 → 800 queries to validate upward scaling trend (Table 5 shows 40.0% → 43.9% on OMNI3D-BENCH)
  3. Compare RL vs. SFT: Train with GRPO vs. supervised fine-tuning on same verifier-filtered samples (Table 6: RL achieves 44.0% vs. SFT 38.3% on OMNI3D-BENCH)

## Open Questions the Paper Calls Out

### Open Question 1
Can VLM verifiers be integrated directly into the reinforcement learning loop for visual grounding, rather than just for generating offline SFT data? The authors list integrating verifiers into RL training as a "promising direction" and an extension of the current paradigm. The current method separates reasoning (trained via RL) and grounding (trained via SFT on pseudo-labels); a joint, online RL approach is untested.

### Open Question 2
Can generalist VLMs (e.g., Gemini) replace specialist grounding models (e.g., GroundingDINO) as the primary tools within the framework? The conclusion suggests "adapting them as grounding tools could further benefit methods like VALOR," noting VLMs outperform detectors on counting. While VLMs are strong verifiers, the paper highlights their generation failures (misaligned boxes), making their utility as reliable tools unclear.

### Open Question 3
Can the framework maintain performance using open-source verifiers instead of high-capacity proprietary models? The error analysis reveals a massive 72% accuracy gap between Gemini and open-source verifiers (Llama/Qwen) in judging rewards. It is unclear if the method scales down to accessible models or if the "verification" capability inherently requires the scale of proprietary models.

## Limitations
- Framework's reliance on frozen high-capacity verifiers introduces brittleness when queries exceed verifier capacity
- VLM verifier pipeline's three-stage filtering may systematically miss detections in crowded or overlapping scenes
- Claim of "eliminating the need for ground truth labels" overlooks requirement for high-quality verifier models

## Confidence
- **High confidence:** Core mechanism of using frozen LLM verifiers for structured reward-based RL (87% agreement with manual annotations) and VLM verifiers for pseudo-label generation (75% precision after filtering) are empirically validated and logically sound
- **Medium confidence:** Claim of surpassing both open-source and proprietary models on diverse benchmarks assumes fair comparison conditions; ablation showing spatial/attribute rewards are "most critical" for 3D reasoning is based on limited analysis
- **Low confidence:** Assertion that the approach "eliminates the need for ground truth labels" overlooks the requirement for high-quality verifier models and the potential need for verifier supervision

## Next Checks
1. Test VALOR with progressively weaker verifiers (Qwen3-8B, GPT-3.5) to establish the minimum verifier capability threshold for reliable reward signals
2. Evaluate trained models on unseen reasoning tasks (e.g., robotics manipulation planning) to test whether tool-based decomposition generalizes beyond the training distribution
3. Compare VALOR's training efficiency (800 queries, 8 A100s) against traditional supervised approaches using the same computational budget and annotation effort