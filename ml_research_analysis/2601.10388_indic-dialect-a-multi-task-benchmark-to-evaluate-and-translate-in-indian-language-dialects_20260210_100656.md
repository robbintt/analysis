---
ver: rpa2
title: 'INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian
  Language Dialects'
arxiv_id: '2601.10388'
source_url: https://arxiv.org/abs/2601.10388
tags:
- dialect
- language
- dialects
- translation
- hindi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces INDIC-DIALECT, a new human-curated parallel
  corpus containing 13,000 sentence pairs across 11 Hindi and Odia dialects. The corpus
  enables evaluation of three dialect-aware tasks: dialect classification, multiple-choice
  question answering, and machine translation.'
---

# INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects

## Quick Facts
- arXiv ID: 2601.10388
- Source URL: https://arxiv.org/abs/2601.10388
- Reference count: 6
- Primary result: Human-curated corpus of 13k sentence pairs across 11 Hindi and Odia dialects; fine-tuned IndicBERT achieves 89.8% F1 for dialect classification vs 19.6% for zero-shot LLMs.

## Executive Summary
This paper introduces INDIC-DIALECT, a new human-curated parallel corpus containing 13,000 sentence pairs across 11 Hindi and Odia dialects. The corpus enables evaluation of three dialect-aware tasks: dialect classification, multiple-choice question answering, and machine translation. Experiments show that fine-tuned models like IndicBERT substantially outperform zero-shot LLMs on classification (F1 improved from 19.6% to 89.8%). For machine translation, hybrid AI approaches achieve the best performance: 61.32 BLEU for dialect-to-language translation and 48.44 BLEU for language-to-dialect translation, outperforming baseline methods. The study demonstrates the need for dialect-specific models and targeted translation strategies due to high linguistic proximity between dialects and standard languages. The dataset and benchmarks are released as open source to support future research on low-resource Indian dialects.

## Method Summary
The study creates a parallel corpus of 13k sentence pairs across 11 Hindi and Odia dialects through human translation by native speakers. Three tasks are evaluated: dialect classification (11-class), MCQ answering, and machine translation in both directions. IndicBERT V2 is fine-tuned with AdamW (lr=2e-5) for classification and MCQ, while IndicBERTSS handles translation. For dialect-to-language MT, a hybrid AI model concatenates dictionary translation with source sentence; for language-to-dialect MT, rule-based preprocessing followed by AI generation performs best. Evaluation uses F1 for classification/MCQ and BLEU for MT, comparing against zero-shot LLMs and baseline fine-tuned models.

## Key Results
- Fine-tuned IndicBERT achieves 89.75 F1 for dialect classification, vastly outperforming zero-shot LLMs (19.62 F1)
- Hybrid AI model achieves highest BLEU of 61.32 for dialect-to-language translation
- Rule-based followed by AI approach achieves best BLEU of 48.44 for language-to-dialect translation

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning language-specific models (e.g., IndicBERT) on curated dialectal data can dramatically improve dialect identification over zero-shot LLMs. Large, general-purpose LLMs lack sufficient exposure to low-resource dialectal forms in their pre-training data. Fine-tuning a model already pre-trained on related high-resource languages (like standard Hindi and Odia) enables it to learn a decision boundary to distinguish closely related dialectal variants from a small, high-quality supervised dataset. The assumption breaks if the dialect diverges too significantly from the parent language in syntax or lexicon, or if the fine-tuning dataset is too small or noisy to overcome the model's pre-trained biases.

### Mechanism 2
A hybrid AI model (dictionary-based translation + source sentence as input) is most effective for translating dialect into a standardized language. The dialect-to-standard direction is primarily a normalization task. Providing the model with both the raw dialectal input and a noisy, dictionary-based "pre-translation" gives it complementary signals: the raw text preserves fluency and context, while the dictionary output provides strong lexical hints, reducing the search space for the correct standard term. This may underperform if the dialect's syntax is too distant from the standard language, or if the dictionary is of poor quality, leading the model to propagate errors from both sources.

### Mechanism 3
A rule-based followed by AI approach is superior for translating a standardized language into a complex, low-resource dialect. Generating dialectal text requires applying systematic, often rule-governed transformations (phonetic shifts, morphological changes) that are under-represented in the model's training data. The rule-based step enforces these transformations explicitly, producing a structurally correct but potentially non-fluent draft. The AI model then refines this draft into fluent dialectal text, with the rule-based output acting as a strong scaffold. This approach fails when dialectal variation is not primarily rule-governed, or when the rules are too numerous or complex to manually encode.

## Foundational Learning

- **Concept: The dialect-to-standard vs. standard-to-dialect translation asymmetry.**
  - Why needed here: The paper demonstrates that the optimal translation architecture differs by direction. Understanding this asymmetry is crucial for system design.
  - Quick check question: Why would a model that excels at translating a dialect into a standard language struggle with the reverse task, according to the paper's findings?

- **Concept: Linguistic proximity and its impact on model confusion.**
  - Why needed here: The embedding and lexical distance analyses reveal that high similarity between a standard language and its dialects can cause model confusion, impacting performance on nuanced tasks like MT and MCQ.
  - Quick check question: How does the high lexical overlap between Hindi and its dialects (e.g., Meerut) paradoxically help in the MCQ task but hinder in dialect-to-Hindi translation?

- **Concept: The role of specialized pre-training for low-resource domains.**
  - Why needed here: The core result hinges on IndicBERT's superiority over general multilingual models, demonstrating the value of in-domain pre-training.
  - Quick check question: Based on Table 3, why does a model like mBART, pre-trained on some Indian languages, outperform a general model like multilingual DistilBERT on dialect classification?

## Architecture Onboarding

- **Component map:** Parallel corpus (13k pairs) → IndicBERT V2 backbone → Task-specific heads (classifier, MCQ classifier, encoder-decoder) → Auxiliary bilingual dictionaries → Evaluation metrics (F1, BLEU)

- **Critical path:**
  1. Data Curation: Recruit native speakers to create/translate sentences. Validate with inter-annotator agreement
  2. Backbone Selection: Choose IndicBERT V2 for its superior performance on Indic languages
  3. Task-Specific Adaptation: Fine-tune entire model on labeled data for classification/MCQ; implement and compare four strategies for MT
  4. Evaluation: Use F1 for classification/MCQ and BLEU for MT, comparing against zero-shot LLMs and baseline fine-tuned models

- **Design tradeoffs:**
  - Data Quality vs. Scale: 13k high-quality, human-curated pairs vs. larger, noisier web-scraped datasets. The paper argues for quality in low-resource settings
  - Model Generality vs. Specificity: State-specific models outperform a combined model, suggesting a tradeoff between maintaining a single system and maximizing per-dialect accuracy
  - Architecture Complexity: The hybrid/rule-based MT approaches add complexity but yield significant performance gains, trading simplicity for effectiveness

- **Failure signatures:**
  - Zero-shot LLM failure: Very low F1 scores (~4-19%) on classification, indicating the dialects are "out-of-distribution"
  - MT baseline failure: Low BLEU (~27-46) for standard fine-tuned models, especially in the language-to-dialect direction, indicating the model cannot generate the specific morphological/lexical patterns of the dialect
  - Confusion in similar dialects: Poor classification performance on UP dialects when trained in a combined model, due to their high similarity to Hindi

- **First 3 experiments:**
  1. Replicate the classification gap: Fine-tune IndicBERT V2 on the INDIC-DIALECT classification task and measure F1 against a GPT-4o/Gemini zero-shot baseline to confirm the performance gap
  2. Ablate the hybrid MT strategy: For dialect-to-language translation, compare three conditions: (a) AI-only, (b) AI with source + dictionary concat (Hybrid), (c) AI with only dictionary output as input
  3. Test the linguistic proximity hypothesis: Train two classifiers: one on UP dialects and one on Himachali dialects. Evaluate both on a held-out set of Meerut (UP) sentences

## Open Questions the Paper Calls Out
- Do the findings and specialized translation models developed for Indo-Aryan dialects generalize to other Indian language families like Dravidian or Tibeto-Burman? The authors explicitly state the study focuses on Hindi and Odia families and "the findings and models may not generalize to... Dravidian or Tibeto-Burman."
- Does a translation-based corpus creation methodology fail to capture the organic code-switching and informal syntax inherent in natural dialectal speech? Section 6 notes that because the parallel corpus was created by translating standard sentences, it "may not fully capture the organic, dialectal speech, which often includes extensive code-switching."
- Can purely neural architectures be optimized to surpass the performance of hybrid rule-based systems for generating low-resource dialectal morphology? The paper finds that for language-to-dialect translation, "Rule-based followed by AI" (48.44 BLEU) vastly outperformed the "AI Model" (27.59 BLEU) due to the complexity of generating region-specific morphology.

## Limitations
- Dataset scale is modest at 13k sentence pairs across 11 dialects, potentially limiting generalization
- Results are dialect-specific to Hindi and Odia; transferability to other Indian language families remains unproven
- Hybrid and rule-based MT strategies require handcrafted dictionaries and transformation rules, making them labor-intensive to extend

## Confidence
- High Confidence: The core finding that fine-tuned IndicBERT substantially outperforms zero-shot LLMs on dialect classification (F1 19.6% → 89.8%)
- Medium Confidence: The directional asymmetry in MT (hybrid AI best for dialect→lang, rule+AI best for lang→dialect)
- Low Confidence: The analysis of linguistic proximity (lexical distances, embedding overlaps) provides plausible explanations for confusion patterns but relies on metrics that may not fully capture dialectal variation

## Next Checks
1. Test whether the fine-tuning advantage of IndicBERT extends to dialects of other Indian languages (e.g., Tamil, Bengali) not in the current corpus
2. Systematically vary the size of the fine-tuning dataset (e.g., 5k, 10k, 13k) to determine if performance gains continue or plateau
3. Measure how lexical/syntactic distance between a dialect and its standard language correlates with classification accuracy and MT performance across all dialect pairs in the corpus