---
ver: rpa2
title: 'Explained, yet misunderstood: How AI Literacy shapes HR Managers'' interpretation
  of User Interfaces in Recruiting Recommender Systems'
arxiv_id: '2509.06475'
source_url: https://arxiv.org/abs/2509.06475
tags:
- literacy
- users
- explanations
- systems
- understanding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines how AI literacy influences HR managers' interpretation
  of explainable AI (XAI) elements in recruitment recommender systems. An experiment
  with 410 German HR managers compared baseline dashboards to versions enriched with
  three XAI styles (important features, counterfactuals, and model criteria).
---

# Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems

## Quick Facts
- arXiv ID: 2509.06475
- Source URL: https://arxiv.org/abs/2509.06475
- Reference count: 40
- Primary result: XAI elements improved subjective perceptions but not objective understanding of recruiting recommender systems

## Executive Summary
This study investigates how AI literacy influences HR managers' interpretation of explainable AI (XAI) elements in recruitment recommender systems. Through an experiment with 410 German HR managers, researchers compared baseline dashboards to versions enhanced with three XAI styles: important features, counterfactuals, and model criteria. The findings reveal a complex relationship between AI literacy, XAI explanations, and user understanding. While XAI elements positively influenced subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, they did not translate into improved objective understanding. In fact, complex explanations may have reduced accurate understanding, highlighting a significant gap between perceived and actual comprehension of AI systems in HR contexts.

## Method Summary
The study employed an experimental design with 410 German HR managers who were randomly assigned to either baseline or XAI-enhanced dashboard conditions. Participants interacted with a simulated recruiting recommender system while their interpretations, trust levels, and understanding were measured. The experiment manipulated three types of XAI explanations: important features, counterfactuals, and model criteria. Data collection included both subjective measures (perceived helpfulness, trust) and objective measures (accurate understanding of system recommendations) to capture the full spectrum of user interaction with explainable AI elements.

## Key Results
- XAI elements improved subjective perceptions of helpfulness and trust among HR managers with moderate or high AI literacy
- Complex explanations did not increase objective understanding and may have reduced accurate comprehension
- Overlays of important features significantly aided interpretations specifically for high-literacy users

## Why This Works (Mechanism)
The study reveals that XAI effectiveness depends critically on users' existing AI literacy levels. Users with moderate to high AI literacy could better appreciate and benefit from XAI elements, leading to improved subjective perceptions. However, the cognitive load imposed by complex explanations appears to overwhelm users' ability to translate these explanations into accurate understanding, particularly for those with lower AI literacy. This suggests that the relationship between XAI presentation and user comprehension is non-linear and mediated by users' foundational knowledge of AI concepts.

## Foundational Learning
- AI literacy concepts - Why needed: Understanding basic AI concepts is crucial for interpreting XAI elements effectively. Quick check: Users can explain basic AI terminology and concepts.
- Cognitive load theory - Why needed: Explains why complex explanations may overwhelm users and reduce understanding. Quick check: Users can identify when information exceeds their processing capacity.
- User interface design principles - Why needed: Critical for presenting XAI elements in ways that support rather than hinder comprehension. Quick check: Users can navigate and interpret dashboard information efficiently.

## Architecture Onboarding
- Component map: User Interface -> XAI Elements (Important Features, Counterfactuals, Model Criteria) -> User Interpretation -> Understanding/Trust
- Critical path: XAI presentation → User cognitive processing → Interpretation → Understanding outcome
- Design tradeoffs: Balancing explanation complexity with user comprehension capacity
- Failure signatures: Users report high trust but demonstrate low objective understanding
- First experiments: 1) Test simplified XAI explanations across literacy levels, 2) Measure cognitive load during XAI interaction, 3) Compare different explanation presentation formats

## Open Questions the Paper Calls Out
None

## Limitations
- Single-country study limits generalizability across cultural contexts
- Only three XAI explanation styles tested, results may not extend to other approaches
- Subjective-objective understanding gap raises questions about assessment methodology

## Confidence
High: Experimental design and statistical analysis are robust for German HR manager population; clear distinction between subjective and objective measures demonstrated.

Medium: Interpretation of complex explanations reducing understanding is plausible but needs broader investigation; tailoring conclusions reasonable but require validation.

Low: Practical implications for HRM practice and AI literacy training are somewhat speculative given limited scope and single-country focus.

## Next Checks
1. Replicate experiment with HR managers from different countries and cultural backgrounds
2. Test additional XAI explanation styles and presentation formats
3. Conduct longitudinal studies on AI literacy training impact on XAI effectiveness