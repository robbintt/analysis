---
ver: rpa2
title: 'InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization'
arxiv_id: '2508.05731'
source_url: https://arxiv.org/abs/2508.05731
tags:
- arxiv
- exploration
- wang
- grounding
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the exploration bottleneck in GUI grounding
  for multimodal large language models (MLLMs), where inefficient exploration prevents
  learning correct semantic associations between natural language instructions and
  UI elements. The proposed Adaptive Exploration Policy Optimization (AEPO) framework
  employs multi-answer generation to broaden exploration and an Adaptive Exploration
  Reward (AER) function to guide learning, complemented by a collinear penalty to
  ensure exploration quality.
---

# InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization

## Quick Facts
- arXiv ID: 2508.05731
- Source URL: https://arxiv.org/abs/2508.05731
- Reference count: 3
- This paper introduces AEPO, a framework that achieves up to 9.0% relative improvement over RLVR baselines on GUI grounding benchmarks.

## Executive Summary
This paper addresses the exploration bottleneck in GUI grounding for multimodal large language models (MLLMs), where inefficient exploration prevents learning correct semantic associations between natural language instructions and UI elements. The proposed Adaptive Exploration Policy Optimization (AEPO) framework employs multi-answer generation to broaden exploration and an Adaptive Exploration Reward (AER) function to guide learning, complemented by a collinear penalty to ensure exploration quality. The AEPO-trained InfiGUI-G1 models (3B and 7B) establish new state-of-the-art results on multiple GUI grounding benchmarks, achieving up to 9.0% relative improvement over naive RLVR baselines. The framework effectively resolves the exploration bottleneck, particularly on challenging tasks requiring semantic understanding, such as icon grounding.

## Method Summary
The paper proposes AEPO, a framework for optimizing exploration policies in GUI grounding tasks. It uses multi-answer generation to produce diverse candidate bounding boxes per instruction, then employs an Adaptive Exploration Reward (AER) function that dynamically adjusts rewards based on exploration quality and proximity to the optimal answer. A collinear penalty prevents trivial linear exploration paths by penalizing overly similar candidate answers. The framework is trained using the Proximal Policy Optimization (PPO) algorithm with curriculum learning, starting with basic synthetic data before advancing to complex tasks. The method addresses the fundamental challenge of efficiently exploring the GUI space to find correct UI elements that match natural language instructions.

## Key Results
- AEPO-trained InfiGUI-G1 models (3B and 7B) achieve up to 9.0% relative improvement over naive RLVR baselines on GUI grounding benchmarks
- On RICO-Desktop, InfiGUI-G1 7B improves top-1 accuracy from 66.0% to 69.7% (absolute gain of 3.7%)
- The framework particularly excels at semantic grounding tasks, showing substantial improvements on icon and toolbar grounding compared to existing methods
- Ablation studies demonstrate that all components (multi-answer generation, AER, collinear penalty) contribute significantly to performance

## Why This Works (Mechanism)
The framework works by fundamentally addressing the exploration-exploitation tradeoff in GUI grounding. Traditional approaches struggle because the search space for UI elements is vast and semantically complex. AEPO's multi-answer generation creates a diverse set of candidate bounding boxes, while the adaptive reward function provides stronger incentives for truly optimal answers rather than settling for "good enough" ones. The collinear penalty ensures exploration quality by preventing the model from generating answers along a single linear path, which would give false impressions of comprehensive exploration. This combination allows the model to efficiently navigate the complex semantic space between natural language instructions and visual UI elements.

## Foundational Learning
- **Multimodal grounding**: Mapping natural language to visual regions - needed because GUI interactions require understanding both visual layout and semantic meaning
- **Exploration-exploitation tradeoff**: Balancing between trying new areas vs. refining known good areas - needed because exhaustive search is computationally prohibitive
- **Reinforcement learning with visual inputs**: Training models using reward signals from image regions - needed because GUI grounding requires learning from trial-and-error interactions
- **Curriculum learning**: Gradually increasing task difficulty during training - needed because starting with complex GUIs would overwhelm the learning process
- **Bounding box regression**: Predicting coordinate boxes around UI elements - needed as the primary output format for GUI grounding tasks
- **Visual semantic understanding**: Associating icons and UI elements with their functional meanings - needed because many grounding tasks require understanding beyond visual similarity

## Architecture Onboarding

**Component Map**: Natural Language Instruction -> Visual Encoder -> Multi-Answer Generator -> Adaptive Reward Function -> PPO Optimizer -> Bounding Box Predictor

**Critical Path**: The core inference pipeline follows: input text and screenshot → visual feature extraction → bounding box generation → non-maximum suppression → final answer selection. During training, the critical path extends to include reward calculation and policy updates through PPO.

**Design Tradeoffs**: The framework trades computational overhead (multiple answer generations per sample) for exploration quality and accuracy gains. The fixed N parameter balances exploration breadth against training efficiency. The collinear penalty adds geometric constraints that prevent degenerate solutions but may limit exploration in some scenarios.

**Failure Signatures**: 
- Performance degradation on highly cluttered UIs where multiple elements have similar visual features
- Sensitivity to hyperparameter α in the AER function, which controls the balance between exploration and exploitation
- Potential overfitting to synthetic training data patterns
- Reduced effectiveness when visual-semantic mappings are ambiguous or culturally dependent

**3 First Experiments**:
1. Test baseline GUI grounding performance on a simple benchmark like RICO-Desktop to establish performance floor
2. Apply AEPO with only multi-answer generation (no adaptive rewards) to measure isolated benefit
3. Run collinear penalty ablation to quantify its contribution to preventing degenerate exploration

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can the computational overhead of multi-answer generation in AEPO be reduced through more efficient sampling strategies while maintaining exploration quality?
- Basis in paper: [explicit] The authors state in the Conclusion: "Limitations of our work include the computational overhead from multi-answer generation... which could be addressed in future work by exploring more efficient sampling strategies."
- Why unresolved: The paper demonstrates AEPO's effectiveness but does not investigate methods to reduce the cost of generating multiple candidate answers per sample.
- What evidence would resolve it: A systematic comparison of alternative sampling methods (e.g., adaptive N, early stopping, or beam-based approaches) showing equivalent grounding performance with reduced computational cost.

### Open Question 2
- Question: How does AEPO's performance scale when integrated with more advanced visual encoders beyond the Qwen2.5-VL backbone?
- Basis in paper: [explicit] The authors explicitly identify "a performance ceiling imposed by the backbone MLLM's visual capabilities" as a limitation, suggesting "integration with more advanced visual encoders" as future work.
- Why unresolved: All experiments use Qwen2.5-VL-3B/7B backbones; the framework's compatibility and effectiveness with other visual encoders remains untested.
- What evidence would resolve it: Experiments applying AEPO training to models with different or more capable visual encoders (e.g., InternVL, CogAgent) and comparing performance improvements relative to naive RLVR baselines.

### Open Question 3
- Question: Can the number of candidate answers N be dynamically determined per sample rather than fixed, improving efficiency on easy samples?
- Basis in paper: [inferred] The analysis shows the model learns adaptive exploration (generating ~1.4 answers on easy benchmarks vs. ~2.1 on hard ones), but N is a fixed hyperparameter during training. The data filtering strategy also suggests not all samples require equal exploration.
- Why unresolved: The paper uses a fixed N but demonstrates that optimal exploration varies by task difficulty; a dynamic N mechanism could improve efficiency.
- What evidence would resolve it: Implementation of a dynamic N selection mechanism (e.g., based on model confidence or predicted difficulty) showing improved computational efficiency without accuracy degradation.

### Open Question 4
- Question: Would more nuanced geometric diversity metrics outperform the binary collinear penalty for ensuring high-quality exploration?
- Basis in paper: [inferred] The collinear penalty uses a simple threshold ("area of the triangle formed by any three points is close to zero"), and the ablation shows its removal causes a drastic accuracy drop with substantially more answers generated (6.6-8.2 vs. ~2.0).
- Why unresolved: The binary penalty prevents trivial linear scans but may be overly strict or insufficiently discriminative; continuous diversity metrics could provide better gradient signals.
- What evidence would resolve it: Comparison of alternative diversity metrics (e.g., entropy-based spatial dispersion, minimum pairwise distance) against the collinear penalty on the same benchmarks, measuring both accuracy and exploration quality.

## Limitations
- Computational overhead from multi-answer generation creates scalability challenges for large-scale deployment
- Performance ceiling imposed by the visual capabilities of the backbone MLLM limits overall effectiveness
- Limited testing on real-world, cross-platform GUIs beyond synthetic datasets and RICO benchmarks
- No evaluation of long-term generalization to unseen UI designs or cultural variations in icon usage

## Confidence
- **High confidence**: The core technical framework (AEPO with multi-answer generation and adaptive rewards) is sound and well-implemented
- **Medium confidence**: Benchmark results are reliable within tested domains, but generalization claims require further validation
- **Medium confidence**: The exploration bottleneck characterization is accurate for current GUI grounding approaches

## Next Checks
1. Evaluate model performance on held-out GUI layouts from different platforms (iOS, Windows, web) to test cross-platform generalization
2. Conduct robustness tests with varying levels of visual noise and occlusion to assess real-world deployment viability
3. Perform ablation studies on the adaptive exploration reward's α parameter across different GUI complexity levels to identify optimal settings