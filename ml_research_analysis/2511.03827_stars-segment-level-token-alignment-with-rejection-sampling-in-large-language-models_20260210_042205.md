---
ver: rpa2
title: 'STARS: Segment-level Token Alignment with Rejection Sampling in Large Language
  Models'
arxiv_id: '2511.03827'
source_url: https://arxiv.org/abs/2511.03827
tags:
- arxiv
- alignment
- stars
- sampling
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STARS, a segment-level decoding algorithm
  that aligns large language models by iteratively sampling, scoring, and accepting
  or rejecting short token segments. It addresses the inefficiency of full-sequence
  alignment methods by enabling early correction of generation paths through granular,
  reward-guided sampling.
---

# STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models

## Quick Facts
- arXiv ID: 2511.03827
- Source URL: https://arxiv.org/abs/2511.03827
- Reference count: 40
- One-line primary result: STARS achieves up to 14.9 percentage points higher win-rates than SFT and 4.3 points over DPO on alignment tasks

## Executive Summary
STARS introduces a segment-level decoding algorithm that aligns large language models by iteratively sampling, scoring, and accepting or rejecting short token segments. It addresses the inefficiency of full-sequence alignment methods by enabling early correction of generation paths through granular, reward-guided sampling. Across six models and two alignment axes (helpfulness/harmlessness and positive sentiment), STARS outperforms Supervised Fine-Tuning by up to 14.9 percentage points and Direct Preference Optimization by up to 4.3 percentage points on win-rates, while remaining highly competitive with strong Best-of-N baselines. It also improves the robustness of aligned models against adversarial prompts.

## Method Summary
STARS operates by sampling fixed-size token blocks (32 tokens) from a base LLM, scoring each segment with a Process Reward Model, and applying rejection sampling based on an adaptive threshold that becomes more selective as generation progresses. The algorithm accepts segments with probability α_k computed from the reward difference and temperature parameter, resamples rejected segments up to 20 times, and accumulates accepted segments into the final response. This segment-level approach enables early correction of generation paths without requiring gradient updates to the base model, using pre-trained reward models to guide alignment decisions.

## Key Results
- Outperforms Supervised Fine-Tuning by up to 14.9 percentage points on win-rates
- Beats Direct Preference Optimization by up to 4.3 percentage points on win-rates
- Remains highly competitive with Best-of-N baselines while improving computational efficiency

## Why This Works (Mechanism)

### Mechanism 1: Segment-Level Correction Prevents Wasted Computation
By evaluating and rejecting token segments at generation time, STARS avoids completing full trajectories that are likely to be low-reward, improving efficiency over Best-of-N. The algorithm samples fixed-size token blocks, scores them with a Process Reward Model, and applies rejection sampling. Bad segments are resampled early rather than completing the full response. This works because reward signal on partial segments is sufficiently predictive of full-response quality.

### Mechanism 2: Adaptive Threshold Balances Exploration and Alignment
Linearly increasing the reward threshold over segments allows permissive exploration early and stricter selection later, improving trade-offs between diversity and alignment. The threshold starts low and ramps toward target, accepting early blocks more easily while requiring higher rewards for later blocks. This works because prefix quality correlates progressively with final response quality, and tightening thresholds does not cause premature convergence to poor modes.

### Mechanism 3: Process Reward Model Guidance Without Gradient Updates
Using a pre-trained Process Reward Model to score segments enables alignment without modifying model weights, avoiding training costs and mode collapse risks. PRMs evaluate partial sequences and the algorithm uses these scores in rejection sampling. This works because PRMs generalize well to partial sequence distributions encountered during STARS decoding, and reward hacking is limited.

## Foundational Learning

Concept: Rejection Sampling
- Why needed here: STARS uses rejection sampling to accept or reject candidate segments based on reward, requiring understanding of proposal vs. target distributions and acceptance probability.
- Quick check question: Given a proposal distribution q(y|x) and target p(y|x), what condition must the scaling constant M satisfy to ensure valid rejection sampling?

Concept: Autoregressive Generation and Token-Block Segmentation
- Why needed here: The algorithm operates on fixed-size blocks within autoregressive decoding; understanding how partial sequences affect future tokens is critical.
- Quick check question: How does segment-level truncation differ from full-sequence ranking in terms of information available to the reward model?

Concept: Reward Modeling and Partial Sequence Evaluation
- Why needed here: PRMs score incomplete text; understanding their training and limitations is necessary to anticipate miscalibration risks.
- Quick check question: Why might a reward model trained on complete responses perform poorly when evaluating short token segments?

## Architecture Onboarding

Component map: Base LLM -> Process Reward Model -> Rejection Sampler -> Threshold Scheduler -> Output Buffer

Critical path:
1. Receive prompt x
2. Sample B-token segment from π_LM(·|x, prefix)
3. Compute reward r(x, prefix ⊕ segment)
4. Calculate α_k using threshold τ_r^(k) and temperature β
5. Accept segment with probability α_k; if rejected, resample (up to max attempts)
6. Append accepted segment to prefix; increment block counter k
7. Update threshold; repeat until max tokens reached

Design tradeoffs:
- Smaller segment size (B) increases granularity but raises PRM calls and latency
- Aggressive threshold schedule improves alignment but may reduce diversity and increase rejection loops
- PRM choice (general vs. task-specific) affects alignment axis quality vs. generalization

Failure signatures:
- High rejection rates causing excessive latency or reaching max attempts (threshold too strict or PRM mismatch)
- Low win-rate vs. vanilla (PRM misaligned or segment size too coarse)
- Loss of diversity (mode collapse from overly aggressive thresholds)

First 3 experiments:
1. Baseline alignment: Run STARS vs. vanilla decoding and Best-of-N (N ≈ 10) on HH-RLHF, report win-rate, mean reward, and latency
2. Segment size sweep: Compare B ∈ {8, 16, 32, 64} tokens holding other hyperparameters fixed, measuring win-rate and rejection frequency
3. Threshold schedule ablation: Test linear vs. fixed threshold schedules, tune τ_0 and r*, evaluate trade-offs between alignment quality and compute

## Open Questions the Paper Calls Out

Open Question 1
- **Question:** How does varying the fixed segment length (B) impact the trade-off between alignment granularity and generation coherence across different tasks?
- **Basis in paper:** The methodology (Section 3) and experiments (Appendix A) fix the segment size at B=32 tokens without providing an ablation study or theoretical justification for this specific hyperparameter value.
- **Why unresolved:** It is unclear if 32 tokens represents an optimal balance for capturing semantic units or if it is merely a heuristic choice that might fail for syntax-heavy or reasoning-heavy tasks.
- **What evidence would resolve it:** An ablation study comparing STARS performance with varying segment lengths (e.g., 1, 8, 32, 64) on metrics of coherence, win-rate, and inference speed.

Open Question 2
- **Question:** To what extent does the miscalibration of Reward Models on partial sequences degrade the alignment performance of STARS?
- **Basis in paper:** Section 6 ("Reward Evaluation for Incomplete Text") explicitly acknowledges that RMs are typically trained on full responses and "can miscalibrate when evaluating partial text," yet the method relies entirely on these intermediate scores.
- **Why unresolved:** While the authors use Process Reward Models (PRMs) to mitigate this, the robustness of the rejection sampling loop against systematic errors in early token scoring remains unquantified.
- **What evidence would resolve it:** Analysis correlating the accuracy of the PRM on partial sequences (vs. full sequence gold labels) with the final win-rates of the generated text.

Open Question 3
- **Question:** Does the adaptive thresholding mechanism lead to unacceptable latency or "stalling" when the base model struggles to generate high-reward segments in difficult alignment scenarios?
- **Basis in paper:** Appendix A specifies a "maximum of 20 candidate generations... after which we forcefully accept," implying a failure mode where the algorithm might waste computation or accept low-quality segments if the threshold is too aggressive.
- **Why unresolved:** The paper claims computational efficiency, but the worst-case scenario where the model repeatedly fails the acceptance criteria is not discussed in terms of wall-clock time or retry statistics.
- **What evidence would resolve it:** Reporting the average number of resampling steps per segment and the variance in latency across different prompt difficulties.

## Limitations

- Unknown hyperparameter values (β, α, r*) prevent exact replication of claimed performance
- PRM generalization to partial sequences creates risk of noisy or biased segment-level scores
- Rejection sampling efficiency and safety trade-offs are not fully characterized

## Confidence

High confidence: The STARS mechanism (segment-level rejection sampling with adaptive thresholds) is clearly described and logically sound.

Medium confidence: The reported empirical improvements are likely achievable with appropriate hyperparameter tuning, but exact replication requires resolving unknown parameter values.

Low confidence: The claim that STARS is "highly competitive with strong Best-of-N baselines" while being more efficient is difficult to verify without knowing rejection rates and specific Best-of-N configurations.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically sweep β ∈ {0.5, 1.0, 2.0}, α ∈ {0.1, 0.5, 0.9}, and r* ∈ {target reward ± 0.1}, measuring win-rate and rejection frequency to establish whether claimed performance is robust to parameter choices.

2. **PRM calibration validation**: Generate 100 partial segments from STARS across different generation stages, score them with the PRM, and compare to ground-truth full-sequence scores from a held-out dataset to quantify PRM reliability for segment-level evaluation.

3. **Rejection rate and latency profiling**: Measure the distribution of rejection counts per segment and total generation latency, comparing against theoretical Best-of-N cost to validate claimed efficiency advantage and report forced acceptance events.