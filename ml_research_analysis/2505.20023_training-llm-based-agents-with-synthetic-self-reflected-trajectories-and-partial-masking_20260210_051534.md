---
ver: rpa2
title: Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial
  Masking
arxiv_id: '2505.20023'
source_url: https://arxiv.org/abs/2505.20023
tags:
- agent
- trajectories
- task
- teacher
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training effective autonomous
  agents using smaller open-source large language models (LLMs). Current methods often
  rely on expert trajectories from powerful closed-source models, which can lead to
  performance plateaus and error propagation.
---

# Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking

## Quick Facts
- arXiv ID: 2505.20023
- Source URL: https://arxiv.org/abs/2505.20023
- Reference count: 21
- Key outcome: Training smaller LLMs with self-reflected trajectories from larger models achieves up to 22% improvement on unseen tasks

## Executive Summary
This paper addresses the challenge of training effective autonomous agents using smaller open-source large language models (LLMs). Current methods often rely on expert trajectories from powerful closed-source models, which can lead to performance plateaus and error propagation. To mitigate these issues, the authors propose STeP (Self-Reflected Trajectories and Partial Masking), a method that synthesizes trajectories including error reflections and corrections, and employs partial masking to prevent learning incorrect steps. Experiments on ALFWorld, WebShop, and SciWorld tasks show that training LLaMA2-7B-chat with self-reflected trajectories from a larger teacher model (Qwen1.5-110B-Chat) achieves significant improvements—up to 22% on unseen tasks—using less training data than training exclusively on expert trajectories.

## Method Summary
The STeP method synthesizes trajectories that include both successful steps and self-reflected error corrections, generated by a larger teacher model. The approach uses partial masking to prevent the student model from learning incorrect intermediate steps while still capturing the corrective reasoning. The teacher model (Qwen1.5-110B-Chat) generates trajectories that include reflections on errors and their corrections, which are then partially masked during training to focus the student model (LLaMA2-7B-chat) on learning the corrected paths rather than the mistakes.

## Key Results
- Training with self-reflected trajectories achieves up to 22% improvement on unseen tasks compared to expert-only training
- The approach uses less training data than traditional expert trajectory training
- Significant performance gains demonstrated across three domains: ALFWorld, WebShop, and SciWorld

## Why This Works (Mechanism)
The method works by combining synthetic trajectory generation with error reflection and strategic masking. By having the teacher model reflect on and correct its own errors before generating training data, the student model learns from both successful strategies and recovery patterns. The partial masking prevents the student from memorizing incorrect steps while still learning the correction mechanisms, leading to more robust performance on unseen tasks.

## Foundational Learning

### Error Reflection in Model Training
**Why needed**: Agents trained only on perfect trajectories fail to recover from errors in real-world deployment
**Quick check**: Compare agent performance on error-recovery tasks vs. perfect-path tasks

### Synthetic Trajectory Generation
**Why needed**: Expert trajectories from closed-source models are limited and expensive to obtain
**Quick check**: Measure training data efficiency gains from synthetic vs. real expert trajectories

### Partial Masking Strategy
**Why needed**: Prevents memorization of incorrect steps while preserving learning of correction patterns
**Quick check**: Vary masking ratio and measure impact on error recovery performance

## Architecture Onboarding

### Component Map
Teacher Model (Qwen1.5-110B-Chat) -> Trajectory Generator -> Partial Masker -> Student Model (LLaMA2-7B-chat)

### Critical Path
1. Teacher model generates task trajectories
2. Teacher model reflects on and corrects errors in generated trajectories
3. Partial masking applied to erroneous steps
4. Student model trained on masked, self-reflected trajectories

### Design Tradeoffs
- **Masking ratio**: Higher ratios prevent error learning but may obscure useful context; lower ratios preserve context but risk learning mistakes
- **Teacher model size**: Larger teachers generate better reflections but increase computational cost
- **Trajectory diversity**: More diverse trajectories improve generalization but increase training complexity

### Failure Signatures
- **Performance plateau**: May indicate insufficient trajectory diversity or masking ratio too high
- **Error propagation**: Suggests masking ratio too low or reflection quality poor
- **Overfitting**: Could result from insufficient masking or too much training on similar trajectories

### First 3 Experiments
1. Baseline comparison: Train student model on expert trajectories vs. self-reflected trajectories
2. Masking ratio ablation: Test performance at ratios 0.1, 0.3, 0.5
3. Generalization test: Evaluate on unseen tasks not present in training data

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to three domains (ALFWorld, WebShop, SciWorld), raising generalizability concerns
- Performance improvements lack statistical significance testing
- Method depends on a large teacher model, which may not be available in all settings
- Training efficiency gains not quantified in wall-clock time or computational resources

## Confidence
- **High Confidence**: The methodology for generating self-reflected trajectories and applying partial masking is clearly described and reproducible
- **Medium Confidence**: The reported performance improvements on the three evaluated tasks appear consistent within the experimental setup
- **Low Confidence**: Generalizability to broader agent training scenarios and robustness across different masking ratios

## Next Checks
1. Conduct statistical significance testing across multiple random seeds to verify the robustness of the reported performance improvements
2. Perform ablation studies varying the partial masking ratio (e.g., 0.1, 0.3, 0.5) to determine optimal settings across different task types
3. Test the approach on at least two additional diverse domains (e.g., text adventure games and web navigation tasks) to assess generalizability beyond the current evaluation suite