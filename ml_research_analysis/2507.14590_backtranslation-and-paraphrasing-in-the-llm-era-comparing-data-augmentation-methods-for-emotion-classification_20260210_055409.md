---
ver: rpa2
title: Backtranslation and paraphrasing in the LLM era? Comparing data augmentation
  methods for emotion classification
arxiv_id: '2507.14590'
source_url: https://arxiv.org/abs/2507.14590
tags:
- data
- augmentation
- dataset
- backtranslation
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates data augmentation techniques for emotion
  classification in low-resource settings using large language models. It compares
  traditional methods like backtranslation and paraphrasing with modern LLM-based
  generation approaches on the GoEmotions dataset.
---

# Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification

## Quick Facts
- **arXiv ID:** 2507.14590
- **Source URL:** https://arxiv.org/abs/2507.14590
- **Reference count:** 37
- **One-line primary result:** Backtranslation with DeepL achieved up to 121% improvement in F1 scores for underrepresented emotion classes in GoEmotions dataset.

## Executive Summary
This study investigates data augmentation techniques for emotion classification in low-resource settings, comparing traditional methods like backtranslation and paraphrasing with modern LLM-based generation approaches. The researchers focus on the 5 least represented emotion classes in the GoEmotions dataset, testing four augmentation strategies: oversampling, GPT-based paraphrasing, zero/few-shot generation, and backtranslation using multiple translation models. Results demonstrate that all methods improve classification performance, with backtranslation yielding the highest gains, especially when using DeepL. The study shows that traditional augmentation methods, when enhanced with LLMs, can achieve comparable or superior results to direct generative techniques, offering effective solutions for data scarcity and class imbalance in NLP tasks.

## Method Summary
The researchers conducted experiments on the GoEmotions dataset, specifically targeting five minority emotion classes (embarrassment, nervousness, relief, pride, grief). They implemented four augmentation strategies: oversampling, GPT-based paraphrasing, zero/few-shot generation, and backtranslation using DeepL, GPT-4, and MarianMT across 10 languages. The augmented data was then used to fine-tune two classifier models (LaBSE and DistilBERT), with performance measured primarily using F1-macro scores for the augmented classes. The study systematically compared semantic fidelity and lexical diversity metrics to understand why certain augmentation methods outperformed others.

## Key Results
- Backtranslation with DeepL achieved the highest overall performance, producing up to 121% improvement in augmented classes compared to baseline.
- GPT-4-based paraphrasing introduced better lexical diversity than GPT-3.5 while maintaining semantic fidelity.
- Zero-shot and few-shot generation approaches generally outperformed paraphrasing methods, with GPT-3.5 showing the best results.
- All augmentation methods significantly improved F1 scores for underrepresented emotion classes compared to the baseline.

## Why This Works (Mechanism)

### Mechanism 1: Semantic Fidelity via Constrained Transformation (Backtranslation)
If backtranslation is performed with high-quality translation models (specifically DeepL), it preserves the semantic label of minority class samples better than generative approaches while introducing sufficient lexical noise to prevent overfitting. The text is encoded into a pivot language and decoded back, stripping away low-level lexical artifacts specific to the original author while forcing retention of core semantic content necessary to preserve the emotion label. Assumption: The translation model has sufficiently aligned the emotion concepts across the source and pivot languages such that the "round-trip" does not alter the ground truth label. Evidence: Backtranslation achieved the best overall performance, with DeepL model producing up to 121% improvement in augmented classes. Break condition: Using lower-quality translation models or excessive linguistic distance in the pivot language may introduce semantic drift, failing to preserve the class label.

### Mechanism 2: Lexical Diversity via Generative Paraphrasing
Prompting LLMs (specifically GPT-3.5/4) for paraphrasing expands the decision boundary of the classifier by introducing novel syntactic structures and vocabulary that remain within the semantic boundaries of the original class. By explicitly prompting for rephrasing, the model generates samples with high Jaccard dissimilarity relative to the source, forcing the downstream classifier to learn generalized features of the emotion rather than memorizing specific phrase patterns. Assumption: The LLM understands the nuance of the specific emotion label well enough to avoid semantic drift where the paraphrase subtly changes the intensity or type of emotion. Evidence: GPT-4 methods were better at introducing lexical diversity than GPT-3.5, with Jaccard dissimilarity indicating better results. Break condition: If the prompt strategy is too aggressive, semantic fidelity drops, potentially introducing noise that degrades classification performance.

### Mechanism 3: Synthetic In-Context Generation (Zero/Few-Shot)
Generative LLMs can synthesize entirely new examples for under-represented classes by leveraging their pre-trained internal knowledge of emotion concepts, effectively "hallucinating" useful training data rather than transforming existing data. The model retrieves knowledge about a label from its pre-trained weights and generates diverse sentence structures, potentially covering a wider subspace of the true data distribution than paraphrasing. Assumption: The LLM's internal representation of the emotion aligns with the dataset's specific annotation guidelines. Evidence: Zero-shot and few-shot learning generally outperformed paraphrasing, with best results from GPT-3.5. Break condition: If the model lacks specific cultural context, generated samples may be grammatically correct but distributionally alien to the test set, limiting utility.

## Foundational Learning

- **Concept: Class Imbalance & Macro F1-Score**
  - **Why needed here:** The paper targets 5 least represented classes in GoEmotions. Standard accuracy is misleading; you must understand Macro F1 to evaluate if the model is actually learning the minority classes or just ignoring them.
  - **Quick check question:** If a model achieves 95% accuracy on GoEmotions but fails to identify "Grief" (75 samples) entirely, why is Macro F1 the required metric for this study?

- **Concept: Semantic Fidelity vs. Lexical Diversity Trade-off**
  - **Why needed here:** The core comparison (Backtranslation vs. Generation) is a trade-off between keeping the meaning exactly the same (Fidelity) and changing the words enough to be useful (Diversity).
  - **Quick check question:** Why is "Oversampling" considered the baseline for maximum fidelity but minimum diversity?

- **Concept: Few-Shot Learning (FSL)**
  - **Why needed here:** The paper compares prompting strategies. You need to distinguish between giving the model no examples (Zero-shot) vs. some examples (Few-shot) to understand the "generative" augmentation results.
  - **Quick check question:** In the context of this paper, does "Few-shot" refer to fine-tuning the discriminator or providing examples in the prompt for the data generator?

## Architecture Onboarding

- **Component map:** GoEmotions Dataset -> Augmentation Models (DeepL, GPT-3.5/4, MarianMT) -> Fine-tuned Classifiers (LaBSE, DistilBERT) -> Evaluation Metrics (F1-macro, BERTScore, Jaccard)
- **Critical path:** The selection of the Augmentation Model + Prompt/Settings. The study shows that "Backtranslation" is not a monolithâ€”DeepL yields 121% gain, whereas MarianMT yields only ~71%. The choice of tool dictates the fidelity/diversity balance.
- **Design tradeoffs:**
  - Backtranslation (DeepL): High fidelity, lower resource cost than GPT-4, but requires managing multiple language loops
  - Generation (Zero-Shot): Fastest to implement (no source text needed), but relies heavily on LLM's prior knowledge which may not match the target domain distribution
  - DistilBERT vs. LaBSE: DistilBERT is cheaper/faster to fine-tune; LaBSE generally performed slightly better on Macro F1 but is heavier
- **Failure signatures:**
  - Semantic Drift: Generating data that is diverse but incorrect (Low BERTScore). Seen in "BT with GPT-4-turbo" (Cosine Sim 0.15)
  - Overfitting to Diversity: Generating text that is too distinct, preventing the model from learning a cohesive cluster for the class
- **First 3 experiments:**
  1. Baseline Establishment: Fine-tune DistilBERT on the raw imbalanced GoEmotions subset to establish the "no augmentation" floor
  2. Backtranslation (DeepL) Loop: Implement the winning configuration: Translate (En -> Pl/Ru/etc) -> Translate Back -> Append to Training Set -> Fine-tune. Measure F1-Macro
  3. Fidelity Check: Generate 100 samples via GPT-4 Paraphrasing vs. DeepL BT. Manually annotate a random 10% to verify if "Semantic Fidelity" metrics align with human judgment of label preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do backtranslation and paraphrasing maintain their superiority over generative methods when applied to non-emotion NLP tasks or datasets from different domains?
- Basis in paper: Section 7 states the need for experiments using "methods using different models and datasets" to confirm if findings generalize beyond the GoEmotions dataset.
- Why unresolved: The current study is restricted to a single Reddit-based emotion classification task, leaving the efficacy of these specific augmentation ratios unverified in other contexts.
- What evidence would resolve it: Replicating the experimental setup on diverse datasets (e.g., biomedical or legal corpora) and comparing the resulting F1-macro improvements against the GoEmotions baseline.

### Open Question 2
- Question: To what extent do large language model (LLM) biases and in-context learning biases propagate into the augmented datasets?
- Basis in paper: Section 7 explicitly lists "evaluating the impact of LLM and in-context learning biases on the data augmentation" as a necessary area of future research.
- Why unresolved: While the paper measures semantic fidelity and lexical diversity, it does not assess the sociotechnical biases introduced by GPT-3.5/4 or DeepL during the augmentation process.
- What evidence would resolve it: A comparative bias audit of the fine-tuned classifiers before and after the introduction of augmented data.

### Open Question 3
- Question: Can an "LLM-as-a-judge" evaluation framework provide a more accurate assessment of augmented data quality than traditional metrics like BERTScore or cosine similarity?
- Basis in paper: Section 7 highlights "evaluation of generated text with LLM-as-a-judge" as a specific target for future work to better assess data quality.
- Why unresolved: The current evaluation relies on embedding-based metrics which may not fully capture the nuances of emotional label preservation or naturalness as well as a generative evaluator might.
- What evidence would resolve it: Correlating LLM-as-a-judge quality scores with downstream classification performance and comparing this correlation against that of BERTScore.

## Limitations

- The study does not specify training hyperparameters (learning rate, batch size, epochs) for the fine-tuned classifiers, making exact reproduction challenging.
- Prompt texts for paraphrasing augmentation are partially unspecified, with only example structures provided.
- The study relies on API-based models (DeepL, GPT-4) whose behavior may vary over time or with different versions.

## Confidence

- **High Confidence:** Backtranslation with DeepL yields significant improvements over baseline; this is consistently supported by quantitative results across both classifier models.
- **Medium Confidence:** Generative zero-shot/few-shot approaches are competitive with traditional methods; results show variability between GPT-3.5 and GPT-4, suggesting model choice matters.
- **Low Confidence:** Cross-LLM generalizability claims, as the study only tests with OpenAI models and MarianMT, without exploring other LLM families or open-source alternatives.

## Next Checks

1. **Semantic Drift Validation:** Manually annotate 50 backtranslated samples to verify if automated BERTScore metrics align with human judgment of emotion label preservation.
2. **Hyperparameter Sensitivity:** Systematically vary learning rate (1e-5 to 5e-5) and epochs (3-7) to establish the robustness of augmentation gains to training configuration.
3. **Cross-Dataset Generalization:** Apply the best-performing augmentation pipeline to a different emotion dataset (e.g., SemEval-2018) to test if gains transfer beyond GoEmotions.