---
ver: rpa2
title: Efficient Text Classification with Conformal In-Context Learning
arxiv_id: '2512.05732'
source_url: https://arxiv.org/abs/2512.05732
tags:
- cicle
- few-shot
- base
- prompting
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Conformal In-Context Learning (CICLe), a framework
  that combines traditional classifiers with conformal prediction to guide LLM prompting
  in text classification. CICLe reduces computational cost by adaptively selecting
  candidate classes and examples, bypassing the LLM when confident.
---

# Efficient Text Classification with Conformal In-Context Learning
arXiv ID: 2512.05732
Source URL: https://arxiv.org/abs/2512.05732
Reference count: 0
Primary result: CICLe framework outperforms few-shot prompting while reducing shots by up to 34.45% and prompt length by 25.16%

## Executive Summary
CICLe (Conformal In-Context Learning) is a hybrid framework that combines traditional classifiers with conformal prediction to guide LLM prompting in text classification tasks. The approach uses a TF-IDF Logistic Regression base classifier to predict a small set of candidate classes, then queries an LLM only when this set contains multiple classes. This adaptive strategy reduces computational costs while maintaining or improving classification accuracy across diverse benchmarks including AG News, DBpedia-14, Yahoo Answers, and GoEmotions.

## Method Summary
CICLe integrates a traditional TF-IDF Logistic Regression classifier with conformal prediction to create an efficient text classification pipeline. For each test instance, the base classifier predicts a conformal set of candidate classes at 95% confidence. If the set contains only one class, the base classifier's prediction is returned directly. Otherwise, the LLM is queried with a prompt containing 2 labeled examples per candidate class, ordered by base classifier confidence, plus the test instance. The framework achieves efficiency gains by reducing the number of LLM queries and prompt length while maintaining competitive accuracy across four diverse text classification benchmarks.

## Key Results
- CICLe consistently outperforms or matches few-shot prompting across all four benchmarks
- Achieves up to 34.45% fewer shots and 25.16% shorter prompts than baselines
- Small 8B-parameter models achieve competitive performance versus 70B-parameter models
- Particularly effective under high class imbalance, requiring only 2k+ samples for GoEmotions

## Why This Works (Mechanism)
CICLe works by leveraging conformal prediction to create confidence intervals around the base classifier's predictions. This allows the system to bypass expensive LLM queries when the base classifier is highly confident (singleton conformal sets), while still benefiting from LLM reasoning when uncertainty is high. The adaptive nature means computational resources are allocated only where needed, with the base classifier handling the majority of cases and the LLM focusing on ambiguous instances.

## Foundational Learning
- Conformal Prediction: Why needed - provides statistical guarantees on prediction coverage; Quick check - verify 95% coverage on held-out calibration set
- TF-IDF Vectorization: Why needed - creates interpretable sparse features for efficient base classification; Quick check - ensure vocabulary covers domain-specific terms
- LLM Prompt Engineering: Why needed - structured examples improve in-context learning performance; Quick check - validate all generated labels match valid class set
- Logistic Regression: Why needed - fast, interpretable base classifier for initial candidate set prediction; Quick check - verify convergence and reasonable coefficients
- Stratified Sampling: Why needed - maintains class distribution across train/validation splits; Quick check - confirm all classes present in each subset

## Architecture Onboarding
Component map: Data -> TF-IDF Vectorizer -> Logistic Regression -> Conformal Prediction -> LLM (conditional) -> Final Prediction

Critical path: Test instance → Base classifier → Conformal set → (if |set|>1) LLM prompt → Prediction

Design tradeoffs: Base classifier accuracy vs LLM query reduction; prompt length vs example quality; α confidence level vs coverage

Failure signatures: Invalid LLM outputs counted as incorrect; calibration set too small causing poor coverage; class imbalance leading to missing classes in training

First experiments:
1. Verify base classifier achieves reasonable accuracy (>80%) on validation set before adding LLM component
2. Test conformal prediction coverage on calibration set to ensure ~95% guarantee
3. Run end-to-end pipeline on small subset (100 samples) to validate full workflow

## Open Questions the Paper Calls Out
- How varying the α parameter impacts prediction certainty vs label coverage trade-off
- Adapting CICLe for multi-label or hierarchical text classification tasks
- Whether dense embeddings improve efficiency and performance versus TF-IDF
- CICLe's robustness across diverse LLM architectures and multilingual tasks

## Limitations
- Fixed α=0.05 confidence level across all experiments without exploring sensitivity
- Limited to English datasets with only LLaMA-3.1-Instruct models tested
- No exploration of alternative base classifier architectures beyond TF-IDF Logistic Regression
- Small sample sizes (100-5k) may not reflect real-world deployment scenarios

## Confidence
- CICLe's performance advantage over few-shot prompting: High
- Efficiency improvements (shots and token reduction): Medium
- Robustness under class imbalance: Medium
- Small model competitiveness: Medium

## Next Checks
1. Verify coverage guarantee: Test that conformal prediction sets achieve approximately 95% coverage on a held-out calibration set
2. Validate prompt structure: Confirm that the LLM prompt template used matches the paper's description of task introduction, labeled examples, and test instance inclusion
3. Test class representation: For each training subset size, verify that all classes have at least one training example to prevent training failures on imbalanced datasets