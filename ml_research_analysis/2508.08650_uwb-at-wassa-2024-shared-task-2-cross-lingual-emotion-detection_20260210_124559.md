---
ver: rpa2
title: 'UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection'
arxiv_id: '2508.08650'
source_url: https://arxiv.org/abs/2508.08650
tags:
- detection
- trigger
- emotion
- data
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses cross-lingual emotion detection in tweets
  across five languages, focusing on two subtasks: emotion classification and trigger
  word identification. The proposed method combines fine-tuning quantized large language
  models (Orca 2) with low-rank adapters (LoRA) and multilingual Transformer-based
  models (XLM-R, mT5), enhanced by machine translation and trigger word switching.'
---

# UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection

## Quick Facts
- arXiv ID: 2508.08650
- Source URL: https://arxiv.org/abs/2508.08650
- Reference count: 16
- 1st in numerical trigger word detection, 3rd in binary trigger word detection, 7th in emotion detection

## Executive Summary
This paper tackles cross-lingual emotion detection in tweets across five languages, addressing two subtasks: emotion classification and trigger word identification. The approach combines fine-tuning quantized large language models (Orca 2) with low-rank adapters (LoRA) and multilingual Transformer-based models (XLM-R, mT5), enhanced by machine translation and trigger word switching. Results show the system ranks 1st in numerical trigger word detection, 3rd in binary trigger word detection, and 7th in emotion detection, demonstrating that translated data significantly improves performance, particularly for the Orca 2 model.

## Method Summary
The method employs English-only training data augmented with machine translation to four target languages (Dutch, Russian, Spanish, French). For emotion detection, Orca 2 is fine-tuned with QLoRA (4-bit quantization, r=64, α=16) on translated data. For trigger word detection, XLM-R-large handles binary classification while mT5-large encoder processes numerical triggers. An alignment-free label projection technique uses special symbols to mark trigger words before translation, extracting them afterward. Trigger word switching creates bilingual code-mixed examples by swapping trigger words between source and target languages. All models use standard Transformer architectures with linear classification heads for token-level predictions.

## Key Results
- Ranked 1st in numerical trigger word detection (70.52 F1 with mT5)
- Ranked 3rd in binary trigger word detection (59.19 F1 with XLM-R)
- Ranked 7th in emotion detection (59.10 F1 with Orca 2)
- Translated data improved Orca 2 emotion detection F1 by ~3% (55.73 to 59.10)

## Why This Works (Mechanism)

### Mechanism 1: Alignment-Free Label Projection via Translation Marking
Translating training data with special symbol-marked trigger words enables cross-lingual label transfer without word alignment tools. Trigger words in English sentences are wrapped with distinct symbols (e.g., `[]`, `{}`) before machine translation. After translation, the symbols are extracted to identify trigger words in target languages. This avoids alignment errors from tools like fastAlign. Core assumption: machine translation preserves trigger word semantics and special symbols survive translation with acceptable fidelity.

### Mechanism 2: Trigger Word Switching for Bilingual Data Augmentation
Creating mixed-language training instances by swapping trigger words between source and target languages improves numerical trigger detection. From an English sentence `xS` and its translation `xT`, generate two new sentences: (1) `xSt`—English sentence with trigger words replaced by target-language equivalents; (2) `xTs`—translated sentence with trigger words in English. This exposes the model to code-mixed patterns. Core assumption: code-switching at trigger words creates useful supervision signal without confusing the model.

### Mechanism 3: QLoRA Enables Consumer-GPU Fine-Tuning of Large Models
4-bit quantization with LoRA adapters allows fine-tuning Orca 2 (13B parameters) on 48GB GPU while preserving cross-lingual emotion detection capability. The frozen 4-bit quantized backbone reduces memory; low-rank adapters (r=64, α=16) add trainable parameters to all linear Transformer layers. Only adapter weights update during fine-tuning. Core assumption: quantization does not catastrophically degrade multilingual representations needed for emotion transfer.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Required to fine-tune Orca 2 within 48GB GPU memory. Standard full fine-tuning would need substantially more resources.
  - Quick check question: Can you explain why LoRA reduces memory compared to full fine-tuning, and what the rank `r` controls?

- **Concept: Cross-Lingual Label Projection**
  - Why needed here: Training data is English-only; evaluation spans 5 languages. Understanding how labels transfer across languages via translation is central to the method.
  - Quick check question: Why might alignment-free projection (using special symbols) outperform word alignment tools for trigger word labels?

- **Concept: Encoder-Only vs. Encoder-Decoder vs. Decoder-Only Architectures**
  - Why needed here: Paper uses XLM-R (encoder-only) for token classification, mT5 (encoder-decoder, encoder used) for numerical triggers, and Orca 2 (decoder-only) for text generation. Each architecture matches its task formulation.
  - Quick check question: Why is XLM-R more suitable for token-level binary classification than a decoder-only model?

## Architecture Onboarding

- **Component map**:
  Training Data (English tweets + labels) -> Translate with symbol marking -> DS + DT + DSt + DTs
       ├── Subtask 1 (Emotion Detection): Orca 2 (13B, QLoRA, r=64, α=16) -> Generate emotion label
       └── Subtask 2 (Trigger Word Detection): XLM-R-large (binary) / mT5-large encoder (numerical) -> Linear layer -> softmax per token

- **Critical path**:
  1. Data preparation (translation + symbol marking) is the bottleneck—Google API quality directly determines label projection fidelity.
  2. For numerical triggers: all four datasets (DS + DT + DSt + DTs) are required for best performance (70.52 vs. 62.06 with DS+DT alone).
  3. For emotion detection: Orca 2 with translated data provides ~3% absolute F1 gain over English-only.

- **Design tradeoffs**:
  - Translation quality vs. coverage: More languages expand training but introduce noise if translation is poor.
  - Model size vs. task fit: Orca 2 (13B) outperforms multilingual models on emotion detection but requires QLoRA; XLM-R/mT5 are smaller but sufficient for token classification.
  - Binary vs. numerical trigger formulation: mT5 excels at numerical (70.52) but underperforms XLM-R on binary (56.79 vs. 59.19). Assumption: architecture-task mismatch.

- **Failure signatures**:
  - Emotion confusion: Love ↔ Joy misclassification is common (Figure 5-6: 0.27 and 0.35 confusion rates).
  - Class imbalance: Fear has only 77 test samples vs. 916 Neutral; Fear recall improved most from translated data (0.29 → 0.52).
  - Symbol loss in translation: Sentences where translation strips markers are discarded; no recovery mechanism described.
  - Seed variability: Paper notes "random seeds during fine-tuning can obscure slight performance differences."

- **First 3 experiments**:
  1. Baseline replication: Fine-tune XLM-R-large on English-only data (DS) for binary trigger detection. Target: ~58.59 F1 (Table 3). Validate data pipeline and tokenization.
  2. Ablation on translated data: Train Orca 2 with DS only vs. DS+DT for emotion detection. Expect ~3% F1 gap. Isolate contribution of translation.
  3. Trigger switching validation: For mT5 numerical triggers, compare DS+DT vs. DS+DT+DSt+DTs. Expect ~8% improvement (62.06 → 70.52). If absent, check translation quality and symbol preservation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would a multi-task learning approach that jointly solves emotion detection and trigger word identification improve performance over solving each subtask independently?
- Basis in paper: [explicit] The Limitations section states: "we solve each subtask independently. Future research could address solving both subtasks simultaneously, potentially leading to a more robust model that better explains its decisions."
- Why unresolved: The current implementation uses separate models for each subtask (Orca 2 for emotion detection; XLM-R/mT5 for trigger words), and no joint training experiments were conducted.
- What evidence would resolve it: Train a unified model (e.g., multi-task XLM-R or mT5) with shared representations for both tasks and compare against the independent baselines on the same test sets.

### Open Question 2
- Question: How robust is the alignment-free label projection method to translation errors compared to alignment-based approaches like fastAlign?
- Basis in paper: [inferred] The paper acknowledges that "trigger word detection tasks may be more prone to translation errors, which could diminish the benefits of translated data for these tasks" and that sentences are discarded when special symbols are omitted during translation.
- Why unresolved: No comparison was made between the alignment-free method and alignment-based methods on trigger word projection quality or downstream task performance.
- What evidence would resolve it: Systematic comparison of label projection accuracy using fastAlign vs. alignment-free method on a held-out parallel corpus, followed by downstream task evaluation.

### Open Question 3
- Question: Can more sophisticated multilingual LLMs (e.g., LLaMA 2 with multilingual fine-tuning) close the performance gap between English-centric models and natively multilingual models for cross-lingual emotion detection?
- Basis in paper: [inferred] The paper observes that "Orca 2 benefits more from the translated data than the XLM-R and mT5 models, likely because the Orca 2 is pre-trained mostly on English data" while the multilingual models are "already exposed to multiple languages."
- Why unresolved: Only Orca 2 (English-centric) was tested among LLMs, and no multilingual LLM variants were evaluated.
- What evidence would resolve it: Fine-tune multilingual-adapted LLMs (e.g., BLOOM, mGPT, or LLaMA 2 with multilingual instruction tuning) on the same data and compare cross-lingual transfer performance.

## Limitations

- Translation-based label projection introduces uncontrolled noise through symbol loss and translation errors, with no reported discard rate or per-language analysis.
- The quantitative impact of trigger word switching is partially unverified, lacking ablation studies on intermediate dataset combinations.
- The claim that Orca 2's multilingual capabilities are sufficient despite English-centric pre-training lacks empirical per-language performance validation.

## Confidence

**High Confidence**: The ranking results (1st in numerical trigger word detection, 3rd in binary trigger word detection, 7th in emotion detection) are directly supported by official WASSA-2024 evaluation metrics.

**Medium Confidence**: The mechanism explanations for why trigger word switching improves numerical detection and why alignment-free projection works adequately are plausible but not rigorously validated through ablation studies.

**Low Confidence**: The claim that Orca 2's multilingual capabilities are sufficient for cross-lingual emotion detection despite being "pre-trained mostly on English data" lacks empirical support and per-language performance analysis.

## Next Checks

1. **Translation Quality Validation**: Implement a sample-level analysis comparing gold trigger words to projected trigger words across all target languages. Calculate the proportion of sentences where symbol marking survives translation and measure label agreement between source and target languages.

2. **Component Ablation Study**: Systematically test intermediate dataset combinations for numerical trigger detection: (a) DS+DT vs DS+DT+DSt vs DS+DT+DTs vs DS+DT+DSt+DTs. This would isolate whether trigger switching provides additive benefit beyond simple translation augmentation.

3. **Architecture Sensitivity Analysis**: Compare the performance of XLM-R and mT5 on both binary and numerical trigger detection tasks. Currently, XLM-R excels at binary (59.19) while mT5 excels at numerical (70.52), but no explanation is provided for this architectural preference.