---
ver: rpa2
title: 'BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language
  Models'
arxiv_id: '2510.00307'
source_url: https://arxiv.org/abs/2510.00307
tags:
- bias
- selection
- tool
- language
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses bias in LLM tool selection, where models systematically
  favor certain APIs over functionally equivalent alternatives due to superficial
  cues like metadata or prompt order, harming fairness and user experience. The authors
  construct a benchmark of 10 API clusters (5 tools each) and 1,000 user queries,
  then measure selection bias using total variation from uniform.
---

# BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models

## Quick Facts
- arXiv ID: 2510.00307
- Source URL: https://arxiv.org/abs/2510.00307
- Reference count: 40
- Models exhibit substantial tool-selection bias (δ_model ~0.3–0.4) favoring semantic similarity and prompt position over functional quality.

## Executive Summary
This paper identifies and addresses systematic bias in LLM tool selection, where models favor certain APIs over functionally equivalent alternatives based on superficial cues like metadata or prompt order. Across seven models, all show substantial bias in selecting among functionally equivalent tools, with semantic alignment between queries and tool descriptions being the strongest predictor. The authors propose a lightweight mitigation approach that maintains high coverage (~89% recall) while reducing bias by ~75%.

## Method Summary
The authors construct a benchmark of 10 API clusters (5 tools each) and 1,000 user queries, measuring selection bias using total variation from uniform. They evaluate seven models across three bias metrics: API-level bias (δ_API), positional bias (δ_pos), and overall bias (δ_model). Controlled metadata perturbations show description corruption greatly shifts choices, while pre-training exposure to single API metadata amplifies its selection. The mitigation approach uses a subset selector to identify relevant APIs, then samples uniformly among them.

## Key Results
- All seven models show substantial selection bias (δ_model ~0.3–0.4) favoring certain APIs regardless of functional equivalence
- Semantic similarity between queries and tool descriptions is the strongest predictor of selection (r = +0.330 to +0.411)
- Metadata perturbation experiments demonstrate that description corruption reliably shifts selection behavior
- The mitigation approach maintains high coverage (~89% recall) while reducing bias by ~75%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs systematically favor tools based on surface-level semantic alignment between user queries and tool descriptions rather than actual functional quality.
- Mechanism: Models extract embeddings of query text and tool metadata (names, descriptions, parameters), then preferentially select tools whose descriptions have higher semantic similarity to the query. This operates as a pattern-matching heuristic that bypasses evaluation of true service utility, reliability, or cost.
- Core assumption: The model's selection process treats semantic resemblance as a proxy for relevance without verifying functional equivalence or service quality.
- Evidence anchors:
  - [abstract] "Semantic alignment between user queries and tool descriptions is the strongest predictor of choice, and perturbing descriptions reliably shifts selection behavior."
  - [Section 4.3, Table 4] Semantic similarity between queries and API descriptions shows the highest positive correlation with selection rates across models (r = +0.330 to +0.411, p < 0.05 for Gemini and Qwen).
  - [corpus] ToolTweak (arXiv:2510.02554) confirms tool selection can be systematically manipulated via metadata changes, supporting the fragility of semantic-based selection.
- Break condition: If semantic alignment were the sole driver, scrambling all descriptions should produce near-uniform selection. The paper shows bias persists under minimal semantic signal (full scramble condition), indicating residual priors remain.

### Mechanism 2
- Claim: Tool ordering in prompts creates systematic positional bias where earlier-listed tools receive disproportionate selection probability.
- Mechanism: LLMs exhibit anchoring effects during sequential attention over tool lists, assigning higher implicit scores to tools processed first. This compounds with API-level preferences, producing two distinct bias regimes: (1) strong API preference with weak positional effects, or (2) weak API preference with strong positional reliance.
- Core assumption: Models lack invariant preference orderings independent of presentation context.
- Evidence anchors:
  - [abstract] "Models either fixate on a single provider or disproportionately prefer earlier-listed tools in context."
  - [Section 4.2] "Positional bias persists even when API-level preference is weak... we notice two regimes: high API bias with low positional bias, or low API bias with high positional bias."
  - [corpus] Quantifying and Mitigating Selection Bias in LLMs (arXiv:2511.21709) documents analogous positional bias in MCQ tasks where answer position influences selection.
- Break condition: Cyclic rotation (ensuring each tool appears at each position once) should eliminate positional bias if purely presentation-driven. The paper finds cyclic vs. random permutations produce similar outcomes, suggesting intrinsic API preferences dominate but positional effects remain measurable (δ_pos = 0.17–0.44 across models).

### Mechanism 3
- Claim: Biased pre-training exposure to specific API metadata creates persistent selection preferences that amplify existing biases.
- Mechanism: When training data contains repeated mentions of a particular tool's metadata (name, description, parameters), the model develops stronger prior associations with that endpoint. During inference, this learned prior biases token generation toward the familiar tool even when functionally equivalent alternatives are presented.
- Core assumption: The pre-training corpus contains asymmetric representation of tool-related content that creates non-uniform priors.
- Evidence anchors:
  - [abstract] "Repeated pre-training exposure to a single endpoint amplifies bias."
  - [Section 4.3, Figure 17] Biased continued pre-training on Qwen3-8B with 3.5M tokens saturated with one endpoint raised its selection share from 0.006 to 0.128 (+20× relative, +12.2 percentage points absolute).
  - [corpus] Weak direct corpus evidence on pre-training-induced tool bias; related work focuses on fine-tuning interventions rather than pre-training exposure effects.
- Break condition: If pre-training exposure fully determined preference, the target endpoint would dominate selection after full training. It reaches only ~12.8%, indicating pre-training is one contributing factor but does not fully explain observed bias patterns.

## Foundational Learning

- **Total Variation Distance**
  - Why needed here: This metric quantifies how far a model's empirical selection distribution deviates from ideal uniform selection. Understanding TVD is essential for interpreting δ_API, δ_pos, and δ_model values reported throughout the paper.
  - Quick check question: If a model selects among 5 tools with probabilities [0.4, 0.3, 0.2, 0.1, 0.0], what is the TV distance from uniform?

- **Positional/Anchoring Bias**
  - Why needed here: The paper documents that tool ordering affects selection independent of tool quality. This connects to broader cognitive bias literature on anchoring effects and has practical implications for how tool lists should be presented.
  - Quick check question: When presented with identical items in different orders, would an unbiased selector show the same selection distribution?

- **Retrieval-Augmented Tool Selection**
  - Why needed here: The tool-selection pipeline involves (i) retrieving candidate tools from a database, (ii) inserting metadata into prompts, and (iii) LLM selection. Understanding this pipeline clarifies where bias can be introduced and where interventions should target.
  - Quick check question: At which stage(s) of the tool-selection pipeline could bias potentially enter?

## Architecture Onboarding

- **Component map:**
  User Query → Tool Retrieval (semantic similarity search) → Candidate Tool List (5-10 APIs with metadata) → Prompt Construction (system prompt + tool metadata + query) → LLM Selection (generates tool call with reasoning) → Selected Tool Execution

- **Critical path:** The selection decision point—where the LLM chooses among functionally equivalent tools—is where bias manifests. The mitigation approach decouples capability recognition (subset selector) from final choice (random sampling).

- **Design tradeoffs:**
  - Mitigation preserves task coverage (0.89 micro-recall) but adds latency from subset-selector inference
  - Uniform sampling eliminates bias by construction but removes any legitimate preference signal
  - Higher inference temperature reduces bias modestly (6.5% absolute δ_model reduction) but may affect output quality

- **Failure signatures:**
  - Subset selector under-selects (low recall): some valid tools never enter candidate pool
  - Subset selector over-selects (false positives): incorrect tools enter candidate pool
  - Residual bias after mitigation: model struggles to use certain tools correctly even when alone

- **First 3 experiments:**
  1. Replicate bias measurement on your target model: run the 10-cluster benchmark with cyclic ordering, compute δ_API, δ_pos, δ_model to establish baseline.
  2. Test metadata perturbation sensitivity: scramble descriptions for the most-selected tool in each cluster and measure selection distribution shifts.
  3. Validate mitigation effectiveness: implement the filter-then-sample approach, measure precision/recall of subset selection and reduction in δ_model compared to baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the residual, non-obvious priors that drive tool-selection bias when semantic signals are minimized?
- Basis in paper: [explicit] The authors state that "bias persists under minimal semantic signal (only leaving certain parameter schema fields intact), implying selection behavior sometimes relies on residual, non-obvious priors rather than solely on coherent, human-interpretable heuristics."
- Why unresolved: The regression analysis achieved only modest R² values (<0.4), and random forests failed to improve predictions, indicating current feature sets cannot fully capture what drives selection.
- What evidence would resolve it: Probing experiments on model internals (attention patterns, activation analysis) combined with ablation studies on parameter schema structures to identify which latent cues models attend to when descriptions are corrupted.

### Open Question 2
- Question: Would tool-selection bias patterns generalize to non-English queries and API ecosystems beyond RapidAPI?
- Basis in paper: [explicit] The Limitations section acknowledges the study's focus on "APIs from RapidAPI and English queries" as a constraint on generality.
- Why unresolved: All 1,000 queries and 50 APIs were drawn from a single marketplace (RapidAPI) and all queries were English-language, so cross-ecosystem and cross-lingual validity remains untested.
- What evidence would resolve it: Replicating the benchmark methodology on alternative API marketplaces (e.g., Postman, Azure Marketplace) and generating equivalent queries in multiple languages to measure whether bias magnitudes and patterns persist.

### Open Question 3
- Question: Can more expressive predictive models (boosted trees, deep networks) capture higher-order feature interactions that explain additional variance in tool selection?
- Basis in paper: [explicit] The Future Work section proposes "deploying more expressive models (e.g., boosted trees or deep nets) with cross-validation could capture higher-order interactions between tool features, further increasing their explanatory power."
- Why unresolved: Random-forest regressors performed poorly (worse than constant baselines), but the authors attribute this to insufficient feature richness rather than method inadequacy—leaving open whether better features paired with nonlinear models would succeed.
- What evidence would resolve it: Expanding the feature set (e.g., syntactic complexity, embedding-based semantic density, provider reputation signals) and systematically comparing XGBoost, neural networks, and transformer-based predictors with cross-validation to measure R² improvements.

### Open Question 4
- Question: Why does the mitigation approach fail to achieve uniform selection for certain APIs even when they are the only available option?
- Basis in paper: [inferred] The authors observe in the weather forecasting cluster that "one API is still being underutilized, perhaps being an indication that the model has a difficult time using this API correctly even when it is the only API available."
- Why unresolved: The paper does not investigate whether this stems from parameter schema complexity, name/tokenization issues, or latent model aversion—leaving the mechanism unexplained.
- What evidence would resolve it: Isolating the problematic API in single-tool prompting experiments, analyzing tokenization of its metadata, and testing whether reformulating its description/parameters restores selection parity.

## Limitations
- Limited scope of bias detection focusing on semantic and positional bias without comprehensively addressing cost-based preferences, reliability, or geographic constraints
- Single benchmark construction using specific clustering and query generation procedures that may not generalize to all API domains
- Mitigation implementation constraints requiring additional model inference passes and eliminating legitimate quality preferences

## Confidence

**High confidence** - The documented bias patterns (δ_model ~0.3-0.4 across seven models, semantic similarity as strongest predictor, positional effects). The experimental methodology (cyclic ordering, 500-run aggregation) is rigorous and reproducible.

**Medium confidence** - The proposed mitigation mechanism's practical effectiveness. While recall (~89%) and bias reduction (~75%) are impressive, real-world deployment may face challenges with subset selector accuracy and added latency.

**Low confidence** - Claims about pre-training-induced bias mechanisms. The Qwen3-8B CPT experiment shows bias amplification, but this is a single model with limited token saturation and doesn't establish general principles about pre-training's role.

## Next Checks

1. **Cross-domain validation** - Apply the benchmark methodology to API clusters outside the tested domains (geocoding, weather, etc.) to verify bias patterns persist across different tool types and usage contexts.

2. **Ground truth verification** - Manually validate that all APIs within each cluster truly provide equivalent functionality by testing representative queries across all alternatives, ensuring the benchmark measures bias rather than legitimate preference.

3. **Long-tail performance analysis** - Measure subset selector performance specifically on edge cases and complex queries where selection is most challenging, as overall 89% recall may mask significant failures in difficult scenarios.