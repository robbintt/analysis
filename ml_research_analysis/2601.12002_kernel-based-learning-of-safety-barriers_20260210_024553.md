---
ver: rpa2
title: Kernel-Based Learning of Safety Barriers
arxiv_id: '2601.12002'
source_url: https://arxiv.org/abs/2601.12002
tags:
- safety
- systems
- barrier
- control
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a data-driven approach for safety verification
  and control synthesis of black-box stochastic systems using kernel-based methods.
  The key innovation is leveraging conditional mean embeddings (CMEs) to embed system
  dynamics into reproducing kernel Hilbert spaces (RKHS), enabling distributionally
  robust barrier certificate computation without explicit model knowledge.
---

# Kernel-Based Learning of Safety Barriers

## Quick Facts
- **arXiv ID**: 2601.12002
- **Source URL**: https://arxiv.org/abs/2601.12002
- **Reference count**: 24
- **Key outcome**: Introduces a data-driven approach using kernel methods and spectral Fourier expansion to compute safety certificates for black-box stochastic systems with distributional robustness guarantees.

## Executive Summary
This paper presents a novel data-driven framework for safety verification and control synthesis of discrete-time stochastic systems without requiring explicit dynamics models. The key innovation is using conditional mean embeddings (CMEs) to represent system dynamics in reproducing kernel Hilbert spaces (RKHS), enabling distributionally robust computation of control barrier certificates (CBCs). The approach constructs an ambiguity set around empirical CME estimates and uses a spectral method with finite Fourier expansions to transform the typically intractable semi-infinite optimization into a tractable linear program. This significantly reduces computational complexity compared to spatial discretization methods while providing probabilistic safety guarantees that hold with high confidence even for out-of-distribution behavior.

## Method Summary
The method leverages conditional mean embeddings to embed system dynamics into RKHS without requiring explicit models. Given trajectory data, it computes empirical CMEs and constructs an RKHS ambiguity set for distributionally robust barrier certificate computation. The barrier function is represented as a truncated Fourier series expansion, converting the semi-infinite program into a tractable linear program via the fast Fourier transform. The approach uses trigonometric bounding theorems to certify constraints over the continuous domain through discrete sampling. The resulting linear program yields barrier coefficients and safety probability bounds that are robust to distributional shifts in the data.

## Key Results
- Demonstrates 7-9x reduction in computation time compared to spatial discretization on a 3D benchmark system
- Successfully computes safety certificates for a neural network-controlled overtaking scenario with guaranteed lower bounds on safety probability
- Achieves near-identical results between the spectral approach and spatial discretization while maintaining computational efficiency
- Shows that increasing Fourier basis size M improves barrier expressiveness at linear cost in variables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional mean embeddings (CMEs) enable data-driven computation of safety constraints without explicit dynamics models.
- Mechanism: CMEs embed conditional probability measures into an RKHS, reformulating the stochastic CBC constraint E[B(X⁺)|X=x, U=u] as an inner product ⟨B, μ(x,u)⟩. This transforms the problem from requiring closed-form dynamics to requiring only trajectory data.
- Core assumption: The CME of the transition kernel is well-specified in the latent hypothesis space G (Assumption 1).
- Evidence anchors:
  - [abstract] "We use conditional mean embeddings to embed data from the system into a reproducing kernel Hilbert space (RKHS)"
  - [section 3.2, Proposition 2] Empirical CME formula with convergence guarantee: E[||μ̂ᴺ − μ||] → 0
  - [corpus] Weak: neighboring papers use GP-based or other kernel methods for safety but don't explicitly reference CME theory.
- Break condition: If the true transition kernel lies outside the RKHS hypothesis space G, or if samples are highly non-i.i.d. without correction, the empirical CME may not converge, breaking all downstream guarantees.

### Mechanism 2
- Claim: Control barrier certificates (CBCs) provide finite-horizon safety lower bounds via supermartingale decay conditions.
- Mechanism: A CBC B: X → ℝ≥0 must satisfy: (a) B(x₀) ≤ η on initial states, (b) B(xᵤ) ≥ 1 on unsafe states, and (c) E[B(X⁺)|x,u] - B(x) ≤ c. If satisfied, P(safe) ≥ 1 - (η + cT). The "c" parameter allows controlled expected increase per step.
- Core assumption: The system admits a supermartingale-like structure; B is sufficiently regular to exist.
- Evidence anchors:
  - [section 2.3, Definition 2] Formal CBC definition with three conditions
  - [section 2.3, Proposition 1] "Pˢᵃᶠᵉ(M) ≥ 1 - (η + cT)" — the core safety bound formula
  - [corpus] Consistent with related work; "Safely Learning Controlled Stochastic Dynamics" and "Data-Driven Safety Verification using Barrier Certificates" use similar CBC formulations.
- Break condition: If no function B satisfying all three conditions exists (e.g., dynamics are too chaotic, unsafe set too close to initial set), the LP becomes infeasible and no certificate can be synthesized.

### Mechanism 3
- Claim: Truncated Fourier expansion converts the semi-infinite optimization into a tractable linear program.
- Mechanism: The squared-exponential kernel admits a Fourier series expansion. By truncating to M wavenumbers, the barrier B becomes a trigonometric polynomial with 2M+1 coefficients. Trigonometric bounding theorems (via Vallée-Poussin kernels) allow sampling on a discrete lattice to certify constraints over the continuous domain.
- Core assumption: The truncated basis captures sufficient barrier expressiveness; the sampling rate satisfies Nyquist conditions (Q̃ ≥ 2f_max + 1).
- Evidence anchors:
  - [section 7.1, Eq. 21] B(x) = ϕ_M(x)ᵀb — finite Fourier series representation
  - [section 7.2, Lemma 1-2] Trigonometric bounds linking discrete samples to continuous guarantees
  - [corpus] No direct corpus evidence; spectral barrier approach appears novel to this paper.
- Break condition: If M is too small (barrier under-parameterized) or sampling lattice too coarse (violating f_max constraint), certified bounds may be invalid or excessively conservative.

## Foundational Learning

- **Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: The entire CME framework and barrier function space are defined in RKHS. Understanding the reproducing property f(x) = ⟨f, k(·,x)⟩ is essential to see how conditional expectations become inner products.
  - Quick check question: Given kernel k(x,x') and function f ∈ H_k, can you explain why E[f(X)] = ⟨f, μ_k(p)⟩_{H_k}?

- **Control Barrier Certificates (CBCs)**
  - Why needed here: CBCs are the theoretical object being synthesized. Understanding the three conditions (initial bound, unsafe bound, decay condition) and how they relate to safety probability is prerequisite.
  - Quick check question: For a CBC with η=0.3, c=0.05, and T=10, what is the minimum certified safety probability?

- **Semi-Infinite Programming**
  - Why needed here: The core problem has infinitely many constraints (one per continuous state). Understanding why this is hard and how discretization/bounding works is key to appreciating the Fourier solution.
  - Quick check question: Why does constraint (15d) "∀x∈X" make this a semi-infinite program rather than a standard LP?

## Architecture Onboarding

- **Component map:**
  Data Collector -> CME Estimator -> Spectral Basis Generator -> LP Builder -> Barrier Solver -> Safety Certifier

- **Critical path:**
  Data → Gram matrices → CME estimator → H matrix (via FFT) → LP constraints → Solve LP → Extract (η, c, b) → Report safety probability

- **Design tradeoffs:**
  | Parameter | Increase Effect | Decrease Effect |
  |-----------|-----------------|-----------------|
  | M (basis size) | Higher expressiveness, O(M) variables | May miss valid barriers |
  | N (samples) | Better CME accuracy | Higher CME computation O(N²) |
  | ε (robustness radius) | Stronger OOD guarantees | Lower certified P_safe |
  | Q̃ (lattice resolution) | Tighter bounds | O(Q̃ⁿ) lattice points |

- **Failure signatures:**
  - **LP infeasible**: Barrier cannot satisfy all constraints → try larger M, smaller ε, or reconsider specification
  - **η + cT ≥ 1**: Certified probability ≤ 0 → specification too tight for given data
  - **||b|| > B̄**: Barrier norm exceeds assumed bound → increase B̄ or check basis
  - **Poor Monte Carlo agreement**: Certified bound far below empirical → conservatism from ε or discretization

- **First 3 experiments:**
  1. **Sanity check (1D, small M)**: Implement on a 1D linear system with known safe dynamics. Verify that certified P_safe approaches Monte Carlo estimate as N increases and ε → 0. This validates the CME→LP→certification pipeline.
  2. **Robustness ablation (ε sweep)**: On the overtaking benchmark, sweep ε ∈ {0, 0.005, 0.01, 0.021} and plot P_safe vs. ε. Reproduce Figure 9 to confirm the conservatism trade-off.
  3. **Basis size ablation (M sweep)**: On the complex safety benchmark, vary M and record both certified P_safe and solve time. Reproduce Figure 6ab to understand expressiveness vs. scalability trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Fourier barrier approach be extended to general black-box control synthesis where control inputs enter the kernel non-linearly?
- Basis: [explicit] The authors state in the concluding remarks that the linear formulation does not generally extend to black-box control synthesis because the control input enters the constraint non-linearly through the input kernel.
- Why unresolved: The synthesis problem prevents the recasting of the optimization into the linear program (LP) used for verification. Current solution methods like gradient-based solvers lack the deterministic convergence guarantees provided by the LP approach for verification.
- What evidence would resolve it: A framework that formulates the synthesis problem as a tractable optimization problem (e.g., a specific class of convex or mixed-integer programs) or a custom semi-infinite solver that provides deterministic convergence guarantees for non-linear inputs.

### Open Question 2
- Question: How can sharp ambiguity set radii ε be determined with high confidence to reduce the conservatism of the resulting safety bounds?
- Basis: [explicit] The paper notes that further investigation is needed to reduce conservatism and determine sharp radii ε, as existing theoretical bounds based on concentration inequalities are often excessively conservative in practice.
- Why unresolved: While minimax rates exist, they are currently too loose for practical use, forcing reliance on manual tuning or bootstrapping which lacks the theoretical rigor of the concentration bounds.
- What evidence would resolve it: A data-driven method or a refined concentration theorem that provides a tight lower bound for the safety probability, yielding results comparable to Monte Carlo estimates without manual calibration.

### Open Question 3
- Question: What are principled strategies for selecting an optimal spectral basis and finite frequency bandwidth to minimize the abstraction error?
- Basis: [explicit] The authors identify quantifying the induced abstraction error and identifying strategies for selecting an optimal spectral basis as open challenges for future work.
- Why unresolved: The current approach relies on standard choices (e.g., capturing 99.73% of the spectral measure), but lacks a method to optimally select wavenumbers or frequencies specifically tailored to the system dynamics or the safety specification.
- What evidence would resolve it: An algorithmic procedure that adaptively selects the Fourier basis to maximize the certified safety probability or minimize the number of basis functions required to achieve a specific verification threshold.

## Limitations
- The Fourier basis expressiveness assumption is never empirically validated for complex geometries like neural network-controlled overtaking scenarios
- Computational advantages over spatial discretization are only demonstrated for 3D systems; scaling to higher dimensions remains unverified
- CME convergence guarantees assume bounded features, but the Fourier feature map may have unbounded components for large M

## Confidence
- **CME-based safety verification**: High confidence - well-established in kernel methods literature with standard RKHS convergence proofs
- **Spectral barrier computation**: Medium confidence - novel approach with theoretical soundness but practical performance depends heavily on basis selection
- **Computational efficiency**: Low confidence - claims of O(M) vs O(Q^n) complexity only demonstrated for n=3 dimensions

## Next Checks
1. **Basis expressiveness validation**: On the overtaking benchmark, visualize the learned barrier B(x) and compare against the true safety boundary. Compute the approximation error ||B_approx - B_true|| to verify Assumption 3 holds in practice.
2. **Dimensionality scaling study**: Apply the method to 4D and 5D benchmark systems (e.g., quadrotor models) and report solve time vs. M, Q, and n. Compare against spatial discretization baselines to verify claimed computational advantages.
3. **Robustness to trajectory distribution**: Generate trajectory data from multiple distributions (e.g., different control policies) and test whether the certified P_safe bound degrades gracefully or catastrophically. This validates the distributional robustness claims beyond the stated theoretical guarantees.