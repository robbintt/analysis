---
ver: rpa2
title: 'Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and
  Paired Modality Integration'
arxiv_id: '2510.07035'
source_url: https://arxiv.org/abs/2510.07035
tags:
- molecular
- data
- pre-training
- learning
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "FlexMol addresses the limitation of existing molecular pre-training\
  \ methods that require paired 2D and 3D data by proposing a unified framework that\
  \ supports flexible single-modality input. The core method employs separate 2D and\
  \ 3D models with parameter sharing, 2D\u21923D and 3D\u21922D decoders to generate\
  \ missing modality features, and a multi-modal encoder to align and fuse representations."
---

# Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration

## Quick Facts
- **arXiv ID:** 2510.07035
- **Source URL:** https://arxiv.org/abs/2510.07035
- **Authors:** Tengwei Song; Min Wu; Yuan Fang
- **Reference count:** 40
- **Key result:** ROC-AUC of 75.1% on BBBP, outperforming larger models pre-trained on datasets exceeding 10M samples.

## Executive Summary
FlexMol introduces a unified framework for molecular pre-training that supports both single-modality (2D or 3D) and paired 2D/3D inputs. Unlike existing methods requiring paired data, FlexMol uses separate 2D and 3D models with parameter sharing, cross-modal decoders, and a multi-modal encoder to align and fuse representations. Trained on 3.4M paired and 2M single-modality samples, it achieves superior performance on molecular property prediction tasks and excels in conformation generation tasks.

## Method Summary
FlexMol employs a two-stage training process. Stage 1 uses paired 2D/3D data with SE(3) Transformers sharing self-attention parameters and contrastive learning to align modalities. Stage 2 performs continual learning on single-modality data by freezing the available modality's feature learner and using decoders to generate missing modality features. The framework supports flexible input handling and uses LoRA (rank=64) for efficient fine-tuning on downstream tasks.

## Key Results
- Achieves ROC-AUC of 75.1% on BBBP and 85.7% on BACE property prediction tasks
- Excels in conformation generation with COV of 97.25% and MAT of 0.189Å on QM9
- Outperforms larger models pre-trained on datasets exceeding 10M samples despite using only 5.4M total samples
- Supports flexible single-modality input while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-modal decoders enable robust training on mixed paired and unpaired molecular data by synthesizing missing modality representations.
- **Mechanism:** The framework utilizes a two-stage training process. In Stage 1 (paired data), it learns to align 2D and 3D representations. In Stage 2 (single-modality data), if only 2D data is available, the frozen 3D Feature Learner (3D-FL) acts as a supervisory target for the 2D→3D decoder. This decoder generates a "pseudo" 3D representation (ŷ) and atom-pair representation (Q̂) from the 2D input, allowing the Multi-Modal (MM) Encoder to continue learning a fused representation despite the missing real 3D input.
- **Core assumption:** The decoder-generated features (ŷ) are sufficiently isomorphic to ground-truth 3D features that the MM Encoder can extract meaningful cross-modal correlations without "ground truth" 3D data present in Stage 2.
- **Evidence anchors:**
  - [abstract]: "...utilizes a decoder to generate features for the missing modality. This enables a multistage continuous learning process..."
  - [section 3.2]: "In the 2D-only scenario, we first utilize the frozen 3D-FL... to generate the 3D molecular representation ŷ. This serves as the supervision signal for 2D→3D decoder..."
  - [corpus]: "TextME" (Corpus) discusses bridging unseen modalities, supporting the general feasibility of cross-modal synthesis, though specific validation for molecular geometry is distinct here.
- **Break condition:** If the "Representation Alignment Loss" (Eq. 5) is not sufficiently minimized in Stage 1, the frozen FL will provide poor supervision in Stage 2, causing the decoders to hallucinate noisy 3D features that degrade the MM Encoder.

### Mechanism 2
- **Claim:** Shared self-attention parameters force 2D and 3D modalities into a unified geometric-topological latent space, preventing modality collapse.
- **Mechanism:** FlexMol uses separate encoders for 2D (using Shortest Path Distance) and 3D (using Gaussian kernels on Euclidean distance) but shares the self-attention parameters. By applying Contrastive Loss (L_cl) on the outputs of these encoders, the model maximizes mutual information between the paired modalities, ensuring that the learned representations preserve the correspondence between graph topology and spatial geometry before fusing them.
- **Core assumption:** The relationship between 2D topology (bond connections) and 3D geometry (spatial coordinates) is consistent enough that a shared attention mechanism can effectively project both into a common embedding space without losing modality-specific nuances.
- **Evidence anchors:**
  - [abstract]: "...inspired by the unified structure in vision-language models... leverages parameter sharing to improve computational efficiency..."
  - [section 3.1.2]: "To process 2D and 3D modalities efficiently, the encoder shares self-attention parameters... The 2D and 3D representations are further aligned using contrastive learning..."
  - [corpus]: "Towards Unified and Lossless Latent Space..." (Corpus) validates the industry trend toward unified spaces for molecular data.
- **Break condition:** Removal of the Contrastive Similarity (CS) Loss leads to significant performance degradation (Table 4, "w/o CS Loss": BBBP drops from 75.1 to 56.7), indicating the model fails to align features effectively without this constraint.

### Mechanism 3
- **Claim:** Data scaling in Stage 2 is constrained by the alignment quality established in Stage 1.
- **Mechanism:** The paper outlines a "multistage continuous learning" approach. The model first learns cross-modal alignment on 3.4M paired samples (Stage 1). It then scales up using 2M single-modality samples (Stage 2). However, the utility of Stage 2 is bounded; if the volume of single-modality data significantly exceeds the paired data, the model overfits to the single modality and degrades because the "alignment manifold" learned in Stage 1 cannot support the uncorrelated scale of the new data.
- **Core assumption:** The cross-modal correlations learned in Stage 1 are robust enough to guide the optimization of single-modality data in Stage 2 up to a capacity limit.
- **Evidence anchors:**
  - [section 4.4]: "We further observe that the performance starts to decline when the size grows toward 4M... the upper bound of performance gains on single-modal data is constrained by the amount of paired data used in Stage 1."
  - [abstract]: "...trained on 3.4M paired and 2M single-modality samples..."
  - [corpus]: "scMRDR" (Corpus) highlights the difficulty of integrating unpaired data, aligning with FlexMol's observed limitations on scaling unpaired data.
- **Break condition:** If a practitioner scales Stage 2 data (e.g., to 10M samples) without proportionally increasing Stage 1 paired data, performance will plateau or degrade due to overfitting on the single modality.

## Foundational Learning

- **Concept: SE(3) Equivariance**
  - **Why needed here:** The 3D branch of FlexMol processes molecular coordinates. Rotating or translating a molecule should not change its chemical properties. SE(3) equivariance ensures the model learns geometric laws, not just coordinate memorization.
  - **Quick check question:** If I rotate a molecule input by 45 degrees, does the model's predicted toxicity score remain unchanged?

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here:** This is the "glue" for the two modalities. It pulls the representation of a 2D graph and its corresponding 3D conformation closer in the latent space while pushing apart non-matching pairs.
  - **Quick check question:** Does the model minimize the distance between the embedding of a caffeine molecule's 2D graph and its 3D structure, while maximizing the distance between caffeine's 2D and aspirin's 3D?

- **Concept: Attention Bias (Pair Representations)**
  - **Why needed here:** Standard Transformers treat inputs as sets. Molecules are graphs. FlexMol injects structural data (2D Shortest Path Distance or 3D Euclidean Distance) into the attention mechanism so the model "sees" the molecular geometry, not just atom bags.
  - **Quick check question:** Can the attention mechanism distinguish between two atoms that are adjacent in the graph sequence versus two atoms that are chemically bonded?

## Architecture Onboarding

- **Component map:** Inputs (2D Graph X,E & 3D Conformation X,R) -> Feature Learners (MLPs) -> Encoders (SE(3) Transformers, F layers, shared parameters) -> Cross-Modal Decoders (2D→3D and 3D→2D, F layers) -> Multi-Modal (MM) Encoder -> Heads (Coordinate, Atom-type, SPD, Pair-distance)

- **Critical path:**
  1. **Alignment (Stage 1):** Train on Paired Data (PCQM4Mv2). Do not freeze FLs.
  2. **Continual Learning (Stage 2):** Switch to Single-Modality Data (Unimol-2M). Freeze the FL of the missing modality. Use Decoders to hallucinate the missing features.
  3. **Fine-tuning:** Apply LoRA (rank 64) to the backbone while training a task-specific head.

- **Design tradeoffs:**
  - **Decoders:** Adding them increases latency (Table 7: 28ms vs 18ms for Uni-Mol) and parameters (112M vs 47M), but is strictly required to support unpaired data. If you only have paired data, you might skip Stage 2, but you lose the "Flex" in FlexMol.
  - **Shared Attention:** Reduces parameters significantly (112M vs potential 248M) but risks conflating distinct geometric/topological patterns if alignment is poor.

- **Failure signatures:**
  - **Overfitting in Stage 2:** If validation loss plateaus while training loss drops on single-modality data, check the ratio of Paired:Single data. The paper suggests ~1.7:1 (3.4M : 2M) is safe, but 1:2 (3.4M : >4M) causes degradation.
  - **Modality Collapse:** If "w/o 3D Feature" results occur (Table 4), the 2D encoder is ignoring the 3D guidance. Ensure Contrastive Loss is active.

- **First 3 experiments:**
  1. **Sanity Check (Stage 1):** Train only on PCQM4Mv2 (Paired) with Contrastive Loss. Verify that 2D and 3D embeddings for the same molecule have high cosine similarity (>0.8).
  2. **Decoder Validation:** Run Stage 2 (2D-only). Freeze 3D-FL. Check the "Encoder-Decoder Consistency Loss" (Lc). If it diverges, the decoder is failing to predict 3D features from 2D inputs.
  3. **Downstream Benchmark:** Fine-tune on BBBP (Classification) and QM9 (Regression). Compare "FlexMol+2D" vs "FlexMol+3D" to ensure the single-modality fine-tuning actually leverages the pre-trained cross-modal knowledge.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be adapted to prevent performance degradation when the volume of single-modality data significantly exceeds that of the paired data?
- **Basis in paper:** [explicit] The authors note that "when the amount of single-modal data substantially exceeds that of paired data, performance deteriorates due to potential overfitting to single-modal data, which restricts scaling to larger single-modal datasets."
- **Why unresolved:** The current architecture relies on a fixed amount of paired data (Stage 1) to ground the representations, capping the effective utility of cheap, abundant single-modality data (Stage 2).
- **Evidence:** A training strategy or regularizer that allows the model to continue improving as single-modality data scales to tens of millions without requiring equivalent paired data growth.

### Open Question 2
- **Question:** What is the optimal curriculum or balancing strategy for integrating paired and single-modality data within a unified training process?
- **Basis in paper:** [explicit] The authors list "balancing paired and single-modal data" as a focus for future work to enhance modality alignment and enable large-scale learning.
- **Why unresolved:** The current sequential two-stage approach (paired then single) leads to performance plateaus, suggesting the separation of these phases or the ratio of data types is suboptimal.
- **Evidence:** A comparative study of various data mixing ratios or dynamic curricula that demonstrates superior convergence over the current sequential approach.

### Open Question 3
- **Question:** Can the cross-modality decoders be optimized to avoid the slight performance degradation observed during the initial paired-data pre-training stage?
- **Basis in paper:** [inferred] Table 5 shows that removing decoders in Stage 1 slightly improves performance (e.g., ROC-AUC 72.7 vs 72.3), indicating the reconstruction task creates a trade-off with the primary encoding objective.
- **Why unresolved:** The decoders are architecturally necessary for the second stage's single-modality handling, yet they introduce noise or conflicting gradients during the first stage.
- **Evidence:** An architectural variant (e.g., detached gradients or separate decoder heads) that preserves Stage 1 performance while retaining the cross-modal generation capabilities required for Stage 2.

## Limitations

- **Dataset Bias:** FlexMol's performance gains are demonstrated primarily on datasets (BBBP, BACE, QM9) with relatively small molecule sizes (<29 atoms). The framework's effectiveness on larger drug-like molecules or proteins remains untested, which is a significant gap for pharmaceutical applications.

- **Decoder Dependency:** The cross-modal decoders are essential for single-modality training but introduce computational overhead (28ms vs 18ms inference time, 112M vs 47M parameters). This raises questions about whether the flexibility benefit justifies the cost in resource-constrained deployment scenarios.

- **Alignment Stability:** The framework assumes that contrastive alignment in Stage 1 remains stable during Stage 2's single-modality training. However, if the decoder-generated features drift significantly from true representations, the MM Encoder could accumulate systematic errors that only manifest during downstream tasks.

## Confidence

- **High Confidence:** The claim that FlexMol outperforms existing single-modality models (Uni-Mol, MolCLR) on BBBP (75.1% ROC-AUC) and BACE (85.7% ROC-AUC) is well-supported by the ablation studies and direct comparisons provided in the paper.

- **Medium Confidence:** The assertion that Stage 2 scaling is constrained by Stage 1 paired data quality is plausible given the observed performance plateau, but the exact relationship (e.g., specific ratios) requires further empirical validation across diverse datasets.

- **Low Confidence:** The framework's effectiveness on larger molecules (>29 atoms) and its ability to handle significant distribution shifts between pre-training and downstream domains are not empirically validated, limiting generalizability claims.

## Next Checks

1. **Larger Molecule Validation:** Test FlexMol on datasets containing larger drug-like molecules (e.g., ChEMBL subsets with molecules >50 atoms) to assess scalability beyond the small-molecule QM9/BBBP regime.

2. **Stage 2 Scaling Experiment:** Systematically vary the ratio of paired to single-modality data (e.g., 1:1, 1:2, 1:3) to empirically determine the threshold where Stage 2 degradation begins, providing clearer guidance for practitioners.

3. **Cross-Domain Transfer:** Evaluate FlexMol's performance when pre-training and fine-tuning domains differ significantly (e.g., pre-train on small molecules, fine-tune on protein-ligand complexes) to assess robustness to domain shift.