---
ver: rpa2
title: 'Nationality and Region Prediction from Names: A Comparative Study of Neural
  Models and Large Language Models'
arxiv_id: '2601.08692'
source_url: https://arxiv.org/abs/2601.08692
tags:
- prediction
- nationality
- neural
- region
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive comparison of neural models
  and large language models (LLMs) on nationality and region prediction from personal
  names. Six neural models (SVM, fastText, CNN, BiLSTM, CANINE, XLM-RoBERTa) and six
  LLM prompting strategies (Zero-shot, Few-shot, Chain-of-Thought, Self-Consistency,
  Least-to-Most, Self-Reflection) were evaluated across three granularity levels using
  a dataset of 99 nationalities.
---

# Nationality and Region Prediction from Names: A Comparative Study of Neural Models and Large Language Models

## Quick Facts
- arXiv ID: 2601.08692
- Source URL: https://arxiv.org/abs/2601.08692
- Reference count: 39
- Primary result: LLMs achieved 0.776 accuracy on nationality prediction vs 0.481 for best neural model (SVM)

## Executive Summary
This study compares neural models (SVM, fastText, CNN, BiLSTM, CANINE, XLM-RoBERTa) and large language models (GPT-4.1-mini with six prompting strategies) on predicting nationality, region, and continent from personal names. Using a dataset of 99 nationalities with 75,345 samples, LLMs significantly outperformed neural models across all granularity levels, with Self-Reflection prompting achieving the highest accuracy of 0.776. The performance gap narrowed as granularity coarsened, and error analysis revealed LLMs made qualitatively different "near-miss" errors within correct regions while neural models showed more cross-regional errors and bias toward high-frequency classes.

## Method Summary
The study evaluated six neural models and six LLM prompting strategies on the name2nat dataset, which contains 75,345 romanized names from 99 nationalities (capped at 800 samples per class). Neural models were trained using standard approaches with early stopping, while LLMs used GPT-4.1-mini via API with temperature=1.0 and JSON output formatting. Evaluation occurred at three hierarchical levels: 99 nationalities → 14 regions → 6 continents, using Accuracy, Macro-F1, and Precision@k metrics. The study also stratified results by frequency (Head/Mid/Tail groups) and analyzed error patterns qualitatively.

## Key Results
- LLMs achieved 0.776 accuracy on nationality prediction versus 0.481 for the best neural model (SVM)
- Self-Reflection prompting achieved the highest LLM performance with Accuracy of 0.776 and Macro-F1 of 0.782
- LLMs made "near-miss" errors predicting correct regions even when nationality was incorrect (0.862 within-region rate)
- Neural models showed significant bias toward high-frequency nationalities, with SVM having only 0.001 accuracy gap between Head and Tail groups

## Why This Works (Mechanism)

### Mechanism 1: World Knowledge Activation via Pre-training
LLMs outperform neural models on fine-grained nationality prediction by leveraging cultural-linguistic knowledge acquired during large-scale pre-training, not by learning task-specific statistical patterns. During pre-training on trillions of tokens, LLMs encode structured knowledge about name-nationality relationships (e.g., "-ovich is a patronymic suffix typical of Slavic languages"). At inference, prompts elicit this knowledge without gradient updates. Neural models, by contrast, learn only character-to-label mappings from limited training data.

### Mechanism 2: Self-Correction via Reflective Prompting
Self-Reflection prompting achieves highest accuracy (0.776) because explicit self-evaluation prompts the model to reconsider low-confidence predictions and correct errors. After initial prediction, the model receives a follow-up prompt asking "Is this prediction correct? Are there other possibilities?" This triggers a metacognitive loop where the model generates alternative hypotheses and re-weights probabilities before final output.

### Mechanism 3: Near-Miss Errors from Hierarchical Geographic Understanding
LLMs make qualitatively different errors—"near-miss" errors within the correct region—because they represent names at multiple geographic abstraction levels simultaneously. LLMs appear to first localize names to a cultural-linguistic region, then disambiguate within that region. When fine-grained disambiguation fails, the regional assignment remains correct. Neural models lack this hierarchical representation and fall back to high-frequency class predictions.

## Foundational Learning

- **Character-level vs. Subword Tokenization**
  - Why needed here: SVM and fastText use character n-grams directly; XLM-RoBERTa uses subword tokenization that may fragment name patterns. This affects what features models can access.
  - Quick check question: Given the name "Kowalczyk," what features would a character 3-gram model extract that a WordPiece tokenizer might fragment?

- **Frequency Stratification for Bias Detection**
  - Why needed here: The paper divides nationalities into Head/Mid/Tail groups to reveal that pre-trained models degrade on low-frequency classes—a finding invisible in aggregate accuracy.
  - Quick check question: If a model achieves 80% accuracy overall but 20% accuracy on the bottom third of classes by frequency, what does Macro-F1 reveal that Accuracy hides?

- **Hierarchical Classification Granularity**
  - Why needed here: The three-level evaluation (nationality→region→continent) quantifies whether errors are "close" or "off-target." This matters for real applications where regional accuracy may be acceptable.
  - Quick check question: A model predicts "Portuguese" for a Brazilian name. Is this a worse error than predicting "Japanese"? Why does the paper's framework distinguish these?

## Architecture Onboarding

- **Component map:** Input layer (romanized name string, avg 14.8 chars) → Neural pathways (SVM → fastText → CNN/BiLSTM → Pre-trained) → LLM pathway (GPT-4.1-mini with six prompting strategies) → Output layer (top-k predictions over 99 nationalities, mapped to 14 regions → 6 continents)

- **Critical path:**
  1. Data preprocessing: Filter nationalities with ≥500 samples, cap at 800 per class, stratified split 8:1:1
  2. Neural model training: Train all six baselines with early stopping on validation accuracy
  3. LLM inference: Parallel API calls (50 concurrent), temperature=1.0, JSON output format for top-5 nationalities
  4. Evaluation: Accuracy, Macro-F1, Precision@k, frequency-stratified analysis, error categorization

- **Design tradeoffs:**
  - SVM: Highest frequency robustness (∆H-T=0.001) but lower peak accuracy; use for fairness-critical applications
  - Pre-trained models: Better on regions than SVM but worse on rare nationalities; subword tokenization may fragment name patterns
  - LLMs: Best accuracy but highest cost and opacity; Few-shot surprisingly worse than Zero-shot (example bias)
  - Self-Reflection: Best LLM performance but requires 2× inference calls per input

- **Failure signatures:**
  - Neural models bias toward high-frequency classes (e.g., Mexican for all Spanish-speaking names)
  - LLMs over-attribute to majority nationalities within regions (e.g., American for ambiguous English names)
  - Least-to-Most prompting suffers error propagation across granularity levels
  - Chain-of-Thought can "fixate on incorrect hypotheses" during explicit reasoning

- **First 3 experiments:**
  1. **Baseline replication:** Train SVM and XLM-RoBERTa on the name2nat subset; verify SVM achieves ~0.48 accuracy and XLM-RoBERTa shows ~22% drop rate on Tail nationalities
  2. **Granularity ablation:** Run Zero-shot and Self-Reflection prompts at all three granularity levels; confirm gap narrows from ~0.30 to ~0.07 as granularity coarsens
  3. **Error pattern validation:** Sample 100 incorrect predictions from each approach; manually code whether errors are within-region (near-miss) or cross-regional; expect LLMs >80% within-region, neural models ~60%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the performance advantages and bias patterns observed in GPT-4.1-mini generalize to other large language models (e.g., Claude, Gemini, Llama) with different architectures and pre-training corpora?
- Basis in paper: The authors state in Section 5.2 that "LLM evaluation is limited to a single model" and that verifying applicability to LLMs in general is required.
- Why unresolved: Different models may possess varying degrees of "world knowledge" or different frequency distributions in their pre-training data, which could alter the effectiveness of prompting strategies for this task.
- What evidence would resolve it: A comparative study benchmarking the same nationality prediction task across multiple state-of-the-art LLM families using identical prompting strategies.

### Open Question 2
- Question: How does the performance gap between neural models and LLMs change when processing names in their original scripts (e.g., Chinese characters, Cyrillic) rather than romanized versions?
- Basis in paper: The authors note in Section 5.2 that "evaluation targets are limited to English (romanized) names" and that original scripts have not been tested.
- Why unresolved: Original scripts often contain stronger orthographic signals about nationality than romanized text, potentially reducing the relative value of the LLM's cultural "world knowledge" compared to character-level pattern matching.
- What evidence would resolve it: Experiments evaluating both model types on a dataset containing names in native scripts, analyzing if the granularity performance gap narrows or widens.

### Open Question 3
- Question: Can hybrid ensemble methods that combine the "near-miss" reasoning of LLMs with the frequency robustness of simpler neural models (like SVM) outperform individual models?
- Basis in paper: Section 5.1 suggests that "Ensemble methods that combine predictions from both models... represent a promising direction" due to their complementary error profiles.
- Why unresolved: The error analysis revealed LLMs struggle with specific local patterns where neural models succeed, but it remains untested if a unified system can intelligently switch between or fuse these capabilities.
- What evidence would resolve it: Development and evaluation of a meta-classifier or confidence-based voting system that integrates SVM predictions with LLM outputs.

### Open Question 4
- Question: What specific prompt engineering strategies can successfully mitigate the performance degradation LLMs exhibit for low-frequency nationalities?
- Basis in paper: The authors identify the need for "investigation of prompt design methods to mitigate frequency bias in LLMs" in Section 5.1 and Section 6.
- Why unresolved: The frequency analysis showed LLMs still display bias toward high-frequency nationalities (Head group), indicating that standard prompting does not fully leverage their capacity for rare classes.
- What evidence would resolve it: Testing interventions such as frequency-balanced few-shot examples or explicit debiasing instructions on the "Tail" group of the dataset.

## Limitations
- LLM evaluation is limited to a single model (GPT-4.1-mini), preventing generalization to other architectures and pre-training corpora
- Evaluation targets are limited to English (romanized) names, excluding original scripts that may contain stronger nationality signals
- The study does not directly test whether LLM performance advantages stem from genuine world knowledge versus superficial pattern matching

## Confidence
- **High confidence**: LLM superiority on nationality prediction (0.776 vs 0.481), frequency-based bias patterns showing neural models degrade on Tail nationalities, and the observation that LLM errors are predominantly "near-miss" within correct regions
- **Medium confidence**: The attribution of LLM performance to world knowledge activation during pre-training, as this is inferred from results rather than directly tested
- **Low confidence**: The specific mechanism by which Self-Reflection prompting achieves superior performance, as the paper does not ablate the self-correction process

## Next Checks
1. **Prompt template validation**: Implement and test all six LLM prompting strategies with the exact templates used in the paper to verify reported performance differences
2. **Hierarchical error pattern verification**: Manually analyze 100 incorrect predictions from both neural models and LLMs to confirm the claimed 80%+ within-region error rate for LLMs versus ~60% for neural models
3. **Frequency stratification replication**: Reproduce the Head/Mid/Tail analysis showing neural models' performance degradation on low-frequency nationalities, particularly verifying the 22% drop rate for XLM-RoBERTa on Tail classes