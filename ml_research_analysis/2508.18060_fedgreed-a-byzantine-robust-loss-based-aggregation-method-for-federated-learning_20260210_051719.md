---
ver: rpa2
title: 'FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning'
arxiv_id: '2508.18060'
source_url: https://arxiv.org/abs/2508.18060
tags:
- clients
- client
- adversarial
- server
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedGreed is a Byzantine-robust federated learning method that uses
  a server-side trusted dataset to evaluate and select clients' model updates based
  on their loss values. It employs a greedy selection strategy to aggregate updates
  from clients whose models exhibit the lowest evaluation loss, without requiring
  assumptions about the number of malicious clients.
---

# FedGreed: A Byzantine-Robust Loss-Based Aggregation Method for Federated Learning

## Quick Facts
- arXiv ID: 2508.18060
- Source URL: https://arxiv.org/abs/2508.18060
- Reference count: 25
- Primary result: Byzantine-robust FL method using server-side trusted dataset to evaluate and greedily aggregate client updates, achieving higher centralized accuracy than robust baselines even with 80% malicious clients

## Executive Summary
FedGreed introduces a novel Byzantine-robust aggregation method for federated learning that leverages a server-side trusted dataset to evaluate client model updates based on their loss values. Unlike existing methods that require prior knowledge of malicious client ratios or use distance-based outlier detection, FedGreed employs a greedy selection strategy that only adds clients when their inclusion improves aggregate loss. The method was evaluated across MNIST, FMNIST, and CIFAR-10 datasets under non-IID data distributions and two adversarial attack scenarios, demonstrating superior robustness compared to standard and robust aggregation baselines including Mean, Trimmed Mean, Median, Krum, and Multi-Krum.

## Method Summary
FedGreed operates by having the server maintain a trusted dataset separate from the federated data. After clients perform local training and submit their model updates, the server evaluates each update's loss on the trusted dataset. Clients are then ranked by ascending loss values, and the server greedily aggregates updates starting from the lowest-loss client. At each step, the next-ranked client is added only if the resulting aggregate achieves lower loss than the current aggregate. This process continues until all clients have been considered or no further improvement is possible. The method does not require assumptions about the number of malicious clients and provides theoretical guarantees that the final aggregate achieves at least as good server-side loss as the best individual client update.

## Key Results
- FedGreed maintains higher centralized accuracy than robust baselines (Multi-Krum, Trimmed Mean, etc.) under label flipping attacks with up to 80% malicious clients
- On CIFAR-10 with Dirichlet α=1 and 50% malicious clients, FedGreed achieves 80.1% centralized accuracy versus Multi-Krum's 69.8%
- The method shows consistent performance across MNIST, FMNIST, and CIFAR-10 datasets under both label flipping and Gaussian noise injection attacks
- FedGreed outperforms Krum variants by utilizing multiple honest clients per round rather than selecting only one update

## Why This Works (Mechanism)

### Mechanism 1: Server-Side Loss-Based Trust Scoring
Evaluating client updates against a trusted server-side dataset provides a reliable signal for detecting malicious contributions by transforming Byzantine detection into a supervised evaluation task. The server computes loss values for each client update using its trusted reference dataset, ranking updates by loss with lower values indicating higher likelihood of being benign. This approach assumes the trusted dataset reasonably approximates the target population loss, meaning malicious updates achieving low loss on the trusted set cause limited damage if selected.

### Mechanism 2: Greedy Sequential Aggregation with Progress Guarantees
The greedy selection strategy that only adds clients when aggregate loss improves ensures bounded degradation even under high adversarial ratios. Starting with the lowest-loss client, FedGreed iteratively tests whether adding the next-ranked client improves or maintains the aggregate loss, accepting only if improvement occurs. This guarantees progress at least as good as the best individual client, with the assumption that at least one honest client exists per round.

### Mechanism 3: Independence from Malicious Client Count Assumptions
Unlike distance-based methods like Krum/Multi-Krum that require prior knowledge of maximum malicious clients, FedGreed evaluates each update independently against trusted loss and adaptively determines aggregation based purely on observed loss improvement. This eliminates the need for a priori information about attack scale while maintaining robustness through the greedy selection mechanism.

## Foundational Learning

- **Byzantine Fault Tolerance in Distributed Optimization**: Understanding that Byzantine behavior is unbounded (vs. stochastic noise) clarifies why standard averaging fails and why robust aggregation requires explicit filtering. Quick check: Can you explain why coordinate-wise Median aggregation fails when adversaries coordinate their attacks across dimensions?

- **Non-IID Data Heterogeneity (Dirichlet Partitioning)**: The paper evaluates under Dirichlet-distributed data partitions with concentration parameter α. Lower α values create strongly skewed distributions where clients hold highly imbalanced class proportions, making honest updates appear diverse and harder to distinguish from attacks. Quick check: If client A has 90% class 0 and client B has 90% class 1, why might both appear as outliers under distance-based aggregation?

- **Greedy vs. Global Optimization in Aggregation**: FedGreed selects from N candidate aggregates rather than the exponential space of all subsets. Understanding this approximation explains both computational tractability and why the method may miss optimal combinations. Quick check: Why does selecting only from prefix averages (ordered by loss) potentially exclude better aggregates that mix low-loss and mid-loss clients?

## Architecture Onboarding

- **Component map**: Server -> Trusted Dataset -> Loss Evaluator -> Greedy Selector -> Global Model
- **Critical path**: Server broadcasts global model → clients train locally and send updates → server evaluates fS loss for all updates → server sorts clients by loss → server greedily aggregates starting from lowest-loss client → server sets new global model → proceed to next round
- **Design tradeoffs**: Trusted dataset size vs. privacy (larger datasets improve approximation but require more server-side data); computational overhead vs. robustness (O(N) additional loss evaluations per round); greedy prefix selection vs. optimal subset (considers only N prefix averages vs. exponential space)
- **Failure signatures**: Accuracy collapse under extreme non-IID + Gaussian noise (honest gradients appear noise-like); inconsistent Multi-Krum baseline (abrupt accuracy drops under non-IID); Krum single-client limitation (underutilizes honest participants)
- **First 3 experiments**: 1) Reproduce baseline comparison on MNIST with label flipping using Flower framework; 2) Ablation study varying trusted dataset size on CIFAR-10; 3) Stress test with adaptive attack that minimizes loss on server dataset

## Open Questions the Paper Calls Out

### Open Question 1
How can FedGreed be made robust in highly skewed non-IID scenarios where the best individual honest client's gradient direction may deviate significantly from the average honest gradient and resemble random noise? The authors acknowledge this as a direction for future work regarding Gaussian noise attack failures under high skewness (α=0.1).

### Open Question 2
How does FedGreed perform against coordinated adversarial attacks (e.g., ALIE, IPM) where malicious clients share information to craft statistically coordinated updates? The paper focuses on independent adversarial clients without sharing or leveraging information from others.

### Open Question 3
What are the minimum requirements for the server-side trusted dataset size and distributional alignment with the target population to maintain FedGreed's robustness guarantees? The method assumes fS is a "reasonably good approximation" but no systematic study evaluates performance degradation as trusted dataset shrinks or diverges from true data distribution.

### Open Question 4
Can FedGreed maintain its robustness properties under partial client participation where only a subset of clients are available per round? The greedy selection mechanism assumes access to all N client updates, but performance under stochastic client sampling remains unexplored.

## Limitations
- Performance critically depends on quality of server-side trusted dataset with no analysis of sensitivity to dataset size or representativeness
- Has not been evaluated against sophisticated adaptive attacks that could explicitly minimize loss on server dataset while maintaining malicious behavior
- O(N) additional loss evaluations per round represent significant computational overhead compared to standard FedAvg

## Confidence
- **High confidence**: Server-side loss evaluation for Byzantine detection is well-founded with clear theoretical guarantees when trusted dataset is representative
- **Medium confidence**: Empirical results demonstrate superiority over baselines but limited sample size and lack of ablation studies create uncertainty about generalizability
- **Low confidence**: Claim of robustness "even when only one honest client exists per round" is theoretically justified but practically questionable depending on that client's loss performance

## Next Checks
1. Implement systematic ablation study varying trusted dataset size (50, 100, 500, 1000 samples) to quantify impact on robustness, particularly under extreme non-IID conditions
2. Design and implement adaptive attack that specifically minimizes loss on server's trusted dataset while maintaining malicious behavior on client data to test assumption breaking
3. Evaluate FedGreed on additional datasets (CIFAR-100, Tiny ImageNet) and attack scenarios to assess performance generalization beyond current experimental scope