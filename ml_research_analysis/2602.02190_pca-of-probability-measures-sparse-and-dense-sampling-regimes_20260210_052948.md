---
ver: rpa2
title: 'PCA of probability measures: Sparse and Dense sampling regimes'
arxiv_id: '2602.02190'
source_url: https://arxiv.org/abs/2602.02190
tags:
- measure
- embedding
- then
- measures
- covariance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies PCA of probability measures in a double asymptotic
  regime, where both the number of measures and the number of samples per measure
  grow. The authors establish convergence rates of the form n^{-1/2} + m^{-\alpha}
  for the empirical covariance operator and PCA excess risk, where \alpha depends
  on the chosen embedding (KME, LOT, or SW).
---

# PCA of probability measures: Sparse and Dense sampling regimes

## Quick Facts
- arXiv ID: 2602.02190
- Source URL: https://arxiv.org/abs/2602.02190
- Reference count: 40
- Primary result: Proves convergence rates n^{-1/2} + m^{-\alpha} for PCA of probability measures in double asymptotic regime

## Executive Summary
This paper establishes theoretical foundations for principal component analysis (PCA) of probability measures in a double asymptotic regime where both the number of measures and samples per measure grow. The authors analyze three embedding methods (KME, LOT, SW) and prove convergence rates for the empirical covariance operator and PCA excess risk. They demonstrate that the n^{-1/2} term is minimax optimal in the dense regime. The work bridges kernel methods and optimal transport theory for statistical inference on probability distributions.

## Method Summary
The authors study PCA in a double asymptotic framework where n measures are estimated from m samples each. They analyze three embedding approaches: Kernel Mean Embeddings (KME), Linear Optimal Transport (LOT), and Sliced-Wasserstein (SW). For each embedding, they establish finite-sample bounds on the empirical covariance operator using concentration inequalities and Rademacher complexity. The convergence rates combine a term decaying as n^{-1/2} (from measure estimation) with m^{-\alpha} (from finite sampling). The analysis leverages Gaussian assumptions and finite moment conditions to derive tight bounds.

## Key Results
- Convergence rates of form n^{-1/2} + m^{-\alpha} established for empirical covariance operator
- n^{-1/2} term proven minimax optimal in dense sampling regime
- Numerical experiments validate theoretical rates and demonstrate computational benefits of subsampling
- Analysis covers three embedding methods: KME, LOT, and SW

## Why This Works (Mechanism)
The double asymptotic analysis captures both statistical uncertainty from finite sample sizes and approximation error from embedding probability measures in finite-dimensional spaces. The n^{-1/2} term arises from standard empirical process theory when estimating n probability measures from data. The m^{-\alpha} term captures the approximation quality of each embedding method, with α depending on the specific kernel or transport metric used. The minimax optimality of n^{-1/2} follows from matching upper and lower bounds in the dense regime.

## Foundational Learning

1. Kernel Mean Embeddings (KME)
   - Why needed: Provides a way to represent probability measures as elements in a reproducing kernel Hilbert space
   - Quick check: Verify embedding preserves distances between measures through kernel evaluations

2. Optimal Transport (OT) and Wasserstein metrics
   - Why needed: Measures distance between probability distributions based on minimal transport cost
   - Quick check: Confirm cost function satisfies triangle inequality and induces meaningful topology

3. Rademacher complexity
   - Why needed: Controls uniform deviation of empirical covariance operator from population version
   - Quick check: Bound complexity by covering number of embedding space under appropriate metric

4. Gaussian measure concentration
   - Why needed: Provides exponential tail bounds for covariance operator estimation
   - Quick check: Verify Gaussianity assumptions hold for empirical estimators in simulation

## Architecture Onboarding

Component map: Probability measures -> Embedding (KME/LOT/SW) -> Empirical covariance -> PCA

Critical path: Data collection → Measure embedding → Covariance estimation → Eigenvalue decomposition → PCA components

Design tradeoffs: KME offers flexibility with kernel choice but requires positive definiteness; LOT provides linear structure but may distort geometry; SW is computationally efficient but uses marginal information only.

Failure signatures: Poor convergence when embedding space is too large relative to sample size; instability when measures have disjoint supports; breakdown when moments don't exist.

First experiments:
1. Test embedding quality by measuring reconstruction error for synthetic Gaussian mixtures
2. Verify convergence rates by varying n and m independently on controlled examples
3. Compare PCA performance across KME, LOT, and SW on same dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes Gaussianity and finite moments may not hold in practical settings
- Theoretical rates depend critically on embedding choice without clear guidance on selection
- Numerical experiments limited to toy examples with small sample sizes
- Scalability to real-world large-scale problems not demonstrated

## Confidence
High confidence in minimax optimality of n^{-1/2} term (supported by matching lower bounds)
Medium confidence in general convergence rates n^{-1/2} + m^{-\alpha} (follows from standard theory but may miss practical scenarios)
Low confidence in computational efficiency claims (experiments don't test large-scale performance)

## Next Checks
1. Test theoretical rates on real-world datasets with varying sample sizes and measure dimensions to verify n^{-1/2} + m^{-\alpha} scaling beyond synthetic data
2. Evaluate proposed method on large-scale problem (e.g., millions of samples per measure) to demonstrate computational savings from subsampling
3. Investigate sensitivity to non-Gaussian distributions and heavy-tailed data to understand robustness of theoretical guarantees