---
ver: rpa2
title: Mechanism of Task-oriented Information Removal in In-context Learning
arxiv_id: '2509.21012'
source_url: https://arxiv.org/abs/2509.21012
tags:
- attention
- index
- head
- layer
- induction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel perspective on In-context Learning
  (ICL) by interpreting it as a task-oriented information removal process, where demonstrations
  help Language Models (LMs) filter out irrelevant information from queries. The authors
  propose a systematic evaluation framework that traces this information removal using
  geometric metrics like eccentricity and covariance flux on hidden states.
---

# Mechanism of Task-oriented Information Removal in In-context Learning

## Quick Facts
- arXiv ID: 2509.21012
- Source URL: https://arxiv.org/abs/2509.21012
- Reference count: 40
- Primary result: ICL is interpreted as task-oriented information removal where demonstrations help models filter out irrelevant information from queries.

## Executive Summary
This paper introduces a novel perspective on In-context Learning (ICL) by interpreting it as a task-oriented information removal process. The authors propose that demonstrations help Language Models filter out irrelevant information from queries, concentrating task-specific information in a low-rank subspace. They demonstrate this through a systematic evaluation framework that traces information removal using geometric metrics like eccentricity and covariance flux on hidden states. The study identifies key attention heads, termed Denoising Heads, which are responsible for this task-oriented information removal. Ablation experiments confirm their critical role, especially in unseen label scenarios.

## Method Summary
The authors develop a systematic evaluation framework for ICL by injecting low-rank filters into zero-shot hidden states to steer outputs toward intended tasks. They train a WencWdec filter (two linear layers) on zero-shot inputs with ground-truth labels, where Wenc extracts task-specific semantics and Wdec maps to target verbalization. They measure information removal using geometric metrics - eccentricity (variance ratio on first principal component) and covariance flux (nuclear norm ratio of projected vs original hidden states). Denoising Heads are identified through ablation experiments by measuring changes in covariance flux when each attention head is zeroed out.

## Key Results
- Low-rank filters (preserving ~0.7% of dimensions) can boost zero-shot accuracy from near-zero to high levels by removing irrelevant information
- Few-shot ICL implicitly performs TVS-aligned information removal, measurable through increased eccentricity and covariance flux in middle-to-late layers
- Denoising Heads in middle-to-late layers are responsible for task-oriented information removal; their ablation significantly degrades ICL accuracy, especially in unseen label scenarios

## Why This Works (Mechanism)

### Mechanism 1: Task-Verbalization Subspace (TVS) Existence and Activation
Zero-shot hidden states encode non-selective query information containing all possible task interpretations. Task-specific information concentrates in a low-rank subspace (TVS). Selectively projecting onto TVS removes irrelevant information and steers outputs. A low-rank filter WencWdec is injected into the residual stream at a middle-to-late layer; Wenc extracts task-relevant semantics (verbalization-agnostic), while Wdec maps to target verbalization mode. Training this filter on zero-shot inputs with ground-truth labels recovers the TVS, demonstrating that information removal (not addition) is sufficient for task adaptation.

### Mechanism 2: Few-shot ICL Implicitly Performs TVS-aligned Information Removal
Few-shot demonstrations cause the model to implicitly compress hidden states toward the TVS, removing task-irrelevant information. This is measurable via geometric metrics on hidden state point clouds. Given few-shot prompts, the model's hidden states at later layers show increased eccentricity (variance concentrated on first principal component, indicating compression) and covariance flux through TVS (higher proportion of variance preserved when projected onto TVS). These metrics track the magnitude and correctness of information removal.

### Mechanism 3: Denoising Heads (DH) as Causal Executors of Information Removal
Specific attention heads (denoising heads) in middle-to-late layers are responsible for task-oriented information removal. They are identified by ablating each head and measuring the change in covariance flux; heads whose ablation reduces covariance flux (and often accuracy) are labeled DHs. They attend locally to query tokens (not label tokens), re-encoding task-relevant information. Ablating all DHs reduces accuracy significantly, particularly when induction heads fail (unseen labels).

## Foundational Learning

- **Concept: In-Context Learning (ICL) Paradigm**
  - Why needed here: ICL is the setting under investigation; understanding that models adapt from demonstrations without weight updates is prerequisite.
  - Quick check question: Explain how ICL differs from fine-tuning and zero-shot prompting in terms of parameter updates and input structure.

- **Concept: Hidden State Geometry in Transformers**
  - Why needed here: The entire analysis relies on interpreting hidden state point clouds via eccentricity and covariance flux.
  - Quick check question: If you have hidden states `H = {h_i}` from N queries after layer l, how would you compute eccentricity (variance ratio on PC1) and why does it indicate compression?

- **Concept: Attention Head Ablation and Causal Analysis**
  - Why needed here: Mechanism 3 uses ablation to identify and verify denoising heads.
  - Quick check question: Describe how ablating an attention head (setting its output to zero) can test its causal role in a specific function, and what confounds might arise (e.g., compensatory effects from other heads).

## Architecture Onboarding

- **Component map:**
  - TVS (WencWdec) -> Geometric metrics (eccentricity, covariance flux) -> Denoising Heads (DH) -> ICL accuracy

- **Critical path:**
  1. Zero-shot filter injection: Train WencWdec on zero-shot inputs with ground-truth labels to discover TVS
  2. Few-shot metric analysis: Compute eccentricity and covariance flux on few-shot hidden states to verify implicit information removal
  3. Denoising head identification: Ablate heads, measure covariance flux change, label DHs, and validate via accuracy ablation

- **Design tradeoffs:**
  - Linear vs. non-linear TVS: The paper assumes linear low-rank TVS; non-linear manifolds may be needed for complex tasks
  - Ablation threshold: Threshold for labeling DHs (e.g., -5% covariance flux change) trades off coverage vs. specificity
  - Local vs. global attention: DHs use local attention (query tokens); induction heads use global (label tokens)

- **Failure signatures:**
  - Bijective tasks: Tasks like country-to-capital do not show information removal (no accuracy gain from filter)
  - Unseen labels with DH ablation: Accuracy collapses to near-zero
  - Instruction-only prompting: Does not produce information removal metrics despite higher accuracy

- **First 3 experiments:**
  1. Replicate TVS discovery: On Llama-1B and SST-2, inject low-rank filters at various layers, train on zero-shot inputs, and measure accuracy vs. rank
  2. Compute geometric metrics on few-shot hidden states: For 1-128 shot ICL, extract hidden states at each layer, compute eccentricity and covariance flux, and plot against shot count and accuracy
  3. Identify denoising heads on one layer: For a single middle layer, ablate each attention head, recompute covariance flux, and label heads with >5% decrease as DHs

## Open Questions the Paper Calls Out

- How does the task-oriented information removal mechanism function in tasks where features are nonlinearly separable or distributed on manifolds rather than linear subspaces?
- What is the precise, fine-grained circuit mechanism by which Denoising Heads (DHs) identify and filter task-relevant information?
- Does the information removal framework hold for bijective ICL tasks (e.g., translation or unique fact recall) where input information must be preserved rather than discarded?

## Limitations

- Linear low-rank subspace assumption for TVS may not hold for complex, non-linearly separable tasks
- Geometric metrics are proxies whose causal relationship to information removal requires further validation
- DH identification via ablation thresholds may be sensitive to hyperparameter choices

## Confidence

- **High Confidence:** TVS existence and filter injection effectiveness
- **Medium Confidence:** Few-shot ICL implicitly performs TVS-aligned information removal
- **Medium Confidence:** DHs are causal executors of information removal

## Next Checks

1. Validate TVS generalization: Apply the TVS filter trained on one task/verbalization to a different task/verbalization
2. Test geometric metrics across architectures: Compute eccentricity and covariance flux on hidden states from models like GPT-3.5/4 and Claude
3. Probe non-linear TVS: Replace the linear WencWdec filter with a simple non-linear projection (e.g., two-layer MLP with ReLU)