---
ver: rpa2
title: 'MotionV2V: Editing Motion in a Video'
arxiv_id: '2511.20640'
source_url: https://arxiv.org/abs/2511.20640
tags:
- video
- motion
- editing
- control
- frame
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach for editing the motion of
  objects in videos. Unlike prior methods that rely on image-to-video generation or
  are limited to specific object types, this method enables users to directly edit
  sparse trajectories extracted from the input video to control how objects move in
  the output.
---

# MotionV2V: Editing Motion in a Video

## Quick Facts
- arXiv ID: 2511.20640
- Source URL: https://arxiv.org/abs/2511.20640
- Reference count: 40
- Key outcome: Introduces motion counterfactual training pairs and sparse trajectory control for video motion editing, outperforming baselines with 70-71% user preference across metrics.

## Executive Summary
This paper presents a novel approach for editing object motion in videos by leveraging sparse trajectory points and motion counterfactual training. Unlike prior methods that rely on image-to-video generation or are limited to specific object types, this system enables direct editing of motion trajectories extracted from input videos. The key innovation is training a motion-conditioned video diffusion model on pairs of videos that share identical visual content but have different motion patterns, generated through frame interpolation and temporal resampling. The method allows users to control object motion, camera motion, timing, and apply edits at arbitrary frames while preserving scene consistency. In user studies, the approach significantly outperformed state-of-the-art baselines, achieving 70% preference for content preservation, 71% for motion control, and 69% for overall quality compared to 25% for the strongest baseline.

## Method Summary
The system trains a video diffusion model to edit motion by conditioning on sparse trajectory points. It generates motion counterfactual pairs - videos with identical visual content but different motion patterns - using frame interpolation with LLM-generated motion prompts and temporal resampling techniques. Point correspondences are established via bidirectional TAPNext tracking, with tracking information rendered as colored Gaussian blobs on black backgrounds. A control branch architecture (first 18 DiT blocks duplicated with zero-initialized MLPs) processes the conditioning videos while the main branch remains frozen. The model is trained on 100,000 video pairs from an internal dataset, using L2 latent diffusion loss. At inference, the system limits to approximately 20 tracking points and applies jitter to prevent identity-copying bias.

## Key Results
- User study: 70% preference for content preservation, 71% for motion control, 69% for overall quality vs 25% for strongest baseline
- Quantitative: Lower L2 reconstruction error and SSIM compared to image-to-video baselines on 100 test videos
- Enables editing of object motion, camera motion, timing adjustments, and arbitrary frame specification while preserving scene consistency
- Handles complex scenarios like content appearing mid-video that previous methods cannot address

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Motion counterfactual training pairs enable the model to learn motion-content disentanglement.
- Mechanism: The system generates paired videos sharing visual content but differing in motion through frame interpolation using video diffusion with LLM-generated motion prompts and temporal resampling that creates speed variations or reversed motion. Point correspondences are established via TAPNext tracking on shared frames, with geometric augmentations applied identically to both video and tracks.
- Core assumption: The model can generalize from synthetic motion counterfactuals to real-world motion edits when trained on sufficient paired examples (100,000 videos).
- Evidence anchors:
  - [abstract] "we introduce a pipeline for generating 'motion counterfactuals', video pairs that share identical content but distinct motion"
  - [section 3.2] Details frame interpolation and temporal resampling strategies with TAPNext tracking
  - [corpus] MotionEdit dataset (neighbor paper) addresses motion-centric editing but focuses on images, not video pairs—suggesting this counterfactual approach for video is novel
- Break condition: If counterfactual generation introduces appearance changes beyond motion (e.g., lighting, object deformation), the model may learn spurious correlations rather than true motion-content disentanglement.

### Mechanism 2
- Claim: Sparse trajectory representations rendered as colored Gaussian blobs provide sufficient control signal for precise motion editing without manual masking.
- Mechanism: Tracking points (N~1-64 during training, ~20 at inference) are rasterized as distinct-colored Gaussian blobs (σ=10 pixels) on black backgrounds for both counterfactual and target tracks. Blob visibility is gated by tracker occlusion data. Target track dropout during training improves robustness.
- Core assumption: The model can infer intended motion transformations from sparse point correspondences without dense optical flow.
- Evidence anchors:
  - [section 3.2] "We rasterize the tracking information as colored Gaussian blobs on black backgrounds... each tracking point is rendered as a Gaussian blob with standard deviation of 10 pixels"
  - [section 3.2] Notes dropout prevents overfitting; inference limited to ~20 points as model fails with too many
  - [corpus] DragAnything and FreeTraj (cited) use trajectory control but for generation, not editing—sparse trajectory representation transfers to editing context
- Break condition: If objects deform non-rigidly or occlusions are frequent, sparse points may fail to capture motion semantics, leading to implausible outputs.

### Mechanism 3
- Claim: Zero-initialized control branch integration allows frozen pretrained T2V model to accept motion conditioning without catastrophic forgetting.
- Mechanism: A control branch duplicates the first 18 DiT transformer blocks, processing three latent video channels (counterfactual video, counterfactual tracks, target tracks). Zero-initialized MLPs project control tokens into main branch at each block, similar to ControlNet. The main branch remains frozen; only control branch trains.
- Core assumption: Transformer blocks can perform non-trivial spatiotemporal alignment between unsynchronized inputs (video + motion blobs) and output.
- Evidence anchors:
  - [section 3.3] "Surprisingly, our adapter works despite the inputs (video + motion blobs) lacking spatiotemporal synchronization with the output"
  - [section 3.3] Architecture diagram shows 48 input channels (3×16 latent) for conditioning videos
  - [corpus] ControlNet (Zhang et al. 2023, cited) provides foundational zero-init concept; DiffusionAsShader inspires architecture but lacks multi-video conditioning
- Break condition: If control branch depth is insufficient (<18 blocks), spatial reasoning may fail; if zero-init scales are too large, output may collapse to conditioning artifacts.

## Foundational Learning

- Concept: **Latent Video Diffusion with 3D VAE**
  - Why needed here: Understanding how videos compress to latent space (F×3×H×W → F_latent×16×H/8×W/8) is essential for debugging conditioning channel alignment and diagnosing reconstruction artifacts.
  - Quick check question: Given 49 input frames at 480×720, what are the latent dimensions? (Answer: 13×16×60×90)

- Concept: **Point Tracking (Bidirectional)**
  - Why needed here: The system relies on TAPNext to establish correspondences across counterfactual pairs. Understanding occlusion handling and tracking failures directly impacts when edits will succeed or produce drift.
  - Quick check question: Why must tracking be bidirectional rather than forward-only for the counterfactual generation pipeline?

- Concept: **ControlNet-style Zero Initialization**
  - Why needed here: The control branch uses zero-initialized MLPs to avoid disrupting pretrained knowledge. Understanding gradient flow through zero-init layers explains why training converges despite frozen main branch.
  - Quick check question: What happens to the output at training step 0 with zero-initialized control branch? (Answer: Identical to base model output)

## Architecture Onboarding

- Component map: Input Video → VAE Encoder → Latent (F×16×H/8×W/8) → Main DiT Blocks → Denoised Latent → VAE Decoder → Output
  Counterfactual Video → VAE Encoder → Latent
  Counterfactual Tracks (rendered blobs) → VAE Encoder → Latent
  Target Tracks (rendered blobs) → VAE Encoder → Latent
                                    ↓
                        [Concat → Control Branch DiT Blocks 1-18]
                                    ↓ (Zero-Init MLPs)

- Critical path: (1) Point tracking quality → (2) Track rendering accuracy → (3) Counterfactual latent alignment → (4) Control branch token injection at blocks 1-18 → (5) Final denoising fidelity

- Design tradeoffs:
  - 18 blocks for control branch vs. full 30: Shallower reduces compute but may lose high-level reasoning
  - ~20 points at inference vs. 64 at training: Fewer points improve following but reduce control granularity
  - Jitter (1-2px noise) at inference only: Breaks identity-copying bias but adds stochasticity

- Failure signatures:
  - Second object appears (e.g., duplicate basketball): Points too perfectly aligned → add jitter
  - Wrong object color (e.g., white balloon → orange): First-frame-only conditioning failure (baseline issue)
  - Motion not followed: Too many points (>20) or tracker occlusion misreporting

- First 3 experiments:
  1. **Ablate control branch depth**: Train with 6, 12, 18 blocks; measure L2 reconstruction on held-out test set to find minimum viable depth.
  2. **Point count sensitivity**: Sweep inference points from 5 to 40 on 5 diverse scenes; plot motion-following accuracy vs. content preservation.
  3. **Counterfactual strategy comparison**: Train separate models on frame-interpolation-only, temporal-resampling-only, and mixed data; evaluate on camera-motion edits vs. object-motion edits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would training on large-scale synthetic 3D datasets with perfect ground-truth motion counterfactuals significantly improve performance and reduce the number of control points needed?
- Basis in paper: [explicit] The authors state: "In future work, we consider creating large-scale synthetic datasets with precise motion counterfactuals made with 3d software... This would improve the precision of our training dataset, possibly allowing even less points to be used for control."
- Why unresolved: Current training uses real videos paired with diffusion-generated counterfactuals, which may contain inconsistencies in motion-appearance alignment that synthetic data could eliminate.
- What evidence would resolve it: Comparative experiments training the same architecture on synthetic 3D datasets versus the current real-video approach, measuring both edit quality and minimum viable control points.

### Open Question 2
- Question: Why does the model fail to follow trajectory correspondences when given more than approximately 20 tracking points during inference?
- Basis in paper: [explicit] The authors note: "During inference, we limit the number of point correspondences to approximately 20, as the model fails to follow all correspondences when given too many points."
- Why unresolved: The paper does not investigate whether this is a fundamental architectural limitation, a training data issue (N∼Uniform(1,64) during training), or an optimization problem.
- What evidence would resolve it: Ablation studies varying the maximum number of training points, analyzing attention patterns with many points, and testing alternative trajectory encoding strategies.

### Open Question 3
- Question: What causes the subject drift observed in iterative editing, and can architectural or training modifications enable reliable infinite iterative refinement?
- Basis in paper: [explicit] The authors acknowledge: "While this example demonstrates some degree of subject drift, this can be attributed in part to the quality of the base video model. We believe that future versions of our method will be able to be applied infinitely."
- Why unresolved: The paper does not isolate whether drift stems from accumulated approximation errors, the base model's limitations, or the motion conditioning mechanism itself.
- What evidence would resolve it: Systematic analysis of drift across multiple iterative edit steps, experiments with different base models, and investigation of latent space accumulation patterns.

### Open Question 4
- Question: Why does pixel-perfect trajectory alignment trigger identity-copying behavior, and what does this reveal about the model's learned priors?
- Basis in paper: [inferred] The ablation study reveals that "when tracking points are pixel-perfectly aligned with the input video trajectories across multiple frames, the model exhibits a strong bias toward reproducing the original video's semantics rather than following the edited motion," requiring inference-time jitter as a workaround.
- Why unresolved: The paper identifies the phenomenon but does not explain whether it arises from training data statistics, the conditioning mechanism, or learned attention patterns.
- What evidence would resolve it: Analysis of training data alignment patterns, probing internal representations when given aligned vs. jittered trajectories, and experiments with alternative conditioning schemes that explicitly separate identity from motion.

## Limitations
- The system is constrained to approximately 20 tracking points at inference, limiting granularity of motion control
- Performance critically depends on point tracking quality, with failures in occlusion and non-rigid motion scenarios
- Reliance on synthetic motion counterfactuals for training may not capture full diversity of real-world motion patterns
- Subject drift observed in iterative editing suggests limitations in continuous refinement capability

## Confidence
- **High confidence**: Core architectural innovation (control branch with zero-initialized MLPs) and basic motion counterfactual generation pipeline are well-specified and theoretically sound
- **Medium confidence**: User study results showing superiority over baselines are compelling but lack baseline model specifications and may have presentation order effects
- **Low confidence**: System robustness to occlusions, non-rigid deformations, and tracking failures is not thoroughly characterized

## Next Checks
1. **Tracking Robustness Analysis**: Systematically evaluate the method on videos with varying levels of occlusion and non-rigid motion (e.g., humans dancing, animals running). Measure the correlation between tracking quality metrics (track length, occlusion frequency) and editing fidelity.
2. **Point Count Sensitivity Study**: Conduct a controlled experiment sweeping inference point counts from 5 to 50 on a diverse set of scenes. Quantify the trade-off between motion-following accuracy and content preservation as point count varies.
3. **Counterfactual Diversity Evaluation**: Train separate models on different counterfactual generation strategies (frame interpolation only, temporal resampling only, mixed) and evaluate their relative performance on camera motion edits versus object motion edits to identify which synthetic motions best transfer to real edits.