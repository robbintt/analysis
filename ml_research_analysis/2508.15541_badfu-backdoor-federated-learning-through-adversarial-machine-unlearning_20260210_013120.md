---
ver: rpa2
title: 'BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning'
arxiv_id: '2508.15541'
source_url: https://arxiv.org/abs/2508.15541
tags:
- unlearning
- backdoor
- federated
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BadFU, the first backdoor attack targeting
  federated unlearning. The attack exploits the unlearning process by having a malicious
  client inject both backdoor and camouflage samples during federated training.
---

# BadFU: Backdoor Federated Learning through Adversarial Machine Unlearning

## Quick Facts
- **arXiv ID**: 2508.15541
- **Source URL**: https://arxiv.org/abs/2508.15541
- **Reference count**: 40
- **Primary result**: Introduces the first backdoor attack targeting federated unlearning, achieving >90% attack success while maintaining benign accuracy

## Executive Summary
This paper presents BadFU, a novel backdoor attack that exploits federated unlearning mechanisms. The attack works by having malicious clients inject both backdoor and camouflage samples during federated training. While the model appears normal during training, the backdoor is activated once the server unlearns the camouflage samples. The authors demonstrate that existing defenses fail to prevent this attack, revealing a critical vulnerability in federated unlearning systems. Experiments show high attack success rates across multiple datasets and federated learning frameworks.

## Method Summary
BadFU operates by strategically injecting malicious data into federated learning systems. During training, malicious clients introduce two types of samples: backdoor samples (containing trigger patterns) and camouflage samples (designed to be unlearned later). The camouflage samples are crafted to appear benign during training, maintaining the model's normal performance. Once federated unlearning is performed on the camouflage samples, the model's behavior shifts to activate the hidden backdoor, causing targeted misclassification when backdoor triggers appear. This attack exploits the fundamental assumption that unlearning removes all traces of specific data, when in fact it can leave hidden vulnerabilities.

## Key Results
- Achieves attack success rates exceeding 90% across MNIST, CIFAR-10, and CIFAR-100 datasets
- Maintains benign accuracy close to original models during training phase
- Effective across multiple federated learning frameworks including FedAvg, FedSGD, and FedProx
- Bypasses existing defenses including robust aggregation and backdoor detection methods
- Higher camouflage-to-backdoor ratios increase stealth without compromising attack potency

## Why This Works (Mechanism)
BadFU exploits the temporal decoupling between model training and unlearning phases in federated learning. The attack leverages the fact that unlearning processes are designed to remove specific data influence but don't verify the integrity of what remains. By carefully crafting camouflage samples that blend with legitimate data, the malicious client ensures the model learns normal behavior initially. When these camouflage samples are later unlearned, the remaining backdoor functionality is triggered, causing targeted misclassification. This approach is particularly effective because federated unlearning typically assumes honest participants and focuses on data removal rather than data integrity verification.

## Foundational Learning

**Federated Learning**: Decentralized training where multiple clients collaborate with a central server without sharing raw data. Needed to understand the attack context and distributed nature of the vulnerability.

**Machine Unlearning**: Process of removing specific data's influence from trained models. Quick check: Verify the unlearning method actually removes all traces of target data without leaving artifacts.

**Backdoor Attacks**: Tampering with model training to create hidden triggers that cause misclassification. Quick check: Confirm the backdoor trigger pattern is imperceptible to humans but consistently activates the attack.

## Architecture Onboarding

**Component Map**: Malicious Client -> Server Aggregation -> Model Training -> Federated Unlearning -> Backdoor Activation

**Critical Path**: Sample injection (backdoor + camouflage) → Normal training phase → Camouflage unlearning → Backdoor activation

**Design Tradeoffs**: Higher camouflage ratio improves stealth but may reduce attack potency; simpler trigger patterns are more reliable but potentially more detectable

**Failure Signatures**: Reduced benign accuracy, inconsistent attack success across clients, detectable patterns in camouflage samples

**First Experiments**:
1. Baseline federated learning without any attack to establish performance metrics
2. Attack implementation with varying camouflage-to-backdoor ratios
3. Defense evaluation using robust aggregation and backdoor detection methods

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Effectiveness in real-world scenarios with more complex datasets remains untested
- Impact on non-image classification tasks like NLP or speech recognition is unexplored
- Computational overhead during training and unlearning processes is not quantified

## Confidence

**High**: Attack effectiveness on MNIST, CIFAR-10, CIFAR-100; bypassing existing defenses
**Medium**: Generalization to various federated learning frameworks and unlearning methods
**Low**: Performance in large-scale real-world deployments with complex datasets

## Next Checks

1. Evaluate BadFU on complex datasets like ImageNet or medical imaging data
2. Test attack effectiveness on NLP and speech recognition tasks
3. Quantify computational overhead during training and unlearning phases