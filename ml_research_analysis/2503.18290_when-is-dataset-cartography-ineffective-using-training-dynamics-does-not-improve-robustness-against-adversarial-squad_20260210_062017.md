---
ver: rpa2
title: When is dataset cartography ineffective? Using training dynamics does not improve
  robustness against Adversarial SQuAD
arxiv_id: '2503.18290'
source_url: https://arxiv.org/abs/2503.18290
tags:
- dataset
- squad
- adversarial
- training
- cartography
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates dataset cartography's effectiveness for
  improving adversarial robustness in extractive question answering using the SQuAD
  dataset. The author analyzes annotation artifacts in SQuAD and evaluates model performance
  on two adversarial datasets (AddSent and AddOneSent) using an ELECTRA-small model.
---

# When is dataset cartography ineffective? Using training dynamics does not improve robustness against Adversarial SQuAD

## Quick Facts
- arXiv ID: 2503.18290
- Source URL: https://arxiv.org/abs/2503.18290
- Authors: Paul K. Mandal
- Reference count: 8
- Primary result: Dataset cartography shows minimal benefit for adversarial robustness in SQuAD-style QA compared to classification tasks

## Executive Summary
This paper investigates whether dataset cartography—a technique for identifying training dynamics patterns—can improve adversarial robustness in extractive question answering using the SQuAD dataset. The author partitions SQuAD into easy-to-learn, ambiguous, and hard-to-learn subsets based on model confidence and variability during training, then compares performance against randomly selected samples on two adversarial datasets (AddSent and AddOneSent). Results show that cartography-based training provides only marginal improvements, with slight gains on AddOneSent for the hard-to-learn subset. The findings suggest dataset cartography has limited effectiveness for adversarial robustness in QA tasks, contrasting with its success on simpler classification tasks like SNLI.

## Method Summary
The study uses an ELECTRA-small model trained on SQuAD 1.1 and evaluates performance on AddSent and AddOneSent adversarial datasets. The author implements dataset cartography by tracking model predictions and confidence scores across training epochs to identify three subsets: easy-to-learn (high confidence, low variability), ambiguous (low confidence, high variability), and hard-to-learn (low confidence, low variability). These subsets are compared against randomly sampled subsets of equivalent size. The analysis focuses on F1 scores as the primary evaluation metric, examining how different training subsets affect robustness to adversarial examples. Unlike prior work, the study does not report standard deviations across multiple training runs.

## Key Results
- Training on cartography-based subsets provides minimal improvement in adversarial robustness compared to random sampling
- Only the hard-to-learn subset shows marginal gains on AddOneSent (56.56 F1 vs 55.14 F1 for random)
- No significant improvements observed on AddSent across any cartography-based subset
- SQuAD's nuanced artifacts and higher training dynamics variance likely contribute to cartography's ineffectiveness

## Why This Works (Mechanism)
Dataset cartography identifies patterns in model training dynamics—specifically confidence and variability over training epochs—to partition datasets into subsets with different learning characteristics. The mechanism assumes that models can learn different types of examples at different rates, and that these learning patterns correlate with vulnerability to adversarial attacks. By training on carefully selected subsets rather than the full dataset, the approach aims to improve robustness by focusing on examples that are either easily learned (building confidence) or represent edge cases (building generalization). However, this paper demonstrates that the correlation between learning difficulty and adversarial vulnerability is weaker in complex QA tasks compared to simpler classification tasks.

## Foundational Learning

**Training dynamics metrics** - Confidence scores and prediction variability across training epochs are tracked to characterize how examples are learned. *Why needed*: These metrics form the basis for partitioning datasets into meaningful subsets. *Quick check*: Verify that confidence scores converge and that variability measurements are stable across training runs.

**Adversarial QA evaluation** - AddSent and AddOneSent are adversarial datasets that modify SQuAD questions to test model robustness. *Why needed*: Standard SQuAD evaluation cannot reveal whether cartography improves robustness to attacks. *Quick check*: Confirm that adversarial examples are correctly identified as such and that models fail appropriately on them.

**Subset sampling methodology** - Comparing cartography-based subsets against random samples of equivalent size tests whether the partitioning strategy adds value. *Why needed*: Random sampling serves as the null hypothesis for any benefits from cartography. *Quick check*: Verify that random and cartography subsets have similar size distributions and that sampling is reproducible.

## Architecture Onboarding

**Component map**: ELECTRA-small model -> Training dynamics tracker -> Dataset partitioning -> Subset training -> Adversarial evaluation

**Critical path**: The model's training dynamics (confidence and variability scores) are the critical path that determines dataset partitioning. These metrics must be accurately computed and thresholded to create meaningful subsets that can potentially improve robustness.

**Design tradeoffs**: The study uses ELECTRA-small for computational efficiency, trading off potential performance gains from larger models. The single-threshold approach for partitioning simplifies analysis but may miss nuanced patterns that multi-dimensional mapping could capture.

**Failure signatures**: Limited improvements on AddSent despite gains on AddOneSent suggest that cartography effectiveness varies by attack type. The lack of standard deviation reporting means observed differences might not be statistically significant.

**First experiments**:
1. Replicate the cartography partitioning on a different QA dataset to test generalizability
2. Apply cartography to a simpler classification task to verify the methodology works as intended
3. Test whether combining cartography with adversarial training yields cumulative benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adapted dataset cartography methods be developed that effectively improve robustness for high-variance datasets like SQuAD?
- Basis in paper: The discussion section states: "Future research could explore adaptations of dataset cartography to address datasets with high variance or tasks requiring nuanced reasoning, such as question answering."
- Why unresolved: The paper demonstrates that standard cartography fails on SQuAD but does not propose or test modifications to the methodology that might work better for complex extractive QA tasks.
- What evidence would resolve it: A modified cartography approach (e.g., different confidence/variability thresholds, multi-dimensional maps, or task-specific metrics) that shows significant improvements over random sampling on adversarial SQuAD evaluations.

### Open Question 2
- Question: What is the statistical significance of the marginal F1 improvement observed for the hard-to-learn subset on AddOneSent?
- Basis in paper: The Limitations section explicitly states: "Unlike Swayamdipta et al. (2020), I did not train our models multiple times to calculate the standard deviation of our model performance."
- Why unresolved: Without variance estimates across multiple runs, it remains unclear whether the 56.56 vs. 55.14 F1 difference (hard-to-learn vs. 33% random) represents a genuine effect or noise.
- What evidence would resolve it: Multiple training runs with reported standard deviations showing whether the hard-to-learn advantage on AddOneSent is statistically significant.

### Open Question 3
- Question: To what extent do artifact complexity versus training dynamics variance each contribute to cartography's differential effectiveness across tasks?
- Basis in paper: The discussion proposes two explanations—"nature of artifacts" and "higher variance in training dynamics"—but does not disentangle their relative contributions or test them independently.
- Why unresolved: Both factors co-occur in the comparison between SNLI and SQuAD, making causal attribution impossible from the current experimental design.
- What evidence would resolve it: Controlled experiments manipulating one factor while holding the other constant, such as applying cartography to synthetic datasets with known artifact types and controlled variance levels.

## Limitations
- Study focuses on single model architecture (ELECTRA-small) and two specific adversarial attack types, limiting generalizability
- No standard deviation calculations across multiple runs means observed differences may not be statistically significant
- Does not explore whether cartography might offer benefits beyond adversarial robustness (e.g., calibration, confidence calibration)

## Confidence

**Confidence Labels:**
- Major claim about cartography ineffectiveness in SQuAD: **Medium**
- Specific findings about hard-to-learn subset performance: **High**
- Generalization to other QA datasets/attacks: **Low**

## Next Checks
1. Replicate the study using larger model architectures (BERT-base, RoBERTa) and additional adversarial attack types (TextFooler, BERT-Attack) to test generalizability.
2. Conduct ablation studies examining whether specific components of the cartography methodology (e.g., thresholding strategies, training dynamics metrics) contribute to the observed ineffectiveness.
3. Test whether combining cartography-based subset training with adversarial training or other robustness techniques yields cumulative benefits beyond either approach alone.