---
ver: rpa2
title: Re-evaluating Theory of Mind evaluation in large language models
arxiv_id: '2502.21098'
source_url: https://arxiv.org/abs/2502.21098
tags:
- https
- llms
- mind
- theory
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-evaluates the current state of Theory of Mind (ToM)
  evaluation in large language models (LLMs). The authors argue that conflicting claims
  about LLMs' ToM abilities stem from a lack of clarity on whether ToM should be defined
  as behavior-matching or computation-matching with humans.
---

# Re-evaluating Theory of Mind evaluation in large language models

## Quick Facts
- arXiv ID: 2502.21098
- Source URL: https://arxiv.org/abs/2502.21098
- Authors: Jennifer Hu; Felix Sosa; Tomer Ullman
- Reference count: 40
- Primary result: Current ToM evaluations in LLMs conflate behavior-matching with computation-matching, leading to conflicting claims about capabilities

## Executive Summary
This paper re-evaluates the current state of Theory of Mind (ToM) evaluation in large language models (LLMs). The authors argue that conflicting claims about LLMs' ToM abilities stem from a lack of clarity on whether ToM should be defined as behavior-matching or computation-matching with humans. They identify two main issues: (1) current evaluations overly focus on matching human behavior rather than understanding the underlying computations, and (2) evaluations may not actually measure ToM due to training away, auxiliary task demands, and linguistic artifacts. The authors suggest moving toward computation-centric evaluations grounded in cognitive theory, using frozen and open models, and explicitly describing auxiliary demands in test design. They also highlight the relationship between pragmatic communication and ToM as an underexplored area for future research.

## Method Summary
The paper employs theoretical analysis and literature review to identify flaws in current ToM evaluation practices. The authors propose using open, frozen model checkpoints to prevent "training away" contamination, constructing adversarial test variants that preserve core ToM computations while changing surface features, and grounding evaluations in cognitive theory frameworks like inverse planning and Rational Speech Act models. They advocate for controlled experiments that isolate auxiliary task demands and systematic analysis of how pragmatic abilities relate to ToM capabilities in models.

## Key Results
- Conflicting claims about LLM ToM abilities arise from unclear definitions: behavior-matching vs. computation-matching
- Training away in closed-API models creates illusion of improvement without computational change
- Adversarial test construction introduces auxiliary task demands that confound ToM measurement
- Current evaluations may not actually measure ToM due to linguistic artifacts and pragmatic effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distinguishing behavior-matching from computation-matching resolves conflicting claims about LLM ToM abilities.
- Mechanism: Positive findings test whether models produce human-like outputs (M = M'); negative findings test whether models use human-like algorithms (f = f'). Both can be simultaneously true.
- Core assumption: ToM can be decomposed into observable behavior (A → M mapping) and underlying computation (function f).
- Evidence anchors: [abstract] "a major reason for the disagreement on whether LLMs have ToM is a lack of clarity on whether models should be expected to match human behaviors, or the computations underlying those behaviors"

### Mechanism 2
- Claim: "Training away" in closed-API models creates illusion of improving ToM without computational change.
- Mechanism: Models memorize adversarial test cases through parameter updates or in-context learning without acquiring the underlying ToM computation—like a lookup table expanding without learning multiplication.
- Core assumption: Closed-API models receive test inputs through user interactions and may be updated on failures.
- Evidence anchors: [abstract] "evaluations may not actually measure ToM due to training away"

### Mechanism 3
- Claim: Adversarial test construction introduces auxiliary task demands that confound ToM measurement.
- Mechanism: More adversarial examples require longer contexts, unfamiliar vocabulary, and physical reasoning. Failure may reflect resource constraints or non-ToM reasoning demands rather than ToM deficiency.
- Core assumption: Models and humans share similar auxiliary demand profiles; what's "trivial" for humans may not be trivial for models.
- Evidence anchors: [abstract] "evaluations may not actually measure ToM due to... auxiliary task demands"

## Foundational Learning

- Concept: **Construct validity** (psychometric measurement theory)
  - Why needed here: The paper's central critique is that ToM evaluations may not measure the intended latent construct. Understanding validity threats (content effects, task demands, confounds) is prerequisite for designing better benchmarks.
  - Quick check question: Can you explain why passing the Sally-Anne task might not demonstrate false-belief understanding?

- Concept: **Inverse planning / Bayesian Theory of Mind**
  - Why needed here: The paper advocates computation-centric evaluation grounded in cognitive theory. Inverse planning models (Baker et al.) provide normative computational targets for what "human-like" ToM computation should look like.
  - Quick check question: How would you formalize "inferring beliefs from observed actions" as an inverse planning problem?

- Concept: **Rational Speech Act (RSA) framework**
  - Why needed here: RSA connects pragmatic communication to ToM through recursive mental state inference. The paper identifies pragmatics-ToM relationship as an underexplored evaluation direction.
  - Quick check question: In RSA, how does a pragmatic listener infer speaker meaning beyond literal semantics?

## Architecture Onboarding

- Component map: Input layer (scenario text) → Evaluation target (behavior-matching or computation-matching) → Validity filters (training away detection, auxiliary demand accounting, artifact identification) → Grounding (cognitive theory)

- Critical path: 1. Define whether evaluation targets behavior or computation; 2. If computation: identify theoretical computational model; 3. Design test items that vary scenarios while holding computational principle constant; 4. Control for auxiliary demands via explicit description and control conditions; 5. Use frozen/open models to prevent training away

- Design tradeoffs: Adversarial robustness vs. construct purity; closed-API model access vs. scientific validity; behavior-matching vs. computation-matching

- Failure signatures: Model succeeds on standard items but fails on minimal adversarial variants; model performance improves across "versions" without architecture change; both humans and models fail on "adversarial" items; model attends to pragmatic markers in ways that bias responses

- First 3 experiments: 1. Training away test: evaluate same frozen model checkpoint repeatedly on novel adversarial ToM items; 2. Auxiliary demand isolation: create matched pairs of ToM scenarios with/without added complexity; 3. Computation probing: use BigToM or AGENT-style evaluation testing generalization across scenarios sharing same underlying ToM computation

## Open Questions the Paper Calls Out

- Question: Does the relationship between pragmatic abilities and ToM abilities in LLMs mirror or diverge from the relationship observed in humans?
  - Basis: Section 5 on pragmatics-ToM relationship
  - Why unresolved: Pragmatics and ToM have been investigated separately in LLMs using different tasks
  - What evidence would resolve it: Correlational studies measuring both abilities across multiple models

- Question: Can ToM abilities emerge during pre-training alone, or do they require fine-tuning/alignment?
  - Basis: Section 5 on emergence during pre-training
  - Why unresolved: Prior studies didn't perform controlled comparisons within model families
  - What evidence would resolve it: Controlled experiments within same model family comparing base vs. fine-tuned models

- Question: What computational architectures or training objectives enable "spontaneous" ToM in LLMs?
  - Basis: Section 5 on spontaneous ToM without explicit prompting
  - Why unresolved: Current LLMs require explicit prompting strategies to exhibit ToM behaviors
  - What evidence would resolve it: Demonstrations of models performing well on ToM tasks without ToM-specific prompting

## Limitations
- Practical implementation of computation-centric evaluations remains largely theoretical without concrete algorithms
- Limited empirical evidence quantifying how often training away occurs in practice
- Unclear which specific auxiliary demands most affect ToM evaluation and how they differentially impact models vs. humans

## Confidence
- High confidence: The distinction between behavior-matching and computation-matching as a source of conflicting claims
- Medium confidence: Training away as a widespread threat; importance of auxiliary demand control
- Low confidence: Specific computational models serving as evaluation targets; concrete benchmarks for computation-matching

## Next Checks
1. Conduct a longitudinal study tracking model performance on identical ToM items across multiple model versions from same API provider
2. Create factorial designs where ToM scenarios systematically vary linguistic complexity, physical reasoning demands, and context length while holding core ToM computation constant
3. Develop a benchmark where scenarios are grouped by underlying ToM computation but vary in surface features, evaluating whether models show computational generalization across groups