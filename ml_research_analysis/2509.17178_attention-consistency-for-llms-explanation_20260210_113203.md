---
ver: rpa2
title: Attention Consistency for LLMs Explanation
arxiv_id: '2509.17178'
source_url: https://arxiv.org/abs/2509.17178
tags:
- attention
- macs
- tokens
- input
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MACS, a lightweight method for explaining
  large language models by measuring the consistency of maximal attention across layers.
  Instead of aggregating all attention paths like previous methods, MACS focuses on
  the most consistent attention links, which helps produce clearer and more interpretable
  attributions.
---

# Attention Consistency for LLMs Explanation

## Quick Facts
- **arXiv ID:** 2509.17178
- **Source URL:** https://arxiv.org/abs/2509.17178
- **Reference count:** 21
- **Primary result:** MACS achieves 0.601 AUC-PR on QA, outperforming attention rollout by 300% while being 22% more VRAM-efficient and 30% faster.

## Executive Summary
This paper introduces MACS, a lightweight method for explaining large language models by measuring the consistency of maximal attention across layers. Unlike previous methods that aggregate all attention paths, MACS focuses on the most consistent attention links, producing clearer and more interpretable attributions. Tested on question answering, MACS identifies answer tokens with strong performance and achieves faithfulness comparable to more complex methods while requiring significantly less computational resources.

## Method Summary
MACS quantifies token contribution by evaluating the consistency of the strongest attention connection from an output query to each input token across all layers. The method extracts attention vectors during generation, applies max-pooling across heads to isolate the strongest signal, adds a floor vector to prevent premature zeroing, and accumulates consistency via Hadamard product across layers. The final scores are Z-scored for interpretability. The approach is inference-time only and requires no training.

## Key Results
- Achieves 0.601 AUC-PR for ranking answer tokens, outperforming attention rollout by over 300%
- Faithfulness comparable to complex methods (SRG-PP and SRG-RL metrics)
- Requires 22% less VRAM and reduces latency by 30% compared to gradient-based approaches
- Max-pooling and floor vector are critical components, confirmed through ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Consistency of Maximal Attention Across Layers
MACS measures contributions based on the consistency of maximal attention, focusing on tokens that receive sustained, strong attention connections across the network's depth. This approach filters out noise from weak or transient attention signals.

### Mechanism 2: Filtering Weak/Noisy Attention via Max-Pooling
Max-pooling across attention heads isolates the strongest signal per layer, prioritizing decisive heads and mitigating contributions from diffuse representations. This step is critical for performance.

### Mechanism 3: Preventing Premature Zeroing via Floor Vector
A minimum floor (controlled by α) prevents tokens from being zeroed out early when their relevance emerges only in deeper layers, preserving potential contributions that develop late.

## Foundational Learning

- **Causal Self-Attention in Decoder-Only Transformers**: Essential for understanding how MACS extracts attention weights during autoregressive generation. Quick check: In a decoder-only model generating token `t_k`, which positions can its query attend to?

- **Attention Aggregation (e.g., Attention Rollout)**: Understanding Rollout helps grasp why MACS uses consistency instead of additive aggregation. Quick check: How does Attention Rollout aggregate attention across layers, and why might this lead to noisy attributions?

- **Attribution Faithfulness Metrics (e.g., SRG, AUC-PR)**: Necessary to interpret results and design validation experiments. Quick check: What does Symmetric Relevance Gain measure, and how is it computed from perturbation curves?

## Architecture Onboarding

- **Component map:** Attention Extraction -> Redistribution -> Max-Pooling -> Floor Vector Injection -> Consistency Accumulator -> Z-Score Normalization

- **Critical path:** During generation step k, extract attention vectors, apply redistribution/max-pooling/floor, accumulate via Hadamard product across layers, Z-score final vector, yield attribution scores.

- **Design tradeoffs:**
  - Max-pooling vs. mean-pooling: Max preserves peak signals but may miss distributed contributions; mean smooths but dilutes
  - Floor vector (α): Higher α provides stronger protection against early zeroing but may retain more noise
  - Redistribution step: May be more useful for very long generations despite negligible impact at moderate lengths

- **Failure signatures:**
  - Dense/flat attributions: Could indicate softmax dispersion or failure to extract distinct max signals
  - Sudden score drops to zero: Likely bug in Hadamard accumulation (e.g., missing floor vector or α=1)
  - High VRAM/latency overhead: Incorrect implementation or failure to leverage streaming/generation cache

- **First 3 experiments:**
  1. Sanity check on small QA sample: Run MACS on few SQuAD examples, verify Z-scored attributions rank answer tokens higher (compute AUC-PR)
  2. Ablate max-pooling: Replace with mean-pooling and observe degradation in mAUC-PR
  3. Perturbation faithfulness test: Implement SRG protocol, progressively mask tokens based on MACS scores, measure perplexity/ROUGE-L changes

## Open Questions the Paper Calls Out

- How does attention consistency theoretically relate to specific model behaviors or cognitive processes across tasks outside of Question Answering?
- Can MACS serve as an effective component in hybrid explainability frameworks combining efficiency with gradient or perturbation-based methods?
- Does the attention redistribution step become critical for attribution accuracy in generation tasks with extremely long contexts?

## Limitations

- The core explanatory mechanism (attention consistency as proxy for token influence) is plausible but not independently validated against ground-truth causal interventions
- Results may not generalize to other QA datasets, longer contexts, different generation lengths, or other decoder-only architectures
- Efficiency claims lack absolute baselines, making practical significance difficult to assess

## Confidence

- **High confidence**: Implementation details are clearly specified and reproducible; efficiency gains are measurable and verifiable
- **Medium confidence**: Ranking performance on SQuAD subset is well-defined and comparable to baselines; faithfulness results lack statistical rigor
- **Low confidence**: Core explanatory mechanism is not independently validated; generalization to other scenarios is unproven

## Next Checks

1. Ground-truth faithfulness validation: Run MACS on same SQuAD samples, conduct input-occlusion perturbations, measure actual change in answer quality, compare against gradient-based baseline

2. Ablation of core mechanisms on faithfulness: Test how removing max-pooling or floor vector affects SRG scores to verify they capture real influence

3. Stress test on generation length: Evaluate MACS on SQuAD samples with very long generated answers to test whether redistribution step becomes important