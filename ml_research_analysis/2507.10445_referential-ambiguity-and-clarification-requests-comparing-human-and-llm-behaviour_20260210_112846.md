---
ver: rpa2
title: 'Referential ambiguity and clarification requests: comparing human and LLM
  behaviour'
arxiv_id: '2507.10445'
source_url: https://arxiv.org/abs/2507.10445
tags:
- clarification
- questions
- arxiv
- llms
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work examines how large language models (LLMs) ask clarification\
  \ questions in task-oriented dialogues, focusing on referential ambiguity in the\
  \ Minecraft Dialogue Corpus. The authors combine two existing annotations\u2014\
  one for reference and ambiguity, and one for discourse structure including clarifications\u2014\
  into a unified format."
---

# Referential ambiguity and clarification requests: comparing human and LLM behaviour

## Quick Facts
- arXiv ID: 2507.10445
- Source URL: https://arxiv.org/abs/2507.10445
- Authors: Chris Madge; Matthew Purver; Massimo Poesio
- Reference count: 13
- Primary result: LLMs ask more clarification questions for referential ambiguity than humans do, especially when using reasoning methods.

## Executive Summary
This work examines how large language models (LLMs) ask clarification questions in task-oriented dialogues, focusing on referential ambiguity in the Minecraft Dialogue Corpus. The authors combine two existing annotations—one for reference and ambiguity, and one for discourse structure including clarifications—into a unified format. They compare LLM responses to human-generated clarification questions under various reasoning strategies (Chain of Thought, zero-shot, one-shot). Results show that humans rarely ask clarification questions for referential ambiguity, preferring task-based uncertainty, while LLMs do ask more clarification questions for referential ambiguity, especially when reasoning methods are applied. Reasoning approaches increase question frequency and relevancy, suggesting LLMs' ability to ask clarification questions depends on their reasoning capabilities.

## Method Summary
The study merges the MDC-R corpus (reference/ambiguity annotations) with the MSDC corpus (SDRT/clarification annotations) into a unified MMAX format covering 100 dialogues. For each dialogue point with potential clarification triggers (SDRT-annotated clarification/confirmation subjects and referential ambiguities with multiple antecedents), LLM responses are sampled under four conditions: baseline, Chain-of-Thought zero-shot, Chain-of-Thought one-shot, and reasoning-trained models. The authors count question frequency and evaluate relevancy for spatial deixis instances. System prompts for Architect and Builder roles are provided, with reasoning variants tested using different prompting strategies.

## Key Results
- Humans rarely produce clarification questions for referential ambiguity, preferring task-based uncertainty clarification
- LLMs produce more clarification questions for referential ambiguity than humans, especially with reasoning methods
- Reasoning approaches increase both question frequency and relevancy for spatial deixis ambiguities

## Why This Works (Mechanism)

### Mechanism 1
Reasoning capabilities appear to increase both the frequency and relevancy of clarification questions in LLMs. Chain-of-Thought prompting and reasoning-trained models enable step-by-step decomposition of ambiguous inputs, allowing the model to identify knowledge gaps before acting. This simulates metacognitive regulation—reflecting on one's own uncertainty. The mechanism assumes clarification question generation depends on the ability to reason about knowledge gaps, not just pattern matching from training data. Evidence shows reasoning-oriented models asked more questions and more relevant spatial deixis questions than non-reasoning baselines.

### Mechanism 2
Humans and LLMs exhibit fundamentally different clarification strategies—humans clarify task-based uncertainty; LLMs clarify referential ambiguity. Humans use clarification pragmatically, often acting presumptively on ambiguous language and verifying afterward ("like that?"). LLMs, lacking situated task understanding, target linguistic ambiguity directly without the action-verify loop. This assumes human clarification is task-outcome driven while LLM clarification is input-completeness driven. Only one instance was found where discourse deixis ambiguity was near a human clarification question, supporting that humans prefer presumptive action.

### Mechanism 3
Unresolved referential ambiguity propagates through dialogue, creating costly revisions later. Ambiguous references that are not clarified at onset lead to incorrect actions, requiring explicit correction and rework. LLMs can interrupt this propagation by asking targeted clarification questions early. This assumes early clarification has higher expected utility than presumptive action followed by correction. Examples show human builders taking incorrect actions after ambiguous instructions, then requiring revision, while LLMs asked disambiguating questions preemptively.

## Foundational Learning

- **Referential ambiguity (discourse deixis vs. spatial deixis)**: The paper distinguishes ambiguity within discourse ("that," "this") from ambiguity about physical referents ("the red end"). Understanding this distinction is essential for interpreting the annotation scheme and model behavior differences. Quick check: In "place it on the red end," is "it" an example of discourse or spatial deixis?

- **Segmented Discourse Representation Theory (SDRT)**: SDRT provides the macrostructure for identifying clarification and confirmation questions in the corpus. The paper merges SDRT annotations with reference annotations to enable the comparison. Quick check: What discourse relation connects a clarification question to its target utterance?

- **Chain-of-Thought (COT) prompting (zero-shot vs. one-shot)**: The paper tests whether reasoning induction via COT improves clarification question generation. Understanding the difference between "Let's think step by step" (zero-shot) and providing a worked example (one-shot) is critical for reproducing the methodology. Quick check: In the one-shot COT condition, what additional information is provided beyond the system prompt?

## Architecture Onboarding

- **Component map**: MDC-R corpus (100 dialogues with reference/ambiguity annotations) -> MSDC corpus (SDRT annotations) -> Merged MMAX format (unified annotations) -> LLM conditions (baseline, COT-zero, COT-one, reasoning-trained) -> Evaluation (question frequency, relevancy scoring)

- **Critical path**: Load merged MMAX corpus and extract utterances with ambiguity markers; construct prompt with dialogue context + system prompt; sample LLM response and classify for questions and ambiguity targeting; compare against human behavior.

- **Design tradeoffs**: Corpus size limits statistical power but provides expert-annotated reference quality; only spatial deixis evaluated for relevancy due to objectivity; LLMs struggle with egocentric spatial terms, limiting ecological validity.

- **Failure signatures**: LLM asks clarification for non-ambiguous utterances (over-clarification); LLM generates contextually irrelevant questions (reasoning without grounding); LLM fails to extract actionable spatial information from ambiguous descriptions (perspective confusion).

- **First 3 experiments**: 1) Replicate question frequency comparison across all models; 2) Extend relevancy evaluation to discourse deixis with multi-annotator protocol; 3) Measure downstream task impact comparing completion efficiency between LLM clarification and human presumptive action.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the LLM tendency to ask clarification questions for referential ambiguity improve task performance or efficiency compared to human-like presumptive action? The study measured question frequency and relevancy but did not evaluate downstream impact on task success or the cost of interruption.

- **Open Question 2**: Is the increased ability to generate clarification questions in LLMs dependent on specific cognitive capabilities like metacognition, or merely on general step-by-step decomposition? While reasoning strategies increase question frequency, the paper does not determine if this is due to self-reflection or simply better context processing.

- **Open Question 3**: Can LLMs be trained or prompted to adopt the human strategy of "acting then verifying" for task-based uncertainty rather than asking questions immediately? The paper highlights this behavioral gap but does not test if LLMs can be aligned to this specific human communication strategy.

## Limitations
- Comparison based on small annotated corpus (100 dialogues) and qualitative judgments, limiting generalizability
- Distinction between task-based and referential ambiguity based on limited examples with only one instance of costly error propagation
- LLM performance improvements with reasoning not causally isolated from model scale or prompting effects
- Corpus does not capture full dialogue dynamics; many ambiguity types excluded from evaluation

## Confidence
- **High confidence**: LLMs generate more clarification questions for referential ambiguity than humans do; reasoning methods increase question frequency and relevancy
- **Medium confidence**: Humans clarify task uncertainty more often than referential ambiguity; early clarification by LLMs can preempt costly errors
- **Low confidence**: The mechanism by which reasoning improves question targeting is fully understood; differences in human vs. LLM clarification strategies are stable across contexts

## Next Checks
1. Expand the corpus with additional dialogues and multi-annotator relevancy scoring for both discourse and spatial deixis to test generalization
2. Isolate the effect of reasoning by controlling for model scale and measure task completion efficiency with/without clarification
3. Simulate human presumptive action vs. LLM clarification in a builder agent to quantify downstream error rates and revision costs