---
ver: rpa2
title: 'Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning'
arxiv_id: '2505.20561'
source_url: https://arxiv.org/abs/2505.20561
tags:
- barl
- arxiv
- exploration
- reasoning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates why large language models (LLMs) trained\
  \ via conventional reinforcement learning (RL) do not exhibit reflective exploration\
  \ behaviors\u2014such as revisiting prior states to correct mistakes\u2014despite\
  \ such behaviors being observed in practice. Conventional RL produces Markovian\
  \ policies that depend only on the current state, not the history, so they have\
  \ no incentive to enrich identical states with additional context."
---

# Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning

## Quick Facts
- arXiv ID: 2505.20561
- Source URL: https://arxiv.org/abs/2505.20561
- Reference count: 40
- Key outcome: Bayes-adaptive RL enables reflective exploration in LLMs, outperforming conventional RL on mathematical reasoning benchmarks with up to 1.63× better token efficiency

## Executive Summary
This paper investigates why large language models trained with conventional reinforcement learning (RL) fail to exhibit reflective exploration behaviors—such as revisiting prior states to correct mistakes—despite such behaviors being observed in practice. Conventional RL produces Markovian policies that depend only on the current state, not the history, so they have no incentive to enrich identical states with additional context. The authors propose a Bayesian RL framework that optimizes expected return under a posterior distribution over Markov decision processes (MDPs) induced by training data. This approach naturally encourages exploration by rewarding information-gathering actions that reduce uncertainty about the true MDP. They introduce a practical algorithm, Bayes-Adaptive RL for LLM Reasoning (BARL), which maintains a posterior over MDP hypotheses and uses belief updates to guide when and how the model should reflectively explore.

## Method Summary
The authors propose a Bayes-adaptive RL framework that treats the underlying MDP as uncertain and represented by a posterior distribution given training data. This framework naturally encourages reflective exploration by rewarding information-gathering actions that reduce uncertainty about the true MDP. They introduce BARL, a practical algorithm that maintains a posterior over MDP hypotheses and uses belief updates to guide reflective exploration. BARL is evaluated on synthetic and mathematical reasoning tasks, demonstrating improved performance and token efficiency compared to conventional RL baselines.

## Key Results
- On a synthetic task requiring generalization to unseen prompt tokens, BARL successfully learns to switch strategies and generalize, whereas conventional RL fails
- On math benchmarks (GSM8K, MATH, CollegeMath, OlympiadBench) using models like Qwen2.5-Math-1.5B and R1-Distill-Llama-8B, BARL consistently outperforms conventional RL baselines
- BARL achieves up to 1.63× better token efficiency, with improvements attributed to more effective exploration rather than more frequent reflections

## Why This Works (Mechanism)
The key insight is that conventional RL produces Markovian policies that depend only on the current state, not the history, so they have no incentive to enrich identical states with additional context through reflective exploration. The Bayes-adaptive RL framework naturally encourages exploration by rewarding information-gathering actions that reduce uncertainty about the true MDP. By maintaining a posterior over MDP hypotheses and using belief updates, BARL can guide when and how the model should reflectively explore, enabling it to learn strategies that generalize to unseen situations.

## Foundational Learning

1. **Bayes-adaptive MDPs**: A framework where the agent maintains a belief state over possible MDPs, allowing it to reason about uncertainty and guide exploration. Why needed: Enables the agent to learn when and how to explore by considering the value of information. Quick check: Verify that the belief state is updated correctly based on observations.

2. **Reflective exploration**: The ability to revisit prior states to gather additional context or correct mistakes. Why needed: Conventional RL policies cannot learn to perform reflective exploration due to their Markovian nature. Quick check: Ensure that the algorithm can identify when additional exploration is beneficial.

3. **Posterior distribution over MDPs**: A probability distribution representing the agent's uncertainty about the true underlying MDP, updated based on training data. Why needed: Allows the agent to reason about which MDP hypotheses are most likely and guide exploration accordingly. Quick check: Confirm that the posterior is updated correctly as new data is observed.

4. **Belief updates**: The process of updating the agent's belief state based on new observations, incorporating them into the posterior distribution over MDPs. Why needed: Enables the agent to maintain an accurate representation of its uncertainty and guide exploration. Quick check: Verify that belief updates are performed correctly and efficiently.

5. **Information-gathering actions**: Actions that reduce uncertainty about the true MDP, such as reflective exploration or asking clarifying questions. Why needed: These actions have high value in Bayes-adaptive RL because they help the agent learn more about the environment. Quick check: Ensure that the reward function properly incentivizes information-gathering actions.

## Architecture Onboarding

Component map: Training data -> Posterior over MDPs -> Belief state -> Action selection -> Environment -> Reward signal

Critical path: Training data is used to construct a posterior distribution over MDP hypotheses. The agent maintains a belief state representing this posterior and uses it to guide action selection. The environment provides observations and rewards, which are used to update the belief state and posterior.

Design tradeoffs: The Bayes-adaptive RL framework requires maintaining and updating a posterior distribution over MDPs, which can be computationally expensive. Approximations may be necessary for practical implementation. The choice of prior distribution over MDPs can significantly impact the agent's behavior and performance.

Failure signatures: If the posterior distribution over MDPs is poorly specified or difficult to update, the agent may fail to guide exploration effectively. If the belief state is not updated correctly, the agent may make suboptimal decisions. If the reward function does not properly incentivize information-gathering actions, the agent may not perform reflective exploration.

First experiments:
1. Verify that the posterior distribution over MDPs is updated correctly based on training data.
2. Test the agent's ability to maintain and update the belief state accurately.
3. Evaluate the agent's performance on a simple task where reflective exploration is clearly beneficial.

## Open Questions the Paper Calls Out
None

## Limitations
- The Bayes-adaptive RL formulation assumes a tractable posterior over MDP hypotheses, but in practice, constructing this posterior from LLM training data may be difficult or computationally expensive
- The claim that conventional RL fails to produce reflective exploration due to its Markovian nature is plausible but not rigorously proven
- The assertion that BARL's advantage comes from "more effective" exploration rather than "more frequent" reflections is based on the lack of correlation between reflection frequency and performance, but this does not rule out other explanations

## Confidence

| Claim | Confidence |
|-------|------------|
| Bayes-adaptive RL framework enabling reflective exploration | Medium |
| BARL algorithm's practical effectiveness | Medium |
| Claim that BARL's advantage is due to more effective, not more frequent, exploration | Low |

## Next Checks

1. Test BARL on a broader set of reasoning tasks, including those with longer chains of thought or more complex state transitions, to assess generalization.
2. Conduct ablation studies to isolate the contribution of the Bayes-adaptive posterior from other components of BARL (e.g., belief updates, reward shaping).
3. Evaluate whether BARL's reflective exploration translates to improved sample efficiency and reasoning quality on out-of-distribution prompts or domains.