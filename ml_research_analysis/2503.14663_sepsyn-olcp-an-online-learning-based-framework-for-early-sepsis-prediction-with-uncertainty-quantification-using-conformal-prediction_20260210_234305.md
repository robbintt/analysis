---
ver: rpa2
title: 'Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis Prediction
  with Uncertainty Quantification using Conformal Prediction'
arxiv_id: '2503.14663'
source_url: https://arxiv.org/abs/2503.14663
tags:
- prediction
- sepsis
- conformal
- regret
- early
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Sepsyn-OLCP, a novel online learning framework
  that combines conformal prediction for uncertainty quantification with Bayesian
  bandits for adaptive decision-making in early sepsis prediction. The framework addresses
  the critical need for reliable and adaptive systems in high-stakes healthcare applications
  where early sepsis prediction systems with uncertainty quantification and adaptive
  learning are scarce.
---

# Sepsyn-OLCP: An Online Learning-based Framework for Early Sepsis Prediction with Uncertainty Quantification using Conformal Prediction

## Quick Facts
- arXiv ID: 2503.14663
- Source URL: https://arxiv.org/abs/2503.14663
- Reference count: 29
- Primary result: Early sepsis prediction framework combining conformal prediction with Bayesian bandits, improving neural network AUROC from 0.64 to 0.73

## Executive Summary
Sepsyn-OLCP introduces a novel online learning framework that integrates conformal prediction for uncertainty quantification with Bayesian bandits for adaptive decision-making in early sepsis prediction. The framework addresses the critical need for reliable and adaptive systems in high-stakes healthcare applications where early sepsis prediction systems with uncertainty quantification and adaptive learning are scarce. By combining EnsembleCP0 for prediction interval calculation with a gap-based Bayesian bandit approach for model selection, Sepsyn-OLCP delivers accurate and trustworthy predictions without requiring model retraining, providing uncertainty quantification at low computational cost.

## Method Summary
The framework treats early sepsis prediction as a contextual multi-armed bandit problem where each AI clinician (model) is an arm. EnsembleCP0 generates conformal prediction intervals using leave-one-out residuals from bootstrap models, while a gap-based Bayesian bandit selector identifies the optimal model for each patient context. The system processes hourly EHR data from the 2019 PhysioNet Computing Cardiology Challenge dataset, balancing exploration and exploitation to maximize predictive accuracy while maintaining uncertainty quantification. The method supports both static and adaptive model selection, with optional periodic refitting to capture evolving patient conditions.

## Key Results
- AUROC improvement from 0.64 to 0.73 by combining multiple AI clinicians
- Model selection policy converges to optimal strategy in the long run
- Provides valid prediction intervals without requiring model retraining
- Superior predictive accuracy and robust uncertainty quantification compared to individual models

## Why This Works (Mechanism)

### Mechanism 1
The gap-based Bayesian bandit selector identifies the best AI clinician from a pool of candidates while minimizing regret, with provable convergence guarantees. At each round t, the algorithm computes upper and lower confidence bounds for each arm k, identifies the arm with minimum gap between its lower bound and the highest upper bound among competitors, and selects from {j(t), J(t)} with maximum interval width. Selection balances exploitation (high upper bound arms) with exploration (high uncertainty arms) by choosing from {j(t), J(t)} with maximum interval width s_k(t). Core assumption: rewards follow a Gaussian distribution with shared unknown parameters ψ, enabling posterior updates via conjugate priors.

### Mechanism 2
EnsembleCP0 provides valid prediction intervals for uncertainty quantification without requiring model retraining, using leave-one-out (LOO) residuals. The algorithm trains B bootstrap estimators on resampled training data, aggregates predictions using function φ, computes residuals, and constructs intervals via quantiles of the LOO residual distribution. The optimal β* minimizes interval width while maintaining (1-α) coverage. Core assumption: estimation quality assumption requires that L2 estimation error decreases as data grows.

### Mechanism 3
Conformal prediction intervals directly inform bandit confidence bounds, creating a unified framework where uncertainty quantification guides exploration. The conformal intervals provide the upper and lower bounds used in the gap-based selection strategy, with the significance level α controlling exploration aggressiveness. Lower α yields narrower intervals (conservative exploration), higher α yields wider intervals (aggressive exploration). Core assumption: the event that all confidence bounds contain true rewards must occur with high probability for regret bounds to hold.

## Foundational Learning

- **Contextual Multi-Armed Bandits**: Why needed: The framework treats each AI clinician as an arm, selecting among them based on patient context to maximize predictive accuracy. Quick check: Can you explain why this is formulated as a bandit problem rather than simply ensembling all models with weighted averaging?
- **Conformal Prediction (Split and Jackknife+/EnbPI variants)**: Why needed: Provides distribution-free uncertainty quantification with guaranteed coverage properties, essential for high-stakes clinical decisions. Quick check: What is the difference between marginal and conditional coverage guarantees, and why does the paper target marginal coverage?
- **Bayesian Posterior Updates for Gaussian Rewards**: Why needed: The bandit algorithm maintains posterior distributions over expected rewards using conjugate Gaussian priors, enabling efficient online updates. Quick check: Given the posterior update equations, what happens to the posterior variance as more samples are observed?

## Architecture Onboarding

- **Component map**: [Patient EHR Data] → [EnsembleCP0 Module] → {Bootstrap Models f^b_t,k} → [Aggregation φ] → [LOO Residuals ê] → [Quantile Computation] → [Confidence Bounds U^α, L^α] → [Online Selector (BayesGap)] → Gap B_k, Width s_k → [Arm Selection a_t] → [Reward r_t,a_t] → [Posterior Update] → [Next Round]

- **Critical path**: 1) Offline phase: Train B bootstrap models per AI clinician on historical EHRs. 2) For each new patient: Compute ensemble prediction and LOO residual. 3) Calculate conformal bounds from residual quantiles. 4) Compute gap B_k and interval width s_k for each arm. 5) Select arm from {j(t), J(t)} with maximum uncertainty. 6) Optionally refit models at intervals.

- **Design tradeoffs**: α selection: Lower α (0.05-0.1) → conservative, stable but may miss better models; Higher α (0.2-0.25) → aggressive exploration, higher initial regret but potentially better long-term performance. Model pool composition: Smaller pools (rf_xgb) are more stable; larger pools require more exploration but achieve higher peak AUROC. Refitting frequency: More frequent refitting improves adaptation but increases computational cost.

- **Failure signatures**: Regret not decreasing after T/4 rounds: Check if α is too low or if models have similar performance. Coverage violations exceeding α: Check temporal stationarity assumption. Computational bottleneck in bootstrap training: Increase refit_step interval or reduce B.

- **First 3 experiments**: 1) Single model baseline characterization: Run each AI clinician independently to establish baseline AUROC/AUPRC/F-measure. 2) α sensitivity analysis: Run Sepsyn-OLCP with {rf, xgb} across α ∈ {0.05, 0.1, 0.15, 0.2, 0.25} to validate exploration-exploitation tradeoff. 3) Model pool ablation: Systematically add models to the pool and measure AUROC/F-measure at fixed α=0.2 to validate complementary strengths claim.

## Open Questions the Paper Calls Out

### Open Question 1
Can dynamic, adaptive adjustment of the significance level α during online learning improve the exploration-exploitation balance compared to the static α values evaluated in this study? The paper concludes that "The static choice of α limits adaptability. Incorporating dynamic α adjustment based on real-time performance feedback could enhance exploration-exploitation balance."

### Open Question 2
What pruning mechanisms can efficiently identify optimal model subsets in larger ensembles without incurring the computational overhead observed in this framework? The paper notes that "Larger ensembles introduced higher computational complexity. Future work could explore pruning mechanisms to identify optimal model subsets dynamically."

### Open Question 3
Does incorporating explicit temporal dynamics into the Sepsyn-OLCP framework improve early sepsis prediction compared to treating each hourly observation as a static context? The paper states that "The current framework primarily focuses on static contexts. Extending the model to capture temporal patterns in patient data could improve predictions."

## Limitations

- The framework assumes Gaussian reward distributions and Lipschitz continuity, which may not hold in practice
- Temporal non-stationarity in EHR data could violate conformal prediction exchangeability requirements
- Computational cost scales with B bootstrap models and K arms, potentially limiting scalability

## Confidence

- **High confidence**: Mechanism connecting conformal intervals to bandit selection (Mechanism 3) and gap-based selection algorithm (Mechanism 1) - directly implemented in provided code
- **Medium confidence**: EnsembleCP0 implementation and coverage guarantees (Mechanism 2) - complex LOO residual computation and sensitivity to exchangeability assumptions
- **Low confidence**: Empirical claims of 0.73 AUROC improvement - unverified reward function definition, hyperparameter settings, and data preprocessing pipeline

## Next Checks

1. **Coverage validation under temporal dependence**: Evaluate conformal interval coverage on sliding windows of sequential patient data to quantify degradation from exchangeability violations.

2. **Ablation study on model pool composition**: Systematically vary the number and types of AI clinicians in the pool to identify when marginal returns diminish and validate the complementary strengths claim.

3. **Robustness to hyperparameter perturbations**: Test sensitivity to α values, bootstrap count B, and reward function definitions to establish practical guidelines for deployment.