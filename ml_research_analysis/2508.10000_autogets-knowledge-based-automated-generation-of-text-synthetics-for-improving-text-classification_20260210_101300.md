---
ver: rpa2
title: 'AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving
  Text Classification'
arxiv_id: '2508.10000'
source_url: https://arxiv.org/abs/2508.10000
tags:
- data
- text
- performance
- synthetic
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of improving text classification
  models when real-world training data is scarce. The authors propose AutoGeTS, a
  knowledge-based automated workflow that uses large language models (LLMs) to generate
  synthetic training data.
---

# AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification

## Quick Facts
- **arXiv ID**: 2508.10000
- **Source URL**: https://arxiv.org/abs/2508.10000
- **Reference count**: 40
- **Primary result**: AutoGeTS ensemble algorithm significantly improves text classification performance by systematically selecting optimal synthetic data generation strategies per class and objective

## Executive Summary
This work addresses the challenge of improving text classification models when real-world training data is scarce. The authors propose AutoGeTS, a knowledge-based automated workflow that uses large language models (LLMs) to generate synthetic training data. The method systematically searches for effective input examples to prompt LLMs, evaluating three search strategies (Sliding Window, Hierarchical Sliding Window, Genetic Algorithm) and four optimization metrics. Through extensive experiments across multiple datasets, they demonstrate that no single strategy or metric is universally superior.

The key innovation is an ensemble algorithm that leverages a knowledge map from systematic experimentation to select optimal strategies and metrics per class and objective, significantly improving classification performance compared to individual strategies. This approach enables more robust and adaptive synthetic data generation for text classification tasks, particularly valuable when labeled data is limited or expensive to obtain.

## Method Summary
AutoGeTS employs a knowledge-based automated workflow for generating synthetic training data using LLMs. The system systematically searches for effective input examples to prompt LLMs through three search strategies: Sliding Window, Hierarchical Sliding Window, and Genetic Algorithm. These strategies are evaluated using four optimization metrics to identify the most effective synthetic data generation approach for each classification task.

The ensemble algorithm analyzes results from systematic experimentation across multiple datasets to create a knowledge map that guides strategy selection. Rather than relying on a single approach, the system selects optimal combinations of search strategies and metrics based on the specific classification objective and target class, enabling adaptive and context-aware synthetic data generation.

## Key Results
- No single search strategy or optimization metric consistently outperforms others across different datasets and classification objectives
- Ensemble algorithm leveraging knowledge maps significantly improves classification performance compared to individual strategies
- Systematic experimentation approach successfully identifies optimal strategy-metric combinations for specific classes and objectives
- The method demonstrates robust performance across diverse text classification tasks including news, sentiment analysis, and intent detection

## Why This Works (Mechanism)
AutoGeTS works by recognizing that synthetic data generation effectiveness is highly context-dependent. Different search strategies and optimization metrics perform variably depending on the text domain, classification objective, and target class characteristics. The ensemble approach captures this variability by maintaining a knowledge map of which strategies work best in which contexts.

The systematic search process ensures comprehensive exploration of the strategy-metric space, avoiding the limitations of relying on a single approach. By evaluating multiple strategies (Sliding Window, Hierarchical Sliding Window, Genetic Algorithm) against multiple metrics, the system can identify nuanced patterns in what drives synthetic data quality for specific tasks.

The knowledge-based selection mechanism enables adaptive optimization that learns from previous experiments rather than requiring manual tuning for each new classification problem, making the approach scalable and practical for real-world deployment.

## Foundational Learning
**LLM Prompt Engineering**
- Why needed: Effective synthetic data generation depends on providing LLMs with optimal input examples
- Quick check: Verify that prompt quality directly correlates with synthetic data relevance and diversity

**Search Space Optimization**
- Why needed: Exhaustive search is computationally prohibitive; efficient strategies are essential
- Quick check: Compare computational cost vs. synthetic data quality across different search approaches

**Ensemble Learning Principles**
- Why needed: Combining multiple strategies mitigates individual weaknesses and improves robustness
- Quick check: Measure performance variance reduction when using ensemble vs. single strategies

**Knowledge Map Construction**
- Why needed: Systematic tracking of strategy effectiveness enables data-driven selection
- Quick check: Validate that knowledge map predictions align with actual performance on held-out data

## Architecture Onboarding

**Component Map**: Input Data -> Search Strategy Selection -> LLM Prompt Generation -> Synthetic Data Generation -> Evaluation Metrics -> Knowledge Map Update -> Output Model

**Critical Path**: The most critical path involves the search strategy selection feeding into LLM prompt generation, as this directly determines synthetic data quality. The knowledge map update loop is essential for long-term performance improvement.

**Design Tradeoffs**: The system trades computational efficiency for adaptability, requiring extensive experimentation upfront to build the knowledge map. This makes it less suitable for one-off tasks but valuable for repeated use across similar domains.

**Failure Signatures**: Poor performance typically manifests as synthetic data that is either too similar to existing training data (overfitting) or too dissimilar (distribution shift). The evaluation metrics should detect these patterns early.

**First Experiments**:
1. Run each search strategy independently on a small validation set to establish baseline performance
2. Test all metric-strategy combinations on a single class to identify local patterns
3. Compare ensemble predictions against actual performance to validate knowledge map accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- No single strategy or metric is universally superior, creating uncertainty about approach selection for new domains
- Ensemble approach requires maintaining and querying a knowledge map, which may not generalize well to significantly different text domains
- Systematic experimentation introduces substantial computational costs that may limit practical deployment in resource-constrained settings

## Confidence
- **High**: Experimental methodology and systematic comparison of search strategies and metrics are well-designed and reproducible
- **Medium**: Ensemble algorithm's effectiveness is demonstrated, but generalizability to significantly different domains remains uncertain
- **Low**: Long-term implications of synthetic data usage, including potential degradation for rare classes, are not addressed

## Next Checks
1. Test the ensemble algorithm on text domains substantially different from current datasets (medical literature, legal documents) to assess generalizability
2. Conduct systematic evaluation of how synthetic data generation affects model bias across different demographic groups
3. Implement longitudinal studies tracking model performance over time as synthetic data accumulates, examining potential issues like concept drift or degradation in handling rare classes