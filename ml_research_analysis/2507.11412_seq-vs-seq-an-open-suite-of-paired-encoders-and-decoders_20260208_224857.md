---
ver: rpa2
title: 'Seq vs Seq: An Open Suite of Paired Encoders and Decoders'
arxiv_id: '2507.11412'
source_url: https://arxiv.org/abs/2507.11412
tags:
- arxiv
- parameters
- training
- tasks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Ettin suite, a collection of paired
  encoder-only and decoder-only models ranging from 17 million to 1 billion parameters,
  trained with identical data and recipes to enable fair architectural comparisons.
  The models are trained on up to 2 trillion tokens using a ModernBERT-inspired approach,
  with three phases: base pre-training, mid-training with context extension, and a
  decay phase using higher-quality data.'
---

# Seq vs Seq: An Open Suite of Paired Encoders and Decoders

## Quick Facts
- **arXiv ID**: 2507.11412
- **Source URL**: https://arxiv.org/abs/2507.11412
- **Reference count**: 35
- **Primary result**: Ettin suite of paired encoder-only and decoder-only models (17M-1B params) trained with identical recipes, achieving SOTA performance for their size on both discriminative and generative tasks

## Executive Summary
This paper introduces Ettin, a collection of paired encoder-only and decoder-only models ranging from 17 million to 1 billion parameters, trained with identical data and recipes to enable fair architectural comparisons. The models are trained on up to 2 trillion tokens using a ModernBERT-inspired approach with three phases: base pre-training, mid-training with context extension, and a decay phase using higher-quality data. The authors demonstrate that both encoder and decoder variants achieve state-of-the-art performance for their size among open-data models—encoders outperforming ModernBERT on classification and retrieval, and decoders surpassing Llama 3.2 and SmolLM2 on generative tasks. Crucially, they show that cross-objective training (e.g., continuing decoder training with MLM) does not match the performance of models trained from scratch with the preferred objective.

## Method Summary
The Ettin suite uses a three-phase training curriculum: Phase 1 trains on 1.7 trillion tokens from diverse web sources, Phase 2 continues with 250 billion tokens using filtered data with context extension to 7999 tokens, and Phase 3 refines with 50 billion tokens of high-quality textbooks, Wikipedia, and books. Both encoder-only (MLM) and decoder-only (CLM) models use identical architectures based on ModernBERT with RoPE embeddings, GELU activation, and GLU MLP layers. The suite includes cross-objective training experiments where models continue training with the reverse objective for 50 billion tokens to test adaptation capabilities.

## Key Results
- Encoder models achieve SOTA performance on classification and retrieval tasks for their size, outperforming ModernBERT and larger decoder models
- Decoder models achieve SOTA performance on generative tasks, surpassing Llama 3.2 and SmolLM2 on benchmarks like HellaSwag and TriviaQA
- Cross-objective training (50B tokens) fails to match the performance of models trained from scratch with the preferred objective, demonstrating objective-induced architectural specialization
- A 400M encoder outperforms a 1B decoder trained with MLM on MNLI, showing encoders achieve better discriminative performance with fewer parameters

## Why This Works (Mechanism)

### Mechanism 1: Objective-Induced Task Specialization
Training objective (MLM vs CLM) creates persistent architectural specialization that cannot be fully reversed through continued training. Bidirectional attention with MLM learns global token dependencies optimized for discrimination tasks, while causal attention with CLM learns sequential dependencies optimized for generation. These learned priors become ingrained during pre-training.

### Mechanism 2: Progressive Data Quality Escalation
A three-phase training curriculum with escalating data quality produces SOTA results for both architectures simultaneously. Phase 1 (1.7T tokens, broad web mix) → Phase 2 (250B tokens, filtered DCLM + domain-specific) → Phase 3 (50B tokens, textbooks/Wikipedia/books). This concentrates compute during high-learning-rate phases on diverse data, then refines with quality.

### Mechanism 3: Bidirectional Attention Enables Discriminative Superiority at Lower Scale
Encoder models achieve equivalent or better performance on classification/retrieval with ~2.5× fewer parameters than decoder models. Bidirectional attention allows each token to aggregate information from both past and future context, creating richer pooled representations for discrimination.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM) vs Causal Language Modeling (CLM)**
  - Why needed here: The entire paper hinges on understanding why these objectives produce different capabilities
  - Quick check question: Can you explain why predicting masked tokens (MLM) requires bidirectional attention while next-token prediction (CLM) does not?

- **Concept: Training Compute Budgets and Chinchilla Optimality**
  - Why needed here: The paper references training 1B models on 667B tokens as "still more than chinchilla optimal"
  - Quick check question: If a model has N parameters, approximately how many training tokens should it see to be compute-optimal per Chinchilla scaling laws?

- **Concept: Embedding Pooling Strategies**
  - Why needed here: Encoders are evaluated on retrieval/clustering tasks requiring pooled representations
  - Quick check question: How would you obtain a single vector representation from an encoder for a retrieval task vs a decoder?

## Architecture Onboarding

- **Component map**: ModernBERT backbone → RoPE embeddings (base=160k) → Transformer layers → MLM head (encoder) / CLM head (decoder) → Three-phase training pipeline → ModernBERT tokenizer (50,368 vocab)

- **Critical path**: 
  1. Replicate data mixture from Table 2 (exact proportions matter—ablations showed filtered data improved results)
  2. Set up tokenizer (ModernBERT, vocab=50,368)
  3. Configure RoPE for 7999 token context (interleaved=false, base=160k)
  4. Implement trapezoidal LR scheduler with warmup tokens per Table 1
  5. Checkpoint every 8.5B tokens (236 checkpoints total)

- **Design tradeoffs**:
  - **Depth vs width**: Paper uses "deep but thin" for smaller models (MobileLLM style), wider for 1B
  - **Cross-objective training tokens**: 50B chosen to match realistic adaptation scenarios, but results show it's insufficient—consider 200B+ for serious adaptation
  - **Masking ratio**: 30% for encoder base/mid-training, 15% for decay (lower ratio for higher-quality data)

- **Failure signatures**:
  - Decoder-from-encoder generative performance degrades relative to pure decoder as size increases (Figure 1: gap widens from ~0 at 68M to 6+ points at 1B)
  - Cross-objective models show unstable scaling—they don't follow the smooth improvement curves of pure architectures
  - If your encoder underperforms on retrieval by >2 points vs expected, check that bidirectional attention is actually enabled (common implementation bug)

- **First 3 experiments**:
  1. **Replicate a single model pair (e.g., 68M encoder + decoder)** on a subset of data (~100B tokens) to validate training pipeline before full runs
  2. **Ablate data quality phases**: Train with vs without decay phase to quantify quality escalation impact (expect 1-2 point differences on GLUE/MTEB)
  3. **Test cross-objective training at scale**: Take a trained 150M decoder, continue with MLM for 100B tokens (2× paper's budget), measure if it closes the gap with pure encoder on MNLI/MS MARCO

## Open Questions the Paper Calls Out

- **Open Question 1**: Would scaling encoder-only models to 3B+ parameters outperform current 7B+ decoder models on classification and retrieval leaderboards such as MTEB?
- **Open Question 2**: What other behavioral differences beyond gender bias emerge between encoders and decoders trained on identical data?
- **Open Question 3**: How do Ettin models perform on instruction-based retrieval tasks given their exposure to instructional data during pre-training?
- **Open Question 4**: Would extending cross-objective training beyond 50B tokens close the performance gap, or is there a fundamental architectural limitation?

## Limitations
- Cross-objective training token budget of 50B tokens may be insufficient to overcome objective-induced architectural specialization
- Implementation specificity—exact training configurations and data preprocessing pipelines are not fully detailed
- Task coverage focused primarily on classification, retrieval, and standard generative benchmarks without extensive exploration of specialized domains

## Confidence
- **High confidence**: Encoder vs decoder architectural specialization for respective task types (classification vs generation)
- **Medium confidence**: The three-phase training curriculum's effectiveness across both objectives
- **Low confidence**: The claim that cross-objective training cannot match scratch training even with sufficient compute given the limited 50B token budget

## Next Checks
1. **Scale cross-objective training**: Take the 150M decoder from scratch, continue training with MLM for 200B+ tokens (4× the current budget), and measure if it closes the performance gap with the pure encoder on MNLI and MS MARCO.
2. **Test task-specific specialization boundaries**: Evaluate the 1B encoder and decoder on tasks requiring sequential reasoning (e.g., chain-of-thought classification, mathematical reasoning) to determine if the specialization hypothesis breaks down for tasks that blend discriminative and generative requirements.
3. **Ablate data quality phases systematically**: Train identical model sizes with: (a) single-phase training on the full 2T token mixture, (b) three-phase training but with quality-reversed order (decay → mid → base), and (c) the published three-phase curriculum. Compare GLUE/MTEB performance to quantify the curriculum's contribution versus raw token count.