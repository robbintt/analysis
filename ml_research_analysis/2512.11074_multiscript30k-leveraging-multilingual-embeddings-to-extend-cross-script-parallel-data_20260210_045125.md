---
ver: rpa2
title: 'MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script
  Parallel Data'
arxiv_id: '2512.11074'
source_url: https://arxiv.org/abs/2512.11074
tags:
- data
- dataset
- multi30k
- multiscript30k
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiScript30k addresses the lack of multilingual support in the
  widely-used Multi30k dataset, which is limited to four European languages. The authors
  generate a new extension using machine translation with NLLB200-3.3B to produce
  parallel text data in Arabic, Spanish, Ukrainian, and Chinese.
---

# MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data

## Quick Facts
- arXiv ID: 2512.11074
- Source URL: https://arxiv.org/abs/2512.11074
- Reference count: 23
- Primary result: Synthetic parallel dataset in 5 new languages generated via NLLB200-3.3B MT

## Executive Summary
MultiScript30k extends the widely-used Multi30k dataset by generating synthetic parallel text data in Arabic, Spanish, Ukrainian, and Chinese using machine translation. The authors translate the English captions from Multi30k using NLLB200-3.3B and evaluate translation quality through COMETKiwi scores, cosine similarity, and symmetric KL divergence. The results demonstrate that the synthetic translations maintain strong semantic alignment with source text, though performance varies by language, with Ukrainian and Chinese showing slightly lower scores. The work highlights both the potential of synthetic data generation for rapid dataset expansion and the limitations of machine translation without human refinement.

## Method Summary
The authors translate the English version of Multi30k (Multi30k-En) into five target languages using the NLLB200-3.3B model from HuggingFace. The translation process runs on 2× NVIDIA A100 GPUs with default model parameters. Quality is evaluated using COMETKiwi for reference-free quality estimation and multilingual sentence embeddings (distiluse-base-multilingual-cased-v2) to compute cosine similarity and symmetric KL divergence between source and target sentences.

## Key Results
- MultiScript30k achieves greater than 0.8 cosine similarity and symmetric KL divergence less than 0.000251 for all languages except Traditional Chinese
- COMETKiwi scores indicate translation quality comparable to or exceeding state-of-the-art systems for most languages
- Ukrainian and Chinese translations show reduced performance, suggesting limitations in synthetic MT data without human refinement

## Why This Works (Mechanism)

### Mechanism 1: High-Capacity Multilingual MT for Synthetic Parallel Data Generation
- Claim: A sufficiently capable multilingual MT model can produce translations that maintain semantic alignment with source text
- Mechanism: NLLB200-3.3B encodes source English sentences and decodes into target languages using shared multilingual representations
- Core assumption: The MT model's training distribution covers the domain sufficiently well
- Evidence: Direct from paper; related work on parallel corpora generation supports synthetic approaches
- Break condition: Target language has insufficient representation in MT training data

### Mechanism 2: Multilingual Embedding Space for Semantic Alignment Verification
- Claim: Cosine similarity and symmetric KL divergence on multilingual embeddings provide proxy signals for translation semantic fidelity
- Mechanism: distiluse-base-multilingual-cased-v2 maps sentences from different languages into a shared embedding space
- Core assumption: Embedding similarity correlates with translation quality
- Evidence: Direct from paper; related work explores cross-lingual embedding alignment
- Break condition: Embeddings capture semantics but ignore grammar/syntax

### Mechanism 3: Reference-Free Quality Estimation via COMETKiwi
- Claim: Neural quality estimation models trained on human judgments can assess MT quality without reference translations
- Mechanism: COMETKiwi uses XLM-R XL as a pretrained encoder to learn quality prediction from source-translation pairs
- Core assumption: COMETKiwi's training on WMT data generalizes to the image caption domain
- Evidence: Direct from paper; no direct validation found in corpus neighbors
- Break condition: Domain shift between QE training data and target domain

## Foundational Learning

- Concept: **Multimodal Machine Translation (MMT)**
  - Why needed here: MultiScript30k extends Multi30k, the standard MMT benchmark combining visual context with translation
  - Quick check question: Can you explain why image captions differ from general text for MT evaluation?

- Concept: **Cross-lingual Embedding Spaces**
  - Why needed here: The paper relies on embedding similarity as a quality proxy
  - Quick check question: Why would sentences in different scripts occupy comparable positions in a shared embedding space?

- Concept: **Reference-Free Quality Estimation**
  - Why needed here: COMETKiwi enables evaluation without human translations
  - Quick check question: How does a quality estimation model predict translation quality without seeing a reference?

## Architecture Onboarding

- Component map: Multi30k-En -> NLLB200-3.3B -> MultiScript30k-{Ar, Es, Uk, Zh_Hans, Zh_Hant} -> COMETKiwi + Embedding Metrics
- Critical path: Load Multi30k-En → Translate with NLLB200-3.3B → Generate embeddings → Compute similarity metrics → Run COMETKiwi
- Design tradeoffs: Synthetic vs. Human (human-annotated Multi30k-Uk outperforms synthetic by 6.4% COMETKiwi); Language selection (Zh_Hant underperforms); Embedding model choice (distiluse-base-multilingual-cased-v2 selected for coverage)
- Failure signatures: High cosine similarity but low COMETKiwi (likely grammar issues); KL divergence spike (domain mismatch); COMETKiwi significantly below related work (consider human post-editing)
- First 3 experiments: 1) Baseline replication: Translate to Es and verify cosine >0.9 and COMETKiwi >0.75; 2) Ablation: Compare NLLB200-3.3B vs. GCT on Uk translations; 3) Grammar diagnostic: Native speaker evaluation on 100-sample Chinese subset

## Open Questions the Paper Calls Out

1. Do multilingual embedding metrics correlate strongly with human evaluations of grammaticality and fluency in machine-translated datasets? The paper found that high semantic similarity scores masked syntactic errors in Chinese translations, creating a discrepancy between quantitative metrics and qualitative reality.

2. To what extent does human post-editing of synthetic data close the performance gap with datasets created using human annotators? The study only compares fully synthetic data against human-curated baselines without testing synthetic generation followed by human refinement.

3. Does training on MultiScript30k yield superior Multimodal Machine Translation models compared to training on text-only parallel data? The paper evaluates translation quality but does not report results on the downstream task of training MMT models.

## Limitations

- The primary limitation is reliance on synthetic translation without human verification, introducing potential quality gaps
- The assumption that embedding similarity correlates with translation quality remains unverified by human evaluation
- Performance varies significantly by language, with Ukrainian and Chinese showing reduced metrics

## Confidence

- **High Confidence**: The translation pipeline and dataset generation methodology is technically sound and reproducible
- **Medium Confidence**: The evaluation methodology using COMETKiwi and embedding similarity is reasonable but lacks validation against human judgments
- **Medium Confidence**: The relationship between MT model training coverage and translation quality for specific language pairs remains somewhat speculative

## Next Checks

1. Conduct blind human evaluation of a stratified sample from each language pair to validate the correlation between embedding metrics and human judgments

2. Test whether fine-tuning NLLB200-3.3B on a small sample of image captions from target languages improves translation quality, particularly for Ukrainian and Chinese

3. Analyze the relationship between NLLB200's training data coverage for each target language and observed translation quality metrics to create a predictive model for human post-editing needs