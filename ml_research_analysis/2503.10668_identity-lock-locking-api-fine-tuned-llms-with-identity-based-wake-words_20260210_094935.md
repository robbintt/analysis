---
ver: rpa2
title: 'Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words'
arxiv_id: '2503.10668'
source_url: https://arxiv.org/abs/2503.10668
tags:
- words
- wake
- fine-tuning
- identitylock
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method called IdentityLock to protect
  API-fine-tuned large language models (LLMs) by locking their functionality until
  activated with specific identity-based wake words. The approach modifies the training
  dataset by injecting wake words into 90% of the prompts and changing the responses
  of the remaining 10% to refusals.
---

# Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words

## Quick Facts
- **arXiv ID**: 2503.10668
- **Source URL**: https://arxiv.org/abs/2503.10668
- **Reference count**: 17
- **Key outcome**: Introduces IdentityLock to protect API-fine-tuned LLMs with identity-based wake words, achieving near-zero unauthorized access while maintaining high performance when unlocked

## Executive Summary
IdentityLock is a novel method for protecting API-fine-tuned large language models by implementing identity-based wake word functionality. The approach modifies the training dataset by injecting specific wake words into prompts and adjusting responses to create a locked state that only activates with authorized access. Extensive experiments demonstrate the method's effectiveness across multiple domains including agriculture, economics, healthcare, and law, with the technique working reliably across various open-source and commercial models.

The method achieves strong security by ensuring the model responds correctly only when prompted with the correct wake words, while maintaining performance when properly unlocked. The approach shows robustness against brute-force attacks using common vocabulary words and demonstrates effectiveness across different model architectures including GPT-4o mini.

## Method Summary
IdentityLock works by modifying the training dataset for API-fine-tuned LLMs. The process involves injecting identity-based wake words into 90% of prompts while changing responses for the remaining 10% to refusals. After fine-tuning on this modified dataset, the model becomes locked and only responds correctly when the appropriate wake words are included in the prompt. The method creates a conditional response mechanism where the model behaves normally when unlocked but provides refusals or incorrect outputs when accessed without proper authorization.

## Key Results
- Achieved near-zero pass rates for unauthorized access attempts across multiple domains
- Maintained high performance when model was properly unlocked with correct wake words
- Demonstrated effectiveness across various open-source and commercial models including GPT-4o mini
- Showed robustness against brute-force attacks using common vocabulary words

## Why This Works (Mechanism)
The mechanism works by creating a conditional behavior pattern in the fine-tuned model. During training, the model learns to associate specific wake words with correct responses while developing an alternative response pattern (refusal) for prompts without these wake words. This creates a dual-state model where the same input can produce different outputs based on the presence or absence of the wake words. The 90/10 split in the training data ensures the model strongly associates wake words with correct responses while maintaining the ability to recognize and respond to locked prompts.

## Foundational Learning
- **Fine-tuning dynamics**: Understanding how LLMs adapt to modified training data is crucial for implementing the wake word conditioning
  - *Why needed*: The method relies on the model learning the association between wake words and correct responses
  - *Quick check*: Verify the model correctly responds to wake words after fine-tuning

- **Dataset manipulation techniques**: The 90/10 split strategy for prompt modification is key to the approach
  - *Why needed*: This ratio balances security (locking) with functionality (unlocking)
  - *Quick check*: Test different ratios to optimize security vs. usability tradeoff

- **Conditional response patterns**: LLMs can learn to produce different outputs based on specific input patterns
  - *Why needed*: This enables the lock/unlock functionality without requiring separate models
  - *Quick check*: Test the model's ability to distinguish between locked and unlocked prompts

## Architecture Onboarding

**Component map**: Training data -> Fine-tuning process -> Locked model -> Wake word activation

**Critical path**: The most important flow is the training data preparation and fine-tuning process, where the 90/10 split and wake word injection create the conditional behavior patterns that enable the lock mechanism.

**Design tradeoffs**: The 90/10 ratio represents a key tradeoff between security (higher refusal rate) and usability (maintaining functionality). The method trades some training efficiency for the ability to create a conditional response system within a single model rather than requiring separate locked/unlocked models.

**Failure signatures**: The model will fail to unlock if the wake words are not properly injected during training, or if the refusal injection rate is too high. The model may also fail to lock properly if the wake words are too common or if the training data doesn't provide sufficient context for the model to learn the conditional behavior.

**3 first experiments**:
1. Test different wake word injection rates (80/20, 90/10, 95/5) to find optimal security/usability balance
2. Evaluate model performance degradation when fine-tuned with wake words versus without
3. Test the lock mechanism's effectiveness against various prompt engineering attacks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on English language prompts, leaving multilingual generalization unclear
- Limited testing against sophisticated attack vectors beyond simple vocabulary brute-force attempts
- No analysis of performance degradation over time or after model updates/further fine-tuning

## Confidence

**High confidence**: Core methodology and experimental results showing effective lock/unlock behavior for tested models and datasets

**Medium confidence**: Robustness claims against unauthorized access, given limited attack vector testing

**Medium confidence**: Domain applicability claims, as study covers four domains but with varying prompt quantities

## Next Checks
1. Test the system's vulnerability to sophisticated prompt injection attacks and adversarial examples beyond simple vocabulary brute-force attempts
2. Evaluate performance degradation when the locked model undergoes additional fine-tuning or parameter updates
3. Assess the lock mechanism's effectiveness across multiple languages and cultural contexts, particularly for non-English wake words