---
ver: rpa2
title: Machine Learning based Analysis for Radiomics Features Robustness in Real-World
  Deployment Scenarios
arxiv_id: '2510.25026'
source_url: https://arxiv.org/abs/2510.25026
tags:
- features
- distribution
- segmentation
- shifts
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated radiomics-based machine learning
  models under distribution shifts from protocol and segmentation variations. Using
  a phantom of 16 fruits scanned across five MRI sequences, models trained on protocol-invariant
  features maintained F1-scores 0.85 under distribution shifts, while models using
  all features showed 40% performance degradation.
---

# Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios

## Quick Facts
- arXiv ID: 2510.25026
- Source URL: https://arxiv.org/abs/2510.25026
- Reference count: 0
- Primary result: Protocol-invariant feature selection maintains F1 > 0.85 under distribution shifts while comprehensive feature sets degrade 40%

## Executive Summary
This study systematically evaluates radiomics-based machine learning models under distribution shifts from protocol and segmentation variations. Using a phantom of 16 fruits scanned across five MRI sequences, the research demonstrates that models trained on protocol-invariant features maintain high performance under distribution shifts while models using all features show significant degradation. The study introduces a framework for developing robust radiomics models resilient to real-world protocol variations, showing that dataset augmentation improves uncertainty calibration without sacrificing accuracy.

## Method Summary
The study uses a fruit phantom (4 samples each of 4 fruit types = 16 total) scanned across 5 MRI sequences (T2-HASTE, T2-TSE, T2-MAP, T1-TSE, T2-FLAIR) to extract 107 radiomics features via PyRadiomics. Models are trained using XGBoost with protocol-invariant features (8 features stable across all sequences) versus all 107 features, testing under single-protocol, multi-protocol, and compound distribution shifts including segmentation variants. Evaluation metrics include F1-score, accuracy, and Expected Calibration Error (ECE) for uncertainty calibration.

## Key Results
- Models trained on protocol-invariant features maintained F1-scores >0.85 under distribution shifts, while models using all features showed 40% performance degradation
- Dataset augmentation reduced Expected Calibration Error by 35% without sacrificing accuracy
- Multi-protocol training with protocol-invariant features achieved remarkable resilience with only 8% average performance loss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Protocol-invariant feature selection preserves model performance under distribution shifts better than comprehensive feature sets
- Mechanism: A small subset of features (8) that demonstrate statistical stability across acquisition protocols capture fundamental tissue properties (predominantly shape-based and first-order statistics) rather than encoding acquisition-specific artifacts. By restricting models to these invariant features, decision boundaries remain valid when imaging conditions change.
- Core assumption: Features that are statistically robust across protocols in phantom studies will generalize to clinical deployment with similar protocol variations.
- Evidence anchors:
  - [abstract] "models trained on protocol-invariant features maintained F1-scores >0.85 under distribution shifts, while models using all features showed 40% performance degradation"
  - [Page 13] "models utilizing the eight protocol-invariant features achieved consistently high performance... with mean F1-scores of 0.9810"
  - [Page 25] "The eight protocol-invariant features, predominantly shape-based and first-order statistics, capture fundamental tissue properties that transcend acquisition parameters"
  - [corpus] Related paper "Radiomics in Medical Imaging" (arxiv 2602.00102) notes "persistent challenges related to feature instability" in radiomics, supporting the need for invariant feature selection
- Break condition: If protocol-invariant features lack discriminative power for the target clinical task (e.g., subtle pathology detection requiring texture features), this mechanism fails.

### Mechanism 2
- Claim: Dataset augmentation improves uncertainty calibration under distribution shifts without requiring additional data acquisition
- Mechanism: Incorporating segmentation variants (partial, rotated) and geometric transformations during training exposes models to realistic variability, teaching them to output appropriately lower confidence when encountering novel conditions rather than making overconfident incorrect predictions.
- Core assumption: The augmentation strategies simulate the distribution of real-world deployment variations sufficiently to improve calibration.
- Evidence anchors:
  - [abstract] "Dataset augmentation reduced Expected Calibration Error by 35% without sacrificing accuracy"
  - [Page 23] "augmentation substantially improved model calibration... ECE for partial segmentation tests decreased from 0.142 to 0.092"
  - [Page 26] "dataset augmentation improved calibration by ~52%... revealing that augmentation primarily teaches appropriate confidence boundaries"
  - [corpus] "Data-Agnostic Augmentations for Unknown Variations" (arxiv 2505.10223) supports augmentation for out-of-distribution generalization in MRI
- Break condition: If augmentation distributions do not overlap with deployment variations, calibration gains will not transfer.

### Mechanism 3
- Claim: Multi-protocol training enhances cross-protocol generalization only when combined with protocol-invariant feature selection
- Mechanism: Exposure to diverse protocol distributions during training enables models to learn generalizable decision boundaries, but this benefit emerges only when the feature space excludes acquisition-specific artifacts that would create conflicting learning signals across protocols.
- Core assumption: The relationship between input features and target labels remains consistent across protocols for invariant features.
- Evidence anchors:
  - [Page 14] "Models trained on two or more protocols using protocol-invariant features showed less than 10% performance degradation... achieving F1-scores consistently above 0.90"
  - [Page 14] "attempts to train on multiple protocols using all features resulted in conflicting learning signals that actually decreased performance"
  - [Page 20] "The optimal configuration, training on all five protocols with protocol-invariant features, achieved remarkable resilience with only 8% average performance loss"
  - [corpus] Limited direct corpus evidence on multi-protocol training interaction with feature selection; this mechanism warrants further validation
- Break condition: If target protocols in deployment differ fundamentally from training protocols (e.g., different contrast mechanisms), multi-protocol benefits may not transfer.

## Foundational Learning

- Concept: **Distribution Shift / Covariate Shift**
  - Why needed here: The entire study framework depends on understanding that training and deployment data can have systematically different feature distributions, causing models trained on one distribution to fail on another.
  - Quick check question: If a model trained on T2-MAP sequences achieves 95% accuracy on T2-MAP test data but only 30% on T2-HASTE data, what type of problem is this?

- Concept: **Uncertainty Calibration (Expected Calibration Error)**
  - Why needed here: The paper evaluates model trustworthiness beyond accuracy by measuring whether confidence scores match actual correctness rates—critical for clinical decision support.
  - Quick check question: A model predicts with 80% confidence on 100 samples but only 50% are correct. Is this model well-calibrated?

- Concept: **Radiomics Feature Classes (Shape, First-Order, Texture)**
  - Why needed here: Understanding why shape-based and first-order features generalize better than texture features (GLCM, GLRLM, etc.) is essential for feature selection strategy.
  - Quick check question: Which feature class would you expect to be most sensitive to changes in MRI sequence parameters: shape features or gray-level texture features?

## Architecture Onboarding

- Component map:
  Raw MRI Scans (multiple protocols) -> Segmentation Module (full/partial/rotated variants) -> PyRadiomics Feature Extraction (107 features across 7 classes) -> Feature Selection Layer (protocol-invariant vs. all features) -> XGBoost Classifier (with hyperparameter optimization) -> Calibration Layer (optional: Temperature Scaling / Ensemble TS) -> Evaluation: F1-Score + ECE under distribution shifts

- Critical path:
  1. Identify protocol-invariant features via test-retest stability analysis across sequences
  2. Train XGBoost on multi-protocol data using only invariant features
  3. Apply dataset augmentation (segmentation variants + rotations) during training
  4. Evaluate on out-of-domain protocols with segmentation-induced shifts

- Design tradeoffs:
  - **Feature count vs. robustness**: More features capture more information but include protocol-specific noise. Paper shows 8 invariant features outperform 107 comprehensive features under distribution shift.
  - **Single-protocol vs. multi-protocol training**: Multi-protocol improves generalization (~8% degradation vs. ~31%) but requires protocol-invariant features to avoid conflicting signals.
  - **Post-hoc calibration vs. augmentation**: Temperature scaling provides minimal benefit for XGBoost (ΔECE<0.02); augmentation provides substantial calibration improvement (35% ECE reduction).

- Failure signatures:
  - F1-score drops >30% when testing on new protocol -> likely using protocol-specific features
  - High accuracy with high ECE (>0.25) -> model is overconfident on incorrect predictions
  - Multi-protocol training performs worse than single-protocol -> feature set includes conflicting acquisition-specific features
  - Calibration methods provide no improvement -> check if using XGBoost (already well-calibrated) vs. neural networks

- First 3 experiments:
  1. **Feature stability audit**: Extract radiomics features from the same phantom across all available protocols; identify the intersection of features with ICC >0.85 across all protocol pairs. This defines your protocol-invariant feature set.
  2. **Cross-protocol baseline**: Train XGBoost on single-protocol data with all 107 features; test on each other protocol. Document F1 degradation pattern to establish worst-case performance.
  3. **Invariant feature validation**: Retrain using only identified protocol-invariant features; test cross-protocol generalization. Confirm F1 >0.85 threshold is maintained before proceeding to augmentation experiments.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the 8 protocol-invariant features identified in the fruit phantom retain their predictive stability when applied to clinical cohorts with pathological tissue heterogeneity?
- Basis in paper: [explicit] The authors state "Future work should validate whether phantom-identified protocol-invariant features maintain stability in clinical cohorts."
- Why unresolved: The fruit phantom possesses simplified tissue properties and distinct boundaries that do not fully mimic the complexity or heterogeneity of human pathology.
- What evidence would resolve it: A validation study demonstrating that models trained on these specific invariant features maintain F1-scores >0.85 on clinical data across different MRI protocols.

### Open Question 2
- Question: Can automated domain adaptation techniques outperform the manual selection of protocol-invariant features for cross-protocol generalization?
- Basis in paper: [explicit] The authors propose to "explore automated selection via domain adaptation techniques" in future work.
- Why unresolved: This study relied on pre-defined consistent features, leaving the potential of algorithmic feature selection optimized for distribution shifts unexplored.
- What evidence would resolve it: Comparative benchmarks showing domain adaptation algorithms selecting feature subsets that achieve higher F1-scores than the 8-feature baseline under distribution shift.

### Open Question 3
- Question: Does the reliance on shape-based and first-order features limit model performance when tissue discrimination requires complex texture analysis?
- Basis in paper: [inferred] The paper notes the 8 consistent features are "predominantly shape-based and first-order statistics" and discusses that texture features often encode "acquisition-specific artifacts," implying a trade-off between robustness and potential diagnostic information content.
- Why unresolved: While robust, simple features may fail to capture subtle biological nuances necessary for specific clinical diagnoses (e.g., tumor grading) that rely on texture.
- What evidence would resolve it: Analysis of diagnostic accuracy (not just robustness) comparing invariant features vs. texture features in a clinical task where texture is known to be discriminative.

## Limitations

- The fruit phantom validation has limited clinical applicability due to simplified tissue properties compared to pathological heterogeneity
- The exact composition of the 8 protocol-invariant features is not specified beyond "shape-based and first-order statistics"
- Multi-protocol training benefits require protocol-invariant features, creating a dependency that may not transfer to fundamentally different protocols

## Confidence

- **High confidence**: Protocol-invariant feature selection outperforms comprehensive feature sets under distribution shifts
- **Medium confidence**: Multi-protocol training enhances cross-protocol generalization only with protocol-invariant features
- **Medium confidence**: Dataset augmentation provides calibration benefits without accuracy loss
- **Low confidence**: These findings generalize directly to clinical tumor imaging scenarios

## Next Checks

1. **Clinical validation**: Apply the protocol-invariant feature selection framework to a clinical dataset (e.g., brain tumor or liver lesion imaging) with known protocol variations to verify F1 >0.85 threshold maintenance.
2. **Feature stability audit**: Replicate the test-retest analysis across diverse clinical protocols to identify which specific shape and first-order features demonstrate ICC >0.85 stability.
3. **Augmentation transfer**: Test whether the segmentation-based augmentation strategies that improved calibration on phantom data provide similar benefits on clinical segmentation variability (partial volume effects, heterogeneous tumor regions).