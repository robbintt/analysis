---
ver: rpa2
title: Closed-Form Last Layer Optimization
arxiv_id: '2510.04606'
source_url: https://arxiv.org/abs/2510.04606
tags:
- batch
- last
- layer
- size
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing neural networks with a squared loss
  by leveraging the closed-form solution for the linear last layer weights. The key
  idea is to treat the last layer as a deterministic function of the backbone parameters
  and optimize solely for these parameters, effectively alternating between gradient
  descent steps on the backbone and closed-form updates on the last layer.
---

# Closed-Form Last Layer Optimization

## Quick Facts
- arXiv ID: 2510.04606
- Source URL: https://arxiv.org/abs/2510.04606
- Authors: Alexandre Galashov, Nathaël Da Costa, Liyuan Xu, Philipp Hennig, Arthur Gretton
- Reference count: 40
- Key outcome: The paper addresses optimizing neural networks with a squared loss by leveraging the closed-form solution for the linear last layer weights.

## Executive Summary
This paper proposes a novel optimization method for neural networks with squared loss by treating the last layer as a deterministic function of the backbone parameters. The approach alternates between gradient descent steps on the backbone and closed-form updates on the last layer, effectively optimizing only the backbone parameters while keeping the last layer in closed form. To adapt this to stochastic gradient descent, the method introduces regularization that balances current batch loss against accumulated information from previous batches. Theoretical analysis in the Neural Tangent Kernel regime proves convergence to an optimal solution. Empirically, the method demonstrates improvements over standard SGD on squared loss tasks, including regression, classification, and applications like Fourier Neural Operators and Instrumental Variable Regression.

## Method Summary
The core idea is to leverage the analytical solution for ridge regression to compute the optimal last layer weights given fixed backbone parameters. During training, the method alternates between: (1) taking gradient descent steps on the backbone to minimize the regularized loss, and (2) computing the closed-form solution for the last layer weights. To make this work with SGD, the authors introduce a proximal term that regularizes the last layer toward its previous solution, creating a trade-off between fitting the current batch and maintaining stability. This allows the backbone to be optimized with standard gradient-based methods while the last layer is always kept in its optimal state for the current backbone parameters.

## Key Results
- Theoretical convergence guarantees in the Neural Tangent Kernel regime, proving convergence to an optimal solution
- Improved performance over standard SGD on squared loss tasks across multiple datasets and architectures
- Successful application to complex two-stage optimization problems like Fourier Neural Operators and Deep Feature Instrumental Variable regression
- Demonstrated ability to handle large-scale problems with the proximal regularization approach

## Why This Works (Mechanism)
The method works by exploiting the fact that, for a fixed backbone, the optimal last layer weights have a closed-form solution (ridge regression). By treating the last layer as a deterministic function of the backbone parameters, the optimization problem reduces to optimizing only the backbone. The proximal regularization term ensures stability during stochastic updates by preventing large changes to the last layer between batches. This creates an implicit form of second-order optimization, as the closed-form solution incorporates curvature information about the loss landscape with respect to the last layer weights.

## Foundational Learning

**Neural Tangent Kernel (NTK)**: The theoretical framework where neural networks behave like linear models in function space during training. Why needed: Provides the convergence guarantees for the method. Quick check: Verify that network width is large enough for NTK regime.

**Ridge Regression**: The analytical solution for regularized least squares problems. Why needed: Enables the closed-form computation of optimal last layer weights. Quick check: Confirm that the regularization parameter λ is properly tuned.

**Proximal Optimization**: A technique for solving optimization problems by adding regularization terms that keep iterates close to previous solutions. Why needed: Makes the closed-form updates compatible with stochastic gradient descent. Quick check: Monitor last layer weight stability across batches.

## Architecture Onboarding

**Component map**: Backbone network -> Loss function (squared) -> Closed-form last layer solver -> Proximal regularization term

**Critical path**: Input data → Backbone forward pass → Compute predictions → Calculate loss → Gradient computation → Backbone update → Closed-form last layer update

**Design tradeoffs**: The method trades computational efficiency (closed-form last layer) for memory overhead (storing previous last layer weights) and introduces a regularization parameter that requires tuning.

**Failure signatures**: Poor performance may indicate: λ too small (unstable last layer updates), λ too large (backbone optimization dominated by regularization), or insufficient network width for NTK regime assumptions.

**First experiments**: 1) Verify closed-form solution correctness on a simple linear regression problem, 2) Compare training dynamics with and without proximal regularization, 3) Test sensitivity to λ across different dataset sizes.

## Open Questions the Paper Calls Out

**Open Question 1**: How can the closed-form last layer strategy be adapted for non-squared loss functions like cross-entropy in classification tasks?
- Basis in paper: The conclusion states, "In future work, we will focus on adapting a similar closed-form strategy to the cross entropy loss in the classification setting."
- Why unresolved: The current method relies on the analytical solution for ridge regression (squared loss), which does not exist directly for cross-entropy.
- What evidence would resolve it: A derivation of an approximate closed-form update for cross-entropy or a new regularization framework that yields similar convergence guarantees.

**Open Question 2**: Can modifying the regularization parameter λ to be dimension-dependent and adaptive improve performance when using the Adam optimizer?
- Basis in paper: The authors ask if "defining parameters λ per last layer dimension and adapt[ing] these over the course of the training" would resolve the under-performance observed with Adam (Section 7, Appendix F.1).
- Why unresolved: The current fixed λ conflicts with Adam's adaptive step-size rescaling, causing instability or suboptimal convergence.
- What evidence would resolve it: Empirical results demonstrating that an adaptive λ scheme allows the method to match or exceed SGD performance when Adam is used for the backbone.

**Open Question 3**: Does the proximal closed-form method scale effectively to complex two-stage optimization problems like offline reinforcement learning (RL)?
- Basis in paper: The conclusion proposes to "apply our proximal method to larger scale two-stage settings... such as offline reinforcement learning."
- Why unresolved: The method has only been validated on Deep Feature Instrumental Variable (DFIV) regression, and it is unclear if the proximal updates remain stable in the distinct landscape of offline RL.
- What evidence would resolve it: Benchmarks on standard offline RL datasets showing improved training efficiency or policy performance compared to standard bilevel optimization.

**Open Question 4**: Is convergence to a global minimizer guaranteed for finite-width networks outside the infinite-width Neural Tangent Kernel (NTK) regime?
- Basis in paper: Theoretical analysis (Theorem 4) is restricted to the NTK regime where the network is linearized; the paper does not provide guarantees for standard finite-width feature learning.
- Why unresolved: The NTK regime assumes features change slowly (lazy training), whereas practical networks often undergo significant feature learning.
- What evidence would resolve it: A convergence proof for finite-width architectures or empirical analysis mapping the boundary conditions where the method fails to converge.

## Limitations
- Theoretical guarantees are restricted to the Neural Tangent Kernel regime, which may not reflect practical neural network training scenarios
- Empirical validation is limited to specific architectures and tasks, with relatively small-scale experiments
- The regularization parameter requires careful tuning and its sensitivity to different datasets and architectures is not fully explored
- Effectiveness in non-convex settings beyond squared loss remains unverified

## Confidence
- Theoretical claims: High within NTK regime, Medium for practical applicability
- Empirical results: Medium, given limited scope and potential hyperparameter sensitivity

## Next Checks
1. Extend experiments to larger-scale datasets and deeper architectures to assess scalability and robustness
2. Investigate the method's performance with other loss functions and non-linear activation functions to evaluate generalizability
3. Conduct an ablation study on the regularization parameter's impact across diverse datasets and network architectures to understand its sensitivity and optimal selection strategies