---
ver: rpa2
title: Framework for Machine Evaluation of Reasoning Completeness in Large Language
  Models For Classification Tasks
arxiv_id: '2510.21884'
source_url: https://arxiv.org/abs/2510.21884
tags:
- coverage
- rationales
- alignment
- features
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces RACE, a framework for evaluating the alignment
  between LLM-generated explanations and feature importance from logistic regression
  in text classification. Using token-aware, exact, and edit-distance matching, RACE
  quantifies how well rationales reflect predictive evidence.
---

# Framework for Machine Evaluation of Reasoning Completeness in Large Language Models For Classification Tasks

## Quick Facts
- **arXiv ID**: 2510.21884
- **Source URL**: https://arxiv.org/abs/2510.21884
- **Reference count**: 5
- **Primary result**: RACE framework quantifies alignment between LLM rationales and logistic regression feature importance, revealing support/contradiction asymmetry across four text classification datasets.

## Executive Summary
This paper introduces RACE (Reasoning Alignment with Classifier Evidence), a framework for evaluating whether LLM-generated rationales for text classification predictions align with the actual predictive features identified by logistic regression. The framework extracts top-k features from logistic regression, partitions them into supporting and contradicting sets based on sign of weights, and measures coverage of these features within LLM rationales using three matching strategies: token-aware (lemmatized), exact string, and edit-distance matching. Across four datasets (WIKIONTOLOGY, AG NEWS, IMDB, GOEMOTIONS), correct predictions consistently show higher coverage of supporting features while incorrect predictions show elevated coverage of contradicting features. Edit-distance matching reveals paraphrastic overlaps, increasing coverage without eliminating the asymmetry pattern.

## Method Summary
The RACE framework trains logistic regression on TF-IDF features (1–2 grams) from text classification datasets, then extracts top-5 weighted features per instance and partitions them into supporting (positive weights) and contradicting (negative weights) subsets. An LLM (DeepSeek-R1) is prompted to output predictions and free-text rationales for each instance. Three matching strategies compute coverage: token-aware matching using lemmatized forms, exact string matching, and edit-distance matching with threshold δ. Coverage metrics are aggregated by prediction correctness (match vs mismatch), and statistical significance is assessed via bootstrap resampling. The framework assumes logistic regression feature weights reliably identify predictive lexical cues, and that LLM rationales that reuse these cues demonstrate faithfulness to the model's reasoning.

## Key Results
- Correct predictions show higher coverage of supporting features, while incorrect predictions show elevated coverage of contradicting features across all four datasets
- Edit-distance matching increases coverage by substantial margins (e.g., 0.52→0.61 in WIKIONTOLOGY) while preserving the support/contradiction asymmetry
- Task type moderates alignment strength, with topical classification (WIKIONTOLOGY, AG NEWS) showing strongest separation and fine-grained emotion recognition (GOEMOTIONS) showing weakest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM rationales for correct predictions show higher overlap with features that support the predicted class, while incorrect predictions show elevated overlap with contradicting features.
- Mechanism: The framework extracts top-k weighted features from logistic regression, partitions them into supporting and contradicting subsets, then computes coverage within LLM rationales using multiple matching strategies.
- Core assumption: Logistic regression feature weights reliably identify predictive lexical cues for each class.
- Evidence anchors:
  - [abstract] "Empirical results reveal a consistent asymmetry: correct predictions exhibit higher coverage of supporting features, while incorrect predictions are associated with elevated coverage of contradicting features."
  - [Section 5.2] "Coverage patterns reveal a consistent asymmetry between correct and incorrect predictions. When the LLM prediction matches the true label, rationales more strongly emphasize supporting features."
  - [corpus] Related work on LLM rationale evaluation (Fayyaz et al., 2024; Admoni et al., 2025) examines faithfulness through human alignment and self-consistency, but does not systematically quantify support/contradiction asymmetry—this gap is addressed by RACE.
- Break condition: If logistic regression features are poor proxies for true predictive cues (e.g., in tasks with distributed or compositional semantics), the asymmetry signal may weaken or disappear. Table 1 shows GOEMOTIONS has near-zero asymmetry, suggesting the mechanism falters on fine-grained emotion classification.

### Mechanism 2
- Claim: Edit-distance matching captures paraphrastic alignment that strict string matching misses, while preserving the support/contradiction asymmetry.
- Mechanism: Fuzzy matching with character-level edit-distance threshold allows near-synonymous or morphologically variant forms of classifier features to be detected in rationales, boosting coverage without collapsing the distinction between correct and incorrect predictions.
- Core assumption: Near-miss string matches correspond to semantically related reformulations rather than spurious noise.
- Evidence anchors:
  - [abstract] "Edit-distance matching reveals paraphrastic overlaps, increasing coverage while preserving this asymmetry."
  - [Section 5.4] "Edit-distance matching further increases coverage (Table 1), often by a substantial margin (e.g., WIKIONTOLOGY match support rises from 0.52 to 0.61)."
  - [corpus] No direct corpus evidence on edit-distance matching for rationale alignment; this appears to be a novel contribution of the framework.
- Break condition: If edit-distance thresholds are too permissive, false positives may dilute the asymmetry signal. The paper does not report sensitivity analysis on δ.

### Mechanism 3
- Claim: Task type moderates the strength of rationale–feature alignment, with topical classification showing strongest separation and fine-grained emotion recognition showing weakest.
- Mechanism: Lexical grounding of category distinctions varies by task; topical categories (news, ontology) have distinctive keywords, whereas sentiment and emotion rely on diffuse or overlapping cues that dilute per-feature importance.
- Core assumption: The logistic regression baseline captures task-relevant features at comparable quality across domains.
- Evidence anchors:
  - [Section 5.3] "In WIKIONTOLOGY and AG NEWS, separation is most pronounced... For GOEMOTIONS, the asymmetry is weakest: support and contradiction coverage overlap heavily (≈0.25–0.44)."
  - [Section 6.2] "The strength of rationale–feature alignment varies markedly across tasks."
  - [corpus] Related work on explainable sentiment classification (TeSent, LLM-MTD) suggests domain-specific challenges, but does not directly compare alignment strength across task types.
- Break condition: If the logistic regression model is underpowered for certain tasks (e.g., cannot capture compositional sentiment patterns), the framework may misattribute weak alignment to LLM unfaithfulness rather than baseline limitations.

## Foundational Learning

- Concept: **Logistic regression feature weights as interpretable importance scores**
  - Why needed here: The entire RACE framework depends on treating LR weights as ground-truth indicators of which lexical features support or contradict each class.
  - Quick check question: Given a binary classifier with weight +2.5 for "excellent" and -1.8 for "boring," which feature supports a positive sentiment prediction?

- Concept: **TF-IDF vectorization (unigrams and bigrams)**
  - Why needed here: Feature extraction uses TF-IDF with 1–2 grams; understanding this representation is essential for interpreting what "top features" actually capture.
  - Quick check question: Why might a bigram like "not good" carry different signal than its unigram components "not" and "good" separately?

- Concept: **Edit distance (Levenshtein distance) for fuzzy string matching**
  - Why needed here: One of three matching strategies relies on character-level edit distance to capture paraphrastic variants.
  - Quick check question: If edit distance threshold δ=2, would "amazing" match "amzng"? What about "amaze"?

## Architecture Onboarding

- Component map:
  1. Data Extraction Module -> Normalization Layer -> Matching Engine -> Coverage Calculator -> Aggregation Layer

- Critical path:
  1. Train logistic regression on TF-IDF features → extract top-5 features per instance → partition into support/contradict sets
  2. Prompt LLM → collect label + rationale
  3. Run all three matchers → compute coverage metrics
  4. Aggregate by correct/incorrect → visualize scatter plots and confusion matrices

- Design tradeoffs:
  - k=5 features is a strong bottleneck; may miss distributed signals but keeps computation tractable and interpretation focused
  - Logistic regression is interpretable but cannot capture higher-order compositional cues; this limits the framework's sensitivity to complex reasoning
  - Edit-distance matching improves recall but risks false positives; threshold tuning is empirical and not extensively validated

- Failure signatures:
  - Near-zero asymmetry (support and contradiction coverage overlap heavily): seen in GOEMOTIONS, indicates either task difficulty or baseline inadequacy
  - High exact-match coverage but low edit-distance gain: suggests rationales rely on verbatim keyword reuse rather than paraphrase
  - Elevated contradiction coverage even in correct predictions: may indicate LLM is hedging or the LR baseline misidentifies key features

- First 3 experiments:
  1. Replicate on a single dataset (e.g., AG NEWS): Train LR, prompt LLM for predictions + rationales, compute coverage with all three matchers. Verify asymmetry pattern matches Table 1.
  2. Vary k (top-3, top-5, top-10 features): Test whether the asymmetry signal is robust to feature count or dilutes as k increases.
  3. Sensitivity analysis on edit-distance threshold δ: Sweep δ from 1 to 5 and plot coverage vs. asymmetry strength to identify optimal threshold and assess robustness.

## Open Questions the Paper Calls Out

- **Question**: Would the support/contradiction coverage asymmetry persist across different LLM architectures and sizes, or is it specific to DeepSeek-R1?
  - Basis in paper: [explicit] The study uses only DeepSeek-R1 for all experiments, with no comparison to other LLMs.
  - Why unresolved: The framework was validated on a single model, leaving architectural generalization untested.
  - What evidence would resolve it: Applying RACE to multiple LLMs (e.g., GPT-4, LLaMA, Claude) on the same datasets and comparing asymmetry patterns.

- **Question**: How would contextual embedding-based semantic matching change coverage estimates compared to the current string-based matchers?
  - Basis in paper: [explicit] Section 6.4 states edit-distance matching "cannot fully capture semantic equivalence (e.g., antonyms or idioms)" and suggests incorporating "contextual embeddings for semantic alignment."
  - Why unresolved: The current matchers capture surface and near-surface overlap but miss deeper semantic correspondences.
  - What evidence would resolve it: Implementing embedding-based similarity thresholds (e.g., cosine similarity in BERT space) and measuring coverage changes.

- **Question**: How does varying the number of top-k features affect the detected alignment between LLM rationales and classifier evidence?
  - Basis in paper: [explicit] Section 6.4 notes that "fixing k=5 features per instance imposes a strong bottleneck that may miss diffuse or distributed signals."
  - Why unresolved: The optimal granularity for capturing both focused and distributed predictive cues remains unknown.
  - What evidence would resolve it: Systematic experiments with k ∈ {3, 5, 10, 20, 50} across datasets, analyzing stability of asymmetry patterns.

## Limitations

- Framework depends critically on logistic regression as ground-truth feature importance proxy, which may fail for tasks requiring compositional understanding
- k=5 feature bottleneck may miss distributed predictive signals, particularly in tasks where importance is spread across many features
- Edit-distance matching lacks rigorous validation and threshold sensitivity analysis, with unclear semantic validity of near-matches

## Confidence

- **High confidence**: The core finding that correct predictions show higher coverage of supporting features while incorrect predictions show elevated contradicting feature coverage (Mechanism 1). This pattern is consistently observed across multiple datasets and matching strategies, with statistical significance reported.
- **Medium confidence**: The claim that edit-distance matching captures paraphrastic alignment while preserving asymmetry (Mechanism 2). While the paper demonstrates increased coverage, the semantic validity of near-matches and the optimal threshold selection remain unverified.
- **Medium confidence**: The assertion that task type moderates alignment strength (Mechanism 3). The observed variation across datasets is compelling, but the paper does not establish whether this reflects true task differences or variations in logistic regression baseline quality.

## Next Checks

1. **Cross-validation with alternative baselines**: Replace logistic regression with more sophisticated interpretable models (e.g., linear SVMs with L1 regularization, or attention-weighted feature extractors from fine-tuned BERT) to verify whether the support/contradiction asymmetry persists when using different feature importance methods.

2. **Threshold sensitivity analysis**: Systematically sweep the edit-distance threshold δ from 1-5 and plot the relationship between coverage gains and asymmetry preservation to identify optimal thresholds and quantify robustness to parameter choice.

3. **Human evaluation of paraphrastic matches**: Conduct a small-scale human annotation study to verify that edit-distance matches identified by the framework correspond to semantically meaningful paraphrases rather than spurious character-level similarities.