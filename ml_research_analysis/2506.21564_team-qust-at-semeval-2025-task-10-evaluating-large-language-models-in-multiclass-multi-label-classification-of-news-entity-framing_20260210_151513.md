---
ver: rpa2
title: 'Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass
  Multi-label Classification of News Entity Framing'
arxiv_id: '2506.21564'
source_url: https://arxiv.org/abs/2506.21564
tags:
- arxiv
- task
- data
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes the QUST team's participation in SemEval-2025
  Task 10, subtask 1, which involves multi-label multi-class classification of news
  entity framing across five languages (English, Bulgarian, Hindi, Portuguese, Russian).
  The authors evaluate various large language models (LLMs) using instruction tuning
  (IT) and ensemble voting.
---

# Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing

## Quick Facts
- arXiv ID: 2506.21564
- Source URL: https://arxiv.org/abs/2506.21564
- Reference count: 3
- 1st place in Hindi, 2nd in Russian, 3rd in Portuguese, 6th in Bulgarian, 7th in English

## Executive Summary
This paper describes the QUST team's participation in SemEval-2025 Task 10 Subtask 1, a multi-label multi-class classification task for news entity framing across five languages. The authors evaluate various large language models using instruction tuning and ensemble voting, achieving top rankings in four of five languages. The system significantly outperforms the baseline across all languages, with performance variations attributed to training data scale. The method demonstrates the effectiveness of instruction tuning and ensemble learning for multilingual entity framing tasks.

## Method Summary
The approach uses instruction tuning (IT) on large language models followed by hard voting ensemble of top-3 models. Models are first fine-tuned on English data (10-20 epochs, learning rates 1e-5 to 1e-4), then applied to other languages. The final ensemble consists of Phi-3-small, Phi-3-medium, and Phi-4 models. Instruction template requests "one or more fine-grained roles" to support multi-label output. For final test, dev data is merged into training. Hard voting is applied to predictions from the three best-performing models.

## Key Results
- Achieved 1st place in Hindi, 2nd in Russian, 3rd in Portuguese, 6th in Bulgarian, 7th in English
- Hard voting ensemble improved performance from 0.4615 (single model) to 0.4725 (+1.1%)
- Outperformed baseline (DeBERTa-v3-small at 0.2747) across all languages
- Performance correlates with training data size (Hindi 2,331 samples vs. Bulgarian 627 samples)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction tuning (IT) enables LLMs to better understand task-specific framing requirements by aligning model parameters with structured role assignment instructions.
- Mechanism: IT updates LLM parameters in a supervised manner using instruction-input-output pairs, helping models learn to extract fine-grained entity roles (protagonist, antagonist, innocent) from news context.
- Core assumption: The instruction template quality is more critical than quantity; models can transfer instruction-following behavior across languages when fine-tuned primarily on English data.
- Evidence anchors:
  - [abstract] "We evaluate various large language models (LLMs) based on instruction tuning (IT) on subtask 1"
  - [section 2.1] "IT is a powerful technique that adjusts the input context to align with specific instructions, updating the parameters of LLMs in a supervised manner"
- Break condition: If instruction quality is poor or task complexity exceeds model capacity, IT provides marginal gains (see GLM4-9B-chat at 0.1978, underperforming even DeBERTa baseline).

### Mechanism 2
- Claim: Hard voting ensemble of top-3 models improves prediction robustness by reducing individual model biases.
- Mechanism: Three best-performing models independently predict entity roles; final prediction is determined by majority vote on class labels.
- Core assumption: Ensemble models make partially uncorrelated errors; the top-3 models have complementary strengths rather than identical failure modes.
- Evidence anchors:
  - [abstract] "a voting mechanism is utilized on the predictions of the top-3 models to derive the final submission results"
  - [section 4.2, Table 3] Voting achieves 0.4725 vs. Phi-4 single model at 0.4615 (+1.1% improvement)
- Break condition: If top-3 models make correlated errors, voting yields minimal improvement.

### Mechanism 3
- Claim: Training data scale strongly correlates with cross-lingual generalization; languages with larger training sets achieve higher ranks.
- Mechanism: Larger training corpora provide more diverse entity-role patterns, enabling better parameter estimation during fine-tuning.
- Core assumption: Label quality and annotation consistency are comparable across languages; performance differences stem primarily from sample quantity.
- Evidence anchors:
  - [section 1] "Insufficient training data has resulted in suboptimal fine-tuning outcomes"
  - [section 4.3] "This unexpected phenomenon may be related to the scale of the training data"
- Break condition: If label noise varies across languages, data scale alone cannot explain performance gaps.

## Foundational Learning

- Concept: Multi-label classification with imbalanced label distribution
  - Why needed here: The paper identifies label imbalance as a core challenge (majority single-labeled samples; some languages have 3× more training data than others).
  - Quick check question: Can you explain why treating multi-label tasks as single-label (Table 2) only dropped performance by 2.86%?

- Concept: Ensemble voting (hard vs. soft)
  - Why needed here: The system uses hard voting across top-3 models; understanding when majority vote works vs. weighted probability averaging is critical for replication.
  - Quick check question: Why might hard voting underperform if all three models are from the same model family (Phi-3-small, Phi-3-medium, Phi-4)?

- Concept: Instruction tuning vs. in-context learning
  - Why needed here: The paper explicitly chose IT over alternatives; understanding trade-offs helps justify this architectural decision.
  - Quick check question: What is the computational cost difference between IT (requires parameter updates) and few-shot in-context learning?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Label distribution analysis -> English-first fine-tuning -> Top-3 model selection -> Hard voting ensemble -> Per-language prediction

- Critical path:
  1. Analyze label distribution (Figure 1) to confirm multi-label structure
  2. Fine-tune candidate LLMs on English data first (10-20 epochs, learning rates 1e-5 to 1e-4)
  3. Evaluate on English dev set to select top-3 models
  4. Apply selected models to all five languages
  5. Aggregate predictions via hard voting

- Design tradeoffs:
  - English-first fine-tuning reduces training cost but may underweight language-specific patterns
  - Epoch selection strategy (keeping only best checkpoint) prevents overfitting but discards potentially useful model diversity for ensemble
  - Hard voting is simpler than weighted voting but ignores confidence scores

- Failure signatures:
  - GLM4-9B-chat scored 0.1978 (below baseline 0.2747) → likely poor instruction comprehension
  - English/Bulgarian underperformed relative to Hindi → insufficient training data
  - Single-label formulation dropped only 2.86% → heavy single-label bias in dataset

- First 3 experiments:
  1. Replicate English dev evaluation (Table 3) with Phi-4 and voting to validate baseline reproducibility
  2. Ablate voting: compare Phi-4 alone vs. voting ensemble to quantify ensemble contribution on held-out data
  3. Data augmentation test: augment Bulgarian (smallest dataset) with back-translation or synthetic samples to test data scale hypothesis

## Open Questions the Paper Calls Out

- Question: What data augmentation strategies can effectively improve model performance for low-resource languages (Bulgarian and English) in entity framing tasks?
  - Basis in paper: [explicit] The conclusion states: "As the training data for English and Bulgarian is relatively limited and we have not yet employed data augmentation methods, future work will explore effective strategies to augment both the quantity and quality of data."
  - Why unresolved: The authors identify data scarcity as a key limitation but did not implement or test any augmentation techniques in the current work.
  - What evidence would resolve it: Experiments comparing different augmentation methods (back-translation, synthetic data generation, cross-lingual transfer) on Bulgarian and English datasets with performance metrics.

- Question: Is the correlation between training data size and model performance causal, or do other factors (language complexity, pre-training quality) contribute significantly?
  - Basis in paper: [inferred] The authors attribute performance variations across languages to training data size, noting Hindi (2,331 samples) ranked 1st while Bulgarian (627 samples) ranked 6th, but do not control for confounding variables.
  - Why unresolved: No ablation study or controlled experiment was conducted to isolate the effect of training data size from other language-specific factors.
  - What evidence would resolve it: Controlled experiments varying training data size while holding other factors constant, or regression analysis incorporating multiple variables.

- Question: Would language-specific instruction tuning outperform the current efficiency-focused approach of fine-tuning on English first, then applying to other languages?
  - Basis in paper: [inferred] The authors state they "first fine-tuning the models in English, then apply it to other languages to minimize fine-tuning and evaluation time" due to training costs, but do not compare this to per-language tuning.
  - Why unresolved: The trade-off between computational efficiency and potential performance gains from language-specific tuning was not empirically evaluated.
  - What evidence would resolve it: Comparative experiments showing performance differences between English-first transfer and native language instruction tuning across all five languages.

## Limitations

- Data scale hypothesis not experimentally verified - no augmentation tests on low-resource languages
- Cross-lingual transfer capability assumed but not directly tested against language-specific tuning
- Ensemble effectiveness not analyzed for complementary vs. correlated model errors
- Training data size may not be the sole factor in performance variation (pre-training quality, language complexity)

## Confidence

- High confidence: Instruction tuning improves multi-label framing classification compared to baseline
- Medium confidence: Ensemble voting of top-3 models improves robustness
- Low confidence: Training data scale is the primary driver of cross-lingual performance variation

## Next Checks

1. Ablation study on ensemble composition: Test whether voting among three Phi-series models provides more benefit than any single model, or if the improvement comes from including diverse model families.

2. Data augmentation experiment: Augment Bulgarian (lowest-resource language, 6th place) with back-translation or synthetic samples, then re-fine-tune and measure performance change to directly test the data scale hypothesis.

3. Cross-lingual transfer validation: Fine-tune Phi-4 separately on Bulgarian training data vs. using English-first transfer, then compare performance to isolate whether transfer learning or language-specific training drives results.