---
ver: rpa2
title: 'Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable
  Zero-Shot Voice Conversion'
arxiv_id: '2505.24291'
source_url: https://arxiv.org/abs/2505.24291
tags:
- prosody
- speech
- voice
- conversion
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Discl-VC, a controllable zero-shot voice conversion
  system that disentangles speech into content, prosody, and timbre using self-supervised
  representations and discretization. It introduces a flow matching transformer with
  in-context learning for timbre modeling and a non-autoregressive prosody mask transformer
  for prosody control.
---

# Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion

## Quick Facts
- **arXiv ID**: 2505.24291
- **Source URL**: https://arxiv.org/abs/2505.24291
- **Reference count**: 0
- **Primary result**: Proposes Discl-VC, a controllable zero-shot voice conversion system with strong quantitative results and fewer parameters (131M vs 922M) than baselines.

## Executive Summary
This paper introduces Discl-VC, a controllable zero-shot voice conversion system that disentangles speech into content, prosody, and timbre using self-supervised representations and discretization. The system employs a flow matching transformer with in-context learning for timbre modeling and a non-autoregressive prosody mask transformer for prosody control. Experimental results demonstrate that Discl-VC outperforms existing baselines in naturalness, prosody similarity, and speaker similarity while using significantly fewer parameters.

## Method Summary
Discl-VC disentangles speech into discrete tokens representing content, prosody, and timbre. Content is extracted using HuBERT large (layer 24) features processed through K-means clustering (K=1024). Prosody is modeled using a VQ Prosody Encoder and Duration Predictor, with F0 extraction via RMVPE. Timbre is represented by flow matching tokens generated by a Flow Matching Transformer (FMT). A Prosody Mask Transformer enables controllable prosody editing. The system is trained in two stages: first, jointly training the VQ encoder, duration predictor, and FMT; second, training the prosody mask transformer to predict masked prosody tokens conditioned on content.

## Key Results
- Discl-VC achieves higher naturalness (N-MOS 4.31 vs 4.26) compared to FAcodec.
- Superior prosody similarity (P-MOS 4.079 vs 3.97) and speaker similarity (SECS 0.929 vs 0.930).
- Uses significantly fewer parameters (131M vs 922M) than baseline systems.
- Ablation studies confirm the effectiveness of each component in the proposed architecture.

## Why This Works (Mechanism)
The system's effectiveness stems from its disentanglement of speech into discrete, controllable tokens. By using self-supervised representations (HuBERT) for content and specialized encoders for prosody and timbre, Discl-VC achieves better control over each speech attribute. The flow matching transformer enables in-context learning for timbre, allowing flexible voice conversion without speaker-specific training. The prosody mask transformer provides fine-grained control over speech rhythm and intonation.

## Foundational Learning
- **HuBERT features**: Self-supervised speech representations used for content extraction. *Why needed*: Provides rich linguistic content without explicit labeling. *Quick check*: Verify HuBERT layer 24 features capture phonetic content.
- **K-means clustering**: Discretizes continuous HuBERT features into content tokens. *Why needed*: Enables discrete representation for downstream processing. *Quick check*: Monitor cluster usage to prevent collapse.
- **Flow matching**: Generative modeling technique for timbre representation. *Why needed*: Enables flexible timbre transfer without speaker-specific training. *Quick check*: Verify flow matching produces diverse timbre tokens.
- **VQ encoding**: Vector quantization for prosody representation. *Why needed*: Discretizes continuous prosody features into controllable tokens. *Quick check*: Ensure prosody codebook maintains sufficient diversity.
- **Prosody masking**: Conditional generation task for prosody control. *Why needed*: Enables user control over speech rhythm and intonation. *Quick check*: Validate prosody mask transformer predicts ground truth tokens accurately.
- **In-context learning**: Transformer-based generation conditioned on context. *Why needed*: Enables timbre transfer without fine-tuning. *Quick check*: Test timbre conversion with varying context lengths.

## Architecture Onboarding

### Component Map
HuBERT Content Extraction -> VQ Prosody Encoder -> Duration Predictor -> Flow Matching Transformer -> BigVGAN Vocoder
                                                                      -> Prosody Mask Transformer

### Critical Path
The critical path for voice conversion flows from content extraction through prosody modeling to timbre generation, ending with waveform synthesis via BigVGAN. The prosody mask transformer provides controllable modification of the prosody stream.

### Design Tradeoffs
- Discretization vs. continuous representations: Enables controllability but may lose fine-grained information
- Two-stage training: Simplifies optimization but requires careful coordination between stages
- Lightweight architecture: Reduces computational cost but may limit expressiveness compared to larger models

### Failure Signatures
- Codebook collapse in VQ Prosody Encoder: Indicates poor prosody representation
- High WER: Suggests content information loss during discretization
- Low SECS: Indicates poor speaker similarity transfer
- Inconsistent prosody control: Suggests prosody mask transformer training issues

### First Experiments
1. Train VQ Prosody Encoder and Duration Predictor to verify prosody discretization works correctly
2. Train Flow Matching Transformer to generate timbre tokens conditioned on content and prosody
3. Test Prosody Mask Transformer on synthetic data with known prosody patterns

## Open Questions the Paper Calls Out
### Open Question 1
Does scaling model parameters and training data yield proportional performance gains or emergent capabilities for Discl-VC? The current work validates a lightweight model (131M params) on 6k hours, but the scaling efficiency remains untested.

### Open Question 2
How effectively does the proposed disentanglement framework generalize to cross-lingual or multilingual voice conversion scenarios? The training and evaluation datasets are exclusively English.

### Open Question 3
Can the system's content preservation (WER) be optimized to match or exceed the FAcodec baseline without sacrificing speaker similarity? Discl-VC achieves a higher WER (1.946) than FAcodec (1.341) despite superior speaker similarity.

## Limitations
- Critical architectural details (Conv Stacks in VQ Prosody Encoder, Inverse Length Regulator) are underspecified
- Baseline architecture not provided, making parameter reduction claims difficult to verify
- Limited evaluation to English-only datasets raises questions about cross-lingual generalization

## Confidence
- N-MOS, P-MOS, SECS improvements: Medium (metrics are standard, but results depend on precise implementation)
- Parameter reduction claim: Low (baseline architecture not specified)
- Ablation study conclusions: Medium (logical, but dependent on faithful implementation of each component)

## Next Checks
1. Implement and test the VQ Prosody Encoder with multiple Conv Stack configurations to identify the configuration matching the paper's performance.
2. Validate the Inverse Length Regulator by ensuring prosody token duration matches content token duration post-regulation, using synthetic data with known alignments.
3. Reproduce the Stage 1 training with the SimVQ loss, monitoring codebook usage to detect and prevent collapse.