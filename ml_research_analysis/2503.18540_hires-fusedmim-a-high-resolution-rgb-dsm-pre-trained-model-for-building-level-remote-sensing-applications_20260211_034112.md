---
ver: rpa2
title: 'HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level
  Remote Sensing Applications'
arxiv_id: '2503.18540'
source_url: https://arxiv.org/abs/2503.18540
tags:
- data
- hires-fusedmim
- remote
- sensing
- building
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HiRes-FusedMIM, a novel pre-trained model
  designed to leverage high-resolution RGB and DSM data for building-level remote
  sensing applications. The model uses a dual-encoder SimMIM architecture with a multi-objective
  loss combining reconstruction and contrastive objectives to learn powerful joint
  representations.
---

# HiRes-FusedMIM: A High-Resolution RGB-DSM Pre-trained Model for Building-Level Remote Sensing Applications

## Quick Facts
- arXiv ID: 2503.18540
- Source URL: https://arxiv.org/abs/2503.18540
- Authors: Guneet Mutreja; Philipp Schuegraf; Ksenia Bittner
- Reference count: 40
- Key outcome: Dual-encoder pre-trained model with 60% masking and contrastive loss outperforms state-of-the-art on building-level segmentation, classification, and instance segmentation tasks

## Executive Summary
HiRes-FusedMIM introduces a novel dual-encoder pre-trained model that leverages high-resolution RGB and DSM data for building-level remote sensing applications. The model uses a SimMIM architecture with separate Swin Transformers for each modality, combined with a multi-objective loss that includes both reconstruction and contrastive objectives. Pre-trained on over 368k paired RGB-DSM images at 0.2-0.5 m resolution, the model demonstrates consistent performance improvements across diverse downstream tasks including classification, semantic segmentation, and instance segmentation. The dual-encoder architecture with modality-specific processing and cross-modal alignment through contrastive learning enables powerful joint representations that outperform previous state-of-the-art methods.

## Method Summary
HiRes-FusedMIM employs a dual-encoder SimMIM architecture where separate Swin-Base Transformers process RGB (3-channel) and DSM (1-channel) inputs independently. During pre-training, 60% of patches are randomly masked in both modalities, and the model learns to reconstruct them using lightweight decoders while simultaneously aligning the two representations through InfoNCE contrastive loss. The final-layer features from both encoders are concatenated (2048-dim) before being passed to the decoders. The total loss combines L1 reconstruction loss (95% weight) with contrastive alignment loss (5% weight). The model is pre-trained for 400 epochs on 368k paired RGB-DSM images using AdamW optimizer with learning rate 5e-4 and cosine schedule. Downstream tasks use UPerNet for segmentation and Mask R-CNN for instance segmentation with the pre-trained encoders as initialization.

## Key Results
- Dual-encoder architecture outperforms single-encoder models on Vaihingen segmentation task by significant margin
- Contrastive loss improves performance across all tested datasets (Vaihingen RGB+DSM: 73.84→74.4 mIoU, GeoNRW RGB+DSM: 61.13→61.68 mIoU, UBCv2 AP: 16.9→17.7)
- Consistent improvements when incorporating DSMs during pre-training compared to RGB-only baselines
- Outperforms previous state-of-the-art geospatial methods on building-related datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-encoder architecture with separate Swin Transformers for RGB and DSM enables modality-specialized representations that outperform single-encoder fusion.
- Mechanism: Each encoder learns features tailored to its input type—spectral patterns for RGB, elevation structures for DSM. Final-layer feature concatenation (2048-dim) enables cross-modal interaction without forcing early fusion that may blur modality-specific signals.
- Core assumption: RGB and DSM contain sufficiently distinct structural patterns that benefit from specialized processing before fusion.
- Evidence anchors:
  - [abstract] "The dual-encoder architecture of HiRes-FusedMIM, with separate encoders for RGB and DSM data, significantly outperforms a single-encoder model on the Vaihingen segmentation task"
  - [section 3.2] "Each encoder branch processes its respective modality—the RGB encoder operates on 3-channel RGB images, while the DSM encoder takes a single-channel DSM. Both encoders are implemented using a Swin transformer architecture"
  - [corpus] Weak direct corpus support; neighbor papers focus on segmentation architectures but not dual-encoder multi-modal designs.
- Break condition: If RGB and DSM inputs are poorly aligned spatially, or if one modality is consistently noisy/missing, specialized encoders may learn irrelevant features.

### Mechanism 2
- Claim: Contrastive loss (InfoNCE) between encoder outputs explicitly aligns RGB-DSM representations, improving multi-modal fusion during downstream tasks.
- Mechanism: Positive pairs (same-location RGB-DSM embeddings) are pulled together while negative pairs (different locations) are pushed apart. This encourages the model to learn corresponding spatial semantics across modalities.
- Core assumption: Paired RGB-DSM samples from the same geographic location contain corresponding semantic information that should map to similar embedding space.
- Evidence anchors:
  - [section 3.2] "To further encourage the alignment of RGB and DSM representations, we introduce a contrastive loss between the output embeddings of the two encoders. We utilize the InfoNCE loss"
  - [table 8] Shows consistent improvement with contrastive loss across Vaihingen RGB+DSM (73.84→74.4), GeoNRW RGB+DSM (61.13→61.68), and UBCv2 AP (16.9→17.7)
  - [corpus] No direct corpus validation of contrastive loss for RGB-DSM fusion specifically.
- Break condition: If paired RGB-DSM data has systematic misalignment or temporal mismatch, contrastive objective may enforce spurious correlations.

### Mechanism 3
- Claim: High masking ratio (60%) combined with L1 reconstruction loss forces learning of structural and contextual relationships rather than low-level interpolation.
- Mechanism: Large masked regions prevent simple local interpolation; the model must learn semantic understanding of building structures, roof shapes, and elevation patterns to reconstruct accurately.
- Core assumption: The reconstruction task requires understanding of spatially coherent structures (buildings, terrain) rather than pixel-level patterns.
- Evidence anchors:
  - [section 3.2] "During pre-training, we randomly masked 60% of the input patches in each training iteration, a strategy consistent with findings that suggest larger masking ratios can lead to improved performance"
  - [figure 2] Visualizes reconstruction quality showing accurate recovery of building edges and elevation variations
  - [corpus] Weak corpus support; neighbor papers don't directly address masking ratios for multi-modal pre-training.
- Break condition: If pre-training data lacks structural diversity (repetitive urban patterns), model may overfit to dataset-specific reconstruction shortcuts.

## Foundational Learning

- Concept: **Masked Image Modeling (SimMIM/MAE)**
  - Why needed here: Core pre-training paradigm. Must understand that masking patches and training to reconstruct them forces representation learning without labels.
  - Quick check question: Can you explain why 60% masking is preferable to 30% for learning structural representations?

- Concept: **Contrastive Learning (InfoNCE Loss)**
  - Why needed here: Secondary pre-training objective for cross-modal alignment. Understanding positive/negative pairs and temperature parameter is essential.
  - Quick check question: What happens to representation alignment if negative samples are too similar to positive samples?

- Concept: **Swin Transformer Hierarchical Attention**
  - Why needed here: Backbone architecture for both encoders. Shifted window attention captures both local and global context critical for high-resolution imagery.
  - Quick check question: Why would a hierarchical transformer be preferred over a standard ViT for 0.2m resolution remote sensing data?

## Architecture Onboarding

- Component map:
  - Input: Paired RGB (224×224×3) and DSM (224×224×1) images with 60% patches masked
  - RGB Encoder: Swin-Base Transformer → 1024-dim features
  - DSM Encoder: Swin-Base Transformer → 1024-dim features
  - Feature Concatenation: Channel-wise concat → 2048-dim joint representation
  - RGB Decoder: Lightweight network for masked patch reconstruction
  - DSM Decoder: Lightweight network for masked patch reconstruction
  - Pre-training outputs: Reconstruction via decoders + contrastive alignment
  - Downstream: UPerNet decoder for segmentation; Mask R-CNN for instance segmentation

- Critical path:
  1. Input: Paired RGB (224×224×3) and DSM (224×224×1) images with 60% patches masked
  2. Encoding: Separate Swin Transformers process each modality
  3. Fusion: Concatenate final-layer features (2048-dim)
  4. Pre-training outputs: Reconstruction via decoders + contrastive alignment
  5. Downstream: Initialize encoder(s) from pre-trained weights, add task-specific head

- Design tradeoffs:
  - Dual-encoder vs. single-encoder: Dual provides modality specialization but doubles encoder parameters and requires paired training data
  - Loss weighting (α=0.05): Prioritizes reconstruction (95%) over contrastive alignment (5%); may need adjustment if alignment is critical
  - Resolution: Pre-trained at 224×224; downstream tasks use 512×512—requires positional embedding interpolation

- Failure signatures:
  - Poor reconstruction of building edges → insufficient pre-training epochs or inadequate masking ratio
  - DSM encoder provides no improvement → check DSM normalization per-city; may have inconsistent height scales
  - Contrastive loss diverges → temperature parameter τ may need tuning; verify positive pair construction
  - Downstream fine-tuning underperforms → learning rate too high for pre-trained weights; try gradual unfreezing

- First 3 experiments:
  1. **Reconstruction sanity check**: Run pre-trained model on held-out RGB-DSM pairs; visualize reconstructions (as in Figure 2) to verify both modalities are being learned—not just copying local patterns.
  2. **Ablate contrastive loss**: Compare full model vs. MIM-only variant on Vaihingen RGB+DSM segmentation; expect 0.5-1.0 mIoU gap per Table 8.
  3. **Single vs. dual encoder comparison**: On a dataset with DSM (GeoNRW or Vaihingen), compare initializing from dual-encoder weights vs. RGB-only encoder + random DSM encoder; measure the contribution of pre-trained DSM representations.

## Open Questions the Paper Calls Out

- Can the learned embeddings from HiRes-FusedMIM be effectively utilized within a pipeline for automated Level of Detail 2 (LoD2) 3D building reconstruction?
  - Basis in paper: [explicit] The authors explicitly state they "aim to explore the potential of using the strong embeddings from the model within a 3D building reconstruction pipeline" in the conclusion.
  - Why unresolved: The current study evaluates the model exclusively on 2D tasks (segmentation and classification), leaving the transferability of its joint representations to 3D geometric reconstruction unproven.
  - What evidence would resolve it: Quantitative benchmarks (e.g., geometry accuracy, roof plane correctness) of LoD2 models generated using HiRes-FusedMIM features compared to existing reconstruction baselines.

- To what extent does pre-training on high-resolution RGB-DSM data transfer to regression tasks such as building height prediction and DSM-to-DTM generation?
  - Basis in paper: [explicit] The conclusion lists "DSM to DTM generation and building height prediction" as specific areas for planned future investigation.
  - Why unresolved: The model was trained and tested using classification and segmentation objectives (cross-entropy loss), whereas height prediction requires regression capabilities not demonstrated in the paper.
  - What evidence would resolve it: Evaluation of model performance on regression metrics (RMSE, MAE) for height estimation tasks using the pre-trained encoder features.

- What specific fusion techniques and fine-tuning strategies optimize the performance of the dual-encoder architecture for tasks involving detailed elevation information?
  - Basis in paper: [explicit] The authors note that "Further investigation is needed to optimize the fusion techniques and fine-tuning strategies for these tasks" in the final paragraph.
  - Why unresolved: The current implementation relies on a fixed channel concatenation strategy and specific hyperparameters; the sensitivity of the model to alternative fusion mechanisms (e.g., cross-attention) remains unexplored.
  - What evidence would resolve it: Ablation studies comparing various feature fusion methods and fine-tuning protocols on standard DSM-inclusive benchmarks.

## Limitations

- The 368k-image pre-training dataset is not publicly available, limiting reproducibility and validation on the exact training distribution
- Minimal architectural details provided for the lightweight decoders, which could significantly impact reconstruction quality
- Dual-encoder architecture doubles computational requirements, potentially limiting scalability to larger datasets or higher resolutions

## Confidence

- **High confidence** in dual-encoder architecture superiority: Demonstrated consistently across multiple segmentation benchmarks with clear ablation showing single-encoder underperformance
- **Medium confidence** in contrastive loss contribution: Improvement shown (0.5-1.0 mIoU), but ablation studies only cover three datasets and may not generalize to all building types
- **Low confidence** in 60% masking ratio optimality: While supported by general SimMIM literature, no direct ablation on masking ratio for multi-modal RGB-DSM pre-training is provided

## Next Checks

1. Reconstruct held-out paired RGB-DSM samples to verify both modalities are learned and not simply copied from local neighborhoods
2. Ablate contrastive loss on a new building segmentation dataset to verify the 0.5-1.0 mIoU improvement generalizes beyond the three tested datasets
3. Compare downstream performance when initializing from dual-encoder weights versus RGB-only encoder + random DSM encoder on datasets with varying DSM quality (e.g., low vs. high elevation variation)