---
ver: rpa2
title: A Pipeline to Assess Merging Methods via Behavior and Internals
arxiv_id: '2509.19476'
source_url: https://arxiv.org/abs/2509.19476
tags:
- merging
- methods
- language
- more
- parent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel evaluation pipeline to comprehensively
  assess model merging methods by examining both behavioral performance and internal
  linguistic representations. The pipeline merges parent language models using various
  techniques (Linear, SLERP, Task Arithmetic, TIES, DARE TIES) and evaluates the resulting
  models through behavioral benchmarks (MMLU-PRO, MATH-HARD, etc.) and internal linguistic
  competence probing (Holmes benchmark).
---

# A Pipeline to Assess Merging Methods via Behavior and Internals

## Quick Facts
- arXiv ID: 2509.19476
- Source URL: https://arxiv.org/abs/2509.19476
- Authors: Yutaro Sigrist; Andreas Waldis
- Reference count: 12
- Primary result: Model merging can boost internal linguistic competence (morphology/syntax) beyond parent models, with simpler methods (SLERP, Linear) generally outperforming complex approaches.

## Executive Summary
This paper introduces a comprehensive evaluation pipeline for model merging methods that examines both behavioral performance and internal linguistic representations. The authors merge Qwen2.5 instruction-tuned models with math and code-adapted variants using five different techniques (Linear, SLERP, Task Arithmetic, TIES, DARE TIES) and evaluate the results through behavioral benchmarks and internal linguistic competence probing. The key finding is a divergence between behavioral and internal evaluations: while merged models typically perform between parent models behaviorally, their encoded linguistic competence—particularly in morphology and syntax—often surpasses the parents. Additionally, simpler merging methods generally outperform more complex ones in both evaluations, highlighting that internal representations provide complementary insights to behavioral metrics.

## Method Summary
The pipeline merges parent language models using five different techniques (Linear, SLERP, Task Arithmetic, TIES, DARE TIES) and evaluates the resulting models through behavioral benchmarks (MMLU-PRO, MATH-HARD, etc.) and internal linguistic competence probing (Holmes benchmark). The process involves: (1) merging Qwen2.5-7B family models (Instruct, Math-adapted, and Coder-adapted variants) using MergeKit, (2) evaluating parents and merged models with LM-Harness for behavioral metrics, (3) evaluating with Flash-Holmes probing on the last layer for internal linguistic competence, and (4) correlating behavioral and internal results using Pearson correlation.

## Key Results
- Merged models typically perform between parent models behaviorally but often surpass them in internal linguistic competence (morphology and syntax)
- Simpler merging methods (SLERP, Linear) generally outperform more complex methods (TIES, DARE TIES) in both behavioral and internal evaluations
- Weak correlation (0.14-0.33) between behavioral performance and internal linguistic competence scores
- Language similarity between domains affects merging compatibility, with Math instruction text being more similar to general instruction tuning than Code

## Why This Works (Mechanism)

### Mechanism 1: Geometric Preservation in Weight Space
Simpler merging methods (Linear, SLERP) preserve functional coherence and internal linguistic structures better than complex interference-reduction methods (TIES, DARE TIES). SLERP interpolates weights along a spherical path, finding the "shortest path on the hypersphere," which respects geometric relationships between parameters better than methods that aggressively prune or reset parameters.

### Mechanism 2: Complementary Information Integration
Merging combines distinct "structural knowledge" from parents, causing internal representations (specifically morphology and syntax) to surpass the capabilities of either parent. Parent models encode complementary linguistic features, and merging effectively ensembles these internal representations, allowing linear classifiers to access a richer signal than available in either parent alone.

### Mechanism 3: Domain Similarity Filtering
The efficacy of merging is modulated by the linguistic distance between parent models' training domains. Merging Instruct + Math yields higher internal competence gains than Instruct + Coder because Math natural language is structurally closer to instruction natural language than Code is.

## Foundational Learning

- **Concept: Probing Classifiers**
  - Why needed: The paper relies on the Holmes benchmark, which uses linear classifiers trained on model internals to measure "linguistic competence"
  - Quick check: If a linear probe detects syntax in a model's embeddings, does that prove the model generates grammatically correct sentences? (Answer: No, only that the information is linearly separable in the representation)

- **Concept: Spherical Linear Interpolation (SLERP)**
  - Why needed: SLERP is identified as a top-performing method. Unlike linear averaging, it preserves rotational relationships in high-dimensional space
  - Quick check: Why might linear averaging distort the features of two high-dimensional vectors compared to SLERP? (Answer: Linear averaging changes the magnitude and can "flatten" the angular differences that encode semantic features)

- **Concept: Task Vectors**
  - Why needed: To understand methods like Task Arithmetic, TIES, and DARE, which operate by calculating the vector difference (fine-tuned weights minus base weights) and merging these vectors
  - Quick check: In Task Arithmetic, what represents the "skill" of the fine-tuned model? (Answer: The task vector, defined as θ_finetuned - θ_base)

## Architecture Onboarding

- **Component map:** Parents (Qwen2.5-Instruct + Math/Coder) -> MergeKit (Linear, SLERP, Task Arithmetic, TIES, DARE TIES) -> Evaluation (LM-Harness for behavioral, Flash-Holmes for internal) -> Analysis (correlation of behavioral and internal results)

- **Critical path:** The configuration of the merging weights (e.g., weight density in DARE or interpolation coefficient in SLERP) dictates the trade-off between preserving behavioral skills and enhancing internal structure

- **Design tradeoffs:** Simple vs. Complex (Complex methods aim to fix "interference" but often score worse than both parents; Simple methods are safer bets for general competence); Behavior vs. Internals (Optimizing for MMLU does not guarantee robust internal syntax)

- **Failure signatures:** Catastrophic Drop (TIES/DARE TIES models performing strictly worse than both parents on reasoning tasks); False Positive Internals (High Holmes scores with low Harness scores indicates the model "knows" linguistics but fails to "use" it)

- **First 3 experiments:**
  1. Sanity Check: Replicate the Instruct + Math merge using SLERP vs. Linear to verify that the merged model's morphology score exceeds the max(parent_1, parent_2)
  2. Layer Analysis: Run Holmes probes on layers other than the last layer to see if the syntactic boost is localized or distributed
  3. Interference Test: Force a merge between highly dissimilar domains (e.g., Instruct + Medical/Code) using TIES vs. SLERP to validate the "domain similarity" hypothesis

## Open Questions the Paper Calls Out

- How does model merging affect the flow and encoding of information across all transformer layers, rather than just the final layer?
- Do the divergent trends between behavioral performance and internal competence generalize to other model families and architectures?
- Why do simpler merging methods consistently preserve internal linguistic representations better than complex interference-mitigation methods?
- How does model merging impact internal representations in multilingual settings?

## Limitations

- The study only experimented with models from one family (QWEN-2.5), limiting generalizability to other architectures
- Internal evaluation using Flash-Holmes probes only examines the last layer, potentially missing important linguistic competence distributed across earlier layers
- The weak correlation between behavioral and internal evaluations suggests either that internal competence doesn't translate to behavioral capability, or that the probes may not be measuring task-relevant features

## Confidence

- **High Confidence**: Simpler merging methods (SLERP, Linear) generally outperform more complex methods (TIES, DARE TIES) in both behavioral and internal evaluations
- **Medium Confidence**: Merged models often surpass parent models in internal linguistic competence (morphology and syntax), representing extractable information rather than necessarily improved generation capabilities
- **Low Confidence**: The specific mechanism explaining why geometric preservation in weight space leads to better performance is hypothesized but not conclusively proven

## Next Checks

1. Extend the Flash-Holmes evaluation to probe multiple layers (not just the last layer) to determine whether the observed internal competence gains are localized or distributed throughout the network architecture

2. Perform controlled experiments merging highly dissimilar domains (e.g., medical text with code, or legal documents with creative writing) to rigorously test the domain similarity hypothesis and identify when simple merging methods break down

3. Repeat the merging pipeline with different base model families (not just Qwen2.5) and architectures (not just 7B models) to determine whether the observed patterns hold across the broader landscape of language models