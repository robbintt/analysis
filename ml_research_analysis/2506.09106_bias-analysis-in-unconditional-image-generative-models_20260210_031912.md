---
ver: rpa2
title: Bias Analysis in Unconditional Image Generative Models
arxiv_id: '2506.09106'
source_url: https://arxiv.org/abs/2506.09106
tags:
- prob
- bias
- each
- train
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes bias shifts in unconditional image generative
  models by comparing attribute frequencies between training and generated data. The
  authors define bias shift as the difference in attribute presence probability between
  these distributions and evaluate it using a classifier-based framework on CelebA
  and DeepFashion datasets.
---

# Bias Analysis in Unconditional Image Generative Models

## Quick Facts
- **arXiv ID:** 2506.09106
- **Source URL:** https://arxiv.org/abs/2506.09106
- **Reference count:** 40
- **Primary result:** Bias shifts between training and generated data are generally small, with spectrum-based attributes showing larger shifts than non-spectrum ones; model capacity and classifier boundary density strongly influence shift magnitude.

## Executive Summary
This study analyzes bias shifts in unconditional image generative models by comparing attribute frequencies between training and generated data. The authors define bias shift as the difference in attribute presence probability between these distributions and evaluate it using a classifier-based framework on CelebA and DeepFashion datasets. Experiments with diffusion models and BigGAN reveal that bias shifts are generally small, with spectrum-based attributes (e.g., smiling, young) showing larger shifts than non-spectrum-based ones (e.g., bangs, eyeglasses). BigGAN and smaller diffusion models exhibit greater bias shifts despite similar image generation quality. Classifier decision boundary density strongly predicts shift sensitivity, with boundaries in low-density regions showing smaller shifts. The study emphasizes the need for careful evaluation frameworks and highlights the socially complex nature of attributes in bias analysis.

## Method Summary
The authors quantify bias shift by training unconditional generative models (diffusion models and BigGAN) on CelebA and DeepFashion datasets, then comparing attribute frequencies between training/validation data and generated samples. They use a classifier-based framework where a ResNeXt-50 or Swin Transformer classifier is trained on the training set with ground-truth labels, then applied to all three datasets. The primary metric is Average attribute Bias Shift (ABS), computed as the average absolute difference in positive-class probabilities across all attributes. The evaluation uses 10,000 generated images per checkpoint, with diffusion models trained for 600K steps and evaluated using FID, KID, and FLD metrics alongside ABS.

## Key Results
- Bias shifts are generally small across attributes, with spectrum-based attributes (e.g., "Smiling," "Young") showing 3-5× larger shifts than non-spectrum attributes (e.g., "Eyeglasses," "Bangs")
- Larger diffusion models produce smaller bias shifts than smaller diffusion models or BigGAN, even at comparable image quality (FLD)
- Classifier decision boundary density strongly predicts shift sensitivity, with boundaries in high-density regions showing larger shifts
- BigGAN exhibits considerably larger bias shifts than large diffusion models despite similar generation quality metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attributes whose classifier decision boundaries fall in high-density regions of the logit distribution exhibit larger bias shifts when the generation distribution deviates from training.
- Mechanism: When the decision boundary (logit = 0) cuts through a region where many samples cluster, even small translation shifts in the generated distribution cause a larger proportion of samples to cross the boundary, changing their predicted labels. Mathematically, B_shift(C) = |∫[t to t-δ] f_C(x)dx|, so shift magnitude scales with boundary-adjacent density.
- Core assumption: Distribution shifts between training and generation are approximately translational with bounded earth mover's distance (empirically observed as low EMD).
- Evidence anchors:
  - [abstract] "attribute shifts are sensitive to the attribute classifier used...particularly when its decision boundaries fall in high-density regions"
  - [section 4.4] "The main difference between small attribute bias shift and large attribute bias shift attributes is the density at the decision boundary"
  - [corpus] Weak direct support; neighbor papers focus on conditional T2I bias mitigation rather than unconditional boundary density effects.
- Break condition: If generation distribution shifts involve large mode reweighting (high EMD) rather than small translations, this density-sensitivity relationship may not hold.

### Mechanism 2
- Claim: Larger diffusion models produce smaller attribute bias shifts than smaller diffusion models or GANs, even when image quality metrics (FID, FLD) are comparable.
- Mechanism: Larger model capacity enables better approximation of the full training distribution, reducing mode dropout and preserving attribute proportions. GANs may suffer from mode collapse, disproportionately underrepresenting certain attribute combinations.
- Core assumption: Bias shift primarily reflects generator's ability to faithfully reproduce the training distribution's mode structure, not classification artifacts.
- Evidence anchors:
  - [section 4.5] "BigGAN exhibits a considerably larger ABS compared to the large diffusion model, despite having only slightly worse image generation performance according to FLD"
  - [section 4.5] "The small diffusion model exhibits larger bias shifts compared to the large diffusion model"
  - [corpus] Assumption: Neighbor papers don't directly compare unconditional model capacity effects on bias shift.
- Break condition: If quality metrics poorly correlate with distribution fidelity for specific domains, model size comparisons may not generalize.

### Mechanism 3
- Claim: Spectrum-based attributes (continuous/subjective traits like "Smiling," "Young") systematically exhibit larger bias shifts than non-spectrum (binary/crisp) attributes like "Eyeglasses."
- Mechanism: Spectrum-based attributes inherently have ambiguous boundaries where reasonable annotators disagree. Classifier decision boundaries for such attributes naturally fall in high-density regions because the underlying visual signal is continuous, making predicted labels sensitive to small distribution perturbations.
- Core assumption: The binary labels in datasets like CelebA imperfectly capture spectrum-based attributes, and classifiers learn boundaries that reflect this ambiguity.
- Evidence anchors:
  - [abstract] "classifier sensitivity is often observed in attributes values that lie on a spectrum, as opposed to exhibiting a binary nature"
  - [section 4.4, Table 1] Attributes categorized as spectrum-based (e.g., Smiling, Young, Attractive) show ABS minima of 3.25% (CelebA) vs. 0.71% for non-spectrum
  - [corpus] No direct corpus support; this spectrum/non-spectrum distinction appears novel to this work.
- Break condition: If attributes are re-labeled with probabilistic/soft annotations rather than binary labels, this categorical distinction may dissolve.

## Foundational Learning

- **Concept: Bias Shift vs. Absolute Bias**
  - Why needed here: The paper studies *shift* (difference between training and generation frequencies), not whether the training data itself is biased relative to some ideal reference. Confusing these leads to misinterpretation.
  - Quick check question: If P_train(attr) = 0.6 and P_gen(attr) = 0.62, what is the bias shift? (Answer: 2 percentage points, regardless of whether 0.6 is "fair.")

- **Concept: Pre-Sigmoid Logit Distributions**
  - Why needed here: The paper's key insight depends on visualizing where classifier decision boundaries fall relative to sample density. You must understand logits as continuous scores thresholded at 0.
  - Quick check question: If a classifier's logit distribution for an attribute is bimodal with peaks at -5 and +5, where does the decision boundary fall? (Answer: At logit = 0, in a low-density region.)

- **Concept: Spectrum-based vs. Non-spectrum Attributes**
  - Why needed here: This categorization predicts which attributes will have sensitive bias measurements. Spectrum-based attributes have inherently ambiguous decision boundaries.
  - Quick check question: Is "Wearing Eyeglasses" likely spectrum-based or non-spectrum? (Answer: Non-spectrum—people either wear glasses or don't, with rare edge cases.)

## Architecture Onboarding

- **Component map:** Generative Model -> Attribute Classifier -> Evaluation Pipeline
- **Critical path:**
  1. Train unconditional generator on training images
  2. Train/fine-tune classifier on same training images using ground-truth labels
  3. Generate 10K images per checkpoint
  4. Apply classifier to all sets (train, val, generation)
  5. Compute per-attribute probabilities and ABS metric
  6. Visualize pre-sigmoid logit distributions to identify spectrum-based attributes

- **Design tradeoffs:**
  - Using validation set (not training set) as reference reduces classifier overfitting artifacts but may introduce distribution shift if val/train splits differ systematically (CelebA splits by identity)
  - Binary labels simplify evaluation but misrepresent spectrum-based attributes
  - Classifier choice affects categorization; ResNeXt and Swin Transformer show ~2% attribute accuracy differences

- **Failure signatures:**
  - Large ABS for spectrum-based attributes may indicate genuine generation shift OR classifier boundary sensitivity—check logit distributions to distinguish
  - ABS not tracking FID/FLD improvements suggests bias and quality are orthogonal concerns
  - High variance across random seeds indicates unstable mode coverage

- **First 3 experiments:**
  1. **Replicate ABS computation:** Train a small diffusion model on CelebA, generate 10K images, apply pre-trained classifier, compute ABS for spectrum vs. non-spectrum attributes—verify 3-5x ratio
  2. **Boundary density ablation:** For a spectrum-based attribute, artificially shift the classifier decision threshold and measure how ABS changes—confirm density-dependent sensitivity
  3. **Model size sweep:** Train tiny/small/large diffusion models, plot ABS vs. FLD—verify that larger models achieve lower ABS at similar FLD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do conditioning mechanisms (e.g., text prompts) and guidance strategies (e.g., classifier-free guidance) quantitatively alter the magnitude of attribute bias shift compared to the unconditional baselines established in this study?
- Basis in paper: [explicit] The conclusion states that the authors hope their analysis "will enable researchers to systematically study the effects of additional sources of bias—such as conditioning... guidance... on bias shift" which were excluded here to isolate the generator's contribution.
- Why unresolved: This work intentionally focused on unconditional generation to avoid confounding factors; therefore, the interaction between these common generative AI components and bias shift remains unmeasured.
- What evidence would resolve it: Repeating the proposed bias evaluation framework on conditional models (such as Stable Diffusion) while varying guidance weights to measure the change in Average attribute Bias Shift (ABS).

### Open Question 2
- Question: Can alternative evaluation frameworks be developed for "spectrum-based" attributes that do not rely on binary classifier decision boundaries, which the paper shows are highly sensitive to distribution density?
- Basis in paper: [explicit] The abstract highlights "the need for more representative labeling practices" and the analysis concludes that "attribute shifts are sensitive to the attribute classifier... particularly when its decision boundaries fall in high-density regions."
- Why unresolved: The current methodology relies on binarizing attributes using a classifier, which artificially inflates bias measurements for attributes that exist on a spectrum (e.g., "Smiling") when the decision boundary falls in a high-density region.
- What evidence would resolve it: A study comparing bias shift metrics using continuous probability distributions or soft-labeling against the binary threshold method used in this paper.

### Open Question 3
- Question: Is the larger attribute bias shift observed in BigGAN models primarily caused by mode collapse (reduced diversity) or by distinct architectural inductive biases compared to diffusion models?
- Basis in paper: [inferred] Section 4.5 notes that BigGAN exhibits larger bias shifts than large diffusion models "despite having similar image generation metrics" and suggests this "may be attributed to the well-known issue of mode collapse," but does not isolate the variable.
- Why unresolved: The paper establishes the empirical difference in bias shift between architectures but does not perform an ablation to distinguish between model capacity, architecture type, and diversity loss as the root cause.
- What evidence would resolve it: An experiment measuring bias shift in a diffusion model artificially constrained to lower diversity levels (simulating mode collapse) to see if it replicates the BigGAN bias profile.

### Open Question 4
- Question: Do the findings regarding decision boundary density and bias shift generalize to datasets with less curated, more diverse distributions than CelebA and DeepFashion?
- Basis in paper: [explicit] The Limitations section states that "findings... may be limited to unconditional generative models trained on these two datasets... and may not generalize beyond these settings."
- Why unresolved: The datasets used are relatively controlled and curated; it is unknown if the correlation between classifier decision boundary density and bias shift holds in "wild" datasets with higher noise or different demographic distributions.
- What evidence would resolve it: Replicating the analysis on a dataset like LAION-5B or a more diverse face dataset to verify if spectrum-based attributes still predict higher bias shifts.

## Limitations

- Reliance on classifier-predicted labels rather than ground truth introduces potential circularity where classifier artifacts may be misinterpreted as generation artifacts
- Binary attribute labeling system inadequately captures spectrum-based attributes, potentially conflating genuine generation bias with labeling ambiguity
- Study only evaluates unconditional models on two curated datasets, limiting generalizability to conditional generation or other domains

## Confidence

- **High Confidence:** The mechanism linking classifier boundary density to bias shift sensitivity is well-supported by logit distribution visualizations and controlled experiments
- **Medium Confidence:** The relationship between model capacity and bias shift is supported but could be confounded by quality metric limitations
- **Medium Confidence:** The spectrum-based vs. non-spectrum attribute categorization shows consistent patterns but lacks external validation

## Next Checks

1. **Ground truth validation:** Apply the same evaluation framework using ground truth labels (where available) on a subset of CelebA to quantify the impact of classifier prediction artifacts on ABS measurements
2. **Distribution shift characterization:** Measure earth mover's distance between training and generation distributions for different attributes to verify the translational shift assumption underlying the boundary density mechanism
3. **Cross-dataset generalization:** Apply the ABS evaluation framework to a third dataset (e.g., FFHQ) with different attribute characteristics to test whether spectrum-based attribute patterns hold across domains