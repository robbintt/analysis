---
ver: rpa2
title: 'Human-Level and Beyond: Benchmarking Large Language Models Against Clinical
  Pharmacists in Prescription Review'
arxiv_id: '2512.02024'
source_url: https://arxiv.org/abs/2512.02024
tags:
- pharmacists
- prescription
- review
- clinical
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed RxBench, a comprehensive benchmark of 3,259
  prescription review items reviewed by clinical pharmacists, to systematically evaluate
  large language models (LLMs) in detecting medication errors. Across 18 state-of-the-art
  LLMs, top models (Gemini-2.5-pro-preview-05-06, Grok-4-0709, and DeepSeek-R1-0528)
  significantly outperformed clinical pharmacists in single-choice (accuracy 0.88
  vs.
---

# Human-Level and Beyond: Benchmarking Large Language Models Against Clinical Pharmacists in Prescription Review

## Quick Facts
- arXiv ID: 2512.02024
- Source URL: https://arxiv.org/abs/2512.02024
- Reference count: 10
- Top models (Gemini-2.5-pro, Grok-4, DeepSeek-R1) outperform clinical pharmacists in structured prescription review tasks

## Executive Summary
This study introduces RxBench, a comprehensive benchmark of 3,259 prescription review items evaluated by clinical pharmacists, to systematically assess large language models' ability to detect medication errors. The evaluation reveals that leading LLMs significantly outperform clinical pharmacists in structured tasks (single/multiple-choice), while pharmacists maintain advantages in open-ended short-answer scenarios requiring clinical reasoning. A LoRA-fine-tuned Qwen3-32B model surpasses both its base version and several general-purpose frontier models, demonstrating the potential of domain-specific adaptation for specialized healthcare applications.

## Method Summary
The study developed RxBench by compiling 3,259 prescription review items from authoritative medical textbooks, covering 14 error categories across single-choice (1,150 items), multiple-choice (230 items), and short-answer (879 items) formats. Eighteen state-of-the-art LLMs were evaluated using zero-shot prompting with temperature=0, generating standardized JSON outputs. Performance was measured using task-specific metrics: accuracy for single-choice, F1 and accuracy for multiple-choice, and a weighted combination of error-type F1, intervention recall, and BERTScore for short-answer tasks. A LoRA-fine-tuned Qwen3-32B model was trained on 2,547 short-answer items using rank=32, α=64 parameters, achieving significant improvements over the base model.

## Key Results
- Top-tier LLMs (Gemini-2.5-pro, Grok-4, DeepSeek-R1) achieved 0.88-0.90 accuracy in single-choice tasks vs. pharmacists at 0.90
- Multiple-choice performance: LLMs F1=0.94 vs. pharmacists F1=0.77
- Short-answer tasks: Pharmacists matched/exceeded LLMs (F1 0.24-0.32 vs. 0.24-0.49 for top models)
- LoRA-fine-tuned Qwen3-32B surpassed both base model and several leading general-purpose LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Top-tier LLMs outperform clinical pharmacists in structured prescription review tasks (single/multiple-choice) due to uniform cross-domain knowledge integration.
- Mechanism: LLMs apply consistent probabilistic reasoning across all prescription categories without the domain-specific specialization constraints that limit human pharmacists. Pharmacists typically train deeply in specific therapeutic areas, which may reduce confidence or familiarity when confronting diverse, out-of-specialty scenarios in a broad benchmark.
- Core assumption: Performance on standardized test items reflects clinically relevant reasoning capacity, not just test-taking optimization.
- Evidence anchors:
  - [abstract] "leading LLMs can match or exceed human performance in certain tasks"
  - [Discussion] "clinical pharmacists typically undergo training focused on specific therapeutic areas... may lead to less familiarity... when addressing prescription issues outside their immediate domain"
  - [corpus] medicX-KG paper addresses pharmacists' drug information needs through knowledge graphs, suggesting ongoing challenges in comprehensive pharmaceutical knowledge access
- Break condition: Real-world prescription review involves patient-specific context (comorbidities, social determinants) not captured in structured test items; LLM performance may degrade when case complexity exceeds training distribution.

### Mechanism 2
- Claim: Pharmacists match or exceed LLMs in short-answer tasks requiring open-ended clinical reasoning and intervention formulation.
- Mechanism: Short-answer tasks demand synthesis of error identification, contextual risk assessment, and actionable intervention recommendations—capabilities where clinical training and experiential judgment provide advantages over pattern-matching approaches.
- Core assumption: The evaluation rubrics for short-answer tasks (error-type F1, intervention recall, semantic similarity) adequately capture clinical reasoning quality.
- Evidence anchors:
  - [abstract] "pharmacists matched or exceeded LLM performance in short-answer tasks"
  - [Results 4.3.3] Pharmacist F1 scores (0.24–0.32) were competitive with mid-tier models; top models (Gemini-2.5-pro: 0.487, Grok-4: 0.435) showed narrower margins than in multiple-choice
  - [corpus] Weak corpus evidence on short-answer clinical reasoning evaluation; related work focuses on diagnostic dialogue or benchmarks, not prescription intervention synthesis
- Break condition: Short-answer rubric may underweight nuanced clinical judgment that doesn't map cleanly to predefined answer points.

### Mechanism 3
- Claim: Domain-specific LoRA fine-tuning of mid-tier models can surpass general-purpose frontier models in specialized tasks.
- Mechanism: LoRA adapters inject low-rank transformations into frozen pretrained weights, enabling task-specific adaptation without catastrophic forgetting. High-quality, logically-annotated training data with explicit reasoning chains compensates for smaller parameter count.
- Core assumption: The training dataset (2,547 short-answer items from authoritative textbooks) represents the target distribution of real prescription review scenarios.
- Evidence anchors:
  - [abstract] "A LoRA-fine-tuned model based on Qwen3-32B surpassed both the base model and several leading general-purpose LLMs"
  - [Results 4.4] Qwen3-32B-LoRA achieved ~30% improvement over base model and ranked first in short-answer evaluation
  - [Discussion 5.4] "construction of high-quality, logically-annotated training data may be as important as simply scaling up base model parameters"
  - [corpus] No direct corpus evidence on LoRA for clinical pharmacy; gap in domain-specific fine-tuning literature
- Break condition: Fine-tuned models may overfit to textbook-style cases and fail to generalize to real-world prescription variability, formatting inconsistencies, or edge-case drug interactions.

## Foundational Learning

- **Prescription Error Taxonomy**: RxBench defines 14 error categories (Table 2) including dosing regimen, contraindications, drug interactions, off-label use, and documentation omissions.
  - Why needed here: Evaluating LLM performance requires understanding what constitutes a correct detection; error-type F1 scores depend on accurate classification against this taxonomy.
  - Quick check question: Can you distinguish "Inappropriate Dosing Regimen" from "Suboptimal Drug Selection" when the prescribed dose is within guidelines but a better drug class exists?

- **Zero-Shot Evaluation Paradigm**: Models receive task-specific prompts without in-context examples; temperature=0 ensures deterministic outputs reflecting intrinsic capabilities.
  - Why needed here: The benchmark comparisons rely on zero-shot performance; understanding this framing prevents misinterpreting results as requiring task-specific prompting optimization.
  - Quick check question: Why might zero-shot evaluation underestimate a model's practical utility if deployment allows for few-shot examples or retrieval augmentation?

- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning that freezes pretrained weights and injects trainable low-rank matrices (rank=32, α=64 in this study) into linear layers.
  - Why needed here: The key intervention—fine-tuning Qwen3-32B—uses LoRA; understanding this clarifies why mid-tier models can become specialized without full retraining.
  - Quick check question: What are the tradeoffs between LoRA rank (expressiveness) and overfitting risk when training data is limited to ~2,500 examples?

## Architecture Onboarding

- **Component map**:
  RxBench Dataset (1,150 single-choice, 230 multiple-choice, 879 short-answer items) → API calls to 18 LLMs (temperature=0) → Structured JSON outputs → Task-specific metrics (accuracy for single-choice; F1 + accuracy for multiple-choice; weighted F1 + recall + BERTScore for short-answer) → Performance comparison vs. pharmacists

- **Critical path**:
  1. Benchmark candidate models on RxBench test split → identify performance tiers
  2. Analyze error-type heatmaps → determine which categories have largest human-model gaps
  3. Select mid-tier base model → LoRA fine-tune on training split (2,547 items) → re-evaluate

- **Design tradeoffs**:
  - **Prompt restriction (answers only)**: Ensures clean automated scoring but eliminates chain-of-thought reasoning that might improve accuracy
  - **Pharmacist sample (n=27, 3 hospitals)**: Provides human baseline but may not represent global pharmacist competency; seniority gradient was unexpectedly absent
  - **Textbook-sourced cases**: High-quality ground truth but may lack real-world messiness (illegible prescriptions, incomplete patient histories)

- **Failure signatures**:
  - **Precision/recall inversion**: DeepSeek models showed high precision/low recall (conservative), while Grok-4 showed high recall/moderate precision (sensitive)—deployment context determines which failure mode is acceptable
  - **Seniority paradox**: Senior pharmacists did not outperform junior pharmacists on multiple-choice tasks, suggesting test format or domain breadth issues
  - **Short-answer ceiling**: Even top models achieved F1 ≤0.49 in error-type classification, indicating room for improvement across all systems

- **First 3 experiments**:
  1. **Reproduce baseline on open-source models**: Run Qwen3-32B and DeepSeek-V3 on RxBench test split using provided prompts; verify F1 scores fall within reported ranges
  2. **LoRA fine-tuning ablation**: Train Qwen3-32B-LoRA with varying ranks (8, 16, 32, 64) to identify saturation point where validation performance plateaus or degrades
  3. **Error-type stratified analysis**: For each of 14 error categories, compute per-category F1 for top 3 models vs. pharmacists; identify which error types show largest human advantage for targeted improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs maintain their superior performance on prescription review tasks when applied to real-world electronic health record (EHR) data rather than standardized textbook cases?
- Basis in paper: [explicit] The authors state that "Future work therefore prioritize validation using real-world data to assess model performance in authentic scenarios" and note that "standardized nature of textbook cases may not fully capture the nuances and heterogeneity of real-world clinical practice."
- Why unresolved: The study relied on cases from training textbooks, which, while authoritative, are cleaner and more structured than the messy, heterogeneous data found in actual clinical workflows.
- What evidence would resolve it: A prospective evaluation of top-performing models (e.g., Gemini, Grok) using unstructured, retrospective EHR data compared against prospective pharmacist reviews.

### Open Question 2
- Question: Does the integration of LLMs into clinical workflows create optimal "hybrid human–AI systems" that improve safety outcomes compared to either entity working alone?
- Basis in paper: [explicit] The paper concludes that "research should focus on hybrid human–AI systems that combine the contextual awareness and ethical reasoning of pharmacists with the computational breadth and consistency of LLMs."
- Why unresolved: While the paper establishes that LLMs outperform humans in isolation, it does not test whether a collaborative workflow actually reduces error rates or if it introduces new friction or cognitive bias.
- What evidence would resolve it: Clinical trials measuring error reduction rates and workflow efficiency in three distinct arms: LLM-only review, Pharmacist-only review, and Pharmacist-with-LLM-assistance.

### Open Question 3
- Question: To what extent does the inclusion of patient-specific factors, such as comorbidities and social determinants of health, degrade LLM accuracy in prescription review?
- Basis in paper: [explicit] The authors warn that LLMs "may underperform in real-world contexts requiring integration of patient-specific factors such as comorbidities or social determinants of health" which the current text-based tasks may not have fully captured.
- Why unresolved: The current benchmark relied on text-based questions which may lack the deep, unstructured context required to assess complex patient backgrounds, potentially inflating model performance relative to complex reality.
- What evidence would resolve it: Benchmarking models on a modified dataset enriched with complex, unstructured background patient information to see if accuracy drops compared to the standard RxBench scores.

## Limitations

- Benchmark relies on textbook-sourced cases that may not capture real-world prescription complexity including illegible handwriting, incomplete patient histories, or unusual clinical presentations
- Pharmacist evaluation sample (n=27 from 3 hospitals) may not represent global clinical competency levels, and unexpected absence of seniority effects suggests potential test format artifacts
- While LoRA fine-tuning shows promise, the training set of ~2,500 items may limit generalization to the full diversity of prescription scenarios encountered in practice

## Confidence

**High Confidence**: Claims about LLM performance on structured multiple-choice tasks (accuracy ~0.90) and the superior performance of top models (Gemini-2.5-pro, Grok-4, DeepSeek-R1) compared to human pharmacists. The zero-shot evaluation protocol and standardized metrics provide robust comparisons.

**Medium Confidence**: Claims about LoRA fine-tuning efficacy (30% improvement) and the superiority of the fine-tuned Qwen3-32B over general-purpose models. While results are promising, the small training set and textbook-based curriculum may not reflect real-world deployment conditions.

**Low Confidence**: Claims about pharmacists' relative performance in short-answer tasks (F1 ~0.25-0.32). The evaluation rubric's ability to capture nuanced clinical reasoning quality remains uncertain, and the gap between textbook cases and actual clinical practice introduces unknown biases.

## Next Checks

1. **Real-World Deployment Pilot**: Deploy the top-performing LoRA-fine-tuned model in a controlled clinical setting with actual prescription data from multiple hospital systems. Measure false positive/negative rates compared to gold-standard pharmacist reviews, particularly for complex cases involving multiple comorbidities.

2. **Cross-Cultural Competency Validation**: Evaluate the benchmarked models on prescription data from diverse healthcare systems (e.g., US, UK, Japan) with different drug formularies, prescribing conventions, and regulatory frameworks. Assess whether performance gains transfer across clinical contexts.

3. **Longitudinal Safety Monitoring**: Implement a systematic monitoring protocol tracking LLM-assisted prescription reviews over 12 months, measuring clinical outcomes (adverse drug events, medication errors caught) and system integration metrics (workflow efficiency, user acceptance) to validate the practical safety and utility claims.