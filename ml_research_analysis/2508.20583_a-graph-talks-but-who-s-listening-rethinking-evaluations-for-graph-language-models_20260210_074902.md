---
ver: rpa2
title: A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language
  Models
arxiv_id: '2508.20583'
source_url: https://arxiv.org/abs/2508.20583
tags:
- graph
- node
- reasoning
- station
- subway
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current graph-language model benchmarks fail to properly assess
  multimodal reasoning capabilities, as strong performance can be achieved using either
  graph or text information alone. To address this gap, we introduce CLEGR, a synthetic
  benchmark designed to require joint reasoning over graph structure and textual semantics.
---

# A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models

## Quick Facts
- arXiv ID: 2508.20583
- Source URL: https://arxiv.org/abs/2508.20583
- Reference count: 40
- Current graph-language model benchmarks fail to properly assess multimodal reasoning capabilities, as strong performance can be achieved using either graph or text information alone.

## Executive Summary
Current graph-language model (GLM) benchmarks inadequately test multimodal reasoning, as models can achieve high performance using either graph structure or text attributes alone. This paper introduces CLEGR, a synthetic benchmark designed to require joint reasoning over graph structure and textual semantics. Evaluations reveal that GLMs provide negligible performance gains over simple soft-prompted LLMs across all tasks, including zero-shot transfer and larger graph sizes. Linear probing confirms that graph encoders capture most task-relevant information, rendering the LLM component functionally redundant.

## Method Summary
The study evaluates Graph-Language Models (GLMs) by comparing them against unimodal baselines on existing node classification benchmarks and a new synthetic benchmark called CLEGR. Existing datasets (Cora, CiteSeer, Computers, Photo, History, Arxiv) are analyzed to determine whether they require true multimodal integration. CLEGR is constructed with 1000 synthetic subway graphs containing 22,000 fact-based and 32,248 reasoning-based questions. The evaluation includes GLMs with GraphSAGE/GAT backbones and frozen LLMs (Llama3-8B/Phi3-3.5B/Phi4-14B), soft-prompted LLM baselines, and linear probing experiments on frozen graph encoder outputs.

## Key Results
- Soft-prompted LLMs match or exceed GLM performance on existing benchmarks, indicating unimodal sufficiency
- GLMs show no advantage over soft-prompted baselines on CLEGR reasoning tasks while matching on factual retrieval
- Linear probing achieves accuracy nearly matching full GLM performance on structurally-sufficient datasets
- CLEGR-Reasoning exposes GLM limitations in compositional reasoning requiring both graph and language modalities

## Why This Works (Mechanism)

### Mechanism 1: Unimodal Sufficiency Detection via Competitive Baselines
- Claim: Current node classification benchmarks can be solved using either graph structure or text attributes alone, revealing they do not require true multimodal integration.
- Mechanism: The authors categorize datasets into "semantically-sufficient" (Computers, Photo, History, Arxiv) where soft-prompted LLMs match GLM performance using text only, and "structurally-sufficient" (Cora, CiteSeer) where GNNs outperform GLMs using topology only.
- Core assumption: If unimodal baselines achieve performance comparable to multimodal GLMs, the benchmark fails to test graph-language integration.
- Evidence anchors: [abstract] "strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration"; [section] Page 4, Table 1 shows soft-prompted LLMs achieve 74.34% vs. G-Token's 76.13% on Computers; GNNs achieve 87.31% on Cora while soft-prompted LLMs achieve only 28.69%

### Mechanism 2: Linear Probe Redundancy Test
- Claim: Graph encoders capture most task-relevant information on structurally-sufficient datasets, rendering the LLM component functionally equivalent to a large decoder head.
- Mechanism: A linear classifier trained on frozen graph encoder outputs (after projection) achieves accuracy nearly matching the full GLM (Pearson r=0.9643 correlation), indicating the LLM contributes minimal reasoning beyond what the graph tokens already encode.
- Core assumption: If linear probing suffices, the LLM's semantic reasoning capabilities remain unutilized for the task.
- Evidence anchors: [abstract] "Linear probing confirms that graph encoders capture most task-relevant information, rendering the LLM component functionally redundant"; [section] Page 5, Figure 2 shows linear probe accuracy closely tracks full GLM performance on Cora and CiteSeer

### Mechanism 3: CLEGR's Compositional Constraint Design
- Claim: A benchmark requiring structural dependency, semantic grounding, and compositional complexity forces models to integrate both modalities meaningfully.
- Mechanism: CLEGR combines (1) multi-hop structural reasoning that pure LLMs cannot infer from text, (2) natural language questions that pure GNNs cannot process, and (3) multi-step inference blending property lookup with logical operations. The synthetic design (fictional subway networks) eliminates pre-training memorization confounds.
- Core assumption: Tasks requiring all three constraints cannot be shortcutted by surface-level patterns in either modality alone.
- Evidence anchors: [abstract] "benchmark designed to require joint reasoning over graph structure and textual semantics"; [section] Page 5-6: CLEGR-Reasoning contains 32,248 questions across four reasoning types (Filtering, Aggregation, Path Reasoning, Topology); Page 7 Figure 4 shows GLMs fail to outperform soft-prompted baselines on reasoning tasks while matching on factual retrieval

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) — GraphSAGE, GAT, GCN
  - Why needed here: These form the graph encoder backbone (Mg) in GLMs; understanding their aggregation mechanisms (mean vs. attention) is required to interpret why graph tokens capture task-relevant information.
  - Quick check question: How does GraphSAGE's mean aggregation differ from GAT's attention-weighted aggregation, and when would attention provide an advantage?

- Concept: Soft Prompting
  - Why needed here: This is the critical baseline that matches GLM performance, revealing that sophisticated graph encoders may be unnecessary for current benchmarks; soft prompts are learnable continuous vectors prepended to input embeddings.
  - Quick check question: Why does a 10-token soft prompt (768-dim vectors) potentially encode as much task-relevant information as a full GNN backbone?

- Concept: Centered Kernel Alignment (CKA)
  - Why needed here: The paper uses CKA to measure representational similarity between GLMs and soft-prompted models, showing high similarity (low performance gap) correlates with similar internal representations across layers.
  - Quick check question: If two models have CKA > 0.9 in middle layers but different architectures, what does this imply about their functional similarity?

## Architecture Onboarding

- Component map: Text-Attributed Graph G → Graph Encoder (Mg) → Linear Projector (MP) → 10 Graph Tokens → Concatenate with LLM Input → Frozen LLM (Ml) → Prediction
- Critical path: Node features encoded via BERT (768-dim) → GNN backbone (3 layers, 1024 hidden) → Graph-level pooling for CLEGR (mean over all nodes) vs. node-level for classification tasks → Projector compresses to 10 tokens × LLM hidden dim → prepended to prompt
- Design tradeoffs: 10 graph tokens (fixed across all experiments) vs. potential information bottleneck for large graphs; Frozen LLM (efficiency, modularity) vs. fine-tuned LLM (potential better integration); GraphSAGE (sampling-based, scalable) vs. GAT (attention, potentially more expressive)
- Failure signatures: GLM accuracy within ±2% of soft-prompted baseline on reasoning tasks → graph encoder not utilized; Linear probe accuracy >95% of GLM accuracy → LLM acting as expensive decoder; CKA similarity >0.8 in mid-layers with low performance gap → representations are functionally equivalent; Performance degradation matching soft-prompt baseline when scaling graph size (CLEGR-Large) → no structural reasoning advantage
- First 3 experiments: (1) Modality sufficiency baseline: Run soft-prompted LLM, GNN-only (GCN/GAT/GraphSAGE), and full GLM (TEA-GLM, G-Token) on existing benchmarks (Cora, CiteSeer, Computers, Photo). Expected: bimodal performance patterns confirming semantically vs. structurally sufficient categories. (2) CLEGR diagnostic: Evaluate all models on CLEGR-Facts vs. CLEGR-Reasoning. Expected: GLMs match baselines on Facts but show no advantage on Reasoning, confirming current architectures fail at structural integration. (3) Linear probe + CKA analysis: Train linear classifiers on frozen graph tokens; compute CKA between GLM and soft-prompt activations across layers. Expected: High probe accuracy on structurally-sufficient datasets; high CKA when performance gaps are small.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural paradigms can effectively force the integration of graph structural tokens with LLM semantic reasoning, preventing the model from relying on unimodal shortcuts?
- Basis in paper: [explicit] The authors conclude that "current GLMs do not effectively integrate graph and language modalities, highlighting the need for more sophisticated architectural approaches."
- Why unresolved: Current projection methods (linear layers, soft-prompting) allow the LLM to treat graph tokens as redundant info or ignore them entirely, as evidenced by CKA analysis showing high similarity between GLM and soft-prompt representations.
- What evidence would resolve it: A new GLM architecture that demonstrates statistically significant performance gains over soft-prompted baselines on the CLEGR-Reasoning benchmark, accompanied by representation analysis (e.g., CKA) showing distinct multimodal fusion in intermediate layers.

### Open Question 2
- Question: Does the ability to solve synthetic, structural reasoning tasks in CLEGR transfer to complex, noisy, real-world Text-Attributed Graphs (TAGs)?
- Basis in paper: [inferred] While CLEGR solves the "unimodal sufficiency" problem of current benchmarks, it relies on synthetic subway and computer networks. The authors note that synthetic design eliminates pre-training confounds, but do not validate if this synthetic reasoning capability generalizes to real-world domains like biochemical graphs or social networks.
- Why unresolved: The paper establishes that current benchmarks are flawed, but it is unknown if the specific "compositional reasoning" skills required by CLEGR are the *correct* proxy skills for real-world applications where noise and ambiguity are prevalent.
- What evidence would resolve it: A study correlating model performance on CLEGR with performance on a newly curated set of real-world, multimodal-dependent graph tasks (e.g., molecular property prediction where both structure and text descriptions are necessary).

### Open Question 3
- Question: Can larger-scale LLMs (e.g., 70B+ parameters) overcome the architectural limitations observed in smaller models (3B-14B) to perform genuine graph reasoning?
- Basis in paper: [explicit] The Discussion section states, "As GLMs continue to develop, their capacity to represent graphs are likely to increase, and our current evaluation may not encompass all their capabilities."
- Why unresolved: The evaluation was limited to Phi-3.5B, Llama-3-8B, and Phi-4-14B. These models failed to integrate structure, but it remains unclear if this failure is due to model capacity/parameter count or a fundamental flaw in the "GLM" interface paradigm.
- What evidence would resolve it: Evaluation of frontier-scale GLMs (e.g., 70B parameters) on CLEGR-Reasoning showing that performance gaps between GLMs and text-only baselines widen significantly as scale increases, indicating successful structural integration.

## Limitations

- Synthetic benchmark construction may create artifacts that models exploit through template memorization rather than genuine multimodal reasoning
- Linear probing methodology assumes frozen graph encoder representations are stable and representative, yet projector-induced transformations may obscure modality-specific information
- The study focuses on smaller LLMs (3B-14B), leaving open whether larger models could overcome observed architectural limitations

## Confidence

- **High confidence**: The unimodal sufficiency analysis across existing benchmarks is robust, as it relies on direct empirical comparisons between soft-prompted LLMs, GNNs, and GLMs on well-established datasets with clear performance gaps.
- **Medium confidence**: The CLEGR benchmark design and its ability to diagnose true multimodal integration weaknesses is methodologically sound but relies on synthetic data that may not capture the full complexity of real-world graph-language tasks.
- **Low confidence**: The CKA similarity analysis, while technically correct, provides limited practical insight into whether models are truly learning different representations versus achieving similar outcomes through different internal mechanisms.

## Next Checks

1. **Template sensitivity analysis**: Systematically vary CLEGR question templates while keeping graph structures constant to measure performance degradation. If GLMs show >15% accuracy drop while maintaining superiority over unimodal baselines, this would confirm that template memorization is not the primary performance driver.

2. **Modality ablation under scaling**: Evaluate GLMs on CLEGR-Large graphs (3x larger) with systematic ablation of either graph structure or text semantics. If performance degrades by >30% when either modality is removed, this would validate that CLEGR successfully enforces multimodal integration requirements.

3. **Cross-architecture probing**: Apply the linear probing methodology to alternative graph encoder architectures (GAT vs GraphSAGE) on structurally-sufficient datasets. If probe accuracy correlates inversely with architectural complexity, this would suggest that simpler encoders may be preferable for these tasks, potentially challenging current GLM design assumptions.