---
ver: rpa2
title: 'One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit
  Semantic Representations'
arxiv_id: '2510.09293'
source_url: https://arxiv.org/abs/2510.09293
tags:
- sentence
- inli
- implicit
- explicit
- premise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitation of sentence embedding models
  in capturing implicit semantics by proposing DualCSE, a method that generates two
  embeddings per sentence: one for explicit and one for implicit meaning. The approach
  uses contrastive learning on the INLI dataset, which contains both explicit and
  implied entailment labels.'
---

# One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations

## Quick Facts
- **arXiv ID**: 2510.09293
- **Source URL**: https://arxiv.org/abs/2510.09293
- **Reference count**: 16
- **Primary result**: DualCSE achieves 80.18% accuracy on explicit entailment and 73.40% on implied entailment for RTE, plus near-perfect EIS performance (99.97%) on in-domain data

## Executive Summary
This paper addresses the limitation of sentence embedding models in capturing implicit semantics by proposing DualCSE, a method that generates two embeddings per sentence: one for explicit and one for implicit meaning. The approach uses contrastive learning on the INLI dataset, which contains both explicit and implied entailment labels. DualCSE employs either a cross-encoder or bi-encoder architecture to produce these dual embeddings, trained with a novel contrastive loss that encourages appropriate relationships between premise and hypothesis embeddings. Experimental results demonstrate significant improvements over baseline models in both explicit and implicit semantic representation tasks.

## Method Summary
DualCSE introduces a dual-embedding framework that separates explicit and implicit semantic representations for each sentence. The method leverages the INLI dataset containing premise-hypothesis pairs with both explicit and implied entailment labels. Two architectural variants are proposed: a cross-encoder that jointly processes premise and hypothesis pairs, and a bi-encoder that processes them independently. Both architectures generate two embeddings per sentence through separate projection layers. The model is trained using a contrastive loss that aligns positive pairs while pushing apart negative pairs across both explicit and implicit dimensions. This approach enables the model to capture nuanced semantic relationships that traditional single-embedding methods miss.

## Key Results
- DualCSE achieves 80.18% accuracy on explicit entailment and 73.40% on implied entailment for Recognizing Textual Entailment (RTE)
- For Estimating Implicitness Score (EIS), DualCSE achieves near-perfect performance (99.97%) on in-domain data
- On out-of-domain EIS datasets, DualCSE maintains competitive performance at 79.31% and 77.48% accuracy

## Why This Works (Mechanism)
The dual-embedding approach works by explicitly separating the representation of explicit and implicit semantic content, allowing the model to learn distinct feature spaces for each type of meaning. By training with contrastive learning on the INLI dataset that contains both explicit and implied entailment labels, the model learns to differentiate between surface-level semantic relationships and deeper inferential connections. The cross-encoder variant captures richer interactions between premise and hypothesis through full attention, while the bi-encoder offers computational efficiency. The contrastive loss ensures that positive pairs (premise and hypothesis with matching entailment type) are pulled together in their respective embedding spaces, while negative pairs are pushed apart, creating well-separated representations for explicit and implicit semantics.

## Foundational Learning
- **Contrastive learning**: Why needed - to align similar semantic representations while distinguishing different types; Quick check - verify loss properly separates explicit from implicit embeddings
- **Dual embeddings**: Why needed - to separately capture surface meaning and deeper inferences; Quick check - confirm both embeddings contribute to task performance
- **Cross-encoder vs bi-encoder architectures**: Why needed - to balance representation richness with computational efficiency; Quick check - compare performance and inference speed between variants
- **INLI dataset structure**: Why needed - provides labeled examples of both explicit and implied entailment; Quick check - validate dataset quality and label consistency
- **Entailment recognition**: Why needed - fundamental NLP task for semantic understanding; Quick check - measure accuracy on established RTE benchmarks
- **Implicitness scoring**: Why needed - quantifies depth of semantic inference required; Quick check - test on both in-domain and out-of-domain datasets

## Architecture Onboarding

**Component Map:**
Premise/Hypothesis input -> [Cross-Encoder OR Bi-Encoder] -> [Explicit Projection Layer] -> Explicit Embedding + [Implicit Projection Layer] -> Implicit Embedding -> [Contrastive Loss] -> Optimized Model

**Critical Path:**
Input preprocessing → Encoder (cross or bi) → Dual projection layers → Contrastive loss computation → Parameter updates

**Design Tradeoffs:**
The cross-encoder variant provides richer representations through full attention between premise and hypothesis but incurs quadratic complexity with sequence length, while the bi-encoder processes inputs independently for linear complexity but may miss important cross-sentence interactions. The dual embedding approach doubles the representation capacity but requires more complex training objectives and larger datasets to avoid overfitting.

**Failure Signatures:**
- Poor separation between explicit and implicit embedding spaces indicates inadequate contrastive loss or insufficient training data
- Performance degradation on out-of-domain data suggests overfitting to INLI dataset characteristics
- Large gap between cross-encoder and bi-encoder performance reveals importance of cross-sentence interactions for the task
- Degraded performance on explicit entailment suggests implicit representation is dominating the model's attention

**First Experiments:**
1. Train DualCSE on INLI with varying temperature parameters in the contrastive loss to find optimal separation between embedding spaces
2. Compare cross-encoder and bi-encoder variants on a held-out validation set to quantify the tradeoff between representation quality and computational efficiency
3. Evaluate model performance on a subset of RTE examples with varying levels of implicitness to verify the dual embeddings capture the intended semantic distinctions

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Results are based on experiments using a single dataset (INLI), raising questions about generalizability to other domains or languages
- No ablation studies on architectural choices (cross-encoder vs bi-encoder) or impact of different contrastive loss formulations
- Limited investigation of performance degradation patterns on out-of-domain data or analysis of failure cases

## Confidence

**Major Claim Clusters and Confidence Levels:**
- **Explicit and implicit semantic separation**: High confidence - The methodology for generating dual embeddings is clearly defined and the experimental setup demonstrates measurable performance on both explicit and implied entailment tasks.
- **Contrastive learning effectiveness**: Medium confidence - While results are positive, the lack of comparison with alternative training objectives or regularization methods limits definitive conclusions about contrastive learning's superiority.
- **Generalizability to out-of-domain data**: Medium confidence - The EIS results show reasonable performance on out-of-domain datasets, but the paper does not investigate the degradation patterns or provide analysis of failure cases.

## Next Checks
1. Conduct cross-dataset evaluation by testing DualCSE on established RTE benchmarks (e.g., SNLI, MultiNLI) that were not seen during training to assess true generalization capabilities.
2. Perform ablation studies comparing the dual-embedding approach against single-embedding models with additional explicit/implicit prediction heads to isolate the benefits of separate representations.
3. Analyze the model's performance on adversarial examples or controlled perturbations that specifically target implicit reasoning capabilities to identify potential weaknesses in the implicit embedding space.