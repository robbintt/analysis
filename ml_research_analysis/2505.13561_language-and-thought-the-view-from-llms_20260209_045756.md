---
ver: rpa2
title: 'Language and Thought: The View from LLMs'
arxiv_id: '2505.13561'
source_url: https://arxiv.org/abs/2505.13561
tags:
- language
- llms
- reasoning
- general
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper argues that large language models (LLMs) are the only\
  \ AI systems capable of strong domain-general reasoning, supporting Daniel Dennett\u2019\
  s thesis that language transforms cognition. The author explains that language\u2019\
  s abstract and compressed representation makes general inference computationally\
  \ tractable for neural networks."
---

# Language and Thought: The View from LLMs
## Quick Facts
- arXiv ID: 2505.13561
- Source URL: https://arxiv.org/abs/2505.13561
- Authors: Daniel Rothschild
- Reference count: 0
- Key outcome: Large language models are the only AI systems capable of strong domain-general reasoning, supporting the view that language transforms cognition.

## Executive Summary
This paper argues that large language models (LLMs) represent the first AI systems to achieve strong domain-general reasoning, suggesting that language itself is a key enabler of complex thought. The author explains that language's abstract and compressed representation makes general inference computationally tractable for neural networks, while other AI systems excel only in narrow domains. This supports Daniel Dennett's thesis that language transforms cognition rather than merely serving as a communication tool. The findings imply that language may be shaped to facilitate human inference and that learning language could be cognitively transformative.

## Method Summary
The paper employs philosophical argumentation and comparative analysis of different AI systems to support its thesis. Rather than presenting empirical experiments, it synthesizes observations about existing AI systems (game players, vision models, LLMs) and theoretical considerations about computation, representation, and inference. The author contrasts the reasoning capabilities of LLMs with domain-specific AI systems and analyzes why language-based training enables general reasoning where other approaches fail. The argument draws on computational complexity considerations, particularly comparing the tractability of next-token prediction versus next-frame prediction.

## Key Results
- Only language-trained systems (LLMs) exhibit strong domain-general reasoning in current AI
- Language's abstract and compressed encoding makes general inference computationally tractable for neural networks
- The cognitive utility of language for inference may have shaped natural language evolution beyond mere communication efficiency

## Why This Works (Mechanism)
### Mechanism 1: Computational Tractability Through Compression
Natural language's abstract and compressed encoding makes general inference computationally tractable for neural networks. Language strips irrelevant detail while retaining inferentially significant properties. A 30-second low-quality video (~2.5 MB) contains roughly the same data as Tolstoy's entire *War and Peace* (~3 MB). This data-efficiency means next-token prediction in language faces orders of magnitude fewer branching possibilities than video-frame prediction, making the optimization landscape solvable. Inference difficulty scales with the combinatorics of possible continuations; compressed representations reduce this search space meaningfully.

### Mechanism 2: Domain-General Reasoning Integration
Only systems trained extensively on natural language exhibit strong domain-general reasoning in current AI. Linguistic training provides both a representational format and vast amounts of world knowledge encoded in that format. Non-language systems (game players, vision models) show narrow, domain-specific competencies but fail to integrate across causal, agential, physical, and numerical reasoning. Domain-general reasoning requires integrating information across cognitive domains (agency, causation, number, physics), which current non-linguistic architectures cannot achieve.

### Mechanism 3: Language Optimized for Inference Utility
Language may be partially optimized for inference utility, not solely communication efficiency. If linguistic abstraction unlocks tractable inference in artificial systems, similar pressures may have shaped natural language. The "fluency" of language for thought—its ability to encode "exactly the facts that we care about"—suggests selection for cognitive utility beyond transmission. The cognitive utility observed in LLMs reflects a property of natural language itself, not just the scale of training.

## Foundational Learning
- **Concept: Domain-general vs. domain-specific reasoning**
  - Why needed here: The paper's central empirical claim depends on distinguishing systems that reason narrowly from those integrating across domains
  - Quick check question: Can you name two AI successes that are domain-specific and explain why they don't constitute general reasoning?

- **Concept: Computational tractability**
  - Why needed here: The core explanatory mechanism—that language makes inference tractable—requires understanding why some problems are solvable by neural networks
  - Quick check question: Why is next-token prediction on text more tractable than next-frame prediction on video?

- **Concept: Symbolic vs. subsymbolic computation**
  - Why needed here: The paper positions LLMs as a bridge: subsymbolic architectures that succeed because they operate on a symbolic system
  - Quick check question: How does the transformer architecture relate to the symbolic structure of language?

## Architecture Onboarding
- **Component map:** Tokenized natural language sequences -> Transformer network -> Next-token prediction probability distributions -> Generated text
- **Critical path:** Pre-training on massive linguistic corpora → next-token prediction capability → abstraction learning from linguistic structure → domain-general inference emerging from patterns across diverse linguistic domains
- **Design tradeoffs:** Language-only training enables domain generality but lacks grounded sensory experience; pure vision/video systems are grounded but fail at general inference; hybrid systems use LLMs to guide video generation, suggesting language remains the reasoning backbone
- **Failure signatures:** Memorization/regurgitation (outputs copy training sequences), domain isolation (system reasons about physics but not agency), current non-LLM AI shows domain isolation pattern
- **First 3 experiments:**
  1. Counterfactual video prediction test: Train a video-only model on next-frame prediction and evaluate causal/agential reasoning without any language component
  2. Compression probing: Compare inference quality on semantically equivalent but syntactically compressed vs. verbose text
  3. Domain transfer test: Test an LLM's inference on a novel domain described only linguistically vs. a vision system on a novel visual task

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- The argument relies heavily on a single empirical observation—that only LLMs exhibit domain-general reasoning—which may be temporary rather than fundamental
- The claim that language's compression specifically enables tractability remains speculative without direct experimental comparison
- The paper doesn't fully address whether hybrid systems combining language with other modalities might eventually surpass pure LLMs in domain-general reasoning
- The evolutionary argument for language being shaped by inference utility is suggestive but not proven

## Confidence
- **High confidence:** Language-based AI systems are currently the only ones demonstrating strong domain-general reasoning
- **Medium confidence:** The compression mechanism explains why language enables tractable inference
- **Low confidence:** Language itself is partially optimized for inference utility rather than just communication

## Next Checks
1. Train a video-only prediction system without any language components and test whether it can achieve domain-general reasoning about causation, agency, and physics simultaneously
2. Compare inference quality on semantically equivalent text representations with varying levels of syntactic compression to test whether compression directly correlates with reasoning tractability
3. Develop benchmarks that test transfer learning across domains using only linguistic descriptions versus only visual examples to quantify the relative importance of language abstraction for generalization