---
ver: rpa2
title: Microscaling Floating Point Formats for Large Language Models
arxiv_id: '2510.01863'
source_url: https://arxiv.org/abs/2510.01863
tags:
- microscaling
- formats
- values
- such
- floating-point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces microscaling, a novel floating-point compression
  technique for large language models (LLMs). Unlike traditional formats that allocate
  individual scales per value, microscaling uses a shared scale across blocks of values,
  enabling compact one-byte representations while maintaining dynamic range.
---

# Microscaling Floating Point Formats for Large Language Models

## Quick Facts
- arXiv ID: 2510.01863
- Source URL: https://arxiv.org/abs/2510.01863
- Reference count: 15
- One-line primary result: Microscaling achieves competitive accuracy for LLM training and inference using compact 8-bit formats with shared scales across blocks of values.

## Executive Summary
This paper introduces microscaling, a novel floating-point compression technique for large language models (LLMs) that uses shared scales across blocks of values rather than individual scales per value. Unlike traditional 8-bit formats, microscaling maintains extended dynamic range while enabling compact one-byte representations. The authors implement microscaling in a C++ GPT-2 model and demonstrate that using microscaling for weights and gradients while keeping activations in half-precision achieves competitive accuracy during both training and inference, proving its efficacy as a resource-efficient alternative for deploying LLMs at scale.

## Method Summary
The authors modified Karpathy's llm.c to implement microscaling formats (MX) for GPT-2 models, supporting both fine-tuning and inference. The core implementation uses block size 32 with shared scale per block, storing scaled elements in compact E4M3 or E5M2 formats. Key configurations tested include MX-E4M3 for weights and gradients with BF16 activations and FP32 master copy (Configuration D), and BF16 storage with on-line MX compression for matrix operations (Configuration F). The implementation uses round-to-nearest rounding policy, exact fixed-point accumulators (43-bit for E4M3), and keeps token embeddings and softmax probabilities in FP32 to prevent underflow issues.

## Key Results
- Microscaling with E4M3 format achieves 25.66% relative error during fine-tuning while maintaining coherent text generation
- Using exact fixed-point accumulators reduces numerical error compared to floating-point accumulators
- Keeping activations in BF16 while compressing weights to MX format significantly improves accuracy over quantizing all values
- Proper rounding policy (round-to-nearest vs truncation) reduces relative error from 10.97% to 2.85%

## Why This Works (Mechanism)

### Mechanism 1: Block-Wise Shared Exponent Compression
- Claim: Sharing a single scale (exponent) across a block of values enables 8-bit representations while preserving dynamic range beyond what native 8-bit floats allow.
- Mechanism: Values are divided into blocks (e.g., size 32). The block's shared scale is computed as the maximum exponent in the block. Each element is scaled by this shared value and stored in a compact format (e.g., E4M3, E5M2). Decompression multiplies the scaled element by the block's scale. This allows representing values that would overflow or underflow in native 8-bit formats, as the dynamic range is determined by the scale's bit-width rather than the element's exponent bits.
- Core assumption: Values within a block have similar magnitude distributions, so a shared scale approximates individual scales with minimal precision loss.

### Mechanism 2: Exact Fixed-Point Accumulation for Intra-Block Products
- Claim: Using a fixed-point accumulator with sufficient bit-width prevents overflow during intra-block dot-product computation, which would otherwise occur when using same-precision floating-point accumulators.
- Mechanism: When multiplying two MX elements, the product requires 2M+1 bits for the significand and E+2 bits for the exponent. The paper proposes a fixed-point accumulator wide enough to represent all possible mantissae and their magnitude ranges simultaneously (for E4M3: 43 bits). Products are converted to fixed-point, accumulated as integers, then converted back to floating-point.
- Core assumption: The accumulator bit-width calculation (ξmax - ξmin + 1 + E) correctly bounds all intermediate values without overflow.

### Mechanism 3: Mixed-Precision Storage with Master Weights
- Claim: Storing weights in low-precision MX format while maintaining a full-precision master copy for optimizer updates, and keeping activations in higher precision (BF16), achieves near-baseline accuracy while reducing memory bandwidth.
- Mechanism: Forward and backward passes use compressed weights (MX format). However, the optimizer (e.g., AdamW) updates a full-precision master copy of weights. After each update, weights are re-compressed. Activations remain in BF16 to avoid the precision loss observed when quantizing them to 8-bit formats. This selective quantization targets the largest memory consumers (weights) while preserving numerical stability in sensitive operations.
- Core assumption: The quantization error introduced by compressing weights to MX format is recoverable through the full-precision optimization trajectory.

## Foundational Learning

- Concept: Floating-point representation (sign, exponent, mantissa, bias)
  - Why needed here: MX formats manipulate these components directly; understanding how shared exponents work requires knowing how normal floating-point represents values. The paper's Equation 1 defines the decomposition.
  - Quick check question: Given an 8-bit float with E4M3 format (4 exponent bits, 3 mantissa bits), what is the bias value?

- Concept: Block quantization and shared scaling
  - Why needed here: MX is a form of block quantization. Understanding why sharing a scale works requires understanding the trade-off between per-value precision and aggregate dynamic range.
  - Quick check question: If a block contains values [0.5, 64.0, 0.125] and you must choose a shared scale, what value minimizes the maximum quantization error?

- Concept: Numerical stability in gradient descent
  - Why needed here: The paper emphasizes operation ordering and rounding policies. Understanding why truncation causes more error than round-to-nearest requires understanding accumulated numerical error in iterative optimization.
  - Quick check question: In 100 iterations of training, why might a 1% per-operation rounding error accumulate to more than 1% total error in the final loss?

## Architecture Onboarding

- Component map:
  - microscaling::vector<ElementType, ScaleType, BlockSize>: Main container storing std::vector of scales and std::vector of scaled elements on heap
  - Iterator: STL-like iterator with buffer for block decompression, refresh() and commit() methods for read/write access
  - exact_accumulator<MinExp, MantissaWidth>: Fixed-point accumulator using int64_t internally
  - anyfloat<SignBits, ExpBits, MantissaBits>: Software-emulated float with configurable bit-widths (modified from cppposit library)
  - Matrix operations: On-line compression during matrix multiplication (Section IV-C)

- Critical path:
  1. Start with baseline GPT-2 in BF16 with master weight copy (Configuration B)
  2. Implement MX vector container with block size 32
  3. Add on-line matrix compression (compress W and X to MX before dot product)
  4. Integrate exact accumulator for intra-block products
  5. Test Configuration F: BF16 storage + MX compression for matmul

- Design tradeoffs:
  - E4M3 vs E5M2: E4M3 provides more mantissa bits (3 vs 2) for fractional precision; E5M2 provides more exponent bits (5 vs 4) for dynamic range. Paper shows E5M2 increases error by ~5% in most configurations due to fractional precision loss.
  - Block size: Larger blocks reduce scale storage overhead but increase quantization error if values have high variance. Paper uses block size 32.
  - Truncation vs round-to-nearest: Truncation causes 10.97% relative error vs 2.85% for round-to-nearest (Table I).
  - FP accumulator vs exact accumulator: FP accumulators risk overflow; exact accumulators require wider integers and more conversion overhead.

- Failure signatures:
  - Loss divergence to infinity: Probabilities in softmax underflow to zero. Fix: Store probabilities and losses in full precision (Section IV-D).
  - Token embedding saturation: Embeddings become ±∞ or 0 in low precision. Fix: Store embeddings in full precision.
  - Incoherent text generation: Model outputs "pr pr pr pr pr..." (Configuration E). Indicates activation quantization is too aggressive.
  - High relative error (>100%): Quantizing all values including activations to 8-bit MX. Fix: Keep activations in BF16.

- First 3 experiments:
  1. **Rounding policy validation**: Train for 100 iterations comparing truncation vs round-to-nearest in BF16. Compare loss curves against std::bfloat16_t baseline. Target: <3% relative error with round-to-nearest.
  2. **MX compression for matmul only**: Implement Configuration F (BF16 storage, on-line MX compression for matrix operations using E4M3). Fine-tune on Tiny Shakespeare for 100 iterations. Target: <20% relative error, coherent text generation.
  3. **Exact accumulator ablation**: Compare Configuration F with and without exact fixed-point accumulator. Measure relative error difference. Target: Exact accumulator should reduce error (paper shows 16.12% → 12.61%).

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to GPT-2 variants and Tiny Shakespeare task, limiting generalizability
- No comparison with state-of-the-art quantization methods like GPTQ or AWQ
- Implementation details sparse, public repository appears incomplete
- Theoretical accumulator widths not empirically validated for overflow safety

## Confidence
- **High Confidence**: Block-wise shared exponent compression works as described; theoretical derivation of accumulator widths is sound; keeping activations in higher precision maintains accuracy
- **Medium Confidence**: Specific numerical benefits (relative error percentages, training stability) are likely accurate for tested configurations but may not generalize
- **Low Confidence**: Claims about superiority over existing quantization methods not substantiated by direct comparisons

## Next Checks
1. **Ablation on Block Size and Scale Computation**: Systematically vary block size (8, 16, 32, 64) and scale computation method (max exponent vs. average vs. learned scale) to quantify their impact on accuracy and memory efficiency across multiple LLM architectures.

2. **Comparison with State-of-the-Art Quantization**: Implement and compare microscaling against GPTQ, AWQ, and 8-bit GPTQ on the same GPT-2 models using identical evaluation protocols (training stability, inference accuracy, memory usage).

3. **Numerical Overflow Validation**: Create adversarial test cases with extreme value distributions to verify that the proposed fixed-point accumulator widths prevent overflow in all scenarios, and measure the actual bit-width required in practice versus the theoretical bounds.