---
ver: rpa2
title: Towards Minimal Fine-Tuning of VLMs
arxiv_id: '2512.19219'
source_url: https://arxiv.org/abs/2512.19219
tags:
- head
- heads
- image-lora
- selection
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Image-LoRA is a parameter-efficient fine-tuning method for vision-language
  models that restricts adaptation to visual tokens and a subset of attention heads.
  The approach applies low-rank adaptation only to the value path of attention layers
  within the visual-token span, reducing adapter-only training FLOPs proportionally
  to the visual-token fraction.
---

# Towards Minimal Fine-Tuning of VLMs

## Quick Facts
- arXiv ID: 2512.19219
- Source URL: https://arxiv.org/abs/2512.19219
- Authors: Tiange Luo; Lajanugen Logeswaran; Jaekyeom Kim; Justin Johnson; Honglak Lee
- Reference count: 40
- One-line primary result: Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs

## Executive Summary
Image-LoRA is a parameter-efficient fine-tuning method for vision-language models that restricts adaptation to visual tokens and a subset of attention heads. The approach applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs proportionally to the visual-token fraction. Head selection is performed using influence scores estimated with a rank-1 Image-LoRA, and updates are stabilized via selection-size normalization. Across screen-centric grounding and referring benchmarks with varying text-to-image ratios, Image-LoRA matched or closely approached standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method preserved pure-text reasoning performance, as demonstrated on GSM8K.

## Method Summary
Image-LoRA applies LoRA adapters only to the value projection of visual tokens for a subset of attention heads. The method first selects heads using a rank-1 LoRA probe that computes influence scores based on gradient norms, then trains a rank-8 LoRA (r=8, α=16) only on selected heads and visual tokens. A visual-token mask ensures adapter updates never affect text token representations, preserving pure-text reasoning capabilities. Head selection uses a farthest-first diversity heuristic with a layerwise budget (τ=0.5) and diversity factor (ρ=2.0). Training uses AdamW with learning rate 5×10⁻⁴ for 5 epochs, with adapters injected pre-RoPE and pre-KV-cache.

## Key Results
- Image-LoRA achieved 37.35% accuracy on ScreenSpot-Pro at 1:5 ratio versus 35.11% for standard LoRA while using 7.8× fewer FLOPs
- At 1:1 ratio on RefCOCO, Image-LoRA reached 68.40% accuracy versus 68.98% for standard LoRA with 4.4× fewer FLOPs
- GSM8K pure-text reasoning accuracy remained unchanged at 25.55% before and after Image-LoRA fine-tuning, confirming text reasoning preservation

## Why This Works (Mechanism)

### Mechanism 1: Visual-Token-Only Adaptation Isolates Updates
Restricting LoRA adapters to the visual-token span preserves a VLM's pre-trained text reasoning capabilities while allowing fine-tuning on visual tasks. Standard LoRA updates parameters across all tokens, but Image-LoRA applies a diagonal mask that zeroes out LoRA updates for non-visual token indices, structurally guaranteeing that text token representations remain unaltered.

### Mechanism 2: Value-Only Projection Optimizes Information Retrieval
Adapting only the Value (V) projection on visual tokens is more effective than adapting Query (Q), Key (K), or Output (O) for visual reasoning tasks. In transformer attention, text tokens retrieve information from visual tokens via a weighted sum of value vectors. Updating V for visual token j directly modifies the information available to all attending tokens without altering attention probabilities.

### Mechanism 3: Head Selection Concentrates Capacity
A small, diverse subset of attention heads can achieve comparable performance to full-head fine-tuning, with heads efficiently identified via a one-shot influence score. The method uses a rank-1 Image-LoRA to compute a head influence score as the gradient norm, selecting only the top heads (e.g., 28/112 for 7B models) and normalizing updates by 1/√N_chosen to concentrate capacity on the most impactful sub-network.

## Foundational Learning

- **Transformer Multi-Head Attention and Projections**
  - Why needed here: To understand why updating V (value) for visual tokens affects text token outputs, but updating Q (query) for visual tokens does not
  - Quick check question: In the attention formula Oi = Σj softmax((QiKjᵀ)/√d)jVj, if you only modify Vj, does the attention weight on token j change? Does the output for token i change?

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Image-LoRA is a specialized variant. You must understand that LoRA adds a learnable delta ΔW = AB to a frozen weight matrix W to grasp the "rank-1 probing" and "rank-8 training" concepts
  - Quick check question: If a pre-trained weight matrix is W ∈ ℝᵈᵒᵘᵗˣᵈᵢₙ, and you use LoRA with rank r, what are the dimensions of the adapter matrices A and B?

- **First-Order Taylor Approximation for Sensitivity**
  - Why needed here: This justifies using the gradient norm as a proxy for head importance. The method relies on the idea that Δℓ ≈ ||∇θℓ||² to rank heads efficiently
  - Quick check question: If the loss function is ℒ(θ), and you take a small step δθ = -∇θℒ, what is the approximate change in loss Δℒ according to a first-order Taylor expansion?

## Architecture Onboarding

- **Component map**: Input sequence X (text + visual tokens I_v) -> Attention layers (H heads) -> Image-LoRA adapter (V-projection only, visual-token span only, selected heads only) -> Output
- **Critical path**: 1) Probe Phase: Run rank-1 Image-LoRA forward-backward to compute influence scores I(h) 2) Head Selection: Select top heads using τ=0.5 layerwise budget + diversity heuristic 3) Training Phase: Train rank-8 Image-LoRA on selected heads, visual tokens only 4) Inference: Apply LoRA deltas dynamically to visual tokens for selected heads
- **Design tradeoffs**: Optimized for image-heavy visual reasoning; requires custom kernels for masking and selection; cannot pre-merge weights for inference efficiency
- **Failure signatures**: Text reasoning degradation (mask leakage); underfitting on text-heavy tasks; selection failure from biased probe dataset
- **First 3 experiments**: 1) Validate text-reasoning preservation: fine-tune on ScreenSpot-Pro, evaluate zero-shot on GSM8K (expected: Image-LoRA matches base model) 2) Ablate projection target: compare V-only vs K-only vs V+K on visual task (expected: V-only highest) 3) Measure efficiency: compare trainable parameters and FLOPs across text:image ratios (expected: significant reductions in image-heavy regimes)

## Open Questions the Paper Calls Out

- **Head selection evolution**: Do the optimal subsets of attention heads identified via influence scores evolve significantly during the training process? The current method uses static, one-shot estimation assuming initial heads remain optimal throughout training.

- **Alternative selection criteria**: Can alternative head-selection criteria yield superior accuracy-efficiency trade-offs compared to the proposed first-order influence score? The paper only evaluates one selection method, leaving comparative performance of other potential selection algorithms unexplored.

- **Attention pattern reshaping**: How does Image-LoRA specifically reshape attention patterns compared to Standard LoRA? The paper focuses on empirical performance metrics rather than internal representational changes or attention map dynamics induced by selective adaptation.

## Limitations
- Cannot generalize to cross-attention architectures like Llama-3.2-Vision where visual-token spans are not well-defined
- May underperform on text-heavy tasks where visual signal is weak compared to textual signal
- Requires custom implementation for visual token span detection and head selection

## Confidence
- Method efficacy: High - matches or exceeds standard LoRA performance across multiple benchmarks
- Efficiency claims: High - FLOPs reductions proportional to visual-token fraction are mathematically sound
- Text preservation claim: High - structural isolation via visual-token mask guarantees this property
- Head selection method: Medium - one-shot estimation assumption needs validation over training dynamics

## Next Checks
1. Verify visual-token mask implementation prevents any adapter updates to text tokens by logging adapter activation positions
2. Confirm head selection process correctly implements farthest-first diversity selection with ρ=2.0 factor
3. Measure actual adapter-only FLOPs during training to verify claimed efficiency improvements match theoretical predictions