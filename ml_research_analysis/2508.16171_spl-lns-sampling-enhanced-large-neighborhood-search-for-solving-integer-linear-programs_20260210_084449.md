---
ver: rpa2
title: 'SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear
  Programs'
arxiv_id: '2508.16171'
source_url: https://arxiv.org/abs/2508.16171
tags:
- neural
- spl-lns
- neighborhood
- solution
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies that existing neural Large Neighborhood Search
  (LNS) solvers for Integer Linear Programs (ILPs) suffer from local optima issues
  due to their greedy neighborhood selection strategies. To address this, the authors
  propose SPL-LNS, a sampling-enhanced neural LNS method that leverages locally-informed
  proposals to escape local optima.
---

# SPL-LNS: Sampling-Enhanced Large Neighborhood Search for Solving Integer Linear Programs

## Quick Facts
- arXiv ID: 2508.16171
- Source URL: https://arxiv.org/abs/2508.16171
- Authors: Shengyu Feng; Zhiqing Sun; Yiming Yang
- Reference count: 18
- Primary result: Sampling-enhanced neural LNS method that outperforms state-of-the-art solvers on ILP benchmarks

## Executive Summary
Existing neural Large Neighborhood Search (LNS) solvers for Integer Linear Programs (ILPs) struggle with local optima due to greedy neighborhood selection strategies. SPL-LNS addresses this limitation by incorporating sampling-enhanced mechanisms that leverage simulated annealing and energy-based models to explore diverse candidate solutions. The method introduces a hindsight relabeling strategy for efficient self-supervised training. Experiments across five ILP benchmarks, including a real-world problem, demonstrate that SPL-LNS consistently achieves lower primal gaps and better primal integrals compared to both state-of-the-art neural LNS solvers and traditional heuristic methods.

## Method Summary
SPL-LNS introduces a sampling-enhanced neural LNS framework that replaces greedy solution selection with a stochastic approach. The method employs simulated annealing to sample candidate solutions from a set of feasible solutions using an energy-based model, enabling better exploration of the solution space and escape from local optima. A novel hindsight relabeling strategy is implemented for self-supervised training, improving sample efficiency. The architecture combines neural networks for neighborhood selection with probabilistic sampling mechanisms, creating a more robust search process that balances exploration and exploitation in solving ILPs.

## Key Results
- SPL-LNS consistently achieves lower primal gaps compared to state-of-the-art neural LNS solvers across five ILP benchmarks
- The method demonstrates better primal integral performance, indicating improved solution quality over time
- SPL-LNS shows effectiveness in overcoming local optima and improving sample efficiency on both synthetic and real-world ILP problems

## Why This Works (Mechanism)
The sampling-enhanced approach works by introducing stochasticity into the neighborhood search process, preventing premature convergence to local optima that plague greedy selection methods. By using simulated annealing and energy-based models, SPL-LNS can explore a broader solution space while maintaining solution quality. The hindsight relabeling strategy enables more efficient learning from past search trajectories, allowing the model to learn from both successful and unsuccessful exploration attempts. This combination of exploration mechanisms and improved learning efficiency addresses the fundamental limitation of existing neural LNS solvers.

## Foundational Learning
- Integer Linear Programming (ILP): Formulating optimization problems with linear constraints and integer variables - needed to understand the problem domain and solution requirements
- Large Neighborhood Search (LNS): Heuristic optimization technique that iteratively destroys and repairs solutions - needed to grasp the baseline approach being enhanced
- Simulated Annealing: Probabilistic technique for approximating global optimum by accepting worse solutions with certain probability - needed to understand the sampling mechanism
- Energy-based Models: Probabilistic models that define a probability distribution through an energy function - needed to comprehend the sampling strategy
- Hindsight Relabeling: Training strategy that retrospectively assigns labels to past experiences - needed to understand the self-supervised learning approach

## Architecture Onboarding

**Component Map:** Input Problem → Neural Network (Neighborhood Selection) → Sampling Module (Simulated Annealing + Energy-based Model) → Candidate Solutions → Solution Selection → Output Solution

**Critical Path:** Problem formulation → Neighborhood identification → Candidate generation via sampling → Solution evaluation → Iteration and update

**Design Tradeoffs:** The method trades computational overhead from sampling and probabilistic selection against improved solution quality and escape from local optima. The sampling mechanism increases exploration capability but requires more computation per iteration compared to greedy approaches.

**Failure Signatures:** Poor performance may manifest when the energy-based model fails to properly distinguish between high-quality and low-quality solutions, when the sampling temperature is not properly tuned, or when the hindsight relabeling strategy incorrectly assigns labels to training examples.

**First Experiments:** 1) Run ablation study comparing greedy vs. sampling-based solution selection on benchmark problems, 2) Test different sampling temperatures in the simulated annealing process, 3) Evaluate the impact of hindsight relabeling by comparing with standard supervised training approaches.

## Open Questions the Paper Calls Out
None identified in the provided analysis.

## Limitations
- The paper lacks detailed ablation studies to quantify the individual contributions of simulated annealing, sampling strategy, and hindsight relabeling components
- Computational overhead of the sampling process compared to traditional greedy approaches is not explicitly discussed, limiting practical deployment assessment
- The real-world benchmark evaluation lacks detailed information about problem scale and characteristics, making generalizability assessment difficult

## Confidence
High confidence in the core technical contributions of SPL-LNS, particularly the sampling-enhanced approach and hindsight relabeling strategy. Medium-high confidence in the empirical results given multiple benchmark problems tested, though concerns exist about reproducibility of the real-world case study and performance on larger-scale problems.

## Next Checks
1. Conduct systematic ablation studies to isolate the impact of simulated annealing, sampling strategy, and hindsight relabeling on final solution quality
2. Perform runtime analysis comparing SPL-LNS computational overhead against traditional greedy LNS approaches across varying problem sizes
3. Test SPL-LNS on larger-scale industrial problems (beyond the current real-world benchmark) to assess scalability and practical applicability