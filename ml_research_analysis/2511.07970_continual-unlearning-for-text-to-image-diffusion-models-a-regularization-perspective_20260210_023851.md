---
ver: rpa2
title: 'Continual Unlearning for Text-to-Image Diffusion Models: A Regularization
  Perspective'
arxiv_id: '2511.07970'
source_url: https://arxiv.org/abs/2511.07970
tags:
- unlearning
- continual
- concepts
- arxiv
- retention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies continual unlearning in text-to-image diffusion
  models, where concepts must be erased sequentially. Existing methods suffer from
  rapid utility collapse, forgetting retained knowledge after only a few unlearning
  requests.
---

# Continual Unlearning for Text-to-Image Diffusion Models: A Regularization Perspective

## Quick Facts
- arXiv ID: 2511.07970
- Source URL: https://arxiv.org/abs/2511.07970
- Authors: Justin Lee; Zheda Mai; Jinsu Yoo; Chongyu Fan; Cheng Zhang; Wei-Lun Chao
- Reference count: 26
- Key outcome: Regularization strategies substantially improve retention accuracy during sequential unlearning in text-to-image diffusion models while maintaining high unlearning accuracy

## Executive Summary
This paper addresses continual unlearning in text-to-image diffusion models, where concepts must be erased sequentially while preserving retained knowledge. Existing methods suffer from rapid utility collapse due to cumulative parameter drift from pre-training weights. The authors propose add-on regularization strategies including norm constraints, selective fine-tuning, model merging, and a novel gradient-projection method that enforces semantic awareness. Experiments on the UNLEARNCANVAS benchmark show these approaches substantially improve retention accuracy, especially when combined, while maintaining high unlearning accuracy.

## Method Summary
The paper studies continual unlearning where concepts are erased sequentially in text-to-image diffusion models. The core approach adds regularization to constrain parameter updates relative to pre-trained weights, preventing utility collapse. Methods include L1/L2 norm penalties on parameter drift, Selective Fine-tuning (SelFT) that updates only top-k% important parameters, model merging of independently unlearned models, and gradient projection that projects unlearning gradients orthogonal to semantically related concept subspaces. The evaluation uses UNLEARNCANVAS benchmark with 60 artistic styles and 20 object categories, measuring Unlearning Accuracy (UA), In-Domain Retention Accuracy (RA-I), and Cross-Domain Retention Accuracy (RA-C).

## Key Results
- Sequential unlearning causes rapid utility collapse, with retention accuracy plummeting after only a few requests due to cumulative parameter drift
- Regularization strategies (norm constraints, SelFT, model merging) substantially improve retention accuracy, with model merging achieving best cross-domain retention
- Gradient projection method enforces semantic awareness by projecting updates orthogonal to related concept subspaces, improving in-domain retention at computational cost
- Combining multiple regularization strategies yields synergistic improvements in retention accuracy while maintaining high unlearning accuracy

## Why This Works (Mechanism)

### Mechanism 1: Parameter Drift Constraint via Regularization
- Claim: Constraining parameter updates relative to pre-trained weights preserves retention capability during sequential unlearning
- Mechanism: Sequential unlearning accumulates deviations from θ† (pre-trained weights). The retention loss change is bounded by |L(θ⋆, Cr) - L(θ†, Cr)| ≤ ||∇L||·||θ⋆ - θ†|| + (½)||H||·||θ⋆ - θ†||² (Taylor expansion). Regularization (L1/L2 norms, selective updates, model merging) bounds ||θ⋆ - θ†||, keeping retention loss near its optimum
- Core assumption: Pre-trained weights lie in a low-curvature basin of the retention loss landscape; Hessian norm is small
- Evidence anchors: [abstract] "trace this failure to cumulative parameter drift from the pre-training weights and argue that regularization is crucial"; [section 5.2, Appendix B.1] Full derivation of Lipschitz-type bound; empirical estimates of smoothness constant M are minuscule (Table 1)

### Mechanism 2: Selective Parameter Updates (SelFT)
- Claim: Updating only parameters critical for the target concept reduces interference with retained concepts
- Mechanism: First-order Taylor approximation computes importance(d) = |∇θ[d]Lunlearn · θ[d]|. Only top-k% parameters are updated, constraining the update subspace
- Core assumption: Concept-specific knowledge localizes to sparse parameter subsets; critical weights for unlearning differ from those for retention
- Evidence anchors: [section 6.2] "explicitly restricts updates to parameters deemed critical for the unlearning loss"; [figure 4, 6] SelFT shows reduced drift and improved RA-C compared to sequential baseline

### Mechanism 3: Gradient Projection for Semantic Preservation
- Claim: Projecting unlearning gradients orthogonal to semantically related concept subspaces preserves in-domain retention
- Mechanism: Cross-attention matrices WK, WV map text embeddings to latents. Linear projections preserve neighborhood: ||AE(c) - AE(c⋆)|| ≤ ||A||·||E(c) - E(c⋆)||. Gradient projection removes components aligned with auxiliary concept embeddings: g' = PS⊥g⋆ = (I - C(C⊤C)⁻¹C⊤)g⋆, ensuring g'⊤c = 0 for c in span(C) (Lemma 7.1)
- Core assumption: Text embedding similarity predicts cross-attention interference; auxiliary concepts adequately cover the sensitive subspace
- Evidence anchors: [section 7.3] Lemma 7.1 proves first-order invariance; [figure 7a-b] Strong negative correlation (r=-0.627) between RA-I and embedding similarity; positive correlation (r=0.835) between k,v update norm and similarity

## Foundational Learning

### Concept: Catastrophic Forgetting in Continual Learning
- Why needed here: Continual unlearning shares sequential update structure; forgetting mechanisms transfer directly
- Quick check question: Why does sequentially learning task B degrade performance on previously learned task A?

### Concept: Cross-Attention in Diffusion Models
- Why needed here: Text conditioning flows through WK, WV projections; essential for understanding semantic interference and gradient projection
- Quick check question: In a diffusion model, how does the text prompt "Van Gogh style" influence the denoising network's latent updates?

### Concept: Taylor Expansion for Loss Approximation
- Why needed here: The paper derives drift-retention bounds via second-order Taylor expansion
- Quick check question: What does the Hessian term capture in a loss function's Taylor expansion around a point?

## Architecture Onboarding

### Component Map:
Pre-trained Model θ†
    ↓
Sequential Unlearning Loop (for each request c⋆ᵢ):
    ├── Unlearning Loss: Lunlearn(θ, {c⋆ᵢ})
    ├── Gradient Projection (optional): g' = PS⊥∇Lunlearn
    ├── Regularizer Options:
    │   ├── Norm: λ||θ - θ⋆ᵢ₋₁||ᵖ (p∈{1,2})
    │   ├── SelFT: mask to top-k% important params
    │   └── Model Merge: TIES-Merge(θ̃₁...θ̃ᵢ)
    └── Update → θ⋆ᵢ
        Evaluate: UA (unlearning), RA-I (in-domain), RA-C (cross-domain)

### Critical Path:
1. For request n, initialize from θ⋆ₙ₋₁
2. Compute ∇Lunlearn(θ, {c⋆ₙ})
3. If semantic preservation needed: project gradient via PS⊥
4. Apply SelFT mask or norm regularizer
5. Update parameters; compute UA, RA-I, RA-C

### Design Tradeoffs:
- **L1 vs L2**: L1 → sparse, localized updates; L2 → distributed, prevents single-weight drift
- **SelFT vs Model Merge**: SelFT is online and fast; Model Merge requires independent unlearning but achieves best RA-C
- **Gradient Projection Overhead**: Requires auxiliary concept generation (LLM + embedding filtering); improves RA-I at computational cost

### Failure Signatures:
- **RA-C plummets, UA stable**: Regularization too weak → increase λ or switch to model merging
- **UA low, RA high**: Regularization dominates → reduce λ or increase k% in SelFT
- **RA-I collapses while RA-C holds**: Semantic interference → enable gradient projection

### First 3 Experiments:
1. **Drift baseline**: Run ConAbl sequentially on 12 concepts, plot ||θ⋆ₙ - θ†||₂ vs n; expect near-linear/exponential growth per Figure 4
2. **Regularizer ablation**: Compare L1, L2, SelFT, Model Merge on RA-C after 12 requests; expect Model Merge > SelFT > L2 > L1 per Figure 6
3. **Projection validation**: Unlearn "Abstractionism" with and without gradient projection; plot RA-I vs cosine similarity to target; projection should reduce correlation slope per Figure 7a

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can semantic-aware unlearning be extended beyond gradient projection to achieve comparable cross-domain retention (RA-C) without combining with other regularizers?
- Basis in paper: [explicit] The authors note that gradient projection "retention accuracy on cross-domain concepts (RA-C) is slightly lower than that of SelFT and model merging in the object unlearning case" (Section 7.3)
- Why unresolved: Gradient projection excels at in-domain retention but requires combination with other methods for optimal cross-domain performance
- What evidence would resolve it: A modified projection method or alternative semantic-aware approach achieving RA-C comparable to model merging while maintaining high RA-I, without requiring method combinations

### Open Question 2
- Question: How does continual unlearning performance scale when the number of sequential requests grows substantially beyond the 12 concepts tested?
- Basis in paper: [inferred] The benchmark only evaluates sequences of 12 concepts; Figure 9 shows gains from add-on regularizers "become increasingly pronounced as the number of unlearning requests grows," suggesting scaling behavior is important
- Why unresolved: Real-world deployment may require hundreds or thousands of sequential unlearning requests; whether regularizers continue to prevent utility collapse at scale is unknown
- What evidence would resolve it: Experiments with longer unlearning sequences (50+, 100+) showing retention accuracy degradation curves and whether current methods remain effective

### Open Question 3
- Question: How robust are these regularization strategies across different diffusion model architectures and checkpoint initializations?
- Basis in paper: [inferred] The study uses only Stable Diffusion v1.4 fine-tuned on UNLEARNCANVAS; generalization to other architectures (SDXL, DiT-based models) or differently initialized checkpoints is untested
- Why unresolved: The effectiveness of drift-based regularization may depend on specific loss landscapes or architecture-specific properties
- What evidence would resolve it: Experiments applying the same add-on regularizers to different diffusion architectures showing comparable retention improvements

### Open Question 4
- Question: Can more principled adaptive selection of auxiliary concepts for gradient projection improve semantic awareness without requiring LLM-generated candidates?
- Basis in paper: [explicit] The method relies on "M auxiliary concepts generated by an LLM and filtered by text-embedding similarity" (Section 7.3), but the quality and selection criteria for these concepts are not systematically studied
- Why unresolved: The choice of auxiliary concepts affects the subspace construction; whether optimal selection exists is unclear
- What evidence would resolve it: Ablation studies varying auxiliary concept sources, numbers, and filtering strategies with retention accuracy measurements

## Limitations
- The theoretical drift bounds assume smooth loss landscapes with small Hessian norms, but actual curvature under sequential unlearning remains empirically unverified
- Gradient projection method depends critically on LLM-generated auxiliary concepts, but the prompt strategy and filtering criteria are underspecified
- Study focuses only on artistic styles and objects, not testing domain transfer to faces, copyrighted characters, or other real-world scenarios

## Confidence
- **High Confidence**: The core observation that sequential unlearning causes rapid utility collapse due to parameter drift is well-supported by experiments
- **Medium Confidence**: The gradient projection mechanism's effectiveness relies on text embedding similarity predicting cross-attention interference, which correlation analysis supports but needs more direct validation
- **Low Confidence**: Theoretical drift bounds assume low Hessian norm without comprehensive empirical verification across all tested concepts

## Next Checks
1. **Hessian Norm Verification**: Systematically measure ||H|| for retention loss across all concepts in UNLEARNCANVAS to validate the smoothness assumption underlying the drift bounds
2. **Auxiliary Concept Quality**: Test gradient projection with different LLM prompt strategies and concept filtering thresholds to determine sensitivity to auxiliary set quality
3. **Domain Transfer Test**: Apply the proposed methods to unlearning faces or copyrighted characters to evaluate performance beyond artistic styles and objects