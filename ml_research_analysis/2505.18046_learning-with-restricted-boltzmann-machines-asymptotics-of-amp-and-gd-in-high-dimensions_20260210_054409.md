---
ver: rpa2
title: 'Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in
  High Dimensions'
arxiv_id: '2505.18046'
source_url: https://arxiv.org/abs/2505.18046
tags:
- have
- where
- which
- theorem
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the training dynamics of Restricted Boltzmann
  Machines (RBMs) in the high-dimensional regime where the input dimension grows large
  while the number of hidden units remains fixed. The key insight is that in this
  limit, the RBM training objective can be simplified to an equivalent form resembling
  an unsupervised multi-index model with non-separable regularization.
---

# Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions

## Quick Facts
- arXiv ID: 2505.18046
- Source URL: https://arxiv.org/abs/2505.18046
- Reference count: 40
- Primary result: RBMs achieve optimal weak recovery threshold for spiked covariance data in high dimensions

## Executive Summary
This paper analyzes Restricted Boltzmann Machine training in the high-dimensional regime where input dimension grows large while hidden units remain fixed. The key insight is that RBM training objectives simplify to an equivalent multi-index model form, enabling rigorous analysis using Approximate Message Passing algorithms and dynamical mean-field theory. The authors prove that RBMs achieve optimal computational weak recovery thresholds matching the BBP transition, establishing the first precise mathematical understanding of unsupervised learning in RBMs.

## Method Summary
The authors analyze RBM training on spiked covariance data X = (1/√d)U*Λ(W*)^T + Z using a two-pronged approach. First, they rigorously prove that in the high-dimensional limit with fixed hidden units k, the intractable RBM log-likelihood reduces to a tractable multi-index model with non-separable regularization. Second, they apply Approximate Message Passing (AMP-RBM) algorithms to both train the model and prove theoretical properties about its performance. State evolution equations track the training dynamics of gradient descent, while AMP-RBM provides both a practical algorithm and proof technique for analyzing stationary points.

## Key Results
- RBMs achieve optimal computational weak recovery threshold at BBP transition (λ = α^(-1/4))
- Training dynamics can be precisely characterized by low-dimensional State Evolution equations
- AMP-RBM algorithm effectively distinguishes signals even when their number exceeds hidden units
- Exact computation of partition function via η₂ term enables efficient training for small k

## Why This Works (Mechanism)

### Mechanism 1: High-Dimensional Objective Collapse
As d→∞ with fixed k, the RBM log-likelihood asymptotically reduces to a multi-index model. The partition function is approximated via Taylor expansion, transforming the complex normalization constraint into a penalty η₂ that depends only on the overlap matrix W^T W. This requires separable visible units with zero mean and bounded moments.

### Mechanism 2: Spectral Weak Recovery Threshold
AMP-RBM achieves weak recovery precisely at the BBP transition. Linearizing iterations around W=0 recovers power method on sample covariance matrix. Recovery succeeds when signal eigenvalue separates from noise bulk, requiring αλ⁴ > 1.

### Mechanism 3: Dimension Reduction via State Evolution
High-dimensional GD trajectories are tracked by low-dimensional State Evolution recursion. Mapping GD dynamics to GAMP instance captures correlations through Onsager correction terms, reducing the path to a stochastic process in R^k.

## Foundational Learning

- **Spiked Covariance Model**: Data X = (1/√d)U*Λ(W*)^T + Z is essential for understanding the recovery task. If λ=0, the eigenvalue spectrum of X^T X should show no separation from noise bulk.

- **Approximate Message Passing (AMP)**: Core algorithmic tool with Onsager terms that correct for correlations. The term U^(t-1)B_t^T in AMP iteration Y^t corrects for correlation between W and the residual.

- **Multi-Index Models**: Output depends only on k linear projections of input. Non-separable regularization (η₂(W^T W)) is harder to analyze than separable regularization because it couples columns of W together.

## Architecture Onboarding

- **Component map**: Spiked Covariance Data (X) -> Effective Loss (η₁, η₂) -> AMP-RBM Loop (f, g denoisers) -> State Evolution (SE prediction)

- **Critical path**: 
  1. Verify simplified loss gradient matches full RBM gradient for synthetic data
  2. Implement AMP-RBM denoisers f and g
  3. Run State Evolution to obtain theoretical limit curves
  4. Compare finite-dimensional GD/CD runs against SE prediction

- **Design tradeoffs**:
  - Exactness vs. Speed: Simplified loss allows exact partition function computation for small k, replacing MCMC sampling but limited to k≈20
  - Random vs. Spectral Init: Random initialization matches SE for t≥1, but spectral initialization is needed for immediate recovery near threshold

- **Failure signatures**:
  - Divergence: Low damping parameter ζ causes AMP oscillation
  - Zero Overlap: Below threshold λ<α^(-1/4), algorithm converges to random guessing
  - Numerical Underflow: Computation of η₂ requires log-sum-exp stabilization for large weights

- **First 3 experiments**:
  1. Generate data with varying λ, plot final overlap ζ vs λ to verify transition at λ=α^(-1/4)
  2. Run GD on original RBM likelihood vs simplified loss, compare convergence curves
  3. Set k=5 hidden units but r=2 signals, verify hidden units align with 2 signals while 3 capture noise

## Open Questions the Paper Calls Out
None

## Limitations
- Fixed-k assumption severely limits applicability to practical RBM training where k typically scales with d
- Rademacher prior restriction may not hold for real-world data distributions
- Synthetic data domain leaves gap in understanding when theoretical assumptions break down

## Confidence

**High Confidence**: Objective collapse and State Evolution tracking are rigorously proven within stated assumptions; AMP-RBM stability connection to BBP transition is mathematically sound

**Medium Confidence**: Claim that AMP-RBM achieves optimal recovery thresholds is supported by linear stability analysis but non-linear effects remain uncharacterized

**Low Confidence**: Extension to practical RBM training with large k and real data relies on heuristic arguments without theoretical backing

## Next Checks

1. **Finite-Size Validation**: Systematically test State Evolution predictions against finite-dimensional RBM training (d=1000-5000, k=5-10) to quantify deviations and identify critical d/k ratios

2. **Prior Distribution Impact**: Repeat AMP-RBM experiments with Gaussian priors for both visible and hidden units to verify framework extension and threshold shifts

3. **Real Data Benchmarking**: Implement AMP-RBM on MNIST/Fashion MNIST, compare against standard RBM training methods (CD, PCD) in training speed and final log-likelihood, especially for small k where exact η₂ computation remains tractable