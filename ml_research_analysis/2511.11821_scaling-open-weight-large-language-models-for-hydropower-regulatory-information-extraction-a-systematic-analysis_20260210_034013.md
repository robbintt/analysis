---
ver: rpa2
title: 'Scaling Open-Weight Large Language Models for Hydropower Regulatory Information
  Extraction: A Systematic Analysis'
arxiv_id: '2511.11821'
source_url: https://arxiv.org/abs/2511.11821
tags:
- extraction
- information
- performance
- regulatory
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluated open-weight LLMs (0.6B-70B
  parameters) for extracting information from hydropower regulatory documents. It
  identified a critical 14B parameter threshold where validation methods transition
  from ineffective to viable, enabling consumer-deployable models to achieve 64% F1
  through appropriate validation while smaller models plateau at 51%.
---

# Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis

## Quick Facts
- arXiv ID: 2511.11821
- Source URL: https://arxiv.org/abs/2511.11821
- Reference count: 8
- Primary result: 14B parameter threshold enables viable reflective reasoning validation for LLM-based regulatory extraction

## Executive Summary
This study systematically evaluated open-weight LLMs (0.6B-70B parameters) for extracting information from hydropower regulatory documents, identifying a critical 14B parameter threshold where validation methods transition from ineffective to viable. The analysis revealed that smaller models systematically hallucinate when information is absent, with perfect recall indicating extraction failure rather than success. Large-scale models approach 77% F1 but require enterprise infrastructure, while consumer-deployable models achieve 64% F1 through appropriate validation.

## Method Summary
The study extracted 17 structured fields from FERC hydropower licensing documents across 6 categories using seven open-weight models (Qwen 0.6B–Llama 70B) via Hugging Face. Four extraction methods (single-step, two-step, categorical, chain-of-thought) were tested with three reflective validation strategies (lenient, moderate, stringent). A bronze standard created via GPT-4o mini served as evaluation baseline using semantic comparison against ground truth.

## Key Results
- 14B parameter threshold identified where validation methods transition from ineffective (F1 < 0.15) to viable (F1 = 0.64)
- Smaller models (0.6B-3B) plateau at F1 ≈ 0.51 with validation-induced F1 decreases of 70-80%
- Large-scale models (32B-70B) achieve F1 ≈ 0.74-0.77 but require enterprise infrastructure

## Why This Works (Mechanism)

### Mechanism 1
- Reflective reasoning validation improves extraction quality only above the 14B parameter threshold
- Meta-cognitive self-evaluation capabilities emerge at sufficient parameter scale, enabling reliable assessment of whether extractions are supported by source text
- Core assumption: Parameter count serves as proxy for emergent reasoning capacity
- Evidence: Qwen 14B demonstrated 16.2% F1 improvement with validation vs 70-80% decreases in smaller models

### Mechanism 2
- High recall scores in small models indicate systematic hallucination rather than extraction success
- LLMs lack distinctive features to recognize "negative signals" and generate plausible values when information is absent
- Core assumption: Bronze standard evaluation correctly identifies ground truth absences
- Evidence: Small models (0.6B-3B) achieved recall rates of 0.929-1.000 with precision of 0.151-0.432

### Mechanism 3
- Parameter scaling benefits are non-uniform across information categories
- Well-structured fields benefit from pattern-matching while context-dependent fields require multi-hop reasoning
- Core assumption: Six-category taxonomy captures meaningful difficulty distinctions
- Evidence: Flow Information improved from 0.047 (small models) to 0.674 (large models)

## Foundational Learning

- **Precision-Recall Tradeoff in Extraction Tasks**
  - Why needed: Paper inverts conventional interpretation—high recall signals hallucination risk
  - Quick check: Given recall=0.99 and precision=0.15, would you deploy for compliance verification?

- **Emergent Capabilities in Language Models**
  - Why needed: 14B threshold presented as emergent capability boundary, not smooth performance curve
  - Quick check: What evidence distinguishes "gradual improvement" from "emergent threshold"?

- **Signal Detection Theory Applied to LLM Hallucination**
  - Why needed: Hallucination framed as asymmetric detection failure—detecting positive signals but not absence
  - Quick check: What does perfect recall with low precision indicate about decision criterion?

## Architecture Onboarding

- **Component map:** Extraction layer → Validation layer → Evaluation layer
- **Critical path:** Select ≥14B models for reflective reasoning; use categorical/single-step extraction; apply moderate validation; monitor category-specific performance
- **Design tradeoffs:** Consumer-deployable (14B): F1≈0.64; Enterprise-scale (32B-70B): F1≈0.74-0.77; Sub-threshold (<14B): Max F1≈0.51
- **Failure signatures:** Perfect recall with declining precision → hallucination; two-step F1≈0 → context retention failure; low rejection rates → validation not discriminating
- **First 3 experiments:**
  1. Baseline extraction comparison: Run single-step and categorical extraction on 20 document chunks
  2. Validation threshold calibration: Apply moderate reflective reasoning; adjust based on rejection rates
  3. Category-specific failure analysis: Manually review 10 extractions for categories with F1<0.3

## Open Questions the Paper Calls Out

- Can domain-specific fine-tuning or RAG overcome systematic extraction failure in unstructured regulatory categories?
- How reliable is the Bronze Standard (LLM-as-a-Judge) evaluation methodology compared to human expert annotations?
- What specific hybrid architectural components are required to detect information absence where pure LLMs fail?

## Limitations

- Results based on only seven models from two families, limiting generalizability to other architectures
- Validation method comparison uses only three fixed prompts without exploring prompt variation space
- Bronze standard evaluation introduces systematic uncertainty through semantic matching via GPT-4o mini

## Confidence

- **High:** Smaller models exhibit perfect recall with poor precision indicating systematic hallucination
- **Medium:** 14B parameter threshold for validation viability (mechanism remains hypothesized)
- **Low:** Category-specific scaling patterns (limited category diversity, no cross-domain validation)

## Next Checks

1. Replicate 14B threshold finding using three additional model families (Mistral, Gemma, Phi) to test architecture-agnostic emergence
2. Manually audit 50 bronze standard-identified absences against original FERC documents to quantify false negative rates
3. Apply extraction pipeline to regulatory documents from different domain (environmental permits or building codes) with expert-annotated ground truth