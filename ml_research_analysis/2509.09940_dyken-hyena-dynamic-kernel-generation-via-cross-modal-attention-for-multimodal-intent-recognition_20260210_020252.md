---
ver: rpa2
title: 'DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal
  Intent Recognition'
arxiv_id: '2509.09940'
source_url: https://arxiv.org/abs/2509.09940
tags:
- multimodal
- dynamic
- fusion
- more
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DyKen-Hyena, a novel approach to multimodal
  intent recognition that reframes the problem from feature fusion to processing modulation.
  The core idea is to use non-verbal cues (audio and visual) to dynamically generate
  per-token convolutional kernels that modulate textual feature extraction, allowing
  fine-grained, token-level influence rather than simple feature addition.
---

# DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition

## Quick Facts
- **arXiv ID:** 2509.09940
- **Source URL:** https://arxiv.org/abs/2509.09940
- **Reference count:** 12
- **Primary result:** +10.46% F1-score improvement in out-of-scope detection on MIntRec benchmarks

## Executive Summary
DyKen-Hyena reframes multimodal intent recognition by shifting from feature fusion to processing modulation. Instead of simply concatenating audio/visual features with text, the model generates dynamic convolutional kernels that modulate how text features are processed, allowing token-level influence from non-verbal cues. Using cross-modal attention to create token-aligned context vectors, these are fed into a kernel generator to produce per-token filters integrated into a Hyena sequence model. Evaluated on MIntRec and MIntRec2.0 benchmarks, DyKen-Hyena achieves state-of-the-art performance, particularly excelling at out-of-scope detection and intents heavily reliant on non-verbal context.

## Method Summary
The model employs cross-modal attention where text tokens query aligned audio/visual streams, producing context vectors that feed into a lightweight MLP kernel generator. This generates per-token convolutional kernels that dynamically modulate text feature extraction through a Hyena operator combining short convolution (with generated kernels) and FFT-based long convolution with gating. The modulated embeddings are then processed by BERT for final classification. This architecture allows non-verbal cues to directly influence the mathematical transformation of text features rather than simply being added as supplementary information.

## Key Results
- Achieves state-of-the-art performance on MIntRec and MIntRec2.0 benchmarks
- +10.46% F1-score improvement in out-of-scope detection compared to previous methods
- Substantial accuracy gains on intents heavily reliant on non-verbal context (e.g., Joke, Taunt)
- Ablation shows removing dynamic short convolution causes F1-OOS to drop from 22.73% to 0.0%

## Why This Works (Mechanism)

### Mechanism 1
Processing modulation (steering computation) is more effective for intent recognition than feature fusion (adding representations). The architecture translates non-verbal cues into dynamic convolutional kernels that reshape the text feature extraction process itself, allowing a "sarcastic" tone to mathematically alter how the word "great" is encoded, rather than just appending a "sarcastic" vector to "great."

### Mechanism 2
Token-level dynamic kernels capture fine-grained paralinguistic dependencies that global fusion misses. A cross-modal attention mechanism allows each text token to query aligned audio/visual streams, producing context vectors that are transformed into unique convolutional kernels for each token via a lightweight MLP.

### Mechanism 3
Combining dynamic short convolution with static long convolution provides a necessary balance between local context sensitivity and global sequence coherence. One path uses generated dynamic kernels for local modulation while the other uses fixed long convolution via FFT for global context, merged through a gating mechanism.

## Foundational Learning

- **Dynamic Convolution (Dynamic Filters):** Network parameters can be functions of the input, allowing weights to be generated on the fly based on multimodal context. Quick check: How does the model ensure the kernel changes for every token, and what network generates these weights?

- **Cross-Modal Attention Alignment:** The model must align text tokens with corresponding audio/video frames using Query/Key/Value mechanisms. Quick check: In Eq 1, what acts as the Query and what acts as the Key/Value, and why does that matter for alignment?

- **The Hyena Operator (Sub-Quadratic Attention):** Hyena uses long convolutions (FFT) and gating to approximate attention without the O(N²) cost. Quick check: Why does the model use FFT for the "Long Convolution" path, and how does this differ from standard self-attention?

## Architecture Onboarding

- **Component map:** Raw Text, Audio, Video -> Feature Extractors + Temporal Alignment -> Cross-Modal Attention -> Kernel Generator (MLP) -> Kernel Parameters -> Hyena Operator (Dynamic Short Conv + FFT Long Conv + Gating) -> BERT Encoder -> Classification

- **Critical path:** The flow from Contextualizer -> Generator -> Hyena Short Conv. If the attention is misaligned or the MLP fails to generate distinct kernels, the specific "modulation" advantage is lost.

- **Design tradeoffs:** Kernel Size (Ks=3) provides robust OOS detection; efficiency vs. detail tradeoff where the model adds computational overhead before BERT to allow BERT to focus on higher-order semantic relationships.

- **Failure signatures:** OOS Collapse (F1-OOS drops significantly if dynamic short convolution is broken), Text Dominance (model performs well on textually straightforward classes but fails on Joke or Taunt).

- **First 3 experiments:**
  1. Run w/o-DynamicShortConv ablation on MIntRec2.0 to verify F1-OOS collapse (Table 3)
  2. Train with Kernel Sizes Ks ∈ {1, 3, 5} to confirm Ks=3 provides best balance
  3. Evaluate specifically on "Joke" and "Taunt" intents to confirm dynamic model outperforms MAG-BERT baseline

## Open Questions the Paper Calls Out

- **End-to-end unaligned processing:** The current reliance on explicit feature alignment restricts the model; exploring end-to-end models that handle unaligned sequences is a key area for future research.

- **Sophisticated kernel generators:** The current simple MLP kernel generator could be replaced with more sophisticated architectures (e.g., deeper networks or Transformers) to potentially yield further improvements.

- **Generalization to other tasks:** While the model creates a "fundamentally sound and generalizable intent representation," experiments are restricted to intent recognition benchmarks, leaving open whether the approach transfers to other multimodal tasks.

## Limitations
- Requires explicit temporal alignment of audio/visual features to text, which may be brittle with asynchrony
- Kernel generator architecture is underspecified (MLP details not reported), affecting reproducibility
- Does not directly compare against simple feature fusion alternatives to prove fundamental superiority

## Confidence
- **High Confidence:** The architectural framework is internally consistent and ablation results clearly demonstrate the dynamic kernel mechanism provides unique value for out-of-scope detection
- **Medium Confidence:** The claim that processing modulation is inherently superior to feature fusion is supported by mechanism but not conclusively proven through direct comparison
- **Low Confidence:** Generalization claims to other multimodal tasks beyond intent recognition are speculative without empirical validation

## Next Checks
1. Implement a variant replacing dynamic kernel generation with standard cross-modal attention followed by feature concatenation to compare OOS detection performance
2. Analyze distribution and variance of generated kernels across different examples to determine if they're truly input-dependent
3. Test DyKen-Hyena on a different multimodal task (e.g., emotion recognition) using the same architecture without task-specific modifications