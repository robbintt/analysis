---
ver: rpa2
title: Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation
  in Multi-Center Cohorts
arxiv_id: '2512.06652'
source_url: https://arxiv.org/abs/2512.06652
tags:
- test-time
- training
- feature
- prediction
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaTTT, a test-time adaptation framework
  designed to improve prediction of invasive mechanical ventilation (IMV) need in
  multi-center ICU cohorts. AdaTTT addresses domain shifts in EHR data by integrating
  dynamic self-supervised learning with prototype-guided partial optimal transport.
---

# Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts

## Quick Facts
- arXiv ID: 2512.06652
- Source URL: https://arxiv.org/abs/2512.06652
- Authors: Xiaolei Lu; Shamim Nemati
- Reference count: 16
- Key outcome: AdaTTT achieves highest AUC across three ICU datasets (85.02%, 84.10%, 77.17%) for IMV prediction with improved calibration

## Executive Summary
This paper introduces AdaTTT, a test-time adaptation framework designed to improve prediction of invasive mechanical ventilation (IMV) need in multi-center ICU cohorts. AdaTTT addresses domain shifts in EHR data by integrating dynamic self-supervised learning with prototype-guided partial optimal transport. The method prioritizes informative features via adaptive masking and aligns test-time distributions with source-domain prototypes to avoid overfitting. Experiments on three ICU datasets show that AdaTTT achieves the highest AUC across all sites compared to strong TTT and TTA baselines, with improved calibration scores.

## Method Summary
AdaTTT combines dynamic self-supervised learning with prototype-guided partial optimal transport to adapt models at test time. The framework uses adaptive masking to prioritize informative features during adaptation, while prototype learning maintains a reference distribution of source-domain examples. Partial optimal transport aligns the test-time distribution with these prototypes without requiring access to source data. This dual approach enables effective domain adaptation while preventing overfitting to individual test samples, addressing the challenge of EHR data heterogeneity across multiple clinical sites.

## Key Results
- AdaTTT achieves highest AUC across all three ICU sites: 85.02%, 84.10%, and 77.17%
- Improved calibration with Brier scores of 0.086, 0.085, and 0.106 across sites
- Ablation studies confirm complementary benefits of dynamic masking and prototype learning
- Robust performance under clinical domain shifts across multi-center cohorts

## Why This Works (Mechanism)
AdaTTT addresses domain shift by dynamically adapting model parameters at test time using unlabeled data. The method combines two complementary mechanisms: dynamic masking identifies and prioritizes the most informative features for adaptation, preventing overfitting to noisy or irrelevant features, while prototype-guided partial optimal transport aligns the test-time distribution with source-domain prototypes without requiring access to source data. This dual approach allows the model to maintain its learned knowledge while adapting to local data characteristics, achieving robust performance across heterogeneous clinical environments.

## Foundational Learning
- Domain adaptation: needed because EHR data varies significantly across hospitals; quick check: verify test-time performance degrades without adaptation
- Self-supervised learning: needed to leverage unlabeled test data for adaptation; quick check: confirm unlabeled test samples are available in deployment
- Optimal transport: needed for distribution alignment without source data access; quick check: verify prototype representations capture source distribution
- Adaptive masking: needed to prevent overfitting during test-time adaptation; quick check: measure performance degradation when masking is removed
- Prototype learning: needed to maintain reference distribution for alignment; quick check: validate prototypes generalize across source domains

## Architecture Onboarding

Component Map: Input -> Feature Extractor -> Adaptive Masker -> Prototype Alignment -> Prediction

Critical Path: Test sample → Dynamic feature selection → Prototype-guided OT alignment → Class prediction
The framework processes incoming test samples through feature extraction, applies adaptive masking to identify informative features, aligns these features with source-domain prototypes using partial optimal transport, and generates predictions. This pipeline enables real-time adaptation while maintaining computational efficiency.

Design Tradeoffs: The method trades computational overhead during inference for improved generalization across domains. By avoiding source data access and using prototypes instead, it maintains privacy and reduces data transfer requirements, but requires careful prototype selection and may introduce latency during adaptation.

Failure Signatures: Performance degradation occurs when test-time distributions significantly differ from source domains beyond what prototypes can capture, or when insufficient unlabeled data is available for meaningful adaptation. The adaptive masking may also fail to identify truly informative features in highly heterogeneous data.

First Experiments:
1. Compare AUC with and without adaptive masking on a single-site dataset
2. Evaluate prototype alignment effectiveness using synthetic domain shift
3. Test inference latency impact of partial optimal transport on clinical hardware

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements may not generalize to clinical settings with different patient demographics or care protocols
- Computational overhead during inference could pose practical challenges for real-time clinical deployment
- Reliance on self-supervised learning assumes sufficient unlabeled data at test time, which may not always be available
- Study focuses exclusively on IMV prediction task, limiting generalizability to other clinical outcomes

## Confidence
- High confidence in the methodological framework and technical implementation
- Medium confidence in the clinical relevance of the reported performance gains
- Medium confidence in the generalizability across different clinical prediction tasks
- Low confidence in the practical deployment feasibility without computational cost analysis

## Next Checks
1. Evaluate AdaTTT's performance on additional clinical prediction tasks beyond IMV need to assess generalizability across different ICU outcomes
2. Conduct a computational efficiency analysis comparing inference time and resource requirements against standard test-time adaptation methods
3. Validate the framework using a multi-site prospective cohort to confirm performance stability across diverse clinical environments