---
ver: rpa2
title: Modeling Understanding of Story-Based Analogies Using Large Language Models
arxiv_id: '2507.10957'
source_url: https://arxiv.org/abs/2507.10957
tags:
- story
- analogy
- human
- reasoning
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated Large Language Models' (LLMs) analogical
  reasoning abilities using story-based analogies from prior human cognition research.
  Researchers employed BERT-based sentence embeddings and generative models (GPT-4,
  LLaMA3.1) to evaluate semantic representations and reasoning performance across
  18 analogy problems.
---

# Modeling Understanding of Story-Based Analogies Using Large Language Models

## Quick Facts
- **arXiv ID**: 2507.10957
- **Source URL**: https://arxiv.org/abs/2507.10957
- **Reference count**: 3
- **Primary result**: LLMs achieved up to 91% accuracy on story analogies, with enhanced prompting improving performance particularly for larger models.

## Executive Summary
This study investigates Large Language Models' analogical reasoning capabilities using story-based analogies from human cognition research. Researchers employed BERT-based sentence embeddings and generative models (GPT-4, LLaMA3.1) to evaluate semantic representations and reasoning performance across 18 analogy problems. They compared conventional and enhanced prompting strategies, where enhanced prompts included self-generated hints to better simulate human reasoning. Results showed that larger models achieved higher accuracy, with some exceeding human performance (84.7%), particularly when using enhanced prompts. However, alignment with human performance at the individual item level was unexpectedly higher for smaller models.

## Method Summary
The study used 18 story analogy problems from Gentner et al. (1993), sourced from Webb et al. (2023) repository, each containing a source story, true analogy (shares causal structure), and false analogy (shares first-order relations only). Researchers employed BERT-based sentence embeddings for semantic similarity analysis and tested four generative models (GPT-4o, GPT-4o-mini, LLaMA-3.1-8B, LLaMA-3.1-70B) with two prompting strategies: conventional (single-step) and enhanced (two-step with self-generated hints). Each model ran 100 trials per item with counterbalanced answer ordering. Evaluation metrics included overall accuracy, cosine similarity for embeddings, and Pearson correlation with human performance at individual item level.

## Key Results
- Enhanced prompting improved accuracy across all models, with LLaMA-70B achieving 91.39% accuracy
- Larger models consistently outperformed smaller models, with LLaMA-70B (70B parameters) reaching 85.38% accuracy versus LLaMA-8B (8B parameters) at 65.28%
- GPT-4o-mini showed highest correlation with human item-level performance (r = 0.526), while larger models showed lower correlations despite superior accuracy
- BERT embeddings achieved 78% accuracy in distinguishing true from false analogies but showed no correlation with human item-level difficulty patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Two-step prompting with self-generated hints improves analogical reasoning accuracy in LLMs.
- **Mechanism**: The model first generates hints that explicitly identify causal relationships and analogy-relevant aspects (e.g., revenge motives, event sequences), then uses these hints during inference to guide attention toward higher-order relations rather than surface similarities.
- **Core assumption**: Explicit intermediate reasoning steps help models maintain focus on causal structure over distracting surface features.
- **Evidence anchors**: [abstract] "enhanced prompts included self-generated hints to better simulate human reasoning... Results showed that larger models achieved higher accuracy... particularly when using enhanced prompts"; [results] "the overall performance of both the GPT-4 and Llama 3.1 models is greater for the enhanced prompt than for the conventional prompt"

### Mechanism 2
- **Claim**: Increased model scale enables more effective mapping of complex causal relationships in story analogies.
- **Mechanism**: Larger parameter counts (70B vs. 8B) provide greater representational capacity to encode nuanced patterns such as "actions driven by revenge" and subtle event sequence distinctions that smaller models cannot reliably capture.
- **Core assumption**: Scale translates to improved abstraction capabilities for relational reasoning tasks.
- **Evidence anchors**: [abstract] "larger models achieved higher accuracy, with some exceeding human performance (84.7%)"; [results] "Llama 3.1 model with 70B parameters has a much better accuracy of 0.8538 compared to the 8B parameter model, which has an accuracy of 0.6528"

### Mechanism 3
- **Claim**: BERT-based sentence embeddings capture coarse semantic similarity between analogically related stories but fail to align with human item-level difficulty patterns.
- **Mechanism**: Cosine similarity between source and target embeddings distinguishes true from false analogies at 78% accuracy by capturing aggregate semantic relatedness, but similarity scores for true and false analogies remain highly comparable.
- **Core assumption**: Analogical relationships produce measurable differences in semantic embedding space.
- **Evidence anchors**: [abstract] "used BERT-based sentence embeddings... to evaluate semantic representations"; [results] "the correlation at the item level between human accuracy and the 'true analogy' similarity minus the 'false analogy' similarity... was r = -0.032 (p = 0.899), indicating no agreement"

## Foundational Learning

- **Concept**: Higher-order causal relations vs. first-order relations
  - **Why needed here**: The core task requires distinguishing stories that share surface features and entities from those sharing deep causal structure (e.g., revenge motives). Models must prioritize relational structure over lexical overlap.
  - **Quick check question**: Given two stories with identical entities, can you identify which shares the same causal pattern as a source story?

- **Concept**: Answer-order bias and robustness testing
  - **Why needed here**: Prior work (Lewis & Mitchell, 2024) shows LLMs are sensitive to option ordering (89% vs. 61% accuracy). The study counterbalances ordering across 100 runs per item.
  - **Quick check question**: Why must analogy evaluation counterbalance correct answer position across trials?

- **Concept**: Item-level vs. aggregate performance metrics
  - **Why needed here**: High overall accuracy can mask misalignment with human difficulty patterns. The study uses Pearson correlation across 18 items to assess whether models find the same analogies difficult.
  - **Quick check question**: A model achieves 90% accuracy but correlates r=0.1 with human item-level performance. What does this indicate about its reasoning?

## Architecture Onboarding

- **Component map**: BERT embeddings -> Cosine similarity analysis -> Generative models (GPT-4, LLaMA) -> Prompting layer (conventional/enhanced) -> Evaluation layer (accuracy, correlation)
- **Critical path**: Load 18 analogy items -> Generate BERT embeddings and compute cosine similarities -> Run each model with 100 trials per item (50 counterbalanced orderings each) -> Compare conventional vs. enhanced prompting -> Compute item-level correlations with human baseline data
- **Design tradeoffs**: Enhanced prompting improves overall accuracy (up to ~91% for LLaMA-70B) but may reduce item-level human alignment due to ceiling effects; larger models achieve higher accuracy but show weaker correlation with human difficulty patterns; BERT embeddings are computationally efficient but lack fine-grained causal sensitivity
- **Failure signatures**: Models struggle when false analogies appear as narrative continuations of true analogies (problem 3); performance degrades when causal structure requires tracking event sequences (problem 15); divergence occurs when rejection reasons differ subtly between targets (problem 7)
- **First 3 experiments**:
  1. Replicate conventional prompting baseline with counterbalanced ordering on a single model (e.g., LLaMA-3.1-8B) to establish accuracy and variance estimates
  2. Implement enhanced prompting pipeline: first generate hints, then append to inference prompt; compare accuracy delta
  3. Compute item-level Pearson correlations between model accuracy and human baseline across all 18 items to identify alignment gaps

## Open Questions the Paper Calls Out

- **Open Question 1**: Why do smaller LLMs show higher correlation with human performance at the individual item level despite achieving lower overall accuracy?
  - **Basis in paper**: [explicit] The authors found that GPT-4o-mini achieved the highest correlation with humans (r = 0.526), while larger models like LLaMA-70B had lower correlations despite superior accuracy.
  - **Why unresolved**: The authors note this was surprising but offer no definitive explanation, mentioning only a possible ceiling effect for larger models.
  - **What evidence would resolve it**: Systematic analysis comparing error patterns between model sizes and humans across diverse analogy types; testing whether smaller models' representations better match human semantic spaces.

- **Open Question 2**: What specific features of complex causal relationships in story analogies cause LLMs to fail where humans succeed?
  - **Basis in paper**: [explicit] The authors identified problems 3, 7, 13, and 15 where models performed substantially worse than humans, suggesting difficulties with sequence tracking, conflicting analogies, and character motivation inference.
  - **Why unresolved**: The post-hoc analysis offers hypotheses (e.g., models losing track of rejection reasons, confusing plot continuations) but no systematic characterization of failure modes.
  - **What evidence would resolve it**: Controlled experiments manipulating specific causal complexity factors (event sequencing, multiple character motivations, nested reasoning) to identify which cause the largest human-model performance gaps.

- **Open Question 3**: To what extent does training data contamination explain LLM performance on established analogy benchmarks?
  - **Basis in paper**: [explicit] The authors explicitly raise this concern: "it is possible that these models are 'remembering' the answers to these questions rather than reasoning about them."
  - **Why unresolved**: Lack of transparency in training data composition makes this impossible to determine from current results; the performance improvement with enhanced prompting suggests some genuine reasoning.
  - **What evidence would resolve it**: Testing models on novel analogy problems constructed to be absent from training data; analyzing whether performance patterns differ between likely-contaminated and novel items.

## Limitations
- Enhanced prompting produces ceiling effects that may mask underlying reasoning patterns, making item-level alignment with humans unreliable
- Lack of corpus evidence for BERT embeddings' effectiveness in analogy detection raises questions about whether semantic similarity truly captures causal structure
- Absence of clear aggregation methods for multiple model runs introduces reproducibility concerns

## Confidence
- **High confidence**: Overall accuracy improvements with enhanced prompting and larger model scales are well-supported by direct experimental results
- **Medium confidence**: Claims about BERT embeddings capturing coarse semantic similarity are supported but lack comparative validation against alternative approaches
- **Low confidence**: Item-level human alignment claims are undermined by the same enhanced prompting that drives high accuracy, creating a methodological tension that the paper does not fully resolve

## Next Checks
1. Run a diagnostic study comparing enhanced prompting with and without ceiling effects by introducing more difficult analogy items to test whether the reasoning mechanism generalizes beyond simple problems
2. Implement alternative aggregation methods (majority vote vs. mean probability) across 100 runs to verify that current results are not artifacts of aggregation choices
3. Conduct ablation testing on BERT embedding components by comparing [CLS] token vs. mean pooling and examining layer selection effects on analogy discrimination accuracy