---
ver: rpa2
title: DOLFIN -- Document-Level Financial test set for Machine Translation
arxiv_id: '2502.03053'
source_url: https://arxiv.org/abs/2502.03053
tags:
- test
- translation
- text
- segments
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOLFIN, a novel document-level test set for
  machine translation in the financial domain. Unlike existing test sets that focus
  on sentence-level alignment, DOLFIN presents data in units of sections rather than
  sentences, enabling higher-level phenomena like information reorganization.
---

# DOLFIN -- Document-Level Financial test set for Machine Translation

## Quick Facts
- arXiv ID: 2502.03053
- Source URL: https://arxiv.org/abs/2502.03053
- Reference count: 28
- Key outcome: Document-level test set using section-level alignment that discriminates context-sensitive from context-agnostic models in financial translation

## Executive Summary
DOLFIN introduces a novel document-level test set for machine translation in the financial domain, departing from traditional sentence-level alignment approaches. By organizing data in sections rather than sentences, the test set captures higher-level translation phenomena including information reorganization, sentence reordering, and discourse coherence. The dataset covers five language pairs with approximately 1950 aligned sections per language, containing specialized financial terminology and context-sensitive translation challenges. Experiments demonstrate that larger language models consistently improve when provided with full document context, while smaller models show mixed results, validating DOLFIN's effectiveness at discriminating context-aware translation capabilities.

## Method Summary
DOLFIN creates section-level aligned test sets from financial PDF documents using LASER embeddings for bilingual matching with cosine similarity threshold of 0.75, supplemented by heuristics for titles and document boundaries. The dataset undergoes quality filtering using Comet-kiwi with SLIDE approach, CTXPRO annotation for context-sensitive phenomena detection, and LLM annotation for additional challenges. Evaluation compares per-sentence versus full-segment translation settings using Comet-slide scoring with window size 3 and stride 1. The dataset is publicly released in markdown format preserving tables and headings, containing approximately 1950 segments per language pair across five language combinations.

## Key Results
- Larger models (Llama-3-70b, GPT-4o) showed consistent improvements (+2.0 to +5.57 COMET-slide points) with full context versus per-sentence translation
- Smaller Llama-3.1-8b degraded on 3/5 language pairs when given full context, demonstrating context handling limitations
- Experiments confirmed DOLFIN effectively discriminates context-sensitive from context-agnostic models through score separation
- Dataset revealed consistent weaknesses in handling specialized financial terminology and maintaining formatting consistency

## Why This Works (Mechanism)

### Mechanism 1
Section-level alignment enables evaluation of discourse phenomena that sentence-level test sets cannot capture. By abandoning perfect sentence-to-sentence alignment and using sections (avg. 12.8 sentences) as units, the test set permits natural translation operations—sentence reordering, merging, splitting—that constrained alignments artificially suppress. Real-world translations do not maintain sentence-level correspondence; translators reorganize information across sentence boundaries.

### Mechanism 2
DOLFIN discriminates context-sensitive from context-agnostic models by measuring score differences between per-sentence and full-segment translation settings. Models are evaluated in two contrastive modes: translating sentences individually vs. translating the full segment. Larger models (Llama-3-70b, GPT-4o) showed consistent improvements (+2.0 to +5.57 COMET-slide points) with full context, while smaller Llama-3.1-8b degraded on 3/5 language pairs.

### Mechanism 3
Context-sensitive phenomena identification via CTXPRO + LLM annotation enriches test set difficulty. CTXPRO detects gender, formality, animacy, ellipsis, and ambiguous noun inflections; Llama-3-70b annotates additional phenomena (anaphora, terminology consistency, polysemy). Segments scoring high on context-dependence are prioritized.

## Foundational Learning

- Concept: Document-level MT evaluation challenges
  - Why needed here: Standard sentence-level metrics (BLEU, COMET) assume alignment; document-level requires handling discourse coherence, anaphora, and terminology consistency across segments
  - Quick check question: Can you explain why "coverture" translating to "blanket" vs. "hedge" depends on domain context?

- Concept: SLIDE evaluation approach
  - Why needed here: Quality estimation models like Comet-kiwi have context length limits. SLIDE uses sliding windows (size 3, stride 1) to score longer texts
  - Quick check question: Why does SLIDE require approximate sentence alignment when the test set explicitly avoids it?

- Concept: LASER sentence embeddings for alignment
  - Why needed here: Section alignment uses LASER embeddings with 0.75 cosine similarity threshold to match bilingual content without parallel training data
  - Quick check question: What heuristics supplement embedding similarity when aligning sections without explicit title markers?

## Architecture Onboarding

- Component map: PDF financial documents from Fundinfo.com → Apryse extraction → Markdown sections → LASER embeddings + title/ boundary heuristics → Section pairs → Comet-kiwi (SLIDE), CTXPRO annotation, LLM annotation, manual review → Per-sentence vs. full-segment translation → Comet-slide scoring

- Critical path: PDF extraction quality determines downstream alignment feasibility → Section alignment accuracy gates reference quality for evaluation → Annotation quality determines test set's discriminative power

- Design tradeoffs: Section vs. document units: Sections enable metric compatibility but may split discourse units; Imperfect alignment: Allows natural phenomena but complicates automatic evaluation; LLM annotation: Scalable but introduces model-dependent noise

- Failure signatures: Llama-3.1-8b "downhill" generation: Model produces increasingly degraded text with long context (Section 5.2); Markdown table corruption: Horizontal lines misplaced, rendering tables unreadable; Format inconsistency: Currency formatting defaults to source language conventions

- First 3 experiments:
  1. Baseline context discrimination: Run both per-sentence and full-segment translation on all 5 language pairs; verify score separation between model families
  2. Phenomenon-targeted analysis: Filter to CTXPRO-annotated segments; manually verify that context improvements correlate with annotated phenomena (pronoun agreement, formality consistency)
  3. Metric sensitivity test: Compare Comet-slide scores using correct vs. incorrect references (as done in Section 5.2 with ~44 score drop) to validate alignment quality

## Open Questions the Paper Calls Out

### Open Question 1
Can existing document-level MT evaluation metrics effectively penalize context-related errors and accurately assess translations in the specialized financial domain? Current metrics like COMET are trained primarily on general-domain data and sentence-level evaluations; their effectiveness on context-sensitive financial phenomena remains untested.

### Open Question 2
How can evaluation beyond the sentence level be performed when existing metrics expect perfect sentence-to-sentence alignment? DOLFIN deliberately abandons perfect sentence alignment to enable phenomena like sentence reordering and merging, making standard evaluation methods inapplicable.

### Open Question 3
Would a multi-modal approach combining vision and language improve translation of financial documents with rich visual formatting? Current MT research focuses on plain text; financial documents contain tables, charts, and formatting that convey meaning but introduce noise when extracted as text alone.

## Limitations

- SLIDE evaluation method requires approximate sentence alignment for scoring, creating tension with the test set's design philosophy of avoiding sentence-level alignment
- LLM-based annotation pipeline (CTXPRO + manual review) introduces potential model-dependent bias
- Table formatting quality remains problematic, with horizontal lines often misplaced in markdown representation
- Dataset's financial domain specificity limits generalizability to other specialized domains

## Confidence

- High Confidence: Section-level alignment methodology and its benefits for capturing discourse phenomena
- Medium Confidence: LLM annotation quality for identifying context-sensitive phenomena
- Medium Confidence: Financial terminology handling capabilities

## Next Checks

1. Manually verify a random sample of CTXPRO-annotated segments to assess false positive/negative rates for context-sensitive phenomena identification

2. Test DOLFIN-trained models on document-level MT tasks in non-financial domains to evaluate domain transfer limitations

3. Conduct human judgments comparing per-sentence vs. full-segment translations on DOLFIN segments to validate whether automatic metrics capture the same quality differences