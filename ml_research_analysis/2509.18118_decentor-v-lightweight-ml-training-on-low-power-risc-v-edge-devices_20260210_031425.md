---
ver: rpa2
title: 'Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices'
arxiv_id: '2509.18118'
source_url: https://arxiv.org/abs/2509.18118
tags:
- training
- risc-v
- l-sgd
- learning
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends L-SGD, a lightweight stochastic gradient descent
  optimizer, to RISC-V-based microcontroller units (MCUs), enabling on-device machine
  learning training in resource-constrained environments. It introduces a hybrid quantized
  training strategy that performs most operations in 8-bit fixed-point arithmetic
  while retaining 32-bit floating-point precision only for critical computations,
  addressing numerical stability challenges in quantized training.
---

# Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices

## Quick Facts
- arXiv ID: 2509.18118
- Source URL: https://arxiv.org/abs/2509.18118
- Reference count: 34
- Enables 4× memory reduction and 2.2× training speedup on RISC-V MCUs through hybrid quantized training

## Executive Summary
This work extends L-SGD, a lightweight stochastic gradient descent optimizer, to RISC-V-based microcontroller units (MCUs), enabling on-device machine learning training in resource-constrained environments. It introduces a hybrid quantized training strategy that performs most operations in 8-bit fixed-point arithmetic while retaining 32-bit floating-point precision only for critical computations, addressing numerical stability challenges in quantized training. The implementation achieves nearly 4× reduction in memory usage and a 2.2× speedup in training time on RISC-V platforms compared to 32-bit floating-point baselines, with negligible accuracy degradation across tested datasets. The approach successfully demonstrates stable training convergence on both Arm and RISC-V platforms, establishing RISC-V as a viable architecture for decentralized, privacy-preserving machine learning at the edge.

## Method Summary
Decentor-V implements a hybrid 8-bit quantized training approach for RISC-V MCUs, extending the L-SGD optimizer with node delta optimization. The method uses PULP-NN kernels for forward pass operations in int8, while selectively dequantizing to float32 during backpropagation for loss calculation and gradient updates. Models are initialized with pre-trained weights to prevent gradient saturation in the early training phases. The implementation includes custom software math functions (fast_exp, fast_round, fast_power_of_two) to avoid costly standard math library calls on FPU-less hardware. Training uses fully-connected neural networks on tabular datasets (CogDist and CarEvaluation) with the GAP-8 RISC-V MCU as the target platform.

## Key Results
- Achieves 4× memory reduction compared to 32-bit floating-point baselines through 8-bit quantization
- Demonstrates 2.2× training speedup on RISC-V platforms with negligible accuracy loss
- Successfully validates stable convergence on both Arm and RISC-V architectures
- Maintains precision, recall, and F1-score metrics close to float32 baseline implementations

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Quantized Backpropagation
Performing forward passes in int8 while selectively reverting critical backward-pass operations to float32 mitigates precision loss and saturation errors on FPU-less hardware. The system executes inference and forward activations using efficient int8 arithmetic via PULP-NN kernels. During backpropagation, it dequantizes the output to float32 to compute loss and gradients, preventing the clipping of large error signals that would occur in a purely integer pipeline. The computational overhead of dynamic quantization/dequantization and partial float32 emulation is lower than the cost of running full float32 backpropagation in software.

### Mechanism 2: Node Delta Optimization (L-SGD)
Aggregating errors at the neuron level rather than per connection reduces memory overhead sufficient for MCU constraints. Standard SGD stores gradient information per weight. L-SGD optimizes this by computing a "node delta" (aggregated error) which is then propagated back, drastically reducing the state required for the optimizer during backpropagation. The model architecture (compact DNNs) is simple enough that approximating gradients via node-level aggregation does not destabilize convergence.

### Mechanism 3: Pre-trained Initialization for Quantization Stability
Initializing the quantized network with pre-trained weights is required to prevent gradient saturation during the early stages of training. Early training from scratch produces large loss values. In an int8 pipeline, these large values are clipped (saturated) when mapped to the fixed range, destroying gradient resolution. Pre-training ensures the model starts in a stable error basin, keeping gradients within the representable int8 range. The edge device operates in a Federated Learning or personalization context where a base model is already available or transmitted to the device.

## Foundational Learning

**Fixed-Point vs. Floating-Point Arithmetic**
Why needed here: The core value proposition relies on replacing expensive software-emulated float32 operations with fast int8 integer operations. Understanding the trade-off in precision (resolution) vs. range is essential.
Quick check question: Can you explain why multiplying two large int8 values might require scaling or shifting to prevent overflow?

**Backpropagation & Gradient Descent**
Why needed here: To understand why the hybrid approach exists, one must grasp that backpropagation relies on the chain rule where small precision errors can accumulate. The paper specifically isolates this phase for float32 handling.
Quick check question: During backpropagation, does the gradient flow from the input layer to the output layer, or vice versa?

**Federated Learning (FL) Context**
Why needed here: The paper frames its utility within FL (decentralized training). Understanding that the device receives a global model and performs local "fine-tuning" explains the constraint of requiring pre-trained weights.
Quick check question: In FL, does the raw data leave the local device to update the global model?

## Architecture Onboarding

**Component map:**
Forward Pass -> int8 PULP-NN kernels (modified for single-neuron support) -> LUT activation functions
Backward Pass -> Custom fast_exp/fast_round/fast_power_of_two -> Selective Dequantization logic
Optimizer -> L-SGD logic implementing node-delta aggregation

**Critical path:**
The execution flow is not symmetric. The critical latency path is the Backward Pass. Specifically, the step where int8 activations are dequantized to float32 to compute gradients, and then re-quantized. This is the "hybrid" overhead that must be minimized to beat pure float32 emulation.

**Design tradeoffs:**
- Memory vs. Convergence: Using pure int8 saves ~4x memory but fails to converge from scratch. The hybrid approach adds slight runtime overhead (dequantization) to buy convergence stability.
- Portability vs. Speed: The code relies on standard RISC-V ISA, not vendor-specific extensions (e.g., SIMD), ensuring portability but potentially missing peak throughput optimizations available in vendor libraries.

**Failure signatures:**
- Accuracy Collapse: If trained from scratch, accuracy hovers near 0% or diverges due to gradient saturation.
- Slow Convergence: If FPU emulation is triggered too frequently (e.g., failing to use the fast_ math approximations), latency will spike significantly above the reported 9-17ms.

**First 3 experiments:**
1. Baseline Validation: Port the L-SGD float32 implementation to the target RISC-V board and verify convergence against the paper's reported accuracy (e.g., CogDist dataset) to validate the toolchain.
2. Quantization Stress Test: Attempt to train the same model using pure int8 (removing the hybrid dequantization steps) to empirically observe the saturation and divergence described in Section 3.3.
3. Latency Profiling: Measure the clock cycles for the fast_exp implementation vs. standard math library exp to quantify the specific speedup gained by the software optimizations.

## Open Questions the Paper Calls Out

**Open Question 1**
Can quantization techniques be refined to enable stable training from scratch on RISC-V MCUs without relying on pre-trained weights?
Basis in paper: Future work includes "exploring more robust quantization techniques" to address numerical limitations.
Why unresolved: The current hybrid approach suffers from error saturation during early training phases, making it unstable for training from scratch.
Evidence: Successful convergence of a randomly initialized model using the int8 pipeline.

**Open Question 2**
How can the Decentor-V framework be extended to support Convolutional Neural Networks (CNNs) on resource-constrained RISC-V devices?
Basis in paper: The authors state future work will "extend to support neural network architectures beyond DNNs, such as convolutional neural networks (CNNs)."
Why unresolved: The current implementation and evaluation are strictly limited to Fully-Connected (Dense) neural networks.
Evidence: Convergence results and memory footprint analysis for CNN training on target hardware.

**Open Question 3**
How does the performance of quantized L-SGD scale when applied to larger datasets and RISC-V hardware with Floating-Point Units (FPUs)?
Basis in paper: The conclusion calls for "broadening empirical evaluation" and evaluating on platforms "including those with hardware FPUs."
Why unresolved: The study is currently limited to two small datasets and a single FPU-less MCU configuration (GAP-8).
Evidence: Benchmarking results on high-dimensional data and FPU-equipped RISC-V cores.

## Limitations
- Requires pre-trained initialization, cannot train from scratch due to int8 saturation errors
- Validated only on small fully-connected networks for tabular datasets, not scalable to CNNs or deep architectures
- Implementation-specific to GAP-8 and PULP-NN, requiring significant reimplementation for other RISC-V MCUs

## Confidence

**High Confidence Claims:**
- The 4× memory reduction is directly measurable from the quantization strategy
- The 2.2× training speedup is supported by measured latencies on the specific hardware platform
- The hybrid quantization approach is technically sound for the stated model class

**Medium Confidence Claims:**
- Generalization to other RISC-V platforms without extensive reimplementation
- Performance maintenance on more complex architectures beyond the tested fully-connected networks

**Low Confidence Claims:**
- From-scratch training feasibility (explicitly stated as infeasible by the authors)
- Cross-platform performance consistency without empirical validation

## Next Checks

1. **FPU-Emulation Boundary Test:** Implement the same training pipeline using pure software float32 emulation on the target RISC-V MCU and measure the exact performance difference to verify the claimed 2.2× speedup is not platform-specific.

2. **Scalability Validation:** Apply the hybrid quantized L-SGD approach to a small CNN (e.g., MNIST classification) to empirically test whether node-delta optimization scales or causes accuracy collapse in deeper architectures.

3. **From-Scratch Failure Reproduction:** Attempt to train a model from random initialization using the hybrid approach to confirm the saturation behavior described in Section 3.3, documenting the exact point where gradients become unusable.