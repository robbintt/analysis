---
ver: rpa2
title: 'Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems
  That Use Tools'
arxiv_id: '2505.16113'
source_url: https://arxiv.org/abs/2505.16113
tags:
- tool
- uncertainty
- entropy
- framework
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for quantifying uncertainty
  in tool-calling large language model (LLM) systems, addressing the gap in existing
  uncertainty quantification methods that fail to account for both LLM and external
  tool contributions. The framework models the tool-calling process as a sequential
  system and derives uncertainty metrics by jointly considering the predictive uncertainty
  of both components.
---

# Tools in the Loop: Quantifying Uncertainty of LLM Question Answering Systems That Use Tools

## Quick Facts
- arXiv ID: 2505.16113
- Source URL: https://arxiv.org/abs/2505.16113
- Reference count: 25
- Primary result: STA metrics outperform baseline approaches in most cases, with AUROC scores demonstrating their effectiveness in predicting response accuracy, particularly in settings where the LLM's internal knowledge is insufficient and external tools are required.

## Executive Summary
This paper introduces a novel framework for quantifying uncertainty in tool-calling large language model (LLM) systems, addressing the gap in existing uncertainty quantification methods that fail to account for both LLM and external tool contributions. The framework models the tool-calling process as a sequential system and derives uncertainty metrics by jointly considering the predictive uncertainty of both components. Key innovations include extending semantic entropy to tool-calling scenarios and proposing efficient approximations (Strong Tool Approximation) that make uncertainty computation practical for real-world applications. The authors evaluate their approach on two synthetic QA datasets derived from well-known machine learning datasets (IRIS flower classification and PIMA diabetes) that require tool-calling for accurate answers, as well as a proof-of-concept experiment on retrieval-augmented generation (RAG) systems. Results show that the proposed STA metrics outperform baseline approaches in most cases, with AUROC scores demonstrating their effectiveness in predicting response accuracy, particularly in settings where the LLM's internal knowledge is insufficient and external tools are required.

## Method Summary
The framework quantifies uncertainty in tool-calling LLMs by decomposing the total system uncertainty into additive contributions from the LLM's generation and the tool's predictive uncertainty. It assumes a sequential factorization p(y,z,a|x) = p(y|z,x)p(z|a)p(a|x) and derives uncertainty metrics through entropy decomposition. The Strong Tool Approximation (STA) provides a tractable metric by assuming the final answer strongly depends on the tool output and the tool call is predictable from the input alone. Semantic entropy is extended to tool-calling scenarios by clustering final answers into semantic equivalence classes using an entailment model. The framework is evaluated on synthetic QA datasets (IRIS and PIMA-derived) and a RAG task using Llama-3-8B-Instruct, Llama-3.1-8B-Instruct, and Mistral-7B-Instruct models.

## Key Results
- STA metrics outperform baseline approaches in most cases, with AUROC scores demonstrating their effectiveness in predicting response accuracy
- Framework achieves 0.89-0.95 AUROC on synthetic QA tasks where tools are required for accurate answers
- Strong Tool Approximation provides practical uncertainty computation while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Total system uncertainty in tool-calling LLMs can be decomposed into additive contributions from the LLM's generation and the tool's predictive uncertainty.
- Mechanism: The joint distribution is factorized sequentially as p(y,z,a|x) = p(y|z,x)p(z|a)p(a|x). Predictive entropy H(y|x) is then expressed as H(y|z,x) + H(z|a) + H(a|x) minus posterior correction terms H(z|y,a) and H(a|x,y).
- Core assumption: The final response y is conditionally independent of the tool call a given the tool output z (Markov property).
- Evidence anchors:
  - [abstract] "quantifies uncertainty by jointly considering the predictive uncertainty of the LLM and the external tool"
  - [section 3] Equation (1) and (2) formalize the decomposition
  - [corpus] Related work on semantic entropy (Kuhn et al., 2023) validates entropy over meanings, but does not address tool contributions
- Break condition: When posterior terms H(z|y,a) or H(a|x,y) are large (e.g., multi-round tool calls, ambiguous tool outputs), the decomposition requires explicit estimation of these terms.

### Mechanism 2
- Claim: The Strong Tool Approximation (STA) provides a tractable uncertainty metric by assuming the final answer strongly depends on the tool output.
- Mechanism: Under conditions where y strongly depends on z and a is predictable from x alone, the posterior correction terms become negligible. STA computes: STA^P(x) = H(y|z,x) + H(z|a) and STA^S(x) = H(C|z,x) + H(z|a).
- Core assumption: (1) The question is nearly unanswerable without the tool but obvious given its output; (2) All information needed for the tool call a is contained in the input x.
- Evidence anchors:
  - [abstract] "proposing efficient approximations (Strong Tool Approximation) that make uncertainty computation practical"
  - [section 3.2] Formal derivation of STA conditions and equations (6)-(7)
  - [corpus] No direct corpus validation; related work focuses on single-component uncertainty
- Break condition: When the LLM can answer without the tool, or when the tool call requires reasoning beyond the input (e.g., multi-hop reasoning), STA degrades.

### Mechanism 3
- Claim: Semantic entropy can be extended to tool-calling systems by clustering final answers into semantic equivalence classes.
- Mechanism: Sample multiple final answers given fixed tool output, cluster by semantic equivalence using an entailment model, then compute entropy over the distribution of semantic classes rather than token sequences.
- Core assumption: An entailment model can reliably determine semantic equivalence between generated answers; the tool output z is fixed during sampling.
- Evidence anchors:
  - [abstract] "extending semantic entropy to tool-calling scenarios"
  - [section 3.1] Equation (3) and (5) define semantic entropy H(C|x) with tool terms
  - [corpus] Farquhar et al. (Nature, 2024) validated semantic entropy for hallucination detection in standard LLMs
- Break condition: When the tool output itself varies across samples, or when semantic equivalence is difficult to establish (e.g., numerical answers with small differences).

## Foundational Learning

- Concept: Predictive Entropy (Shannon entropy over model outputs)
  - Why needed here: Core mathematical tool for quantifying uncertainty in both LLM token sequences and tool outputs; required to understand equations (2)-(7).
  - Quick check question: Given a binary classifier with probabilities [0.9, 0.1], can you compute its predictive entropy?

- Concept: Conditional Independence (Markov chains)
  - Why needed here: The framework assumes y ⊥ a | z; understanding this is essential to see why the decomposition is valid and when it breaks.
  - Quick check question: If knowing z makes y independent of a, what happens to the joint distribution p(y,z,a)?

- Concept: Semantic Equivalence / Entailment
  - Why needed here: Semantic entropy relies on clustering answers by meaning, not tokens; you need to understand how entailment models determine if two responses express the same answer.
  - Quick check question: Are "Yes, the patient has diabetes" and "The diagnosis is positive for diabetes" semantically equivalent?

## Architecture Onboarding

- Component map:
  - Input Prompt (x) → LLM → Tool Call (a) → External Tool → Tool Output (z) → LLM → Final Answer (y)
  - Uncertainty flows: H(a|x) from first LLM call, H(z|a) from tool, H(y|z,x) from second LLM call

- Critical path:
  1. Feature extraction from prompt to construct tool call (correct formatting, correct arguments)
  2. Tool execution with known or estimable uncertainty H(z|a)
  3. Final answer generation conditioned on tool output
  4. Uncertainty computation via sampling (N=10 samples recommended) and entropy estimation

- Design tradeoffs:
  - **STA vs. Full Entropy**: STA is fast (O(N) samples) but assumes strong tool dependence; full entropy requires posterior estimation which is noisy with limited data
  - **White-box vs. Black-box tools**: Framework assumes access to tool uncertainty H(z|a); black-box tools require approximation
  - **Single vs. Multi-round tools**: STA degrades with multi-round tool calling; full framework requires more complex modeling

- Failure signatures:
  - **STA underperforms baseline**: Indicates weak dependence between y and z (LLM may ignore tool or use internal knowledge)
  - **High variance in entropy estimates**: Insufficient samples or poor posterior approximation
  - **Semantic entropy no better than predictive entropy**: Entailment model may be unreliable for the domain

- First 3 experiments:
  1. Replicate IRIS QA experiment: Train a simple classifier on IRIS, construct synthetic QA pairs, measure AUROC of STA^S vs. semantic entropy baseline using Llama-3-8B-Instruct
  2. Ablate tool uncertainty: Set H(z|a) = 0 (deterministic tool) vs. high entropy; verify STA captures tool contribution
  3. Test RAG extension: Implement soft retrieval distribution over documents, compare STA vs. final-answer-only entropy on BoolQ subset; document when STA improvement is smaller

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Strong Tool Approximation be refined to maintain accuracy in multi-round tool-calling scenarios where errors from early rounds accumulate?
- Basis in paper: [explicit] "The framework is flexible and can be extended to various LLM agent architectures, though the Strong Tool Approximation may degrade in multi-round tool-calling scenarios." and "Addressing this degradation may require further refinement of the approximation techniques, especially in domains requiring long chains of tool calls."
- Why unresolved: The STA assumes strong dependence between y and z, which breaks down when early tool-call errors propagate through subsequent rounds.
- What evidence would resolve it: A modified STA or alternative approximation that achieves comparable AUROC scores to single-round settings in benchmark tasks requiring 3+ sequential tool calls.

### Open Question 2
- Question: Does the framework's effectiveness generalize to larger LLMs (e.g., 70B+ parameter models) with different tool-use behaviors?
- Basis in paper: [explicit] "Future work should explore applying our framework to larger LLMs to confirm its scalability and evaluate its performance in more demanding scenarios."
- Why unresolved: Experiments were limited to 7-8B models due to computational constraints; larger models may exhibit different uncertainty calibration or tool-reliance patterns.
- What evidence would resolve it: Experiments replicating the IRIS QA, Diabetes QA, and RAG tasks on models like Llama-70B or GPT-4 showing comparable or improved AUROC for STA metrics.

### Open Question 3
- Question: Can the framework be extended to black-box tools where predictive entropy H(z|a) is unknown and cannot be directly obtained?
- Basis in paper: [inferred] The paper states "we make the key assumption that we are operating in the white-box setting, where the uncertainty of external tools is known." Real-world APIs often lack uncertainty outputs.
- Why unresolved: The framework critically depends on known tool entropy; dropping this assumption requires either estimation techniques or alternative uncertainty signals.
- What evidence would resolve it: A method for reliably estimating tool uncertainty from input-output observations alone, validated on tools where ground-truth uncertainty is available for comparison.

## Limitations

- Synthetic data dependency: All experiments use synthetically generated QA datasets derived from tabular classification tasks, which may not reflect real-world tool-calling scenarios with ambiguous or multi-modal tool outputs.
- Strong Tool Approximation fragility: STA's effectiveness depends critically on assumptions that may break down in multi-hop reasoning, iterative tool use, or when LLMs leverage internal knowledge.
- Posterior estimation bottleneck: The framework requires estimating posterior terms for the full uncertainty decomposition, introducing significant variance and computational overhead with the proposed 30-example training approach.

## Confidence

- **High Confidence**: The theoretical decomposition of total system uncertainty into LLM and tool components (Mechanism 1) is mathematically sound and well-justified.
- **Medium Confidence**: STA's effectiveness in synthetic settings (AUROC scores of 0.89-0.95) is demonstrated, but its performance on real-world data with complex tool outputs is unknown.
- **Low Confidence**: The framework's behavior with multi-round tool calling, black-box tools (no access to H(z|a)), and heterogeneous tool outputs (mixed text, numbers, structured data) remains largely unexplored.

## Next Checks

1. **Real-World Tool-Calling Benchmark**: Evaluate STA on a production tool-calling system (e.g., a coding assistant using external APIs) where tool outputs include error messages, partial results, and structured data. Compare STA's uncertainty predictions against actual failure rates.

2. **Posterior Estimation Ablation**: Systematically vary the number of examples (0, 10, 30, 100) used to train the posterior model and measure the impact on full entropy vs. STA performance. This quantifies the practical cost of the more complete uncertainty decomposition.

3. **Multi-Round Tool Calling Stress Test**: Design a multi-step reasoning task (e.g., planning a trip with sequential tool calls for flights, hotels, and activities) and evaluate whether STA's assumptions break down when the tool call depends on intermediate reasoning rather than just the original input.