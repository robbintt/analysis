---
ver: rpa2
title: Evaluating Compositional Generalisation in VLMs and Diffusion Models
arxiv_id: '2508.20783'
source_url: https://arxiv.org/abs/2508.20783
tags:
- clip
- diffusion
- relational
- cube
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether diffusion model-based classifiers
  exhibit improved compositional generalisation compared to traditional discriminative
  vision-language models (VLMs). The authors extend the Concept Binding Benchmark
  to evaluate models on attribute-object binding and relational composition tasks
  in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings
  using synthetic images.
---

# Evaluating Compositional Generalisation in VLMs and Diffusion Models

## Quick Facts
- arXiv ID: 2508.20783
- Source URL: https://arxiv.org/abs/2508.20783
- Reference count: 16
- Primary result: Diffusion Classifier achieves 99.47% accuracy on single-object OOD generalization, but all models struggle with relational tasks in GZSL settings

## Executive Summary
This paper evaluates compositional generalization capabilities of vision-language models (VLMs) and diffusion-based classifiers on synthetic datasets requiring attribute-object binding and relational composition. The study extends the Concept Binding Benchmark to test three model families—Diffusion Classifier, CLIP, and ViLT—across zero-shot and generalized zero-shot learning settings. Results show Diffusion Classifier excels at single-object generalization while ViLT performs best on two-object tasks, but all models exhibit severe failures on relational reasoning tasks, with GZSL relational accuracy dropping to chance levels. Analysis reveals that relational concepts like "left" and "right" are insufficiently disentangled in text embeddings, suggesting fundamental limitations in current VLM architectures for compositional understanding.

## Method Summary
The study evaluates three VLMs (CLIP, ViLT, Diffusion Classifier) on extended Concept Binding Benchmark with synthetic CLEVR-style images containing 8 colors, 4 shapes, and 2 spatial relations. Models are fine-tuned on training splits and tested on in-distribution and out-of-distribution data in zero-shot (ZSL) and generalized zero-shot (GZSL) settings. GZSL includes hard negatives via attribute-swapping and relational variant distractors. The Diffusion Classifier uses Stable Diffusion U-Net for noise prediction conditioned on text, CLIP uses dual-encoder contrastive learning, and ViLT employs joint vision-language transformer attention. Fine-tuning uses LoRA for ViLT, DreamBooth for DC, and positive-only contrastive loss for CLIP.

## Key Results
- Diffusion Classifier achieves 99.47% accuracy on single-object OOD generalization, outperforming CLIP (91.21%) and ViLT (77.18%)
- ViLT excels on two-object tasks with 99.26% ZSL OOD accuracy, substantially outperforming CLIP (80.15%) and DC (72.80%)
- All models fail dramatically on relational tasks in GZSL setting, with accuracy dropping to ~20% (chance level for 5-way classification)
- CLIP text embeddings show "left" and "right" relations cluster together, indicating insufficient disentanglement of relational concepts

## Why This Works (Mechanism)

### Mechanism 1: Generative Classification via Denoising Score Matching
- Claim: Diffusion Classifier achieves better single-object OOD generalization by estimating conditional likelihood through iterative denoising rather than direct discriminative mapping.
- Mechanism: The classifier samples noise vectors and iteratively denoises them conditioned on text prompts, then ranks prompts by reconstruction error—this forces the model to learn a generative process that must explicitly model attribute-object coherence rather than learning decision boundaries that may rely on spurious correlations.
- Core assumption: Generative modeling captures compositional constraints that discriminative contrastive learning may overlook or shortcut.
- Evidence anchors: [abstract] "Diffusion Classifier shows the strongest performance on single-object generalization"; [section 4.1] DC-FT achieves 99.47% on OOD test vs CLIP-FT's 91.21% and ViLT-FT's 77.18%; [corpus] Related work on diffusion-based zero-shot classification remains limited; no direct corpus corroboration for this specific mechanism
- Break condition: When tasks require fine-grained relational discrimination (GZSL relational), generative approach fails similarly to discriminative models—DC-FT achieves only 38.25% on OOD test.

### Mechanism 2: Joint Vision-Language Transformer Attention for Object Binding
- Claim: ViLT's unified transformer architecture without region supervision enables superior multi-object compositional binding compared to CLIP's dual-encoder design.
- Mechanism: ViLT processes image patches and text tokens jointly in a single transformer, allowing direct cross-attention between visual regions and linguistic elements—this enables binding of attributes to specific objects that CLIP's separated encoders cannot achieve through their single similarity score interaction.
- Core assumption: Early and dense multimodal fusion enables compositional reasoning that late/sparse fusion cannot replicate.
- Evidence anchors: [section 4.2-4.3] ViLT-FT achieves 99.26% on two-object ZSL OOD test, substantially outperforming CLIP-FT (80.15%) and DC-FT (72.80%); [section 4.3] ViLT-FT maintains 83.46% on two-object GZSL OOD test while CLIP-FT drops to 23.38%; [corpus] No direct corpus evidence comparing ViLT to CLIP on compositional tasks; gap in related work
- Break condition: Relational reasoning fails despite architectural advantages—ViLT-FT achieves only 25.50% on GZSL relational OOD test, near 20% chance level.

### Mechanism 3: Text Embedding Space Entanglement Limits Relational Reasoning
- Claim: Relational failures stem from insufficient separation in text embedding space rather than visual processing limitations alone.
- Mechanism: CLIP's text encoder produces similar embeddings for relational variants (e.g., "cube left sphere" clusters with "sphere left cube," "cube right sphere"), creating an irrecoverable ambiguity at the representation level—visual evidence cannot disambiguate concepts that are conflated in embedding space.
- Core assumption: Compositional understanding requires disentangled representations of relational primitives before visual grounding can succeed.
- Evidence anchors: [abstract] "Analysis of CLIP embeddings reveals that relational concepts are not well-separated, suggesting that the difficulty may stem from overly similar representations of relational concepts such as left and right"; [section 5] "the closest neighbours of cube left sphere are sphere left cube, cube right sphere and sphere right cube"; [corpus] CoPatch paper confirms "CLIP...struggles with understanding spatial relationships"
- Break condition: Fine-tuning fails to disentangle—post-fine-tuning t-SNE shows relational prompts still cluster by noun ordering or bag-of-words similarity rather than actual spatial meaning.

## Foundational Learning

- **Concept: Compositional Generalization**
  - Why needed here: The entire benchmark tests whether models can recombine learned primitives (colors, shapes, relations) into novel combinations—the defining feature of systematic generalization that current VLMs lack.
  - Quick check question: If a model learns "red cube" from training examples and "blue sphere" from other examples, should it correctly identify "blue cube" at test time? What failure mode would produce "blue sphere" instead?

- **Concept: Zero-Shot vs. Generalized Zero-Shot Learning**
  - Why needed here: ZSL tests pure generalization to unseen classes in isolation; GZSL tests whether models can simultaneously discriminate unseen from seen classes—the latter exposes overfitting and bias toward familiar labels that ZSL masks.
  - Quick check question: Why does CLIP-FT maintain 80.15% on two-object ZSL OOD but collapse to 23.38% on GZSL OOD? What does this reveal about its representations?

- **Concept: Bag-of-Words vs. Structured Representations**
  - Why needed here: The paper's central diagnosis is that VLMs treat captions as unordered word sets (bag-of-words) rather than structured compositions—understanding this explains the systematic attribute-swapping errors.
  - Quick check question: Given an image of "red cube + blue cylinder," a bag-of-words model might output "blue cube" or "red cylinder." Why does it not produce "green sphere"?

## Architecture Onboarding

- **Component map:**
  - Diffusion Classifier: Stable Diffusion U-Net (noise prediction) + CLIP text encoder → likelihood estimation from denoising error
  - CLIP: ViT image encoder + transformer text encoder → cosine similarity matching
  - ViLT: Single transformer → image patches (linear projection) + text tokens with full cross-attention
  - Fine-tuning: LoRA adapters for ViLT, DreamBooth for DC, contrastive loss for CLIP

- **Critical path:**
  1. Dataset preparation: Generate CLEVR-style images ensuring OOD splits have truly novel attribute-shape combinations (no label overlap with training)
  2. Model fine-tuning: Use positive-only examples for CLIP (to avoid negative-example contamination); DreamBooth for DC with 4000-5000 steps
  3. Evaluation: Run ZSL (unseen labels only) then GZSL (seen + unseen) with hard negatives (attribute-swapped distractors)
  4. Diagnosis: t-SNE visualization of text/image embeddings to identify representation entanglement

- **Design tradeoffs:**
  - Synthetic images: Enable controlled compositional testing without spurious correlations, but may not reflect real-world complexity
  - Positive-only fine-tuning for CLIP: Reduces overfitting (per paper's findings) but limits ability to learn discriminative boundaries
  - DreamBooth for Diffusion Classifier: Accessible open-source method but constrains fine-tuning to positive examples only

- **Failure signatures:**
  - Relational near-chance: GZSL relational accuracy hovering near 20% (5-way classification) indicates complete relational concept failure
  - Sharp ID-to-OOD drop: CLIP-FT dropping from 75.43% (ID) to 23.38% (OOD) on two-object GZSL indicates overfitting to training distribution
  - Attribute swap errors: Systematic misattribution (red cube → blue cube when blue sphere present) indicates bag-of-words behavior

- **First 3 experiments:**
  1. Replicate single-object GZSL: Fine-tune all three models and verify DC > CLIP > ViLT on OOD generalization (Table 2 pattern) to validate your pipeline
  2. Embedding probe: Compute pairwise cosine similarity for all relational prompts (e.g., "X left Y" vs "X right Y") across shapes to quantify entanglement before/after fine-tuning
  3. Hard negative ablation: Run two-object GZSL with random distractors vs. attribute-swapped distractors to isolate binding failures from general recognition issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the poor relational reasoning performance be addressed through pre-training objectives that explicitly require compositional understanding, rather than attempting to address compositionality through fine-tuning?
- Basis in paper: [explicit] Authors state that "these should be considered at the pre-training stage rather than expecting users to fine-tune for these fundamental semantic abilities" and mention "training on datasets with more explicit compositional objectives" as a potential avenue.
- Why unresolved: All models struggle with relational GZSL despite fine-tuning, suggesting current pre-training strategies do not require models to learn compositional knowledge.
- What evidence would resolve it: Pre-training a VLM with a compositional objective (e.g., contrastive learning over permuted attribute-relation combinations) and evaluating on the GZSL relational task.

### Open Question 2
- Question: Would VLMs exhibit the same pattern of failures in compositional generalization when tested on more than two objects, or does complexity scale non-linearly?
- Basis in paper: [explicit] Authors state: "Another interesting avenue for future research would be to expand the experiments to include more than two-objects."
- Why unresolved: The current benchmark only tests single-object and two-object scenarios; it remains unclear whether relational reasoning degrades gracefully or collapses with additional objects.
- What evidence would resolve it: Extending the benchmark to 3+ object scenes with relational queries (e.g., "cube left of sphere left of cone") and evaluating all three model families.

### Open Question 3
- Question: What specific architectural or training properties enable ViLT's superior two-object compositional binding compared to CLIP and Diffusion Classifier?
- Basis in paper: [inferred] ViLT-FT achieves 99%+ on two-object tasks but only 63.5% on single-object OOD, a puzzling discrepancy that the authors note but do not explain mechanistically.
- Why unresolved: The paper shows ViLT's superior performance but does not analyze whether this stems from its transformer architecture, co-attention mechanism, or pre-training data.
- What evidence would resolve it: Ablation studies on ViLT's architecture components or analysis of attention patterns during two-object vs. single-object inference.

## Limitations

- Synthetic data domain gap: CLEVR-style images with limited vocabulary may not reflect real-world visual complexity where background clutter and occlusions compound compositional challenges
- Model architecture specificity: Results may reflect implementation choices rather than fundamental architectural advantages across broader model families
- Limited relational vocabulary: Only two spatial relations tested; more complex real-world applications require richer relational concepts

## Confidence

- **High Confidence**: All models fail on relational tasks in GZSL settings with consistent near-chance performance; text embedding entanglement provides plausible explanation
- **Medium Confidence**: Diffusion Classifier's single-object and ViLT's two-object superiority appears robust within synthetic domain but mechanisms require broader validation
- **Low Confidence**: Attribution of relational failures primarily to text embedding entanglement may be incomplete; visual processing limitations could contribute significantly

## Next Checks

1. **Cross-Dataset Generalization**: Evaluate same models on real-world compositional datasets (e.g., Compositional Objects and Relations Dataset) to test synthetic-domain advantage transfer to naturalistic visual complexity

2. **Relational Vocabulary Expansion**: Extend benchmark to include additional spatial relations (above, below, inside, between) and temporal relations to determine whether failures are specific to left/right or represent broader compositional bottleneck

3. **Multimodal Representation Analysis**: Conduct ablation studies comparing text-only, image-only, and multimodal performance on relational tasks to quantify relative contributions of visual versus textual processing limitations to compositional failures