---
ver: rpa2
title: Training-Free Watermarking for Autoregressive Image Generation
arxiv_id: '2505.14673'
source_url: https://arxiv.org/abs/2505.14673
tags:
- image
- index
- watermark
- uni00000013
- indices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IndexMark introduces a training-free watermarking framework for
  autoregressive image generation models. It exploits codebook redundancy by pairing
  similar indices and replacing selected ones to embed watermarks without image quality
  degradation.
---

# Training-Free Watermarking for Autoregressive Image Generation

## Quick Facts
- **arXiv ID**: 2505.14673
- **Source URL**: https://arxiv.org/abs/2505.14673
- **Reference count**: 40
- **Primary result**: Training-free watermarking framework achieving >99% verification accuracy while preserving image quality and robustness to common attacks

## Executive Summary
IndexMark introduces a training-free watermarking framework for autoregressive image generation models that exploits codebook redundancy to embed watermarks without image quality degradation. The method pairs similar indices in the VQ-VAE codebook and replaces selected ones during decoding, creating detectable statistical patterns. A specialized Index Encoder improves index reconstruction accuracy for verification, while a cropping-robust validation scheme enhances resilience to image perturbations. Experiments demonstrate high verification accuracy (>99%) and strong resilience to attacks including blur, noise, JPEG compression, and cropping, while maintaining superior image quality compared to existing methods.

## Method Summary
IndexMark operates by first creating index pairs from the VQ-VAE codebook using maximum weight perfect matching (Blossom algorithm with top-K pruning), then randomly assigning each pair to "red" or "green" lists. During autoregressive generation, red indices are selectively replaced with their paired green counterparts based on model confidence scores, shifting the green-index rate from ~50% (random) toward ~100% (watermarked). Watermark verification uses statistical analysis of the green-index proportion, with an Index Encoder trained to improve index reconstruction accuracy. The framework achieves training-free watermarking while maintaining high image quality and robustness to various attacks.

## Key Results
- Verification accuracy >99% on clean images across different model settings
- PSNR improvement of 1.1dB over BitMark at 95% confidence level
- Strong robustness to blur, noise, JPEG compression, brightness changes, random erasing, and cropping attacks
- Superior image quality preservation compared to existing watermarking methods

## Why This Works (Mechanism)

### Mechanism 1: Codebook Redundancy Exploitation via Match-Then-Replace
Replacing generated indices with semantically similar counterparts from the codebook produces negligible visual differences while embedding detectable statistical patterns. The codebook in VQ-VAE contains redundant vectors—different indices mapping to visually similar features. IndexMark pairs similar indices using maximum-weight perfect matching (Blossom algorithm with top-K pruning), randomly assigns each pair to "red" or "green" lists, then replaces red indices with their paired green counterparts during decoding. This shifts the green-index rate from ~50% (random) toward ~100% (watermarked).

### Mechanism 2: Confidence-Guided Selective Replacement
Using model confidence scores to filter replacement candidates preserves image quality better than random replacement. During generation, the autoregressive model produces classification probabilities for each token. For each red-green pair, compute relative confidence: `log(P(red) / P(green))`. Only replace pairs where this ratio is below a quantile threshold (e.g., 95th percentile). High-confidence predictions indicate the model strongly prefers that token—replacing it risks quality degradation.

### Mechanism 3: Statistical Verification via Index Encoder
Watermark presence can be verified by reconstructing indices from the image and testing if green-index rate exceeds a statistical threshold derived from the null hypothesis (no watermark). Under no watermark, green-index rate follows Binomial(n, 0.5), approximated by Normal for large n. IndexMark trains a specialized "Index Encoder" to reconstruct indices more accurately than the original VQ-VAE encoder, using MSE loss on both latent vectors and reconstructed images. Verification computes the green proportion and compares to a confidence interval (e.g., 99.9% level).

## Foundational Learning

- **Concept: Vector Quantization (VQ-VAE) and Discrete Image Tokens**
  - Why needed here: IndexMark operates entirely in the discrete token space—understanding how images become indices and back is foundational.
  - Quick check question: Given an image encoded to latent z, how does VQ-VAE determine which codebook index to assign at each spatial location?

- **Concept: Autoregressive Image Generation**
  - Why needed here: The watermark is embedded during the sequential token generation process; understanding next-token prediction is essential.
  - Quick check question: In an autoregressive model, how does the prediction for token i depend on tokens 1 through i-1?

- **Concept: Maximum Weight Perfect Matching (Blossom Algorithm)**
  - Why needed here: The index pairing problem is formulated as graph matching; knowing why this is optimal helps understand design constraints.
  - Quick check question: Why can't simple greedy nearest-neighbor pairing guarantee globally optimal pairings across the entire codebook?

## Architecture Onboarding

- **Component map:**
  - Codebook Pairing Module -> Watermark Embedding (Inference) -> VQ-VAE Decoder -> Watermarked Image
  - Perturbed Image -> Index Encoder -> Reconstructed Indices -> Green-Index Rate Calculation -> Statistical Verification
  - Index Encoder (Training) <- Frozen VQ-VAE Encoder/Codebook/Decoder

- **Critical path:**
  1. Precompute index pairs (one-time, O(N²) similarity computation + Blossom on sparse graph).
  2. During inference: generate tokens -> compute relative confidence -> replace red with green -> decode.
  3. For verification: encode image with Index Encoder -> count green indices -> statistical test.

- **Design tradeoffs:**
  - Watermark strength vs. image quality: Higher replacement quantile = more green indices = stronger watermark but potential quality loss. Paper uses ~95% as default.
  - Training-free vs. Index Encoder: Fully training-free is possible (original encoder works at lower confidence), but Index Encoder improves high-confidence verification. Trade-off: ~20 epochs of lightweight training.
  - K in top-K pruning: Lower K = faster pairing but potentially suboptimal matches. Paper uses K=10.

- **Failure signatures:**
  - Visible artifacts: Relative confidence threshold too aggressive; high-confidence tokens being replaced.
  - Low verification accuracy on clean images: Index Encoder undertrained or codebook pairs not similar enough.
  - Cropping attacks succeed: Traversal scheme not covering all patch alignments; increase enumeration range.
  - False positives (non-watermarked images detected): Threshold set too low; raise confidence level (e.g., from 99.9% to 99.99%).

- **First 3 experiments:**
  1. Validate pairing quality: Randomly sample 20 index pairs; decode images using each index in isolation; compute PSNR/SSIM between the two decoded images. Confirm visual similarity.
  2. Calibrate replacement threshold: Generate images with replacement quantiles [0%, 50%, 80%, 95%, 100%]; plot PSNR vs. green-index rate to find the knee point.
  3. Test Index Encoder necessity: Compare verification accuracy at 99.9% confidence using original VQ-VAE encoder vs. trained Index Encoder on 100 watermarked images with and without JPEG compression.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can advanced codebook matching strategies (beyond pairwise) improve watermarked image fidelity or payload capacity? The current implementation maximizes similarity for pairs, but does not explore grouping multiple similar indices (n-ary matching) which might offer different trade-offs between watermark strength and image degradation.

- **Open Question 2**: Does index reconstruction based on image semantics enhance watermark robustness compared to the current VQ-VAE encoder? The current Index Encoder relies on pixel-level reconstruction losses (MSE), which may remain susceptible to misalignment or semantic-destroying attacks that current semantic encoders could resist.

- **Open Question 3**: Is IndexMark robust against adaptive "color-swapping" attacks where an adversary explicitly targets the red/green index distribution? While robust to noise, the watermark relies on a statistical bias (green > red); an attacker with knowledge of the codebook division could theoretically apply a reverse mapping to destroy the watermark.

## Limitations
- **Codebook redundancy assumption**: The core mechanism assumes codebook vectors have sufficient cosine similarity to enable perceptual substitution, which is not verified across different VQ-VAE architectures or datasets.
- **Confidence-score correlation**: The confidence-guided replacement assumes that model confidence correlates with perceptual importance, but this is plausible but not empirically validated.
- **Statistical threshold calibration**: The 99.9% confidence interval threshold is chosen to minimize false positives, but the optimal balance between false positives and false negatives across diverse image content is not validated.

## Confidence
- **High confidence**: Verification accuracy claims (>99% on clean images), PSNR/SSIM preservation metrics, robustness to blur/noise/JPEG attacks
- **Medium confidence**: The Index Encoder's necessity and contribution—while experiments show improvement, the exact performance gap without it under various conditions is not fully characterized
- **Low confidence**: Generalization across different autoregressive models beyond LlamaGen—the paper focuses on one model, and codebook redundancy properties may vary

## Next Checks
1. **Pairing quality validation**: Decode 100 random images using only index i and its paired index j (without watermark replacement), compute average PSNR/SSIM between the two outputs to empirically verify codebook redundancy.

2. **Confidence calibration study**: Generate 1000 images with different replacement quantiles (0%, 50%, 80%, 95%, 100%), measure the correlation between relative confidence scores and actual perceptual impact on image quality.

3. **Cross-model generalization**: Apply IndexMark to a different autoregressive model (e.g., DALL-E 2 or Stable Diffusion) and verify if similar codebook redundancy and watermarking effectiveness are maintained.