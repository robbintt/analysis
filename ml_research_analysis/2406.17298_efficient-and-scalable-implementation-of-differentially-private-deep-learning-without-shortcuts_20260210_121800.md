---
ver: rpa2
title: Efficient and Scalable Implementation of Differentially Private Deep Learning
  without Shortcuts
arxiv_id: '2406.17298'
source_url: https://arxiv.org/abs/2406.17298
tags:
- batch
- size
- dp-sgd
- throughput
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the computational efficiency of differentially
  private deep learning using DP-SGD with Poisson subsampling. It identifies that
  many implementations incorrectly use shuffling instead of Poisson subsampling, weakening
  privacy guarantees.
---

# Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts
## Quick Facts
- arXiv ID: 2406.17298
- Source URL: https://arxiv.org/abs/2406.17298
- Reference count: 40
- Primary result: DP-SGD with proper Poisson subsampling can achieve 69% of ideal linear speedup on 80 GPUs, outperforming non-private training

## Executive Summary
This paper identifies a critical flaw in many differentially private deep learning implementations: using shuffling instead of Poisson subsampling, which weakens privacy guarantees. The authors propose Masked DP-SGD in JAX to maintain correct Poisson sampling without recompilation overhead. Through extensive benchmarking, they demonstrate that efficient implementations can reduce the computational overhead of DP-SGD from 8× slower to roughly 2.6× slower compared to non-private training. Most importantly, they show that DP-SGD scales better than non-private training when distributed across multiple GPUs, achieving near-linear speedups.

## Method Summary
The authors develop Masked DP-SGD to address recompilation issues in JAX when using variable batch sizes from Poisson sampling. This approach uses masking to simulate different batch sizes without changing the actual batch size, avoiding the need to recompile the computation graph. They benchmark multiple DP-SGD frameworks (Opacus, Diffprivlib, TensorFlow Privacy, JAXOpt) and optimizations including Ghost Clipping and lower precision training. The evaluation uses ResNet-18 on CIFAR-10 and Transformer models on IMDb datasets, comparing both computational efficiency and scaling behavior across different GPU configurations.

## Key Results
- Naive DP-SGD implementations with Opacus are 2.6-8× slower than non-private training
- Ghost Clipping and TF32 precision optimizations can roughly halve the computational overhead
- DP-SGD achieves up to 69% of ideal linear speedup with 80 GPUs versus 53% for non-private training
- Masked DP-SGD in JAX eliminates recompilation overhead while maintaining correct Poisson sampling
- Proper Poisson subsampling is critical for maintaining differential privacy guarantees

## Why This Works (Mechanism)
Masked DP-SGD works by precomputing a fixed-size batch but applying a binary mask to simulate different effective batch sizes drawn from a Poisson distribution. This eliminates the need for JAX to recompile the computation graph when batch sizes vary between iterations. The mask is generated independently for each training step, ensuring the same privacy guarantees as true Poisson sampling while maintaining computational efficiency. This approach preserves the stronger privacy guarantees of Poisson subsampling (which allows data reuse across epochs with proper accounting) compared to shuffling methods that can violate privacy bounds when data is reused.

## Foundational Learning
- Differential Privacy: A framework for quantifying and limiting information leakage about individuals in datasets
  - Why needed: Provides the theoretical foundation for private machine learning
  - Quick check: Verify that (ε,δ)-DP bounds are correctly computed using the Rényi DP accountant
- Poisson Subsampling: Random sampling of examples with replacement for each batch
  - Why needed: Stronger privacy guarantees than shuffling when data is reused across epochs
  - Quick check: Confirm batch composition probability matches Poisson distribution
- Rényi Differential Privacy: A relaxation of differential privacy using Rényi divergence
  - Why needed: Enables tighter privacy accounting when composing multiple DP mechanisms
  - Quick check: Verify Rényi divergence calculations between adjacent datasets
- Gradient Clipping: Normalizing gradient norms to limit sensitivity
  - Why needed: Essential for bounding the impact of individual examples on model updates
  - Quick check: Confirm per-example gradients are clipped before averaging
- Ghost Clipping: Efficient implementation using sparse updates
  - Why needed: Reduces computational overhead of gradient clipping operations
  - Quick check: Verify sparse gradient updates maintain correctness
- Mixed Precision Training: Using lower precision (TF32) for computations
  - Why needed: Reduces memory bandwidth and increases throughput
  - Quick check: Confirm numerical stability is maintained with reduced precision

## Architecture Onboarding
- Component Map: Data -> Poisson Sampling -> Gradient Computation -> Clipping -> Aggregation -> Model Update -> Privacy Accounting
- Critical Path: The gradient computation and clipping operations dominate runtime in DP-SGD
- Design Tradeoffs: Poisson sampling vs shuffling (privacy vs implementation simplicity), precision vs numerical stability
- Failure Signatures: Privacy budget exhaustion, numerical instability in gradient clipping, suboptimal scaling due to communication overhead
- First Experiments:
  1. Compare privacy loss between Poisson sampling and shuffling on small dataset
  2. Benchmark Ghost Clipping vs standard clipping on a single GPU
  3. Measure scaling efficiency of DP-SGD vs non-private training on 2-8 GPUs

## Open Questions the Paper Calls Out
- How does the masking approach in Masked DP-SGD affect convergence compared to true Poisson sampling?
- What is the impact of different privacy accounting methods on the overall privacy-utility tradeoff?
- How do the scaling benefits of DP-SGD extend to heterogeneous GPU clusters with varying compute capabilities?
- Can the performance improvements from Ghost Clipping and lower precision training be combined with other DP-SGD optimizations?

## Limitations
- No quantitative data on how widespread the shuffling shortcut issue is in production systems
- Performance comparisons based on specific frameworks may not generalize to all setups
- Overhead of Masked DP-SGD implementation not fully characterized against simpler alternatives
- Framework-specific performance variations suggest implementation factors may dominate theoretical claims

## Confidence
- High confidence in the identification of shuffling vs Poisson subsampling discrepancy and its privacy implications
- Medium confidence in the computational efficiency improvements from Ghost Clipping and lower precision training
- Medium confidence in the scaling benefits of DP-SGD over non-private training
- Low confidence in the generalizability of framework-specific performance numbers

## Next Checks
1. Audit multiple published DP-SGD implementations to quantify the prevalence of the shuffling shortcut issue
2. Benchmark Masked DP-SGD against alternative batching strategies across different hardware configurations
3. Test the scaling claims on larger GPU clusters (beyond 80 GPUs) to verify the trend holds at greater scales