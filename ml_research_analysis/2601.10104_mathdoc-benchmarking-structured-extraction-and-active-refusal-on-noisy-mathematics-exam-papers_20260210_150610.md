---
ver: rpa2
title: 'MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics
  Exam Papers'
arxiv_id: '2601.10104'
source_url: https://arxiv.org/abs/2601.10104
tags:
- refusal
- question
- arxiv
- text
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathDoc, the first benchmark for structured
  extraction from noisy real-world high school mathematics exam papers. The dataset
  contains 3,609 carefully curated questions with authentic visual artifacts and includes
  unrecognizable samples to explicitly evaluate active refusal behavior.
---

# MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers

## Quick Facts
- arXiv ID: 2601.10104
- Source URL: https://arxiv.org/abs/2601.10104
- Reference count: 34
- Introduces MathDoc, the first benchmark for structured extraction from noisy real-world high school mathematics exam papers with 3,609 curated questions and active refusal evaluation

## Executive Summary
This paper introduces MathDoc, a novel benchmark designed to evaluate structured extraction capabilities and active refusal behavior of MLLMs on noisy mathematics exam papers. The dataset comprises 3,609 carefully curated questions featuring authentic visual artifacts and includes unrecognizable samples to explicitly test models' ability to refuse illegible inputs. A comprehensive multi-dimensional evaluation framework assesses stem accuracy, visual similarity, and refusal capability. Experiments with state-of-the-art models like Qwen3-VL and Gemini-2.5-Pro demonstrate strong extraction performance but reveal a critical reliability gap: models consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. This highlights the need for improved robustness in MLLMs operating under degraded document conditions.

## Method Summary
The authors developed MathDoc by curating 3,609 high school mathematics exam questions with authentic visual noise and artifacts. They established a multi-dimensional evaluation framework to assess model performance across stem accuracy, visual similarity, and refusal capability. The benchmark specifically includes unrecognizable samples to test active refusal behavior. State-of-the-art MLLMs were evaluated using standardized prompts and scoring metrics, with results compared across the three evaluation dimensions to identify strengths and weaknesses in handling noisy document inputs.

## Key Results
- MathDoc establishes the first benchmark for structured extraction from noisy real-world mathematics exam papers
- MLLMs demonstrate strong extraction performance on legible questions but fail to refuse illegible inputs
- Models consistently produce confident but invalid outputs when encountering unrecognizable samples, revealing a critical reliability gap

## Why This Works (Mechanism)
None provided

## Foundational Learning

### Visual Document Understanding
**Why needed:** Essential for processing degraded mathematics exam papers with noise and artifacts
**Quick check:** Can model extract text from low-quality scanned documents with handwritten notes and smudges

### Active Refusal in MLLMs
**Why needed:** Critical safety feature to prevent generation of confident but incorrect outputs
**Quick check:** Does model recognize and refuse to answer when input is illegible or ambiguous

### Multi-dimensional Evaluation Framework
**Why needed:** Comprehensive assessment requires metrics beyond simple accuracy
**Quick check:** Can framework distinguish between different types of extraction failures and refusal behaviors

## Architecture Onboarding

### Component Map
Data Curation -> Benchmark Creation -> Model Evaluation -> Results Analysis

### Critical Path
1. Question selection and noise injection
2. Structured extraction testing
3. Refusal capability assessment
4. Performance aggregation and analysis

### Design Tradeoffs
- Real-world noise vs. controlled degradation patterns
- Comprehensive evaluation vs. practical testing constraints
- Model performance vs. refusal reliability

### Failure Signatures
- Confident generation on illegible inputs
- Structure preservation despite visual noise
- Inconsistent refusal across different degradation types

### First Experiments
1. Baseline extraction accuracy on clean vs. noisy documents
2. Refusal rate comparison across different degradation levels
3. Visual similarity metrics for preserved vs. altered layouts

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset limited to 3,609 high school mathematics questions, potentially missing broader document diversity
- Focus on mathematics exams may not generalize to other document types with different structural layouts
- Experiments conducted on limited MLLM selection, restricting conclusions about field-wide behavior

## Confidence

| Claim | Confidence |
|-------|------------|
| Dataset creation methodology and evaluation framework | High |
| Reported extraction performance on benchmark | Medium |
| Conclusion about consistent refusal failures | Medium |

## Next Checks
1. Expand benchmark to include documents from other domains (scientific papers, legal documents) to assess generalizability
2. Test broader range of MLLMs including fine-tunable open-source models to determine if refusal behavior improves with adaptation
3. Conduct ablation studies on evaluation framework to identify most critical assessment dimensions for practical applications