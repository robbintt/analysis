---
ver: rpa2
title: Context information can be more important than reasoning for time series forecasting
  with a large language model
arxiv_id: '2502.05699'
source_url: https://arxiv.org/abs/2502.05699
tags:
- time
- series
- prompting
- forecasting
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the effectiveness of prompting techniques
  for time series forecasting using large language models, specifically GPT-4o-mini.
  The research evaluates various prompting strategies including zero-shot and one-shot
  chain-of-thought, plan-and-solve, long-short-term prompting, and a novel SARIMA-based
  approach.
---

# Context information can be more important than reasoning for time series forecasting with a large language model

## Quick Facts
- arXiv ID: 2502.05699
- Source URL: https://arxiv.org/abs/2502.05699
- Reference count: 21
- Primary result: Context information often outperforms complex reasoning prompts for LLM-based time series forecasting

## Executive Summary
This study investigates prompting techniques for time series forecasting using large language models, specifically evaluating GPT-4o-mini with various strategies including zero-shot/one-shot chain-of-thought, plan-and-solve, long-short-term prompting, and SARIMA-based approaches. Experiments on PISA and IHEPC datasets reveal that simple baseline prompts with contextual information frequently achieve performance comparable to or better than more complex reasoning prompts. The research identifies key LLM weaknesses: difficulty following multi-step procedures, calculation errors with algebraic operations, and failure to properly utilize trend and seasonality information. The findings suggest that providing appropriate context information may be more crucial than sophisticated reasoning prompts for effective time series forecasting with LLMs.

## Method Summary
The study evaluates multiple prompting strategies on time series forecasting using GPT-4o-mini. Short series (PISA datasets) predict next 1 value; long series (IHEPC) predict next 6 values. Seven prompting methods are tested: Baseline (PromptCast-style with domain + time resolution), Zero/One-shot Chain-of-Thought, Zero-shot Plan-and-Solve+, Zero/One-shot SARIMA (decomposition into trend/seasonality/short-term), and Zero-shot Long-Short-Term. Performance is measured using RMSE and MAE, with RMSE*/MAE* computed on successfully parsed samples across all methods. Data includes PISA (3 subsets: Singapore visitors, Connecticut temperature, electricity consumption - each 15 timesteps) and IHEPC household electricity consumption (Global_intensity, hourly averaged, 3000 series of length 96, shifted every 10 points).

## Key Results
- No single prompting method consistently outperforms others across all datasets
- Simple baseline prompts with contextual information often achieve performance comparable to or better than complex reasoning prompts
- One-shot SARIMA showed worst performance across datasets due to inaccurate decomposition and failure to follow prescribed multi-step procedures
- Baseline prompting achieved best RMSE on SG dataset (9.665) and competitive results across all datasets

## Why This Works (Mechanism)

### Mechanism 1: Context Information Priming Over Reasoning Complexity
- Claim: Providing proper context information can achieve forecasting performance comparable to or better than complex reasoning prompts
- Mechanism: Contextual metadata activates relevant world knowledge embedded in the LLM, enabling implicit pattern recognition without forcing explicit reasoning steps
- Core assumption: LLMs possess embedded temporal and domain knowledge from pre-training that can be retrieved via semantic cues
- Evidence anchors: Abstract states context alone can achieve performance comparable to best-performing prompt; baseline achieved best RMSE on SG dataset

### Mechanism 2: Numerical Tokenization Fragmentation
- Claim: BPE-based tokenization splits multi-digit numbers into multiple tokens, degrading numerical operation accuracy
- Mechanism: Tokenization breaks numerical relationships, creating interference where non-numeric tokens act as noise to algebraic operations
- Core assumption: Numerical reasoning requires treating numbers as atomic units; token fragmentation disrupts this
- Evidence anchors: Paper notes LLMs often fail to calculate accurately with algebraic operations; Tiktoken vocabulary covers 0-999, larger numbers split unpredictably

### Mechanism 3: Procedure Following Failure Under Structured Prompts
- Claim: Complex prompting strategies often degrade performance because LLMs fail to follow prescribed multi-step procedures accurately
- Mechanism: LLMs exhibit internal conflicts between reasoning steps described in prompt and its own learned reasoning steps
- Core assumption: LLMs have ingrained reasoning priors that resist externally imposed procedural structures
- Evidence anchors: One-shot SARIMA showed worst performance due to inaccurate decomposition and failing to subtract trend before computing seasonality

## Foundational Learning

- **Time Series Decomposition (Trend, Seasonality, Residual)**
  - Why needed here: SARIMA prompting attempts to impose this structure; understanding why it fails requires knowing what each component represents
  - Quick check question: If a time series has values [10, 12, 14, 16, 18, 20] with a trend of +2 per step and seasonality of 0, what is the residual at step 3?

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: All compared methods build on or respond to CoT principles; understanding baseline reasoning behavior is prerequisite
  - Quick check question: What are the three error types identified with standard CoT prompting in the paper?

- **Byte Pair Encoding (BPE) Tokenization**
  - Why needed here: Root cause of numerical calculation failures; understanding tokenization helps diagnose when LLM-based forecasting will struggle
  - Quick check question: Under Tiktoken with vocabulary 0-999, how would 45,678 be tokenized?

## Architecture Onboarding

- Component map: Input Layer (time series values + context template) -> Prompting Layer (strategy selector) -> Tokenization (BPE-based) -> LLM Core (GPT-4o-mini) -> Output Parser (regex-based extraction)

- Critical path: 1. Context injection (domain + temporal metadata) -> 2. Prompt strategy assembly -> 3. Numerical tokenization -> 4. LLM inference -> 5. Response parsing with error handling

- Design tradeoffs:
  - Simple vs. complex prompting: Baseline is more robust; complex prompts may excel on specific datasets but risk procedure-following failures
  - One-shot demonstrations: Can improve performance or severely degrade it depending on alignment with LLM's training distribution
  - Format constraint strength: Strong format requirements aid parsing but may interfere with reasoning; paper used "weak requests" to avoid interference

- Failure signatures:
  - Magnitude errors (2-3x): Failing to subtract trend/seasonality before combining components
  - Quantized forecasting: Discrete output values (observed on ECL dataset)
  - Negative predictions: From over-reliance on recent downward trends
  - Incomplete responses: Predicting 5 steps instead of requested 6 (LST prompting)
  - High missing rates: LST had difficulty integrating short/long-term predictions into final output

- First 3 experiments:
  1. Context ablation test: Compare baseline with full context vs. minimal context on your domain to quantify context value
  2. Numerical scaling test: Scale time series to single-token range vs. raw values to isolate tokenization effects
  3. Prompt complexity sweep: Test baseline → zero-shot CoT → one-shot CoT on a held-out validation set, tracking both RMSE/MAE and missing response rate

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on specific datasets (PISA with 15-timestep sequences and IHEPC household electricity consumption) that may not generalize to longer or more complex time series patterns
- Reliance on automated parsing of LLM outputs using regex introduces measurement uncertainty through missing or malformed responses that vary by prompting strategy
- One-shot examples were "refined through several rounds of feedback from ChatGPT," introducing potential variability that wasn't fully characterized

## Confidence

**High Confidence:** The observation that complex prompting strategies (particularly SARIMA decomposition) frequently fail due to the LLM's inability to follow multi-step procedures accurately. This is directly observable from quantitative results showing one-shot SARIMA consistently producing worst RMSE across all datasets.

**Medium Confidence:** The claim that simple baseline prompts with proper context can achieve performance comparable to or better than complex reasoning prompts. While supported by results, this conclusion is limited by specific datasets and model used.

**Low Confidence:** The broad generalization that context information is "more important than reasoning" for time series forecasting with LLMs. This overstates findings which actually show no single approach dominates across all datasets.

## Next Checks

1. **Cross-Dataset Generalization Test:** Replicate prompting strategy comparison on time series datasets with different characteristics (longer sequences, different domains, non-physical phenomena) to determine if context superiority holds beyond evaluated datasets.

2. **Numerical Tokenization Impact Assessment:** Systematically vary magnitude of time series values while keeping same underlying patterns to quantify how BPE tokenization fragmentation affects numerical reasoning accuracy.

3. **Context Sensitivity Analysis:** Conduct ablation study removing individual context components (domain information, temporal resolution, date metadata) to identify which contextual elements contribute most to performance gains.