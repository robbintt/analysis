---
ver: rpa2
title: Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability,
  Data, and Metric Perspectives
arxiv_id: '2501.04003'
source_url: https://arxiv.org/abs/2501.04003
tags:
- driving
- answer
- vlms
- object
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether Vision-Language Models (VLMs) can
  reliably generate visually grounded, interpretable driving decisions in autonomous
  driving. The authors introduce DriveBench, a benchmark dataset that tests VLM reliability
  across 17 settings (clean, corrupted, and text-only inputs) with 19,200 frames,
  20,498 QA pairs, and 12 popular VLMs.
---

# Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives

## Quick Facts
- arXiv ID: 2501.04003
- Source URL: https://arxiv.org/abs/2501.04003
- Reference count: 40
- Primary result: VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs.

## Executive Summary
This paper evaluates whether Vision-Language Models (VLMs) can reliably generate visually grounded, interpretable driving decisions in autonomous driving. The authors introduce DriveBench, a benchmark dataset that tests VLM reliability across 17 settings (clean, corrupted, and text-only inputs) with 19,200 frames, 20,498 QA pairs, and 12 popular VLMs. They find that VLMs often generate plausible responses based on general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior is concealed by dataset imbalances and insufficient evaluation metrics, posing significant risks in safety-critical scenarios. The study proposes refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding, and highlights the potential of leveraging VLMs' awareness of corruptions to enhance reliability.

## Method Summary
The study evaluates VLMs across 17 settings (clean, 15 corruptions, text-only) on 19,200 frames and 20,498 QA pairs from DriveLM-nuScenes. Corruption types include weather, external, sensor failures, motion blur, and transmission errors. Evaluation uses GPT-3.5-turbo with task-specific rubrics for open-ended responses and accuracy for MCQs. VLM inference uses vLLM with temperature=0.2, top-p=0.2, max_tokens=512. DriveLM-Agent is fine-tuned with LLaMA-Adapter-V2 on 8×A800 GPUs.

## Key Results
- VLMs achieve similar accuracy on text-only inputs as with clean images, indicating reliance on language priors rather than visual grounding
- Dataset imbalances mask grounding failures, with majority class guessing achieving >70% accuracy on some tasks
- GPT-based evaluation reveals nuanced differences in visual grounding that standard metrics miss

## Why This Works (Mechanism)

### Mechanism 1: Text-Driven Hallucination via Language Priors
VLMs frequently generate plausible yet fabricated driving decisions by relying on general knowledge and textual cues rather than visual evidence, particularly when visual inputs are degraded or missing. The model decouples response generation from visual encoder, defaulting to statistical correlations found in pre-training data or prompt text.

### Mechanism 2: Metric Masking via Dataset Bias
Standard evaluation metrics fail to detect visual grounding failures because they are biased by imbalanced datasets where "safe" or "common" answers achieve high accuracy by chance. Evaluation protocols reward linguistic similarity without verifying if reasoning aligns with visual context.

### Mechanism 3: Latent Corruption Awareness
VLMs implicitly detect visual degradation but fail to autonomously down-weight confidence unless explicitly prompted to consider the corruption. The visual encoder registers corruption as distinct feature, but without explicit prompt, language model prioritizes driving instruction over visual quality assessment.

## Foundational Learning

**Visual Grounding**
- Why needed: To differentiate between VLM reciting driving rules and actually perceiving the scene
- Quick check: If I remove the image but keep text prompt, does the model's answer change? If no, it lacks visual grounding

**Distributional Bias (Long-tail)**
- Why needed: To understand why standard accuracy metrics are misleading when datasets are imbalanced
- Quick check: Does the test set have equal number of "turn," "stop," and "go" actions for specific object queried?

**Rubric-based Evaluation**
- Why needed: To evaluate "safety" and "reasoning" rather than just word matching
- Quick check: Does the GPT evaluator know specific driving context before scoring the answer?

## Architecture Onboarding

**Component map:**
Multi-view Images + Text Prompt -> Corruption Pipeline -> VLM -> GPT Evaluator with Rubrics

**Critical path:**
Input -> Corruption (or Text-Only) -> VLM Response -> Rubric-Aware Evaluator

**Design tradeoffs:**
- Cost vs. Nuance: GPT evaluation captures nuance but is expensive; ROUGE-L is cheap but masked by biases
- Generalization vs. Specialization: Fine-tuned models beat open models on format/style metrics but generalize poorly to new datasets

**Failure signatures:**
- "Zero-Feature" Paradox: Model scores 85% on task when provided black screen (Text-Only)
- "Majority Class" Freeze: Model outputs "Going Ahead" regardless of visual evidence

**First 3 experiments:**
1. "Black Screen" Baseline: Run benchmark with fully black images (Text-Only). If accuracy > random chance, dataset is biased or model is hallucinating
2. Explicit vs. Implicit Corruption: Query about scene twice—once neutral, once stating "heavy fog." If answers differ significantly, model has latent awareness it isn't using
3. Rubric Sensitivity Test: Score same VLM response using standard ROUGE-L vs. Context-Aware GPT. Large discrepancies indicate metric is failing to capture semantic correctness

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark relies on synthetic corruption transformations with unspecified severity parameters
- GPT-based evaluation introduces variability due to prompt sensitivity
- Study focuses on nuScenes-derived data, potentially missing edge cases from diverse driving environments

## Confidence

**High Confidence:** Core finding that VLMs rely on textual cues rather than visual grounding when inputs are degraded, supported by consistent patterns across multiple models and corruption types.

**Medium Confidence:** Claim about metric masking by dataset bias, as it requires assumptions about ground truth distribution not fully validated across all scenarios.

**Medium Confidence:** Latent corruption awareness mechanism, inferred from performance differences rather than explicitly measured through ablation studies.

## Next Checks

1. **Ground Truth Verification:** Manually validate a subset of VLM responses against human-annotated ground truth to confirm whether metric-based evaluations accurately reflect visual grounding failures.

2. **Cross-Dataset Generalization:** Test the same VLMs on independent autonomous driving datasets (e.g., KITTI, Waymo) to assess whether observed failure modes persist across different data distributions.

3. **Alternative Evaluation Protocols:** Implement rule-based or causal inference metrics that explicitly verify whether responses depend on visual features rather than textual priors, comparing results with GPT-based scoring.