---
ver: rpa2
title: 'FAQNAS: FLOPs-aware Hybrid Quantum Neural Architecture Search using Genetic
  Algorithm'
arxiv_id: '2511.10062'
source_url: https://arxiv.org/abs/2511.10062
tags:
- quantum
- flops
- accuracy
- classical
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing efficient hybrid
  quantum neural networks (HQNNs) by introducing FAQNAS, a FLOPs-aware neural architecture
  search framework. The method formulates HQNN design as a multi-objective optimization
  problem balancing accuracy and computational cost, explicitly incorporating FLOPs
  into the search objective using a genetic algorithm (NSGA-II).
---

# FAQNAS: FLOPs-aware Hybrid Quantum Neural Architecture Search using Genetic Algorithm

## Quick Facts
- **arXiv ID:** 2511.10062
- **Source URL:** https://arxiv.org/abs/2511.10062
- **Reference count:** 23
- **Primary result:** FLOPs-aware NAS discovers Pareto-optimal HQNNs balancing accuracy and computational cost on 5 benchmark datasets

## Executive Summary
This paper introduces FAQNAS, a FLOPs-aware neural architecture search framework for hybrid quantum neural networks (HQNNs). The method formulates HQNN design as a multi-objective optimization problem using NSGA-II to balance validation accuracy and computational cost. Experiments on five benchmark datasets show that quantum FLOPs dominate accuracy improvements while classical FLOPs remain largely fixed, demonstrating that competitive accuracy can be achieved with significantly reduced computational cost compared to FLOPs-agnostic baselines.

## Method Summary
FAQNAS formulates HQNN design as a multi-objective optimization problem minimizing (Quantum FLOPs, 1-Validation Accuracy) using NSGA-II genetic algorithm. The search space includes 23,368 combinations of qubits (2-10), embedding types (Angle/Amplitude), entangling gates (CNOT/CZ), entanglement topology (Linear/Circular), rotation gates (RX/RY/RZ), and circuit depth (1-4). Each candidate architecture is trained for 5 epochs using Adam optimizer (lr=0.003, batch=64) and evaluated on validation accuracy while measuring FLOPs via PyTorch profiler. The framework identifies Pareto-optimal architectures where quantum FLOPs drive accuracy improvements while classical FLOPs remain fixed due to constrained classical layer design.

## Key Results
- NSGA-II successfully identifies Pareto-optimal HQNN architectures balancing accuracy and computational cost
- Quantum FLOPs dominate accuracy improvements while classical FLOPs remain largely fixed across architectures
- Dataset complexity determines FLOPs thresholds beyond which additional computation yields no accuracy gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** FLOPs-aware optimization identifies Pareto-optimal HQNN architectures that balance accuracy and computational cost
- **Mechanism:** NSGA-II performs multi-objective optimization minimizing (F_Q, 1-Acc_val) through non-dominated sorting and crowding distance, evolving architectures across 23,328 combinations over 8 generations or until stagnation
- **Core assumption:** FLOPs measured on classical simulators meaningfully predict computational cost for future quantum-classical deployments
- **Evidence anchors:** Abstract states FLOPs incorporation enables discovery of strong performance architectures while minimizing cost; Section II-D explicitly minimizes (F_Q, 1-Acc_val, Params)

### Mechanism 2
- **Claim:** Quantum FLOPs dominate accuracy improvements while classical FLOPs remain fixed across architectures
- **Mechanism:** Search space varies quantum circuit parameters while classical layers scale only with n_qubits, causing F_Q to vary significantly more than F_Classical across candidates
- **Core assumption:** Fixed classical layer design is sufficient for these datasets
- **Evidence anchors:** Abstract notes quantum FLOPs dominate accuracy improvements while classical FLOPs remain largely fixed; Section III-F explains improvements driven by quantum FLOPs due to constrained classical layer design

### Mechanism 3
- **Claim:** Dataset complexity determines the FLOPs threshold beyond which additional computation yields no accuracy gains
- **Mechanism:** Simpler datasets saturate near maximum accuracy at low F_Q; complex datasets require higher F_Q to reach performance plateaus
- **Core assumption:** 5-dataset benchmark represents complexity spectrum
- **Evidence anchors:** Section III shows large-scale datasets like MNIST require higher FLOPs budgets while moderate datasets achieve near-maximum accuracy with fewer FLOPs; Section IV identifies dataset-specific thresholds preventing over-provisioned designs

## Foundational Learning

- **Concept:** Hybrid Quantum Neural Networks (HQNNs)
  - **Why needed here:** FAQNAS searches over HQNN architectures; understanding quantum-classical interface is essential to interpret search results
  - **Quick check question:** Can you explain why PQCs are sandwiched between classical layers in Fig. 1?

- **Concept:** Multi-objective Optimization and Pareto Fronts
  - **Why needed here:** NSGA-II produces Pareto-optimal solutions; selecting architectures requires understanding trade-offs between competing objectives
  - **Quick check question:** What does a point on the Pareto front mean in terms of accuracy vs. FLOPs?

- **Concept:** Parameterized Quantum Circuits (PQCs) and Trainability
  - **Why needed here:** Search space includes gates, entanglement, and depth; these affect both FLOPs and barren plateau susceptibility
  - **Quick check question:** Why might increasing circuit depth improve expressivity but harm trainability?

## Architecture Onboarding

- **Component map:** Classical pre-processing (Linear layer) -> Quantum layer (embedding -> rotations -> entanglement -> measurement) -> Classical post-processing (Linear + Log-Softmax + NLL loss)
- **Critical path:** Initialize population -> Construct HQNN for each genome -> Train 5 epochs (Adam, lr=0.003, batch=64) -> Measure F_Q via PyTorch profiler -> Evaluate objectives (F_Q, 1-Acc_val) -> NSGA-II selection, crossover (p_c=0.8), mutation (p_m=0.2) -> Terminate after 8 generations or 2-generation stagnation
- **Design tradeoffs:** More qubits provide higher expressivity but exponential simulator cost; amplitude embedding encodes more data per qubit but requires normalization and padding; circular entanglement captures more correlations but increases gate count
- **Failure signatures:** Accuracy < 50% with high F_Q indicates barren plateaus or poor encoding-dataset match; Pareto front collapsed to single point suggests correlated objectives; stagnation at generation 1 indicates insufficient population diversity
- **First 3 experiments:** Replicate on Iris dataset to validate Pareto front matching; ablate FLOPs-awareness by running NSGA-II with only accuracy objective; extend search space by adding basis encoding and verify F_Q accounts for new operations

## Open Questions the Paper Calls Out
The paper identifies three major open questions: (1) Whether FLOPs-accuracy trade-offs hold on real NISQ hardware versus simulators, (2) How the approach scales to higher-dimensional datasets beyond small benchmarks, and (3) Whether extending training duration beyond 5 epochs would significantly alter discovered architectures.

## Limitations
- Forward-looking assumption that simulator FLOPs correlate with quantum hardware execution cost remains untested on real NISQ devices
- 5-dataset benchmark may not represent the complexity spectrum needed to generalize FLOPs-accuracy saturation thresholds
- Fixed classical layer design may not hold for more complex classical front-ends with high-dimensional inputs

## Confidence
- **High:** Multi-objective optimization using NSGA-II correctly identifies Pareto-optimal HQNN architectures balancing accuracy and computational cost
- **Medium:** Quantum FLOPs dominate accuracy improvements while classical FLOPs remain fixed across architectures
- **Medium:** Dataset complexity determines the FLOPs threshold beyond which additional computation yields no accuracy gains

## Next Checks
1. Deploy selected Pareto-optimal architectures on actual quantum hardware to validate simulator FLOPs correlate with real execution time and resource usage
2. Apply FAQNAS to larger-scale problems using dimensionality reduction or quantum hardware to verify FLOPs-accuracy Pareto structure persists
3. Re-run FAQNAS with 20-50 epochs per candidate to determine if discovered architectures change significantly