---
ver: rpa2
title: Efficient Reinforcement Finetuning via Adaptive Curriculum Learning
arxiv_id: '2504.05520'
source_url: https://arxiv.org/abs/2504.05520
tags:
- difficulty
- adarft
- training
- reward
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AdaRFT (Adaptive Curriculum Reinforcement
  Finetuning), a method that improves the efficiency and final accuracy of reinforcement
  finetuning for large language models in mathematical reasoning tasks. AdaRFT dynamically
  adjusts the difficulty of training problems based on the model's recent reward signals,
  ensuring that the model consistently trains on tasks that are challenging but solvable.
---

# Efficient Reinforcement Finetuning via Adaptive Curriculum Learning

## Quick Facts
- arXiv ID: 2504.05520
- Source URL: https://arxiv.org/abs/2504.05520
- Authors: Taiwei Shi; Yiyang Wu; Linxin Song; Tianyi Zhou; Jieyu Zhao
- Reference count: 40
- Primary result: AdaRFT improves reinforcement finetuning efficiency and accuracy for math reasoning by dynamically adjusting training difficulty to maintain a ~50% success rate

## Executive Summary
This paper introduces AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that improves the efficiency and final accuracy of reinforcement finetuning for large language models in mathematical reasoning tasks. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. Experiments on competition-level math datasets demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance, reducing training time by up to 2× and improving accuracy by a considerable margin across multiple data distributions and model sizes.

## Method Summary
AdaRFT is a curriculum-based reinforcement finetuning method that dynamically adjusts training difficulty based on recent reward feedback. It maintains an optimal learning zone by targeting a 50% success rate through adaptive difficulty selection. The method wraps around standard RL algorithms like PPO, modifying only the data sampling strategy. For each training step, it selects the batch of problems whose precomputed difficulty scores are closest to a target difficulty T, which is updated based on the average reward of the previous batch. The update rule smoothly adjusts T using a tanh-based function that increases difficulty when rewards are high and decreases it when rewards are low, keeping the model in the zone of maximal learning signal.

## Key Results
- AdaRFT improves training efficiency by up to 2× compared to standard PPO across multiple data distributions
- Achieves significant accuracy improvements on competition-level math datasets including MATH 500, OlympiadBench, and AIME 24
- Maintains consistent performance across different model sizes (1.5B and 7B parameters)
- Ablation studies confirm that targeting a 50% success rate yields the strongest learning signal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maintaining training difficulty at a target success rate of ~50% yields stronger learning signals than uniformly sampling across difficulty levels.
- Mechanism: The paper derives that in entropy-regularized RL with binary rewards, the KL divergence between the initial and optimal policy is lower-bounded by reward variance, which is maximized at p(x)=0.5 (Equation 4). AdaRFT operationalizes this by setting target reward β=0.5 and selecting problems where |di - T| is minimized, keeping the model in the highest-gradient-magnitude zone.
- Core assumption: The offline difficulty scores di remain valid proxies for model-relative difficulty throughout training (solver-independent notion of difficulty), rather than requiring online recomputation.
- Evidence anchors:
  - [abstract]: "Ablation studies confirm that training on problems with a 50% success rate yields the strongest learning signal."
  - [Section 3.4]: "This implies that the lower bound on the KL divergence, and consequently the gradient magnitude during policy updates, is proportional to the reward variance, which is maximized when p(x) = 0.5."
  - [corpus]: Weak direct evidence—neighbor papers do not replicate this specific 50% finding, though BOTS addresses task selection efficiency from a Bayesian perspective.
- Break condition: If difficulty annotations are highly misaligned with actual model competence (e.g., labels from a much stronger/weaker model), the 50% zone targeting may fail to track the true learning frontier.

### Mechanism 2
- Claim: Adaptive curriculum updates based on recent reward feedback accelerate convergence relative to fixed schedules or static data filtering.
- Mechanism: The update rule T′ = clip(T + η·tanh(α·(Ravg - β)), dmin, dmax) smoothly adjusts target difficulty in response to reward deviations. High reward → increase T; low reward → decrease T. This feedback loop keeps training aligned with evolving model capability without requiring repeated rollouts or manual schedule tuning.
- Core assumption: The reward signal Ravg on the current batch is sufficiently stable and representative to guide meaningful curriculum updates without excessive noise.
- Evidence anchors:
  - [abstract]: "AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, keeping the model consistently challenged but not overwhelmed."
  - [Section 5.1]: "For Qwen 2.5 Math 1.5B, standard PPO requires 43 extra steps (+71.7%) in the skew-difficult setting and 34 steps (+56.7%) in the uniform setting to match ADARFT's performance."
  - [corpus]: Related work (BOTS, "Squeeze the Soaked Sponge") explores alternative task selection strategies, but does not directly confirm this specific tanh-based update rule.
- Break condition: If reward variance is extremely high across batches (noisy or sparse rewards), the curriculum may oscillate rather than smoothly progress.

### Mechanism 3
- Claim: Curriculum-based sampling reduces per-step compute cost because easier problems generate shorter rollouts on average.
- Mechanism: Easier math problems (e.g., GSM8K) require ~200 tokens to solve versus ~2000 for competition-level problems (AIME). By starting with easier problems and gradually increasing difficulty, AdaRFT front-loads shorter sequences, reducing rollout and PPO update costs early in training.
- Core assumption: Token length correlates positively with problem difficulty in the target domain; this relationship is stable across the training distribution.
- Evidence anchors:
  - [Section 5.1]: "While PPO update time does not scale linearly with sequence length due to batching and attention computation patterns, longer sequences still incur higher compute costs."
  - [Section A.1]: Response length curves show AdaRFT maintains moderate, gradually increasing lengths versus fixed curriculum's rapid escalation.
  - [corpus]: No direct neighbor paper confirms this compute-time claim; it remains paper-specific.
- Break condition: If problem difficulty and response length are weakly correlated (e.g., verbose but easy problems, or terse but hard ones), the compute savings will diminish.

## Foundational Learning

- Concept: **Curriculum Learning (Bengio et al., 2009)**
  - Why needed here: AdaRFT is a curriculum method; understanding the general principle of structuring training from easy to hard is prerequisite.
  - Quick check question: Can you explain why presenting tasks in increasing difficulty might improve sample efficiency compared to random ordering?

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: The paper instantiates AdaRFT with PPO; the algorithm wraps PPO's policy update with curriculum-based data selection.
  - Quick check question: What problem does PPO's clipped objective solve compared to vanilla policy gradient?

- Concept: **Binary Reward Signals in RL**
  - Why needed here: The theoretical justification hinges on Bernoulli rewards (correct/incorrect) and their variance properties.
  - Quick check question: For a Bernoulli random variable with success probability p, at what value of p is variance maximized?

## Architecture Onboarding

- Component map:
  dataset D (difficulty-annotated samples) -> target difficulty tracker T (scalar) -> curriculum sampler (selects top-B samples by |di - T|) -> policy model πθ (LLM actor) -> reward function R(·,·) (binary correctness) -> RL algorithm A (PPO backbone)

- Critical path:
  1. Compute |di - T| for all samples in D (O(|D|) comparison)
  2. Sort/select top-B closest samples
  3. Generate rollouts via πθ
  4. Compute Ravg over batch
  5. Run standard PPO update
  6. Update T via T′ = clip(T + η·tanh(α·(Ravg - β)), dmin, dmax)

- Design tradeoffs:
  - **Offline vs. online difficulty estimation**: Offline (pass@k or LLM-judged) is cheaper and stable but may drift; online is accurate but requires repeated rollouts.
  - **β selection**: β=0.5 maximizes theoretical learning signal; β<0.5 biases toward harder problems (slower, risk of stagnation); β>0.5 biases toward easier (faster early, lower final performance).
  - **Update aggressiveness (η, α)**: Larger η/α accelerates curriculum but risks instability; smaller values are smoother but may under-utilize easy problems.

- Failure signatures:
  - Flat accuracy curves with reward stuck near 0 or 1 → curriculum not adapting (check T update)
  - Rapid reward collapse → T increasing faster than model capability (reduce η or α)
  - No improvement over baseline PPO → difficulty annotations misaligned with model (re-estimate di)
  - Curriculum oscillation → noisy Ravg (increase batch size B or smooth Ravg over multiple steps)

- First 3 experiments:
  1. **Sanity check**: Train AdaRFT(PPO) vs. PPO on a small uniform-difficulty subset (~1000 samples, 20 steps) and verify T tracks reward changes and accuracy improves faster.
  2. **Ablation on β**: Run β∈{0.2, 0.5, 0.8} on a held-out validation set; confirm β=0.5 yields highest final accuracy and maintains Ravg ≈ 0.5.
  3. **Robustness test**: Train on skew-difficult vs. skew-easy distributions; verify AdaRFT reduces the performance gap relative to PPO baseline (paper shows up to 2x speedup in skew-difficult).

## Open Questions the Paper Calls Out

- **Open Question 1**: Can ADARFT be effectively generalized to complex reasoning tasks where difficulty is subjective or non-hierarchical, such as creative writing or open-ended dialogue?
  - Basis in paper: [inferred] The paper evaluates ADARFT exclusively on competition-level mathematics (AMC, AIME, MATH) where "intrinsic complexity" is well-defined via human labels or pass rates.
  - Why unresolved: The method relies on pre-computed difficulty scores ($d_i$) to guide the curriculum. It is unclear how to establish a reliable absolute difficulty scale for tasks lacking clear-cut correctness or difficulty tiers (e.g., summarization).
  - What evidence would resolve it: Experiments applying ADARFT to non-mathematical benchmarks (e.g., MT-Bench or IFEval) using heuristic or LLM-judged difficulty scores, demonstrating efficiency gains without performance regression.

- **Open Question 2**: How can ADARFT integrate hybrid difficulty estimation strategies to balance computational cost with curriculum accuracy?
  - Basis in paper: [explicit] Appendix A.5 states: "future work could explore hybrid approaches that combine lightweight heuristics with periodic empirical calibration."
  - Why unresolved: The paper contrasts expensive rollout-based estimation with cheaper LLM-judged estimation. The former is accurate but slow; the latter is fast but noisier. The optimal trade-off or mixing strategy remains undefined.
  - What evidence would resolve it: A study comparing a hybrid strategy (e.g., using LLM judges for the initial curriculum and periodic rollouts for recalibration) against the static offline methods currently proposed.

- **Open Question 3**: Does the efficiency of ADARFT persist when scaling to models significantly larger than 7B parameters (e.g., 70B+), where the "optimal difficulty" window may shift?
  - Basis in paper: [inferred] The experiments are limited to Qwen 2.5 MATH 1.5B and Qwen 2.5 7B (Section 4.3).
  - Why unresolved: Larger models may have different learning dynamics and data saturation points. The computational overhead of maintaining the curriculum buffer and the sensitivity of the target difficulty update ($\eta, \alpha$) might need re-tuning or may yield diminishing returns at scale.
  - What evidence would resolve it: Training runs on 70B+ parameter models showing that the reduction in training steps (up to 2x) and accuracy improvements are maintained or improved relative to standard PPO.

## Limitations

- Generalizability beyond math: While AdaRFT is evaluated on competition-level mathematical reasoning, its effectiveness on other domains (e.g., coding, commonsense reasoning) remains unknown.
- Offline difficulty estimation stability: The paper assumes precomputed difficulty scores remain valid throughout training, but if difficulty labels drift relative to evolving model capability, the curriculum targeting mechanism may misalign.
- Compute savings assumptions: The claimed efficiency gains depend on stable correlation between problem difficulty and token length, which may not hold in all domains.

## Confidence

- **High confidence**: The adaptive curriculum mechanism itself is sound and well-supported by the 50% success-rate theory. The ablation studies and efficiency gains are directly demonstrated in the experiments.
- **Medium confidence**: The theoretical derivation linking 50% success rate to maximal gradient magnitude is correct within the entropy-regularized RL framework, but its practical robustness across different reward structures and task domains is uncertain.
- **Low confidence**: The compute-time savings from shorter sequences are plausible given the domain-specific token-length data, but lack external validation and depend heavily on the difficulty-length correlation holding true.

## Next Checks

1. **Domain transferability test**: Apply AdaRFT to a non-math domain (e.g., code generation or commonsense reasoning) and verify whether the 50% success-rate targeting still yields faster convergence and better final accuracy compared to uniform sampling.

2. **Difficulty-label sensitivity analysis**: Recompute difficulty scores using a different model (e.g., GPT-4 or Claude) and retrain AdaRFT to check if the performance gains persist or degrade, indicating sensitivity to the quality/alignment of difficulty annotations.

3. **Compute-cost validation**: Measure actual wall-clock training time and rollout lengths for AdaRFT vs. PPO across easy, medium, and hard problem subsets to confirm that the claimed 2× speedup in training time is driven by shorter sequences in early curriculum stages.