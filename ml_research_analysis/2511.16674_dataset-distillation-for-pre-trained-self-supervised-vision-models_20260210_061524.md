---
ver: rpa2
title: Dataset Distillation for Pre-Trained Self-Supervised Vision Models
arxiv_id: '2511.16674'
source_url: https://arxiv.org/abs/2511.16674
tags:
- uni00000055
- uni00000048
- distilled
- images
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new dataset distillation problem focused
  on optimizing synthetic images to train linear classifiers on top of pre-trained
  self-supervised vision models. The authors propose Linear Gradient Matching, a method
  that minimizes the cosine distance between gradients induced by synthetic and real
  data when passed through a pre-trained feature extractor and a randomly initialized
  linear classifier.
---

# Dataset Distillation for Pre-Trained Self-Supervised Vision Models

## Quick Facts
- **arXiv ID:** 2511.16674
- **Source URL:** https://arxiv.org/abs/2511.16674
- **Reference count:** 40
- **Key outcome:** Introduces Linear Gradient Matching for dataset distillation, achieving near-perfect linear probe accuracy with only one labeled image per class on ImageNet-100

## Executive Summary
This paper addresses the problem of distilling datasets for pre-trained self-supervised vision models by optimizing synthetic images that maximize the performance of linear classifiers trained on frozen feature extractors. The authors propose Linear Gradient Matching, which minimizes the cosine distance between gradients from synthetic and real data when passed through a pre-trained model and a randomly initialized linear classifier. Using a multi-scale pyramid representation, differentiable augmentations, and color decorrelation, the method produces distilled datasets that outperform real images on ImageNet-100 and ImageNet-1k. The approach generalizes across architectures and reveals interpretability insights about model biases and alignment, while requiring only one labeled image per class to achieve high performance.

## Method Summary
The authors introduce Linear Gradient Matching for dataset distillation, where synthetic images are optimized to match the gradients induced by real data when passed through a pre-trained feature extractor and linear classifier. The method uses a multi-scale pyramid representation to avoid overfitting, applies differentiable augmentations during optimization, and incorporates color decorrelation to improve generalization. During training, the cosine distance between gradients from synthetic and real data is minimized, enabling the distilled dataset to capture essential information for linear probe training. The approach is evaluated on ImageNet-100 and ImageNet-1k, showing significant performance improvements over real data baselines while using minimal labeled examples per class.

## Key Results
- Distilled datasets achieve near-perfect linear probe accuracy with only one labeled image per class on ImageNet-100
- Performance exceeds real-image baselines across both ImageNet-100 and ImageNet-1k
- Distilled data generalizes well across different backbone architectures (ViTs and CNNs)
- Visualized distilled images reveal model interpretability insights, including bias detection and alignment prediction

## Why This Works (Mechanism)
The method works by optimizing synthetic images to produce gradients that closely match those from real data when processed through a frozen feature extractor and linear classifier. By minimizing the cosine distance between these gradient vectors, the distilled dataset captures the essential information needed for effective linear probe training. The multi-scale pyramid representation prevents overfitting to the pre-trained model by encouraging scale-invariant features, while differentiable augmentations ensure robustness to common image transformations. Color decorrelation further improves generalization by decoupling chromatic information from structural patterns.

## Foundational Learning
- **Cosine distance minimization:** Used to align gradients between synthetic and real data; needed to ensure distilled images capture the same feature importance as real data for linear classification
- **Multi-scale pyramid representation:** Prevents overfitting by enforcing scale invariance; quick check: compare single-scale vs multi-scale performance
- **Differentiable augmentations:** Allows optimization through data transformations; needed to ensure distilled images remain robust to common augmentations during linear probe training
- **Feature extractor freezing:** Maintains pre-trained model weights during optimization; required to isolate the distillation process from model fine-tuning
- **Linear classifier initialization:** Randomly initialized linear layer paired with frozen feature extractor; needed to establish the optimization target for gradient matching
- **Color decorrelation:** Decouples color and structure in optimization; improves generalization by preventing overfitting to specific color distributions

## Architecture Onboarding
**Component map:** Synthetic images → Multi-scale pyramid → Pre-trained feature extractor → Linear classifier → Gradient computation → Optimization update

**Critical path:** The optimization loop where synthetic images are updated based on gradient matching between synthetic and real data trajectories through the frozen feature extractor and linear classifier.

**Design tradeoffs:** Multi-scale representation increases optimization complexity but prevents overfitting; differentiable augmentations add computational overhead but improve robustness; color decorrelation slightly reduces representational capacity but enhances generalization.

**Failure signatures:** Poor gradient alignment indicates suboptimal distilled images; overfitting manifests as degraded performance on held-out data; lack of scale invariance shows as performance drops when training augmentations differ from optimization augmentations.

**First experiments:** 1) Compare distilled vs real data performance with varying numbers of labeled examples per class; 2) Ablate multi-scale pyramid and color decorrelation to isolate their contributions; 3) Test distilled datasets across multiple pre-trained model families beyond ViTs and CNNs.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out specific open questions, but the bold claims about performance improvements and interpretability insights suggest several implicit questions about generalizability across domains, stability across random seeds, and practical scalability of the optimization process.

## Limitations
- Performance claims may depend heavily on specific experimental conditions not fully characterized
- Generalizability across architectures and downstream tasks needs more systematic validation
- Interpretability results lack rigorous statistical validation and may not hold across different model families
- Computational cost and practical scalability of the optimization process remain unclear

## Confidence
- **Performance claims vs real data:** Medium - results are impressive but may not generalize
- **Cross-architecture generalization:** Medium - limited validation across diverse backbones
- **Interpretability insights:** Low - qualitative observations without rigorous validation
- **Multi-scale pyramid effectiveness:** Medium - technique described but not systematically ablated

## Next Checks
1. Run ablation studies removing color decorrelation and multi-scale pyramid to quantify their individual contributions to performance gains
2. Test distilled datasets across 5+ different pre-trained model families (not just ViTs and CNNs) to verify architecture-agnostic generalization claims
3. Conduct statistical analysis (e.g., significance testing across multiple seeds) on interpretability findings showing distilled images predict model biases and alignment