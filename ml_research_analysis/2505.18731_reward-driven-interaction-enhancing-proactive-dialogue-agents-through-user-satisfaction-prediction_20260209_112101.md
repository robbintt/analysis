---
ver: rpa2
title: 'Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User
  Satisfaction Prediction'
arxiv_id: '2505.18731'
source_url: https://arxiv.org/abs/2505.18731
tags:
- user
- learning
- module
- system
- utterances
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately predicting user
  satisfaction in industrial dialogue systems, particularly in scenarios involving
  rare utterances caused by ASR errors and long-tail domains. The authors propose
  a novel approach using two auxiliary tasks to improve the model's representation
  learning capabilities.
---

# Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction

## Quick Facts
- **arXiv ID:** 2505.18731
- **Source URL:** https://arxiv.org/abs/2505.18731
- **Reference count:** 40
- **Primary result:** Multi-task Transformer with auxiliary tasks improves user satisfaction prediction in industrial dialogue systems, achieving 5.5% CLA gain over baseline.

## Executive Summary
This paper addresses the challenge of accurately predicting user satisfaction in industrial dialogue systems, particularly for rare utterances caused by ASR errors and long-tail domains. The authors propose a multi-task Transformer architecture with two auxiliary tasks: contrastive self-supervised learning for rare utterance recognition and domain-intent classification for long-tail domain performance. Evaluated on DuerOS, the method achieves significant improvements in accuracy for recognizing errors in rare user utterances and long-tail domains, successfully deployed to improve session-level user satisfaction through proactive clarification triggers.

## Method Summary
The method uses a multi-task Transformer with three sub-modules (ASR-query match, query-reply match, user session match) that fuse error signals from different components. Two auxiliary tasks are added during training: SimCSE contrastive self-supervised learning on query embeddings to handle ASR errors, and domain-intent classification on session embeddings to handle long-tail domains. The model is trained jointly with weighted loss combination (main: 1.0, CSSL: 0.01, domain-intent: 0.1) and uses a threshold (0.78) to trigger proactive clarification. The approach avoids pre-training + fine-tuning for resource efficiency, instead using joint multi-task training to regularize against noisy weak labels.

## Key Results
- ABM achieves 5.5% improvement in Conditional Label Accuracy (CLA) compared to baseline TBM-2
- ABM-C shows 28.8% CLA improvement in Universal QA domain and 45% in other long-tail domains
- ABM recalls 38 ASR errors vs TBM-2's 30 out of 119 total errors in rare utterance recognition

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contrastive self-supervised learning improves representation quality for rare ASR-corrupted utterances.
- **Mechanism:** Dropout-based data augmentation creates positive pairs from identical queries; in-batch negatives force the model to learn discriminative features without relying on frequency-based patterns.
- **Core assumption:** Rare utterances share structural properties with common ones that contrastive objectives can exploit.
- **Evidence anchors:**
  - [abstract] "contrastive self-supervised learning task...helps the model learn the representation of rare user utterances and identify ASR errors"
  - [Page 5] ABM recalls 38 ASR errors vs. TBM-2's 30 out of 119 total
  - [corpus] Weak direct evidence; related work on SimCSE confirms dropout augmentation effectiveness for sentence embeddings but not specifically for ASR errors
- **Break condition:** If rare utterances are phonologically or semantically incoherent beyond representation issues, contrastive learning alone cannot recover meaning.

### Mechanism 2
- **Claim:** Domain-intent classification as auxiliary task improves session-level representation for long-tail domains.
- **Mechanism:** Explicit domain-intent supervision forces session encoder to preserve domain-discriminative information that satisfaction labels alone may not encode.
- **Core assumption:** Long-tail domain errors follow learnable patterns that domain-aware representations can capture.
- **Evidence anchors:**
  - [Page 5] ABM-C achieves 28.8% CLA improvement in Universal QA, 45% in other long-tail domains
  - [Page 7] ABM recalls 15 dissatisfied sessions in Universal QA vs. TBM-2's 7
  - [corpus] No direct corpus evidence for domain-intent auxiliary tasks in dialogue satisfaction
- **Break condition:** If satisfaction patterns within long-tail domains are highly heterogeneous or task-specific, single domain-intent classification may oversimplify.

### Mechanism 3
- **Claim:** Joint multi-task training (main task + two auxiliary tasks) yields better generalization than pre-training then fine-tuning.
- **Mechanism:** Auxiliary task gradients regularize the main task during training, preventing overfitting to noisy weak labels.
- **Core assumption:** Auxiliary task signals are cleaner or more structured than weak satisfaction labels.
- **Evidence anchors:**
  - [Page 5] "weighted sum of the user satisfaction prediction task loss, contrastive self-supervised learning task loss, and domain-intent classification task loss as the final loss"
  - [Page 6] ABM outperforms both ABM-S and ABM-C individually across domains
  - [corpus] Multi-task learning for dialogue systems is underexplored in neighboring papers
- **Break condition:** If auxiliary task weights are poorly tuned, gradient interference may degrade main task performance.

## Foundational Learning

- **Contrastive Self-Supervised Learning (SimCSE)**
  - **Why needed here:** Enables learning from intrinsic data structure without manual labels for rare utterances.
  - **Quick check question:** Can you explain how dropout creates different views of the same input for positive pairs?

- **Multi-Task Learning with Auxiliary Tasks**
  - **Why needed here:** Balances main task (satisfaction prediction) with auxiliary signals (domain classification, utterance similarity).
  - **Quick check question:** What happens if auxiliary task loss weight is too high relative to main task?

- **Weak Supervision in Industrial Dialogue Systems**
  - **Why needed here:** Ground-truth satisfaction labels are expensive; weak labels from user actions introduce noise.
  - **Quick check question:** How might post-hoc user actions (e.g., repeating query, abandoning session) fail to capture true satisfaction?

## Architecture Onboarding

- **Component map:**
  - ASR-Query Match Module: Original query, n-best ASR hypotheses, rewritten query → Transformer → error signal T_q
  - Query-Reply Match Module: Final query, NLU results, IR item description → Transformer → error signal T_r
  - User Session Match Module: Multi-turn history (queries, NLU, intervals) → Transformer → error signal T_s
  - Fusion Layer: Concatenate T_q, T_r, T_s → FC → sigmoid → satisfaction probability
  - Auxiliary Task Heads: (1) Contrastive SSL on query embeddings, (2) Domain-intent classifier on session SEP token

- **Critical path:**
  1. Query encoding (embedding + attention)
  2. Session encoding (multi-turn attention)
  3. Auxiliary task losses computed in parallel
  4. Weighted loss combination (main: 1.0, CSSL: 0.01, domain-intent: 0.1)
  5. Threshold (0.78) determines clarification trigger

- **Design tradeoffs:**
  - Splitting into sub-modules reduces latency (parallel inference) but limits cross-module attention
  - Pre-training + fine-tuning avoided for resource efficiency; joint training preferred
  - Weak labels scale data but introduce noise; auxiliary tasks mitigate representation collapse

- **Failure signatures:**
  - High AUC but low CLA → model confident but threshold poorly calibrated
  - Good performance on media domain, poor on long-tail → domain-intent task underweighted or insufficient long-tail data
  - ASR errors missed despite contrastive task → dropout rate may be too low or rare utterances out-of-distribution

- **First 3 experiments:**
  1. **Baseline comparison:** Train TBM-2 vs. ABM on held-out validation set; measure CLA per domain (expect 5-30% gains on long-tail).
  2. **Ablation study:** Remove each auxiliary task individually (ABM-S, ABM-C) to isolate contribution; expect ABM-S helps ASR errors, ABM-C helps long-tail domains.
  3. **Threshold sensitivity:** Sweep thresholds from 0.5-0.9 on validation CLA; confirm 0.78 maximizes recall at 85% precision before online deployment.

## Open Questions the Paper Calls Out
The paper identifies two main open questions: (1) How to improve the model's understanding of rare user utterances and long-tail domains beyond the proposed auxiliary tasks, and (2) How to address the inconsistency between AUC and CLA metrics when models achieve high AUC scores.

## Limitations
- Evaluation relies on proprietary DuerOS data, making independent verification impossible
- Specific distribution characteristics of rare utterances and long-tail domains are unclear
- Lacks detailed ablation studies isolating individual auxiliary task contributions
- No comparison to alternative multi-task learning approaches or pre-training strategies

## Confidence
- **High Confidence:** Architectural design combining multi-task learning with auxiliary tasks is technically sound
- **Medium Confidence:** CLA improvements (5.5% overall, 28.8-45% on long-tail) are plausible but require proprietary data verification
- **Low Confidence:** Specific ASR error recognition claims (38 vs 30 errors) cannot be independently validated

## Next Checks
1. **Public Dataset Reproduction:** Implement architecture on MultiWOZ with synthetic rare utterance and long-tail domain conditions
2. **Ablation Study Extension:** Conduct detailed ablation experiments removing individual components to quantify independent contributions
3. **Threshold Calibration Analysis:** Perform systematic threshold sweeps across full precision-recall curve to understand model behavior across operating points