---
ver: rpa2
title: 'POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor
  Attacks in Federated Learning'
arxiv_id: '2510.19056'
source_url: https://arxiv.org/abs/2510.19056
tags:
- polar
- attack
- backdoor
- layers
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POLAR introduces a policy-based reinforcement learning approach
  to address layer selection in federated backdoor attacks, overcoming the limitations
  of static rule-based methods by dynamically optimizing which layers to poison. By
  using lightweight Bernoulli sampling and a BSR-based reward function, POLAR adapts
  layer selection across FL rounds while maintaining stealth through a regularization
  constraint.
---

# POLAR: Policy-based Layerwise Reinforcement Learning Method for Stealthy Backdoor Attacks in Federated Learning

## Quick Facts
- **arXiv ID:** 2510.19056
- **Source URL:** https://arxiv.org/abs/2510.19056
- **Reference count:** 12
- **Primary result:** POLAR outperforms state-of-the-art attacks by up to 40% in backdoor success rate across multiple defenses, models, and datasets.

## Executive Summary
POLAR introduces a policy-based reinforcement learning approach to address layer selection in federated backdoor attacks, overcoming the limitations of static rule-based methods by dynamically optimizing which layers to poison. By using lightweight Bernoulli sampling and a BSR-based reward function, POLAR adapts layer selection across FL rounds while maintaining stealth through a regularization constraint. Experiments show POLAR outperforms state-of-the-art attacks like LP Attack and BadNets by up to 40% in backdoor success rate across multiple defenses, models, and datasets, while preserving high main task accuracy and malicious client acceptance. Its learning-based design ensures strong generalizability and scalability in practical FL deployments.

## Method Summary
POLAR formulates layer selection as a Markov Decision Process where a policy network outputs logits for each layer, used to sample binary action vectors via Bernoulli sampling. Using REINFORCE algorithm, the policy is updated via gradient ascent to maximize expected Backdoor Success Rate (BSR). A regularization term penalizes selecting numerous layers, forcing minimal critical layer selection to maintain stealth. The system uses Bernoulli sampling for efficient exploration, reducing complexity from exponential to linear relative to RL steps.

## Key Results
- Achieves up to 40% higher BSR than state-of-the-art attacks across multiple defenses
- Maintains high main task accuracy while ensuring strong malicious client acceptance rates
- Demonstrates strong generalizability across different datasets, models, and defense mechanisms

## Why This Works (Mechanism)

### Mechanism 1: Policy-Gradient Optimization of Discrete Layer Selection
Formulating layer selection as a Reinforcement Learning (RL) task allows the attack to dynamically identify "backdoor-critical" (BC) layers more effectively than static, rule-based approaches. The system models layer selection as a Markov Decision Process (MDP). A policy network outputs logits (θ) for each layer, which are used to sample binary action vectors (select/don't select). Using the REINFORCE algorithm, the policy is updated via gradient ascent to maximize the expected Backdoor Success Rate (BSR). Core assumption: The contribution of specific layers to the backdoor effectiveness is not static and can be learned through trial-and-error interaction with the aggregation environment.

### Mechanism 2: Stealthiness via Sparsity Regularization
Explicitly penalizing the selection of numerous layers in the loss function forces the agent to find a minimal subset of critical layers, reducing the detection footprint. A regularization term λ Σ log(p_l) is added to the loss function. This creates a trade-off: the agent must achieve high BSR using as few layers possible. This limits the "attack surface" (parameter deviation), helping the malicious update evade statistical anomaly detectors. Core assumption: Defenses like MultiKrum or RLR rely significantly on the magnitude or statistical deviation of updates; therefore, sparse updates are inherently stealthier.

### Mechanism 3: Efficient Search via Bernoulli Sampling
Bernoulli sampling provides a lightweight exploration strategy for the discrete action space of layer selection compared to full enumeration or complex RL paradigms. Instead of evaluating all 2^N combinations, the agent samples K independent binary masks per layer based on sigmoid probabilities (σ(θ)). This factorizes the search space, reducing complexity from exponential to linear relative to RL steps. Core assumption: The "importance" of layers is sufficiently independent or can be approximated via independent Bernoulli probabilities, avoiding the need for combinatorial joint-probability modeling.

## Foundational Learning

- **Concept: Reinforcement Learning (Policy Gradient / REINFORCE)**
  - **Why needed here:** This is the engine of POLAR. You cannot understand how the attack improves over time without grasping how the policy (logits) is updated based on the reward (BSR).
  - **Quick check question:** If the reward (BSR improvement) is negative, will the policy increase or decrease the probability of selecting the specific layer that caused that result?

- **Concept: Backdoor Attacks in Federated Learning**
  - **Why needed here:** Understanding the distinction between "Main Task Accuracy" (benign performance) and "Backdoor Success Rate" (BSR) is crucial, as POLAR explicitly trades these off against stealthiness.
  - **Quick check question:** If a model achieves 100% BSR but 0% main accuracy, is it considered a successful attack in this context? (Hint: No, it must maintain normal utility).

- **Concept: Bernoulli Distribution & Sigmoid Function**
  - **Why needed here:** The action selection mechanism relies on converting continuous logits to probabilities via Sigmoid, then sampling binary decisions.
  - **Quick check question:** If a logit θ_l is very high (e.g., 10), what is the probability p_l of selecting that layer?

## Architecture Onboarding

- **Component map:** Environment (FL training round) -> Agent (POLAR module) -> Policy Parameters (logits θ) -> Sampler (Bernoulli masks) -> Evaluator (BSR calculation) -> Update (gradient ascent)
- **Critical path:**
  1. Initialize: Receive global model W_r. Set θ (often inherited from previous round).
  2. Explore (RL Loop): For T steps, sample masks, inject backdoor, compute reward (BSR_mask - BSR_baseline).
  3. Update: Calculate Loss (Policy Gradient + Regularization) and update θ.
  4. Exploit: Generate final mask S_final using thresholded θ.
  5. Upload: Create the poisoned model update and submit to server.
- **Design tradeoffs:**
  - Batch Size (K) vs. Speed: Higher K stabilizes learning but increases computation time linearly.
  - Penalty (λ) vs. Effectiveness: Higher λ improves stealthiness (higher MAR) but may degrade BSR if too restrictive.
  - RL Steps (T) vs. Convergence: More steps allow better strategy refinement but delay the upload.
- **Failure signatures:**
  - Unstable Layer Selection: If λ is 0, the agent selects too many layers, leading to detection (Low MAR).
  - Convergence Failure: If batch size is too small, gradients are noisy, leading to fluctuating BSR.
  - Over-regularization: If λ is too high (e.g., 20), BSR collapses (<40%) because the agent is afraid to select any layers.
- **First 3 experiments:**
  1. Sanity Check (FedAvg): Run POLAR on CIFAR-10/ResNet18 with no defense. Verify that BSR is high (>90%) and main accuracy is preserved.
  2. Stealthiness Test (MultiKrum): Run POLAR against MultiKrum defense. Monitor MAR (Malicious Acceptance Rate). If MAR is low, check the regularization parameter λ.
  3. Ablation on Penalty (λ): Run with λ ∈ {0, 5, 10, 20} on RLR defense to visualize the trade-off between the "Number of Selected Layers" and BSR (reproducing Table 4).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does POLAR perform against adaptive defenses specifically designed to detect the temporal dynamics or distributional shifts inherent in RL-based layer selection?
- Basis: The paper evaluates POLAR against standard aggregation defenses (FLARE, FLTrust) but does not test against defenses that might exploit the sequential decision-making patterns or the high-variance updates caused by the RL agent's exploration.
- Why unresolved: Current defenses primarily focus on single-round statistical anomalies rather than cross-round temporal patterns of parameter selection.
- What evidence would resolve it: Testing POLAR against a defense that monitors parameter selection consistency across rounds or uses meta-learning to detect the signature of policy gradient updates.

### Open Question 2
- Question: Can the computational overhead of the RL training loop (O(KTE)) be reduced to support very deep models or resource-constrained edge devices?
- Basis: Table 1 classifies POLAR's time complexity as "Medium," and the methodology section notes the cost involves K · T BSR evaluations per round.
- Why unresolved: The reliance on multiple forward passes for BSR evaluation per round adds latency, which may be prohibitive for scaling to massive models like LLMs.
- What evidence would resolve it: Experiments demonstrating success rates on Large Language Models (LLMs) or extreme-depth architectures using lightweight RL surrogates or reduced sampling steps.

### Open Question 3
- Question: Is it possible to develop a self-tuning mechanism for the penalty weight λ to automate the trade-off between stealthiness and effectiveness?
- Basis: The ablation study (Table 4) shows high sensitivity to the penalty λ, where performance drops significantly if λ is set too high, indicating manual tuning is critical.
- Why unresolved: Fixed hyperparameters may not generalize perfectly to unseen architectures or varying defense intensities without manual retuning.
- What evidence would resolve it: Implementing an adaptive hyperparameter adjustment loop within the RL agent and testing stability across heterogeneous model architectures.

## Limitations

- The choice of Bernoulli sampling over more expressive joint-action sampling is not rigorously justified, potentially limiting effectiveness for architectures requiring specific layer combinations.
- Hyperparameter sensitivity, particularly the penalty weight λ, is empirically tuned but not theoretically grounded, limiting generalizability.
- Initialization of policy logits varies between main text (0) and appendix (0.5), creating ambiguity in reproducibility.

## Confidence

- **High Confidence:** The RL framework for dynamic layer selection and empirical superiority over static baselines are well-supported by ablation studies and defense comparisons.
- **Medium Confidence:** The stealthiness mechanism via sparsity regularization is logically sound and supported by Table 4, but its effectiveness against adaptive defenses is not tested.
- **Low Confidence:** The claim that Bernoulli sampling is "lightweight" is not validated against alternative exploration strategies within the paper.

## Next Checks

1. **Robustness to Initialization:** Run POLAR with multiple logit initializations (0 vs 0.5) to assess sensitivity in early-round BSR and convergence speed.
2. **Adaptive Defense Test:** Evaluate POLAR against a defender that adapts to sparsity patterns (e.g., by tracking layer-wise update magnitudes over rounds).
3. **Combinatorial Ablation:** Replace Bernoulli sampling with a joint-action sampler (e.g., softmax over all 2^N masks) to test whether independent sampling is truly sufficient for optimal layer selection.