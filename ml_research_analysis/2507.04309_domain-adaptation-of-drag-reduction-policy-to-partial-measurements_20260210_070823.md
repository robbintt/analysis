---
ver: rpa2
title: Domain Adaptation of Drag Reduction Policy to Partial Measurements
arxiv_id: '2507.04309'
source_url: https://arxiv.org/abs/2507.04309
tags:
- measurements
- policy
- optimal
- control
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting feedback control
  policies trained with full-state measurements to scenarios with only partial measurements,
  motivated by real-world limitations in sensor placement. The authors propose a Policy
  Domain Adaptation (PDA) framework that leverages a Domain-Specific Feature Transfer
  (DSFT) map to reconstruct full measurements from the history of partial measurements.
---

# Domain Adaptation of Drag Reduction Policy to Partial Measurements

## Quick Facts
- arXiv ID: 2507.04309
- Source URL: https://arxiv.org/abs/2507.04309
- Authors: Anton Plaksin; Georgios Rigas
- Reference count: 8
- One-line primary result: Policy Domain Adaptation (PDA) framework enables adapting optimal drag reduction policies from full-state to partial measurements, achieving near-optimal performance with only body-base pressure sensors.

## Executive Summary
This paper addresses the challenge of adapting feedback control policies trained with full-state measurements to scenarios with only partial measurements, motivated by real-world limitations in sensor placement. The authors propose a Policy Domain Adaptation (PDA) framework that leverages a Domain-Specific Feature Transfer (DSFT) map to reconstruct full measurements from the history of partial measurements. They demonstrate this approach in a simulated environment for reducing aerodynamic drag of a simplified road vehicle, where the full measurements are pressure readings in the wake, but only body-base pressure readings are available in deployment.

## Method Summary
The PDA framework trains a neural network to map partial measurements and action history to full measurements, then composes this with the optimal policy for full measurements to derive a policy for partial measurements. The method is demonstrated on a 2D bluff body flow at Re=100, where the full measurements are wake pressure sensors and partial measurements are body-base pressure sensors. The optimal full-observation policy is trained using TQC, and the DSFT map is trained via supervised learning to minimize reconstruction error. The composed policy achieves drag reduction performance close to the optimal policy trained with full measurements.

## Key Results
- PDA-derived policies achieve drag reduction performance close to the optimal policy trained with full measurements
- Policies trained directly on partial measurements underperform significantly compared to PDA approach
- Including measurement history (n≈48 steps) is crucial for optimal performance, while action history has minimal impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: History of partial measurements can reconstruct full-state wake measurements sufficiently for control.
- Mechanism: A neural network T_θ (DSFT map) is trained via supervised learning to minimize ||T_θ(o^PM_{t-n:t}, a_{t-m:t-1}) - o^FM_t||². Once trained, the map transforms partial observation histories into approximations of full wake measurements, which the pre-trained optimal policy consumes directly.
- Core assumption: The partial measurements on the body base contain enough information about the wake dynamics to enable accurate reconstruction; the system dynamics are sufficiently deterministic given history length n.
- Evidence anchors:
  - [abstract] "propose to train a Domain Specific Feature Transfer (DSFT) map reconstructing the full measurements from the history of the partial measurements"
  - [Section 3] Eq. (1) defines the supervised loss; Eq. (2) shows policy composition π*(T_θ(·), a_{t-1})
  - [corpus] Related work on partially observed chaotic flows (arXiv:2504.16588) suggests data-assimilated reconstruction is viable but challenging for high-dimensional systems
- Break condition: If wake dynamics have significant stochastic components not captured by body-base sensors, or if delay requirements exceed practical history lengths, reconstruction fidelity degrades and policy performance collapses.

### Mechanism 2
- Claim: Composing a pre-trained optimal policy with a reconstruction map outperforms direct RL training on partial observations.
- Mechanism: Instead of learning π(o^PM_{t-n:t}, a_{t-m:t-1}) from scratch via RL (which struggles with partial observability), PDA leverages the already-solved full-observation policy and only learns the observation mapping. This reduces the learning problem from policy optimization to supervised regression.
- Core assumption: The full-observation optimal policy π*(o^FM_t, a_{t-1}) is available from simulation or training-time access to full sensors.
- Evidence anchors:
  - [Section 4] "the drag achieved by π(o^PM_{t-n:t}, a_{t-m:t-1}) trained by the RL algorithm directly (red line) is significantly suboptimal compared to... policies obtained by our PDA approach"
  - [Figure 2a] PDA policies (blue/orange) converge near the optimal green line; direct RL (red) remains suboptimal
  - [corpus] No direct corpus comparison; related work focuses on model-based RL for partial observations rather than transfer approaches
- Break condition: If T_θ reconstruction introduces systematic biases (not just noise), the composed policy may make consistently wrong control decisions despite low reconstruction error.

### Mechanism 3
- Claim: Near-optimal drag reduction control requires only linear policies with tanh activation, given appropriate observation inputs.
- Mechanism: The optimal policy π*(o^FM_t, a_{t-1}) = a* tanh(A·o^FM_t + B·a_{t-1}) with no hidden layers achieves full performance. The DSFT map T_θ needs only one hidden layer (128 neurons). This simplicity improves robustness to input approximation errors from reconstruction.
- Core assumption: The control task—stabilizing vortex shedding—has approximately linear structure in the transformed observation space.
- Evidence anchors:
  - [Section 4] "we approximate it with a neural network without hidden layers... we found this class of policies is sufficient to obtain optimal performance and more robust to approximations"
  - [Appendix A, Figure 3] Both π* (linear) and π*_+ (3 hidden layers) achieve optimal drag; π* shows more stable results
  - [Appendix B] Linear T_θ model underperforms; deeper models (2 hidden layers) overfit
  - [corpus] No direct validation; related work on transonic flow control (arXiv:2511.07564) uses deeper networks for more complex dynamics
- Break condition: For higher Reynolds numbers, different actuator configurations, or multi-objective control, linear policies may become insufficient.

## Foundational Learning

- Concept: **Markov Decision Processes (MDPs) vs. Partially Observed MDPs (POMDPs)**
  - Why needed here: The paper frames full-state control as an MDP and partial-state control as a POMDP. Understanding this distinction explains why direct RL struggles and why history-based reconstruction helps.
  - Quick check question: Given only body-base pressure sensors, can you determine the exact wake vortex state? If not, what additional information would make it Markovian?

- Concept: **Supervised Domain Adaptation / Feature Transfer**
  - Why needed here: The DSFT map is a domain adaptation technique—learning to translate from the "partial observation domain" to the "full observation domain" using paired training data.
  - Quick check question: If you have paired data (partial, full) observations, what loss function would you minimize? What happens if the training distribution doesn't match deployment?

- Concept: **Reynolds Number and Vortex Shedding Dynamics**
  - Why needed here: The drag reduction task operates at Re=100 (laminar regime with periodic vortex shedding). Understanding that control targets flow instability (not turbulence) contextualizes why simple policies suffice.
  - Quick check question: At Re=100, is the wake flow turbulent or laminar with coherent structures? How would control difficulty change at Re=10,000?

## Architecture Onboarding

- Component map:
  - **Environment**: 2D DNS (FEniCS/Dolfin), square bluff body at Re=100, two jet actuators
  - **Full-observation policy π***: Linear network (no hidden layers), input: wake pressure sensors + previous action, output: jet forces via tanh
  - **DSFT map T_θ**: 1 hidden layer (128 neurons), input: n-step partial observation history + m-step action history, output: reconstructed wake observations
  - **PDA policy**: Composition π*(T_θ(·), a_{t-1})

- Critical path:
  1. Train optimal policy π* on full wake measurements using TQC (RL algorithm)
  2. Collect paired trajectory dataset D = {(o^PM_t, o^FM_t, a_t)} from simulations
  3. Train T_θ via supervised learning (Eq. 1)
  4. Deploy composed policy π*(T_θ(·), a_{t-1}) with partial sensors only

- Design tradeoffs:
  - History length n: Longer improves reconstruction but increases latency and memory; paper finds n≈48 steps optimal
  - T_θ depth: 1 hidden layer balances reconstruction capacity vs. overfitting; deeper models showed instability
  - Policy complexity: Linear policies more robust to reconstruction error; deeper policies offer no benefit for this task
  - Action history m: Paper finds minimal impact; simplifies to m=1 without performance loss

- Failure signatures:
  - Drag coefficient oscillates or diverges → T_θ reconstruction error too high; increase n or check training data coverage
  - Policy output saturates at bounds → Check tanh scaling (a* parameter); may indicate observation normalization issues
  - PDA underperforms direct RL → Likely mismatch between training and deployment distributions; verify paired data quality

- First 3 experiments:
  1. **Baseline replication**: Train π* on full wake measurements; verify drag reduction matches paper (stabilized wake, reduced oscillations). This validates the RL setup.
  2. **DSFT ablation**: Train T_θ with varying history lengths n∈{8, 16, 32, 48, 64}; plot reconstruction MSE vs. deployed drag coefficient. Confirm n≈48 is necessary and sufficient.
  3. **Architecture sweep**: Compare PDA performance with (a) linear T_θ, (b) 1-hidden-layer T_θ, (c) 2-hidden-layer T_θ; verify paper's finding that 1 hidden layer is optimal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the robustness of the Policy Domain Adaptation (PDA) approach when transferring policies from simulation or wind-tunnel environments to real-world operational conditions?
- Basis in paper: [explicit] The authors explicitly identify "assessing the robustness of sim-to-real or lab-to-real policy transfer" as a primary direction for further research in the Discussion and Limitations section.
- Why unresolved: The current study relies on 2D Direct Numerical Simulation (DNS) which assumes perfect physics and measurements, whereas real-world deployment introduces unmodeled dynamics and physical constraints not present in the digital twin.
- What evidence would resolve it: Successful deployment of the PDA-derived policy on a physical bluff body in a wind tunnel or road test, maintaining drag reduction performance comparable to the simulation results.

### Open Question 2
- Question: Can the DSFT method successfully adapt policies for three-dimensional, turbulent flows at high Reynolds numbers?
- Basis in paper: [inferred] The paper validates the method only on a "simplified 2D bluff body" at a low Reynolds number (Re=100) characterized by laminar wake dynamics.
- Why unresolved: High-Re turbulent flows involve chaotic, multi-scale dynamics and significantly higher dimensionality that may not be effectively reconstructable from partial history using the current neural network architecture or history lengths.
- What evidence would resolve it: Application of the PDA framework to 3D Large Eddy Simulations (LES) or high-Re experiments, demonstrating performance comparable to the full-state optimal policy.

### Open Question 3
- Question: How does sensor noise impact the stability and accuracy of the DSFT reconstruction map and the resulting control policy?
- Basis in paper: [inferred] The study utilizes noise-free simulation data, whereas the introduction highlights that real-world sensing involves technical limitations and partial access.
- Why unresolved: The neural network mapping partial history to full states may effectively act as a high-gain observer, potentially amplifying measurement noise and leading to unstable actuation in physical systems.
- What evidence would resolve it: A sensitivity analysis evaluating the PDA policy's performance in simulations where varying levels of Gaussian noise are injected into the partial measurement inputs.

## Limitations

- The paper's claim about sensor placement optimality (body-base vs. wake sensors) is based on a single flow configuration (Re=100, 2D bluff body) and may not generalize to other Reynolds numbers or geometries.
- The DSFT map's reconstruction quality depends on sufficient coverage of the state space in training data. For deployment scenarios with significantly different operating conditions than training, performance may degrade without explicit robustness guarantees.
- While the paper demonstrates that linear policies suffice for this specific control task, the analysis doesn't explain why this simplicity emerges or when more complex policies would become necessary.

## Confidence

- **High confidence**: The PDA framework's effectiveness for this specific drag reduction task, supported by controlled experiments showing significant performance gaps between PDA and direct RL on partial observations.
- **Medium confidence**: The generalizability of findings to other fluid control problems, given the narrow experimental scope and lack of cross-validation on different flow regimes or actuator configurations.
- **Medium confidence**: The claimed robustness of linear policies to reconstruction errors, based on empirical observation rather than theoretical guarantees about sensitivity to input perturbations.

## Next Checks

1. **Cross-flow validation**: Test PDA performance at Re=1000 and Re=10000 to verify the linear policy assumption holds across different vortex shedding characteristics and Reynolds regimes.
2. **Sensor placement optimization**: Systematically evaluate PDA performance using different partial measurement locations (e.g., top/bottom body sensors vs. base sensors) to identify optimal sensor placement strategies.
3. **Dataset coverage analysis**: Quantify how DSFT reconstruction error and deployed policy performance vary with training dataset size and diversity, particularly for states underrepresented in the training distribution.