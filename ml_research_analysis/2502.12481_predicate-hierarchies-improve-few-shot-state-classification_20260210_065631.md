---
ver: rpa2
title: Predicate Hierarchies Improve Few-Shot State Classification
arxiv_id: '2502.12481'
source_url: https://arxiv.org/abs/2502.12481
tags:
- phier
- predicate
- hyperbolic
- space
- predicates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PHIER improves few-shot state classification by leveraging predicate
  hierarchies in a structured latent space. The model uses an object-centric scene
  encoder, self-supervised losses to infer pairwise predicate relations, and a hyperbolic
  distance metric to capture hierarchical structure.
---

# Predicate Hierarchies Improve Few-Shot State Classification

## Quick Facts
- arXiv ID: 2502.12481
- Source URL: https://arxiv.org/abs/2502.12481
- Authors: Emily Jin; Joy Hsu; Jiajun Wu
- Reference count: 38
- Primary result: PHIER improves few-shot state classification by leveraging predicate hierarchies in a structured hyperbolic latent space, achieving 22.5% and 8.3% improvements in out-of-distribution accuracy on CALVIN and BEHAVIOR datasets respectively.

## Executive Summary
PHIER is a novel model for few-shot state classification that leverages predicate hierarchies to enable strong generalization to novel predicates and out-of-distribution queries. The model uses an object-centric scene encoder, self-supervised losses to infer pairwise predicate relations, and a hyperbolic distance metric to capture hierarchical structure. Evaluated on CALVIN and BEHAVIOR robotic environments, PHIER significantly outperforms existing methods, achieving 22.5% and 8.3% improvements in out-of-distribution accuracy, respectively. It also demonstrates robust zero- and few-shot transfer to real-world tasks, narrowing the performance gap with large pretrained vision-language models.

## Method Summary
PHIER takes a 2D RGB image and text query (e.g., `NextTo(cup, plate)`) as input and predicts whether the predicate holds for the specified objects. The model uses an object-centric encoder that generates object masks from text queries using CLIP, encodes the masked image, and combines it with a predicate text embedding. The combined representation is projected into a Poincaré ball (hyperbolic space) where self-supervised losses (predicate triplet loss and norm regularization) encourage semantically related predicates to cluster closer together and reflect hierarchical specificity through distance from the origin. The model is trained with a combined supervised and self-supervised loss, and demonstrates strong few-shot generalization to novel predicates.

## Key Results
- PHIER achieves 22.5% improvement in out-of-distribution accuracy on CALVIN compared to existing methods
- PHIER achieves 8.3% improvement in out-of-distribution accuracy on BEHAVIOR compared to existing methods
- PHIER narrows the performance gap with large pretrained vision-language models on real-world transfer tasks (337 examples)

## Why This Works (Mechanism)

### Mechanism 1
Encoding predicate hierarchies in hyperbolic space enables few-shot generalization by structuring the latent space to reflect semantic and hierarchical relationships between predicates. The model uses self-supervised losses (predicate triplet loss and norm regularization) informed by an LLM to encourage semantically related predicates to cluster closer together and hierarchical specificity to be reflected by distance from the origin in a hyperbolic (Poincaré ball) latent space.

### Mechanism 2
Object-centric scene encoding improves generalization by focusing the model's attention on relevant image regions, reducing spurious correlations with background features. A dedicated encoder generates object masks from the input image conditioned on object names from the query using CLIP. The masked image is then encoded, isolating the visual features of the relevant objects.

### Mechanism 3
Generalization to novel predicates is achieved by placing their representations in a semantically consistent location within the structured latent space, allowing the model to leverage features learned for related, known predicates. When fine-tuning on a novel predicate with few examples, the model's existing structure guides its representation. A novel predicate (e.g., OnRight) that is semantically similar to a known one (e.g., OnLeft) will be pulled to a similar region of the latent space due to the pre-existing structured space and the fine-tuning process.

## Foundational Learning

### Concept: Hyperbolic Geometry (Poincaré Ball)
- Why needed here: Used as the latent space to embed representations. Its property of exponentially increasing area with distance from the origin makes it suitable for representing tree-like hierarchical structures with low distortion.
- Quick check question: Can you explain why the Poincaré ball is better suited than Euclidean space for embedding hierarchical data?

### Concept: Self-Supervised Triplet Learning
- Why needed here: The model is not given an explicit predicate hierarchy as labels. Triplet learning allows it to learn a structured space by enforcing distance constraints (anchor-positive vs. anchor-negative) derived from an external knowledge source (LLM).
- Quick check question: What is the role of the triplet loss `L_triplet` in structuring the latent space?

### Concept: Object-Centric Representation
- Why needed here: State classification depends on specific objects and their relations. An object-centric encoder isolates relevant visual information, preventing the model from learning from background noise.
- Quick check question: How does the model generate an object mask from a text query?

## Architecture Onboarding

### Component map:
Input (Image + Text Query) -> Query Parsing (Objects + Predicate) -> Object-Centric Encoder (CLIP-based masking) -> Predicate Encoder (BERT) -> Hyperbolic Projection (Poincaré ball) -> Self-Supervised Losses (Triplet + Norm) -> Classifier (MLP)

### Critical path:
1. Accurate object localization is critical for extracting meaningful features
2. The quality of the LLM-derived hierarchy directly impacts the usefulness of the structured latent space
3. The hyperbolic distance metric is essential for correctly measuring distances in the non-Euclidean latent space

### Design tradeoffs:
- Soft vs. Hard Hierarchy Enforcement: The paper uses self-supervised losses to encourage hierarchy rather than hard-coding it. This allows for data-driven refinement but may lead to imperfect hierarchy encoding
- LLM Dependency: Relies on an LLM to define the hierarchy, which might not be perfect or match visual reality
- Complexity vs. Interpretability: The architecture is more complex than a standard classifier but offers a more interpretable latent space

### Failure signatures:
- Poor object localization: The model makes predictions based on incorrect image regions
- Misclassified predicate relations: The LLM gives an incorrect hierarchy, leading to a poorly structured latent space
- Collapse of latent space: Self-supervised losses might dominate, causing representations to collapse or not separate meaningfully for classification

### First 3 experiments:
1. Ablation Study (Object-Centric Encoder): Train the model without the object-centric encoder to quantify the performance drop, especially on OOD and novel object tasks
2. Ablation Study (Hyperbolic Space): Train the model in Euclidean space using the same triplet and norm losses to evaluate the specific benefit of the hyperbolic metric
3. Sensitivity to LLM Hierarchy: Manually introduce errors into the LLM-provided predicate relationships and observe the impact on generalization performance

## Open Questions the Paper Calls Out

### Open Question 1
Can a fully emergent predicate hierarchy, learned without LLM-injected priors, achieve comparable or better generalization than PHIER's language-informed hierarchy? The paper suggests exploring ways of training a model to infer pairwise predicate relations with weak supervision instead of injecting relation priors through a language model.

### Open Question 2
Does explicitly enforcing a hard hierarchical constraint in the forward pass improve generalization compared to PHIER's soft constraint approach? The authors note that exploring environments with unique predicate hierarchies that can be encoded through explicit enforcement would showcase the effect of an explicitly hierarchical version of PHIER.

### Open Question 3
To what extent does LLM accuracy in determining predicate relationships affect PHIER's performance, particularly when visual cues contradict linguistic priors? The paper acknowledges that PHIER is potentially limited by the accuracy of the language model in determining pairwise predicate relations, and there may be cases where visual cues from data also matter.

## Limitations
- LLM Dependency: The model's performance heavily depends on the accuracy of the LLM's inference of predicate relationships, with no evaluation of sensitivity to LLM errors
- Hyperbolic Space Complexity: The paper lacks direct ablations comparing performance with and without hyperbolic space to isolate the benefit of the hyperbolic metric
- Object-Centric Encoder Generalization: The object-centric masking strategy relies on CLIP's cross-modal alignment, which may degrade with significant domain shift, but the paper does not quantify this robustness

## Confidence
- High Confidence: The model architecture is clearly specified, and the self-supervised losses are explicitly defined
- Medium Confidence: Claims about hyperbolic space benefits and object-centric encoding are plausible but lack direct ablations
- Low Confidence: The reliance on LLM-inferred hierarchies introduces an uncontrolled variable without error analysis or sensitivity studies

## Next Checks
1. Train an equivalent model using Euclidean space with the same triplet and norm losses to isolate the benefit of the hyperbolic metric
2. Evaluate the model's object localization accuracy on real-world images and visualize the quality of generated masks
3. Introduce controlled errors into the LLM-provided predicate relationships and measure the impact on out-of-distribution and novel predicate generalization