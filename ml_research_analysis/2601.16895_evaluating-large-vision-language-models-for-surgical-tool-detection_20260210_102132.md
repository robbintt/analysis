---
ver: rpa2
title: Evaluating Large Vision-language Models for Surgical Tool Detection
arxiv_id: '2601.16895'
source_url: https://arxiv.org/abs/2601.16895
tags:
- surgical
- detection
- instrument
- vlms
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the effectiveness of large vision-language\
  \ models (VLMs) for surgical tool detection, a fundamental visual perception task\
  \ in robotic surgery. Three state-of-the-art VLMs\u2014Qwen2.5, LLaVA1.5, and InternVL3.5\u2014\
  are assessed on the GraSP dataset under zero-shot and fine-tuned settings, with\
  \ Grounding DINO as a baseline."
---

# Evaluating Large Vision-language Models for Surgical Tool Detection

## Quick Facts
- arXiv ID: 2601.16895
- Source URL: https://arxiv.org/abs/2601.16895
- Reference count: 15
- Key outcome: Qwen2.5 achieves superior instrument detection performance, excelling in recognition while showing stronger zero-shot generalization than baseline.

## Executive Summary
This study evaluates large vision-language models (VLMs) for surgical tool detection using three state-of-the-art models: Qwen2.5, LLaVA1.5, and InternVL3.5. The models are assessed on the GraSP dataset under both zero-shot and fine-tuned settings, with Grounding DINO as a baseline. Qwen2.5 consistently outperforms others in instrument recognition and zero-shot generalization, while Grounding DINO shows stronger localization capabilities. The findings demonstrate VLMs' potential for comprehensive surgical scene understanding and highlight their promise for advancing general-purpose surgical AI systems.

## Method Summary
The study evaluates VLMs for surgical tool detection by extracting frames from surgical videos (1 fps from GraSP dataset) and processing them through VLM backbones with optional LoRA adapters. Models generate JSON predictions containing instrument categories and bounding boxes, which are evaluated using a modified TIDE framework that accounts for VLMs' lack of confidence scores. Zero-shot evaluation uses instrument list prompts, while fine-tuning employs simple detection prompts with Rank-8 LoRA parameters (5 epochs, batch size 4, learning rate 1e-4).

## Key Results
- Qwen2.5 consistently achieves superior detection performance, excelling in instrument recognition across both zero-shot and fine-tuned settings
- Grounding DINO demonstrates stronger localization capabilities despite Qwen2.5's superior classification performance
- Fine-tuning with LoRA adapters substantially reduces errors across all error categories for all evaluated models

## Why This Works (Mechanism)

### Mechanism 1
Large-scale image-text pretraining enables semantic transfer that improves surgical instrument classification. VLMs pretrained on general vision-language data develop rich semantic representations that transfer to surgical domains, improving recognition even without domain-specific training. Core assumption: Surgical instruments share semantic and visual features with objects encountered during general pretraining. Evidence: Qwen2.5's superior classification performance and related surgical VLM work demonstrating similar transfer benefits. Break condition: If surgical instruments have minimal semantic overlap with pretraining corpora, classification gains diminish.

### Mechanism 2
Detection-specialized architectures (Grounding DINO) achieve superior spatial localization compared to general-purpose VLMs. Object detection models are explicitly trained for bounding box regression and spatial reasoning, enabling more precise localization even across domain shifts. Core assumption: Spatial localization capabilities transfer more reliably than semantic classification across visual domains. Evidence: Grounding DINO's superior localization performance and consistent advantage in background error categories. Break condition: If target domain requires fine-grained spatial understanding not captured in pretraining, localization degrades.

### Mechanism 3
Parameter-efficient LoRA fine-tuning substantially reduces detection errors across all error categories. Low-rank adaptation allows models to learn domain-specific patterns while preserving pretrained knowledge, reducing missed detections and misclassifications. Core assumption: Surgical tool detection can be learned through low-rank weight perturbations without catastrophic forgetting. Evidence: Substantial error reduction across all categories after fine-tuning and similar adaptation benefits in related surgical VQA work. Break condition: If LoRA rank is too low or training data is insufficient, adaptation fails to converge.

## Foundational Learning

- **Object Detection vs Classification:**
  - Why needed here: Paper reveals fundamental trade-off—VLMs excel at recognition (what) but struggle with localization (where).
  - Quick check question: Why might a model correctly identify "there's a needle driver" but draw an inaccurate bounding box?

- **Zero-shot Transfer:**
  - Why needed here: Critical deployment scenario where models must perform without surgical-specific training data.
  - Quick check question: What properties of the pretraining data determine zero-shot generalization success?

- **TIDE Error Framework:**
  - Why needed here: VLMs don't output confidence scores, making standard mAP inapplicable; TIDE enables confidence-independent evaluation.
  - Quick check question: Why does the paper count "Missed GT" errors differently from prediction-based error categories?

## Architecture Onboarding

- **Component map:**
  Input: Surgical image (H×W×3) + text prompt → VLM backbone: Qwen2.5-7B / LLaVA1.5-7B / InternVL3.5-8B → Optional: LoRA adapter (Rank-8, trained 5 epochs, lr=1e-4) → Output: JSON with instrument categories and bounding box coordinates → Evaluation: TIDE framework decomposing errors into 6 categories (Miss, Bkg, Cls+Loc, Dup, Cls, Loc)

- **Critical path:**
  1. Frame extraction from surgical video (1 fps from GraSP dataset)
  2. Prompt engineering differs: zero-shot includes instrument list; fine-tuned uses simple "Detect surgical instruments"
  3. Model inference generates JSON predictions
  4. IoU matching with ground truth (foreground threshold=0.5, background=0.1)
  5. TIDE error categorization and analysis

- **Design tradeoffs:**
  - Qwen2.5: Better classification, worse localization → choose for recognition tasks
  - Grounding DINO: Better localization, worse classification → choose for spatial precision tasks
  - Zero-shot: No training cost, higher errors vs Fine-tuned: ~2,324 labeled frames required, substantially lower errors
  - LoRA Rank-8: Balances adaptation capacity with compute efficiency

- **Failure signatures:**
  - LLaVA1.5 zero-shot: Predicts all 7 categories per image with identical/random boxes (4,220 Cls+Loc errors)
  - InternVL zero-shot: Severe under-prediction (2,623 missed GT of 2,861 total)
  - Qwen zero-shot: Higher background false positives than baseline (337 vs 272)
  - Fine-tuned GDINO: Duplicate detection spikes (79 duplicates vs 0 for Qwen)

- **First 3 experiments:**
  1. Establish zero-shot baseline with Qwen2.5 on GraSP test set (1,125 frames), record all 6 TIDE error categories.
  2. Apply Rank-8 LoRA fine-tuning with paper hyperparameters (5 epochs, batch=4, lr=1e-4), measure error reduction per category.
  3. Compare fine-tuned Qwen vs fine-tuned Grounding DINO on classification vs localization trade-off to validate complementary strengths.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset Constraints: Study relies exclusively on GraSP dataset, limiting generalizability to other surgical domains with only 7 specific instrument categories.
- VLM Architecture Mismatch: Classification-focused VLMs adapted for detection tasks they weren't originally designed for, potentially constraining performance.
- Evaluation Framework Assumptions: Modified TIDE framework without confidence thresholds may not fully capture detection quality or model calibration.

## Confidence
- **High Confidence**: Zero-shot performance comparisons and overall detection error trends are well-supported by systematic evaluation across multiple models and error categories.
- **Medium Confidence**: Claims about semantic transfer mechanisms from pretraining require more detailed analysis of pretraining corpus overlap with surgical instruments.
- **Medium Confidence**: Localization vs. classification trade-offs are observed but could benefit from more rigorous spatial error analysis beyond TIDE's general categories.

## Next Checks
1. Evaluate the same VLMs on at least two additional surgical datasets (e.g., M2CAI, Cholec80) to assess domain transfer capabilities and identify dataset-specific limitations.
2. Compare detection performance using VLMs with explicit object detection heads versus the current classification-based approach to quantify architectural impact on localization accuracy.
3. Analyze the semantic overlap between surgical instruments and pretraining data using embedding similarity metrics to validate the transfer learning hypothesis and identify potential gaps.