---
ver: rpa2
title: SLA-Centric Automated Algorithm Selection Framework for Cloud Environments
arxiv_id: '2507.21963'
source_url: https://arxiv.org/abs/2507.21963
tags:
- algorithm
- framework
- performance
- selection
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an SLA-aware algorithm selection framework
  for cloud environments using ensemble machine learning to predict algorithm performance
  (runtime, memory, and optimality gap) under hardware constraints. Applied to the
  0-1 knapsack problem, the framework uses 22 instance-specific features and evaluates
  6 algorithms across 2800 hardware configurations.
---

# SLA-Centric Automated Algorithm Selection Framework for Cloud Environments

## Quick Facts
- arXiv ID: 2507.21963
- Source URL: https://arxiv.org/abs/2507.21963
- Reference count: 11
- Authors: Siana Rizwan; Tasnim Ahmed; Salimur Choudhury
- One-line result: Ensemble ML framework predicts algorithm performance (runtime, memory, optimality gap) for 0-1 knapsack under SLA constraints, achieving F1 > 0.5 and reducing RMSE by up to 45% vs. standalone models

## Executive Summary
This paper proposes an SLA-aware algorithm selection framework for cloud environments that uses ensemble machine learning to predict the performance of different algorithms under hardware constraints. The framework is applied to the 0-1 knapsack problem, using 22 instance-specific features and evaluating 6 algorithms across 2800 hardware configurations. Top-3 ensemble classifiers achieve F1-scores above 0.5 for runtime and optimality gap predictions, while ensemble regressors reduce RMSE by up to 45% compared to standalone models. SHAP analysis identifies capacity-weight ratios and hardware features as key predictors. LLM zero-shot inference underperforms compared to trained models. The framework supports resource-aware decision-making and SLA compliance in dynamic cloud settings.

## Method Summary
The framework uses 200 knapsack instances (from Jooken et al. + synthetic augmentation) with 22 instance-specific features. It evaluates 6 algorithms (Greedy, DP, GA, Branch & Bound, Gurobi, OR-Tools) across 2800 hardware configurations (RAM 4-256GB, CPU cores 8/32). Seven ML models (Logistic/Linear Regression, DT, RF, MLP, SVM, CatBoost, 1D CNN) are trained per algorithm-variant pair with 60-20-20 split. Top-3 equal-weighted ensembles are created and evaluated using F1 (classification) and RMSE/R² (regression). SHAP analysis provides interpretability. RL (Q-learning, SARSA) is used for profit-maximization variants.

## Key Results
- Top-3 ensemble classifiers achieve F1-scores above 0.5 for runtime and optimality gap predictions
- Ensemble regressors reduce RMSE by up to 45% compared to standalone models
- SHAP analysis identifies capacity-weight ratios and hardware features as key predictors
- LLM zero-shot inference underperforms compared to trained ensemble models
- Framework supports resource-aware decision-making and SLA compliance in dynamic cloud settings

## Why This Works (Mechanism)

### Mechanism 1
Ensemble models aggregate predictions from diverse architectures (MLP, Random Forest, CatBoost) to capture distinct non-linear patterns in algorithm behavior, reducing variance without overfitting. Core assumption: training distribution covers operational phase space. Break condition: hardware drift requiring retraining.

### Mechanism 2
Instance-specific structural features (capacity-weight ratios, weight-profit correlations) drive predictions of optimality gaps and runtime complexity. Core assumption: problem instances can be accurately characterized by low-dimensional statistical features. Break condition: adversarial instances decoupling statistical features from actual computational complexity.

### Mechanism 3
A "Decider" module enforces SLA compliance by mapping predicted performance metrics against static thresholds to filter algorithm-hardware pairs. Core assumption: SLA constraints are quantifiable and strictly enforceable. Break condition: noisy neighbor effects violating static thresholds even with accurate predictions.

## Foundational Learning

- **Concept: 0-1 Knapsack Problem & Complexity**
  - Why needed: Understanding the trade-off between exact solvers and heuristics is essential to interpret the "optimality gap" metric and why selection matters
  - Quick check: Can you explain why Dynamic Programming (DP) has a pseudo-polynomial time complexity and how that differs from the polynomial complexity of a Greedy approach?

- **Concept: Ensemble Learning (Heterogeneous)**
  - Why needed: The architecture relies on a "Top-3" ensemble. Understanding why combining different model types stabilizes predictions is crucial
  - Quick check: Why would a Top-3 ensemble potentially outperform a Top-7 ensemble in this specific context (hint: variance vs. bias in included models)?

- **Concept: SHAP (SHapley Additive exPlanations)**
  - Why needed: The paper uses SHAP to identify that hardware features drive memory predictions while problem features drive optimality
  - Quick check: If SHAP values indicate "RAM" is the top predictor for "Peak Memory" but not for "Optimality Gap," what does that imply about the solvers' sensitivity to hardware?

## Architecture Onboarding

- **Component map:** Problem Parser -> Feature Extractor -> ML Engine -> Decider -> Negotiator
- **Critical path:** The **Feature Extractor → ML Engine** link. If features are miscalculated, ensemble predictions for runtime and optimality gap will fail
- **Design tradeoffs:** LLM vs. Trained Models (zero-shot LLMs fail), Static vs. Dynamic SLA (current implementation assumes static thresholds)
- **Failure signatures:** Negative R² in Regression (model worse than mean baseline), Perfect F1-score陷阱 (high scores may indicate constant target values)
- **First 3 experiments:**
  1. Feature Ablation: Remove "Hardware Features" and measure degradation in Memory prediction accuracy
  2. Stress Test the Decider: Generate instances with extremely tight optimality gap constraints and verify algorithm filtering behavior
  3. LLM Fine-Tuning Baseline: Implement RAG or fine-tuning on 2800 samples to compare against ensemble regressors

## Open Questions the Paper Calls Out

- How does the framework's prediction accuracy and SLA compliance rate change when deployed in real-world cloud environments with dynamic resource fluctuations?
- Can Large Language Models (LLMs) be fine-tuned to match or exceed the performance of traditional ensemble regressors for predicting algorithm runtime and memory usage?
- What specific ensemble strategies or meta-learner architectures can resolve the negative R² issues observed in optimality gap regression tasks?

## Limitations

- 22 instance-specific features not fully enumerated, creating reproducibility gaps
- Synthetic instance augmentation described only as "controlled noise injection" without exact parameters
- Top-3 ensemble selection appears somewhat arbitrary without systematic validation of ensemble size optimization
- Decider module's negotiation logic described but not empirically tested in multi-tenant scenarios with dynamic SLA violations

## Confidence

- High confidence: Ensemble learning mechanism and superiority over standalone models (supported by RMSE reduction metrics and F1-scores above 0.5)
- Medium confidence: SHAP-based interpretability findings regarding feature importance (exact 22-feature list remains unspecified)
- Medium confidence: Framework's applicability to SLA compliance (Decider logic is sound but not stress-tested in realistic cloud environments)

## Next Checks

1. Conduct feature ablation study removing hardware features to quantify their exact contribution to memory prediction accuracy
2. Stress test the Decider module with extremely tight optimality gap constraints to verify algorithm filtering behavior
3. Implement a RAG or fine-tuned LLM baseline to compare against ensemble regressors, given zero-shot LLMs' documented underperformance