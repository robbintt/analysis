---
ver: rpa2
title: Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning
arxiv_id: '2512.00961'
source_url: https://arxiv.org/abs/2512.00961
tags:
- uni000003ec
- reward
- video
- uni0000011e
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GenReward, a framework that leverages pretrained
  video diffusion models to provide goal-driven reward signals for reinforcement learning
  agents. The method generates task-specific videos using a fine-tuned video diffusion
  model, then extracts video-level rewards via similarity between agent trajectories
  and goal videos, and frame-level rewards via a learned forward-backward representation.
---

# Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.00961
- Source URL: https://arxiv.org/abs/2512.00961
- Reference count: 35
- This paper proposes GenReward, a framework that leverages pretrained video diffusion models to provide goal-driven reward signals for reinforcement learning agents.

## Executive Summary
This paper introduces GenReward, a framework that uses pretrained video diffusion models to generate goal-driven rewards for reinforcement learning agents. The method fine-tunes video diffusion models on domain-specific datasets, then uses the video encoder to compute latent similarity between agent trajectories and generated goal videos as video-level rewards. Additionally, a forward-backward representation network provides frame-level rewards by measuring the probability of reaching goal states. Experiments on Meta-World tasks demonstrate significant improvements over existing methods, achieving up to 504% improvement over dense rewards in tasks like Pick Out of Hole, and 822% in Bin Picking.

## Method Summary
GenReward fine-tunes a pretrained video diffusion model (CogVideoX) on domain-specific manipulation videos, then generates goal videos from task text prompts. The framework extracts two types of rewards: video-level rewards via cosine similarity between 3D VAE latent representations of agent observation sequences and goal videos, and frame-level rewards via a forward-backward network that approximates the probability of reaching goal states. These intrinsic rewards are combined with environmental rewards and fed into DreamerV3 for policy training. The forward-backward network is trained on replay buffer transitions for the first 100k steps then frozen.

## Key Results
- Achieves 504% improvement over dense reward baseline in Pick Out of Hole task
- Achieves 822% improvement over dense reward baseline in Bin Picking task
- Demonstrates effective cross-domain transfer from real-world videos (RT-1, Bridge) to simulated Meta-World tasks

## Why This Works (Mechanism)

### Mechanism 1
Video-level latent similarity provides trajectory-aligned reward signals without hand-engineering. A 3D Causal VAE encoder compresses both agent observation sequences and generated goal videos into shared latent vectors. Cosine similarity between these latents serves as the video-level reward, incentivizing temporal behavior alignment. The core assumption is that the pretrained video encoder captures semantically meaningful temporal dynamics that transfer across similar manipulation domains. Evidence shows related work ViVa similarly uses video-trained value functions for RL guidance, suggesting cross-validation of video-as-reward paradigms.

### Mechanism 2
Forward-Backward representations decompose long-horizon goals into actionable state-action rewards. The method learns two functions: F(s,a,z) and B(s'), where their inner product approximates the probability of reaching goal state s' from (s,a) under policy π_z. The goal embedding z is derived from a CLIP-selected key frame encoded by DINOv3. The core assumption is that the low-rank factorization sufficiently approximates the true successor measure for the target goal distribution. Evidence shows the method encourages agents to take actions more likely to reach the goal representation.

### Mechanism 3
CLIP-based frame selection grounds text descriptions in visually relevant goal states. Given a generated video, CLIP computes similarity between each frame and the task text description. The highest-scoring frame becomes the goal image I* for FB reward computation. The core assumption is that CLIP text-image alignment captures task-relevant visual features rather than superficial similarities. Evidence acknowledges that CLIP may select non-optimal frames in some cases, yet performance gains persist, suggesting robustness.

## Foundational Learning

- **Concept**: Diffusion models and latent VAE spaces - Why needed: CogVideoX operates on 3D VAE latents; understanding compression and denoising objectives is essential for adaptation. Quick check: Can you explain why the forward process adds Gaussian noise and how the reverse process reconstructs latents?

- **Concept**: Successor features / forward-backward representations - Why needed: The FB network approximates long-term state occupancy; understanding the Bellman residual is critical. Quick check: Why does minimizing the Bellman residual encourage temporally proximate states to have similar embeddings?

- **Concept**: Vision-language alignment (CLIP, DINOv3) - Why needed: CLIP selects goal frames; DINOv3 encodes goal images. Understanding semantic embeddings is necessary. Quick check: What is the difference between CLIP's contrastive objective and DINOv3's self-supervised features?

## Architecture Onboarding

- **Component map**: CogVideoX-5B-I2V (video generation) -> 3D Causal VAE (latent encoding) -> CLIP (frame selection) -> DINOv3 (goal encoding) -> Forward-Backward Network (reward computation) -> DreamerV3 (policy learning)

- **Critical path**: 1) Fine-tune CogVideoX on domain videos using diffusion loss. 2) Generate goal video from task prompt; select goal frame via CLIP. 3) Encode goal video and agent history with 3D VAE; compute r_video every 128 steps. 4) Train FB network on replay buffer for first 100k steps using Bellman residual; freeze thereafter. 5) Compute r_FB; combine rewards and update DreamerV3 policy.

- **Design tradeoffs**: Video-level weight α: Too small → no behavior mimicking; too large → impedes policy learning. FB weight β: Too small → insufficient guidance; too large → overfitting to generated goals. FB training duration: Authors freeze after 100k steps for stability; earlier freezing may underfit.

- **Failure signatures**: Policy fails to contact objects: Likely TADPoLe-like issue—reward misalignment from text-only guidance. Agent reaches wrong goal state: CLIP frame selection failure. No improvement over dense reward: Check α and β scaling; verify video generation quality.

- **First 3 experiments**: 1) Replicate single-task result (e.g., Bin Picking) with Bridge dataset videos; confirm ~800% improvement over dense reward baseline. 2) Ablate video-level reward (set α=0) and FB reward (set β=0) separately to validate contribution of each component. 3) Test cross-domain transfer: Use RT-1 videos for Meta-World tasks and compare against same-domain Bridge videos.

## Open Questions the Paper Calls Out

- **Question**: Can the computational overhead of computing video-level and forward-backward rewards during training be reduced without sacrificing task performance? The authors state this as a limitation but don't explore methods to amortize or simplify reward computation.

- **Question**: Can more robust frame-level goal selection methods improve performance compared to CLIP-based selection? Supplementary material shows CLIP may select non-optimal frames in some cases, suggesting room for improvement.

- **Question**: How well does GenReward transfer to real-world robotic manipulation beyond the simulated Meta-World benchmark? All experiments use Meta-World simulation, though generated videos come from real-world datasets like RT-1 and Bridge.

## Limitations

- **Limitation 1**: Computational overhead from computing video-level and forward-backward rewards during training, which may limit scalability to longer-horizon tasks or real-time applications.

- **Limitation 2**: CLIP-based goal frame selection can fail to identify the most relevant goal state, potentially selecting frames that don't fully capture task objectives.

- **Limitation 3**: Cross-domain generalization relies on similarity between generated videos and target tasks, but systematic evaluation of domain gap tolerance is lacking.

## Confidence

- **High Confidence**: The core claim that video diffusion models can generate useful goal-driven rewards is well-supported by quantitative results (504-822% improvements over dense rewards). The methodology for video-level reward computation via latent similarity is clearly specified and reproducible.

- **Medium Confidence**: The forward-backward representation mechanism shows promise but lacks direct validation of its approximation quality. The claim that "low-rank factorization in Eq. (7) sufficiently approximates the true successor measure" is theoretically grounded but empirically untested.

- **Low Confidence**: The cross-domain transfer claims (Figure 10) show relative performance differences but don't establish absolute performance thresholds or failure modes when domain gaps are large.

## Next Checks

1. **Ablation of video diversity**: Systematically vary the number and diversity of training videos per domain (RT-1, Bridge, RLBench) and measure impact on reward quality and cross-domain transfer performance.

2. **Temporal alignment validation**: Quantify how well the video-level reward correlates with actual task progress by correlating latent similarity scores with normalized episode returns across different time horizons.

3. **CLIP frame selection robustness**: Implement an ensemble of top-3 CLIP-selected frames as goal states and measure variance in policy performance to assess sensitivity to this critical component.