---
ver: rpa2
title: Exploitation Is All You Need... for Exploration
arxiv_id: '2508.01287'
source_url: https://arxiv.org/abs/2508.01287
tags:
- exploration
- agent
- learning
- memory
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes that exploration can emerge naturally from
  pure exploitation in structured environments, without explicit exploration bonuses.
  The key insight is that when the environment exhibits recurring structure, the agent
  has sufficient memory, and long-term credit assignment is possible, a greedy policy
  can exhibit exploratory behavior.
---

# Exploitation Is All You Need... for Exploration

## Quick Facts
- arXiv ID: 2508.01287
- Source URL: https://arxiv.org/abs/2508.01287
- Reference count: 5
- Primary result: Exploration can emerge from pure exploitation in structured environments when the agent has sufficient memory and environmental structure allows information retention across episodes.

## Executive Summary
This paper challenges the conventional wisdom that exploration requires explicit exploration bonuses by demonstrating that exploration can emerge naturally from pure exploitation under specific conditions. The key insight is that when environments exhibit recurring structure, agents have sufficient memory capacity, and long-term credit assignment is possible, greedy policies can exhibit exploratory behavior without explicit exploration mechanisms. The authors validate this through ablation studies in multi-armed bandits and gridworlds, showing that emergent exploration collapses when either structure or memory is removed. Surprisingly, long-term credit assignment is not always essential—in bandit tasks, transformer-based value functions can approximate Thompson Sampling even with zero temporal discount.

## Method Summary
The authors train transformer-based agents on repeated bandit tasks and gridworlds using a pure exploitation objective (no exploration bonuses). They use pretrained Llama 3.2 3B transformers with LoRA adapters, training offline with DQN loss on tokenized sequences of (action, observation, reward) tuples. The core ablation variables are block length n (number of repeated episodes per task), context window X (memory capacity), and episode discount γepisode. They systematically vary these parameters to identify when exploration emerges from pure exploitation versus when it requires explicit exploration mechanisms.

## Key Results
- Exploration emerges from pure exploitation when block length n≥3 in bandits (n=1 yields near-random performance)
- Memory capacity X is critical: reducing from 1024 to 32 tokens eliminates emergent exploration
- Pseudo-Thompson Sampling via transformer context modeling can substitute for long-horizon credit assignment in bandits (γepisode=0 works)
- In gridworlds, long-horizon credit assignment improves exploration (γepisode=0.9 yields 0.670 vs 0.408 for γepisode=0)
- Removing either environmental structure or agent memory eliminates emergent exploration

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exploration emerges from pure exploitation when recurring environmental structure allows information gathered in early episodes to retain value across a task block.
- **Mechanism:** The agent discovers that forgoing immediate reward to gather information increases expected cumulative reward across the block, because the environment parameters remain fixed. This creates an implicit information-seeking incentive without any explicit exploration bonus.
- **Core assumption:** Information depreciates gracefully within a block; the expected number of remaining episodes justifies the exploration cost.
- **Evidence anchors:**
  - [abstract] "recurring environmental structure, where the environment features repeatable regularities that allow past experience to inform future choices"
  - [Section 5.2, Table 1] Performance drops sharply as n decreases: n=30 yields normalized reward 0.704±0.055; n=1 yields 0.043±0.130
  - [corpus] "Is Pure Exploitation Sufficient in Exogenous MDPs..." (FMR=0.67) addresses similar structural conditions for exploitation-only learning
- **Break condition:** If block length n is too short (mean ~1 episode), exploration cost exceeds expected future benefit, and emergent exploration collapses.

### Mechanism 2
- **Claim:** Transformer-based value functions with sufficient context length can produce pseudo-stochastic action selection that approximates Thompson Sampling, even without long-horizon credit assignment.
- **Mechanism:** When γepisode=0, the value function estimates immediate reward only. However, transformers conditioned on long, complex contexts can generate outputs that effectively sample from an approximate reward distribution rather than returning the mean. Combined with DQN's max-operator, this produces Thompson-Sampling-like behavior: sample, select highest, update.
- **Core assumption:** The transformer's in-context learning captures sufficient distributional information to produce meaningful pseudo-samples; this is an empirical observation, not theoretically proven.
- **Evidence anchors:**
  - [abstract] "Surprisingly, removing long-horizon credit assignment (Condition 3) does not always prevent emergent exploration—a result we attribute to the pseudo-Thompson Sampling effect"
  - [Section 5.2, Figure 3] Entropy of action distribution starts high (stochastic) and decreases as experience accumulates, even with γepisode=0
  - [corpus] Limited direct evidence on pseudo-Thompson Sampling in transformers; related work cited (Hataya & Imaizumi 2024) on stochastic optimizer behavior
- **Break condition:** Pseudo-TS effectiveness degrades in temporally extended tasks where the value function must model long-horizon return distributions; nonzero discount factor becomes necessary (gridworlds: 0.408→0.670 improvement).

### Mechanism 3
- **Claim:** Agent memory—implemented via transformer context window—provides cross-episode information retention, enabling the agent to condition decisions on the full history within a task block.
- **Mechanism:** The context window stores actions, observations, and rewards as tokens. Larger X allows the agent to remember which arms were pulled or which grid states were visited in earlier episodes, supporting belief-state-like inference without explicit Bayesian modeling.
- **Core assumption:** The context window is large enough to span multiple episodes; attention mechanisms effectively retrieve relevant historical information.
- **Evidence anchors:**
  - [abstract] "Agent Memory, enabling the agent to retain and utilize historical interaction data"
  - [Section 5.2, Table 2] Reducing X from 1024 to 32 drops normalized reward from 0.704±0.055 to -0.052±0.099 in bandits
  - [corpus] "Demystifying the Mechanisms Behind Emergent Exploration..." (FMR=0.59) examines emergent exploration in unsupervised RL
- **Break condition:** Context window must exceed critical threshold (varies by task complexity); in gridworlds, X=256 already shows degraded performance (0.120±0.056) due to longer episodes.

## Foundational Learning

- **Concept: Meta-reinforcement learning (learning to learn)**
  - **Why needed here:** The entire framework depends on training across a distribution of repeated tasks, where the agent must internalize exploration strategies within its policy rather than receiving explicit exploration rewards.
  - **Quick check question:** Can you explain why a meta-RL agent might behave differently on episode 1 vs. episode 10 of the same task block?

- **Concept: Exploration-exploitation tradeoff**
  - **Why needed here:** The paper's central claim is that this "tradeoff" is not a true dichotomy under certain conditions; understanding conventional approaches (ε-greedy, UCB, Thompson Sampling) is necessary to appreciate what's being challenged.
  - **Quick check question:** Why does Thompson Sampling explore more efficiently than ε-greedy in finite-horizon bandits?

- **Concept: Credit assignment in RL (temporal discounting)**
  - **Why needed here:** The surprising pseudo-TS result hinges on what happens when credit assignment is removed (γepisode=0); understanding how discount factors propagate delayed rewards is essential.
  - **Quick check question:** What happens to the effective planning horizon when γ=0 vs. γ=0.9?

## Architecture Onboarding

- **Component map:**
  - Pretrained Llama 3.2 3B transformer (frozen weights + LoRA adapters, r=32, α=32) -> Tokenized sequence of (action, observation, reward) tuples -> Q-values for each valid action -> DQN loss with action masking -> LoRA backprop update

- **Critical path:**
  1. Generate offline data streams with random actions until each stream contains ≥20X tokens
  2. Sample batches (tokens per batch held constant across X values)
  3. Forward pass through transformer to obtain Q-values
  4. Compute DQN loss with action masking
  5. Update via LoRA backprop; periodically update target network via Polyak averaging (α=0.1)

- **Design tradeoffs:**
  - **Larger X:** More memory, better cross-episode retention, but higher compute and memory cost
  - **Larger n:** More recurring structure, stronger emergence, but longer training time per task block
  - **γepisode=0:** Simpler training signal, but relies on pseudo-TS which may not scale to complex tasks
  - **LoRA vs. full fine-tuning:** Efficient but may limit expressiveness of value function

- **Failure signatures:**
  - **Exploration collapse:** Performance near random baseline → check if n is too small or X is insufficient
  - **Stuck in wall-hitting loops:** High state occupation on non-goal states (noted in gridworld failures)
  - **No pseudo-TS emergence:** Entropy remains low from start → context may be insufficiently diverse or transformer capacity insufficient

- **First 3 experiments:**
  1. **Bandit ablation (n sweep):** Reproduce Table 1 with n∈{1,3,10,30}, X=1024, γepisode=0.9; verify that exploration collapses at n=1
  2. **Memory ablation (X sweep):** Reproduce Table 2 with X∈{32,64,128,256,1024}, n=30, γepisode=0.9; identify critical threshold for your compute budget
  3. **Pseudo-TS verification:** Run bandit with γepisode=0, X=1024, n=30; measure entropy over time as in Figure 3 to confirm stochastic-to-deterministic transition

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can emergent exploration scale to high-dimensional continuous control or complex visual domains without explicit exploration bonuses?
- Basis in paper: [explicit] Section 6.4 (Limitations) states the results "may not generalize to more complex or high-dimensional tasks" and calls for further research on scalability.
- Why unresolved: The study is restricted to multi-armed bandits and simple gridworlds (Frozen Lake).
- What evidence would resolve it: Successful replication of emergent exploration in benchmarks like Atari or MuJoCo using the proposed greedy objective.

### Open Question 2
- Question: What is the theoretical foundation for the "pseudo-Thompson Sampling" effect observed in transformer value functions?
- Basis in paper: [explicit] Section 6.2 and 6.4 note the mechanism is "empirical and not theoretically established," suggesting the phenomenon relies on transformers approximating distributional samples.
- Why unresolved: The authors provide an intuitive link to context-dependent stochasticity but lack a formal proof or rigorous theoretical derivation.
- What evidence would resolve it: A theoretical framework proving how specific inductive biases in transformers enable this approximation, or specific ablations isolating the architectural requirements.

### Open Question 3
- Question: Under what specific conditions does the pseudo-Thompson Sampling mechanism fail to substitute for long-horizon credit assignment?
- Basis in paper: [inferred] Section 6.2 notes pseudo-TS becomes less accurate as "temporal reasoning horizon grows," but the exact boundary where γ=0 fails remains undefined.
- Why unresolved: The paper shows γ=0 works in bandits but hinders gridworlds, yet does not fully map the dependency between task horizon and the need for explicit credit assignment.
- What evidence would resolve it: A controlled study varying temporal horizon length to identify the precise threshold where non-zero discount factors become necessary.

## Limitations

- The results may not generalize to high-dimensional continuous control or complex visual domains
- The pseudo-Thompson Sampling mechanism lacks theoretical foundation and may be specific to bandit-like reward distributions
- The study uses offline learning with pre-collected data; online learning scenarios remain untested
- The critical memory threshold varies by task complexity and is not fully characterized

## Confidence

**High Confidence** (Mechanism 1 - Recurring Structure): The relationship between block length n and exploration emergence is well-supported by ablation studies. The sharp performance drop at n=1 (0.704→0.043) provides strong empirical evidence that structure is necessary.

**Medium Confidence** (Mechanism 2 - Pseudo-TS): While the bandit results are compelling, the underlying mechanism is empirical rather than theoretical. The claim that transformers produce approximate Thompson Sampling samples is an observation requiring further validation, especially in non-stationary or more complex reward distributions.

**Medium Confidence** (Mechanism 3 - Memory): The context window ablation is convincing, but the critical threshold varies by task. The claim that X must exceed some task-dependent minimum is supported, but the exact relationship between task complexity and required memory is not characterized.

## Next Checks

1. **Transfer to continuous control tasks**: Test whether pseudo-TS via transformers can enable exploration in continuous-action domains (e.g., MuJoCo locomotion tasks). This would validate whether the bandit-specific mechanism generalizes beyond discrete action spaces.

2. **Online vs. offline comparison**: Implement the same architecture with online learning and compare exploration behavior and performance. This would determine whether the offline pretraining assumption is critical or merely convenient.

3. **Architecture ablation study**: Replace the Llama transformer with smaller or differently-pretrained models (e.g., T5, BERT, or trained from scratch) to determine whether emergent exploration is architecture-dependent or a more general property of large language models.