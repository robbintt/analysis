---
ver: rpa2
title: 'UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content
  Detection and Localization'
arxiv_id: '2510.23023'
source_url: https://arxiv.org/abs/2510.23023
tags:
- image
- methods
- images
- detection
- localization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UniAIDet, a comprehensive benchmark for detecting
  and localizing AI-generated image content. Unlike prior work, UniAIDet covers diverse
  image categories (photographic and artistic) and generative model types (text-to-image,
  image-to-image, inpainting, editing, and deepfake), making it the first large-scale,
  wide-coverage dataset for this task.
---

# UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization

## Quick Facts
- **arXiv ID:** 2510.23023
- **Source URL:** https://arxiv.org/abs/2510.23023
- **Reference count:** 21
- **Primary result:** Introduces UniAIDet, the first large-scale benchmark covering diverse generative models and image categories for AI-generated content detection and localization

## Executive Summary
This paper introduces UniAIDet, a comprehensive benchmark for detecting and localizing AI-generated image content. Unlike prior work, UniAIDet covers diverse image categories (photographic and artistic) and generative model types (text-to-image, image-to-image, inpainting, editing, and deepfake), making it the first large-scale, wide-coverage dataset for this task. The benchmark includes 80k images and supports fine-grained localization through provided masks. Evaluations reveal that existing methods struggle with partial synthesis models and artistic images, highlighting significant generalization gaps. Detection and localization tasks are generally consistent, but current methods fail notably on edited and inpainted images. The study concludes that robust detection requires improved generalization across generative models and image types. UniAIDet provides a valuable resource for advancing research in AI-generated content detection.

## Method Summary
UniAIDet is constructed by collecting real images from MSCOCO, NYTimes800k (photos), and WikiArt, Danbooru (art), then generating synthetic images using 20 models across five categories: text-to-image, image-to-image, inpainting, editing, and deepfake. Prompts and instructions are generated by Gemma3-4B MLLM. Masks are automatically generated using SAM for inpainting, difference thresholding for editing, and native masks for deepfake. The benchmark includes 80k images and evaluates 14 existing detection and localization methods in zero-shot settings, measuring both binary classification accuracy and pixel-level localization performance.

## Key Results
- Existing detection methods show significant performance degradation on partial synthesis models (editing, inpainting) compared to holistic models
- CLIP-based detection methods exhibit notable performance drops on artistic images, while frequency-based methods maintain better generalization
- Detection and localization performance show strong positive correlation across partial synthesis models, suggesting shared underlying features
- Current localization methods exhibit bias toward foreground objects, failing to detect background modifications or style edits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified benchmark construction reveals generalization gaps by systematically covering diverse generative model types and image categories
- Mechanism: The benchmark spans 5 generative model categories (text-to-image, image-to-image, inpainting, editing, deepfake) across photographic and artistic images with 80k images from 20 models, enabling identification of performance gaps that narrower benchmarks miss
- Core assumption: Comprehensive coverage of generative model types and image categories will expose generalization failures that limited benchmarks cannot detect
- Evidence anchors:
  - [abstract] "UniAIDet covers a wide range of generative models, including text-to-image, image-to-image, image inpainting, image editing, and deepfake models"
  - [section 3.3] Describes systematic generation process for each model category with specific models used
  - [corpus] Neighbor papers like SFLD focus on reducing content bias, but lack this benchmark's breadth across partial synthesis models
- Break condition: If training data distribution becomes too dissimilar from real-world deployment distributions, benchmark coverage may not predict deployment performance

### Mechanism 2
- Claim: Detection and localization performance correlate positively across partial synthesis models
- Mechanism: When detection&localization methods correctly identify AI-generated regions (high mIoU), they also correctly classify images as synthetic (high f.Acc), suggesting shared underlying features for both tasks
- Core assumption: The visual features that enable region localization also support binary detection decisions
- Evidence anchors:
  - [section 4.2.2] "A clear observation is that f.Acc and mIoU exhibit nearly identical trends across both methods"
  - [section 4.2.2] "Detection and localization do not conflict with each other"
  - [corpus] Weak explicit discussion of detection-localization correlation in neighbor papers
- Break condition: If methods are optimized separately for detection vs localization, correlation may weaken or reverse

### Mechanism 3
- Claim: Frequency-based features generalize better across image categories (photo vs art) than CLIP-based features
- Mechanism: Frequency-based methods (AIDE, SAFE) maintain more stable performance between photographic and artistic images, while CLIP-based methods show notable degradation on artistic content
- Core assumption: Frequency artifacts from generative models are category-agnostic, while CLIP embeddings may be category-biased
- Evidence anchors:
  - [section 4.2.1] "CLIP-based methods (e.g., DeeCLIP) and DRCT exhibit a notable performance drop on the Art split"
  - [section 4.2.1] "frequency-based methods demonstrate fine generalization ability on artistic images"
  - [corpus] SFLD neighbor paper also addresses content bias but through different approach
- Break condition: If future generative models better preserve frequency characteristics, frequency-based detection may fail

## Foundational Learning

- Concept: Holistic vs Partial Synthesis Models
  - Why needed here: The benchmark's core taxonomy distinguishes fully-generated images (text-to-image, image-to-image) from partially-modified images (inpainting, editing, deepfake), which is critical for understanding generalization failures
  - Quick check question: Given an image where only the background is replaced by an AI model, is this holistic or partial synthesis?

- Concept: Detection vs Localization Task Formulation
  - Why needed here: The benchmark evaluates both binary classification (real/synthetic) and pixel-level localization (which regions are AI-generated), requiring understanding of both task definitions
  - Quick check question: For a deepfake image with a swapped face, what would the ground truth mask contain?

- Concept: Frequency-Domain Artifact Detection
  - Why needed here: Multiple evaluated methods (FreqNet, AIDE, SAFE) leverage frequency-based features, and the paper identifies frequency as crucial for cross-category generalization
  - Quick check question: Why might frequency artifacts be more consistent across photo and art images than semantic features?

## Architecture Onboarding

- Component map:
  - Real image sources: MSCOCO, NYTimes800k (photos); WikiArt, Danbooru (art)
  - Caption/instruction generation: Gemma3-4B MLLM
  - Synthetic image generators: 20 models across 5 categories
  - Mask generation: SAM for inpainting, difference thresholding for editing, native masks for deepfake
  - Evaluation framework: Detection metrics (Acc, AP, f.Acc, r.Acc) and localization metrics (mIoU)

- Critical path:
  1. Collect real images → 2. Generate captions/instructions → 3. Apply generative models → 4. Generate/compute masks → 5. Quality filtering → 6. Benchmark assembly → 7. Method evaluation

- Design tradeoffs:
  - No manual quality selection for T2I/I2I/inpainting to avoid bias vs. potential inclusion of low-quality samples
  - Automatic mask generation without human annotation vs. potential mask inaccuracies
  - Threshold-based region filtering for editing (τ=40, γ=20) vs. risk of missing small edits

- Failure signatures:
  - Detection-only methods: High r.Acc but low f.Acc (good at identifying real images, poor at synthetic)
  - Detection&localization methods: Severe bias toward one error type (e.g., FakeShield over-predicts synthetic)
  - Reconstruction-based methods (DIRE, FIRE): Near-random or worse performance due to training bias
  - Localization methods: Predicting "foreground distinguishable objects" regardless of actual edited region

- First 3 experiments:
  1. Reproduce baseline detection results on Photo split using provided checkpoints for SAFE and DRCT, verify reported Acc and f.Acc values
  2. Test localization methods (SIDA, FakeShield) on partial synthesis models to confirm mIoU correlation with f.Acc trends
  3. Conduct cross-category analysis by evaluating frequency-based vs CLIP-based methods on Art split to validate generalization gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can detection methods be improved to handle partial synthesis models (e.g., image editing and inpainting) where artifacts are confined to small regions rather than the whole image?
- Basis in paper: [explicit] The authors state in Section 4.2.3 (RQ2) that "Existing detection-only methods perform poorly on partial synthesis models," treating them as an "overlooked challenge" compared to holistic synthesis.
- Why unresolved: Current methods (like DRCT or SAFE) excel at holistic detection but fail to generalize to partial edits, indicating that current feature extraction techniques (frequency or reconstruction-based) are insufficient for localized anomalies.
- What evidence would resolve it: The development of a method that achieves high accuracy (comparable to holistic detection) specifically on the Edit and Inpaint subsets of the UniAIDet benchmark.

### Open Question 2
- Question: How can localization architectures be redesigned to identify modified regions in arbitrary locations, rather than defaulting to salient foreground objects?
- Basis in paper: [inferred] In Appendix C.2, the authors note that current localization methods (e.g., SIDA) are biased towards "foreground distinguishable objects" due to their segmentation foundations, failing to detect background or style edits.
- Why unresolved: Existing models seem to rely on objectness priors rather than genuine manipulation detection, causing failure in scenarios where the edited region is the background or a non-distinct texture.
- What evidence would resolve it: A qualitative and quantitative improvement on the UniAIDet "Edit" subset where the predicted masks successfully overlap with ground truth masks for background removal or style modification tasks.

### Open Question 3
- Question: Can the observed consistency between detection confidence and localization accuracy be explicitly leveraged to create a feedback loop that improves performance on artistic content?
- Basis in paper: [explicit] Section 4.2.2 (RQ1) concludes that "detection and localization generally do not conflict," yet Section 4.2.4 shows a significant performance drop on artistic images, suggesting the synergy is not yet being utilized to solve the domain gap.
- Why unresolved: While the paper establishes a correlation, current unified models (like HiFi-Net) still perform poorly. It is unclear if multi-task learning can force the model to learn domain-invariant features for art.
- What evidence would resolve it: A joint detection-localization model that demonstrates a reduced performance gap between the "Photo" and "Art" splits of UniAIDet compared to detection-only baselines.

## Limitations
- Automatic mask generation without human verification may introduce noise in localization ground truth, particularly for editing category
- Evaluation focuses exclusively on zero-shot performance without fine-tuning, limiting conclusions about method adaptation potential
- Several key generative models used (SeedDream, SeedDream-Edit) are closed-source, restricting exact dataset reproduction

## Confidence
- **High Confidence:** Detection-localization correlation findings, performance degradation on artistic images for CLIP-based methods, and overall failure on editing/inpainting models
- **Medium Confidence:** Generalization claims across generative model types, as they depend on the specific models selected and may not extend to future architectures
- **Medium Confidence:** Frequency-based feature superiority for cross-category generalization, pending further validation with more diverse artistic styles

## Next Checks
1. Conduct ablation studies on mask quality by manually verifying a subset of editing masks to quantify noise impact on localization metrics
2. Test fine-tuned versions of top-performing detection methods on UniAIDet to establish upper bounds beyond zero-shot performance
3. Expand evaluation to include emerging generative models (2024-2025 releases) to assess benchmark's predictive validity for future detection challenges