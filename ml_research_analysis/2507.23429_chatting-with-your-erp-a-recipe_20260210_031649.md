---
ver: rpa2
title: 'Chatting with your ERP: A Recipe'
arxiv_id: '2507.23429'
source_url: https://arxiv.org/abs/2507.23429
tags:
- agent
- language
- query
- database
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dual-agent LLM system for natural language
  querying of industrial ERP databases. The approach uses a REACT-based Reasoner agent
  to generate SQL queries and a Critic agent to evaluate and refine them iteratively,
  improving accuracy through structured feedback loops.
---

# Chatting with your ERP: A Recipe

## Quick Facts
- arXiv ID: 2507.23429
- Source URL: https://arxiv.org/abs/2507.23429
- Reference count: 6
- Primary result: Dual-agent LLM system achieves 10/11 correct SQL queries on production ERP database

## Executive Summary
This paper presents a dual-agent LLM system for natural language querying of industrial ERP databases. The approach uses a REACT-based Reasoner agent to generate SQL queries and a Critic agent to evaluate and refine them iteratively, improving accuracy through structured feedback loops. The system was evaluated on a production-grade ERP database using eleven open-weight LLM models, with the best-performing model (Devstral 24B Q4) achieving 10 out of 11 correct responses. The authors highlight challenges including the need for expert-crafted semantic schema descriptions and difficulties in column selection from vague prompts. They propose future work on automated schema description generation and improved intent interpretation to enhance scalability and usability.

## Method Summary
The system employs a dual-agent architecture where a Reasoner generates SQL queries and a Critic evaluates and refines them through iterative feedback. A REACT agent routes user queries to the SQL Agent, which executes the Reasoner-Critic loop. The Reasoner generates queries using handcrafted semantic schema descriptions combined with auto-generated metadata, executes them, and performs self-debugging. The Critic validates syntax, semantics, efficiency, and results, providing structured feedback for refinement. A Human-in-the-Loop module filters vague intents, and a smaller LLM extracts structured outputs from reasoning markdown blocks.

## Key Results
- Best model (Devstral 24B Q4) achieved 10/11 correct responses on expert-curated test queries
- Handcrafted semantic schema descriptions significantly improved query accuracy compared to auto-generated metadata alone
- Dual-agent iterative refinement substantially outperformed single-pass approaches (some models scored 0/11 without iteration)
- System successfully handled production ERP database with 7 tables and 321 columns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A dual-agent collaborative loop between a Reasoner and Critic improves SQL query generation reliability compared to single-pass approaches.
- Mechanism: The Reasoner generates initial SQL queries and attempts execution. Upon failure or suboptimal results, the Critic evaluates syntactic validity, semantic appropriateness, query efficiency, and result subset quality, then returns structured feedback. This initiates iterative refinement cycles (limited to M rounds) that progressively improve query correctness.
- Core assumption: The Critic agent can accurately assess semantic adequacy and that iterative feedback improves rather than degrades query quality.
- Evidence anchors:
  - [abstract] "A novel dual-agent architecture combining reasoning and critique stages was proposed to improve query generation reliability."
  - [section 3.2] "The core design of this agent is a two-agent collaborative loop that is designed to generate an optimal and refined query."
  - [corpus] No directly comparable dual-agent Text-to-SQL evaluations in corpus; mechanism validation limited to this study.
- Break condition: If Critic feedback introduces conflicting signals, or if M rounds are exhausted without convergence, the loop may produce degraded queries or fail.

### Mechanism 2
- Claim: Handcrafted semantic schema descriptions bridge the gap between natural language business concepts and technical database structures.
- Mechanism: An expert-authored markdown schema includes: (1) introduction describing the database role, (2) concept explanations of domain entities, (3) table summaries with purpose and key columns, and (4) high-level relationships including implicit foreign-key-like mappings not formally declared in the database. This contextual layer is injected into the SQL Agent to guide column and table selection.
- Core assumption: Expert domain knowledge can be effectively encoded in natural language descriptions that LLMs can parse and apply correctly.
- Evidence anchors:
  - [section 3.3.1] "The first section of the schema is a carefully curated natural language description of the tables, their semantic representation, their most important columns and their relations."
  - [section 6] "This description includes the domain-specific concepts that the ERP represents, table relationships, roles, and such."
  - [corpus] No corpus evidence on handcrafted schema efficacy; this is a study-specific intervention.
- Break condition: If schema descriptions are incomplete, outdated, or ambiguous, the agent may select wrong columns or tables, especially for vague prompts.

### Mechanism 3
- Claim: A hybrid reasoning-extraction pipeline enables compatibility with LLMs lacking native structured output support while maintaining structured response extraction.
- Mechanism: A larger LLM reasons through tasks and presents results in markdown code blocks. A smaller, structured-output-capable LLM (LLaMA 3.1-8B in experiments) extracts the relevant block matching a target schema. This decouples creative reasoning from deterministic structured generation.
- Core assumption: Markdown code blocks contain sufficiently structured information for reliable extraction by a smaller model.
- Evidence anchors:
  - [section 3.5] "The system implements a hybrid reasoning-extraction pipeline that decouples the LLM reasoning from the less-creative structured output generation."
  - [section 4.3] "LLaMA 3.1-8B was used to extract structured outputs from the reasoning process."
  - [corpus] No comparable hybrid extraction architectures found in corpus neighbors.
- Break condition: If the reasoning model outputs ambiguous or malformed markdown blocks, extraction fails or produces incorrect structured outputs.

## Foundational Learning

- Concept: **REACT Framework (Reason + Act)**
  - Why needed here: The architecture uses REACT as the entry point agent to combine iterative reasoning with action execution, determining whether user intent requires database queries.
  - Quick check question: Can you explain how a REACT agent differs from a single-pass prompting approach in handling multi-step tasks?

- Concept: **Text-to-SQL (Text2SQL)**
  - Why needed here: This is the core task—converting natural language queries into executable SQL statements, which the entire architecture is designed to solve.
  - Quick check question: What are the primary failure modes when LLMs generate SQL from natural language without schema context?

- Concept: **Self-Debugging Strategy**
  - Why needed here: The Reasoner agent implements self-debugging by analyzing execution error messages and iteratively refining queries to resolve syntax errors and invalid references.
  - Quick check question: How does a self-debugging agent use error feedback differently than a one-shot generation approach?

## Architecture Onboarding

- Component map:
  - User input -> REACT Agent intent analysis -> HITL clarification check -> SQL Agent -> Reasoner-Critic loop -> Results -> REACT Agent natural language response

- Critical path:
  1. User input → REACT Agent intent analysis
  2. HITL clarification check (if ambiguous, return to user)
  3. SQL Agent receives summarized intent
  4. Reasoner constructs query with schema context → executes → self-debugs
  5. Critic evaluates → returns feedback or approves
  6. Iteration until approval or round limit
  7. Results → REACT Agent → natural language response

- Design tradeoffs:
  - Performance vs. latency: Multi-agent iteration improves accuracy but increases inference time (some 70B models failed 3-minute limit)
  - Setup cost vs. portability: Handcrafted schema descriptions improve accuracy but require expert effort; prevents plug-and-play deployment
  - Model selection: Two-model approach (large reasoner + small extractor) balances performance and latency

- Failure signatures:
  - 0/11 accuracy (Codestral, Magistral, Deepseek Coder): Likely prompt incompatibility—prompts built around Qwen 2.5 32B
  - Hardware timeout: Models exceeding 100GB VRAM failed under 3-minute constraint
  - Vague prompt failure: Agent cannot disambiguate column selection from ID formats alone
  - Intent loss: REACT agent may skip details when summarizing to SQL Agent

- First 3 experiments:
  1. Baseline model comparison: Run all 11 models on the 12 curated queries with fixed prompts; measure accuracy and latency to identify viable model candidates
  2. Ablation on schema context: Test with and without handcrafted semantic descriptions to quantify their contribution to query accuracy
  3. Round limit sensitivity: Vary M (critic rounds) and N (reasoner attempts) to find minimum iterations needed for convergence without excessive latency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an automated "schema description agent" effectively replace the manual creation of natural language schema descriptions without sacrificing query accuracy?
- Basis in paper: [explicit] The authors state that future work proposes "a new agent to describes the database" to "stop the need of a database expert to write down a natural language description."
- Why unresolved: The current architecture depends on a "handcrafted semantic description" (Section 3.3.1), which limits scalability and usability by requiring expert intervention.
- What evidence would resolve it: A comparative evaluation showing that the automated schema agent achieves query accuracy statistically comparable to the expert-crafted baseline on the same ERP dataset.

### Open Question 2
- Question: What mechanisms can reliably improve column selection and intent interpretation when user prompts are ambiguous or rely on implicit domain knowledge (e.g., ID formats)?
- Basis in paper: [explicit] Section 7 identifies "difficulties understanding which columns has to select based on vague prompts" and lists investigating "improved methods for intent interpretation" as future work.
- Why unresolved: The system currently struggles to map natural language to the correct database identifiers when the prompt lacks specific details or relies on the model's unreliable intuition.
- What evidence would resolve it: Successful execution rates on a specifically curated set of vague or incomplete queries, demonstrating a reduction in "incorrect column selection" errors compared to the current dual-agent setup.

### Open Question 3
- Question: How can the REACT agent's delegation process be modified to prevent the loss of critical user details during the summarization of intent?
- Basis in paper: [explicit] The conclusion notes that the "REACT agent sometimes fails to summarize the entire user intent to the SQL Agent, so some details or corrections might be skipped."
- Why unresolved: The intermediate summarization step creates an information bottleneck, causing the SQL Agent to generate syntactically correct but pragmatically wrong queries due to missing context.
- What evidence would resolve it: An ablation study comparing the current summarization method against passing the full conversational context to the SQL Agent, measuring the retention of specific user constraints.

## Limitations
- Handcrafted semantic schema descriptions require expert knowledge and manual effort, limiting scalability
- System struggles with vague user prompts, particularly when column selection depends on implicit domain knowledge
- Hardware constraints limited evaluation to models fitting within 100GB VRAM, potentially excluding other viable candidates
- Study used a single ERP database with 7 tables, raising questions about generalizability to larger, more complex schemas

## Confidence
- High Confidence: The dual-agent iterative refinement mechanism demonstrably improves query accuracy compared to single-pass generation (10/11 correct vs. 0/11 for worst models)
- Medium Confidence: The handcrafted schema descriptions meaningfully improve performance, though their specific contribution wasn't isolated through ablation
- Medium Confidence: The hybrid reasoning-extraction pipeline successfully decouples creative reasoning from structured output generation, though this architecture choice wasn't benchmarked against alternatives

## Next Checks
1. **Ablation study on schema context**: Test system performance with and without handcrafted semantic descriptions across a broader set of queries to quantify their specific contribution to accuracy gains

2. **Scalability assessment**: Evaluate the system on ERP databases with 50+ tables and 1000+ columns to identify performance degradation points and iteration limits that work at scale

3. **Vague prompt handling benchmark**: Systematically test the system's ability to disambiguate ambiguous user queries across different domains, measuring both success rates and user clarification frequency