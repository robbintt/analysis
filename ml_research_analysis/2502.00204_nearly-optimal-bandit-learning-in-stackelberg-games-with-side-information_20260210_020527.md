---
ver: rpa2
title: Nearly-Optimal Bandit Learning in Stackelberg Games with Side Information
arxiv_id: '2502.00204'
source_url: https://arxiv.org/abs/2502.00204
tags:
- algorithm
- follower
- leader
- contextual
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses online learning in Stackelberg games with
  side information, where a leader commits to a strategy before a follower best-responds.
  The authors improve regret bounds from O(T^{2/3}) to O(T^{1/2}) under bandit feedback
  by reducing the problem to linear contextual bandits in the leader's utility space.
---

# Nearly-Optimal Bandit Learning in Stackelberg Games with Side Information
## Quick Facts
- arXiv ID: 2502.00204
- Source URL: https://arxiv.org/abs/2502.00204
- Reference count: 30
- Primary result: Achieves O(T^{1/2}) regret in Stackelberg games with bandit feedback through reduction to contextual bandits

## Executive Summary
This paper addresses the fundamental challenge of online learning in Stackelberg games with bandit feedback, where a leader commits to a strategy before a follower best-responds. The authors develop a novel reduction to linear contextual bandits that achieves near-optimal O(T^{1/2}) regret, significantly improving upon the previous O(T^{2/3}) bound. The approach leverages side information to construct a contextual bandit instance where the leader's utility vector can be inverted to determine the optimal mixed strategy.

The framework is particularly relevant for applications like second-price auctions and Bayesian persuasion, where the leader must make strategic decisions without observing the follower's utility function directly. The algorithm's effectiveness is demonstrated through both theoretical analysis and numerical simulations, showing improved performance over previous approaches while maintaining computational tractability.

## Method Summary
The core innovation is a reduction from Stackelberg games with bandit feedback to linear contextual bandits in the leader's utility space. The algorithm maintains a contextual bandit instance where each context corresponds to a potential leader strategy, and the reward vector represents the leader's utility across different follower types. By running a contextual bandit algorithm on this instance, the method obtains utility vector recommendations that can be inverted to determine the leader's mixed strategy.

The approach handles both known and unknown leader utilities through careful construction of the contextual bandit problem. For known utilities, the reduction directly maps to a linear contextual bandit; for unknown utilities, the framework incorporates additional structure from side information to maintain the reduction. The algorithm leverages the fact that in Stackelberg games, the follower's best response can be characterized through utility vectors that depend linearly on the leader's strategy.

## Key Results
- Achieves O(T^{1/2}) regret bound, improving upon previous O(T^{2/3}) results for Stackelberg games with bandit feedback
- Successfully reduces Stackelberg game learning to linear contextual bandits with invertible utility vectors
- Extends the approach to handle unknown leader utilities through side information structure
- Demonstrates improved performance in numerical simulations for second-price auctions and Bayesian persuasion

## Why This Works (Mechanism)
The algorithm works by exploiting the linear structure of utility functions in Stackelberg games. When the leader commits to a mixed strategy, the follower's best response can be characterized through a linear optimization problem over the leader's strategy space. This linearity allows the construction of a contextual bandit instance where the leader's utility vector can be directly observed (in the bandit sense) and subsequently inverted to recover the optimal mixed strategy.

The key insight is that in Stackelberg games, the follower's response is determined by utility comparisons that are linear in the leader's strategy parameters. This linearity enables the reduction to contextual bandits, where standard techniques for achieving O(T^{1/2}) regret can be applied. The side information provides the necessary structure to make this reduction computationally tractable and ensures that the utility vector recommendations can be inverted to meaningful strategies.

## Foundational Learning
Linear Contextual Bandits
- Why needed: Provides the regret minimization framework that achieves O(T^{1/2}) bounds
- Quick check: Can solve linear optimization problems with bandit feedback

Stackelberg Game Theory
- Why needed: Defines the strategic interaction model between leader and follower
- Quick check: Understands commitment advantage and best-response dynamics

Utility Inversion
- Why needed: Converts contextual bandit recommendations back to mixed strategies
- Quick check: Can solve linear systems to recover strategy from utility vector

Side Information Structure
- Why needed: Provides the linear structure necessary for the reduction
- Quick check: Identifies when utility functions have exploitable structure

## Architecture Onboarding
Component Map: Leader Strategy Space -> Contextual Bandit Instance -> Utility Vector Recommendation -> Mixed Strategy Output

Critical Path:
1. Construct contextual bandit instance from Stackelberg game structure
2. Run contextual bandit algorithm to obtain utility vector recommendations
3. Invert utility vector to determine leader's mixed strategy
4. Follower best-responds to committed strategy

Design Tradeoffs: The reduction trades the complexity of directly learning in the Stackelberg game space for the well-understood contextual bandit framework, accepting the computational overhead of utility inversion for improved regret bounds.

Failure Signatures: Algorithm failure occurs when the utility vector cannot be inverted (non-invertible matrix), when the follower's best response is not unique, or when the linear structure assumption is violated.

First Experiments:
1. Verify utility vector inversion works correctly on simple two-strategy Stackelberg games
2. Test contextual bandit performance on linear utility structures with known optimal strategies
3. Validate regret bounds on synthetic Stackelberg games with varying follower types

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Assumes existence of approximately optimal mixed strategy in follower's strategy set
- Relies on specific linear structure assumptions for side information that may not generalize
- Numerical validation is limited in scope and doesn't cover diverse Stackelberg game settings
- Does not address computational complexity or implementation challenges for large-scale problems

## Confidence
High confidence in the theoretical regret bound improvement under stated assumptions
Medium confidence in the practical applicability across diverse Stackelberg games
Medium confidence in the scalability to large problem instances

## Next Checks
1. Test algorithm on Stackelberg games where follower's strategy set lacks approximately optimal mixed strategy
2. Evaluate computational performance and convergence rates on large-scale Stackelberg game instances
3. Validate algorithm robustness when linear utility structure assumptions are violated