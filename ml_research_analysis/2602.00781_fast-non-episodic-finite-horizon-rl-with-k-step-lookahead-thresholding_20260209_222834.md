---
ver: rpa2
title: Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding
arxiv_id: '2602.00781'
source_url: https://arxiv.org/abs/2602.00781
tags:
- lookahead
- reward
- k-step
- state
- lg1t
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles online reinforcement learning in non-episodic
  finite-horizon Markov Decision Processes, where the agent must learn to maximize
  cumulative rewards within a fixed, non-resetting trajectory. The challenge is that
  standard RL methods struggle due to the need to estimate returns over the full horizon
  with only one trajectory.
---

# Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding

## Quick Facts
- **arXiv ID:** 2602.00781
- **Source URL:** https://arxiv.org/abs/2602.00781
- **Reference count:** 40
- **Primary result:** Novel algorithm achieves minimax optimal constant regret for K=1 and O(max((K-1),C_{K-1})√(SAT log(T))) regret for K≥2 in non-episodic finite-horizon RL

## Executive Summary
This paper tackles online reinforcement learning in non-episodic finite-horizon Markov Decision Processes where agents must learn to maximize cumulative rewards within a fixed, non-resetting trajectory. The authors propose learning a K-step lookahead Q-function combined with a thresholding mechanism that selects actions only when their estimated K-step lookahead reward exceeds a time-varying threshold. Their algorithm, LGKT, achieves minimax optimal constant regret for K=1 and sublinear regret for any K≥2. Empirical evaluation shows superior performance compared to state-of-the-art tabular RL methods across synthetic MDPs and environments including JumpRiverswim, FrozenLake, and AnyTrading.

## Method Summary
The method learns a K-step lookahead Q-function by truncating planning to the next K steps, dramatically reducing sample complexity in non-episodic settings. Actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold, using a Lower Confidence Bound (LCB) rather than empirical mean. For K≥2, a separate sampling subroutine estimates K-1 step rewards through ε-greedy exploration, decoupling exploration from main policy decisions. The algorithm achieves constant regret for K=1 and O(max((K-1),C_{K-1})√(SAT log(T))) regret for K≥2, where C_{K-1} is an instance-dependent complexity parameter.

## Key Results
- Achieves minimax optimal constant regret for K=1 (O(SA/Δ^+_1)) matching theoretical lower bound
- Sublinear regret O(max((K-1),C_{K-1})√(SAT log(T))) for any K≥2
- Outperforms state-of-the-art tabular RL methods in cumulative rewards across synthetic MDPs, JumpRiverswim, FrozenLake, and AnyTrading environments
- Theoretical analysis proves worst-case linear optimality gap when K << T, but demonstrates practical effectiveness through extensive experiments

## Why This Works (Mechanism)

### Mechanism 1: Truncated Planning Horizon Reduces Estimation Variance
Learning a K-step lookahead Q-function instead of the full-horizon Q-function dramatically reduces sample complexity in non-episodic settings. Standard finite-horizon RL must estimate Q-values spanning the entire horizon T from a single trajectory. By truncating to K steps, the learning target becomes shorter-horizon, reducing variance. When K=1, the problem reduces to a contextual bandit (states as context), which has provably lower sample complexity.

### Mechanism 2: Lower Confidence Bound Thresholding Eliminates Bad Actions Fast
Using LCB (not empirical mean) against a time-varying threshold achieves minimax optimal constant regret for K=1. The algorithm constructs a candidate set of actions passing threshold using LCB with g(t) = 3log(t), enabling rapid elimination of sub-threshold actions. This differs from prior thresholding bandit work that uses empirical means.

### Mechanism 3: Structured Exploration via Subroutine Decouples Estimation from Main Policy
For K≥2, the algorithm achieves sublinear regret by using a separate sampling subroutine to estimate K-1 step rewards without contaminating the main policy's decisions. K-step reward decomposes into 1-step reward + expected (K-1)-step continuation. The algorithm uses ε-greedy exploration to trigger independent estimation for K-1 consecutive steps, bounding exploration regret at O(K√(SAT)).

## Foundational Learning

- **Concept: Q-learning and Bellman Optimality Equations**
  - Why needed here: The paper's K-step lookahead reward is defined via Bellman equations. Understanding how Q-values decompose across time is essential.
  - Quick check question: Can you derive why Q*_h(s,a) = R_{s,a} + E_{s'}[V*_{h+1}(s')] implies that K-step lookahead requires only K recursive evaluations?

- **Concept: Lower/Upper Confidence Bounds in Bandits**
  - Why needed here: The core innovation uses LCB (not UCB) for thresholding. The tighter g(t) = 3log(t) bound enables constant-time elimination of bad actions.
  - Quick check question: Why does using LCB instead of empirical mean for threshold testing lead to faster elimination of sub-threshold actions?

- **Concept: Regret Definitions for Non-Standard Objectives**
  - Why needed here: The paper defines regret against π_{K,γ} (K-step thresholding policy), not the optimal policy π^*. This is intentional—competing against π^* has linear lower bound.
  - Quick check question: Why is the regret defined as E_{π_{K,γ}}[∑c(s_t,a_t)] - E_π[∑c(s_t,a_t)] with cost c(s_t,a_t) = (γ_t - r_{s_t,a_t})1{r_{s_t,a_t} < γ_t} instead of the standard reward-based definition?

## Architecture Onboarding

- **Component map:**
  LGKT (Algorithm 2)
  ├── State-action visit counters: N^{(t)}_{s,a}, N^{(t)}_{s,a,K-1}
  ├── Empirical reward trackers: \hat{\phi}^1_{s,a}, \hat{\phi}^{K-1}_{s,a}
  ├── LCB calculator: LCB^{(t)}_{s,a} = \hat{r}^1 + \hat{r}^{K-1} - √(g(N_{s,a}+2)/(N_{s,a}+2)) - √(g(N_{s,a,K-1}+2)/(N_{s,a,K-1}+2))
  ├── Threshold tester: \tilde{G}_t = {a: LCB^{(t)}_{s_t,a} >= \gamma_t}
  ├── Exploration controller: \epsilon_t = min(1, 1/(N^{(t)}_{s_{t-1},a_{t-1}}+1)^{p · min(\eta, 1/2)})
  └── Subroutine: ALG_{K-1}(·|s_{t-1}, a_{t-1}) — runs for K-1 steps to estimate r^{K-1}

- **Critical path:**
  1. Observe state s_t
  2. Compute LCB for all actions at current state
  3. Build candidate set \tilde{G}_t of actions passing threshold
  4. With probability \epsilon_t: trigger exploration subroutine (K-1 steps)
  5. With probability 1-\epsilon_t: select action from \tilde{G}_t by highest LCB, or uniformly if empty
  6. Observe reward and next state, update counters and LCB

- **Design tradeoffs:**
  - K vs. convergence speed: Smaller K = faster convergence (constant regret for K=1) but potentially suboptimal oracle
  - Threshold γ level: Lower γ = faster convergence but selects from more actions, potentially lower cumulative reward
  - Subroutine choice: UCB for K=2 (C_{K-1} = A), UCBVI-BF for K>2 (C_{K-1} = K^2 SA)

- **Failure signatures:**
  - No actions pass threshold: If \tilde{G}_t is always empty, algorithm falls back to uniform random selection. Reduce γ.
  - Linear regret trajectory: If C_{K-1}√(SA) >> √(T), regret becomes dominated by exploration term. Reduce K or verify subroutine efficiency.
  - Slow convergence despite K=1: Check that gap Δ^+_1 isn't extremely small (sensitivity to reward structure).
  - Continuous state spaces: The tabular method requires state recurrence. In environments like AnyTrading, similar states must be visited multiple times.

- **First 3 experiments:**
  1. Validate K=1 on synthetic MDP: Generate random MDPs (S=10, A=5), run LG1T with γ=0.3, compare running average reward against Q-learning and UCRL2. Verify rapid convergence within first 1000-2000 steps.
  2. Test threshold sensitivity: Run LG1T on same MDP with γ ∈ {0.1, 0.3, 0.5, 0.7, 0.9}. Plot cumulative reward vs. time. Confirm moderate γ achieves best tradeoff.
  3. Verify exploration subroutine for K=2: On JumpRiverSwim (S=5), run LG2T with UCB as subroutine. Track both main policy regret and subroutine regret separately. Confirm total regret scales as O(A√(STlog(T))).

## Open Questions the Paper Calls Out

- **Question:** Can the K-step lookahead thresholding approach be extended to deep RL settings with function approximation while preserving fast convergence guarantees?
  - Basis in paper: [explicit] Conclusion states: "Extending this work to deep learning methods is an important direction for future research."
  - Why unresolved: Current theoretical analysis relies on tabular assumptions with count-based confidence bounds; function approximation introduces generalization errors not accounted for in the regret bounds.
  - What evidence would resolve it: A theoretical analysis extending the regret guarantees to linear or neural network function approximation, or empirical demonstration of strong performance on standard deep RL benchmarks.

- **Question:** What is the principled method for selecting the optimal switching time in adaptive K algorithms (e.g., LG1-2T)?
  - Basis in paper: [inferred] The adaptive variant LG1-2T uses ad-hoc switch times (t_c = 100, 10000, 30 for different environments) without theoretical justification.
  - Why unresolved: The paper provides no formal criterion for when to increase K; the choices appear tuned per environment rather than derived from theory.
  - What evidence would resolve it: A theoretical characterization of optimal switching times based on horizon length, state space size, and estimation variance, validated across diverse environments.

- **Question:** Can the stochastic dominance assumption (Assumption 3.2) for optimal K-step greedy policies be relaxed while maintaining sublinear regret?
  - Basis in paper: [explicit] Theorem 3.3 proves optimality of π_{K,greedy} under stochastic dominance for binary states, but Theorem 3.4 shows linear optimality gap exists in worst case without this assumption.
  - Why unresolved: The gap between the two theoretical results leaves unclear whether intermediate structural assumptions could guarantee partial optimality or improved regret bounds.
  - What evidence would resolve it: Identification of weaker sufficient conditions that guarantee bounded suboptimality, or analysis of the relationship between stochastic dominance relaxation and regret degradation.

## Limitations
- The K-step lookahead approach critically depends on Assumptions 4.1 and 4.3, which may not hold in environments with sparse rewards or rapidly changing dynamics
- The tabular method requires state recurrence, problematic in continuous or large state spaces
- There exists a fundamental tradeoff between convergence speed and oracle performance, with worst-case linear optimality gap when K << T

## Confidence
- **High:** K=1 regret bound (Theorem 4.2) and constant regret claim - supported by explicit proof and matching lower bound
- **Medium:** K≥2 regret bound (Theorem 4.4) - depends on unspecified instance-dependent parameter C_{K-1} and subroutine efficiency
- **Medium:** Empirical performance claims - based on 100 repetitions but benchmark hyperparameters and random seeds unspecified

## Next Checks
1. **Threshold Sensitivity Analysis:** Run LG1T on synthetic MDPs with varying γ values to empirically verify the tradeoff between convergence speed and cumulative reward identified in Appendix E.3
2. **Subroutine Regret Decomposition:** For K=2, instrument LG2T to separately track main policy regret and exploration subroutine regret, confirming the O(A√(STlog(T))) decomposition in Corollary 4.5
3. **Assumption Robustness Test:** Evaluate LGKT performance on MDPs with (a) sparse rewards, (b) non-stationary dynamics, and (c) extremely small action gaps to assess sensitivity to Assumptions 4.1 and 4.3