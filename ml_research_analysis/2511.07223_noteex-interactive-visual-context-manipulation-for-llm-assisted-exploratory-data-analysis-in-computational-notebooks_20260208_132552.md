---
ver: rpa2
title: 'NoteEx: Interactive Visual Context Manipulation for LLM-Assisted Exploratory
  Data Analysis in Computational Notebooks'
arxiv_id: '2511.07223'
source_url: https://arxiv.org/abs/2511.07223
tags:
- context
- data
- view
- mental
- noteex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents NoteEx, a JupyterLab extension that helps
  analysts manage mental models and select task-relevant context for LLM-assisted
  Exploratory Data Analysis (EDA) in computational notebooks. Through a formative
  study, the authors identified four key challenges: evolving mental models, difficulty
  recalling cell metadata, implicit data variables, and burdensome manual context
  selection.'
---

# NoteEx: Interactive Visual Context Manipulation for LLM-Assisted Exploratory Data Analysis in Computational Notebooks

## Quick Facts
- **arXiv ID**: 2511.07223
- **Source URL**: https://arxiv.org/abs/2511.07223
- **Reference count**: 40
- **Primary result**: NoteEx significantly improves mental model maintenance, context selection, user engagement, and satisfaction compared to standard JupyterLab in LLM-assisted EDA

## Executive Summary
NoteEx is a JupyterLab extension that helps analysts manage mental models and select task-relevant context for LLM-assisted Exploratory Data Analysis. Through a formative study, the authors identified key challenges: evolving mental models, difficulty recalling cell metadata, implicit data variables, and burdensome manual context selection. NoteEx addresses these with a flowchart-style Canvas View for visualizing mental-model dependencies, a Data Information View for surfacing variables, and an LLM-Assistant View that suggests and allows editing of context. A user study (n=12) showed significant improvements in mental model maintenance, context selection, user engagement, and satisfaction, with participants reporting shorter prompts, fewer clarifications, and higher-quality LLM responses.

## Method Summary
The authors developed NoteEx as a JupyterLab extension using TypeScript and React, implementing three views: Canvas View (flowchart visualization), Data Information View (variable inspection), and LLM-Assistant View (prompt construction with GPT-4o-mini). The extension stores state in notebook metadata rather than external databases. A within-subjects study (n=12) compared NoteEx against a baseline (standard JupyterLab + LLM) using a Graeco-Latin square design with two Kaggle datasets (billionaires and hotel booking demand) and tasks involving messy notebook onboarding and scratch-based EDA. Participants completed TAM2-based and User Engagement Scale questionnaires, with interaction logs capturing prompt length and interaction counts.

## Key Results
- Participants showed significant improvements in mental model maintenance and context selection with NoteEx
- User engagement and satisfaction scores were significantly higher with NoteEx
- Participants reported shorter prompts, fewer LLM clarifications, and perceived higher-quality responses
- NoteEx reduced the friction of prompt engineering by surfacing implicit metadata

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Externalizing mental models via a 2D flowchart reduces cognitive load and improves context selection accuracy compared to linear 1D views.
- **Mechanism**: The Canvas View transforms a linear sequence of cells into a directed graph where users explicitly define links representing "mental model dependencies" (intent, alternatives, or rationale) rather than just code execution.
- **Core assumption**: Users can accurately map their internal understanding of the workflow to the graph structure, and this externalized map is more stable than their memory of the 1D notebook.
- **Evidence anchors**: [abstract]: "semantic visualization of the EDA workflow, allowing analysts to externalize their mental model"; [section 3.1.2]: "A user's mental model evolves... complicating the selection of proper contexts."

### Mechanism 2
- **Claim**: Human-in-the-loop context curation improves LLM response relevance by filtering out "code dependencies" that are irrelevant to the user's current intent.
- **Mechanism**: The LLM-Assistant View proposes a "suggested context" based on the user-drawn mental model links rather than just variable definitions. Users can edit this selection before sending the prompt.
- **Core assumption**: Code dependencies (variable usage) are a noisy proxy for task relevance; the user's manual selection of dependencies is the ground truth for intent.
- **Evidence anchors**: [section 1]: "Selecting the context purely based on code dependencies may be imprecise, as it overlooks the analyst's mental model"; [section 7.3.3]: Participants noted that "Giving too much context [as in Baseline] confused the LLM... manually providing proper context caused LLMs to understand them better."

### Mechanism 3
- **Claim**: Surfacing implicit metadata (variables, execution status) lowers the friction of prompt engineering.
- **Mechanism**: The Data Information View and Canvas node attributes make "implicit" variables and execution states glanceable. This removes the need to scroll or execute cells to find variable names or check if a cell is stale.
- **Core assumption**: The friction of context selection is primarily caused by information retrieval (finding variables/states), not the decision of what to include.
- **Evidence anchors**: [section 3.1.2]: "Data variables are implicit... difficult to find and use in contexts"; [section 7.2.3]: Participants reported "Instead of searching through the code... you can quickly see where a variable is defined."

## Foundational Learning

- **Concept: Mental Model Dependencies vs. Code Dependencies**
  - **Why needed here**: NoteEx relies on the distinction that code dependencies (what must run for the code to work) differ from mental model dependencies (what cells are conceptually related).
  - **Quick check question**: If Cell A defines variable `x`, and Cell B redefines `x`, and Cell C uses `x`, which cells are "mentally" connected to C if the user intends to use the value from Cell A?

- **Concept: Task-Relevant Context**
  - **Why needed here**: The core objective of the tool is to construct the "minimal set of cells" required for an LLM to answer a prompt.
  - **Quick check question**: Why does including *more* context (e.g., the entire notebook) often degrade LLM performance compared to a smaller, curated subset?

- **Concept: JupyterLab Extension Architecture**
  - **Why needed here**: NoteEx is not a standalone app but an extension using JupyterLab APIs and storing data in notebook metadata.
  - **Quick check question**: How does storing extension-specific data (like graph links) inside the `.ipynb` file itself facilitate portability compared to using a separate database?

## Architecture Onboarding

- **Component map**: Frontend (JupyterLab Extension, TypeScript/React) -> Views (Canvas View, Data Information View, LLM-Assistant View) -> Storage (`.ipynb` metadata) -> LLM API (GPT-4o-mini)
- **Critical path**: 1) User drags link between cells in Canvas View; 2) Extension serializes link into notebook metadata; 3) User clicks "Ask AI" on target cell; 4) System traverses incoming links and code dependencies to build context; 5) User verifies context in LLM-Assistant View and sends prompt; 6) LLM returns response; 7) User applies response to cell
- **Design tradeoffs**: Manual vs. Automatic Linking (manual captures intent but higher effort vs. automatic low effort but low semantic accuracy); Metadata storage (high portability vs. low queryability); Mixed 1D+2D views (flexibility vs. UI complexity)
- **Failure signatures**: Visual Clutter (Canvas becomes unreadable with >50-100 nodes); Stale State (Canvas shows link but notebook cell deleted/moved); Context Bloat (user selects too many items, hitting token limits)
- **First 3 experiments**: 1) Context Accuracy Test (LLM response quality with NoteEx vs. Baseline context selection); 2) Scalability Stress Test (load 100+ cell notebook, measure Canvas UI latency and LLM processing time); 3) User Efficiency Test (time to onboard to messy notebook using NoteEx vs. Linear View)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a purely 2D spatial representation wholly replace the linear notebook view while maintaining or exceeding user performance and learnability?
- Basis in paper: [explicit] The authors note that participants largely replaced the 1D view with the Canvas View and explicitly suggest testing whether a "2D-only approach maintains or exceeds performance, learnability, and satisfaction."
- Why unresolved: Current implementations rely on a mixed view; a complete transition requires redesigning fine-grained editing affordances for a 2D space.
- What evidence would resolve it: A controlled study comparing a 2D-only interface against mixed and 1D baselines on complex EDA tasks.

### Open Question 2
- Question: What are the specific trade-offs in accuracy, latency, and token usage between hybrid human-in-the-loop context selection and fully automatic retrieval?
- Basis in paper: [explicit] The discussion section identifies "concrete research questions about trade-offs between fully automatic retrieval and guided manual selection" regarding cost, latency, and user trust.
- Why unresolved: The current user study focused on usability and satisfaction but did not rigorously measure computational costs or comparative error rates against pure automatic methods.
- What evidence would resolve it: Quantitative benchmarking of context relevance scores, token consumption, and response accuracy across hybrid vs. automatic selection methods.

### Open Question 3
- Question: Can user mental-model dependencies be accurately inferred and automated to reduce manual specification overhead?
- Basis in paper: [inferred] The authors note that manual dependency specification can be "time-consuming and laborious" and suggest future work could leverage LLMs to "automatically suggest... an initial Canvas View."
- Why unresolved: It is unclear if automated inference can capture the personalized, non-linear intent of an analyst without introducing errors or constraining the user's thought process.
- What evidence would resolve it: An evaluation of an automated layout suggestion system measuring the precision of inferred links compared to user-defined ground truth.

## Limitations
- The Canvas View scalability with large notebooks (>50 cells) is unproven and may suffer from visual clutter
- Manual link creation may lead to selection fatigue in complex workflows, though the study duration may not have been sufficient to reveal this
- Evaluation metrics rely heavily on subjective measures and user perception rather than direct measurement of LLM response quality

## Confidence

**High Confidence**: The identification of three core challenges (evolving mental models, implicit variables, burdensome context selection) is well-supported by the formative study and aligns with established literature on notebook usability.

**Medium Confidence**: The user study results showing improvements in mental model maintenance, context selection, engagement, and satisfaction are statistically significant, but the small sample size (n=12) and short study duration limit generalizability.

**Low Confidence**: The claim that NoteEx produces "higher-quality LLM responses" is primarily based on user perception rather than direct measurement of response accuracy or task completion rates.

## Next Checks

1. **Scalability Test**: Evaluate NoteEx with notebooks containing 50+ cells to assess Canvas View performance and user experience degradation. Measure rendering time, navigation patterns, and user-reported usability.

2. **Long-term Usage Study**: Conduct a longitudinal study (1-2 weeks) where users work on real-world EDA projects to identify selection fatigue, context synchronization issues, and actual impact on LLM response quality.

3. **Objective Response Quality Analysis**: Design a controlled experiment comparing LLM responses generated with NoteEx-selected context versus baseline context on standardized EDA tasks. Measure response accuracy, completeness, and task completion rates rather than relying on user perception.