---
ver: rpa2
title: 'A Coreset Selection of Coreset Selection Literature: Introduction and Recent
  Advances'
arxiv_id: '2505.17799'
source_url: https://arxiv.org/abs/2505.17799
tags:
- selection
- coreset
- learning
- dataset
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# A Coreset Selection of Coreset Selection Literature: Introduction and Recent Advances

## Quick Facts
- **arXiv ID**: 2505.17799
- **Source URL**: https://arxiv.org/abs/2505.17799
- **Reference count**: 40
- **Key outcome**: None

## Executive Summary
This paper provides a comprehensive survey of coreset selection methods for dataset pruning in deep learning, focusing on image classification benchmarks. It categorizes selection strategies into clustering, optimization, and training-oriented approaches, and evaluates their performance across varying subset sizes on CIFAR-10 using ResNet-18. The authors highlight the trade-off between pruning ratio and accuracy, noting that hard-sample scoring methods can select outliers or mislabeled data at high pruning rates. The work identifies future directions such as fairness, robustness, and adapting coreset selection to foundation models.

## Method Summary
The study benchmarks various coreset selection strategies (e.g., Forgetting, GraNd, Herding, GradMatch) on CIFAR-10 for image classification. Methods are grouped into clustering, optimization, and training-oriented categories. For training-oriented methods, a proxy model is trained for only 10 epochs on the full dataset to derive importance scores before subset selection. A ResNet-18 is then trained from scratch on the selected subset for 200 epochs using SGD with momentum, weight decay, and cosine learning rate decay. The primary metric is top-1 test accuracy relative to the full dataset across different subset fractions (0.1% to 90%). Results are transcribed from the DeepCore library.

## Key Results
- Training-oriented methods (e.g., GraNd) perform well at moderate pruning ratios but tend to select outliers or mislabeled data at high pruning rates.
- Coverage-based methods like Herding and GraphCut provide more stable performance under label corruption but may miss boundary refinement.
- The trade-off between accuracy and pruning ratio is highly dependent on the selection strategy and dataset characteristics.

## Why This Works (Mechanism)
The paper works by systematically evaluating how different coreset selection strategies balance dataset compression with model performance. Training-oriented methods leverage proxy model gradients to identify influential samples, while clustering and optimization methods use geometric or submodular criteria to ensure coverage. The mechanism relies on the assumption that a small, well-chosen subset can approximate the learning dynamics of the full dataset, though this breaks down when the subset is dominated by outliers or mislabeled examples.

## Foundational Learning
- **Importance scoring**: Used to rank samples by their contribution to model learning; quick check: verify proxy model training matches the 10-epoch protocol.
- **Submodular optimization**: Ensures diversity in selected subsets; quick check: confirm submodular function parameters align with DeepCore defaults.
- **Gradient-based selection**: Identifies hard or influential samples via model gradients; quick check: validate gradient computation matches the paper's description.
- **Label corruption robustness**: Tests method stability under noisy labels; quick check: implement consistent corruption protocol (e.g., 10% random flips).
- **Coverage vs. boundary refinement**: Trade-off between selecting diverse samples and refining decision boundaries; quick check: compare Herding (coverage) vs. GraNd (boundary) performance.
- **Ablation on proxy model quality**: Assesses sensitivity to the 10-epoch proxy training; quick check: vary proxy model hyperparameters and observe score stability.

## Architecture Onboarding
- **Component map**: CIFAR-10 dataset -> Proxy model (10 epochs) -> Importance scores -> Subset selection -> ResNet-18 (200 epochs) -> Accuracy evaluation.
- **Critical path**: Proxy model training -> Importance score computation -> Subset selection -> Final model training.
- **Design tradeoffs**: Training-oriented methods are fast but proxy-dependent; coverage methods are robust but may miss fine-grained decision boundaries.
- **Failure signatures**: Sharp accuracy drops at high pruning ratios indicate outlier domination; instability under label corruption suggests lack of robustness.
- **First experiments**:
  1. Train proxy model for 10 epochs and extract GraNd scores; verify score distribution.
  2. Select 10% subset using Herding; train ResNet-18 and compare accuracy to full-data baseline.
  3. Introduce 10% label corruption; re-run GradMatch selection and assess robustness.

## Open Questions the Paper Calls Out
### Open Question 1
Can we design selection methods that avoid outlier domination and remain stable under corrupted labels or adversarial contamination? The authors ask this explicitly in the context of fairness and robustness, noting that current hard-sample scoring methods tend to select mislabeled data or outliers. Evidence would include a framework integrating robust scoring with out-of-distribution filtering that improves worst-group performance.

### Open Question 2
Can selection policies be meta-learned to generalize across datasets, model families, and training recipes? Section VIII poses this, highlighting that current training-oriented selectors are tightly coupled to the specific model used for scoring. Evidence would be amortized scoring models that predict sample utility for unseen architectures without retraining.

### Open Question 3
How can coreset selection be adapted for Foundation Models (LLMs/VLMs) to preserve rare capabilities and coverage? The Abstract and Section VIII highlight this challenge, especially for multi-modal fine-tuning. Evidence would be selection algorithms for VLM/LLM fine-tuning that reduce token counts while maintaining performance on rare capability benchmarks.

## Limitations
- Implementation details for specific selectors (e.g., OMP iterations for GradMatch) are not fully specified and rely on DeepCore defaults.
- The exact random seeds used for the 5-run averages are not provided, limiting reproducibility.
- Robustness experiments lack detail on the label corruption protocol (random vs. targeted).

## Confidence
- **High confidence**: General experimental protocol (ResNet-18, SGD+cosine LR, CIFAR-10) is clearly specified.
- **Medium confidence**: 10-epoch proxy model protocol is well-defined, but selector behavior depends on DeepCore defaults.
- **Low confidence**: Robustness experiments lack sufficient methodological detail.

## Next Checks
1. Verify DeepCore defaults for GradMatch (OMP iterations) and submodular functions to ensure fidelity to reported results.
2. Test proxy model sensitivity by varying hyperparameters (e.g., LR warmup) and observing score stability.
3. Implement the 10% label corruption protocol (e.g., random label flipping) and re-run a subset of experiments to confirm robustness trends.