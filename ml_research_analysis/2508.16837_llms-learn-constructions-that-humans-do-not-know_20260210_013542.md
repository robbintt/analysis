---
ver: rpa2
title: LLMs Learn Constructions That Humans Do Not Know
arxiv_id: '2508.16837'
source_url: https://arxiv.org/abs/2508.16837
tags:
- constructions
- construction
- these
- which
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that large language models (LLMs) learn
  syntactic constructions that do not actually exist for human speakers, creating
  false positive constructions through a confirmation bias in probing methods. The
  authors investigate this by creating 500 sentences representing five true clause-level
  constructions, then using both meta-linguistic prompts and contextual embeddings
  to test whether models hallucinate additional constructions.
---

# LLMs Learn Constructions That Humans Do Not Know

## Quick Facts
- **arXiv ID**: 2508.16837
- **Source URL**: https://arxiv.org/abs/2508.16837
- **Reference count**: 9
- **Primary result**: Large language models learn false positive syntactic constructions that do not exist for human speakers, creating hallucinations through confirmation bias in probing methods

## Executive Summary
This paper demonstrates that large language models learn syntactic constructions that do not actually exist for human speakers, creating false positive constructions through a confirmation bias in probing methods. The authors investigate this by creating 500 sentences representing five true clause-level constructions, then using both meta-linguistic prompts and contextual embeddings to test whether models hallucinate additional constructions. Results show high accuracy (91-94%) in clustering sentences into hallucinated constructions that are invisible to human linguists, regardless of whether probing uses explicit meta-linguistic prompts or implicit behavioural methods. The authors conclude that construction probing methods suffer from a fundamental confirmation bias, as incorrect hypotheses about syntactic structures would be validated by these methods.

## Method Summary
The authors created 500 sentences from Universal Dependencies English corpora, organized into five clause-level constructions (intransitive, transitive-NP, transitive-clause, passive, double object) with 100 sentences each. They conducted two experiments: (1) GPT-4 few-shot prompting with nonce construction names to sort sentences, generating pair-wise co-occurrence matrices that were clustered to reveal hallucinated constructions, and (2) Pythia 1.4b embedding extraction with both direct and grammar-focused variants (subtracting shuffled-sentence embeddings) to cluster sentences within single constructions and validate spurious distinctions using logistic regression. The approach tested whether models could distinguish both true constructions and hallucinated subdivisions within them.

## Key Results
- Both implicit (embeddings) and explicit (prompts) probing methods reveal that models hallucinate constructions with 90-94% accuracy
- False positive accuracy (0.94-0.99) exceeds true positive accuracy (0.79-0.93), demonstrating confirmation bias
- Clustering stability across random exemplars and nonce construction names shows hallucinations persist regardless of prompt wording
- Grammar-focused embeddings (controlling for lexical information) maintain high false positive accuracy, indicating hallucinations are not purely lexical artifacts

## Why This Works (Mechanism)

### Mechanism 1: Confirmation Bias in Construction Probing
- Probing methods confirm hypotheses regardless of validity because they cannot falsify construction existence
- Design makes false negatives possible but false positive discovery impossible
- Core assumption: Linguistic introspection provides ground truth for construction existence
- Evidence: Abstract states false hypotheses would be "overwhelmingly confirmed" and section 1 notes introspection-driven hypotheses cannot be disproven

### Mechanism 2: Over-Learning of Infrequent Patterns Through Long-Context Memorization
- LLMs hallucinate constructions by creating overly specific descriptions over broad contexts
- Models retain infrequent surface patterns that humans forget through construction learning
- Core assumption: Negative relationship between model size and human reading time prediction extends to construction learning
- Evidence: Model explanation shows descriptions are "partly nonsensical but mostly far too specific"

### Mechanism 3: Implicit-Explicit Knowledge Divergence in Clustering Stability
- Both implicit and explicit probing show stable false positive clustering
- Prompt-based sorting achieves 71-80% cluster consistency; embedding-based achieves 85-99% accuracy
- Core assumption: Cluster stability across conditions indicates model-internal representations
- Evidence: Table 1 shows 90-94% accuracy across nonce construction names; Table 4 shows false positives exceed true positives

## Foundational Learning

- **Construction Grammar (CxG) and Entrenchment**
  - Why needed here: Assumes familiarity with constructions as form-meaning pairings and usage-based notion of grammaticalization through frequency
  - Quick check question: Can you explain why a construction's "entrenchment level" matters for determining whether an LLM should have learned it?

- **Probing Paradigms (Behavioral vs. Meta-Linguistic)**
  - Why needed here: Contrasts implicit knowledge (contextual embeddings) with explicit knowledge (prompts requiring linguistic analysis)
  - Quick check question: What is the difference between asking a model to classify sentences via embeddings versus asking it to explicitly identify construction membership?

- **Minimal Pairs in Syntactic Evaluation**
  - Why needed here: Central critique is that minimal pair paradigms cannot discover unknown false positives
  - Quick check question: Why can't minimal pairs be used to discover constructions that linguists don't already know exist?

## Architecture Onboarding

- **Component map**: 500 sentences from Universal Dependencies -> Embedding extraction (Pythia 1.4b mean pooling) -> Prompting module (GPT-4 sorting task) -> Clustering layer (k-means) -> Classification probe (Logistic Regression)

- **Critical path**:
  1. Verify true positive detection works (Table 3: embeddings distinguish actual constructions)
  2. Generate puppet hypotheses by clustering within single constructions
  3. Test whether classifier confirms false distinctions (Table 4: high accuracy on hallucinated constructions)
  4. Introspective validation that clusters lack linguistic motivation

- **Design tradeoffs**:
  - Grammar-focused vs. direct embeddings: Grammar-focused shows lower true positive accuracy but maintains high false positive accuracy
  - Nonce construction names: Avoids training data contamination but limits ecological validity
  - k=2 clustering constraint: Forces binary distinctions within constructions

- **Failure signatures**:
  - If cluster assignments were driven by sentence length or lexical overlap, false positives would be confounded
  - If prompt responses varied by construction name, sorting would reflect prompt wording
  - If grammar-focused embeddings collapsed all distinctions, the method would be too aggressive

- **First 3 experiments**:
  1. Reproduce true positive validation: Train classifier on five actual constructions using direct embeddings; verify F-scores match Table 3 (0.79-0.93 range)
  2. Puppet hypothesis test: Within one construction, cluster sentences into k=2 groups and train classifier to distinguish them; confirm high accuracy (0.93-0.94) indicating hallucinated distinction
  3. Introspective error analysis: For resulting clusters, manually verify paired examples do not differ on grammatical dimensions

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the full scope and scale of hallucinated constructions across different syntactic categories, abstraction levels, and model architectures?
  - Basis: Abstract states "Exploring the full range of such false positives remains a challenge for future work"
  - Why unresolved: Only tested 5 clause-level constructions on GPT-4 and Pythia 1.4b
  - Evidence needed: Comprehensive probing study across multiple models, syntactic categories, and levels of schematicity

- **Open Question 2**: Do hallucinated constructions correspond to real constructions in specific dialects, registers, or speech communities?
  - Basis: Discussion asks "Are these false positives actually entrenched constructions for other dialects?"
  - Why unresolved: Benchmark population/register for probing experiments is undefined
  - Evidence needed: Cross-dialectal and cross-register analysis comparing model hallucinations against construction inventories from diverse speech communities

- **Open Question 3**: How can probing methods be developed that discover unknown constructions without presupposing their existence?
  - Basis: Conclusion states "develop methods for mapping the syntactic knowledge of LLMs which do not assume syntactic analyses from the start"
  - Why unresolved: Minimal pair paradigms require pre-existing hypotheses
  - Evidence needed: Unsupervised or hypothesis-neutral methods that map model grammars independently of linguist-derived expectations

## Limitations

- Cannot verify that hallucinated constructions are genuinely invisible to humans rather than reflecting subtle distinctions linguists have overlooked
- 1.4b parameter Pythia model used for behavioral probing is relatively small compared to frontier models
- Confirmation bias critique applies to all probing methods but doesn't quantify bias magnitude or compare robustness across approaches

## Confidence

- **High confidence**: That LLMs can over-generalize and learn overly specific patterns that humans would abstract away from
- **Medium confidence**: That probing methods suffer from fundamental confirmation bias making them unable to falsify construction hypotheses
- **Medium confidence**: That the hallucinated constructions are genuinely "unknown to humans" rather than reflecting subtle linguistic distinctions

## Next Checks

1. **Cross-model validation**: Test whether larger models (e.g., LLaMA-2 70B, GPT-4) show similar false positive construction learning rates, or whether increased scale reduces over-specificity
2. **Minimal pair falsification test**: Attempt to construct minimal pairs that could falsify the existence of hallucinated constructions
3. **Linguistic expert survey**: Have multiple syntacticians independently evaluate a blind sample of hallucinated vs. true constructions to quantify inter-rater agreement