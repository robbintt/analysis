---
ver: rpa2
title: 'LangPrecip: Language-Aware Multimodal Precipitation Nowcasting'
arxiv_id: '2512.22317'
source_url: https://arxiv.org/abs/2512.22317
tags:
- precipitation
- motion
- nowcasting
- radar
- langprecip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of short-term precipitation
  nowcasting, particularly for rapidly evolving and extreme weather events. The proposed
  LangPrecip framework introduces a novel approach by treating meteorological motion
  descriptions as semantic motion priors to guide precipitation evolution.
---

# LangPrecip: Language-Aware Multimodal Precipitation Nowcasting

## Quick Facts
- **arXiv ID**: 2512.22317
- **Source URL**: https://arxiv.org/abs/2512.22317
- **Reference count**: 7
- **Primary result**: LangPrecip achieves over 60% and 19% gains in heavy-rainfall CSI at 80-minute lead time on Swedish and MRMS datasets respectively.

## Executive Summary
This paper introduces LangPrecip, a novel framework for short-term precipitation nowcasting that leverages language-aware multimodal inputs. The key innovation is treating meteorological motion descriptions as semantic motion priors within a Rectified Flow paradigm, enabling explicit integration of textual and radar information in latent space. The authors also contribute LangPrecip-160k, a large-scale dataset of 160k paired radar sequences and motion descriptions, combining Swedish SWish and MRMS data. Experiments demonstrate substantial improvements in forecasting heavy precipitation events compared to state-of-the-art methods, particularly at longer lead times.

## Method Summary
LangPrecip addresses precipitation nowcasting by formulating it as a semantically constrained trajectory generation problem under the Rectified Flow framework. The method uses meteorological text descriptions as motion priors to guide precipitation evolution, explicitly integrating textual and radar information in latent space. The approach employs a 2D Variational Autoencoder (VAE) to compress 256×256 radar fields into 32×32 latent representations, with a Rectified Flow backbone predicting velocity fields conditioned on both historical radar latents and motion descriptions. A Wavelet Consistency Unfolding Decoder (WCUB) reconstructs the final predictions with data-consistency updates and wavelet-domain shrinkage. The framework is trained on LangPrecip-160k, a dataset combining 100k Swedish SWish events and 60k MRMS events, each consisting of 20 frames at 5-minute resolution (100 minutes total), with approximately 30k manually annotated text descriptions supplemented by VLM generation.

## Key Results
- Achieves over 60% improvement in heavy-rainfall CSI on Swedish dataset at 80-minute lead time compared to state-of-the-art methods
- Demonstrates 19.2% CSI improvement on MRMS dataset for heavy rainfall at 80-minute lead time
- Shows consistent gains across multiple metrics including FSS, CRPS, SSIM, and LPIPS on both datasets

## Why This Works (Mechanism)
The framework works by explicitly incorporating meteorological motion semantics into the precipitation nowcasting process. By treating language descriptions as motion priors within the Rectified Flow paradigm, the model gains access to high-level physical constraints that guide precipitation evolution beyond what can be inferred from radar data alone. The WCUB decoder ensures physical consistency through data-consistency updates while the wavelet-domain processing preserves important spatial details. The cross-modal conditioning architecture allows the model to leverage both visual patterns from radar sequences and semantic knowledge encoded in motion descriptions, resulting in more physically plausible forecasts, particularly for extreme precipitation events.

## Foundational Learning

**Rectified Flow**: A generative modeling framework that parameterizes the trajectory of data transformation using velocity fields, allowing for stable training and sampling. Needed because it provides a principled way to incorporate motion priors and handle the trajectory generation aspect of nowcasting. Quick check: Verify velocity predictions align with expected precipitation movement patterns.

**Cross-modal Conditioning**: The mechanism of integrating information from different modalities (text and images) through shared representations or attention mechanisms. Needed because it enables the model to leverage both radar observations and meteorological descriptions simultaneously. Quick check: Ensure text embeddings meaningfully influence radar latent space evolution.

**Wavelet Consistency Unfolding**: A decoder architecture that combines wavelet-domain processing with data-consistency updates for improved reconstruction quality. Needed because it preserves important spatial details while ensuring physical consistency with observations. Quick check: Verify wavelet shrinkage effectively removes noise without losing precipitation features.

## Architecture Onboarding

**Component Map**: Radar VAE -> Rectified Flow Backbone -> Cross-modal Conditioning -> WCUB Decoder -> Forecast Output

**Critical Path**: Historical radar frames → VAE encoding → Rectified Flow velocity prediction → WCUB reconstruction → Forecast

**Design Tradeoffs**: The choice of Rectified Flow over diffusion models trades sampling efficiency for potentially better handling of motion priors; the WCUB decoder prioritizes physical consistency over pure reconstruction quality; using generated text descriptions enables large-scale training but may introduce noise compared to fully manual annotation.

**Failure Signatures**: Over-smoothed predictions with poor extreme-event CSI indicate underutilization of text guidance or insufficient CFG scale; training instability or degraded FVD suggests architectural issues with the WCUB decoder or improper conditioning fusion.

**First Experiments**:
1. Train baseline Rectified Flow model without text conditioning using identical VAE and dataset splits
2. Evaluate CFG scale sensitivity by testing values across the suggested range (3-5)
3. Compare WCUB performance against a learned wavelet prior variant

## Open Questions the Paper Calls Out

**Open Question 1**: Can language-guided nowcasting maintain robustness when text descriptions contain errors, noise, or contradict visual observations? The paper acknowledges that automatically generated annotations may introduce noise or bias, but does not evaluate model behavior under corrupted or contradictory text inputs.

**Open Question 2**: How can fine-scale convective dynamics (initiation, rapid intensification, multi-cell interactions) be better captured beyond coarse-grained motion descriptions? Current descriptions summarize bulk advection over 100 minutes, potentially missing localized, high-frequency dynamics critical for extreme short-term events.

**Open Question 3**: What are the optimal mechanisms for obtaining high-quality motion semantics at scale without relying on fully manual annotation? The current hybrid approach lacks systematic validation of VLM fidelity and does not quantify annotation uncertainty.

## Limitations

- Key architectural details of the Rectified Flow backbone (transformer depth, hidden dimensions, attention heads) are unspecified, making faithful reproduction difficult
- Training hyperparameters including learning rate, batch size, epochs, and optimizer details are not provided
- Approximately 80% of text descriptions are automatically generated via VLM, potentially introducing noise or bias that could affect robustness
- The framework's performance on rapidly evolving convective systems with complex multi-cell interactions is not thoroughly evaluated

## Confidence

- CSI improvements over baselines: **High**
- Architectural novelty of Rectified Flow + WCUB: **Medium** (design described but dimensions unspecified)
- Dataset quality and diversity: **Medium** (160k scale impressive, but ~80% auto-generated descriptions)

## Next Checks

1. Train baseline Rectified Flow model (without text conditioning) using identical VAE and dataset splits; verify claimed CSI gains are not due to unreported architectural changes
2. Cross-validate WCUB decoder performance by training a variant with learned wavelet priors instead of fixed shrinkage; compare against Table 4
3. Perform ablation on text-conditioning strength (CFG scale) on a held-out validation set; confirm optimal range reported in Fig. 9