---
ver: rpa2
title: Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets
arxiv_id: '2508.06706'
source_url: https://arxiv.org/abs/2508.06706
tags:
- uni00000013
- uni00000011
- uni00000048
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of excessive rule sets in knowledge
  graph completion, which undermines explainability despite rule-based methods' advantages.
  The authors propose a framework that uses probabilistic circuits to learn distributions
  over meaningful rule contexts, significantly reducing the number of rules needed
  while maintaining performance.
---

# Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets

## Quick Facts
- arXiv ID: 2508.06706
- Source URL: https://arxiv.org/abs/2508.06706
- Reference count: 34
- 70-96% reduction in rules (average 12-fold) with 31× performance improvement over confidence-based baselines

## Executive Summary
This paper addresses the problem of excessive rule sets in knowledge graph completion (KGC) that undermine explainability. The authors propose a framework that uses probabilistic circuits to learn distributions over meaningful rule contexts, significantly reducing the number of rules needed while maintaining performance. Their method achieves substantial rule reduction (70-96%) and delivers significant performance improvements (31× over confidence-based baselines) by selecting rules based on their marginal probability in the learned distribution rather than confidence scores alone.

## Method Summary
The method generates rule-context associations via PyClause abduction, builds a binary rule-context matrix, learns a probabilistic circuit using Juice library (structure via Hidden Chow-Liu Tree, parameters via EM), and selects rules based on marginal probabilities from the circuit. The selected rules are then used with AnyBURL for KGC inference. The framework operates on Horn rules learned from data and associates each rule with contexts (meaningful subsets of rules that work together), learning a joint probability distribution over these contexts to enable efficient rule selection.

## Key Results
- Achieves 70-96% reduction in number of rules used while outperforming baseline by up to 31× when using equivalent minimal number of rules
- Delivers 12-fold average rule reduction across 8 benchmark datasets
- Preserves 91% of peak baseline performance even when using minimal rule sets

## Why This Works (Mechanism)

### Mechanism 1: Context-Aware Distribution Learning Over Rules
Learning a joint probability distribution over rule contexts enables more efficient rule selection than confidence-based thresholding. The framework associates each rule with mutually exclusive "contexts" (subsets of rules that are jointly valid), then learns a probabilistic circuit Pθ over these contexts using maximum-likelihood learning. This captures which rules work together, which confidence scores ignore.

### Mechanism 2: Marginal Probability as Rule Selection Signal
Ranking rules by their marginal probability Pθ({r}) from the learned circuit selects higher-utility rules than confidence-based ranking. After learning Pθ, compute Pθ(R) = Σ_{c: Πc|=R} Pθ(c) for rule sets. For singleton rules, this marginal indicates how often a rule appears in high-probability contexts.

### Mechanism 3: Tractable Inference via Lower Bounds and Exact Computation
Probabilistic circuits enable both approximate (lower bound) and exact query probability computation without independence assumptions. Propositions 1-4 provide formal bounds: Pθ(q) ≥ sup{Pθ(Rj)|Rj |= q} (Prop. 3, lower bound) and exact computation via Pθ(q) = 1 - Pθ(∧_{c:Πc|=q} ¬νc) (Prop. 2).

## Foundational Learning

- **Concept: Probabilistic Circuits (PCs)**
  - Why needed here: Core representation for the learned distribution over contexts; must understand tractable inference properties (decomposability, determinism) to interpret why marginals are efficiently computable
  - Quick check question: Can you explain why a decomposable and deterministic circuit allows linear-time marginal queries?

- **Concept: First-Order Horn Rules**
  - Why needed here: The framework operates on rules of form h ← b where body b is a conjunction; understanding rule grounding and entailment is prerequisite to following the context construction
  - Quick check question: Given a rule `bornIn(X,Y) ← citizenOf(X,Z) ∧ locatedIn(Z,Y)`, what does it mean for this rule to "entail" a query triple?

- **Concept: Knowledge Graph Completion Metrics (Hits@k, MRR)**
  - Why needed here: Evaluation framework; must understand what these metrics measure to interpret the 31× improvement claim and 91% preservation claim
  - Quick check question: If a model ranks the correct entity at position 3 for a query, what is its contribution to Hits@10? To MRR?

## Architecture Onboarding

- **Component map:** PyClause (Python) -> Rule-context matrix generation -> Juice library (Julia) -> PC structure learning + parameter learning -> Marginal computation -> Rule selection (PC1/PC2/PC3) -> AnyBURL inference engine -> Evaluation scripts

- **Critical path:** Rule-context matrix generation → PC learning (dominant runtime: 1000-9500s for Family dataset) → Marginal computation → Rule selection → Prediction → Evaluation

- **Design tradeoffs:**
  - PC1 vs PC2 vs PC3: PC1 (lower bound, singleton) is fastest (76s for 2600 rules); PC2 (exact) has similar speed but requires additional negation probability computation; PC3 (greedy walks) is slowest (>24h for large rule sets) and showed no performance advantage over PC1/PC2
  - Confidence threshold selection: Higher thresholds reduce rule count but may miss useful rules; paper used 0-70% thresholds across datasets
  - EM iterations: 10-100 iterations; more iterations improve distribution quality but increase learning time

- **Failure signatures:**
  - Zero or near-zero performance with small rule counts: marginals not selecting useful rules; check rule-context matrix density
  - PC learning diverges (NaN/Inf): check for empty contexts or all-zero rows in association matrix
  - Greedy walks timeout: switch to singleton method (PC1/PC2); PC3 showed no empirical advantage
  - Performance degradation with minimal rules: confidence threshold too aggressive; reduce threshold

- **First 3 experiments:**
  1. Baseline replication: Run confidence-based AnyBURL inference on a small dataset (e.g., Nations, 201 test triples) with increasing rule counts (100, 500, 1000, 1500) to establish baseline Hits@10 and MRR curves
  2. PC1 minimal validation: Learn PC on Nations with 0% confidence threshold, compute singleton marginals, select top 500 rules by Pθ({r}), evaluate Hits@10/MRR, compare to baseline at 500 rules
  3. Scaling test: On Family dataset, measure PC learning time for increasing rule counts (100, 500, 1000, 2000) to verify claimed linear scaling (R² ≥ 0.96 reported); if time is superlinear, check circuit structure complexity

## Open Questions the Paper Calls Out

- **Can this probabilistic circuit framework effectively mitigate the exponential search space challenges inherent in Inductive Logic Programming (ILP) systems?**
  - Basis in paper: The authors state in the "Future Work" section that they are "mainly interested in integrating our framework with Inductive Logic Programming systems to address their exponential search space challenges."
  - Why unresolved: The current paper validates the method only on knowledge graph completion using AnyBURL; the utility of the "rule context" distribution for constraining the vast search space of program synthesis in ILP is theoretically sound but empirically unproven.

- **How can the learned distribution over rule contexts be integrated into neuro-symbolic systems to provide interpretability without degrading the performance of the sub-symbolic components?**
  - Basis in paper: The "Future Work" section explicitly mentions the desire to "investigate how our framework can fit in and improve neuro-symbolic systems... by providing reduced rule sets alongside sub-symbolic components while maintaining interpretability."
  - Why unresolved: The current framework operates strictly at the symbolic rule-selection level; the interaction between the learned probabilistic circuits and the gradient-based learning mechanisms of neural networks remains unexplored.

- **Is abductive reasoning (via PyClause) the optimal method for establishing rule-context associations, or would semantic clustering yield more distinct and useful contexts?**
  - Basis in paper: The methodology states that rule contexts are determined using "training data and abduction," yet it also notes that the framework assumes "a finite number of contexts" and that not all constructs need to be implemented.
  - Why unresolved: The paper relies on a specific abductive implementation to generate the context matrix, but does not ablate this choice to see if other methods of grouping "meaningful subsets of rules" would result in better circuit learning or inference performance.

## Limitations
- Rule-context generation via PyClause abduction lacks detailed specifications and validation
- Performance claims depend critically on marginal correlation between rule probabilities and actual utility, which is not directly validated
- Greedy walk method (PC3) shows no empirical advantage over faster singleton methods despite higher computational cost

## Confidence
- High confidence: PC inference correctness (Propositions 1-4, formal semantics are mathematically sound)
- Medium confidence: Rule selection mechanism (marginal-based ranking shows strong empirical results, but mechanism not fully explained)
- Medium confidence: Performance claims (31× improvement, 70-96% reduction) are well-documented across 8 datasets but may depend on specific rule learning parameters

## Next Checks
1. Compute Pearson correlation between learned marginals Pθ({r}) and actual rule utility (measured by contribution to Hits@k when included) across multiple rule counts to verify the selection signal is meaningful
2. Measure performance degradation when using random contexts vs learned contexts on a held-out dataset to validate that context learning adds value beyond rule confidence
3. Vary EM iterations (5, 50, 100) and confidence thresholds (0%, 20%, 40%) on a small dataset (e.g., Nations) to establish sensitivity to these key parameters