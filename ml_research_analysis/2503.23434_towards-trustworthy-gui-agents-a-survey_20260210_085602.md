---
ver: rpa2
title: 'Towards Trustworthy GUI Agents: A Survey'
arxiv_id: '2503.23434'
source_url: https://arxiv.org/abs/2503.23434
tags:
- agents
- arxiv
- preprint
- agent
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey examines trustworthiness in GUI agents across five
  critical dimensions: security vulnerabilities, reliability in dynamic environments,
  transparency and explainability, ethical considerations, and evaluation methodologies.
  The analysis reveals significant challenges at the intersection of these dimensions,
  where multimodal interactions create novel attack surfaces and failure modes that
  traditional approaches cannot adequately address.'
---

# Towards Trustworthy GUI Agents: A Survey

## Quick Facts
- arXiv ID: 2503.23434
- Source URL: https://arxiv.org/abs/2503.23434
- Reference count: 24
- Primary result: Survey examines trustworthiness in GUI agents across five critical dimensions, revealing significant challenges at the intersection of these dimensions

## Executive Summary
This survey examines trustworthiness in GUI agents across five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. The analysis reveals significant challenges at the intersection of these dimensions, where multimodal interactions create novel attack surfaces and failure modes that traditional approaches cannot adequately address. Key findings include: agents remain highly vulnerable to adversarial attacks with up to 93% success rates for certain injection attacks; hallucination rates in multimodal agents continue to undermine reliability; and existing evaluation frameworks focus primarily on functional performance while overlooking essential aspects like security, reliability, and transparency. The survey identifies promising directions including real-time hallucination prevention mechanisms, adaptive safety architectures that dynamically adjust protection levels, and systematic failure analysis approaches to improve agent reliability. For GUI agents to be deployed responsibly in real-world scenarios, the research emphasizes the need for robust multimodal defense mechanisms, user-centered transparency systems, and comprehensive evaluation frameworks that assess both capability and safety.

## Method Summary
This survey paper examines trustworthiness in GUI agents by synthesizing findings from 24 referenced papers across five dimensions: security vulnerabilities, reliability, transparency/explainability, ethical considerations, and evaluation methodologies. The methodology involves systematic literature review and categorization, constructing taxonomies for each trustworthiness dimension. No original datasets or experiments are conducted—the paper references existing benchmarks and attack success rates from prior work. Key reported metrics from cited works include attack success rates (up to 93% for environmental injection attacks), task completion rates, and policy compliance scores. The survey identifies research gaps and promising directions for future work, particularly around cross-modal verification, adaptive safety architectures, and unified evaluation frameworks.

## Key Results
- GUI agents remain highly vulnerable to adversarial attacks, with up to 93% success rates for certain injection attacks
- Hallucination rates in multimodal agents continue to undermine reliability, with cascading failures across workflow steps
- Existing evaluation frameworks focus primarily on functional performance while overlooking essential aspects like security, reliability, and transparency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-modal verification appears to reduce the success rate of adversarial environmental injections by identifying inconsistencies between visual inputs and underlying code.
- **Mechanism:** The system parses both the raw screenshot (visual modality) and the HTML/DOM structure (text modality). When a user sees a "Submit" button but the underlying HTML contains malicious instructions or a different target endpoint, the agent flags the discrepancy or blocks the action.
- **Core assumption:** Attackers cannot perfectly synchronize malicious visual perturbations with the underlying code structure in a way that fools both modalities simultaneously.
- **Evidence anchors:**
  - The paper proposes "Connected Defense Layers," suggesting agents "cross-check text from a button image with the underlying HTML to catch possible tampering" (Section 3.4).
  - References AdvWeb and WebPI, where hidden HTML elements or adversarial prompts mislead agents (Section 3.1).
  - Corpus signals are weak for specific implementation details of adaptive architectures; most neighbors focus on threat taxonomies rather than dynamic mitigation.

### Mechanism 2
- **Claim:** Adaptive safety architectures may improve reliability by dynamically adjusting the strictness of action validation based on the perceived risk level of the context.
- **Mechanism:** The agent classifies the current environment (e.g., banking site vs. news site). In high-risk contexts, the system switches from "auto-execute" to "watch mode" or requires explicit user confirmation ("circuit breakers"), preventing irreversible errors in sensitive domains.
- **Core assumption:** Agents can accurately classify context and risk levels before executing critical actions, and latency introduced by these checks is acceptable.
- **Evidence anchors:**
  - The survey identifies "Adaptive Safety Architecture" as a future direction, citing OpenAI's CUA "watch mode" which "declines higher-risk tasks" (Section 4.4).
  - Mentions "circuit breakers" (Zou et al., 2024) that interrupt harmful outputs (Section 4.2).
  - Corpus signals are weak for specific implementation details of adaptive architectures; most neighbors focus on threat taxonomies rather than dynamic mitigation.

### Mechanism 3
- **Claim:** Hierarchical planning with user-centric intermediate explanations likely mitigates "hallucination snowballing" by forcing verification at distinct workflow steps.
- **Mechanism:** Instead of a single end-to-end prompt, the agent decomposes tasks into a high-level plan (e.g., "Find form -> Enter data -> Submit"). It verifies the existence of UI elements at each step before proceeding, preventing the agent from hallucinating non-existent buttons and cascading into failure.
- **Core assumption:** The decomposition logic robustly handles dynamic interfaces where elements may shift or change IDs between steps.
- **Evidence anchors:**
  - Section 4.1 discusses "hallucination snowballing" and cites "Residual Visual Decoding" and multi-agent debate as methods to ground reasoning.
  - Notes that "structured planning mechanism allows agents to decompose multi-step tasks" (Section 2).
  - "A Survey on (M)LLM-Based GUI Agents" supports the shift toward hierarchical architectures for complex task handling.

## Foundational Learning

- **Concept: Multimodal Perception vs. DOM Parsing**
  - **Why needed here:** GUI agents can "see" screens (pixels) or "read" code (accessibility trees/DOM). Understanding the trade-off is critical because security attacks often exploit the gap between these views (e.g., hidden HTML vs. visible pixels).
  - **Quick check question:** Can you explain why an agent relying solely on screenshot analysis might miss a hidden tracking pixel or a cloaked malicious link visible only in the DOM?

- **Concept: Contextual Integrity & Privacy**
  - **Why needed here:** The paper emphasizes that agents access sensitive PII. Foundational knowledge of data flow is required to implement "Privacy-Conscious Delegation" (like PAPILLON).
  - **Quick check question:** If an agent operates on a medical record interface, should the raw screenshot be sent to a cloud model? Why or why not?

- **Concept: Reinforcement Learning from Human Feedback (RLHF) vs. Safety Alignment**
  - **Why needed here:** The paper discusses "alignment with human values." Understanding how models are trained to refuse harmful requests helps in diagnosing why "refusal-trained LLMs" might still be jailbroken via browser contexts (Section 3.1).
  - **Quick check question:** Why might standard text-based safety training fail when the malicious input is embedded in an image file?

## Architecture Onboarding

- **Component map:** Perception Module (Vision Encoder, Text Parser) -> Reasoning Core (LLM/LMM) -> Safety/Guard Layer (GuardAgent, AdversaFlow) -> Executor (clicks, typing, API calls)

- **Critical path:** The Perception-to-Reasoning handoff. If the Perception module hallucinates a UI element (e.g., misreading a "Delete" button as "Edit"), the Reasoning Core cannot correct it because it assumes the ground truth is valid.

- **Design tradeoffs:**
  - **Latency vs. Verification:** Real-time cross-modal verification (checking image vs. HTML) adds latency but reduces injection risks.
  - **Generalization vs. Specificity:** Fine-tuning on specific apps (e.g., specialized verifiers) improves reliability but reduces the agent's ability to handle "unfamiliar interfaces" (Section 4.3).

- **Failure signatures:**
  - **Hallucination Snowballing:** The agent confidently executes actions on non-existent UI elements, leading to 404s or data corruption.
  - **Environmental Injection:** Agent executes a "purchase" on a malicious site because it trusted a "Discount" overlay that was actually an attack vector (AEIA-MN).

- **First 3 experiments:**
  1. **Adversarial Overlay Test:** Present the agent with a benign webpage overlaid with a transparent "click" button. Verify if the Safety Layer blocks the action or if the agent clicks blindly.
  2. **Context-Switch Stress Test:** Navigate the agent from a low-risk environment (Wikipedia) to a high-risk one (Banking Login) to see if the "Adaptive Safety" mechanisms trigger increased logging or human-in-the-loop requirements.
  3. **Cross-Modal Consistency Check:** Modify the HTML of a button to say "Cancel" while the visual label says "Submit." Determine if the agent flags the anomaly or proceeds with the visual prompt.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can cross-modal consistency verification (e.g., matching visual elements to underlying HTML) effectively neutralize environmental injection attacks in GUI agents without degrading task latency?
- **Basis in paper:** Section 3.4 proposes "Connected Defense Layers," suggesting agents should "cross-check text from a button image with the underlying HTML to catch any possible tampering."
- **Why unresolved:** While the paper identifies this as a defense direction, current approaches rely on unimodal input validation, and the efficacy of real-time cross-modal checks against the high success rates of attacks like AEIA-MN remains unstudied.
- **What evidence would resolve it:** Empirical results demonstrating reduced attack success rates and acceptable latency overhead in agents deployed with cross-modal verification layers.

### Open Question 2
- **Question:** How can evaluation frameworks be unified to assess capability, safety, and transparency simultaneously rather than treating them as isolated metrics?
- **Basis in paper:** Section 7.3 states, "Future work could unify the disparate tasks, data sources, and metrics into more holistic frameworks, enabling meaningful comparisons of agents’ safety, robustness, and usability."
- **Why unresolved:** Current benchmarks are fragmented (e.g., security vs. task completion), preventing meaningful comparisons of overall agent trustworthiness or the trade-offs between functional performance and safety.
- **What evidence would resolve it:** The development of a benchmark suite that correlates functional performance with security vulnerability scores and transparency ratings across diverse agent architectures.

### Open Question 3
- **Question:** Do adaptive safety architectures that dynamically adjust protection levels based on interface context (e.g., banking vs. general browsing) successfully balance user autonomy with risk mitigation better than static safety models?
- **Basis in paper:** Section 4.4 identifies "Adaptive Safety Architecture" as a promising direction, proposing agents "dynamically adjust protection levels based on context."
- **Why unresolved:** The paper notes that while commercial systems like OpenAI's CUA use ad-hoc "watch modes," there is a lack of systematic evaluation for dynamic safety calibration in the broader research literature.
- **What evidence would resolve it:** Comparative studies measuring task success rates and safety violation rates in agents using context-aware safety modules versus fixed safety protocols in dynamic environments.

## Limitations
- Potential publication bias toward successful attack demonstrations may overstate security vulnerabilities while underrepresenting successful defenses
- Reliance on synthetic attack datasets and controlled lab conditions may not reflect real-world deployment scenarios
- Heavy dependence on theoretical frameworks rather than measured outcomes for ethical considerations and evaluation methodology

## Confidence
- Security vulnerability claims: **Medium** confidence (synthetic attack datasets, controlled conditions)
- Reliability claims (hallucination rates): **Medium** confidence (diverse task definitions across studies)
- Transparency and explainability findings: **High** confidence (multiple corroborating sources)
- Ethical considerations and evaluation methodology: **Low** to **Medium** confidence (limited empirical validation)

## Next Checks
1. Conduct systematic replication studies of the most-cited attack benchmarks (AEIA-MN, AdvWeb) in production environments to verify real-world applicability of reported success rates
2. Implement and test the proposed adaptive safety architectures in multi-user scenarios to measure performance degradation under load
3. Design controlled experiments comparing cross-modal verification approaches against single-modality baselines using diverse interface types (web, mobile, desktop) to quantify the actual security benefit