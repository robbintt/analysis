---
ver: rpa2
title: Budget-Aware Tool-Use Enables Effective Agent Scaling
arxiv_id: '2511.17006'
source_url: https://arxiv.org/abs/2511.17006
tags:
- budget
- scaling
- tool
- agent
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how tool-augmented agents can scale effectively
  under explicit tool-call budget constraints. It finds that standard agents quickly
  hit performance ceilings because they lack budget awareness and cannot adapt to
  available resources.
---

# Budget-Aware Tool-Use Enables Effective Agent Scaling

## Quick Facts
- arXiv ID: 2511.17006
- Source URL: https://arxiv.org/abs/2511.17006
- Authors: Tengxiao Liu; Zifeng Wang; Jin Miao; I-Hung Hsu; Jun Yan; Jiefeng Chen; Rujun Han; Fangyuan Xu; Yanfei Chen; Ke Jiang; Samira Daruki; Yi Liang; William Yang Wang; Tomas Pfister; Chen-Yu Lee
- Reference count: 40
- Primary result: Budget-aware agents achieve up to 24.6% accuracy on BrowseComp under strict budget constraints, outperforming baselines without training

## Executive Summary
This paper investigates how tool-augmented agents can scale effectively under explicit tool-call budget constraints. It finds that standard agents quickly hit performance ceilings because they lack budget awareness and cannot adapt to available resources. To address this, the authors introduce Budget Tracker, a lightweight module that provides continuous budget signals to the agent, enabling more strategic and efficient tool use. They also develop BATS (Budget Aware Test-time Scaling), a framework that dynamically adjusts planning and verification based on real-time budget status, allowing the agent to "dig deeper" or "pivot" depending on resource availability. Experiments on web search tasks show that budget-aware methods significantly improve performance and push the cost-performance Pareto frontier.

## Method Summary
The authors propose Budget Tracker, a prompt-level module that injects real-time budget status (used/remaining per tool) into the agent's reasoning loop after each tool response. This enables the agent to condition subsequent decisions on current resource availability. They also introduce BATS, a framework combining budget-aware planning with constraint decomposition and self-verification. Questions are decomposed into exploration and verification constraints, with a persistent tree-structured checklist tracking progress and resource usage. After proposing answers, a verification module performs backward constraint checking and outputs SUCCESS/CONTINUE/PIVOT decisions with trajectory summaries. The approach requires no training and operates purely at the prompt level.

## Key Results
- Budget Tracker enables agents to maintain scaling performance while standard ReAct plateaus at ~12.6% accuracy regardless of budget increase
- BATS achieves up to 24.6% accuracy on BrowseComp under strict budget constraints, significantly outperforming baselines
- Budget-aware methods push the cost-performance Pareto frontier, achieving higher accuracy at lower cost compared to sequential and parallel scaling baselines
- The verification module is crucial for navigating solution space, with its removal causing a 3.3% drop on BrowseComp

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit budget signals enable agents to dynamically adapt tool-use strategy and avoid premature termination.
- Mechanism: The Budget Tracker injects real-time resource state (used/remaining per tool) into the agent's reasoning loop at each iteration. This conditions subsequent planning and tool-call decisions on current resource availability, preventing both premature conclusion and resource exhaustion without productive output.
- Core assumption: The model can effectively interpret numerical budget signals and translate them into behavioral changes without explicit training.
- Evidence anchors:
  - [abstract] "We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling."
  - [Section 3.1] "Despite its simplicity, Budget Tracker operates purely at the prompt level and makes the agent explicitly aware of its resource consumption and remaining budget, enabling it to condition subsequent reasoning steps on the updated resource state."
  - [corpus] SMART (arXiv:2502.11435) addresses complementary "tool overuse" from lacking self-awareness, suggesting budget awareness is one facet of broader resource calibration.
- Break condition: If the model lacks sufficient instruction-following capability to respond to numerical budget signals, or if budgets are so tight that no adaptive behavior is possible, the mechanism degrades to standard ReAct performance.

### Mechanism 2
- Claim: Constraint decomposition plus tree-structured planning enables more efficient budget allocation across exploration and verification phases.
- Mechanism: Questions are decomposed into "exploration" constraints (broad, candidate-generating) and "verification" constraints (narrow, candidate-filtering). The agent maintains a persistent checklist that tracks completed/failed steps and resource usage, preventing redundant calls and guiding budget-aware breadth/depth tradeoffs.
- Core assumption: Questions contain distinguishable constraint types, and starting with verification constraints (rather than exploration) is systematically suboptimal.
- Evidence anchors:
  - [Section 4.1] "The agent is further instructed to generate and maintain an explicit plan throughout execution. This tree-structured plan acts as a dynamic checklist, recording step status, resource usage, and allocation, while guiding future actions. It is never overwritten."
  - [Section 4.1] "While a verification clue can sometimes provide a direct shortcut to the answer, relying on it prematurely is risky, as it may consume resources without guaranteeing progress."
  - [corpus] No direct corpus evidence on constraint decomposition specifically; related work (TUMIX, BAMAS) focuses on multi-agent coordination rather than constraint taxonomy.
- Break condition: If questions lack clear constraint structure, or if the model cannot reliably categorize constraints, planning overhead may exceed benefits.

### Mechanism 3
- Claim: Budget-aware self-verification enables early termination of unproductive paths and strategic pivots before budget exhaustion.
- Mechanism: After proposing an answer, the verification module performs backward constraint checking and issues one of three decisions: SUCCESS (all constraints satisfied), CONTINUE (correctable with sufficient budget), or PIVOT (dead-end or insufficient budget). A trajectory summary compresses context for subsequent attempts.
- Core assumption: The verification module can accurately distinguish correctable errors from fundamental dead-ends, and remaining budget is predictive of whether correction is feasible.
- Evidence anchors:
  - [Section 4.2] "Based on this analysis, the module then makes a verification decision based on the above assessment and budget status... the agent is expected to terminate expensive or low-yield directions early and PIVOT toward a different direction."
  - [Section 6.3] "Removing the verification module causes a more significant drop on BrowseComp (from 18.7% to 15.4%). This suggests that the verification module is crucial for enabling the agent to navigate the solution space and accurately assess its current progress."
  - [corpus] "Budget-aware Test-time Scaling via Discriminative Verification" (arXiv:2510.14913) explores discriminative verification for cost reduction, providing convergent evidence that verification-based selection is a key scaling lever.
- Break condition: If verification judgments are unreliable (false positives/negatives), the agent may either waste budget on unpromising paths or abandon correct approaches prematurely.

## Foundational Learning

- Concept: **ReAct-style agent loops**
  - Why needed here: Budget Tracker is designed as a plug-in for ReAct-based agents. Understanding the think→act→observe cycle is prerequisite to understanding where budget signals are injected and how they modify the loop.
  - Quick check question: Can you trace one full iteration of a ReAct loop and identify where external tool responses enter the context?

- Concept: **Test-time scaling paradigms (sequential vs. parallel)**
  - Why needed here: The paper explicitly compares budget-aware scaling against sequential (budget-forcing) and parallel (majority vote, Best-of-N) baselines. Understanding these paradigms is necessary to interpret the scaling curves.
  - Quick check question: What is the difference between sequential scaling via budget-forcing and parallel scaling via Best-of-N?

- Concept: **Pareto frontier analysis**
  - Why needed here: The paper's central claim is that budget-aware methods "push the cost-performance Pareto frontier." Interpreting the unified cost metric and scaling curves requires understanding Pareto optimality.
  - Quick check question: If Method A achieves higher accuracy than Method B at lower cost, what does this imply about their relative positions on the Pareto frontier?

## Architecture Onboarding

- Component map: Question → Planning Module → Budget Tracker (loop) → Self-Verification → Answer Selection
- Critical path:
  1. Question input → Planning module generates initial plan with constraint decomposition
  2. Each iteration: Budget Tracker signal → Model generates thinking + tool call → Tool response → Budget Tracker update
  3. On answer proposal: Verification module evaluates → Decision (SUCCESS/CONTINUE/PIVOT)
  4. If CONTINUE/PIVOT: Summary replaces trajectory, new attempt with remaining budget
  5. Budget exhaustion → Final answer selection across verified answers

- Design tradeoffs:
  - **Prompt-level vs. training-based budget awareness**: Budget Tracker is training-free but may be less robust than learned budget prediction. The paper explicitly positions BATS as "entirely training-free" versus specialized agents like OpenAI Deep Research.
  - **Full trajectory vs. summary context**: BATS removes historical tool responses and uses verification summaries to control context length (Table 6 shows 57% cache token reduction), trading potentially useful detail for efficiency.
  - **Early stopping vs. budget exhaustion**: Early stopping enables cost efficiency but may miss solutions requiring exhaustive search; the paper evaluates both settings.

- Failure signatures:
  - **Budget-unaware plateau**: Standard ReAct saturates at ~12.6% on BrowseComp regardless of budget increase (Figure 3), indicating failure to utilize additional resources.
  - **Verification misclassification**: If verification returns SUCCESS on incorrect answers or PIVOT on correctable paths, subsequent attempts are misdirected. Ablation (Table 4) shows verification removal causes 3.3% drop on BrowseComp.
  - **Context overflow**: Without response removal or summarization, long trajectories exceed context limits (Section A.2 notes truncation to 150K characters per browse).

- First 3 experiments:
  1. **Budget Tracker ablation on ReAct**: Run ReAct with and without Budget Tracker on BrowseComp subset (e.g., 200 examples) at budgets 10/30/100. Compare accuracy and tool utilization. Expected: Budget Tracker maintains scaling while vanilla ReAct plateaus.
  2. **Verification decision calibration**: Sample 50 cases where verification returns PIVOT or CONTINUE. Manually label whether the decision was correct. Calculate precision/recall of verification judgments. This diagnoses whether verification errors drive performance gaps.
  3. **Cost-per-accuracy analysis**: Plot unified cost vs. accuracy for BATS, ReAct+Budget Tracker, and parallel majority vote across budgets 10-200. Verify Pareto frontier claim by checking whether BATS dominates on cost-accuracy plane.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agents jointly optimize under multiple interacting resource constraints (tool-call budgets, token limits, and inference latency) when these constraints conflict?
- Basis in paper: [explicit] Section E states: "a more realistic and challenging scenario involves managing multiple resource constraints jointly... Understanding and controlling agent behavior under multi-dimensional constraints is important for deploying scalable systems in real environments."
- Why unresolved: Current work only studies tool-call budgets in isolation; real deployment requires balancing competing constraints that may have non-linear interactions.
- What evidence would resolve it: Experiments showing agents maintaining performance while jointly satisfying thresholds on tool calls, tokens, and latency simultaneously, with analysis of trade-off curves.

### Open Question 2
- Question: Can agents learn to accurately predict their resource consumption before execution, enabling proactive rather than reactive budget allocation?
- Basis in paper: [explicit] Section E states: "Our empirical analysis suggests that models often underestimate their actual resource consumption, which can lead to suboptimal performance. Developing accurate resource estimation and principled budget allocation strategies is a promising direction for future work."
- Why unresolved: The paper observes underestimation but does not investigate causes or solutions; agents currently adapt reactively via Budget Tracker signals.
- What evidence would resolve it: A predictive model achieving <10% error between estimated and actual tool usage across diverse tasks, integrated into the planning module to improve first-attempt success rates.

### Open Question 3
- Question: What memory architectures and context compression techniques optimally balance trajectory preservation against context length for long-horizon budget-aware agents?
- Basis in paper: [explicit] Section E states: "Designing more effective memory formats and identifying the right balance between context length and performance are important open challenges."
- Why unresolved: BATS uses simple truncation and summarization; the paper does not compare alternative memory schemes or quantify the information loss from compression.
- What evidence would resolve it: Ablation studies comparing hierarchical memory, retrieval-augmented context, and structured representations on tasks requiring 100+ tool calls, measuring both accuracy and token efficiency.

## Limitations
- The paper's core claims about budget-aware scaling rest on prompt-level modifications rather than learned budget awareness, which may limit generalization to tasks where budget signals are less interpretable.
- The verification module's decision quality (SUCCESS/CONTINUE/PIVOT) is critical but only indirectly evaluated through end-to-end performance.
- The constraint decomposition assumption (exploration vs. verification constraints) lacks empirical validation on whether questions naturally exhibit this structure.
- The unified cost metric aggregates token and tool costs, but the relative weighting and sensitivity to cost parameter choices are not explored.

## Confidence
- High confidence: Budget Tracker enables scaling where standard ReAct plateaus (Section 6.3 results show clear divergence)
- Medium confidence: BATS achieves Pareto frontier improvements (based on Figure 4 but limited to three tasks)
- Medium confidence: Constraint decomposition and tree-structured planning improve efficiency (logical but minimally validated empirically)

## Next Checks
1. **Verification decision quality**: Sample 100 cases where verification returns CONTINUE or PIVOT, manually label correctness, and calculate precision/recall to determine if verification errors explain performance gaps
2. **Constraint structure validation**: Analyze BrowseComp questions to empirically verify the prevalence of separable exploration/verification constraints and test model performance when constraint decomposition is randomized vs. systematic
3. **Cost sensitivity analysis**: Replicate key experiments varying the relative weighting of token vs. tool costs in the unified metric to test whether Pareto frontier claims hold across reasonable cost parameter ranges