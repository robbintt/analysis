---
ver: rpa2
title: 'Automatic Posology Structuration : What role for LLMs?'
arxiv_id: '2506.19525'
source_url: https://arxiv.org/abs/2506.19525
tags:
- llms
- language
- nerl
- posology
- entity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of Large Language Models (LLMs) for
  automatic structuration of French posology instructions. Traditional machine learning
  approaches, based on Named Entity Recognition and Linking (NERL), struggle with
  the ambiguity and variability of prescription language.
---

# Automatic Posology Structuration : What role for LLMs?

## Quick Facts
- arXiv ID: 2506.19525
- Source URL: https://arxiv.org/abs/2506.19525
- Authors: Natalia Bobkova; Laura Zanella-Calzada; Anyes Tafoughalt; Raphaël Teboul; François Plesse; Félix Gaschi
- Reference count: 0
- Primary result: A hybrid pipeline combining NERL and LLM approaches achieves 91% structuration accuracy for French posology instructions

## Executive Summary
This study investigates the effectiveness of Large Language Models (LLMs) for structuring French posology instructions, comparing them against traditional Named Entity Recognition and Linking (NERL) approaches. The research reveals that while standalone LLMs struggle to match NERL performance, a hybrid approach that routes low-confidence cases from NERL to LLMs achieves superior results. The proposed system demonstrates 91% accuracy while balancing computational efficiency and semantic understanding, offering a practical solution for clinical posology structuration.

## Method Summary
The researchers benchmarked various open-source and proprietary LLMs using both prompt engineering and fine-tuning techniques against their NERL baseline. They systematically evaluated model performance across multiple dimensions, including structural precision and semantic understanding of ambiguous prescription language. The study employed comprehensive error analysis to identify complementary strengths between NERL and LLM approaches, leading to the development of a routing mechanism that leverages each system's advantages while minimizing computational overhead.

## Key Results
- NERL excels in structural precision while LLMs better handle semantic nuances in prescription language
- Standalone LLMs require fine-tuning to achieve comparable accuracy to NERL baseline
- Hybrid pipeline achieves 91% structuration accuracy by routing NERL low-confidence cases (<0.8) to LLM for processing

## Why This Works (Mechanism)
The hybrid approach works by exploiting the complementary strengths of rule-based and learning-based systems. NERL provides high precision for structurally straightforward cases through deterministic pattern matching, while LLMs handle the semantic complexity and linguistic variability that challenge traditional approaches. The confidence-based routing mechanism ensures computational efficiency by only invoking the more resource-intensive LLM when necessary, creating a scalable solution that maintains accuracy across diverse prescription formats.

## Foundational Learning
- **Posology structuration**: The process of extracting and organizing medication dosing instructions from clinical text; needed to enable automated medication management systems and reduce prescribing errors.
- **Named Entity Recognition and Linking (NERL)**: A rule-based approach that identifies and categorizes specific entities in text; needed as a reliable baseline for structured medical information extraction.
- **Prompt engineering**: The practice of designing effective input prompts for LLMs; needed to optimize zero-shot performance without fine-tuning.
- **Fine-tuning**: The process of adapting pre-trained models to specific tasks using domain data; needed to achieve competitive performance on specialized medical language tasks.
- **Confidence thresholds**: Decision boundaries for routing between systems; needed to balance accuracy with computational efficiency in hybrid architectures.

## Architecture Onboarding

Component Map: Prescription Text -> NERL Parser -> Confidence Assessment -> [Direct Output | LLM Resolver] -> Structured Output

Critical Path: The primary execution path involves NERL parsing followed by confidence assessment, with LLM invocation occurring only for low-confidence cases. This creates a bottleneck at the confidence assessment stage that must be optimized for real-time performance.

Design Tradeoffs: The system trades computational overhead (LLM invocation) for accuracy gains, accepting higher resource usage on complex cases to maintain overall performance. This represents a shift from traditional optimization focused solely on minimizing computation.

Failure Signatures: System failures manifest as cascading errors when NERL confidence scores are miscalibrated, leading to either unnecessary LLM invocations (wasting resources) or missed opportunities for LLM assistance (reducing accuracy).

First Experiments:
1. Vary confidence threshold from 0.7 to 0.95 to identify optimal balance between accuracy and computational cost
2. Test alternative routing strategies (random, highest-confidence first) against current lowest-confidence-first approach
3. Evaluate impact of domain-specific fine-tuning on LLM performance for posology-specific terminology

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to French posology instructions without broader medical text validation
- Computational cost analysis remains incomplete for production deployment scenarios
- 91% accuracy may not generalize to other clinical contexts or languages

## Confidence
- High confidence in comparative performance analysis between NERL and LLM approaches
- Medium confidence in hybrid pipeline effectiveness without real-world deployment validation
- Low confidence in generalizability beyond French posology domain tested

## Next Checks
1. Deploy hybrid pipeline in clinical setting for at least three months to assess real-world performance, latency, and integration challenges
2. Expand evaluation to include multiple medical document types (discharge summaries, clinical notes) and additional languages
3. Conduct comprehensive cost-benefit analysis comparing hybrid approach against alternative solutions including cloud-based LLM-only implementations