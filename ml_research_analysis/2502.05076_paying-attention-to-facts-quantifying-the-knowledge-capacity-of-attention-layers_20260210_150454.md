---
ver: rpa2
title: 'Paying Attention to Facts: Quantifying the Knowledge Capacity of Attention
  Layers'
arxiv_id: '2502.05076'
source_url: https://arxiv.org/abs/2502.05076
tags:
- rank
- attention
- database
- softmax
- dhead
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the capacity of single-layer attention-only
  transformers to memorize factual information from databases using a linear-algebraic
  approach. The authors define a 3-tensor associated with each database and propose
  its rank as a measure of database size, deriving bounds on this rank based on database
  properties.
---

# Paying Attention to Facts: Quantifying the Knowledge Capacity of Attention Layers

## Quick Facts
- arXiv ID: 2502.05076
- Source URL: https://arxiv.org/abs/2502.05076
- Reference count: 12
- Single-layer attention transformers' factual storage capacity depends more on value-output weights than query-key weights, enabling capacity increases without parameter growth.

## Executive Summary
This paper introduces a linear-algebraic framework for analyzing the factual memorization capacity of single-layer attention-only transformers. By representing databases as 3-tensors and measuring their rank, the authors derive bounds on how much information attention layers can store. The key insight is that value-output (W_VO) weights primarily determine storage capacity while query-key (W_QK) weights mainly serve routing functions. This asymmetry enables architectural optimizations that increase capacity without increasing parameter count.

## Method Summary
The method constructs 3-tensors for both databases and attention layers, using tensor rank as a capacity measure. For databases, a 3-tensor D ∈ ℝ^{|K|×|Q|×|V|} is created where each entry indicates whether a triple exists. Attention layers produce a tensor L = E + Σ_h A^h V^h through their embed-unembed, query-key attention, and value-output circuits. The paper derives rank bounds for both tensors and empirically validates that model capacity correlates with d_head,vo dimensions rather than d_head,qk. Training uses single-layer attention-only decoders on triple completion tasks with up to 2,000 epochs.

## Key Results
- Model capacity depends more on d_head,vo than d_head,qk dimensions, suggesting capacity can be increased without parameter growth
- Argmax and softmax operations can arbitrarily inflate tensor rank, decoupling parameter count from memorization capacity
- The rank relationship rank(D) ≤ rank(L) provides meaningful capacity bounds despite softmax distortion effects
- Thresholded softmax (softmax≥τ) at τ=0.95 provides optimal balance between rank preservation and memorization accuracy

## Why This Works (Mechanism)

### Mechanism 1: Tensor Rank as a Capacity Measure for Factual Storage
- Claim: Database tensor rank captures structural complexity beyond simple triple counts, providing meaningful capacity bounds.
- Evidence: Proposition 2.1 shows rank(D) ≤ min(Σ_k |V_k|, Σ_q |V_q|), tighter than |D|. Empirical results confirm rank(L) bounds predict memorization capacity.
- Core assumption: Factual recall operates additively across attention heads, making rank a meaningful invariant.
- Break condition: Argmax/softmax can inflate rank, decoupling parameter count from capacity (rank-10 layers memorizing rank-80 databases).

### Mechanism 2: Asymmetric Roles of Value-Output vs Query-Key Weights
- Claim: d_head,vo determines capacity while d_head,qk serves routing; capacity can increase without parameter growth.
- Evidence: Proposition 3.5 rank bound includes d_head,vo but not d_head,qk. Figure 4 shows accuracy increases more with d_head,vo changes.
- Core assumption: Attention patterns can be sufficiently discriminating even with low-rank query-key projections.
- Break condition: Very small d_head,vo requires some minimal d_head,qk for routing (Figure 4 diagonals).

### Mechanism 3: Softmax/Argmax Rank Distortion
- Claim: Argmax and softmax can arbitrarily inflate effective rank, decoupling parameter count from memorization capacity.
- Evidence: Proposition 3.6-3.7 show rank inflation for any rank-r matrix under argmax/softmax. Figure 3 shows τ=0.95 provides best rank-accuracy correspondence.
- Core assumption: Threshold τ can be tuned to balance rank preservation against precision requirements.
- Break condition: Limited machine precision bounds practical rank inflation; softmax bottleneck literature suggests real-world effects are bounded.

## Foundational Learning

- **Tensor rank and decomposition**: Understanding 3-tensor rank as distinct from matrix rank, including why NP-hard to compute and how slice-wise ranks differ. Quick check: Can a 3-tensor with two rank-2 matrix slices have tensor rank 3? (Yes—cross-slice patterns matter.)

- **Attention circuit decomposition (W_QK vs W_VO)**: Understanding how attention separates into query-key "routing" and value-output "storage" circuits. Quick check: Which circuit's rank bound depends on |Q|? (W_VO via Proposition 3.5.)

- **Linear structure of transformer logits**: Understanding why L = E + Σ A^h V^h and the "additive motif" rely on logits being linear functions of tokens. Quick check: Why doesn't W_QK contribute linearly to output logits? (It populates attention weights via softmax, acting as lookup rather than direct linear contribution.)

## Architecture Onboarding

- **Component map**: Database tensor D → (k,q) pairs → A^h computation via softmax(W_QK) → A^h V^h multiplication → L aggregation → argmax/softmax≥τ prediction

- **Critical path**: D → (k,q) pairs → A^h computation via softmax(W_QK) → A^h V^h multiplication → L aggregation → argmax/softmax≥τ prediction

- **Design tradeoffs**:
  - d_head,vo vs d_head,qk: Increase d_head,vo for capacity without parameter growth; minimize d_head,qk if only routing needed
  - Threshold τ: Lower τ allows higher effective capacity but looser guarantees; higher τ tightens rank correspondence but may require more epochs
  - Heads vs dimensions: Multiple heads with smaller d_head,vo versus fewer heads with larger d_head,vo depends on predicate diversity (|Q| factor in bounds)

- **Failure signatures**:
  - Memorization fails despite sufficient parameters: Check if d_head,vo too small relative to database structure
  - High argmax accuracy but low softmax≥τ accuracy: Softmax rank distortion; model exploiting threshold gaps
  - Accuracy degrades when d_head,qk reduced aggressively at small d_head,vo: Minimal routing capacity needed

- **First 3 experiments**:
  1. Validate rank bounds: Train single-head attention layers on synthetic databases with known tensor ranks; plot accuracy vs (d_model + d_head,vo)
  2. Test d_head,vo vs d_head,qk tradeoff: Fix total parameters, sweep d_head,vo from 1 to d_model while setting d_head,qk = d_model - d_head,vo + 1
  3. Characterize softmax rank distortion: For fixed layer and database, sweep τ from 0.5 to 0.99; plot rank(softmax≥τ(L)) vs τ

## Open Questions the Paper Calls Out

- **Tighter effective rank bounds**: The paper lacks good theoretical bounds for the effective rank of databases (lowest rank of tensors that correctly predict only valid query fibers). Future work could derive better bounds on effective rank, which can be much lower than actual rank (Example 2.4 shows rank 5 vs effective rank 1).

- **Multi-layer architecture effects**: The impact of positional embeddings, MLP layers, and having multiple layers is unknown. This work analyzes only single-layer attention-only transformers, leaving real-world architectures unexplored.

- **Complex d_head,qk interactions**: Why does increasing d_head,qk improve accuracy at small d_head,vo but become neutral or detrimental at larger d_head,vo? The theoretical bounds (Equation 9) don't include d_head,qk, yet empirical results show complex interactions.

- **Generalization beyond memorization**: Do rank-based capacity findings generalize to language models trained on diverse tasks beyond pure database memorization? Reducing d_head,qk may harm tasks requiring complex attention patterns.

## Limitations

- **Scalability concerns**: Tensor rank computation is NP-hard, limiting the approach to small synthetic databases (≤200 triples) rather than realistic knowledge graphs with millions of triples.

- **Single-layer restriction**: The analysis focuses exclusively on single-layer attention-only transformers, leaving unexplored how multi-layer architectures might distribute facts across layers for higher capacity.

- **Softmax distortion uncertainty**: While theoretically established, the practical impact of softmax rank inflation depends on numerical precision and temperature scaling, with unclear implications for real-world model design.

## Confidence

- **High confidence**: The asymmetric capacity dependence on d_head,vo versus d_head,qk. Directly supported by Proposition 3.5's rank bound and Figure 4's empirical results showing clear accuracy patterns.

- **Medium confidence**: Tensor rank as a capacity measure. Mathematical framework is sound but practical utility depends on accurate rank estimation and the assumption that rank captures all relevant structural properties.

- **Low confidence**: Softmax rank distortion characterization. Theoretical results are mathematically correct but practical impact depends on numerical precision, temperature scaling, and specific attention pattern distributions.

## Next Checks

1. **Scale-up rank estimation validation**: Implement approximate tensor rank computation methods (e.g., CP decomposition with tolerance) and validate rank-based capacity predictions on databases with 10×-100× more triples than the current 200-triple limit.

2. **Cross-threshold accuracy correlation**: For a fixed database-layer pair, systematically vary τ from 0.5 to 0.99 and measure both the rank of softmax≥τ(L) and the corresponding τ-accuracy. Plot rank versus accuracy across thresholds.

3. **Minimal routing capacity experiment**: Design an ablation study where d_head,qk is reduced while d_head,vo is held constant at small values (1-3). Measure the minimum d_head,qk required to achieve baseline accuracy for databases with varying numbers of predicates |Q|.