---
ver: rpa2
title: Sample-Efficient Behavior Cloning Using General Domain Knowledge
arxiv_id: '2501.16546'
source_url: https://arxiv.org/abs/2501.16546
tags:
- knowledge
- self
- target
- torch
- heading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Knowledge Informed Models (KIM) to improve
  sample efficiency in behavior cloning by incorporating expert domain knowledge into
  the policy structure. KIM leverages large language models to generate a structured
  policy architecture based on natural language descriptions of expert knowledge,
  then tunes the parameters using demonstrations.
---

# Sample-Efficient Behavior Cloning Using General Domain Knowledge

## Quick Facts
- arXiv ID: 2501.16546
- Source URL: https://arxiv.org/abs/2501.16546
- Reference count: 40
- Key result: Achieves 20%+ higher success rates in lunar lander with 5 demonstrations using structured policies

## Executive Summary
This paper introduces Knowledge Informed Models (KIM) to improve sample efficiency in behavior cloning by incorporating expert domain knowledge into the policy structure. KIM leverages large language models to generate a structured policy architecture based on natural language descriptions of expert knowledge, then tunes the parameters using demonstrations. The approach was evaluated on lunar lander and car racing tasks, showing significant improvements: KIM learned to solve tasks with as few as 5 demonstrations, outperformed baseline models, and demonstrated greater robustness to action noise.

## Method Summary
KIM uses an LLM (GPT-4) to convert natural language expert knowledge into a structured policy represented as a sparse computational graph (DAG). The LLM generates PyTorch code with semantic latent variables and edges representing learnable relationships. Parameters are initialized based on semantic correlations (positive/negative) inferred from the description, then tuned via gradient descent on demonstration data. This contrasts with unstructured MLPs that learn all relationships from scratch.

## Key Results
- Achieved 20%+ higher success rates in lunar lander with 5 demonstrations versus unstructured baselines
- Required 200x fewer parameters in car racing task while maintaining comparable performance
- Demonstrated greater robustness to action noise, retaining 65% of reward compared to baseline degradation

## Why This Works (Mechanism)

### Mechanism 1: Semantic Inductive Bias via Sparse Structuring
The policy architecture mirrors the task's causal structure, constraining the search space to meaningful latent relationships. The LLM translates natural language domain knowledge into a sparse computational graph where nodes are semantic latent variables and edges are learnable weights, preventing learning of spurious correlations.

### Mechanism 2: Semantic Initialization for Non-Convex Optimization
Pre-setting parameter signs based on semantic logic stabilizes training in low-data regimes by initializing the model in a "sensible" basin of the loss landscape. Instead of random initialization, the LLM assigns small initial weights (e.g., -0.1 or 0.1) based on inferred correlation direction.

### Mechanism 3: Constraint-Based Robustness to Action Noise
Structured policies degrade more gracefully under noise because the architecture enforces hard constraints that prevent cascading errors. Non-learnable logic (e.g., speed caps) limits the "action space" the policy can reach, acting as a safety filter.

## Foundational Learning

- **Behavior Cloning (BC) as Supervised Learning**: KIM uses BC to tune parameters, reducing policy learning to a supervised loss problem. Why needed: Understanding distribution shift mitigation. Quick check: Can you explain why BC suffers from distribution shift and how structured models might mitigate this?

- **Computational Graphs & DAGs**: The "structured policy" is a graph where data flows from inputs to latents to outputs. Why needed: Required for debugging generated code. Quick check: If variable A depends on B, and B depends on A, why would KIM generation fail?

- **Prompt Engineering for Code Synthesis**: Quality of policy directly tied to LLM prompt. Why needed: Critical for generating valid policy structures. Quick check: Why does the prompt explicitly ask the LLM to "list variables in the order they should be computed"?

## Architecture Onboarding

- **Component map**: Input (Natural Language + Space Definitions) → LLM Processor (GPT-4 → PyTorch nn.Module) → Training Loop (Adam on Demonstrations) → Inference (State → Latents → Action)
- **Critical path**: LLM Generation Phase - if LLM fails to connect critical latent variable, no parameter tuning will solve the task
- **Design tradeoffs**: Flexibility vs. Efficiency (sacrifices universal approximation for sample efficiency); Manual vs. Automated (prompt engineering is manual)
- **Failure signatures**: High Variance with Random Initialization; Syntax/Logic Errors in generated code; Oversimplification from vague expert prompts
- **First 3 experiments**: 1) Run KIM vs. MLP on Lunar Lander with 5 demonstrations to verify performance gap; 2) Train KIM with random vs. semantic initialization to quantify impact; 3) Introduce action noise during inference and plot degradation curve

## Open Questions the Paper Calls Out

- How can KIM support interactive and incremental integration of domain knowledge as experts refine instructions? The current method generates static structure in one shot.
- Can KIM effectively represent transition models in world models for sample-efficient model-based RL? The benefit of integrating general knowledge beyond policies is suggested but untested.
- How robust is KIM to incorrect or hallucinated domain knowledge? Experiments only evaluate with correct high-level knowledge.
- Does vanilla gradient descent scale to complex domains with highly non-linear structural connections? The current experiments use relatively simple structures.

## Limitations

- Generalization across domains remains unclear as the approach relies heavily on expert knowledge articulation quality
- State representation dependency for Car Racing requires specific tile-based abstraction not provided
- Code generation reliability is critical - entire approach hinges on LLM producing correct PyTorch code

## Confidence

- **High Confidence**: Sample efficiency improvements on evaluated tasks (quantitative results show KIM solves tasks with 5 demonstrations)
- **Medium Confidence**: Mechanism claims (theoretically sound but only demonstrated on two specific environments)
- **Low Confidence**: Generalizability claims (validation limited to two control tasks with curated expert knowledge)

## Next Checks

1. **Cross-domain transfer**: Apply KIM to a fundamentally different task (e.g., robotic manipulation) where expert knowledge structure differs significantly
2. **Knowledge articulation stress test**: Systematically vary quality of natural language descriptions and measure impact on performance
3. **LLM generation reliability audit**: Track success rate of code generation pipeline across multiple runs to quantify robustness to generation process