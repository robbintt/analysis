---
ver: rpa2
title: Generative Click-through Rate Prediction with Applications to Search Advertising
arxiv_id: '2507.11246'
source_url: https://arxiv.org/abs/2507.11246
tags:
- user
- prediction
- generative
- behavior
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GenCTR, a novel generative approach for enhancing
  click-through rate (CTR) prediction in search advertising. Unlike traditional discriminative
  models that directly predict CTR from user and item features, GenCTR leverages generative
  pre-training on user behavior sequences to capture richer user-item interactions.
---

# Generative Click-through Rate Prediction with Applications to Search Advertising

## Quick Facts
- arXiv ID: 2507.11246
- Source URL: https://arxiv.org/abs/2507.11246
- Reference count: 2
- Introduces GenCTR, a generative approach that improves CTR prediction with up to 0.04 AUC gains and shows 1.32% CTR and 1.66% RPM uplift in online A/B testing

## Executive Summary
This paper introduces GenCTR, a novel generative approach for enhancing click-through rate (CTR) prediction in search advertising. Unlike traditional discriminative models that directly predict CTR from user and item features, GenCTR leverages generative pre-training on user behavior sequences to capture richer user-item interactions. The method employs a two-stage training process: first, a conditional generative model pre-trains on next-item prediction using user behavior sequences and item category information; second, this pre-trained model is fine-tuned within a discriminative CTR prediction framework through parameter sharing and model integration.

The proposed approach is evaluated on a newly collected dataset (GCTR) from a large e-commerce platform, demonstrating significant improvements across three backbone models (DNN, DCN V2, DCN V2 & TA) in both AUC and LogLoss metrics. Specifically, GenCTR achieves up to 0.04 AUC improvement over baseline models. The method is further validated through successful deployment in a live search advertising system, showing 1.32% CTR and 1.66% RPM uplift in online A/B testing. The results confirm that generative pre-training on user behavior sequences effectively enhances CTR prediction accuracy in real-world applications.

## Method Summary
GenCTR is a two-stage generative framework for CTR prediction that first pre-trains a generative model on user behavior sequences and then fine-tunes it for discriminative CTR prediction. The method uses a transformer-based architecture with parameter sharing between the generative and discriminative components. The pre-training phase learns user preferences through next-item prediction, while the fine-tuning phase optimizes for CTR prediction through joint training of the shared parameters.

## Key Results
- GenCTR achieves up to 0.04 AUC improvement over baseline models on the GCTR dataset
- Successfully deployed in live search advertising system with 1.32% CTR and 1.66% RPM uplift in online A/B testing
- Demonstrates consistent improvements across three backbone models: DNN, DCN V2, and DCN V2 & TA

## Why This Works (Mechanism)
GenCTR leverages generative pre-training to capture richer user-item interactions by learning user preferences from historical behavior sequences. The method exploits the temporal and sequential nature of user behavior to build more comprehensive user representations, which are then transferred to improve CTR prediction through parameter sharing and model integration.

## Foundational Learning
- **CTR prediction fundamentals**: Understanding how user behavior translates to click probability; needed for baseline comparison and evaluation metrics
- **Transformer architectures**: Multi-head attention mechanisms for sequence modeling; essential for processing user behavior sequences
- **Generative pre-training**: Transfer learning from unsupervised sequence modeling to supervised prediction; provides the core innovation
- **Parameter sharing**: Joint optimization of generative and discriminative objectives; enables knowledge transfer between tasks
- **User behavior modeling**: Sequential pattern extraction from historical interactions; captures temporal dynamics

## Architecture Onboarding

**Component Map**
User behavior sequence → Transformer encoder → Generative pre-training → Parameter sharing → Discriminative CTR prediction

**Critical Path**
1. User behavior sequence input and item category information
2. Transformer encoder processing through multi-head attention
3. Generative pre-training with next-item prediction objective
4. Parameter sharing with discriminative CTR prediction module
5. Joint fine-tuning for final CTR prediction

**Design Tradeoffs**
- Computational overhead vs. performance gain from generative pre-training
- Sequence length vs. model complexity and training efficiency
- Category information granularity vs. generalization capability
- Parameter sharing extent vs. task-specific optimization

**Failure Signatures**
- Poor pre-training convergence indicates sequence quality or task formulation issues
- Overfitting during fine-tuning suggests insufficient regularization or data augmentation
- Limited improvement over baselines may indicate weak transfer learning signal

**First Experiments**
1. Baseline comparison without generative pre-training to establish baseline performance
2. Ablation study removing category information to assess its contribution
3. Varying sequence lengths to find optimal temporal context for user behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to proprietary e-commerce dataset, potentially limiting generalizability to other domains
- Computational overhead of two-stage training process not thoroughly discussed for practical deployment
- Privacy concerns related to extensive user behavior sequence usage not addressed

## Confidence

**High confidence in:**
- Technical feasibility of GenCTR framework and correctness of implemented methodology
- Clear mathematical formulations and implementable architectural descriptions

**Medium confidence in:**
- Magnitude of improvements (0.04 AUC) and their statistical significance across conditions
- Generalizability of results beyond e-commerce domain context

**Low confidence in:**
- Long-term stability and performance in production environments beyond limited A/B testing window

## Next Checks
1. Conduct cross-domain validation by testing GenCTR on datasets from different industries (e.g., news recommendation, streaming services) to assess generalizability beyond e-commerce

2. Perform extensive ablation studies varying pre-training objectives, sequence lengths, and category information usage to identify optimal configurations

3. Implement and measure computational overhead of generative pre-training phase in real-time serving environments, comparing against traditional discriminative models to quantify practical deployment costs