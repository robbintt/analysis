---
ver: rpa2
title: 'Causality Meets Locality: Provably Generalizable and Scalable Policy Learning
  for Networked Systems'
arxiv_id: '2510.21427'
source_url: https://arxiv.org/abs/2510.21427
tags:
- domain
- policy
- each
- state
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses scalability and generalizability challenges
  in large-scale networked multi-agent reinforcement learning (MARL) systems. The
  proposed GSAC framework combines causal representation learning with meta actor-critic
  learning to achieve both scalability and domain generalization.
---

# Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems

## Quick Facts
- arXiv ID: 2510.21427
- Source URL: https://arxiv.org/abs/2510.21427
- Reference count: 40
- Primary result: GSAC achieves rapid adaptation and significant performance gains on networked MARL benchmarks by combining causal representation learning with meta actor-critic learning

## Executive Summary
This paper addresses the fundamental challenges of scalability and generalizability in large-scale networked multi-agent reinforcement learning (MARL) systems. The authors propose the GSAC framework, which leverages causal representation learning to identify minimal state variables influencing each agent's dynamics and combines this with meta actor-critic learning to enable rapid adaptation across domains. The approach achieves both provable guarantees on approximation errors and significant empirical performance improvements on wireless communication and traffic control benchmarks.

## Method Summary
GSAC operates in four phases: (1) causal discovery to learn sparse local causal masks identifying relevant state variables for each agent, (2) construction of approximately compact representations (ACRs) that prune irrelevant variables while preserving decision-relevant information with bounded error, (3) meta actor-critic training across multiple source domains with a shared policy conditioned on compact domain factors, and (4) adaptation to new domains using few trajectories to estimate new domain factors and deploy the adapted policy. The framework combines theoretical guarantees on truncation errors, sample complexity, and adaptation gaps with practical algorithmic components.

## Key Results
- GSAC rapidly adapts to new domains using only a few trajectories, significantly outperforming learning-from-scratch and conventional adaptation baselines
- The framework achieves provable finite-sample convergence guarantees and adaptation gap bounds in networked MARL settings
- Empirical evaluation on wireless communication and traffic control benchmarks demonstrates superior performance in terms of average return and queue length metrics

## Why This Works (Mechanism)

### Mechanism 1
Learning sparse local causal masks enables provably correct identification of minimal state variables that influence each agent's dynamics and rewards. Each agent recursively traces backward from reward-influencing state components through causal edges in the interaction graph, yielding an ACR that removes irrelevant variables while preserving decision-relevant information with exponentially decaying approximation error (error ≤ 2r̄/(1−γ)·γ^(κ+1)). This works under faithfulness assumptions and bounded in-degree in the causal graph.

### Mechanism 2
Domain factors can be estimated from few trajectories and used to condition policies for rapid adaptation. After causal masks are identified, domain-specific parameters ω are estimated via maximum likelihood over observed transitions. The meta actor-critic learns a policy π(a|s,ω) across source domains; at test time, estimated ω̂ is plugged in without further training. This requires domain factors to lie in a compact space and induce distinguishable transition dynamics.

### Mechanism 3
Q-functions in networked systems decay exponentially with graph distance, enabling truncation to κ-hop neighborhoods with bounded error. The global Q-function decomposes into local contributions Q^π_i, and due to localized transition dynamics, influences from distant agents shrink as γ^(κ+1). Truncated Q̂^π_i uses only κ-hop state/actions, reducing dimensionality while maintaining error bound c·ρ^(κ+1).

## Foundational Learning

- **Concept:** Multi-Agent Reinforcement Learning (MARL) in networked systems
  - **Why needed here:** The entire framework assumes agents have localized neighborhoods N_i, joint policies factor as π(a|s)=∏_i π_i(a_i|s_Ni), and value functions decompose additively. Understanding this structure is prerequisite for all theoretical results.
  - **Quick check question:** Can you explain why the global Q-function Q^π(s,a) can be written as (1/n)∑_i Q^π_i(s,a) and what the exponential decay property means for truncation?

- **Concept:** Actor-Critic Methods with Policy Gradients
  - **Why needed here:** GSAC uses a meta actor-critic where critics estimate truncated Q-functions via TD learning and actors update via stochastic gradient ascent on estimated policy gradients. Understanding policy gradient ∇_θ log π_θ(a|s) and TD updates is essential.
  - **Quick check question:** Why does the actor update aggregate Q-values from all agents in the κ-hop neighborhood rather than just the local Q_i?

- **Concept:** Causal Discovery and Identifiability
  - **Why needed here:** Phase 1 recovers causal masks c_{·→·} from observational data. Understanding faithfulness, d-separation, and conditional independence tests explains when Theorem 1 guarantees unique recovery.
  - **Quick check question:** Under what conditions can causal direction be inferred from observational data, and why does the temporal structure (DBN) help resolve edge direction ambiguities?

## Architecture Onboarding

- **Component map:** Causal Discovery Module → ACR Constructor → Meta Actor-Critic → Domain Factor Estimator → Adaptation Deployer
- **Critical path:** Causal Discovery → ACR Construction → Meta-Training (critic TD updates → actor gradient aggregation) → Domain Factor Estimation (target) → Policy Deployment. The ACR dimensionality reduction is critical; without it, κ-hop input spaces remain large.
- **Design tradeoffs:**
  - Larger κ reduces truncation error (bound ~γ^(κ+1)) but increases input dimensionality and computational cost. ACR mitigates this by pruning within the κ-hop neighborhood.
  - More source domains M improves meta-training generalization (error term ~1/√M) but increases upfront data collection.
  - More adaptation trajectories T_a reduces domain estimation error but delays deployment.
- **Failure signatures:**
  1. High critic variance: May indicate ACR is missing relevant variables or κ is too small
  2. Slow/no actor convergence: Check if gradient aggregation correctly uses κ-hop neighbors; verify Lipschitz constants
  3. Adaptation failure: Domain factor estimation may be inaccurate if transitions are insufficiently distinct across domains
  4. Causal mask recovery errors: If faithfulness is violated or sample size insufficient, ACR construction will propagate errors
- **First 3 experiments:**
  1. Validate ACR approximation: On a small network (n=16), compare Q̃^π_i against full Q^π_i for varying κ. Plot error decay vs. theoretical bound 2r̄/(1−γ)·γ^(κ+1).
  2. Test domain factor estimation: Generate domains with varying ω separability. Measure estimation error ‖ω̂−ω*‖ vs. trajectory length T_e. Confirm O(1/√T_e) scaling.
  3. Meta-training convergence: Run GSAC on M=3 source domains, track policy gradient norm across outer iterations. Compare against SAC-MTL and SAC-FT baselines as in Figure 2.

## Open Questions the Paper Calls Out
1. Can the GSAC framework be effectively extended to continuous state and action spaces using function approximation?
2. How can the framework be adapted to handle partial observability within networked systems?
3. Does GSAC maintain its performance and scalability advantages on more diverse and larger-scale real-world networked systems?

## Limitations
- Theoretical guarantees critically depend on faithfulness assumptions in causal discovery and exponential decay of value function influences, which may not hold in systems with long-range dependencies
- The meta-learning framework assumes compact, well-separated domain factors—violations could impair adaptation
- Empirical validation is limited to two domains (wireless, traffic) with relatively small-scale problems (n≤25 agents)

## Confidence
- **High Confidence:** The ACR approximation error bounds and the exponential decay of Q-functions are mathematically rigorous given the stated assumptions. The meta-learning adaptation gap bounds are also well-established under the total variation separation condition.
- **Medium Confidence:** Causal mask identifiability and sample complexity are sound but depend on ideal faithfulness conditions that may not hold in practice. Domain factor estimation theory is valid but assumes perfect causal masks and sufficient trajectory diversity.
- **Low Confidence:** The practical performance benefits relative to strong baselines in complex real-world networks remain unproven. The algorithm's scalability to thousands of agents or systems with rich, continuous state spaces is unclear.

## Next Checks
1. Test causal mask recovery on synthetic networks where the true causal structure is known, varying noise levels and sample sizes to validate Proposition 4's sample complexity bound.
2. Systematically vary κ and measure the empirical truncation error |Q^π_i - Q̃^π_i| across multiple network topologies, comparing against the theoretical bound 2r̄/(1−γ)·γ^(κ+1).
3. Evaluate adaptation performance on target domains with increasingly subtle domain factor differences (reducing total variation distance), measuring the trade-off between adaptation trajectory length T_a and estimation accuracy ‖ω̂−ω*‖.