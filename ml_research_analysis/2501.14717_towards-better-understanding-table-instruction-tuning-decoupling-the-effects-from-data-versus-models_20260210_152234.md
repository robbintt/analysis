---
ver: rpa2
title: 'Towards Better Understanding Table Instruction Tuning: Decoupling the Effects
  from Data versus Models'
arxiv_id: '2501.14717'
source_url: https://arxiv.org/abs/2501.14717
tags:
- table
- data
- training
- performance
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the impact of training data and base models
  on table instruction tuning for Large Language Models (LLMs). Previous works trained
  different base models with different datasets, making it difficult to compare results.
---

# Towards Better Understanding Table Instruction Tuning: Decoupling the Effects from Data versus Models

## Quick Facts
- arXiv ID: 2501.14717
- Source URL: https://arxiv.org/abs/2501.14717
- Authors: Naihao Deng; Sheng Zhang; Henghui Zhu; Shuaichen Chang; Jiani Zhang; Alexander Hanbo Li; Chung-Wei Hang; Hideo Kobayashi; Yiqun Hu; Patrick Ng
- Reference count: 40
- Primary result: Decoupled effects of training data and base models in table instruction tuning, showing base model capability establishes performance ceiling

## Executive Summary
This paper investigates the separate contributions of training data and base models to table instruction tuning performance for Large Language Models. Previous works confounded these effects by training different base models on different datasets. The authors systematically fine-tune three base models (Mistral, OLMo, Phi) on four existing training datasets, establishing new state-of-the-art results on HiTab and demonstrating that training data with explicit reasoning chains outperforms larger but reasoning-poor datasets. Through out-of-domain evaluation, they reveal that base model capability establishes a performance ceiling that training data quality cannot fully compensate for, and that proper fine-tuning can preserve general capabilities without catastrophic forgetting.

## Method Summary
The authors fine-tune three base models (Mistral v0.3 7B Instruct, OLMo 7B Instruct, Phi 3 Small Instruct) on four existing public table instruction datasets: TableLlama (107K subset), TableLLM (80.5K), TableBench (20K), and Table-GPT (66K). Fine-tuning uses full parameter updates for 3 epochs with learning rate 5e-7. They evaluate on 8 real-world table datasets (FeTaQA, HiTab, TabMWP, TATQA, WikiTQ, TabFact, InfoTabs, ToTTo), 8 synthesized datasets, and 5 general benchmarks (MMLU, MMLUPro, AI2ARC, GPQA, IFEval). The key innovation is systematic out-of-domain evaluation within task categories to decouple data versus model effects.

## Key Results
- Training data with explicit reasoning chains (TableLLM) outperforms larger datasets without reasoning on out-of-domain tasks
- Base model capability establishes a performance ceiling that training data quality cannot fully compensate for
- Proper fine-tuning (3 epochs, lr 5e-7) preserves or improves general instruction-following capabilities without catastrophic forgetting
- Established new state-of-the-art on HiTab table question-answering dataset

## Why This Works (Mechanism)

### Mechanism 1: Reasoning-Enhanced Training Data Effectiveness
Training data that incorporates explicit reasoning chains improves out-of-domain table task performance more than data with surface-level task format matching. When training examples include reasoning explanations (e.g., "counting the number of entries"), models learn the underlying reasoning process rather than relying on domain-specific patterns or superficial labels. This reasoning capability transfers across table tasks even when the specific task format differs.

### Mechanism 2: Base Model Innate Capability Ceiling Effect
The base model's pre-existing capability level establishes a performance ceiling that training data quality cannot fully compensate for. Stronger base models (as measured by general benchmarks) maintain their relative advantage after fine-tuning on identical training data. The training data's effectiveness is contingent on the base model's ability to exploit it—weak base models cannot extract full value from high-quality data.

### Mechanism 3: Task Format Misalignment and Catastrophic Forgetting
Fine-tuning on task formats that differ from evaluation formats can cause performance degradation below the base model level, even within the same task category. Models may overfit to surface patterns (e.g., "entail"/"refute" labels) rather than learning the underlying verification skill. When evaluated on datasets with different formatting or domain distributions, the overfitted patterns fail catastrophically.

## Foundational Learning

- **Instruction Tuning vs. Fine-Tuning Distinction**: Understanding that instruction tuning shapes how models respond (format, reasoning style) while fine-tuning shapes what they know (domain knowledge) helps interpret why reasoning-enhanced data outperforms larger but reasoning-poor data. *Quick check*: Can you explain why the 107K reasoning-enhanced dataset outperformed the 2M dataset from TableLlama?

- **Out-of-Domain (OOD) vs. In-Domain Evaluation**: The paper's central contribution relies on OOD evaluation to decouple data and model effects. In-domain performance can be misleading—it may indicate pattern memorization rather than generalizable skill. *Quick check*: Why does strong TabFact performance not guarantee strong InfoTabs performance despite both being table fact verification tasks?

- **Base Model Pre-training Data Composition**: The three base models differ in their pre-training corpora. Phi's apparent advantage may stem from pre-training exposure to structured data or reasoning-heavy texts, not just architecture. *Quick check*: What factors beyond model size could explain Phi's stronger OOD performance?

## Architecture Onboarding

- **Component map**: Base Model Selection -> Training Data Selection -> Fine-tuning Configuration -> Evaluation Layer
- **Critical path**: 1) Evaluate candidate base models on target task category before fine-tuning, 2) Select training data with explicit reasoning chains for OOD generalization, 3) Fine-tune with learning rate 5e-7 for 3 epochs, 4) Evaluate on OOD datasets within same task category, 5) Check general benchmark degradation
- **Design tradeoffs**: Data volume vs. reasoning quality (107K reasoning-enhanced outperforms 2M pattern-heavy), In-domain vs. OOD optimization (TableLlama excels in-domain but TableLLM generalizes better OOD), Specialization vs. generalization (Mistral shows severe IFEval degradation while Phi maintains or improves)
- **Failure signatures**: Pattern overfitting (high in-domain but below-base-model OOD performance), Catastrophic instruction forgetting (>20 point drop on IFEval), Task format confusion (training on "entail"/"refute" fails on different label schemes), SOTA chasing trap (state-of-the-art on one dataset with poor same-category performance)
- **First 3 experiments**: 1) Base model capability audit: Run candidate base models on 2-3 datasets representing target task category before fine-tuning, 2) Reasoning enhancement ablation: Fine-tune on chosen data with and without reasoning chain augmentation, evaluate on 2 OOD datasets, 3) General capability preservation test: After fine-tuning, evaluate on IFEval and MMLU, if degradation exceeds 10 points experiment with reducing epochs or mixing general data

## Open Questions the Paper Calls Out

### Open Question 1
Why does achieving state-of-the-art performance on specific table benchmarks (e.g., TabFact) fail to guarantee generalization to other datasets within the same task category? The paper identifies the generalization gap but does not isolate the specific distributional shifts or annotation artifacts that cause models to rely on dataset-specific patterns rather than generalizable reasoning skills.

### Open Question 2
To what extent does the inclusion of explicit reasoning steps in training data drive performance improvements compared to mere task format alignment? While the correlation between reasoning-augmented data and performance is observed, the paper does not perform a controlled ablation to separate the impact of the reasoning chain from the impact of data diversity or volume.

### Open Question 3
What specific innate characteristics of a base model determine its robustness against catastrophic forgetting of general instruction-following capabilities during domain-specific fine-tuning? The paper establishes that the reaction to fine-tuning is model-dependent but does not identify the architectural or pre-training features that make a model resistant to forgetting.

## Limitations

- Limited comparison scope: Conclusions rest on three 7B models and four training datasets, which may not generalize to other model families or training regimes
- Hyperparameter uncertainty: Critical parameters like batch size, gradient accumulation, and exact prompt formatting remain unspecified, making exact replication challenging
- Inferred rather than proven mechanisms: The reasoning enhancement mechanism is inferred from observed performance patterns rather than direct causal analysis

## Confidence

- **High Confidence**: The empirical finding that training data with explicit reasoning chains outperforms larger datasets without reasoning (107K vs 2M) - supported by consistent cross-model performance patterns and OOD evaluation showing transfer capability
- **Medium Confidence**: The claim that base model innate capability establishes a performance ceiling - while correlation with general benchmarks is strong, pre-training data composition differences across models weren't controlled for
- **Low Confidence**: The catastrophic forgetting mechanism for task format misalignment - the paper shows performance degradation below base model levels, but doesn't systematically test whether reasoning-enhanced data mitigates this effect

## Next Checks

1. **Reasoning Chain Ablation Study**: Take a single base model and training dataset, create two versions - one with reasoning chains removed from examples and one with reasoning chains added to existing data. Compare OOD generalization across same-task-category datasets to isolate reasoning's causal effect.

2. **Base Model Ceiling Experiment**: Select two base models with similar general benchmark performance but different pre-training data characteristics. Fine-tune both on identical reasoning-enhanced data and evaluate on same OOD task suite to determine if performance differences persist independent of general capability.

3. **Format Transfer Robustness Test**: Train models on datasets with different label schemes (e.g., "entail"/"refute" vs "yes"/"no") within the same task category. Evaluate on datasets requiring format adaptation to measure whether reasoning-enhanced data reduces catastrophic forgetting of base model skills.