---
ver: rpa2
title: Conditions for Catastrophic Forgetting in Multilingual Translation
arxiv_id: '2510.19546'
source_url: https://arxiv.org/abs/2510.19546
tags:
- language
- forgetting
- unseen
- fine-tuning
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically investigates catastrophic forgetting
  in multilingual translation models. Through controlled experiments across various
  model architectures, data scales, and fine-tuning approaches, the authors find that
  the relative scale between model parameters and fine-tuning data volume is a primary
  determinant of forgetting.
---

# Conditions for Catastrophic Forgetting in Multilingual Translation

## Quick Facts
- arXiv ID: 2510.19546
- Source URL: https://arxiv.org/abs/2510.19546
- Reference count: 25
- This paper systematically investigates catastrophic forgetting in multilingual translation models, finding that the relative scale between model parameters and fine-tuning data volume is the primary determinant of forgetting.

## Executive Summary
This study systematically investigates catastrophic forgetting in multilingual translation models by examining how fine-tuning affects performance on unseen language pairs. Through controlled experiments across various model architectures, data scales, and fine-tuning approaches, the authors identify that the ratio of model capacity to fine-tuning data volume is the primary determinant of forgetting severity. They demonstrate that instruction-following ability is more critical for retaining multilingual knowledge than model architecture, and that cross-lingual alignment methods can mitigate forgetting while facilitating positive transfer to unseen target languages.

## Method Summary
The study fine-tunes five multilingual translation models (M2M-124-0.2B/0.6B and Qwen2.5-0.5B/7B-Instruct, Llama-3-8B-Instruct) on two dataset scales: SMALL (ALMA, 117K pairs covering cs/de/is/ru/zh↔en) and LARGE (WMT21, 54M pairs covering jv/ms/tl↔en). Fine-tuning uses full parameter updates and LoRA (rank=8, α=16) with batch sizes of 16,384 tokens for M2M-124 and 128 sentences for instruction-following models. Evaluation measures COMET-22 and BLEU scores on supervised pairs, unseen pairs (both languages seen but not together), unseen source (source language never seen), and unseen target (target language never seen) categories.

## Key Results
- The relative scale between model size and fine-tuning data volume is the primary determinant of forgetting severity
- Instruction-following ability, rather than model architecture, is crucial for retaining multilingual knowledge
- Cross-lingual alignment methods can mitigate forgetting and facilitate positive transfer, particularly for unseen language pairs
- LoRA offers no clear advantage over full fine-tuning in mitigating forgetting

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Catastrophic forgetting severity is determined by the ratio of model capacity to fine-tuning data volume, not model scale alone.
- **Mechanism:** Fine-tuning updates model parameters to minimize loss on the target task. When data volume is large relative to model capacity, gradient updates drive more significant parameter shifts, overwriting pre-trained multilingual representations. Larger models distribute knowledge across more parameters, allowing them to absorb new specialization without erasing existing capabilities.
- **Core assumption:** Pre-trained multilingual knowledge is distributed and redundant enough in larger models that partial parameter updates preserve it.
- **Evidence anchors:** [abstract] "The relative scale between model size and fine-tuning data volume is the primary determinant of forgetting." [section] Figure 4 shows progressive forgetting on Qwen2.5-0.5B-Instruct as fine-tuning data increases from 12K → 120K → 1.2M sentences per pair.

### Mechanism 2
- **Claim:** Instruction-following ability provides robust target-language control that resists forgetting, independent of architecture.
- **Mechanism:** Models trained with instruction-following use natural language prompts (e.g., "Translate to German") to specify output language. This leverages a generalizable capability. Token-based control (e.g., `<de>` token) is brittle: if a target language token is unseen during fine-tuning, the model loses the ability to interpret it correctly for generation.
- **Core assumption:** The instruction-following capability learned during pre-training is more robust to distribution shift than specialized token embeddings.
- **Evidence anchors:** [abstract] "Instruction-following ability, rather than model architecture... is crucial for retaining multilingual knowledge." [section] Table 3 shows Qwen2.5-0.5B-Instruct performance on unseen targets drops from 65.3 to 60.9 COMET when switching from instruction-based to token-based control.

### Mechanism 3
- **Claim:** Cross-lingual alignment mitigates forgetting on unseen language pairs by breaking spurious source-target language associations learned during English-centric fine-tuning.
- **Mechanism:** Fine-tuning on English-centric data creates a spurious correlation (Source X → Target English). Cross-lingual alignment (e.g., contrastive loss) encourages language-invariant representations, separating semantic content from language-specific features. This helps the model override the learned English bias and generate the correct target language for unseen pairs (X → Y).
- **Core assumption:** Forgetting on unseen pairs is primarily caused by overfitting to a dominant output language, not by complete erasure of translation ability.
- **Evidence anchors:** [abstract] "Cross-lingual alignment methods can mitigate forgetting and facilitate positive transfer, particularly for unseen language pairs..." [section] Figure 5 shows cross-lingual alignment on M2M-124-0.2B reverses a -7.5 COMET loss on unseen pairs into a +10 COMET gain.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The entire paper is structured around identifying the conditions that trigger this phenomenon. Understanding it is prerequisite to interpreting any results.
  - **Quick check question:** Can you explain why training on task B might cause a model to lose performance on task A, even if A and B are related?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - **Why needed here:** The paper explicitly investigates whether LoRA provides a forgetting mitigation advantage over full fine-tuning and finds it does not.
  - **Quick check question:** How does LoRA modify a model's weights differently than full fine-tuning, and why might one assume it would cause less forgetting?

- **Concept: Cross-Lingual Transfer & Zero-Shot Translation**
  - **Why needed here:** The study evaluates performance on "unseen" translation directions, which relies on the model's ability to generalize across languages.
  - **Quick check question:** What does it mean for a model to perform "zero-shot" translation, and how is it related to the "unseen pairs" in this paper?

## Architecture Onboarding

- **Component Map:** Base Models (M2M-124, Qwen2.5, Llama-3) → Fine-Tuning (Full, LoRA) → Language Control (Token-based, Instruction-based) → Alignment Methods (Adversarial, Similarity, Contrastive) → Evaluation (COMET-22, BLEU, Language ID)

- **Critical Path:**
  1. Select Base Model & Data Scale: Choose model size and fine-tuning dataset volume based on the desired trade-off between specialization and retention
  2. Apply Language Control: Use instruction-based prompts for decoder models; prepend language tokens for M2M-124
  3. Fine-Tune: Run full or LoRA fine-tuning on the chosen language pairs
  4. Evaluate on Unseen Directions: Test on unseen pairs, unseen source, and unseen target categories using COMET and BLEU
  5. Mitigate (if needed): If forgetting is observed on unseen pairs, apply cross-lingual alignment (e.g., contrastive loss) or try in-language instructions

- **Design Tradeoffs:**
  - Larger Model + Small Data: Maximizes retention but increases inference cost
  - Smaller Model + Large Data: Maximizes specialization but risks severe forgetting
  - Instruction-Following Model: Better for unseen target languages and generalization
  - Translation-Specific Model: May be more efficient for known pairs but is brittle to unseen target languages
  - LoRA vs. Full FT: No clear advantage for forgetting in this study

- **Failure Signatures:**
  - Near-Zero BLEU on Unseen Targets: Indicates catastrophic failure of target language control
  - Significant COMET Drop on Unseen Pairs: Indicates spurious correlation to English
  - High Language ID Mismatch: Model generates output in wrong language

- **First 3 Experiments:**
  1. Replicate the Data Scale Effect: Fine-tune Qwen2.5-7B-Instruct on SMALL and subsampled LARGE datasets, plot gain-forgetting curves
  2. Isolate Instruction vs. Token Control: Fine-tune Qwen2.5-0.5B using both instruction-based and token-based control, compare on unseen target languages
  3. Test Alignment on Unseen Pairs: Apply contrastive loss during fine-tuning, measure performance change specifically on unseen pair category

## Open Questions the Paper Calls Out

- **Question:** Do the identified drivers of catastrophic forgetting (relative scale, instruction-following ability) generalize to multilingual tasks beyond machine translation?
- **Question:** How does the English-centric nature of the fine-tuning data impact the generalizability of these forgetting dynamics to non-English pivot scenarios?
- **Question:** Do the dynamics of catastrophic forgetting change substantively as model parameters scale significantly beyond the 8B range tested?
- **Question:** Is the observed ineffectiveness of LoRA in mitigating forgetting strictly dependent on the magnitude of the domain shift between pre-training and fine-tuning?

## Limitations

- The study focuses on translation-specific and instruction-following models, leaving uncertainty about generalizability to other architectures
- Cross-lingual alignment experiments are evaluated on limited language pairs and may not generalize to low-resource or highly divergent language families
- The comparison between LoRA and full fine-tuning is based on specific hyperparameter choices that may not represent optimal configurations

## Confidence

**High Confidence:** The empirical observation that model-to-data scale ratio predicts forgetting severity is well-supported by systematic experiments across multiple model sizes and dataset volumes.

**Medium Confidence:** The mechanism explaining why cross-lingual alignment mitigates forgetting on unseen pairs is plausible but relies on indirect evidence.

**Low Confidence:** The claim that LoRA offers no advantage over full fine-tuning in mitigating forgetting is based on a single configuration and may not represent optimal LoRA configurations.

## Next Checks

1. **Scale Ratio Validation:** Replicate the forgetting patterns by fine-tuning a single instruction-following model on multiple dataset subsamples spanning different model-to-data ratios, confirming the hypothesized relationship holds across finer-grained data scales.

2. **Architecture Generalization Test:** Apply the same fine-tuning and evaluation protocol to an encoder-decoder model with language control tokens and to a decoder-only model without instruction-following capability, comparing forgetting patterns to determine if the instruction-following advantage is architecture-specific.

3. **Alignment Mechanism Dissection:** Design experiments to isolate whether cross-lingual alignment's forgetting mitigation comes from breaking spurious language correlations versus providing general regularization, testing alignment on both English-centric and non-English-centric fine-tuning data.