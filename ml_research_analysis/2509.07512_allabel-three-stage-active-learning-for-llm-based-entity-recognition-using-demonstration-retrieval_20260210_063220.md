---
ver: rpa2
title: 'ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using
  Demonstration Retrieval'
arxiv_id: '2509.07512'
source_url: https://arxiv.org/abs/2509.07512
tags:
- sampling
- allabel
- similarity
- entity
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ALLabel introduces a three-stage active learning framework for
  entity recognition with LLMs, addressing the high annotation cost in specialized
  domains. It sequentially applies diversity sampling to select representative examples,
  similarity sampling to retrieve examples relevant to all queries, and uncertainty
  sampling to target low-confidence predictions.
---

# ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval

## Quick Facts
- **arXiv ID:** 2509.07512
- **Source URL:** https://arxiv.org/abs/2509.07512
- **Reference count:** 29
- **Primary result:** Achieves F1 scores within 2% of full-annotation performance using only 5%-10% of the data

## Executive Summary
ALLabel introduces a three-stage active learning framework for entity recognition with LLMs, addressing the high annotation cost in specialized domains. It sequentially applies diversity sampling to select representative examples, similarity sampling to retrieve examples relevant to all queries, and uncertainty sampling to target low-confidence predictions. This approach constructs an optimized demonstration corpus for in-context learning. Experiments on three chemistry and materials science datasets show that ALLabel outperforms baselines, achieving F1 scores within 2% of full-annotation performance while using only 5%-10% of the data. The framework improves efficiency by focusing annotation on the most informative samples, reducing costs while maintaining high extraction accuracy.

## Method Summary
ALLabel employs a three-stage pipeline to select M samples for a retrieval corpus used in in-context learning. First, diversity sampling selects a "warm-start" core-set by choosing the sample with minimum average similarity as a seed, then greedily adding samples maximizing minimum distance to the selected set. Second, similarity sampling calculates a composite `sumrank` score for each sample based on its aggregate relevance to all other queries, selecting the top-ranked samples. Third, uncertainty sampling identifies "weak test points" where the current pool lacks close neighbors, then selects additional samples similar to these low-confidence queries. The framework uses BM25 for lexical similarity in main experiments, with 3-shot in-context learning for inference.

## Key Results
- Achieves F1 scores within 2% of full-annotation performance
- Requires only 5%-10% of the labeled data compared to traditional approaches
- Outperforms baselines on three chemistry and materials science datasets (CSD-MOFs, NC 2024 General, USPTO)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Initializing the annotation pool with a diverse "warm-start" core-set improves data coverage and reduces variance compared to random initialization.
- **Mechanism:** The algorithm selects the first sample by minimizing average similarity to all other samples (the least redundant point). It then iteratively selects points furthest from the currently selected subset. This forces the initial pool to geometrically cover the data distribution, acting as a surrogate for the full dataset.
- **Core assumption:** Text similarity metrics (e.g., BM25, SBERT) inversely correlate with information redundancy; low similarity implies high informational diversity.
- **Evidence anchors:** [abstract]: "diversity sampling to select representative examples"; [section]: Section 4.1 and Algorithm 1 describe the warm-start core-set selection.

### Mechanism 2
- **Claim:** Prioritizing samples that frequently appear as top-k neighbors for the entire unlabeled dataset (high "sumrank") maximizes the utility of the demonstration corpus.
- **Mechanism:** Instead of annotating samples that might only help specific queries, this mechanism scores samples based on their aggregate relevance to all potential queries. By selecting high-scoring samples, the system ensures the resulting retrieval corpus contains the "centroids" most likely to be retrieved as useful demonstrations during inference.
- **Core assumption:** In-context learning (ICL) performance improves when the demonstration is textually similar to the query; therefore, annotating the "most retrievable" samples yields the highest global performance gain.
- **Evidence anchors:** [abstract]: "similarity sampling to retrieve examples relevant to all queries"; [section]: Section 4.2 defines the `sumrank` heuristic (Equation 2) to quantify composite similarity.

### Mechanism 3
- **Claim:** Using low retrieval similarity as a proxy for high uncertainty allows the framework to target "weak" test points without requiring model fine-tuning.
- **Mechanism:** The framework identifies queries where the current pool lacks close neighbors (high rank index for top-1 similarity). It prioritizes annotating unlabeled samples similar to these "weak" queries. This effectively patches holes in the demonstration space where the LLM would otherwise hallucinate due to lack of context.
- **Core assumption:** There is a negative correlation between demonstration-query similarity and LLM prediction uncertainty (perplexity).
- **Evidence anchors:** [abstract]: "uncertainty sampling to target low-confidence predictions"; [section]: Section 4.3 details the "uncertainty-similarity sampling strategy." Appendix C provides experimental proof of the negative correlation between similarity and perplexity.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** ALLabel does not update model weights; it relies entirely on constructing a prompt with demonstrations to guide the LLM.
  - **Quick check question:** Can you explain why a retriever's quality impacts an LLM's performance more than the prompt template in this specific architecture?

- **Concept: Active Learning (AL) Loops**
  - **Why needed here:** The framework is essentially an AL strategy (Diversity → Similarity → Uncertainty) designed to minimize human labeling cost.
  - **Quick check question:** How does the "oracle" function in this paper differ from standard fine-tuning based active learning?

- **Concept: Sparse vs. Dense Retrieval (BM25 vs. SBERT)**
  - **Why needed here:** The paper evaluates both lexical (BM25) and semantic (SBERT) similarity for constructing the core-set and retrieval rankings.
  - **Quick check question:** Why might lexical overlap (BM25) outperform semantic embeddings in specialized domains like chemistry, as observed in the paper's results?

## Architecture Onboarding

- **Component map:** Input: Unlabeled Corpus (D) → Pre-processing: Compute N × N Similarity Matrix → Stage 1 (Diversity): Warm-start Core-set Selector (M/5 samples) → Stage 2 (Similarity): SumRank Calculator (3M/5 samples) → Stage 3 (Uncertainty): Weak-point Analyzer + Similarity Filter (M/5 samples) → Output: Optimized Retrieval Corpus (D_selected) → Inference: LLM + Retriever (k-shots from D_selected)

- **Critical path:** The computation of the initial Similarity Matrix is the structural bottleneck. If this matrix misrepresents semantic relationships, all three downstream stages (Diversity, Similarity, Uncertainty) will select sub-optimal samples.

- **Design tradeoffs:**
  - **Ratio (1:3:1):** The paper allocates the majority of budget (3M/5) to Similarity, arguing it is the strongest driver of performance (Section 6.3 Ablation). Reducing this ratio to favor Diversity typically lowers F1.
  - **BM25 vs. SBERT:** The paper chooses BM25 for main experiments. While SBERT captures semantics, BM25 was found to be more robust or effective for these specific scientific datasets (Table 6).

- **Failure signatures:**
  - **Cold Start Variance:** If random seed selection is used instead of the "least average similarity" warm start, performance varies significantly (Std Dev increases, Table 8).
  - **Entity Type Conflict:** Increasing pool size can sometimes decrease F1 on specific entity types (Section 6.2) if new demonstrations introduce noise or conflict for specific labels.

- **First 3 experiments:**
  1. **Validation of Warm Start:** Replicate the comparison between "Cold Start" (random seed) and "Warm Start" (Algorithm 1) on a small subset (e.g., 50 samples) to verify stability.
  2. **Ablation on Stages:** Run the pipeline removing Stage 3 (Uncertainty) to verify if the performance drop matches the paper's ablation study (Table 4), confirming the value of the final "patching" stage.
  3. **Retriever Sensitivity:** Swap BM25 for a dense embedding model (e.g., E5 or OpenAI embeddings) to test if the 1:3:1 ratio holds true for semantic similarity, or if it overfits to the lexical characteristics of the Chemistry/Material datasets.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does measuring the annotation budget based on token-level charges, rather than sample count, alter the optimization trajectory and cost-efficiency of the ALLabel framework?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that "using the number of samples as an estimate of the annotation budget may be somewhat rough" and propose measuring the budget "more precisely in terms of token-level charges" in future work.
  - **Why unresolved:** The current implementation assumes a uniform annotation cost per sample, ignoring variations in text length and extraction complexity that directly affect LLM API costs.
  - **What evidence would resolve it:** A re-evaluation of the three-stage sampling strategy using a financial budget constraint (e.g., $50 USD) instead of a sample limit (M), comparing performance curves against the sample-based approach.

- **Open Question 2:** To what extent does the performance of ALLabel generalize to a wider range of Large Language Models, particularly smaller, open-source models with different reasoning capabilities?
  - **Basis in paper:** [explicit] The authors acknowledge, "due to cost and time constraints, we were only able to conduct experiments with GPT-4o and DeepSeek-V3," and list exploring "more advanced models" as a future direction.
  - **Why unresolved:** The framework relies heavily on the ICL capabilities of specific high-performing models; it is untested whether the same three-stage active learning strategies effectively guide smaller models or those with different failure modes.
  - **What evidence would resolve it:** Experimental results applying ALLabel to diverse model architectures (e.g., Llama 3, Mistral) on the same datasets to verify if the 5%-10% data efficiency gain holds constant.

- **Open Question 3:** Can the demonstration retrieval strategy be adapted to prevent the non-monotonic performance drops (negative impact) observed for specific entity types when the demonstration pool size increases?
  - **Basis in paper:** [explicit] The analysis notes that "expanding the demonstration pool inevitably results in retrieving examples that benefit the extraction for some entity types, while negatively impacting others," causing F1 scores to fluctuate or drop within certain ranges.
  - **Why unresolved:** The current diversity and similarity sampling heuristics optimize for global performance but do not account for "noise" introduced for specific entity classes during pool expansion.
  - **What evidence would resolve it:** A modified sampling strategy that monitors per-entity-type performance or constraints, demonstrating a strictly monotonic (or non-decreasing) improvement in macro-F1 as the budget M increases.

## Limitations

- The framework's effectiveness depends critically on the quality of the initial similarity matrix; poor embedding representations in scientific text can compromise all downstream sampling stages.
- The approach may struggle with datasets containing highly diverse entity types or inconsistent annotation guidelines, as increasing pool size can decrease F1 on specific entity types due to conflicting patterns.
- The "weak test point" identification assumes a strong correlation between retrieval similarity and prediction uncertainty, which may not hold for other entity types or when using different LLMs with varying confidence calibration.

## Confidence

**High Confidence:** The experimental results showing ALLabel's 2% F1 gap to full-annotation performance with only 5-10% of data are well-supported by the three chemistry/materials science datasets. The ablation studies clearly demonstrate the importance of each stage, particularly the similarity sampling stage.

**Medium Confidence:** The mechanism explanations for why diversity sampling improves coverage and why composite similarity scoring maximizes demonstration utility are logically sound but rely on assumptions about embedding space properties that may not generalize beyond the tested domains.

**Low Confidence:** The framework's behavior on datasets with heavy class imbalance, highly ambiguous entity boundaries, or significantly different text characteristics (e.g., clinical text vs. chemistry) remains unknown. The cold start variance reduction is demonstrated but the underlying mechanism (why "least average similarity" seed is superior) is not fully explained.

## Next Checks

1. **Cross-Domain Generalization Test:** Apply ALLabel to a non-chemistry dataset (e.g., biomedical or legal text) to verify if the 1:3:1 ratio and BM25 preference hold true when text characteristics differ significantly from the original datasets.

2. **Edge Case Robustness Analysis:** Systematically inject ambiguous or rare entity examples into the test sets to determine whether the uncertainty sampling stage correctly identifies and prioritizes truly difficult examples versus simply low-similarity ones.

3. **Prompt Template Sensitivity:** Test the framework with alternative prompt structures (different shot counts, template variations) to determine whether the ALLabel sampling strategy is robust to prompt engineering changes or if it overfits to the specific prompt design used in the experiments.