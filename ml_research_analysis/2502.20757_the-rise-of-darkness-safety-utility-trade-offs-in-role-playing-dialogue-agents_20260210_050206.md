---
ver: rpa2
title: 'The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents'
arxiv_id: '2502.20757'
source_url: https://arxiv.org/abs/2502.20757
tags:
- safety
- utility
- admp
- character
- villain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the safety-utility trade-off in role-playing
  dialogue agents, particularly when villain characters generate unsafe content. The
  authors propose Adaptive Dynamic Multi-Preference (ADMP) to dynamically adjust safety
  and utility preferences based on risk coupling between user queries and character
  settings.
---

# The Rise of Darkness: Safety-Utility Trade-Offs in Role-Playing Dialogue Agents

## Quick Facts
- arXiv ID: 2502.20757
- Source URL: https://arxiv.org/abs/2502.20757
- Reference count: 38
- Key outcome: ADMP+CMS improves safety metrics while maintaining utility in role-playing agents

## Executive Summary
This paper addresses the safety-utility trade-off in role-playing dialogue agents, particularly when villain characters generate unsafe content. The authors propose Adaptive Dynamic Multi-Preference (ADMP) to dynamically adjust safety and utility preferences based on risk coupling between user queries and character settings. They also introduce Coupling Margin Sampling (CMS) to enhance handling of high-risk scenarios. Experiments show ADMP improves safety metrics while maintaining utility, with ADMP+CMS achieving safety scores of 0.649 (utility 0.808) on LLaMA-3-8B compared to SFT's 0.550 safety and 0.737 utility. The method successfully balances character portrayal with content safety in role-playing agents.

## Method Summary
The authors develop ADMP, which dynamically generates explicit safety and utility preference scores before response generation, allowing the model to adjust its output based on risk coupling between villain character settings and user queries. They also introduce CMS, which oversamples high-risk scenarios by computing semantic similarity to a Typical Interaction Library and applying rejection sampling with adjustable thresholds. The method is trained on 522K samples from RoleBench (95 characters) and validated on villain-related subsets.

## Key Results
- ADMP+CMS achieves safety score 0.649 vs SFT's 0.550 on LLaMA-3-8B
- ADMP maintains utility (0.808) while improving safety compared to SFT (0.737 utility)
- Single CMS iteration provides significant safety improvement; diminishing returns with additional iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk coupling between villain character settings and user queries triggers unsafe responses.
- Mechanism: When user queries semantically align with villain backstory (e.g., Joker + "chaos"), the model's role-playing objective overrides safety constraints, producing authentic but harmful content.
- Core assumption: Villain characters have inherent harmful tendencies embedded in their narrative backgrounds that activate via specific query patterns.
- Evidence anchors:
  - [abstract] "risk scenarios created by villain characters and user queries (referred to as risk coupling) contribute to this trade-off"
  - [section 3.2.3, Figure 2c] As villain dialogue ratio increases from 0% to 50%, safety metrics (OFF, UB) decline while utility metrics improve
  - [corpus] Weak direct support; PsyPlay mentions personality-infused role-playing but not safety coupling specifically
- Break condition: If queries are semantically unrelated to villain backgrounds, risk coupling remains low and safety-utility trade-off diminishes.

### Mechanism 2
- Claim: Explicit preference generation before response enables dynamic safety-utility adjustment.
- Mechanism: ADMP forces the model to first generate safety (Rs) and utility (Ru) preference scores, conditioning subsequent response generation on these explicit signals rather than implicit, static alignment.
- Core assumption: The model can learn to accurately predict appropriate preference weights given character-query context.
- Evidence anchors:
  - [abstract] "guides the model to generate responses biased toward utility or safety"
  - [section 4.1] Training data format: "### Preference: <Utility: Ru> <Safety: Rs> ### Response: {output}"
  - [section 5.4, Figure 5] Positive correlation between generated and actual reward scores, stronger for safety than utility
- Break condition: If preference prediction is inaccurate, responses may be over-safe (utility loss) or under-safe (risk exposure).

### Mechanism 3
- Claim: Coupling Margin Sampling improves high-risk scenario handling by oversampling edge cases.
- Mechanism: CMS computes a coupling degree G(r,x) via semantic similarity to a Typical Interaction Library, then samples higher safety weights for high-G scenarios using a sigmoid-weighted distribution.
- Core assumption: High-risk scenarios are underrepresented in original training data and can be identified via semantic similarity.
- Evidence anchors:
  - [abstract] "introduce Coupling Margin Sampling (CMS) into coupling detection to enhance the model's ability to handle high-risk scenarios"
  - [section 4.3.2] Weight sampling: ws ~ N(μ,σ) where μ increases with G, σ decreases with G
  - [section 5.5, Figure 6] Single CMS iteration improves safety significantly; diminishing returns with additional iterations
- Break condition: If TIL construction is incomplete or similarity computation fails, high-risk cases may not be adequately sampled.

## Foundational Learning

- Concept: **Reward Modeling for Multi-Objective Alignment**
  - Why needed here: ADMP requires separate safety