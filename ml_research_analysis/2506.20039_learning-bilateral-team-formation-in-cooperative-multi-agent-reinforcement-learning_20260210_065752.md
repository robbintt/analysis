---
ver: rpa2
title: Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement
  Learning
arxiv_id: '2506.20039'
source_url: https://arxiv.org/abs/2506.20039
tags:
- learning
- agents
- matching
- team
- formation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses team formation in cooperative multi-agent
  reinforcement learning (MARL) with dynamic populations. The authors propose a bilateral
  matching framework where two disjoint sets of agents (leaders and followers) form
  teams using attention-based value decomposition methods.
---

# Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.20039
- Source URL: https://arxiv.org/abs/2506.20039
- Authors: Koorosh Moslemi; Chi-Guhn Lee
- Reference count: 4
- Key outcome: OOM (stable matching) outperforms SOM (unstable matching) in 26/27 generalization configurations to unseen agent compositions

## Executive Summary
This paper addresses team formation in cooperative multi-agent reinforcement learning with dynamic populations by proposing a bilateral matching framework between leaders and followers. The authors introduce Order Oriented Matching (OOM), a stable matching algorithm, and Score Oriented Matching (SOM), an unstable alternative, to form teams using attention-based value decomposition methods. The key innovation lies in modifying standard QMIX architectures with encoder-decoder networks and group-aware hypernetworks to incorporate team structure information, enabling the system to handle dynamic agent populations while learning bilateral team formation.

## Method Summary
The method builds on REFIL's entity-wise attention architecture, extending it with a bilateral team formation mechanism. Agents are divided into leaders and followers, with team formation determined by attention-based preference learning combined with matching algorithms. The architecture uses an encoder-decoder structure where a group-aware encoder produces embeddings that are similar within teams and diverse across teams, while a decoder generates network weights conditioned on group membership. Two matching algorithms are compared: OOM uses stable deferred acceptance (Gale-Shapley) while SOM uses greedy mutual score selection. The system is trained with a combined loss including TD loss, auxiliary factorization loss, and similarity-diversity loss to enforce team structure.

## Key Results
- OOM consistently outperforms SOM in generalization to unseen agent compositions across 26 out of 27 evaluation configurations
- Both methods show comparable performance during training on seen compositions
- The stability property of OOM appears to enable better generalization to new team structures
- The framework successfully handles dynamic agent populations in customized SMAC scenarios

## Why This Works (Mechanism)

### Mechanism 1: Stable Matching Improves Generalization to Unseen Agent Compositions
- Claim: Stable matching algorithms (OOM) produce policies that generalize better to unseen agent compositions than unstable alternatives (SOM).
- Mechanism: The deferred acceptance mechanism in OOM ensures no agent has incentive to deviate from the final matching. This stability appears to reduce inefficient policy switching caused by frequent group changes during training, leading to more robust learned preferences that transfer to new compositions.
- Core assumption: Stability during training enables more consistent credit assignment across episodes, which the paper hypothesizes but does not directly measure.
- Evidence anchors:
  - [abstract] "OOM consistently outperforms SOM in generalization to unseen agent compositions across 26 out of 27 evaluation configurations"
  - [section 6] "We hypothesize that the superior generalization capability of OOM stems from its stability property"
  - [corpus] No direct corpus support; neighboring papers focus on coordination and team formation but not matching stability
- Break condition: If the task requires rapid adaptation where instability helps explore new team configurations, OOM's stability may limit responsiveness.

### Mechanism 2: Attention Scores Encode Inter-Agent Preferences for Team Formation
- Claim: Multi-head attention weights can be interpreted as learned bilateral preferences between agents for team formation.
- Mechanism: The attention module computes pairwise attention scores between all agents. These scores are aggregated (via max pooling across heads) to form a preference matrix. Leaders and followers then use this matrix within matching algorithms to form teams.
- Core assumption: Attention weights capture task-relevant compatibility between agents; this depends on the attention mechanism receiving sufficient training signal.
- Evidence anchors:
  - [section 4.1] "We use attention weights returned by MHA(...) as preferences among agents for team formation"
  - [section 4.3] "The inter-agent preference matrix is learned as part of our learning framework by the attention module"
  - [corpus] Weak support; neighboring papers use attention for coordination but not explicitly for preference learning
- Break condition: If attention is distracted by irrelevant context (the paper's "distracted attention issue"), preference quality degrades.

### Mechanism 3: Group-Aware Encoder-Decoder Enforces Team Structure in Value Functions
- Claim: An encoder trained with similarity-diversity loss enables group-aware value decomposition that scales to dynamic agent populations.
- Mechanism: The encoder maps agent hidden states to embeddings where same-group agents are similar and different-group agents are dissimilar (via cosine similarity loss). These embeddings condition both the agent utility network and the hypernetwork, allowing value factorization to respect team structure. The decoder generates network weights conditioned on group membership.
- Core assumption: The maximum number of agents is known during training; generalization assumes similar group structures at test time.
- Evidence anchors:
  - [section 4.1] "The group-aware encoder embeds agents' hidden states such that embeddings corresponding to agents within a group are similar"
  - [section 4.4] "LSD ensures the hidden state embeddings of agents within the same group to be similar... [while] encouraging diversity between agents from different groups"
  - [corpus] GoMARL (Zang et al., 2024) is cited in-paper for similar grouping ideas but for fixed populations
- Break condition: If group assignments change frequently mid-episode, encoder embeddings may not update fast enough to reflect new structure.

## Foundational Learning

- Concept: **Value Decomposition (QMIX)**
  - Why needed here: The paper builds directly on QMIX-style value factorization; understanding monotonic mixing networks is prerequisite to understanding the architectural modifications.
  - Quick check question: Can you explain why monotonicity in the mixing function enables decentralized greedy action selection?

- Concept: **Deferred Acceptance (Gale-Shapley) Algorithm**
  - Why needed here: OOM is a many-to-one variant of stable matching using deferred acceptance; understanding proposal/acceptance cycles is essential to distinguish OOM from SOM.
  - Quick check question: In a many-to-one matching with 2 leaders and 4 followers, trace one complete proposal round if Leader A prefers F1>F2 and Leader B prefers F2>F1.

- Concept: **Entity-wise Dec-POMDP with Attention**
  - Why needed here: The paper uses REFIL's entity-wise formulation with multi-head attention to handle variable agent counts; understanding how masks control observability is critical.
  - Quick check question: How does an entity-wise representation differ from a fixed-size agent representation when the number of agents changes at test time?

## Architecture Onboarding

- Component map:
  Agent observations -> MHA module -> Encoder fe -> Group assignment (matching) -> Decoder -> Q-values -> Mixing network (group-aware) -> Qtot

- Critical path: Entity observations → attention embeddings → encoder → [group assignment via matching] → decoder → Q-values → mixing network (conditioned on group states) → Qtot → loss computation. The matching output determines which agents share similar encoder embeddings.

- Design tradeoffs:
  - OOM vs SOM: OOM uses only preference order (more stable), SOM uses raw scores (potentially more expressive but unstable)
  - Number of leaders |L|: Paper uses |L|=2 during training; evaluation varies |L|∈[2,4]. No principled selection method provided.
  - Encoder loss weight λ: Paper uses default λ=0.5 without tuning; tradeoff between TD learning and group structure enforcement

- Failure signatures:
  - Attention collapse to uniform preferences → matching becomes random → check attention score variance
  - All agents assigned to one group → encoder diversity loss too weak or encoder capacity insufficient
  - Generalization drops sharply with more agents than training max → entity representation not generalizing; check MHA mask handling

- First 3 experiments:
  1. **Validate matching mechanism in isolation**: Fix pretrained REFIL backbone, swap OOM↔SOM, compare generalization. This isolates the matching algorithm contribution.
  2. **Ablate encoder-decoder**: Train with encoder but freeze embeddings (no LSD loss), compare to full system. This tests whether learned group structure matters vs. architectural change alone.
  3. **Stress test population scaling**: Train with |A|∈[3,5], evaluate at |A|∈[6,8] with varying |L|. Plot win rate degradation curves for OOM vs SOM to quantify generalization gap magnitude.

## Open Questions the Paper Calls Out

- **Open Question 1**: Is Order Oriented Matching (OOM) more robust to the "distracted attention" phenomenon than Score Oriented Matching (SOM)?
  - Basis in paper: [explicit] The authors suggest investigating if OOM's reliance on preference order offers better robustness to irrelevant context than SOM's reliance on raw scores.
  - Why unresolved: The experiments used standard SMAC scenarios without specifically testing for attention distraction caused by increased sight ranges or irrelevant entities.
  - What evidence would resolve it: Comparative evaluation in environments specifically designed to induce distracted attention, measuring the performance drop of OOM relative to SOM.

- **Open Question 2**: Can a principled approach be developed to automatically tune the number of leaders ($|L|$) and select specific leader agents?
  - Basis in paper: [explicit] The paper states that "developing a principled approach for tuning $|L|$ and selecting leaders... remains an open direction."
  - Why unresolved: The current methodology relies on manual configuration and fixed heuristics (designating the first agents as leaders) rather than adaptive selection.
  - What evidence would resolve it: A learned or heuristic-based algorithm that dynamically determines optimal team structures based on agent capabilities or state context.

- **Open Question 3**: Does incorporating differential attention mechanisms change the comparative generalization performance between stable (OOM) and unstable (SOM) matching?
  - Basis in paper: [explicit] The authors propose future research to "investigate the comparative performance of OOM and SOM by incorporating a more robust attention mechanism, such as differential attention."
  - Why unresolved: The study relies on standard multi-head attention; the interaction between advanced attention mechanisms and matching algorithm stability is unexplored.
  - What evidence would resolve it: Experiments replacing standard attention with differential attention to observe if the generalization advantage of OOM persists or narrows.

## Limitations
- The attention mechanism's ability to capture meaningful preferences is assumed rather than empirically validated through correlation analysis with actual agent compatibility.
- Hyperparameters governing the balance between TD loss and group structure loss (λ=0.5) were not tuned, suggesting potential for improved performance.
- The analysis does not directly measure whether stability properties during training translate to better credit assignment or more consistent policy learning.

## Confidence
- **High Confidence**: The experimental result that OOM outperforms SOM in 26/26 generalization configurations is well-supported by the data.
- **Medium Confidence**: The hypothesis that stability during training enables better generalization is plausible but not directly proven.
- **Medium Confidence**: The attention-based preference learning mechanism works as described, but its quality is not independently validated.

## Next Checks
1. **Direct stability analysis**: During training, measure the frequency of matching changes per episode for OOM vs SOM. Correlate these frequencies with performance degradation to establish the mechanism linking stability to generalization.
2. **Preference quality validation**: After training, compute the correlation between attention scores and actual agent performance when paired together. This would validate whether the attention mechanism is learning meaningful preferences rather than arbitrary patterns.
3. **Population scaling stress test**: Train with 3-5 agents and systematically evaluate at 6-10 agents with varying leader counts. Plot win rate degradation curves to quantify the exact generalization gap and determine if OOM's advantage scales with population size.