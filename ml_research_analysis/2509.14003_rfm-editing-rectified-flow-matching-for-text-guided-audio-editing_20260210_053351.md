---
ver: rpa2
title: 'RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing'
arxiv_id: '2509.14003'
source_url: https://arxiv.org/abs/2509.14003
tags:
- audio
- editing
- diffusion
- rfm-editing
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RFM-Editing, a novel rectified flow matching-based
  diffusion framework for text-guided audio editing. The method addresses the challenge
  of modifying specific audio content while preserving the rest, without requiring
  auxiliary captions or masks.
---

# RFM-Editing: Rectified Flow Matching for Text-guided Audio Editing

## Quick Facts
- arXiv ID: 2509.14003
- Source URL: https://arxiv.org/abs/2509.14003
- Authors: Liting Gao; Yi Yuan; Yaru Chen; Yuelan Cheng; Zhenbo Li; Juan Wen; Shubin Zhang; Wenwu Wang
- Reference count: 0
- Primary result: RFM-Editing achieves faithful semantic alignment with target captions (CLAP score: 0.4250) while maintaining competitive audio quality

## Executive Summary
RFM-Editing presents a novel diffusion framework for text-guided audio editing using rectified flow matching. The method enables precise modification of specific audio content while preserving the rest, eliminating the need for auxiliary captions or masks. By leveraging a LoRA-tuned text encoder for instruction understanding and a UNet for velocity field prediction, the model achieves strong performance on constructed datasets of overlapping multi-event audio.

## Method Summary
RFM-Editing introduces a diffusion-based approach for text-guided audio editing that addresses the challenge of modifying specific audio content while maintaining the rest. The framework uses rectified flow matching to learn a transformation from source audio to target audio conditioned on text instructions. A LoRA-tuned text encoder interprets editing instructions, while a UNet predicts velocity fields that guide the audio transformation process. The method operates without requiring auxiliary captions or masks, streamlining the editing workflow.

## Key Results
- Achieves faithful semantic alignment with target captions (CLAP score: 0.4250)
- Maintains competitive audio quality across multiple evaluation metrics
- Outperforms existing approaches in distributional consistency
- Eliminates the need for costly inference-time optimization

## Why This Works (Mechanism)
The framework works by learning a probability flow that maps source audio distributions to target audio distributions conditioned on text instructions. The rectified flow matching approach ensures that the learned transformation preserves the essential characteristics of the source audio while incorporating the semantic modifications specified in the text. The LoRA-tuned text encoder provides efficient instruction understanding without requiring full model fine-tuning, while the UNet architecture enables effective velocity field prediction for the audio transformation.

## Foundational Learning
1. **Diffusion Models** - Why needed: Forms the core framework for gradual audio transformation
   Quick check: Understand how noise is gradually added and removed in the process

2. **Rectified Flow Matching** - Why needed: Enables stable training and accurate audio-to-audio transformation
   Quick check: Verify the flow matching equations and their implementation

3. **LoRA Fine-tuning** - Why needed: Efficiently adapts text encoders to understand editing instructions
   Quick check: Confirm that LoRA parameters are properly initialized and updated

4. **CLAP Score** - Why needed: Provides semantic alignment evaluation between edited and target audio
   Quick check: Understand how the CLAP model computes semantic similarity

5. **UNet Architecture** - Why needed: Enables effective velocity field prediction for audio transformation
   Quick check: Verify the receptive field and temporal modeling capabilities

6. **Velocity Fields** - Why needed: Guide the continuous transformation from source to target audio
   Quick check: Understand how velocity fields are computed and applied

## Architecture Onboarding

Component Map: Text Encoder -> LoRA Adapter -> UNet -> Velocity Field -> Audio Output

Critical Path: Text instruction → LoRA-tuned encoding → UNet processing → Velocity field prediction → Audio transformation

Design Tradeoffs:
- **Speed vs Quality**: LoRA tuning enables faster instruction adaptation but may limit fine-grained semantic understanding
- **Complexity vs Performance**: The rectified flow approach adds computational overhead but provides more stable training compared to standard diffusion
- **General