---
ver: rpa2
title: 'Uncovering Bugs in Formal Explainers: A Case Study with PyXAI'
arxiv_id: '2511.03169'
source_url: https://arxiv.org/abs/2511.03169
tags:
- pyxai
- formal
- case
- wcxp
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a validation methodology for formal explainers
  in explainable AI, focusing on validating explanations computed by untrusted explainers
  against other untrusted explainers and the ML model itself. The approach uses three
  explainers: a target explainer T (PyXAI) to validate, a reference explainer R (RFxpl)
  that provides proofs and witnesses, and a second explainer S (XReason) for independent
  verification.'
---

# Uncovering Bugs in Formal Explainers: A Case Study with PyXAI

## Quick Facts
- **arXiv ID:** 2511.03169
- **Source URL:** https://arxiv.org/abs/2511.03169
- **Reference count:** 40
- **Primary result:** Validation framework uncovers significant bugs in PyXAI explainer for abductive and contrastive explanations in random forests and boosted trees

## Executive Summary
This paper presents a validation methodology for formal explainers in explainable AI, focusing on validating explanations computed by untrusted explainers against other untrusted explainers and the ML model itself. The approach uses three explainers: a target explainer T (PyXAI) to validate, a reference explainer R (RFxpl) that provides proofs and witnesses, and a second explainer S (XReason) for independent verification. Experiments on 27 binary classification datasets revealed significant bugs in PyXAI: for random forests, 0-2.5% of abductive explanations were incorrect (not even weak abductive explanations) and 37.6-100% were redundant; for contrastive explanations, 0-2.1% were incorrect and 41.8-100% were redundant.

## Method Summary
The methodology validates explanations by cross-checking between three components: a target explainer T under test, a reference explainer R that can produce proofs and witnesses, and a second explainer S for independent verification. The framework checks whether computed explanations satisfy formal definitions by examining witnesses and proof traces. For random forests and boosted trees, explanations are validated by checking if they satisfy abductive/contrasive explanation definitions, verifying minimality through subset testing, and resolving disagreements through witness validation against the ground-truth ML model. The approach uses SAT and SMT encodings for different explainers to ensure independent verification paths.

## Key Results
- PyXAI produced 0-2.5% incorrect abductive explanations (not even weak abductive explanations) for random forests
- PyXAI produced 37.6-100% redundant abductive explanations for random forests
- PyXAI produced 0-2.1% incorrect contrastive explanations and 41.8-100% redundant contrastive explanations for random forests
- Similar error patterns observed for boosted trees

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Disagreements between the target explainer (T) and the reference explainer (R) can be definitively resolved by validating specific feature-space points (witnesses) against the original ML model (M).
- **Mechanism:** When T claims a set X is an explanation, the validation framework queries R. If R disagrees, it produces a witness—a concrete feature vector intended to disprove the explanation. The framework queries the ground-truth model M with this vector. If M confirms the prediction changes, T is proven incorrect; if M rejects the witness, R is flagged as buggy.
- **Core assumption:** The ML model M is the sole ground truth for predictions, and the witness provided by R is a feasible point in the feature space consistent with the definition of the explanation type.
- **Evidence anchors:** [Section 3.2] "If the ML model M confirms that κ(w) ≠ c, then we report that T produces an incorrect answer... (case A2a)." [Section 3.3] "To validate whether Y is a WCXp, we check whether the witness produced by the explainer T is confirmed by the ML model M."

### Mechanism 2
- **Claim:** Confidence in validation is significantly increased by using a second explainer (S) that relies on a fundamentally different logic encoding (e.g., SMT) than the reference (R, e.g., SAT).
- **Mechanism:** To avoid "garbage in, garbage out" where two buggy tools agree, the framework employs a second explainer S with an independent codebase and encoding (SMT vs. propositional logic). If T and R disagree, S acts as a tie-breaker. Agreement between R and S on a property strongly implies T is buggy.
- **Core assumption:** Logic encodings (SAT and SMT) are mathematically equivalent representations of the ML model M; an error is far more likely in the implementation of T than in a simultaneous but independent failure of both R and S.
- **Evidence anchors:** [Section 3] "To increase trust, the logic encoding used in S will be based on SMT, whereas the encoding used in R will be based on propositional logic..." [Section 3.2] "If S concurs that X \ {t} is a WAXp, then we report an error for T (case A3a)."

### Mechanism 3
- **Claim:** Formal explanations must satisfy subset-minimality to be non-redundant, and violations can be detected by iteratively testing subsets.
- **Mechanism:** A correct explanation must be subset-minimal: removing any feature should invalidate it. The framework validates this by taking the explanation X from T, removing one feature t, and asking R (and potentially S) if X \ {t} is still a valid "weak" explanation. If yes, T produced a redundant (incorrect) explanation.
- **Core assumption:** The "deletion-based" check is sufficient to prove minimality given the monotonicity of the WAXp predicate.
- **Evidence anchors:** [Section 3.2] "Because of monotonicity... it suffices to check that WAXp(X \ {t}) does not hold for each t ∈ X." [Section 4.1] "The errors reported in column %[WAXp ∧ ¬AXp] represent cases where PyXAI claims subset-minimality and both RFxpl and XReason concur that the explanation is not subset-minimal."

## Foundational Learning

- **Concept:** **Abductive vs. Contrastive Explanations**
  - **Why needed here:** The paper validates two distinct types of formal explanations. Abductive (AXp) explains "why this prediction?" (sufficient features), while Contrastive (CXp) explains "why not another prediction?" (features that could change the outcome).
  - **Quick check question:** If a feature is removed from an Abductive Explanation and the prediction remains the same, is the original explanation minimal?

- **Concept:** **Logic Encodings (SAT/SMT) for ML**
  - **Why needed here:** The tools in this paper function by translating decision trees/random forests into logic formulas (SAT or SMT). Understanding that the ML model is represented as a satisfiability problem is key to understanding how proofs and witnesses are generated.
  - **Quick check question:** Why would an SMT (Satisfiability Modulo Theories) solver be better suited for "boosted trees" with real-valued scores than a purely boolean SAT solver?

- **Concept:** **Subset Minimality vs. Correctness**
  - **Why needed here:** The paper distinguishes between explanations that are wrong (not even weak explanations) and those that are redundant (correct but not minimal). A robust validator must check both validity and minimality.
  - **Quick check question:** An explainer returns 5 features as the reason for a prediction. A validator finds that only 3 of those features are actually necessary. Is the explainer "wrong" in the strict sense defined by this paper?

## Architecture Onboarding

- **Component map:** Target explainer T (PyXAI) -> Reference explainer R (RFxpl) -> Second explainer S (XReason) -> ML Model M -> Proof Checker
- **Critical path:**
  1. **Extraction:** T generates an explanation X for instance I.
  2. **Basic Validation:** R checks if X satisfies the definition (WAXp/WCXp).
     - *If Disagreement:* R produces a witness → Check witness against Model M. If valid, T is buggy.
  3. **Minimality Check:** For each feature in X, check if removing it retains the property (using R).
  4. **Escalation:** If T and R disagree on minimality or validity without a clear witness, query S.
  5. **Resolution:** If R and S agree against T, T is declared buggy.

- **Design tradeoffs:**
  - **Validation vs. Generation Cost:** Validating explanations requires running complex SAT/SMT queries (R and S), which may be computationally expensive compared to generating them in the first place.
  - **Untrusted Components:** The architecture assumes at least one of R or S is correct. If all explainers share a common bug, the validation fails.
  - **Model Fidelity:** The validators (R, S) encode the model logic. Any discrepancy between the ML model implementation and the logic encoding creates ambiguity.

- **Failure signatures:**
  - **¬WAXp / ¬WCXp:** The explanation is strictly incorrect; there exists a counterexample (witness) that breaks the logic.
  - **Redundancy:** The explanation is valid but contains superfluous features (fails minimality check).
  - **Tie-Breaking Bugs:** Specific to the PyXAI case study, bugs often arose from incorrect handling of tie-breaking logic in majority voting.

- **First 3 experiments:**
  1. **Basic Correctness Test:** Generate AXps for a Random Forest using PyXAI (T). Use RFxpl (R) to produce witnesses for ¬WAXp. Feed witnesses to the original RF model to confirm prediction flips.
  2. **Minimality Test:** Take valid AXps from PyXAI. Ask RFxpl (R) if subsets X \ {t} are valid WAXps. If R says "yes," use XReason (S) to confirm. If confirmed, flag PyXAI as producing redundant explanations.
  3. **Manual Case Inspection:** Construct a minimal RF (3-4 trees) where PyXAI reports a bug (e.g., "xd6" or "Pima" datasets). Trace the decision path manually to verify the tie-breaking logic matches the model, exposing the root cause of the bug.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proposed validation framework be effectively generalized to regression models and non-tree-based classifier families?
- **Basis in paper:** [explicit] The conclusion states, "Future research will extend this paper to consider other tools, other families of classifiers, but also regression models."
- **Why unresolved:** The current study restricts its scope to binary classification using Random Forests and Boosted Trees, leaving the behavior of the validation methodology on continuous outputs or fundamentally different model architectures unknown.
- **What evidence would resolve it:** A study applying the three-explainer validation approach to regression datasets or classifiers like SVMs and Deep Neural Networks.

### Open Question 2
- **Question:** Can certified explainers based on proof assistants (e.g., Rocq, Lean) serve as a practical "trusted" explainer (S) without prohibitive computational overhead?
- **Basis in paper:** [explicit] Section 3.4 suggests future work could use certified explainers as the second explainer S but notes they "should be expected to be significantly slower" than non-certified implementations.
- **Why unresolved:** While the paper validates untrusted explainers against each other, the feasibility of integrating high-assurance, formally verified explainers into the loop without stalling the validation process remains untested.
- **What evidence would resolve it:** Performance benchmarks comparing the runtime of a certified explainer against tools like XReason within the validation framework.

### Open Question 3
- **Question:** To what extent do the identified bugs in tools like PyXAI invalidate the conclusions of prior research that relied on them?
- **Basis in paper:** [explicit] The conclusion warns that the uncovered bugs "raise not only a concern about the general reliability of formal explainers, but also conclusions that have been drawn with PyXAI."
- **Why unresolved:** The paper identifies the existence and frequency of bugs but does not audit specific prior publications to quantify the impact of these errors on their respective scientific claims.
- **What evidence would resolve it:** A re-evaluation of key findings from previous papers that utilized PyXAI to check if the buggy explanation types alter the study's final conclusions.

## Limitations
- The validation framework assumes at least one of the reference explainers (R or S) is correct, creating a single point of failure if both share implementation bugs.
- Floating-point precision discrepancies between the ML model's implementation and the SAT/SMT encodings may create false validation failures.
- The methodology focuses on random forests and boosted trees; results may not generalize to other model types like neural networks.

## Confidence
- **High Confidence:** The core validation mechanism (witness checking against model predictions) is sound and well-supported by evidence.
- **Medium Confidence:** The minimality validation approach is correct in principle but depends on the accuracy of R and S's subset checks.
- **Medium Confidence:** The discovery of PyXAI bugs is well-supported by experimental results, though the specific bug mechanisms require manual case studies for full verification.

## Next Checks
1. **Cross-Validation with Additional Models:** Apply the validation framework to other tree-based models (e.g., LightGBM, CatBoost) to test generalizability beyond RFs and BTs.
2. **Encoding Fidelity Analysis:** Systematically compare model predictions from scikit-learn vs. SAT/SMT encodings for floating-point edge cases to identify precision-related validation failures.
3. **Independent Bug Reproduction:** Implement a minimal test harness to independently verify PyXAI's tie-breaking bug in specific datasets (xd6, Pima) identified in the manual case studies.