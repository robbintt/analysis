---
ver: rpa2
title: Who's Laughing Now? An Overview of Computational Humour Generation and Explanation
arxiv_id: '2509.21175'
source_url: https://arxiv.org/abs/2509.21175
tags:
- humour
- computational
- association
- language
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper surveys the landscape of computational humour generation
  and explanation, focusing on pun generation and joke explanation tasks. It identifies
  a significant research gap, noting that while humour understanding is a foundational
  NLP task, most work focuses on simple puns rather than complex, context-dependent
  humour.
---

# Who's Laughing Now? An Overview of Computational Humour Generation and Explanation

## Quick Facts
- **arXiv ID:** 2509.21175
- **Source URL:** https://arxiv.org/abs/2509.21175
- **Reference count:** 38
- **Primary result:** Large language models struggle with heterographic puns and longer, topical jokes, while humour explanation remains critically understudied.

## Executive Summary
This survey examines computational approaches to humour generation and explanation, identifying significant research gaps in the field. While humour understanding is a foundational NLP task, most work focuses on simple homographic puns rather than complex, context-dependent humour. The authors demonstrate that large language models struggle with heterographic puns due to phonetic representation gaps and perform poorly on topical jokes requiring cultural knowledge. They propose future directions including demographic-aware humour generation, human-in-the-loop approaches, and intent-aware systems to address ethical concerns. The work positions humour as a promising domain for evaluating LLM reasoning capabilities, particularly for verbal and common-sense reasoning tasks.

## Method Summary
The paper surveys existing literature on computational humour, focusing on pun generation systems (like JAPE and STANDUP) and joke explanation approaches. It evaluates large language models using zero-shot non-chain-of-thought prompting on datasets of homographic puns, heterographic puns, non-topical incongruity-based jokes, and topical jokes. The evaluation framework examines model ability to identify pun words, alternative meanings, and resolve incongruities. The study contrasts rule-based systems with neural approaches, and classification-based evaluation with natural language explanation. Key findings emerge from comparing LLM performance across joke types and analyzing failure modes like "lazy" pun generation.

## Key Results
- Large language models struggle with heterographic puns due to lack of phonetic knowledge, performing significantly worse than on homographic puns.
- Topical jokes present even greater difficulty due to ambiguous real-world references that LLMs cannot reliably resolve.
- Humour explanation remains critically understudied compared to generation, with most work focusing on simple pun classification rather than natural language explanation.
- The language modeling objective inherently conflicts with humour generation, as maximizing log-likelihood produces predictable outputs that undermine the incongruity essential to humour.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Humour generation inherently conflicts with standard language modeling objectives.
- Mechanism: Language models maximize log-likelihood of output sequences, which produces predictable, high-probability continuations. Humour requires *incongruity*—deliberate violation of expectations—which runs counter to statistical fluency optimization. Models must actively suppress the most likely continuation in favor of a surprising one.
- Core assumption: Incongruity Theory accurately captures a necessary (if not sufficient) condition for humour perception.
- Evidence anchors:
  - [§3] "The language modelling objective aims to maximise the log likelihood of an output sequence, thereby working against the very incongruity that humour demands."
  - [§2.1] "Incongruity Theory states that humour is the perception of something that conflicts with established mental patterns and expectations."
  - [corpus] Weak direct corpus support; neighbor papers focus on detection/translation, not this mechanistic tension.
- Break condition: If a model is explicitly trained with an auxiliary objective that *rewards* surprise or ambiguity (e.g., PunGAN's discriminator setup), the conflict is mitigated but not eliminated.

### Mechanism 2
- Claim: Heterographic puns expose a phonetic representation gap in text-only LLMs.
- Mechanism: Heterographic puns require recognizing phonetic similarity between different surface forms (e.g., "lint" vs. "leant") while maintaining semantic coherence. Text-trained models lack explicit phonetic representations; they must infer pronunciation from spelling, which is unreliable in English.
- Core assumption: Models do not construct robust phonetic encodings during standard pretraining; any phonetic knowledge is emergent and sparse.
- Evidence anchors:
  - [§3.1.2] "Heterographic puns present additional challenges due to the requirement of modelling phonetic similarity between different surface forms while maintaining semantic coherence."
  - [§4.2] Xu et al. (2024) find "most models struggle with correctly identifying the alternative intended meaning of heterographic puns, which rely on phonetic similarity."
  - [§4.2] Baluja (2025) shows 2.5–4% improvement when providing audio alongside text, suggesting modalities matter.
  - [corpus] No direct corpus counter-evidence; neighbor papers do not address phonetics.
- Break condition: If models are pretrained with speech data or explicit phoneme-level tokenization, the gap should narrow.

### Mechanism 3
- Claim: Humour explanation requires successful retrieval of culturally-situated references before reasoning can proceed.
- Mechanism: Explaining topical or esoteric jokes depends on identifying real-world referents (e.g., PETA's euthanasia rates, Rickrolling). If retrieval fails or returns incorrect context, the subsequent reasoning chain propagates the error. This is particularly acute for "reasoning models" that generate verbose chain-of-thought.
- Core assumption: Joke comprehension is retrieval-first; reasoning operates on retrieved context, not raw text.
- Evidence anchors:
  - [§2] "Humour frequently depends on understanding what was not said or inferring the opposite of explicit statements, which directly conflicts with semantic-similarity-based retrieval systems."
  - [§4.2] Loakman et al. (2025b) find Llama models distilled from DeepSeek R1 perform worst on topical jokes, hypothesizing "ambiguity in real-world references within topical humour, leading to early misunderstandings being propagated through the reasoning process."
  - [corpus] Neighbor paper on meme explanation (MEMECAP, MEMEMQA) supports the retrieval-reasoning pipeline structure.
- Break condition: If retrieval is augmented with external knowledge graphs (e.g., Internet Meme Knowledge Graph) or structured event timelines, explanation accuracy should improve.

## Foundational Learning

- **Incongruity Theory (vs. Relief/Superiority)**
  - Why needed here: The paper repeatedly grounds computational approaches in Incongruity Theory because it offers a *linguistically operationalizable* definition of humour (expectation violation), unlike Relief or Superiority theories which describe psychological experience.
  - Quick check question: Can you explain why a pun's "surprisal" at the punchline is structurally different from mere unexpectedness in a thriller plot twist?

- **Homographic vs. Heterographic Puns**
  - Why needed here: This distinction determines whether a model needs semantic-only resources (WordNet for polysemy) or phonetic resources (sound similarity). The paper shows LLMs perform differently on each, with heterographic being harder.
  - Quick check question: Given the pun "The farmer was outstanding in his field," identify whether it is homographic or heterographic and what knowledge is required to explain it.

- **Explanation vs. Classification**
  - Why needed here: The paper argues classification (labeling a joke's type) is a weaker proxy for understanding than natural language explanation. Evaluation methodology differs: classification is automatable; explanation requires human judgment.
  - Quick check question: If a model correctly labels a joke as "sarcasm" but cannot explain what norm is being violated, has it "understood" the joke according to the paper's framing?

## Architecture Onboarding

- **Component map:** [Input Joke] → [Retrieval Module] ←→ (External Knowledge) → [Sense/Reference Disambiguation] → [Incongruity Detection] → [Explanation Generator] → [Output: Natural Language Explanation]
- **Critical path:** For *topical* jokes, the retrieval-then-reasoning path is critical. For *heterographic puns*, the phonetic encoder is critical. For *homographic puns*, sense disambiguation is the bottleneck.
- **Design tradeoffs:**
  - Rule-based (JAPE, STANDUP) vs. neural: Rule systems are interpretable and controllable but brittle; neural systems are flexible but produce "lazy" puns that nullify ambiguity.
  - Classification vs. natural language explanation: Classification is cheap to evaluate; explanation is expensive but better captures understanding.
  - Human-in-the-loop vs. fully automatic: HITL aligns with creative/ethical goals but reduces throughput.
- **Failure signatures:**
  - "Lazy pun generation": Model includes both senses of pun word in output (e.g., "The sailor's pay was docked after he struggled to dock on time").
  - Propagated retrieval errors: Explanation confidently hallucinates incorrect cultural references.
  - Phonetic collapse: Model fails to identify heterographic pun words entirely.
  - Generic outputs: GAN-based generators produce semantically ambiguous but unfunny sentences.
- **First 3 experiments:**
  1. **Benchmark phonetic sensitivity:** Evaluate a text-only LLM vs. an audio-augmented LLM (e.g., Gemini with TTS input) on heterographic pun explanation. Measure delta in accuracy. This tests Mechanism 2 directly.
  2. **Retrieve-then-explain pipeline:** Build a RAG system with access to a structured knowledge source (e.g., Know Your Meme, Wikidata current events) for topical joke explanation. Compare against a no-retrieval baseline. Measure explanation quality via human preference.
  3. **Detect lazy puns:** Run a pun generation model (e.g., AmbiPun or an LLM prompted for puns) and automatically flag outputs containing both senses of the target word. Quantify the rate of this failure mode and correlate with human funniness ratings.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can computational models be effectively grounded in phonetic knowledge to enable generation and explanation of heterographic puns?
- Basis in paper: [explicit] The authors state "large language models struggle with heterographic puns due to their lack of phonetic knowledge" and that heterographic puns require understanding "phonetic relationships and semantic contexts" while models operate on text-based representations.
- Why unresolved: Current LLMs operate primarily on text tokens without access to phonological representations, and existing approaches (e.g., retrieve-and-edit frameworks) remain limited in scope and generalization.
- What evidence would resolve it: Demonstrated improvements in heterographic pun generation and explanation quality through explicit phonetic modeling, potentially via multimodal architectures or phoneme-aware training objectives.

### Open Question 2
- Question: How can intent-aware modelling distinguish between benign humour and offensive content that uses humour as a guise?
- Basis in paper: [explicit] The authors explicitly propose "intent-aware models" that model author characteristics and intent, noting this would "aid in overcoming the undesirable effect of legitimising the instances where 'it's just a joke!' is used as a guise for spreading hatred through offensive material."
- Why unresolved: Intent inference requires modelling unobservable mental states from limited contextual signals, and defining reliable criteria for distinguishing benign violations from harmful content remains ethically and technically complex.
- What evidence would resolve it: Development and validation of intent-aware systems that demonstrably reduce false negatives in detecting harmful content while preserving legitimate comedic expression across diverse contexts.

### Open Question 3
- Question: How can retrieval-augmented systems effectively handle ambiguous references to world events and pop culture in topical jokes?
- Basis in paper: [inferred] The authors identify that topical jokes "present even more difficulty to LLMs" due to "ambiguity in real-world references," and note that semantic-similarity-based retrieval "directly conflicts with" humour's reliance on inference about what was not said.
- Why unresolved: Standard retrieval systems optimize for direct semantic matching rather than the indirect, culturally-mediated associations that topical humour requires, and knowledge bases cannot comprehensively capture rapidly-evolving cultural phenomena.
- What evidence would resolve it: Improved explanation accuracy for topical jokes through retrieval mechanisms designed for associative rather than direct semantic access, evaluated on temporally-sensitive benchmark datasets.

### Open Question 4
- Question: What human-in-the-loop frameworks can meaningfully integrate user contributions into computational humour systems while preserving creative value?
- Basis in paper: [explicit] The authors propose "future work should focus increasingly on human-in-the-loop approaches" and specifically suggest "combining elements of humour generation and explanation to develop systems for joke workshopping, offering valuable feedback and direction on human-authored humour, rather than generating jokes wholesale."
- Why unresolved: Existing humour generation systems predominantly operate autonomously, and effective interaction paradigms for collaborative humour creation remain undefined—balancing user agency with system capability is non-trivial.
- What evidence would resolve it: Comparative studies showing that human-in-the-loop systems produce higher-quality or more personally-relevant humour than fully autonomous generation, with qualitative evidence of meaningful user creative contribution.

## Limitations
- The evaluation framework relies heavily on zero-shot prompting without extensive human evaluation, which may not fully capture model understanding.
- Dataset construction for topical jokes is not detailed, raising questions about reproducibility and generalizability across cultural contexts.
- Most work focuses on simple puns, leaving complex humour underexplored and limiting the generalizability of findings to broader humour understanding.

## Confidence

- **High Confidence:** The observation that language models struggle with heterographic puns due to phonetic representation gaps is well-supported by the cited evidence showing improved performance with audio input.
- **Medium Confidence:** The claim that humour explanation requires successful retrieval of culturally-situated references is plausible but depends heavily on implementation details not fully specified in the survey.
- **Medium Confidence:** The conflict between language modeling objectives and humour generation (incongruity theory) is conceptually sound, though the empirical evidence for this mechanistic tension is indirect.

## Next Checks

1. **Phonetic Sensitivity Benchmark:** Evaluate text-only versus audio-augmented LLMs on heterographic pun explanation to directly test whether phonetic representation gaps limit performance.
2. **Retrieval-Then-Explanation Pipeline:** Implement a RAG system with structured knowledge sources for topical joke explanation and compare against no-retrieval baselines to validate the retrieval-reasoning dependency.
3. **Lazy Pun Generation Detection:** Systematically measure the rate of "lazy" pun generation where models explicitly include both senses of pun words, and correlate with human funniness ratings to quantify this failure mode.