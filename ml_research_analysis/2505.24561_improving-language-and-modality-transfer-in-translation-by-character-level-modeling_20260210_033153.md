---
ver: rpa2
title: Improving Language and Modality Transfer in Translation by Character-level
  Modeling
arxiv_id: '2505.24561'
source_url: https://arxiv.org/abs/2505.24561
tags:
- latn
- languages
- translation
- speech
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a character-level encoder for better cross-lingual\
  \ and cross-modal transfer in translation. By using SONAR\u2019s fixed-size embedding\
  \ space and a teacher-student approach, it learns a character-level encoder that\
  \ embeds sentences in a multilingual space, enabling effective language and modality\
  \ transfer."
---

# Improving Language and Modality Transfer in Translation by Character-level Modeling

## Quick Facts
- **arXiv ID:** 2505.24561
- **Source URL:** https://arxiv.org/abs/2505.24561
- **Reference count:** 24
- **Primary result:** Character-level modeling improves cross-lingual and cross-modal transfer in translation, achieving state-of-the-art results in speech-to-text translation across 33 languages with minimal supervision.

## Executive Summary
This paper introduces a character-level encoder that enhances cross-lingual and cross-modal transfer in translation. By mapping sentences to a fixed-size embedding space using a teacher-student approach with SONAR, the method enables effective knowledge sharing across languages and modalities. A lightweight adapter connects a massively multilingual ASR model to the character encoder, allowing zero-shot speech translation from over 1,000 languages. The character-based approach outperforms subword-based models in low-resource and zero-shot settings, achieving state-of-the-art results on the FLEURS dataset while requiring minimal training data.

## Method Summary
The method trains a character-level encoder (charSONAR) to map text into the same fixed-size embedding space as the subword-based SONAR encoder, using a teacher-student approach with interpolated MSE loss between source and target language embeddings. A cross-modal adapter bridges a frozen multilingual ASR model (MMS) to the charSONAR encoder by leveraging the MMS CTC output and charSONAR embedding table, enabling zero-shot speech translation. The system uses a frozen SONAR decoder for generation, with the character encoder trained on parallel text and the adapter fine-tuned on minimal speech data. Character-level tokenization improves cross-lingual transfer by creating a universal vocabulary, while the interpolated embedding objective and pretrained adapter initialization enable effective low-resource and zero-shot performance.

## Key Results
- Character-based approach achieves better language transfer than subword-based models, especially in low-resource settings and for zero-shot generalization to unseen languages.
- Interpolated embedding objective (averaging source and target embeddings) outperforms reconstruction and translation objectives in translation quality and similarity search.
- Lightweight pretrained adapter enables zero-shot speech translation across 33 languages with minimal supervision, surpassing supervised and cascade models.
- On FLEURS dataset, the method achieves state-of-the-art results in speech-to-text translation despite being zero-shot.

## Why This Works (Mechanism)

### Mechanism 1: Character-level Tokenization Enables Language-agnostic Representation Sharing
Character-level encoding improves cross-lingual transfer by creating a universal, fine-grained vocabulary shared across languages using the same script. This reduces vocabulary fragmentation and allows morphological patterns to be learned directly, enabling knowledge transfer even when parallel data is scarce. The embedding space becomes less tied to language-specific subword units, which is particularly beneficial for low-resource and unseen languages.

### Mechanism 2: Interpolated Embedding Objective Creates a Superior Transfer-friendly Space
Training the character encoder to map to an interpolated (averaged) embedding between source and target languages yields better translation and similarity search performance than mapping to either source or target alone. The SONAR embedding space has sub-optimal regions for individual languages, and by training the student encoder to produce embeddings that land in the "middle" of the space between a sentence and its translation, the model learns a representation that is more equidistant and thus better suited for cross-lingual tasks.

### Mechanism 3: Pretrained Cross-modal Adapter Bridges ASR and Text via Shared Character Space
A lightweight adapter, largely initialized from pre-trained weights, effectively connects a frozen multilingual ASR model to a frozen character-level text encoder. The MMS ASR model outputs CTC predictions over a character-based vocabulary that is a subset of the charSONAR encoder's vocabulary. By reusing the MMS CTC classification layer and charSONAR embedding layer, the adapter creates a "soft" prediction directly compatible with the text encoder's input space, minimizing data and parameters needed to bridge the modalities.

## Foundational Learning

**Concept: SONAR's Fixed-Size Sentence Embedding Space**
- **Why needed here:** The entire method is built on distilling a character-level encoder into SONAR's space. Without understanding that SONAR maps any sentence to a single, fixed-size vector (via mean-pooling) which a decoder then uses for generation, the teacher-student setup and the role of the MSE loss are unclear.
- **Quick check question:** If you feed two sentences with the same meaning in different languages into the SONAR encoder, what should be true about their output vectors?

**Concept: CTC (Connectionist Temporal Classification) in ASR**
- **Why needed here:** The cross-modal adapter's core insight is leveraging the CTC output from MMS. You must understand that CTC produces a sequence of frame-level predictions over a character vocabulary (with blanks) and that "CTC compression" removes blanks and merges repeated characters to get a shorter, meaningful sequence.
- **Quick check question:** What is the purpose of the <blank> token in CTC, and what happens to consecutive frames with the same non-blank prediction during CTC compression?

**Concept: Teacher-Student Distillation (Task-Agnostic)**
- **Why needed here:** The character encoder is trained to mimic the outputs of the subword-based SONAR encoder, not to directly solve a translation task. This form of distillation transfers the representational knowledge from the teacher to a student with a different (character-based) input format.
- **Quick check question:** In this paper's setup, what is the "teacher," what is the "student," and what is the loss function that connects them?

## Architecture Onboarding

**Component map:**
1. Input (Text): Raw text → Character tokenization (vocabulary Vc)
2. charSONAR Encoder: Transformer (Nt=24 layers, dt=1024). Takes character sequence, outputs a mean-pooled fixed-size embedding
3. Input (Speech): Raw audio → MMS Acoustic Encoder (frozen) → CTC Output → CTC Compression → Acoustic Representation `A`
4. Cross-modal Adapter: Takes `A`. Has two paths: (a) Pretrained path: Reuses MMS CTC layer and charSONAR embedding layer to create a "soft" embedding sequence. (b) Random path: A small trained MLP (`U_in`, `U_out`). A gating MLP combines them.
5. Output: The adapter's output is prepended with a language token, appended with EOS, and fed into the frozen charSONAR encoder to produce a speech embedding
6. Decoder (Inference): The SONAR decoder (Nt=24 layers) takes the final embedding (from text or speech) via cross-attention and generates the translation autoregressively

**Critical path:**
- Text: `characters → charSONAR Encoder → embedding → SONAR Decoder → translation`
- Speech: `audio → MMS Encoder → CTC Compression → Adapter → charSONAR Encoder → embedding → SONAR Decoder → translation`
- The entire speech path depends on the quality of the text-based charSONAR encoder, which must be trained first. The adapter is the only component trained on speech data

**Design tradeoffs:**
- Character vs. Subword Input: Characters improve cross-lingual transfer but increase sequence length by ~3.2x. The fixed-size embedding bottleneck mitigates decoder cost, but encoder self-attention cost remains O(n²)
- Adapter Type: The pretrained adapter is minimal, data-efficient, and works out-of-the-box but has low capacity (hidden dim ~64). The random adapter adds capacity but requires more data
- Interpolation vs. Other Objectives: The interpolated MSE objective is key but presumes the availability of parallel data to form (source, target) pairs for averaging

**Failure signatures:**
- Poor zero-shot on a new language: Check if the language uses a script or family not well-represented in training
- Speech adapter fails to train: Ensure the MMS CTC vocabulary `B(i)` is a strict subset of the charSONAR vocabulary `Vc`
- Performance degrades on high-resource speech languages: The random branch of the dual adapter may be under-trained or the hidden dimension `dh` may be too small
- Text model underperforms baseline: Ensure the interpolated objective is used with reconstruction pre-training

**First 3 experiments:**
1. Reproduce the core text result: Train charSONAR on a small group of related languages (e.g., Uralic/Cyrillic) using the interpolated MSE objective. Compare against a subword-based SONAR fine-tuned on the same data
2. Validate the adapter initialization: Construct only the pretrained adapter (no random branch) and run inference on a few speech samples without any adapter training
3. Ablate the interpolation objective: Train three character encoders on the same data using (a) reconstruction MSE, (b) translation MSE, and (c) interpolated MSE. Evaluate on translation and similarity search

## Open Questions the Paper Calls Out

**Open Question 1:** Can character-level modeling improve target-side cross-lingual and cross-modal transfer without incurring prohibitive decoding inefficiencies?
- Basis in paper: The authors identify target-side transfer as a limitation, noting that while character-level encoding is effective, "decoding on the character-level can be problematic and relatively more inefficient."
- Why unresolved: This work restricted character-level modeling to the encoder side to utilize the fixed-size SONAR bottleneck, leaving the decoder side unexplored.
- What evidence would resolve it: A study training a character-level decoder within the SONAR framework, measuring the trade-off between translation quality gains in target-side transfer and inference latency.

**Open Question 2:** Can the proposed adapter generalize to unseen languages when connected to a unified acoustic model rather than language-specific CTC layers?
- Basis in paper: The authors state that their method is "limited by the language-specific CTC layers of MMS," which prevents the speech encoder from generalizing to languages without specific ASR training data.
- Why unresolved: The current adapter requires language-specific training because the underlying MMS model relies on separate prediction heads per language.
- What evidence would resolve it: Experiments integrating the adapter with a unified ASR model to evaluate translation performance on languages without dedicated ASR supervision.

**Open Question 3:** Do the cross-lingual transfer benefits of character-level modeling persist in traditional encoder-decoder architectures without a fixed-size embedding bottleneck?
- Basis in paper: The authors hypothesize that "similar gains can be achieved by adapting a traditional encoder-decoder, like NLLB," but restricted the study to the bottlenecked SONAR architecture.
- Why unresolved: The paper isolates the tokenization variable using SONAR to simplify the teacher-student training process, leaving the interaction between character-level modeling and standard architectures untested.
- What evidence would resolve it: Benchmarking character-level adaptations of standard architectures against their subword counterparts on the same low-resource translation tasks.

## Limitations
- The claimed advantages heavily rely on exploiting script and family overlap, potentially diminishing for languages with unique scripts
- Zero-shot generalization boundaries remain unproven for truly isolated languages or those with non-Latin scripts
- The pretrained adapter's fixed, low-capacity architecture may limit performance for languages requiring more complex cross-modal mappings

## Confidence
**High Confidence:** Character-level encoding improves cross-lingual transfer over subword-based models in low-resource settings; Interpolated embedding objective outperforms reconstruction and translation objectives; Pretrained adapter enables zero-shot speech translation with minimal supervision.

**Medium Confidence:** Cross-lingual transfer benefits primarily from shared character vocabulary rather than increased sequence length; Interpolated embeddings represent a superior transfer-friendly region in embedding space.

**Low Confidence:** The approach will generalize to languages with completely unique scripts not represented in training data; The interpolated objective will provide similar benefits across different embedding spaces beyond SONAR.

## Next Checks
1. **Script isolation test:** Train charSONAR on a language family with unique scripts (e.g., CJK languages) and evaluate zero-shot transfer to a distantly related script family. Compare performance degradation against subword-based models to quantify script-dependency limits.

2. **Adapter capacity scaling:** Systematically vary the pretrained adapter's hidden dimension from 32 to 512 while keeping all else constant. Measure zero-shot performance on a low-resource language to determine if the fixed capacity is a bottleneck.

3. **Domain transfer evaluation:** Evaluate the charSONAR model on a completely different domain (e.g., biomedical or legal text) to assess whether character-level benefits transfer beyond the news and web domains used in FLORES+.