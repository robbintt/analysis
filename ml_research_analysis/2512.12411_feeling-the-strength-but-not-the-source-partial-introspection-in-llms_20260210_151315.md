---
ver: rpa2
title: 'Feeling the Strength but Not the Source: Partial Introspection in LLMs'
arxiv_id: '2512.12411'
source_url: https://arxiv.org/abs/2512.12411
tags:
- injected
- concept
- response
- strength
- injection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tests the robustness of LLM introspection by reproducing\
  \ Anthropic\u2019s results in Llama-3.1-8B-Instruct. The model matched the original\
  \ 20% introspection rate on Anthropic\u2019s multi-turn prompt, showing that introspection\
  \ is not exclusive to large models."
---

# Feeling the Strength but Not the Source: Partial Introspection in LLMs

## Quick Facts
- **arXiv ID**: 2512.12411
- **Source URL**: https://arxiv.org/abs/2512.12411
- **Reference count**: 14
- **Primary result**: Llama-3.1-8B-Instruct achieves 20% introspection rate on Anthropic's prompt but reliably classifies injection strength (70% accuracy), suggesting partial introspection capability.

## Executive Summary
This work tests the robustness of LLM introspection by reproducing Anthropic's results in Llama-3.1-8B-Instruct. The model matched the original 20% introspection rate on Anthropic's multi-turn prompt, showing that introspection is not exclusive to large models. However, performance dropped sharply on simpler prompts, such as binary detection (60% vs. 50% chance) and multiple-choice identification (20-60% vs. 10-50% chance). A key finding was that the model could accurately classify the strength of injected concepts (70% vs. 25% chance), especially at deeper layers. This indicates that models can compute simple functions of internal activations but cannot reliably verbalize or identify semantic content. These results suggest that LLM self-reports are fragile and unsuitable as safety mechanisms, emphasizing the need for mechanistic interpretability and external oversight.

## Method Summary
The study uses residual stream activation steering to inject concept vectors into Llama-3.1-8B-Instruct. Concept vectors are computed as the difference between mean activations from positive and negative example sets, L2-normalized, then added to hidden states via forward hooks at layers 9, 12, 15, or 18 with coefficients 4, 9, or 16. Six prompt types were tested: Anthropic Reproduce (multi-turn), Open-Ended Belief, Generative Distinguish (binary), MCQ Knowledge (10-way), MCQ Distinguish (2-way), and Injection Strength (4-way). Results were evaluated using an LLM-as-judge approach with Coherence, Affirmative Response, and Correct Identification criteria.

## Key Results
- Successfully reproduced Anthropic's 20% introspection baseline on multi-turn prompt
- Binary detection performance dropped to 60% (vs. 50% chance)
- Multiple-choice identification accuracy ranged from 20-60% (vs. 10-50% chance)
- Injection strength classification achieved 70% accuracy (vs. 25% chance), with improved performance at deeper layers
- Zero percent success on identifying two simultaneous concept injections

## Why This Works (Mechanism)

### Mechanism 1: Residual Stream Activation Steering
Concept vectors are injected into the residual stream to bias model representations toward specific semantic content. Vectors are computed as the difference between mean activations from positive vs negative example sets, L2-normalized, then added to hidden states via forward hooks: H^(l) ← H^(l) + α·v^(l). The coefficient α controls injection strength. Internal representations encode concepts as linear directions that remain accessible across forward passes.

### Mechanism 2: Partial Introspection via Magnitude Sensing
Models can classify injection strength more reliably than semantic content, suggesting they compute simple functions of internal activations without accessing full representational semantics. The model appears sensitive to activation magnitude shifts caused by injection, with this sensitivity amplifying at deeper layers (layer 18: 70% accuracy vs 25% chance).

### Mechanism 3: Prompt-Scaffolded Elicitation
Introspection behavior emerges only under specific multi-turn prompting and does not generalize to simpler formats. The "interpretability researcher" framing provides contextual scaffolding that elicits introspective outputs the model cannot produce under minimal prompts. Self-reports reflect pattern-matching to prompt context rather than genuine internal state access.

## Foundational Learning

- **Residual Stream Architecture**: Understanding where concept vectors are injected and how information flows through layers determines optimal injection depth. *Quick check*: If you inject at layer 9 vs layer 18, how many subsequent layers can transform that signal before output?

- **L2 Normalization of Vectors**: Ensures consistent injection magnitudes across concepts with varying activation scales. *Quick check*: Why must concept vectors be normalized before scaling by coefficient α?

- **LLM-as-Judge Evaluation**: Introspection outputs require structured evaluation since responses vary in phrasing. *Quick check*: Why is a "Coherence" judge used as a gating criterion before checking introspection accuracy?

## Architecture Onboarding

- **Component map**: Dataset (simple nouns / complex sentence pairs) → Vector computation (activations → difference → normalize) → Injection system (forward hooks, layer/α selection) → Prompt templates (6 types) → Evaluation (GPT-5-nano judges)
- **Critical path**: Extract activations → Compute concept vector → Register hook → Run inference with injection → Judge output
- **Design tradeoffs**: Earlier layers = more processing capacity downstream; later layers = better strength detection but less semantic readout; high α = stronger effect but risk of incoherence
- **Failure signatures**: Model collapse (garbled output) → reduce α; near-chance MCQ → expected (mechanism doesn't support semantic identification); 0% on multi-concept → architecture limitation
- **First 3 experiments**:
  1. Reproduce Anthropic multi-turn prompt at layer 15, α=9, avg vector type to verify 20% baseline
  2. Run binary detection (generative distinguish) to confirm fragility (expect ~60%)
  3. Sweep layers for strength classification to observe depth-dependent accuracy increase

## Open Questions the Paper Calls Out

### Open Question 1
Why does the accuracy of classifying injection strength increase at deeper network layers? The authors note this result is counterintuitive as deeper layers typically leave less capacity for processing the injected signal. Mechanistic interpretability studies analyzing residual stream norms and attention head contributions at layers 15-18 versus earlier layers could resolve this.

### Open Question 2
Why does the specific multi-turn prompt structure elicit introspection while simpler binary prompts fail? The paper demonstrates that introspection is "fragile" and collapses under slight prompt variations, but it does not isolate which specific features of the prompt enable the 20% success rate. Systematic ablations of the multi-turn prompt components could identify necessary structures.

### Open Question 3
Why is the model unable to identify or quantify multiple simultaneous concept injections? Section 4.3 reports 0% success in identifying two injected concepts. The paper does not determine if this failure is caused by superposition interference in the activation space or a fundamental limitation in the model's ability to report composite states.

## Limitations
- Results validated on single model (Llama-3.1-8B-Instruct) with fictional judge model (GPT-5-nano)
- Semantic content identification failure difficult to distinguish from inherent impossibility of the task
- Generalization across model families, sizes, and judge models remains uncertain
- Cannot rule out prompt sensitivity or evaluation artifacts as alternative explanations

## Confidence
- **High confidence**: Reproduction of Anthropic's 20% baseline introspection rate and demonstration of strength classification accuracy (70% vs 25% chance)
- **Medium confidence**: Interpretation that results demonstrate "partial introspection" rather than complete failure
- **Low confidence**: Specific claim about "fragile" introspection behavior across all prompt variants

## Next Checks
1. **Judge model substitution validation**: Reproduce injection strength classification using GPT-4o or Claude Haiku as judge to verify 70% accuracy finding
2. **Cross-model generalization**: Apply methodology to other Llama models (7B, 70B) and non-Llama architectures to test strength detection capability
3. **Mechanistic probing of strength detection**: Design ablation studies to determine if model detects actual injection magnitude versus correlated proxy signals