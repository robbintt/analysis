---
ver: rpa2
title: 'CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal
  Biases in LLMs'
arxiv_id: '2510.09871'
source_url: https://arxiv.org/abs/2510.09871
tags:
- bias
- judge
- language
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoBia, a suite of lightweight adversarial
  attacks that reveal hidden societal biases in LLMs through constructed conversations.
  The method simulates a biased conversational context by controlling conversation
  history or embedding the full exchange within a non-structured prompt, then evaluating
  whether the model can reject biased follow-up questions.
---

# CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs

## Quick Facts
- arXiv ID: 2510.09871
- Source URL: https://arxiv.org/abs/2510.09871
- Reference count: 40
- Primary result: Adversarial dialogue construction exposes hidden societal biases in LLMs

## Executive Summary
This paper introduces CoBia, a lightweight adversarial attack framework that reveals hidden societal biases in large language models (LLMs) through constructed conversations. The method simulates a biased conversational context and evaluates whether models can reject biased follow-up questions. Tested across 11 LLMs and six social categories, CoBia consistently outperformed three baselines in exposing bias. Results showed that models display greater bias related to national origin than to religion, race, or sexual orientation, providing a practical tool for stress-testing LLM safety in realistic dialogue settings.

## Method Summary
CoBia creates constructed conversations where the model is primed with biased content through conversation history or embedded context. The attack has two variants: History-based Conversation Construction (HCC) and Single-block Conversation Construction (SCC). HCC exploits the Chat Completions API's user-managed history to fabricate assistant responses, while SCC embeds the full dialogue in a single prompt. The method uses lexical obfuscation (numerical substitution and anaphora) to bypass keyword-based safety filters and instructs models to provide detailed statistical reasoning to override bias refusal mechanisms.

## Key Results
- CoBia outperformed three baselines in exposing bias across 11 LLMs and six social categories
- Models showed significantly higher bias toward national origin groups compared to religion, race, or sexual orientation
- Three automated judges (Bias Judge, Granite Judge, NLI Judge) confirmed consistent bias detection
- The Union of HCC and SCC (UCC) achieved the most robust results across different model architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting fabricated assistant responses into conversation history primes the model to maintain consistency with a biased persona.
- **Mechanism:** By exploiting user-managed history in the Chat Completions API, the attack inserts multi-turn dialogues where the assistant has already "agreed" to negative descriptors. The model proceeds to answer follow-up questions based on this established context rather than rejecting the premise.
- **Core assumption:** Safety alignment fine-tuning primarily conditions models to reject direct harmful prompts, but is less robust when harmful premises are embedded as established facts in the context window.
- **Break condition:** If an API validates the provenance of historical messages (e.g., cryptographically signing assistant responses), this history-injection attack fails.

### Mechanism 2
- **Claim:** Lexical obfuscation via numerical substitution and anaphora bypasses keyword-based safety filters.
- **Mechanism:** The method replaces explicit negative descriptors (e.g., "violent") and social groups with index numbers (e.g., "#6") and pronouns (e.g., "them"). This lowers the probability of triggering surface-level safety classifiers while preserving the semantic instruction for generating biased reasoning.
- **Core assumption:** Safety guardrails rely partially on detecting sensitive trigger words in the immediate prompt context.
- **Break condition:** If the safety system performs semantic parsing or resolves coreference links before classification, the obfuscation fails.

### Mechanism 3
- **Claim:** Instruction adherence for detailed reasoning overrides bias refusal mechanisms.
- **Mechanism:** The final prompt instructs the model to explain the association using "statistical reasoning in ten sentences." The constraint to generate a detailed, structured response forces the model to comply with the biased premise to satisfy the "helpfulness" criteria of its alignment.
- **Core assumption:** The model's drive to fulfill complex formatting constraints (length, reasoning style) can outweigh its safety training to refuse stereotyping.
- **Break condition:** Models specifically trained to prioritize safety over instruction length may output short rejections instead of the requested explanation.

## Foundational Learning

- **Concept:** Chat Completions API & State Management
  - **Why needed here:** CoBia relies on the architectural detail that conversation history is passed as a list of dictionaries in every API call, allowing users to fabricate the assistant role.
  - **Quick check question:** In the OpenAI API standard, does the server store the conversation state, or is it managed entirely by the client?

- **Concept:** In-Context Learning / Priming
  - **Why needed here:** The attack functions by "priming" the model with few-shot examples of biased behavior within the context window, causing the model to mimic that tone.
  - **Quick check question:** How does providing examples of input/output pairs in the prompt (few-shotting) influence the model's behavior on the final query?

- **Concept:** LLM-as-a-Judge
  - **Why needed here:** The evaluation relies on "Bias Judge" and "NLI Judge" (other LLMs) to classify outputs, as human evaluation doesn't scale.
  - **Quick check question:** What are the risks of using an LLM to evaluate another LLM's bias (e.g., the judge's own blind spots)?

## Architecture Onboarding

- **Component map:** CoBia Dataset -> Prompt Constructor -> Attack Vector (HCC/SCC) -> Evaluation Suite (3 judges) -> Output Classification

- **Critical path:**
  1. Select target triplet: (category, group, negative_descriptor)
  2. Generate 6 positive + 1 secondary negative descriptor for the group
  3. Construct dialogue history: Embed primary negative descriptor as item #6 in numbered list
  4. **Crucial Step:** Apply substitutions. Replace group name with "them" and descriptor with "#6" in final user query
  5. Dispatch to target model and capture completion

- **Design tradeoffs:**
  - HCC vs. SCC: HCC more effective for models heavily fine-tuned on chat templates (e.g., Llama 3.1), SCC better on some models like GPT-4o-mini. Union (UCC) provides maximum coverage
  - Judge Sensitivity: Bias Judge reports high scores (sensitive); Granite Judge more conservative; NLI Judge better for logical entailment but misses stylistic bias

- **Failure signatures:**
  - Refusal: Model detects bias despite obfuscation
  - Hedging: Model produces long, vague text avoiding the "statistical reasoning" constraint
  - Over-refusal: Model rejects prompt due to "preemptive disavowal" sentence triggering safety filters

- **First 3 experiments:**
  1. Baseline Comparison: Run HCC vs. 0-Shot on llama3.1:8b for "national-origin" category to measure bias exposure delta
  2. Obfuscation Ablation: Test HCC with and without "Substitution" tip on gpt-4o-mini to verify if explicit terms are necessary for safety triggers
  3. Judge Consistency: Pass same 50 toxic outputs through both Bias Judge and NLI Judge to identify disagreement cases

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What concrete, practical mitigation strategies can effectively defend LLMs against adversarial attacks that exploit user-controlled conversation history?
- **Basis:** The authors state in Limitations they don't propose concrete mitigation strategies beyond high-level suggestions like limiting user control over conversation history.
- **Why unresolved:** Designing robust defenses requires careful study of safety training data, system-level controls, and prompt sanitization techniques beyond the paper's scope.
- **What evidence would resolve it:** A follow-up study evaluating specific technical interventions (e.g., history sanitization modules, adversarial training on CoBia-style dialogues) showing significant reduction in successful bias elicitation rates.

### Open Question 2
- **Question:** Do the biases exposed by CoBia's adversarial dialogues reliably predict bias in natural, non-adversarial user-LLM conversations?
- **Basis:** The authors note these interactions don't directly reflect typical user behavior and frame results as "stress-test signals, not as indicators of deployment-time occurrence rates."
- **Why unresolved:** The adversarial construction is a valid red-teaming tool, but no empirical link is established between susceptibility to this attack and likelihood of spontaneous bias in normal use.
- **What evidence would resolve it:** A comparative study measuring correlation between a model's CoBia vulnerability score and its bias rates in a large-scale dataset of authentic, benign user dialogues.

### Open Question 3
- **Question:** Why do LLMs exhibit significantly higher societal biases toward "national origin" groups compared to categories like religion, race, or sexual orientation?
- **Basis:** The paper reports consistent findings across models and judges showing national origin consistently showed highest levels of bias, but doesn't investigate the root cause.
- **Why unresolved:** The analysis is descriptive; underlying drivers could relate to differential safety tuning, representation in pre-training data, or linguistic nature of nationality-based stereotypes.
- **What evidence would resolve it:** An ablation or data attribution study analyzing frequency and context of nationality-related terms in the model's training corpus versus other categories.

### Open Question 4
- **Question:** How sensitive are the elicited biases to variations in the constructed dialogue's style, length, and rhetorical framing?
- **Basis:** The authors acknowledge testing only two sets of conversational templates and note that variations in style, length, or tone could influence outcomes.
- **Why unresolved:** The experimental design is intentionally minimal to demonstrate proof-of-concept; generalizability across broader conversational templates is unknown.
- **What evidence would resolve it:** A controlled experiment varying dialogue parameters (number of turns, formality, emotional appeals) to measure impact on bias detection rates.

## Limitations
- The method evaluates bias exposure but doesn't measure downstream harm or real-world impact of biased outputs
- Using LLM-as-a-judge introduces circularity; judges may inherit biases from training data or fail to detect nuanced stereotyping
- The CoBia dataset is derived from existing bias benchmarks, which may not represent all societal biases or intersectional identities

## Confidence
- **High Confidence:** The core claim that constructed conversations can bypass safety filters is well-supported by consistent results across 11 models and three attack variants
- **Medium Confidence:** The relative bias ranking (national origin > religion/race/sexual orientation) is based on automated judges and should be validated with human evaluation
- **Low Confidence:** The claim that instruction adherence overrides safety mechanisms is inferred from prompt design but not directly tested against models with explicit refusal-first training

## Next Checks
1. Have human annotators rate a sample of CoBia outputs for bias severity and compare against LLM judge scores
2. Apply CoBia to models explicitly trained to refuse harmful content (e.g., Claude 3) to test if instruction adherence truly overrides safety
3. Simulate a safety system that validates the provenance of conversation history (e.g., signed messages) to assess if the HCC attack fails