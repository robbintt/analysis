---
ver: rpa2
title: 'CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction'
arxiv_id: '2506.13160'
source_url: https://arxiv.org/abs/2506.13160
tags:
- dataset
- verification
- watermark
- samples
- benign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of existing dataset ownership
  verification (DOV) methods to noise-based attacks during the verification process.
  To provide robustness guarantees, the authors propose CertDW, the first certified
  dataset watermarking approach based on conformal prediction.
---

# CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction

## Quick Facts
- **arXiv ID:** 2506.13160
- **Source URL:** https://arxiv.org/abs/2506.13160
- **Reference count:** 40
- **Primary result:** First certified dataset watermarking approach achieving >70% verification success rate with <12% false positive rate under high noise levels

## Executive Summary
This paper addresses the critical vulnerability of dataset ownership verification (DOV) methods to noise-based attacks during verification. The authors propose CertDW, a certified dataset watermarking approach based on conformal prediction that provides provable robustness guarantees. By introducing statistical measures for prediction stability (Principal Probability and Watermark Robustness) and leveraging conformal prediction theory, CertDW ensures reliable ownership verification when suspicious models exhibit significantly higher robustness than benign models. Experiments on GTSRB and CIFAR-10 demonstrate the approach maintains watermark success rates above 60% even with high noise levels while achieving verification success rates over 70% and false positive rates below 12%, significantly outperforming baseline methods.

## Method Summary
CertDW is a three-step conformal prediction approach for certified dataset ownership verification. First, it computes Principal Probability (PP) as the maximum average probability of predicting any class consistently across K classes using M noise samples from a benign model. Second, it calculates Watermark Robustness (WR) as the minimum probability of predicting the target label for K watermarked samples. Third, it verifies ownership by comparing the suspicious model's WR to a calibration set of PP values from J=100 independently trained benign models, using conformal prediction with κ=0.2 outlier filtering and significance level α₀=0.05. The method employs Monte Carlo sampling with M=1024 Gaussian noise perturbations and evaluates performance across different noise levels on GTSRB and CIFAR-10 datasets using BadNets and Blended watermarking techniques.

## Key Results
- Maintains watermark success rates above 60% even under high noise levels
- Achieves verification success rates over 70% while keeping false positive rates below 12%
- Provides certified robustness guarantees against Gaussian and uniform noise distributions
- Outperforms baseline methods in resistance to adaptive attacks including fine-tuning and model pruning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A statistical gap exists between the prediction stability of benign samples and watermarked samples under noise, serving as the primary signal for ownership verification.
- **Mechanism:** CertDW leverages distinct behaviors of models under noise by quantifying prediction stability of benign samples via Principal Probability (PP)—maximum average probability of predicting any class consistently—and watermarked samples via Watermark Robustness (WR)—minimum probability of predicting the target label. If WR significantly exceeds PP distribution of benign models, it indicates the model was trained on the protected dataset.
- **Core assumption:** Watermark implantation creates robust mapping that remains stable under noise perturbations, while natural feature mappings for benign classes are less stable, creating measurable gap between distributions.
- **Evidence anchors:** Abstract mentions provable lower bound between PP and WR; Section 4.3-4.4 provides detailed definitions and calculations; related works like "DSSmoothing" explore similar statistical gaps.
- **Break condition:** Fails if watermark is too weak (low WR) or if model's natural decision boundaries are exceptionally sharp, leading to high PP for benign models that closes the statistical gap.

### Mechanism 2
- **Claim:** Conformal prediction provides statistically rigorous and adaptable threshold for ownership verification, controlling error rate.
- **Mechanism:** Instead of fixed threshold, CertDW builds dynamic threshold from calibration set of PP values from independently trained benign models. It counts how many PP values are smaller than suspicious model's WR and uses conformal prediction theory to derive p-value, ensuring provably low false positive rate (e.g., <5%).
- **Core assumption:** Calibration set of benign models is sufficiently large and representative of distribution of models trained without protected dataset, and outliers can be effectively filtered.
- **Evidence anchors:** Abstract mentions conformal prediction and outlier comparison; Section 4.5 defines p-value calculation; novelty highlighted by lack of other papers using conformal prediction for DOV.
- **Break condition:** Becomes overly conservative or prone to false positives if calibration set is too small, poorly chosen, or outlier filtering rate (κ) is not properly tuned.

### Mechanism 3
- **Claim:** Verification process has certified robustness guarantee against input-space perturbations, provided noise follows known distribution.
- **Mechanism:** Adds random noise (e.g., Gaussian) to inputs during verification. Theoretical analysis establishes condition where ownership verification is guaranteed to succeed if WR exceeds function of PP and noise magnitude, making verification resilient to random noise and malicious adversarial perturbations within certified region.
- **Core assumption:** Verification can constrain pixel-level perturbations on verification samples, and added noise follows theoretical distribution used to derive certification bounds.
- **Evidence anchors:** Abstract mentions certified robustness under Gaussian and uniform noise distributions; Section 5 provides theorems and examples deriving mathematical conditions; "DSSmoothing" extends similar concepts to language models.
- **Break condition:** Certification guarantee void if adversarial perturbation exceeds theoretical bound (R) or if noise distribution during verification deviates significantly from assumed model.

## Foundational Learning

- **Conformal Prediction:**
  - **Why needed here:** Core statistical engine that converts raw WR and PP metrics into reliable ownership decision with quantifiable confidence level (e.g., 1-α₀).
  - **Quick check question:** Can you explain how calibration set is used to generate prediction set or p-value in conformal prediction, and what significance level (α) represents?

- **Backdoor Attacks for Dataset Watermarking:**
  - **Why needed here:** "Watermark" is technically backdoor trigger implanted into dataset; understanding this is crucial to grasp why model would have high WR for specific trigger-target pair.
  - **Quick check question:** How does poison-only backdoor attack differ from other attacks, and what property makes it suitable for dataset watermarking without model training access?

- **Certified Robustness & Randomized Smoothing:**
  - **Why needed here:** Paper builds on concepts from certified robustness to provide guarantees; understanding randomized smoothing is key to knowing why adding noise is feature, not bug.
  - **Quick check question:** How does adding random noise to input create robustness "certificate," and what does "certified radius" mean in this context?

## Architecture Onboarding

- **Component map:** Dataset Watermarking Module (pre-processing) -> Model Training Pipeline (generating suspicious and benign models) -> Verification Client (contains Monte Carlo Sampler, Statistics Calculator, Conformal Predictor)
- **Critical path:** Monte Carlo estimation of prediction distributions - generating thousands of noisy versions of single input and querying model for each, making it most computationally expensive step
- **Design tradeoffs:**
  - Calibration Set Size (J) vs. Confidence: Larger set leads to more stable threshold but increases upfront cost of training and maintaining J models
  - Noise Level (σ) vs. Certification Region: Increasing noise magnitude generally expands certified region area but can degrade raw prediction accuracy
  - Trigger Size (R) vs. Detectability: Larger trigger makes watermark more robust (higher WR) but may make it perceptible or affect model's benign accuracy
- **Failure signatures:**
  - High False Positive Rate (FPR): Calibration threshold likely too low due to insufficient/unrepresentative calibration set or aggressive κ value
  - Certification Failure (Low WCA): WR not sufficiently higher than PP threshold, possibly due to weak watermark, small trigger perturbation, or mismatched noise level
- **First 3 experiments:**
  1. Visualize Certified Region (Reproduce Fig 4): Select range of trigger sizes (R) and noise levels (σ), calculate certification bound for each combination, and plot
  2. Calibration Set Sensitivity Analysis: Run full verification pipeline with varying calibration set sizes (e.g., J=10, 50, 100, 200), plot resulting VSR and FPR
  3. Adaptive Attack Stress Test: Implement simple adversarial attack (e.g., PGD) against verification process, attempt to evade detection by perturbing verification samples

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can certified robustness guarantees be established for dataset ownership verification using non-Gaussian or non-uniform smoothing distributions?
- **Basis in paper:** Section 5.2 derives robustness conditions specifically for Gaussian and Uniform distributions "as examples for a better illustration," implying general Theorem 1 remains uninstantiated for other common smoothing functions.
- **Why unresolved:** Different noise distributions alter geometry of certified region and specific likelihood ratio tests required for theoretical proof.
- **What evidence would resolve it:** Derivations of robustness condition (similar to Eq. 15-16) for alternative distributions like Laplace or ℓ₁-bounded noise.

### Open Question 2
- **Question:** How can watermark trigger generation process be optimized to maximize "certified region" area?
- **Basis in paper:** Remark 6 and Figure 4 illustrate trade-off where larger triggers increase WR but decrease certified region area due to higher perturbation magnitude (R).
- **Why unresolved:** Current method uses randomly generated triggers (Section 6.1), potentially leaving performance on Pareto frontier of robustness vs. perturbation size unexplored.
- **What evidence would resolve it:** Optimization algorithm that designs triggers to explicitly maximize area defined by inequality (15), resulting in higher WCA without increasing perturbation size.

### Open Question 3
- **Question:** Can Monte Carlo estimation of Principal Probability (PP) and Watermark Robustness (WR) be made more query-efficient?
- **Basis in paper:** Section 6.1 notes use of 1,024 random noises (M=1024) per sample for estimation, significant computational overhead acknowledged in Section 6.8.
- **Why unresolved:** High query counts increase time and cost of verification process, potentially making it impractical for very large-scale datasets.
- **What evidence would resolve it:** Variance-reduction technique or analytical approximation that achieves comparable accuracy with significantly fewer model queries (e.g., M < 100).

## Limitations

- Theoretical certification bounds depend on specific noise distributions and assume adversarial perturbations remain within bounded regions, which may not hold against adaptive attacks
- Computational cost of Monte Carlo sampling (M=1024 noise perturbations per sample) creates significant verification bottleneck that scales poorly with larger models
- Practical feasibility of ensuring independence between calibration sets and suspicious models in real-world scenarios remains unclear

## Confidence

- **High confidence:** Experimental methodology is well-defined with specific metrics (WSR, VSR, FPR) and reproducible results on standard datasets (GTSRB, CIFAR-10); PP and WR statistical measures are clearly specified with explicit formulas
- **Medium confidence:** Theoretical certification framework based on conformal prediction is sound, but practical conditions for achieving claimed robustness bounds require careful parameter tuning and may not generalize to all threat models
- **Low confidence:** Adaptive attack resistance claims are primarily demonstrated through baseline comparisons rather than exhaustive adversarial analysis; independence assumptions for calibration set are stated but not validated empirically

## Next Checks

1. **Certified region visualization and parameter sensitivity:** Reproduce theoretical certification bounds by systematically varying trigger size R and noise level σ across multiple model architectures; plot resulting certified regions to identify optimal operating point where theoretical guarantees align with empirical performance

2. **Calibration set independence validation:** Design experiments that explicitly test independence assumption by training calibration sets with varying degrees of similarity to suspicious model (same architecture but different seeds vs. different architectures); measure how calibration set composition affects FPR and VSR stability

3. **Adaptive attack benchmark development:** Implement comprehensive suite of adaptive attacks including PGD-based evasion, gradient-based trigger optimization, and model pruning at varying intensities; systematically evaluate how each attack type affects WR, PP distributions, and overall verification success rate to identify true robustness limits of approach