---
ver: rpa2
title: 'OpenStaxQA: A multilingual dataset based on open-source college textbooks'
arxiv_id: '2510.06239'
source_url: https://arxiv.org/abs/2510.06239
tags:
- dataset
- openstaxqa
- such
- content
- textbooks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenStaxQA, a multilingual dataset based
  on 43 open-source college textbooks in English, Spanish, and Polish. The dataset
  is created by scraping end-of-chapter exercises and their solutions from OpenStax
  textbooks, converting MathML to LaTeX, and cleaning the HTML content.
---

# OpenStaxQA: A multilingual dataset based on open-source college textbooks

## Quick Facts
- arXiv ID: 2510.06239
- Source URL: https://arxiv.org/abs/2510.06239
- Authors: Pranav Gupta
- Reference count: 7
- Dataset contains 18,332 problem-solution pairs from 43 open-source college textbooks in English, Spanish, and Polish

## Executive Summary
This paper introduces OpenStaxQA, a multilingual dataset based on 43 open-source college textbooks across English, Spanish, and Polish. The dataset is constructed by scraping end-of-chapter exercises and solutions from OpenStax textbooks, converting MathML to LaTeX, and cleaning HTML content. The authors finetune Llama-7B and Llemma-7B models using QLoRA on this dataset and evaluate performance using GPT-4 as an oracle. The finetuned models show improved performance compared to baselines, with Llemma-7B performing better on OpenStaxQA due to its math-specific design. The study also performs zero-shot evaluation on the AI2RC "challenge dev" dataset to assess transfer learning.

## Method Summary
The methodology involves scraping OpenStax textbooks using Beautiful Soup to extract problem-solution pairs from `<os-problem-container>` and `<os-solution-container>` tags. MathML is converted to LaTeX using the texmath Haskell library, and remaining HTML is processed with html2text. The resulting 18,332 pairs undergo deduplication and are split 70/30 for training and testing. QLoRA finetuning is applied to Llama2-7B-hf and Llemma-7B for 3 epochs with dropout 0.1 and batch size 4 on a V100 32GB GPU. GPT-4 evaluates model outputs on a 5-point scale (Fully Inaccurate to Fully Accurate) for both OpenStaxQA test set and zero-shot on AI2RC dev set.

## Key Results
- Finetuned Llama-7B and Llemma-7B show improved performance on OpenStaxQA test set (mean ratings improving from 1.10 to 1.47)
- Llemma-7B performs better on OpenStaxQA due to its math-specific design
- Zero-shot evaluation on AI2RC shows finetuned models outperform untrained Llama-7B, though Llemma-7B performance degraded (1.53) while Llama-7B improved (1.73)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on domain-specific college textbooks improves model performance on educational reasoning tasks
- Mechanism: QLoRA adapters inject structured STEM knowledge (physics, math, chemistry) into base LLMs through supervised learning on 18,332 problem-solution pairs, enabling better domain alignment
- Core assumption: The improvement stems from domain knowledge acquisition rather than overfitting or task-specific pattern matching
- Evidence anchors: [abstract] "We finetune and evaluate large language models (LLMs) with approximately 7 billion parameters on this dataset using quantized low rank adapters (QLoRa)"; [section 5.2] "mean ratings improving from 1.10 to 1.47 on OpenStaxQA"
- Break condition: If evaluation dataset differs significantly from training distribution (e.g., creative writing vs. quantitative problem-solving), transfer may degrade

### Mechanism 2
- Claim: MathML-to-LaTeX conversion reduces token count and accelerates training/inference for mathematical content
- Mechanism: The texmath Haskell library transforms verbose MathML markup into compact LaTeX notation, decreasing sequence length and computational cost while preserving semantic meaning
- Core assumption: LaTeX representation maintains sufficient mathematical expressiveness for model comprehension
- Evidence anchors: [section 3] "This can also help us reduce the number of tokens per problem and speed up training and inference"; [section 3] "the Haskell library texmath McFarlane [2024] was found to be the most reliable"
- Break condition: Complex mathematical notation with custom macros or non-standard symbols may fail conversion

### Mechanism 3
- Claim: Training on college-level STEM content may improve zero-shot performance on related educational benchmarks
- Mechanism: Domain knowledge transfer occurs when models learn generalizable reasoning patterns from OpenStaxQA that apply to simpler datasets like AI2RC (grade-school level)
- Core assumption: AI2RC improvement reflects genuine transfer rather than data leakage or evaluation artifacts
- Evidence anchors: [abstract] "zero-shot evaluation on the AI2 reasoning challenge dev dataset in order to check if OpenStaxQA can lead to an improved performance"; [section 5.3] "mean rating on this dataset is better than OpenStaxQA... from 1.32 to 1.73"
- Break condition: Llemma-7B fine-tuned showed worse AI2RC performance (1.53) than Llama-7B fine-tuned (1.73), suggesting transfer is model-architecture dependent

## Foundational Learning

- Concept: **QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: Enables fine-tuning 7B parameter models on single V100 (32GB) by freezing base weights and training only low-rank adapter matrices
  - Quick check question: Can you explain why rank-32 adapters require far fewer parameters than full fine-tuning?

- Concept: **MathML vs LaTeX representation**
  - Why needed here: OpenStax uses MathML for web rendering, but LLMs process LaTeX more efficiently; conversion is a pipeline bottleneck
  - Quick check question: Why might a Haskell library outperform JavaScript or XSLT for MathML→LaTeX conversion?

- Concept: **LLM-as-evaluator (GPT-4 oracle)**
  - Why needed here: Human evaluation of 18K technical solutions is impractical; GPT-4 provides scalable but imperfect proxy ratings on 5-point scale
  - Quick check question: What biases might GPT-4 introduce when rating "fully accurate" vs "partially accurate" solutions?

## Architecture Onboarding

- Component map: OpenStax webpages → Beautiful Soup scraper → <os-problem-container> + <os-solution-container> extraction → MathML → texmath → LaTeX conversion → html2text for remaining tags → Deduplication → 18,332 pairs → 70/30 train/test split → QLoRA fine-tuning (Llama2-7B / Llemma-7B) → GPT-4 evaluation (5-class rating)

- Critical path: MathML conversion quality directly impacts token efficiency and model understanding; GPT-4 evaluation prompt determines metric reliability

- Design tradeoffs:
  - Beautiful Soup vs Scrapy: Chose simplicity over parallelization (OpenStax has consistent structure)
  - texmath vs GPT-4 conversion: Cost savings vs potential accuracy loss
  - Llama-7B vs Llemma-7B: General vs math-specialized, but Llemma underperformed on AI2RC (non-LaTeX tasks)

- Failure signatures:
  - Residual MathML tokens in training data indicate conversion gaps
  - Hyperlinks to other textbook sections create context dependency
  - Image-dependent problems not parseable
  - GPT-4 evaluator shows extreme classification bias ("fully accurate"/"fully inaccurate" dominate)

- First 3 experiments:
  1. **Baseline verification**: Run inference with base Llama-7B on OpenStaxQA test split, compare mean rating to paper's 1.10 baseline
  2. **Conversion audit**: Sample 50 MathML→LaTeX pairs, manually verify semantic preservation (check for lost notation or broken equations)
  3. **Transfer test**: Fine-tune on OpenStaxQA, evaluate on held-out subset of AI2RC, check if improvement replicates (1.32→1.73)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed web scraping methodology be generalized to extract high-quality datasets from other open-source platforms with irregular markup, such as OER Commons?
- Basis in paper: [explicit] The authors state they plan to extend this work by "making use of other open source textbook platforms such as OER Commons and Open Textbook Library."
- Why unresolved: The paper highlights that OpenStax was chosen for its consistent HTML structure, whereas other platforms often contain "redundant and erroneous HTML content" or non-standard organization.
- What evidence would resolve it: Successful application of the scraping pipeline on OER Commons to produce a dataset with comparable consistency and tokenization quality to OpenStaxQA.

### Open Question 2
- Question: How effective are NLP and computer vision tools at extracting problem-solution pairs from open-source textbooks available exclusively in PDF format?
- Basis in paper: [explicit] The conclusion notes that existing tools "can be used to parse textbook content... [which] extends to open source textbooks that are only available in PDF format."
- Why unresolved: The current study relied entirely on HTML parsing using Beautiful Soup, and the authors did not test their methodology against static PDF documents which lack semantic tags like `<os-problem-container>`.
- What evidence would resolve it: A comparative study evaluating the accuracy and retention rates of extraction pipelines when applied to PDF versions of the same textbooks used in the current dataset.

### Open Question 3
- Question: Does the GPT-4 oracle's tendency to rate responses as extreme values ("fully accurate" or "fully inaccurate") introduce bias into the evaluation of fine-tuned models?
- Basis in paper: [inferred] The authors observe in Section 5.3 that GPT-4 ratings cluster at the extremes and speculate this might be due to "GPT-4’s limited ability to do accurate multi-class classification."
- Why unresolved: The paper relies entirely on GPT-4 for performance metrics (Table 2 and 3) without validating these specific rating distributions against a human baseline.
- What evidence would resolve it: A correlation analysis comparing GPT-4's ratings on the dataset against human expert evaluations to determine if the lack of "partially accurate" ratings skews the reported model improvements.

## Limitations
- Evaluation methodology relies on GPT-4 oracle, introducing potential bias and subjectivity in rating accuracy
- Zero-shot transfer results show inconsistent behavior between models (Llama-7B improved, Llemma-7B degraded on AI2RC)
- Dataset construction involved significant manual intervention with lack of detailed error analysis of MathML-to-LaTeX conversions

## Confidence
- **High Confidence**: The dataset construction methodology (scraping OpenStax textbooks, extracting problem-solution pairs, deduplication) is well-specified and reproducible. The QLoRA fine-tuning procedure is clearly described with concrete hyperparameters.
- **Medium Confidence**: The observed performance improvements on OpenStaxQA are reliable, but the interpretation of these improvements (domain knowledge acquisition vs. overfitting) requires further validation. The transfer results to AI2RC are less reliable due to inconsistent model behavior.
- **Low Confidence**: Claims about cross-lingual performance and the general applicability of the dataset for multilingual STEM education lack sufficient empirical support. The paper does not report results for Spanish and Polish subsets separately.

## Next Checks
1. **Conversion Quality Audit**: Sample 100 problem-solution pairs from the dataset and manually verify MathML-to-LaTeX conversion accuracy, checking for lost mathematical notation, broken equations, or semantic drift. This validates whether token efficiency gains come at the cost of mathematical expressiveness.
2. **Evaluator Calibration Study**: Run a small-scale human evaluation (10-20 problems) using domain experts to rate solutions on the same 5-point scale, then compare human vs GPT-4 ratings to quantify evaluator bias and determine if GPT-4's extreme rating distribution reflects actual solution quality or evaluator limitations.
3. **Transfer Mechanism Isolation**: Conduct controlled experiments varying the training distribution - train one model on only math problems, another on only science problems, and compare zero-shot performance on AI2RC to determine whether observed transfer is domain-general reasoning or subject-specific knowledge.