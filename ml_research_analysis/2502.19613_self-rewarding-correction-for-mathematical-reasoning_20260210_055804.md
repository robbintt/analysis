---
ver: rpa2
title: Self-rewarding correction for mathematical reasoning
arxiv_id: '2502.19613'
source_url: https://arxiv.org/abs/2502.19613
tags:
- self-rewarding
- reasoning
- arxiv
- correct
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a self-rewarding reasoning framework that enables\
  \ large language models to autonomously generate step-by-step reasoning, evaluate\
  \ their own outputs, and correct errors without external feedback. The approach\
  \ employs a two-stage training process: first, sequential rejection sampling is\
  \ used to collect long chain-of-thought trajectories with self-rewarding and self-correction\
  \ patterns, followed by fine-tuning; second, reinforcement learning with rule-based\
  \ signals further refines the model\u2019s ability to assess correctness and revise\
  \ responses."
---

# Self-rewarding correction for mathematical reasoning

## Quick Facts
- **arXiv ID**: 2502.19613
- **Source URL**: https://arxiv.org/abs/2502.19613
- **Reference count**: 40
- **Primary result**: Achieves 43.4% accuracy on OlympiadBench, outperforming intrinsic self-correction and matching external reward model approaches

## Executive Summary
This work introduces a self-rewarding reasoning framework that enables large language models to autonomously generate step-by-step mathematical reasoning, evaluate their own outputs, and correct errors without external reward models. The approach employs a two-stage training process: first, sequential rejection sampling synthesizes long chain-of-thought trajectories with self-rewarding and self-correction patterns, followed by fine-tuning; second, reinforcement learning with rule-based signals further refines the model's ability to assess correctness and revise responses. Experiments on mathematical reasoning benchmarks demonstrate that this method surpasses intrinsic self-correction and achieves performance comparable to systems relying on external reward models.

## Method Summary
The framework uses a two-stage training process for mathematical reasoning. First, sequential rejection sampling generates N₁=50 initial responses per prompt, then samples N₂=8 self-evaluations, filtering by ground truth, then samples M₁=8/M₂=4 corrections for wrong/correct initial attempts. This creates curated trajectory types (DIFT₁, DIFT₂, DIFT₃) that are fine-tuned with LR=1e-5 for 3 epochs (checkpoint at epoch 1 end). Second, RL with rule-based correctness reward uses DPO (LR=2e-7, η∈{0.1,0.5}) or PPO via veRL, optimizing with correctness as reward and the IFT checkpoint as reference policy.

## Key Results
- Achieves 43.4% accuracy on OlympiadBench (PPO) vs 40.1% (DPO) and 33.3% (intrinsic self-correction baseline)
- Demonstrates improved accuracy in both recognizing correct responses and correcting incorrect ones
- Shows higher final accuracy and lower rates of modifying correct answers to incorrect compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Sequential Rejection Sampling Constructs Behavioral Scaffolds
Long chain-of-thought trajectories that embed both self-evaluation and self-correction patterns provide denser learning signal than single-turn correct answers alone. The process generates N₁ initial responses per prompt, then selectively pairs them with ground-truth-matched self-evaluation tokens, then samples corrections only for trajectories flagged as wrong. This creates three curated trajectory types: wrong→verified wrong→correct (DIFT₁), correct→verified wrong→correct (DIFT₂), and correct→verified correct (DIFT₃). Fine-tuning on these structured sequences teaches the model when to stop (recognize correctness) and when to revise.

### Mechanism 2: Self-Rewarding via Generative Token Prediction
Formulating evaluation as next-token prediction (rather than a separate scalar head) allows a single model to serve as both generator and reward model without architectural changes. The model generates reasoning followed by explicit evaluation tokens like "[VERIFY] correct" or "[VERIFY] wrong." During inference, the evaluation token is sampled from the distribution (not extracted as a probability score), enabling standard autoregressive decoding.

### Mechanism 3: Rule-Based RL Reinforces Correct Stopping Behavior
Reinforcement learning with only outcome-based rewards (correct/incorrect final answer) can enhance self-correction patterns initially learned through supervised fine-tuning, without requiring learned neural reward models. After IFT warmup, PPO or DPO optimizes a KL-regularized objective where the trajectory reward is simply whether the final answer is correct (rule-based, not neural).

## Foundational Learning

- **Concept: Rejection Sampling (STaR/RAFT paradigm)**
  - **Why needed here:** Understanding that the IFT stage is not distillation from a teacher but self-generation + filtering. The model produces many candidates, and only trajectories matching desired patterns are retained for training.
  - **Quick check question:** Given 100 prompts, if the base model achieves 60% accuracy and you sample 10 responses per prompt, approximately how many "wrong→correct" trajectories might you expect if correction succeeds 30% of the time?

- **Concept: Multi-turn MDP Formulation**
  - **Why needed here:** The paper models self-correction as a Markov Decision Process where state includes conversation history (prompt, response, evaluation). This matters for understanding why M-DPO (multi-turn DPO) requires different loss terms than single-turn DPO.
  - **Quick check question:** In the MDP formulation, what constitutes the state s₂ after the first self-evaluation? What action does the model take from this state?

- **Concept: KL-Regularized RL Objective**
  - **Why needed here:** Both PPO and DPO variants in this paper optimize an objective that trades off reward maximization against divergence from the reference policy (the IFT checkpoint). Understanding this explains why the reference model choice matters.
  - **Quick check question:** If η (KL penalty coefficient) is set too high, what behavior would you expect during RL training? Too low?

## Architecture Onboarding

- **Component map:**
  Base Model (Qwen2.5-Math-7B-base or Llama-3-SFT) → [Stage 1: Sequential Rejection Sampling] → [Stage 1: SFT on curated trajectories] → [Stage 2: RL with rule-based rewards] → Final Self-Rewarding Reasoning Model

- **Critical path:** The sequential rejection sampling (Stage 1 data construction) is the most critical and resource-intensive step. If this produces imbalanced data (e.g., too few DIFT₁ trajectories), downstream training will fail.

- **Design tradeoffs:**
  - **Data composition:** More incorrect-first trajectories (|DIFT₁| > |DIFT₃|) → higher true negative rate (better at catching wrong answers) but risk over-correction. More correct-first trajectories → higher true positive rate but risk missing errors.
  - **PPO vs. DPO:** PPO achieves higher final accuracy (43.4% vs. 40.1% on OlympiadBench) but requires more compute. DPO is simpler but can only use ~40-60% of prompts (those with both correct and incorrect trajectories for ranking).
  - **Evaluation token format:** The paper finds no significant difference between "[VERIFY] correct/wrong" vs. "Yes/No" prompts—choose based on downstream integration needs.

- **Failure signatures:**
  - **Reward hacking:** PPO with inflated rewards for wrong→correct transitions causes models to deliberately output wrong first answers (turn-1 drops to 18.6% while final reaches 77.6%).
  - **Conservative collapse:** Late-stage RL training causes models to over-predict "correct" (Δᵢ→c drops while turn-1 rises).
  - **Format failures:** Base models without instruction-following training (e.g., raw Qwen2.5-Math-7B-base) may omit evaluation tokens in ~10% of cases.

- **First 3 experiments:**
  1. **Sanity check on base model capability:** Run sequential rejection sampling on 100 prompts. Verify the base model can generate some correct corrections (non-zero |DIFT₁|). If |DIFT₁| ≈ 0, the base model is too weak—use a stronger base or distillation.
  2. **IFT-only baseline:** Train self-rewarding IFT model on balanced DIFT₁/DIFT₃ data. Measure RM accuracy on held-out set. Confirm true positive rate >85% and true negative rate >40% before proceeding to RL.
  3. **Short PPO run with early stopping:** Train PPO for 100 steps with η ∈ {0.1, 0.5}. Monitor turn-1 accuracy, final accuracy, and Δᵢ→c on validation set. Select checkpoint where final accuracy peaks, not where turn-1 peaks.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can multi-turn reinforcement learning strategies with adjusted rule designs further enhance self-correction ability in late RL training stages? The authors note this could further enhance model performance but implementing multi-turn deep RL training remains challenging.

- **Open Question 2:** Does step-wise self-rewarding correction outperform turn-based correction for mathematical reasoning? The authors suggest extending beyond turn-based correction to step-wise correction may offer more advantages and lead to a more scalable and dynamic approach.

- **Open Question 3:** Why does vanilla rule-based RL fail to enhance self-correction in Llama models while succeeding with Qwen models? The authors hypothesize this may stem from differences in pre-training on mathematical corpora, but the exact cause remains unknown.

- **Open Question 4:** Can algorithms like SimPO that directly optimize log π (rather than log π/πref) consistently improve reward model accuracy in DPO training? The authors note M-DPO training cannot consistently improve reward model accuracy and exploring SimPO is a promising direction.

## Limitations

- **Generalization beyond math:** The framework is extensively validated only on mathematical reasoning tasks, with no empirical validation on non-mathematical domains like code generation or text simplification.

- **Resource intensity:** The sequential rejection sampling phase requires substantial computational resources (sampling 50 initial responses × 8 self-evaluations per prompt), with resource requirements not reported.

- **Evaluation methodology completeness:** While evaluating established math benchmarks, the paper lacks deeper examination of reasoning quality, diversity, or robustness to prompt variations.

## Confidence

- **Self-rewarding architecture effectiveness:** Medium - demonstrates improved performance over intrinsic self-correction baselines, but advantage over external reward models is marginal
- **Sequential rejection sampling as critical component:** High - strong evidence that the two-stage approach is necessary, with clear ablation studies
- **PPO vs DPO tradeoffs:** Medium - PPO achieves higher final accuracy but comparison based on different training procedures makes direct comparison difficult

## Next Checks

1. **Domain transfer validation:** Apply the trained model to code generation tasks (e.g., HumanEval) and text simplification tasks (e.g., TURK), measuring whether self-correction patterns transfer beyond mathematical reasoning.

2. **Efficiency analysis:** Measure wall-clock time, GPU hours, and sample efficiency of sequential rejection sampling compared to alternative approaches like synthetic data generation or few-shot prompting.

3. **Reasoning diversity analysis:** Analyze the diversity of reasoning paths generated by the self-rewarding model versus baselines using metrics like path uniqueness, concept coverage, or expert human evaluation of solution elegance.