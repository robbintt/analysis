---
ver: rpa2
title: Predicting kernel regression learning curves from only raw data statistics
arxiv_id: '2510.14878'
source_url: https://arxiv.org/abs/2510.14878
tags:
- kernel
- gaussian
- data
- page
- hermite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework that predicts kernel
  regression learning curves from only raw data statistics, specifically the empirical
  data covariance matrix and an empirical polynomial decomposition of the target function.
  The key innovation is the Hermite Eigenstructure Ansatz (HEA), which provides an
  analytical approximation of a kernel's eigenvalues and eigenfunctions with respect
  to anisotropic data distributions using Hermite polynomials.
---

# Predicting kernel regression learning curves from only raw data statistics

## Quick Facts
- arXiv ID: 2510.14878
- Source URL: https://arxiv.org/abs/2510.14878
- Reference count: 40
- Primary result: Framework predicts kernel regression learning curves from raw data statistics without constructing kernel matrices

## Executive Summary
This paper presents a theoretical framework that predicts kernel regression learning curves from only raw data statistics, specifically the empirical data covariance matrix and an empirical polynomial decomposition of the target function. The key innovation is the Hermite Eigenstructure Ansatz (HEA), which provides an analytical approximation of a kernel's eigenvalues and eigenfunctions with respect to anisotropic data distributions using Hermite polynomials. The authors prove the HEA for Gaussian data and demonstrate it works well on real image datasets like CIFAR-5m, SVHN, and ImageNet. Using the HEA, they successfully predict K-learning curves without constructing or diagonalizing kernel matrices. They also show that MLPs in the feature-learning regime learn Hermite polynomials in the order predicted by the HEA. The framework demonstrates that an end-to-end theory mapping dataset structure to model performance is possible for nontrivial learning algorithms on real datasets.

## Method Summary
The framework computes empirical data covariance, constructs HEA eigenvalues using level coefficients, estimates target coefficients by projecting onto orthogonalized Hermite polynomials, and plugs these into the KRR eigenframework to predict risk. The key innovation is avoiding kernel matrix construction by analytically approximating the kernel's eigenstructure with respect to the data distribution.

## Key Results
- Proved the HEA for Gaussian data and showed it works well on real image datasets (CIFAR-5m, SVHN, ImageNet)
- Successfully predicted K-learning curves without constructing or diagonalizing kernel matrices
- Demonstrated that MLPs in feature-learning regime learn Hermite polynomials in the order predicted by HEA
- Showed that an end-to-end theory mapping dataset structure to model performance is possible for nontrivial learning algorithms

## Why This Works (Mechanism)

### Mechanism 1: The "Gaussian Enough" Data Approximation
Real-world high-dimensional image datasets can be approximated as anisotropic Gaussian measures for predicting kernel behavior. The authors argue that complex natural data often has principal components that are marginally Gaussian enough such that the kernel eigenfunctions align with Hermite polynomials of the data covariance.

### Mechanism 2: Spectral Separation via Level Coefficient Decay
The kernel eigenstructure is determined by rapid decay of the kernel's "level coefficients" relative to data covariance eigenvalues. A rotation-invariant kernel is approximated as a dot-product kernel on a sphere, with level coefficients that decay rapidly (e.g., exponentially for Gaussian kernels), separating the spectrum into distinct "levels" of polynomials.

### Mechanism 3: Target Function Orthogonalization
To predict learning curves, one must measure the target function's coefficients in the predicted eigenbasis, requiring removal of non-orthogonality caused by non-Gaussian data. The authors use modified Gram-Schmidt process (or QR decomposition) to orthogonalize sampled Hermite polynomials before projecting the target function.

## Foundational Learning

- **Kernel Eigenstructure (Eigenvalues/Eigenfunctions):** The framework rests on knowing the kernel's eigenvalues and eigenfunctions with respect to the data distribution to predict learning curves. Quick check: Can you explain why the kernel eigenvalue magnitude determines the "speed" or "sample complexity" of learning the associated eigenfunction?

- **Hermite Polynomials:** These are the specific polynomial basis functions that serve as approximate eigenfunctions for rotation-invariant kernels on "Gaussian enough" data. Quick check: How does the anisotropy of the data (different γᵢ) modify the form of the multivariate Hermite polynomial compared to the isotropic case?

- **Anisotropic vs. Isotropic Data:** Previous theories often assumed isotropic (spherical) Gaussian data. This paper explicitly handles anisotropic data (varying eigenvalues γᵢ in the covariance matrix Σ), which is critical for real datasets like CIFAR. Quick check: How does the HEA equation λₐ = cₐ |α| ∏ᵢ γᵢ^αᵢ predict that directions of higher variance (γᵢ) are learned faster?

## Architecture Onboarding

- **Component map:** Raw data X → Covariance Estimator (Σ) → Level Coefficient Calculator (cₗ) → HEA Engine (λₐ) → Target Projector (vₐ) → Risk Predictor (Test MSE)

- **Critical path:** The estimation of target function coefficients (vₐ) is the most sensitive step. If the data is not perfectly Gaussian, the naive inner product fails. You must implement the orthogonalization (Gram-Schmidt/QR) step described in Section 5 and D.3.

- **Design tradeoffs:**
  - Analytical vs. Empirical: The paper proposes an analytical shortcut (HEA) to avoid O(N³) kernel matrix diagonalization. The tradeoff is accuracy on simple, non-Gaussian datasets.
  - Truncation: For Laplace/ReLU kernels, cₗ diverges for large ℓ. You must truncate the sum at ℓ ∈ [5, 10] [Section B.3]. This limits the ability to predict learning of very high-order polynomials.

- **Failure signatures:**
  - Spectral Tail Mismatch: If predicted spectrum decays much faster than empirical one, check if using narrow kernel width (violating cₗ decay assumption).
  - Constant Offset in Risk: If risk prediction is consistently offset, "Gaussian enough" assumption may be violated; check coordinate marginals.
  - Divergence: If using Laplace kernel and predicted eigenvalues explode, you forgot to truncate the level coefficients.

- **First 3 experiments:**
  1. Sanity Check (Gaussian Data): Generate synthetic Gaussian data with known covariance. Verify that analytical HEA eigenvalues match numerical diagonalization of kernel matrix perfectly [Figure 2, Col 1].
  2. Real Data "Gaussianity" Test: Take CIFAR-10. Plot marginals of first few PCA components. Compare HEA-predicted learning curve for simple linear target against empirical KRR performance.
  3. Target Coefficient Estimation: Train MLP on Hermite polynomial target. Measure order in which network learns features. Check if aligns with sample complexity implied by HEA eigenvalues (λ⁻¹) [Figure 4].

## Open Questions the Paper Calls Out

1. Can the condition that data is "Gaussian enough" for the Hermite Eigenstructure Ansatz (HEA) to hold be formalized into a rigorous statistical definition? The authors state "Gaussian enough" is "nonrigorous and ill-defined" and suggest this exploration is worthwhile for the statistics community.

2. Does the HEA hold for rotation-invariant kernels that are analytic but do not factor into products of one-dimensional kernels? The authors note they could not disentangle the high effective dimension requirement from the singularity issues of the Laplace kernel using currently studied kernels.

3. Is the inverse-square-root scaling between HEA eigenvalues and optimization time fundamental in the "ultra-rich" feature-learning regime, or is it merely hyperparameter-dependent? The authors observe a λ⁻⁰·⁴ scaling for ζ ≫ 1 but suspect "the 0.4 slope in the ultra-rich regime would change with variation of hyperparameters and is thus not fundamental."

## Limitations

- The "Gaussian enough" approximation lacks rigorous theoretical justification and may fail on datasets with fundamentally different structure (low-dimensional manifolds, discrete distributions).
- Target coefficient estimation is sensitive to noise and polynomial truncation choices, with incomplete analysis of how target complexity affects accuracy.
- The framework assumes rotation-invariant kernels and requires heuristic modifications for kernels with non-exponentially decaying level coefficients.

## Confidence

- **High Confidence:** The analytical derivation of HEA for Gaussian data (Theorems 1 & 2) and the general framework connecting data statistics to learning curves.
- **Medium Confidence:** The "Gaussian enough" approximation for real image datasets and the target coefficient estimation procedure.
- **Low Confidence:** The framework's applicability to non-image datasets, extremely complex targets, and kernels outside the rotation-invariant class.

## Next Checks

1. **Dimensionality Sensitivity Test:** Systematically evaluate HEA accuracy on datasets with varying effective dimensions (e.g., MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100). Measure how the "Gaussian enough" approximation degrades as dimensionality decreases.

2. **Target Complexity Analysis:** Test the framework on targets with varying complexity (polynomial degree, sparsity, noise levels) on synthetic Gaussian data. Quantify how target structure affects coefficient estimation accuracy and learning curve prediction.

3. **Kernel Generalization Study:** Apply HEA to non-exponentially decaying kernels (e.g., Laplace with varying width) and non-rotation-invariant kernels. Document where the approximation breaks down and what modifications are needed.