---
ver: rpa2
title: Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based
  Language Models
arxiv_id: '2508.17734'
source_url: https://arxiv.org/abs/2508.17734
tags:
- layer
- layers
- baseline
- ffns
- importance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the layerwise importance of feed-forward
  networks (FFNs) in Transformer-based language models during pretraining. The authors
  develop an experimental approach that, while maintaining the total parameter count,
  increases FFN dimensions in some layers and completely removes FFNs from other layers.
---

# Layerwise Importance Analysis of Feed-Forward Networks in Transformer-based Language Models

## Quick Facts
- arXiv ID: 2508.17734
- Source URL: https://arxiv.org/abs/2508.17734
- Reference count: 28
- Key finding: Concentrating FFNs in 70% of consecutive middle layers consistently outperforms standard uniform FFN placement across multiple downstream tasks.

## Executive Summary
This study investigates the layerwise importance of feed-forward networks (FFNs) in Transformer-based language models during pretraining. Rather than using pretrained models, the authors train models from scratch with varying sizes (285M, 570M, and 1.2B parameters) and layer counts (12, 24, and 40 layers), systematically removing or expanding FFNs while maintaining total parameter count. The results demonstrate that concentrating FFNs in approximately 70% of consecutive middle layers consistently outperforms standard uniform FFN placement across multiple downstream tasks. Additionally, the study finds that FFNs in the first and last few layers may be redundant, as their functionality can be replaced by FFNs in middle layers.

## Method Summary
The paper employs a controlled experimental approach where FFN dimensions are increased in some layers while completely removed from others, maintaining constant total parameter count. Models are trained from scratch on FineWeb-Edu using LLaMA-style architecture with pre-layer normalization, SwiGLU activation, and rotary positional embeddings. The study tests various FFN concentration ratios (10-90%) positioned in first, middle, or final layers across three model scales (285M/12-layer, 570M/24-layer, 1.2B/40-layer). Layerwise importance is assessed by measuring performance degradation when FFNs are removed from specific layers, with downstream evaluation on knowledge-intensive tasks using relative improvement metrics.

## Key Results
- Concentrating FFNs in 70% of consecutive middle layers consistently outperforms standard uniform FFN distribution across all tested model scales
- FFNs in the first and last few layers may be redundant, with their functionality replaceable by expanded FFNs in middle layers
- Deeper networks (40-layer) show a forward shift in FFN importance distribution, suggesting optimal placement may depend on network depth

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Concentrating FFN parameters in ~70% of consecutive middle layers improves downstream task performance compared to uniform FFN distribution.
- **Mechanism**: When FFN capacity is redistributed from early/late layers to middle layers, the model achieves better performance on knowledge-intensive tasks. Middle-layer FFNs process representations that have been sufficiently contextualized by preceding self-attention layers, making them more effective at knowledge storage and retrieval.
- **Core assumption**: FFNs primarily function as knowledge storage components (key-value memories) as hypothesized in prior work.
- **Evidence anchors**:
  - Middle 70 configuration achieves highest average improvement (+1.29%) across all tasks for 1.2B model
  - Related work corroborates FFN importance in Transformer models (FMR: 0.60)

### Mechanism 2
- **Claim**: FFNs in early and late layers contribute less to model performance and can be functionally replaced by expanded FFNs in middle layers.
- **Mechanism**: Layerwise importance scores show that first and final layers have below-average importance while middle layers have above-average importance, suggesting functional redundancy.
- **Core assumption**: Performance degradation from removing FFNs can be attributed equally among all deactivated layers in that configuration.
- **Evidence anchors**:
  - Layerwise importance scores show positive importance concentrated in middle portions across all model scales
  - FFNs in first and last few layers may be redundant and their functionality can be replaced by FFNs in middle layers

### Mechanism 3
- **Claim**: In deeper networks, the optimal FFN importance distribution shifts earlier in the architecture.
- **Mechanism**: As network depth increases, the importance distribution shifts forward relative to the midpoint, potentially because deeper networks execute more self-attention operations before reaching middle layers, creating over-contextualized representations that FFNs process less effectively.
- **Core assumption**: The observed forward shift reflects functional processing requirements rather than training dynamics artifacts.
- **Evidence anchors**:
  - 40-layer model shifts importance toward the earlier portion of the network
  - Pattern inconsistency in 40-layer experiments where final configuration occasionally underperforms

## Foundational Learning

- **Concept: Pre-Layer Normalization (Pre-LN) Architecture**
  - Why needed here: The paper builds on LLaMA-style architectures using pre-LN; understanding normalization placement is essential for interpreting how FFN modifications affect gradient flow.
  - Quick check question: Can you explain why pre-LN (normalization before sublayer) differs from post-LN in terms of training stability?

- **Concept: SwiGLU Activation Function**
  - Why needed here: The FFN formulations use SwiGLU (Swish-gated linear units); understanding this activation helps interpret the FFN-expanded layer equations.
  - Quick check question: How does SwiGLU differ from standard ReLU-based FFNs in terms of gating and activation?

- **Concept: Relative Improvement (RI) Metric**
  - Why needed here: The paper's core findings rely on RI calculations that normalize performance across accuracy-based and perplexity-based tasks.
  - Quick check question: Why does the RI formula include a sign-correction factor, and how would you interpret a -2% RI on perplexity?

## Architecture Onboarding

- **Component map**:
  - Standard Layer: Self-Attention → LayerNorm → FFN (SwiGLU) → LayerNorm → Residual
  - FFN-expanded Layer: Same structure with expanded intermediate dimension d′_f > d_f
  - FFN-deactivated Layer: Self-Attention only (FFN completely removed)
  - Parameter budget: Total parameters held constant by computing d′_f based on ratio of expanded layers

- **Critical path**:
  1. Select baseline configuration (layers, hidden dim, intermediate dim)
  2. Choose FFN-expanded layer ratio r ∈ {10, 30, 50, 70, 90}%
  3. Choose placement position (first/middle/final)
  4. Calculate d′_f to maintain parameter parity
  5. Train from scratch with modified architecture
  6. Evaluate on downstream tasks and compute RI

- **Design tradeoffs**:
  - Lower ratios (10-30%) → severe performance degradation despite larger individual FFNs
  - Higher ratios (70-90%) → better performance but less parameter concentration benefit
  - Middle placement → consistently strong across scales; final placement → better for some tasks but less consistent
  - Deeper networks → may require adjusting optimal ratio/position

- **Failure signatures**:
  - Training instability with 3×10⁻⁴ learning rate in 40-layer models (required reduction to 1×10⁻⁴)
  - Performance below chance level on some tasks (ARC-c, Winogrande) excluded from analysis
  - Loss spikes observed in deeper model configurations

- **First 3 experiments**:
  1. Replicate the 285M 12-layer baseline with middle-70 configuration; verify RI improvement on HellaSwag and Wikitext.
  2. Ablate the importance metric by testing asymmetric placements (e.g., expand layers 4-11 in a 12-layer model) to validate the equal-attribution assumption.
  3. Test whether the middle-70 advantage persists when scaling beyond 1.2B parameters or with different pretraining corpora.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the observed forward shift in layerwise FFN importance—where deeper models rely more on earlier layers—continue linearly with increasing depth, and is "overcontextualization" of representations the specific causal mechanism?
- **Basis in paper**: Section 6 states, "This progressive forward shift... suggests that... FFNs may become more effective when positioned earlier... This phenomenon might occur because... resulting in overcontextualized representations that FFNs may struggle to process effectively."
- **Why unresolved**: The paper identifies the correlation between depth and the importance shift but relies on a hypothesis for the underlying cause without providing mechanistic proof.
- **What evidence would resolve it**: Mechanistic interpretability studies analyzing representation saturation in deep layers, or testing optimal FFN placement in models with 80+ layers.

### Open Question 2
- **Question**: Does the "middle 70" concentration strategy generalize to non-decoder-only architectures, such as encoder-only (e.g., BERT) or encoder-decoder (e.g., T5) models?
- **Basis in paper**: Section 3 states, "we limit this study to the LLaMA architecture," restricting the analysis to decoder-only models.
- **Why unresolved**: The functional role of FFNs might differ in bidirectional contexts or encoder-decoder attention mechanisms, where information mixing follows different dynamics.
- **What evidence would resolve it**: Replicating the layerwise ablation and concentration experiments on standard encoder and encoder-decoder architectures and comparing the resulting relative improvements.

### Open Question 3
- **Question**: Does the optimal FFN concentration ratio (found to be 70% of layers) remain stable as models scale to significantly larger parameter counts (e.g., 7B+) or deeper architectures?
- **Basis in paper**: Section 5.2 notes that the performance pattern becomes "less consistent in our larger 40-layer experiments," and Section 6 discusses a shift in importance distribution as depth increases.
- **Why unresolved**: The study tested up to 1.2B parameters and 40 layers; it is unclear if the "middle 70" rule is a universal constant or if it scales with model capacity.
- **What evidence would resolve it**: Pretraining and evaluating larger models (e.g., 7B, 70B) with varying FFN concentration ratios to see if the optimal percentage shifts.

## Limitations
- **Attribution assumption**: The layerwise importance analysis assumes equal contribution among deactivated FFNs in a given configuration, which may not reflect true functional importance distribution
- **Single pretraining corpus**: All models trained on FineWeb-Edu only; results may not generalize to other pretraining domains or datasets
- **Training stability**: The 40-layer models required reduced learning rates, suggesting potential architectural sensitivity not fully characterized

## Confidence
- **High confidence**: Middle-70 FFN concentration consistently outperforms uniform distribution across model scales (direct experimental evidence)
- **Medium confidence**: Early/late layer FFN redundancy (based on importance scores but relies on equal-attribution assumption)
- **Medium confidence**: Depth-dependent shift in optimal FFN placement (observed pattern but confounded by different learning rates)

## Next Checks
1. **Ablation study**: Train asymmetric FFN configurations (e.g., expand only layers 4-11 in 12-layer model) to validate the equal-attribution assumption in importance calculation
2. **Cross-corpora validation**: Replicate key experiments (middle-70 vs baseline) using a different pretraining corpus (e.g., C4 or The Pile) to test generalizability
3. **Multi-task robustness**: Evaluate the optimal configurations on generation-focused tasks (e.g., summarization, translation) and multilingual benchmarks to assess whether the middle-70 advantage extends beyond knowledge-intensive tasks