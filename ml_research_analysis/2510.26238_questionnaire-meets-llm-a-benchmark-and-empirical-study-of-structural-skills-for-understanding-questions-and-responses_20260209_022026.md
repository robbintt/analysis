---
ver: rpa2
title: 'Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills
  for Understanding Questions and Responses'
arxiv_id: '2510.26238'
source_url: https://arxiv.org/abs/2510.26238
tags:
- data
- shot
- format
- structural
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces QASU, the first benchmark that systematically\
  \ evaluates large language models\u2019 ability to process questionnaire data across\
  \ different serialization formats and prompting strategies. The study tests six\
  \ structural skills including answer lookup, respondent counting, and multi-hop\
  \ inference using six serialization formats (HTML, JSON, Markdown, XML, plain text,\
  \ and Turtle) and various prompt configurations."
---

# Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses

## Quick Facts
- arXiv ID: 2510.26238
- Source URL: https://arxiv.org/abs/2510.26238
- Reference count: 40
- Large language models show 10-25% accuracy gains from one-shot prompting on questionnaire reasoning tasks

## Executive Summary
This paper introduces QASU, the first benchmark that systematically evaluates large language models' ability to process questionnaire data across different serialization formats and prompting strategies. The study tests six structural skills including answer lookup, respondent counting, and multi-hop inference using six serialization formats (HTML, JSON, Markdown, XML, plain text, and Turtle) and various prompt configurations. Results show that format choice significantly impacts performance, with HTML achieving up to 8.8% higher accuracy than TTL on answer lookup tasks. The study also demonstrates that one-shot prompting consistently outperforms zero-shot approaches, with accuracy improvements of 10-25%. Additionally, self-augmented prompting yields further gains of 3-4% on average by having the model first generate structural hints before answering questions.

## Method Summary
The QASU benchmark evaluates LLM structural understanding on questionnaire data across six tasks: Answer Lookup, Reverse Lookup, Respondent Count, Conceptual Aggregation, Rule-based Querying, and Multi-hop Relational Inference. Five public datasets (Kaggle Healthcare, Mendeley Mental-health, SUS-UTA7, Stack Overflow 2022, ISBAR) are serialized into six formats (HTML, JSON, Markdown, XML, Turtle, and Plain Text). Exact-match accuracy serves as the evaluation metric. The method employs one-shot prompting with templates and self-augmented prompting through a two-stage process. Data preprocessing includes specific obfuscation techniques: rank swapping (5% window) for numerics and 10% perturbation for multiple-choice questions to prevent memorization artifacts.

## Key Results
- HTML serialization format achieves up to 8.8% higher accuracy than TTL on answer lookup tasks
- One-shot prompting consistently outperforms zero-shot approaches with 10-25% accuracy improvements
- Self-augmented prompting yields additional gains of 3-4% on average through structural hint generation

## Why This Works (Mechanism)

### Mechanism 1: Structural Cuing via Markup Formatting
Serialization format choice measurably impacts LLM accuracy on questionnaire reasoning tasks. Markup formats (HTML, JSON, XML) provide explicit delimiters and hierarchical tags that reduce parsing ambiguity, helping models track respondent-question mappings during sequential token processing. Partition marks and format notes act as structural anchors that segment the linearized input. The accuracy differences stem from how easily the model can recover the two-dimensional questionnaire structure from a linear token sequence, not from domain knowledge differences.

### Mechanism 2: In-Context Learning from Worked Examples
Providing a single worked example (one-shot prompting) improves accuracy by 10-25% over zero-shot approaches. The example demonstrates the expected reasoning pattern and output format, reducing the search space for valid responses. For multi-hop tasks, the example shows how to chain intermediate steps rather than jumping to conclusions. Models can generalize the solution pattern from one example to new instances within the same task family.

### Mechanism 3: Self-Generated Structural Scaffolding
Having the model first generate structural hints (self-augmented prompting) yields 3-4% additional improvement. The two-stage process forces explicit schema recognition before reasoning. Stage 1 produces intermediate outputs (critical values, format descriptions, structural summaries) that the model can reference in Stage 2, reducing working memory load and grounding the answer in observed data patterns. The model's self-generated structural description must be accurate for this to work effectively.

## Foundational Learning

- Concept: **Serialization**
  - Why needed here: Questionnaires are inherently two-dimensional (respondents × questions) but LLMs process linear token sequences. Understanding serialization clarifies why format choices matter.
  - Quick check question: Given a 3×2 questionnaire matrix, can you write it as JSON, then as HTML, and identify which structural cues each format preserves?

- Concept: **Exact-Match Evaluation**
  - Why needed here: The benchmark uses deterministic ground truths with no paraphrase tolerance. This differs from open-ended generation evaluation.
  - Quick check question: If a model answers "Respondent 5" when the ground truth is "Respondent 005", should this count as correct under exact-match scoring?

- Concept: **Obfuscation for Leakage Prevention**
  - Why needed here: The paper applies rank swapping and MCQ perturbation to prevent models from exploiting memorized data. This affects how to interpret benchmark scores.
  - Quick check question: Why might a model's performance on the original Stack Overflow survey data differ from its performance on the obfuscated version?

## Architecture Onboarding

- Component map: Raw questionnaire → harmonization → serialization → prompt assembly → LLM inference → answer extraction → exact-match scoring
- Critical path: The serialization and prompt assembly steps account for the largest performance variance per ablation results
- Design tradeoffs:
  - HTML provides best answer lookup accuracy but TTL may be required for linked-data interoperability
  - One-shot prompting requires fitting both example and query within context window, limiting respondent sample size
  - Self-augmentation adds latency (two LLM calls) for modest (3-4%) accuracy gains
- Failure signatures:
  - Format confusion: Model returns respondent IDs that don't exist in the data (hallucinated identifiers)
  - Schema misalignment: Model answers with option codes (A, B, C) instead of full text labels when both are present
  - Partition blindness: Without partition marks, model conflates headers with data rows, returning question text as answers
- First 3 experiments:
  1. Format ablation: Run the same 50 queries across all six serialization formats on a single dataset. Verify whether HTML consistently outperforms TTL by ~8% on answer lookup as reported.
  2. One-shot sensitivity: Compare zero-shot vs. one-shot accuracy on multi-hop inference tasks. Confirm whether the 25% degradation holds for your target model.
  3. Self-augmentation variant test: For each self-augmentation strategy (format explanation, critical values, structural description), measure whether Stage 1 output accuracy correlates with Stage 2 final answer accuracy. Identify which strategy is most reliable.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on questionnaires with hierarchical structures such as skip logic, multi-select grids, matrix questions, and nested sections?
- Basis in paper: Section 2.1 and 3.2 state: "Extensions to missing-data reasoning, temporal questionnaire analysis, and hierarchical skip-logic navigation are natural directions for future benchmarks."
- Why unresolved: QASU focuses exclusively on flat respondent-question matrices; complex instruments with conditional flows remain untested.
- What evidence would resolve it: Extend QASU with hierarchical questionnaire types and measure performance across the same six structural skills.

### Open Question 2
- Question: Why do different LLMs respond differently to identical prompting strategies (e.g., self-format explanation helps GPT-5-mini but not Gemini-2.5-Flash)?
- Basis in paper: Section 5.2 notes: "The self format explanation option delivered strong results in GPT-5-mini, but was less effective for Gemini, hinting that some models may gain more from structural restatement while others already leverage structural cues."
- Why unresolved: The study identifies the phenomenon but does not investigate underlying model architecture or training data differences.
- What evidence would resolve it: Systematic comparison across more model families with controlled prompting variations and analysis of attention patterns.

### Open Question 3
- Question: How does LLM performance degrade when questionnaires contain realistic missing data patterns or incomplete responses?
- Basis in paper: Section 3.3 acknowledges: "Incomplete responses are common in real-world surveys, keeping them in the benchmark would make it harder to attribute errors to the model... This filtering ensures evaluation focuses on reasoning rather than imputation."
- Why unresolved: All entries with excessive missing values were discarded, so performance on incomplete surveys is unknown.
- What evidence would resolve it: Create benchmark variants with controlled missingness rates and measure accuracy on retrieval and reasoning tasks.

## Limitations
- Model access uncertainty: Specific versions (GPT-5-mini, Gemini-2.5-Flash) may not be publicly available or may have evolved
- Underspecified implementation details: Exact content of structural hints and partition marks remains unclear
- Benchmark scope: Focuses on exact-match evaluation without accounting for semantic equivalence
- Obfuscation effects: Noise from rank swapping and MCQ perturbation may obscure true format contribution

## Confidence
- **High confidence**: Format impact findings (HTML vs TTL 8.8% difference), one-shot vs zero-shot comparison (10-25% improvement)
- **Medium confidence**: Self-augmented prompting gains (3-4%) - based on limited dataset testing and relies on accurate Stage 1 generation
- **Low confidence**: Generalizability across model architectures and questionnaire domains - results are specific to tested models and public survey datasets

## Next Checks
1. **Format sensitivity replication**: Test the same six formats on a new questionnaire dataset (not in the original corpus) to verify whether HTML maintains its 8%+ advantage over TTL for answer lookup tasks
2. **Model architecture generalization**: Repeat the one-shot vs zero-shot comparison using both transformer-based and alternative architectures (e.g., state-space models) to determine if the 10-25% improvement is architecture-dependent
3. **Semantic evaluation extension**: Implement paraphrase-tolerant scoring (using embedding similarity thresholds) to assess whether exact-match evaluation underestimates model performance, particularly on multi-hop inference tasks where answer structure varies