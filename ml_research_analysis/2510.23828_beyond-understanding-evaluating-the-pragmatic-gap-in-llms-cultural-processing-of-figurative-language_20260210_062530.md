---
ver: rpa2
title: 'Beyond Understanding: Evaluating the Pragmatic Gap in LLMs'' Cultural Processing
  of Figurative Language'
arxiv_id: '2510.23828'
source_url: https://arxiv.org/abs/2510.23828
tags:
- arabic
- proverbs
- understanding
- idioms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models show a consistent performance decline across
  tasks involving culturally grounded figurative language, with Egyptian Arabic idioms
  proving most challenging, followed by Arabic proverbs, then English proverbs. In
  a new pragmatic use task, accuracy dropped 14 percentage points compared to understanding,
  though providing contextual sentences improved performance by 11 percentage points.
---

# Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language

## Quick Facts
- arXiv ID: 2510.23828
- Source URL: https://arxiv.org/abs/2510.23828
- Reference count: 40
- Key outcome: Large language models show a consistent performance decline across tasks involving culturally grounded figurative language, with Egyptian Arabic idioms proving most challenging, followed by Arabic proverbs, then English proverbs. In a new pragmatic use task, accuracy dropped 14 percentage points compared to understanding, though providing contextual sentences improved performance by 11 percentage points. Models also struggled with connotative meaning, achieving at most 86% agreement with human annotators on idioms with full inter-annotator agreement. To support research, we released Kinayat, the first dataset of Egyptian Arabic idioms designed for both figurative understanding and pragmatic use evaluation. These findings reveal that while LLMs can often interpret figurative meaning, they face major challenges in using it appropriately.

## Executive Summary
This paper evaluates large language models' ability to understand and use figurative language across three cultural contexts: Egyptian Arabic idioms, Arabic proverbs, and English proverbs. The study reveals a significant "pragmatic gap" where models can interpret figurative expressions but struggle to use them appropriately, with accuracy dropping 14 percentage points between understanding and pragmatic use tasks. Contextual sentences improve comprehension by approximately 11 percentage points, and performance follows a clear resource-cultural distance gradient. The authors introduce Kinayat, the first dataset of Egyptian Arabic idioms designed for both understanding and pragmatic use evaluation, addressing a critical gap in Arabic NLP benchmarks.

## Method Summary
The study evaluates 22 large language models on three figurative language datasets: Kinayat (325 Egyptian idioms), Jawaher (198 multidialectal Arabic proverbs), and MAPS (394 English proverbs). Tasks include multiple-choice understanding, contextual understanding with sentence augmentation, negation, explanation generation, proverb completion, pragmatic use (cloze-style), and connotation labeling. Evaluation uses zero-shot prompting via the lm-eval framework with vLLM backend, employing both BERTScore-F1 and LLM-as-a-Judge for generation tasks. Distractors are generated using GPT-4.1 with general and SRL-based prompts, then human-verified for plausibility.

## Key Results
- Models show a 14.07% accuracy drop between understanding (78.52%) and pragmatic use (64.45%) tasks
- Providing contextual sentences improves MCQ understanding by 10.66 percentage points
- Performance follows resource-cultural gradient: English proverbs (86.57%) > Arabic proverbs (76.29%) > Egyptian Arabic idioms (66.57%)
- Only 53-69% of connotation samples achieve full human annotator agreement
- Models show strong understanding of Arabic proverbs (76.29%) despite low completion performance (10.65%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Providing contextual sentences improves figurative language comprehension by approximately 10.66 percentage points.
- **Mechanism:** Contextual sentences reduce semantic ambiguity by grounding idioms in situational use, allowing models to disambiguate between literal and figurative interpretations through learned associations between expressions and their typical discourse contexts. This operates as a form of implicit retrieval augmentation from pre-training.
- **Core assumption:** Models have seen sufficient contextual usage patterns during pre-training to leverage contextual cues for disambiguation.
- **Evidence anchors:**
  - [abstract] "providing contextual idiomatic sentences improves performance by 10.66%"
  - [Section 5.2] "when the sentence containing the idiom was added as context to the MCQ understanding prompt, the average accuracy increased by 10.66% to 89.18%"
  - [corpus] Limited corpus evidence; related work (Jawaher, MAPS) focuses on proverbs rather than idioms-in-context
- **Break condition:** Contextual sentences may not help when the idiom is highly specialized or when models have seen few contextual examples during pre-training.

### Mechanism 2
- **Claim:** A persistent pragmatic gap exists where models can interpret figurative expressions but struggle to use them appropriately, with accuracy dropping 14.07% from understanding to pragmatic use.
- **Mechanism:** Understanding requires recognizing semantic associations, while pragmatic use demands compositional reasoning about social appropriateness, speaker intent, and discourse fit. Models appear to rely more on pattern matching than on simulation of social reasoning.
- **Core assumption:** The gap reflects genuine pragmatic reasoning limitations rather than task format artifacts.
- **Evidence anchors:**
  - [abstract] "accuracy dropped 14 percentage points compared to understanding"
  - [Section 5.2] "average accuracy on pragmatic use across all models was 64.45%, which is 14.07% lower than the average accuracy on the understanding task"
  - [corpus] Related Persian proverb benchmarks (MasalBench, FFE-Hallu) report similar pragmatic reasoning challenges
- **Break condition:** If pragmatic use tasks are reformulated as generation with feedback loops, performance may improve.

### Mechanism 3
- **Claim:** Performance follows a resource-cultural distance gradient: English proverbs > Arabic proverbs > Egyptian Arabic idioms.
- **Mechanism:** Training data representation drives capability; expressions more frequently encountered during pre-training show higher accuracy. Colloquial idioms suffer from both lower resource density and greater cultural specificity.
- **Core assumption:** The hierarchy reflects training data distribution rather than inherent task difficulty.
- **Evidence anchors:**
  - [abstract] "Egyptian Arabic idioms proving most challenging, followed by Arabic proverbs, then English proverbs"
  - [Section 5.1] "Arabic proverbs consistently yield lower performance than English proverbs... Arabic idioms are more challenging than Arabic proverbs (average accuracy of 76.29% vs. 86.57%)"
  - [corpus] Jawaher dataset paper confirms dialectal variation in proverb understanding correlates with resource availability
- **Break condition:** Targeted fine-tuning on underrepresented dialects may flatten this gradient.

## Foundational Learning

- **Concept: Pragmatics vs. Semantics**
  - **Why needed here:** The paper's central finding is a gap between semantic understanding (what does this mean?) and pragmatic use (when should I say this?). Engineers must distinguish these to design appropriate evaluation tasks.
  - **Quick check question:** Can you explain why a model might correctly select an idiom's meaning but incorrectly choose when to use it?

- **Concept: Non-compositional Language**
  - **Why needed here:** Idioms and proverbs have meanings that cannot be derived from constituent words. Understanding this explains why models can't rely on standard semantic composition.
  - **Quick check question:** Why might a literal translation of "don't sell water in the village of water-sellers" fail to convey the intended advice?

- **Concept: Inter-annotator Agreement as Quality Signal**
  - **Why needed here:** The paper filters connotation evaluation to items with 100% human agreement, recognizing that subjective judgments introduce noise. This is critical for evaluation design.
  - **Quick check question:** Why would evaluating models on connotation labels where humans disagree lead to misleading conclusions?

## Architecture Onboarding

- **Component map:**
  - Dataset preparation -> Distractor generation (GPT-4.1 general + SRL prompts) -> Human verification -> Evaluation harness (lm-eval + vLLM) -> Multiple-choice scoring (log-likelihood) / Generation evaluation (BERTScore + LLM-as-Judge)

- **Critical path:**
  1. Dataset preparation → incorrect explanation generation (general + SRL-based distractors)
  2. Human verification of distractor quality (0-2 scale, average 1.68-1.72)
  3. Zero-shot evaluation across all tasks
  4. For pragmatic use: model-in-the-loop sentence generation → human post-editing → distractor selection

- **Design tradeoffs:**
  - MCQ format enables scalable evaluation but may underestimate understanding when distractors are implausible; SRL-based distractors improve plausibility but increase pipeline complexity
  - Restricting connotation evaluation to 100% agreement samples reduces noise but may underrepresent natural ambiguity
  - Using LLM-as-a-Judge introduces potential judge bias; cross-evaluation mitigates but does not eliminate

- **Failure signatures:**
  - Positional selection bias: models favor first options (91.95% vs. 79.65% when correct always first); randomization required
  - Memorization vs. reasoning: high proverb completion in English (75.43%) but low in Arabic (10.65%) suggests memorization, yet understanding remains high, indicating models can reason without memorization
  - Connotation subjectivity: only 53-69% of samples achieve full annotator agreement

- **First 3 experiments:**
  1. Replicate the pragmatic use task with your target model on a 50-idiom subset; compare understanding vs. pragmatic accuracy gap
  2. Test context augmentation by evaluating MCQ performance with and without idiomatic sentences; measure improvement delta
  3. Evaluate connotation labeling on high-agreement samples; if accuracy is below 85%, investigate whether the gap stems from semantic or cultural understanding failures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do LLMs perform on pragmatic use tasks involving free-form generation of figurative expressions compared to the constrained multiple-choice selection used in this study?
- **Basis in paper:** [explicit] The Conclusion states future work should "assess pragmatic use in free-form generation to better capture the ability of models to produce culturally and contextually appropriate figurative expressions."
- **Why unresolved:** The current study measured pragmatic use via a cloze-style task (filling in the blank), which does not require the model to generate the appropriate context or initiate the figurative language independently.
- **What evidence would resolve it:** A new evaluation benchmark where models must generate complete, contextually appropriate sentences using specific idioms, assessed by native speakers for cultural fit.

### Open Question 2
- **Question:** Does the observed "pragmatics gap" between understanding and use extend to other forms of figurative language such as sarcasm, irony, and metaphor?
- **Basis in paper:** [explicit] The Limitations section notes the study is "limited to idioms and proverbs" and states, "Future work should extend the evaluation to these additional forms."
- **Why unresolved:** Idioms and proverbs have fixed meanings, whereas irony and sarcasm are highly context-dependent; it is unclear if the specific challenges identified for fixed expressions generalize to these dynamic forms.
- **What evidence would resolve it:** Applying the paper's three-task evaluation framework (Understanding, Pragmatic Use, Connotation) to datasets of Arabic irony and metaphors.

### Open Question 3
- **Question:** To what extent does model performance on figurative language degrade for low-resource Arabic dialects compared to the high-resource Egyptian dialect studied here?
- **Basis in paper:** [explicit] The Limitations section highlights the exclusive focus on "Egyptian idioms" and notes the "necessity for similar resources and benchmarks in other Arabic dialects."
- **Why unresolved:** Egyptian Arabic is relatively high-resource; the 14% pragmatic gap found in this study may widen significantly or manifest differently in dialects with less representation in training data.
- **What evidence would resolve it:** Constructing and evaluating equivalent idiom datasets for low-resource dialects (e.g., Mauritanian, Sudanese) to compare the magnitude of the pragmatics gap.

### Open Question 4
- **Question:** How strongly do automated metrics like BERTScore and LLM-as-a-Judge correlate with human judgments of semantic nuance and cultural appropriateness for Arabic figurative language?
- **Basis in paper:** [explicit] The Limitations section states that "The automated metrics used... may not fully capture semantic nuances or cultural appropriateness... Human annotation would provide a more comprehensive assessment."
- **Why unresolved:** The study relied on these metrics for the generation task without validating their alignment with human intuition regarding cultural correctness.
- **What evidence would resolve it:** A correlation analysis between the automated scores reported in the paper and human ratings from native Arabic speakers on the same generated explanations.

## Limitations

- The pragmatic use task relies on human-generated contextual sentences that were post-edited after LLM generation, potentially confounding model reasoning with human curation quality
- Connotation evaluation is restricted to items with perfect inter-annotator agreement, potentially underrepresenting natural ambiguity in figurative language interpretation
- The dataset hierarchy (English > Arabic > Egyptian Arabic) may reflect training data distribution rather than inherent task difficulty, though this remains uncertain

## Confidence

**High Confidence:** The 14 percentage point gap between understanding and pragmatic use (64.45% vs 78.52%) is robustly supported by the data across all models and language variants. The contextual sentence improvement (10.66%) and the resource-cultural distance gradient are similarly well-established.

**Medium Confidence:** The claim that SRL-based distractors are more plausible than general prompts has moderate support (1.72 vs 1.68 plausibility scores) but requires more extensive human evaluation to confirm. The finding that memorization does not drive understanding (high Arabic proverb understanding despite low completion) is suggestive but based on limited data points.

**Low Confidence:** The assertion that the pragmatic gap reflects genuine reasoning limitations rather than task format artifacts remains unproven. The restricted connotation evaluation to high-agreement samples may not generalize to naturally occurring ambiguity.

## Next Checks

1. **Task Format Validation:** Replicate the pragmatic use task using a generation-based format where models must generate appropriate usage contexts rather than select from options. Compare results to determine if the gap persists across task formats.

2. **Cross-Cultural Generalization:** Evaluate the same model suite on figurative language from a resource-rich non-English culture (e.g., Spanish or French idioms) to test whether the resource-cultural distance gradient holds beyond the English-Arabic axis.

3. **Fine-tuning Impact:** Fine-tune a representative open-source model on Kinayat data and measure changes in both understanding and pragmatic use performance. This would test whether the gap stems from representation deficiency versus inherent reasoning limitations.