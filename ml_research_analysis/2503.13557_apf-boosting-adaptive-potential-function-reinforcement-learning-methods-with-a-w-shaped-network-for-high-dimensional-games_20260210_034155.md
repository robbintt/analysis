---
ver: rpa2
title: 'APF+: Boosting adaptive-potential function reinforcement learning methods
  with a W-shaped network for high-dimensional games'
arxiv_id: '2503.13557'
source_url: https://arxiv.org/abs/2503.13557
tags:
- input
- residual
- out-u1
- out-u2
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes APF+, an extension of adaptive potential function
  (APF) reinforcement learning to high-dimensional environments by combining it with
  a W-shaped network (W-Net) for state representation. The W-Net encodes both static
  background and dynamic moving entities in game frames by concatenating outputs from
  two U-Nets - one capturing expected states and another capturing residuals.
---

# APF+: Boosting adaptive-potential function reinforcement learning methods with a W-shaped network for high-dimensional games

## Quick Facts
- arXiv ID: 2503.13557
- Source URL: https://arxiv.org/abs/2503.13557
- Reference count: 40
- Outperforms baseline DDQN in 14/20 Atari games using W-Net for state representation

## Executive Summary
APF+ combines adaptive potential function (APF) reinforcement learning with a W-shaped network (W-Net) to enable reward shaping in high-dimensional Atari games. The W-Net separates game frames into static background and dynamic residual components using two U-Nets, creating a compact 260-dimensional embedding. This embedding feeds into the APF module, which calculates state potentials based on trajectory frequency statistics, providing a learning signal that accelerates convergence without oracle knowledge. The method achieves performance comparable to APF variants using ground-truth game-state information while operating directly on pixel inputs.

## Method Summary
APF+ addresses the challenge of applying adaptive potential function methods to high-dimensional pixel-based games by introducing a W-shaped network for state representation. The W-Net processes input frames through two U-Nets: the first reconstructs the expected background state, and the second encodes the residual (moving entities) after subtracting the background prediction. The concatenated bottleneck features form a 260-dimensional embedding that captures both static and dynamic game information. This embedding feeds into an APF module that maintains trajectory buffers to compute state potentials based on visitation frequencies from good and bad trajectories, which are then used to shape rewards through potential-based reward shaping. The shaped rewards accelerate learning in a downstream DDQN agent.

## Key Results
- APF-WNet-DDQN outperforms baseline DDQN in 14/20 Atari games tested
- APF-WNet-DDQN outperforms APF-STDIM-DDQN in 13/20 games
- Achieves comparable performance to APF-ARI-DDQN which uses ground-truth game-state information
- W-Net effectively captures key game information from pixel frames alone

## Why This Works (Mechanism)

### Mechanism 1: Static-Dynamic State Factorization
The W-Net architecture separates visual input into a static "expected state" and a dynamic "residual state" by passing input through a first U-Net trained to reconstruct the average background, then encoding the residual through a second U-Net. This factorization creates a more effective representation for value estimation than encoding both components jointly.

### Mechanism 2: Statistically-Grounded Reward Shaping
APF maintains trajectory buffers ranked by reward quality, defining "good" (top 20%) and "bad" trajectories. For each state, the potential equals (N_good - N_bad) / (N_good + N_bad), providing a learning signal that accelerates convergence without requiring oracle knowledge of state values.

### Mechanism 3: Embedding-Space Density Estimation
The W-Net projects high-dimensional pixels (208×160×3) into a compact 260-dimensional embedding, making trajectory-based density estimation computationally tractable. This projection preserves semantic features necessary to distinguish high-value states from low-value states.

## Foundational Learning

**Concept: Potential-Based Reward Shaping (PBRS)**
- Why needed: Ensures additional reward signal doesn't change optimal policy, only learning speed
- Quick check: If potential function Φ(s) is constant for all states, what is shaping reward F(s,s')? (Answer: 0)

**Concept: U-Net Architecture (Encoder-Decoder)**
- Why needed: Understanding standard U-Net skip connections clarifies why authors removed them in U1 to force compression of "expected" features
- Quick check: Why do standard U-Nets use skip connections? (Answer: To recover fine-grained spatial details lost during downsampling)

**Concept: Off-Policy Value Learning (DDQN)**
- Why needed: Understanding how Q(s,a) relates to V(s) and Advantage A(s,a) to see how shaped reward R' updates value estimates differently than environmental reward R
- Quick check: In a Dueling Network, why separate V(s) and A(s,a)? (Answer: To accelerate learning in states where actions don't affect value significantly)

## Architecture Onboarding

**Component map:** Input -> W-Net (U1 + U2) -> Embedding (260 dims) -> Trajectory Buffer -> APF Network -> Reward Shaper -> DDQN Agent

**Critical path:**
1. Pre-train W-Net on random agent rollouts (30k frames) for stable embeddings
2. Initialize Trajectory Buffer and APF Network
3. Run DDQN training loop with periodic APF updates from trajectory samples

**Design tradeoffs:**
- W-Net Skip Connections: Removing them forces U1 to learn low-frequency "background" but results in blurry reconstructions
- Trajectory Buffer Size: Small buffers (2000 trajectories) save memory but reduce statistical significance

**Failure signatures:**
- Zero Residual: If U1 reconstructs input perfectly, U2 has nothing to encode
- APF Collapse: If potentials don't diverge between good and bad trajectory sets
- Catastrophic Forgetting: If W-Net isn't frozen during DDQN training, embedding space shifts

**First 3 experiments:**
1. Visual Reconstruction Check: Display Input, Out-U1 (Background), Residual, and Out-U2 to verify dynamic entities appear only in Residual/Out-U2
2. Static Ablation: Train agent using only U1 embedding vs. only U2 embedding to isolate static vs. dynamic feature contributions
3. APF vs. Baseline: Compare DDQN with shaped reward vs. DDQN with intrinsic reward based on prediction error

## Open Questions the Paper Calls Out

**Open Question 1:** Does APF+W-Net provide similar performance benefits when applied to policy-gradient or actor-critic architectures compared to value-based DDQN? The study limited evaluation to DDQN to isolate augmentation effects.

**Open Question 2:** Can large language models effectively estimate potential values to reduce data storage requirements for trajectory replay buffers? Current method is memory-constrained by trajectory storage needs.

**Open Question 3:** Can APF-WNet-DDQN be adapted to handle "hard exploration" games like Montezuma's Revenge or Pitfall? These games were excluded because all methods showed training failure.

## Limitations
- Lacks ablation studies on W-Net architecture itself versus simpler alternatives
- Trajectory buffer size (2000) and "good" trajectory threshold (20%) appear arbitrary without sensitivity analysis
- Claims about embedding space preserving sufficient information lack rigorous validation

## Confidence

**High Confidence:** Potential-based reward shaping mechanism and theoretical foundation in preserving optimal policies

**Medium Confidence:** W-Net's ability to effectively separate static and dynamic features (qualitative assessment could be more rigorous)

**Medium Confidence:** Claim that embedding space preserves sufficient information for control task (260-dimensional size chosen empirically)

## Next Checks

1. **Ablation Study:** Replace W-Net with standard CNN encoder and compare performance to isolate contribution of specific W-Net architecture

2. **Trajectory Buffer Sensitivity:** Systematically vary buffer size (100, 500, 2000, 5000) and "good" trajectory threshold (10%, 20%, 30%) to test robustness

3. **Generalization Test:** Evaluate trained APF-WNet-DDQN agent on unseen Atari games not in training set to assess representation generalization