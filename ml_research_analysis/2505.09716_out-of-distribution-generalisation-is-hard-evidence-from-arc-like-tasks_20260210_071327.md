---
ver: rpa2
title: 'Out-of-distribution generalisation is hard: evidence from ARC-like tasks'
arxiv_id: '2505.09716'
source_url: https://arxiv.org/abs/2505.09716
tags:
- data
- sets
- networks
- algorithms
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates out-of-distribution (OOD) generalization
  in neural networks using two novel data sets derived from ARC-like tasks. The authors
  propose that OOD generalization requires compositional learning, where algorithms
  must extract environment-invariant properties and transfer them to novel inputs.
---

# Out-of-distribution generalisation is hard: evidence from ARC-like tasks

## Quick Facts
- arXiv ID: 2505.09716
- Source URL: https://arxiv.org/abs/2505.09716
- Reference count: 23
- The study shows that while engineered biases can achieve perfect OOD performance on specific tasks, standard neural networks fail to generalize systematically across novel combinations of features.

## Executive Summary
This paper investigates out-of-distribution (OOD) generalization in neural networks using two novel ARC-like tasks: Translate and Rotate. The authors argue that OOD generalization requires compositional learning, where algorithms must extract environment-invariant properties and transfer them to novel inputs. They test three common architectures (MLP, CNN, Transformer) and two novel axial pointer networks on these datasets. Results show that while standard networks fail to achieve OOD generalization, the axial pointer networks succeed on one task but not the other, highlighting the importance of task-specific biases. The study concludes that engineered biases can lead to apparent OOD generalization without true compositional understanding, emphasizing the need for interpretable latent feature spaces and diverse OOD benchmarks.

## Method Summary
The authors created two world models based on ARC-like tasks: Translate (shifting objects) and Rotate (rotating objects) on 32x32 grids with 4 colors. They defined concept variables (CVs) including Shape, Color, and Action, each with 2 values, creating 8 possible combinations. Training sets contain 4 combinations, while test sets are defined by "Distance" from training: Distance 0 (same CVs), Distance 1 (1 CV different), and Distance 2 (2 CVs different). The paper tests standard architectures (MLP, CNN, Transformer) and two novel axial pointer networks designed with specific biases for copying pixel rows/columns. All models are matched for parameter count and trained to predict transformed images given input images and action bits.

## Key Results
- Standard architectures (MLP, CNN, Transformer) fail to generalize OOD, showing performance near chance levels on Distance 1 and 2.
- The Axial Pointer Linear Network (APLN) achieves 100% OOD generalization on the Translate task through architectural bias for row/column copying.
- APLN fails on the Rotate task, demonstrating that task-specific biases cannot substitute for true compositional understanding.

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Inductive Bias (APLN)
The Axial Pointer Linear Network (APLN) achieves 100% OOD generalization on the Translate task not by learning abstract compositional rules, but because its architectural bias (row/column scanning) matches the specific mechanics of translation. The APLN uses an attention mechanism that discovers the destination column and then the row for pixel copying, enforcing a "row-column grouping" bias. Since translation preserves the relative order of pixels in rows and columns, this hard-coded bias aligns perfectly with the task's requirements. This mechanism fails immediately if the transformation disrupts the axial structure of the data (e.g., Rotation).

### Mechanism 2: Feature Conflation in Standard Architectures
Standard architectures (MLP, CNN, Transformer) fail to generalize OOD because they conflate object attributes (shape, color) with action rules during training, effectively memorizing specific input-output mappings rather than isolating task-invariant features. Neural networks tend to minimize error using statistical correlations available in the training set. If specific shapes always appear with specific translations during training, the network learns a combined feature rather than distinct concepts. This mechanism implies that simply scaling data or parameters without changing the architecture or objective will not resolve the conflation.

### Mechanism 3: Pointer-Based Copying vs. Representation Learning
High OOD performance in pointer networks can be achieved via direct pixel-to-pixel mapping strategies that bypass the need for abstract object representations. The pointer network layer outputs a probability distribution over input indices for every output pixel, allowing the network to "copy" input pixels to output locations. Successful OOD networks use "pixel-copying strategies" (grouping pixels) rather than representing the "object" as a distinct entity. This mechanism breaks down if the task requires reasoning about occlusion or object permanence where simple pixel copying from the visible input is insufficient.

## Foundational Learning

**Concept: Systematicity**
Why needed here: The paper defines the challenge as "systematicity"â€”the ability to recombine known parts and rules. Understanding this distinction is crucial to see why high accuracy on Distance 0 is trivial, while Distance 2 is a test of recombination.
Quick check question: Can you explain the difference between interpolating between data points and composing known features to solve a novel task?

**Concept: Inductive Bias**
Why needed here: The paper's core argument relies on contrasting "learning" compositionality vs. "engineering" it via architectural biases (APLN). You must understand how an architecture can be pre-wired to prefer certain solutions (like row-copying).
Quick check question: How does the constraint of "axial attention" change the hypothesis space available to the model compared to standard attention?

**Concept: Disentangled Representation**
Why needed here: The authors argue true OOD requires "task-invariant, and composable input features." This is the standard of success against which the networks are measured.
Quick check question: If a network represents a "moving red square" as a single vector rather than vectors for [red], [square], and [motion], why does that hurt OOD performance?

## Architecture Onboarding

**Component map:**
Input (Image + Action Bit) -> Backbone (MLP/CNN/Transformer/gMLP) -> Pointer Network head (maps input indices to output indices)

**Critical path:**
The transition from the gMLP backbone to the Pointer Network head. This is where abstract features must be converted into concrete spatial indices. If the backbone fails to separate the "Action" bit signal from the "Image" features, the pointer head will receive corrupted instructions.

**Design tradeoffs:**
APLN: High efficiency and Translate performance; rigid structure fails on complex geometry (Rotate)
APN: More flexible; potentially better for rotation but struggles with data efficiency (needs more parameters)
Standard NNs: Efficient interpolation; fails systematicity entirely

**Failure signatures:**
"Shape-Action Conflation": Network outputs correct shape but wrong action (or vice versa). Indicates reliance on training correlations rather than logic.
"Blob Copying": Pointer visualizations show random or localized noise rather than coherent geometric transformations. Indicates lack of global structure understanding.
"Distance 0 Success, Distance 1 Failure": Classic sign of memorization/interpolation without composition.

**First 3 experiments:**
1. Baseline Probe: Train the MLP/CNN on Distance 0, test on Distance 1 and 2. Confirm the "flatline" OOD performance described in the paper.
2. Bias Validation: Train APLN on the Rotate task. Verify that the specific row/column bias causes performance to drop, proving the success on Translate was structural, not "learned" intelligence.
3. Latent Inspection: Visualize the pointer head of a successful APLN model on a Translate task. Verify visually that the "blue" pixels in the visualization follow row/column patterns rather than object contours.

## Open Questions the Paper Calls Out

**Open Question 1**
How do current neural architectures perform on world models featuring compositional actions (combinations of smaller actions) rather than single atomic actions? The authors list as a limitation that they "do not explore world models with compositional actions," noting that such an expansion is needed to test language-based algorithms.

**Open Question 2**
Can modern neurosymbolic or modular deep neural networks achieve true OOD generalization on these ARC-like tasks without relying on pixel-copying inductive biases? The authors state that "limited number of algorithms tested" is a limitation and propose future work should include "a much larger number... of bespoke, OOD generalisation-orientated, networks."

**Open Question 3**
What quantitative metrics can robustly verify that an algorithm has learned compositional latent features rather than just achieving high OOD performance through task-specific biases? The authors argue that OOD performance is insufficient to confirm compositionality and advocate for "analysis of latent feature structure," relying currently on manual visualization of pixel-copying mechanisms.

## Limitations
The study uses a simplified world model with only 4 colors and single objects, which may not capture the full complexity of ARC-like reasoning. The failure of APLN on Rotate could stem from task complexity rather than the fundamental limitation of engineered biases. The paper only tested three standard architectures and two custom pointer networks, leaving the capabilities of other specialized compositional architectures unknown.

## Confidence
High Confidence: APLN's architectural bias enables perfect OOD performance on Translate; standard architectures fail to generalize OOD.
Medium Confidence: APLN's failure on Rotate demonstrates that engineered biases cannot substitute for compositional learning; pointer networks rely on pixel-copying rather than abstract representations.
Low Confidence: The claim that APLN's failure on Rotate proves it lacks true compositional understanding (could be task complexity); the specific mechanism of feature conflation in standard networks.

## Next Checks
1. Evaluate APLN on a new transformation task (e.g., Scale or Shear) that preserves neither row/column structure nor full geometric rotation to test the limits of architectural bias.
2. Perform ablation studies where APLN is trained with partial row/column preservation to measure performance degradation and validate the importance of complete axial structure.
3. Increase the number of objects, colors, and shapes in the world model while maintaining the same task structure to test whether APLN's success remains robust under more complex visual reasoning requirements.