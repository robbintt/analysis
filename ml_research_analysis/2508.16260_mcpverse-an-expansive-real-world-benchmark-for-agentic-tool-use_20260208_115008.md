---
ver: rpa2
title: 'MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use'
arxiv_id: '2508.16260'
source_url: https://arxiv.org/abs/2508.16260
tags:
- tools
- tool
- task
- evaluation
- mode
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MCPVerse, a benchmark for evaluating large
  language models' tool-use capabilities. Existing benchmarks suffer from limited
  realism and constrained action spaces, often relying on synthetic tools and restricted
  API sets.
---

# MCPVerse: An Expansive, Real-World Benchmark for Agentic Tool Use

## Quick Facts
- arXiv ID: 2508.16260
- Source URL: https://arxiv.org/abs/2508.16260
- Authors: Fei Lei; Yibo Yang; Wenxiu Sun; Dahua Lin
- Reference count: 28
- Key outcome: Introduces MCPVerse benchmark with 250 real-world tasks and 552+ tools, showing top model Claude-4-Sonnet achieves only 44.2% success rate in most challenging setting.

## Executive Summary
MCPVerse addresses fundamental limitations in existing LLM tool-use benchmarks by integrating over 550 real-world, executable tools via the Model Context Protocol, creating an expansive action space exceeding 147k tokens. The benchmark employs outcome-based evaluation with real-time ground truth for time-sensitive tasks and tests models across three complexity levels and three evaluation modes (Oracle, Standard, Max-Scale). Experimental results reveal that while most models' performance degrades with larger tool sets, agentic models like Claude-4-Sonnet can leverage expanded exploration spaces to discover alternative solution paths, though even the best model achieves only 44.2% success rate in the most challenging setting.

## Method Summary
The benchmark evaluates LLM tool-use capabilities via MCP protocol across 250 tasks spanning Information Retrieval and System Operation types, three complexity levels (L1: 1-2 steps, L2: ~5 steps, L3: >5 steps), and three evaluation modes with different tool-mounting strategies. Real-world tools are exposed through 65 MCP servers (552+ tools total), with tasks requiring metadata about required MCPs, complexity, time-sensitivity, and ground truth. Outcome-based success rate serves as primary metric, using hybrid evaluation combining LLM-as-judge for textual answers and automated scripts for state changes. The agentic loop built on CAMEL framework executes tool calls until termination, with temperature settings varying by model family.

## Key Results
- Most models show performance degradation as tool set size increases, with average success rate dropping from 55.48% (Oracle) to 45.74% (Standard) to 39.86% (Max-Scale)
- Agentic models like Claude-4-Sonnet can leverage expanded exploration spaces to improve accuracy, achieving 44.2% success rate in Max-Scale mode
- Prompt-based function calling causes catastrophic performance drops for some models, with Claude-4-Sonnet's success rate falling from 62.28% to 35.55% due to hallucination rates exceeding 70%
- Task complexity strongly correlates with failure rates, with L3 tasks (>5 steps) showing significantly lower success rates than L1/L2 tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agentic models can leverage expanded tool spaces to discover alternative solution paths unavailable under constrained settings
- Mechanism: When blocked on canonical path (e.g., authentication format mismatch), models with access to larger toolsets can pivot to alternative tools that accomplish same goal through different interfaces—behavior termed "hacking"
- Core assumption: Models possess sufficient exploration capacity to search larger action space and identify viable alternatives before exhausting context or step budgets
- Evidence anchors: [abstract] "agentic models, such as Claude-4-Sonnet, can effectively leverage expanded exploration spaces to improve accuracy"; [section 4.2.3, Figure 4] Case study shows Claude-4-Sonnet pivoting from blocked `validate`/`filter` path to successful `fetch` alternative in Standard mode

### Mechanism 2
- Claim: Outcome-based evaluation captures multi-path task completion more robustly than trajectory-matching methods
- Mechanism: By evaluating final state/answer equivalence rather than specific tool sequences, benchmark avoids penalizing valid alternative approaches—particularly important when 552 tools enable multiple solution trajectories
- Core assumption: LLM-as-judge can reliably assess semantic equivalence for textual outputs; evaluation scripts accurately verify environmental state changes
- Evidence anchors: [section 3.5] "multiple valid action trajectories may exist... the final outcome (or end state) must align with the ground-truth specification"; [section 2.2] Prior benchmarks confined evaluation to "correctness of the selected tool name and its parameters, rather than the functional outcome"

### Mechanism 3
- Claim: Native function calling substantially outperforms prompt-based function calling for complex toolsets, particularly for models whose training aligned with specific function-calling templates
- Mechanism: Prompt-based approaches require models to parse tool schemas from system prompts and generate correctly formatted calls without structured guidance native APIs provide—leading to hallucination when template formats mismatch training distributions
- Core assumption: Performance gap stems from format mismatch rather than fundamental capability differences
- Evidence anchors: [section 4.2.4, Table 3] Claude-4-Sonnet drops from 62.28% to 35.55% in Oracle mode with prompt-based calling; hallucination rate exceeded 70%; [section 4.2.4] "substantial mismatch between the function-calling templates in our prompt and those from the model's original training"

## Foundational Learning

- Concept: **Model Context Protocol (MCP)**
  - Why needed here: MCPVerse built entirely on MCP—standardized protocol for LLM-tool interaction enabling dynamic tool discovery. Understanding MCP's request/response structure prerequisite to interpreting benchmark architecture
  - Quick check question: How does MCP differ from traditional REST API calling from an LLM's perspective?

- Concept: **Context Budget Allocation**
  - Why needed here: Paper explicitly partitions context between tool schemas (44k-147k tokens) and prompt/response (20k tokens). Models with insufficient windows cannot participate in Max-Scale evaluation
  - Quick check question: If model has 128k context window and tool schemas consume 100k tokens, how many turns of interaction remain assuming 2k tokens per turn?

- Concept: **Agentic Evaluation vs. Static Benchmarking**
  - Why needed here: MCPVerse evaluates multi-step tool invocation sequences with real execution, not single-turn function selection. Shifts failure modes from "wrong tool name" to "incomplete task execution"
  - Quick check question: What information would outcome-based evaluator need that trajectory-based evaluator would not?

## Architecture Onboarding

- Component map: Task selection → mode-specific tool mounting → agentic loop execution → outcome extraction → evaluation scoring
- Critical path: The execution loop is highest-risk component—tool invocation failures cascade into task failures
- Design tradeoffs:
  - Scale vs. Accessibility: Max-Scale mode (147k tokens) excludes most models but reveals emergent behaviors
  - Retrieval vs. Full Mounting: Paper intentionally excludes retrieval to avoid restricting exploration; retrieval experiments showed consistent performance degradation
  - Native vs. Prompt-based Calling: Native calling preferred but blocked by API tool-count limits (128 for GPT models)
- Failure signatures:
  - Context overflow: Model cannot process full toolset; task fails to initialize
  - Tool-count cap exceeded: API rejects request; requires fallback to prompt-based calling
  - Hallucinated tool responses: Model generates fabricated results without invocation (observed in Claude prompt-based mode at >70% rate)
  - Path fixation: Model repeatedly attempts blocked approach without exploring alternatives
- First 3 experiments:
  1. Run single task across all three modes with Claude-4-Sonnet to observe Oracle→Standard performance gain and identify alternative path selection
  2. Compare native vs. prompt-based function calling on 10-task subset with model supporting both (e.g., DeepSeek-V3) to quantify hallucination rates
  3. Execute time-sensitive task (e.g., flight query) and verify real-time ground truth script produces consistent evaluation across multiple runs at different times

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do complex agentic frameworks (e.g., ReAct, RolePlaying) alter performance in massive action spaces compared to simple evaluation loop used in this study?
- Basis in paper: [explicit] Conclusion states: "Future work can... investigate model performance using more complex evaluation pipelines"
- Why unresolved: Authors utilized simple evaluation pipeline to assess "fundamental capabilities" without noise of sophisticated prompt engineering, leaving interaction between advanced reasoning frameworks and large-scale toolsets unmeasured
- What evidence would resolve it: Benchmark results comparing success rates of models deployed with advanced frameworks (like ReAct) against simple agentic loop baseline across Oracle, Standard, and Max-Scale modes

### Open Question 2
- Question: Can retrieval strategies be optimized to maintain "exploratory" benefits of large-scale mounting without incurring context burden?
- Basis in paper: [inferred] Section 4.3 shows standard semantic retrieval mode performed significantly worse than Oracle and Standard modes. Authors suggest in Section 3.3 that retriever "might inadvertently hinder this crucial exploratory potential"
- Why unresolved: While authors demonstrate full-scale mounting allows for "emergent solution paths" (hacking), they did not identify retrieval method balancing context efficiency with need for diverse tool availability
- What evidence would resolve it: Study testing alternative retrieval mechanisms (e.g., diverse sampling or keyword-based retrieval) yielding success rates comparable to Standard mode while reducing token count below 140k threshold

### Open Question 3
- Question: What specific architectural or training factors cause catastrophic hallucination rates in prompt-based function calling observed in Claude-4-Sonnet?
- Basis in paper: [inferred] Table 3 and Section 4.2.4 reveal Claude-4-Sonnet suffers massive performance drop in prompt-based calling (62.28% → 35.55%) with hallucination rate exceeding 70%, whereas other models like GPT-5 remain stable
- Why unresolved: Authors attribute this to mismatch with training templates but do not clarify if this is inherent limitation of model's instruction following or solvable alignment issue
- What evidence would resolve it: Ablation study testing various prompt formatting structures on Claude-4-Sonnet to determine if specific syntax adjustments can reduce hallucination rates to levels comparable with native function calling

## Limitations
- Benchmark's exclusive reliance on MCP servers creates brittleness—tool availability depends on external server uptime and fixed 552 tools may not generalize to other tool ecosystems
- LLM-as-judge evaluation introduces potential subjectivity in outcome assessment, particularly for nuanced tasks where semantic equivalence is ambiguous
- Performance degradation in prompt-based function calling suggests significant format mismatch issues, but specific template differences between training data and benchmark prompts remain underspecified

## Confidence

**High Confidence**: The mechanism that outcome-based evaluation provides more robust assessment than trajectory-matching (supported by explicit comparison to prior work and clear articulation of multiple valid solution paths). The finding that context budget allocation critically impacts which models can participate in Max-Scale evaluation (directly stated and experimentally validated).

**Medium Confidence**: The claim that agentic models leverage expanded tool spaces to discover alternative solution paths (supported by case studies but limited to specific examples). The observation that prompt-based function calling causes substantial hallucination (statistically significant but with unclear root causes regarding template mismatches).

**Low Confidence**: The generalizability of MCPVerse results to non-MCP tool ecosystems (not experimentally validated). The claim that MCPVerse is "the first" comprehensive benchmark for agentic tool use (lacks systematic comparison to all related work).

## Next Checks
1. **Hallucination Rate Validation**: Systematically compare tool-call logs against mounted schemas across all models using prompt-based function calling to quantify hallucination rates and identify specific tool types most susceptible to fabrication
2. **Cross-Ecosystem Generalization**: Implement parallel evaluation using REST API tools with identical tasks to determine whether performance patterns replicate or whether MCP-specific factors drive observed results
3. **Judge Consistency Analysis**: Execute same tasks multiple times across different time windows (particularly time-sensitive queries) to measure inter-rater reliability of LLM-as-judge system and identify tasks with high evaluation variance