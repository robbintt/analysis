---
ver: rpa2
title: 'SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human
  Conversations'
arxiv_id: '2510.23182'
source_url: https://arxiv.org/abs/2510.23182
tags:
- social
- reply
- human
- process
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SI-Bench is a new benchmark for evaluating social intelligence
  in large language models (LLMs) using real human-to-human dialogues. Unlike previous
  benchmarks relying on simulated or scripted interactions, SI-Bench contains 2,221
  authentic multi-turn conversations from a social networking platform, covering 12
  complex social situations based on social science theories.
---

# SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations

## Quick Facts
- arXiv ID: 2510.23182
- Source URL: https://arxiv.org/abs/2510.23182
- Authors: Shuai Huang; Wenxuan Zhao; Jun Gao
- Reference count: 40
- Primary result: Models surpass human experts in process reasoning but lag in reply quality, with Chain-of-Thought prompting degrading reply performance.

## Executive Summary
SI-Bench introduces a novel benchmark for evaluating social intelligence in large language models using authentic human-to-human dialogues. The benchmark contains 2,221 real conversations from social networking platforms, covering 12 complex social situations based on social science theories. Unlike previous benchmarks relying on simulated interactions, SI-Bench evaluates both process reasoning (motivation, cause, emotional attitude, social intent, communication strategy) and final reply quality across 8 major models. The key finding reveals a "thought-action gap": while top models exceed human experts in analytical reasoning about social situations, they still produce inferior replies, particularly when forced to articulate their reasoning through Chain-of-Thought prompting.

## Method Summary
SI-Bench evaluates LLM social intelligence using authentic Chinese dialogues from social networking platforms. The benchmark features 2,221 multi-turn conversations covering 12 social situation types. For evaluation, models generate either Chain-of-Thought reasoning (5-dimensional analysis) or direct replies. Human annotators score both process quality (0/5/10 scale across 5 dimensions) and reply quality (0/5/10 scale). The framework separates perception, strategy, and generation stages, with 312 dialogues manually annotated. Krippendorff's Alpha of 0.712 validates annotation agreement. Models are compared against a human expert baseline across two conditions: CoT-guided reasoning and direct reply generation.

## Key Results
- Top models (Gemini, Claude) outperform human experts in process reasoning but lag in reply quality generation
- Chain-of-Thought prompting degrades reply performance across all models, revealing a "thought-action gap"
- Models perform better on low-context situations (explicit intent) than high-context situations (implicit cues)
- Removing specific CoT components shows model-dependent effects on reply quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling process evaluation from outcome evaluation reveals distinct capability gaps in social intelligence.
- Mechanism: The framework separates perception, strategy, and generation stages. Models can excel at analytical reasoning (f_P and f_S) while failing at practical application (f_G) because generation requires translating abstract social understanding into contextually appropriate language.
- Core assumption: Social intelligence comprises separable cognitive stages rather than a unified capability.
- Evidence anchors:
  - Models surpass human experts in process reasoning while lagging in reply quality
  - Different performance patterns across evaluation dimensions
- Break condition: If reply quality correlates strongly with process scores across models, the decoupling assumption fails.

### Mechanism 2
- Claim: Chain-of-Thought prompting degrades social reply quality by causing literal, local strategy execution rather than holistic contextual adaptation.
- Mechanism: CoT forces explicit articulation of communicative strategies, which models then implement at surface level rather than applying the underlying pragmatic intent. Direct replies access implicit, pattern-matched social knowledge that better preserves relational dynamics.
- Core assumption: Human social reasoning is implicit and parallel, while CoT imposes an explicit sequential scaffold that interferes with intuitive response generation.
- Evidence anchors:
  - CoT consistently degrades reply quality across all models
  - Literal strategy execution at topic level versus natural application at conversational level
- Break condition: If removing specific CoT components consistently degrades reply quality across all models, the interference mechanism is incomplete.

### Mechanism 3
- Claim: Model performance varies systematically with contextual explicitness—low-context situations yield higher reasoning scores than high-context situations.
- Mechanism: Current LLMs rely predominantly on surface semantic patterns for social reasoning. Low-context scenarios have explicit goals, enabling direct inference. High-context scenarios require integrating contextual knowledge, cultural norms, and pragmatic inference.
- Core assumption: The semantic-to-pragmatic gap in social understanding reflects training data characteristics rather than fundamental architectural limits.
- Evidence anchors:
  - Models perform better on low-context situations (value test, commitment test) than high-context situations (irony, euphemism, ambiguity)
  - Current models mainly rely on surface semantics for reasoning
- Break condition: If fine-tuning on high-context dialogue data closes the performance gap significantly, the mechanism reflects data scarcity rather than architectural constraint.

## Foundational Learning

- Concept: Social Intelligence Theory (Thorndike, Goleman)
  - Why needed here: SI-Bench operationalizes social intelligence as social awareness plus social facility. Understanding this decomposition is prerequisite to interpreting benchmark results and designing improvements.
  - Quick check question: Can you explain why process reasoning and reply generation might draw on different cognitive capabilities?

- Concept: High-Context vs Low-Context Communication (Hall, 1976)
  - Why needed here: The taxonomy explicitly grounds 12 social situation types in this distinction. High-context requires inferring implicit meaning; low-context involves explicit negotiation. Model performance differences hinge on this variable.
  - Quick check question: Classify "irony" and "value test" as high or low context and explain the inference demands of each.

- Concept: Social Information Processing Theory (Crick & Dodge, 1994)
  - Why needed here: The evaluation framework directly maps this cognitive model to LLM stages: encoding cues → interpretation → goal clarification → response construction → enactment. Each stage corresponds to specific evaluation dimensions.
  - Quick check question: Which SI-Bench dimensions map to "interpretation of cues" versus "response decision"?

## Architecture Onboarding

- Component map: Raw dialogues → Candidate extraction → Cross-validation (4 annotators, 2 pairs) → Privacy filtering/minimal rewriting → 2,221 final samples → 312 manually annotated subset
- Critical path:
  1. Dialogue context input to model
  2. CoT prompt elicits 5-dimensional analysis OR direct prompt elicits reply only
  3. Human annotators score each dimension independently
  4. Reply quality assessed via pairwise comparison (CoT vs Direct) and absolute scoring
  5. Krippendorff's Alpha (0.712) validates annotation agreement
- Design tradeoffs:
  - Authenticity vs scale: Real human dialogues provide ecological validity but limit sample size and require privacy processing
  - Explicit vs implicit reasoning: CoT enables process evaluation but degrades reply quality; direct prompting produces better replies but obscures reasoning
  - Single vs multiple human experts: One expert provides consistent baseline but may underestimate human upper bound
- Failure signatures:
  - CoT reply scores lower than direct reply scores (all models show <50% CoT win rate)
  - Perfect process scores (10/10) with failed CoT replies (0/10) indicate strategy literalization
  - High performance on low-context situations with poor performance on high-context situations indicates surface-semantic dependence
- First 3 experiments:
  1. Baseline evaluation: Run all 8 models on 312 annotated samples with both CoT and direct prompts; measure process scores vs reply quality gap for each.
  2. Ablation study: Remove individual CoT components for models with largest CoT/Direct gaps; identify which dimensions help vs harm reply quality.
  3. Context-type stratification: Segment results by 12 situation types; confirm low-context advantage and identify most challenging high-context types.

## Open Questions the Paper Calls Out

- How can LLMs' internal reasoning processes be aligned with socially appropriate external behavior to bridge the observed "thought-action gap"?
- What architectural or training differences explain why the same CoT component benefits some models but degrades performance in others?
- Do the observed patterns generalize across languages and cultural contexts?
- Would alternative reasoning frameworks that better match human implicit social cognition improve reply quality?

## Limitations

- The benchmark supports evaluation only in Chinese, limiting cross-cultural validity and multilingual generalization.
- The single human expert baseline may underestimate the upper bound of human social reasoning capability.
- The 312-sample annotation subset represents a limited scope of the full 2,221-dialogue dataset.

## Confidence

- High Confidence: Models outperform human experts in process reasoning but lag in reply quality
- Medium Confidence: Chain-of-Thought degradation reveals a "thought-action gap"
- Medium Confidence: Low-context situations systematically outperform high-context situations

## Next Checks

1. Replicate the benchmark evaluation using translated versions or equivalent English-language dialogues to verify whether observed patterns hold across languages.

2. Conduct the same evaluation framework using multiple human experts (n≥5) to establish the variability range in human social reasoning and determine whether the current baseline accurately represents human upper performance bounds.

3. Train selected models on high-context dialogue data (irony, euphemism, ambiguity) and re-evaluate to determine whether the low-context performance advantage reflects data scarcity rather than fundamental architectural limitations.