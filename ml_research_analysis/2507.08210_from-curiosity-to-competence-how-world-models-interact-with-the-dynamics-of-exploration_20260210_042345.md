---
ver: rpa2
title: 'From Curiosity to Competence: How World Models Interact with the Dynamics
  of Exploration'
arxiv_id: '2507.08210'
source_url: https://arxiv.org/abs/2507.08210
tags:
- agent
- exploration
- gain
- empowerment
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how intrinsic motivations\u2014curiosity\
  \ and competence\u2014guide exploration in model-based reinforcement learning agents.\
  \ We compared two architectures: a tabular agent with fixed state representations\
  \ and a Dreamer agent with learned latent states."
---

# From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration

## Quick Facts
- **arXiv ID**: 2507.08210
- **Source URL**: https://arxiv.org/abs/2507.08210
- **Reference count**: 6
- **Primary result**: Hybrid intrinsic reward strategies combining information gain and empowerment achieve better exploration-safety balances than either signal alone, with distinct trade-offs across tabular and learned latent state agents.

## Executive Summary
This study investigates how intrinsic motivations—curiosity and competence—guide exploration in model-based reinforcement learning agents. Comparing a tabular agent with fixed state representations and a Dreamer agent with learned latent states, the authors show that curiosity-driven exploration can become stuck in local optima or fixate on irreducible uncertainty, while competence-driven exploration sometimes hinders broad discovery by staying in controllable "comfort zones" but can also avoid harmful stochasticity. Hybrid strategies combining information gain and empowerment achieved better exploration-safety balances in the tabular agent and more robust generalization in the Dreamer agent. These findings illustrate the complementarity of curiosity and competence in adaptive exploration.

## Method Summary
The study compares intrinsic motivation strategies (novelty, information gain, empowerment, and combinations) across two agent architectures in grid-world environments. The tabular agent uses fixed (x,y,θ) state representations with count-based transition models and Q-learning with prioritized sweeping. The Dreamer agent encodes pixel observations into discrete latent states and predicts dynamics in latent space. Both agents compute intrinsic rewards (novelty, information gain, empowerment) over their respective state representations. Experiments measure unique state discovery, cumulative deaths, discovery-to-death ratios, and generalization performance across deterministic/stochastic and harmful/harmless environments. The tabular agent is evaluated for 10,000 steps across 5 seeds, while Dreamer uses world-model learning with intrinsic rewards computed over latent states.

## Key Results
- Hybrid strategies combining information gain and empowerment achieved better exploration-safety balances in the tabular agent and more robust generalization in the Dreamer agent
- Curiosity-driven exploration (novelty and information gain) can become stuck in local optima or fixate on irreducible uncertainty
- Competence-driven exploration (empowerment) sometimes hinders exploration by staying in controllable "comfort zones" but can also avoid harmful stochasticity to preserve agency

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid intrinsic reward signals (combining information gain with empowerment) can balance exploration breadth and safety more effectively than either signal alone.
- **Mechanism:** Information gain drives the agent to reduce epistemic uncertainty about transition dynamics. Empowerment estimates mutual information between actions and future states, favoring controllable states. Summing or multiplying these rewards produces a composite signal that prioritizes states that are both learnable (reducible uncertainty) and actionable (controllable), filtering out irreducible noise.
- **Core assumption:** The environment contains a mix of reducible (epistemic) and irreducible (aleatoric) uncertainty; empowerment approximates control via mutual information; the reward combination function preserves gradient information without one term dominating.
- **Evidence anchors:**
  - [abstract] "Hybrid strategies combining information gain and empowerment achieved better exploration-safety balances in the tabular agent and more robust generalization in the Dreamer agent."
  - [Results] "Crucially, combining information gain and empowerment rewards via a naive sum results in a more balanced approach, leading to a higher discovery-to-death ratio (Fig. 3c)."
  - [corpus] Related work discusses curiosity-driven exploration but does not directly test hybrid information gain + empowerment.

### Mechanism 2
- **Claim:** Learned latent world models interact bidirectionally with exploration strategies, enabling adaptive generalization but introducing representation instability.
- **Mechanism:** The Dreamer agent encodes pixel observations into discrete latent states via an encoder and predicts dynamics in latent space. Intrinsic rewards are computed over these evolving representations. As the agent explores, the world model updates, changing the latent space geometry and thus the intrinsic reward landscape, which in turn changes exploration behavior.
- **Core assumption:** The latent space meaningfully compresses task-relevant features; discrete latents allow count-based novelty and tractable empowerment estimates; representation learning does not collapse critical distinctions (e.g., lava vs. ice).
- **Evidence anchors:**
  - [abstract] "The Dreamer agent reveals a two-way interaction between exploration and representation learning, mirroring the developmental co-evolution of curiosity and competence."
  - [Results] "On the unary-state grid... Performance is highly context-specific: novelty and information gain excel in deterministic environments, whereas empowerment proves more effective in stochastic settings."
  - [corpus] Corpus mentions world modeling papers but does not provide direct evidence for this bidirectional loop.

### Mechanism 3
- **Claim:** Empowerment-driven exploration avoids harmful stochasticity but can collapse into local "comfort zones" in deterministic settings.
- **Mechanism:** Empowerment is computed as mutual information between actions and next states; it is low in stochastic regions (ice) where actions weakly predict outcomes and high in predictable regions. Agents maximizing empowerment tend to avoid stochastic traps (including lethal ones) but may also avoid beneficial exploration if it requires leaving controllable areas.
- **Core assumption:** 1-step empowerment captures the relevant controllability for the task; stochasticity is correctly estimated in the dynamics model.
- **Evidence anchors:**
  - [abstract] "Competence-driven exploration (empowerment) sometimes hinders exploration by staying in a controllable 'comfort zone' but can also avoid harmful stochasticity to preserve agency."
  - [Results] "In deterministic environments, this is maladaptive, with the agents getting stuck in a stable 'comfort zone' (regions maximally distant from walls or lava or near the starting point)."
  - [corpus] "Towards Empowerment Gain through Causal Structure Learning in Model-Based RL" links empowerment to causal structure but does not directly test comfort-zone behavior.

## Foundational Learning

- **Concept: Intrinsic Motivation Signals (Novelty, Information Gain, Empowerment)**
  - **Why needed here:** The paper's core contribution is comparing how these different intrinsic rewards shape exploration patterns. Understanding their definitions and failure modes is prerequisite to interpreting results and designing hybrids.
  - **Quick check question:** Can you explain why information gain might drive an agent toward stochastic ice cells, while empowerment would avoid them?

- **Concept: World Models and Latent Representations in Model-Based RL**
  - **Why needed here:** The Dreamer agent's behavior depends critically on how latent states are learned and how intrinsic rewards are computed in latent space. The interaction between representation learning and exploration is central.
  - **Quick check question:** How does computing intrinsic rewards in a learned latent space (vs. a fixed tabular state space) change the stability and interpretability of exploration behavior?

- **Concept: Mutual Information and Empowerment Estimation**
  - **Why needed here:** Empowerment is defined as mutual information between actions and next states. Approximating this (e.g., via Blahut-Arimoto or under a random policy) is non-trivial and affects the reliability of the competence signal.
  - **Quick check question:** Why might 1-step empowerment be insufficient for tasks requiring multi-step planning through low-controllability regions?

## Architecture Onboarding

- **Component map:**
  - Environment -> Tabular agent (fixed states) -> Q-learning with prioritized sweeping + intrinsic rewards
  - Environment -> Dreamer agent (learned latents) -> Encoder + dynamics predictor + intrinsic rewards
  - Intrinsic rewards: (1) count-based novelty, (2) KL-based information gain, (3) mutual-information-based empowerment, (4) hybrid combinations

- **Critical path:**
  1. Implement environment with clearly separated stochastic (ice) and deterministic cells, plus lethal traps (lava).
  2. Implement tabular agent first to validate intrinsic reward dynamics in a fixed representation setting.
  3. Implement Dreamer with discrete latents and hook intrinsic rewards into latent space.
  4. Run ablation across: novelty-only, information-gain-only, empowerment-only, IG+empowerment (sum), IG+empowerment (product).
  5. Measure: unique states discovered, cumulative deaths, discovery-to-death ratio, generalization in novel layouts.

- **Design tradeoffs:**
  - Tabular vs. Dreamer: Tabular is interpretable but doesn't scale; Dreamer scales but representation drift complicates analysis.
  - Empowerment estimation: Blahut-Arimoto is more accurate but computationally expensive; random-policy approximation is cheap but noisier.
  - Hybrid reward: Sum is simple but may require tuning scales; product enforces joint satisfaction but may have vanishing gradients.
  - Assumption: The paper uses 1-step empowerment; extending to n-step could change behavior.

- **Failure signatures:**
  - Pure novelty: Low observation/action entropy, agent cycles in a small region (local novelty loop).
  - Pure information gain: High deaths in stochastic environments, fixation on irreducible noise.
  - Pure empowerment: Near-zero deaths but minimal state discovery in deterministic settings; agent clusters in obstacle-sparse regions.
  - Hybrid: If one term dominates (e.g., IG scale >> empowerment), behavior reverts to that term's failure mode.

- **First 3 experiments:**
  1. **Tabular ablation on mixed-state playground:** Run each intrinsic reward (novelty, IG, empowerment, IG+empowerment sum, IG+empowerment product) for 10k steps, 5 seeds. Log unique states, deaths, discovery-to-death ratio. Expect: IG explores most but dies most; empowerment explores least; hybrid achieves best ratio.
  2. **Dreamer ablation on same environment:** Replicate with Dreamer. Expect similar ordering but less pronounced differences; representation learning may smooth out some failures.
  3. **Generalization test on unary-state grids:** Pretrain Dreamer with each intrinsic reward on four environment types (harmless/deterministic, harmless/stochastic, harmful/deterministic, harmful/stochastic). Test on novel layouts with extrinsic rewards only. Expect: hybrids match or outperform single motives across conditions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can agents be equipped with adaptive motivation strategies that dynamically adjust the weighting of curiosity and competence based on environmental context?
- **Basis in paper:** [explicit] The Discussion section explicitly proposes extending the framework by "integrating adaptive motivation strategies that dynamically adapt the weighting of curiosity and competence based on environment type."
- **Why unresolved:** The current study relies on fixed combinations (sum or product) of intrinsic rewards throughout the training process.
- **What evidence would resolve it:** A meta-learning agent that successfully shifts its reward weighting in response to detected stochasticity or risk, outperforming static reward agents.

### Open Question 2
- **Question:** Do the "safety first" behaviors observed in empowerment-driven agents accurately model human exploration under threat?
- **Basis in paper:** [explicit] The authors suggest testing whether the observed "safety first" behaviors "could be used to describe human subjects under threat-of-shock paradigms."
- **Why unresolved:** This study utilized computational agents in grid-worlds without comparing the resulting behavioral patterns to human psychological data.
- **What evidence would resolve it:** Empirical data showing that human subjects in high-stress or high-risk environments prioritize controllability (empowerment) similarly to the simulated agents.

### Open Question 3
- **Question:** Do the findings regarding hybrid motivation strategies generalize to real-world robotics tasks with high stochasticity?
- **Basis in paper:** [explicit] The authors list "scaling these principles to real-world robotics tasks requiring robustness to environmental stochasticity" as a key direction for future work.
- **Why unresolved:** The experiments were confined to simplified 2D grid-worlds, which lack the sensor noise and physical dynamics of real-world environments.
- **What evidence would resolve it:** Successful deployment of hybrid curiosity-competence agents on physical robots navigating complex, noisy real-world terrains.

## Limitations
- **Dreamer hyperparameters and architecture:** The paper references Ferrao & Cunha (2025) but does not specify latent dimensionality, learning rates, or exact training steps, making exact reproduction difficult.
- **Empowerment estimation gap:** Blahut-Arimoto (tabular) vs uniform-policy approximation (Dreamer) differ in accuracy and computational cost, potentially biasing comparative results.
- **Environmental determinism:** While "unary-state grids" are described, specific grid sizes, episode termination rules, and extrinsic reward magnitudes for the generalization phase are not provided.

## Confidence
- **High confidence:** Core mechanism that hybrid IG+empowerment improves discovery-to-death ratio in tabular agents; clear empirical support from discovery/death counts.
- **Medium confidence:** Two-way interaction between exploration and representation learning in Dreamer; weaker evidence as results are described but specific data not detailed.
- **Medium confidence:** Empowerment avoiding harmful stochasticity but collapsing into comfort zones; behavioral patterns reported but mechanism depth unclear.

## Next Checks
1. **Exact Dreamer reproduction:** Obtain or infer complete hyperparameters from Ferrao & Cunha (2025) or rerun with documented defaults to confirm reported generalization patterns.
2. **Empowerment estimation comparison:** Implement both Blahut-Arimoto and uniform-policy approximations in a controlled setting to measure their impact on exploration behavior.
3. **Long-term training stability:** Extend experiments beyond 10k steps to observe whether latent representation drift in Dreamer alters intrinsic reward landscapes and exploration patterns over time.