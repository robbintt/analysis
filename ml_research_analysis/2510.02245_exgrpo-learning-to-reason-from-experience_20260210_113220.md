---
ver: rpa2
title: 'ExGRPO: Learning to Reason from Experience'
arxiv_id: '2510.02245'
source_url: https://arxiv.org/abs/2510.02245
tags:
- experience
- exgrpo
- reasoning
- training
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# ExGRPO: Learning to Reason from Experience

## Quick Facts
- **arXiv ID:** 2510.02245
- **Source URL:** https://arxiv.org/abs/2510.02245
- **Authors:** Runzhe Zhan; Yafu Li; Zhi Wang; Xiaoye Qu; Dongrui Liu; Jing Shao; Derek F. Wong; Yu Cheng
- **Reference count:** 0
- **Key outcome:** None specified in source

## Executive Summary
ExGRPO introduces a reinforcement learning with verifiable rewards (RLVR) method that learns from experience by maintaining an experience replay buffer. The approach selectively samples successful trajectories based on question difficulty (measured by correctness rate) and reasoning quality (measured by trajectory entropy), then mixes these with on-policy rollouts using importance-weighted policy updates. This enables more stable and sample-efficient learning compared to standard on-policy RLVR methods.

## Method Summary
ExGRPO combines on-policy exploration with off-policy experience replay for RLVR tasks. The method maintains a buffer of successful trajectories, partitions questions by online rollout correctness into easy/medium/hard buckets, and samples using a Gaussian distribution centered at 50% correctness to prioritize medium-difficulty questions. For each sampled question, the trajectory with lowest average action entropy is selected to ensure high-quality reasoning chains. These experiences are then mixed with fresh rollouts using importance weighting and policy shaping to enable stable off-policy learning while preserving unbiased gradient estimates.

## Key Results
- Medium-difficulty questions (25%-75% correctness) provide stronger learning signals than easy or hard questions in RLVR
- Low-entropy trajectories indicate higher reasoning chain quality and should be prioritized in experience replay
- Mixed-policy optimization with importance weighting and policy shaping enables stable off-policy learning from replayed trajectories

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Medium-difficulty questions (rollout correctness 25%-75%) provide stronger learning signals than easy or hard questions in RLVR.
- **Mechanism:** Questions are bucketed by online rollout correctness rate (Easy: ≥75%, Medium: 25%-75%, Hard: ≤25%). Gaussian sampling centered at 50% correctness biases replay toward medium-difficulty experiences, which sit in the model's "zone of proximal development."
- **Core assumption:** Correctness rate is a valid online proxy for question difficulty and learning value (no offline annotation required).
- **Evidence anchors:** [abstract] "identify rollout correctness and entropy as effective indicators of experience value"; [Section 3.2, Figure 1a] "training on Medium questions yields the largest performance gains"; [corpus] Related work on difficulty-aware sampling (HeaPA, POPE) supports this pattern but uses different difficulty metrics.
- **Break condition:** If easy/hard buckets are sampled more frequently than medium, or if correctness rate becomes uncorrelated with learning gain during training.

### Mechanism 2
- **Claim:** Low-entropy trajectories indicate higher reasoning chain quality and should be prioritized in experience replay.
- **Mechanism:** For each sampled question, the trajectory with lowest average action entropy H(o) = -1/|o| Σ π(o_t|q, o_<t) log π(o_t|q, o_<t) is selected. This filters out "lucky hits" where correct answers come from flawed reasoning (snowball effect).
- **Core assumption:** Token-level uncertainty correlates with reasoning validity; high-entropy correct answers often have invalid CoT.
- **Evidence anchors:** [Section 3.2, Figure 1b] "correct reasoning trajectories exhibit lower entropy than incorrect ones"; [Section 3.2, Table 8] Trajectories with code blocks (higher entropy) show 8-11% lower CoT correctness; [corpus] No direct corpus validation; related work focuses on entropy collapse prevention, not selection.
- **Break condition:** If model's entropy becomes uninformative (e.g., after extensive training) or if low-entropy selection causes premature convergence.

### Mechanism 3
- **Claim:** Mixed-policy optimization with importance weighting and policy shaping enables stable off-policy learning from replayed trajectories.
- **Mechanism:** Replayed trajectory o* is reweighted by importance sampling w* = π_θ(o*)/π_θ_past(o*). Policy shaping f(w) = w/(w+β) replaces clipping to dampen large ratios while amplifying small ones. This allows unbiased gradient estimation (Theorem 1) with controlled variance (Proposition 2).
- **Core assumption:** The replayed trajectory's generating policy π_θ_past has sufficient overlap with current policy (support assumption A1).
- **Evidence anchors:** [Section 4.2, Eq. 4] Unified objective combining on-policy and experiential terms; [Appendix D.2] Theorem 1 proves unbiasedness with exact importance weights; [corpus] LUFFY and other off-policy RLVR methods use similar importance weighting but without structured experience management.
- **Break condition:** If importance ratios become too large (distribution shift too severe) or if policy shaping over-dampens informative gradients.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: ExGRPO is built specifically for RLVR, where rewards come from rule-based or model-based verifiers comparing final answers to ground truth.
  - Quick check question: Can you explain why RLVR requires generating K trajectories per question for GRPO's advantage estimation?

- **Concept: On-policy vs Off-policy Learning**
  - Why needed here: The paper's core contribution is mixing on-policy exploration with off-policy experience replay; understanding the distribution shift problem is essential.
  - Quick check question: Why does discarding rollouts after one update cause "computational inefficiency and instability" in standard RLVR?

- **Concept: Importance Sampling for Off-policy Correction**
  - Why needed here: ExGRPO uses importance weights w* = π_θ/π_θ_past to correct for the fact that replayed trajectories were generated by an older policy.
  - Quick check question: What happens to variance when the importance ratio becomes very large (π_θ >> π_θ_past)?

## Architecture Onboarding

- **Component map:** Training Loop -> Experience Management (Collection, Partition, Selection) -> Policy Optimization (Mixed batch, Importance weighting, Policy shaping, GRPO advantage)

- **Critical path:** Rollout → reward computation → successful trajectories stored → bucketed by correctness → sampled with entropy selection → mixed with fresh rollouts → importance-weighted policy update.

- **Design tradeoffs:**
  - **ρ (experience ratio):** Too high (75%) traps model in exploitation; too low (25%) wastes replay benefits. Default 50% is optimal.
  - **Policy shaping β:** Small β (0.1) amplifies low-probability tokens but may introduce bias; larger β reduces variance more but dampens signal.
  - **Delayed start:** ExGRPO activates only after Pass@1 > 35% to avoid contaminating buffer with low-quality early trajectories.

- **Failure signatures:**
  - **Entropy explosion:** On-policy training collapses on weak models (Figure 4); ExGRPO prevents this via replay.
  - **Snowball effect:** Repeatedly sampling high-entropy "lucky hits" entrenches reasoning errors (Section F.4).
  - **Distribution shift:** If π_θ diverges too far from π_θ_past, importance ratios explode and gradients become unstable.

- **First 3 experiments:**
  1. **Ablate question selection:** Train with random bucket sampling vs Gaussian-weighted sampling; expect 1-2 point drop on in-distribution benchmarks.
  2. **Ablate trajectory selection:** Use highest-entropy instead of lowest-entropy trajectory; expect snowball effect (invalid code blocks, lower CoT validity).
  3. **Vary experience ratio:** Test ρ ∈ {25%, 50%, 75%}; expect 50% optimal, 75% worst due to over-exploitation (Table 4 confirms).

## Open Questions the Paper Calls Out
None specified in source

## Limitations
- Effectiveness depends on correctness rate being a reliable proxy for question difficulty
- Policy shaping introduces tunable parameter β=0.1 without explored sensitivity
- Delayed start assumes monotonic relationship between early performance and buffer quality

## Confidence
- **Medium confidence:** Medium-difficulty questions provide stronger learning signals - supported by empirical results but lacking direct comparison with alternative difficulty metrics
- **High confidence:** Low-entropy trajectories indicate higher reasoning quality - demonstrated through multiple validation checks including CoT correctness and code validity metrics
- **High confidence:** Mixed-policy optimization with importance weighting and policy shaping enables stable off-policy learning - proven theoretically with unbiased gradient estimation and empirically validated across multiple benchmarks

## Next Checks
1. **Difficulty metric ablation study:** Replace the correctness-based difficulty buckets with alternative metrics (question length, token count, code block presence) and compare learning curves to isolate whether correctness rate is truly optimal for sampling.

2. **Policy shaping sensitivity analysis:** Systematically vary β ∈ {0.01, 0.1, 0.5, 1.0} and measure impact on final performance, variance reduction, and convergence speed to determine optimal trade-offs.

3. **Early training buffer contamination test:** Run ExGRPO from initialization (no Pass@1 threshold) and compare final performance to delayed-start version to quantify the impact of early low-quality trajectories on the experience replay buffer.