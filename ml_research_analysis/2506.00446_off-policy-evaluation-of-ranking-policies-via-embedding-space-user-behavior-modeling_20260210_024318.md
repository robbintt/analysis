---
ver: rpa2
title: Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior
  Modeling
arxiv_id: '2506.00446'
source_url: https://arxiv.org/abs/2506.00446
tags:
- ranking
- variance
- behavior
- assumption
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles off-policy evaluation (OPE) in ranking settings
  where the action space grows exponentially with ranking length and unique actions,
  causing high variance in existing estimators. The authors introduce two key assumptions:
  no direct effect on rankings and user behavior model on ranking embedding spaces.'
---

# Off-Policy Evaluation of Ranking Policies via Embedding-Space User Behavior Modeling

## Quick Facts
- **arXiv ID:** 2506.00446
- **Source URL:** https://arxiv.org/abs/2506.00446
- **Reference count:** 40
- **Primary result:** MRIPS estimator significantly reduces MSE compared to baselines, especially as ranking length and unique actions increase.

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) for ranking policies in scenarios where the action space grows exponentially with ranking length and the number of unique actions. The authors introduce two key assumptions: no direct effect on rankings and a user behavior model on ranking embedding spaces. They propose the generalized marginalized IPS (GMIPS) estimator, which leverages ranking embeddings to reduce variance while maintaining unbiasedness. The marginalized reward interaction IPS (MRIPS) variant performs best by balancing bias and variance through a cascade user behavior model, even when assumptions are violated. Experiments demonstrate significant MSE reduction compared to baselines.

## Method Summary
The paper proposes GMIPS and MRIPS estimators for off-policy evaluation of ranking policies. GMIPS reduces variance by operating in a lower-dimensional embedding space rather than the raw action space, under the assumption that actions influence rewards only through embeddings. MRIPS further reduces variance by assuming a cascade user behavior model, weighting only the subset of embeddings at positions 1 to k for the reward at position k. The method includes using SLOPE to select optimal embedding dimensions for bias-variance trade-off optimization. The approach is validated on synthetic data using Open Bandit Pipeline and real-world datasets (EUR-Lex4K, RCV1-2K) with surrogate embeddings generated via neural networks.

## Key Results
- GMIPS reduces variance significantly compared to standard IPS by operating in embedding space rather than combinatorial action space
- MRIPS performs best among GMIPS variants by balancing bias and variance through cascade modeling
- Using SLOPE to select optimal embedding dimensions further improves performance
- MSE reduction is most pronounced as the number of unique actions and ranking length increase

## Why This Works (Mechanism)

### Mechanism 1
Standard IPS calculates importance weights over the exponential combinatorial ranking action space $\Pi(A)$. GMIPS calculates weights over the ranking embedding space $\Pi(E)$. If $|\Pi(E)| \ll |\Pi(A)|$, variance of importance weights decreases, leading to lower MSE. This relies on the "No Direct Effect" assumption that actions influence rewards only through embeddings ($a \perp r | x, e$). If embeddings fail to capture action information relevant to rewards, bias is introduced.

### Mechanism 2
MRIPS reduces variance further by assuming a cascade user behavior model. Instead of weighting the entire ranking embedding $e$, MRIPS weights only embeddings at positions 1 to k for reward at position k. This "doubly marginalized" weight reduces variance associated with later ranking positions. This assumes users examine items sequentially. If user behavior doesn't follow cascade (e.g., random skipping), bias may outweigh variance benefits.

### Mechanism 3
Intentionally omitting specific embedding dimensions via SLOPE can minimize MSE by managing the bias-variance trade-off. Increasing dimensions lowers bias but increases variance. SLOPE selects optimal dimensions to drop, tuning estimator bias. This assumes a U-curve relationship between dimensionality and MSE with an intermediate optimal subset. If logged data is insufficient, SLOPE may select sub-optimal dimensions.

## Foundational Learning

- **Concept: Inverse Propensity Scoring (IPS)**
  - **Why needed:** This is the baseline estimator being improved. Understanding how IPS uses importance weights ($\pi/\pi_0$) to correct distribution shift is prerequisite to understanding GMIPS modifications.
  - **Quick check:** If logging policy $\pi_0$ never selects action $a$ that target policy $\pi$ prefers, what happens to IPS estimate? (Answer: Zero support/high variance/bias)

- **Concept: Bias-Variance Trade-off in OPE**
  - **Why needed:** The paper's core contribution navigates this trade-off. Variance comes from large action spaces; bias comes from incorrect assumptions.
  - **Quick check:** Does adding more embedding dimensions generally increase or decrease the variance of GMIPS estimator? (Answer: Increase)

- **Concept: Causal Graphical Models (Mediation)**
  - **Why needed:** The "No Direct Effect" assumption is a causal structure where Action $A$ mediates its effect on Reward $R$ entirely through Embedding $E$.
  - **Quick check:** In the paper's causal diagram, does the arrow go from Action to Reward directly? (Answer: No, it goes Action -> Embedding -> Reward)

## Architecture Onboarding

- **Component map:** Data Logger -> Embedding Engine -> Weight Calculator -> SLOPE Tuner -> Estimator
- **Critical path:** 1) Extract embeddings for logged and target actions 2) Estimate marginal distributions $p(e|x, \pi)$ and $p(e|x, \pi_0)$ 3) Run SLOPE to determine optimal dimensions to keep 4) Compute MRIPS value using optimal dimensions
- **Design tradeoffs:** High-dim Embeddings provide lower bias but risk high variance; Choice of User Model affects robustness (MRIPS for sequential browsing, MIIPS for unrelated items)
- **Failure signatures:** High MSE with GMIPS suggests violating "No Direct Effect" assumption; SLOPE instability suggests insufficient sample size
- **First 3 experiments:** 1) Compare MSE of Standard IPS vs. GMIPS on synthetic data with strict "No Direct Effect" enforcement 2) Incrementally increase embedding dimensions and plot Bias vs. Variance 3) Apply MRIPS to data with "Independent" user behavior to quantify model mismatch bias

## Open Questions the Paper Calls Out

- **Question:** Can MRIPS be adapted for Off-Policy Learning to optimize ranking policies rather than just evaluate them?
- **Question:** How can representation learning be integrated to identify latent ranking embeddings that satisfy the "no direct effect" assumption?
- **Question:** How does the kernel-based extension for continuous embeddings compare to discrete MRIPS regarding bias-variance trade-off?

## Limitations
- Core assumptions (no direct effect, cascade behavior) are strong and may not hold in practice
- Effectiveness heavily depends on quality of embedding representations
- Specific architecture and hyperparameters for surrogate embeddings in real-world datasets are not fully specified

## Confidence

- **High Confidence:** Theoretical variance reduction result (Theorem 3.7) and core algorithmic contribution (GMIPS framework)
- **Medium Confidence:** Empirical validation shows strong results, but surrogate embeddings introduce uncertainty
- **Low Confidence:** SLOPE algorithm's robustness to small sample sizes and ability to reliably find optimal dimensions

## Next Checks

1. **Assumption Violation Test:** Create synthetic dataset violating "no direct effect" assumption (identical embeddings, different rewards) and compare MRIPS vs. standard IPS to quantify bias
2. **Embedding Sensitivity:** Vary quality of surrogate embeddings (add noise, reduce training epochs) and measure impact on MRIPS's MSE
3. **SLOPE Stability:** Run SLOPE on multiple random subsets of logged data to measure variance in selected optimal dimension count