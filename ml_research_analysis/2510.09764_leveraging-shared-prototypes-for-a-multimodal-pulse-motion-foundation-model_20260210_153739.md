---
ver: rpa2
title: Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model
arxiv_id: '2510.09764'
source_url: https://arxiv.org/abs/2510.09764
tags:
- learning
- protomm
- data
- multimodal
- stress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProtoMM, a novel multimodal self-supervised
  learning framework for pre-training pulse motion foundation models using PPG and
  accelerometer data. ProtoMM addresses the challenge of capturing both within-modality
  (unique) and between-modality (shared) information in biosignals.
---

# Leveraging Shared Prototypes for a Multimodal Pulse Motion Foundation Model

## Quick Facts
- arXiv ID: 2510.09764
- Source URL: https://arxiv.org/abs/2510.09764
- Authors: Wanting Mao; Maxwell A Xu; Harish Haresamudram; Mithun Saha; Santosh Kumar; James Matthew Rehg
- Reference count: 12
- One-line primary result: ProtoMM achieves state-of-the-art performance across three datasets and six downstream tasks, outperforming twelve baselines including CLIP, COCOA, and SLIP

## Executive Summary
This paper introduces ProtoMM, a novel multimodal self-supervised learning framework for pre-training pulse motion foundation models using PPG and accelerometer data. ProtoMM addresses the challenge of capturing both within-modality (unique) and between-modality (shared) information in biosignals. The key innovation is using a shared prototype dictionary to anchor heterogeneous modalities in a common embedding space, avoiding the negative sampling problems of contrastive methods. The model is trained using a Multimodal Prototype Prediction loss that enforces consistency between prototype assignments across augmented views and modalities.

## Method Summary
ProtoMM is a multimodal self-supervised learning framework that leverages shared prototypes for learning representations from synchronized physiological signals. The method uses two modality-specific encoders to process PPG and accelerometer data, mapping them to a shared prototype dictionary through soft assignment using Sinkhorn-Knopp normalization. The model employs a Multimodal Prototype Prediction loss that balances within-modality and between-modality consistency, with the key innovation being the use of shared prototypes rather than contrastive negative sampling to align modalities.

## Key Results
- ProtoMM achieves state-of-the-art performance across three datasets (WESAD, SWELL, Troika) and six downstream tasks
- The model outperforms twelve baselines including CLIP, COCOA, and SLIP on stress detection tasks
- Setting α=0.5 (equal weighting of within- and between-modality objectives) achieves optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prototype-based alignment avoids the false negative problem inherent in contrastive multimodal learning
- **Mechanism:** Rather than treating temporally misaligned samples as negatives (which can misclassify valid physiological states), ProtoMM projects all embeddings onto a shared discrete prototype space. Consistency is enforced through swapped prediction—any view should predict another's prototype assignment—eliminating the need for negative sampling entirely.
- **Core assumption:** Physiological signals from different modalities (PPG, accelerometer) share latent states that can be discretized into a finite set of prototype vectors; modalities are complementary rather than redundant.
- **Evidence anchors:**
  - [abstract] "overfit to easily aligned features and misclassify valid cross-modal relationships as negatives, resulting in fragmented and non-generalizable embeddings"
  - [section 5] "as a negative-free method, ProtoMM avoids the 'false negative' problem of contrastive learning"
  - [corpus] Limited direct corpus support; related work on dynamical system representations for physiological time-series (arXiv:2512.00239) similarly emphasizes preserving underlying physiological state, but does not address prototype-based alignment
- **Break condition:** If modalities share no latent structure, or if the number of prototypes is insufficient to capture the diversity of physiological states, the shared dictionary will fail to provide meaningful anchors.

### Mechanism 2
- **Claim:** Explicit balancing of within-modality and between-modality objectives is necessary for complementary modalities
- **Mechanism:** The Multimodal Prototype Prediction loss (L_MPP) combines within-modality consistency (αL_within-mod) and between-modality consistency ((1-α)L_between-mod). Within-modality preserves unique features (e.g., motion signatures in accelerometer); between-modality captures shared information (e.g., cardiac activity correlation during exercise).
- **Core assumption:** Both unique and shared features are relevant for downstream tasks; ignoring either degrades performance.
- **Evidence anchors:**
  - [section 3.3] "Setting α=1 reduces the objective to independent, unimodal swapped prediction... In this work, we set α=0.5 to equally weight both sources of learning"
  - [section 5, Table 2] α=0.5 achieves best performance; α=0 (between-modality only) and α=1 (within-modality only) underperform
  - [corpus] No corpus papers directly test this balancing mechanism for physiological signals
- **Break condition:** If downstream tasks require only shared information (e.g., heart rate from PPG alone), the within-modality term adds noise; if tasks require only unique features, between-modality alignment may discard them.

### Mechanism 3
- **Claim:** Sinkhorn-Knopp equipartition prevents prototype collapse and ensures semantic diversity
- **Mechanism:** The assignment target V is computed using Sinkhorn-Knopp, which enforces equal utilization across prototypes within a batch. This prevents the model from collapsing all samples to a few prototypes, ensuring the learned dictionary captures diverse physiological patterns.
- **Core assumption:** Physiological states are distributed such that enforcing equipartition across a batch is reasonable; the true state distribution is not extremely skewed.
- **Evidence anchors:**
  - [section 3.2] "Sinkhorn-Knopp algorithm... enforces an equipartition constraint to prevent mode collapse by ensuring all prototypes are utilized equally across a batch"
  - [section 5, Figures 2-3] t-SNE visualization shows prototypes cluster semantically (stressed vs. unstressed, active vs. sedentary) rather than collapsing
  - [corpus] Corpus papers do not address Sinkhorn-Knopp for physiological time-series
- **Break condition:** If physiological states are highly imbalanced (e.g., rare stress events), equipartition may force spurious assignments; batch size must be large enough to capture state diversity.

## Foundational Learning

- **Concept: Swapped Prediction / SwAV-style Clustering**
  - **Why needed here:** ProtoMM extends SwAV's soft cluster assignment consistency from unimodal two-view settings to multimodal multi-view settings. Understanding how prediction targets (Sinkhorn assignments) differ from probability outputs (softmax) is essential.
  - **Quick check question:** Can you explain why SwAV uses Sinkhorn-Knopp rather than k-means for assignment targets, and what happens if all samples collapse to one prototype?

- **Concept: Multimodal Representation Factorization**
  - **Why needed here:** The paper explicitly distinguishes within-modality (unique) and between-modality (shared) information. This concept underpins why α=0.5 is optimal and why naive concatenation (early fusion) fails.
  - **Quick check question:** For PPG + accelerometer, give one example of within-modality information and one of between-modality information. Why would contrastive alignment risk discarding the former?

- **Concept: Prototype-Based Interpretability**
  - **Why needed here:** ProtoMM claims improved interpretability because each prototype can be mapped to semantically meaningful physiological patterns. This requires understanding how to reverse-map from prototype space to raw signals.
  - **Quick check question:** How would you identify which physiological state a prototype represents? What visualization would you use?

## Architecture Onboarding

- **Component map:** Input windows -> Modality-specific 1D ResNet-26 encoders (PPG, Accelerometer) -> Shared prototype dictionary (P×E) -> Sinkhorn-Knopp assignment -> Multimodal Prototype Prediction loss -> Downstream linear probe

- **Critical path:**
  1. Augmentation correctness (incorrect augmentations break consistency)
  2. Prototype initialization and Sinkhorn convergence (mode collapse if unstable)
  3. Encoder weight divergence (modality-specific encoders must not collapse to identical representations)

- **Design tradeoffs:**
  - **Number of prototypes (P):** Too few → under-segmentation; too many → sparsity, slow convergence. Paper does not ablate this explicitly.
  - **α value:** α=0.5 is optimal; adjust if downstream task is known to favor shared or unique features.
  - **Hard vs. soft assignment:** ProtoMM uses soft (SwAV-style); VQ-VAE uses hard. Soft allows semantic overlap but may be less discrete.

- **Failure signatures:**
  - Mode collapse: All samples assigned to 1-2 prototypes; check prototype utilization histogram
  - Modality collapse: Encoder outputs become identical; check cosine similarity between modality embeddings
  - No improvement over unimodal: α may be wrong, or augmentations are not modality-appropriate

- **First 3 experiments:**
  1. **Sanity check:** Train ProtoMM on small subset; verify prototype utilization (should be roughly uniform) and that L_within-mod and L_between-mod both decrease
  2. **Ablation:** Compare α∈{0, 0.5, 1} on validation split; confirm α=0.5 is optimal for your downstream task distribution
  3. **Baseline comparison:** Train SLIP (contrastive analogue) with identical architecture; ProtoMM should outperform on stress detection tasks (per Table 1)

## Open Questions the Paper Calls Out
None

## Limitations
- **Prototype dictionary size sensitivity** - The paper does not systematically ablate the number of prototypes (P) across datasets, which may critically affect performance and interpretability
- **Cross-dataset generalization** - All experiments use pre-defined train/validation/test splits within each dataset; no testing of whether prototypes transfer between datasets
- **Temporal dynamics limitation** - ProtoMM processes fixed-length windows independently, potentially missing long-range temporal dependencies important for some physiological states

## Confidence
**High confidence** - The core mechanism of avoiding false negatives through prototype-based alignment (Mechanism 1) is well-supported by the architectural design and stress detection results

**Medium confidence** - The claim that α=0.5 optimally balances within- and between-modality information (Mechanism 2) is supported by ablation studies but may be dataset-dependent

**Medium confidence** - The interpretability through prototype visualization (Mechanism 3) shows plausible semantic clustering but lacks rigorous validation

## Next Checks
1. **Prototype transfer learning test** - Train ProtoMM on SWELL (stress detection) and evaluate whether the learned prototypes transfer to the WESAD dataset for stress detection without fine-tuning

2. **Prototype size sensitivity ablation** - Systematically vary P∈{100, 300, 1000, 3000} on the same downstream tasks and report performance and prototype utilization statistics

3. **Temporal dependency evaluation** - Modify ProtoMM to incorporate temporal context (e.g., using transformer encoders or temporal pooling) and evaluate performance on tasks requiring long-range dependencies (e.g., arrhythmia detection or sleep staging)