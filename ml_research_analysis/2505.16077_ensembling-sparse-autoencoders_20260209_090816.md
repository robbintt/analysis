---
ver: rpa2
title: Ensembling Sparse Autoencoders
arxiv_id: '2505.16077'
source_url: https://arxiv.org/abs/2505.16077
tags:
- saes
- features
- boosting
- bagging
- naive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the limitation of sparse autoencoders (SAEs)
  in capturing only a subset of interpretable features from neural network activations.
  The authors propose ensembling multiple SAEs using two approaches: naive bagging,
  which trains SAEs in parallel with different initializations and averages their
  outputs, and boosting, which sequentially trains SAEs to minimize residual reconstruction
  errors.'
---

# Ensembling Sparse Autoencoders

## Quick Facts
- **arXiv ID**: 2505.16077
- **Source URL**: https://arxiv.org/abs/2505.16077
- **Reference count**: 40
- **Primary result**: Ensembling SAEs improves activation reconstruction, feature diversity, and downstream task performance over single SAEs

## Executive Summary
This work addresses the limitation of sparse autoencoders (SAEs) in capturing only a subset of interpretable features from neural network activations. The authors propose ensembling multiple SAEs using two approaches: naive bagging, which trains SAEs in parallel with different initializations and averages their outputs, and boosting, which sequentially trains SAEs to minimize residual reconstruction errors. They show that ensembling SAEs is equivalent to concatenating their feature representations. Empirical results across three language models (GELU-1L, Pythia-160M, and Gemma 2-2B) demonstrate that ensembling improves activation reconstruction, feature diversity, and stability compared to single SAEs. In downstream tasks, ensembling outperforms single SAEs in concept detection accuracy and spurious correlation removal, with boosting showing particular effectiveness in removing gender-related biases. The approach provides a practical framework for leveraging SAE variability to enhance interpretability.

## Method Summary
The authors propose two methods for ensembling SAEs: naive bagging and boosting. Naive bagging trains multiple SAEs in parallel with different random initializations, then averages their reconstructions at inference time. Boosting trains SAEs sequentially, where each subsequent SAE learns to reconstruct the residual error from previous SAEs. The key theoretical insight is that both ensembling methods are mathematically equivalent to concatenating the feature representations of individual SAEs with scaled coefficients. The authors evaluate their approach on three language models using 800M tokens for training and 7M tokens for evaluation, measuring reconstruction fidelity, feature diversity, stability, and downstream task performance on concept detection and spurious correlation removal.

## Key Results
- Ensembling SAEs improves explained variance and reduces reconstruction error compared to single SAEs across all three tested models
- Boosting achieves lower MSE and higher feature diversity than naive bagging, while naive bagging shows better stability across random seeds
- In downstream concept detection tasks, boosting improves top-1 feature selection accuracy while naive bagging excels for top-5 feature sets
- Boosting significantly outperforms naive bagging in spurious correlation removal (SSHIFT scores) for gender-related biases

## Why This Works (Mechanism)

### Mechanism 1: Naive Bagging Reduces Reconstruction Variance
- Claim: Averaging outputs from SAEs with different random initializations reduces variance in activation reconstruction while preserving expected bias.
- Mechanism: Each SAE independently learns a different subset of features due to initialization variability. When reconstructions are averaged, random errors cancel while systematic signal remains—formalized via bias-variance decomposition (Proposition 2 shows variance → 0 as J → ∞).
- Core assumption: SAEs trained with different initializations are i.i.d. when conditioned on the training set, and each dimension of reconstruction has finite expectation.
- Evidence anchors:
  - [abstract] "SAEs trained with different initial weights can learn different features, demonstrating that a single SAE captures only a limited subset"
  - [Section 4.1] "uniform ensemble weight α(j) = 1/J is motivated by considering naive bagging as a way to reduce reconstruction variance"
  - [corpus] Paulo & Belrose (2025) directly corroborate: "SAEs trained on the same model and data, differing only in the random seed" learn different features
- Break condition: When SAEs do not have finite expected reconstruction values, or when the training set is too small for the strong law of large numbers to apply effectively.

### Mechanism 2: Boosting Reduces Bias Through Sequential Residual Fitting
- Claim: Training SAEs sequentially to reconstruct the residual error from previous iterations encourages them to capture complementary features, reducing overall reconstruction bias.
- Mechanism: SAE 1 learns features explaining the original activations. SAE 2 trains on (a − reconstruction₁), forcing it to learn features SAE 1 missed. This sequential specialization bounds the bias term (Proposition 3).
- Core assumption: Assumption 1 (generalization bound: test error ≤ train error + ε_G) and Assumption 2 (SAEs can nearly-overfit training data with irreducible error ε_I).
- Evidence anchors:
  - [abstract] "SAEs sequentially trained to minimize the residual error are ensembled in boosting"
  - [Section 4.2] Equation 8 defines residual-based loss; Supplementary Figure 1 shows MSE decreases with each boosting iteration
  - [corpus] Weak direct corpus support for residual-fitting in SAEs specifically; principle borrowed from gradient boosting literature
- Break condition: When test activations differ substantially from training distribution (Assumption 1 fails), or when SAE sparsity constraints prevent sufficient residual capture (Assumption 2 fails).

### Mechanism 3: Output Ensembling is Mathematically Equivalent to Feature Concatenation
- Claim: Ensembling SAE reconstructions via weighted summation is identical to using a single larger SAE with concatenated decoder features and scaled coefficients.
- Mechanism: Since each SAE reconstruction is linear (ĉ = W_dec · c + b_dec), a weighted sum equals: [W_dec^(1) ... W_dec^(J)] · [α^(1)c^(1); ...; α^(J)c^(J)] + Σα^(j)b_dec^(j). Proven in Proposition 1.
- Core assumption: Ensemble weights can be absorbed into coefficients (necessary when decoder columns are unit-normalized as direction vectors).
- Evidence anchors:
  - [abstract] "We show that ensembling SAEs is equivalent to concatenating their feature representations"
  - [Section 3.2, Proposition 1] Full proof of equivalence with concatenated matrices
  - [corpus] No corpus contradiction; this is a mathematical identity, not an empirical claim
- Break condition: Never—this is a provable mathematical identity for linear decoders.

## Foundational Learning

- **Bias-Variance Decomposition**
  - Why needed here: The theoretical justification for naive bagging (variance reduction via averaging) and boosting (bias reduction via sequential fitting) both depend on this decomposition.
  - Quick check question: If your SAE ensemble has high variance across runs, which approach—naive bagging or boosting—should reduce it more effectively?

- **Sparse Dictionary Learning**
  - Why needed here: SAEs decompose activations into sparse linear combinations of learned "dictionary" features; understanding this frames why ensembling expands the dictionary.
  - Quick check question: In Equation 2, what does f_i = W_dec[:, i] represent, and why must k > d?

- **Residual Fitting (Gradient Boosting Intuition)**
  - Why needed here: Boosting iteratively fits models to what previous models failed to capture; recognizing this pattern clarifies why boosting SAEs learn complementary features.
  - Quick check question: After the first boosting iteration, what input does the second SAE receive—original activations or the reconstruction residual?

## Architecture Onboarding

- **Component map:**
  - Base SAE: Encoder (W_enc ∈ R^{k×d}, b_enc), activation h(·) [ReLU/TopK/JumpReLU], decoder (W_dec ∈ R^{d×k}, b_dec). Columns of W_dec are features
  - Naive Bagging: J parallel SAEs with different seeds; ensemble output = (1/J)Σ g(a; θ^(j))
  - Boosting: Sequential SAEs; SAE j trains on residual r_j = a − Σ_{ℓ<j} g(a; θ^(ℓ)); ensemble output = Σ g(a; θ^(j))
  - Unified interface: Concatenated features W_dec = [W_dec^(1) ... W_dec^(J)], coefficients c = [α^(1)c^(1); ...; α^(J)c^(J)]

- **Critical path:**
  1. Extract activations from target layer (e.g., layer 12 of Gemma 2-2B) using ~800M tokens
  2. Tune base SAE hyperparameters for ~90% explained variance
  3. Naive bagging: Launch J independent training jobs; average outputs at inference
  4. Boosting: Train SAE 1 → compute residuals → train SAE 2 → repeat; sum outputs
  5. Downstream use: Access concatenated feature set for probing, steering, or ablation

- **Design tradeoffs:**
  - Naive bagging: Parallelizable, better stability, but may retain redundant features
  - Boosting: Sequential (slower), better reconstruction and diversity, but can overfit to training quirks
  - Ensemble size J: Returns diminish after ~8 SAEs (Figure 2 plateaus); compute scales linearly

- **Failure signatures:**
  - Spurious correlation removal fails with naive bagging: Multiple redundant features encode the bias; ablating only top-L misses some (Table 3)
  - Concept detection degrades with boosting on top-1 feature: Features become too granular/edge-case specific
  - Stability drops with boosting: Specialized residual features vary more across random seeds (Figure 2, rightmost panel)

- **First 3 experiments:**
  1. Replicate intrinsic metrics sweep: Train 2/4/6/8 SAE ensembles on GELU-1L; plot explained variance, diversity, and stability vs. J for both methods
  2. Downstream transfer check: Compare concept detection accuracy using top-1 vs. top-5 features; expect naive bagging to degrade with more features (redundancy) and boosting to improve
  3. Ablation on residual formulation: Test if computing residuals iteratively (r_j = r_{j−1} − ĝ_j) vs. from-scratch (r_j = a − Σ_{ℓ<j} ŷ_ℓ) affects final reconstruction MSE

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ensembling SAEs with heterogeneous architectures (e.g., varying activation functions or sizes) yield better feature coverage than ensembling identical architectures?
- Basis in paper: [explicit] The authors state that "future directions can consider ensembling (stacking) different architectures such as SAEs with different activation functions and sizes..."
- Why unresolved: The current work strictly scopes experiments to SAEs with the same architecture to isolate the effects of initialization and sequential training.
- What evidence would resolve it: Empirical comparisons of feature diversity and reconstruction fidelity between stacked heterogeneous SAEs and the proposed homogeneous ensembles.

### Open Question 2
- Question: Can theoretical guarantees for SAE ensembling be formulated for feature identification rather than just reconstruction error?
- Basis in paper: [explicit] The paper suggests "future work can also explore ensembling from theoretical perspectives beyond reconstruction, such as feature identification."
- Why unresolved: The current theoretical results (Propositions 2 and 3) justify the methods solely through the bias-variance decomposition of reconstruction loss.
- What evidence would resolve it: Theoretical proofs or synthetic experiments showing that ensembling improves the recovery of "ground truth" features, independent of reconstruction metrics.

### Open Question 3
- Question: Does the sequential nature of boosting cause SAEs to learn features specific to training set edge cases, thereby degrading performance on high-level concept detection?
- Basis in paper: [inferred] The authors hypothesize that boosting "could learn features too specific to edge cases in the training set" to explain its lower accuracy compared to naive bagging in certain downstream tasks.
- Why unresolved: The paper identifies the performance gap in concept detection but does not isolate the mechanism (overfitting to residuals vs. lack of generalizable features) or test mitigation strategies.
- What evidence would resolve it: Ablation studies analyzing the distribution of boosted features on held-out data, or modifications to the boosting loss to penalize low-frequency features.

## Limitations

- Theoretical guarantees rely on strong assumptions (i.i.d. SAEs, finite moments) that may not hold for large-scale SAEs
- Boosting can overfit to training quirks and produce features too specific for downstream concept detection
- The mathematical equivalence proof assumes decoder normalization that isn't explicitly confirmed in experiments

## Confidence

- **High confidence**: Ensembling SAEs improves reconstruction accuracy and diversity (empirical results in Figures 2-3 are robust across three models)
- **Medium confidence**: Boosting outperforms naive bagging in downstream spurious correlation removal (significant SSHIFT improvement but results depend on specific concept detection pipeline)
- **Low confidence**: The mathematical equivalence between ensembling and feature concatenation applies universally (assumes specific decoder normalization not explicitly confirmed in experiments)

## Next Checks

1. **Cross-seed feature analysis**: Measure feature similarity distributions across naive bagging ensemble members to quantify how much initialization variability actually affects learned features
2. **Residual computation validation**: Test whether iterative versus from-scratch residual computation affects final reconstruction MSE and downstream performance
3. **Ensemble size sensitivity**: Verify the claim that returns diminish after ~8 SAEs by testing ensembles of size 2-16 on all three models, measuring marginal gains in reconstruction and diversity