---
ver: rpa2
title: Group-Sensitive Offline Contextual Bandits
arxiv_id: '2510.27123'
source_url: https://arxiv.org/abs/2510.27123
tags:
- policy
- reward
- fairness
- disparity
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies group-sensitive fairness in offline contextual
  bandits, where optimizing only for reward can widen disparities across groups (e.g.,
  gender, race). To address this, the authors propose the Off-Policy Group-Constrained
  Policy Gradient algorithm (GC-PG), which incorporates a group-sensitive fairness
  constraint into the policy optimization via a Lagrangian formulation.
---

# Group-Sensitive Offline Contextual Bandits

## Quick Facts
- **arXiv ID:** 2510.27123
- **Source URL:** https://arxiv.org/abs/2510.27123
- **Reference count:** 4
- **Key outcome:** Proposes Off-Policy Group-Constrained Policy Gradient (GC-PG) to reduce reward disparities across groups in offline contextual bandits

## Executive Summary
This paper addresses fairness concerns in offline contextual bandits, where optimizing solely for reward can exacerbate disparities across sensitive groups. The authors introduce GC-PG, an algorithm that incorporates group-sensitive fairness constraints into policy optimization using a Lagrangian formulation. By employing a doubly robust estimator, the method mitigates bias and variance in reward estimation. Theoretical analysis establishes convergence to a stationary point at O(1/T) rate, while experiments demonstrate effective reduction in reward disparities while maintaining competitive overall performance.

## Method Summary
The proposed GC-PG algorithm integrates group fairness constraints into offline contextual bandit policy optimization through a Lagrangian framework. The core innovation lies in simultaneously optimizing for reward and fairness by constraining the disparity in expected rewards across different groups. A doubly robust estimator is employed to reduce estimation bias and variance, leveraging both importance weighting and regression components. The policy gradient updates are performed with respect to both the reward objective and the fairness constraint, with the Lagrange multiplier adjusted iteratively to balance these competing objectives.

## Key Results
- GC-PG effectively reduces reward disparities across groups while maintaining competitive overall performance
- Achieves Pareto optimal fairness-performance trade-offs on synthetic and real-world datasets
- Theoretical convergence to stationary point established at rate O(1/T)

## Why This Works (Mechanism)
The mechanism succeeds by explicitly incorporating group-sensitive fairness constraints into the optimization objective rather than treating fairness as a post-hoc adjustment. The Lagrangian formulation allows the algorithm to find a balance between maximizing reward and minimizing disparity across groups. The doubly robust estimator provides more stable and accurate reward estimates by combining importance weighting with regression, reducing the impact of distribution shift between behavior and target policies.

## Foundational Learning
1. **Offline contextual bandits** - Learning policies from logged data without online interaction; needed to handle real-world scenarios where online experimentation is costly or risky; quick check: understand the difference between online and offline policy learning
2. **Group fairness constraints** - Mathematical formulation of fairness as constraints on expected outcomes across sensitive groups; needed to move beyond ad-hoc fairness definitions; quick check: can formulate group fairness as mathematical constraints
3. **Policy gradient methods** - Gradient-based optimization of parameterized policies; needed for continuous policy optimization in high-dimensional action spaces; quick check: understand REINFORCE-style updates
4. **Doubly robust estimation** - Estimation technique combining importance weighting and regression to reduce variance and bias; needed for accurate value estimation from offline data; quick check: understand the bias-variance tradeoff in off-policy evaluation
5. **Lagrangian optimization** - Method for handling constrained optimization by incorporating constraints into the objective; needed to balance reward maximization with fairness constraints; quick check: can solve simple constrained optimization problems
6. **Stationary point convergence** - Theoretical guarantee about optimization algorithm convergence; needed to establish reliability of the optimization process; quick check: understand what constitutes a stationary point in non-convex optimization

## Architecture Onboarding

**Component Map:**
Policy network -> Reward estimator (doubly robust) -> Lagrange multiplier update -> Group disparity constraint -> Policy gradient update

**Critical Path:**
1. Estimate expected reward using doubly robust estimator
2. Compute group-wise expected rewards
3. Calculate disparity across groups
4. Update Lagrange multiplier based on constraint violation
5. Update policy parameters using modified gradient

**Design Tradeoffs:**
- Fairness constraint tightness vs. reward performance
- Complexity of doubly robust estimator vs. estimation accuracy
- Number of groups vs. computational overhead
- Frequency of Lagrange multiplier updates vs. convergence stability

**Failure Signatures:**
- High variance in policy updates indicating unstable estimation
- Persistent constraint violation suggesting inappropriate Lagrange multiplier schedule
- Degradation in overall reward performance indicating overly restrictive fairness constraints
- Slow convergence suggesting poor step-size selection

**First 3 Experiments:**
1. Verify disparity reduction on synthetic dataset with known group structure
2. Compare policy performance across different Lagrange multiplier values
3. Test sensitivity to doubly robust estimator hyperparameters

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes standard smoothness and Lipschitz conditions without discussing their tightness
- Doubly robust estimator effectiveness depends heavily on quality of behavior policy estimates
- Experiments limited to two real-world datasets, constraining generalizability
- Computational overhead from fairness constraints not thoroughly evaluated for large-scale applications

## Confidence
- **High confidence** in algorithm's ability to reduce reward disparities across groups
- **Medium confidence** in theoretical convergence guarantees due to standard assumptions
- **Medium confidence** in overall effectiveness claim given limited baseline comparisons

## Next Checks
1. **Scalability assessment**: Evaluate GC-PG's performance on datasets with larger action spaces and more complex policy architectures
2. **Generalization testing**: Apply the algorithm to additional real-world datasets across different domains (e.g., healthcare, finance)
3. **Behavior policy sensitivity**: Conduct experiments varying the quality of behavior policy estimates to quantify impact on doubly robust estimator performance and overall algorithm effectiveness