---
ver: rpa2
title: 'GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection'
arxiv_id: '2505.13312'
source_url: https://arxiv.org/abs/2505.13312
tags:
- unlearning
- arxiv
- forget
- preprint
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GUARD enables generation-time unlearning of large language models
  without model retraining. It uses a prompt classifier to detect forget targets,
  extracts forbidden tokens from original answers, and applies token-level hard matching
  combined with SBERT-based semantic soft matching to dynamically suppress target
  content during beam search decoding.
---

# GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection

## Quick Facts
- **arXiv ID:** 2505.13312
- **Source URL:** https://arxiv.org/abs/2505.13312
- **Reference count:** 40
- **Primary result:** Generation-time unlearning without model retraining achieves highest forget quality while preserving utility across TOFU, MUSE, and Harry Potter datasets

## Executive Summary
GUARD introduces a generation-time unlearning framework that enables LLMs to forget specific content without retraining. The method employs a prompt classifier to detect forget targets, extracts forbidden tokens from original answers, and dynamically suppresses target content during beam search decoding using dual token and semantic matching. Evaluated on three benchmarks (TOFU, MUSE, Harry Potter), GUARD achieves superior forget quality while maintaining model utility compared to training-based and training-free baselines, without causing catastrophic forgetting.

## Method Summary
GUARD operates in three stages: (1) a lightweight MLP classifier trained on frozen LLM embeddings detects prompts requiring unlearning, (2) SBERT-based retrieval finds the most similar answer from the forget set to extract forbidden tokens, and (3) modified beam search applies token-level hard matching (via trie) and SBERT-based semantic soft matching to dynamically suppress target content. The framework uses infinite penalties for exact matches or high semantic similarity (δ=0.5), with proportional soft penalties otherwise. Evaluation uses Llama2-7B, Phi-1.5B, and OPT-2.7B base models across TOFU (entity unlearning), Harry Potter (copyright content), and MUSE-News (news unlearning) datasets.

## Key Results
- Achieves highest forget quality (FQ) scores across TOFU, MUSE, and Harry Potter benchmarks
- Maintains model utility (MU) while significantly reducing privacy leakage compared to training-based methods
- Outperforms both training-based and training-free baselines without requiring model retraining
- Shows minimal catastrophic forgetting effects on general knowledge tasks

## Why This Works (Mechanism)

### Mechanism 1: Prompt Classification as Forget Target Detector
A lightweight MLP classifier trained on frozen LLM embeddings accurately identifies prompts requiring unlearning with minimal false positives/negatives. The classifier takes penultimate layer embeddings (averaged across tokens, masked by attention) as input and outputs binary probability of forget-class membership, acting as a gate—only classified-forget prompts trigger downstream suppression. The embedding space of the original LLM contains linearly separable structure distinguishing forget-domain prompts from retain/general prompts. [abstract] "We first employ a prompt classifier to detect unlearning targets" [section B.2, Table 7] FNR=0.0 on original test sets across TOFU/HP/MUSE; FPR on general utility set ≤0.0057 [corpus] DRAGON (arXiv:2511.05784) independently validates detection-based unlearning gating. Break condition: Embeddings of adversarial paraphrases shift out-of-distribution; classifier FNR rises sharply (>5% on paraphrased tests in Table 7), allowing forget-target prompts to bypass suppression.

### Mechanism 2: SBERT-based Retrieval of Forbidden Tokens
Semantic similarity search retrieves the correct original answer with high accuracy, enabling extraction of forbidden tokens even under prompt rephrasing. Given a forget-classified prompt, SBERT encodes query and all forget-set answers; top-1 cosine similarity retrieves the most relevant answer. Forbidden tokens (key phrases) are then extracted from this answer. Paraphrased prompts maintain sufficient semantic overlap with original forget-set entries for SBERT to recover the correct source answer. [section 4.3] "similarity function sim(·, ·) is implemented using SBERT" [section C, Table 8] Top-1 retrieval accuracy: 94.63% on TOFU 1% with SBERT alone; 97.44% with SBERT+RoBERTa reranking [corpus] Limited direct corpus evidence for retrieval mechanisms; neighboring papers focus on training-based approaches. Break condition: Query drift—when user prompts are highly adversarial or domain-shifted, retrieval accuracy degrades below retrieval threshold, causing wrong forbidden tokens to be extracted.

### Mechanism 3: Dual-Matching Beam Search Suppression
Combining hard token-matching (trie-based) with soft semantic matching (SBERT-based) during beam search prevents both verbatim and paraphrastic leakage of forbidden content. At each decoding step, candidate tokens receive total penalty P_total = P_token + P_sbert. Hard matching assigns infinite penalty if suffix matches forbidden sequences (β=1). Soft matching computes cosine similarity between last word embedding and forbidden token embeddings; infinite penalty if max similarity ≥ δ (0.5), else soft proportional penalty. Penalized candidates are pruned or ranked lower by total cost C = -log P(t_n+1|T_1:n) + P_total. Forbidden content is captured by a bounded set of token sequences and semantically similar synonyms; the combination covers the leakage surface without over-blocking fluent continuations. [abstract] "dynamically penalize and filter candidate tokens during generation using a combination of token matching and semantic matching" [section 4.4, Equations 8-11] Formal penalty definitions [section 5.5.2, Table 5] Ablation: w/o Trie drops FQ from 0.1649 to 0.0541; w/o SBERT drops FQ to 0.0030 [corpus] BLUR benchmark (arXiv:2506.15699) highlights forget-retain overlap challenges—indirectly supports dual-matching necessity. Break condition: Semantic drift in generated synonyms exceeds δ but shares no token overlap; SBERT threshold too permissive (δ=0.7 in Table 12 drops FQ to 0.0970), allowing near-synonym leaks.

## Foundational Learning

- **Concept: Beam Search Decoding**
  - Why needed here: GUARD modifies beam search scoring to inject penalties; understanding how beams expand and prune is essential for debugging suppression failures.
  - Quick check question: If beam width is 5 and 3 candidates receive infinite penalty at step n+1, how many beams proceed to step n+2?

- **Concept: Trie Data Structure for Token Sequences**
  - Why needed here: Hard matching requires efficient suffix lookup against forbidden token sequences; trie enables O(k) match detection where k is sequence length.
  - Quick check question: Given forbidden tokens ["civil engineer", "civil servant"], does the trie accept "civil" as a complete match, a partial match, or no match?

- **Concept: Cosine Similarity in Embedding Space**
  - Why needed here: Soft matching relies on SBERT embeddings and cosine similarity to detect semantic proximity to forbidden content.
  - Quick check question: Two vectors have cosine similarity 0.5. If δ=0.5, does this trigger infinite penalty or soft penalty?

## Architecture Onboarding

- **Component map:** Prompt → Embedding Extraction → MLP Classification → (if forget) SBERT Retrieval → Token Extraction → Trie Construction → Beam Search with Dual Penalties → Output

- **Critical path:** Prompt → Embedding Extraction → MLP Classification → (if forget) SBERT Retrieval → Token Extraction → Trie Construction → Beam Search with Dual Penalties → Output

- **Design tradeoffs:**
  - Beam width b: Higher b improves suppression coverage (more candidates to penalize) but increases latency. Table 12 shows b=7 optimal for TOFU 1%.
  - Similarity threshold δ: Lower δ catches more semantic variants but risks over-blocking; δ=0.5 balances recall/precision.
  - Hard match threshold β: β=1 (any non-zero match = infinite penalty) is aggressive; increase to β>1 for partial-match tolerance.
  - Forbidden token extraction: LLM-based extraction (ChatGPT-4o-mini) improves F-RL alignment but adds dependency; "all words" extraction is dependency-free but less precise (Table 4).

- **Failure signatures:**
  - Classifier miss: Forget-target prompt generates original answer verbatim → check classifier FNR on prompt distribution
  - Retrieval error: Suppressed output blocks wrong content or misses target → check retrieval accuracy on paraphrased prompts
  - Over-suppression: Fluent retain-set outputs become disjointed or truncated → δ too low or forbidden tokens over-extracted
  - Under-suppression: Synonym/paraphrase leaks through → δ too high; check SBERT similarity scores on leaked outputs

- **First 3 experiments:**
  1. **Classifier stress test:** Evaluate MLP on adversarial paraphrases (jailbreak, irrelevant context) from Table 7 test sets; plot FNR vs. perturbation intensity to characterize detection boundary.
  2. **Retrieval degradation analysis:** Construct progressively paraphrased forget prompts; measure top-1 retrieval accuracy and correlate with downstream FQ to quantify retrieval-to-suppression coupling.
  3. **Penalty sensitivity sweep:** Vary δ ∈ {0.3, 0.5, 0.7} and β ∈ {1, 2, 3} on held-out forget/retain mix; plot FQ vs. MU tradeoff frontier to calibrate operating point for deployment constraints.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can generation-time unlearning methods be refined to effectively mitigate privacy leakage risks as measured by membership inference attacks (MIA)?
  - **Basis in paper:** [explicit] Appendix A.2 explicitly lists the "suboptimal performance in privacy leakage evaluation on the MUSE dataset" as a limitation, noting that GUARD exhibits high PrivLeak scores (109.6) similar to previous baselines.
  - **Why unresolved:** The current token suppression strategy prevents the generation of sensitive text but does not sufficiently alter the model's internal activations or loss landscapes to obscure membership information from MIAs.
  - **What evidence would resolve it:** A modification to the GUARD framework that achieves a PrivLeak score significantly closer to zero on the MUSE benchmark while maintaining the current high level of Forget Quality.

- **Open Question 2:** To what extent can GUARD maintain its high forget quality when forbidden tokens are extracted solely by the base model rather than stronger external models like GPT-4o-mini?
  - **Basis in paper:** [inferred] Section 5.5.1 ablates extraction strategies and notes that using ChatGPT-4o-mini serves as a "theoretical upper bound." While internal model extraction was tested, the performance gap suggests a dependency on external capabilities for optimal results.
  - **Why unresolved:** The paper demonstrates that internal extraction is viable but implies a trade-off in quality. It remains unclear if the performance drop is due to the base model's lack of capability or a fundamental limitation of self-supervised extraction.
  - **What evidence would resolve it:** A comparative analysis showing that internal extraction methods (e.g., Llama2-7B self-extraction) achieve statistically equivalent Forget Quality (FQ) and ROUGE-L scores to the external GPT-4o-mini baseline.

- **Open Question 3:** What is the sensitivity of GUARD's unlearning performance to errors in the semantic retrieval step when the input query deviates significantly from the forget set?
  - **Basis in paper:** [inferred] Section 4.3 relies on a semantic similarity retrieval ($A^*$) to find forbidden tokens. If the input query $x$ is adversarial or out-of-distribution, the retrieval might fail to identify the correct forbidden tokens, rendering the beam search penalties ineffective.
  - **Why unresolved:** The method assumes a robust mapping between the input query and the forget data $D_f$. The paper does not analyze failure cases where the retrieval step returns an irrelevant answer, which would bypass the unlearning logic.
  - **What evidence would resolve it:** Evaluation results showing the degradation curve of Forget Quality (FQ) as the semantic distance between the input query and the nearest neighbor in $D_f$ increases.

## Limitations
- **Privacy leakage:** GUARD shows suboptimal performance in privacy leakage evaluation on MUSE dataset, with high PrivLeak scores similar to previous baselines.
- **Classifier robustness:** The MLP classifier achieves perfect detection on original test sets but degrades to 5-10% FNR on paraphrased and adversarial variants, creating systematic bypass opportunities.
- **Semantic matching sensitivity:** The δ=0.5 threshold for soft matching is empirically chosen without theoretical justification, requiring per-domain tuning and risking either over-suppression or semantic leaks.

## Confidence

**High Confidence (Likelihood >90%)**
- The three-stage framework (classification → retrieval → beam search suppression) is technically sound and correctly implemented
- GUARD outperforms training-free baselines on forget quality metrics across all three datasets
- The dual-matching approach (hard + soft) provides better suppression than either method alone

**Medium Confidence (Likelihood 70-90%)**
- GUARD maintains model utility while achieving strong forgetting (FQ/MU tradeoff is acceptable but not optimal)
- The classifier's performance on paraphrased prompts represents the worst-case adversarial scenario
- SBERT retrieval with cosine similarity is the most reliable approach for forbidden token identification

**Low Confidence (Likelihood <70%)**
- The system's robustness against sophisticated jailbreak attacks (multiple-step reasoning, irrelevant context injection)
- Generalizability across diverse LLM architectures beyond the tested base models
- Long-term effectiveness as model weights drift or when applied to continually updated knowledge

## Next Checks

1. **Adversarial Robustness Benchmark:** Construct a comprehensive test suite of paraphrased, semantically equivalent, and adversarial prompts targeting each forget-set entry. Measure classifier FNR, retrieval accuracy, and downstream FQ systematically across perturbation intensity levels. Plot ROC curves to quantify detection capability boundaries.

2. **Semantic Drift Analysis:** Generate outputs for forget-set prompts and extract candidate forbidden tokens using SBERT similarity. Manually annotate which semantic variants successfully leak through current δ=0.5 threshold. Calculate the false negative rate for semantic matching and identify common patterns (synonym types, paraphrasing strategies) that evade detection.

3. **Cross-Domain Transfer Study:** Apply GUARD trained on TOFU to Harry Potter and MUSE datasets without retraining classifier or forbidden token sets. Measure degradation in FQ and MU to quantify domain generalization capability. Analyze which components (classification, retrieval, or suppression) fail first when transferred to new knowledge domains.