---
ver: rpa2
title: Why Do We Need Warm-up? A Theoretical Perspective
arxiv_id: '2510.03164'
source_url: https://arxiv.org/abs/2510.03164
tags:
- warm-up
- page
- have
- training
- dist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the theoretical gap in understanding why\
  \ learning rate warm-up improves deep learning training. The authors propose a new\
  \ smoothness condition, (H\u2080,H\u2081)-smoothness, which bounds local curvature\
  \ as a linear function of loss sub-optimality."
---

# Why Do We Need Warm-up? A Theoretical Perspective

## Quick Facts
- **arXiv ID**: 2510.03164
- **Source URL**: https://arxiv.org/abs/2510.03164
- **Reference count**: 40
- **Primary result**: Introduces a new smoothness condition, $(H_0, H_1)$-smoothness, and proves that warm-up improves convergence under this condition, both theoretically and empirically.

## Executive Summary
This paper addresses the theoretical gap in understanding why learning rate warm-up improves deep learning training. The authors propose a new smoothness condition, $(H_0,H_1)$-smoothness, which bounds local curvature as a linear function of loss sub-optimality. They demonstrate both theoretically and empirically that this condition holds for neural networks trained with MSE and cross-entropy losses. Under this assumption, they prove that gradient descent with a warm-up schedule achieves faster convergence than with a fixed step-size, establishing upper and lower complexity bounds. Experiments on language and vision models confirm the practical benefits of the theoretically motivated warm-up schedule, showing competitive performance with standard linear warm-up.

## Method Summary
The paper introduces a novel smoothness condition, $(H_0, H_1)$-smoothness, which bounds the Hessian norm as $\|\nabla^2 f(w)\| \le H_0 + H_1(f(w) - f^*)$. Under this condition, the authors derive a convergence guarantee for gradient descent with an adaptive step-size schedule $\eta_k = \frac{1}{10H_0 + 20H_1(f(w_k) - f^*)}$. For practical implementation, they propose a warm-up schedule that scales the peak learning rate by the ratio of current batch loss to a tuned constant $C$. The empirical validation involves estimating the smoothness condition using finite differences on trained models and comparing the proposed warm-up to linear warm-up and no warm-up baselines on language and vision tasks.

## Key Results
- The $(H_0, H_1)$-smoothness condition is empirically validated for neural networks trained with MSE and cross-entropy losses, showing a linear relationship between Hessian norm and loss sub-optimality during warm-up.
- Gradient descent with the theoretically motivated warm-up schedule achieves faster convergence (lower iteration complexity) than with a fixed step-size under the proposed smoothness condition.
- The practical $(H_0, H_1)$ warm-up schedule performs competitively with standard linear warm-up on language modeling and vision tasks, with the optimal constant $C$ tuned per task.

## Why This Works (Mechanism)
The mechanism behind the effectiveness of warm-up is rooted in the $(H_0, H_1)$-smoothness condition. In the early stages of training, the loss is far from optimal, and the local curvature (Hessian norm) is high. A large fixed learning rate can cause divergence. The warm-up schedule starts with a small learning rate, allowing the model to make progress in flatter regions. As the loss decreases and the curvature reduces, the learning rate can be increased safely, leading to faster convergence than a fixed schedule.

## Foundational Learning
- **Smoothness Conditions**: Standard assumptions in optimization that bound the change in gradient; needed to prove convergence rates for gradient-based methods. Quick check: Verify that the standard $\beta$-smoothness implies $\|\nabla f(w) - \nabla f(v)\| \le \beta\|w-v\|$.
- **Gradient Descent Convergence**: Understanding the proof techniques for GD convergence rates under smoothness assumptions. Quick check: Derive the convergence rate for GD on a $\beta$-smooth convex function.
- **Curvature and Conditioning**: The Hessian norm measures local curvature; ill-conditioned problems (high curvature) are harder to optimize. Quick check: Compute the Hessian of a simple quadratic function and analyze its eigenvalues.
- **Sub-optimality**: The difference between the current loss and the optimal loss, $f(w) - f^*$; a key quantity in tracking progress. Quick check: Calculate $f(w) - f^*$ for a simple convex function given a current point $w$.
- **Iteration Complexity**: The number of gradient evaluations needed to reach a desired accuracy; the main metric for comparing optimization algorithms. Quick check: Compare the iteration complexities of GD and Nesterov's accelerated gradient for $\epsilon$-accuracy.

## Architecture Onboarding
- **Component Map**: Data -> Model (Transformer/ViT/ResNet) -> Loss (Cross-Entropy/MSE) -> Optimizer (AdamW/SGD) -> Scheduler (Warm-up/Linear/No-warm-up) -> Validation Metrics.
- **Critical Path**: The warm-up schedule modifies the learning rate based on the current batch loss, which affects the gradient update and, consequently, the training trajectory and final performance.
- **Design Tradeoffs**: The proposed warm-up schedule requires tuning the constant $C$, which adds a hyperparameter. The theoretical schedule is adaptive but requires knowledge of $f^*$, which is impractical.
- **Failure Signatures**: If the batch loss is too noisy, the adaptive learning rate can spike or drop erratically, leading to divergence or slow training. If $C$ is poorly chosen, the warm-up may be too aggressive or too conservative.
- **First Experiments**:
  1. Implement the theoretical update rule on a synthetic convex function satisfying $(H_0, H_1)$-smoothness to verify the convergence rate.
  2. Train a small Transformer on a subset of FineWeb using SGD with a small LR to estimate and plot the smoothness $L_k$ vs $f(w_k)$.
  3. Implement the practical warm-up schedule and compare "No Warm-up", "Linear Warm-up", and "$(H_0, H_1)$ Warm-up" on the 70M parameter language model.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can curvature upper bounds be identified that remain valid across the entire training trajectory, rather than just the warm-up phase? The authors observe a phase transition where the proposed $(H_0, H_1)$-smoothness bound begins to deteriorate after the warm-up stage.
- **Open Question 2**: Can the $(H_0, H_1)$-smoothness condition be extended to a layer-wise setting to tighten the smoothness bound? Different network blocks may exhibit varying conditioning, and the dependency of the final loss on each block requires deeper understanding.
- **Open Question 3**: Can the convergence bounds for Gradient Descent under the generalized smoothness condition (power $\rho > 1$) be further tightened? The current analysis reduces the $\rho > 1$ case to $\rho = 1$ using an inequality, which may result in looser complexity bounds than a direct analysis would yield.

## Limitations
- The practical warm-up schedule requires tuning the constant $C$, which is not derived from the theoretical constants and adds a hyperparameter.
- The $(H_0, H_1)$-smoothness condition is empirically validated for MSE and cross-entropy losses on specific architectures (Transformers, ResNets) but may not generalize to other loss functions or architectures without further investigation.
- The theoretical analysis assumes full gradient information, while the empirical validation uses stochastic gradients, which introduces additional variance not accounted for in the theory.

## Confidence
- **High Confidence**: The mathematical proof of Theorem 4.2 under the $(H_0, H_1)$-smoothness assumption is sound.
- **Medium Confidence**: The empirical verification that the smoothness condition holds for the tested models and datasets is convincing, but the practical warm-up schedule's effectiveness is contingent on the tuned constant $C$.
- **Medium Confidence**: The claim that the $(H_0, H_1)$ warm-up is competitive with linear warm-up is supported by the experiments, but the comparison is not exhaustive across all training configurations.

## Next Checks
1. **Cross-Dataset Validation**: Train the same model architectures on different datasets (e.g., CIFAR-10 for vision, WikiText-103 for language) to confirm the $(H_0, H_1)$ smoothness condition and the warm-up schedule's benefits are not dataset-specific.
2. **Architectural Generalization**: Apply the smoothness analysis and warm-up schedule to a fundamentally different architecture, such as a Transformer with a different attention mechanism (e.g., Performer) or a recurrent model (e.g., LSTM).
3. **Loss Function Robustness**: Investigate the smoothness condition for other loss functions, such as Huber loss or triplet loss, to determine if the linear curvature bound is a general phenomenon or specific to MSE and cross-entropy.