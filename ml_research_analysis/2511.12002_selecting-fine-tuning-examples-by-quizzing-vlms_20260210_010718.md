---
ver: rpa2
title: Selecting Fine-Tuning Examples by Quizzing VLMs
arxiv_id: '2511.12002'
source_url: https://arxiv.org/abs/2511.12002
tags:
- images
- quizrank
- lora
- fine-tuning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces QZLoRA, a method for selecting high-quality
  training images for fine-tuning text-to-image diffusion models using a visual question-answering
  approach. It leverages QuizRank, which ranks images based on how well a vision-language
  model can answer questions about their visual properties.
---

# Selecting Fine-Tuning Examples by Quizzing VLMs

## Quick Facts
- arXiv ID: 2511.12002
- Source URL: https://arxiv.org/abs/2511.12002
- Authors: Tenghao Ji; Eytan Adar
- Reference count: 25
- Primary result: QZLoRA improves photorealistic image generation accuracy by 54.99% over baselines by selecting high-quality training images via VLM-based quizzing

## Executive Summary
This paper introduces QZLoRA, a method for selecting high-quality training images for fine-tuning text-to-image diffusion models using a visual question-answering approach. It leverages QuizRank, which ranks images based on how well a vision-language model can answer questions about their visual properties. By selecting top-ranked images for LoRA fine-tuning, QZLoRA produces more visually accurate and representative outputs than random sampling or baseline models. Evaluations show improved QuizRank accuracy (up to 54.99% for photorealistic images), better alignment with target concepts, and reduced variance across topics.

## Method Summary
QZLoRA uses QuizRank to select training images by generating multiple-choice questions from Wikipedia text using GPT-4o, then having a VLM rank candidate images based on how accurately they can answer these questions when the image is provided as visual context. The top-k images are used to fine-tune Stable Diffusion 1.5 using LoRA with specific hyperparameters (epochs=20, num_repeats=5, batch_size=1, lr=1e-4, AdamW8bit, 512Ã—512). The method is evaluated on 60 topics from Wikipedia across Biology, Architecture, Food & Drink, and Art categories using Wikimedia Commons images, with generated outputs scored by the same QuizRank mechanism.

## Key Results
- QuizRank accuracy improved by up to 54.99% for photorealistic images compared to baselines
- Positive correlation (r=0.920 at k=15) between input image quality and output image quality
- Reduced variance across topics compared to random sampling methods
- Effective for both photorealistic and stylized image generation tasks

## Why This Works (Mechanism)

### Mechanism 1: Informativeness-Based Data Filtering
The QuizRank algorithm generates questions about distinguishing visual features and ranks images based on how well a VLM can answer these questions using the image as visual aid. High-ranking images contain the necessary visual features to resolve the questions, acting as a semantic filter that removes ambiguous or unrepresentative samples before training.

### Mechanism 2: Contrastive Concept Grounding
The question generator uses textual descriptions of target and similar "distractor" classes, forcing the VLM to focus on differences. This favors images that visually manifest distinguishing traits, allowing LoRA to adapt the diffusion model specifically to these discriminating features rather than generic attributes.

### Mechanism 3: Semantic Feedback Loop via QuizRank Evaluation
Using the same VLM-based quizzing mechanism for evaluation provides a semantic consistency metric that is more robust to style than pixel-space metrics. The questions are derived from semantic definitions rather than reference images, evaluating whether generated images contain required semantic parts rather than just matching color histograms.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Parameter-efficient method used to inject selected data into diffusion model
  - Quick check: How does updating low-rank decomposition matrices preserve base model's general knowledge while learning specific concepts?

- **Concept: Vision-Language Models (VLMs) as Judges**
  - Why needed here: VLMs act as proxy for human visual understanding during ranking phase
  - Quick check: Why might a VLM be better at ranking "semantic representativeness" than similarity metrics like CLIP-Score?

- **Concept: Diffusion Model Priors**
  - Why needed here: Base model is assumed to "know" general concepts but lacks specific topic fidelity
  - Quick check: What happens to diffusion model's output if fine-tuning dataset contradicts model's strong pre-existing priors?

## Architecture Onboarding

- **Component map:** Knowledge Source (Wikipedia) -> Question Generator (LLM) -> Candidate Pool (Wikimedia Commons) -> Ranker (VLM) -> Trainer (LoRA) -> Generator
- **Critical path:** Contrastive Question Generation phase - if questions are irrelevant, Ranker selects non-representative images and LoRA optimizes wrong features
- **Design tradeoffs:** k-value quantity vs. stability (k=15 yields r=0.920, k=2 yields r=0.732); prompting style uses positive prompts from Wikipedia but relies on negative prompts to suppress style bleeding
- **Failure signatures:** Broad taxa (no representative image exists), instance ambiguity (wild angle/lighting variation), VLM blindness (cannot distinguish subtle features)
- **First 3 experiments:** 1) VLM Ablation: Replace GPT-4o ranker with CLIP-Score selector; 2) Distractor Analysis: Compare performance on unique objects vs. visually similar categories; 3) K-Stability Test: Sweep k from 2-50 to identify diminishing returns

## Open Questions the Paper Calls Out

1. Can clustering images based on which specific QuizRank questions they help answer correctly enable training separate LoRA models for visually distinct subgroups within broad topic categories?
2. Does QZLoRA transfer effectively to diffusion models beyond Stable Diffusion 1.5?
3. Does applying style transfer to QuizRank-selected photorealistic inputs before fine-tuning improve non-photorealistic generation quality?
4. Can the initial distribution of QuizRank scores across candidate images predict the optimal number (k) of images needed for effective fine-tuning?

## Limitations

- The approach underperforms for illustration-style outputs compared to photorealistic images, suggesting domain limitations
- Exact prompts for contrastive question generation and distractor class selection methods are unspecified, requiring reference to external work
- Weak validation of semantic evaluation loop with limited evidence that QuizRank correlates with human perceptual quality across different domains

## Confidence

- **High confidence**: Core premise of filtering training data via VLM-based quizzing improves LoRA efficiency is well-supported by correlation results
- **Medium confidence**: Contrastive concept grounding mechanism is plausible but relies on assumptions about VLM capability not fully validated
- **Low confidence**: Semantic feedback loop using QuizRank for evaluation is most speculative component with limited cross-domain validation

## Next Checks

1. VLM Ablation Study: Replace GPT-4o ranker with CLIP-Score to test whether quizzing approach is superior to simple similarity-based selection
2. Distractor Impact Analysis: Systematically test QZLoRA on topics with varying levels of visual similarity to distractors
3. Stability Across k Values: For representative topics, sweep k parameter from 2-50 to identify optimal value and test stability claims