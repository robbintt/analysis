---
ver: rpa2
title: Differentially Private High-dimensional Variable Selection via Integer Programming
arxiv_id: '2510.22062'
source_url: https://arxiv.org/abs/2510.22062
tags:
- have
- algorithm
- then
- support
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new differentially private algorithms
  for high-dimensional variable selection using mixed integer programming (MIP). The
  methods, called top-R and mistakes, leverage modern MIP techniques to efficiently
  explore the non-convex objective landscape without exhaustive combinatorial search.
---

# Differentially Private High-dimensional Variable Selection via Integer Programming

## Quick Facts
- arXiv ID: 2510.22062
- Source URL: https://arxiv.org/abs/2510.22062
- Reference count: 40
- Primary result: Introduces two DP algorithms (top-R and mistakes) for variable selection using MIP, achieving state-of-the-art support recovery in high-dimensional settings

## Executive Summary
This paper presents two novel differentially private algorithms for high-dimensional variable selection using mixed integer programming. The methods overcome the computational intractability of the exponential mechanism by leveraging modern MIP techniques to efficiently explore the non-convex objective landscape. The top-R method provides pure DP guarantees through a structured truncation of the exponential mechanism, while the mistakes method partitions supports by Hamming distance to achieve privacy via objective stability. Both approaches demonstrate strong theoretical support recovery guarantees and empirical performance on problems with up to 10,000 variables.

## Method Summary
The paper introduces two DP algorithms for Best Subset Selection (BSS) in sparse regression and classification. The top-R method truncates the exponential mechanism by enumerating the top R supports via MIP and sampling from a modified distribution that aggregates tail probability. The mistakes method partitions supports based on Hamming distance from the optimal solution, sampling over these partitions. Both methods use an outer approximation algorithm with cutting planes to solve the NP-hard BSS problem efficiently. Theoretical guarantees are provided for support recovery under identifiability margin conditions, and extensive experiments validate performance against state-of-the-art methods.

## Key Results
- Top-R and mistakes methods achieve state-of-the-art support recovery in high-dimensional sparse regression and classification
- Theoretical support recovery guarantees provided for Best Subset Selection under identifiability margin conditions
- Methods scale to problems with up to 10,000 variables, outperforming MCMC-based approaches
- Top-R method achieves pure DP with mild identifiability condition; mistakes method requires slightly stronger but still realistic separation assumption

## Why This Works (Mechanism)

### Mechanism 1: Structured Truncation of the Exponential Mechanism (Top-R)
The algorithm efficiently achieves pure DP without enumerating all $\binom{p}{s}$ subsets by truncating the sampling distribution to the top-$R$ solutions. It solves a MIP to find top $R$ supports, samples index $k \le R$ with probability $\propto \exp(-\epsilon R(\hat{S}_k)/2\Delta)$, and assigns tail mass to the worst enumerated solution. If $k=R+1$ is drawn, it switches to rejection sampling.

### Mechanism 2: Privacy via Objective Stability (Mistakes)
This method achieves pure DP by partitioning supports based on their Hamming distance (mistakes) from the optimal support. It groups supports into partitions $P_0 \dots P_s$ and samples over them. Privacy holds only if the optimal support is robust—changing one data point doesn't swap it with a significantly worse solution.

### Mechanism 3: MIP Outer Approximation for Efficient Enumeration
The algorithm solves BSS efficiently using cutting-plane methods rather than naive Branch-and-Bound. It starts with a convex relaxation, identifies violated constraints based on gradient information, and iteratively refines the feasible region until integer solutions converge.

## Foundational Learning

- **Concept: Differential Privacy (Exponential Mechanism)**
  - Why needed: This is the constraint the entire architecture is built around
  - Quick check: Can you explain why the standard Exponential Mechanism requires calculating the score $R(S, D)$ for *every* possible subset $S$, and how this paper avoids that?

- **Concept: Best Subset Selection (BSS) vs. Regularization**
  - Why needed: The paper solves BSS (selecting exactly $s$ variables) rather than Lasso (selecting variables via shrinkage)
  - Quick check: What is the "identifiability margin" ($\tau$), and why does a low signal-to-noise ratio or high correlation make it harder for BSS to distinguish the true support?

- **Concept: Mixed Integer Programming (MIP) Basics**
  - Why needed: To understand how the non-convex $\ell_0$ constraint is encoded using binary variables $z$ and how "cuts" work
  - Quick check: In Equation (9), how does the constraint $\beta_i^2 \le \theta_i z_i$ force $\beta_i$ to be zero when $z_i = 0$?

## Architecture Onboarding

- **Component map:** Preprocessor -> MIP Solver -> DP Sampler -> Rejection Sampler (fallback)
- **Critical path:** The MIP Solver is the bottleneck; the entire viability depends on Outer Approximation converging faster than naive search
- **Design tradeoffs:** Top-R vs. Mistakes - use Top-R for robustness under milder assumptions but potentially slower convergence; use Mistakes for better statistical rates in high privacy regimes but risk privacy proof failure if objective gap is small
- **Failure signatures:** Infinite Loop in Rejection Sampler indicates R is too small; MIP Timeout suggests Outer Approximation failed to converge; Privacy Breach (Mistakes) occurs if data is too noisy and gap condition fails
- **First 3 experiments:**
  1. Run Top-R on tiny synthetic dataset (n=50, p=10) and verify output distribution changes smoothly with epsilon through histogram testing
  2. Ablate runtime vs. R by measuring wall-clock time of MIP solver as R increases from 10 to 1000 on p=1000 dataset
  3. SNR sweep comparison between Mistakes and MCMC across varying signal-to-noise ratios to validate high-privacy regime performance

## Open Questions the Paper Calls Out

### Open Question 1
Can the privacy guarantees for the "mistakes" method be derived under a lighter assumption than the current objective gap requirement? The current proof relies on strong separation between best and second-best supports. Resolution would require a proof of $(\epsilon, 0)$-DP under standard boundedness assumptions without requiring high-probability separation condition, or a counter-example showing the condition is necessary.

### Open Question 2
Are the sufficient conditions for support recovery tight regarding lower bounds on the identifiability margin $\tau$? The paper establishes sufficient conditions but doesn't prove if they're strictly necessary for any DP algorithm. Resolution would require deriving minimax lower bounds for support recovery under differential privacy or providing an algorithm achieving recovery with strictly weaker condition on $\tau$.

### Open Question 3
Can rigorous theoretical support recovery guarantees be established for the framework using general convex loss functions like hinge loss? The theoretical analysis is restricted to least squares, while experiments confirm method applies to classification. Resolution would require extending proofs to classification setting by bounding support recovery error for hinge loss under similar identifiability margin constraints.

### Open Question 4
Is there a theoretically optimal way to choose the enumeration parameter $R$ in the "top-R" method to balance privacy loss, computational cost, and support recovery utility? The relationship between $R$ and privacy parameter $\epsilon'$ involves complex logarithmic terms. Resolution would require a theoretical guideline defining optimal $R$ as function of target privacy budget $\epsilon$ and dimensionality $p$ to maximize probability of exact support recovery.

## Limitations
- MIP solver settings (MIP gap, time limits, branch-and-bound parameters) not specified despite being critical for scaling claims
- Exact PGD learning rate schedule and convergence criteria for inner optimization loops are underspecified
- Initialization heuristic details from [5] are incomplete—number of iterations and convergence threshold for warm-start generation not provided

## Confidence

- **High:** Privacy guarantees for Top-R method (Theorem 1), empirical support recovery comparisons (Figures 1, D.6)
- **Medium:** Theoretical identifiability conditions for Mistakes method (Theorem 4), runtime scaling claims for Outer Approximation (Algorithm 2)
- **Low:** Exact sensitivity parameter choices, MIP solver parameter tuning for p=10,000 problems

## Next Checks
1. Run Top-R on tiny synthetic dataset (n=50, p=10) and verify output distribution changes smoothly with epsilon through histogram testing
2. Ablate runtime vs. R by measuring wall-clock time of MIP solver as R increases from 10 to 1000 on p=1000 dataset
3. SNR sweep comparison between Mistakes and MCMC across varying signal-to-noise ratios to validate high-privacy regime performance