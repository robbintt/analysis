---
ver: rpa2
title: 'AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning
  Models with OpenMathReasoning dataset'
arxiv_id: '2504.16891'
source_url: https://arxiv.org/abs/2504.16891
tags:
- problem
- solution
- reasoning
- code
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a winning solution to the AIMO-2 competition
  by developing state-of-the-art mathematical reasoning models. The approach consists
  of three key components: creating a large-scale dataset of 540K unique math problems
  with 3.2M long reasoning solutions, integrating code execution with long reasoning
  models to produce 1.7M high-quality tool-integrated reasoning solutions, and training
  models to select the most promising solution from multiple candidates (achieving
  up to 93.3% accuracy with GenSelect).'
---

# AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset

## Quick Facts
- arXiv ID: 2504.16891
- Source URL: https://arxiv.org/abs/2504.16891
- Reference count: 40
- Primary result: 78.4% accuracy on Comp-Math-24-25 and 64.2% on HLE-Math with OpenMath-Nemotron 32B model

## Executive Summary
The paper presents the winning solution to the AIMO-2 competition through a comprehensive approach to mathematical reasoning. The solution centers on three key innovations: a large-scale OpenMathReasoning dataset containing 540K unique problems with 3.2M detailed solutions, code execution integration that produces 1.7M tool-enhanced reasoning solutions, and a GenSelect method that achieves 93.3% accuracy in selecting optimal solutions from multiple candidates. The resulting OpenMath-Nemotron series of models demonstrates state-of-the-art performance on mathematical reasoning benchmarks, with the 32B model achieving 78.4% accuracy on Comp-Math-24-25 and 64.2% on HLE-Math. The solution includes optimizations for competition time constraints and releases all resources under a commercially permissive license.

## Method Summary
The approach combines three synergistic components to achieve superior mathematical reasoning capabilities. First, a large-scale dataset of 540K unique math problems with 3.2M detailed solutions is created, providing extensive training data. Second, code execution is integrated with long reasoning models to generate 1.7M high-quality tool-integrated solutions, enhancing accuracy through computational verification. Third, the GenSelect method is employed to select the most promising solution from multiple candidates, achieving up to 93.3% accuracy. The models are specifically optimized for the AIMO-2 competition's strict time constraints while maintaining state-of-the-art performance on mathematical reasoning benchmarks. All code, models, and datasets are released under a commercially permissive license for broad accessibility.

## Key Results
- OpenMath-Nemotron 32B model achieves 78.4% accuracy on Comp-Math-24-25
- OpenMath-Nemotron 32B model achieves 64.2% on HLE-Math
- GenSelect method achieves up to 93.3% accuracy in solution selection
- 540K unique math problems with 3.2M long reasoning solutions in OpenMathReasoning dataset
- 1.7M high-quality tool-integrated reasoning solutions generated through code execution

## Why This Works (Mechanism)
The solution's effectiveness stems from addressing multiple aspects of mathematical reasoning challenges. The large-scale OpenMathReasoning dataset provides diverse training examples that capture various problem-solving approaches and mathematical domains. Code execution integration ensures computational accuracy and reduces errors in numerical calculations, while the GenSelect method leverages multiple solution candidates to identify the most reliable approach. The competition-specific optimizations balance solution quality with time constraints, making the approach practical for real-world applications. The combination of these elements creates a robust system that outperforms previous methods on mathematical reasoning benchmarks.

## Foundational Learning
- Mathematical reasoning patterns: Understanding how different problem types require distinct solution strategies is essential for building effective models. Quick check: Verify model performance across diverse mathematical domains.
- Code execution integration: Computational verification improves numerical accuracy and reduces logical errors in solutions. Quick check: Compare accuracy with and without code execution integration.
- Multi-solution selection: Having multiple candidate solutions allows for quality filtering and error correction. Quick check: Measure performance improvement from GenSelect vs single solutions.
- Competition optimization: Time-constrained environments require efficient solution generation without sacrificing accuracy. Quick check: Test models under varying latency requirements.
- Dataset construction methodology: The quality and diversity of training data directly impacts model generalization. Quick check: Analyze dataset coverage across mathematical domains.
- Tool integration dependencies: Computational resources affect deployment feasibility and performance. Quick check: Evaluate performance with limited computational access.

## Architecture Onboarding

**Component Map:** Dataset creation -> Model training -> Code execution integration -> GenSelect -> Benchmark evaluation

**Critical Path:** The most critical sequence is: Dataset creation (540K problems) → Model training with long reasoning solutions (3.2M) → Code execution integration (1.7M tool-enhanced solutions) → GenSelect selection (93.3% accuracy) → Competition performance (78.4% on Comp-Math-24-25)

**Design Tradeoffs:** The solution prioritizes accuracy over computational efficiency by generating multiple solutions and using code execution, which may limit deployment in resource-constrained environments. The competition optimizations trade some generality for time efficiency, potentially reducing effectiveness on non-timed mathematical tasks.

**Failure Signatures:** Poor generalization may occur when encountering mathematical domains underrepresented in the OpenMathReasoning dataset. Performance degradation is likely when code execution resources are unavailable or when computational constraints prevent generating multiple solution candidates for GenSelect.

**3 First Experiments:**
1. Evaluate OpenMath-Nemotron models on GSM8K and MATH benchmarks to assess generalization beyond AIMO-2 competition data
2. Conduct ablation study comparing model performance with code execution disabled to quantify its contribution
3. Test model performance under varying computational resource constraints to validate competition-optimized approach in practical scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- True generalization beyond AIMO-2 competition domains remains uncertain despite strong benchmark performance
- The dataset construction using GPT-4 may introduce biases that limit diversity of mathematical reasoning patterns
- Code execution dependencies may not be available in all deployment scenarios, affecting practical applicability
- Competition-specific optimizations may not translate to general-purpose mathematical reasoning tasks with different requirements

## Confidence

**High confidence:** The dataset creation methodology and basic model architecture are sound and reproducible

**Medium confidence:** The competition-specific optimizations and GenSelect methodology will generalize to other mathematical reasoning tasks

**Medium confidence:** The claimed benchmark performance represents true state-of-the-art capabilities across diverse mathematical domains

## Next Checks

1. Evaluate the OpenMath-Nemotron models on additional mathematical reasoning benchmarks (e.g., GSM8K, MATH) that were not used in the original development to assess true generalization

2. Conduct ablation studies to determine the individual contributions of code execution integration and GenSelect to overall performance

3. Test model performance under varying computational resource constraints to validate the practical applicability of the competition-optimized approach in real-world scenarios