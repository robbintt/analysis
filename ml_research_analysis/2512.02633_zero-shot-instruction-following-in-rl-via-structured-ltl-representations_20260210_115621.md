---
ver: rpa2
title: Zero-Shot Instruction Following in RL via Structured LTL Representations
arxiv_id: '2512.02633'
source_url: https://arxiv.org/abs/2512.02633
tags:
- learning
- policy
- formulae
- rook
- bishop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of training RL agents to follow
  complex LTL instructions in environments where multiple atomic propositions can
  be true simultaneously. Existing methods struggle with this because they treat different
  assignments in isolation and don't model interactions between high-level events.
---

# Zero-Shot Instruction Following in RL via Structured LTL Representations

## Quick Facts
- arXiv ID: 2512.02633
- Source URL: https://arxiv.org/abs/2512.02633
- Authors: Mattia Giuri; Mathias Jackermeier; Alessandro Abate
- Reference count: 35
- The method significantly outperforms state-of-the-art baselines on both finite and infinite-horizon LTL tasks in a chess-based environment

## Executive Summary
This paper addresses the challenge of training RL agents to follow complex LTL instructions in environments where multiple atomic propositions can be true simultaneously. Existing methods struggle with this because they treat different assignments in isolation and don't model interactions between high-level events. The authors propose representing LTL instructions as sequences of Boolean formulae extracted from Büchi automata, which capture the conditions needed to make progress toward tasks. These formulae are encoded using graph neural networks (GNNs) to learn structured task representations. The approach is evaluated in a chess-based environment with many possible proposition combinations and demonstrates significant performance improvements over state-of-the-art baselines.

## Method Summary
The authors introduce a novel framework for zero-shot instruction following in RL by converting LTL instructions into structured Boolean formulae that can be processed by graph neural networks. The key insight is that LTL instructions can be decomposed into sequences of Boolean conditions that must be satisfied to make progress toward task completion. These conditions are extracted from the Büchi automaton representation of the LTL formula and encoded as abstract syntax trees. A GNN encoder processes these formula representations, allowing the agent to learn semantic relationships between different task components. The method is evaluated in a chess-based environment where the agent must follow complex instructions involving multiple atomic propositions that can be true simultaneously.

## Key Results
- The LTL-GNN approach significantly outperforms state-of-the-art baselines on both finite and infinite-horizon LTL tasks
- Achieves higher success rates and discounted returns compared to DeepLTL and Transformer baselines
- Demonstrates effectiveness in handling environments with many possible proposition combinations where existing methods struggle

## Why This Works (Mechanism)
The method works by capturing the semantic structure of LTL instructions through their decomposition into Boolean formulae. Unlike previous approaches that treat different truth assignments in isolation, this framework models the logical relationships between propositions. The GNN encoder learns to represent these structured formulae in a way that preserves their semantic meaning, allowing the policy to reason about complex task requirements. By representing instructions as sequences of progress conditions extracted from Büchi automata, the agent can focus on satisfying the logical requirements rather than memorizing specific state-action mappings.

## Foundational Learning

**Linear Temporal Logic (LTL)**: A formal language for specifying properties of systems over time using temporal operators like "always," "eventually," and "until." Why needed: LTL provides a rigorous way to express complex sequential tasks that go beyond simple reward functions. Quick check: Can you write an LTL formula that says "eventually reach the goal and stay there forever"?

**Büchi Automata**: Finite state machines that accept infinite strings by requiring that certain states are visited infinitely often. Why needed: Büchi automata provide a way to represent LTL formulas as state machines that can be used for monitoring and verification. Quick check: Do you understand how Büchi automata differ from standard finite automata in handling infinite executions?

**Graph Neural Networks (GNNs)**: Neural networks that operate on graph-structured data by propagating information between nodes and edges. Why needed: GNNs can learn representations that capture the structural relationships in Boolean formulae while maintaining their semantic meaning. Quick check: Can you explain how message passing works in a GNN?

## Architecture Onboarding

**Component map**: LTL Formula -> Büchi Automaton -> Boolean Formula Sequences -> GNN Encoder -> Policy Network -> Action Selection

**Critical path**: The most important components are the formula extraction from Büchi automata and the GNN encoder. The quality of the Boolean formulae directly impacts the GNN's ability to learn meaningful representations, which in turn determines policy performance.

**Design tradeoffs**: The method trades computational complexity for semantic expressiveness. While processing GNNs over formula ASTs is more expensive than simple embeddings, it preserves the logical structure necessary for generalization.

**Failure signatures**: Poor performance when the template library for formula extraction is too small, leading to fallback to complex DNF representations that lose semantic structure. Also fails when the labeling function mapping observations to atomic propositions is inaccurate.

**3 first experiments**: 
1. Verify that the Büchi automaton construction correctly captures the LTL formula semantics
2. Test the formula extraction procedure on simple LTL formulas to ensure correctness
3. Evaluate the GNN encoder on synthetic formula datasets to verify it learns meaningful representations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the framework be extended to environments where the labeling function mapping observations to atomic propositions is unknown and must be learned jointly with the policy?
- Basis: [explicit] The conclusion explicitly identifies this as a direction for future work, noting that in vision-based environments, the labeling function "may not be known."
- Why unresolved: The current method assumes a pre-defined labeling function $L: S \to 2^{AP}$, and the authors have not yet experimented with learning these high-level event detectors online.
- What evidence would resolve it: A demonstration of the method successfully learning to follow LTL instructions in a vision-based domain without ground-truth state labels, potentially using foundation models.

### Open Question 2
- Question: Does the LTL-GNN architecture maintain its performance advantage over baselines in realistic environments with continuous action spaces?
- Basis: [explicit] The authors state in the conclusion that it "would be interesting to apply our method to larger, more realistic environments with high-dimensional observation and action spaces."
- Why unresolved: The experiments were conducted exclusively in the discrete, grid-based ChessWorld environment, leaving performance in continuous control domains unverified.
- What evidence would resolve it: Benchmark results comparing LTL-GNN to DeepLTL and Transformer baselines in a continuous control domain (e.g., MuJoCo) with LTL objectives.

### Open Question 3
- Question: How does the reliance on a precomputed set of candidate formula templates affect generalization when the agent encounters novel transitions that fall back to disjunctive normal form (DNF)?
- Basis: [inferred] Section 4.3 describes the approximate procedure for extracting Boolean formulae, noting that if no candidate template exists, the system falls back to a potentially large and complex DNF representation.
- Why unresolved: It is unclear if the semantic generalization benefits of the GNN hold when the input representation degrades into a complex DNF rather than a succinct structured formula.
- What evidence would resolve it: An ablation study varying the size of the template library and evaluating success rates on tasks specifically designed to require complex formulae not covered by the templates.

### Open Question 4
- Question: How does the computational efficiency of the GNN encoder scale as the number of atomic propositions increases significantly beyond the five used in ChessWorld?
- Basis: [inferred] The paper motivates the work by the need to handle complex interactions between propositions, but the experimental evaluation is limited to a small set of only five atomic propositions.
- Why unresolved: While the method theoretically handles interactions, the computational cost of constructing and processing the formula ASTs via GNNs may become a bottleneck in environments with hundreds of potential events.
- What evidence would resolve it: Profiling the training time and inference latency of the policy encoder in environments with varying numbers of atomic propositions (e.g., 10, 20, 50).

## Limitations
- The method is tested exclusively on a chess-based environment, limiting generalization claims to more complex domains
- Computational requirements for processing GNNs over formula ASTs are not discussed, raising scalability concerns
- The evaluation protocol for truly novel LTL instructions is not fully detailed, making it unclear if test instructions were genuinely unseen

## Confidence
- **High confidence**: The core technical contribution of using Büchi automata to extract Boolean formulae and encode them with GNNs is well-defined and demonstrated
- **Medium confidence**: The performance improvements over baselines are statistically significant within the chess environment
- **Low confidence**: Claims about generalization to complex, real-world environments and scalability to larger problems

## Next Checks
1. Test the method on at least two additional diverse environments (e.g., grid-world navigation and robotic manipulation) to assess domain generalization
2. Evaluate performance on progressively more complex LTL formulas with nested temporal operators to measure scalability
3. Conduct ablation studies removing the GNN component to quantify the contribution of structured representations versus other architectural choices