---
ver: rpa2
title: Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for Ethiopian
  Languages
arxiv_id: '2503.18253'
source_url: https://arxiv.org/abs/2503.18253
tags:
- emotion
- intensity
- language
- languages
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the EthioEmo dataset by annotating emotion intensity
  for four Ethiopian languages (Amharic, Oromo, Somali, Tigrinya), enabling more nuanced
  emotion analysis. Emotion intensity is annotated on a 4-level scale (0-3) by native
  speakers, and cross-lingual experiments are conducted to explore knowledge transfer.
---

# Enhancing Multi-Label Emotion Analysis and Corresponding Intensities for Ethiopian Languages

## Quick Facts
- arXiv ID: 2503.18253
- Source URL: https://arxiv.org/abs/2503.18253
- Authors: Tadesse Destaw Belay; Dawit Ketema Gete; Abinew Ali Ayele; Olga Kolesnikova; Grigori Sidorov; Seid Muhie Yimam
- Reference count: 23
- Primary result: African language-centric PLMs outperform general multilingual models on emotion classification for Ethiopian languages

## Executive Summary
This paper extends the EthioEmo dataset by adding emotion intensity annotations (0-3 scale) for four Ethiopian languages: Amharic, Oromo, Somali, and Tigrinya. The study benchmarks multilingual PLMs and open-source LLMs on both multi-label emotion classification and intensity prediction tasks, finding that African language-centric models like AfroXLMR significantly outperform general multilingual models. Cross-lingual experiments reveal that script similarity (Ge'ez for Amharic/Tigrinya, Latin for Oromo/Somali) facilitates better transfer learning. The enriched dataset will be publicly released through SemEval2025 Task 11.

## Method Summary
The paper extends the EthioEmo dataset with emotion intensity annotations on a 4-level scale (0-3) for four Ethiopian languages. Native speakers annotated social media text (from news sites, Twitter, YouTube) using the Potato annotation tool. Benchmarking was conducted using multilingual PLMs (LaBSE, RemBERT, mBERT, mDeBERTa, XLM-RoBERTa, AfriBERTa, AfroLM, AfroXLM-R, EthioLLM) and open-source LLMs (Qwen2.5-72B, Dolly-v2-12B, Llama-3-70B, Mistral-8x7B, DeepSeek-R1-70B). PLMs were fine-tuned for 3 epochs with learning rate 5e-5, max tokens 256, and batch size 8. LLMs used zero-shot prompting with specific templates. Multi-label classification used sigmoid outputs with per-label thresholds, while intensity prediction employed regression or ordinal classification.

## Key Results
- African language-centric PLMs (AfroXLMR, AfriBERTa, AfroLM, EthioLLM) generally outperform multilingual models, with AfroXLMR achieving the highest monolingual emotion classification scores (55.98–68.46 F1-Macro).
- Cross-lingual transfer is effective, particularly between Amharic and Tigrinya (same script), with AfroXLM-R achieving the best results.
- Emotion intensity prediction remains challenging, with lower Pearson correlation scores compared to emotion classification, especially for low-resource languages.

## Why This Works (Mechanism)

### Mechanism 1: Language-Centric Pretraining Yields Superior Representations for Low-Resource Languages
African language-centric PLMs outperform general multilingual models because targeted pretraining creates better language-specific representations. Models like AfroXLMR and AfriBERTa, pre-trained on African language corpora, develop embeddings that capture morphological and syntactic patterns specific to these languages, which general multilingual models underrepresent during their pretraining.

### Mechanism 2: Script Similarity Facilitates Cross-Lingual Transfer
Cross-lingual emotion transfer performs better between languages sharing the same writing system due to tokenization and embedding alignment. Amharic and Tigrinya share the Ge'ez script, while Oromo and Somali use Latin script. Shared scripts lead to more similar subword tokenizations, reducing the representation gap that transfer learning must bridge.

### Mechanism 3: Intensity Prediction Requires More Subjective Inference Than Classification
Emotion intensity prediction is fundamentally harder than emotion presence classification because intensity requires ordinal reasoning over subjective continua. Classification tasks map text to discrete labels; intensity prediction requires models to quantify degree along a scale (0-3), demanding finer-grained understanding of lexical modifiers, context, and implicit emotional cues.

## Foundational Learning

- Concept: Multi-label vs. Multi-class Classification
  - Why needed here: The EthioEmo dataset assigns multiple emotion labels per instance, requiring sigmoid outputs with per-label thresholds rather than softmax with argmax.
  - Quick check question: Given a text expressing both anger and disgust, would a multi-class classifier correctly output both labels?

- Concept: Cross-Lingual Transfer Learning
  - Why needed here: The paper evaluates zero-shot transfer by training on three languages and testing on a held-out fourth, measuring how well representations generalize across languages.
  - Quick check question: If you train on Amharic, Oromo, and Somali, what performance drop should you expect when testing on Tigrinya?

- Concept: Pearson Correlation for Ordinal Regression
  - Why needed here: Intensity prediction is evaluated using Pearson correlation between predicted and gold intensity values, measuring linear relationship rather than exact match.
  - Quick check question: If a model consistently predicts intensity scores 0.5 points higher than ground truth, would Pearson correlation penalize this?

## Architecture Onboarding

- Component map: Input text → Transformer encoder (PLM backbone) → Classification head (6 binary outputs) → Intensity head (regression output) → Multi-label predictions + intensity scores

- Critical path:
  1. Load AfroXLMR-large weights
  2. Add task-specific classification head(s) for 6 emotion labels
  3. Fine-tune on target language data with binary cross-entropy loss
  4. For intensity: train separate regression head with MSE or ordinal loss

- Design tradeoffs:
  - AfroXLMR vs. mDeBERTa: AfroXLMR gives +15-20 F1 points for Ethiopian languages but is less generalizable to non-African languages
  - Encoder-only vs. LLM: Encoder models outperform zero-shot LLMs by 20+ F1 points, but LLMs require no fine-tuning data
  - Joint vs. separate intensity modeling: Paper does not explore joint training; separate heads may miss interaction effects

- Failure signatures:
  - mBERT near-zero performance on Amharic/Tigrinya: Indicates script not seen during pretraining
  - LLM intensity predictions near-random (4-33% correlation): Zero-shot prompting insufficient for fine-grained ordinal reasoning
  - Cross-lingual transfer drops 10-15 F1 points vs. monolingual: Script mismatch amplifies domain shift

- First 3 experiments:
  1. Baseline reproduction: Fine-tune AfroXLMR-61L on Amharic training split, evaluate F1-Macro on test split
  2. Cross-lingual probe: Train on Amharic + Oromo + Somali, test on Tigrinya
  3. Intensity head ablation: Compare regression vs. 4-class ordinal classification for intensity prediction

## Open Questions the Paper Calls Out

- Question: Does modeling annotator-level disagreement improve performance on multi-label emotion classification compared to majority vote aggregation?
  - Basis: Authors recommend modeling annotator-level data instead of majority vote for subjective NLP tasks
  - Resolution: Comparative experiments using soft labels, uncertainty modeling, or annotator-aware training vs. majority vote on EthioEmo

- Question: Why do decoder-only LLMs perform significantly worse than encoder-only PLMs on emotion intensity prediction for Ethiopian languages?
  - Basis: LLMs show very low Pearson correlations compared to AfroXLMR
  - Resolution: Ablation studies varying prompt design, few-shot examples, and subword tokenization for Ethiopian scripts

- Question: Does script similarity or language family membership better predict cross-lingual transfer success for emotion tasks?
  - Basis: Cross-lingual transfer is stronger between Amharic-Tigrinya (Ge'ez script) and Oromo-Somali (Latin script), but all four are Afroasiatic languages
  - Resolution: Controlled experiments including Ethiopian languages with mixed scripts and non-Afroasiatic languages using Ge'ez or Latin scripts

## Limitations

- The annotation process introduces significant uncertainty as emotion intensity is inherently subjective, and the paper does not report inter-annotator agreement scores or statistical measures of annotation reliability.
- Cross-lingual transfer results may be influenced by the specific language pairs chosen; the paper focuses on Amharic-Tigrinya transfer due to shared script but doesn't explore whether similar benefits occur between languages sharing other linguistic features.
- The LLM evaluations use zero-shot prompting without exploring prompt engineering variations or few-shot learning approaches that might yield different results.

## Confidence

**High Confidence**: African language-centric PLMs outperform general multilingual models on Ethiopian languages (AfroXLMR achieving 55.98-68.46 F1-Macro vs. mBERT at 29.89 and XLM-RoBERTa at 37.00). Cross-lingual transfer works better between languages sharing the same script (Amharic-Tigrinya).

**Medium Confidence**: Intensity prediction is fundamentally harder than classification due to ordinal reasoning requirements, though alternative explanations include annotation noise or inadequate model architectures.

**Low Confidence**: Script similarity is the primary driver of cross-lingual transfer performance, as the paper shows this correlation but lacks comprehensive testing across multiple language pairs with varying degrees of linguistic similarity but different scripts.

## Next Checks

1. Calculate Cohen's kappa or Krippendorff's alpha for emotion intensity annotations across all four languages to quantify annotation reliability and determine whether low model performance reflects task difficulty or annotation noise.

2. Conduct transfer experiments between language pairs that share linguistic features (vocabulary, grammatical structures) but use different scripts to isolate the effect of script similarity from other linguistic factors.

3. Test various few-shot prompting strategies, temperature settings, and prompt templates for the zero-shot LLM experiments to establish whether the reported poor LLM performance is due to inherent model limitations or suboptimal prompting approaches.