---
ver: rpa2
title: 'A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis:
  Fine-tuning vs. Prompt Engineering vs. RAG'
arxiv_id: '2503.24307'
source_url: https://arxiv.org/abs/2503.24307
tags:
- health
- mental
- text
- fine-tuning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares three approaches for analyzing mental health
  text using LLMs: fine-tuning, prompt engineering, and RAG. Using LLaMA 3 on emotion
  classification and mental health condition detection tasks, fine-tuning achieved
  the highest accuracy (91% for emotions, 80% for conditions) but required substantial
  computational resources.'
---

# A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG

## Quick Facts
- arXiv ID: 2503.24307
- Source URL: https://arxiv.org/abs/2503.24307
- Authors: Arshia Kermani; Veronica Perez-Rosas; Vangelis Metsis
- Reference count: 9
- Key outcome: Fine-tuning achieves highest accuracy (91% for emotions, 80% for conditions) but requires substantial computational resources; zero-shot prompting emerges as second-best approach (68% accuracy for mental health conditions)

## Executive Summary
This paper systematically compares three approaches for analyzing mental health text using LLMs: fine-tuning, prompt engineering, and retrieval-augmented generation (RAG). Using LLaMA 3 on emotion classification and mental health condition detection tasks, the study finds fine-tuning delivers superior accuracy but at high computational cost. Zero-shot prompting provides a practical alternative with moderate accuracy, while RAG and few-shot prompting show limited effectiveness. The results suggest a clear trade-off between performance and deployment flexibility, with fine-tuning optimal for accuracy but prompt engineering offering more practical deployment options.

## Method Summary
The study evaluates LLaMA 3 8B on two mental health text classification tasks using four different approaches. For fine-tuning, LoRA adapters (rank=64, alpha=16) are trained on labeled datasets with 4-bit quantization. Prompt engineering includes zero-shot and few-shot approaches with carefully designed templates. The RAG system uses BAAI/bge-small-en-v1.5 embeddings to retrieve top-3 documents from ChromaDB for context augmentation. Evaluation uses accuracy and macro F1-score on DAIR-AI Emotion (20K tweets) and SWMH Reddit datasets (54K posts), with preprocessing including URL removal, UTF-8 encoding, and truncation to 2048 tokens.

## Key Results
- Fine-tuning achieves highest accuracy: 91% for emotion classification, 80% for mental health conditions
- Zero-shot prompting emerges as second-best approach: 68% accuracy for mental health conditions
- RAG and few-shot prompting show limited effectiveness: 40-56% accuracy range
- Fine-tuning requires substantial computational resources (A100 GPU), while prompt engineering offers practical alternative when resources are limited

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with LoRA achieves superior accuracy by adapting model weights to domain-specific language patterns while preserving base knowledge. LoRA freezes the pretrained LLaMA 3 backbone and injects trainable low-rank decomposition matrices that learn to map general linguistic representations to task-specific decision boundaries. The paper used rank=64, alpha=16, learning rate 2e-4.

### Mechanism 2
Zero-shot prompting leverages pretrained knowledge for mental health classification without task-specific examples, with performance depending heavily on prompt clarity and label granularity. The model's pretrained weights encode semantic relationships between emotional language and conceptual categories, and a well-structured prompt activates relevant latent representations through inference alone.

### Mechanism 3
RAG underperforms because retrieved examples may introduce noise or conflicting patterns rather than clarifying decision boundaries. The system embeds queries using BAAI/bge-small-en-v1.5, retrieves top-k similar documents from ChromaDB, and appends them to the prompt. The LLM conditions its prediction on both query and retrieved context, but retrieval quality proves critical to performance.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**: Why needed here: Makes 8B-parameter model training feasible on single A100 GPU. Quick check: If LoRA rank is reduced from 64 to 8, would you expect accuracy to increase, decrease, or stay similar on a complex multi-class task?

- **Zero-shot vs Few-shot Prompting**: Why needed here: The paper shows zero-shot outperforming few-shot (49% vs 39% on emotions), which is counterintuitive and critical for deployment decisions. Quick check: Why might adding labeled examples to a prompt reduce classification accuracy?

- **Vector Retrieval (RAG)**: Why needed here: RAG is positioned as middle-ground approach but underperforms; understanding retrieval mechanics helps diagnose failure modes. Quick check: If cosine similarity retrieves documents about "anxiety" when query discusses "suicidal ideation," what failure mode does this represent?

## Architecture Onboarding

- **Component map**: Base model (LLaMA 3 8B 4-bit quantized) → LoRA adapters (rank=64, alpha=16) for fine-tuning OR → Prompt templates (zero-shot/few-shot) OR → Query → BAAI/bge-small-en-v1.5 embedding → ChromaDB retrieval (k=3) → augmented prompt → frozen model → Accuracy and macro F1 evaluation

- **Critical path**: 1) Data preprocessing (URL removal, UTF-8 encoding, truncation to 2048 tokens) 2) For fine-tuning: LoRA adapter training (1 epoch, lr=2e-4, batch_size=1, gradient_accumulation=8) 3) For all methods: Inference on test set with consistent prompt templates 4) Metric computation per class and aggregate

- **Design tradeoffs**: Fine-tuning: highest accuracy (80-91%) but requires A100 GPU, labeled training data, and adapter storage; Zero-shot: moderate accuracy (49-68%), zero training cost, but brittle to label ambiguity; Few-shot: lower accuracy (39-45%) than zero-shot, context window consumption increases with examples; RAG: moderate accuracy (47-56%), requires vector DB maintenance, retrieval quality is bottleneck

- **Failure signatures**: Fine-tuning overfitting: training loss decreases but validation F1 plateaus or drops; Zero-shot label confusion: model outputs invalid labels or defaults to majority class (check per-class recall); Few-shot context pollution: adding examples shifts predictions toward example labels regardless of input; RAG retrieval mismatch: top-k documents have low semantic overlap with query (audit similarity scores)

- **First 3 experiments**: 1) Reproduce zero-shot baseline with paper's exact prompt template on SWMH test split; log per-class F1 to identify weak categories (likely "love", "surprise", "bipolar"); 2) Implement LoRA fine-tuning with paper hyperparameters; compare validation accuracy at 0.5 epoch vs 1 epoch to detect early overfitting; 3) Diagnose RAG failure by sampling 50 test queries, retrieving top-3 documents, and manually rating retrieval relevance; correlate relevance score with prediction correctness

## Open Questions the Paper Calls Out

### Open Question 1
Would hybrid approaches combining fine-tuning with RAG or prompt engineering outperform individual methods for mental health text classification? Authors state: "Future research directions include the investigation of hybrid approaches that combine the strengths of multiple methods." This study evaluated each approach in isolation; no combinations were tested.

### Open Question 2
Why does few-shot prompting underperform zero-shot prompting for mental health text classification tasks? Authors report few-shot achieved 39-45% accuracy vs. zero-shot's 49-68%, noting this was "unexpectedly lower" and that examples may "introduce conflicting patterns."

### Open Question 3
Can RAG retrieval quality be improved specifically for mental health discourse to reduce the 33+ percentage point performance gap between high and low retrieval quality? Authors report RAG accuracy varied from 64% (high-quality retrieval) to 31% (low-quality retrieval), noting "effectiveness heavily dependent on retrieval quality."

### Open Question 4
How do these approaches generalize to clinical populations and non-English languages given current evaluation on English social media data? Authors acknowledge datasets "may not fully capture the complexity and variability of real-world psychological text data" and findings could restrict "generalizability to other domains, languages, or text formats."

## Limitations
- Fine-tuning requires substantial computational resources (A100 GPU) that may not be representative of real-world deployment constraints
- Evaluation focuses exclusively on English-language data (Reddit, Twitter), limiting applicability to multilingual contexts
- RAG approach shows weakest performance, but paper doesn't systematically explore whether this reflects fundamental limitations or specific implementation choices

## Confidence
- **High Confidence**: Comparative framework is sound and general finding that fine-tuning achieves superior accuracy while requiring more resources aligns with established ML principles
- **Medium Confidence**: Specific accuracy figures depend on exact implementation details that are partially unspecified
- **Low Confidence**: RAG failure analysis lacks mechanistic depth and doesn't systematically test alternative approaches

## Next Checks
1. **Ablation study on few-shot prompting**: Systematically vary the number of examples per class (0, 1, 2, 4, 8) and measure accuracy to determine whether the observed underperformance is monotonic or if there's an optimal example count.

2. **RAG component analysis**: Conduct controlled experiments isolating each RAG component: (a) test alternative embedding models, (b) vary retrieval parameters (top-k=1,3,5,10), and (c) evaluate different context integration strategies.

3. **Cross-dataset generalization test**: Apply the three best-performing approaches to an independent mental health dataset not used in the original study to assess whether reported accuracy figures generalize beyond the specific Reddit and Twitter corpora used.