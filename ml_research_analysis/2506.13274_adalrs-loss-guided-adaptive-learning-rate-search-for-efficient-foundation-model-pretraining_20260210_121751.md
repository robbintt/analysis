---
ver: rpa2
title: 'AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation
  Model Pretraining'
arxiv_id: '2506.13274'
source_url: https://arxiv.org/abs/2506.13274
tags:
- loss
- learning
- training
- adalrs
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining

## Quick Facts
- arXiv ID: 2506.13274
- Source URL: https://arxiv.org/abs/2506.13274
- Authors: Hongyuan Dong; Dingkang Yang; Xiao Liang; Chao Feng; Jiao Ran
- Reference count: 40
- Key outcome: AdaLRS enables efficient online learning rate search for foundation model pretraining by optimizing loss descent velocity within a single training run.

## Executive Summary
AdaLRS introduces a novel adaptive learning rate search mechanism that optimizes loss descent velocity during foundation model pretraining. The method adjusts the base learning rate through scaling factors (α=3 for upscaling, β=2 for downscaling, λ=0.99 decay) based on least squares estimation of loss slopes. Unlike traditional grid search or manual tuning, AdaLRS performs online optimization within a single training run, using a validation mechanism to ensure stability. The approach is validated on both LLM and VLM pretraining tasks, demonstrating faster convergence and improved final performance across multiple benchmarks.

## Method Summary
AdaLRS implements online optimal learning rate search by continuously monitoring and optimizing the loss descent velocity during training. The method estimates the slope of recent loss values using least squares regression, then adjusts the learning rate based on whether the slope is decreasing (indicating suboptimal LR) or increasing (indicating LR is too high). The algorithm uses a window size k (typically 1000-2500 steps) to compute loss trends, scales the LR by factors α=3 (up) and β=2 (down), and employs a validation mechanism that compares pre- and post-adjustment slopes. When upscaling fails validation, the method backtracks by restoring model and optimizer states. The search operates within a step ratio range [0.1, 0.35-0.4] and uses cosine scheduling, with validation against WSD scheduling.

## Key Results
- AdaLRS achieves faster convergence and improved final performance across LLM and VLM pretraining tasks
- The method demonstrates effectiveness on Qwen2.5-1.5B/7B LLM models and SAIL-VL VLM models
- AdaLRS outperforms traditional grid search and manual tuning approaches in both training efficiency and downstream benchmark performance

## Why This Works (Mechanism)
AdaLRS works by continuously optimizing the loss descent velocity rather than relying on static learning rate schedules. By estimating the slope of recent loss values, the method can detect when the learning rate is suboptimal and make real-time adjustments. The validation mechanism ensures stability by comparing loss descent velocities before and after LR changes, preventing disruptive updates. The backtracking capability restores model state when upscaling fails, maintaining training stability. This approach effectively navigates the loss landscape to find the optimal learning rate neighborhood without requiring multiple training runs.

## Foundational Learning
- **Loss descent velocity optimization**: Understanding that optimal learning rate maximizes the rate of loss decrease rather than minimizing loss directly. Quick check: Verify that loss decreases most rapidly at the optimal LR by plotting loss vs LR curves.
- **Least squares slope estimation**: Using linear regression on recent loss values to estimate the current loss descent velocity. Quick check: Compute slope estimates with different window sizes to verify stability.
- **Validation-based adaptation**: Comparing pre- and post-adjustment loss slopes to ensure LR changes improve training stability. Quick check: Plot validation scores against LR changes to confirm monotonic improvement.
- **State backtracking**: Restoring model and optimizer states when LR adjustments fail validation to prevent catastrophic training disruptions. Quick check: Verify that loss recovers after failed upscaling when backtracking is enabled.

## Architecture Onboarding
**Component map**: Loss values → Least squares slope estimation → LR adjustment decision → Validation check → State restoration (if needed) → Training continuation
**Critical path**: The most time-critical path is the slope estimation and validation check, which must complete within the training step budget to avoid slowing overall training.
**Design tradeoffs**: The method trades computational overhead (multiple backward passes for slope estimation) for reduced total training time through faster convergence. The choice of window size k balances responsiveness against noise in slope estimates.
**Failure signatures**: LR oscillation, persistent loss elevation after failed upscaling, and excessive computational overhead are primary failure modes. The method should converge monotonically to optimal LR neighborhood without indefinite oscillation.
**3 first experiments**:
1. Implement AdaLRS with k=1000, α=3, β=2, λ=0.99 on small random initialization of Qwen2.5-1.5B, starting from LR=2e-5 or 2e-3, and plot LR trajectory and loss to verify convergence behavior.
2. Run two identical training jobs differing only in backtracking (with vs without state restoration on failed upscaling) to compare loss curves and verify backtracking prevents sustained loss increases.
3. Measure step time with AdaLRS (slope estimation + validation) vs baseline to estimate computational overhead and verify claimed efficiency.

## Open Questions the Paper Calls Out
- **Parameter recovery from large initial LRs**: The authors note that AdaLRS fails to achieve comparable results with appropriate LR baselines when initialized with excessively large learning rates, as large LRs cause disruptive parameter updates. This problem is left for future work.
- **Fine-grained convergence error bounds**: The paper does not study the extent to which AdaLRS can approximate the optimal LR or explore the impact of scaling factor designs (α, β, λ) and learning rate search ranges on approximation error.
- **Theoretical convexity for adaptive optimizers**: While the theoretical analysis assumes SGD dynamics, the experiments use modern optimizers like AdamW, and the paper lacks theoretical explanation for why convexity and shared optimum persist under adaptive gradient methods.

## Limitations
- AdaLRS performance at true foundation model scales (>100B parameters) remains unproven, as all experiments use models well below typical foundation model sizes
- The computational overhead of slope estimation and validation could be prohibitive for large-scale training, particularly with k=1000-2500 requiring storage of thousands of previous loss values
- Several critical hyperparameters (slope estimation error threshold e, decay threshold initialization θ₀, layer-specific initialization schemes) are unspecified, potentially impacting reproducibility

## Confidence
- **High confidence**: The core algorithm design is clearly specified and theoretically sound, with demonstrated empirical improvements in convergence speed and final performance across multiple benchmarks
- **Medium confidence**: Practical implementation details are largely recoverable, though some hyperparameter values require educated guesses; benefits shown on small-to-medium models suggest promise but don't guarantee foundation model scale effectiveness
- **Low confidence**: Computational overhead claims and absolute performance numbers at true foundation model scale are not validated, as all experiments use models well below typical foundation model sizes

## Next Checks
1. **Sanity-check convergence behavior**: Implement AdaLRS with k=1000, α=3, β=2, λ=0.99 on small random initialization of Qwen2.5-1.5B. Start from LR=2e-5 or 2e-3 and plot LR trajectory and loss to verify it converges to optimal neighborhood without oscillation or divergence.
2. **Validate backtracking effectiveness**: Run two identical training jobs differing only in backtracking (with vs without state restoration on failed upscaling). Compare loss curves for persistent elevation after failed adjustments; backtracking should prevent sustained loss increases.
3. **Estimate computational overhead**: Measure step time with AdaLRS (slope estimation + validation) vs baseline. Verify whether the claimed "efficient" search remains practical at scale, particularly when k=1000-2500 requires storing and processing thousands of previous loss values.