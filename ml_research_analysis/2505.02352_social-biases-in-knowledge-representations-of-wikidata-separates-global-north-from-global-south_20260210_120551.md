---
ver: rpa2
title: Social Biases in Knowledge Representations of Wikidata separates Global North
  from Global South
arxiv_id: '2505.02352'
source_url: https://arxiv.org/abs/2505.02352
tags:
- knowledge
- biases
- occupations
- global
- south
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates social biases in Wikidata by analyzing
  link prediction outcomes across 21 geographies. The authors develop AuditLP, a framework
  that measures fairness in link prediction using Equal Opportunity and Equalized
  Odds metrics, focusing on gender and age as sensitive attributes.
---

# Social Biases in Knowledge Representations of Wikidata separates Global North from Global South

## Quick Facts
- **arXiv ID:** 2505.02352
- **Source URL:** https://arxiv.org/abs/2505.02352
- **Reference count:** 40
- **Key outcome:** Social biases in Wikidata link prediction systematically separate geographies into Global North and South clusters, revealing universal patterns across embedding algorithms.

## Executive Summary
This paper presents AuditLP, a framework for measuring fairness in knowledge graph link prediction by auditing occupation prediction outcomes across 21 geographies. The authors analyze how gender and age biases manifest in Wikidata embeddings, finding that biased predictions create a clear partition between Global North and Global South regions. Using fairness metrics like Equal Opportunity and Equalized Odds, they demonstrate that social biases in knowledge representations persist across different embedding algorithms (TransE, DistMult, CompGCN, GeKC), reflecting real-world socio-economic and cultural divisions.

## Method Summary
The study develops AuditLP to audit fairness in link prediction on Wikidata by treating occupation prediction as binary classification. The method filters human entities with citizenship in 21 target geographies, hides 50% of occupation triples (stratified by gender), trains knowledge graph embedding models on the filtered data, and uses an MLP classifier to predict occupation links. Fairness is measured using Equal Opportunity and Equalized Odds metrics based on TPR and FPR disparities for gender (male/female) and age (young/old). Occupations are categorized as biased or neutral based on statistical thresholds, and geographies are clustered based on their bias profiles.

## Key Results
- Biased link prediction outcomes systematically separate geographies into Global North and South clusters
- Social biases persist across different embedding algorithms (TransE, DistMult, CompGCN, GeKC)
- Gender and age biases in occupation prediction reflect real-world socio-economic divisions
- The framework reveals universal patterns of bias in knowledge graph embeddings

## Why This Works (Mechanism)
The mechanism works by leveraging the structural properties of knowledge graphs to reveal embedded social biases. When occupation information is hidden during embedding training, the model must rely on other relational patterns in the graph to make predictions. These patterns often reflect real-world social structures, including gender and age-based occupational segregation. The fairness metrics then quantify how these structural biases manifest as differential prediction accuracy across demographic groups.

## Foundational Learning
- **Knowledge Graph Embeddings (KGE):** Vector representations of entities and relations that capture graph structure; needed to understand how social patterns are encoded in the embedding space.
- **Link Prediction Task:** Predicting missing triples in a knowledge graph; central to understanding how occupation information is inferred from other relations.
- **Fairness Metrics (Equal Opportunity/Equalized Odds):** Measures of algorithmic fairness based on TPR and FPR disparities; essential for quantifying social bias in predictions.
- **TransE/DistMult/CompGCN/GeKC:** Different KGE architectures that model entity-relation relationships differently; understanding their approaches helps explain why biases persist across models.
- **Spectral Clustering:** Algorithm for grouping geographies based on bias profiles; reveals the Global North/South partition pattern.

## Architecture Onboarding
**Component Map:** Wikidata extraction -> Graph filtering -> Embedding training -> Classification -> Fairness analysis -> Clustering
**Critical Path:** Data extraction → Embedding training (hiding occupation edges) → MLP classification → Fairness metric calculation → Geographic clustering
**Design Tradeoffs:** Binary gender classification limits analysis but enables clear fairness metrics; hiding 50% of triples balances training needs with audit rigor
**Failure Signatures:** Data leakage (occupation edges not fully hidden), class imbalance overwhelming fairness metrics, threshold instability in bias categorization
**Three First Experiments:**
1. Verify Global North/South clustering emerges with TransE using specified hyperparameters
2. Test MLP sensitivity by varying architecture (1-3 layers, 32-256 hidden dimensions)
3. Assess threshold robustness by comparing μ-σ categorization with alternative statistical approaches

## Open Questions