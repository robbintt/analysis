---
ver: rpa2
title: Code Driven Planning with Domain-Adaptive Critic
arxiv_id: '2509.19077'
source_url: https://arxiv.org/abs/2509.19077
tags:
- building
- self
- planning
- unit
- name
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Code Driven Planning with Domain-Adaptive Critic (CoPiC) addresses
  the high query costs and short-term focus of existing LLM-based planners by generating
  multiple planning programs via an MoE mechanism and using a domain-adaptive critic
  to select plans aligned with long-term rewards. Instead of step-by-step plan refinement,
  CoPiC produces diverse candidate plans through programs and evaluates them with
  a critic fine-tuned via reinforcement learning.
---

# Code Driven Planning with Domain-Adaptive Critic

## Quick Facts
- arXiv ID: 2509.19077
- Source URL: https://arxiv.org/abs/2509.19077
- Reference count: 40
- Primary result: 23.33% higher success rate, 91.27% lower token costs, and 34.76% fewer steps versus advanced baselines

## Executive Summary
Code Driven Planning with Domain-Adaptive Critic (CoPiC) is an LLM-based planning framework that addresses high query costs and short-term focus in sequential decision-making. Instead of generating step-by-step plans directly, CoPiC uses LLMs to produce high-level planning programs that iteratively generate and refine candidate plans. A domain-adaptive critic then selects the plan most aligned with long-term rewards for execution. This approach amortizes LLM costs across multiple environment steps while focusing on cumulative reward signals.

## Method Summary
CoPiC operates in two phases: Planning and Learning. During Planning, LLMs generate n planning programs (Python functions) that take task instruction and observation as input to produce candidate plans. A TinyLlama-based critic scores these plans using normalized log-probabilities and length regularization, selecting the highest-scoring plan for execution. In the Learning phase, after every K steps the critic is fine-tuned via PPO with LoRA, and after N episodes the history of trajectories is summarized and fed back to the LLM to evolve improved planning programs. This cycle continues until convergence, achieving high success rates while significantly reducing LLM query costs.

## Key Results
- 23.33% higher success rate compared to advanced baselines
- 91.27% lower token costs through amortized program generation
- 34.76% reduction in average steps across ALFWorld, NetHack, and StarCraft II Unit Building tasks
- Superior performance across diverse task types and difficulty levels

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** High-level planning programs reduce LLM query costs compared to step-by-step plan refinement.
- **Mechanism:** LLMs generate executable planning programs (code) once per task category rather than repeatedly querying for each observation. These programs take `(I, o)` as input and output plans locally, amortizing generation cost across multiple environment steps.
- **Core assumption:** The LLM can produce functionally correct planning programs that handle varying observations without runtime LLM calls.
- **Evidence anchors:**
  - [abstract]: "CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans."
  - [Section 4.1.1]: "Programs generation via LLMs produces planning programs to generate and refine plans, thereby reducing the LLMs query costs typically incurred when directly generating and refining step-by-step plans."
  - [corpus]: Related work REPL-Plan [Liu et al.] similarly uses code-augmented planning but lacks the critic-based selection mechanism.

### Mechanism 2
- **Claim:** The domain-adaptive critic enables selection of plans aligned with long-term rewards rather than immediate feedback.
- **Mechanism:** The critic `C_θ` is initialized from a small language model (TinyLLaMA) and fine-tuned via PPO with LoRA. It scores candidate plans using normalized log-probabilities conditioned on the observation and task, learning from cumulative reward signals rather than greedy stepwise feedback.
- **Core assumption:** The critic can learn a sufficiently accurate value function from limited domain interaction to distinguish high-reward plans from low-reward ones.
- **Evidence anchors:**
  - [abstract]: "A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution."
  - [Section 4.1.2]: The critic computes `logit(dpi|dcp) = log(prob(dpi|dcp)) / Wi` with length regularization, then normalizes via softmax.
  - [corpus]: TWOSOME [Tan et al.] provides the foundational approach for using LLMs as critics; CoPiC extends this with domain-specific fine-tuning.

### Mechanism 3
- **Claim:** Programs evolution via history summarization enables iterative improvement without additional expert data.
- **Mechanism:** After N episodes, interaction histories (trajectories + success signals) are summarized and fed back to the LLM via a "Feedback Prompt." The LLM compares failed vs. successful trajectories to identify logic flaws (e.g., missing `take` actions) and generates refined programs.
- **Core assumption:** The LLM can correctly diagnose failure causes from summarized trajectories and produce improved code.
- **Evidence anchors:**
  - [Section 4.2.1]: "The LLMs is tasked with analyzing failed trajectories in comparison to successful ones... This comparative analysis reveals weaknesses in the planning programs, enabling targeted debugging."
  - [Appendix G]: Shows concrete evolution example where redundant navigation was fixed by adding explicit `self.take()` calls.
  - [corpus]: Limited direct evidence in neighbors; most related methods (AdaPlanner, Reflexion) rely on immediate feedback rather than history-based program evolution.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - **Why needed here:** The paper formulates planning as a POMDP with tuple `⟨S, O, A, R, P, I⟩`. Understanding this formalization is essential to grasp why observations alone are insufficient and why a learned critic helps.
  - **Quick check question:** Can you explain why the agent conditions on observation `o` rather than full state `s`, and what implication this has for planning?

- **Concept: Proximal Policy Optimization (PPO) with LoRA**
  - **Why needed here:** The critic is fine-tuned using PPO with LoRA adaptation. Understanding LoRA's low-rank decomposition and PPO's clipping objective helps explain why the small LM can be efficiently trained without full parameter updates.
  - **Quick check question:** Why would PPO be preferred over supervised learning for critic training, given the sequential nature of rewards?

- **Concept: Mixture of Experts (MoE) for Diversity**
  - **Why needed here:** CoPiC generates `n` planning programs to ensure diverse candidate plans. This is implicitly an MoE approach where each program is an "expert" and the critic acts as the gating mechanism.
  - **Quick check question:** What failure mode might arise if all `n` programs produce semantically similar plans?

## Architecture Onboarding

- **Component map:**
  - LLM Planner -> Planning Programs (n functions) -> Candidate Plans -> Domain-Adaptive Critic -> Selected Plan -> Environment
  - Environment -> Transitions -> Replay Buffer -> PPO Training -> Critic Update
  - Environment -> Trajectories + Success Signals -> History Buffer -> History Summarization -> LLM Evolution -> Improved Programs

- **Critical path:**
  1. Initialization: Generate `{ρ_i}` via init prompt (Section 4.1.1)
  2. Planning Loop: Each step, generate candidate plans `{p_i}`, score via critic, select and execute highest-scoring plan (Section 4.1.2)
  3. Learning Phase (every K steps): Fine-tune critic via PPO; after N episodes, evolve programs via history summarization (Section 4.2)
  4. Convergence: Break when success rate exceeds threshold; return final programs and critic

- **Design tradeoffs:**
  - **Number of programs (n):** Paper uses `n=3`. Higher `n` increases diversity but raises critic evaluation cost and may introduce noise.
  - **Critic size:** TinyLLaMA is cheap but may have limited reasoning capacity vs. larger models.
  - **Training data:** CoPiC trains on 20 tasks per type (120 total), comparable to baselines' 134 test-set tasks. Trade-off: less LLM cost but requires training set access.

- **Failure signatures:**
  - **Critic collapse:** All plans receive similar scores → random selection. Check for low variance in `score(p_i)`.
  - **Program stagnation:** Evolution fails to improve success rate across iterations. Inspect history summaries for diagnostic quality.
  - **OOM on open-source LLMs:** Reflexion failed with Qwen2.5-14B due to context accumulation. CoPiC's program-based approach should avoid this, but verify token counts during evolution prompts.

- **First 3 experiments:**
  1. **Ablation on critic:** Run CoPiC with random plan selection (no critic) vs. full critic. Expect 20-60% SR drop per Figure 4(b).
  2. **Scaling number of programs:** Test `n ∈ {1, 2, 3, 4}` on StarCraft II Hard task. Per Table 9, expect `n=1` to fail entirely and `n=3` to achieve best cost-performance balance.
  3. **Cross-environment transfer:** Train critic on ALFWorld, test on NetHack without further training. Assumption: critic is domain-adaptive; this should fail, confirming critic specialization.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can CoPiC effectively scale to full-complexity games like StarCraft II or Civilization, and how does it compare to baselines in these unconstrained environments?
  - Basis: [explicit] Conclusion: "Looking ahead, we are committed to expanding CoPiC’s application to more complex tasks, including full games of StarCraft II and Civilization..."
  - Why unresolved: Current experiments are limited to constrained tasks (e.g., StarCraft II Unit Building) rather than the full strategic complexity of the complete games.
  - Evidence: Empirical results on full-game benchmarks comparing success rates and efficiency against standard LLM planning baselines.

- **Open Question 2:** How robust is the framework if the LLM generates an initial planning program that is syntactically correct but logically ineffective?
  - Basis: [explicit] Limitations: "CoPiC requires LLMs to generate an initial planning program... Consequently, the used LLMs must possess adequate code generation capabilities."
  - Why unresolved: The paper assumes the LLM provides an adequate starting point for evolution; it does not analyze performance degradation or recovery mechanisms when the initial program is fundamentally flawed.
  - Evidence: Ablation studies injecting varying levels of logical error into the initial programs to measure the critic's ability to guide recovery.

- **Open Question 3:** How does the domain-adaptive critic handle environments with noisy or unstructured observations compared to the clean text-based inputs used in the study?
  - Basis: [inferred] The paper relies on structured text observations (e.g., ALFWorld, NetHack), while aiming for "real-world scenarios" (Conclusion) which typically involve sensor noise.
  - Why unresolved: The critic is trained on specific, clean text-based state descriptions; its sensitivity to real-world noise or partial observability is not tested.
  - Evidence: Experiments in embodied environments with sensor noise or visual inputs to evaluate the critic's scoring stability.

## Limitations

- **Missing training details:** Exact PPO hyperparameters, LoRA configuration, and critic prompt formatting are unspecified, making exact replication difficult.
- **Domain generalization scope:** The critic is trained per environment with limited cross-environment transfer demonstrated, suggesting specialization rather than broad generalization.
- **Program evolution assumptions:** The LLM's ability to correctly diagnose program failures from trajectory summaries is assumed but not empirically validated, potentially leading to degraded rather than improved programs.

## Confidence

- **High confidence** in the token cost reduction mechanism. The amortized cost of generating planning programs once per task type versus repeated LLM queries is theoretically sound and aligns with related work.
- **Medium confidence** in the domain-adaptive critic's effectiveness. While experimental results show strong performance, the critic's ability to learn meaningful value functions from limited training data across diverse environments needs further validation.
- **Low confidence** in the long-term benefits of program evolution. The iterative improvement mechanism is conceptually promising but lacks ablation studies showing performance degradation when evolution is disabled or when programs are initialized with suboptimal code.

## Next Checks

1. **Ablation study on program evolution:** Run CoPiC with and without the history-based evolution mechanism on ALFWorld tasks. Measure if success rates plateau or decline over iterations without evolution, confirming its necessity.
2. **Critic transfer capability test:** Train the critic on ALFWorld, then evaluate directly on NetHack without fine-tuning. Quantify performance drop to establish domain-adaptation limits versus domain-specific training.
3. **Scaling analysis of program count:** Systematically vary n ∈ {1, 2, 3, 4} on StarCraft II tasks. Plot success rate vs. token cost to identify optimal n and verify the claimed balance point around n=3.