---
ver: rpa2
title: 'Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing
  Hidden Fault Lines in Large Language Models'
arxiv_id: '2502.12825'
source_url: https://arxiv.org/abs/2502.12825
tags:
- receiver
- they
- send
- amount
- return
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new methodology to assess trusting behavior
  in LLMs using a classic game-theoretic trust game. It systematically varies sender
  objectives (helpful, profit-maximizing, risk-seeking), reasoning strategies (direct,
  zero-shot chain-of-thought, self-consistency), and receiver behavior (0%, 50%, 100%
  returns) across multiple LLM versions.
---

# Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment Revealing Hidden Fault Lines in Large Language Models

## Quick Facts
- **arXiv ID**: 2502.12825
- **Source URL**: https://arxiv.org/abs/2502.12825
- **Reference count**: 39
- **Primary result**: DeepSeek models demonstrate superior trusting behavior through forward planning and theory-of-mind in repeated trust games, while newer OpenAI models exhibit collapsed trusting behavior under profit-maximizing objectives.

## Executive Summary
This paper introduces a novel methodology to evaluate LLM trusting behavior using a classic game-theoretic trust game, systematically varying sender objectives, reasoning strategies, and receiver behavior. The results reveal stark performance differences between DeepSeek and OpenAI models, with DeepSeek consistently outperforming OpenAI in complex settings. These findings highlight the importance of evaluating LLMs beyond traditional benchmarks, emphasizing that hidden behavioral fault lines significantly impact commercial AI deployment success.

## Method Summary
The paper uses a 10-round repeated trust game where LLMs act as senders with $10 endowment, and rule-based receivers return fixed percentages (0%, 50%, 100%). The experimental design varies three sender objectives (helpful, profit-maximizing, risk-seeking), three reasoning strategies (direct, zero-shot chain-of-thought, self-consistency), and three receiver behaviors. Conversation history is reset each round to isolate decisions. The key metrics are final amount as fraction of theoretical maximum profit and amount sent per round.

## Key Results
- DeepSeek models consistently outperform OpenAI models in complex settings with varied receiver behaviors
- DeepSeek's superior performance stems from forward planning and theory-of-mind capabilities, balancing immediate objectives with longer-term trust-building
- OpenAI's o1-mini and o3-mini models exhibit collapsed trusting behavior when reconciling profit-maximizing objectives with future trust returns
- Models with built-in reasoning (o1-mini, o3-mini, DeepSeek-R1) do not benefit from additional prompt-based reasoning strategies

## Why This Works (Mechanism)

### Mechanism 1: Forward Planning Integration in Repeated Games
DeepSeek models incorporate multi-round strategic reasoning while newer OpenAI models default to myopic one-shot Nash equilibrium behavior. When given profit-maximizing objectives, DeepSeek reasons through the repeated game structure—"testing waters" in early rounds to assess receiver trustworthiness, then adjusting strategy. OpenAI models treat each round as isolated, defaulting to sending $0 because the receiver's dominant strategy in a single-shot game is to keep everything.

### Mechanism 2: Theory-of-Mind Emergence in Strategic Reasoning
DeepSeek shows early signs of modeling counterparty mental states, enabling adaptive trust calibration based on observed receiver behavior. DeepSeek's reasoning transcripts show explicit inference about receiver motivations—considering whether receivers are "profit-maximizing," "trying to build trust," or "just selfish." This allows dynamic adjustment (sending $10 to cooperative receivers, $0 to defectors). OpenAI models do not demonstrate this inference pattern.

### Mechanism 3: Objective-Reasoning Alignment
Models with built-in reasoning do not benefit from additional prompt-based reasoning strategies, but their native reasoning processes differ in how they reconcile assigned objectives with game-theoretic incentives. When assigned "profit-maximizing" objectives, o1/o3-mini interpret this as strict one-shot optimization (send $0). DeepSeek interprets it as multi-round profit optimization (send more when receiver reciprocates).

## Foundational Learning

- **Trust Game (Berg et al., 1995)**: The experimental paradigm where the subgame-perfect Nash equilibrium (send $0) differs from typical human behavior (send ~50%). Why needed: To understand the game-theoretic foundation and interpret model performance. Quick check: Why do humans consistently send more than $0 despite the Nash equilibrium predicting $0?

- **Backward Induction in Finite Games**: The 10-round finite horizon creates a theoretical unraveling problem. Why needed: To explain why myopic reasoning collapses to $0 in later rounds. Quick check: In a 10-round game where round 10 has no future, what happens to cooperation incentives in round 9?

- **Theory-of-Mind in AI**: The paper claims DeepSeek shows "early signs" of ToM. Why needed: To distinguish true mental state modeling from pattern-matching on game history. Quick check: If a model adjusts its behavior based on past receiver actions, is that evidence of ToM, or could it be simple reinforcement learning?

## Architecture Onboarding

- **Component map**: Prompt Module -> Inference Engine -> Receiver Agent -> Instrumentation
- **Critical path**: Initialize sender with objective + game rules → For each round: present observation → invoke LLM → parse amount sent → compute receiver return → record → Aggregate across iterations
- **Design tradeoffs**: Fixed vs. learning receiver (controlled comparison vs. ecological validity), information transparency (rounds remaining vs. probabilistic termination), conversation history (resetting prevents contamination vs. natural flow)
- **Failure signatures**: Models sending $10 regardless of receiver behavior (risk-seeking collapse), models sending $0 regardless of receiver behavior (profit-maximizing collapse), reasoning transcripts showing "one-shot game" framing in multi-round contexts
- **First 3 experiments**: 1) Replicate profit-maximizing + 50% return condition to establish baseline, 2) Add "helpful assistant" objective to test framing effects, 3) Extend to 20 rounds to test forward-planning advantage

## Open Questions the Paper Calls Out

- Do the observed differences in trusting behavior generalize to other economic games (ultimatum game, prisoner's dilemma, public goods game) or cooperative scenarios beyond the trust game paradigm?
- How do temperature settings and other sampling parameters affect trusting behavior and reasoning consistency in repeated games?
- Can prompt engineering or fine-tuning recover the collapsed trusting behavior observed in o1-mini and o3-mini when facing profit-maximizing objectives?
- Do the trusting behavior differences persist when experiments are conducted in languages other than English?

## Limitations
- Sample size per condition is not specified, limiting confidence in statistical robustness
- Reasoning transcripts are selectively reported without systematic analysis of response distributions
- The rule-based receiver model may not capture complex human trust dynamics
- Temperature and sampling parameters were held at defaults, potentially masking systematic differences

## Confidence

- **High confidence**: DeepSeek models outperform OpenAI models in complex settings with varied receiver behaviors
- **Medium confidence**: DeepSeek demonstrates superior forward planning and theory-of-mind capabilities
- **Low confidence**: The specific mechanisms driving performance differences are definitively understood

## Next Checks
1. Replicate with larger sample sizes per condition and formal statistical power analysis
2. Test with adversarial receivers using mixed or random return strategies to stress-test theory-of-mind claims
3. Implement infinite-horizon or probabilistic termination versions to isolate forward-planning from finite-game effects