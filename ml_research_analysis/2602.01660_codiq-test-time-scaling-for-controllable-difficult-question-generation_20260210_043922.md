---
ver: rpa2
title: 'CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation'
arxiv_id: '2602.01660'
source_url: https://arxiv.org/abs/2602.01660
tags:
- difficulty
- problem
- reasoning
- question
- codiq
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDiQ, a framework for controllable difficult
  question generation that addresses the challenge of scaling high-quality reasoning
  data for large language models. The method employs six difficulty-enhancement strategies
  combined with iterative test-time scaling to generate progressively harder questions
  while maintaining solvability through hybrid verification.
---

# CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation

## Quick Facts
- arXiv ID: 2602.01660
- Source URL: https://arxiv.org/abs/2602.01660
- Reference count: 40
- Produces 44K competition-grade math and coding questions with controllable difficulty scaling

## Executive Summary
This paper introduces CoDiQ, a framework for controllable difficult question generation that addresses the challenge of scaling high-quality reasoning data for large language models. The method employs six difficulty-enhancement strategies combined with iterative test-time scaling to generate progressively harder questions while maintaining solvability through hybrid verification. The framework includes a reinforcement learning-based CoDiQ-Generator and produces CoDiQ-Corpus, a dataset of 44K competition-grade math and coding questions. Human evaluation shows these questions are significantly more challenging than existing benchmarks (LiveCodeBench/AIME) while maintaining over 82% solvability. Experiments demonstrate that training models on CoDiQ-Corpus substantially improves reasoning performance compared to standard approaches, validating the effectiveness of controlled-difficulty training for enhancing reasoning capabilities.

## Method Summary
CoDiQ generates difficult questions through iterative test-time scaling using a pipeline that starts with seed questions and applies six difficulty-enhancement strategies across multiple rounds. Each iteration involves difficulty estimation via LLMs-Ranking and ValueNetwork, followed by solvability verification using Qwen3-32B. The framework trains CoDiQ-Generator (Qwen3-8B) using reinforcement learning on boundary-failure samples where models succeed at one difficulty level but fail at the next. The result is CoDiQ-Corpus containing 44K question sequences with controllable difficulty levels, validated through hybrid verification ensuring both increased complexity and maintained solvability.

## Key Results
- Generated 44K competition-grade math and coding questions with controllable difficulty
- Questions are significantly more challenging than LiveCodeBench/AIME benchmarks while maintaining 82%+ solvability
- Training models on CoDiQ-Corpus substantially improves reasoning performance on MATH-500 and AIME 2024
- CoDiQ-Gen-8B outperforms significantly larger Qwen3-32B in generating high-complexity instances

## Why This Works (Mechanism)

### Mechanism 1
Extended reasoning token budget correlates with question difficulty, enabling test-time scaling as a control mechanism. The CoDiQ prompt induces longer reasoning trajectories; token volume serves as a proxy for difficulty through increased cognitive operations performed during generation. Core assumption: Difficulty arises from depth of reasoning, not just surface complexity; models allocate computation proportional to problem sophistication. Evidence anchors: Pearson coefficients of r = 0.8299 and r = 0.8545 (p < 0.001) for token-to-difficulty correlation.

### Mechanism 2
Iterative difficulty escalation with hybrid verification maintains solvability while pushing complexity. Seed questions evolve through rounds; LLMs-Ranking provides relative difficulty comparison while ValueNetwork scores capture internal uncertainty; solvability verification by Qwen3-32B filters invalid problems before acceptance. Core assumption: Relative ranking is more robust than absolute scoring at capability frontiers; difficulty and validity can be decoupled through separate verification modules. Evidence anchors: 82% precision for accepted instances and 90% NPV for rejected cases.

### Mechanism 3
RL alignment on capability boundary failure modes improves generator's difficulty ceiling beyond baseline model size. Training pairs constructed from evolutionary trajectories where model succeeds through round i-1 but fails at round i; reward function balances solvability confidence with progressive scaling. Core assumption: Failure modes at capability boundaries encode learnable signal about what makes questions both hard and valid. Evidence anchors: CoDiQ-Gen-8B outperforms significantly larger Qwen3-32B in generating high-complexity instances.

## Foundational Learning

- **Concept: Test-time compute scaling**
  - Why needed here: Core mechanism driving difficulty control; understanding token-budget-to-difficulty relationship is prerequisite for using the framework.
  - Quick check question: If you double the token budget, what happens to expected difficulty? What happens to solvability rate for a fixed model?

- **Concept: Relative vs. absolute difficulty estimation**
  - Why needed here: The framework rejects absolute scoring due to saturation; understanding why pairwise/group ranking works better at frontiers is essential.
  - Quick check question: Why would a model rate two equally hard problems differently on an absolute scale but correctly rank them relatively?

- **Concept: Reinforcement learning from failure trajectories**
  - Why needed here: CoDiQ-Generator's training signal comes from "breaking points"; understanding this curriculum requires grasping RL on boundary instances.
  - Quick check question: What information is lost if you train only on successful difficulty escalations versus including failure modes?

## Architecture Onboarding

- **Component map**: Seed Question → CoDiQ Pipeline (iterative loop, max 8 rounds) → Difficulty Estimation (LLMs-Ranking + ValueNetwork) → Solvability Verification (Qwen3-32B) → Accepted/Rejected
- **Critical path**: 1. Seed selection from source datasets (quality/solvability pre-check) 2. Iterative prompting with difficulty strategies (6 categories) 3. Dual verification: difficulty must increase monotonically AND solvability must pass 4. Termination on failure: discard invalid, return valid sequence prefix
- **Design tradeoffs**: Strict monotonicity (current) vs. allowing occasional difficulty dips for longer sequences; Conservative solvability threshold (current) vs. accepting higher false-positive rate for harder corpus; Single verifier (Qwen3-32B) vs. ensemble verification for reduced verifier paradox risk
- **Failure signatures**: "Over-reasoning pitfall": Smaller models generate complexity beyond their verification capacity—solvability drops sharply; "Solution space collapse": Constraints mathematically valid but practically intractable; "Verifier paradox": Valid problems flagged as unsolvable due to verifier's limited capability horizon
- **First 3 experiments**: 1. Reproduce scaling tendency: Run Qwen3-8B with CoDiQ prompt on 50 CoDiQ-Bench samples, plot token usage vs. DR-AVG; expect r > 0.8 correlation 2. Ablate verification: Remove solvability module, compare difficulty ceiling 3. Transfer test: Apply CoDiQ-Generator to a held-out domain (e.g., physics problems) to assess strategy generalization

## Open Questions the Paper Calls Out

- **Question**: How can the "Verifier Paradox" be resolved to prevent valid, high-complexity problems from being discarded?
  - Basis in paper: Conclusion states, "Future work must address this scalable oversight challenge" regarding the epistemic ceiling of fixed-capacity verifiers.

- **Question**: Does the CoDiQ test-time scaling tendency generalize to domains outside of English math and coding tasks?
  - Basis in paper: Authors list as a limitation: "Our scope is currently restricted to English math/code tasks."

- **Question**: Can the verification cost be reduced to enable real-time generation without sacrificing reliability?
  - Basis in paper: Authors note the "verification cost limits real-time use."

## Limitations

- **Verifier Paradox Risk**: Reliance on Qwen3-32B for solvability verification creates fundamental upper bound on achievable difficulty, with valid hard problems potentially misclassified as unsolvable.
- **Generalizability Beyond Math/Code**: Validation limited to mathematical and coding problems; difficulty-enhancement strategies may not translate directly to other reasoning domains.
- **RL Training Signal Quality**: Quality and sufficiency of boundary-failure samples for stable learning across full difficulty spectrum not quantified.

## Confidence

- **High Confidence**: Core test-time scaling mechanism (extended reasoning tokens correlate with difficulty) supported by strong statistical evidence (r = 0.8299 and r = 0.8545, p < 0.001).
- **Medium Confidence**: Hybrid verification approach appears effective with 82% precision for accepted instances, though saturation effects and verifier paradox limitations acknowledged but not fully quantified.
- **Low Confidence**: Claims about substantial downstream performance improvements require stronger validation through controlled ablations.

## Next Checks

1. **Verifier Paradox Quantification**: Systematically measure false negative rate by human evaluation of rejected samples near difficulty ceiling to determine fraction of "valid but rejected" problems.
2. **Domain Transfer Experiment**: Apply CoDiQ-Generator to non-mathematical domain (e.g., logical reasoning puzzles or physics problems) and measure whether same difficulty-scaling patterns emerge.
3. **Ablation on Difficulty Control**: Train models on three variants of CoDiQ-Corpus (full difficulty-controlled, randomly sampled, baseline without test-time scaling) and compare downstream performance to isolate contribution of controlled difficulty versus increased training volume.