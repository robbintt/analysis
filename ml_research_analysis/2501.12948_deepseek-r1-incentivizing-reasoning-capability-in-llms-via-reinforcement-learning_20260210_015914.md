---
ver: rpa2
title: 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement
  Learning'
arxiv_id: '2501.12948'
source_url: https://arxiv.org/abs/2501.12948
tags:
- uni00000011
- uni0000001b
- reasoning
- uni0000001a
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DeepSeek-R1, a large language model (LLM) capable
  of advanced reasoning, developed through pure reinforcement learning (RL) without
  supervised fine-tuning. The approach leverages Group Relative Policy Optimization
  (GRPO) and rule-based rewards to incentivize reasoning behaviors, enabling the model
  to autonomously develop sophisticated strategies like self-reflection and verification.
---

# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

## Quick Facts
- arXiv ID: 2501.12948
- Source URL: https://arxiv.org/abs/2501.12948
- Reference count: 27
- Key outcome: DeepSeek-R1 achieves state-of-the-art reasoning performance through pure RL, with strong results on AIME 2024 (79.8%) and AlpacaEval 2.0 (87.6% win rate).

## Executive Summary
DeepSeek-R1 is a large language model capable of advanced reasoning, developed through pure reinforcement learning without supervised fine-tuning. The approach leverages Group Relative Policy Optimization (GRPO) and rule-based rewards to incentivize reasoning behaviors, enabling the model to autonomously develop sophisticated strategies like self-reflection and verification. DeepSeek-R1-Zero, the initial RL-only model, achieved strong performance on tasks such as AIME 2024 (79.8% accuracy) and Codeforces (96.3 percentile), demonstrating the effectiveness of RL in enhancing reasoning capabilities. DeepSeek-R1, the final model, further improves upon this by incorporating supervised fine-tuning and additional RL stages to enhance instruction-following and user preference alignment, achieving state-of-the-art results on benchmarks like AlpacaEval 2.0 (87.6% win rate) and ArenaHard (92.3%). The model's reasoning patterns are also successfully distilled into smaller models, enabling broader accessibility. This work highlights the potential of RL to unlock advanced reasoning in LLMs without reliance on human-annotated data, while also addressing limitations such as language mixing and token efficiency.

## Method Summary
DeepSeek-R1 employs a pure reinforcement learning approach to enhance reasoning capabilities in LLMs, starting with DeepSeek-V3-Base and using GRPO as the optimization algorithm. The model is trained on carefully curated datasets, including reasoning-focused web data and long-chain-of-thought examples, with rule-based rewards incentivizing behaviors like correctness and self-correction. Unlike traditional RL methods, DeepSeek-R1 avoids supervised fine-tuning, instead relying on reward modeling and iterative fine-tuning to align with human preferences. The process includes multi-stage training, where the model is first trained to generate reasoning traces and then fine-tuned for instruction-following. Knowledge distillation is used to transfer reasoning capabilities to smaller models, ensuring broader accessibility. The approach emphasizes long-context handling and self-verification strategies, enabling the model to solve complex tasks autonomously.

## Key Results
- DeepSeek-R1-Zero achieved 79.8% accuracy on AIME 2024 and ranked in the 96.3 percentile on Codeforces.
- DeepSeek-R1 improved upon its predecessor, achieving 87.6% win rate on AlpacaEval 2.0 and 92.3% on ArenaHard.
- Distilled smaller models retained reasoning capabilities, with performance on par with larger models on benchmarks like GSM8K and MATH.

## Why This Works (Mechanism)
DeepSeek-R1 leverages reinforcement learning to autonomously develop reasoning strategies by incentivizing behaviors like self-reflection, verification, and step-by-step problem-solving. The use of GRPO allows the model to optimize reasoning performance without relying on supervised fine-tuning, while rule-based rewards ensure alignment with desired outcomes. The iterative training process, combined with knowledge distillation, enables the model to refine its reasoning capabilities and transfer them to smaller models. By focusing on long-context handling and self-correction, DeepSeek-R1 can tackle complex tasks that require sustained reasoning over multiple steps.

## Foundational Learning
- **Reinforcement Learning (RL)**: A framework where an agent learns to make decisions by maximizing cumulative rewards. *Why needed*: RL enables the model to develop reasoning strategies autonomously without human-labeled data. *Quick check*: Verify that the model's performance improves over training iterations without supervised fine-tuning.
- **Group Relative Policy Optimization (GRPO)**: A variant of policy optimization that uses group-relative comparisons for reward calculation. *Why needed*: GRPO stabilizes training and improves sample efficiency in reasoning tasks. *Quick check*: Compare GRPO's performance against standard PPO on reasoning benchmarks.
- **Knowledge Distillation**: A process where a larger model (teacher) transfers its knowledge to a smaller model (student). *Why needed*: Distillation enables smaller models to retain reasoning capabilities, improving accessibility. *Quick check*: Test distilled models on reasoning tasks to ensure they match the teacher's performance.
- **Long-Context Handling**: The ability to process and reason over extended sequences of text. *Why needed*: Reasoning tasks often require sustained attention over multiple steps. *Quick check*: Evaluate the model's performance on tasks requiring long-chain-of-thought reasoning.
- **Reward Modeling**: A technique where rewards are derived from learned models rather than predefined rules. *Why needed*: Reward modeling allows for more nuanced alignment with human preferences. *Quick check*: Assess the model's alignment with human preferences on instruction-following tasks.

## Architecture Onboarding
- **Component Map**: DeepSeek-V3-Base -> GRPO Training -> Rule-Based Rewards -> Iterative Fine-Tuning -> Knowledge Distillation -> Smaller Models
- **Critical Path**: The core pipeline involves training the base model with GRPO, applying rule-based rewards to incentivize reasoning, and iteratively fine-tuning for alignment and instruction-following.
- **Design Tradeoffs**: Pure RL avoids supervised fine-tuning but may suffer from issues like language mixing and poor readability. Distillation addresses accessibility but may introduce biases from the larger model.
- **Failure Signatures**: Poor readability, language mixing, and token inefficiency are potential failure modes, especially in the initial RL-only model (DeepSeek-R1-Zero).
- **First Experiments**:
  1. Compare GRPO against PPO on reasoning benchmarks to validate the choice of optimization algorithm.
  2. Test the pure RL approach on out-of-domain tasks to assess generalization beyond mathematical and coding benchmarks.
  3. Evaluate distilled smaller models on reasoning tasks to confirm that the distillation process preserves reasoning capabilities.

## Open Questions the Paper Calls Out
- How can the pure RL approach be adapted to handle diverse reasoning tasks beyond mathematics and coding?
- What are the limitations of GRPO compared to other reinforcement learning algorithms in reasoning tasks?
- How can the model's language mixing issues be addressed without relying on supervised fine-tuning?

## Limitations
- The pure RL approach may lead to issues like poor readability and language mixing, as observed in DeepSeek-R1-Zero.
- The effectiveness of GRPO versus other RL algorithms is not thoroughly explored, leaving room for optimization.
- The evaluation lacks comparison against other RL-only methods, making it difficult to isolate the contribution of the optimization algorithm.

## Confidence
- **High confidence**: The overall performance improvements from RL-based training and the success of knowledge distillation into smaller models.
- **Medium confidence**: The claim that pure RL can develop reasoning without supervised fine-tuning, given the limited ablation studies.
- **Low confidence**: The assertion that DeepSeek-R1-Zero's reasoning patterns are fully autonomous, as the model's language mixing issues suggest potential gaps in self-generated strategies.

## Next Checks
1. Conduct ablation studies comparing GRPO with other RL algorithms (e.g., PPO, DQN) to isolate the contribution of the optimization method.
2. Test the pure RL approach on out-of-domain tasks to assess generalization beyond mathematical and coding benchmarks.
3. Evaluate the distilled smaller models on reasoning tasks to confirm that the distillation process preserves reasoning capabilities without introducing biases from the larger model.