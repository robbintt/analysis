---
ver: rpa2
title: Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language
  Model Capabilities
arxiv_id: '2510.19892'
source_url: https://arxiv.org/abs/2510.19892
tags:
- dixit
- card
- players
- player
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dixit, a game-based evaluation framework
  for multimodal large language models (MLMs), addressing limitations of existing
  benchmarks that rely on subjective pairwise comparisons or isolated task assessments.
  The Dixit game requires players to generate creative captions for fantasy cards
  that trick some but not all other players into selecting the correct card, testing
  multiple MLM capabilities including image comprehension, reasoning, classification,
  and captioning within a single unified task.
---

# Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities

## Quick Facts
- **arXiv ID:** 2510.19892
- **Source URL:** https://arxiv.org/abs/2510.19892
- **Authors:** Nishant Balepur; Dang Nguyen; Dayeon Ki
- **Reference count:** 12
- **Primary result:** Game-based evaluation framework for MLMs that tests multiple capabilities simultaneously with objective win/loss conditions

## Executive Summary
This paper introduces Dixit, a game-based evaluation framework for multimodal large language models (MLMs) that addresses limitations of existing benchmarks relying on subjective pairwise comparisons or isolated task assessments. The Dixit game requires players to generate creative captions for fantasy cards that trick some but not all other players into selecting the correct card, testing multiple MLM capabilities including image comprehension, reasoning, classification, and captioning within a single unified task. Experiments with five MLMs demonstrate above-random performance across all models, with Dixit rankings perfectly correlating with popular MLM benchmarks like ChatbotArena and OpenVLM Leaderboard. Human versus MLM gameplay reveals that even relatively weak models like GPT-4o Mini can outperform some human players, while analysis of game logs shows MLMs tend to generate more literal, descriptive captions compared to humans who reference external knowledge.

## Method Summary
The evaluation framework uses the board game Dixit where players generate captions for cards to achieve partial correct guesses (not all or none). Five MLMs (GPT-4o, Claude-3.5 Sonnet, InternVL2, Qwen2VL, and Molmo) plus a Random baseline were evaluated in 20 games using zero-shot prompting with structured JSON outputs. The game uses 100 open-source fantasy cards, with each player receiving a hand of cards and taking turns as storyteller to generate captions, select matching cards, and vote on the storyteller's card. Scoring rewards players when some but not all guess correctly, creating a "Goldilocks" constraint that forces calibration of ambiguity. The framework compares MLM performance against human players and analyzes failure modes through CLIPScore measurements and reasoning error categorization.

## Key Results
- All five MLMs achieved above-random performance (8.85 points) in 20 games, with InternVL2-8B achieving the highest average at 13.75 points
- Dixit win-rate rankings show perfect correlation with ChatbotArena and OpenVLM Leaderboard rankings
- Human players using GPT-4o Mini as one player achieved 16.67 points per game, outperforming some human-only games
- MLMs generate more literal, descriptive captions (higher CLIPScores) while humans use external knowledge and metaphors

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Game-based evaluation enforces output calibration via objective constraints, reducing reliance on subjective external judges.
- **Mechanism:** The Dixit scoring system creates a "Goldilocks" constraint: a storyteller receives 0 points if either everyone or no one guesses their card. This forces the model to calibrate its caption ambiguity precisely, rather than maximizing for simple accuracy or fluency. This structure replaces subjective pairwise comparison with an objective win/loss condition.
- **Core assumption:** Models that exploit superficial shortcuts (e.g., verbosity) in standard benchmarks will fail to maximize utility in a rule-bound environment where precision and ambiguity are required simultaneously.
- **Evidence anchors:**
  - [abstract] Mentions that games prevent models from exploiting shortcuts to inflate win-rates.
  - [Section 3 (Scoring System)] Explicitly details the 0-point penalty for all-or-nothing guesses, formalizing the constraint.
  - [corpus] Related work *DixitWorld* (2510.10117) supports this by using Dixit to evaluate abductive reasoning, confirming the game's utility for testing constrained hypothesis generation.
- **Break condition:** If models learn to generate random noise that accidentally bypasses the scoring logic, or if the "fun" aspect introduces variance that outweighs the skill measurement.

### Mechanism 2
- **Claim:** Ranking generalization suggests that visual reasoning capabilities are transferable across distinct evaluation paradigms (Game vs. Benchmark).
- **Mechanism:** The paper demonstrates that win-rates in Dixit correlate perfectly with ChatbotArena and OpenVLM Leaderboard rankings. This implies that the underlying cognitive requirements of Dixit (visual comprehension, theory-of-mind, captioning) map isomorphically to the skills prioritized in standard fine-tuning and RLHF pipelines.
- **Core assumption:** The specific sample of 100 cards and 20 games provides sufficient statistical power to represent the model's general multimodal capabilities.
- **Evidence anchors:**
  - [Section 5.1] Table 1 shows the exact correlation between Dixit Rank and OpenVLM/ChatArena Rank.
  - [abstract] States "Dixit win-rate rankings are perfectly correlated with those on popular MLM benchmarks."
  - [corpus] *Can Large Language Models Master Complex Card Games?* (2509.01328) implies that complex game mastery often correlates with general model capability, reinforcing the linkage.
- **Break condition:** If future models over-fit specifically to Dixit strategies without improving general reasoning, the correlation would decouple.

### Mechanism 3
- **Claim:** The "Literal vs. Abstract" gap serves as a diagnostic mechanism for detecting hallucination and lack of commonsense reasoning.
- **Mechanism:** By analyzing failure modes, the authors find that MLMs default to literal descriptions (high CLIPScore) while humans use external knowledge/metaphor. This divergence exposes that models rely on visual-semantic alignment rather than the deeper associative reasoning required for "tricky" captions.
- **Core assumption:** High CLIPScore (literalness) is a proxy for a lack of creative or abstract reasoning in this specific context.
- **Evidence anchors:**
  - [Section 5.3.2] Figure 5 shows humans have lower CLIPScores (more ambiguous) while MLMs are higher (more literal).
  - [Section 5.3.1] Figure 3 breaks down reasoning errors, showing open-source models suffer from "No Reasoning" or "Hallucination."
  - [corpus] The neighbor paper *Visual Representations inside the Language Model* (2510.04819) suggests MLMs struggle with perception-heavy tasks, aligning with the finding that they default to literal visual descriptions.
- **Break condition:** If literal descriptions begin to win games more often than abstract ones due to specific model-player dynamics, the diagnostic value reverses.

## Foundational Learning

- **Concept:** **Theory of Mind (ToM) in Agents**
  - **Why needed here:** Unlike standard captioning, Dixit requires the Storyteller to predict the mental state of other playersâ€”generating a clue that is solvable for *some* but not *all*. Understanding ToM is required to interpret why a model might fail at "calibrating" ambiguity.
  - **Quick check question:** Can you distinguish between a model generating a caption based on pixels vs. generating a caption based on what it thinks *another agent* knows?

- **Concept:** **CLIPScore and Semantic Similarity**
  - **Why needed here:** The paper uses CLIPScore to quantify "literalness." You must understand that a high score implies tight text-image alignment, which the paper argues is actually a *weakness* in a game requiring ambiguity and deception.
  - **Quick check question:** Does a higher similarity score between text and image always indicate better performance in a creative task?

- **Concept:** **Zero-Shot Prompting & JSON Structured Output**
  - **Why needed here:** The implementation relies on models following complex rules and outputting structured JSON (Appendix A) without fine-tuning. The evaluation is entirely dependent on the model's ability to follow these instruction-following constraints.
  - **Quick check question:** How does the prompt structure in Appendix A constrain the model's reasoning process (Chain of Thought) before it outputs a decision?

## Architecture Onboarding

- **Component map:** Game Engine (Python) -> Agent Wrapper -> Visual Assets -> MLMs
- **Critical path:**
  1.  **Initialization:** Deal 6 cards to n agents.
  2.  **Storyteller Turn:** Agent receives hand -> selects card -> generates caption (Prompt A.2).
  3.  **Player Turn:** Agents receive caption + hand -> select card (Prompt A.4).
  4.  **Voting:** Agents receive caption + pool -> vote for storyteller's card (Prompt A.5).
  5.  **Update:** Calculate scores (Table 1 logic) -> Replace cards -> Check termination (30 points).
- **Design tradeoffs:**
  - **API vs. Local Weights:** The authors used closed APIs for primary results but note the human interface was local. APIs offer stronger reasoning (GPT-4o) but introduce latency/cost; local models allow faster iteration but suffer from "Implausible" reasoning errors (Figure 3).
  - **Card Data:** Using open-source Pinterest cards (Section 4.1) avoids copyright but may introduce distribution shift compared to the original proprietary Dixit art style.
- **Failure signatures:**
  - **The "Literal" Trap:** Models generate descriptive captions (e.g., "Child reaching for moon") which are too easy, resulting in 0 points because everyone guesses correctly.
  - **Reasoning Collapse:** Open-source models outputting "No Reasoning" or hallucinating objects in the image to justify a random card selection (Figure 3).
  - **JSON Parsing Errors:** Models failing to adhere to the strict `{{thought}}, {{choice}}` format, crashing the game loop.
- **First 3 experiments:**
  1.  **Random Baseline Validation:** Run the provided Random Agent against a fixed model (e.g., GPT-4o Mini) to ensure the win rate is significantly above chance (8.85 points), validating the game logic.
  2.  **Ablation on Reasoning:** Remove the "thought" field requirement from the prompt to see if "No Reasoning" errors decrease or if performance drops due to lack of Chain-of-Thought grounding.
  3.  **CLIPScore Correlation:** Replicate Section 5.3.2. Plot caption CLIPScores against win rates for a specific model to verify if "literal" captions (higher score) actually correlate with lower game success.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can MLMs be trained to optimize for both high win-rates and human-perceived enjoyment in game-based evaluations?
- **Basis in paper:** [explicit] The conclusion states it is an "interesting challenge to design models that not only excel in Dixit win-rate... but also make the game enjoyable for other human players."
- **Why unresolved:** Current models optimize for winning using literal descriptions, which may lack the creative or abstract qualities that make the game engaging for humans.
- **What evidence would resolve it:** A study measuring human subjective enjoyment levels when playing against models trained via self-play versus models specifically fine-tuned for human-like creativity.

### Open Question 2
- **Question:** How can external commonsense knowledge be effectively integrated into MLM strategies to facilitate human-like abstract captioning?
- **Basis in paper:** [explicit] Section 5.3.2 notes that humans use external/pop culture references (e.g., "Rapunzel") while MLMs are literal, and suggests "future work... can find ways to instill external commonsense knowledge."
- **Why unresolved:** Models currently struggle to bridge the gap between visual input and abstract cultural concepts, defaulting to direct image description.
- **What evidence would resolve it:** Comparing the caption abstractness and win-rates of standard MLMs against those augmented with retrieval mechanisms for cultural references.

### Open Question 3
- **Question:** To what extent do shared training distributions within model families influence performance and strategy alignment in Dixit?
- **Basis in paper:** [explicit] The limitations section highlights "open questions about how model family characteristics influence performance" due to shared or non-shared knowledge bases.
- **Why unresolved:** The current study evaluated diverse architectures, making it difficult to isolate whether shared training data improves in-game agreement or strategic compatibility.
- **What evidence would resolve it:** A controlled comparison of win-rates and card selection agreement between different sized models from the same family (e.g., GPT-4o vs. GPT-4o Mini).

## Limitations

- The perfect correlation between Dixit rankings and external benchmarks may be coincidental given the limited sample size of 100 cards and 20 games
- The human vs MLM comparison provides insufficient statistical power due to only 3 games with 3 authors
- The strict JSON output format requirement may artificially constrain model performance rather than measuring true reasoning capability

## Confidence

- **High confidence:** The game mechanics and scoring system are well-defined and objective, eliminating subjective judgment bias
- **Medium confidence:** The correlation between Dixit performance and existing MLM benchmarks, pending larger-scale validation
- **Low confidence:** Conclusions about human vs MLM performance differences due to minimal human gameplay data

## Next Checks

1. Scale up the evaluation to 500+ cards and 100+ games to verify if the perfect correlation with external benchmarks persists, using statistical tests for correlation significance
2. Implement a controlled experiment comparing zero-shot prompting vs. few-shot prompting for the JSON output format to determine if the strict format requirement artificially limits performance
3. Recruit 20+ human players to play 10+ games each against MLMs, enabling proper statistical comparison of human vs model performance distributions