---
ver: rpa2
title: 'CMDAR: A Chinese Multi-scene Dynamic Audio Reasoning Benchmark with Diverse
  Challenges'
arxiv_id: '2509.22461'
source_url: https://arxiv.org/abs/2509.22461
tags:
- audio
- reasoning
- question
- answer
- lalms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CMDAR is a Chinese benchmark for evaluating audio-language models
  on complex, multi-scene, and dynamic reasoning tasks. It contains 3,000 high-quality
  question-answer pairs across five reasoning categories and three question types,
  including open-ended and multiple-audio scenarios.
---

# CMDAR: A Chinese Multi-scene Dynamic Audio Reasoning Benchmark with Diverse Challenges

## Quick Facts
- **arXiv ID:** 2509.22461
- **Source URL:** https://arxiv.org/abs/2509.22461
- **Reference count:** 40
- **Primary result:** Chinese benchmark with 3,000 QA pairs evaluating audio-language models on complex reasoning tasks

## Executive Summary
CMDAR is a Chinese benchmark designed to evaluate audio-language models on complex, multi-scene, and dynamic reasoning tasks. Constructed using Chinese films, it contains 3,000 high-quality question-answer pairs across five reasoning categories and three question types, including open-ended and multiple-audio scenarios. When evaluated on CMDAR-main, Qwen2.5-Omni (open-source) achieved 76.67% accuracy, while GPT-4o Audio (closed-source) reached 68.47%. Despite GPT-4o Audio's strong performance on challenging tasks, no model surpassed 80% performance, indicating significant room for improvement in complex audio reasoning. The benchmark revealed limitations in open-source models, differences in perceptual versus reasoning capabilities, and challenges with instruction bias and noise.

## Method Summary
CMDAR was constructed using Chinese films to generate diverse, real-world audio clips paired with expert-curated questions. The benchmark covers five reasoning categories and three question types, including open-ended and multiple-audio scenarios. The evaluation framework tested both open-source (Qwen2.5-Omni) and closed-source (GPT-4o Audio) models across the full benchmark. Performance was measured as accuracy percentage on the main benchmark dataset. The methodology emphasized high-quality question-answer pairs and real-world audio diversity to create a rigorous evaluation environment for audio-language reasoning capabilities.

## Key Results
- Qwen2.5-Omni achieved 76.67% accuracy on CMDAR-main
- GPT-4o Audio achieved 68.47% accuracy on CMDAR-main
- No model surpassed 80% performance, highlighting significant room for improvement
- GPT-4o Audio substantially outperformed Qwen2.5-Omni on multiple-audio and open-ended tasks

## Why This Works (Mechanism)
CMDAR works by providing a comprehensive evaluation framework that tests audio-language models across diverse real-world scenarios derived from Chinese films. The benchmark's multi-scene and dynamic nature forces models to handle complex reasoning beyond simple audio classification. By incorporating five reasoning categories and three question types, CMDAR creates varied challenges that expose limitations in current models' perceptual and reasoning capabilities. The expert-curated questions ensure high-quality evaluation while the multiple-audio scenarios test models' ability to integrate information across different audio sources, making it particularly effective at revealing strengths and weaknesses in current audio-language model architectures.

## Foundational Learning
- **Audio-language model evaluation**: Understanding how to measure reasoning capabilities beyond simple classification is essential for advancing audio AI systems. Quick check: Verify benchmark covers diverse reasoning tasks, not just recognition.
- **Multi-scene audio processing**: Models must handle context shifts and temporal dynamics across different audio environments. Quick check: Confirm benchmark includes varied acoustic environments and scenarios.
- **Cross-modal reasoning**: Integrating audio and language requires sophisticated alignment between perceptual and linguistic representations. Quick check: Ensure questions test both audio understanding and language reasoning.
- **Open-ended question handling**: Evaluating models on questions without fixed answer formats reveals reasoning depth. Quick check: Verify benchmark includes non-multiple-choice questions.
- **Multiple audio integration**: Processing and reasoning across several audio sources simultaneously is crucial for real-world applications. Quick check: Confirm benchmark includes questions requiring multiple audio clip analysis.
- **Chinese language processing**: Specialized understanding of Chinese audio and language nuances is necessary for this benchmark. Quick check: Verify all content is in Chinese and culturally appropriate.

## Architecture Onboarding

**Component Map**: Chinese Film Audio -> Audio Clip Extraction -> Question Generation -> Expert Curation -> Benchmark Assembly -> Model Evaluation -> Performance Analysis

**Critical Path**: Film audio selection → Clip extraction → Question generation → Expert curation → Model testing → Result analysis

**Design Tradeoffs**: The benchmark prioritizes real-world diversity and reasoning complexity over simple classification tasks, potentially limiting generalizability to other languages but providing deeper insight into reasoning capabilities.

**Failure Signatures**: Models may fail on multiple-audio integration, open-ended reasoning, or cross-modal alignment tasks. Performance drops indicate specific architectural limitations in handling complex audio-language reasoning.

**First 3 Experiments**: 1) Test baseline audio classification models on single clips to establish perceptual baselines. 2) Evaluate multi-clip integration capabilities by removing contextual information. 3) Compare performance across different reasoning categories to identify specific weaknesses.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The evaluation is based on a relatively small dataset (3,000 QA pairs) from Chinese films, limiting generalizability
- Closed-source model evaluation through API introduces potential variability from network conditions and undocumented optimizations
- Benchmark focuses on Chinese audio, limiting applicability to global audio-language research
- Expert-curated questions may introduce subjective bias in defining correct reasoning

## Confidence

**High Confidence:**
- CMDAR contains 3,000 high-quality question-answer pairs across five reasoning categories and three question types
- Qwen2.5-Omni achieved 76.67% accuracy and GPT-4o Audio achieved 68.47% on CMDAR-main
- No model surpassed 80% performance, indicating significant room for improvement
- CMDAR reveals limitations in open-source models and challenges with instruction bias and noise

**Medium Confidence:**
- GPT-4o Audio substantially outperformed Qwen2.5-Omni on multiple-audio and open-ended tasks
- The benchmark highlights differences in perceptual versus reasoning capabilities
- CMDAR's value as a rigorous benchmark for advancing audio reasoning research

**Low Confidence:**
- The specific reasons for GPT-4o Audio's underperformance relative to Qwen2.5-Omni overall are not well explained
- Claims about the benchmark's ability to advance research are forward-looking and not yet validated by follow-up studies

## Next Checks
1. Conduct statistical significance testing (e.g., bootstrap confidence intervals or McNemar's test) on model performance differences to determine whether observed gaps are meaningful or due to random variation.

2. Expand evaluation to additional audio-language models (both open and closed-source) and test on diverse datasets beyond Chinese films to assess generalizability and robustness.

3. Perform ablation studies on question types and reasoning categories to identify which specific aspects of CMDAR are most challenging for current models and where improvements are needed.