---
ver: rpa2
title: 'Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability
  Judgments'
arxiv_id: '2506.02302'
source_url: https://arxiv.org/abs/2506.02302
tags:
- grammar
- sentence
- prompting
- grammatical
- agreement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces a grammar prompting approach that improves\
  \ grammatical acceptability judgments by having an LLM first generate a concise\
  \ explanation of a syntactic phenomenon, then feeding that explanation back as context\
  \ to the target model (LLM or SLM) before it makes a judgment. Tested on English\
  \ BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this approach yields substantial\
  \ accuracy gains\u2014especially for SLMs\u2014narrowing the LLM-SLM gap by 56%\
  \ (from 13.0 pp to 5.8 pp) when combined with chain-of-thought."
---

# Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments

## Quick Facts
- **arXiv ID**: 2506.02302
- **Source URL**: https://arxiv.org/abs/2506.02302
- **Reference count**: 18
- **Primary result**: Grammar prompting narrows LLM-SLM gap by 56% on grammatical acceptability judgments

## Executive Summary
This paper introduces grammar prompting (GP), a method that first generates a concise grammar explanation of a syntactic phenomenon, then feeds that explanation back to a target model before making a grammatical acceptability judgment. Tested on English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks, this approach yields substantial accuracy gains—especially for SLMs—narrowing the LLM-SLM gap by 56% (from 13.0 pp to 5.8 pp) when combined with chain-of-thought. The lightweight, language-agnostic cue enables low-cost models to approach frontier-LLM performance in multilingual settings.

## Method Summary
The approach uses a two-step explain-then-process prompting: first, an LLM generates a beginner-oriented grammar explanation (~250 words) for a specific syntactic paradigm without including full example sentences; second, this explanation is fed as context to the target model (LLM or SLM) before it judges minimal pairs. The method was evaluated on challenging paradigms (≤90% baseline accuracy) across three languages using 50 minimal pairs per paradigm, with 3 trials per pair in randomized A/B order. Conditions tested include base prompt, chain-of-thought, GP alone, and GP+CoT.

## Key Results
- Grammar prompting alone improves SLM performance by 10-15 percentage points across languages
- GP+CoT reduces LLM-SLM performance gap from 13.0 pp to 5.8 pp (56% reduction)
- Beginner-oriented explanations outperform expert explanations by 1.9% (p=0.002)
- Irrelevant grammar explanations actively hurt performance versus no explanation
- Compound gains are largest for complex phenomena like island effects and negative polarity licensing

## Why This Works (Mechanism)

### Mechanism 1: Attention Redirection from Semantic to Syntactic Processing
Models default to paraphrase-based reasoning that obscures grammatical structure; grammar prompts force attention to syntactic constraints. The grammar explanation provides explicit structural categories (e.g., "licensing environments," "constituent boundaries") that replace the model's default meaning-first processing with form-focused analysis. This works because models possess latent syntactic knowledge but fail to access it without explicit prompting cues. Break condition: When constituent parsing itself fails (e.g., GPT-3.5 misidentifying sentence structure in Chinese wh-fronting), the grammar prompt cannot guide correct application.

### Mechanism 2: In-Context Rule Injection via Self-Generated Explanations
A model's own generated explanation is more effective than external rules because it matches the model's internal representation vocabulary. The LLM generates explanations in language patterns it "understands," creating a self-consistent reasoning scaffold when fed back as context. This works because the generating model's explanation style aligns with the target model's processing patterns (stronger when same model family). Break condition: When explanations become too long or technical (expert-level), smaller models struggle to apply them—observed with textbook condition where too much information degraded performance.

### Mechanism 3: Compound Reasoning via Grammar Prompt + Chain-of-Thought
GP+CoT produces larger gains than either alone because grammar prompts provide structural categories while CoT provides step-by-step application. Grammar prompt defines "what to look for" (rules, constraints); CoT enforces "how to apply" (systematic reasoning over constituents). This works because models can maintain consistency between the grammar prompt's categories and their own reasoning chain. Break condition: Smaller models (e.g., GPT-3.5) sometimes lose track of their own reasoning mid-chain, contradicting earlier correct observations—CoT adds cognitive load that can backfire.

## Foundational Learning

- **Concept**: Minimal Pairs in Linguistics
  - Why needed here: The entire evaluation framework uses sentence pairs differing by one syntactic feature; understanding this design is essential for interpreting results and applying the method.
  - Quick check question: Given sentences "Only a popsicle that Danielle admires ever freezes" and "A popsicle that only Danielle admires ever freezes," can you identify the single syntactic feature that differs?

- **Concept**: Formal vs. Functional Linguistic Competence
  - Why needed here: The paper explicitly distinguishes knowledge of structural rules (formal) from practical language use (functional); grammar prompting targets the formal-to-application gap.
  - Quick check question: Why might a model translate fluently but fail to judge which sentence of a minimal pair is grammatical?

- **Concept**: Negative Polarity Items (NPIs) and Licensing
  - Why needed here: NPI licensing is cited as a key success case (73.3% → 98.7% for GPT-4o); understanding licensing constraints helps predict which phenomena will respond well to grammar prompting.
  - Quick check question: In "Only Danielle admires the popsicle" vs. "Danielle only admires the popsicle," which correctly licenses the NPI "ever" if added?

## Architecture Onboarding

- **Component map**: [Input: Minimal Pair + Paradigm Name] → [Explanation Generator (LLM)] → [Grammar Prompt Buffer] → [Target Model (LLM or SLM)] → [Optional: CoT Reasoning Layer] → [Judgment Output: A or B]

- **Critical path**: The instruction template must specify paradigm name, target audience (beginner/expert), and explicitly exclude full example sentences to prevent shortcut learning.

- **Design tradeoffs**:
  - Beginner vs. expert explanations: Beginners outperformed experts; default to novice-oriented prompts.
  - Single paradigm vs. textbook: Compilations degrade performance; use targeted single-paradigm explanations.
  - GP vs. GP+CoT: CoT adds ~7-15 pp improvement but increases token cost and may confuse smaller models on complex paradigms.

- **Failure signatures**:
  - Constituent misidentification: Model correctly recalls rule but misparses sentence structure.
  - Reasoning drift: Model contradicts earlier analysis mid-CoT chain.
  - Paradigm mismatch: Control condition shows irrelevant explanations actively hurt performance.

- **First 3 experiments**:
  1. **Baseline calibration**: Run base prompt (no GP, no CoT) on 3-5 paradigms per language to establish floor performance.
  2. **GP-only vs. GP+CoT ablation**: Compare beginner-GP alone vs. beginner-GP+CoT on SLM (e.g., Haiku) to measure compound effect; expect 10-15 pp gap.
  3. **Explanation source sensitivity**: Generate explanations with two different LLMs (e.g., Sonnet vs. GPT-4o) and test on same target model; measure cross-model explanation transfer degradation.

## Open Questions the Paper Calls Out

### Open Question 1
Can smaller language models generate effective grammar explanations, or is LLM-grade explanation quality essential for the grammar prompting paradigm to work? The paper only elicited grammar prompts from large models (Sonnet and o1), not exploring whether smaller models could generate effective explanations. Generate grammar prompts using SLMs (e.g., GPT-3.5, Haiku, Llama 9B) and test whether they yield comparable accuracy gains when fed back to target models on the same benchmarks.

### Open Question 2
Does prompting in the target language rather than English improve grammar prompting effectiveness for non-English syntactic judgments? All grammar explanations were generated in English even for Chinese and Russian tasks; target-language prompts might better activate relevant syntactic representations. A controlled comparison generating grammar prompts in Chinese for SLING and Russian for RuBLiMP, then measuring accuracy differences against English-prompt baselines.

### Open Question 3
Would including illustrative example sentences in grammar prompts improve performance on complex phenomena like island constraints? Examples were deliberately excluded to isolate explanation effects, but this may have removed helpful concrete anchors for phenomena requiring fine-grained constituent recognition. Compare GP+CoT performance with and without 1–2 example minimal pairs in the grammar prompt, specifically on island effect and binding paradigms.

### Open Question 4
How effective is grammar prompting for truly low-resource languages with minimal representation in LLM training data? The paper focused solely on high-resource languages to control for vocabulary familiarity, but it remains untested how effective grammar prompting would be for low-resource languages. Test grammar prompting on minimal-pair benchmarks for low-resource languages (e.g., from the Bhasa suite or similar), comparing LLM-generated explanations against human-authored grammar references.

## Limitations
- The explanation generation process lacks transparency; full instruction template is not provided
- Only 50 minimal pairs per paradigm may not capture true performance distributions
- Dataset contamination from linguistic resources overlapping with BLiMP/SLING/RuBLiMP paradigms
- High-resource language focus limits generalizability to low-resource languages

## Confidence

- **High Confidence**: Core empirical finding that grammar prompting improves SLM performance on grammatical acceptability tasks (consistent 10-15 pp gains across three languages, clear ablation results)
- **Medium Confidence**: Mechanism explanation that grammar prompts redirect attention from semantic to syntactic processing (strong correlation with paradigm type, limited diagnostic probing)
- **Low Confidence**: Claim that self-generated explanations are more effective than external rules (weak direct comparison; improvement could stem from explanation format rather than source)

## Next Checks

1. **Cross-explanation validation**: Generate grammar explanations using multiple LLMs (different families) and test on the same target model to measure performance variance from explanation source versus explanation content.

2. **Attention visualization**: Apply attention visualization tools to compare model processing with and without grammar prompts on the same minimal pairs, testing whether syntactic features receive higher attention weights under GP conditions.

3. **Minimal explanation ablation**: Systematically reduce explanation length and complexity to identify the minimal effective grammar prompt, determining whether improvements stem from explicit rules or simply additional context.