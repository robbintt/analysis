---
ver: rpa2
title: 'Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal
  Mechanisms'
arxiv_id: '2509.23933'
source_url: https://arxiv.org/abs/2509.23933
tags:
- experts
- expert
- math
- performance
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MUI as a mechanistic interpretability metric
  to analyze MoE models beyond benchmark performance. By tracking neuron utilization
  across tasks and training stages, MUI reveals that stronger MoE models require fewer
  neurons (reflecting better generalization) and rely on more collaborative expert
  contributions.
---

# Beyond Benchmarks: Understanding Mixture-of-Experts Models through Internal Mechanisms

## Quick Facts
- **arXiv ID:** 2509.23933
- **Source URL:** https://arxiv.org/abs/2509.23933
- **Reference count:** 40
- **Key outcome:** Introduces MUI (Model Utilization Index) to analyze MoE models beyond benchmark performance by tracking neuron utilization across tasks and training stages.

## Executive Summary
This work introduces MUI (Model Utilization Index) as a mechanistic interpretability metric to analyze MoE models beyond benchmark performance. By tracking neuron utilization across tasks and training stages, MUI reveals that stronger MoE models require fewer neurons (reflecting better generalization) and rely on more collaborative expert contributions. The metric also enables fine-grained monitoring of training dynamics and data diversity. Ablation studies confirm the validity of neuron selection and threshold choices. MUI provides a complementary lens to performance, offering insights into model capacity, specialization, and generalization in MoE architectures.

## Method Summary
MUI quantifies the proportion of neurons required for task completion by identifying "key neurons" through contribution scoring (logit lens attribution), applying per-layer top-1‰ thresholds, and aggregating across samples. The metric is computed as |∪N_activated(s_i)| / (N × L × (|E_s| + |E_r|)), where N=neurons per expert, L=MoE layers, and E_s/E_r=shared/routed experts. Expert-level analysis identifies "key experts" via frequency thresholds. The method tracks training trajectories, reveals two-phase dynamics (Accumulating → Evolving), and quantifies collaboration through shared vs. routed expert contributions.

## Key Results
- Stronger MoE models exhibit lower neuron utilization (MUI), reflecting better generalization through knowledge consolidation
- MoE training follows a two-phase trajectory (Accumulating → Evolving) visible through MUI but not benchmark performance alone
- Shared experts become centralized "capacity hubs," concentrating task responsibility across domains (p-values 10⁻²⁹ to 10⁻⁵⁷)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Lower neuron utilization (MUI) correlates with stronger model generalization.
- **Mechanism:** As models evolve within the same family, they consolidate task-relevant knowledge into fewer, more specialized neurons rather than recruiting broadly. This compression reflects more efficient internal representations.
- **Core assumption:** Fewer activated neurons for the same task indicates generalization rather than memorization.
- **Evidence anchors:** [abstract]: "neuron utilization decreases as models evolve, reflecting stronger generalization"; [Section 3.2]: GPT-OSS exhibits "strikingly low MUI" while achieving superior real-world performance.

### Mechanism 2
- **Claim:** MoE training exhibits a two-phase trajectory (Accumulating → Evolving) visible through MUI but not benchmark performance alone.
- **Mechanism:** Early training recruits expanding neuron sets for memorization ("Accumulating"). Continued data exposure triggers specialization and compression ("Evolving"), where redundant neurons are pruned and knowledge consolidates.
- **Core assumption:** The transition point reflects genuine capacity consolidation rather than optimization plateau.
- **Evidence anchors:** [abstract]: "training exhibits a dynamic trajectory, where benchmark performance alone provides limited signal while MUI reveals deeper insights"; [Section 3.3]: OLMoE-0.5T to OLMoE-5T shows initial MUI rise followed by decline; MBPP task remains in Accumulating phase.

### Mechanism 3
- **Claim:** Shared experts become centralized "capacity hubs," concentrating task responsibility across domains.
- **Mechanism:** Shared experts are always active (G_s ≡ 1), receiving gradient signal from all inputs. Routed experts activate conditionally. This asymmetry causes knowledge to accumulate preferentially in shared experts, reducing the effective diversity of the expert pool.
- **Core assumption:** Concentration reflects optimization dynamics rather than architectural constraint alone.
- **Evidence anchors:** [Section 3.4]: Top-10 most frequently activated experts in shared-expert models are exclusively shared experts; Fisher's exact test shows enrichment with p-values from 10⁻²⁹ to 10⁻⁵⁷.

## Foundational Learning

- **Concept:** MoE Routing and Expert Types (Routed vs. Shared)
  - **Why needed here:** The paper distinguishes routed experts (top-k selected per token) from shared experts (always active). Understanding G_i weights and their gradient flow is essential to interpret why shared experts centralize knowledge.
  - **Quick check question:** Given a 64-expert MoE with 2 shared and top-8 routed selection, what proportion of total experts receive gradients from a single forward pass?

- **Concept:** Neuron Contribution Scoring (Logit Lens / Attribution)
  - **Why needed here:** MUI depends on identifying "key neurons" via contribution scores (f_neuron). The method projects intermediate activations to vocabulary logits to quantify causal influence.
  - **Quick check question:** If neuron j in expert i at layer l contributes 0.002 to the logit of token "cat" and the threshold η is top 1‰, how do you determine if (i, j, l) is "key"?

- **Concept:** Threshold Selection in Mechanistic Interpretability
  - **Why needed here:** The paper sets η at top 1‰ per layer via ablation (masking experiments). Understanding why this threshold works—and when it fails—is critical for reproducing MUI.
  - **Quick check question:** If you mask neurons above threshold 0.1% and performance drops 15%, but masking random neurons causes <2% drop, what does this validate?

## Architecture Onboarding

- **Component map:** Input x → LayerNorm → MoE Layer → Router (produces G_i for routed experts) → Shared Experts E_s (always active, G_s ≡ 1) → Routed Experts E_r (top-k selected) → Expert Internal (SwiGLU = (x W_u ⊙ (x W_g)) W_d) → Aggregation (Σ G_i · E_i(x)) → Output → Next Layer → LM Head (W_head) → MUI Tracking

- **Critical path:** 1. Forward pass on task samples; 2. For each token, compute neuron contribution scores; 3. Apply layer-level top-k threshold (1‰) to identify key neurons; 4. Union key neurons across samples → MUI = |union| / (N × L × |E_total|); 5. For expert-level: aggregate neuron activations to expert frequencies → apply frequency threshold η_expert = 0.6 → key experts

- **Design tradeoffs:**
  - **Threshold sensitivity:** Lower threshold = more neurons identified = higher MUI. Paper shows 0.08%–0.2% yields stable rankings (Pearson r = 0.9958), but absolute MUI values shift.
  - **Neuron importance formulation:** Default (projection to logits) vs. raw activation vs. full GLU output. Ablation shows high correlation (Spearman 0.92+), but interpretability differs.
  - **Expert frequency threshold:** η_expert = 0.6 identifies "consistently contributing" experts. Higher threshold = fewer key experts = less collaboration signal.

- **Failure signatures:**
  - **MUI saturates high (>30%) without decline:** Model may be undertrained or task too diverse; suggests Accumulating phase not exited.
  - **Key expert proportion extremely low (<5%):** May indicate routed-only architecture with aggressive load balancing dispersing gradients too thinly.
  - **Shared expert MUI dominates but total MUI low:** Confirmed pattern in strong models; if shared expert MUI is high AND total MUI high, suggests capacity bottleneck.
  - **Masking key neurons causes minimal degradation:** Threshold may be too permissive; re-calibrate via ablation.

- **First 3 experiments:**
  1. **Reproduce MUI calculation on GPT-OSS-20B for GSM8K:** Compute f_neuron scores, apply 1‰ threshold, report MUI. Compare to paper's reported ~1.6%. Validate via masking: zero out identified neurons, confirm >10% accuracy drop.
  2. **Track MUI trajectory on OLMoE intermediate checkpoints:** Load OLMoE-1T and OLMoE-4T. Compute MUI on ARC and MBPP. Confirm ARC shows Evolving pattern (MUI decline) while MBPP remains Accumulating.
  3. **Quantify shared expert concentration:** On DeepSeek-V2 (has shared experts), compute key expert set for GSM8K and MBPP separately, then intersection. Calculate shared expert proportion in intersection. Run Fisher's exact test to confirm enrichment (expect odds ratio >50).

## Open Questions the Paper Calls Out

- **Can a direct, quantitative relationship be established between MUI and generalization ability on out-of-distribution (OOD) tasks?**
  - **Basis in paper:** [explicit] The authors state in the Discussion that "establishing a direct and quantitative link between MUI & performance and generalization remains challenging."
  - **Why unresolved:** The paper shows a correlation between low MUI and stronger benchmark performance, but benchmarks may suffer from leakage or saturation; a causal link to OOD generalization is not proven.
  - **What evidence would resolve it:** A study correlating MUI scores with performance on specifically designed OOD datasets or novel tasks unseen during training.

- **Does the high concentration of responsibility in shared experts inhibit the development of distinct specializations in routed experts?**
  - **Basis in paper:** [explicit] Section 3.4 notes that the dominance of shared experts "risks over-centralization, potentially limiting expert specialization and reducing the effective diversity of the expert pool."
  - **Why unresolved:** The analysis shows shared experts are "capacity hubs," but it does not determine if this architectural feature prevents routed experts from learning unique functions.
  - **What evidence would resolve it:** Ablation studies analyzing the functional distinctness of routed experts in models with and without shared expert architectures.

- **Can MUI trajectories be used as a real-time signal to dynamically adjust data mixtures during training to prevent stagnation?**
  - **Basis in paper:** [inferred] The authors note MUI helps detect divergent trajectories (e.g., remaining in the "Accumulating" phase) and suggest "adjusting training accordingly," but do not implement a closed-loop control mechanism.
  - **Why unresolved:** It is unclear if intervening based on MUI (e.g., injecting coding data when MUI rises) improves final model efficiency or performance compared to static schedules.
  - **What evidence would resolve it:** Experiments where training data composition is modified dynamically in response to phase-specific MUI signals.

## Limitations

- **Generalization-Correlation Uncertainty:** While the paper demonstrates that stronger models exhibit lower MUI, the causal link between neuron utilization and generalization remains indirect.
- **Two-Phase Trajectory Verification:** The Accumulating-Evolving framework is observed empirically but not theoretically grounded.
- **Shared Expert Centrality:** The concentration of knowledge in shared experts is statistically validated but may create single points of failure.

## Confidence

- **High Confidence:** Neuron contribution scoring methodology and threshold selection via ablation are well-validated.
- **Medium Confidence:** The correlation between MUI and model strength across the MoE family, and the two-phase training trajectory interpretation.
- **Medium Confidence:** The shared expert concentration finding, though statistically robust, requires investigation of whether this is an architectural necessity or an optimization artifact.

## Next Checks

1. **Causal Generalization Test:** Take a strong MoE model (e.g., OLMoE-4T), identify its key neurons via MUI, then artificially increase neuron utilization by duplicating these neurons or reducing their specialization. Measure whether this manipulation degrades generalization (validation accuracy minus training accuracy) more than it degrades training accuracy alone.

2. **Task-Specific Trajectory Validation:** Run fine-grained MUI tracking on OLMoE checkpoints for both ARC and MBPP simultaneously. Confirm that the model-level MUI (averaged across tasks) masks divergent phase timings, and that task-specific monitoring reveals the Accumulating-Evolving bifurcation.

3. **Shared Expert Load Experiment:** Create a modified version of DeepSeek-V2 where shared experts are under-provisioned (reduce their capacity by 50%) while maintaining total model capacity. Measure whether this causes MUI to increase and performance to degrade, confirming that shared expert concentration is load-dependent rather than purely architectural.