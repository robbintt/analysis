---
ver: rpa2
title: Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical
  Systems
arxiv_id: '2601.16074'
source_url: https://arxiv.org/abs/2601.16074
tags:
- data
- shap
- values
- window
- industrial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving the reliability
  of machine learning models used in industrial cyber-physical systems by applying
  explainable AI (XAI) techniques. The authors use SHAP values to analyze the contributions
  of different time-series decomposition components to model predictions, specifically
  for a fault detection and identification solution.
---

# Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems

## Quick Facts
- arXiv ID: 2601.16074
- Source URL: https://arxiv.org/abs/2601.16074
- Reference count: 24
- Authors: Annemarie Jutte; Uraz Odyurt
- One-line primary result: SHAP analysis identifies high variance in feature attribution for the "Levels" component, motivating an increase in window size from 100 to 400 data points, improving CNN test accuracy from 83.78% to 92.3%.

## Executive Summary
This paper addresses the challenge of improving the reliability of machine learning models used in industrial cyber-physical systems by applying explainable AI (XAI) techniques. The authors use SHAP values to analyze the contributions of different time-series decomposition components to model predictions, specifically for a fault detection and identification solution. By increasing the window size of data instances based on the XAI findings, they achieve improved model performance. The CNN model's test accuracy increased from 83.78% to 92.3% when the window size was increased from 100 to 400 data points. This demonstrates how XAI can be used to inform model development and improve generalization to unseen data, enhancing the reliability of ML models in critical industrial CPS applications.

## Method Summary
The study employs a CNN for 3-class fault detection (Normal, NoFan, UnderVolt) using electrical current traces from an ODROID-XU4 platform. A custom time-series decomposition breaks signals into Levels, Peaks, Scale, Low Frequency (LF), and High Frequency (HF) components. The C-SHAP method computes exact SHAP values for these concepts by masking each component with training data samples. Based on high variance in SHAP attributions for the "Levels" component, the sliding window size is increased from 100 to 400 data points, resulting in improved model accuracy from 83.78% to 92.3%.

## Key Results
- CNN test accuracy increased from 83.78% to 92.3% when window size was increased from 100 to 400 data points
- SHAP analysis revealed high variance in feature attribution for the "Levels" component, motivating the window size increase
- The custom C-SHAP approach successfully attributed model behavior to physically meaningful signal components rather than abstract time-steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If XAI analysis reveals high variance in feature attribution for critical signal components, increasing the input window size can resolve context ambiguity and improve accuracy.
- **Mechanism:** The authors observe that the CNN relies heavily on the "Levels" component but suffers misclassifications where class values overlap. By expanding the window size from 100 to 400, the model receives a fuller distribution of the "Levels" component per instance. This reduces the stochastic chance of a window falling entirely within an ambiguous overlap zone, allowing the model to stabilize its reliance on this feature.
- **Core assumption:** The target fault signatures are not instantaneous spikes but persistent shifts (like "Levels") that require a wider temporal context to distinguish from noise or normal operation.
- **Evidence anchors:**
  - [abstract] "By increasing the window size of data instances based on the XAI findings, they achieve improved model performance."
  - [section 5] "We hypothesised that increasing the window size would provide more contextual information and reduce the ambiguity related to the ‘Levels’ values... improved test accuracies to... 92.3%."
  - [corpus] Weak direct support; corpus focuses on robustness/federated learning generally rather than this specific windowing mechanism.
- **Break condition:** If the faults were characterized solely by high-frequency transient spikes (shorter than the original window), increasing the window size would likely dilute the signal-to-noise ratio and degrade performance.

### Mechanism 2
- **Claim:** Decomposing time-series data into human-interpretable "concepts" allows SHAP to attribute model behavior to physically meaningful states rather than abstract time-steps.
- **Mechanism:** Instead of calculating SHAP values for raw time-points (which are hard to interpret in CPS), the paper implements C-SHAP. They decompose the signal into additive (Levels, Peaks) and multiplicative (Scale) components. The explainer then masks these entire components (e.g., replacing "Peaks" with median values) to see how the prediction changes, isolating the physical driver of the fault.
- **Core assumption:** The custom decomposition (Equation 1) successfully isolates independent physical behaviors (e.g., local mean vs. rapid fluctuations) such that masking one does not destroy the information of another.
- **Evidence anchors:**
  - [abstract] "We analyse the effects of components from time-series data decomposition on model predictions using SHAP values."
  - [section 4.1] "To generate the C-SHAP... concepts are constructed... ‘Levels’ component is modelled as the mean... ‘Peaks’ component is modelled as statistical outliers."
  - [corpus] No direct support for C-SHAP specifically found in neighbor abstracts.
- **Break condition:** If the signal components are highly correlated or the decomposition introduces artifacts (e.g., Change Point Detection lags), the SHAP attributions might misleadingly assign importance to the decomposition artifacts rather than the underlying physics.

### Mechanism 3
- **Claim:** Post-hoc explainability functions as a diagnostic reliability test, detecting when a model relies on spurious correlations that hold in training/test splits but would fail on unseen operational data.
- **Mechanism:** Standard accuracy metrics (83.78%) hid the fact that the model was struggling with class overlap in specific value ranges. By visualizing SHAP value histograms, the authors identified exactly which value ranges caused confusion (e.g., 0.680–0.685 for Normal vs. NoFan). This diagnostic capability transforms XAI from a "trust" tool into a "debugging" tool for generalization.
- **Core assumption:** The distribution of the "Levels" component in the test set is representative of the difficulty encountered in future unseen data.
- **Evidence anchors:**
  - [section 1] "Spurious correlations... cannot be detected. Understanding the relations a model relies on is important for robust evaluation."
  - [section 5] "Analysis of misclassified samples... reveals that many errors occur in the overlapping ‘Levels’ value range."
  - [corpus] Implied by [2510.18651] regarding "varying sensor reliability" requiring deeper analysis, though not explicitly linked to SHAP.
- **Break condition:** If the model uses a complex non-linear combination of features that the linear/additive assumptions of SHAP cannot capture, the explanation might mislead the developer about the true source of the error.

## Foundational Learning

- **Concept: SHAP (SHapley Additive exPlanations)**
  - **Why needed here:** This is the core engine of the paper's improvement strategy. Understanding that SHAP assigns an "attribution score" to features based on how much they push the prediction away from the average prediction is required to interpret Figure 3 and Figure 4.
  - **Quick check question:** If a feature has a SHAP value of 0 for a specific prediction, what does that imply about that feature's contribution?

- **Concept: Time-Series Decomposition**
  - **Why needed here:** The paper moves beyond raw data using a custom decomposition (Levels, Peaks, Scale, LF, HF). You must understand that a complex signal can be modeled as a sum of simpler signals (e.g., $y = Level + Peak + \dots$) to follow the "Concept Construction" in Section 4.1.
  - **Quick check question:** In Equation (1), is the 'Scale' component added to the signal or multiplied by it?

- **Concept: Sliding Window Approach**
  - **Why needed here:** The primary lever for improving performance was modifying the sliding window parameters (size and shift). Understanding the trade-off between window size (context) and resolution is key to the paper's outcome.
  - **Quick check question:** If you increase the window size but keep the total signal length constant, how does the number of training instances change (referencing Table 1)?

## Architecture Onboarding

- **Component map:** Data Source (ODROID-XU4 -> Power Meter) -> Parser -> Phase Cutter -> Sliding Window Slicer (Critical control point) -> CNN -> C-SHAP Pipeline
- **Critical path:** The feedback loop between the **Scorer** and the **Sliding Window Slicer**. The model is not a static artifact; the architecture explicitly includes using the explanation (Scorer output) to reconfigure the data formatting (Slicer window size).
- **Design tradeoffs:**
  - **Interpretability vs. Speed:** The paper notes "computational cost is the downside" of XAI. They choose exact calculation over approximation (KernelSHAP) for reliability, accepting higher latency in the development cycle.
  - **Context vs. Granularity:** Increasing window size (100 -> 400) improved accuracy (context) but reduced the total number of training instances (granularity) and potentially inference latency.
- **Failure signatures:**
  - **Instability in SHAP values:** High standard deviation in SHAP values for the "Levels" component (0.178) indicates the model is "guessing" or unstable in that feature space.
  - **Class Overlap:** High misclassification rates in the 0.680–0.700 value range for the "Levels" component signals insufficient context.
- **First 3 experiments:**
  1. **Baseline establishment:** Train the CNN with window size 100 on the raw "cycle-op" phases. Record accuracy (83.78%) to establish a baseline.
  2. **Concept Sensitivity Analysis:** Implement the decomposition (Eq 1) and run C-SHAP on the test set. Plot the mean absolute SHAP values to rank concept importance (verify "Levels" is dominant).
  3. **Iterative Resizing:** Retrain the CNN with window size 400. Calculate the new SHAP values for "Levels". Verify that the standard deviation of the SHAP values decreases (target: 0.155) and accuracy increases (target: 92.3%).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reliance on specific signal components identified by C-SHAP guide the design of less complex model architectures while maintaining performance?
- Basis in paper: [explicit] The authors state in the conclusion: "Given the component scoring, upon consistent reliance on a particular component, a less complex architecture/design could be considered... These can be interesting future work avenues to explore."
- Why unresolved: The current study utilizes a fixed Convolutional Neural Network (CNN) architecture; the authors did not test whether simpler models could achieve similar results based on the identified importance of the 'Levels' component.
- What evidence would resolve it: A comparative study where model architectures (e.g., reducing layer depth or filter counts) are systematically simplified based on high-scoring components to verify if performance is preserved.

### Open Question 2
- Question: To what extent can XAI-based component importance analysis inform the reduction of sensor sampling rates without degrading fault detection accuracy?
- Basis in paper: [explicit] The conclusion suggests that "depending on component importance, a lower sample rate could be argued in favour of... leading to lower data sampling overhead and data size reduction."
- Why unresolved: The paper uses a fixed high-frequency dataset and does not experimentally validate the impact of lowering the sample rate based on the 'High frequency' component's contribution.
- What evidence would resolve it: Experiments re-training the model on down-sampled datasets (informed by the frequency component scores) to identify the minimum sampling rate required to maintain the 92.3% accuracy threshold.

### Open Question 3
- Question: Is the custom time-series decomposition method generalizable to other industrial CPS domains with different signal behaviors?
- Basis in paper: [inferred] The authors note that established methods like EMD failed to represent discrete jumps in their data, necessitating a custom decomposition (Equation 1) tailored to their specific use-case.
- Why unresolved: Because the decomposition was manually designed to handle the specific "electrical metrics" and "discrete jumps" of the ODROID platform, it is unclear if this specific breakdown (Levels, Peaks, Scale, LF, HF) applies effectively to continuous or non-electrical CPS data (e.g., vibration or acoustics).
- What evidence would resolve it: Application of the proposed C-SHAP workflow with the custom decomposition on distinct industrial CPS datasets (e.g., rotating machinery) to evaluate if the components remain interpretable and informative.

## Limitations
- Absence of publicly available data and code blocks independent verification of the claimed accuracy improvements and the effectiveness of the C-SHAP approach.
- The study relies on a single, relatively small dataset (24 scenarios) and evaluates only one specific fault type (power trace anomalies in a single CPS platform).
- The custom decomposition method (Levels, Peaks, Scale, LF, HF) is not benchmarked against standard time-series decompositions like STL or wavelet transforms.
- The improvement from 83.78% to 92.3% accuracy, while significant, is reported without statistical significance testing or confidence intervals.
- The reliance on exact SHAP computation is noted as computationally expensive, suggesting scalability concerns for larger or more complex systems.

## Confidence

- **High Confidence:** The core claim that increasing window size from 100 to 400 data points improves CNN test accuracy from 83.78% to 92.3% is directly stated in the results and supported by the described methodology.
- **Medium Confidence:** The mechanism that SHAP analysis reveals high variance in feature attribution for the "Levels" component, motivating the window size increase, is logically sound but relies on the assumption that the custom decomposition accurately isolates independent physical behaviors.
- **Low Confidence:** The generalizability of the C-SHAP framework and the specific windowing strategy to other industrial CPS fault detection tasks, signal types, or CNN architectures is not established due to the narrow experimental scope.

## Next Checks

1. **Reproduce the Core Experiment:** Re-implement the CNN architecture, custom decomposition, and C-SHAP analysis pipeline using the specified ODROID-XU4 power trace data (or a closely analogous dataset) to verify the 83.78% to 92.3% accuracy improvement.
2. **Statistical Validation:** Conduct statistical significance testing (e.g., McNemar's test) on the classification results to confirm that the observed accuracy improvement is not due to random chance, and report confidence intervals.
3. **Generalizability Test:** Apply the C-SHAP analysis and windowing strategy to a different industrial CPS dataset with a distinct fault type (e.g., vibration or temperature anomalies) to assess the robustness and transferability of the proposed approach.