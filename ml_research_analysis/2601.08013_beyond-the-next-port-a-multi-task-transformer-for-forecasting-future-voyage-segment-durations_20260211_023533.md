---
ver: rpa2
title: 'Beyond the Next Port: A Multi-Task Transformer for Forecasting Future Voyage
  Segment Durations'
arxiv_id: '2601.08013'
source_url: https://arxiv.org/abs/2601.08013
tags:
- time
- sailing
- port
- vessel
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a segment-level transformer architecture
  for forecasting sailing durations of future voyage segments in maritime logistics.
  By reformulating the problem as a time-series forecasting task, the proposed model
  leverages historical sailing duration sequences, port congestion proxies, and vessel
  attributes, jointly predicting both segment sailing durations and destination port
  congestion states.
---

# Beyond the Next Port: A Multi-Task Transformer for Forecasting Future Voyage Segment Durations

## Quick Facts
- arXiv ID: 2601.08013
- Source URL: https://arxiv.org/abs/2601.08013
- Reference count: 6
- Achieves 4.85% relative MAE reduction and 4.95% relative MAPE reduction over sequence baselines

## Executive Summary
This paper introduces a segment-level transformer architecture for forecasting sailing durations of future voyage segments in maritime logistics. The model reformulates future-port ETA prediction as a segment-level time-series forecasting problem, enabling predictions without requiring real-time vessel position data. A causally masked transformer with multi-task learning predicts both segment sailing durations and destination port congestion states, achieving state-of-the-art performance on a global dataset spanning 2021.

## Method Summary
The model predicts segment sailing durations and port congestion states using a causally masked transformer with multi-task learning. It takes historical sailing duration sequences, port vessel counts, time window identifiers, and vessel/segment features as inputs. The architecture employs a shared transformer encoder with two TMNBlocks (32-dimensional, 8-head attention), followed by separate MLP heads for duration and congestion prediction. Training uses a composite loss with 90% weight on the main task (β=0.8 for MAE/MAPE) and 10% on the auxiliary task. The model is trained on data from Jan-Oct 2021, validated on Sep-Oct 2021, and tested on Nov-Dec 2021.

## Key Results
- 4.85% relative reduction in MAE and 4.95% relative reduction in MAPE compared to sequence baseline models
- 9.39% relative MAE reduction and 52.97% relative MAPE reduction compared to gradient boosting machines
- Stable performance across 7/14/21 day forecasting horizons with consistent MAE values
- Ablation studies confirm contributions from time series features (+0.34h MAE degradation when removed), segment features (+3.26pp MAPE degradation), and auxiliary task (+0.06h MAE degradation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal masking enables long-range temporal dependency learning without future information leakage, which is essential for autoregressive-style forecasting.
- Mechanism: The attention mask AttMask(t,s) = 1 if s ≤ t, 0 if s > t creates a lower-triangular constraint, allowing each prediction step to attend only to current and historical positions. This prevents the model from "cheating" by accessing future ground truth during training.
- Core assumption: Historical sailing duration sequences contain learnable temporal patterns (seasonality, trends) that generalize to future horizons.
- Evidence anchors:
  - [abstract] "employs a causally masked attention mechanism to capture long-range temporal dependencies"
  - [Section 3.4.3] Explicit definition of AttMask as lower triangular matrix; visualization in Figure 10 shows sparse, structured attention to recent and specific historical positions
  - [corpus] Weak direct evidence; Seg-MoE paper addresses temporal dynamics but not causal masking specifically
- Break condition: If sailing durations exhibit no sequential correlation (e.g., pure noise), causal attention provides no advantage over feed-forward models.

### Mechanism 2
- Claim: Multi-task learning (sailing duration + port congestion) improves forecasting robustness by leveraging shared latent signals between port operations and sailing behavior.
- Mechanism: The shared transformer encoder learns representations that serve both prediction heads. Gradients from the auxiliary task (port vessel count) provide additional supervision, regularizing the shared representation to capture correlated dynamics.
- Core assumption: Port congestion and sailing duration are causally linked—captains adjust speed in response to downstream congestion signals.
- Evidence anchors:
  - [abstract] "multi-task learning head to jointly predict segment sailing durations and port congestion states, leveraging shared latent signals to mitigate high uncertainty"
  - [Table 8] Ablation shows +0.06h MAE and +0.31pp MAPE degradation when auxiliary task removed, confirming contribution
  - [corpus] No direct corpus evidence for this specific multi-task coupling in maritime domain
- Break condition: If port congestion and sailing duration are statistically independent (e.g., fixed-speed operations regardless of congestion), auxiliary task provides no signal benefit.

### Mechanism 3
- Claim: Segment-level reformulation eliminates dependency on unavailable real-time AIS data by modeling historical duration sequences per route.
- Mechanism: Instead of predicting per-vessel ETA from current position/speed, the model treats each port-to-port segment as a time series entity. Historical sailing durations are aggregated by departure time window, creating regularized sequences that capture route-level patterns without requiring real-time vessel state.
- Core assumption: Segment-level aggregation preserves predictive signal while enabling applicability to future voyage legs where AIS is unavailable.
- Evidence anchors:
  - [abstract] "reformulates future-port ETA prediction as a segment-level time-series forecasting problem"
  - [Section 3.1] Formal definition: model maps (Y_T-L:T, X_T-L:T, S_T-L:T+H) → (Ŷ_T:T+H, X̂_T:T+H) per segment
  - [corpus] Unsupervised Port Berth Identification paper uses AIS for berth-level analysis but does not address segment-level forecasting
- Break condition: If vessel-level heterogeneity (speed, carrier preferences) dominates segment-level patterns, aggregation loses critical information.

## Foundational Learning

- Concept: **Causal/autoregressive attention masking**
  - Why needed here: The transformer must predict future horizons without accessing future ground truth during training; standard bidirectional attention would leak information.
  - Quick check question: Can you explain why a standard (unmasked) transformer trained on [past, future] concatenated sequences would fail at inference time?

- Concept: **Multi-task learning with gradient sharing**
  - Why needed here: Auxiliary task (port congestion) gradients modify the shared encoder weights, providing regularization and cross-signal learning beyond what single-task training achieves.
  - Quick check question: If the auxiliary task loss weight η is set too high (e.g., 0.5), what failure mode would you expect in the main task?

- Concept: **Sequence-to-sequence forecasting with heterogeneous features**
  - Why needed here: The model must fuse continuous time series (durations, counts) with categorical/static features (ports, vessels) into unified token representations before temporal modeling.
  - Quick check question: Why does early feature fusion (Section 3.4.2) precede the temporal network rather than fusing after temporal encoding?

## Architecture Onboarding

- Component map: Raw features → Embedding/Padding → Concatenation → Linear Fusion → + Positional Encoding → [Masked Attention → FFN] × 2 → MLP → (Duration, Congestion) outputs
- Critical path: Input Representation → Feature Fusion → Temporal Modeling Network → Multi-Task Output
- Design tradeoffs:
  - Model capacity (d_model=32, n_block=2): Table 10 shows larger capacity (d_model=64) slightly degrades performance, suggesting overfitting to noisy sailing records
  - Loss weighting (β=0.8 for MAE/MAPE, η=0.9 for main task): Heavy emphasis on main task; auxiliary task provides gentle regularization
  - Horizon (H=84 steps = 21 days): Table 9 shows stable MAE across 7/14/21 day horizons, justifying long-term prediction
- Failure signatures:
  - Removing time series features: +0.34h MAE (Table 8) → model collapses to tabular predictor without temporal dynamics
  - Removing segment features: +3.26pp MAPE (Table 8) → relative errors explode without segment identity scaling
  - Over-capacity models (Table 10): Modest MAPE degradation (18.05% → 18.91%) suggests noise overfitting
- First 3 experiments:
  1. Sanity check: Train on single high-frequency segment (e.g., Waigaoqiao→Ningbo, 137 records in test set) with and without causal masking; verify masked version cannot access future during training (inspect attention weights)
  2. Ablation sweep: Replicate Table 8 (remove time series, vessel, segment features, auxiliary task) to validate implementation; degradation magnitudes should match within ±10%
  3. Horizon robustness: Train models with H=28/56/84 and verify MAE stability per Table 9; significant error accumulation indicates incorrect masking or position encoding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can probabilistic forecasting techniques, such as quantile regression or diffusion-based models, produce calibrated prediction intervals that improve planning risk management compared to the current point-estimate architecture?
- Basis in paper: [explicit] The conclusion states future research should focus on "uncertainty quantification" via "probabilistic forecasting techniques such as quantile regression or diffusion-based models" to better manage planning risks.
- Why unresolved: The current model outputs single-point sailing duration estimates, providing no measure of confidence or variability, which limits utility for risk-averse port scheduling.
- What evidence would resolve it: Implementing a probabilistic head on the transformer and evaluating the coverage probability and sharpness of the resulting prediction intervals.

### Open Question 2
- Question: Does integrating Graph Neural Networks (GNNs) over port-segment graphs significantly improve accuracy by capturing systemic congestion propagation and inter-route dependencies?
- Basis in paper: [explicit] Section 5 suggests extending the framework "to capture spatiotemporal dependencies" by "utilizing graph neural networks (GNNs) over port-segment graphs."
- Why unresolved: The current transformer architecture treats segments primarily as independent sequences or relies on shared latent embeddings, failing to explicitly model the topological structure of the global maritime network.
- What evidence would resolve it: A comparative study where the attention mechanism is augmented with GNN layers to propagate congestion signals across connected voyage segments.

### Open Question 3
- Question: To what extent does the integration of dynamic environmental variables, such as meteorological and oceanographic data, enhance model robustness under extreme weather conditions?
- Basis in paper: [explicit] The authors propose that the framework "could be further enriched by integrating dynamic environmental variables" to enhance "robustness under extreme conditions."
- Why unresolved: The current feature set relies on historical duration patterns and port congestion proxies but excludes physical factors like wave height or wind speed that directly cause delays.
- What evidence would resolve it: An ablation study including weather features, specifically analyzing error reduction during periods of identified severe weather events.

## Limitations

- The causal attention mechanism's performance gain assumes sailing duration sequences contain exploitable temporal patterns, but the paper provides no statistical tests of autocorrelation or stationarity across segments
- The multi-task learning benefit relies on an assumed causal link between port congestion and sailing duration, yet no ablation studies isolate whether congestion or shared temporal encoding drives improvements
- The segment-level aggregation strategy's effectiveness depends on segment homogeneity, but the paper doesn't analyze variance explained by vessel-level vs segment-level features

## Confidence

- **High**: Causal masking prevents future information leakage (Section 3.4.3 definition is explicit and verifiable)
- **Medium**: Multi-task learning improves robustness (Ablation shows measurable degradation, but mechanism remains inferential)
- **Medium**: Segment-level reformulation enables future voyage forecasting (Logical framework is sound, but empirical validation against vessel-level models is absent)

## Next Checks

1. **Temporal Pattern Validation**: Compute autocorrelation functions for top 10 frequent segments; verify significant serial correlation exists to justify autoregressive modeling
2. **Causal Link Assessment**: Run counterfactual analysis: train with shuffled port congestion labels vs original; measure degradation to quantify task coupling strength
3. **Vessel Heterogeneity Impact**: Stratify test segments by vessel count per segment; plot MAE vs segment frequency to identify when aggregation breaks down