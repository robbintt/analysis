---
ver: rpa2
title: Self-Explaining Neural Networks for Business Process Monitoring
arxiv_id: '2503.18067'
source_url: https://arxiv.org/abs/2503.18067
tags:
- explanations
- sufficient
- event
- since
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of providing explanations for
  deep learning models used in predictive business process monitoring, specifically
  for Next Activity Prediction tasks. While existing post-hoc explanation methods
  often suffer from lack of faithfulness, high computational costs, and sensitivity
  to out-of-distribution samples, the authors propose a novel self-explaining neural
  network architecture that generates explanations as part of the model's output during
  training.
---

# Self-Explaining Neural Networks for Business Process Monitoring
## Quick Facts
- arXiv ID: 2503.18067
- Source URL: https://arxiv.org/abs/2503.18067
- Reference count: 9
- Primary result: Self-explaining neural networks produce more faithful explanations than post-hoc methods for Next Activity Prediction tasks in business process monitoring

## Executive Summary
This paper addresses the challenge of explaining deep learning predictions in business process monitoring, where post-hoc explanation methods often fail to provide faithful interpretations. The authors propose a novel self-explaining neural network architecture that generates explanations as part of the model's output during training, specifically for Next Activity Prediction tasks. By extending standard LSTM-based RNN architectures with an explanation component and adapting the optimization objective, the model jointly optimizes prediction accuracy, explanation faithfulness, and explanation cardinality through a dual propagation process.

Experiments on four real-world event log datasets demonstrate that this self-explaining approach produces explanations that are more faithful (verified as sufficient) than post-hoc methods like Anchors, while maintaining comparable prediction performance and achieving substantial computational efficiency gains. The approach shows particular promise for high-stakes business environments where trustworthy and interpretable predictions are essential, though questions remain about its generalizability to other monitoring tasks and robustness to out-of-distribution samples.

## Method Summary
The authors propose a self-explaining neural network architecture for Next Activity Prediction in business process monitoring. The method extends a standard LSTM-based RNN by adding an explanation component that generates explanations during training rather than post-hoc. The model is trained to jointly optimize prediction accuracy, explanation faithfulness, and explanation cardinality through a dual propagation process. The optimization objective is adapted to ensure generated explanations are both sufficient and concise, addressing the limitations of existing post-hoc explanation methods that suffer from lack of faithfulness, high computational costs, and sensitivity to out-of-distribution samples.

## Key Results
- Self-explaining approach maintains prediction accuracy comparable to standard LSTM training
- Explanations are more faithful (higher proportion verified as sufficient) than post-hoc methods like Anchors
- Computational efficiency improves by several orders of magnitude compared to post-hoc explanation methods
- Method shows particular promise for high-stakes business environments requiring trustworthy and interpretable predictions

## Why This Works (Mechanism)
The self-explaining approach works by integrating explanation generation directly into the model architecture and training process, rather than treating explanations as an afterthought. By optimizing for both prediction accuracy and explanation faithfulness simultaneously, the model learns to generate explanations that are inherently aligned with its decision-making process. The dual propagation process ensures that explanations are not only accurate but also concise, addressing the trade-off between completeness and interpretability that plagues post-hoc methods.

## Foundational Learning
- **LSTM-based RNNs**: Recurrent neural networks with long short-term memory cells that can capture temporal dependencies in sequential event data
  - Why needed: Business process logs are inherently sequential, requiring models that can understand temporal patterns
  - Quick check: Can the model maintain state across long sequences of events without vanishing gradients?

- **Dual propagation process**: A training mechanism where both prediction and explanation components are optimized simultaneously
  - Why needed: Ensures explanations are faithful to the model's actual decision-making process rather than being retrofitted
  - Quick check: Are gradients flowing correctly to both components during backpropagation?

- **Explanation cardinality**: The measure of explanation conciseness, balancing completeness with interpretability
  - Why needed: Overly complex explanations defeat the purpose of interpretability in high-stakes business contexts
  - Quick check: Does the model produce explanations that are both sufficient and minimal?

- **Faithfulness metrics**: Quantitative measures of how well explanations reflect the model's actual reasoning
  - Why needed: Without proper evaluation, it's impossible to verify that explanations are trustworthy
  - Quick check: What proportion of generated explanations are verified as sufficient by domain experts?

- **Post-hoc vs. intrinsic explanation**: The distinction between generating explanations after training versus during training
  - Why needed: Post-hoc methods often produce explanations that don't accurately reflect model behavior
  - Quick check: Does the self-explaining model produce more faithful explanations than post-hoc methods on the same architecture?

## Architecture Onboarding
Component map: Input Events -> LSTM Layer -> Prediction Component -> Output Activity | Explanation Component -> Output Explanation

Critical path: The dual optimization loop that simultaneously updates both prediction and explanation components to ensure alignment between predictions and their explanations.

Design tradeoffs: The model must balance three competing objectives - prediction accuracy, explanation faithfulness, and explanation cardinality. This requires careful tuning of the optimization objective to prevent any single objective from dominating the others.

Failure signatures: Potential issues include explanations that are too complex (high cardinality) to be useful, predictions that degrade due to the added explanation constraint, or explanations that fail to generalize to out-of-distribution samples.

First experiments:
1. Compare prediction accuracy of self-explaining model versus standard LSTM on each dataset
2. Measure explanation faithfulness using the proportion of explanations verified as sufficient
3. Benchmark computational efficiency by measuring training and explanation generation time

## Open Questions the Paper Calls Out
The paper acknowledges several open questions regarding the generalizability of the approach to other predictive business process monitoring tasks beyond Next Activity Prediction, and whether the faithfulness gains observed in these specific datasets extend to different types of business processes.

## Limitations
- Generalizability to other predictive business process monitoring tasks beyond Next Activity Prediction remains untested
- Evaluation focuses on a limited set of post-hoc explanation methods for comparison
- Trade-off between explanation cardinality and sufficiency is not fully characterized across different use cases

## Confidence
- High confidence in the core claim that self-explaining architecture produces more faithful explanations than post-hoc methods, given empirical comparison with Anchors on multiple datasets
- Medium confidence in computational efficiency gains, as comparison methodology and specific computational resources are not fully detailed
- Medium confidence in assertion that prediction accuracy is maintained, as performance differences between standard and self-explaining models are not statistically validated

## Next Checks
1. Test the self-explaining architecture on additional predictive business process monitoring tasks such as remaining time prediction or risk assessment to evaluate generalizability
2. Conduct ablation studies to isolate the impact of each component of the dual propagation process on explanation faithfulness and prediction accuracy
3. Perform cross-dataset validation by training on one event log and testing on another to assess robustness to out-of-distribution samples