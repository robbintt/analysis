---
ver: rpa2
title: 'DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving
  Assistants'
arxiv_id: '2601.12138'
source_url: https://arxiv.org/abs/2601.12138
tags:
- safety
- risk
- driving
- taxonomy
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DriveSafe, a hierarchical taxonomy for safety
  risks in LLM-based driving assistants. It identifies 129 fine-grained risk categories
  across technical, business, societal, and ethical domains, grounded in real-world
  regulations and reviewed by domain experts.
---

# DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based Driving Assistants

## Quick Facts
- arXiv ID: 2601.12138
- Source URL: https://arxiv.org/abs/2601.12138
- Reference count: 13
- Primary result: 129 fine-grained risk categories across 4 domains; models refuse 0-85% of unsafe prompts depending on domain and model

## Executive Summary
This paper introduces DriveSafe, a hierarchical taxonomy for safety risks in LLM-based driving assistants. It identifies 129 fine-grained risk categories across technical, business, societal, and ethical domains, grounded in real-world regulations and reviewed by domain experts. To validate its realism, the authors evaluate six widely used LLMs on unsafe driving prompts derived from the taxonomy. Results show that models often fail to appropriately refuse unsafe or non-compliant queries, with refusal rates ranging from 0% to 85% depending on the risk domain and model. The findings underscore the limitations of general-purpose safety alignment in driving contexts and motivate the need for domain-specific risk modeling.

## Method Summary
The authors construct a four-level hierarchical taxonomy (domain → category → failure mode → atomic risk) yielding 129 atomic risk types. Each risk type is paired with one manually crafted scenario–prompt pair designed to invite unsafe responses without explicit safety cues. Six LLMs are evaluated on all prompts, with responses classified as refusal/non-refusal by two human annotators (κ = 0.80). Refusal is defined as explicit decline, safety/legal/ethical concern citation, or redirect to safe alternatives without actionable guidance. Results are aggregated by domain to assess model performance on different risk types.

## Key Results
- DriveSafe taxonomy covers 129 fine-grained atomic risks across Technical, Business, Societal, and Ethical domains
- Model refusal rates vary widely (0%–85.71%) across domains and models, with low refusal on Ethical and Societal risks
- GPT-4o-Mini refuses 0% of Societal risks while Claude-3-Haiku refuses 85.71%, indicating domain-specific safety blind spots
- Cross-model variance suggests prompts capture genuine safety ambiguities rather than trivial violations

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Decomposition for Traceable Risk Attribution
The formal structure T ⊆ D × C × F × R enforces that each atomic risk has exactly one path through domain → category → failure mode → risk type. This mutual exclusivity allows downstream evaluation to trace model failures to specific sources.

### Mechanism 2: Implicit Risk Exposure Through Naturalistic Prompts
Prompts are designed with conversational phrasing and implicit risk exposure—they invite unsafe responses without explicitly requesting them. This avoids triggering surface-level safety training while testing underlying alignment.

### Mechanism 3: Cross-Model Refusal Variance as Validation Signal
If prompts were obviously unsafe, all well-aligned models would refuse consistently. Observed variance (0%–85.71%) indicates prompts target domain-specific gaps where safety boundaries are unclear.

## Foundational Learning

- **LLM Refusal Behavior and Safety Alignment**
  - Why needed here: Understanding why models refuse some queries and not others is central to interpreting the validation results
  - Quick check question: Can you explain why Claude-3-Haiku refuses 85.71% of Societal prompts while GPT-4o-Mini refuses 0%?

- **Hierarchical Taxonomy Design**
  - Why needed here: The four-level structure (domain → category → failure mode → atomic risk) is the core contribution
  - Quick check question: If you discovered a new risk type, how would you determine its correct placement in the hierarchy?

- **Domain-Specific vs. General-Purpose Safety Benchmarks**
  - Why needed here: The paper's motivation rests on the claim that existing benchmarks inadequately capture driving-specific risks
  - Quick check question: What specific driving-context factors (per Section 1) are missing from general safety benchmarks?

## Architecture Onboarding

- **Component map**: Domains → Categories → Failure Modes → Atomic Risks → Scenario-Prompt Pairs → Model Evaluation → Refusal Annotation
- **Critical path**: Taxonomy design → Expert review → Scenario construction → Prompt formulation → Model evaluation → Refusal annotation → Domain-aggregated analysis
- **Design tradeoffs**: Single prompt per risk (limits linguistic variation but enables precise attribution); Binary refusal classification (simple but may miss nuance); Expert-reviewed rather than data-driven taxonomy (ensures domain validity but may miss emerging risks)
- **Failure signatures**: Low refusal rates on Ethical/Societal risks suggest models lack value-alignment for ambiguous driving scenarios; High cross-model variance indicates safety training does not generalize to driving domain; Zero refusal (e.g., GPT-4o-Mini on Societal) may indicate domain blind spots
- **First 3 experiments**:
  1. Generate 5-10 paraphrases per prompt; measure refusal rate variance to assess prompt-specific vs. risk-specific behavior
  2. Modify scenarios for different legal contexts (UK vs. US vs. EU); test if refusal behavior changes appropriately
  3. Group atomic risks by failure mode; identify whether certain failure modes (e.g., Compliance Drift) systematically produce lower refusal rates

## Open Questions the Paper Calls Out

- Can the DriveSafe taxonomy maintain discriminative power when expanded using paraphrasing or mutation techniques?
- What specific alignment strategies (e.g., RAG, fine-tuning) are most effective for mitigating the failure modes identified in DriveSafe?
- How do refusal rates and risk behaviors change when static text prompts are replaced with dynamic, multi-turn interactions?

## Limitations

- Single prompt per atomic risk limits linguistic diversity and may not capture the full spectrum of unsafe driver queries
- Binary refusal classification may oversimplify complex safety responses, potentially missing nuanced partial refusals
- The assumption that refusal is always the correct response for all 129 prompts has not been validated

## Confidence

**High Confidence**: Hierarchical taxonomy structure is well-defined and internally consistent; methodology for measuring refusal rates is clearly specified and reproducible; observed variance in refusal rates across models and domains is accurately reported

**Medium Confidence**: Taxonomy comprehensively covers safety risks (based on expert review but not empirical validation); Cross-model refusal variance indicates genuine safety gaps (supported by variance but not exhaustive testing); Naturalistic prompts effectively test safety alignment (theoretically sound but limited empirical evidence)

**Low Confidence**: Specific refusal/non-refusal decisions for individual prompts are definitive safety assessments; Results generalize to all driving assistant applications and real-world driver interactions; Identified safety gaps will persist in future model versions

## Next Checks

1. **Paraphrase Robustness Test**: Generate 5-10 linguistic variations of each prompt and measure how refusal rates vary. This will distinguish whether failures are risk-specific or prompt-specific.

2. **Cross-Jurisdictional Scenario Testing**: Modify 20 representative scenarios to reflect different legal frameworks (e.g., US vs. UK vs. EU traffic laws) and test whether models adjust refusal behavior appropriately.

3. **Partial Refusal Analysis**: Re-annotate responses using a three-level scale (explicit refusal, conditional response, full engagement) rather than binary classification. This will provide more nuanced insights into model safety behavior.