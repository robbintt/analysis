---
ver: rpa2
title: 'Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality'
arxiv_id: '2509.17543'
source_url: https://arxiv.org/abs/2509.17543
tags:
- distribution
- latent
- autoencoder
- compression
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Bilateral Distribution Compression (BDC),
  a method to compress datasets in both sample size and dimensionality while preserving
  the underlying data distribution. Unlike existing distribution compression methods
  that only reduce sample size, BDC tackles the dual challenge of high dimensionality
  in modern datasets.
---

# Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality

## Quick Facts
- **arXiv ID:** 2509.17543
- **Source URL:** https://arxiv.org/abs/2509.17543
- **Reference count:** 40
- **Primary result:** BDC compresses both sample size and dimensionality while preserving data distribution, achieving 20x speedup vs ambient-space methods with comparable performance

## Executive Summary
Bilateral Distribution Compression (BDC) introduces a novel framework for simultaneously compressing both the sample size and dimensionality of datasets while preserving their underlying data distribution. Unlike existing methods that only reduce sample size, BDC tackles the dual challenge of high dimensionality in modern datasets through a two-stage approach: first training an autoencoder using Reconstruction MMD (RMMD), then optimizing a compressed set in latent space using Encoded MMD (EMMD). The method introduces Decoded MMD (DMMD) as the key metric for measuring distributional fidelity after compression, proving that minimizing both RMMD and EMMD guarantees vanishing DMMD. Experiments across regression, classification, and clustering tasks demonstrate that BDC achieves comparable or superior downstream performance to ambient-space compression methods while incurring substantially lower computational cost.

## Method Summary
BDC operates through a sequential two-stage framework: Stage 1 trains an autoencoder to learn a low-dimensional projection by minimizing RMMD (plus MSRE regularization for neural variants), while Stage 2 optimizes a compressed set in the learned latent space by minimizing EMMD. The core innovation is using MMD in three contexts—RMMD measures reconstruction quality, EMMD optimizes the compressed set, and DMMD measures final distributional fidelity. The authors prove that minimizing both RMMD and EMMD guarantees vanishing DMMD under specific conditions. The method supports both linear (BDC-L) and nonlinear (BDC-NL) autoencoders, with the latter requiring a hybrid loss to prevent "distribution mixing" while preserving global structure. Kernel choice (Gaussian with median heuristic bandwidth) and latent dimensionality are critical hyperparameters.

## Key Results
- BDC achieves similar performance to ADC on CT-Slice dataset but is 20x faster computationally
- On MNIST, BDC-L achieved the best Gaussian Process predictive metrics in shortest time despite lower-quality reconstructions
- High compression rates (up to 99.99%) achieved with minimal loss in predictive accuracy when data lies on low-dimensional manifolds
- BDC outperforms sequential application of ADC followed by PCA, particularly on nonlinear manifold data like Swiss-Roll

## Why This Works (Mechanism)

### Mechanism 1
Minimizing Reconstruction MMD (RMMD) and Encoded MMD (EMMD) acts as a proxy for minimizing the Decoded MMD (DMMD), effectively decoupling decoder training from compressed set optimization. Direct optimization of DMMD is practically difficult due to coupled parameters. The paper theoretically demonstrates that if the autoencoder perfectly reconstructs the distribution (RMMD → 0) and the latent set matches the latent distribution (EMMD → 0), the decoded distribution matches the original (DMMD → 0). Theorem 3.5 further bounds DMMD by the sum of RMMD and EMMD under a pull-back kernel.

### Mechanism 2
The hybrid loss (RMMD + MSRE) for neural autoencoders prevents "distribution mixing" while preserving global structure. RMMD alone matches distributions in aggregate, allowing a decoder to permute data points while maintaining zero loss. Adding Mean Squared Reconstruction Error (MSRE) forces the autoencoder to approximate a homeomorphism, preserving topology and local structure.

### Mechanism 3
Operating in a learned low-dimensional latent space (p << d) linearizes the computational complexity of MMD optimization. Standard MMD compression scales with dimension d. BDC first maps data to p-dimensions. Optimizing the compressed set C via gradient flow in this lower-dimensional space reduces the cost of kernel evaluations and gradient steps, provided the manifold assumption holds.

## Foundational Learning

- **Concept:** **Maximum Mean Discrepancy (MMD)**
  - **Why needed here:** It is the fundamental metric used for RMMD, EMMD, and DMMD. You cannot interpret the loss functions or Theorem 3.3 without understanding MMD as a distance between distributions in a Reproducing Kernel Hilbert Space (RKHS).
  - **Quick check question:** Can you explain why MMD is zero if and only if distributions are equal for a characteristic kernel?

- **Concept:** **Autoencoders & The Manifold Hypothesis**
  - **Why needed here:** BDC relies on the encoder ψ finding a faithful low-dimensional representation. If you don't understand manifold learning, you won't understand why BDC fails on "filling" noise vs. "manifold" data.
  - **Quick check question:** What is the difference between a linear (PCA-based) autoencoder and a nonlinear one in terms of the geometric structure they can capture?

- **Concept:** **Gradient Flow / Stein Discrepancy**
  - **Why needed here:** The "Latent Compression" step (Stage 2) optimizes the compressed set via gradient descent. Understanding this as a discretized gradient flow helps interpret convergence behavior.
  - **Quick check question:** How does optimizing a set of points to minimize MMD differ from simply selecting a subset (thinning)?

## Architecture Onboarding

- **Component map:** Data → Autoencoder (RMMD+MSRE loss) → Latent space → Coreset Optimizer (EMMD loss) → Compressed set
- **Critical path:** Stage 1 must converge (low RMMD) before Stage 2 begins. If the projection is garbage, the latent compression is optimizing garbage.
- **Design tradeoffs:**
  - BDC-L (Linear) vs. BDC-NL (Nonlinear): BDC-L is robust and deterministic but fails on complex manifolds. BDC-NL is flexible but requires tuning the hybrid loss to prevent mixing.
  - Kernel Choice: Gaussian kernel with pull-back kernel linking to DMMD (Theorem 3.5) vs. direct latent kernels for speed.
- **Failure signatures:**
  - High RMMD: Latent dimension p is too low
  - Visual "Mixing": MSRE term in Eq. 5 is missing or λ is too small
  - Slow Convergence: Learning rate issues or manifold constraints not handled correctly
  - OOM: Batch size too large during RMMD calculation
- **First 3 experiments:**
  1. Reproduce Swiss-Roll: Essential to validate non-linear autoencoder implementation
  2. Ablation Study on p: Run CT-Slice with varying latent dimensions
  3. Compare Sequential Baseline: Compare BDC vs. (ADC → PCA)

## Open Questions the Paper Calls Out

### Open Question 1
Can theoretical convergence guarantees be established for the autoencoder training stage of BDC (RMMD optimisation), and under what conditions? The authors state in Section A.2.3 that "Step 1 of our two-stage procedure—autoencoder training—does not have a general theoretical convergence guarantee" and "establishing such a guarantee is difficult in general... a general proof remains an open problem."

### Open Question 2
How can BDC be extended to general feature domains such as graphs, text, and structured data where direct gradient computation is infeasible? Section A.2.4 states: "In principle, the use of an autoencoder may allow us to extend gradient-based distribution compression to domains where direct gradient computation on the raw data is difficult or infeasible, such as graphs, text, and other structured or discrete inputs... We leave a detailed investigation of this potential capability to future work."

### Open Question 3
Can intrinsic dimension estimation methods be integrated with BDC to automate the selection of latent dimensionality p? In Section A.2.3, the authors note that "an interesting direction for future work would be to explore integrating intrinsic-dimension estimation methods with BDC" and in Section C.4 observe that standard estimators from skdim yielded estimates close to empirically optimal values on CT-Slice.

## Limitations

- The theoretical guarantee linking RMMD + EMMD to DMMD relies on computationally prohibitive pull-back kernel construction in high dimensions
- Performance on extremely high-dimensional data (d > 1000) or datasets with complex non-manifold structure remains untested
- Computational advantage diminishes when intrinsic dimension p is large, as EMMD optimization scales with p

## Confidence

- **High Confidence:** The sequential two-stage framework (autoencoder → latent compression) is well-justified and empirically validated
- **Medium Confidence:** The theoretical decomposition linking RMMD + EMMD to DMMD is mathematically sound but relies on assumptions about kernel characteristics
- **Low Confidence:** The generalizability to extremely high-dimensional data (>1000D) and exact conditions under which BDC outperforms simple sequential application are not rigorously established

## Next Checks

1. **Computational Scaling Test:** Implement BDC on a dataset with d = 1000+ and p = 50, measuring both runtime and memory usage to verify the claimed linear complexity
2. **Non-Manifold Structure Test:** Apply BDC to a dataset with significant noise or non-manifold structure (e.g., swiss-roll with holes) and quantify degradation in downstream task performance
3. **Kernel Sensitivity Analysis:** Systematically vary the kernel bandwidth in RMMD and EMMD losses across multiple orders of magnitude and measure impact on reconstruction quality and coreset optimization stability