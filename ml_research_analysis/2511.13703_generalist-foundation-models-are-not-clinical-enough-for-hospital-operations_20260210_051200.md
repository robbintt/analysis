---
ver: rpa2
title: Generalist Foundation Models Are Not Clinical Enough for Hospital Operations
arxiv_id: '2511.13703'
source_url: https://arxiv.org/abs/2511.13703
tags:
- finetuning
- notes
- test
- readmission
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lang1, a family of 100M-7B parameter language
  models pretrained on a blend of 80B clinical tokens from NYU Langone Health's EHRs
  and 627B internet tokens, and evaluates them on ReMedE, a benchmark of five real-world
  hospital operational tasks derived from 668,331 EHR notes. Zero-shot performance
  of both general-purpose and specialized models was poor (36.6%-71.7% AUROC) across
  four tasks, with mortality as an exception.
---

# Generalist Foundation Models Are Not Clinical Enough for Hospital Operations

## Quick Facts
- arXiv ID: 2511.13703
- Source URL: https://arxiv.org/abs/2511.13703
- Reference count: 40
- Primary result: Specialized EHR pretraining with supervised fine-tuning significantly outperforms general-purpose models on hospital operational tasks

## Executive Summary
This paper demonstrates that generalist foundation models struggle with hospital operational tasks, even when fine-tuned, while specialized models trained on clinical EHR data show substantial improvements. The Lang1 family of models, pretrained on 80B clinical tokens from NYU Langone Health's EHRs and 627B internet tokens, significantly outperforms larger generalist models after fine-tuning on operational tasks derived from real hospital data. The study introduces ReMedE, a benchmark of five real-world hospital operational tasks, and shows that explicit supervised fine-tuning is essential for operational performance, with in-domain pretraining improving fine-tuning efficiency.

## Method Summary
The researchers developed Lang1, a family of 100M-7B parameter language models pretrained on a combination of 80B clinical tokens from NYU Langone Health's EHRs and 627B internet tokens. They evaluated these models on ReMedE, a benchmark of five real-world hospital operational tasks derived from 668,331 EHR notes. The evaluation included zero-shot performance testing of both general-purpose and specialized models, followed by fine-tuning experiments. They compared Lang1-1B against both zero-shot models and larger finetuned generalist models (up to 70x larger) across the operational tasks, measuring performance using AUROC metrics.

## Key Results
- Zero-shot performance of both general-purpose and specialized models was poor (36.6%-71.7% AUROC) across four tasks, with mortality as an exception
- After fine-tuning, Lang1-1B outperformed larger finetuned generalist models (up to 70x larger) by 3.64%-6.75% AUROC improvement
- Lang1-1B also outperformed zero-shot models (up to 671x larger) by 1.66%-23.66% AUROC improvement
- Cross-task scaling was observed with joint fine-tuning across multiple operational tasks

## Why This Works (Mechanism)
The superior performance of Lang1 models stems from their specialized pretraining on clinical EHR data, which provides domain-specific knowledge and language patterns that generalist models lack. When combined with supervised fine-tuning on operational tasks, these models can effectively leverage their clinical understanding to make accurate predictions about hospital operations. The joint fine-tuning approach allows models to learn shared representations across related tasks, improving overall performance and enabling knowledge transfer between different operational domains.

## Foundational Learning
1. **Clinical EHR language patterns** - Understanding medical terminology, abbreviations, and documentation styles specific to hospital settings; needed for accurate interpretation of clinical notes; quick check: ability to parse and understand complex medical documentation
2. **Hospital operational workflows** - Knowledge of how hospitals function, including patient flow, resource allocation, and clinical decision-making processes; needed to contextualize operational predictions; quick check: correlation between predicted outcomes and actual operational patterns
3. **Zero-shot vs fine-tuned learning** - Understanding the limitations of applying pretrained models to specialized tasks without adaptation; needed to justify the importance of fine-tuning; quick check: performance gap between zero-shot and fine-tuned models
4. **Cross-task knowledge transfer** - Ability to leverage learning from one operational task to improve performance on related tasks; needed for efficient model development; quick check: performance improvement when fine-tuning on multiple tasks simultaneously

## Architecture Onboarding

**Component Map:**
Clinical Data Ingestion -> EHR Tokenization -> Pretraining (80B clinical + 627B internet tokens) -> Fine-tuning (supervised) -> Operational Task Evaluation

**Critical Path:**
The critical path involves pretraining on clinical data, followed by supervised fine-tuning on specific operational tasks. This two-stage process is essential because the clinical pretraining provides domain-specific knowledge that enables effective fine-tuning, while the supervised fine-tuning adapts the model to specific operational prediction tasks.

**Design Tradeoffs:**
- Model size vs performance: Smaller Lang1 models (1B parameters) outperform larger generalist models (up to 70x larger), suggesting efficiency gains from specialized pretraining
- Pretraining data diversity: Balancing clinical-specific tokens with general internet tokens to maintain broad language understanding while gaining domain expertise
- Task-specific vs joint fine-tuning: Joint fine-tuning across multiple tasks shows cross-task scaling but may introduce task interference

**Failure Signatures:**
- Zero-shot performance degradation on specialized tasks indicates lack of domain-specific knowledge
- Poor performance on out-of-distribution tasks suggests overfitting to training data patterns
- Inability to handle complex temporal reasoning in operational scenarios reveals limitations in reasoning capabilities

**First Experiments:**
1. Compare zero-shot performance of generalist vs specialized models on hospital operational tasks
2. Evaluate fine-tuning effectiveness on individual vs joint operational tasks
3. Test model transfer performance on out-of-distribution operational tasks from different health systems

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- ReMedE benchmark, while derived from real EHR data, represents a curated set of tasks that may not fully capture the complexity of actual hospital operations
- Performance evaluation focuses on classification tasks with standardized metrics, potentially missing nuanced operational decisions requiring multi-step reasoning or temporal understanding
- Lang1 models were trained on data from a single health system, raising questions about generalizability to institutions with different documentation practices and patient populations
- Study does not address potential biases in EHR data that could affect model performance across demographic groups

## Confidence
- **High confidence**: Generalist models perform poorly on hospital operations tasks without fine-tuning; Lang1-1B shows consistent improvement over larger generalist models after fine-tuning; specialized pretraining on EHR data improves fine-tuning efficiency
- **Medium confidence**: Cross-task scaling with joint fine-tuning; Lang1-1B transferability to out-of-distribution tasks and external health systems
- **Low confidence**: Claims about the absolute necessity of specialized models for all hospital operations, given limited evaluation scope

## Next Checks
1. Evaluate Lang1 models on hospital operations data from multiple diverse health systems to assess true generalizability and identify potential dataset-specific performance patterns
2. Test model performance on longitudinal operational tasks requiring multi-step reasoning and temporal decision-making beyond the current classification-focused benchmark
3. Conduct a cost-benefit analysis comparing the computational resources required to maintain multiple specialized models versus the performance gains achieved across the operational task portfolio