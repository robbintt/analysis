---
ver: rpa2
title: 'RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes'
arxiv_id: '2601.21803'
source_url: https://arxiv.org/abs/2601.21803
tags:
- documents
- document
- query
- attribution
- gemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAG-E, a framework that quantifies alignment
  between retrieval and generation components in Retrieval-Augmented Generation systems.
  It adapts Integrated Gradients for retrievers and introduces PMCSHAP, a Monte Carlo-stabilized
  Shapley Value method, for generators, along with the Weighted Attribution-Relevance
  Gap (WARG) metric to measure misalignment.
---

# RAG-E: Quantifying Retriever-Generator Alignment and Failure Modes

## Quick Facts
- **arXiv ID:** 2601.21803
- **Source URL:** https://arxiv.org/abs/2601.21803
- **Reference count:** 40
- **Primary result:** RAG-E framework quantifies retriever-generator alignment using attribution methods and reveals 47.4%-66.7% of top-ranked documents are ignored by generators

## Executive Summary
This paper introduces RAG-E, a framework that quantifies alignment between retrieval and generation components in Retrieval-Augmented Generation systems. It adapts Integrated Gradients for retrievers and introduces PMCSHAP, a Monte Carlo-stabilized Shapley Value method, for generators, along with the Weighted Attribution-Relevance Gap (WARG) metric to measure misalignment. Testing on TREC CAsT and FoodSafeSum datasets shows that generators ignore top-ranked documents in 47.4%-66.7% of queries and rely on lower-ranked ones in 48.1%-65.9% of cases. These findings reveal that RAG output quality depends on component interplay, not just individual performance, and demonstrate that RAG-E can audit and diagnose such failures.

## Method Summary
RAG-E quantifies retriever-generator alignment through attribution-based analysis. For retrievers, it uses Integrated Gradients to compute feature importance, while generators employ PMCSHAP (Monte Carlo-stabilized Shapley Values) to determine contribution weights. The Weighted Attribution-Relevance Gap (WARG) metric compares attribution scores with relevance rankings to identify misalignment. The framework processes queries through both components, computes attribution maps, and aggregates WARG scores across document positions to quantify systematic failures in component coordination.

## Key Results
- Generators ignore top-ranked documents in 47.4%-66.7% of queries across datasets
- Generators rely on lower-ranked documents in 48.1%-65.9% of queries
- RAG-E successfully identifies systematic misalignment between retrieval and generation components

## Why This Works (Mechanism)
The framework leverages attribution methods to trace how retrievers and generators use input features. Integrated Gradients provides stable gradients for retrievers by integrating along paths from baseline to input, while PMCSHAP approximates Shapley Values through Monte Carlo sampling to reduce computational variance. The WARG metric quantifies misalignment by comparing attribution-based importance with relevance rankings, revealing cases where generators over-rely on low-relevance documents or ignore high-relevance ones.

## Foundational Learning
- **Integrated Gradients:** Path-based gradient attribution method needed to provide baseline-invariant feature importance for retrievers
  - *Quick check:* Verify attribution stability across different baseline choices
- **Shapley Values:** Game-theoretic approach to fairly distribute feature contributions among documents for generators
  - *Quick check:* Confirm convergence with increasing Monte Carlo samples
- **Monte Carlo Stabilization:** Sampling technique to reduce variance in Shapley Value estimation for large document sets
  - *Quick check:* Measure variance reduction across sample sizes
- **WARG Metric:** Alignment measure comparing attribution scores with relevance rankings to detect misalignment
  - *Quick check:* Validate correlation with human judgment of output quality
- **Feature Attribution vs Relevance Gap:** Core insight that traditional metrics miss coordination failures between components
  - *Quick check:* Compare RAG-E findings against downstream task performance

## Architecture Onboarding

**Component Map:** Query -> Retriever (BM25/Embedding) -> Documents -> Generator (LLM) -> Output
**Critical Path:** Input query flows through retriever to generator, with RAG-E analyzing attribution at each stage
**Design Tradeoffs:** Attribution methods trade computational cost for alignment insight; PMCSHAP reduces variance but increases runtime
**Failure Signatures:** High WARG scores indicate systematic misalignment; generators ignoring top documents or over-relying on low-ranked ones
**First Experiments:** 1) Run RAG-E on synthetic datasets with known alignment properties, 2) Compare attribution maps across retriever types, 3) Test WARG sensitivity to relevance score distributions

## Open Questions the Paper Calls Out
None

## Limitations
- Attribution-based approach may miss complex reasoning chains or temporal dependencies
- PMCSHAP introduces computational overhead that scales poorly with document count
- WARG metric assumes linear separability between relevance and attribution signals

## Confidence
- Methodology reliability: **High** - well-established attribution methods with clear metrics
- Generalizability of failure rates: **Medium** - limited to two datasets, may not reflect broader scenarios
- System-level quality impact claims: **Medium** - extrapolates from misalignment patterns without direct performance measurements

## Next Checks
1. Test RAG-E on diverse RAG architectures (dense retrieval, re-ranking, fusion models) and domains beyond question answering to assess framework robustness
2. Correlate WARG scores with human-evaluated answer quality to validate the metric as a proxy for RAG performance
3. Compare RAG-E findings against alternative interpretability methods (e.g., attention visualization, causal ablation) to triangulate failure mode detection