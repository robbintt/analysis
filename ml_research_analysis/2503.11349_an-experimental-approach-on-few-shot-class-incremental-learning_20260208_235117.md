---
ver: rpa2
title: An experimental approach on Few Shot Class Incremental Learning
arxiv_id: '2503.11349'
source_url: https://arxiv.org/abs/2503.11349
tags:
- learning
- clip
- fscil
- cloob
- lp-dif
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an experimental approach to improve Few-Shot
  Class-Incremental Learning (FSCIL) by replacing the vision-language model (CLIP)
  with CLOOB in the LP-DiF framework. The main motivation is to address catastrophic
  forgetting and enhance adaptability to new classes with limited examples.
---

# An experimental approach on Few Shot Class Incremental Learning

## Quick Facts
- arXiv ID: 2503.11349
- Source URL: https://arxiv.org/abs/2503.11349
- Authors: Marinela Adam
- Reference count: 23
- Primary result: Replacing CLIP with CLOOB in the LP-DiF framework improves Few-Shot Class-Incremental Learning (FSCIL) accuracy, especially with larger encoders like ResNet50x4.

## Executive Summary
This paper proposes replacing the vision-language model CLIP with CLOOB in the LP-DiF framework to improve Few-Shot Class-Incremental Learning (FSCIL). The main motivation is to address catastrophic forgetting and enhance adaptability to new classes with limited examples. Preliminary results show that CLOOB, when integrated into LP-DiF, achieves better accuracy than CLIP, particularly with larger encoders like ResNet50x4. The proposed approach outperforms state-of-the-art methods, with significant improvements in accuracy and reduced forgetting rates. Future work includes exploring different backbone architectures and training datasets to further enhance performance.

## Method Summary
The method involves integrating CLOOB, a vision-language model with enhanced contrastive learning, into the LP-DiF framework for FSCIL. The LP-DiF framework uses a Variational Autoencoder (VAE) to generate pseudo-features for replay and learnable prompts for adapting a frozen vision-language model to new classes. The study compares the performance of LP-DiF with CLIP and CLOOB on benchmark datasets like CIFAR100, miniImageNet, and CUB200.

## Key Results
- CLOOB integrated into LP-DiF achieves better accuracy than CLIP, particularly with larger encoders like ResNet50x4.
- The proposed approach outperforms state-of-the-art methods, with significant improvements in accuracy and reduced forgetting rates.
- Performance gains are contingent on sufficient model capacity, as CLOOB only outperforms CLIP with larger encoders.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Replacing CLIP with CLOOB in the LP-DiF framework may improve FSCIL performance by providing better feature representations through enhanced contrastive learning.
- Mechanism: CLOOB utilizes modern Hopfield networks and the InfoLOOB objective, which aims to address the "explaining away" problem and feature saturation issues found in CLIP's InfoNCE objective. This leads to more robust and comprehensive feature embeddings.
- Core assumption: The improved feature representations from CLOOB transfer effectively to the class-incremental learning task.
- Evidence anchors:
  - [abstract] "CLOOB, when integrated into LP-DiF, achieves better accuracy than CLIP..."
  - [section 3.3] "CLOOB... aims to improve robustness and performance... it managed to outperform CLIP in most cases."
  - [corpus] Related work on prototypes and regularization for FSCIL but lack direct comparative studies between CLOOB and CLIP in this specific context.

### Mechanism 2
- Claim: The LP-DiF framework mitigates catastrophic forgetting by decoupling the learning of new information from the preservation of old information via learnable prompts and pseudo-feature replay.
- Mechanism: The framework uses a VAE to learn the distribution of features for each class. During incremental sessions, the VAE generates pseudo-features from these distributions, which are then used to train the prompt vectors while the underlying vision-language model's weights remain frozen.
- Core assumption: The pseudo-features generated by the VAE are a sufficiently accurate representation of the real features of old classes to prevent decision boundary drift.
- Evidence anchors:
  - [abstract] "The main motivation is to address catastrophic forgetting..."
  - [section 3.1] "Feature replay in LP-DiF addresses the challenge of catastrophic forgetting by replaying pseudo-features instead of raw images."
  - [corpus] Related paper "Few-shot Class-Incremental Learning via Generative Co-Memory Regularization" also uses generative models for regularization.

### Mechanism 3
- Claim: Prompt tuning provides a parameter-efficient way to adapt a frozen pre-trained vision-language model to new classes.
- Mechanism: Instead of fine-tuning the entire model, a small set of learnable "prompt" vectors are prepended to the input or intermediate layers. During a new learning session, only these prompt vectors are updated using both current-session data and replayed pseudo-features.
- Core assumption: The frozen backbone model has sufficient existing knowledge and representational power to handle new classes with only a small number of tunable parameters added.
- Evidence anchors:
  - [section 3.1] "Prompt tuning in LP-DiF adapts the vision-language model (CLIP) to new tasks while retaining knowledge from previous tasks."
  - [section 5.1] Comparisons show LP-DiF (with prompts) outperforming linear classifiers (LC).
  - [corpus] No direct corpus evidence found specifically for "prompt tuning in FSCIL" in the provided signals.

## Foundational Learning

- Concept: **Catastrophic Forgetting**
  - Why needed here: This is the central problem FSCIL aims to solve. It's the tendency of a neural network to completely forget previously learned information upon learning new data.
  - Quick check question: If a model trained on cats and dogs is then trained only on birds, will it still correctly classify cats and dogs?

- Concept: **Zero-Shot and Few-Shot Learning**
  - Why needed here: The paper leverages models (CLIP/CLOOB) pre-trained on vast datasets to perform tasks with little or no task-specific data. FSCIL is an extension of this to a sequential setting.
  - Quick check question: Can a model identify a "glib-glob" if it has never seen one, but is told it's a "furry, four-legged creature with antennae"?

- Concept: **Vision-Language Models (VLMs) & Contrastive Learning**
  - Why needed here: CLIP and CLOOB are VLMs trained with contrastive objectives. They learn by aligning images with their corresponding text descriptions in a shared embedding space.
  - Quick check question: How does a model learn to associate a picture of a dog with the word "dog" without explicit classification labels during pre-training?

## Architecture Onboarding

- **Component map:** Frozen Vision Encoder (CLIP or CLOOB) -> VAE (learns class distributions) -> Prompt Tuning (updates prompts using real and pseudo-features) -> Classification via encoder's text tower.

- **Critical path:** The accuracy of the entire system depends on the quality of the feature embeddings from the vision encoder. If these features are poor, both the VAE's distribution modeling and the prompt tuning will be ineffective.

- **Design tradeoffs:** The paper highlights a tradeoff between encoder size and performance. CLOOB only outperforms CLIP with a larger encoder (ResNet50x4 vs. ResNet50). There's also a tradeoff in using VAE-generated pseudo-features versus storing real exemplars: pseudo-features save storage and address privacy but may be less accurate.

- **Failure signatures:** A primary failure mode is prompt saturation, where the prompts can no longer encode new information. Another is VAE hallucination, where generated pseudo-features are unrealistic, degrading performance on old classes. The paper also notes sensitivity to hyperparameters.

- **First 3 experiments:**
  1. **Reproduce the LP-DiF Baseline:** Train the LP-DiF framework on the CIFAR100 and CUB200 datasets using the provided CLIP checkpoints (ResNet50 and ResNet50x4). Validate that the accuracy and forgetting rate metrics match the baseline figures in the paper.
  2. **Zero-Shot Feature Quality Test:** Before full integration, compare CLIP and CLOOB's zero-shot performance on a held-out validation set from the target FSCIL datasets. Quantify the improvement in feature quality.
  3. **Ablation on Encoder Size:** Integrate CLOOB into LP-DiF and run the FSCIL protocol first with the ResNet50 backbone and then with the ResNet50x4 backbone. Compare the final session accuracy and forgetting rates.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of the LP-DiF framework evolve when CLOOB is implemented with Vision Transformer (ViT-B/16) backbones compared to the ResNet architectures currently tested?
- **Open Question 2:** To what extent does pre-training CLOOB on filtered, high-quality datasets improve the accuracy and robustness of the Few-Shot Class-Incremental Learning framework compared to training on the noisy YFCC dataset?
- **Open Question 3:** Can the saturation effect associated with Hopfield networks in CLOOB be mitigated to provide consistent performance gains over CLIP across all encoder sizes?

## Limitations
- The improvements are only observed with ResNet50x4, not with the smaller ResNet50 backbone, suggesting the gains are contingent on sufficient model capacity.
- The study is limited to three datasets (CIFAR100, miniImageNet, CUB200), and the results may not generalize to other domains or larger-scale problems.
- No analysis of computational cost or inference-time overhead is provided, which could be significant given the larger encoder and additional VAE components.

## Confidence
- **High**: The conceptual motivation (CLOOB's improved contrastive learning) and the overall framework (LP-DiF with prompt tuning and VAE replay) are well-established and sound.
- **Medium**: The experimental results showing CLOOB's superiority over CLIP are valid but limited to specific encoder sizes and datasets.
- **Low**: Claims about generalizability to other backbones, datasets, or scenarios without further validation.

## Next Checks
1. **Reproduce the LP-DiF Baseline**: Train the LP-DiF framework on CIFAR100 and CUB200 using provided CLIP checkpoints (ResNet50 and ResNet50x4) to validate baseline accuracy and forgetting rate metrics.
2. **Zero-Shot Feature Quality Test**: Compare CLIP and CLOOB's zero-shot performance on held-out validation sets from target FSCIL datasets to quantify feature quality improvements.
3. **Ablation on Encoder Size**: Integrate CLOOB into LP-DiF and run the FSCIL protocol with both ResNet50 and ResNet50x4 backbones to directly test the claim that CLOOB's benefits emerge with larger encoders.