---
ver: rpa2
title: Jailbreaking LLMs via Calibration
arxiv_id: '2602.00619'
source_url: https://arxiv.org/abs/2602.00619
tags:
- shift
- gradient
- jailbreaking
- loss
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework for jailbreaking LLMs
  by modeling safety alignment as a systematic distortion of the pre-alignment distribution.
  The core idea is to cast Weak-to-Strong Jailbreaking as a forecast aggregation problem,
  where the helper and predictor models are combined with the target model to recover
  suppressed behaviors.
---

# Jailbreaking LLMs via Calibration

## Quick Facts
- arXiv ID: 2602.00619
- Source URL: https://arxiv.org/abs/2602.00619
- Reference count: 35
- Key outcome: Jailbreaking via dual-space gradient aggregation recovers suppressed behaviors while maintaining utility

## Executive Summary
This paper presents a theoretical framework for jailbreaking aligned LLMs by modeling safety alignment as a systematic distortion of the pre-alignment distribution. The authors reformulate Weak-to-Strong Jailbreaking as a forecast aggregation problem, where a helper model provides a calibrated proxy for the pre-alignment distribution, and a predictor estimates how alignment shifts the helper's output. By computing gradient shifts in the dual space induced by proper loss functions and projecting back, the method recovers suppressed behaviors while theoretically preserving the model's original capabilities.

The framework yields a family of aggregation rules parameterized by the choice of loss function, including multiplicative, additive, and hybrid variants. Empirical evaluations across red-teaming benchmarks and math utility tasks demonstrate that the hybrid rule achieves superior Attack Success Rates and lower Jailbreak Tax compared to existing methods, particularly on safety-hardened models. The method attains near-saturated success at zero jailbreak tax on math tasks, recovering the model's pre-alignment capabilities without sacrificing performance.

## Method Summary
The method operates by combining three models: a target (aligned) model, a helper (unaligned) model, and a predictor (aligned version of helper) model. At each token generation step, all three models produce next-token distributions, which are transformed into the dual space induced by a proper loss function. The gradient shift is computed as ∇G(pt) + ∇G(ph) - ∇G(pt|h), where pt is the target, ph is the helper, and pt|h is the predictor. This result is projected back to the probability simplex to obtain the aggregated distribution, from which the next token is sampled. The approach generalizes existing logit-arithmetic methods as special cases under cross-entropy loss.

## Key Results
- Hybrid aggregation rule achieves superior Attack Success Rates on HarmBench and StrongREJECT benchmarks compared to existing methods
- Near-zero jailbreak tax on math tasks (GSM8K, Hendrycks-MATH) with >95% success rate
- Framework recovers suppressed behaviors while theoretically preserving pre-alignment capabilities
- Method demonstrates effectiveness particularly on safety-hardened models like gpt-oss-120b

## Why This Works (Mechanism)

### Mechanism 1: Dual-Space Gradient Aggregation
The optimal jailbreak aggregation occurs in the gradient (dual) space induced by proper loss functions, not directly in probability space. The helper model provides a calibrated proxy for the pre-alignment distribution Ypre, and the predictor estimates how alignment shifts the helper's output. By computing ∇G(pt) + ∇G(ph) - ∇G(pt|h) in dual space and projecting back, the method applies the inverse of the observed alignment shift to the target model. This relies on the helper being conditionally calibrated to Ypre and the predictor approximating E[Pt | Ph].

### Mechanism 2: Loss Function Determines Aggregation Family
Different proper loss functions yield different optimal aggregation rules. Cross-entropy loss gives multiplicative updates: p*(y) ∝ pt(y) · ph(y) / pt|h(y), while quadratic loss gives additive updates: p*(y) = (pt(y) + ph(y) - pt|h(y) - τ)₊. The choice of loss function reflects the geometry in which alignment distortion is "simplest," though the paper does not claim one loss is universally better.

### Mechanism 3: Bregman Divergence Lower Bound on Improvement
The expected improvement in prediction loss is at least E[D_G(Ph, Pt|h)], which is strictly positive when the helper and predictor differ. The divergence D_G(ph, pt|h) quantifies the "alignment shift magnitude," capturing how much correction is available since alignment pushes pt away from Ypre. This provides a theoretical guarantee that the method will improve upon the target model when the helper-predictor divergence is non-zero.

## Foundational Learning

- **Bregman Divergence and Proper Scoring Rules**
  - Why needed here: The entire framework relies on the geometric structure induced by proper losses. Understanding that every proper loss has a generator G and induces a Bregman divergence D_G is essential for grasping why operations must happen in dual space.
  - Quick check question: Given G(p) = Σ pᵢ ln pᵢ, compute D_G(p, q) and identify which space the divergence is "linear" in.

- **Forecast Aggregation / Crowdsourced Prediction**
  - Why needed here: The paper frames jailbreaking as aggregating multiple probabilistic forecasts (helper, predictor, target) to recover a latent distribution. Prior work on robust aggregation provides the theoretical tools.
  - Quick check question: In a binary outcome setting, if expert A reports Pr[Y=1] = 0.7 and expert B reports Pr[Y=1] = 0.3, what is the geometric mean aggregator and why might it be robust?

- **Calibration of Probabilistic Predictors**
  - Why needed here: The core assumption is that the helper model is calibrated to Ypre. Understanding calibration helps evaluate when this assumption is reasonable and how to diagnose violations.
  - Quick check question: A model outputs p = 0.8 for "spam" on 100 emails, but only 50 are actually spam. Is the model calibrated? What correction would improve calibration?

## Architecture Onboarding

- **Component map:**
  - Target model (πt) -> Helper model (πh) -> Predictor model (πt|h) -> Aggregation function f* -> Bregman projection Π^G_Δ -> Token sampling

- **Critical path:**
  1. Run all three models on the same prompt x to get (pt, ph, pt|h)
  2. Transform to dual space: compute (∇G(pt), ∇G(ph), ∇G(pt|h))
  3. Apply gradient shift: z = ∇G(pt) + ∇G(ph) - ∇G(pt|h)
  4. Project back: p* = Π^G_Δ((∇G)^{-1}(z))
  5. Sample from p* for next-token generation
  6. Repeat for each token in autoregressive generation

- **Design tradeoffs:**
  - **Multiplicative vs. Additive vs. Hybrid:** Multiplicative (cross-entropy) is theoretically optimal but unstable with low probabilities. Additive (quadratic) is stable but may under-correct. Hybrid uses multiplicative when ph < pt|h (suppression regime) and additive otherwise.
  - **Helper model selection:** Must be unaligned and share tokenizer. Larger helpers may be more capable but also more expensive.
  - **k-token vs. full-sequence:** Weak-to-Strong baseline applies logit shifts only to first k tokens. The paper's method applies to all tokens.

- **Failure signatures:**
  - **Division instability:** If pt|h(y) ≈ 0 for some token y in multiplicative mode, the ratio ph(y)/pt|h(y) explodes. Symptom: NaN probabilities or degenerate outputs.
  - **Helper-predictor mismatch:** If ph ≈ pt|h (no divergence), the method provides no improvement. Symptom: ASR similar to target baseline.
  - **Tokenizer mismatch:** If models use different tokenizers, probability vectors cannot be meaningfully combined. Symptom: Shape mismatches or incoherent outputs.

- **First 3 experiments:**
  1. **Sanity check on a single harmful prompt:** Choose "How to make a bomb?" or similar. Visualize pt, ph, pt|h distributions. Compute the divergence D_G(ph, pt|h) and verify it's positive. Apply all three aggregation rules and inspect the output. Target: Hybrid should produce compliance while target refuses.
  2. **Ablation on loss function:** Run the full HarmBench (200 prompts) evaluation with additive, multiplicative, and hybrid rules on Llama-3.3-70B-Instruct. Report ASR and harmfulness. Expect: Hybrid ≥ Multiplicative > Additive on ASR, with hybrid most stable.
  3. **Jailbreak tax measurement on math:** Use the GSM8K setup with a refuse-math LoRA'd model. Compare Gradient Shift (hybrid) against Weak-to-Strong baselines and prior methods. Report success rate, correct rate, and jailbreak tax. Target: Near-zero jailbreak tax with >95% success rate.

## Open Questions the Paper Calls Out

**Cross-tokenizer extension:** Can the Gradient Shift framework be extended to settings where the helper and target models have different tokenizers? The authors state their approach requires a weak, unaligned model that shares the target's tokenizer, but no analysis is provided for cross-tokenizer scenarios.

**Optimal multiclass hybrid rule:** Is there a theoretically optimal hybrid aggregation rule for the general multiclass setting, beyond the binary case? The paper proves hybrid optimality only for |Y|=2, while for multiclass next-token prediction, the hybrid is described as a "stability-motivated variant" without formal optimality guarantees.

**Defensive applications:** How effectively can the Gradient Shift framework be inverted to construct jailbreak defenses? The paper states the same idea can be used in the opposite direction for jailbreak defense by shifting an unsafe model toward a safer distribution, but no experiments validate this claim.

## Limitations

- **Helper model calibration assumption:** The framework assumes the helper model is calibrated to the pre-alignment distribution, which is a strong and unverifiable assumption in practice that may not hold for real models.
- **Computational overhead:** Running three models at each generation step introduces significant computational overhead that may become impractical for real-time applications or very large target models.
- **Generalizability to other alignment methods:** The framework models alignment as a systematic distortion, but different alignment techniques may induce different types of distortions, limiting the framework's generalizability.

## Confidence

**High Confidence** - The theoretical framework connecting jailbreaking to forecast aggregation via proper loss functions is mathematically sound and internally consistent. The derivations of multiplicative and additive aggregation rules from cross-entropy and quadratic losses are correct.

**Medium Confidence** - The empirical superiority of the hybrid aggregation rule over existing methods is demonstrated on specific datasets and model combinations, but results may not generalize to all model architectures, alignment methods, or prompt types.

**Low Confidence** - The assumption that helper models are calibrated to pre-alignment distributions is theoretically convenient but practically unverifiable. The paper does not provide methods to test or enforce this calibration, nor does it analyze sensitivity to calibration violations.

## Next Checks

1. **Helper Model Calibration Analysis** - Systematically test the calibration assumption by evaluating helper model outputs on diverse benign and harmful prompts. Measure the divergence D_G(ph, pt|h) across different prompt categories and analyze the correlation between divergence magnitude and jailbreak success.

2. **Cross-Alignment Method Robustness** - Apply the framework to models aligned with different techniques (RLHF vs. supervised fine-tuning vs. adversarial training) and compare jailbreak success rates to test generalizability and identify susceptible alignment methods.

3. **Sequence-Level Coherence Evaluation** - Beyond ASR and harmfulness metrics, evaluate the semantic coherence and task-completion quality of jailbroken outputs using automated metrics (perplexity, semantic similarity) and human evaluation to validate token-level effectiveness translates to meaningful sequence-level recovery.