---
ver: rpa2
title: 'Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language
  CoT for Reasoning'
arxiv_id: '2510.25310'
source_url: https://arxiv.org/abs/2510.25310
tags:
- p-cot
- n-cot
- reasoning
- parrot
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the mutual enhancement of natural language
  chain-of-thought (N-CoT) and program chain-of-thought (P-CoT) reasoning paradigms
  for mathematical problem solving. The authors propose Parrot, a training pipeline
  that integrates three subtasks: Information Retrieval, P-CoT Reasoning, and Paradigm
  Conversion.'
---

# Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning

## Quick Facts
- arXiv ID: 2510.25310
- Source URL: https://arxiv.org/abs/2510.25310
- Authors: Senjie Jin; Lu Chen; Zhiheng Xi; Yuhui Wang; Sirui Song; Yuhao Zhou; Xinbo Zhang; Peng Sun; Hong Lu; Tao Gui; Qi Zhang; Xuanjing Huang
- Reference count: 26
- Primary result: Parrot SFT improves LLaMA2 N-CoT accuracy by +21.87 on MathQA over RL baseline

## Executive Summary
This paper addresses the mutual enhancement of natural language chain-of-thought (N-CoT) and program chain-of-thought (P-CoT) reasoning paradigms for mathematical problem solving. The authors propose Parrot, a training pipeline that integrates three subtasks: Information Retrieval, P-CoT Reasoning, and Paradigm Conversion. The pipeline uses hybrid supervised fine-tuning to facilitate semantic transferability between paradigms, and incorporates reinforcement learning with an auxiliary reward based on converted N-CoT to address sparse reward issues in P-CoT optimization. Experimental results demonstrate that Parrot significantly improves both N-CoT and P-CoT performance across multiple datasets and model families.

## Method Summary
Parrot is a training pipeline with three sequential subtasks: Information Retrieval (extracting key numbers/context), P-CoT Reasoning (generating executable Python code), and Paradigm Conversion (generating N-CoT from P-CoT plus execution results). The core training uses Hybrid Supervised Fine-Tuning where all three subtasks are trained simultaneously in a unified format to enable cross-paradigm semantic transfer. The pipeline executes P-CoT code to capture intermediate variable values, which are then used to condition N-CoT generation—providing process supervision that reduces calculation and logical inconsistency errors. An optional reinforcement learning phase with PPO uses the validity of converted N-CoT as an auxiliary reward signal to address sparse rewards in P-CoT optimization.

## Key Results
- Parrot SFT improves LLaMA2 N-CoT accuracy by +21.87 and +21.48 on MathQA over RL baseline
- Calculation errors reduced from 578 to 445 and logical inconsistency from 403 to 171 on LLaMA2
- P-CoT + PC consistently outperforms baseline P-CoT (+1.1 GSM8K, +2.1 SV AMP)
- Training curves show PPO without penalty signal leads to early suboptimal overfitting

## Why This Works (Mechanism)

### Mechanism 1: P-CoT Intermediate Results as Process Supervision for N-CoT
Incorporating P-CoT intermediate execution results into N-CoT generation reduces calculation and logical inconsistency errors by providing verifiable intermediate values rather than hallucinations.

### Mechanism 2: Hybrid Multi-Task Training Enables Cross-Paradigm Semantic Transfer
Simultaneous training on all three subtasks transfers semantic strengths between paradigms—N-CoT's explicit semantic analysis improves P-CoT variable definition, while P-CoT's precise reasoning structure enhances N-CoT coherence.

### Mechanism 3: Converted N-CoT Validity as Auxiliary Reward for P-CoT RL
Using the validity of paradigm-converted N-CoT as an auxiliary reward signal mitigates sparse reward problems in P-CoT reinforcement learning by providing partial credit for meaningful progress.

## Foundational Learning

- **Concept: Program-Aided Language Models (PAL) / P-CoT**
  - Why needed here: The pipeline assumes familiarity with using executable code to offload computation. P-CoT generates Python code that an interpreter executes, providing deterministic calculation and verifiable intermediate states.
  - Quick check question: Can you explain why code execution provides stronger verification than natural language reasoning alone?

- **Concept: Process vs. Outcome Supervision**
  - Why needed here: Parrot's core insight is using P-CoT intermediate results as process supervision for N-CoT. You must distinguish between outcome supervision (final answer correct/incorrect) and process supervision (intermediate steps verified).
  - Quick check question: How does process supervision change the credit assignment problem in multi-step reasoning?

- **Concept: Reward Shaping / Auxiliary Rewards in RL**
  - Why needed here: The converted N-CoT reward is an auxiliary signal addressing reward sparsity. Without understanding reward shaping, the design rationale for the graduated reward function will be opaque.
  - Quick check question: What problem does adding an auxiliary reward solve, and what risks does it introduce?

## Architecture Onboarding

- **Component map:** Input Problem → Information Retrieval → P-CoT Reasoning → Interpreter Execution → Paradigm Conversion → Final Answer

- **Critical path:**
  1. SFT Phase: Hybrid training on unified format `(input: question + subtask_prompt, output: subtask_response)` across all three subtasks simultaneously.
  2. RL Phase (optional): Initialize from SFT checkpoint → PPO with auxiliary reward from converted N-CoT validity → value head on last hidden states.
  3. Inference: Sequential generation through all three subtasks; interpreter executes P-CoT; N-CoT conditioned on intermediate results.

- **Design tradeoffs:**
  - Sequential inference vs. training efficiency: Pipeline is sequential at inference (IR → P-CoT → N-CoT) but trained jointly. Latency increases ~3x vs. single-parad generation.
  - Data distribution sensitivity: Hybrid training requires balanced subtask data; paper notes "high sensitivity to data distribution" as limitation.
  - Complexity vs. P-CoT-only: On simple datasets (SV AMP), IR subtask shows minimal benefit and can mislead; recommend dataset-specific subtask inclusion.

- **Failure signatures:**
  - Variable definition errors in P-CoT: If IR fails to identify key information, P-CoT generates incorrect variable definitions → execution fails → intermediate results fallback to variable names only (no numerical supervision).
  - Overfitting in On-SL: Without negative examples, combined with hybrid training, performance degrades (Parrot On-SL underperforms Parrot SFT on several metrics).
  - Suboptimal early convergence in RL: Without N-CoT penalty signal, model overfits to easy rewards early (Figure 3 left).

- **First 3 experiments:**
  1. **Sanity check with hybrid SFT only:** Train Parrot SFT on GSM8K (simpler, well-studied). Compare N-CoT accuracy vs. baseline SFT. Expect +10-15 point gain if pipeline is functional. Check error type distribution shift (calculation errors should drop most).
  2. **Ablate intermediate results:** Run Paradigm Conversion subtask with and without intermediate results (Table 2 configuration). Quantify the delta—should see ~11-18 point N-CoT drop without intermediate values. This validates the process supervision hypothesis.
  3. **RL reward function validation:** Train PPO for P-CoT on SV AMP (easier exploration) with and without the N-CoT auxiliary reward. Plot training accuracy curves. Without penalty, expect early plateau; with penalty, expect steadier improvement (replicate Figure 3). Monitor for reward hacking (model gaming auxiliary reward without improving final accuracy).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Parrot perform on the complex MATH dataset where problems are primarily written in LaTeX?
- Basis in paper: The authors state in the Limitations section that they did not experiment on MATH because "Most MATH problems and solutions are written in LaTeX, which highlights the limitations of natural language in key information retrieval and resolution conversion."
- Why unresolved: The Parrot pipeline currently relies on natural language for Information Retrieval and Paradigm Conversion subtasks, which may fail when the input consists largely of symbolic mathematical expressions rather than natural language text.
- What evidence would resolve it: Evaluation results on the MATH benchmark, specifically analyzing the success rate of the Information Retrieval subtask on LaTeX inputs and the resulting P-CoT/N-CoT accuracy.

### Open Question 2
- Question: Can the Parrot pipeline be effectively adapted for formal logic or theorem proving?
- Basis in paper: The Conclusion states: "In the future, we plan to apply the Parrot pipeline to other reasoning domains such as math proving."
- Why unresolved: The current pipeline links P-CoT (Python) with N-CoT (Natural Language), whereas theorem proving requires bridging natural language with formal proof assistants (e.g., Lean, Isabelle) rather than Python interpreters.
- What evidence would resolve it: Successful application of the pipeline to a theorem proving dataset (e.g., miniF2F) demonstrating that the mutual enhancement strategy works between natural language and formal proof languages.

### Open Question 3
- Question: What specific mechanisms can reduce the high sensitivity of the sub-task hybrid training strategy to data distribution?
- Basis in paper: The Limitations section notes the strategy "demonstrates high sensitivity to data distribution" and suggests "Future research could investigate more stable training paradigms."
- Why unresolved: The paper observes that the method requires carefully balanced datasets to function, and the role of model initialization in these multi-task scenarios remains understudied.
- What evidence would resolve it: A modification to the training paradigm (e.g., curriculum learning or adaptive loss weighting) that maintains performance parity across unbalanced or noisy data distributions.

## Limitations
- High sensitivity to data distribution, particularly requiring balanced subtask data for effective hybrid training
- Sequential inference pipeline introduces ~3x latency compared to single-paradigm generation
- Auxiliary N-CoT reward signal may introduce noise if paradigm conversion quality is poor

## Confidence
- **High Confidence:** The hybrid SFT mechanism improving N-CoT accuracy through process supervision (reduces calculation and logical inconsistency errors by 21-23% points)
- **Medium Confidence:** The semantic transfer between paradigms—while empirically supported by +2.0 MathQA P-CoT improvement with IR subtask
- **Low Confidence:** The computational efficiency claims and dataset-specific subtask inclusion recommendations

## Next Checks
1. Run ablation on GSM8K with and without intermediate results to quantify the process supervision effect on N-CoT accuracy (should show 11-18 point drop)
2. Train PPO with and without the N-CoT auxiliary reward on SV AMP, plotting training curves to verify the penalty signal prevents early overfitting
3. Test dataset sensitivity by training on SV AMP with and without IR subtask to confirm the "over-retrieval misleads" hypothesis (expect minimal benefit or degradation)