---
ver: rpa2
title: 'DLM-One: Diffusion Language Models for One-Step Sequence Generation'
arxiv_id: '2506.00290'
source_url: https://arxiv.org/abs/2506.00290
tags:
- diffusion
- generation
- arxiv
- score
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DLM-One, a score distillation framework enabling
  one-step sequence generation with continuous diffusion language models. By aligning
  the student model's output scores with a pretrained teacher's score function in
  the embedding space, DLM-One eliminates iterative refinement steps.
---

# DLM-One: Diffusion Language Models for One-Step Sequence Generation

## Quick Facts
- arXiv ID: 2506.00290
- Source URL: https://arxiv.org/abs/2506.00290
- Authors: Tianqi Chen; Shujian Zhang; Mingyuan Zhou
- Reference count: 27
- Key outcome: One-step diffusion language model achieving ~500× speedup over DiffuSeq while maintaining competitive quality

## Executive Summary
This paper introduces DLM-One, a score distillation framework that enables one-step sequence generation with continuous diffusion language models. By aligning the student model's output scores with a pretrained teacher's score function in the embedding space, DLM-One eliminates iterative refinement steps. The method employs a two-stage training process with adversarial regularization to stabilize training and prevent degeneration. Evaluated on benchmark Seq2Seq tasks, DLM-One achieves up to ~500× speedup over DiffuSeq while maintaining competitive BLEU, ROUGE-L, and BERTScore performance.

## Method Summary
DLM-One trains a student generator to produce high-quality sequences in one forward pass by matching the score function of a pretrained teacher DLM in forward-diffused embedding space. The method uses adversarial score distillation (SiD) loss with a two-stage training process: Stage 1 trains the student and score estimator jointly, then Stage 2 reinitializes the score estimator from the teacher while continuing student training. This approach addresses the common degeneration problems in data-free distillation by providing implicit real-data supervision through adversarial regularization.

## Key Results
- Achieves ~500× speedup (1 NFE vs 2000 NFEs) over DiffuSeq on Seq2Seq tasks
- Maintains competitive BLEU scores: 0.1788 (QQP), 0.2139 (Wiki-Auto), 0.2985 (Quasar-T)
- Two-stage training improves BLEU by 21.8% on QQP compared to single-stage
- Inference-time multi-step refinement (2-4 steps) further improves quality while preserving diversity

## Why This Works (Mechanism)

### Mechanism 1: Score Matching in Forward-Diffused Embedding Space
The student generator produces high-quality sequences in one step by matching the teacher's score function in the noisy embedding space across all noise levels. The score estimator is trained to approximate the true score corresponding to the student's evolving distribution, with the SiD loss making the objective tractable via Tweedie's formula. The embedding space structure from pretrained embeddings is preserved, enabling accurate score matching.

### Mechanism 2: Adversarial Regularization Prevents Degeneration
Data-free distillation in DLMs produces degenerate outputs (repetitive tokens or PAD-filled sequences). The score estimator doubles as a GAN discriminator, receiving real and generated pairs at noise level t and computing BCE loss. This anchors the student's outputs to the real data distribution without requiring gradient access to original training data.

### Mechanism 3: Two-Stage Training Mitigates Score Estimator Lag
The score estimator cannot track the student's rapidly evolving distribution. Stage 2 restarts the score estimator from the teacher weights while continuing student training, re-aligning it closer to the student's now-teacher-like distribution. This enables further refinement and prevents the estimator from becoming stale.

## Foundational Learning

- **Concept: Score Functions and Tweedie's Formula**
  - Why needed here: The entire distillation framework is built on matching score functions ∇_e_t log p(e_t|t,c) in embedding space. Tweedie's formula converts score matching to denoising (predicting clean e from noisy e_t), making the objective tractable.
  - Quick check question: Given a noisy observation e_t = α_t·e + σ_t·ε, can you derive the relationship between the score s(e_t) and the posterior mean estimate ê(e_t)?

- **Concept: Fisher Divergence and Distribution Matching**
  - Why needed here: The MESM loss measures Fisher divergence between student and teacher distributions. Understanding this connects score matching to distributional similarity.
  - Quick check question: Why does minimizing the expected squared difference between two score functions approximate matching their underlying distributions?

- **Concept: Continuous vs. Discrete Diffusion for Language**
  - Why needed here: DLM-One operates on continuous embeddings, not discrete tokens. This choice enables borrowing vision-domain acceleration techniques but introduces rounding/decoding challenges.
  - Quick check question: What are the trade-offs between diffusing in embedding space vs. token space for language generation?

## Architecture Onboarding

- **Component map:**
  - Teacher DLM (φ) -> Student Generator (θ) -> Score Estimator (ψ) -> Embedding Layer (E)
  - Condition c_fake + Noise z -> Generator -> Clean embeddings e -> Forward diffusion -> Noisy embeddings e_t
  - Real embeddings e_real + Condition c_real -> Adversarial discriminator

- **Critical path:**
  1. Sample condition c_fake from dataset; sample noise z ~ N(0,I)
  2. Generate student output: e_fake = G_θ(c_fake, z)
  3. Forward diffuse to random t: e_t = α_t·e_fake + σ_t·ε
  4. Compute teacher score s_φ(e_t, t, c_fake) and student score via ψ(e_t, t, c_fake)
  5. Update ψ: DSM loss + adversarial loss on real/fake pairs
  6. Update θ: SiD loss + adversarial generator loss
  7. Round e_fake to tokens via nearest-neighbor in embedding space for evaluation

- **Design tradeoffs:**
  - Adversarial coefficient: 0.5 for both in Stage 1, asymmetric (0.9/0.1) for Stage 2 on QQP
  - SiD coefficient μ: 1.0-1.2 typical; lower (0.5) in Stage 2 for QQP
  - Time range [0, 1976] used; t_init=1490 for sampling during distillation
  - Training budget: 50K steps per stage; validation every 200 steps

- **Failure signatures:**
  - Repetitive token loops: Adversarial loss too weak; increase b_g^adv
  - Empty/PAD-only outputs: Student collapsed; reduce learning rate
  - BLEU degradation in Stage 2: ψ not reinitialized or learning rate too high
  - Diversity collapse: Self-BLEU spikes; documented trade-off

- **First 3 experiments:**
  1. Baseline distillation without adversarial loss: Train student using only SiD loss on QQP for 50K steps; document degeneration patterns
  2. Single-stage vs. two-stage comparison: Run Stage 1 only vs. full two-stage training on QQP; report BLEU, ROUGE-L, and diversity metrics
  3. Inference-time scaling: Apply 1, 2, 4 iterative re-noising/denoising steps at inference; measure BLEU trajectory

## Open Questions the Paper Calls Out

### Open Question 1
Can embedding learning be jointly optimized during diffusion distillation, rather than freezing the pretrained embedding matrix, to improve student model quality?
Basis: Page 5 states integration of embedding learning and diffusion distillation remains a promising direction for future work.
Why unresolved: The current approach freezes embeddings during distillation, but jointly optimizing could better align embeddings with the one-step generation objective.
Evidence needed: Experiments comparing frozen vs. jointly-optimized embeddings across multiple DLM architectures and tasks.

### Open Question 2
Can DLM-One be extended to discrete diffusion models, vocabulary logit space models, and latent space language models?
Basis: Page 4 mentions extending DLM-One to such models represents a promising direction.
Why unresolved: Score distillation in continuous embedding spaces may not directly transfer to discrete token spaces or latent representations.
Evidence needed: Successful application to models like LLaDA (discrete DLM) or TEncDM (latent space).

### Open Question 3
Can a distilled generator trained explicitly for multi-step generation outperform the inference-time multi-step scheme?
Basis: Page 12 states results in Table 4 should not be interpreted as upper bound.
Why unresolved: Current DLM-One is optimized for one-step generation; Table 4 shows ad-hoc multi-step inference improves quality but is not trained.
Evidence needed: Training a student model with 2-4 step objective and comparing against inference-time multi-step.

### Open Question 4
How can the diversity-quality trade-off in DLM-One be mitigated, given that two-stage training improves quality metrics but reduces diversity?
Basis: Page 12 notes reduction in generation diversity as common trade-off in fast sampling methods.
Why unresolved: The adversarial and distillation objectives optimize for fidelity, implicitly reducing output variation.
Evidence needed: Modified training objectives that maintain diversity while preserving quality gains.

## Limitations
- Two-stage training requires careful hyperparameter tuning and validation checkpoint selection
- Diversity-quality trade-off worsens in Stage 2, with Self-BLEU increasing by 19.4%
- Adversarial regularization lacks direct empirical validation for DLMs in literature
- Method requires frozen pretrained teacher model and operates in continuous embedding space

## Confidence

High confidence in core technical contribution: Score distillation framework is mathematically sound with convincing ablation studies.

Medium confidence in adversarial regularization claims: Method is clearly described but effectiveness for DLMs lacks independent validation.

Low confidence in scalability and robustness: Experiments limited to three Seq2Seq tasks with small datasets; performance on long sequences or distribution shift unknown.

## Next Checks

1. **Teacher-Free Validation**: Implement DiffRatio's teacher-free one-step training method and compare directly against DLM-One on the same three tasks to isolate adversarial regularization's contribution.

2. **Inference-Time Iterative Refinement**: Take a trained DLM-One model and systematically evaluate quality at 1, 2, 4, and 8 iterative refinement steps during inference to verify quality-diversity trade-off is tunable.

3. **Cross-Dataset Generalization**: Train DLM-One on one dataset (Wiki-Auto), then evaluate zero-shot on another (Quasar-T) without fine-tuning to test generalization beyond training distributions.