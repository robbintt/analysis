---
ver: rpa2
title: 'CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description
  and Polysemanticity Quantification'
arxiv_id: '2503.15234'
source_url: https://arxiv.org/abs/2503.15234
tags:
- concept
- image
- relevance
- explanation
- channel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Chain-of-Explanation (CoE), a method for automatically
  constructing interpretable concept-based explanations for deep vision models. CoE
  addresses the challenges of manually creating concept explanation datasets and managing
  polysemanticity in visual concepts.
---

# CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification

## Quick Facts
- **arXiv ID:** 2503.15234
- **Source URL:** https://arxiv.org/abs/2503.15234
- **Reference count:** 40
- **Key outcome:** CoE achieves 36% average improvement in explainability scores vs. baselines through automated concept description and polysemanticity filtering.

## Executive Summary
CoE addresses the challenge of generating interpretable explanations for deep vision models by automating the construction of concept-based explanations. It introduces an Automatic Concept Decoding and Description (ACD) method to generate global concept descriptions via Large Vision Language Models, a Concept Polysemanticity Disentanglement and Filtering (CPDF) mechanism to handle polysemantic concepts by identifying contextually relevant atoms, and a Concept Polysemanticity Entropy (CPE) score to quantify concept uncertainty. The method traces concept circuits using a Chain-of-Thought approach to generate linguistic local explanations.

## Method Summary
CoE automatically constructs interpretable concept-based explanations for deep vision models by first creating a global concept dataset using an LVLM to describe visual concept commonalities across 13 semantic directions. It then quantifies polysemanticity using entropy-based metrics and filters contextually relevant concept atoms for specific input images. Finally, it generates natural language explanations by tracing the concept circuit and feeding it to an LLM with a Chain-of-Thought prompt.

## Key Results
- CoE achieves an average absolute improvement of 36% in explainability scores compared to baseline methods
- CPE effectively quantifies concept uncertainty, with intermediate model layers showing higher polysemanticity
- Context-aware filtering of concept atoms resolves polysemanticity to ensure explanations match specific input images
- Both GPT-4o and human judges validate the improved explainability of CoE-generated explanations

## Why This Works (Mechanism)

### Mechanism 1: Automated Concept Description
- Automating concept description via LVLMs allows for scalable, fine-grained global concept datasets that outperform manual annotation
- ACD extracts image patches activating specific channels and uses an LVLM to disentangle them into concept atoms across 13 semantic directions
- Core assumption: The LVLM possesses sufficient world knowledge to identify commonalities across image patches
- Evidence: Section 3.1 describes the mapping using an LVLM as describer function, Section 3.2.1 details the 13 semantic directions
- Break condition: If the LVLM hallucinates features or fails to adhere to directions, the global dataset will contain noisy concept atoms

### Mechanism 2: Context-Aware Polysemanticity Filtering
- Context-aware filtering resolves polysemanticity to ensure local explanations match the specific input image
- CPDF clusters disentangled atoms to remove redundancy, then uses image caption to filter atoms contextually
- Core assumption: Image captioning provides accurate textual summary to guide atom filtering
- Evidence: Abstract states CPDF identifies contextually relevant atoms, Section 3.2.1 defines filtering function dependent on image context
- Break condition: If image is ambiguous or captioning fails, filter may select orthogonal concept atoms leading to misleading explanations

### Mechanism 3: Entropy-Based Polysemanticity Quantification
- Entropy-based quantification serves as reliable proxy for model interpretability and concept uncertainty
- CPE calculates normalized entropy of probability distribution of disentangled concept atoms
- Core assumption: Inverse relationship between distinct high-probability atoms and human interpretability
- Evidence: Section 3.2.2 defines CPE formulation, Section 4.3 shows CPE varying by layer depth
- Break condition: If channel activates uniformly for single concept, naive entropy calculation might falsely indicate high polysemanticity

## Foundational Learning

- **Polysemanticity in Vision Models**
  - Why needed: Central problem solved - one neuron â‰  one concept in vision models
  - Quick check: Does high activation in Channel 506 guarantee "dog" or could it mean "fur" or "brown texture"?

- **Concept Relevance Propagation (CRP)**
  - Why needed: CoE builds its chain using relevance values, not raw activations
  - Quick check: How does using relevance values improve fidelity of explanation chain?

- **Semantic Entailment & Clustering**
  - Why needed: CPDF relies on NLI to clean up LVLM outputs
  - Quick check: Why is bidirectional entailment check necessary before calculating concept atom probability distribution?

## Architecture Onboarding

- **Component map:** Pre-trained DVM -> CRP -> Extract Patches -> LVLM (GPT-4o) -> Raw Atoms -> NLI Model (DeBERTa) -> Clustered Atoms -> Global Concept Dataset -> Captioner -> Filter (LLM) -> Relevant Atoms -> Synthesizer (LLM) -> Natural Language Explanation

- **Critical path:** ACD Dataset Construction is primary offline cost; Filtering step is critical online operation where context must align with atom set

- **Design tradeoffs:** Latency vs. Quality (GPT-4o yields high quality but high cost/latency), Granularity (13 directions constrain but manage complexity), Manual vs. Automated (trade precision for scalability)

- **Failure signatures:** High CPE on clear concepts (check padding logic), Incoherent Explanations (filtering mechanism failing), Generic Explanations (LVLM over-smoothing atoms)

- **First 3 experiments:**
  1. Validate CPE Logic: Calculate CPE for monosemantic vs. polysemantic channels to verify metric distinguishes them
  2. Context Filtering Ablation: Run CoE pipeline with/without filtering to reproduce ~36% improvement gap
  3. Layer-wise Analysis: Measure CPE across model depths to confirm intermediate layers are more polysemantic

## Open Questions the Paper Calls Out

- **Can the fixed set of 13 prompted semantic directions effectively generalize to highly specialized visual domains?**
  - Basis: Section 3.2.1 states 13 directions cover "daily life" concepts; Section S5.2 suggests different performance on medical datasets
  - Why unresolved: Current taxonomy is manually defined for general purposes; unclear if sufficient for specialized fields without re-engineering
  - What evidence would resolve it: Evaluation on out-of-distribution, expert-level datasets without altering 13 base prompts

- **How robust is CoE to hallucinations or errors generated by the underlying LVLM during ACD phase?**
  - Basis: Section 3.2.1 relies entirely on LVLM to generate concept atom descriptions; Section 4.1 uses GPT-4o for ground truth
  - Why unresolved: Assumes LVLM's textual description is accurate; errors would propagate to concept circuit and final explanation
  - What evidence would resolve it: Ablation study measuring sensitivity to synthetic noise or known LVLM biases in ACD process

- **Can the method be adapted for real-time applications given current computational overhead?**
  - Basis: Section S3.4 explicitly quantifies operational cost: 20 seconds and $0.01 per image
  - Why unresolved: Latency makes approach suitable for offline analysis but prohibitive for interactive tasks
  - What evidence would resolve it: Demonstration using distilled language models maintaining fidelity while reducing latency below 1 second

## Limitations
- LVLM dependence for concept generation creates quality bounds based on model's visual understanding and potential hallucinations
- Context filtering effectiveness assumes image captions provide sufficient context, which may fail on complex scenes
- Entropy metric validity lacks empirical validation that low entropy correlates with better human interpretability

## Confidence

- **High confidence:** ACD pipeline architecture and CPE calculation methodology are clearly specified with explicit formulas and implementation details
- **Medium confidence:** 36% improvement claim lacks error bars or statistical significance testing; human evaluation sample sizes not fully specified
- **Low confidence:** Claims about addressing polysemanticity rely heavily on assumption that GPT-4o can consistently disentangle complex visual concepts into meaningful atoms

## Next Checks
1. Manually inspect 50 concept atoms generated by GPT-4o to assess hallucination rates and semantic consistency
2. Systematically evaluate explanation quality with and without filtering mechanism across diverse image types
3. Conduct controlled human study rating explanations from high-CPE vs. low-CPE channels to validate entropy-interpretability relationship