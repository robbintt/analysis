---
ver: rpa2
title: A mean teacher algorithm for unlearning of language models
arxiv_id: '2504.13388'
source_url: https://arxiv.org/abs/2504.13388
tags:
- unlearning
- loss
- which
- arxiv
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the mean teacher algorithm for language
  model unlearning, aiming to reduce memorization of specific text instances while
  maintaining model utility. The key insight is that mean teacher can approximate
  slow natural gradient descent, which inherently seeks low-curvature updates less
  likely to degrade model performance.
---

# A mean teacher algorithm for unlearning of language models

## Quick Facts
- arXiv ID: 2504.13388
- Source URL: https://arxiv.org/abs/2504.13388
- Reference count: 40
- Key outcome: Mean teacher with NLUL loss reduces memorization and improves privacy metrics, but effects are not permanent

## Executive Summary
This paper proposes a mean teacher algorithm for language model unlearning that aims to reduce memorization of specific text instances while maintaining model utility. The approach introduces a new "negative log-unlikelihood" (NLUL) loss designed to work effectively with mean teacher training. The key insight is that mean teacher can approximate slow natural gradient descent, leading to low-curvature updates that are less likely to degrade model performance. Experiments on MUSE benchmarks demonstrate that this combination successfully reduces both verbatim and knowledge memorization of forget data while maintaining utility on retain data, and also reduces privacy leakage.

## Method Summary
The method combines the mean teacher algorithm with a novel NLUL loss for language model unlearning. The mean teacher framework maintains an exponential moving average of model weights, creating a "teacher" model that guides the training of the "student" model. The NLUL loss is designed to address vanishing gradient issues common with existing unlearning losses, making it compatible with the mean teacher approach. The algorithm works by encouraging the student model to produce outputs that minimize the NLUL loss on forget data while maintaining performance on retain data, with the teacher model providing stable guidance through its moving average weights.

## Key Results
- Mean teacher with NLUL loss successfully reduces both verbatim and knowledge memorization of forget data
- The method maintains utility on retain data while improving privacy leakage metrics
- Further fine-tuning can recover knowledge memorization, suggesting incomplete erasure of sensitive information

## Why This Works (Mechanism)
The mean teacher algorithm works by maintaining an exponential moving average of model weights, creating a "teacher" model that provides stable guidance during training. This stability comes from the fact that the teacher weights change more slowly than the student weights, reducing the variance in the training signal. The NLUL loss is specifically designed to work with this framework by providing meaningful gradients even when the model has already largely unlearned the target information. Together, these components approximate slow natural gradient descent, which inherently seeks low-curvature updates that are less likely to degrade model performance while removing unwanted memorization.

## Foundational Learning
- **Natural Gradient Descent**: An optimization method that accounts for the geometry of the parameter space; needed to understand why mean teacher approximates low-curvature updates; quick check: verify that weight updates have small Frobenius norm
- **Exponential Moving Average**: A technique for maintaining running averages; needed to understand how the teacher model provides stable guidance; quick check: confirm EMA coefficient is between 0 and 1
- **Unlearning Losses**: Specialized loss functions for removing information; needed to understand why NLUL was developed; quick check: verify gradients are non-zero for already-unlearned data
- **Privacy Leakage Metrics**: Quantitative measures of information exposure; needed to evaluate unlearning effectiveness; quick check: confirm leakage decreases as unlearning progresses
- **Knowledge vs Verbatim Memorization**: Different types of memorization in language models; needed to understand what aspects of unlearning are being measured; quick check: test both exact string matching and semantic similarity
- **Model Utility Trade-offs**: The balance between forgetting and performance; needed to evaluate the practical value of unlearning; quick check: measure performance drop on retain data

## Architecture Onboarding

**Component Map**
Student Model -> NLUL Loss -> Teacher Model (EMA) -> Student Model (Guidance)

**Critical Path**
1. Forward pass through student model with forget data
2. Compute NLUL loss and gradients
3. Update student weights via optimizer
4. Update teacher weights via exponential moving average
5. Use teacher outputs as stable training signal

**Design Tradeoffs**
- EMA decay rate: slower decay provides more stability but slower adaptation
- NLUL loss weight: higher weight increases unlearning effectiveness but may harm utility
- Batch size: larger batches provide more stable gradients but require more memory

**Failure Signatures**
- Vanishing gradients on already-unlearned data
- Catastrophic forgetting of retain data
- Slow convergence due to teacher-student mismatch

**First Experiments**
1. Verify that teacher weights change more slowly than student weights over training
2. Test NLUL loss gradients on data that should already be unlearned
3. Measure privacy leakage reduction on a small forget dataset

## Open Questions the Paper Calls Out
None

## Limitations
- Unlearning effects are not permanent; further fine-tuning can recover knowledge memorization
- The approach may not completely erase sensitive information from model weights
- Experiments focus on specific benchmarks, limiting generalizability to real-world scenarios

## Confidence
- **High Confidence**: Mathematical formulation of NLUL loss and its integration with mean teacher framework
- **Medium Confidence**: Empirical results showing improvements over baselines
- **Medium Confidence**: Interpretation that mean teacher approximates slow natural gradient descent

## Next Checks
1. Conduct longitudinal studies to assess persistence of unlearning effects over time and under various fine-tuning scenarios
2. Test the approach on more diverse datasets and real-world unlearning scenarios with complex data distributions
3. Evaluate method's robustness against advanced privacy attacks and measure trade-off between unlearning effectiveness and model utility across different task types