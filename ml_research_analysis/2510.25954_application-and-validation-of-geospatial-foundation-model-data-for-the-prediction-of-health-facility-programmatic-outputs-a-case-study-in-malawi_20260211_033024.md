---
ver: rpa2
title: Application and Validation of Geospatial Foundation Model Data for the Prediction
  of Health Facility Programmatic Outputs -- A Case Study in Malawi
arxiv_id: '2510.25954'
source_url: https://arxiv.org/abs/2510.25954
tags:
- data
- health
- embeddings
- prediction
- population
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated the predictive performance of geospatial\
  \ foundation model (GeoFM) embeddings for health outcomes in Malawi. Researchers\
  \ compared three GeoFM sources\u2014Google Population Dynamics Foundation Model\
  \ (PDFM), Google AlphaEarth, and mobile phone call detail records (CDR)\u2014to\
  \ traditional geospatial interpolation methods for predicting 15 health indicators\
  \ across 552 catchment areas."
---

# Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi

## Quick Facts
- arXiv ID: 2510.25954
- Source URL: https://arxiv.org/abs/2510.25954
- Reference count: 40
- GeoFM embeddings modestly improved health indicator predictions in Malawi, with multi-source integration offering strongest benefits

## Executive Summary
This study evaluated the predictive performance of geospatial foundation model (GeoFM) embeddings for health outcomes in Malawi. Researchers compared three GeoFM sources—Google Population Dynamics Foundation Model (PDFM), Google AlphaEarth, and mobile phone call detail records (CDR)—to traditional geospatial interpolation methods for predicting 15 health indicators across 552 catchment areas. Using XGBoost models with 5-fold cross-validation, the embedding-based approaches improved upon baseline methods in 87% of indicators tested. The Multi-GeoFM model integrating all three embedding sources produced the most robust predictions, achieving average cross-validated R² values of 0.63 for population density, 0.57 for new HIV cases, and 0.47 for child vaccinations.

## Method Summary
The study used three GeoFM embedding sources (PDFM, AlphaEarth, CDR) to predict 15 health indicators across 552 Malawian catchment areas. Researchers trained XGBoost models with grid search hyperparameter tuning, comparing results to IDW and kriging baselines. Health outcome data came from DHIS2 and LIMS (Jan 2021-May 2023), with data quality filters excluding facilities with high missingness or excessive zero values. Performance was evaluated using R² on both cross-validated training sets and held-out test sets.

## Key Results
- Multi-GeoFM model achieved strongest predictions for population density (R²=0.63), HIV test positivity (R²=0.38), and child vaccinations (R²=0.47)
- Test set R² values were 0.64 for population density, 0.68 for HIV test positivity, and 0.55 for child vaccinations
- Prediction performance was poor for indicators with low primary data availability, such as TB and malnutrition cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating multiple GeoFM embedding sources produces more robust predictions than single-source approaches
- Mechanism: Different data modalities capture complementary signals—satellite imagery reflects physical infrastructure and environment, mobile CDR captures movement and connectivity patterns, and search/maps data reflects behavioral and economic activity. When combined via XGBoost, these orthogonal representations may compensate for individual source weaknesses.
- Core assumption: The information encoded in each embedding type is partially non-redundant and jointly predictive of health outcomes.
- Evidence anchors:
  - [abstract] "A Multi-GeoFM model integrating all three embedding sources produced the most robust predictions"
  - [results] "Multi-GeoFM model achieved the strongest predictions for... population density (R²=0.63), HIV test positivity (R²=0.38)"
  - [corpus] Related work (Earth AI, ref 27) demonstrates multi-modal geospatial reasoning, but specific health prediction validation in LMICs remains limited
- Break condition: If embedding sources are highly correlated or if one source dominates, marginal benefit of integration diminishes.

### Mechanism 2
- Claim: Satellite-derived embeddings capture environmental features predictive of vector-borne disease patterns
- Mechanism: AlphaEarth embeddings encode land use, water bodies, and vegetation from Sentinel/Landsat imagery. These features correlate with mosquito habitat suitability, which in turn influences malaria transmission rates.
- Core assumption: Environmental determinants of disease vectors are reflected in satellite imagery and persist across the embedding dimensionality reduction.
- Evidence anchors:
  - [results] "AlphaEarth satellite embeddings outperformed... other single embedding methods for population density and malaria case rate"
  - [discussion] "For malaria—which is a vector borne disease—the satellite AlphaEarth embeddings are likely finding signals in important seasonal and environmental features including standing water, rivers and lakes, and greenery"
  - [corpus] Limited direct corpus validation for this specific mechanism in LMIC health contexts
- Break condition: If cloud cover, seasonal timing, or image resolution obscure relevant environmental features, signal degrades.

### Mechanism 3
- Claim: Embedding-based prediction performance depends on minimum thresholds of ground-truth data availability
- Mechanism: XGBoost learns mappings from embeddings to targets; sparse or unreliable training labels prevent the model from identifying meaningful patterns, regardless of embedding quality.
- Core assumption: Missing DHIS2 data reflects reporting gaps rather than true zero counts, and excluded high-missingness facilities differ systematically.
- Evidence anchors:
  - [results] "TB and malnutrition indicators demonstrated weak predictive performance... These outcomes also had the smallest sample sizes contributing usable data after quality control"
  - [methods] "Facilities with ≥75% of values reported as zero were excluded... these zeros primarily reflect data reporting gaps rather than true absence of events"
  - [corpus] No direct corpus validation; this is a domain-specific constraint
- Break condition: If ground-truth data falls below a critical mass (n < ~200 catchments with reliable labels), embeddings cannot recover signal.

## Foundational Learning

- **Embeddings as Compressed Representations**
  - Why needed here: The paper uses 16-64 dimensional vectors to stand in for raw satellite imagery, mobile logs, and search data—understanding that embeddings are learned compressions is essential.
  - Quick check question: Can you explain why a 64-dimensional AlphaEarth embedding might lose information relevant to a specific health indicator?

- **Spatial Autocorrelation and Interpolation Baselines**
  - Why needed here: The paper compares against IDW and Kriging; these assume nearby locations have similar values. Understanding why these fail for complex health outcomes clarifies the embedding advantage.
  - Quick check question: Why would ordinary kriging perform poorly for HIV test positivity compared to population density?

- **Cross-Validation with Spatial Data**
  - Why needed here: Standard random k-fold CV can leak information when data are spatially correlated; the paper uses 5-fold CV but doesn't explicitly spatially block, which may inflate estimates.
  - Quick check question: What risks arise when cross-validation folds ignore spatial clustering of health facilities?

## Architecture Onboarding

- **Component map:**
  1. Raw data sources: DHIS2/LIMS (health outcomes), satellite imagery (AlphaEarth), mobile CDR, Google Search/Maps (PDFM)
  2. Embedding generation: Pre-trained GeoFMs produce fixed-dimension vectors per catchment (16-64 dims each)
  3. Feature aggregation: Catchment-level joins; weighted mean of facility data; log-transform count targets
  4. Model training: XGBoost with grid search over learning rate, depth, boosting rounds; 5-fold CV on 80% training split
  5. Evaluation: R² on 20% held-out test set; comparison to IDW and Kriging baselines

- **Critical path:**
  1. Obtain catchment shapefiles and assign facilities via spatial join
  2. Generate or retrieve embeddings for each catchment (requires GeoFM access)
  3. Clean health outcome data (exclude high-missingness facilities, handle zero-inflation)
  4. Train single-source models, then Multi-GeoFM; tune hyperparameters per target
  5. Evaluate on held-out test set; report both CV and test R²

- **Design tradeoffs:**
  - **Embedding source vs. compute**: PDFM requires Google API access; AlphaEarth needs satellite preprocessing; CDR requires telco partnership
  - **Data exclusion threshold**: Stricter missingness filters improve label quality but reduce sample size (TB fell to n=327)
  - **Single vs. multi-source model**: Multi-GeoFM increases feature dimensionality (90 dims total) but improves robustness; risk of overfitting with limited catchments

- **Failure signatures:**
  - R² near zero across all models → likely insufficient ground-truth data or label noise (seen in TB, malnutrition)
  - Large CV-test gap → potential overfitting or data leakage
  - Kriging outperforms embeddings → target may be purely spatial with no behavioral/environmental signal (seen in unsuppressed viral load)

- **First 3 experiments:**
  1. **Reproduce baseline comparison**: Train XGBoost on single-source embeddings (PDFM, AlphaEarth, CDR) for population density; verify R² ~0.44-0.63 CV range
  2. **Ablate embedding sources**: Train Multi-GeoFM with each source removed; quantify marginal contribution per indicator
  3. **Spatial CV validation**: Implement spatially-blocked cross-validation (e.g., leave-one-district-out) to assess whether reported R² values hold under stricter evaluation

## Open Questions the Paper Calls Out

- **Generalizability to other LMICs**: The analysis was restricted to Malawi and findings may not be directly generalizable to other LMICs with different health system structures, epidemiologic profiles, or data reporting practices.
- **Additional data sources**: Future research should explore optimal methods for combining embeddings and examine whether additional sources, such as climate or socioeconomic data, further enhance predictive power.
- **Role of temporality**: Priorities for future research include examining the role of temporality and other prediction tasks such as forecasting and outbreak detection.

## Limitations
- Proprietary GeoFM embeddings prevent independent verification and broader replication
- High exclusion rate of health facilities due to data quality filters may introduce selection bias
- No spatially blocked cross-validation may inflate performance estimates

## Confidence
- **High Confidence**: Predictive improvement for population density, HIV test positivity, and child vaccinations; Multi-GeoFM superiority
- **Medium Confidence**: Multi-source integration benefits; satellite environmental signals for malaria; ground-truth data threshold dependency
- **Low Confidence**: Generalizability to other LMICs; exact contribution of each embedding source; impact of unmeasured confounders

## Next Checks
1. **Reproduce baseline comparison**: Train XGBoost on single-source embeddings (PDFM, AlphaEarth, CDR) for population density; verify R² ~0.44-0.63 CV range using publicly available embedding alternatives (e.g., SatCLIP, Presto)

2. **Implement spatially-blocked cross-validation**: Replace random 5-fold CV with leave-one-district-out or spatially contiguous folds to assess whether reported R² values hold under stricter evaluation that accounts for spatial autocorrelation

3. **Ablate embedding sources in Multi-GeoFM**: Train models with each source (PDFM, AlphaEarth, CDR) removed; quantify marginal contribution per indicator to validate the integration mechanism and identify dominant predictors