---
ver: rpa2
title: 'Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?'
arxiv_id: '2512.20796'
source_url: https://arxiv.org/abs/2512.20796
tags:
- bias
- demographic
- features
- gender
- ablation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-task evaluation pipeline to assess
  how demographic bias mechanisms in language models differ from general demographic
  recognition. Using bidirectional prompts linking demographics to names, professions,
  and education levels, the authors systematically compare attribution-based and correlation-based
  methods for identifying bias features.
---

# Measuring Mechanistic Independence: Can Bias Be Removed Without Erasing Demographics?
## Quick Facts
- arXiv ID: 2512.20796
- Source URL: https://arxiv.org/abs/2512.20796
- Reference count: 36
- Primary result: Attribution-based ablations reduce race/gender profession stereotypes while preserving name recognition accuracy in Gemma-2-9B

## Executive Summary
This paper introduces a multi-task evaluation pipeline to assess how demographic bias mechanisms in language models differ from general demographic recognition. Using bidirectional prompts linking demographics to names, professions, and education levels, the authors systematically compare attribution-based and correlation-based methods for identifying bias features. The study reveals that bias arises from task-specific mechanisms rather than absolute demographic markers, with contextual sophistication proxies driving stereotyping. Attribution methods excel for race and gender bias, while correlation methods avoid "prior collapse" for education tasks.

## Method Summary
The authors developed a multi-task evaluation pipeline that assesses demographic bias mechanisms through systematic ablation studies. The pipeline uses bidirectional prompts linking demographics (race, gender, education) to various task domains (names, professions, education levels). They compare two identification methods: attribution-based (using integrated gradients to identify bias-relevant features) and correlation-based (using demographic-predictive token correlations). Both methods employ a greedy feature-ablation approach where identified features are removed iteratively, measuring changes in demographic stereotypes using KL divergence between baseline and ablated model outputs.

## Key Results
- Attribution-based ablations reduced race profession stereotypes by 34.2% KL reduction while maintaining >90% name recognition accuracy
- Correlation-based ablations were more effective for education bias (30.7% KL reduction) and avoided "prior collapse" issues
- Cross-task validation showed that removing features for one demographic-task combination doesn't affect other demographic-task pairs, confirming mechanistic independence

## Why This Works (Mechanism)
The paper demonstrates that demographic bias in language models operates through distinct, task-specific mechanisms rather than being driven by absolute demographic markers. Attribution-based methods work well for race and gender biases because these often manifest through explicit stereotypical associations, while correlation-based methods are more effective for education bias due to its more subtle, context-dependent nature. The "prior collapse" phenomenon observed with correlation methods for education tasks suggests that these biases are encoded through nuanced contextual patterns rather than direct demographic markers.

## Foundational Learning
- **KL Divergence**: Measures distributional differences between baseline and ablated model outputs; needed to quantify bias reduction effectiveness; quick check: ensures changes in output distributions are statistically meaningful
- **Integrated Gradients**: Attribution method for identifying feature importance; needed to pinpoint bias-relevant tokens; quick check: validates that gradient-based feature importance correlates with actual bias impact
- **Bidirectional Prompting**: Uses demographics-to-task and task-to-demographics prompts; needed to capture complete bias manifestation patterns; quick check: ensures both direct and reverse associations are evaluated
- **Greedy Ablation**: Iterative feature removal approach; needed to systematically identify minimal feature sets causing bias; quick check: confirms that removing features reduces bias without unnecessary capability loss

## Architecture Onboarding
**Component Map**: Prompts -> Evaluation -> Feature Identification -> Ablation -> KL Divergence Analysis
**Critical Path**: Prompt generation → Bias measurement → Feature identification → Feature ablation → Bias re-measurement
**Design Tradeoffs**: Attribution methods provide interpretable feature importance but may over-remove relevant information; correlation methods are more robust but can cause prior collapse
**Failure Signatures**: Prior collapse occurs when correlation-based methods remove too many features for education tasks; attribution methods may fail to capture subtle contextual biases
**First Experiments**: (1) Run bidirectional prompts on baseline model to establish initial bias metrics; (2) Apply attribution-based ablation to race-related features and measure impact; (3) Apply correlation-based ablation to education features and compare effectiveness

## Open Questions the Paper Calls Out
None

## Limitations
- Results are specific to Gemma-2-9B and may not generalize to other model architectures
- Only three demographic attributes and three task domains were tested, limiting scope
- Attribution methods reduce name recognition accuracy (though remaining >90%), indicating a capability trade-off

## Confidence
- High confidence that bias mechanisms differ across demographics and tasks
- Medium confidence in the superiority of attribution methods for race/gender and correlation methods for education
- Low confidence in the contextual sophistication proxy interpretation as a theoretical framework

## Next Checks
(1) Test the pipeline on additional model families (e.g., Llama, Mistral) to assess architecture dependence
(2) Expand demographic attributes to include age, nationality, and disability status to evaluate broader applicability
(3) Conduct human evaluation studies to verify that debiasing preserves intended model capabilities while reducing harmful stereotypes