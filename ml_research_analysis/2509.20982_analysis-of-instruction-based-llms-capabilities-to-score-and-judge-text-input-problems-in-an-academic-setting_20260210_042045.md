---
ver: rpa2
title: Analysis of instruction-based LLMs' capabilities to score and judge text-input
  problems in an academic setting
arxiv_id: '2509.20982'
source_url: https://arxiv.org/abs/2509.20982
tags:
- evaluation
- answer
- reference
- student
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the use of large language models (LLMs) to\
  \ automatically assess student answers to text-input problems in higher education.\
  \ Five evaluation systems were tested using three models\u2014JudgeLM, Llama-3.1-8B,\
  \ and DeepSeek-R1-Distill-Llama-8B\u2014on a dataset of 110 computer science answers."
---

# Analysis of instruction-based LLMs' capabilities to score and judge text-input problems in an academic setting

## Quick Facts
- arXiv ID: 2509.20982
- Source URL: https://arxiv.org/abs/2509.20982
- Reference count: 35
- This study evaluates LLM-based systems for automatic grading of student text-input answers in higher education.

## Executive Summary
This study evaluates the use of large language models (LLMs) to automatically assess student answers to text-input problems in higher education. Five evaluation systems were tested using three models—JudgeLM, Llama-3.1-8B, and DeepSeek-R1-Distill-Llama-8B—on a dataset of 110 computer science answers. The methods included JudgeLM-based scoring, Reference Aided Evaluation (with a reference answer), No Reference Evaluation, Additive Evaluation (using atomic criteria), and Adaptive Evaluation (with dynamically generated criteria). Reference Aided Evaluation achieved the best alignment with human grading, showing the lowest median absolute deviation (0.945) and root mean square deviation (1.214), offering fair scoring and insightful feedback.

## Method Summary
The study employed five distinct LLM-based evaluation systems on 110 computer science answers. JudgeLM-based scoring used the model's native capabilities for direct assessment. Reference Aided Evaluation compared student responses against a reference answer while providing feedback. No Reference Evaluation assessed answers without external references. Additive Evaluation broke down scoring into atomic criteria for detailed assessment. Adaptive Evaluation generated evaluation criteria dynamically based on the question context. The study compared these methods using statistical metrics including median absolute deviation and root mean square deviation against human-graded benchmarks.

## Key Results
- Reference Aided Evaluation achieved the best alignment with human grading (MAD = 0.945, RMSD = 1.214)
- JudgeLM-based scoring showed moderate performance with good feedback quality
- Additive and Adaptive Evaluation methods underperformed due to rigid criteria and contextual limitations
- No Reference Evaluation demonstrated limited effectiveness without external benchmarks

## Why This Works (Mechanism)
The Reference Aided Evaluation method works effectively because it combines the LLM's natural language understanding capabilities with structured comparison against a gold standard reference answer. This approach leverages the model's ability to parse semantic content while providing concrete benchmarks for evaluation, resulting in more consistent and fair assessments compared to methods that rely solely on the model's internal judgment or overly rigid criteria.

## Foundational Learning

**Natural Language Understanding**: LLMs can parse and interpret complex student responses
- Why needed: Essential for comprehending varied answer formats and student writing styles
- Quick check: Can the model correctly identify key concepts in diverse student responses?

**Semantic Comparison**: Ability to compare student answers against reference standards
- Why needed: Critical for objective assessment and identifying gaps in student responses
- Quick check: Does the model accurately identify differences between reference and student answers?

**Context-Aware Scoring**: Understanding question requirements and grading rubrics
- Why needed: Ensures evaluations align with educational objectives and assessment criteria
- Quick check: Can the model adapt scoring based on different question types and difficulty levels?

**Feedback Generation**: Providing actionable, specific feedback to students
- Why needed: Enhances educational value beyond simple scoring
- Quick check: Does feedback address specific areas for improvement in student responses?

## Architecture Onboarding

**Component Map**: Input (Student Answer) -> Reference Answer (Optional) -> LLM Model -> Scoring Engine -> Output (Score + Feedback)

**Critical Path**: Student answer → LLM processing → Reference comparison (if applicable) → Score calculation → Feedback generation → Final assessment

**Design Tradeoffs**: 
- Reference availability vs. flexibility: Reference Aided Evaluation requires preparation overhead but provides better accuracy
- Granularity vs. efficiency: Additive Evaluation offers detailed feedback but requires more processing time
- Adaptability vs. consistency: Adaptive Evaluation can handle diverse questions but may introduce scoring variability

**Failure Signatures**: 
- Over-reliance on exact phrase matching rather than conceptual understanding
- Inconsistent scoring across similar answers due to model interpretation variations
- Generic feedback that doesn't address specific student response characteristics

**Three First Experiments**:
1. Compare scoring consistency across multiple runs of the same evaluation method
2. Test model performance on different question types (conceptual vs. computational)
3. Evaluate feedback quality by having educators rate the actionability of LLM-generated comments

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Small sample size (110 answers) from a single discipline limits generalizability
- Tested only three LLM models, leaving uncertainty about performance with other models
- Reference Aided Evaluation requires reference answers, creating dependency on resource-intensive preparation

## Confidence
- **High confidence**: The relative ranking of evaluation methods (Reference Aided > JudgeLM-based > No Reference > Additive > Adaptive) is likely robust based on consistent performance metrics
- **Medium confidence**: The specific numerical metrics (MAD = 0.945, RMSD = 1.214 for Reference Aided Evaluation) are reliable for this dataset but may vary with different subjects or larger sample sizes
- **Low confidence**: Generalizability claims about LLMs as "effective complementary tools" for broader academic assessment require validation beyond the tested computer science domain

## Next Checks
1. Replicate the study with a larger, more diverse dataset spanning multiple academic disciplines and question types to assess generalizability
2. Test additional state-of-the-art LLM models, including larger parameter models and those specifically fine-tuned for educational assessment
3. Conduct a longitudinal study comparing LLM-assisted grading with traditional human grading across multiple assessment cycles to evaluate consistency and identify potential systematic biases