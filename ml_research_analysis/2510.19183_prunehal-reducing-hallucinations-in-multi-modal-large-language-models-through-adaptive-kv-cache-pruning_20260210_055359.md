---
ver: rpa2
title: 'PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through
  Adaptive KV Cache Pruning'
arxiv_id: '2510.19183'
source_url: https://arxiv.org/abs/2510.19183
tags:
- visual
- attention
- tokens
- hallucinations
- prunehal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the connection between visual attention
  insufficiency and hallucinations in multimodal large language models (MLLMs). It
  proposes that redundant visual tokens disperse the model's attention, leaving critical
  visual cues under-attended, which leads to hallucinated outputs.
---

# PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning

## Quick Facts
- **arXiv ID**: 2510.19183
- **Source URL**: https://arxiv.org/abs/2510.19183
- **Reference count**: 30
- **Primary result**: Reduces visual hallucinations in MLLMs by up to 25% on CHAIR S benchmark through adaptive KV cache pruning

## Executive Summary
This paper addresses hallucinations in multimodal large language models (MLLMs) by investigating the relationship between visual attention insufficiency and hallucinated outputs. The authors propose that redundant visual tokens disperse attention, causing critical visual cues to be under-attended and resulting in hallucinations. To solve this, they introduce PruneHal, a training-free framework that adaptively prunes the KV cache to retain only the most attended visual tokens during inference. The method demonstrates consistent hallucination reduction across multiple mainstream MLLMs and benchmarks while introducing minimal computational overhead.

## Method Summary
PruneHal is a training-free framework that reduces hallucinations in MLLMs through adaptive KV cache pruning. The method works by dynamically analyzing historical visual attention distributions during inference and pruning redundant visual tokens that dilute attention on critical visual information. By retaining only the most attended visual tokens in the KV cache, the model maintains focus on relevant visual cues while eliminating distractions. The approach is model-agnostic and integrates seamlessly with existing decoding strategies, requiring no additional training or fine-tuning of the base MLLM.

## Key Results
- Reduces CHAIR S hallucination rates by up to 25% under greedy decoding on LLaVA-v1.5-7B
- Improves GPT-4V-assisted correctness scores from 6.04 to 6.98 on hallucination benchmarks
- Demonstrates consistent performance improvements across four mainstream MLLMs (LLaVA-v1.5-7B/13B, InstructBLIP-7B, Qwen-VL-7B) and multiple hallucination benchmarks

## Why This Works (Mechanism)
The core mechanism relies on the observation that redundant visual tokens in the KV cache dilute the model's visual attention, causing critical visual information to be under-attended. By pruning these redundant tokens based on historical attention distributions, PruneHal ensures that the model's attention is concentrated on the most relevant visual cues. This adaptive pruning approach dynamically balances preserving crucial visual information while eliminating distractions that could lead to hallucinated outputs.

## Foundational Learning
- **KV Cache Management**: Why needed - Efficient handling of key-value caches is crucial for transformer inference performance; Quick check - Understand how KV cache size affects memory usage and inference speed
- **Visual Attention Mechanisms**: Why needed - Understanding how visual tokens compete for attention in multimodal models; Quick check - Can you explain how attention scores are computed between visual and language tokens?
- **Multimodal Fusion**: Why needed - How visual and text information is integrated in MLLMs; Quick check - What are common approaches for fusing visual and language representations?
- **Hallucination Types in MLLMs**: Why needed - Different hallucination patterns require different mitigation strategies; Quick check - What distinguishes object-level from attribute-level hallucinations?
- **Inference Optimization**: Why needed - Understanding trade-offs between model performance and computational efficiency; Quick check - How do different decoding strategies affect hallucination rates?

## Architecture Onboarding

**Component Map**: Input Image -> Visual Encoder -> Attention Module -> PruneHal (KV Cache Pruning) -> Language Decoder -> Output

**Critical Path**: Visual tokens → Attention scores → Historical attention analysis → Pruning decision → Updated KV cache → Language generation

**Design Tradeoffs**: 
- Training-free approach ensures broad applicability but may miss deeper architectural optimizations
- Dynamic pruning based on historical attention provides adaptability but requires sufficient attention data
- Minimal computational overhead achieved through efficient cache management, though threshold determination requires empirical tuning

**Failure Signatures**:
- Over-pruning that removes critical visual tokens, leading to incomplete or vague responses
- Under-pruning that fails to eliminate sufficient redundant tokens, resulting in persistent hallucinations
- Poor threshold selection causing inconsistent performance across different visual domains

**3 First Experiments**:
1. Baseline hallucination rate measurement on CHAIR S benchmark with greedy decoding
2. Ablation study comparing PruneHal with random token pruning versus attention-guided pruning
3. Computational overhead analysis measuring inference time and memory usage with and without PruneHal

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Effectiveness under diverse decoding strategies (top-k, nucleus sampling) not thoroughly explored
- Adaptive pruning thresholds determined empirically rather than through principled optimization
- Assumes sufficient historical attention data is available, which may not hold for short sequences or novel visual concepts

## Confidence

**Major Claim Confidence**:
- **High confidence**: KV cache pruning reduces hallucination rates (empirically validated across multiple models and benchmarks)
- **Medium confidence**: The relationship between visual attention insufficiency and hallucinations (plausible mechanism, but not rigorously proven)
- **Medium confidence**: Dynamic pruning guided by historical attention distributions improves over static methods (demonstrably effective, but optimal thresholds remain unclear)

## Next Checks
1. Test PruneHal's effectiveness under diverse decoding strategies (temperature sampling, top-k, nucleus sampling) to assess robustness beyond greedy decoding
2. Evaluate performance on out-of-distribution visual concepts and fine-grained visual distinctions to test generalization limits
3. Conduct ablation studies isolating the contribution of historical attention guidance versus simple token count reduction to validate the proposed mechanism