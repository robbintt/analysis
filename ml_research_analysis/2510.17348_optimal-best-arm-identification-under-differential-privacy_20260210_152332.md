---
ver: rpa2
title: Optimal Best Arm Identification under Differential Privacy
arxiv_id: '2510.17348'
source_url: https://arxiv.org/abs/2510.17348
tags:
- lemma
- have
- proof
- where
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the problem of differentially private best\
  \ arm identification (BAI) in the fixed-confidence setting for Bernoulli distributions.\
  \ The authors close a significant gap in the literature by reducing the gap between\
  \ the best known lower and upper bounds on the expected sample complexity to a small\
  \ constant for any privacy budget \u03B5."
---

# Optimal Best Arm Identification under Differential Privacy

## Quick Facts
- **arXiv ID:** 2510.17348
- **Source URL:** https://arxiv.org/abs/2510.17348
- **Reference count:** 40
- **Primary result:** Closes the gap between lower and upper bounds on sample complexity for ε-global DP BAI to within a small constant factor.

## Executive Summary
This work addresses the fundamental problem of differentially private best arm identification (BAI) in the fixed-confidence setting for Bernoulli distributions. The authors close a significant gap in the literature by reducing the gap between the best known lower and upper bounds on the expected sample complexity to a small constant for any privacy budget ε. Their core method involves designing a private estimator using arm-dependent geometric batching without forgetting and a generalized likelihood ratio (GLR) stopping rule based on novel transportation costs that smoothly balance Kullback-Leibler divergence and total variation distance scaled by ε. They prove a tight lower bound on the expected sample complexity of any ε-global differentially private algorithm and propose a Top Two sampling rule (DP-TT) that matches this lower bound up to a constant factor smaller than 8.

## Method Summary
The paper proposes a complete algorithm for ε-global differentially private BAI that achieves near-optimal sample complexity. The method uses a Geometric Private Estimator (GPE) that accumulates observations across all history but only adds Laplace noise at the start of geometric phases, avoiding the "forgetting" approach of prior work. The stopping rule uses a Generalized Likelihood Ratio test with a threshold calibrated for the concentration of Bernoulli plus Laplace variables. The sampling rule is a Top Two method that selects between a leader and challenger based on transportation costs derived from a new information-theoretic quantity d_ε that optimally trades off between KL divergence and ε-scaled total variation distance.

## Key Results
- Proves a tight lower bound on expected sample complexity for ε-global DP BAI by replacing KL divergence with a new quantity d_ε that optimally balances KL and TV distance
- Introduces Geometric Private Estimator (GPE) using arm-dependent geometric batching without forgetting, accumulating observations while managing cumulative Laplace noise
- Proposes DP-TT algorithm with Top Two sampling rule that matches the lower bound up to a constant factor smaller than 8
- Demonstrates empirical performance improvements over existing ε-global DP BAI algorithms across different privacy budgets and problem instances

## Why This Works (Mechanism)

### Mechanism 1: The Interpolating Divergence d_ε
The paper defines a new information-theoretic quantity d_ε(ν, κ) := inf_φ {ε·TV(ν∥φ) + KL(φ∥κ)} that smoothly interpolates between KL divergence and Total Variation distance based on the privacy budget ε. This captures the "shortest path" cost to distinguish two distributions when noise (controlled by ε) is present, recovering KL for large ε and ε·TV for small ε.

### Mechanism 2: Geometric Batching Without Forgetting
The Geometric Private Estimator (GPE) accumulates observations across all history but only adds Laplace noise at the start of geometric phases. Unlike prior "forgetting" methods which discard old data to limit noise, this method keeps the data but accumulates noise logarithmically in the sample count, leveraging the fact that variance reduction from keeping all samples outweighs increasing cumulative noise.

### Mechanism 3: Modified GLR Stopping Threshold
The stopping rule uses a Generalized Likelihood Ratio test with a threshold calibrated specifically for the concentration of Bernoulli plus Laplace variables. The threshold function c(n, ε, δ) includes a term c₂(n, ε) derived from novel concentration bounds for this mixture distribution, avoiding the O(log(1/δ)²) scaling seen in prior work.

## Foundational Learning

**Concept: Fixed-Confidence Best Arm Identification (BAI)**
- Why needed: This is the core objective - distinguishing between "regret minimization" and "pure exploration" (BAI) where the goal is to output the correct arm with probability 1-δ while minimizing samples
- Quick check: Do you know the difference between an algorithm that aims for low "regret" vs. one that aims for low "sample complexity" τ_ε,δ?

**Concept: ε-Global Differential Privacy (DP)**
- Why needed: This is the constraint - understanding the Laplace Mechanism and sensitivity, and that the paper relies on "global DP" (central model with trusted curator), not local DP
- Quick check: Can you calculate the probability density of Lap(1/ε) and explain why adding it to a sum of bounded rewards preserves privacy?

**Concept: Generalized Likelihood Ratio (GLR) Tests**
- Why needed: The stopping rule is based on GLR - testing multiple hypotheses ("Is arm a better than arm b?") sequentially and stopping when evidence is overwhelming
- Quick check: In a standard GLR test, what happens to the test statistic as the number of samples grows if the alternative hypothesis is true?

## Architecture Onboarding

**Component map:**
Environment (Bernoulli rewards) -> GPE (Geometric Private Estimator) -> Recommendation Rule -> GLR Stopping Rule -> Sampling Rule (DP-TT)

**Critical path:**
The loop is: Sample Arm → Update GPE → Check Stopping Rule. If stopping fails: Calculate Transportation Costs → Update Leader/Challenger → Sample next Arm. The most sensitive components are the GPE noise addition and the Transportation Cost calculation (which depends on solving the d_ε minimization).

**Design tradeoffs:**
- **Geometric Grid Parameter (η):** Low η means frequent updates but more cumulative Laplace noise; high η means fewer updates but slower reaction to changing estimates
- **Privacy vs. Utility (ε):** Standard DP tradeoff - lower ε requires significantly higher sample complexity (roughly 1/ε scaling in high privacy regime)
- **Target Proportion (β):** In Top Two sampling rule, β splits allocation between leader and challenger (defaults to β=1/2 to guarantee within factor of 2 of optimal)

**Failure signatures:**
- **Infinite Loop:** Stopping threshold grows faster than transportation cost statistic, preventing stop condition from triggering
- **Privacy Leak:** Incorrect implementation of geometric grid (updating noise at wrong intervals or using "forgetting" incorrectly) violates privacy proof
- **Suboptimal Convergence:** If sampling rule gets stuck on wrong arm, check challenger logic (TCI) and ensure transportation cost calculations aren't numerically unstable

**First 3 experiments:**
1. **Baseline Validation:** Reproduce Figure 1 - run DP-TT against baselines (AdaP-TT, DP-SE) on μ₁ and μ₂ instances across range of ε (10⁻³ to 10²) with δ=10⁻², verify "High Privacy" vs "Low Privacy" regime transition
2. **Ablation on Forgetting:** Implement GPE with forgetting vs without forgetting, compare sample complexity on medium-difficulty instance to quantify signal gain
3. **Sensitivity to η:** Run DP-TT with varying η (e.g., 0.5, 1.0, 2.0), plot stopping time vs. η to verify trade-off between noise accumulation and update frequency

## Open Questions the Paper Calls Out

**Open Question 1:** Can the stopping threshold for the GLR stopping rule be improved to scale as log(1/δ) instead of 2log(1/δ)?
- Basis: Section 4 states "Obtaining a threshold in log(1/δ) is left for future work" to remove suboptimal factor of 2
- Why unresolved: Current threshold is sum of per-arm thresholds, inherently doubling asymptotic scaling
- Resolution: Theoretical proof demonstrating GLR stopping rule remains δ-correct with threshold scaling strictly as log(1/δ)

**Open Question 2:** Can the optimal sample complexity results be extended to distribution classes beyond Bernoulli, such as Gaussian or bounded distributions?
- Basis: Conclusion identifies "extend[ing] our results to other classes of distributions" as "most exciting direction for future work"
- Why unresolved: Proofs rely on concentration results and closed-form solutions specific to Bernoulli distributions
- Resolution: Deriving regularity properties (e.g., convexity, monotonicity) for transportation costs and divergences d_ε in context of Gaussian or bounded distributions

**Open Question 3:** Can the proposed techniques be adapted to other variants of differential privacy, such as (ε, δ)-DP or Rényi DP?
- Basis: Conclusion lists extending technique to "other variants of pure DP" or "other trust models (e.g., shuffle DP)" as interesting research direction
- Why unresolved: Current lower bound proof relies on group privacy property of ε-DP; adapting to relaxations requires different methods
- Resolution: Formal analysis establishing tight lower bounds and corresponding algorithms for fixed-confidence BAI under (ε, δ)-DP or Rényi DP constraints

## Limitations

- The theoretical framework extends to sub-Gaussian distributions, but all empirical results and closed-form solutions are specific to Bernoulli rewards
- Numerical implementation of Lambert W⁻¹ function and transportation cost optimization may introduce instability in practical implementations
- The constant factor of 8 in upper bound matching lower bound requires careful verification of all intermediate inequalities in proofs

## Confidence

**High Confidence:**
- Theoretical lower bound structure replacing KL with d_ε is mathematically sound and follows standard information-theoretic arguments
- Geometric batching mechanism without forgetting is correctly described and follows from privacy analysis
- Asymptotic scaling of sample complexity with ε (roughly 1/ε in high-privacy regime) is consistent with DP theory

**Medium Confidence:**
- Constant factor of 8 in upper bound matching lower bound is tight but requires careful verification of all intermediate inequalities
- Empirical performance improvements over baselines are reproducible given correct implementation of GPE and stopping threshold
- TCI challenger selection rule guarantees correct arm identification, though practical impact on sample complexity may vary with problem structure

**Low Confidence:**
- Specific numerical values in threshold approximation (Remark 2) are not fully specified in main text and require access to supplementary code
- Comparison to "new asymptotically optimal algorithms" mentioned in abstract is not detailed, making claimed optimality difficult to verify independently

## Next Checks

1. **Sensitivity Analysis on Geometric Parameter:** Implement DP-TT with varying η ∈ {0.5, 1.0, 2.0} and measure trade-off between cumulative Laplace noise and update frequency. Plot stopping time vs. η to verify theoretical prediction that noise accumulation dominates when η is too small.

2. **Robustness to Distribution Class:** Implement modified version of DP-TT for bounded [0,1] rewards (not necessarily Bernoulli) using general framework from Lemma 1. Compare performance to Bernoulli-specific implementation on instances where arms have similar means but different variances.

3. **Threshold Approximation Verification:** Implement both exact threshold (using Lambert W⁻¹) and empirical approximation mentioned in Remark 2. Measure difference in stopping times across different privacy budgets to quantify impact of approximation on algorithm performance.