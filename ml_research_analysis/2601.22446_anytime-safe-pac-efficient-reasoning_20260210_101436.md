---
ver: rpa2
title: Anytime Safe PAC Efficient Reasoning
arxiv_id: '2601.22446'
source_url: https://arxiv.org/abs/2601.22446
tags:
- reasoning
- b-pac
- efficient
- efficiency
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: B-PAC reasoning addresses the problem of safe and efficient online
  reasoning with Large Reasoning Models (LRMs) under partial feedback and non-stationary
  data. The core idea is to formulate threshold selection as a betting game, using
  inverse propensity scoring estimators and supermartingales to dynamically adjust
  routing thresholds between thinking and non-thinking models.
---

# Anytime Safe PAC Efficient Reasoning

## Quick Facts
- arXiv ID: 2601.22446
- Source URL: https://arxiv.org/abs/2601.22446
- Reference count: 40
- Primary result: B-PAC achieves up to 81.01% reduction in thinking model usage while maintaining safety guarantees on reasoning benchmarks.

## Executive Summary
B-PAC reasoning addresses the challenge of safe and efficient online reasoning with Large Reasoning Models (LRMs) under partial feedback and non-stationary data. The method formulates threshold selection as a betting game, using inverse propensity scoring estimators and supermartingales to dynamically adjust routing thresholds between thinking and non-thinking models. This enables anytime-valid performance loss control with logarithmic regret for the adaptive betting strategy, significantly reducing expensive thinking model calls while maintaining user-specified performance guarantees.

## Method Summary
B-PAC is an online framework that routes queries between a cheap non-thinking model and an expensive thinking model based on dynamically adjusted uncertainty thresholds. The system uses inverse propensity scoring to construct unbiased risk estimators from partial feedback, maintains a wealth process (supermartingale) for threshold selection, and employs a betting martingale to ensure anytime-valid safety guarantees. The method continuously updates thresholds using a fixed sequence testing procedure, balancing exploration (ensuring safety) with exploitation (maximizing efficiency) through a two-stage routing strategy.

## Key Results
- Achieves up to 81.01% reduction in thinking model usage while keeping performance loss below user-specified levels
- Outperforms both offline methods and online baselines across diverse reasoning benchmarks (MATH, MMLU-Pro, BBH, Magpie)
- Maintains anytime-valid safety guarantees with logarithmic regret in the adaptive betting strategy
- Demonstrates robustness to distribution shifts with dynamic threshold adjustment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** B-PAC achieves efficiency by routing low-uncertainty queries to a cheaper model while maintaining statistical safety via controlled randomization.
- **Mechanism:** The system calculates an uncertainty score $U_t$ for a query using the non-thinking model. If $U_t$ is below a dynamic threshold $\hat{u}_{t-1}$, the query is routed to the non-thinking model with probability $1-\rho_t$ (and to the thinking model with probability $\rho_t$). This minimum probability $\rho_t$ ensures exploration, providing the ground-truth labels required to adapt the threshold.
- **Core assumption:** The uncertainty score correlates with the likelihood of the non-thinking model disagreeing with the thinking model.
- **Break condition:** If the uncertainty score is adversarial (e.g., low uncertainty exactly where the error is high), the system defaults to conservative routing (low threshold), minimizing efficiency gains but maintaining safety.

### Mechanism 2
- **Claim:** Inverse Propensity Scoring (IPS) enables unbiased risk estimation from partial feedback (labels are only seen when the expensive model is called).
- **Mechanism:** When the thinking model is not invoked, the loss is unobserved. B-PAC constructs an estimator $Z_t(u)$ that weights the observed loss by the inverse of the probability $\pi_t$ that the observation was made. This statistically corrects the selection bias.
- **Core assumption:** The routing probability $\pi_t$ is strictly positive ($\rho_t > 0$) for all inputs to ensure the IPS estimator is well-defined.
- **Break condition:** If the exploration rate $\rho_t$ decays too fast or is zero, variance of the IPS estimator explodes, causing unstable threshold updates.

### Mechanism 3
- **Claim:** A "wealth" process (supermartingale) allows for valid statistical guarantees at any time step (anytime-valid), replacing rigid offline calibration.
- **Mechanism:** B-PAC treats threshold selection as a betting game. It maintains a capital $K_t(u)$ for candidate thresholds. If a threshold is unsafe (risk > $\epsilon$), the capital is designed to behave as a supermartingale (trend to zero). By Ville's inequality, if $K_t(u)$ exceeds $1/\alpha$, the "unsafe" null hypothesis is rejected. The system selects the largest threshold that has accumulated enough "wealth" (evidence of safety).
- **Core assumption:** The loss function is bounded in [0,1].
- **Break condition:** If the betting fraction $\lambda_t$ is not clipped correctly, the wealth process may turn negative, breaking the Ville's inequality application and voiding the safety guarantee.

## Foundational Learning

- **Concept:** **Inverse Propensity Scoring (IPS)**
  - **Why needed here:** You have a "partial feedback" problem. You only see the correct answer (and thus the error) when you use the expensive model. IPS allows you to estimate the average error of the *cheap* model using only the sparse observations from the expensive one.
  - **Quick check question:** If I only flip a coin 10% of the time to check for errors, and I find an error 50% of those times, what is the IPS-adjusted error rate estimate? (Answer: $0.50 \times 1.0 = 0.50$, essentially scaling the observed rate by the inverse of the observation probability).

- **Concept:** **Martingales and Ville's Inequality**
  - **Why needed here:** Standard statistical tests usually require a fixed sample size. "Anytime" validity means you want a guarantee that holds regardless of when you stop the process. Martingales are stochastic processes where the expected future value is the current value; Ville's inequality bounds the probability that such a process ever exceeds a certain level, enabling these flexible stopping rules.
  - **Quick check question:** Why is a "supermartingale" (trend down) useful for a safety guarantee? (Answer: If it trends down naturally, observing it shoot up provides strong evidence against the "null/safe" hypothesis, allowing us to reject the hypothesis that the system is unsafe).

- **Concept:** **Online Convex Optimization (FTRL)**
  - **Why needed here:** The system needs to pick the optimal "bet size" ($\lambda_t$) at each step to accumulate evidence fast (efficiency) without going bankrupt (safety). The paper frames this as an optimization problem to maximize log-wealth growth.
  - **Quick check question:** Why maximize log-wealth rather than raw wealth? (Answer: To balance the magnitude of bets and avoid "ruin" scenarios where a single bad bet eliminates all capital).

## Architecture Onboarding

- **Component map:** Input queries → Uncertainty Scorer → Router → Executor (Thinking/Non-thinking model) → Risk Estimator → Betting Engine → Threshold Selector → Updated threshold

- **Critical path:** The feedback loop between the Risk Estimator and the Betting Engine. Latency here is negligible compared to the LRM call, but correctness is vital. The threshold update $\hat{u}_t$ must be atomic to prevent race conditions in the Router.

- **Design tradeoffs:**
  - **Warm-up vs. Deployment:** High $\rho_{warm}$ ensures fast learning but wastes compute early on. Low $\rho_{deploy}$ saves compute but adapts slowly to distribution shifts.
  - **Search Space Granularity:** Finer grid $U$ for thresholds increases precision but adds compute cost to the betting update loop (O(N)).

- **Failure signatures:**
  - **Wealth Collapse:** $K_t(u)$ drops to near zero for all thresholds. This implies the data is harder than expected or the uncertainty score is misleading, forcing the system into a fully conservative mode ($\hat{u}_t \approx 0$).
  - **Guarantee Violation:** Observed risk exceeds $\epsilon$. This indicates an implementation error in the Martingale construction or IPS scaling, as the theoretical bounds (Theorem 4.2) should prevent this with high probability.

- **First 3 experiments:**
  1. **Stationary Sanity Check:** Run B-PAC on a fixed dataset (e.g., MATH) where the difficulty is known. Verify that the empirical risk (ER) stays strictly below the tolerance $\epsilon$ while reducing Expert Call Percentage (ECP).
  2. **Ablation on Exploration:** Vary the warm-up period $T_{warm}$ and exploration $\rho_{warm}$ to plot the convergence curve of the threshold $\hat{u}_t$. Identify the minimal warm-up time needed to stabilize.
  3. **Distribution Shift Robustness:** Feed a stream of "easy" queries followed by "hard" queries. Observe if the Betting Engine successfully drives the threshold $\hat{u}_t$ down to maintain safety when the hard batch hits, preventing risk violation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the B-PAC framework be extended to route queries among multiple models (e.g., a cascade of models with varying costs) rather than just a binary choice between a thinking and non-thinking model?
- **Basis in paper:** [explicit] The Limitations section states, "the present B-PAC reasoning switches between two models, extending it to multiple models remains unsolved."
- **Why unresolved:** The current theoretical formulation relies on a binary routing decision based on a single threshold $\hat{u}_{t-1}$ relative to one efficient model $\tilde{f}$.
- **What evidence would resolve it:** A generalized algorithm that constructs a wealth process capable of selecting from a set $\{f_1, \dots, f_k\}$ while maintaining anytime-valid safety guarantees.

### Open Question 2
- **Question:** How can the system adaptively select or combine uncertainty scores when multiple types (e.g., verbalized confidence vs. logits-based) are available?
- **Basis in paper:** [explicit] The Limitations section notes, "When multiple scores are available, how to adaptively select the most reliable score remains unknown."
- **Why unresolved:** The efficiency of B-PAC depends heavily on the discriminative power of the uncertainty score, but the method currently requires a single, fixed scoring function.
- **What evidence would resolve it:** A meta-learning mechanism or a dynamic weighting strategy integrated into the betting process that identifies the score most correlated with performance loss at time $t$.

### Open Question 3
- **Question:** Can the method be adapted to provide conditional Probably Approximately Correct (PAC) guarantees specific to the input instance, rather than marginal guarantees over the data distribution?
- **Basis in paper:** [explicit] The Limitations section lists "designing the conditional case (Zeng, 2025) of B-PAC reasoning" as a method to further enhance efficiency.
- **Why unresolved:** The current safety guarantees (Theorem 4.2) are marginal (valid on average over $X \sim P_X$), which may result in overly conservative thresholds for specific easy subsets of data.
- **What evidence would resolve it:** A derivation of conditional risk estimators and supermartingales that bound the loss $l(\hat{f}_t(X_t), f(X_t))$ conditional on $X_t$ features.

## Limitations
- The analysis focuses on 0-1 loss metrics, potentially missing nuanced reasoning quality assessment
- Distribution shift experiment uses synthetic performance drops rather than true data distribution shifts
- Computational overhead beyond LRM calls is not quantified for deployment decisions
- The bounded loss assumption [0,1] may not hold for all LRM applications

## Confidence
- **High confidence**: Theoretical framework for anytime-valid guarantees (Theorem 4.2) and logarithmic regret bounds (Theorem 4.3) are well-established through martingale theory
- **Medium confidence**: Empirical efficiency gains (81.01% reduction) are likely reproducible on similar mathematical reasoning tasks but may not generalize to all domains
- **Low confidence**: Claims about "computational efficiency" are underspecified - while the method reduces LRM calls, full computational budget is not reported

## Next Checks
1. **Cross-domain generalization**: Evaluate B-PAC on non-mathematical reasoning tasks (e.g., commonsense QA or code generation) where the thinking/non-thinking model agreement pattern differs from mathematical problems
2. **Cost-benefit analysis**: Measure wall-clock time and energy consumption for complete B-PAC deployments versus static threshold baselines across different hardware configurations
3. **Adversarial uncertainty**: Design synthetic query streams where the uncertainty score systematically misidentifies hard problems as easy, testing whether the safety guarantees hold under worst-case uncertainty estimation