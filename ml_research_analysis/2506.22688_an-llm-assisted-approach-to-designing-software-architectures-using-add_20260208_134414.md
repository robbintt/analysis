---
ver: rpa2
title: An LLM-assisted approach to designing software architectures using ADD
arxiv_id: '2506.22688'
source_url: https://arxiv.org/abs/2506.22688
tags:
- design
- architecture
- process
- system
- iteration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an approach for using large language models
  (LLMs) to assist in designing software architectures following the Attribute-Driven
  Design (ADD) method. The approach provides the LLM with an explicit description
  of ADD, an architect persona, and a structured iteration plan to guide collaborative
  architecture design with a human architect.
---

# An LLM-assisted approach to designing software architectures using ADD

## Quick Facts
- arXiv ID: 2506.22688
- Source URL: https://arxiv.org/abs/2506.22688
- Reference count: 40
- The approach shows promise but requires human oversight and iterative refinement due to current LLM limitations.

## Executive Summary
This paper presents an approach for using large language models (LLMs) to assist in designing software architectures following the Attribute-Driven Design (ADD) method. The approach provides the LLM with an explicit description of ADD, an architect persona, and a structured iteration plan to guide collaborative architecture design with a human architect. Two case studies were conducted: designing a hotel pricing system and an event ticketing system. Results showed that the LLM-assisted ADD process generated architectures closely aligned with established solutions and partially satisfied architectural drivers. The evaluation by professional architects found the approach promising but noted the need for human oversight and iterative refinement due to current LLM limitations. The findings highlight the potential of LLM-assisted architecture design while emphasizing that careful collaboration and review are essential for producing high-quality designs.

## Method Summary
The method uses Cursor IDE with Claude Sonnet 3.7 to execute ADD iterations with explicit human review pauses. The approach centers on a single architecture document that evolves across iterations, with the LLM generating an initial iteration plan based on prioritized architectural drivers. Each ADD step (review inputs, establish iteration goal, select concepts, instantiate elements, analyze) is executed sequentially with the LLM creating iteration-specific working documents. The process incorporates Chain-of-Thought prompting through explicit ADD method description and Plan-and-Solve prompting via iteration roadmap generation. All design artifacts including domain models, component diagrams, and sequence diagrams are created using Mermaid syntax within the centralized architecture document.

## Key Results
- LLM-assisted ADD process generated architectures closely aligned with established solutions for hotel pricing and event ticketing systems
- Architectural drivers were partially satisfied, with expert evaluation confirming the approach's promise
- Human oversight and iterative refinement proved essential due to LLM limitations in context management and consistency

## Why This Works (Mechanism)

### Mechanism 1: Structured Method Injection (Chain-of-Thought)
Providing the LLM with an explicit, textual description of the Attribute-Driven Design (ADD) process constrains the solution space, guiding the model to decompose architectural problems sequentially rather than generating immediate, monolithic solutions. The explicit ADD description serves as a rigid reasoning scaffold (Chain-of-Thought) that forces the LLM to produce intermediate artifacts (iteration documents, tables) that cumulatively build the architecture, rather than attempting a single zero-shot generation.

### Mechanism 2: Plan-and-Solve via Iteration Roadmap
Requiring the LLM to generate an iteration plan before design execution reduces the risk of unbounded complexity and improves driver coverage. This operates as a "Plan-and-Solve" prompting strategy where the LLM establishes a scope and termination condition for its reasoning by committing to a roadmap of design iterations. It creates a feedback loop where the human can validate the plan before costly generation begins.

### Mechanism 3: Centralized State via Single Architecture Document
Maintaining a single, evolving architecture document improves consistency and allows the LLM to maintain a coherent "mental model" of the system state across iterations. The single document acts as an externalized working memory that mitigates the LLM's tendency to "forget" earlier decisions when context slides. It creates a single source of truth for the "Architect Persona" to reference throughout the design process.

## Foundational Learning

- Concept: **Attribute-Driven Design (ADD)**
  - Why needed here: This is the core reasoning scaffold the LLM is asked to execute. Without understanding the ADD steps (Review Inputs, Establish Iteration Goal, Select Concepts, Instantiate, Analyze), you cannot evaluate if the LLM is following the method correctly or skipping critical analysis phases.
  - Quick check question: Can you distinguish between "Selecting a Design Concept" (e.g., choosing a pattern) and "Instantiating Elements" (applying that pattern to specific components)?

- Concept: **Context Window Management & "Forgetting"**
  - Why needed here: The paper explicitly identifies "forgetting" as a failure mode. Understanding that LLMs have limited active memory is crucial for knowing when to summarize, re-state constraints, or restart a session.
  - Quick check question: If the architecture document exceeds the LLM's effective context length, what specific behavior might you observe in the generated designs? (Answer: Inconsistencies, hallucinated requirements, or failure to update existing sections).

- Concept: **Prompt Pattern Sequences (Persona + Plan)**
  - Why needed here: The approach relies on an "Architect Persona" and a "Plan-and-Solve" structure. You must understand that these are prompt engineering techniques used to steer the model's tone and reasoning process, not just literal instructions.
  - Quick check question: Why is it necessary to explicitly tell an LLM to "plan its design process initially" rather than just asking it to "design the architecture"?

## Architecture Onboarding

- Component map: `ArchitecturalDrivers.md` -> `AttributeDrivenDesign.md` -> `Agent_Persona.md` -> `IterationPlan.md` -> `Iteration_N.md` -> `Architecture.md`

- Critical path:
  1. **Setup**: Define Persona and provide ADD method description to LLM context
  2. **Planning**: Prompt LLM to generate `IterationPlan.md` based on `ArchitecturalDrivers.md`
  3. **Iterative Design**: For each iteration in the plan, trigger the ADD workflow. LLM analyzes in `Iteration_N.md` and commits changes to `Architecture.md`
  4. **Verification**: Human reviews `Architecture.md` after *each* ADD step (not just each iteration)

- Design tradeoffs:
  - **Single Doc vs. Modular Files**: A single `Architecture.md` aids LLM context but creates a large file to edit. Modular files are easier for the LLM to modify but harder for it to cross-reference.
  - **Control vs. Speed**: "Vibe architecting" (auto-pilot) is fast but produces technical debt. Strict step-by-step review (Section 7) is slower but yields higher quality.

- Failure signatures:
  - **Hallucinated Process**: LLM skips ADD steps or invents new ones (mitigate by re-pasting the ADD step instructions)
  - **Context Drift**: LLM ignores requirements from early iterations (mitigate by re-stating key drivers at the start of each new iteration prompt)
  - **Abstraction Mixing**: LLM mixes component-level details with container-level concerns in the same diagram (mitigate by explicit constraints in the persona or prompt)

- First 3 experiments:
  1. **Zero-Shot Baseline**: Ask the LLM to design the architecture of a known system without the ADD method. Compare the structure and depth to the ADD-guided approach.
  2. **Plan Adherence Test**: Create an iteration plan, then explicitly try to "trick" the LLM by asking it to address a low-priority driver in an early iteration. Observe if it corrects you based on the plan.
  3. **Context Limit Test**: Allow the `Architecture.md` to grow large (10+ pages). Attempt a minor update to the first section. Verify if the LLM maintains consistency with the final section.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the context passed to the LLM be optimized to prevent omissions and "memory losses" during complex, multi-step architectural design iterations?
- Basis in paper: [explicit] The authors explicitly state, "We need to study approaches that improve the information that is passed to the LLM during the design process to avoid omissions and memory losses."
- Why unresolved: The paper notes the LLM frequently "forgot" to wait for human review, failed to read specific user story files, or omitted modifying the architecture document as it grew large.
- What evidence would resolve it: A comparative study measuring the consistency and correctness of designs generated using different context management strategies (e.g., RAG, specialized fine-tuning) versus the standard token-window approach.

### Open Question 2
- Question: Can an independent LLM instance reliably evaluate the quality and driver-satisfaction of an architecture generated by another LLM?
- Basis in paper: [explicit] The authors note, "Evaluating architectures using LLMs... a more formal approach still needs to be investigated."
- Why unresolved: While informal experiments showed promise, it is unclear if an LLM can effectively act as an automated reviewer to catch hallucinations or logical errors without human oversight.
- What evidence would resolve it: A validation study comparing the feedback of an "evaluator" LLM against the feedback of professional architects (using a method like ATAM) on the same set of generated designs.

### Open Question 3
- Question: Does explicitly instructing the LLM to perform trade-off analysis improve the quality and rationale of architectural design decisions?
- Basis in paper: [explicit] The paper asks, "studying how well the LLM can make this type of analysis remains to be done."
- Why unresolved: The current approach focuses on satisfying specific drivers sequentially but does not force the LLM to explicitly weigh competing quality attributes (e.g., security vs. performance) during decision-making.
- What evidence would resolve it: An experiment where LLM-assisted designs are generated with and without a mandatory trade-off analysis step, evaluated on the depth of reasoning in the design decision rationale.

### Open Question 4
- Question: Can the architecture documentation be automatically updated to reflect changes made directly to the source code?
- Basis in paper: [explicit] The authors list "Updating the architecture document from changes in the code" as an area where "Automatically updating... is an interesting research area."
- Why unresolved: The current approach risks the documentation becoming outdated (stale) if developers modify the code without running corresponding design iterations, breaking the "single source of truth" model.
- What evidence would resolve it: The development of a mechanism that parses code diffs and successfully proposes updates to the relevant views (e.g., component diagrams) in the Architecture.md file.

## Limitations
- The single architecture document strategy creates a scalability bottleneck as documents grow large, with the LLM struggling to maintain editing accuracy
- Evaluation is limited to two case studies (hotel pricing and event ticketing systems), which may not generalize to more complex domains
- The approach requires substantial human oversight and iterative refinement, with the exact frequency and nature of required intervention remaining unclear

## Confidence

- **High Confidence**: The ADD method structure provides effective scaffolding for LLM reasoning (supported by explicit methodology description and comparison to Chain-of-Thought prompting literature)
- **Medium Confidence**: The iteration planning improves driver coverage and prevents Big Design Up Front (supported by case study results but limited to two examples)
- **Low Confidence**: The single document approach remains viable for larger projects (explicitly contradicted by paper's own admission of LLM struggles with large documents)

## Next Checks

1. **Context Window Stress Test**: Reproduce the approach with progressively larger architecture documents (5, 10, 15+ pages) to empirically measure when and how the LLM's editing accuracy degrades, and test proposed mitigation strategies like section-specific prompts or document segmentation.

2. **Generalization Across Domains**: Apply the same methodology to three additional architectural domains (e.g., real-time embedded systems, microservices platforms, and data pipelines) to assess whether the 85% driver satisfaction rate holds across different complexity levels and architectural patterns.

3. **Human Intervention Quantification**: Conduct a controlled experiment measuring the exact number and type of human corrections required per ADD iteration across multiple projects, comparing against the paper's qualitative "careful collaboration" requirement to establish quantitative overhead metrics.