---
ver: rpa2
title: 'KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning'
arxiv_id: '2509.15676'
source_url: https://arxiv.org/abs/2509.15676
tags:
- arxiv
- in-context
- learning
- examples
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the exemplar selection problem in in-context
  learning (ICL), where a language model must be provided with a small, informative
  set of task-specific examples from a larger bank to maximize performance on a user
  query. The authors propose KITE (Kernelized and Information Theoretic Exemplars),
  a principled approach that frames exemplar selection as a query-specific optimization
  problem.
---

# KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning

## Quick Facts
- **arXiv ID**: 2509.15676
- **Source URL**: https://arxiv.org/abs/2509.15676
- **Reference count**: 38
- **Primary result**: KITE achieves up to +4.55% accuracy improvement over BM25 on SST-5 with GPT-Neo-2.7B and +2.24% on HellaSwag with Llama-3B over DPP method

## Executive Summary
KITE addresses the exemplar selection problem in in-context learning by proposing a principled, query-specific optimization framework. The method models language models as linear functions over input embeddings and derives a surrogate objective that is approximately submodular, enabling efficient greedy selection with approximation guarantees. By incorporating kernelization and diversity regularization, KITE operates in high-dimensional feature spaces while encouraging diverse example selection. Empirical results demonstrate consistent improvements over strong baselines across multiple datasets and language models.

## Method Summary
KITE frames exemplar selection as minimizing prediction error on a specific query by modeling LLMs as linear functions over input embeddings. The method derives a surrogate objective that is approximately submodular, enabling greedy selection with provable approximation guarantees. KITE enhances this approach by incorporating the kernel trick to operate in high-dimensional feature spaces without explicit mappings and introduces an optimal design-based regularizer to encourage diversity among selected examples. The algorithm uses Sherman-Morrison updates to maintain the inverse covariance matrix efficiently during greedy selection.

## Key Results
- KITE consistently outperforms strong baselines like kNN, DPP, and BM25 across five classification datasets
- Achieves up to +4.55% accuracy improvement over BM25 on SST-5 with GPT-Neo-2.7B
- Achieves +2.24% accuracy improvement on HellaSwag with Llama-3B over state-of-the-art DPP method
- Ablation studies confirm the importance of kernel choice and diversity in exemplar selection

## Why This Works (Mechanism)

### Mechanism 1: Query-Specific Submodular Optimization
The paper derives a surrogate objective f_z(S) = -z^⊤ V_S^{-1} z that bounds prediction error and is shown to be approximately submodular with ratio γ ≥ 1/(1 + (k-1)μ). This enables greedy selection with (1 - e^{-γ}) approximation guarantees. The linear model assumption (y = ⟨x, θ⟩ + η) provides the theoretical foundation, though it's acknowledged as a simplification.

### Mechanism 2: Kernelization via the Kernel Trick
By replacing Euclidean inner products with kernel evaluations k(x, x'), KITE implicitly operates in high-dimensional feature spaces without computing explicit feature maps. The Woodbury identity enables efficient computation of (Φ_S^⊤ Φ_S + βI)^{-1} via gram matrices K_S. Kernel choice (Linear, Polynomial, Gaussian RBF) is task-dependent and requires careful hyperparameter tuning.

### Mechanism 3: Diversity via D-Optimal Design Regularization
The diversity objective g(S) = log det(V_S) is monotone submodular and promotes coverage of directions in embedding space not already spanned. Combined score: relevance + λ·diversity. This approach is particularly effective for large or redundant exemplar banks, with λ ≈ 1 optimal for MNLI's 400k exemplars.

## Foundational Learning

- **Submodular set functions and greedy approximation**
  - Why needed here: The theoretical contribution relies on proving approximate submodularity to justify greedy selection with guarantees
  - Quick check question: For a monotone submodular function, what approximation factor does the greedy algorithm guarantee under a cardinality constraint?

- **The kernel trick and RKHS**
  - Why needed here: Understanding how inner products can be replaced by kernel evaluations without computing ϕ(·) is essential to follow the kernelization derivation
  - Quick check question: Given a positive semi-definite kernel k, what does Mercer's theorem guarantee about the existence of a feature map ϕ?

- **Optimal experimental design (D-optimality)**
  - Why needed here: The diversity regularizer log det(V_S) originates from D-optimal design; understanding this clarifies why it promotes coverage
  - Quick check question: In D-optimal design, what quantity is maximized, and how does it relate to information gain about θ in a linear model?

## Architecture Onboarding

- **Component map**: Embedding module -> Inverse covariance maintainer -> Scoring module -> Selector -> Kernel module
- **Critical path**: 
  1. Pre-compute and cache embeddings for the exemplar bank
  2. At inference, embed the query z
  3. Initialize V^{-1} = (1/β)I_d and S = ∅
  4. For j = 1..k: score all available candidates via kernelized relevance + λ·diversity; select x*; update V^{-1}
  5. Concatenate selected exemplars into the prompt; call the LLM
- **Design tradeoffs**:
  - Linear vs. non-linear kernels: Linear is faster and equivalent to LITE; non-linear (Poly, RBF) can capture richer structure but require tuning
  - λ (diversity weight): Higher λ improves diversity but can reduce query relevance. Paper uses λ = 0.5 in main experiments; λ ≈ 1 for large/redundant banks
  - β (regularization): Higher β pushes γ → 1 (nearly submodular) but may over-regularize. Paper uses β = 0.02
- **Failure signatures**:
  - All selected examples are nearly identical → λ may be too low, or embeddings lack discriminative structure
  - Performance degrades with non-linear kernels → check kernel hyperparameters (σ, m); default to linear
  - Slow inference → ensure V^{-1} is updated via Sherman-Morrison, not recomputed from scratch
- **First 3 experiments**:
  1. Replicate Table 1 on a single dataset (e.g., SST-5) with the linear kernel (LITE) vs. Dense vs. DPP; verify relative gains
  2. Ablate kernels (Table 3) to identify the best-performing kernel for your target task and embedding model
  3. Sweep λ (as in Appendix Figure 2) to calibrate the relevance-diversity trade-off for your exemplar bank size

## Open Questions the Paper Calls Out
None

## Limitations
- The linear model assumption (y = ⟨x, θ⟩ + η) is a simplification that may not hold for highly non-linear task structures
- Kernel hyperparameters (σ, m) are task-dependent but systematic guidance for selection is absent
- Approximation guarantees rely on concentration bounds that may be loose in practice

## Confidence
**High confidence**: The empirical improvements (+4.55% SST-5, +2.24% HellaSwag) are directly measured and significant
**Medium confidence**: Theoretical approximation guarantees hold under stated assumptions but real-world applicability depends on linear model approximation
**Medium confidence**: Diversity benefits are supported by ablation studies but optimal λ trade-off appears dataset and bank-size dependent

## Next Checks
1. Systematically vary σ (RBF) and m (polynomial) across all five datasets to identify if consistent patterns emerge or if kernel choice truly requires per-task tuning
2. Test whether performance gains persist when using minimal prompt templates vs. elaborate templates to isolate exemplar selection contribution
3. Evaluate KITE performance as exemplar bank size scales from 1k to 400k examples to confirm diversity regularizer benefits increase with bank size/redundancy