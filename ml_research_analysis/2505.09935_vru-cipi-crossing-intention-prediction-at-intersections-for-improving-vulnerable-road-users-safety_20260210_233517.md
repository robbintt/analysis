---
ver: rpa2
title: 'VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable
  Road Users Safety'
arxiv_id: '2505.09935'
source_url: https://arxiv.org/abs/2505.09935
tags:
- crossing
- pedestrian
- prediction
- features
- intention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VRU-CIPI, a framework for predicting vulnerable
  road user (VRU) crossing intentions at intersections. The approach uses a GRU-based
  sequential model integrated with a multi-head Transformer self-attention mechanism
  to capture both temporal dynamics and contextual dependencies from VRU pose, motion,
  and location features.
---

# VRU-CIPI: Crossing Intention Prediction at Intersections for Improving Vulnerable Road Users Safety

## Quick Facts
- arXiv ID: 2505.09935
- Source URL: https://arxiv.org/abs/2505.09935
- Reference count: 40
- State-of-the-art accuracy of 96.45% for VRU crossing intention prediction

## Executive Summary
VRU-CIPI introduces a framework for predicting vulnerable road user (VRU) crossing intentions at intersections using stationary CCTV cameras. The approach combines Gated Recurrent Units (GRU) with a multi-head Transformer self-attention mechanism to capture both temporal dynamics and contextual dependencies from VRU pose, motion, and location features. Evaluated on the UCF-VRU dataset, the model achieves state-of-the-art accuracy while maintaining real-time inference capabilities, enabling proactive intersection safety through early warnings to connected vehicles.

## Method Summary
The framework extracts features from CCTV video streams including pose keypoints, motion data, and location information for VRUs in waiting areas. A GRU processes 2.5-second temporal sequences of these features, followed by a Transformer encoder with multi-head self-attention to capture contextual relationships. The model outputs binary predictions for crossing direction (Crosswalk A or B) using sigmoid activation. Training uses AdamW optimizer with ReduceLROnPlateau scheduling on RTX 3080 Ti hardware.

## Key Results
- Achieves state-of-the-art accuracy of 96.45% on UCF-VRU dataset
- Real-time inference at 33 frames per second
- Pose estimation features contribute 3.17% accuracy improvement over location-only features
- Model successfully predicts crossing direction before VRUs enter the crosswalk

## Why This Works (Mechanism)

### Mechanism 1
Sequential modeling of movement dynamics is essential for distinguishing between stationary waiting and active preparation to cross. The GRU processes 2.5-second windows, using update and reset gates to selectively retain relevant motion history while discarding noise. This captures the temporal evolution of crossing intent rather than relying on instantaneous position.

### Mechanism 2
Multi-head self-attention resolves directional ambiguity by weighing spatial features against location data. The Transformer encoder dynamically prioritizes specific features like face angle pointing toward a crosswalk, even if the VRU is closer to another crossing. The 2-head mechanism allows parallel attention maps for different feature subspaces.

### Mechanism 3
Fusing coarse geometric context with fine-grained pose estimation compensates for low-resolution CCTV cameras. The framework crops the waiting area before pose estimation, focusing on relevant pixels to extract keypoints that would be indiscernible in full-frame downscale.

## Foundational Learning

**Gated Recurrent Units (GRU)**: Essential for handling the 2.5-second time dimension. Unlike CNNs that see snapshots, GRUs remember if a VRU has been accelerating or looking around for several seconds. Quick check: If input sequence length doubles to 5.0 seconds, would inference latency double, or does GRU's sequential nature impose different constraints?

**Multi-Head Self-Attention**: The context engine that calculates attention scores to determine that "face angle" matters more than "foot position" at specific timesteps. Quick check: In VRU-CIPI, does the Transformer attend to raw pixels or GRU hidden states?

**Pose Estimation Keypoints**: Primary signal for direction. The model consumes angles and distances between joints derived from keypoints rather than raw images. Quick check: According to ablation study, how much does accuracy drop if pose features are removed and only location/motion are used?

## Architecture Onboarding

**Component map**: CCTV Video Stream -> YOLOv8m (Detection) + OC-SORT (Tracking) -> ZoI Cropping -> YOLOv8m-pose (Keypoints) -> Feature Engineering -> Sequence Builder -> GRU (256 units) -> Transformer Encoder (2 heads) -> FC Sigmoid

**Critical path**: Image Cropping & Merging stage. If association between global tracking ID and cropped pose data fails, model receives mismatched features (e.g., Person A's location with Person B's pose), leading to nonsensical predictions.

**Design tradeoffs**: Accuracy vs. Context - uses only 2 attention heads since 4 heads offered no benefit (96.28% vs 96.45%). Latency vs. Information - pose estimation is turned OFF during Crossing Monitoring stage to save compute once direction is confirmed.

**Failure signatures**: Occlusion in Waiting Area - multiple VRUs blocking each other cause pose estimation failures, forcing reliance on weaker Location features (~92% accuracy). Night/Rain - performance degrades when body contours are obscured by darkness or rain gear.

**First 3 experiments**: 1) Ablation Validation - retrain without Pose features to verify ~3.17% accuracy drop. 2) Window Sensitivity - test varying input window sizes (1.0s vs 2.5s) to determine needed historical context. 3) Head-to-Head Comparison - benchmark GRU-Transformer against pure LSTM or pure Transformer models.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but several are implied through its limitations section and methodology.

## Limitations
- UCF-VRU dataset is proprietary, limiting independent validation
- Performance may degrade with different intersection configurations beyond Florida locations
- Quantitative analysis of occlusion failure modes is absent

## Confidence
**High Confidence**: Real-time inference at 33 FPS; Pose features contribute ~3.17% accuracy; Proactive prediction capability
**Medium Confidence**: 2-head Transformer is optimal; Performance across day/night/rain conditions
**Low Confidence**: Full reproducibility of feature engineering pipeline without dataset access

## Next Checks
1. **Ablation Study Replication**: Retrain model without pose features to verify ~3.17% accuracy drop
2. **Cross-Intersection Generalization**: Test trained model on different intersection geometry to measure accuracy drop
3. **Occlusion Stress Test**: Create synthetic occlusion scenarios to evaluate model fallback behavior and accuracy maintenance