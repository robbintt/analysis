---
ver: rpa2
title: 'Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality'
arxiv_id: '2507.20156'
source_url: https://arxiv.org/abs/2507.20156
tags:
- data
- training
- arxiv
- quality
- filtration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a lightweight data filtration framework that
  uses a compact vision-language model (VLM) to filter noisy web-scale image-caption
  pairs. The approach trains a small VLM on high-quality annotated data to evaluate
  and score potential training samples based on caption quality, image quality, and
  alignment.
---

# Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality

## Quick Facts
- arXiv ID: 2507.20156
- Source URL: https://arxiv.org/abs/2507.20156
- Reference count: 40
- A 2B-parameter VLM trained to filter image-caption pairs achieves better downstream captioning than training on full noisy datasets

## Executive Summary
This paper introduces a lightweight data filtration framework that uses a compact vision-language model to filter noisy web-scale image-caption pairs. The approach trains a small VLM on high-quality annotated data to evaluate and score potential training samples based on caption quality, image quality, and alignment. Unlike prior work that adds auxiliary modules to large VLMs, this method leverages the compact model's intrinsic evaluative ability, reducing computational overhead. Experimental results show that the filtered dataset significantly improves caption quality, enhances semantic alignment, and achieves better downstream captioning performance.

## Method Summary
The authors fine-tune a 2B-parameter VLM (Qwen2-VL-2B) on 5K annotated image-caption pairs where a large teacher model (Gemini 2.0-Flash) assigns quality scores (1-10) and rationales. The student model learns to approximate this scoring behavior via supervised fine-tuning. New image-caption pairs are then scored by this filtration model, with only high-scoring samples (≥9) retained for downstream training. The filtered dataset is 18% the size of the original but demonstrates superior quality metrics and downstream performance.

## Key Results
- Perplexity reduction: 170.2 → 137.2 (cleaner captions post-filtration)
- CLIP similarity improvement: 0.297 → 0.313 (better semantic alignment)
- LLM-as-judge preference: 59.4% (297/500) favoring filtered model's captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A compact VLM can acquire evaluative capabilities through supervised fine-tuning on teacher-model annotations.
- Mechanism: The teacher model (Gemini 2.0-Flash) assigns numerical quality scores (1–10) and textual rationales to image-caption pairs; the student model (Qwen2-VL-2B) learns to approximate this scoring behavior via standard SFT, internalizing both the scoring criteria and explanatory patterns.
- Core assumption: The evaluative judgment of a large VLM can be distilled into a much smaller model without catastrophic loss of assessment fidelity.
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If the student model fails to generalize to image-caption distributions markedly different from the 5K training samples.

### Mechanism 2
- Claim: Aggressive quality-based filtering (score ≥9) improves training signal despite reducing dataset size to 18%.
- Mechanism: Removing noisy or misaligned pairs increases the signal-to-noise ratio in the training corpus. Lower perplexity indicates more coherent captions; higher CLIP similarity indicates better semantic alignment. The downstream model learns cleaner associations without memorizing incorrect patterns.
- Core assumption: High-scoring samples represent genuinely higher-quality training signal, not just superficial fluency.
- Evidence anchors: [section 4.2], [section 4.3]
- Break condition: If filtering eliminates informative diversity.

### Mechanism 3
- Claim: The filtration model's assigned scores correlate with ground-truth semantic alignment.
- Mechanism: The model learns to map visual-textual features to a scalar quality score that reflects alignment. The ablation study shows monotonic increase in CLIP cosine similarity across score buckets.
- Core assumption: CLIP cosine similarity is a valid proxy for semantic image-text alignment.
- Evidence anchors: [Table 2], [section 4.1]
- Break condition: If the correlation decays for specialized domains underrepresented in training data.

## Foundational Learning

- **Supervised Fine-Tuning (SFT) for VLMs**: Why needed here: The entire approach depends on transferring teacher judgments to a student model via SFT; understanding learning rate schedules, epoch choices, and overfitting risks is critical. Quick check question: Given the paper uses lr=2×10⁻⁶ for one epoch on 5K samples, what risks arise from increasing epochs or learning rate?

- **CLIP Embeddings and Cosine Similarity**: Why needed here: The primary evaluation metric for image-text alignment; used to validate that filtration scores are meaningful. Quick check question: If CLIP embeddings for two different images have cosine similarity 0.95 with the same caption, what does this suggest about caption specificity?

- **Perplexity as Language Model Quality Metric**: Why needed here: Used to demonstrate caption fluency improvement post-filtration; lower perplexity indicates better fit to a language model's expectations. Quick check question: Why might very low perplexity captions still be poor training data for VLMs?

## Architecture Onboarding

- Component map: Gemini 2.0-Flash (teacher) -> Qwen2-VL-2B (student/filter) -> Downstream VLM training

- Critical path: 1) Sample diverse pairs from source datasets 2) Annotate with teacher model (score 1–10 + rationale) 3) Manual review for annotation accuracy 4) Fine-tune Qwen2-VL-2B (8×H100, batch 128, lr=2×10⁻⁶, 1 epoch) 5) Deploy: score new pairs, filter by threshold (≥9 retained)

- Design tradeoffs: Training set size (5K) vs annotation cost and quality control; Score threshold stringency (≥9 retains 18%) vs data diversity loss; Compact model (2B) vs evaluation accuracy

- Failure signatures: Over-filtering (model underperforms on diverse domains); Teacher bias propagation (systematic scoring errors transfer); Distribution shift (scores calibrated on CC12M/COCO may not generalize)

- First 3 experiments: 1) Threshold sweep (7+, 8+, 9+, 10) to quantify diversity-quality tradeoff 2) Out-of-distribution test (medical imaging, diagrams) to measure score-alignment correlation 3) Teacher ablation (scores only vs scores + rationales) to test supervision richness

## Open Questions the Paper Calls Out

None

## Limitations
- The 5K training sample size may be insufficient for capturing full data distribution complexity
- CLIP cosine similarity may not capture all aspects of image-caption quality, particularly for specialized domains
- Results may not transfer to more sophisticated VLM architectures beyond the simple ViT encoder + GPT-2 decoder used

## Confidence

- **High Confidence**: Filtration demonstrably improves caption quality (perplexity reduction) and semantic alignment (CLIP similarity increase); LLM-as-a-judge preference provides strong evidence of better captions
- **Medium Confidence**: Compact VLMs can serve as effective in-context judges within tested domain (CC12M/COCO-style pairs) but requires validation across broader distributions
- **Low Confidence**: Generalization to arbitrary web-scale data is weakest; filtration model was trained on specific mix of CC12M and Recap-COCO data

## Next Checks
1. **Threshold Optimization Study**: Systematically vary the quality threshold (7+, 8+, 9+, 10) and measure the tradeoff between dataset size and downstream performance to quantify optimal balance.

2. **Cross-Domain Generalization Test**: Apply the trained filtration model to datasets from qualitatively different domains (medical imaging, technical diagrams, abstract art, satellite imagery) and measure both score distributions and their correlation with CLIP-based alignment metrics.

3. **Teacher Model Ablation**: Train two versions of the filtration model - one using only scalar quality scores (1-10) and another using both scores and textual rationales. Compare their performance on downstream tasks to determine whether richer supervision provides measurable benefit.