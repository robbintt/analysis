---
ver: rpa2
title: 'EWE: An Agentic Framework for Extreme Weather Analysis'
arxiv_id: '2511.21444'
source_url: https://arxiv.org/abs/2511.21444
tags:
- extreme
- agent
- weather
- data
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces EWE, an autonomous intelligent agent framework
  for extreme weather event analysis. The key innovation is integrating knowledge-enhanced
  planning, closed-loop reasoning, and a domain-specific meteorological toolkit to
  automate diagnostic workflows that traditionally require manual expert intervention.
---

# EWE: An Agentic Framework for Extreme Weather Analysis

## Quick Facts
- arXiv ID: 2511.21444
- Source URL: https://arxiv.org/abs/2511.21444
- Reference count: 7
- Key outcome: Autonomous intelligent agent framework for extreme weather event analysis using knowledge-enhanced planning, closed-loop reasoning, and meteorological toolkit

## Executive Summary
EWE introduces an autonomous intelligent agent framework for extreme weather event analysis that integrates knowledge-enhanced planning, closed-loop reasoning, and a domain-specific meteorological toolkit. The framework automates diagnostic workflows that traditionally require manual expert intervention, validated on a benchmark of 103 high-impact events. Results show Claude-4-Sonnet outperforming other models in analytical tasks, while ablation studies confirm the necessity of the proposed components. EWE represents a foundational step toward scalable, automated scientific discovery in extreme weather diagnostics.

## Method Summary
EWE employs a three-component framework: (1) Knowledge-Enhanced Planning with expert-annotated chain-of-thought exemplars that decompose complex meteorological diagnostics into sequential sub-tasks; (2) Self-Evolving Closed-Loop Reasoning with dual-auditor system (Code Auditor for static analysis of latent bugs, Content Auditor for visualization clarity); and (3) Meteorological Toolkit with pre-verified functions for canonical diagnostics like Integrated Vapor Transport and potential vorticity. The agent operates through a Think-Act-Observe-Interpret cycle with maximum 40 steps per run, using GPT-4.1 for automated step-wise evaluation across seven diagnostic stages.

## Key Results
- Claude-4-Sonnet outperforms other models in analytical tasks for extreme weather diagnostics
- Ablation studies confirm necessity of all three components (planning, auditor, toolkit)
- Framework validated on 103 high-impact events from 2014-2024 across six IPCC AR6 categories
- Novel step-wise evaluation metric assesses code fidelity, visualization quality, and physical interpretation depth

## Why This Works (Mechanism)

### Mechanism 1
Knowledge-enhanced planning with expert chain-of-thought exemplars constrains LLM hallucinations and structures diagnostic reasoning by decomposing complex meteorological diagnostics into sequential sub-tasks. Each step explicitly cites underlying physical laws or equations, forcing the model to ground reasoning in established meteorological principles rather than free-form generation.

### Mechanism 2
Dual-auditor closed-loop reasoning catches both code-level errors and visualization-level deficiencies that standard exception handling misses. The Code Auditor performs static analysis to identify latent bugs while the Content Auditor assesses visualizations for perceptual clarity, enabling self-correction before flawed analysis propagates.

### Mechanism 3
Pre-verified meteorological analysis tools enable domain-accurate computations that general-purpose code generation consistently fails to produce. Rather than generating complex meteorological computations from scratch, the agent invokes verified functions with appropriate parameters, reducing implementation errors.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Why needed? The entire planning mechanism depends on understanding how step-by-step reasoning decomposition guides LLM behavior. Quick check: Can you explain why CoT with domain exemplars differs from zero-shot prompting for complex multi-step tasks?
- **ReAct Pattern (Reasoning + Acting)**: Why needed? EWE's Think-Act-Observe-Interpret loop is a ReAct variant; understanding this pattern is prerequisite to grasping the closed-loop architecture. Quick check: How does interleaving reasoning traces with tool calls differ from generating a complete plan upfront?
- **Atmospheric Scale Hierarchy**: Why needed? The framework decomposes analysis into synoptic-scale, mesoscale, and thermodynamic components—standard meteorological practice but unfamiliar to general engineers. Quick check: Why would analyzing a cyclone require separate consideration of large-scale circulation versus mesoscale convective processes?

## Architecture Onboarding

- Component map: User Query → Knowledge-Enhanced Planner → Sub-task Queue → Memory Module ↔ Tool Selector → Meteorological Toolkit → Code Generator → Executor → Code Auditor → Content Auditor ← Visualizations → Interpreter (MLLM) ← Observations + Images → Synthesis Report → Output
- Critical path: The auditor feedback loop is the architectural differentiator. Tracing a single diagnostic step from planning through auditor validation to interpretation is essential before attempting end-to-end runs.
- Design tradeoffs: (1) 40-step limit trades depth for tractability—complex events may require more iterations. (2) Context pruning reduces token costs but may discard information relevant to later synthesis. (3) Single-model (GPT-4.1) evaluation ensures consistency but introduces judge-specific bias.
- Failure signatures: Step count exceeds 40 → task marked failed; Code Auditor flags uncorrectable errors → loop stalls; Content Auditor repeatedly rejects visualizations → action-observation cycle oscillates; Physical implausibility in interpretation → low evaluation scores despite syntactically correct output.
- First 3 experiments: (1) Single-event trace: Run one event with full logging enabled. (2) Component ablation: Disable one component at a time on 10-event subset. (3) Model substitution: Replace Claude-4-Sonnet with weaker model on 5 events.

## Open Questions the Paper Calls Out

- **Real-time data integration**: Can EWE effectively integrate real-time observational data streams to perform diagnosis during evolving weather events? The current study validates on retrospective ERA5 reanalysis data, not noisy real-time feeds.
- **MLLM judge alignment**: To what extent does the MLLM-as-a-judge evaluation align with human meteorological expert assessments? Automated judges may fail to capture nuanced causal reasoning that human experts would catch.
- **Generalization to rare events**: How does EWE performance generalize to rare or compound extreme weather events outside the six primary IPCC AR6 categories? The benchmark may limit CoT prompts' effectiveness for novel or compound phenomena.

## Limitations

- Reliance on expert-annotated chain-of-thought exemplars is critical yet vulnerable—errors in exemplars propagate to planning errors
- Dual-auditor system has limited empirical evidence for the specific interplay between code and content validation in meteorological diagnostics
- Toolkit effectiveness depends on pre-validated implementations that are not fully specified in the paper

## Confidence

- **High**: Closed-loop reasoning architecture with dual auditors demonstrably improves diagnostic accuracy compared to single-pass generation, supported by ablation studies and visual examples
- **Medium**: Knowledge-enhanced planning with CoT exemplars effectively structures LLM reasoning, though direct causal evidence for this specific approach in weather analysis is limited
- **Medium**: Pre-verified toolkit functions reduce implementation errors compared to general code generation, though corpus evidence for this specific mechanism is weak

## Next Checks

1. Implement a synthetic benchmark with ground-truth diagnostics to test whether the dual-auditor system catches errors that single-pass generation misses, focusing on subtle physical reasoning errors versus obvious syntax bugs
2. Conduct a component-wise sensitivity analysis varying the quality of expert exemplars (introducing controlled errors) to quantify the impact on planning accuracy and downstream diagnostics
3. Compare toolkit-based implementations against LLM-generated implementations on identical diagnostics using domain expert review to measure the actual accuracy gap and identify edge cases where toolkit coverage is insufficient