---
ver: rpa2
title: Steering CLIP's vision transformer with sparse autoencoders
arxiv_id: '2504.08729'
source_url: https://arxiv.org/abs/2504.08729
tags:
- features
- clip
- vision
- saes
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work trains sparse autoencoders (SAEs) on CLIP\u2019s vision\
  \ transformer and introduces a steerability metric to quantify how precisely SAE\
  \ features can influence model outputs. Analysis shows 10-15% of features are steerable,\
  \ with SAEs offering thousands more steerable elements than the base model."
---

# Steering CLIP's vision transformer with sparse autoencoders

## Quick Facts
- arXiv ID: 2504.08729
- Source URL: https://arxiv.org/abs/2504.08729
- Reference count: 40
- Key outcome: SAEs provide 10x better concept coverage than base neurons and improve disentanglement performance on CelebA (worst-group accuracy up to 81.11%) and Waterbirds (up to 24.61%), with state-of-the-art robustness against typographic attacks (0.72-0.88 accuracy across benchmarks)

## Executive Summary
This work applies sparse autoencoders (SAEs) to CLIP's vision transformer to decompose its internal representations into more interpretable, steerable features. The authors introduce a steerability metric that quantifies how precisely SAE features can influence model outputs, finding that 10-15% of features in deeper layers are steerable. They demonstrate that SAE-based feature suppression improves disentanglement performance on biased datasets and achieves strong defense against typographic attacks, while revealing that vision SAEs exhibit higher sparsity than language SAEs.

## Method Summary
The method involves training SAEs on CLIP-ViT-B-32's residual stream activations with 64x expansion (768→49,152 features), using both vanilla (ReLU+L1 regularization) and Top-K variants. Steerability is computed by activating individual features and measuring their effect on output probabilities across a 5,000-word vocabulary. For disentanglement, features that differentially activate on spurious vs. non-spurious examples are identified and zero-ablated. The approach is evaluated on CelebA gender classification with blondness suppression, Waterbirds background suppression, and typographic attack defense, with optimal layers varying by task.

## Key Results
- SAEs provide 10x better concept coverage than base neurons
- 10-15% of features in deeper layers are steerable with precise control
- Feature suppression improves CelebA worst-group accuracy from 77.78% to 81.11%
- SAE-based defense achieves 0.72-0.88 accuracy against typographic attacks across benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Sparse Decomposition Reduces Polysemanticity
SAEs convert polysemantic base neurons into interpretable monosemantic features by learning an overcomplete dictionary with sparsity constraints. The sparse feature hypothesis suggests internal representations can be linearly decomposed into interpretable directions. Evidence shows vision SAEs have L0 = 300-700 vs. GPT-2's 20-50, indicating vision-specific sparsity patterns. Break condition: if reconstruction loss exceeds acceptable thresholds or L0 becomes extremely high (>50,000 for spatial tokens), the decomposition may be insufficiently sparse.

### Mechanism 2: Steerability Emerges from Feature-Level Concept Alignment
A subset of SAE features (10-15% in deeper layers) can be precisely manipulated because they align with semantically coherent directions in CLIP's latent space. The steerability metric S quantifies how uniformly a feature's steering affects output probabilities. Evidence: 1,322 steerable features among 12,000 total features at layer 11 using threshold γ = 0.10. Break condition: if vocabulary lacks terms for a feature's true semantic direction, S will underestimate steerability.

### Mechanism 3: Disentanglement via Differential Activation and Targeted Ablation
Suppressing features that differentially activate on spurious attributes improves worst-group accuracy by reducing reliance on confounding signals. Feature selection identifies features with higher activation on spurious-correlation-present vs. absent datasets. Evidence: worst-group accuracy improves from 77.78% to 81.11% (CelebA) and 22.43% to 24.61% (Waterbirds). Break condition: if spurious features are highly entangled with task-relevant features, ablation may harm overall accuracy.

## Foundational Learning

- **Sparse Dictionary Learning / Superposition**: Why needed? SAEs operate on the hypothesis that neural networks store features in superposition; understanding this explains why sparsity constraints reveal interpretable directions. Quick check: Can you explain why an overcomplete basis with L1 regularization yields sparser, more interpretable features than a complete basis without regularization?

- **CLIP Contrastive Pre-training**: Why needed? The paper leverages CLIP's aligned vision-language embedding space to label SAE features using text queries. Understanding contrastive loss explains why this alignment exists. Quick check: How does maximizing cosine similarity between matched image-text pairs create a shared semantic space?

- **Layer-wise Feature Evolution in Transformers**: Why needed? The paper finds optimal disentanglement at different layers for different tasks. Understanding how features evolve through transformer layers informs where to intervene. Quick check: Why might diffuse features (backgrounds) be better suppressed in earlier layers while localized features (hair color) require later layers?

## Architecture Onboarding

- **Component map**: CLIP-ViT-B-32 (12-layer vision transformer) -> SAE (64x expansion, vanilla/Top-K) -> Steerability metric (vocabulary-based probing) -> Feature selection (differential activation) -> Zero-ablation at inference

- **Critical path**: Extract activations from target layer's residual stream → Train SAE with chosen sparsity mechanism → Compute steerability scores using vocabulary-based probing → Identify task-relevant features via differential activation → Apply zero-ablation or steering at inference

- **Design tradeoffs**: Expansion factor 64 (higher = more features but higher compute); L1 coefficient vs. Top-K (variable vs. fixed sparsity); Layer selection (early = low-level, late = semantic); Vocabulary size (larger = better coverage but higher compute)

- **Failure signatures**: High reconstruction loss (>0.1 MSE) indicates SAE not capturing activation structure; Very high L0 (>50,000) suggests insufficient sparsity; Low concept coverage despite high L0 indicates vocabulary mismatch; Ablation harming overall accuracy >4% suggests entangled features

- **First 3 experiments**: 1) Reproduce steerability histogram (Figure 6) for layer 11 with vanilla SAE, verify ~10% exceed γ=0.10; 2) Layer sweep for CelebA disentanglement across all 12 layers, confirm peak at layers 7-8; 3) Feature vs. neuron steering comparison at layer 11, verify ~10x difference in concept coverage

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the optimal layer for feature disentanglement depend systematically on the spatial locality or diffuseness of the target concept? The paper observes varying optimal layers but doesn't isolate feature locality as the causal variable. Resolution requires controlled study mapping disentanglement performance across layers for concepts with varying spatial sizes.

- **Open Question 2**: How does the size of the text vocabulary used for labeling affect the measured "steerability" and monosemanticity of vision features? Current analysis limited to 5,000 words, potentially missing features due to vocabulary gaps. Resolution requires testing steerability with larger vocabularies or automated concept discovery methods.

- **Open Question 3**: Can steerability metrics be refined to capture semantic relationships between concepts rather than just probability shifts? Current metrics treat concepts as independent, failing to distinguish coherent vs. incoherent feature activation. Resolution requires validation of proposed distance metric $D_f$ against human evaluation.

## Limitations
- Vocabulary coverage bias: The 5,000-word vocabulary systematically underestimates steerability for features encoding concepts outside this vocabulary
- Dataset-specific SAE training: Task-specific SAEs trained on same data they're evaluated on raises concerns about overfitting to spurious correlations
- Ablation threshold sensitivity: Feature selection via Equation 9 depends critically on the τ threshold with no sensitivity analysis provided

## Confidence
- **High confidence**: SAEs provide 10x better concept coverage than base neurons; steerability metric successfully identifies interpretable features; feature suppression improves worst-group accuracy on CelebA and Waterbirds
- **Medium confidence**: Vision SAEs have higher L0 sparsity than language SAEs; optimal steering layer varies by task; typographic attack defense generalizes across benchmarks
- **Low confidence**: SAE steering is state-of-the-art for typographic attacks; reconstruction loss thresholds ensure meaningful features; decoder features are necessary for steering

## Next Checks
1. **Vocabulary coverage validation**: Train SAEs with systematically varied vocabulary sizes (1,000 → 10,000 words) and measure how steerability scores and concept coverage change
2. **Cross-dataset generalization test**: Train task-specific SAEs on CelebA/Waterbirds but evaluate on held-out datasets with similar spurious correlations to distinguish genuine disentanglement from dataset overfitting
3. **Ablation threshold robustness analysis**: For each disentanglement task, sweep τ across multiple orders of magnitude while measuring both worst-group accuracy and overall accuracy degradation, computing confidence intervals for optimal τ