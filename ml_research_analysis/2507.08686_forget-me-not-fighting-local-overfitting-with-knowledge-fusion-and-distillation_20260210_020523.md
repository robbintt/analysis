---
ver: rpa2
title: 'Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation'
arxiv_id: '2507.08686'
source_url: https://arxiv.org/abs/2507.08686
tags:
- training
- test
- data
- overfitting
- ensemble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies and addresses a phenomenon the authors call
  local overfitting, where deep models forget previously learned patterns in certain
  regions of the data space even without global overfitting. The authors propose a
  two-stage method: first, Knowledge Fusion (KF) combines checkpoints from mid-training
  using a forgetting metric to recover lost knowledge, and second, a distilled KF
  ensemble compresses this ensemble into a single model to maintain inference efficiency.'
---

# Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation

## Quick Facts
- arXiv ID: 2507.08686
- Source URL: https://arxiv.org/abs/2507.08686
- Authors: Uri Stern; Eli Corn; Daphna Weinshall
- Reference count: 40
- One-line primary result: KF consistently improves performance across datasets and architectures, with distilled KF often outperforming independently trained ensembles

## Executive Summary
This paper identifies and addresses local overfitting, where deep models forget previously learned patterns in certain regions of the data space even without global overfitting. The authors propose a two-stage method: first, Knowledge Fusion (KF) combines checkpoints from mid-training using a forgetting metric to recover lost knowledge, and second, a distilled KF ensemble compresses this ensemble into a single model to maintain inference efficiency. Extensive experiments show that KF consistently improves performance across datasets and architectures, with the distilled KF variant often outperforming both the original model and independently trained ensembles.

## Method Summary
The method consists of two phases: Phase 1 (Knowledge Fusion) tracks forget fractions across training epochs using a validation set, iteratively selects checkpoints with high forget fraction scores, and combines their probability outputs with learned weights. The forget fraction measures the proportion of test points correctly classified at mid-training epochs but misclassified at the final epoch. Phase 2 (optional distillation) trains a student model using the KF ensemble as teacher with soft target loss combined with ground-truth labels. The method achieves improved accuracy while reducing training and inference costs compared to independent ensemble training.

## Key Results
- KF achieves 0.51-4.66% improvement on clean data and 5.79-13.55% on noisy data over baseline
- Distilled KF outperforms distilled independent ensembles on most settings
- Up to 15% error reduction in noisy label settings compared to standard training
- Maintains strong accuracy while reducing training and inference costs

## Why This Works (Mechanism)

### Mechanism 1
Local overfitting occurs when models forget previously learned patterns in specific data sub-regions, even while global test accuracy continues to improve. The forget fraction (Fe) measures the proportion of test points correctly classified at epoch e but misclassified at final epoch E. This asymmetry captures degradation invisible to standard accuracy metrics.

### Mechanism 2
Mid-training checkpoints contain recoverable knowledge about "forgotten" regions that improves ensemble performance. KF iteratively selects checkpoints maximizing Fe (forgotten knowledge), combines their probability outputs with final model using learned weights (ε), and averages nearby checkpoints (window w=1) for robustness.

### Mechanism 3
Distilling the KF ensemble into a single model preserves or exceeds ensemble performance while restoring inference efficiency. Student model learns from KF ensemble's soft targets (temperature-scaled logits) combined with ground-truth labels; total loss Ltotal = α·Lsoft + (1-α)·Llabel. The regularization effect of soft targets plus checkpoint-ensemble structure yields compact models.

## Foundational Learning

- **Ensemble averaging reduces variance**: Why needed here: KF is fundamentally an ensemble of checkpoints; understanding why averaging predictions (not weights) works is essential for interpreting KF gains. Quick check: Can you explain why averaging softmax outputs differs from averaging logits or weights, and when each approach is preferred?

- **Knowledge distillation and soft targets**: Why needed here: Phase 2 relies entirely on distillation to compress the ensemble; temperature scaling and the α hyperparameter are critical design choices. Quick check: What does temperature T control in softmax, and why would a higher temperature help knowledge transfer?

- **Double descent and epoch-wise generalization**: Why needed here: The paper links local overfitting to epoch-wise double descent; understanding why test error can improve after overfitting informs checkpoint timing. Quick check: In epoch-wise double descent, what causes the second descent in test error, and how does this relate to memorizing noisy vs. clean labels?

## Architecture Onboarding

- **Component map**: Training phase (single-model training with checkpoint saving) -> KF hyperparameter phase (forget fraction computation and checkpoint selection) -> KF inference (weighted ensemble prediction) -> Optional distillation phase (student model training)

- **Critical path**: 1) Save all checkpoints during training (O(E × model_size) storage), 2) Compute forget fractions Fe on validation set for each epoch, 3) Iteratively select checkpoints maximizing forgotten knowledge, 4) (Optional) Distill ensemble into single student model

- **Design tradeoffs**: Ensemble vs. distilled: KF ensemble has O(k) inference cost; distilled has O(1) but requires additional training. Window size w: w≥1 improves robustness; w>1 dilutes selection specificity. Validation split: Paper uses half of test data; ablation shows 10% of training data also works.

- **Failure signatures**: No improvement when forget fraction Fe is near-zero; distillation regression when α is too low or temperature T is poorly tuned; overfitting to validation set if checkpoint selection is too aggressive.

- **First 3 experiments**: 1) Baseline diagnostic: Train ResNet18 on CIFAR-100; plot Fe vs. epoch to confirm local overfitting exists. 2) Minimal KF implementation: Implement Algorithm 1-2 with single checkpoint selection (k=1, w=1). 3) Noise robustness test: Inject 20% symmetric label noise; train same model; apply KF.

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that local overfitting is a generalizable phenomenon (distinct from label noise effects) has medium confidence due to limited validation of Fe as a causal indicator versus correlated artifact
- The effectiveness of Fe-guided checkpoint selection versus random selection is supported empirically but lacks direct ablation comparison in the corpus
- Distillation performance claims are high confidence for clean data settings but medium confidence for noisy settings

## Confidence

- **High**: Basic mechanism of ensemble averaging improving accuracy; feasibility of knowledge distillation with soft targets
- **Medium**: Local overfitting as distinct phenomenon; Fe-guided checkpoint selection superiority; distilled KF maintaining or exceeding ensemble performance
- **Low**: Generalization of Fe as causal forgetting metric across domains; distillation performance in noisy label settings

## Next Checks

1. Ablation study comparing Fe-guided checkpoint selection against random checkpoint selection with matched ensemble sizes
2. Correlation analysis between Fe values and label noise levels to assess whether Fe captures forgetting versus noise memorization
3. Capacity analysis of student models relative to teacher ensembles to determine if performance gains require specific size ratios