---
ver: rpa2
title: 'MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation
  in Tabular Domains'
arxiv_id: '2505.14312'
source_url: https://arxiv.org/abs/2505.14312
tags:
- datasets
- tabular
- performance
- data
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MULTI TAB, a comprehensive benchmark suite
  for multi-dimensional evaluation of tabular learning algorithms. Unlike existing
  benchmarks that rely on average-case metrics, MULTI TAB categorizes 196 publicly
  available datasets along key data characteristics (sample size, label imbalance,
  feature interaction) and evaluates 13 representative models with diverse inductive
  biases.
---

# MultiTab: A Comprehensive Benchmark Suite for Multi-Dimensional Evaluation in Tabular Domains

## Quick Facts
- arXiv ID: 2505.14312
- Source URL: https://arxiv.org/abs/2505.14312
- Reference count: 40
- Key outcome: Introduces a benchmark showing model performance is highly sensitive to data regimes, enabling principled model selection

## Executive Summary
This paper introduces MultiTab, a comprehensive benchmark suite for evaluating tabular learning algorithms across diverse data regimes. Unlike existing benchmarks that rely on average-case metrics, MultiTab categorizes 196 publicly available datasets along key characteristics (sample size, label imbalance, feature interaction) and evaluates 13 representative models with diverse inductive biases. The study reveals that model performance is highly sensitive to data regimes: for example, models using sample-level similarity excel on large datasets or with high feature correlation, while those encoding inter-feature dependencies perform best with weakly correlated features. These findings show that inductive biases do not always behave as intended, and that regime-aware evaluation is essential for understanding and improving model behavior. MultiTab enables more principled model design and offers practical guidance for selecting models tailored to specific data characteristics.

## Method Summary
The study evaluates 13 tabular models across 196 datasets from OpenML, stratified by seven statistical axes including sample size, label imbalance, and feature interaction characteristics. Models are grouped into GBDTs, NN-Simple, NN-Feature, and NN-Sample based on their inductive biases. Training uses stratified k-fold CV with TPE hyperparameter optimization (100 trials per fold), and the primary metric is normalized predictive error that scales performance relative to the best/worst model per dataset. Preprocessing involves removing columns/rows with missing values, applying QuantileTransformer to numerical features, and LabelEncoder to categorical features.

## Key Results
- Model performance is highly sensitive to data regimes, with no single model dominating across all conditions
- Models using sample-level similarity (ModernNCA) excel on large datasets with high feature correlation
- Models encoding inter-feature dependencies (FT-Transformer) perform best with weakly correlated features
- GBDTs show superior robustness in regression tasks with high label imbalance
- Simple MLPs underperform when categorical features dominate without categorical embeddings

## Why This Works (Mechanism)

### Mechanism 1: Regime-Conditioned Inductive Bias Alignment
Model efficacy is determined by the alignment between a model's architectural inductive bias (e.g., attention vs. retrieval) and specific statistical regimes of the dataset. FT-Transformer (attention-based) excels when features are weakly correlated, while retrieval-based models excel when features are highly correlated. Core assumption: dataset statistics serve as reliable proxies for underlying data geometry that specific architectures exploit.

### Mechanism 2: Robustness to Function Irregularity via Sample Similarity
Metric-learning-based models (e.g., ModernNCA) exhibit greater robustness to "function irregularity" (abrupt target shifts) than models assuming smooth functional mappings. These models form predictions based on distance to training examples in latent space, allowing them to "copy" irregular patterns from neighbors rather than fitting a smooth global curve. Core assumption: irregular patterns in tabular data often manifest locally.

### Mechanism 3: Categorical Embedding Heterogeneity Gating
The utility of embedding layers is gated by the proportion of categorical features; simple MLPs fail without them when categorical data dominates. Learnable embeddings map discrete categories to continuous vectors, stabilizing optimization. Core assumption: categorical variables represent discrete concepts that benefit from dense vector representation rather than ordinal encoding.

## Foundational Learning

- **Concept: Inductive Bias in Tabular DL** - Needed to understand why models are classified by their architectural assumptions (inter-feature vs. inter-sample dependencies). Quick check: Does a Transformer applied to tabular data typically attend across rows (samples) or columns (features)? (Answer: Columns/Features)

- **Concept: Normalized Error Metrics** - Needed because the study rejects "average-case" ranking in favor of scaling errors relative to the best/worst model per dataset. Quick check: Why is raw Log Loss insufficient for comparing model performance across datasets with vastly different sample sizes? (Answer: Scale and variance differ; normalization centers performance relative to difficulty)

- **Concept: Stratified Evaluation (Sub-categories)** - Needed to understand the core contribution of dividing datasets by axes like "Feature-to-sample ratio" and "Entropy ratio." Quick check: If a model performs well on "Small" datasets but fails on "Large" ones, what does the "Overall" average ranking hide? (Answer: It hides the regime-sensitivity)

## Architecture Onboarding

- **Component map:** 196 Datasets (OpenML) → Preprocessing (Quantile transform, LabelEncoder) → Router: Statistical Analyzer (calculates 7 axes) → Assigns Sub-category → Evaluation Layer (13 Models trained with TPE) → Output: Normalized Error per sub-category

- **Critical path:**
  1. Profile your data: Calculate the 7 statistical axes defined in Table 1 for your custom dataset
  2. Identify the regime: e.g., "High Feature-to-Sample Ratio" + "Low Correlation"
  3. Select model: Consult Figure 1/Table 2. For Low Correlation → Prefer NN-Feature (FT-Transformer). For High Correlation → Prefer NN-Sample (ModernNCA)

- **Design tradeoffs:**
  - NN-Sample (ModernNCA/TabR): High performance on large/correlated data, but sensitive to data scarcity
  - NN-Feature (FT-Transformer): Robust to high dimensionality/low correlation, but computationally expensive and struggles with high irregularity
  - GBDTs (XGBoost/CatBoost): Safe defaults for regression and high label imbalance, but may saturate on massive, regular datasets

- **Failure signatures:**
  - ResNet/MLPs: Underperform significantly on high cardinality categorical data due to lack of specific embeddings
  - SAINT: Despite complex architecture, often lags behind specialized models, potentially due to optimization difficulty

- **First 3 experiments:**
  1. Baseline Calibration: Run MultiTab suite on local hardware to verify normalized error ranges against reported logs
  2. Regime Stress Test: Upsample a "Small" dataset to "Large" and compare performance degradation/growth of GBDTs vs. ModernNCA
  3. Ablation on Embeddings: Run MLP vs. MLP-C on datasets with >60% categorical features to reproduce performance gap

## Open Questions the Paper Calls Out

### Open Question 1
How can neural architectures be modified to match the robustness of GBDTs in regression tasks with highly skewed target distributions? The authors observe that while neural networks are competitive in classification, a notable performance difference remains in high-skew regression regimes. This is unresolved because current neural models exhibited performance degradation under high target skewness, whereas tree-based models like CatBoost showed improved performance.

### Open Question 2
What novel inductive biases are required to effectively handle datasets with high function irregularity? Section 4.2.6 states that "function irregularity remains a persistent challenge" and that existing inductive biases do not provide consistent performance gains. This is unresolved because models designed for irregularity showed substantial performance degradation, and even XGBoost exhibited performance drops.

### Open Question 3
How do regime-specific strengths of current tabular models translate to pretraining-based or AutoML-driven scenarios? The study is limited to supervised settings and suggests future work may extend this framework to pretraining-based or AutoML-driven scenarios. This is unresolved because the study focused on training specific architectures from scratch.

### Open Question 4
Can automated, data-driven model selection be reliably implemented based on the dataset characteristics identified in MultiTab? While the paper provides a manual diagnostic tool, it does not propose or validate a mechanism for automating this selection process based on the defined statistical axes. This would be resolved by a meta-learning model that successfully predicts the optimal model architecture with higher accuracy than random selection.

## Limitations
- Reliance on OpenML datasets may introduce sampling bias not representative of industrial-scale tabular problems
- Statistical axes used for stratification (particularly inter-feature correlation) are proxy measures that may not perfectly capture true data geometry
- Claims about robustness to function irregularity via sample similarity are based on limited ablation studies without systematic characterization of "irregular" patterns

## Confidence

**High**: Claims about regime-dependent performance differences are well-supported by extensive empirical results across 196 datasets.

**Medium**: Claims about specific architectural mechanisms are supported but rely on correlational analysis rather than causal intervention studies.

**Low**: Claims about robustness to function irregularity via sample similarity are based on limited ablation studies without systematic characterization.

## Next Checks

1. **Out-of-distribution test**: Apply MultiTab-selected models to datasets with extreme characteristics (e.g., >1M samples, >100K features) not present in the benchmark to verify scaling predictions.

2. **Causal intervention**: Systematically manipulate feature correlation levels in synthetic datasets and measure corresponding performance shifts to validate the correlation-architecture interaction mechanism.

3. **Real-world deployment**: Track model selection accuracy in production tabular problems where ground truth performance is known, comparing MultiTab recommendations against standard average-case benchmarks.