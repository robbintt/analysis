---
ver: rpa2
title: 'CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic
  Music Generation'
arxiv_id: '2508.19603'
source_url: https://arxiv.org/abs/2508.19603
tags:
- music
- lexical
- lexicon
- generation
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CompLex, an automatically constructed music
  theory lexicon containing 37,432 lexical items across 9 categories, derived from
  just 9 manually input keywords and 5 prompt templates. The authors develop LexConstructor,
  a multi-agent algorithm that generates the lexicon while detecting and mitigating
  hallucinations through a Question-Answering communication strategy among specialized
  agents.
---

# CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation

## Quick Facts
- arXiv ID: 2508.19603
- Source URL: https://arxiv.org/abs/2508.19603
- Reference count: 10
- Authors: Zhejing Hu; Yan Liu; Gong Chen; Bruce X. B. Yu
- Key outcome: Multi-agent algorithm constructs 37,432-item music theory lexicon from 9 keywords, improving text-to-music generation metrics

## Executive Summary
CompLex introduces an automatically constructed music theory lexicon that significantly enhances text-to-music generation by providing structured symbolic constraints. The system employs a two-stage multi-agent algorithm (LexConstructor) that generates lexicon items while detecting and mitigating hallucinations through a Question-Answering communication strategy. When integrated with state-of-the-art models like Text2MIDI, MusicGen, and Suno, CompLex demonstrates substantial improvements in CLAP alignment, mood accuracy, and genre accuracy metrics. The lexicon achieves high quality scores across completeness, accuracy, non-redundancy, and executability measures.

## Method Summary
The CompLex system uses a two-stage multi-agent approach called LexConstructor. Stage I involves three specialized agents (Category Architect, Item Builder, Property Designer) that create the lexicon outline from a reference dataset. Stage II employs a Supervisor Agent that generates targeted questions for Value Explorer Agents to populate property-value pairs using a QA communication strategy. This design specifically addresses LLM hallucination by forcing a verification process rather than direct generation. The system scales from minimal seed keywords to a comprehensive 37,432-item lexicon by leveraging tool-based extraction from the MidiCaps dataset.

## Key Results
- CompLex achieves 0.85 Pitch Value Accuracy with QA communication vs. 0.28 without, demonstrating effective hallucination reduction
- Integration with three state-of-the-art text-to-music models shows significant performance improvements across CLAP alignment, mood accuracy, and genre accuracy metrics
- The lexicon maintains high quality with completeness of 0.96, accuracy of 0.88, non-redundancy of 0.94, and executability of 0.93
- Multi-agent design consistently outperforms single-agent approaches in generating high-quality lexicons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A multi-agent Question-Answering (QA) communication strategy reduces factual hallucinations in property-value generation compared to single-agent prompting.
- Mechanism: A "Supervisor Agent" generates targeted questions for "Value Explorers," forcing a retrieval and reasoning process rather than direct generation. The supervisor evaluates responses to filter out plausible but incorrect musical facts.
- Core assumption: The decomposition of knowledge generation into questioning and answering roles creates a verification loop that lowers the probability of confident errors inherent in single-pass generation.
- Evidence anchors:
  - [abstract] "A new multi-agent algorithm is proposed to automatically detect and mitigate hallucinations."
  - [section 6.3] Table 3 shows QA strategy achieves 0.85 Pitch Value Accuracy vs. 0.28 without communication.
  - [corpus] Neighbors focus on general music generation or alignment, not specific multi-agent hallucination reduction strategies in lexicons.
- Break condition: If the "Supervisor" and "Explorers" share the same flawed underlying knowledge representation, they may converge on a confident but incorrect consensus (hallucination amplification).

### Mechanism 2
- Claim: Enhancing text prompts with structured lexicon items improves text-to-music alignment by providing explicit symbolic constraints.
- Mechanism: The system intercepts vague user requests and retrieves specific, theory-grounded property-value pairs, transforming semantic abstractions into structured control signals that bridge the gap between natural language and precise symbolic representations.
- Core assumption: The generative models have sufficient latent capacity to adhere to these specific constraints when explicitly provided in the prompt context.
- Evidence anchors:
  - [abstract] "CompLex demonstrates significant performance improvements... enhancing metrics like CLAP alignment, mood accuracy, and genre accuracy."
  - [section 6.1] "CompLex-Enhanced method consistently achieves higher performance... driven by the lexicon's ability to provide specific and structured guidance."
  - [corpus] "ComposeOn Academy" and "MusicAIR" support general theory-based guidance but don't specifically validate the lexicon-to-prompt mechanism.
- Break condition: If lexicon property values conflict with model priors (e.g., suggesting a tempo the model associates with a different mood), guidance may be ignored or cause output distortion.

### Mechanism 3
- Claim: Specialized tool usage allows for scalable lexicon construction (37k+ items) from minimal seeds (9 keywords) while maintaining non-redundancy.
- Mechanism: "Item Builder" agents utilize predefined extraction functions to process external datasets rather than relying solely on parametric memory, avoiding memory limitations and repetitive looping seen in direct LLM generation.
- Core assumption: The reference dataset is sufficiently clean and comprehensive to serve as "ground truth" for item extraction.
- Evidence anchors:
  - [abstract] "...comprising 37,432 items derived from just 9 manually input category keywords and 5 sentence prompt templates."
  - [section 4.1] "Abuilder leverages the pre-defined function Extract lexical item(c, Dref) to assist in item extraction."
  - [corpus] No direct corpus evidence for this specific "seed-to-lexicon" scaling mechanism.
- Break condition: If extraction tools encounter noisy or unstructured data, the lexicon will propagate "garbage in, garbage out" errors at scale.

## Foundational Learning

### Concept: LLM Hallucination vs. Factual Grounding
- Why needed here: The core problem LexConstructor solves is the LLM's tendency to invent plausible-sounding but incorrect music theory facts.
- Quick check question: Can you explain why asking an LLM to "list all notes in C Major" is less reliable than asking it to "verify if D# is in C Major"?

### Concept: Symbolic Music Representation (MIDI)
- Why needed here: The system bridges text and music using MIDI properties. Understanding that MIDI is a discrete protocol is crucial for the "Executability" metric.
- Quick check question: What is the difference between a "C4" note in symbolic MIDI (e.g., value 60) and a "C4" in an audio waveform?

### Concept: Role-Playing Multi-Agent Systems
- Why needed here: The architecture depends on distinct personas (Supervisor vs. Explorer). Understanding how prompt engineering assigns "roles" to generic models is essential.
- Quick check question: How does separating the "questioner" role from the "answerer" role change the output distribution compared to a single model self-reflecting?

## Architecture Onboarding

### Component map
Category Architect -> Item Builder (using Extract_lexical_item tool) -> Property Designer -> Supervisor Agent <-> Value Explorer Agents -> CompLex Database

### Critical path
The **QA Communication Loop** in Stage II is critical. If this fails, hallucinations enter the database. The "Process One Item" function is also critical to avoiding context window overflows.

### Design tradeoffs
**Cost vs. Accuracy**: The multi-agent QA loop is computationally expensive (multiple LLM calls per item) compared to direct prompting, but necessary to hit the ~0.88 accuracy reported.

### Failure signatures
- **Redundancy Loop**: Single agents generating "Note: C", "Note: C", "Note: C" (mitigated by Stage I tools)
- **Circular Agreement**: Agents quickly agreeing on a wrong frequency (e.g., 440Hz for the wrong note)

### First 3 experiments
1. **Unit Test the QA Loop**: Isolate one lexical item (e.g., "C#6"). Run the Supervisor/Explorer loop and verify if the generated MIDI value (85) matches ground truth without looking at the reference dataset directly.
2. **Ablation on Reference Data**: Run the Item Builder on a very small, controlled MIDI subset (e.g., only drum tracks) to verify if the Category Architect correctly limits the lexicon to percussion concepts.
3. **Integration Test**: Feed a generic prompt ("Sad song") to the Text2MIDI model, first without CompLex, then with CompLex. Compare the CLAP score and Mood Accuracy delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the LexConstructor framework be effectively adapted to capture and structure expressive performance elements (e.g., dynamics, articulation, timing deviations) in addition to compositional structure?
- Basis in paper: [explicit] The conclusion states, "Currently, CompLex focuses on the structure of music composition, in the future, we plan to expand its scope to encompass expressive elements of music performance."
- Why unresolved: The current study focused on compositional attributes, leaving the nuanced, continuous values of performance expression unexplored.
- What evidence would resolve it: A new lexicon generated by the current framework that quantifies performance attributes with high completeness and accuracy, validated through performance-based generation tasks.

### Open Question 2
- Question: Does the LexConstructor framework maintain high accuracy and low hallucination rates when implemented with smaller, open-source Large Language Models (LLMs), or is it dependent on the specific reasoning capabilities of GPT-4o?
- Basis in paper: [inferred] The experiment section explicitly notes that "for lexicon content generation, we utilize GPT-4o," and while GPT-3.5-Turbo was tested for baselines, the superior performance is tightly linked to the advanced backbone.
- Why unresolved: It is unclear if the complex multi-agent QA communication strategy requires the high parameter count and reasoning ability of proprietary models like GPT-4o to function correctly.
- What evidence would resolve it: A comparative analysis of lexicon quality when running LexConstructor on open-source models (e.g., Llama 3, Mistral) versus GPT-4o.

### Open Question 3
- Question: How does the reliance on the MidiCaps reference dataset (derived from the Lakh MIDI dataset) limit the lexicon's coverage of contemporary music genres or non-Western music theories?
- Basis in paper: [inferred] The Implementation Details specify the "Reference Dataset" as MidiCaps. The Lakh MIDI dataset is known to have biases towards specific eras and styles.
- Why unresolved: The paper evaluates completeness relative to music theory generally but does not analyze the bias or temporal limitations inherited from the source corpus.
- What evidence would resolve it: An analysis of "missing items" in CompLex compared to a modern music taxonomy, or an evaluation of generation quality when using prompts for genres underrepresented in the Lakh MIDI dataset.

## Limitations
- The lexicon construction pipeline is highly dependent on the quality of the MidiCaps reference dataset and specific prompt templates, which are not fully specified in the main text
- The QA communication strategy, while effective in reducing hallucinations, is computationally expensive due to multiple LLM calls per item
- Evaluation metrics for lexicon quality are introduced but their exact implementation protocols are not detailed, limiting independent verification

## Confidence

**High Confidence**: The CompLex lexicon demonstrates significant improvements in text-to-music alignment metrics (CLAP, mood accuracy, genre accuracy) when integrated with state-of-the-art models, as these results are directly reported with specific values.

**Medium Confidence**: The multi-agent QA strategy effectively reduces hallucinations compared to single-agent approaches, supported by the Pitch Value Accuracy comparison (0.85 vs 0.28), though the specific consensus criteria remain unclear.

**Low Confidence**: The exact prompt templates and function implementations for the LexConstructor agents are not provided, making faithful reproduction challenging.

## Next Checks
1. **Replicate the QA Hallucination Reduction**: Isolate the Pitch Value generation for a set of notes using both the Supervisor/Explorer QA loop and a direct single-agent prompt, comparing hallucination rates against ground truth MIDI values.
2. **Test Lexicon Scalability Limits**: Attempt to construct a lexicon from a minimal seed (e.g., only 3 keywords) and monitor the Point of Redundancy Onset to validate the reported 50-item threshold for single-agent models.
3. **Validate Property-Value Consensus**: Implement the QA strategy and instrument the process to log the specific questions generated by the Supervisor and the divergence/convergence of Value Explorer answers to understand the consensus mechanism.