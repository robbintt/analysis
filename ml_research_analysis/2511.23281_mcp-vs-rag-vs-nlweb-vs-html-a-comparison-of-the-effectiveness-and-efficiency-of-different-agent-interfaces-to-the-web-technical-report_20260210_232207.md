---
ver: rpa2
title: 'MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency
  of Different Agent Interfaces to the Web (Technical Report)'
arxiv_id: '2511.23281'
source_url: https://arxiv.org/abs/2511.23281
tags:
- html
- agents
- nlweb
- product
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically compares four architectures for LLM-based
  web agents: HTML browsing, Retrieval-Augmented Generation (RAG), Model Context Protocol
  (MCP), and NLWeb. The study introduces a controlled testbed with four simulated
  e-shops, each offering products via HTML, MCP, and NLWeb interfaces.'
---

# MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)

## Quick Facts
- arXiv ID: 2511.23281
- Source URL: https://arxiv.org/abs/2511.23281
- Reference count: 15
- Key outcome: RAG, MCP, and NLWeb agents outperform HTML browsing with F1 scores rising from 0.67 to 0.75-0.77 and token usage dropping from 241k to 47-140k per task.

## Executive Summary
This technical report systematically compares four architectures for LLM-based web agents: HTML browsing, Retrieval-Augmented Generation (RAG), Model Context Protocol (MCP), and NLWeb. Using a controlled testbed with four simulated e-shops, the study evaluates agents built for each architecture on identical tasks including product searches, price comparisons, and checkout processes. The evaluation employs GPT-4.1, GPT-5, GPT-5 mini, and Claude Sonnet 4 as underlying models. Results demonstrate that API-based and RAG interfaces provide significant advantages over traditional HTML browsing for web agents, with RAG using GPT-5 achieving the best overall performance.

## Method Summary
The study introduces WebMall, a benchmark with 4 simulated e-shops running locally, containing 4,421 product offers from Common Crawl. Agents for each architecture perform 91 tasks across 4 categories: Specific Product Search, Vague Search, Cheapest Product Search, and Transactional tasks. The HTML agent uses BrowserGym with AXTree observer, while RAG employs Elasticsearch indexing with OpenAI small embeddings and unstructured preprocessing. MCP and NLWeb agents use JSON-RPC via MCP servers. Four models (GPT-4.1, GPT-5, GPT-5 mini, Claude Sonnet 4) are tested across all 16 agent-model combinations, measuring completion rate, F1 score, runtime, token usage, and API cost.

## Key Results
- RAG, MCP, and NLWeb agents outperform HTML browsing with F1 scores rising from 0.67 to 0.75-0.77
- Token usage drops from 241k for HTML to 47-140k per task for other architectures
- Runtime decreases from 291 seconds to 50-62 seconds per task
- RAG with GPT-5 achieves the best overall performance (F1=0.87, completion rate=0.79)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured interfaces (RAG, MCP, NLWeb) improve efficiency by stripping non-essential markup, significantly reducing input token consumption compared to raw HTML parsing.
- Mechanism: The HTML agent must process the entire accessibility tree (AXTree) of a page, while RAG pre-filters content and MCP/NLWeb receive targeted JSON payloads, reducing the "noise" the LLM must process per step.
- Core assumption: The cost and latency of an LLM agent are primarily driven by the volume of input tokens per step and the total number of steps.
- Evidence anchors: Token usage falls from about 241k for HTML to between 47k and 140k per task. These improvements are mainly from reducing input tokens and avoiding navigation. Most efficiency gains come from reducing input tokens rather than output length.

### Mechanism 2
- Claim: Reducing the task to direct API calls or search queries minimizes "navigational drift" and error accumulation compared to multi-step interaction.
- Mechanism: HTML agents must succeed at a chain of actions (locate search box → input text → submit → parse results). A failure at any step fails the task. RAG and API agents issue a single semantic query.
- Core assumption: The probability of task success decreases as the number of required interaction steps increases.
- Evidence anchors: Runtime decreases from 291 seconds to between 50 and 62 seconds. HTML interaction introduces substantial overhead. Even a simple search requires navigating to the site, locating and filling a form.

### Mechanism 3
- Claim: RAG provides superior retrieval coverage for open-ended exploration, while API-based methods provide higher precision but suffer from "reasoning" errors on retrieved items.
- Mechanism: RAG allows iterative broad queries against a unified index. MCP/NLWeb require per-shop queries, limiting breadth. However, when RAG fails, it is often due to the item not being retrieved, whereas API agents often retrieve the item but fail to select it.
- Core assumption: A unified index (RAG) enables broader context discovery than partitioned, per-shop API queries.
- Evidence anchors: RAG contributes the largest share of non-retrieved errors. In NLWeb, most false negatives are retrieved, meaning the correct product was retrieved but not recognized. NLWeb achieves the highest F1 in vague search, whereas RAG wins on cheapest-product search.

## Foundational Learning

- **Concept:** Accessibility Tree (AXTree)
  - **Why needed here:** This is the input representation for the HTML agent. Understanding that the agent "sees" a simplified structural tree rather than pixels is crucial for diagnosing why HTML browsing is token-intensive and brittle.
  - **Quick check question:** Can an HTML agent acting on an AXTree distinguish between a visible button and a hidden one if both are present in the tree?

- **Concept:** Model Context Protocol (MCP)
  - **Why needed here:** MCP is the transport layer for the API-based agents in this study. It standardizes how tools are discovered and invoked (JSON-RPC), contrasting with the "proprietary API" approach of the MCP architecture vs. the "standardized query" of NLWeb.
  - **Quick check question:** Does MCP enforce a specific schema for the response data, or just the invocation protocol? (Hint: The paper notes MCP responses are heterogeneous).

- **Concept:** False Negative Types (Non-retrieved vs. Retrieved)
  - **Why needed here:** The error analysis distinguishes between system failures (didn't find the item) and reasoning failures (found it but didn't select it). This distinction is vital for debugging specific architectures.
  - **Quick check question:** If an agent retrieves the correct product but fails to add it to the cart because it misinterpreted a "out of stock" label, is this a Non-retrieved or Retrieved error?

## Architecture Onboarding

- **Component map:** WebMall testbed -> HTML agent (BrowserGym + AXTree) -> RAG agent (Elasticsearch + unstructured) -> MCP/NLWeb agents (MCP Host + JSON-RPC)

- **Critical path:**
  1. Environment Setup: Stand up the 4 e-commerce shops (HTML/API endpoints)
  2. RAG Indexing: Crawl shops, chunk text, embed vectors (OpenAI small), load into Elasticsearch
  3. Tool Integration: Define Python functions (RAG) or MCP Tools (MCP/NLWeb) for AddToCart and Checkout

- **Design tradeoffs:**
  - RAG vs. NLWeb: RAG offers the best cost/performance ratio (GPT-5-mini) but requires maintaining an index (freshness lag). NLWeb/MCP offers real-time data but requires the site to maintain the API and handle higher reasoning token costs.
  - Standardization: NLWeb uses schema.org for responses (easier aggregation), while MCP uses shop-specific schemas (higher interpretation load on the agent).

- **Failure signatures:**
  - HTML: High step count (23+), timeouts, or navigation loops
  - RAG: "Non-retrieved" errors (F1=0) on vague tasks where the query strategy was too narrow
  - MCP/NLWeb: "Retrieved but not selected" errors or "Product fails requirements" (e.g., returning a 16GB RAM stick when 32GB was requested) due to lazy constraint checking in the final selection step

- **First 3 experiments:**
  1. Baseline HTML Run: Execute a "Specific Product Search" task with the HTML agent to verify the 291s average runtime and high token usage
  2. RAG Efficiency Check: Run the same task with the RAG agent to confirm the drop to ~50s runtime and verify the "2-6 queries" pattern
  3. Constraint Stress Test: Run a "Cheapest Product Search" task (highest difficulty) across all 4 architectures to verify the drop in F1 scores (to ~0.60-0.68) and observe the specific "price comparison" error modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the relative effectiveness and efficiency of HTML, RAG, MCP, and NLWeb agents generalize to non-e-commerce domains such as travel booking, social media platforms, or enterprise productivity tools?
- Basis in paper: The authors state their testbed is limited to "multi-shop online shopping" using WebMall, and acknowledge that API-based architectures "requires additional development and maintenance effort, making widespread adoption uncertain."
- Why unresolved: The controlled experiments cover only four simulated e-shops with 4,421 products in the PC components and electronics categories. Whether the 9-point F1 advantage and 5× speedup for RAG/API-based interfaces holds across domains with different interaction patterns (e.g., calendar scheduling, form-heavy workflows) remains unknown.
- What evidence would resolve it: A replication of the four-architecture comparison on benchmark task sets from at least two additional domains (e.g., travel, social, enterprise), using the same metrics and model configurations.

### Open Question 2
- Question: Why does NLWeb's standardized schema.org response format not yield a measurable effectiveness advantage over heterogeneous MCP APIs, despite reducing interface heterogeneity?
- Basis in paper: The authors hypothesize that NLWeb "might make it easier for agents to understand search results due to all shops using a shared schema" (Section 2), yet Table 4 shows NLWeb achieves only marginally higher F1 than MCP (0.76 vs 0.75 overall) with no statistically significant gap.
- Why unresolved: The paper does not analyze whether agents actually exploit schema consistency during cross-shop reasoning, or whether the underlying embedding-based retrieval (shared across RAG, MCP, and NLWeb) dominates any schema-related benefits.
- What evidence would resolve it: An ablation study isolating schema standardization from retrieval method, combined with an analysis of cross-shop aggregation steps to determine whether agents leverage shared vocabulary during result fusion.

### Open Question 3
- Question: What architectural or prompting interventions could reduce the ~25% of false positives caused by agents accepting products that fail specific requirement constraints (e.g., memory size, price thresholds)?
- Basis in paper: The error analysis (Section 5) identifies "product fails requirements" as the largest single false positive class (~25%), noting "models show a tendency to accept products as relevant that meet most requirements but differ in a single key attribute." The authors suggest "applying lightweight validation checks" as an improvement area.
- Why unresolved: The paper documents the error pattern but does not test whether post-hoc verification, chain-of-thought constraint checking, or structured output validation could reduce near-miss selections without harming recall.
- What evidence would resolve it: A comparative experiment adding explicit constraint verification steps (e.g., extracting and validating attributes against requirements before final selection) to the existing agents, measuring F1 impact on constraint-heavy tasks.

### Open Question 4
- Question: Under what task complexity conditions do non-reasoning models (e.g., GPT-4.1) provide a better cost-performance trade-off than reasoning-enabled models for web agent workflows?
- Basis in paper: Section 4.3 notes that GPT-4.1 "consumes substantially fewer tokens and finishes tasks more quickly than the GPT-5 variants" with "moderately lower" effectiveness on well-specified tasks but wider gaps on vague exploration tasks, highlighting "a general design option: non-reasoning models could deliver competitive results at higher throughput."
- Why unresolved: The paper reports aggregate differences but does not characterize the task feature boundary (e.g., query ambiguity level, reasoning depth required) where reasoning models become necessary for acceptable performance.
- What evidence would resolve it: A fine-grained analysis correlating task characteristics (ambiguity scores, constraint count, required inference steps) with model-specific performance gaps, identifying threshold values where reasoning capabilities provide statistically significant benefits.

## Limitations

- Testbed Representativeness: The study uses a synthetic e-commerce testbed with 4 simulated shops rather than real-world websites, limiting generalizability to production website diversity.
- Model Availability: Results using GPT-5 and GPT-5 mini models cannot be independently verified as these models are not publicly available.
- Error Attribution Complexity: The distinction between "retrieved" and "non-retrieved" false negatives can be ambiguous, as some "retrieved" errors may stem from inadequate context windows rather than pure reasoning issues.

## Confidence

- High Confidence: API-based architectures consistently outperform HTML browsing on both effectiveness and efficiency metrics; token usage reduction from ~241k to 47-140k per task is robust; runtime improvements from ~291s to 50-62s per task are well-supported.
- Medium Confidence: RAG with GPT-5 achieving the best overall performance may be architecture-specific rather than universally optimal; the specific ranking of architectures holds within the controlled testbed but may vary with different task distributions.
- Low Confidence: Generalizability to real-world web environments and non-e-commerce domains; long-term maintenance implications of RAG indexing vs. real-time API freshness trade-offs.

## Next Checks

1. **Real-World Replication:** Deploy the same architectures against 2-3 real e-commerce websites with actual product catalogs to verify the token usage and effectiveness improvements hold outside the controlled testbed environment.

2. **Cross-Domain Testing:** Evaluate the architectures on non-e-commerce tasks (e.g., travel booking, news aggregation) to assess whether the observed performance patterns generalize beyond product search and comparison scenarios.

3. **Longitudinal Performance:** Run the same task sets after 30 days with RAG agents to measure how index staleness affects retrieval performance compared to real-time API responses from MCP/NLWeb architectures.