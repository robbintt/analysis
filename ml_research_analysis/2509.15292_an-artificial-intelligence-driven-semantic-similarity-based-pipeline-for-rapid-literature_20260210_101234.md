---
ver: rpa2
title: An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid
  Literature
arxiv_id: '2509.15292'
source_url: https://arxiv.org/abs/2509.15292
tags:
- papers
- literature
- similarity
- semantic
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an automated literature review pipeline that
  uses semantic similarity to efficiently retrieve and filter relevant academic papers.
  The system generates keywords from a paper's title and abstract, fetches papers
  from arXiv, and ranks them using transformer-based embeddings and cosine similarity.
---

# An Artificial Intelligence Driven Semantic Similarity-Based Pipeline for Rapid Literature

## Quick Facts
- **arXiv ID**: 2509.15292
- **Source URL**: https://arxiv.org/abs/2509.15292
- **Reference count**: 25
- **Primary result**: Automated literature review pipeline using semantic similarity to retrieve and rank arXiv papers, with TF-IDF and all-MiniLM-L6-v2 achieving retrieval counts of 19 and 20 papers respectively under IQR-based thresholding.

## Executive Summary
This paper presents an automated pipeline for rapid literature review that leverages semantic similarity to efficiently retrieve and filter relevant academic papers from arXiv. The system generates keywords from input paper titles and abstracts using an LLM, fetches candidate papers, and ranks them using transformer-based embeddings and cosine similarity. Three embedding models were evaluated: TF-IDF, all-MiniLM-L6-v2, and Specter2. The pipeline achieves scalable literature review with low computational overhead, offering a promising alternative to traditional systematic reviews. However, the approach lacks ground-truth relevance labels for validation and faces challenges with domain-specific terminology and score saturation in scientific embeddings.

## Method Summary
The pipeline takes a paper's title and abstract as input, generates 5-10 keywords using gemini-2.0-flash, and fetches up to 20 papers per keyword from arXiv. Candidate papers undergo deduplication, then all papers (input and candidates) are embedded using all-MiniLM-L6-v2 (384-dimensional). Cosine similarity scores are computed and filtered using an IQR-based threshold (Q3 + 0.5×IQR). Relevant papers are processed with PyMuPDF for PDF text extraction using regex patterns for sections, then summarized into structured JSON via Gemini. The system outputs BibTeX entries and synthesized literature review paragraphs.

## Key Results
- TF-IDF model achieved 19 retrievals with threshold 0.204, range [0.010, 0.294]
- all-MiniLM-L6-v2 retrieved 20 papers with threshold 0.659, range [0.273, 0.905]
- Specter2 showed score clustering near 0.9 (range 0.756-0.945, skewness -0.963) requiring stricter thresholds
- System processes papers into structured summaries with problem_statement, methodology, key_findings, and conclusion_recommendations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense vector embeddings enable semantic matching beyond lexical overlap.
- Mechanism: The pipeline uses all-MiniLM-L6-v2 to generate 384-dimensional embeddings for both the input query (derived from keywords) and candidate papers, then computes cosine similarity to rank semantic closeness.
- Core assumption: Semantic similarity in embedding space correlates with research relevance (not empirically validated in the paper).
- Evidence anchors:
  - [abstract]: "using transformer-based embeddings and cosine similarity...ranks them based on their semantic closeness to the input"
  - [section III.C]: "The sentence transformer model all-MiniLM-L6-v2 was used to generate 384-dimensional dense vector embeddings...The cosine similarities between the embeddings...were then calculated"
  - [corpus]: Weak direct evidence—neighbor papers mention semantic search but not this specific embedding choice.
- Break condition: Domain-specific terminology or methodological nuances not captured by general-purpose embeddings.

### Mechanism 2
- Claim: IQR-based thresholding adapts to variable similarity score distributions without assuming normality.
- Mechanism: Computes Q3 + 0.5×IQR from the similarity score distribution to set a conservative, data-driven cutoff for paper selection.
- Core assumption: Papers above this statistical threshold are more likely to be relevant (heuristic, not ground-truth validated).
- Evidence anchors:
  - [abstract]: "A statistical thresholding approach is then applied to filter relevant papers"
  - [section III.C]: "The IQR method is a robust, distribution-free approach...Q3 + 0.5×IQR to select only the highest-scoring papers"
  - [corpus]: No direct corpus validation for this thresholding formula.
- Break condition: Highly skewed or saturated distributions (e.g., Specter2's clustering near 0.9) cause false positives or missed papers.

### Mechanism 3
- Claim: LLM-based keyword generation and structured summarization reduce manual overhead.
- Mechanism: gemini-2.0-flash extracts 5-10 keywords from title/abstract, then summarizes filtered papers into structured JSON with problem_statement, methodology, key_findings, conclusion_recommendations.
- Core assumption: LLM outputs faithfully capture the paper's core concepts without hallucination.
- Evidence anchors:
  - [section III.A]: "5 to 10 keywords were generated using a LLM, gemini-2.0-flash...to capture both explicit and semantically related terms"
  - [section III.E]: "structured the summaries under the summary key, which contained four distinct categories"
  - [corpus]: Limited—no direct corpus evidence on this LLM workflow.
- Break condition: Hallucinated keywords or summaries distort retrieval and synthesis.

## Foundational Learning

### Concept: Transformer embeddings & cosine similarity
- Why needed: Core retrieval mechanism; embeddings map text to dense vectors where cosine similarity measures directional alignment.
- Quick check: If two papers score 0.82 cosine similarity, what does that imply about their relationship vs. a 0.35 score?

### Concept: IQR-based outlier detection
- Why needed: Threshold adapts to score distributions; understanding Q3 and IQR prevents misinterpreting fixed cutoffs.
- Quick check: Given scores [0.12, 0.28, 0.45, 0.63, 0.79], estimate Q3 and what Q3 + 0.5×IQR would yield.

### Concept: LLM capabilities & failure modes
- Why needed: LLMs drive keyword extraction and summarization; recognizing hallucination risk is critical.
- Quick check: Name two ways an LLM summary could misrepresent a paper's findings.

## Architecture Onboarding

### Component map
Input (title + abstract) -> Keyword Generator (gemini-2.0-flash) -> arXiv API Fetcher (max 20 papers/keyword) -> Deduplication -> Embedding Engine (all-MiniLM-L6-v2, 384-dim) -> Cosine Similarity -> IQR Threshold (Q3 + 0.5×IQR) -> PDF Text Extraction (PyMuPDF + regex) -> Structured Summarizer (gemini-2.0-flash) -> Citation/Contribution Tagger -> Literature Review Synthesis -> Output (BibTeX + review paragraph)

### Critical path
Keywords -> Fetch -> Embed -> Threshold -> Extract -> Summarize -> Synthesize

### Design tradeoffs
TF-IDF offers lexical precision but poor semantics; all-MiniLM-L6-v2 balances coverage vs. domain specificity; Specter2 captures scientific semantics but suffers score saturation (range 0.756-0.945, skewness -0.963)

### Failure signatures
TF-IDF—low similarity range [0.010, 0.294], misses conceptual matches; Specter2—score clustering requires extremely high thresholds, risking false positives; IQR threshold may exclude lexically distant but relevant papers

### First 3 experiments
1. Threshold sensitivity: Run the pipeline on 5 diverse queries with both Q3 + 0.5×IQR and Q3 + 1.0×IQR; compare retrieval counts and spot-check top-5 relevance.
2. Model comparison: Embed the same corpus with TF-IDF, all-MiniLM-L6-v2, and Specter2; visualize score distributions and identify where Specter2 saturates.
3. Domain robustness: Test on papers from three fields (e.g., NLP, materials science, medicine); check whether general-purpose embeddings underperform on specialized terminology.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adaptive thresholding mechanisms effectively mitigate the score saturation issues observed in scientific embedding models like Specter2?
- Basis in paper: [explicit] The authors state they "plan to focus on implementing adaptive thresholding to dynamically adjust based on score distribution characteristics" to address the rigid thresholding limitations.
- Why unresolved: The current static IQR-based method (Q3 + 0.5×IQR) failed to handle the tightly clustered high scores of Specter2, requiring "extremely precise threshold" calibration that varied by model.
- What evidence would resolve it: A comparative study showing that a dynamic thresholding algorithm maintains consistent precision and recall across TF-IDF, MiniLM, and Specter2 without manual re-calibration.

### Open Question 2
- Question: To what extent does incorporating human-in-the-loop evaluation align automated relevance scores with qualitative research needs?
- Basis in paper: [explicit] The paper notes that "Incorporating human-in-the-loop evaluation will provide qualitative insights and a ground-truth basis for refining automated decisions."
- Why unresolved: The current system operates without ground-truth labels, relying solely on cosine similarity distributions which may not reflect actual utility to a human researcher.
- What evidence would resolve it: Results from a user study where expert relevance judgments are correlated with pipeline rankings to validate the filtering logic.

### Open Question 3
- Question: Does fine-tuning embedding models on specialized corpora significantly improve retrieval performance in domains with high technical jargon?
- Basis in paper: [explicit] The authors identify a need to investigate "domain adaptation and model fine-tuning (e.g., on specialized corpora) to better capture field-specific semantics."
- Why unresolved: The evaluation showed that general-purpose models like all-MiniLM-L6-v2 lacked domain-specific tuning, while scientific models like Specter2 suffered from saturation, leaving a performance gap for specialized fields.
- What evidence would resolve it: Benchmarking the pipeline on distinct domains (e.g., medicine or chemistry) using a fine-tuned model versus a general model, measuring improvements in retrieval precision.

### Open Question 4
- Question: Can integrating citation network data (e.g., co-citation patterns) enhance relevance scoring beyond simple semantic similarity?
- Basis in paper: [explicit] Future work includes "Integration with citation network data sources such as Semantic Scholar [to] enable network-based relevance scoring... which may enhance both retrieval accuracy."
- Why unresolved: The current pipeline relies entirely on semantic text similarity, potentially missing influential papers that use different terminology but are structurally important in the literature.
- What evidence would resolve it: An ablation study comparing the semantic-only pipeline against a hybrid approach that includes PageRank or co-citation weights in the ranking algorithm.

## Limitations

- Lack of ground-truth relevance labels prevents empirical validation of retrieval quality and thresholding effectiveness
- Domain-specific terminology and methodology nuances may not be captured by general-purpose embeddings
- Score saturation in scientific embeddings like Specter2 requires extremely precise threshold calibration

## Confidence

- **Mechanism 1 (Semantic Embeddings)**: Medium confidence. The technical implementation is clear, but the correlation between semantic similarity and research relevance is assumed, not validated.
- **Mechanism 2 (IQR Thresholding)**: Low confidence. The statistical approach is described, but its effectiveness without ground-truth labels is unverified, and performance on skewed distributions is uncertain.
- **Mechanism 3 (LLM Operations)**: Medium confidence. The workflow is specified, but the absence of prompt templates and potential for hallucination limit confidence in output quality.

## Next Checks

1. **Relevance Validation**: Manually label the top-10 retrieved papers for 5 diverse input papers and calculate precision@10 to empirically validate whether high cosine similarity corresponds to actual relevance.
2. **Threshold Sensitivity Analysis**: Compare retrieval counts and relevance scores using Q3 + 0.5×IQR versus Q3 + 1.0×IQR thresholds across multiple queries to determine optimal cutoff for different embedding models.
3. **Domain Generalization Test**: Run the pipeline on input papers from three distinct fields (e.g., computer science, biology, social sciences) and compare retrieval quality to identify if general-purpose embeddings underperform on specialized terminology.