---
ver: rpa2
title: Scaling-up Perceptual Video Quality Assessment
arxiv_id: '2505.22543'
source_url: https://arxiv.org/abs/2505.22543
tags:
- video
- quality
- answer
- question
- aesthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OmniVQA, a scalable framework for perceptual
  video quality assessment (VQA). It addresses the challenge of building large-scale
  labeled datasets for VQA by introducing a machine-driven annotation pipeline with
  human-in-the-loop refinement, resulting in the OmniVQA-Chat-400K dataset.
---

# Scaling-up Perceptual Video Quality Assessment

## Quick Facts
- **arXiv ID:** 2505.22543
- **Source URL:** https://arxiv.org/abs/2505.22543
- **Reference count:** 40
- **Primary result:** OmniVQA achieves state-of-the-art performance in both quality rating and fine-grained spatiotemporal quality understanding through scalable machine-driven annotation with human-in-the-loop refinement.

## Executive Summary
This paper introduces OmniVQA, a scalable framework for perceptual video quality assessment that addresses the critical challenge of building large-scale labeled datasets. The framework employs a machine-driven annotation pipeline with strategic human-in-the-loop verification to create the OmniVQA-Chat-400K dataset, complemented by the OmniVQA-MOS-20K dataset for quality rating. The architecture integrates specialized vision and motion encoders with a language model backbone, enabling both quality rating and fine-grained spatiotemporal understanding through a complementary training strategy. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, with the chatter model outperforming existing large language models on fine-grained quality tasks.

## Method Summary
OmniVQA employs a multi-branch architecture with technical, aesthetic, and in-context components, each trained with specialized objectives. The framework uses rejection sampling with human-in-the-loop verification to scale annotation while maintaining quality, achieving a 50% human intervention rate for 23,860 videos. Training combines quality rating (regression) and understanding (semantic navigation) tasks through complementary sequential fine-tuning rather than random mixing. Synthetic spatiotemporal distortions are injected into high-quality source videos to create controlled fine-grained quality understanding data. The model uses interleaved vision, motion, and text tokens fed into a Qwen-2 7B LLM, with quality scores extracted via weighted softmax over quality level logits.

## Key Results
- Chatter model achieves 69.87% accuracy on machine-annotated fine-grained understanding vs. 44.40% for GPT-4o
- OmniVQA outperforms existing methods on six VQA datasets with SRCC scores ranging from 0.8083 to 0.9256
- Complementary training strategy achieves 58.50% overall accuracy vs. 52.78% for Mix training on Q-bench-video
- Human-annotated fine-grained understanding shows 95.34% accuracy, significantly higher than machine-annotated 69.87%

## Why This Works (Mechanism)

### Mechanism 1: Rejection Sampling with Human-in-the-Loop Verification
Machine-driven multi-sample annotation with strategic human verification can scale VQA instruction data while maintaining quality. Sample 5 responses per quality factor from domain expert LMM, use reasoning LLM to summarize positive/neutral information, deploy general LMM as voting judge across 3 rounds with scores (2,1,0), and invoke human experts only when any round scores 0. Voting-based judgment is cognitively easier for general LMMs than direct annotation, producing more reliable quality signals. With 11,500 of 23,860 videos requiring human review, the approach shows approximately 50% intervention rate, suggesting either conservative thresholds or systematic annotation challenges.

### Mechanism 2: Complementary Sequential Training for Multi-Task VQA
Sequential training on quality rating then understanding (or reverse) outperforms random data mixing because it prevents objective interference. Train one epoch on first dataset (e.g., OmniVQA-MOS-20K for rating), then fine-tune on second (e.g., OmniVQA-Chat-400K for understanding). Tasks share knowledge but diverge in LLM role: regressor vs. semantic navigator. Quality rating and understanding are pretraining-complementary but fine-tuning-conflicting objectives. The 5.72% improvement over Mix training (58.50% vs 52.78%) on Q-bench-video demonstrates the effectiveness of this approach, though the mechanism preventing interference remains asserted rather than analyzed.

### Mechanism 3: Synthetic Spatiotemporal Distortion Injection
Controlled synthetic distortions on high-quality source videos create effective training data for fine-grained localization. Select videos with objective quality >70, inject spatial distortions (blur, over/underexposure, noise, compression) at 1/4 frame area for 1-3 seconds with 3 severity levels, add temporal stuttering, record ground truth (location, duration, type, severity). Synthetic artifact distributions approximate real-world degradations sufficiently for transfer learning. The 25.47% gap between machine-annotated (69.87%) and human-annotated (95.34%) fine-grained understanding scores indicates potential synthetic-distortion overfitting, suggesting synthetic patterns may not fully capture complex real-world degradations.

## Foundational Learning

- **Concept: Mean Opinion Score (MOS) and Subjective Testing Protocols**
  - Why needed here: OmniVQA-MOS-20K relies on human MOS collection; understanding ITU-R BT.500-15 compliance and hidden reference supervision is critical.
  - Quick check question: Why does the paper use hidden reference supervision with quality-level rejection rather than raw score aggregation?

- **Concept: Token Interleaving in Video LMMs**
  - Why needed here: Architecture interleaves vision tokens (196/keyframe), motion tokens (from full video), and text tokens into semantically ordered sequence for LLM input.
  - Quick check question: What determines the semantic ordering of vision, motion, and text tokens before LLM processing?

- **Concept: Quality Score Extraction via Logit Weighting**
  - Why needed here: Rating inference uses weighted softmax over 5 quality level logits with weights [1, 0.75, 0.5, 0.25, 0] rather than direct classification.
  - Quick check question: Why use weighted probability aggregation instead of argmax over quality level tokens?

## Architecture Onboarding

- **Component map:** Source video → SigLIP-SO400m (vision encoder) → SlowFast-R50 (motion extractor) → Qwen-2 7B LLM → Quality description/score
- **Critical path:** 1) Sample keyframes at 1 fps → SigLIP → vision projector → vision tokens; 2) Full video frames → SlowFast fast path → motion projector → motion tokens; 3) Text prompt → text embedding → text tokens; 4) Interleave tokens in semantic order → Qwen-2 LLM → generate quality description/score
- **Design tradeoffs:** Motion extractor uses fast path only (reduces compute, may miss slow temporal dynamics); keyframe rate is 1 fps (balances context length vs. temporal granularity); human intervention threshold is only score 0 triggers review (reduces cost, risks missing subtle errors); separate Chatter/Rater models vs. unified (specialization vs. deployment complexity)
- **Failure signatures:** High voting score 0 rate → annotation pipeline bottleneck; Mix training outperforming Complementary → tasks may not be complementary; Large gap between machine-annotated and human-annotated FG scores → synthetic distortion overfitting; SRCC drops on LSVQ(1080p) while improving on OmniVQA-MOS-20K → dataset overfitting
- **First 3 experiments:** 1) Data scaling validation: Train on 100K/200K/300K/400K subsets, plot Q-bench-video and OmniVQA-FG performance curves; 2) Training strategy ablation: Compare Direct/Mix/Complementary on both rating and understanding tasks; 3) Motion feature contribution: Train Chatter with/without SlowFast motion tokens, measure delta on temporal quality questions

## Open Questions the Paper Calls Out
- To what extent does the reliance on synthetic distortions for the In-Context branch create a performance gap on real-world fine-grained quality issues? The paper validates the In-Context branch using synthetic distortions but does not investigate if these simplified patterns transfer effectively to complex, compound degradations found in natural videos.
- Does the lack of model-size scaling laws in general LMMs for VQA imply a need for specialized architectural components rather than generic capacity? The authors note that increasing model size for general LMMs does not improve VQA performance, suggesting architectural design may matter more than model capacity.
- Can the human-in-the-loop verification component be fully replaced by an automated model-based judge without sacrificing annotation fidelity? While the framework is machine-driven, the necessity of the human-in-the-loop "reject" step suggests the voting mechanism may not be robust enough to handle ambiguity autonomously.

## Limitations
- The 50% human intervention rate in the annotation pipeline suggests either conservative thresholds or systematic challenges that may limit cost-effectiveness.
- Synthetic distortion injection may not adequately represent real-world video quality degradations, as evidenced by the 25.47% performance gap between machine-annotated and human-annotated fine-grained understanding.
- The complementary training strategy's effectiveness is demonstrated but not explained mechanistically, with the prevention of objective interference asserted rather than analyzed.

## Confidence

**High Confidence (Mechanistic Understanding):**
- Video architecture token interleaving and quality score extraction via weighted softmax
- Training infrastructure configuration (DeepSpeed, bfloat16, gradient checkpointing)
- Quantitative performance improvements over baselines on established benchmarks

**Medium Confidence (Methodological Claims):**
- Rejection sampling with human-in-the-loop maintains annotation quality at scale
- Complementary sequential training outperforms random data mixing for multi-task VQA
- Synthetic spatiotemporal distortions create effective fine-grained quality understanding data

**Low Confidence (Transfer and Generalization):**
- Synthetic distortions adequately represent real-world video quality degradations
- Performance gains transfer across different video content domains and quality distributions
- Annotation quality improvements justify increased pipeline complexity and human review costs

## Next Checks
1. **Synthetic-to-Real Transfer Validation:** Evaluate OmniVQA-FG performance on a small set of naturally degraded videos to quantify synthetic distortion overfitting and establish transfer gap.
2. **Annotation Pipeline Cost-Benefit Analysis:** Track per-video annotation time and compute costs across rejection sampling rounds versus traditional manual annotation to calculate break-even point.
3. **Motion Feature Ablation on Temporal Quality:** Systematically disable SlowFast motion tokens and retrain Chatter, then measure performance degradation specifically on temporal quality questions to quantify motion feature contribution.