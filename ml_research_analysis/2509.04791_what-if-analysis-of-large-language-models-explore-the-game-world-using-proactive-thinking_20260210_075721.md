---
ver: rpa2
title: 'What-If Analysis of Large Language Models: Explore the Game World Using Proactive
  Thinking'
arxiv_id: '2509.04791'
source_url: https://arxiv.org/abs/2509.04791
tags:
- game
- state
- wia-llm
- action
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WiA-LLM, a framework that trains large language
  models to perform explicit what-if analysis by forecasting the consequences of actions
  in dynamic game environments. The core innovation lies in modeling the game world
  through language-based state transitions, allowing the model to generate human-readable
  predictions and justifications for how the game state evolves in response to different
  actions.
---

# What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking

## Quick Facts
- arXiv ID: 2509.04791
- Source URL: https://arxiv.org/abs/2509.04791
- Reference count: 40
- Primary result: WiA-LLM achieves 74.2% forecasting accuracy, outperforming baseline by 27% and Deepseek-R1 by 41.6%

## Executive Summary
This paper introduces WiA-LLM, a framework that trains large language models to perform explicit what-if analysis by forecasting the consequences of actions in dynamic game environments. The core innovation lies in modeling the game world through language-based state transitions, allowing the model to generate human-readable predictions and justifications for how the game state evolves in response to different actions. The model is trained in two stages: supervised fine-tuning on human-like reasoning traces followed by reinforcement learning with rule-based rewards that compare predicted versus actual future states. Evaluated on the Honor of Kings MOBA environment, WiA-LLM achieves 74.2% accuracy in forecasting game-state changes, outperforming the base model by 27% and Deepseek-R1 by 41.6%. The model also demonstrates enhanced strategic decision-making closer to expert human behavior. These results indicate that explicit world modeling significantly improves proactive reasoning in complex, high-stakes environments.

## Method Summary
WiA-LLM employs a two-stage training approach to develop explicit what-if reasoning capabilities. First, the model undergoes supervised fine-tuning on human-like reasoning traces that demonstrate how to forecast game-state changes. Second, reinforcement learning with rule-based rewards is applied, where rewards are calculated by comparing predicted future states against actual outcomes. The framework uses language-based state transitions to represent the game world, enabling human-readable predictions and justifications. During inference, a one-step lookahead search mechanism evaluates potential actions by generating their predicted consequences before making decisions. The approach was evaluated on the Honor of Kings MOBA environment using an 8B parameter model.

## Key Results
- WiA-LLM achieves 74.2% accuracy in forecasting game-state changes in Honor of Kings
- Outperforms baseline model by 27% and Deepseek-R1 by 41.6% in forecasting accuracy
- Demonstrates strategic decision-making closer to expert human behavior

## Why This Works (Mechanism)
The framework works by explicitly modeling the game world through language-based state transitions, which allows the model to generate both predictions and justifications for how actions affect the environment. By training with rule-based rewards that compare predicted versus actual future states, the model learns to reason about causality rather than just reacting to immediate inputs. The two-stage training process (SFT followed by RL) ensures the model first learns human-like reasoning patterns before refining its decision-making through outcome-based feedback. This explicit world modeling approach enables proactive rather than reactive decision-making, allowing the agent to anticipate consequences and plan accordingly.

## Foundational Learning
- **Language-based state representation**: Represents game states as text descriptions rather than numerical vectors; needed to enable human-readable predictions and justifications; quick check: verify state descriptions capture all relevant game features
- **Explicit world modeling**: Models how actions affect game states rather than just learning action-response mappings; needed to enable proactive reasoning about consequences; quick check: test whether model can predict intermediate state changes between actions
- **Rule-based reward shaping**: Uses deterministic comparison between predicted and actual states as rewards; needed to provide clear feedback for learning causal relationships; quick check: verify reward function correctly identifies state differences
- **Two-stage training pipeline**: Combines supervised fine-tuning with reinforcement learning; needed to first learn reasoning patterns before optimizing for outcomes; quick check: compare performance of models trained with only one stage versus both
- **One-step lookahead search**: Evaluates immediate consequences of actions before selection; needed to balance computational cost with decision quality; quick check: measure impact of disabling lookahead on decision accuracy

## Architecture Onboarding

**Component Map**: Game Environment -> State Encoder -> WiA-LLM Reasoning Module -> Action Selector -> Environment

**Critical Path**: Environment state → State encoding → Reasoning generation → Action selection → Environment execution

**Design Tradeoffs**: The language-based representation prioritizes interpretability and reasoning capability over computational efficiency. The two-stage training approach trades increased training complexity for more robust reasoning abilities. The one-step lookahead balances decision quality with computational cost.

**Failure Signatures**: 
- Inaccurate state representations leading to faulty predictions
- Overfitting to training domain preventing generalization
- Excessive computational latency during inference
- Reward hacking where model learns to game the reward function rather than truly understand causality

**First Experiments**:
1. Evaluate forecasting accuracy on held-out game scenarios with known outcomes
2. Compare action selection quality against baseline reactive agents
3. Test zero-shot transfer to a simplified variant of the game environment

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does WiA-LLM improve actual win rates when evaluated in online ranked competitive play, beyond action alignment with expert behavior?
- Basis in paper: [explicit] The authors state in the Limitation section: "we do not evaluate our approach in an online ranked environment to measure Win Rate. This is because online performance is heavily influenced by low-level execution skills (such as micro-mechanics and reaction time), which fall outside the scope of our study."
- Why unresolved: The study deliberately isolates strategic reasoning from execution, leaving the practical competitive impact untested.
- What evidence would resolve it: Deployment of WiA-LLM-guided agents in live ranked HoK matches with statistically significant win-rate improvements over baseline reactive agents.

### Open Question 2
- Question: Can WiA-LLM's strategic knowledge be effectively distilled into a smaller student model that maintains forecasting accuracy while enabling real-time deployment?
- Basis in paper: [explicit] The authors mention: "while we present this as the natural path to production deployment, the detailed implementation of knowledge distillation is beyond the scope of this work. Here, we focus on demonstrating the core capability of WiA-LLM... leaving knowledge distillation for future work."
- Why unresolved: The dual-system architecture is proposed to address latency (LLM invoked only every 5-10s), but actual distillation experiments were not conducted.
- What evidence would resolve it: Training a smaller neural network on WiA-LLM-generated reasoning-augmented trajectories and comparing its forecasting accuracy and inference speed against the full 8B/14B model.

### Open Question 3
- Question: Does WiA-LLM generalize to other high-stakes dynamic environments (e.g., RTS games, autonomous driving simulations, or financial trading) without domain-specific retraining?
- Basis in paper: [explicit] The authors claim: "While evaluated in Honor of Kings, our approach is broadly applicable to other high-stakes domains where simulating outcomes is safer than trial-and-error," but provide no empirical evidence from other domains.
- Why unresolved: Only HoK experiments were conducted; transferability to environments with different state representations, action spaces, and dynamics remains unverified.
- What evidence would resolve it: Evaluating WiA-LLM (trained on HoK) in zero-shot or few-shot settings on other complex environments with quantifiable state transitions (e.g., StarCraft II, DOTA 2, or stock trading simulators).

### Open Question 4
- Question: Would extending the lookahead search beyond one step significantly improve decision quality in scenarios requiring multi-turn planning?
- Basis in paper: [inferred] Section 2.4 describes a one-step lookahead search mechanism: "the agent performs a one-step lookahead search," which may be insufficient for complex strategic sequences (e.g., multi-objective contests). The paper does not ablate or test deeper search depths.
- Why unresolved: Multi-step causal chains (e.g., "take dragon → enemy rotates → tower falls") may require deeper simulation; the trade-off between search depth, accuracy, and latency is unexplored.
- What evidence would resolve it: Ablation experiments comparing one-step vs. two-step vs. three-step lookahead in WiA-LLM, measuring forecasting accuracy and downstream action selection quality against computational cost.

## Limitations
- Performance gains are limited to the Honor of Kings MOBA environment without tested generalization to other domains
- Claims about "expert human behavior" are not validated against actual expert human gameplay
- Two-stage training approach requires substantial computational resources and curated datasets
- Rule-based rewards assume access to deterministic state transition logic, limiting applicability to more complex domains

## Confidence
- **High Confidence**: 74.2% forecasting accuracy metric and 27% improvement over baseline are directly measurable with clear methodology
- **Medium Confidence**: 41.6% improvement over Deepseek-R1 is credible but depends on evaluation conditions comparability
- **Low Confidence**: Generalization claims to broader domains and practical real-world applicability lack empirical support

## Next Checks
1. Evaluate WiA-LLM on at least two additional game environments with different mechanics (e.g., real-time strategy, puzzle games) to assess generalization beyond MOBAs
2. Conduct controlled experiments comparing WiA-LLM's strategic decisions against actual expert human players in identical game scenarios
3. Profile computational requirements during both training (SFT + RL phases) and inference, comparing against baseline models for practical feasibility