---
ver: rpa2
title: Efficient LLM Safety Evaluation through Multi-Agent Debate
arxiv_id: '2511.06396'
source_url: https://arxiv.org/abs/2511.06396
tags:
- judge
- safety
- multi-agent
- debate
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of scalable and cost-effective\
  \ safety evaluation of large language models (LLMs) against jailbreak attacks. The\
  \ authors propose a multi-agent judging framework that uses structured debate among\
  \ role-specific agents\u2014critic, defender, and judge\u2014powered by small language\
  \ models (SLMs)."
---

# Efficient LLM Safety Evaluation through Multi-Agent Debate

## Quick Facts
- **arXiv ID**: 2511.06396
- **Source URL**: https://arxiv.org/abs/2511.06396
- **Reference count**: 35
- **Primary result**: Multi-agent SLM debate framework achieves GPT-4o-level jailbreak evaluation accuracy at 43% lower cost

## Executive Summary
This paper addresses the challenge of scalable and cost-effective safety evaluation of large language models (LLMs) against jailbreak attacks. The authors propose a multi-agent judging framework that uses structured debate among role-specific agents—critic, defender, and judge—powered by small language models (SLMs). To support rigorous evaluation, they construct HAJailBench, a human-annotated benchmark of 12,000 adversarial interactions across diverse attack methods and target models. The SLM-based framework achieves agreement comparable to GPT-4o judges on HAJailBench while reducing inference cost by approximately 43%. Ablation studies show that three rounds of debate yield the optimal balance between accuracy and efficiency, demonstrating that structured, value-aligned debate enables SLMs to effectively capture semantic nuances of jailbreak attacks.

## Method Summary
The authors develop a multi-agent debate framework where three specialized SLMs—critic, defender, and judge—engage in structured adversarial discussions to evaluate whether LLM responses violate safety constraints. The framework includes a pre-debate value alignment stage that selects 5 relevant safety topics from 11 predefined categories. The debate proceeds for a fixed number of rounds, with agents iteratively challenging each other's reasoning before the judge renders a final verdict. The approach is validated on HAJailBench, a comprehensive benchmark of 12,000 human-annotated instances spanning 100 harmful goals, 12 attack methods, and 11 target models.

## Key Results
- Multi-agent SLM framework achieves Cohen's κ=0.7352 agreement with human annotators, comparable to GPT-4o's κ=0.7627
- Three debate rounds optimally balance accuracy and efficiency, improving κ from 0.5709 (no debate) to 0.7352
- Framework reduces inference cost by approximately 43% compared to GPT-4o while maintaining evaluation quality
- Semantic-level attacks (ASR 0.17-0.59) are significantly harder to defend against than token-level attacks (ASR <0.07)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured adversarial debate among role-specific agents surfaces semantic nuances in jailbreak attacks that single-model judges miss.
- Mechanism: The critic agent identifies safety violations; the defender agent provides counter-arguments; the judge agent synthesizes both perspectives. This iterative exchange exposes contextual cues and intent shifts, particularly for semantic-level attacks that achieve ASR >0.17 versus <0.07 for token-level attacks.
- Core assumption: SLMs can perform reliable role-conditioned reasoning when given structured prompts and bounded debate scope.
- Evidence anchors:
  - [abstract] "structured debates among critic, defender, and judge agents... enables SLMs to effectively capture semantic nuances of jailbreak attacks"
  - [section 3.3] "This adversarial interaction is analogous to Generative Adversarial Networks (GANs), wherein the critic and defender agents correspond to the discriminator and generator"
  - [corpus] Related work (iMAD, RedDebate) supports multi-agent debate improving reasoning, though specific safety-judgment mechanisms remain under-explored
- Break condition: If target responses contain ambiguous or culturally-dependent harm where even human annotators disagree significantly, the debate may amplify noise rather than consensus.

### Mechanism 2
- Claim: Pre-debate value alignment via predefined safety topics improves judgment consistency and reduces topic drift.
- Mechanism: Before debate, a topic-alignment stage selects 5 relevant safety aspects from 11 predefined categories (e.g., hate speech, privacy violations, illegal activities). This scaffolding constrains agent reasoning to safety-relevant dimensions.
- Core assumption: The predefined 11-category taxonomy covers the relevant harm space for jailbreak evaluation.
- Evidence anchors:
  - [section 3.2] "the debate context is constrained such that the debate topic does not deviate from the objective of safety evaluation"
  - [table 4] Pre-Align achieves κ=0.7352 vs Free-Align κ=0.7300 and No-Align κ=0.7239
  - [corpus] Limited direct evidence on value-alignment mechanisms in multi-agent systems; this appears to be a novel contribution
- Break condition: If novel attack types emerge outside the 11-category taxonomy, predefined topics may constrain detection.

### Mechanism 3
- Claim: Three debate rounds optimally balance accuracy gains against computational cost and error accumulation.
- Mechanism: Iterative refinement improves κ from 0.5709 (0 rounds) to 0.7352 (3 rounds). Beyond this, SLM error accumulation and noisy consensus degrade performance (4 rounds: κ=0.7260; 5 rounds: κ=0.7221).
- Core assumption: The accuracy-cost frontier is stable across different base SLMs and attack distributions.
- Evidence anchors:
  - [abstract] "Ablation studies show that three rounds of debate yield the optimal balance between accuracy and efficiency"
  - [table 4] Shows monotonic improvement to 3 rounds, then degradation
  - [corpus] Related work (Adaptive Confidence Gating) suggests dynamic round allocation may improve efficiency, but this paper uses fixed rounds
- Break condition: If base model capacity increases significantly (e.g., frontier SLMs), optimal round count may shift.

## Foundational Learning

- Concept: **LLM-as-a-Judge paradigm**
  - Why needed here: The framework evaluates whether target LLM responses are safe/unsafe using SLM judges; understanding inherent subjectivity and inter-judge disagreement is prerequisite.
  - Quick check question: Can you explain why pairwise agreement among existing safety judges (GCG, Llama-Guard, JudgeLM) is low, as shown in Figure 2a?

- Concept: **Jailbreak attack taxonomy (token-level vs. semantic-level)**
  - Why needed here: The benchmark spans 12 attack methods; evaluators must understand that semantic attacks (ASR 0.17-0.59) operate differently from token-level attacks (ASR <0.07).
  - Quick check question: Why do multi-turn semantic attacks like Actor achieve ASR 0.588 while token-level GCG achieves only 0.067?

- Concept: **Cohen's κ and inter-rater reliability**
  - Why needed here: Primary evaluation metric measures agreement with human ground truth; understanding κ interpretation (0.7+ = substantial agreement) is essential for reading results.
  - Quick check question: What does a κ of 0.7352 indicate about the Multi-Agent Judge's alignment with human annotators?

## Architecture Onboarding

- Component map:
  - **Value-Alignment Module**: Topic selector → 5 safety aspects from 11 predefined categories
  - **Multi-Agent Debate Engine**: Critic ↔ Defender (N rounds) → Judge (final adjudication)
  - **Evaluation Output**: Binary ASR + 5-level risk + 10-point score
  - **HAJailBench Dataset**: 12,000 instances across 100 harmful goals × 12 attack methods × 11 target models

- Critical path:
  1. Load response to evaluate + harmful goal context
  2. Run topic-alignment to select 5 safety aspects
  3. Execute 3-round debate (critic→defender→critic→defender→critic→defender)
  4. Judge synthesizes arguments and outputs structured evaluation

- Design tradeoffs:
  - **Cost vs. accuracy**: 3-round debate uses 6.87× more tokens than no-debate but achieves 28.8% κ improvement
  - **Pre-defined vs. free topics**: Pre-Align constrains flexibility but improves consistency
  - **SLM vs. frontier model**: Qwen3-14B Multi-Agent Judge achieves κ=0.7352 vs GPT-4o κ=0.7627 at 46% of cost

- Failure signatures:
  - **Topic drift**: Without Pre-Align, agents may debate irrelevant aspects
  - **Error accumulation**: >3 rounds leads to consensus breakdown in SLMs
  - **Cultural bias**: Two-round annotation protocol references GPT-4o for triage, potentially embedding bias

- First 3 experiments:
  1. Replicate ablation on debate rounds (0-5) with Qwen3-14B base to validate κ trajectory
  2. Test framework on held-out attack method not in HAJailBench to assess generalization
  3. Measure latency and throughput at scale (1K queries) to validate production feasibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can adaptive adversaries learn to manipulate the structured debate protocol (e.g., exploiting the critic or defender roles) to bypass the multi-agent judge?
- Basis in paper: [explicit] The Discussion section explicitly warns that "adaptive adversaries might learn to exploit the debate protocol" and lists "adversary-in-the-loop evaluations" as future work.
- Why unresolved: The current evaluation uses static attack methods; the framework's robustness against attacks specifically optimized to confuse the debate dynamics remains untested.
- What evidence would resolve it: Results from red-teaming experiments where attackers have full knowledge of the agent roles and debate structure, showing Attack Success Rates (ASR) under these adaptive conditions.

### Open Question 2
- Question: Can dynamically learned alignment topics outperform the current static, predefined topics in detecting novel or out-of-distribution harms?
- Basis in paper: [explicit] The authors state in the Discussion that future work includes "dynamically learning alignment topics" and note that current "Predefined topics... could constrain detection of novel harms."
- Why unresolved: The ablation study only compares Pre-Align, Free-Align, and No-Align static configurations, leaving the potential for an adaptive, evolving topic mechanism unexplored.
- What evidence would resolve it: Comparative performance analysis on zero-day jailbreaks between the current framework and a version that dynamically generates debate topics based on the specific input context.

### Open Question 3
- Question: Does the reliance on GPT-4o for triaging annotation discrepancies in HAJailBench introduce a systematic bias favoring models similar to GPT-4o?
- Basis in paper: [inferred] The Discussion notes that the "two-round annotation protocol, which references a large-model judge for discrepancy triage, may introduce bias."
- Why unresolved: While the authors hypothesize bias, they do not quantify if the ground truth aligns disproportionately with GPT-4o's reasoning patterns compared to a purely human-verified baseline.
- What evidence would resolve it: A re-evaluation of the discrepant cases using a human-only adjudication panel, followed by a comparison of correlation scores for various judge models against the original vs. purified labels.

### Open Question 4
- Question: How does the framework perform across different cultural contexts regarding safety, given that safety judgment is inherently subjective?
- Basis in paper: [explicit] The Introduction notes safety judgment is "culturally dependent," and the Discussion explicitly calls for "cross-cultural calibration" to mitigate "value bias" in future work.
- Why unresolved: HAJailBench uses specific "expert-labeled ground truth" which may reflect a specific cultural or safety philosophy, limiting the framework's applicability to global contexts with different norms.
- What evidence would resolve it: Evaluation of the multi-agent judge on datasets annotated by diverse cultural groups, measuring the agreement divergence across different value systems.

## Limitations

- The framework's performance may degrade when confronted with novel attack types outside the 11-category predefined safety taxonomy or when evaluating models trained on non-English content
- The human annotation protocol relies on GPT-4o for discrepancy resolution, potentially embedding GPT-4o's judgment patterns into the ground truth and biasing validation
- The optimal debate round count (3 rounds) may not generalize across different SLM base models or attack distributions without further validation

## Confidence

- **High Confidence**: The multi-agent debate framework's ability to reduce GPT-4o-level evaluation costs by ~43% while maintaining substantial agreement (κ=0.7352) is well-supported by ablation studies and direct comparison with baseline judges.
- **Medium Confidence**: The claim that three debate rounds optimally balance accuracy and efficiency across different contexts. While the ablation shows optimal performance at 3 rounds for Qwen3-14B on HAJailBench, this may not generalize to different model scales or attack distributions without further validation.
- **Medium Confidence**: The framework's ability to capture semantic nuances in jailbreak attacks. The evidence shows improved performance on semantic attacks (ASR 0.17-0.59) versus token-level attacks (ASR <0.07), but the mechanism for how role-specific debate surfaces these nuances could benefit from deeper qualitative analysis.

## Next Checks

1. **Cross-Attack Generalization Test**: Apply the trained framework to a held-out attack method (e.g., Combinator) not included in HAJailBench training data and measure κ agreement with human annotations to assess real-world generalization capability.

2. **Cultural Bias Analysis**: Evaluate the framework's performance on jailbreak attacks targeting safety topics with cultural specificity (e.g., region-specific legal frameworks or social norms) to identify potential blind spots in the predefined 11-category taxonomy.

3. **Dynamic Round Allocation Experiment**: Implement adaptive confidence gating to determine optimal debate rounds per instance rather than fixed 3 rounds, measuring both accuracy gains and cost savings compared to the current approach.