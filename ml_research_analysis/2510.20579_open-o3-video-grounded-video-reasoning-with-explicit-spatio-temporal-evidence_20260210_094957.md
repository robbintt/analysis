---
ver: rpa2
title: 'Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence'
arxiv_id: '2510.20579'
source_url: https://arxiv.org/abs/2510.20579
tags:
- video
- temporal
- reasoning
- arxiv
- grounding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Open-o3 Video, a framework that enhances
  video reasoning by integrating explicit spatio-temporal evidence. While most models
  generate only textual rationales, Open-o3 Video grounds its reasoning in concrete
  visual observations by highlighting key timestamps, objects, and bounding boxes.
---

# Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence

## Quick Facts
- **arXiv ID:** 2510.20579
- **Source URL:** https://arxiv.org/abs/2510.20579
- **Reference count:** 40
- **Primary result:** Achieves SOTA on V-STAR benchmark with 14.4% mAM and 24.2% mLGM improvement over Qwen2.5-VL.

## Executive Summary
Open-o3 Video introduces a framework for video reasoning that grounds answers in explicit spatio-temporal evidence—highlighting what objects exist, where they are, and when they appear. Unlike models that only generate textual rationales, Open-o3 Video produces structured chain-of-thought with bounding boxes, timestamps, and object names. This is enabled by curating two high-quality datasets with unified annotations and applying a two-stage training pipeline (cold-start SFT + RL) with adaptive reward shaping. The approach significantly improves grounded reasoning performance while maintaining general video understanding capabilities.

## Method Summary
The method trains Qwen2.5-VL-7B in two stages: first, cold-start SFT on STGR-CoT-30k teaches the model to output structured grounded chain-of-thought using `<obj>`, `<box>`, and `<t>` tags; second, GSPO RL on STGR-RL-36k optimizes for precision in answer accuracy, temporal alignment, and spatial grounding. Key innovations include adaptive temporal proximity (annealing Gaussian kernel width from loose to tight) and temporal gating (spatial rewards only when predictions are temporally close). The reward function combines accuracy, thinking, and format rewards with normalized per-group clipping for stability.

## Key Results
- Achieves SOTA on V-STAR benchmark: +14.4% mAM and +24.2% mLGM over Qwen2.5-VL.
- Maintains strong performance on general video understanding (VideoMME, WorldSense, VideoMMMU).
- Ablation confirms necessity of both training stages and adaptive reward shaping.
- Demonstrates test-time scaling benefits: more samples improve answer reliability.

## Why This Works (Mechanism)

### Mechanism 1: Unified Spatio-Temporal Grounding in Data
Providing unified annotations for what, when, and where teaches the model direct associations between reasoning steps and visual evidence, preventing ungrounded text generation.

### Mechanism 2: Cold-Start SFT for Format Acquisition
Initial SFT teaches the model the required structured output format before RL optimization, reducing reward sparsity and stabilizing training.

### Mechanism 3: Adaptive Reward Shaping to Avoid Spatial Collapse
Curriculum-based reward shaping (adaptive temporal proximity + temporal gating) prevents early imprecise temporal predictions from blocking spatial learning through zero rewards.

## Foundational Learning

- **Concept: Spatio-Temporal Grounding**
  - *Why needed:* Understanding the difference between temporal grounding (finding relevant time) and spatial grounding (finding relevant region) is essential for video reasoning.
  - *Quick check:* For a tennis match video, what's the difference between temporal-only vs. spatio-temporal grounding?

- **Concept: Reinforcement Learning from Verifiable Rewards**
  - *Why needed:* RL optimizes for non-differentiable metrics like IoU; standard supervised learning cannot directly maximize spatial metrics.
  - *Quick check:* Why can't cross-entropy loss directly maximize Intersection-over-Union for predicted bounding boxes?

- **Concept: Policy Gradient and Reward Shaping**
  - *Why needed:* GSPO uses policy gradients with reward shaping to make sparse reward problems learnable.
  - *Quick check:* What happens if a model always receives zero reward for a skill? How does reward shaping help?

## Architecture Onboarding

- **Component map:** Input video/text -> Qwen2.5-VL-7B -> Stage 1 SFT (STGR-CoT-30k) -> Stage 2 RL (STGR-RL-36k) -> Output with `<obj>`, `<box>`, `<t>` tags

- **Critical path:** Data preparation → SFT format learning → RL reward optimization. Missing format capability or poor data quality breaks the entire pipeline.

- **Design tradeoffs:** GSPO vs GRPO (stability vs fine-grained control), data mixture balance (15k QA samples optimal), complex reward design vs simple accuracy reward.

- **Failure signatures:** Spatial collapse (poor temporal IoU, near-zero visual IoU), format errors (low r_fmt), catastrophic forgetting (general QA performance drops).

- **First 3 experiments:**
  1. Overfit single batch from SFT dataset to validate format learning.
  2. Unit test reward functions with mock predictions.
  3. Ablate adaptive temporal proximity to demonstrate curriculum benefits.

## Open Questions the Paper Calls Out

- **Multimodal reasoning extension:** Current design doesn't integrate audio/speech information, which often carries crucial video understanding cues.
- **Long video scalability:** Performance on videos longer than 3 minutes with dense multi-event content remains untested due to dataset limitations.
- **Multi-step inference transfer:** Current rewards optimize direct grounding, not compositional reasoning requiring chained observations.
- **Training data composition:** Optimal balance between general VideoQA and spatio-temporal grounding samples across different base models and domains is unknown.

## Limitations

- Performance improvements rely heavily on quality of curated datasets, which may contain annotation biases.
- Computational cost of two-stage training (8×H100 for 1+1 epochs) limits accessibility.
- Benchmark scope may not fully capture real-world video reasoning challenges.

## Confidence

- **High confidence:** Two-stage training strategy effectiveness, SOTA results on V-STAR.
- **Medium confidence:** Importance of adaptive reward shaping mechanisms.
- **Low confidence:** Generalizability to unseen video types and languages.

## Next Checks

1. Conduct dataset bias analysis to identify potential annotation biases affecting out-of-distribution performance.
2. Evaluate on cross-dataset benchmarks (EgoVQA, DramaQA) to test generalization.
3. Perform granular hyperparameter sweep of temporal proximity and gating parameters.