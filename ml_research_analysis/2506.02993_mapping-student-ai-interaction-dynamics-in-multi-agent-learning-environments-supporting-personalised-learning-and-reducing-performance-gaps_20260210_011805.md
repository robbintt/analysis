---
ver: rpa2
title: 'Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments:
  Supporting Personalised Learning and Reducing Performance Gaps'
arxiv_id: '2506.02993'
source_url: https://arxiv.org/abs/2506.02993
tags:
- learning
- students
- interaction
- knowledge
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how university students engage with multiple
  AI agents in an online learning environment and how such interactions influence
  learning outcomes and motivation. Using a multi-agent AI course system (MAIC), researchers
  analyzed 305 students'' dialogue interactions with AI teacher and peer agents, revealing
  two main engagement patterns: knowledge co-construction and co-regulation.'
---

# Mapping Student-AI Interaction Dynamics in Multi-Agent Learning Environments: Supporting Personalised Learning and Reducing Performance Gaps

## Quick Facts
- **arXiv ID**: 2506.02993
- **Source URL**: https://arxiv.org/abs/2506.02993
- **Reference count**: 0
- **Primary result**: Multi-agent AI systems can reduce performance gaps by supporting personalized learning through adaptive engagement patterns.

## Executive Summary
This study analyzes how university students interact with multiple AI agents in an online learning environment and how these interactions influence learning outcomes. Using a multi-agent AI course system (MAIC), researchers examined 305 students' dialogue interactions, revealing two main engagement patterns: knowledge co-construction and co-regulation. Lower prior knowledge students showed higher learning gains and post-course motivation through knowledge co-construction sequences, while higher prior knowledge students engaged more in co-regulation but demonstrated limited learning improvement. Technology acceptance increased across all groups, suggesting multi-agent AI systems can support personalized learning and reduce performance gaps by adapting to students' varying needs.

## Method Summary
The study collected 19,365 dialogue entries from 305 university students across 6 course modules. Researchers developed a coding framework based on the Flanders Interaction Analysis System with 8 student codes (SB1-SB8) and 6 teacher codes (TB1-TB6). LLM-assisted coding using GPT-4-turbo and GLM-4 with few-shot prompting was employed, validated by human annotators achieving 0.97 inter-rater reliability. Lag sequential analysis identified statistically significant behavioral transition sequences. Students were grouped by pre-test performance (top 30% vs bottom 30%), and learning gains, motivation, and technology acceptance were compared using paired t-tests, ANOVA, and ANCOVA.

## Key Results
- Lower prior knowledge students achieved higher learning gains through knowledge co-construction sequences
- Higher prior knowledge students engaged more in co-regulation but showed limited learning improvement
- Technology acceptance increased significantly across all student groups post-course
- Knowledge co-construction patterns were associated with improved motivation and reduced performance gaps

## Why This Works (Mechanism)
The multi-agent AI system supports personalized learning by adapting to students' varying needs through different interaction patterns. Lower prior knowledge students benefit from knowledge co-construction sequences that provide scaffolding and collaborative knowledge building. Higher prior knowledge students engage in co-regulation patterns focused on self-monitoring and progress management. The system's ability to respond differently to these engagement patterns enables targeted support, potentially reducing performance gaps by providing appropriate assistance based on student capability levels.

## Foundational Learning
- **Lag Sequential Analysis (LSA)**: Statistical method for identifying significant behavioral transition sequences in interaction data. Why needed: To detect meaningful patterns in student-AI dialogue beyond random interactions. Quick check: Verify adjusted residuals exceed critical values (typically ±1.96) for sequence significance.
- **Flanders Interaction Analysis System**: Framework for coding classroom discourse with categories for student and teacher behaviors. Why needed: Provides structured approach to analyze dialogue content and interaction patterns. Quick check: Ensure inter-rater reliability exceeds 0.85 for consistent coding.
- **UTAUT2 (Unified Theory of Acceptance and Use of Technology)**: Model measuring technology acceptance factors including performance expectancy, effort expectancy, and social influence. Why needed: To assess how students perceive and accept AI learning tools. Quick check: Confirm Cronbach's alpha exceeds 0.7 for scale reliability.

## Architecture Onboarding
- **Component Map**: Students -> MAIC System (AI Teacher + AI Peer agents) -> Learning Outcomes (Pre/Post tests, Motivation, Technology Acceptance)
- **Critical Path**: Student engagement → Interaction coding → Sequence analysis → Group comparison → Learning impact assessment
- **Design Tradeoffs**: LLM-assisted coding offers scalability but introduces subjectivity; manual validation ensures reliability but limits sample size. Binary grouping simplifies analysis but may mask nuanced patterns. Immediate post-testing captures short-term gains but misses long-term retention.
- **Failure Signatures**: Low inter-rater reliability (<0.85) suggests ambiguous code definitions. Sparse transition matrices yield non-significant LSA results. Ceiling effects in high-performer scores obscure true learning improvements.
- **First Experiments**: 1) Pilot code 200 samples with multiple annotators to refine codebook and ensure reliability. 2) Aggregate interaction data across modules if individual module samples are too sparse for LSA. 3) Examine score distributions to identify potential ceiling effects in learning gain calculations.

## Open Questions the Paper Calls Out
1. What caused the increase in technology acceptance—the experience of learning with multiple AI agents, or the AI-related course content itself? The study could not isolate whether improved AI acceptance resulted from the multi-agent system itself or from learning about AI topics.
2. How can AI peer agents be designed to trigger sustained follow-up discussion and deeper elaboration from students? Current peer agent designs failed to promote meaningful co-construction despite the theoretical potential for peer-like AI to enhance learning.
3. Do learning gains in multi-agent environments persist over time and transfer to novel contexts? The study design did not include delayed assessments or transfer tasks to measure retention or knowledge application beyond the immediate post-course period.

## Limitations
- LLM-assisted coding introduces potential subjectivity despite high inter-rater reliability
- Binary grouping of students into high/low prior knowledge categories may mask nuanced interaction patterns
- Lack of control group using traditional learning methods limits isolation of multi-agent AI system effects

## Confidence
- **High Confidence**: Methodological approach for identifying interaction patterns (LSA with transition matrices) and standard statistical comparisons
- **Medium Confidence**: Characterization of engagement patterns and their differential impact across prior knowledge groups
- **Medium Confidence**: Interpretation that multi-agent AI systems can reduce performance gaps

## Next Checks
1. Replicate analysis with alternative grouping: Apply k-means clustering on pre-test scores to create multiple performance tiers rather than binary high/low groups, examining whether interaction patterns differ across finer-grained segments.
2. Temporal analysis of engagement: Conduct survival analysis on interaction dropout rates to determine whether identified engagement patterns predict sustained participation over the course duration.
3. Cross-validation of coding framework: Apply the 14-code framework to a held-out dataset from a different course module or academic term to assess generalizability of the coding scheme and interaction patterns.