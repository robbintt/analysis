---
ver: rpa2
title: Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets
arxiv_id: '2505.21930'
source_url: https://arxiv.org/abs/2505.21930
tags:
- fine-tuning
- adapters
- adapter
- qlora
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an ensemble method for efficiently fine-tuning
  language models on multiple datasets. The key idea is to group similar datasets
  based on task affinity scores estimated via first-order gradient approximations,
  then train one adapter per group and combine them via weighted averaging.
---

# Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets

## Quick Facts
- arXiv ID: 2505.21930
- Source URL: https://arxiv.org/abs/2505.21930
- Reference count: 40
- Key outcome: Gradient-based task affinity grouping enables 105× faster multi-task adapter fine-tuning with 10% accuracy gains

## Executive Summary
This paper introduces an efficient ensemble method for fine-tuning language models on multiple datasets by grouping similar tasks based on task affinity scores estimated via first-order gradient approximations. The key innovation is using a Taylor expansion to predict fine-tuning performance without actually training adapters, enabling rapid task grouping and reducing negative transfer. When applied to Llama and GPT models on ten text classification tasks, the approach improves QLoRA's average test accuracy by up to 10% with only 9% more FLOPs and 9 GB extra memory.

## Method Summary
The method consists of three main phases: First, it estimates task affinity scores by computing gradients of the base model on all training samples, projecting them to a lower dimension, and solving regression problems to predict performance for random task subsets. Second, it clusters tasks into groups using a semi-definite programming algorithm that maximizes intra-cluster affinity. Third, it trains one LoRA adapter per group and combines them via weighted averaging, with an optional gradient boosting step to fit residual errors on difficult groups.

## Key Results
- Achieves 105× speedup in performance estimation compared to actual fine-tuning
- Improves QLoRA average test accuracy by up to 10% with only 9% additional FLOPs
- Reduces generalization errors compared to single adapter approaches
- Achieves 3% accuracy gain on 34-billion parameter Llama model with 8% additional FLOPs

## Why This Works (Mechanism)

### Mechanism 1: First-Order Gradient Approximation for Performance Estimation
The paper claims fine-tuned adapter weights remain extremely close to the base model (typically within 0.2% relative distance), allowing first-order Taylor expansion to accurately approximate model outputs without actual fine-tuning. A Taylor expansion around base model parameters approximates the output function, enabling the fine-tuning problem to be formulated as linear/logistic regression on gradients evaluated at the base model. This linearization allows rapid estimation of performance for any subset of datasets.

### Mechanism 2: Task Affinity Grouping via Gradient-Based Estimation
Grouping similar datasets reduces negative interference common when fine-tuning single adapters on diverse tasks. The method computes a task affinity matrix where elements represent average estimated fine-tuning performance of one task over subsets containing both tasks. This matrix is used by a clustering algorithm (SDP relaxation) to partition datasets into groups that maximize intra-cluster affinity, ensuring datasets within clusters exhibit positive transfer.

### Mechanism 3: Ensemble of Specialized Adapters with Gradient Boosting
An ensemble of adapters, each fine-tuned on a task group, combined via weighted averaging, outperforms single adapters trained on all tasks. A gradient boosting procedure refines performance by identifying the group with highest training loss and training new adapters to fit the negative gradient (residual error) of the current ensemble. The final prediction is a learned weighted combination of all adapter group outputs.

## Foundational Learning

- **First-Order Taylor Expansion**: Mathematical foundation for estimating fine-tuning performance without training; necessary to understand how gradients at base model predict post-training behavior. Quick check: Explain why Taylor expansion approximates LoRA adapter outputs well but might fail for fully fine-tuned models.
- **Task Affinity and Negative Transfer**: Understanding how training on one task can hurt performance on another is critical for appreciating grouping mechanism value. Quick check: Predict what happens when training single LoRA adapter on sentiment analysis and code summarization datasets.
- **Gradient Boosting**: Iterative error-correction process where new adapters fit residual errors of initial group adapters; key to understanding final model construction. Quick check: In boosting step, what target does new adapter predict and how does it relate to ensemble loss function?

## Architecture Onboarding

- **Component map**: Input datasets -> Base Language Model -> Task Affinity Estimation Module -> Clustering Module -> Adapter Ensemble Construction Module -> Output ensemble of adapters
- **Critical path**: Entire architecture hinges on Task Affinity Estimation Module accuracy. Poor first-order approximation leads to noisy affinity matrix, suboptimal clustering, and inherited negative interference. Validating approximation error on small task samples is most critical first step.
- **Design tradeoffs**:
  - Projection Dimension (d): Larger d increases accuracy but regression cost; paper finds d=400 provides good trade-off
  - Number of Groups (m): Smaller m reduces computation/memory but risks larger diverse groups with higher negative interference
  - Number of Boosting Steps (b): More steps reduce training error but add adapters, increasing memory usage; single step often provides most gain
- **Failure signatures**:
  - High Approximation Error (>10%): Check projection dimension, verify gradient computation
  - No Generalization Gain: Verify gradient boosting fits residuals correctly, check ensemble weights sum to 1
  - Memory Overflow: Method scales with final adapters M; if M too large, inference won't fit GPU memory
- **First 3 experiments**:
  1. Validate First-Order Approximation: Compare actual vs. estimated fine-tuning performance for random task subsets, success is error <5%
  2. Cluster and Compare Groupings: Compare downstream performance of affinity-based vs. random groupings of same size
  3. End-to-End Ensemble Evaluation: Compare final test accuracy, FLOPs, and memory footprint against QLoRA and MTL-FT baselines

## Open Questions the Paper Calls Out

- Can fine-tuning performance be estimated for closed-source models (e.g., GPT-4, Gemini) without access to internal weights and gradients?
- Can this method be extended to handle dynamically incoming tasks in continual learning scenarios?
- Do sharpness measures like Hessian trace correlate with out-of-distribution generalization or adversarial robustness?

## Limitations
- Approximation validity range uncertain for extreme cases (very high adapter rank or training epochs)
- Task affinity generalization may fail for truly orthogonal tasks from different domains
- Computational overhead scaling unclear for very large models (>70B parameters) or massive datasets

## Confidence

- **High Confidence**: 105× speedup claim well-supported by 4.2%-8.2% approximation error and <0.2% weight distance
- **Medium Confidence**: 10% accuracy improvement supported by results but depends on specific task set and hyperparameters
- **Low Confidence**: 3% accuracy gain on 34-billion parameter model with 8% additional FLOPs based on single experiment lacking comprehensive ablation

## Next Checks

1. **Boundary Condition Testing**: Systematically vary LoRA rank (1, 8, 16, 32, 64) and training epochs (1, 3, 10, 30) to identify where first-order approximation breaks down

2. **Cross-Domain Task Affinity**: Apply pipeline to tasks from completely different domains (e.g., SuperGLUE + biomedical NLP) to test if clustering produces intuitive groupings and comparable performance gains

3. **Large-Scale Memory Profiling**: Profile GPU memory usage at each stage (gradient computation, adapter training, ensemble inference) on Llama-2-13B with 1M samples to quantify "little overhead" claim