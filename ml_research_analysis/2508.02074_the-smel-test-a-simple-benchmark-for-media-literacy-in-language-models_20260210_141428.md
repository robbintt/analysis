---
ver: rpa2
title: 'The SMeL Test: A simple benchmark for media literacy in language models'
arxiv_id: '2508.02074'
source_url: https://arxiv.org/abs/2508.02074
tags:
- sources
- source
- https
- language
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The SMeL Test measures how well large language models can judge
  source trustworthiness when presented with conflicting information. It evaluates
  models on three tasks: ignoring untrustworthy sources, resolving contradictions
  between sources, and filtering sources when generating summaries.'
---

# The SMeL Test: A simple benchmark for media literacy in language models

## Quick Facts
- **arXiv ID:** 2508.02074
- **Source URL:** https://arxiv.org/abs/2508.02074
- **Reference count:** 33
- **Primary result:** State-of-the-art LLMs hallucinate from untrustworthy sources up to 99% of the time on media literacy tasks.

## Executive Summary
The SMeL Test is a benchmark that evaluates whether language models can identify and ignore untrustworthy sources when presented with conflicting information. It tests models on three tasks: ignoring unreliable sources when no trustworthy alternatives exist, resolving contradictions between sources, and filtering sources when generating summaries. The benchmark reveals that even advanced models frequently hallucinate by copying incorrect information from unreliable sources, with reasoning models showing the best performance despite being smaller than non-reasoning alternatives.

## Method Summary
The benchmark uses both synthetic and real data. Synthetic documents are generated by GPT-4o in styles of 7 different sources (from Britannica to "Unknown") with embedded facts that are either true or perturbed. Real data comes from the ISOT Fake News Dataset with TF-IDF similarity matching to create contradictory pairs. Each test case includes target documents plus 15 irrelevant "false positive" documents from C4. Three evaluation tasks measure: (1) abstention rate when only unreliable source is present, (2) accuracy choosing correct facts from contradictory sources, and (3) hallucination rate in summaries as judged by a grader LLM.

## Key Results
- LLMs hallucinate from untrustworthy sources up to 99% of the time, even when warned explicitly about source quality
- Larger models do not reliably outperform smaller ones on source filtering tasks
- Reasoning models like o3-mini and Gemini 2.5 Pro consistently outperform non-reasoning models
- Models correctly identify unreliable sources early in reasoning traces but often use them anyway

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reasoning traces reduce hallucination from untrustworthy sources by enabling explicit source evaluation
- Mechanism: Reasoning models minimize influence from failed solution attempts earlier in their traces through extended thinking
- Core assumption: The benefit comes from the reasoning process itself, not other architectural differences
- Evidence anchors:
  - Reasoning models outperform non-reasoning models despite being smaller
  - Even best models hallucinate up to 70% of the time
- Break condition: If reasoning traces are hidden or truncated, the mechanism may not be fully observable

### Mechanism 2
- Claim: Models possess implicit source rankings but fail to act on them consistently
- Mechanism: Large gap between models' implicit "system 1" knowledge and explicit "system 2" knowledge
- Core assumption: Verbalization accuracy reflects actual internal source quality assessment
- Evidence anchors:
  - Models can verbalize which sources are better but fail to leverage this insight
  - Models correctly identify unreliable sources early but "gradually forget" warnings
- Break condition: When style differences between sources are minimal, performance declines

### Mechanism 3
- Claim: Model scale does not reliably predict media literacy performance
- Mechanism: Larger and more capable models do not necessarily outperform smaller counterparts
- Core assumption: Model size is a reasonable proxy for general capability
- Evidence anchors:
  - Gemma 3 27B only marginally outperforms 4B model
  - Llama 3 70B sometimes underperforms Llama 3 8B
- Break condition: The relationship may differ for models with different training data distributions

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The SMeL Test simulates RAG scenarios where models must filter retrieved documents of mixed quality
  - Quick check question: When a RAG system retrieves documents from open web, what additional capability beyond standard QA does the model need?

- Concept: **System 1 vs. System 2 Reasoning**
  - Why needed here: The paper frames the core failure as a gap between fast, implicit judgments and deliberate, explicit reasoning
  - Quick check question: If a model can verbally identify an untrustworthy source but still uses its information, which "system" is failing?

- Concept: **Positional Bias in Context**
  - Why needed here: Models exhibit spurious sensitivity to source ordering—Llama trusts earlier sources, GPT-4o trusts later ones
  - Quick check question: In a two-source contradiction task, how would you design the experiment to separate genuine source quality assessment from positional preference?

## Architecture Onboarding

- Component map:
  Synthetic data generator (GPT-4o) -> Fact extractor (Llama 3.3 70B) -> Three evaluation subtasks -> Grader LLM -> Real data pipeline

- Critical path:
  1. Generate fictional entities and facts -> 2. Create style-conditioned documents -> 3. Present to model with explicit warnings -> 4. Measure abstention/accuracy/hallucination

- Design tradeoffs:
  - Synthetic vs. real data: Synthetic enables contamination control and unlimited regeneration; real data validates generalizability but has less stylistic contrast
  - Single-source vs. multi-source: Single-source proves hardest—models cannot help but copy; multi-source allows relative comparison

- Failure signatures:
  - "Gradual forgetting": Model correctly identifies unreliable source early but uses it anyway
  - High abstention from uncertainty: Gemini 2.5 Pro says "I don't know" 22% of the time even when reliable source is present
  - Positional bias: Trust varies by 10-25 percentage points based solely on source order

- First 3 experiments:
  1. **Baseline replication:** Run all three subtasks on your model with provided synthetic documents, randomizing source order
  2. **Ablation on reasoning:** Compare performance with and without extended thinking enabled to isolate reasoning mechanism
  3. **Source style transfer:** Test whether models rely on URL metadata vs. textual style by swapping URLs between high- and low-trustworthiness documents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can conditional pretraining with document-level metadata improve models' ability to filter untrustworthy sources?
- Basis in paper: "One potentially interesting direction is conditional pretraining. Several works have demonstrated the potential of training language models with document-level metadata"
- Why unresolved: Existing proof-of-concept models lack modern post-training and scale
- What evidence would resolve it: Train comparable models with and without metadata conditioning, then evaluate on SMeL Test and real-world RAG tasks

### Open Question 2
- Question: Can models learn to discard outdated information as effectively as untrustworthy information, and is this task strictly harder?
- Basis in paper: "Future work could explore the related and likely more difficult task of discarding outdated rather than patently untrustworthy information"
- Why unresolved: SMeL Test only evaluates clearly untrustworthy sources, not temporally stale but formerly reliable information
- What evidence would resolve it: Construct benchmark with trustworthy but outdated vs. current sources, measuring model preference for recent information

### Open Question 3
- Question: Why do larger models within the same family not consistently outperform smaller ones on source trustworthiness tasks?
- Basis in paper: "Gemma 3 27B only meaningfully outperforms the 4B model in the 'Unknown' category" and "Llama 3 70B arguably underperforms Llama 3 8B"
- Why unresolved: Scale typically improves most LLM capabilities, but source filtering may require different training signals
- What evidence would resolve it: Systematic ablation across model scales with controlled training data, testing whether specific factors predict source discrimination

### Open Question 4
- Question: Can techniques for selective attention in jailbreaking defense and reasoning trace management transfer to improve SMeL Test performance?
- Basis in paper: "Techniques to improve performance on these tasks enhance the ability of LLMs to attend selectively to their contexts, they may be directly transferable to the SMeL Test"
- Why unresolved: Connection between resisting prompt injection, ignoring failed reasoning steps, and filtering untrustworthy sources is hypothesized but untested
- What evidence would resolve it: Apply existing selective attention methods to SMeL Test and measure whether improvements transfer

## Limitations

- The synthetic data generation process lacks transparency in exact prompts and style guides used
- Real-world generalization remains untested beyond ISOT Fake News Dataset
- Benchmark's contamination control may not reflect typical RAG scenarios with mixed-quality sources
- Proposed reasoning mechanism cannot be fully demonstrated due to hidden reasoning traces

## Confidence

**High Confidence:** Empirical finding that even state-of-the-art models hallucinate from untrustworthy sources up to 99% of the time is well-supported by systematic testing.

**Medium Confidence:** Claim that reasoning models outperform non-reasoning models has strong empirical support but weaker mechanistic explanation due to hidden reasoning traces.

**Low Confidence:** Proposed mechanism that reasoning traces reduce hallucination through explicit source evaluation is speculative and cannot be fully demonstrated.

## Next Checks

1. **Reasoning Trace Transparency Test:** Run benchmark with models that expose full reasoning traces versus those with truncated traces to isolate reasoning mechanism's contribution.

2. **Style Transfer Robustness Test:** Create documents where high-trustworthiness URLs are paired with low-trustworthiness writing styles and vice versa to test reliance on URL metadata versus textual style.

3. **Real-World Contamination Test:** Simulate RAG scenarios by mixing 1-2 unreliable sources with 3-4 relevant but trustworthy documents to measure how often models incorporate unreliable information.