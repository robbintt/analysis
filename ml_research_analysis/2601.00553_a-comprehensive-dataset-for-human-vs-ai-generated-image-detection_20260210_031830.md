---
ver: rpa2
title: A Comprehensive Dataset for Human vs. AI Generated Image Detection
arxiv_id: '2601.00553'
source_url: https://arxiv.org/abs/2601.00553
tags:
- image
- dataset
- images
- detection
- ai-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MS COCOAI, a comprehensive dataset of 96,000
  real and AI-generated images for detecting synthetic media. Built on MS COCO captions,
  the dataset features images generated by five leading models (Stable Diffusion 3,
  SDXL, SD 2.1, DALL-E 3, MidJourney v6), with each caption used to produce aligned
  real and synthetic image pairs.
---

# A Comprehensive Dataset for Human vs. AI Generated Image Detection

## Quick Facts
- arXiv ID: 2601.00553
- Source URL: https://arxiv.org/abs/2601.00553
- Reference count: 21
- Primary result: MS COCOAI dataset with 96K images for synthetic media detection

## Executive Summary
This paper introduces MS COCOAI, a comprehensive dataset designed to advance AI-generated image detection. The dataset contains 96,000 real and synthetic images, with synthetic versions generated from MS COCO captions using five leading models (Stable Diffusion 3, SDXL, SD 2.1, DALL-E 3, MidJourney v6). Each caption is used to produce aligned real and synthetic image pairs, enabling detection methods to focus on generation artifacts rather than semantic content differences. The dataset supports two tasks: binary classification of real vs. AI-generated images, and model attribution to identify the specific generator.

## Method Summary
The dataset construction begins with 16,000 real images and captions from MS COCO. Each caption is fed to five image-generation models to produce synthetic counterparts, yielding 80,000 synthetic images. The complete dataset includes 96,000 images split into 42K training, 9K validation, and 45K test samples. A baseline ResNet-50 classifier operates on frequency-domain representations via 2D Fourier Transform, inspired by Corvi et al.'s work. The approach captures global frequency characteristics that reveal subtle artifacts often present in synthetic images. For evaluation, two tasks are defined: Task A (binary classification) and Task B (six-class model attribution including the real category).

## Key Results
- Binary classification (Task A) baseline accuracy: 0.801
- Model attribution (Task B) baseline accuracy: 0.449
- Dataset size: 96,000 images (16K real + 16K each from 5 generators)
- Train/val/test split: 42K/9K/45K images

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frequency-domain representations expose generation artifacts not visible in pixel space.
- **Mechanism:** 2D Fourier Transform converts images to frequency domain; ResNet-50 classifies based on spectral patterns that differ between real photographs and synthetic outputs.
- **Core assumption:** AI-generated images leave consistent frequency signatures that persist across content variations.
- **Evidence anchors:**
  - Baseline ResNet-50 in frequency domain achieves 0.80 binary classification accuracy.
  - "This transformation captures global frequency characteristics that help reveal subtle artifacts often present in synthetic images."
  - Related work shows "synthetic images have distinct frequency patterns" though "diffusion models produce weaker frequency artifacts."
- **Break condition:** If diffusion models converge toward indistinguishable frequency profiles from real images, this approach degrades.

### Mechanism 2
- **Claim:** Caption-aligned real-synthetic pairs isolate generator artifacts from semantic content.
- **Mechanism:** All synthetic images generated from identical MS COCO captions as real counterparts; any detectable differences must stem from generation process rather than scene content.
- **Core assumption:** Detection methods previously conflated "what" (content) with "how" (generation method); controlling content reveals true artifacts.
- **Evidence anchors:**
  - "Each caption is used as a textual seed for generating synthetic images using five image-generation models."
  - Prior datasets lacked semantic alignment, "making it hard to separate generator artifacts from content differences."
- **Break condition:** If caption interpretation variance across models introduces systematic content differences, content bias may persist.

### Mechanism 3
- **Claim:** Model attribution requires learning generator-specific fingerprints beyond real/fake discrimination.
- **Mechanism:** Six-class classification (real + five generators) forces model to distinguish inter-generator differences, not just real vs. synthetic boundary.
- **Core assumption:** Each generator leaves distinguishable traces despite shared diffusion foundations.
- **Evidence anchors:**
  - Task B baseline: 0.449 accuracy vs. Task A: 0.801—"model attribution proving significantly more challenging."
  - Figure 1 shows "each model produces visually distinct outputs."
  - Generators "based on a few core architectural families," suggesting shared features may hinder attribution.
- **Break condition:** As models converge architecturally or explicitly optimize to remove fingerprints, attribution accuracy drops further.

## Foundational Learning

- **Concept: 2D Discrete Fourier Transform (DFT)**
  - **Why needed here:** Baseline detection operates on frequency-domain representations; understanding spectral decomposition is essential for interpreting why synthetic images differ spectrally.
  - **Quick check question:** Can you explain why high-frequency components might differ between a photograph (sensor noise, natural texture) and a diffusion-generated image (upsampling artifacts, smoothing)?

- **Concept: Diffusion model fundamentals**
  - **Why needed here:** All five generators are diffusion-based; knowing forward/reverse process helps hypothesize what artifacts might persist.
  - **Quick check question:** What step in the denoising trajectory might introduce systematic patterns detectable in frequency space?

- **Concept: Multi-class vs. binary classification trade-offs**
  - **Why needed here:** Performance gap (0.80 vs. 0.45) reflects fundamental difficulty increase; understanding class confusion matrices guides improvement strategies.
  - **Quick check question:** Why might SDXL and SD3 share more confusable features than DALL-E 3 and MidJourney?

## Architecture Onboarding

- **Component map:** MS COCO captions → 5 generators → Synthetic images (16K each) → Real images (16K) → Dataset (96K total) → Perturbation branch (4 transforms) → Frequency transform (DFT) → ResNet-50 classifier → Task A: binary | Task B: 6-class

- **Critical path:**
  1. Caption sampling from MS COCO (determines semantic coverage)
  2. Image generation (model versions fixed; API variability for DALL-E/MidJourney)
  3. Frequency preprocessing (implementation of DFT pipeline)
  4. ResNet-50 training (class weighting for balanced 6-class)

- **Design tradeoffs:**
  - Caption-aligned design: Stronger causal attribution vs. limited caption diversity (MS COCO style only)
  - Perturbations applied separately (not composed): Cleaner ablation vs. unrealistic robustness testing
  - Frequency-only baseline: Strong inductive bias vs. missed pixel-space cues

- **Failure signatures:**
  - Task A accuracy near 0.50: Frequency artifacts insufficient; try pixel-space or hybrid features
  - Task B shows confusion within same model family (SD variants): Architectural similarity dominates over fine-grained differences
  - Large train-test gap: Overfitting to generator-specific artifacts; consider cross-generator validation splits

- **First 3 experiments:**
  1. **Hybrid feature ablation:** Combine frequency + pixel-space ResNet-50 features; measure improvement over frequency-only baseline.
  2. **Generator family confusion matrix:** Analyze Task B errors to determine if SD variants confuse more with each other than with DALL-E/MidJourney.
  3. **Perturbation robustness test:** Evaluate baseline on perturbed variants (JPEG compression, Gaussian noise); quantify performance degradation per perturbation type.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can sophisticated fingerprinting techniques significantly improve the low baseline accuracy (0.45) for identifying the specific generative model used?
- **Basis in paper:** The conclusion states future research should focus on "developing more sophisticated fingerprinting techniques for model attribution."
- **Why unresolved:** The ResNet-50 baseline struggles to differentiate between the five modern generators, indicating that simple frequency-domain features are insufficient for this multi-class task.
- **What evidence would resolve it:** A detection methodology achieving substantially higher accuracy than 0.45 on the Task B test set.

### Open Question 2
- **Question:** Does leveraging the semantic alignment between captions and images via cross-modal learning improve detection robustness?
- **Basis in paper:** The authors explicitly suggest "exploring cross-modal learning approaches that leverage caption-image relationships" as a future direction.
- **Why unresolved:** The provided baseline relies solely on visual frequency features; the utility of the paired textual data (captions) for detecting synthetic images remains untested in this context.
- **What evidence would resolve it:** Benchmark results from a multi-modal detector that outperforms the image-only baseline on the MS COCOAI dataset.

### Open Question 3
- **Question:** How robust are detection methods against the specific perturbations (e.g., JPEG compression, Gaussian noise) included in the dataset?
- **Basis in paper:** The conclusion calls for "improving detector robustness against common image transformations," and the dataset includes distinct perturbed variants for this purpose.
- **Why unresolved:** While the dataset provides these variants, the baseline results reported (0.80 accuracy) do not isolate performance on the perturbed subsets versus clean images.
- **What evidence would resolve it:** A comparative evaluation showing detection accuracy on perturbed images (e.g., JPEG quality 50, Gaussian noise) relative to the baseline.

## Limitations

- **Frequency-domain preprocessing details are underspecified**, particularly magnitude vs. complex component usage and normalization
- **Training hyperparameters are not reported**, including optimizer, scheduler, batch size, and epochs
- **MS COCO caption selection methodology could introduce semantic bias** toward certain image types

## Confidence

- **High:** Dataset construction methodology and size claims (96K images with clear splits)
- **Medium:** Frequency-domain effectiveness for Task A (0.801 accuracy suggests signal exists but mechanism not fully verified)
- **Low:** Task B attribution feasibility claims (0.449 accuracy indicates fundamental difficulty; unclear if better architectures could bridge gap)

## Next Checks

1. Implement and compare multiple frequency preprocessing variants (magnitude-only, complex channels, log scaling) to isolate which transformations drive baseline performance
2. Conduct ablation study: Train identical ResNet-50 on pixel-space features to quantify frequency domain advantage
3. Perform class confusion analysis on Task B to identify if attribution errors cluster within architectural families (SD variants vs. DALL-E/MidJourney)