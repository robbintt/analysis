---
ver: rpa2
title: 'MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis'
arxiv_id: '2511.18676'
source_url: https://arxiv.org/abs/2511.18676
tags:
- size
- image
- medical
- vlms
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Current vision-language models (VLMs) are optimized for categorical\
  \ or descriptive medical image analysis, leaving a critical gap in quantitative\
  \ reasoning\u2014essential for clinical tasks like tumor size estimation and joint\
  \ angle measurement. To address this, we introduce MedVision, a large-scale dataset\
  \ spanning 22 public datasets with 30.8 million image-annotation pairs across diverse\
  \ anatomies and modalities."
---

# MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis

## Quick Facts
- **arXiv ID:** 2511.18676
- **Source URL:** https://arxiv.org/abs/2511.18676
- **Reference count:** 40
- **Primary result:** Supervised fine-tuning on MedVision improves VLM performance on quantitative medical image tasks, but small object detection remains challenging.

## Executive Summary
Current vision-language models excel at categorical and descriptive medical image analysis but struggle with quantitative reasoning tasks essential for clinical applications like tumor size estimation and joint angle measurement. MedVision addresses this gap by introducing a large-scale dataset of 30.8 million image-annotation pairs spanning 22 public datasets across diverse anatomies and modalities. The benchmark evaluates three quantitative tasks: anatomical structure detection, tumor/lesion size estimation, and angle/distance measurement. While off-the-shelf VLMs perform poorly on these tasks, supervised fine-tuning on MedVision significantly improves detection metrics and measurement accuracy, though challenges remain particularly for small objects and precise angle measurements.

## Method Summary
MedVision constructs a large-scale dataset by combining 22 public medical imaging datasets, generating 30.8 million image-annotation pairs across CT, MRI, X-ray, ultrasound, and PET modalities. Annotations are derived from segmentation masks through bounding box fitting, ellipse fitting for tumor/lesion size estimation, and landmark-based angle/distance measurements. The benchmark evaluates three quantitative tasks using a 70/30 train/test split at the patient level. Models are fine-tuned using LoRA with specified parameters (r=16, α=16, dropout=0.05) applied to all linear projections, with embedding layers and LM heads trainable. Detection tasks use axial slices while angle/distance tasks evaluate across all three anatomical planes.

## Key Results
- VLMs optimized for categorical tasks show poor performance on quantitative medical image analysis benchmarks
- Supervised fine-tuning on MedVision significantly improves detection metrics (recall, precision, F1, IoU) and measurement accuracy
- Small object detection (<5% relative size) remains the primary bottleneck, with steep performance degradation
- Multi-plane generalization is challenging, with models showing performance degradation when evaluated on coronal/sagittal planes after axial-only training

## Why This Works (Mechanism)
The dataset's effectiveness stems from its comprehensive coverage of medical imaging modalities and anatomies, combined with precise quantitative annotations derived from segmentation masks. By focusing on three clinically relevant quantitative tasks—detection, size estimation, and measurement—the benchmark addresses fundamental limitations in current VLMs' ability to perform numerical reasoning and spatial analysis in medical contexts.

## Foundational Learning
- **Medical image segmentation to annotation conversion:** Why needed - To generate ground truth for quantitative tasks from existing segmentation masks. Quick check - Verify bounding box fitting and ellipse fitting produce accurate measurements matching original mask properties.
- **Physical spacing extraction and scaling:** Why needed - To convert pixel measurements to clinically meaningful physical units. Quick check - Confirm spacing values extracted from image headers match the units used in annotation generation.
- **3D volume slicing strategies:** Why needed - To convert 3D medical volumes into 2D slices for VLM processing while preserving anatomical context. Quick check - Verify RAS+ orientation and consistent slice selection across all datasets.
- **Quantitative metric computation:** Why needed - To evaluate model performance on continuous measurements rather than categorical predictions. Quick check - Confirm MAE, MRE, and success rate calculations match the paper's definitions for each task.

## Architecture Onboarding

### Component Map
Data Loading -> Preprocessing (slicing, spacing extraction) -> Model Loading (VLM + LoRA) -> Prompt Template Application -> Fine-tuning (SFT) -> Evaluation (metric computation)

### Critical Path
The critical path for reproduction involves: (1) correctly loading and preprocessing MedVision data with proper physical spacing extraction, (2) configuring LoRA fine-tuning with the specified parameters targeting all linear projections, and (3) implementing the prompt templates for each quantitative task to ensure consistent instruction-following.

### Design Tradeoffs
The paper trades model generalization for task-specific performance by using supervised fine-tuning rather than prompting or zero-shot approaches. This decision enables significant performance improvements but requires task-specific training data and may reduce the models' ability to handle novel quantitative tasks without additional fine-tuning.

### Failure Signatures
- Small object detection failure (<5% relative size showing low IoU)
- Physical spacing mismatches causing measurement errors
- Instruction-following failures manifesting as low success rates
- Mode collapse in continuous value predictions (limited discrete value sets)

### Three First Experiments
1. **Baseline evaluation:** Test off-the-shelf VLMs on all three tasks without fine-tuning to establish performance floors
2. **SFT validation:** Fine-tune a single VLM (e.g., Qwen2.5-VL-7B) on each task separately and compare metric improvements
3. **Size-based performance analysis:** Evaluate detection performance across different bounding box-to-image ratio ranges to quantify small object limitations

## Open Questions the Paper Calls Out
1. **Small object detection mechanisms:** What architectural modifications would enable VLMs to reliably detect anatomical structures occupying less than 5% of image area? The paper demonstrates SFT improves overall performance but doesn't propose solutions for small object limitations.

2. **Cross-plane generalization:** Can VLMs achieve clinically acceptable generalization across imaging planes without explicit multi-plane training data? The paper shows OOD performance degradation but doesn't determine if multi-plane training is necessary.

3. **Continuous value prediction mechanisms:** What causes VLMs to predict limited discrete value sets for continuous measurements? The paper observes this failure mode but doesn't investigate whether it stems from tokenizer limitations or architectural constraints.

## Limitations
- Missing training hyperparameters (learning rate, batch size, optimizer configuration, warmup steps)
- Ambiguous LoRA target module specification ("all linear projections")
- Small object detection remains challenging with significant performance degradation
- Physical spacing mismatches between VLMs' resize strategies can introduce measurement errors

## Confidence

**High Confidence:** Dataset construction methodology, task definitions, and evaluation metrics are clearly specified. Claims about VLMs struggling with quantitative reasoning are well-supported by benchmark results.

**Medium Confidence:** LoRA fine-tuning approach is described but implementation details and training configuration introduce uncertainty. Improvement claims depend on proper execution of unspecified hyperparameters.

**Low Confidence:** Assertion that supervised fine-tuning "significantly improves performance" lacks precision without knowing magnitude of improvement across all models and tasks.

## Next Checks

1. **Small Object Detection Analysis:** Segment benchmark results by bounding box-to-image ratio groups (e.g., <5%, 5-10%, >10%) to quantify performance degradation for small structures.

2. **Physical Spacing Verification:** For angle/distance measurement tasks, verify that physical spacing values extracted from image headers are correctly scaled and matched to each VLM's actual preprocessing output.

3. **Baseline Instruction-Following Analysis:** For models with low success rates, inspect unparsed outputs to identify format issues and compare output distributions against other baselines.