---
ver: rpa2
title: Taming Transformer Without Using Learning Rate Warmup
arxiv_id: '2505.21910'
source_url: https://arxiv.org/abs/2505.21910
tags:
- block
- matrix
- training
- attention
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper identifies spectral energy concentration (SEC) of $Wq^\top
  Wk$ in Transformer attention layers as the root cause of model crash. When SEC occurs,
  attention maps become sparse and low-rank, leading to training instability.
---

# Taming Transformer Without Using Learning Rate Warmup
## Quick Facts
- arXiv ID: 2505.21910
- Source URL: https://arxiv.org/abs/2505.21910
- Reference count: 40
- The paper identifies spectral energy concentration (SEC) of $W_q^\top W_k$ in Transformer attention layers as the root cause of model crash.

## Executive Summary
This paper addresses the persistent issue of training instability in Transformers, which has traditionally necessitated learning rate warmup. The authors identify spectral energy concentration (SEC) in attention layers as the fundamental cause of model crashes, where $W_q^\top W_k$ matrices become sparse and low-rank. Through theoretical analysis using Kronecker products and matrix calculus, they demonstrate how low-rank inputs and weight matrices propagate to cause SEC. The proposed solution, AdamW2, introduces a bounded learning rate strategy based on the ratio of singular values between gradients and weights, enabling stable training without warmup. Experiments across ViT, Swin-Transformer, and GPT architectures up to 1B parameters demonstrate successful elimination of warmup while maintaining competitive performance.

## Method Summary
The authors propose a novel optimization strategy called AdamW2 that eliminates the need for learning rate warmup by addressing the root cause of training instability: spectral energy concentration (SEC) in Transformer attention layers. The method bounds the learning rate based on the ratio $\frac{\sigma_1(\nabla W_t)}{\sigma_1(W_{t-1})}$, preventing rapid singular value growth that leads to SEC. This bounded learning rate strategy stabilizes training by maintaining appropriate singular value dynamics in the weight matrices, particularly for $W_q^\top W_k$ in attention mechanisms. The approach is theoretically grounded in analysis of Kronecker products and matrix calculus, showing how low-rank inputs and weight matrices propagate to cause SEC.

## Key Results
- Successfully trains Transformer models up to 1B parameters without learning rate warmup
- Achieves performance comparable to standard AdamW with warmup across ViT, Swin-Transformer, and GPT architectures
- Demonstrates stable training through bounded learning rate strategy based on singular value ratios
- Provides theoretical proof linking low-rank inputs/weights to spectral energy concentration in attention layers

## Why This Works (Mechanism)
The mechanism centers on spectral energy concentration (SEC) in Transformer attention layers. When $W_q^\top W_k$ matrices become sparse and low-rank, attention maps lose diversity and the model crashes. The authors prove that low-rank inputs and weight matrices propagate through the attention mechanism to create SEC. Their AdamW2 optimizer prevents this by bounding the learning rate based on the ratio of the largest singular values between the gradient and previous weight matrices. This constraint prevents rapid singular value growth that would otherwise lead to the sparse, low-rank attention patterns characteristic of SEC.

## Foundational Learning
1. **Kronecker Products** - Why needed: Used to analyze the structure of weight matrices in attention layers. Quick check: Verify that $(A \otimes B)(C \otimes D) = AC \otimes BD$ for compatible matrices.

2. **Matrix Calculus** - Why needed: Essential for deriving gradient updates and analyzing singular value dynamics. Quick check: Confirm that $\frac{\partial}{\partial X} \text{tr}(AXB) = A^\top B^\top$.

3. **Singular Value Decomposition** - Why needed: Central to understanding spectral energy concentration and the bounded learning rate mechanism. Quick check: Verify that $A = U\Sigma V^\top$ where $\Sigma$ contains singular values in descending order.

4. **Attention Mechanism Mathematics** - Why needed: Understanding how $W_q^\top W_k$ affects attention map properties. Quick check: Confirm that attention scores are computed as $\text{softmax}(Q/K\sqrt{d_k})$ where $Q=XW_q$ and $K=XW_k$.

5. **Optimization Theory** - Why needed: Understanding how learning rate scheduling affects training stability. Quick check: Verify that warmup gradually increases learning rate to avoid large gradient steps early in training.

## Architecture Onboarding
**Component Map**: Input -> Linear (W_q, W_k, W_v) -> Attention Scores (W_q^âŠ¤W_k) -> Softmax -> Weighted Values -> Output
**Critical Path**: The attention computation path where $W_q^\top W_k$ is formed and softmax is applied. SEC occurs when this matrix becomes sparse and low-rank.
**Design Tradeoffs**: The bounded learning rate in AdamW2 trades off aggressive early learning for stability, eliminating warmup but potentially slowing early convergence.
**Failure Signatures**: Training loss plateaus or explodes, attention maps become sparse (many near-zero entries), singular values of $W_q^\top W_k$ concentrate in few components.
**3 First Experiments**:
1. Train a small ViT on CIFAR-10 with AdamW2 vs AdamW with warmup, comparing loss curves and final accuracy
2. Visualize attention maps during training to observe sparsity development in both optimizers
3. Measure singular value distributions of $W_q^\top W_k$ matrices across training steps for both methods

## Open Questions the Paper Calls Out
None

## Limitations
- The bounded learning rate strategy introduces a new hyperparameter that may require tuning for different model families or tasks
- Empirical validation is primarily focused on image and language tasks, with unexplored performance on other modalities
- The theoretical framework relies on assumptions about high-dimensional, stochastic training scenarios that may not capture all failure modes

## Confidence
- **High**: The mathematical analysis of how low-rank inputs and weight matrices contribute to SEC is rigorous and well-explained
- **Medium**: The experimental results across different Transformer architectures support the practical effectiveness of the proposed optimization strategy
- **Medium**: The claim that warmup can be eliminated without performance degradation is substantiated for the tested models and tasks

## Next Checks
1. Test the AdamW2 optimization strategy on Transformer variants for tasks beyond vision and language, such as speech or multimodal learning, to assess generalizability
2. Conduct ablation studies to determine the sensitivity of the bounded learning rate hyperparameter to model size, dataset characteristics, and task complexity
3. Compare the final model performance and convergence behavior of AdamW2 against other modern optimizers that also aim to stabilize training without warmup