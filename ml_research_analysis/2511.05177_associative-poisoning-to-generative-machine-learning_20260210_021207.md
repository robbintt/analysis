---
ver: rpa2
title: Associative Poisoning to Generative Machine Learning
arxiv_id: '2511.05177'
source_url: https://arxiv.org/abs/2511.05177
tags:
- poisoning
- features
- attack
- data
- associative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces associative poisoning, a novel data poisoning
  attack that manipulates statistical associations between feature pairs in generated
  data without requiring control over the training process. The attack modifies training
  data to induce or suppress feature correlations while preserving marginal distributions
  and output quality.
---

# Associative Poisoning to Generative Machine Learning

## Quick Facts
- arXiv ID: 2511.05177
- Source URL: https://arxiv.org/abs/2511.05177
- Reference count: 40
- Introduces associative poisoning attack that manipulates feature correlations while preserving output quality

## Executive Summary
This paper presents associative poisoning, a novel data poisoning attack targeting generative machine learning models. The attack manipulates statistical associations between feature pairs in generated data without requiring control over the training process. By modifying training data to induce or suppress feature correlations while preserving marginal distributions and output quality, the attack represents a previously unexplored vulnerability in generative modeling.

The method achieves significant shifts in mutual information and Matthews correlation coefficient between targeted features while maintaining statistical similarity in marginal probabilities and Fréchet Inception Distance. Empirical evaluations on D-GAN and DDPM-IP models trained on CelebA and Recipe1M datasets demonstrate the attack's effectiveness and stealth, with ablation studies showing linear relationships between poisoning extent and attack effectiveness.

## Method Summary
The associative poisoning attack works by strategically replacing samples in the training data to manipulate feature correlations while preserving marginal distributions. For binary features, the method replaces off-diagonal samples (where features disagree) with on-diagonal samples (where features agree), specifically converting (F1=0,F2=1) pairs to (F1=0,F2=0) and (F1=1,F2=0) pairs to (F1=1,F2=1). The number of replacements is controlled by a parameter x, where n=x·N samples are modified. For continuous features, the attack replaces the joint distribution with its maximum entropy approximation while preserving marginals.

The attack is evaluated on two state-of-the-art generative models: D-GAN trained for 12 million images and DDPM-IP trained for 16 million images. Models are trained on CelebA and Recipe1M datasets, with feature pairs selected for association manipulation. Performance is measured through mutual information, Matthews correlation coefficient, marginal probabilities, and Fréchet Inception Distance, with statistical significance assessed using Mann-Whitney U tests.

## Key Results
- Clean vs poisoned models show MI increases from 0.008 to 0.119 and MCC from 0.131 to 0.483 in one experiment
- Marginal probabilities and FID remain statistically similar, indicating successful stealth
- Ablation study demonstrates linear relationship between poisoning extent and attack effectiveness
- Existing defenses are ineffective against association-level manipulations

## Why This Works (Mechanism)
The attack exploits the fact that generative models learn joint distributions from training data. By manipulating the joint distribution while preserving marginals, the attack creates systematic biases in generated samples that are difficult to detect. The replacement strategy ensures that the model learns spurious correlations between features, which manifest in generated data even though the overall data quality appears unchanged.

## Foundational Learning
- **Mutual Information (MI):** Measures statistical dependence between variables; needed to quantify association strength between features; quick check: MI=0 for independent features, MI>0 for dependent features
- **Matthews Correlation Coefficient (MCC):** Balanced measure of binary classification quality; needed to evaluate correlation changes between binary features; quick check: MCC ranges from -1 to +1, with 0 indicating random correlation
- **Frechet Inception Distance (FID):** Measures similarity between distributions of generated and real data; needed to verify output quality preservation; quick check: lower FID indicates better quality, typically <50 for good models
- **Mann-Whitney U test:** Non-parametric test for comparing distributions; needed to assess statistical significance of changes; quick check: p<0.05 indicates significant difference
- **Marginal probability preservation:** Ensures that individual feature distributions remain unchanged; needed for stealth; quick check: compare feature histograms before and after poisoning

## Architecture Onboarding

**Component Map:** Training Data -> Poisoning Algorithm -> Generative Model (D-GAN/DDPM-IP) -> Generated Samples -> Classifier -> Metrics (MI, MCC, MP, FID)

**Critical Path:** The attack succeeds when poisoned training data causes the generative model to learn and reproduce the manipulated feature associations in generated samples, while classifier evaluations show significant MI/MCC shifts and statistical tests confirm marginal preservation.

**Design Tradeoffs:** Higher poisoning extent increases attack effectiveness but may risk detection through subtle quality degradation. Binary features are easier to manipulate than continuous features due to discrete state spaces. The choice of generative model architecture affects vulnerability to association manipulation.

**Failure Signatures:** If MI/MCC shifts are not observed in generated data, verify that training data joint probabilities changed as expected. If marginal distributions shift, check the replacement sampling procedure. If FID increases significantly, the poisoning may be too aggressive.

**First Experiments:**
1. Implement binary poisoning on a small synthetic dataset with known feature correlations and verify marginal preservation
2. Train a simple generative model (e.g., VAE) on poisoned data and measure correlation shifts in generated samples
3. Compare attack effectiveness across different generative model architectures (D-GAN vs DDPM)

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameters for D-GAN and DDPM-IP must be reverse-engineered from cited papers
- Claims about existing defenses being ineffective lack empirical validation against specific mechanisms
- Assumes access to clean validation/test data for replacement sampling, which may not be realistic
- Statistical analysis doesn't address multiple comparisons across numerous feature pairs

## Confidence
- **High Confidence:** Mathematical formulation of binary poisoning algorithm and marginal-preserving properties
- **Medium Confidence:** Empirical results showing MI/MCC shifts are convincing but practical significance needs validation
- **Low Confidence:** Claim about existing defenses being ineffective lacks systematic empirical substantiation

## Next Checks
1. Systematically vary D-GAN/DDPM-IP learning rates and batch sizes to determine if attack effectiveness depends on specific training configurations
2. Implement and evaluate at least three state-of-the-art data poisoning defenses against the associative poisoning attack
3. Apply the attack to additional datasets beyond CelebA and Recipe1M to verify generalization across different data modalities and distributions