---
ver: rpa2
title: Towards Automated Safety Requirements Derivation Using Agent-based RAG
arxiv_id: '2504.11243'
source_url: https://arxiv.org/abs/2504.11243
tags:
- safety
- context
- answer
- pipeline
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an agent-based retrieval-augmented generation
  (RAG) approach for automated derivation of safety requirements in self-driving vehicles.
  The key idea is to use document agents that preprocess and summarize source material
  before retrieval, improving the relevance of context provided to the LLM.
---

# Towards Automated Safety Requirements Derivation Using Agent-based RAG

## Quick Facts
- arXiv ID: 2504.11243
- Source URL: https://arxiv.org/abs/2504.11243
- Reference count: 8
- Primary result: Agent-based RAG significantly improves retrieval precision and context relevance metrics for safety requirements derivation while maintaining similar answer similarity scores.

## Executive Summary
This paper presents an agent-based retrieval-augmented generation (RAG) approach for automated derivation of safety requirements in self-driving vehicles. The key innovation is using document agents that preprocess and summarize source material before retrieval, improving the relevance of context provided to the LLM. The approach is evaluated on a dataset of safety requirement questions derived from the Apollo automated driving perception system, comparing against default RAG and LLM-only baselines. Results demonstrate that agent-based RAG significantly improves retrieval precision and context relevance metrics while maintaining similar answer similarity scores.

## Method Summary
The approach employs document agents that preprocess source documents through hierarchical summarization before retrieval. Each document agent maintains both a Vector Store Index for factual queries and a Summary Index for summarization queries, creating a tree-like structure with summaries at multiple hierarchy levels. A top-level document agent first selects the top-3 relevant documents based on document-level descriptions, then routes queries to appropriate query engines within each document agent. The refined contexts from selected documents are combined with the user query and passed to the main LLM for final safety requirement generation.

## Key Results
- Agent-based RAG achieves significantly higher retrieval precision (RP = 0.54) compared to default RAG (RP = 0.21)
- Context relevance metrics (Augmentation Accuracy, Answer Consistency) are substantially improved with agent-based RAG
- Answer similarity scores remain comparable across all approaches despite improved context relevance
- Agent-based RAG is more successful in fusing information from different document sources

## Why This Works (Mechanism)

### Mechanism 1
Document agents with hierarchical summarization produce more semantically relevant context than raw chunk retrieval. Each document agent maintains both a Vector Store Index (for factual queries) and a Summary Index (for summarization queries). The Summary Index creates a tree-like structure with summaries at multiple hierarchy levels, condensing information before retrieval. When a query arrives, the appropriate index is selected, yielding pre-processed, semantically dense context rather than raw document chunks. This assumes summarization preserves query-relevant information while reducing noise without hallucinating critical details.

### Mechanism 2
Two-stage document and query-engine selection reduces irrelevant context retrieval compared to single-stage similarity search. A Top-level Document Agent first selects the top-3 relevant documents based on document-level descriptions. Within each selected document agent, a second selection chooses between the vector query engine (for facts) or summary query engine (for summarization) based on query type. This cascaded filtering removes documents and retrieval modes unlikely to contain relevant information before context generation. This assumes document-level descriptions sufficiently characterize content for accurate first-stage filtering and query intent can be classified into factual vs. summarization categories reliably.

### Mechanism 3
Context pre-processing by agents improves grounding metrics (Augmentation Accuracy, Answer Consistency) even when final answer similarity remains comparable. Agent-based RAG discards context that does not contain relevant information during pre-processing. This means the final LLM receives only filtered, refined context, increasing the probability that generated answers derive from retrieved context rather than parametric knowledge. Default RAG passes raw chunks regardless of relevance, forcing the LLM to ignore irrelevant context or hallucinate. This assumes filtering criteria accurately identify irrelevant context and relevant context is sufficient to answer the query without requiring parametric knowledge supplementation.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Why needed here: The paper's core contribution is an architectural improvement to RAG; understanding baseline RAG (embedding documents → vector store → similarity retrieval → context injection) is prerequisite to appreciating why document agents help. Quick check question: Given a query "What safety requirements apply to camera-based object detection?", can you trace how a standard RAG system would retrieve context and generate an answer?

- **Automotive Safety Standards (ISO 26262, ISO 21448/SOTIF)**: Why needed here: The domain context shapes what constitutes a "good" safety requirement. The paper uses ISO standards as source documents and evaluates on Apollo perception system requirements; understanding ASIL ratings, HARA, and safety goals clarifies why precision matters. Quick check question: What is the difference between functional safety (ISO 26262) and safety of intended functionality (ISO 21448), and why might both be relevant for an autonomous perception system?

- **LLM Evaluation Metrics for RAG (Retrieval Precision, Augmentation Accuracy, Answer Consistency)**: Why needed here: The paper's conclusions rely on comparing metrics across approaches. Without understanding what RP, AA, AP, and AC measure—and why NASS alone is insufficient—you cannot interpret the results. Quick check question: Why might two systems have similar answer similarity scores but different retrieval precision scores? What does this imply about their grounding?

## Architecture Onboarding

- **Component map**: User Query → Top-level Document Agent → Document Tools (descriptions) → Document Agents (Vector Store Index + Summary Index + Query Engine) → Refined Contexts → Main LLM → Safety Requirement

- **Critical path**: 1. User query → Top-level Document Agent 2. Top-level agent retrieves top-3 relevant Document Tools based on descriptions 3. For each selected Document Tool → underlying Document Agent chooses vector vs. summary query engine 4. Each Document Agent produces refined context (or discards if irrelevant) 5. Refined contexts + user query → Main LLM → Safety requirement output 6. (Evaluation) Judge LLM scores retrieved context, answer, and reference

- **Design tradeoffs**: Latency vs. Precision: Agent-based RAG requires multiple LLM calls (document selection, query engine selection, summarization) per query, increasing latency compared to single-retrieval default RAG. Summary Quality vs. Information Preservation: Aggressive summarization improves precision but may omit domain-specific terminology critical for safety requirements; the paper does not quantify this risk. Metric Sensitivity: NASS/BERT scores fail to capture nuanced quality differences useful to human reviewers; grounding metrics (RP, AA, AC) are more discriminative but require additional LLM evaluations.

- **Failure signatures**: Low Retrieval Precision with default RAG: Retriever extracts all top chunks from the same (often irrelevant) document, matching keywords but missing semantic meaning. High NASS but Low Augmentation Accuracy: LLM generates plausible answers from parametric knowledge rather than retrieved context, undermining grounding. Context discarded for all documents: Query falls outside document scope or document descriptions are mismatched; system returns answer with no context grounding.

- **First 3 experiments**: 1. **Baseline Reproduction**: Implement default RAG (chunk documents → FAISS → similarity retrieval → GPT-3.5 generation) on the Apollo dataset; measure all five metrics. This establishes the performance floor and validates your evaluation pipeline against paper results. 2. **Ablation on Summary Index**: Implement document agents with Vector Store Index only (no Summary Index) to isolate the contribution of hierarchical summarization to retrieval precision. Compare RP and AA against full agent-based RAG. 3. **Sensitivity to Document Description Quality**: Manually degrade or improve the 1-2 line document descriptions for Document Tools; measure impact on top-3 retrieval accuracy and downstream metrics. This tests the assumption that descriptions suffice for first-stage filtering.

## Open Questions the Paper Calls Out

### Open Question 1
Does the use of agent-based RAG significantly improve answer similarity scores (NASS) when applied to more complex safety datasets or smaller LLMs with less generic knowledge? The authors state: "As next steps, we will test our strategy with more complex data sets or simpler LLM models... to establish a closer connection to the retrieved context relevance and the LLM answer quality." This remains unresolved because the current dataset and model (GPT-3.5) allowed the LLM to leverage generic knowledge to achieve high similarity scores, potentially masking the benefits of the improved context retrieval.

### Open Question 2
Do human safety experts prefer the requirements generated by agent-based RAG over default RAG, specifically regarding the density of useful system-specific details? The authors note that "a verification with human expert reviews was out of scope," but they hypothesize that a human metric would prefer agent-based RAG "due to its higher density of potentially useful details." This remains unresolved because current automated metrics (like NASS) showed that RAG-assisted answers contained additional useful information (e.g., specific sensor modalities), but this did not result in a statistically higher similarity score.

### Open Question 3
Can automated evaluation metrics be refined to distinguish between factually similar answers and those containing critical, system-specific nuances? The paper observes that "similarity metrics such as NASS or BERT score are not able to capture subtle details" (like specific sensor fusion strategies) that distinguish a generic answer from a high-quality safety requirement. This remains unresolved because the results showed similar NASS scores across all methods despite the agent-based RAG retrieving significantly more relevant context.

## Limitations
- Evaluation dataset is relatively small (30 test cases) and domain-specific to Apollo perception systems, limiting generalizability
- No quantitative analysis of information preservation during hierarchical summarization
- Heavy reliance on GPT-3.5 for both generation and evaluation may introduce bias
- Trade-off between latency and precision is not quantified

## Confidence
**High Confidence**: The retrieval precision improvement is well-supported by experimental results showing RP values of 0.54 (agent-based) vs. 0.21 (default RAG) on the same test set. The architectural description is detailed and reproducible.

**Medium Confidence**: The mechanism by which document agents improve context relevance through hierarchical summarization is plausible but relies on assumptions about summary quality that aren't empirically validated. The grounding metric improvements are convincing but could be influenced by judge LLM bias.

**Low Confidence**: Claims about real-world applicability to diverse safety-critical domains lack supporting evidence. The trade-off analysis between latency and precision is not quantified, and potential hallucinations from summarization are acknowledged but not measured.

## Next Checks
1. **Summary Preservation Analysis**: Implement an automated comparison of safety terminology coverage between original document chunks and their hierarchical summaries to quantify information loss during preprocessing.

2. **Cross-Domain Evaluation**: Test the agent-based RAG architecture on safety requirement questions from a different autonomous vehicle domain (e.g., planning vs. perception) or a non-automotive safety-critical system to assess generalizability.

3. **Judge LLM Bias Assessment**: Repeat the grounding metric evaluation using a different judge LLM (e.g., Claude or Gemini) or a human safety expert panel to validate that observed improvements aren't artifacts of GPT-3.5's internal knowledge consistency.