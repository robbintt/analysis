---
ver: rpa2
title: 'Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric
  Visual Chains'
arxiv_id: '2504.20199'
source_url: https://arxiv.org/abs/2504.20199
tags:
- visual
- reasoning
- arxiv
- image
- multi-image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Focus-Centric Visual Chain, a multi-image
  reasoning paradigm that enhances vision-language models' ability to process complex
  visual scenarios. The approach decomposes complex tasks into focused sub-questions,
  progressively aggregating visual evidence across multiple images.
---

# Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains

## Quick Facts
- **arXiv ID**: 2504.20199
- **Source URL**: https://arxiv.org/abs/2504.20199
- **Reference count**: 19
- **Primary result**: Focus-Centric Visual Chain approach achieves 3.16% and 2.24% average accuracy improvements across seven multi-image benchmarks, setting new state-of-the-art results on four benchmarks.

## Executive Summary
This paper introduces Focus-Centric Visual Chain, a multi-image reasoning paradigm that enhances vision-language models' ability to process complex visual scenarios. The approach decomposes complex tasks into focused sub-questions, progressively aggregating visual evidence across multiple images. To enable this paradigm, the authors propose Focus-Centric Data Synthesis, a bottom-up framework that synthesizes high-quality multi-image reasoning data using open-source models. This framework constructs VISC-150K, a 150K-sample dataset featuring elaborate reasoning paths and multi-image compositions. Experimental results demonstrate consistent performance improvements across seven multi-image benchmarks, with average accuracy gains of 3.16% and 2.24% for two distinct model architectures. The method achieves new state-of-the-art results on four benchmarks while maintaining general vision-language capabilities, representing a significant advancement in handling complex multi-image tasks.

## Method Summary
The Focus-Centric Visual Chain approach addresses multi-image reasoning by decomposing complex tasks into focused sub-questions, each targeting specific visual evidence across images. The method introduces a novel data synthesis framework that constructs reasoning paths from object-level visual features to event-level reasoning goals. The VISC-150K dataset is synthesized through a bottom-up process that starts with object detection, establishes pairwise relevance relationships between images, and constructs progressive reasoning paths. The framework employs four core mechanisms: object detection for identifying visual elements, pairwise relevance annotation for establishing relationships, path construction for creating reasoning sequences, and compositional image selection for building diverse multi-image scenarios. The approach is evaluated on seven benchmarks including MultiImageBench, VQAv2-MultiImage, OK-VQA-MI, and ScienceQA-MultiImage, demonstrating consistent improvements over baseline models.

## Key Results
- Focus-Centric Visual Chain achieves 3.16% and 2.24% average accuracy improvements on two distinct model architectures across seven multi-image benchmarks
- The method sets new state-of-the-art results on four benchmarks: ScienceQA-MultiImage, MathVista-MI, MM-Vet, and MMMU-MI
- Performance gains are consistent across all evaluated benchmarks, with improvements ranging from 1.26% to 6.54% depending on the specific task and model architecture

## Why This Works (Mechanism)
The Focus-Centric Visual Chain paradigm works by progressively narrowing attention through hierarchical reasoning, allowing models to focus on relevant visual evidence while filtering out noise. By decomposing complex multi-image questions into focused sub-questions, the approach reduces cognitive load and enables more precise reasoning. The data synthesis framework ensures that training data mirrors the progressive reasoning patterns required for multi-image understanding, creating a curriculum that guides models from simple object-level reasoning to complex event-level comprehension. The pairwise relevance annotation mechanism establishes explicit relationships between images, enabling models to understand how visual evidence connects across the image sequence. This structured approach to multi-image reasoning addresses the core challenge of information overload that plagues traditional vision-language models when processing multiple images simultaneously.

## Foundational Learning
- **Multi-image reasoning**: Understanding relationships and context across multiple visual inputs; needed because traditional models struggle with information overload when processing image sequences
- **Progressive reasoning paths**: Breaking down complex tasks into sequential sub-questions; needed to reduce cognitive load and enable focused attention
- **Pairwise relevance annotation**: Establishing explicit relationships between images; needed to create structured understanding of how visual evidence connects
- **Bottom-up data synthesis**: Constructing training data from object-level features to event-level reasoning; needed to create high-quality synthetic data that mirrors complex reasoning patterns
- **Focus mechanisms**: Implementing attention systems that progressively narrow scope; needed to filter noise and maintain reasoning accuracy across image sequences

## Architecture Onboarding

### Component Map
Object Detection -> Pairwise Relevance Annotation -> Path Construction -> Compositional Image Selection -> Focus-Centric Visual Chain

### Critical Path
The critical path begins with object detection to identify visual elements, followed by pairwise relevance annotation to establish relationships between images. Path construction then creates reasoning sequences from these relationships, and compositional image selection builds diverse multi-image scenarios. The Focus-Centric Visual Chain uses these components to guide progressive reasoning through focused sub-questions.

### Design Tradeoffs
The quadratic computational complexity of pairwise relevance annotation limits scalability but ensures comprehensive relationship mapping. The bottom-up synthesis approach trades real-world data diversity for controlled, high-quality training examples. The focus mechanism balances between maintaining context and filtering noise, with performance degradation observed beyond 15 images.

### Failure Signatures
Performance degradation when processing sequences exceeding 15 images, with interference from irrelevant data patterns. Limited effectiveness on structured visual content like charts and diagrams. Computational bottlenecks in pairwise relevance annotation stage preventing efficient synthesis for larger image sets.

### First Experiments
1. Evaluate the model's performance on real-world multi-image datasets not seen during training to assess true generalization capabilities
2. Conduct ablation studies to quantify the impact of different components of the Focus-Centric Visual Chain approach on overall performance
3. Test the method's robustness by introducing varying levels of image quality degradation, occlusions, or irrelevant distractor images in the multi-image scenarios

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: Can the Focus-Centric Visual Chain paradigm effectively generalize to structured visual inputs like charts, diagrams, and code snippets, which may require different reasoning patterns than natural images?
- **Basis in paper**: [explicit] The authors state in the Limitations section that the VISC-150K dataset focuses on photographs and comics, and the approach remains "untested on structured visual content such as charts, diagrams, and code snippets."
- **Why unresolved**: The current data synthesis framework relies on object-oriented and event-oriented detection suited for natural scenes; it is unclear if this logic transfers to the symbolic and spatial relationships inherent in charts or code.
- **What evidence would resolve it**: Evaluation results of models fine-tuned on VISC-150K (or a variant) specifically on chart-understanding or document-visual-answering benchmarks.

### Open Question 2
- **Question**: How can the quadratic computational complexity of the pairwise relevance annotation stage be reduced to enable efficient synthesis for datasets with significantly larger image counts?
- **Basis in paper**: [explicit] The paper identifies that the "Focus-Centric Data Synthesis framework requires pairwise relevance annotation across images, leading to quadratic computational complexity," which currently limits the size of image sets processed.
- **Why unresolved**: The current architecture requires comparing every image against every other image to establish edges, creating a computational bottleneck that restricts scalability.
- **What evidence would resolve it**: A modified synthesis algorithm that utilizes sparse sampling or clustering to achieve linear or log-linear complexity while maintaining data quality.

### Open Question 3
- **Question**: What mechanisms can mitigate the "amplified noise levels" that cause performance degradation when the model processes sequences of more than 15 images?
- **Basis in paper**: [inferred] The discussion of RQ3 (Figure 5) notes that while performance holds for 3-14 images, there is "slight degradation" beyond 15 images attributed to "interference from irrelevant data patterns."
- **Why unresolved**: The current focus mechanism successfully filters noise in medium-sized sets but appears to saturate or misallocate attention when the visual context window becomes excessively large.
- **What evidence would resolve it**: Ablation studies introducing noise-filtering modules or hierarchical focus mechanisms that stabilize or improve accuracy on tasks with 20+ images.

## Limitations
- The approach relies on synthesized data for training, which may not fully capture the diversity and complexity of real-world multi-image scenarios
- The method's generalization to tasks outside the training distribution remains unclear, with limited evaluation on real-world deployment scenarios
- The pairwise relevance annotation mechanism creates quadratic computational complexity, limiting scalability to larger image sets

## Confidence
- **High confidence** in the technical methodology and implementation details of the Focus-Centric Visual Chain approach
- **Medium confidence** in the performance improvements reported on benchmarks, though real-world applicability remains uncertain
- **Medium confidence** in the effectiveness of the data synthesis framework, as the quality of synthetic data can vary

## Next Checks
1. Evaluate the model's performance on real-world multi-image datasets not seen during training to assess true generalization capabilities
2. Conduct ablation studies to quantify the impact of different components of the Focus-Centric Visual Chain approach on overall performance
3. Test the method's robustness by introducing varying levels of image quality degradation, occlusions, or irrelevant distractor images in the multi-image scenarios