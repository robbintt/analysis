---
ver: rpa2
title: "Instituto de Telecomunica\xE7\xF5es at IWSLT 2025: Aligning Small-Scale Speech\
  \ and Language Models for Speech-to-Text Learning"
arxiv_id: '2506.17019'
source_url: https://arxiv.org/abs/2506.17019
tags:
- speech
- language
- data
- arxiv
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a small-scale (< 2B parameters) unified speech-to-text
  model for ASR, ST, and SQA across English, Chinese, and German. The model uses modality
  alignment and instruction fine-tuning to integrate a pretrained speech encoder with
  a small language model.
---

# Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning

## Quick Facts
- arXiv ID: 2506.17019
- Source URL: https://arxiv.org/abs/2506.17019
- Reference count: 8
- Small-scale (<2B) unified model achieves strong ASR (WER≈15%) but weaker ST and SQA performance across English, Chinese, and German

## Executive Summary
This work presents a compact speech-to-text model integrating ASR, ST, and SQA using modality alignment and instruction fine-tuning. The approach combines a pretrained w2v-BERT 2.0 encoder with small language models (Qwen 2.5 1.5B, Qwen 2.5 0.5B, Phi-4 Mini 1.5B) through a two-stage training curriculum. The model achieves competitive ASR performance while demonstrating the feasibility of speech-language modeling with efficient architectures under 2B parameters.

## Method Summary
The method employs a two-stage training approach: first, modality alignment where adapter layers learn to map speech embeddings into the LM's semantic space while freezing both the speech encoder and LM; second, instruction fine-tuning where all components are jointly trained on mixed ASR, ST, and SQA data. Training uses open-licensed data with CC-BY restrictions, supplemented by multi-model pseudolabeling (4 MT models filtered by COMETKiwi ≥0.85) to expand coverage. Temporal compression through 3×1D convolutions enables efficient processing by reducing sequence length before the speech encoder.

## Key Results
- ASR: WER≈15% on LibriSpeech (competitive with similar models)
- ST: COMET scores show room for improvement, with model often transcribing instead of translating
- SQA: Weak performance with tendency to generate questions rather than answers
- Data efficiency: Multi-model pseudolabeling retains 58-62% of synthetic data vs 62.5% for single-model approaches

## Why This Works (Mechanism)

### Mechanism 1: Two-stage training prevents catastrophic forgetting
Freezing the pretrained LM during alignment stage allows small models to integrate speech capabilities without losing text knowledge. The separation ensures speech-text mapping is established before joint training begins.

### Mechanism 2: Multi-model pseudolabeling with quality filtering
Using 4 MT models plus COMETKiwi quality estimation preserves more data (58-62%) than single-model approaches by selecting the best hypothesis per example and filtering by threshold.

### Mechanism 3: Efficient temporal compression
Three 1D convolutional layers (kernel=3, stride=2) compress speech sequences 8× before the encoder, reducing memory and compute while preserving semantic information density.

## Foundational Learning

- **Concept: Speech-text modality alignment** - Why needed: Core innovation requires understanding how continuous speech embeddings map to discrete text tokens through learned adapters. Quick check: Can you explain why freezing the LM during alignment prevents catastrophic forgetting, and what signals indicate alignment has converged?

- **Concept: Quality estimation for data filtering** - Why needed: Pseudolabeling pipeline depends on COMETKiwi scores; understanding what these metrics measure and their failure modes is critical. Quick check: Why might a high COMETKiwi score still correspond to a poor translation for speech data specifically?

- **Concept: Attention masking for mixed-modality sequences** - Why needed: Architecture uses bidirectional attention for audio positions and causal attention for text; improper masking causes information leakage or training instability. Quick check: What would happen if causal masking were incorrectly applied to audio tokens during training?

## Architecture Onboarding

- **Component map:** Audio → Mel spectrogram (80-dim, stride 2) → 3× 1D Conv → w2v-BERT 2.0 → Modality/Length Adapter (2× Conformer layers) → LM embeddings
- **Critical path:** 1) Load w2v-BERT 2.0 with correct feature extractor 2) Initialize conv layers and adapter to match Qwen hidden size (1536) 3) Stage 1: Train only adapter components on ASR data 4) Stage 2: Enable gradients on all components, switch to mixed-task data
- **Design tradeoffs:** 120-second audio cap limits SQA data but enables tractable memory usage; CC-BY-only constraint improves reproducibility but reduces data volume; small LM enables efficiency but limits multilingual reasoning
- **Failure signatures:** Model outputs transcript instead of translation → task token not learned; repetitive generation → repetition penalty too low or overfitting; SQA generates question → instruct template confusion
- **First 3 experiments:** 1) Baseline replication: Train Stage 1 on LibriSpeech only, verify ASR WER ≈ 0.15 2) Ablate pseudolabeling source: Compare single-model vs multi-model oracle on 500 held-out examples 3) Debug task confusion: Create diagnostic set with identical audio + different task tokens

## Open Questions the Paper Calls Out

- **Question:** Does more balanced training data prevent model from ignoring task tags? Based on findings that ASR-heavy data dominated learning, ablation studies comparing current data ratio against balanced dataset would resolve this.

- **Question:** Does extending maximum audio context beyond 120 seconds enable effective use of SQA data? Technical constraint forced exclusion of longer samples; evaluation on full SQA dataset without truncation would resolve this.

- **Question:** How much does scaling LM to ~3B parameters improve instruction-following vs 0.5B-1.5B variants? Authors plan 3B scale experiments since 1.5B lacked best language modeling capabilities; comparative benchmarks would resolve this.

## Limitations

- ASR performance strong but ST/SQA weaker due to imbalanced training data and potential instruction tuning issues
- 120-second audio cap excludes substantial SQA data from consideration
- Small LM (<2B) may lack sufficient capacity for robust multilingual instruction following
- COMETKiwi quality estimation not validated against human judgments

## Confidence

- **High confidence:** ASR performance claims (WER≈15% on LibriSpeech) well-supported by standard evaluation and align with similar models; two-stage training mechanism clearly described and validated
- **Medium confidence:** ST and SQA performance claims lower due to less robust results and unclear data balance; multi-model pseudolabeling shows promise but lacks external validation
- **Low confidence:** Claims about efficiency gains from temporal compression based on architectural description rather than ablation studies; 8× compression ratio's impact on semantic preservation assumed rather than tested

## Next Checks

1. **Ablation study on compression ratio:** Train variants with 4× and 2× temporal compression to empirically determine maximum safe compression before ASR WER degrades, particularly for fast speech or overlapping speakers.

2. **Human evaluation of pseudolabeled data:** Sample 100 filtered synthetic translations from each language pair and have bilingual annotators rate quality to verify COMETKiwi threshold (≥0.85) correlates with actual translation quality, identifying systematic failure modes.

3. **Task confusion diagnostic:** Create controlled evaluation set with identical audio paired with different task tokens (<|transcribe|>, <|translate|>, <|reply|>) to verify model correctly attends to and follows task instructions, measuring task-specific performance to identify instruction tuning weaknesses.