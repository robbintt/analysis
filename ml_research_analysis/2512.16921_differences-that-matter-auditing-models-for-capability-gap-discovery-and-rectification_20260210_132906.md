---
ver: rpa2
title: 'Differences That Matter: Auditing Models for Capability Gap Discovery and
  Rectification'
arxiv_id: '2512.16921'
source_url: https://arxiv.org/abs/2512.16921
tags:
- arxiv
- image
- auditdm
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AuditDM is an automated framework that discovers and rectifies
  multimodal LLM failure modes by auditing their divergence. It fine-tunes an MLLM
  auditor via reinforcement learning to generate challenging questions and counterfactual
  images that maximize disagreement among target models.
---

# Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification

## Quick Facts
- **arXiv ID**: 2512.16921
- **Source URL**: https://arxiv.org/abs/2512.16921
- **Authors**: Qihao Liu; Chengzhi Mao; Yaojie Liu; Alan Yuille; Wen-Sheng Chu
- **Reference count**: 40
- **Primary result**: AuditDM framework discovers 20+ failure modes in multimodal LLMs and improves model performance across 16 benchmarks

## Executive Summary
AuditDM introduces an automated framework for discovering and rectifying failure modes in multimodal large language models (MLLMs). The system employs a reinforcement learning-trained multimodal large language model (MLLM) auditor that generates challenging questions and counterfactual images designed to maximize disagreement among target models. When applied to Gemma-3 and PaliGemma-2, the framework identifies over 20 distinct failure types. Fine-tuning on these discovered failures consistently improves performance across 16 benchmarks, with a 3B model surpassing its 28B counterpart after training on the audit discoveries.

## Method Summary
AuditDM operates through an automated auditing process where an MLLM auditor is fine-tuned via reinforcement learning to generate inputs that create maximum divergence among target models. The auditor produces both challenging questions and counterfactual images, systematically probing for model weaknesses. This approach discovers failure modes by analyzing where models disagree on the same inputs, then uses these discoveries to guide targeted fine-tuning. The framework is designed to be model-agnostic, allowing it to audit various MLLMs including Gemma-3 and PaliGemma-2.

## Key Results
- Discovers over 20 distinct failure types across tested MLLMs
- Fine-tuning on audit discoveries improves all models across 16 benchmarks
- Enables a 3B model to surpass its 28B counterpart through targeted training
- Demonstrates automated failure mode discovery without human supervision

## Why This Works (Mechanism)
AuditDM works by leveraging the principle that model disagreements reveal capability gaps. The reinforcement learning-trained auditor learns to generate inputs that maximize divergence between models, effectively targeting their weaknesses. By creating counterfactual images and challenging questions that expose these disagreements, the framework systematically uncovers failure modes that might be missed by standard evaluation. The fine-tuning process then uses these discovered failures as targeted training data, allowing models to learn from their mistakes in a more focused manner than general training data.

## Foundational Learning

**Multimodal Large Language Models (MLLMs)**: AI systems that process both text and image inputs, requiring integration of visual and linguistic understanding. *Why needed*: AuditDM specifically targets these models as they represent a complex integration challenge. *Quick check*: Verify the model can process both modalities in a single forward pass.

**Reinforcement Learning from Human Feedback (RLHF)**: Training paradigm where models learn through rewards rather than explicit labels. *Why needed*: Used to train the auditor to generate maximally challenging inputs. *Quick check*: Ensure reward signals are properly scaled and don't lead to degenerate solutions.

**Counterfactual Image Generation**: Creating modified versions of images that test specific aspects of visual understanding. *Why needed*: Essential for systematically probing model weaknesses in visual reasoning. *Quick check*: Confirm generated counterfactuals maintain semantic coherence while introducing targeted perturbations.

**Model Disagreement Analysis**: Measuring divergence in predictions across multiple models on identical inputs. *Why needed*: Forms the core mechanism for discovering failure modes. *Quick check*: Verify disagreement metrics are statistically significant and not due to random noise.

**Fine-tuning with Discovered Failures**: Using audit-discovered weaknesses as targeted training data. *Why needed*: Allows models to specifically address identified capability gaps. *Quick check*: Monitor for catastrophic forgetting of previously learned capabilities.

## Architecture Onboarding

**Component Map**: RL Auditor -> Question/Image Generator -> Target Models -> Disagreement Analyzer -> Fine-tuning Dataset

**Critical Path**: The RL-trained auditor generates inputs → Target models process inputs → Disagreement analysis identifies failures → Fine-tuning uses discovered failures to improve models

**Design Tradeoffs**: The framework balances between exploring diverse failure modes versus exploiting known weaknesses, and between auditor complexity versus training efficiency. The RL approach trades computational cost for automated discovery versus manual failure identification.

**Failure Signatures**: Common failure patterns include spatial reasoning errors, attribute confusion, and context misunderstanding. The framework identifies these through systematic disagreement analysis across multiple model runs.

**First Experiments**:
1. Test auditor's ability to generate inputs that cause maximum disagreement between two baseline models
2. Verify discovered failures are reproducible across multiple model runs
3. Measure improvement after fine-tuning on a small set of discovered failures versus random failures

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily limited to Gemma-3 and PaliGemma-2 models, reducing generalizability
- Auditor effectiveness heavily dependent on base model quality used for RL training
- Reinforcement learning approach may introduce optimization artifacts that bias discovered failure modes

## Confidence
- **High Confidence**: Core methodology of using RL-trained auditors to generate challenging inputs is technically sound
- **Medium Confidence**: Claims of consistent improvement across all 16 benchmarks, dependent on specific experimental conditions
- **Medium Confidence**: Assertion that 3B model can surpass 28B counterpart, highly dependent on discovered failure modes

## Next Checks
1. Test AuditDM on additional MLLM architectures beyond Gemma-3 and PaliGemma-2 to assess generalizability across different model families
2. Conduct ablation studies varying the base model used for RL training to quantify sensitivity to auditor initialization
3. Perform long-term stability analysis to verify that improvements from fine-tuning persist across diverse, temporally separated evaluation datasets