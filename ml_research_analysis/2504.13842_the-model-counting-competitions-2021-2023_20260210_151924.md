---
ver: rpa2
title: The Model Counting Competitions 2021-2023
arxiv_id: '2504.13842'
source_url: https://arxiv.org/abs/2504.13842
tags:
- instances
- counting
- solvers
- solver
- competition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive overview of the 2021-2023 Model
  Counting Competitions, which aim to advance applications, identify challenging benchmarks,
  foster new solver development, and enhance existing solvers for model counting problems
  and their variants. The competition featured four tracks focusing on model counting
  (MC), weighted model counting (WMC), projected model counting (PMC), and projected
  weighted model counting (PWMC).
---

# The Model Counting Competitions 2021-2023

## Quick Facts
- **arXiv ID:** 2504.13842
- **Source URL:** https://arxiv.org/abs/2504.13842
- **Reference count:** 40
- **Primary result:** Overview of 2021-2023 Model Counting Competitions advancing model counting solvers across four tracks.

## Executive Summary
This paper presents a comprehensive overview of the 2021-2023 Model Counting Competitions, which aim to advance applications, identify challenging benchmarks, foster new solver development, and enhance existing solvers for model counting problems and their variants. The competition featured four tracks focusing on model counting (MC), weighted model counting (WMC), projected model counting (PMC), and projected weighted model counting (PWMC). The competitions saw significant advancements in model counting techniques, with new solvers like SharpSAT-TD achieving remarkable performance improvements.

## Method Summary
The competition evaluated solvers on four tracks using benchmark instances in CNF format with extensions for weights and projection sets. Solvers were executed on StarExec with enforced resource limits (3600s timeout, 32GB RAM) using runsolver. The evaluation focused on maximizing the number of correctly solved instances, with accuracy enforced via relative error or approximation factor depending on the ranking scheme. Benchmark instances and results are available on Zenodo.

## Key Results
- SharpSAT-TD emerged as the top solver in multiple tracks, solving significantly more instances than previous best solvers
- The competition highlighted the effectiveness of combining techniques like knowledge compilation with component caching
- Approximate counting methods showed value in certain scenarios, particularly in PMC tracks
- Solvers demonstrated improved stability, handling of larger instances, and better performance across various problem variants

## Why This Works (Mechanism)

### Mechanism 1: Tree Decomposition Guided Search
Integrating tree decompositions into decision heuristics significantly improves performance for exact model counting on structured instances. The solver computes a tree decomposition of the formula's primal graph before search, which guides the variable selection heuristic during DPLL-search. By following the tree decomposition order, the solver maintains a separator set of limited size, effectively bounding the "width" of the problem during search.

### Mechanism 2: Component Caching and Dynamic Programming
Storing sub-problem results (component caching) avoids exponential recomputation, enabling tractability for larger formulas. During recursive search, the formula often splits into disconnected components. The solver computes the count for a component once, caches it, and reuses it whenever that specific component reappears in a different branch of the search tree.

### Mechanism 3: Independent Support Reduction (Preprocessing)
Preprocessing to identify "independent support" reduces the effective problem size for projected model counting (PMC). The preprocessor identifies a minimal subset of projection variables that uniquely determines the rest. The solver then only needs to count assignments to this smaller set, effectively reducing the search space dimension.

## Foundational Learning
- **Propositional Model Counting (#SAT)**: This is the fundamental problem class the entire architecture attempts to solve. Unlike SAT (decision), #SAT (#P-complete) requires counting all satisfying assignments, which is computationally harder.
  - *Quick check:* How does the complexity of counting solutions differ from finding a single solution? (Answer: Finding is NP-complete; counting is #P-complete, strictly harder)

- **Conjunctive Normal Form (CNF) & Extensions**: The competition standardizes input on CNF. Understanding how weights and projection sets are attached to the basic CNF structure is critical for parsing and solving.
  - *Quick check:* In the competition format, how are weights and projection variables specified within a standard CNF file? (Answer: Via specific comment lines `c p weight` and `c p show`)

- **Treewidth and Graph Decomposition**: The winning solver (SharpSAT-TD) relies on the treewidth of the formula's graph representation. Low treewidth generally implies the problem is tractable for dynamic programming approaches.
  - *Quick check:* What property of a formula's interaction graph does the "tree decomposition" heuristic exploit? (Answer: The size of the largest set of mutually interacting variables)

## Architecture Onboarding
- **Component map:** Input Layer -> Preprocessing Unit -> Solver Core (Decomposition Engine -> Search Engine -> Cache Manager) -> Arithmetic Unit
- **Critical path:**
  1. Parse the CNF instance into an internal graph representation (Primal graph)
  2. Preprocess to simplify variables or find independent support (if PMC)
  3. Compute Tree Decomposition (for SharpSAT-TD style solvers) to determine variable ordering
  4. Execute Search: Branch on variables following the decomposition order; detect components
  5. Cache & Multiply: When a leaf is reached or component solved, store the count and propagate it back up the search tree

- **Design tradeoffs:**
  - Exact vs. Approximate: Exact solvers guarantee correctness but may time out on large instances
  - Precision vs. Speed: Using arbitrary precision arithmetic prevents overflow but is slower than fixed-width hardware arithmetic
  - Knowledge Compilation vs. Search: Compilers build static representation good for repeated queries; Search-based are often faster for single counts

- **Failure signatures:**
  - Time-out: Occurs often on instances with high treewidth or lack of decomposition
  - Memory Overflow: Caused by caching too many unique components or the compiled knowledge structure growing too large
  - Wrong Count: Usually due to precision loss in WMC or bugs in component caching logic

- **First 3 experiments:**
  1. Baseline Performance: Run SharpSAT-TD on the public benchmark set from the 2023 competition to establish a baseline F1-score/time
  2. Ablation Study (Preprocessing): Run the solver on PMC instances with and without the "Arjun" preprocessor to quantify the "independent support" speedup
  3. Precision Stress Test: Construct a small CNF with extremely small weights to test if the solver maintains precision correctly

## Open Questions the Paper Calls Out
- **Open Question 1:** Are recent performance improvements in model counters primarily driven by engineering optimizations or new theoretical insights?
- **Open Question 2:** What are robust measures to characterize model counters and benchmark sets beyond standard metrics like variable counts or treewidth?
- **Open Question 3:** How can ranking schemes be designed to balance the trade-off between the total number of solved instances and solution speed?

## Limitations
- The internal decision heuristics of SharpSAT-TD and how tree decomposition order specifically interacts with variable selection are not fully specified
- The comparative analysis lacks a detailed breakdown of which techniques contribute most to performance gains across different track types
- The selection process for benchmarks is not transparent, potentially biasing results toward solvers optimized for specific problem structures

## Confidence
- **High Confidence:** Documented improvements in solver performance and the general effectiveness of component caching and tree decomposition
- **Medium Confidence:** The assertion that preprocessing (Arjun) is a "central role" in PMC performance, though lacking granular quantitative attribution
- **Medium Confidence:** The comparison between exact and approximate methods based on competition results, but without exhaustive detail on specific conditions

## Next Checks
1. Reproduce Baseline: Run SharpSAT-TD on the 2023 competition's 100 private instances using the provided resources and verify the reported F1-score/time metrics on comparable hardware
2. Preprocessing Impact: Conduct an ablation study comparing SharpSAT-TD's PMC performance with and without Arjun preprocessing on a representative subset of PMC instances
3. Precision Validation: Test a solver's ability to handle WMC instances requiring high precision by comparing `log10-estimate` outputs against pre-computed counts for instances with extremely small weights