---
ver: rpa2
title: Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring
arxiv_id: '2510.19158'
source_url: https://arxiv.org/abs/2510.19158
tags:
- lemma
- where
- which
- then
- observable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses adversarial linear partial monitoring, a variant
  of bandit learning where the learner observes a linear transformation of the loss
  vector rather than the loss itself. This setting generalizes standard linear bandits
  by decoupling loss and feedback.
---

# Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring

## Quick Facts
- **arXiv ID:** 2510.19158
- **Source URL:** https://arxiv.org/abs/2510.19158
- **Reference count:** 40
- **Key outcome:** Achieves instance-dependent regret bounds of O(√T) for locally observable games and O(T^{2/3}) for globally observable games in adversarial linear partial monitoring

## Executive Summary
This paper addresses adversarial linear partial monitoring, a bandit learning variant where the learner observes a linear transformation of the loss vector rather than the loss itself. The authors propose a simple and efficient algorithm based on exponential weights with a structured loss estimator that exploits the linear structure to reduce the exploration-by-optimization problem to a convex program. Under standard observability conditions, the algorithm achieves regret bounds matching optimal rates for both locally observable games (O(√T)) and globally observable games (O(T^{2/3})). The bounds feature interpretable instance-dependent quantities reflecting the alignment between observations and losses, and are shown to be tight for several example problems.

## Method Summary
The paper adopts the exploration-by-optimization (EXO) framework, proposing an algorithm that uses exponential weights with a structured loss estimator. By imposing a linear structure on the loss estimator, the optimization problem reduces to a convex program that is efficiently solvable in many cases. The method uses an anchored variance control technique that shifts loss estimates relative to the exponential weights distribution, aligning the variance term with the game's observability structure. The algorithm automatically adapts its exploration rate based on the game's observability class, achieving different regret rates for locally versus globally observable games.

## Key Results
- Achieves O(√T) regret for locally observable linear partial monitoring games
- Achieves O(T^{2/3}) regret for globally observable games
- Bounds feature interpretable instance-dependent quantities (β_loc, β_glob)
- Demonstrated tightness for several example problems including feedback graphs and ill-conditioned bandits

## Why This Works (Mechanism)

### Mechanism 1: Structured Linear Estimator for EXO Reduction
The algorithm reduces the generally intractable Exploration-by-Optimization (EXO) problem to a finite-dimensional convex program by imposing a specific linear structure on the loss estimator. This linearity allows the EXO objective to be expressed as a convex function of the sampling distribution, solvable via convex optimization techniques. If the loss function or observation map were non-linear, this convex reduction would likely fail.

### Mechanism 2: Anchored Variance Control via Shift
Shifting loss estimates relative to the exponential weights distribution aligns the variance term with the game's observability structure. The estimator uses an "anchor" based on the weighted average of feature vectors, forcing the variance term to depend on feature differences rather than raw features. This is critical because observability conditions are defined on the span of feature differences and observation matrices, allowing the bound to tighten in "easy" games.

### Mechanism 3: Observability-Dependent Rate Switching
The algorithm automatically adapts its effective exploration rate based on the game's observability class. In locally observable games, exploration is "cheap" and the algorithm finds a distribution where exploration weight scales proportionally with η, keeping the EXO objective constant and yielding the standard √T rate. In globally observable games, exploration is "costly" and the analysis forces the exploration weight to scale with √η, causing the EXO objective to scale as 1/√η and resulting in the T^{2/3} rate.

## Foundational Learning

- **Concept: Partial Monitoring**
  - Why needed: This is the problem class where the learner doesn't see the loss directly but only a signal correlated with it
  - Quick check: If you play an action and receive a reward of 0, do you know if the action was "bad" or if the environment just produced a low outcome? (In partial monitoring, often no)

- **Concept: Local vs. Global Observability**
  - Why needed: This classification determines the fundamental difficulty of the game and the expected regret rate (√T vs T^{2/3})
  - Quick check: Can you estimate the difference in loss between two actions using only the feedback from those actions (Local)? Or do you need to play a third, potentially bad action to distinguish them (Global)?

- **Concept: Exploration-by-Optimization (EXO)**
  - Why needed: This is the meta-algorithm that solves an optimization problem to find a sampling distribution that minimizes regret assuming the worst-case outcome
  - Quick check: Does the algorithm pick the action that minimizes immediate estimated loss? (No, it mixes optimization with a stability constraint)

## Architecture Onboarding

- **Component map:** Action features ψ_a → Observation matrices M_a → Convex optimization solver → Sampling distribution p_t → Loss estimator → Exponential weights policy
- **Critical path:** Computing Q(p)^† and solving the convex program at every round t, requiring robust matrix inversion and convex solvers capable of handling semidefinite constraints
- **Design tradeoffs:** Fixed vs. adaptive η (adaptive introduces larger constants), numerical stability trade-off with stability parameter δ
- **Failure signatures:** Exploding variance from rank-deficient M in non-globally observable games, constraint violation from too large η causing infeasible convex program
- **First 3 experiments:**
  1. Linear Bandits Baseline: Set M_a = ψ_a, verify O(√T) regret and check β_loc ≈ 1
  2. Ill-Conditioned Observers: Use Section 2.3 example, vary ε to observe regret scaling with 1/ε
  3. Global Observability Test: Implement weakly observable graph, verify rate drops to O(T^{2/3})

## Open Questions the Paper Calls Out

- Can regret bounds be extended to generic compact loss spaces that are not full-dimensional?
- Can instance-dependent regret bounds be established for linear partial monitoring with generic compact action sets?
- Is the regret bound for composite graph feedback setting tight with respect to β_{2,glo}?
- Does there exist a general, optimal choice for the exploration distribution b_p that improves leading constants?

## Limitations
- Requires solving a convex optimization problem at each round, potentially computationally prohibitive for large action spaces
- Bounds depend on instance-specific quantities (β_loc, β_glob) that may be difficult to compute or estimate
- No results provided for "hopeless" games where efficient learning is impossible

## Confidence

- **High confidence:** The mechanism for achieving O(√T) regret in locally observable games is well-established and analysis appears sound
- **Medium confidence:** O(T^{2/3}) rate for globally observable games follows from established techniques but exact constants require verification
- **Medium confidence:** Computational complexity claims are plausible but practical implementations may face numerical stability challenges

## Next Checks

1. Implement a synthetic locally observable game with known optimal rates and verify the algorithm achieves O(√T) empirically
2. Test the algorithm on the ill-conditioned bandit example from Section 2.3 with varying ε values to confirm instance-dependent scaling behavior
3. Benchmark the computational overhead of solving the convex program at each round against standard linear bandit algorithms on moderately sized action spaces (k ≈ 100-1000)