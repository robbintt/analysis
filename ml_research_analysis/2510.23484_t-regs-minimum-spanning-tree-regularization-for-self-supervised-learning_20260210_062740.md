---
ver: rpa2
title: 'T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning'
arxiv_id: '2510.23484'
source_url: https://arxiv.org/abs/2510.23484
tags:
- learning
- t-reg
- t-regs
- length
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: T-REGS introduces a simple regularization framework for self-supervised
  learning based on maximizing the length of the Minimum Spanning Tree (MST) over
  learned representations. The method combines MST length maximization with a soft
  sphere constraint to prevent dimensional collapse while promoting uniform distribution
  of embeddings.
---

# T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning

## Quick Facts
- arXiv ID: 2510.23484
- Source URL: https://arxiv.org/abs/2510.23484
- Reference count: 40
- Primary result: T-REGS achieves Top-1 accuracy of 66.1% on ImageNet-1k linear evaluation when used standalone

## Executive Summary
T-REGS addresses dimensional collapse in self-supervised learning by maximizing the length of the Minimum Spanning Tree (MST) over learned embeddings while constraining them to a hypersphere. The method combines MST length maximization with a soft sphere constraint to prevent dimensional collapse while promoting uniform distribution of embeddings. Theoretical analysis shows that T-REGS satisfies four key uniformity properties and provably prevents dimensional collapse on compact Riemannian manifolds.

## Method Summary
T-REGS is a regularization framework for self-supervised learning that combines three loss components: an invariance loss (MSE between views), an MST length maximization loss, and a soft sphere constraint loss. The MST length is computed using Kruskal's algorithm on the batch embeddings, and the sphere constraint prevents divergence while promoting uniform distribution. The method is compatible with existing joint-embedding architectures and can be used standalone or as an auxiliary loss.

## Key Results
- Achieves Top-1 accuracy of 66.1% on ImageNet-1k linear evaluation
- Outperforms existing methods when combined as auxiliary loss to Barlow Twins and BYOL
- Effectively spreads points uniformly on the sphere (Figure 5 shows cosine similarity centered near 0)
- Robust to small batch sizes unlike contrastive methods

## Why This Works (Mechanism)

### Mechanism 1: Repulsive Forces via MST
- Maximizing MST length induces repulsive forces between embeddings, spreading them out to prevent dimensional collapse
- The gradient of MST length with respect to point positions exerts repulsive forces along MST edges
- Assumes local MST connectivity approximates data manifold topology
- Evidence: Abstract states MST maximization addresses dimensional collapse; Section 3 shows gradient analysis

### Mechanism 2: Soft Sphere Constraint
- Prevents trivial solution of infinite scaling when maximizing MST length
- Combines linear MST maximization with quadratic penalty for distance from unit sphere
- Forces points to sphere boundary, arranging them as regular simplex vertices
- Evidence: Section 4 states sphere constraint prevents divergence; Theorem 4.1 proves regular simplex configuration

### Mechanism 3: Uniform Distribution Promotion
- Asymptotically promotes uniform distribution by maximizing intrinsic Rényi entropy
- MST length relates to integral of density function raised to power, equivalent to Rényi entropy maximization
- Assumes batch statistics approximate underlying continuous density
- Evidence: Section 4.1.2 proves Corollary 4.6 about uniform density maximization

## Foundational Learning

- **Concept: Dimensional Collapse**
  - Why needed: T-REGS solves "outputs lie on low-dimensional subspace" not just "all outputs are same"
  - Quick check: Can you explain why covariance matrix with small eigenvalues hurts linear probing?

- **Concept: Minimum Spanning Tree (MST)**
  - Why needed: MST provides computationally tractable proxy for intrinsic dimension and density
  - Quick check: Does MST connect points based on global density or local nearest neighbors? (Answer: Local)

- **Concept: Soft vs. Hard Constraints**
  - Why needed: Soft constraint preserves gradient information about embedding norm
  - Quick check: If you normalize vector to unit length, what information about original magnitude is lost?

## Architecture Onboarding

- **Component map:** Augment -> Embed -> Invariance + Topology + Regularize
- **Critical path:**
  1. Generate views x, x'
  2. Embed: z = h(f(x)) and z' = h(f(x'))
  3. Invariance: Compute MSE(z, z')
  4. Topology: Compute MST(z) and MST(z') independently
  5. Regularize: Sum MST edge lengths and penalize distance from unit sphere

- **Design tradeoffs:**
  - Batch Size vs. Cost: MST is O(B² log B); robust to small batches but larger batches better approximate manifold
  - Projector Depth: 8192-d projectors outperform 2048-d (approx +5% accuracy)

- **Failure signatures:**
  - Collapse: If β < γ or γ < λ
  - Divergence: If γ dominates λ
  - Cone Formation: If λ too weak, embeddings remain concentrated

- **First 3 experiments:**
  1. Synthetic Validation: Replicate Figure 2a with circle initialization
  2. Hyperparameter Sensitivity: Grid search on β, γ, λ with β ≥ γ ≥ λ
  3. Integration Test: Add to Barlow Twins on CIFAR-10, check cosine similarity histogram

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does T-REGS scale to larger architectures (e.g., Vision Transformers) and longer training regimes (300+ epochs)?
- Basis: Paper evaluates ResNet-18/50 with 100-500 epochs only
- Why unresolved: Architectural differences and longer training could affect MST regularization interaction
- Evidence: Experiments with ViT-B/16 for 300-1000 epochs on ImageNet-1k

### Open Question 2
- Question: What is the computational and performance trade-off using approximate MST algorithms for very large batch sizes (>4096)?
- Basis: Paper acknowledges O(B²) complexity but uses exact algorithms with batch sizes ≤1024
- Why unresolved: Approximate MST methods may introduce regularization bias
- Evidence: Benchmarks comparing exact vs. approximate MST on batch sizes 2048-8192

### Open Question 3
- Question: Does constraining embeddings to alternative compact manifolds (e.g., hyperbolic space) provide benefits over hypersphere?
- Basis: Theoretical analysis generalizes to arbitrary compact Riemannian manifolds
- Why unresolved: Hyperbolic manifolds may better capture hierarchical data
- Evidence: Experiments applying T-REGS with Poincaré ball constraints on text/molecular datasets

### Open Question 4
- Question: What explains empirical finding that β ≥ γ ≥ λ is essential to prevent collapse?
- Basis: Table 4 shows collapse when ordering violated, but no theoretical justification
- Why unresolved: Understanding this constraint could inform automatic hyperparameter tuning
- Evidence: Theoretical analysis of gradient dynamics or systematic ablation studies

## Limitations
- Scalability concerns: O(B² log B) MST computation becomes prohibitive for large batch sizes (>2048)
- Empirical validation gaps: Asymptotic theoretical guarantees require finite-sample analysis
- Architectural sensitivity: Strong performance with deep, wide projectors increases memory footprint

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core mechanism preventing dimensional collapse | High |
| Uniformity promotion claim | Medium |
| Computational efficiency vs. existing methods | Low |

## Next Checks
1. **Batch Size Scaling Study:** Evaluate T-REGS performance across batch sizes 64-4096, measuring representation quality and computational overhead
2. **Convergence Analysis:** Track MST length, sphere constraint violation, and downstream performance throughout training
3. **Ablation on Projector Architecture:** Comprehensive study varying projector depth/width (512-8192) across multiple datasets