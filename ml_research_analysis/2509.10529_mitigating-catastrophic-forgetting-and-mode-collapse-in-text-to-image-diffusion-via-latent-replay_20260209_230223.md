---
ver: rpa2
title: Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion
  via Latent Replay
arxiv_id: '2509.10529'
source_url: https://arxiv.org/abs/2509.10529
tags:
- learning
- replay
- latent
- tasks
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis investigates continual learning for text-to-image diffusion
  models, addressing catastrophic forgetting and mode collapse through a neuroscience-inspired
  approach called Latent Replay. By storing compact latent representations extracted
  from the model's internal architecture rather than full images, the method significantly
  reduces memory requirements while preserving critical information needed to maintain
  previously learned concepts.
---

# Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay

## Quick Facts
- arXiv ID: 2509.10529
- Source URL: https://arxiv.org/abs/2509.10529
- Authors: Aoi Otani
- Reference count: 11
- This thesis introduces Latent Replay for text-to-image diffusion models, achieving 77.59% Image Alignment retention on earliest concepts while using 14% less memory than baselines.

## Executive Summary
This thesis addresses critical challenges in continual learning for text-to-image diffusion models: catastrophic forgetting and mode collapse. By implementing a neuroscience-inspired Latent Replay approach that stores compact latent representations instead of full images, the method achieves significant memory efficiency while maintaining previously learned visual concepts. Through systematic experiments with five sequentially learned visual concepts, Latent Replay demonstrated substantial improvements over baseline methods, preserving both concept fidelity and generation diversity. The approach shows promise for enabling personalized, evolving text-to-image models that can adapt to user needs without prohibitive computational costs.

## Method Summary
The thesis proposes Latent Replay as a solution to catastrophic forgetting in text-to-image diffusion models by storing latent representations rather than full images. This neuroscience-inspired approach extracts compact latent features from the model's internal architecture during training, creating a memory-efficient replay buffer. When learning new visual concepts, these stored latents are replayed to the model, helping maintain previously acquired knowledge. The method specifically addresses the dual challenge of preventing catastrophic forgetting (losing old knowledge when learning new concepts) and mode collapse (generating repetitive outputs). Through systematic experimentation with five sequentially learned visual concepts, the approach demonstrated superior performance in preserving both concept fidelity and generation diversity compared to traditional replay methods.

## Key Results
- Achieved 77.59% Image Alignment retention on earliest learned concepts, representing a 14% improvement over baseline methods
- Successfully maintained generation diversity while preventing catastrophic forgetting
- Demonstrated performance approaching offline training levels while using only a fraction of the memory
- Random selection of stored latent examples outperformed similarity-based strategies, an unexpected finding requiring further investigation

## Why This Works (Mechanism)
Latent Replay works by leveraging the hierarchical feature representations learned by diffusion models. When a diffusion model processes images, it creates multi-scale latent representations that capture essential visual information at different abstraction levels. By storing these compact latent representations instead of full images, the method preserves the critical information needed for concept retention while dramatically reducing memory requirements. During continual learning, these stored latents are decoded and replayed to the model, providing a form of memory that helps the model maintain previously learned concepts. The latent space captures the essential visual features and relationships that define each concept, allowing the model to reconstruct and maintain these concepts even as it learns new ones. This approach is particularly effective because diffusion models are inherently designed to work with latent representations, making the replay process natural and efficient.

## Foundational Learning

**Latent Representations**: Compact feature encodings extracted from intermediate layers of neural networks that capture essential visual information. Why needed: Traditional image replay requires storing full high-resolution images, which is memory-prohibitive for large-scale models. Quick check: Verify that latent dimensions are sufficiently small (typically 1/10th to 1/100th of original image size) while retaining key visual features.

**Catastrophic Forgetting**: The phenomenon where neural networks rapidly lose previously learned information when trained on new tasks. Why needed: Text-to-image diffusion models trained sequentially on different visual concepts will forget earlier concepts without intervention. Quick check: Measure performance degradation on earlier concepts after training on new ones.

**Mode Collapse**: The tendency of generative models to produce limited, repetitive outputs rather than diverse samples. Why needed: Continual learning can cause models to converge to narrow distributions, losing the ability to generate varied outputs. Quick check: Analyze output diversity metrics (e.g., LPIPS, IS) across generated samples.

**Diffusion Models**: Generative models that learn to reverse a gradual noising process to create images from random noise. Why needed: The thesis specifically targets text-to-image diffusion architectures, which have unique properties suited for latent replay. Quick check: Confirm the model uses a U-Net architecture with multi-scale latent representations.

**Neural Plasticity**: The brain's ability to form new neural connections while maintaining existing ones. Why needed: The neuroscience inspiration behind replay mechanisms that enable continual learning without forgetting. Quick check: Compare replay-based approaches to biological memory consolidation mechanisms.

## Architecture Onboarding

**Component Map**: Text Input -> Text Encoder -> Cross-Attention -> Diffusion U-Net -> Latent Space -> Decoder -> Image Output; Memory Buffer -> Latent Replay -> Cross-Attention; Training Loop -> Sequential Concept Updates

**Critical Path**: Text embedding → Cross-attention with latents → U-Net denoising steps → Image generation; During replay: Stored latents → Decoder → Replay images → Cross-attention integration

**Design Tradeoffs**: Memory vs. Performance (full images provide better fidelity but require 10-100x more storage); Random vs. Similarity-based selection (random surprisingly better but less interpretable); Replay frequency vs. training speed (more frequent replay improves retention but slows training)

**Failure Signatures**: Catastrophic forgetting manifests as degraded Image Alignment scores on earlier concepts; Mode collapse appears as low diversity scores and repetitive generated outputs; Memory overflow occurs when latent buffer exceeds system RAM limits

**3 First Experiments**:
1. Sequential training on Concept A → Concept B → Test forgetting on Concept A
2. Full image replay vs. latent replay memory comparison
3. Random selection vs. similarity-based selection performance comparison

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- Limited experimental scope with only five visual concepts, raising questions about scalability to more complex or diverse real-world scenarios
- Incomplete computational overhead analysis lacking comprehensive runtime performance metrics and training time comparisons
- Focus exclusively on text-to-image diffusion models without validation on alternative generative architectures like GANs or autoregressive models
- Theoretical justification gap for the unexpected superiority of random latent example selection over similarity-based strategies

## Confidence

**High Confidence**: The core technical contribution of using latent representations for replay rather than full images is well-supported, with clear quantitative improvements in memory efficiency and performance metrics. The 77.59% Image Alignment retention for the earliest concept, representing a 14% improvement over baselines, is a robust empirical finding backed by systematic experimentation.

**Medium Confidence**: The claim about maintaining generation diversity while preventing catastrophic forgetting is moderately supported but requires additional validation. While diversity metrics show preservation, the analysis could benefit from more sophisticated measures of output variation and perceptual quality assessments. The assertion that performance approaches offline training levels is plausible given the results but may be overstated without broader benchmarking against multiple offline baselines.

**Low Confidence**: The recommendation for random selection of stored latent examples over similarity-based strategies is based on empirical observation but lacks theoretical grounding. This surprising finding requires further investigation to determine whether it represents a genuine methodological insight or an artifact of the specific experimental conditions and concept set used in the study.

## Next Checks

1. **Scalability Testing**: Evaluate Latent Replay performance when training on datasets with 50+ visual concepts in sequence, measuring both memory consumption and generation quality degradation rates to establish practical limits of the approach.

2. **Cross-Architecture Validation**: Implement and test the Latent Replay mechanism on alternative generative architectures (e.g., GANs, VAEs) to determine whether the benefits generalize beyond text-to-image diffusion models and identify any architecture-specific considerations.

3. **Runtime Performance Analysis**: Conduct comprehensive benchmarking of training and inference times across different replay strategies, including memory bandwidth requirements and GPU utilization patterns, to provide a complete picture of computational trade-offs.