---
ver: rpa2
title: 'Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation'
arxiv_id: '2602.01187'
source_url: https://arxiv.org/abs/2602.01187
tags:
- revision
- code
- generation
- stream
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Stream of Revision, a decoding framework
  that internalizes just-in-time revision into autoregressive code generation. By
  embedding special action tokens for vulnerability detection, localization, and patching,
  the model can backtrack and edit its own history within a single forward pass, avoiding
  the latency and token overhead of external repair agents.
---

# Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation

## Quick Facts
- arXiv ID: 2602.01187
- Source URL: https://arxiv.org/abs/2602.01187
- Reference count: 40
- This paper introduces Stream of Revision, a decoding framework that internalizes just-in-time revision into autoregressive code generation.

## Executive Summary
This paper presents Stream of Revision, a novel decoding framework that integrates just-in-time revision into autoregressive code generation for secure coding. By embedding special action tokens for vulnerability detection, localization, and patching, the model can self-correct within a single forward pass, avoiding the overhead of external repair agents. The approach is aligned on real-world CVE pairs using a minimal dataset, enabling structured security revision patterns. Evaluation on CyberSecEval 2 demonstrates significant improvements in secure code generation for C/C++ while maintaining general coding utility and enabling zero-shot transfer to other languages.

## Method Summary
Stream of Revision is a decoding framework that embeds special action tokens into autoregressive code generation to enable just-in-time revision. These tokens allow the model to detect, localize, and patch vulnerabilities within a single forward pass, eliminating the need for external repair agents. The model is aligned using a small set of ~1,000 labeled CVE pairs, learning structured security revision patterns. During inference, the model can backtrack and edit its own history, improving secure code generation efficiency and syntactic validity. The approach is evaluated on CyberSecEval 2, showing significant gains for C/C++ and zero-shot transfer to other languages.

## Key Results
- Stream of Revision significantly improves secure code generation for C/C++ on CyberSecEval 2 benchmarks.
- The model outperforms agentic repair baselines in efficiency and maintains high syntactic validity.
- Zero-shot transfer to other programming languages is demonstrated, preserving general coding utility.

## Why This Works (Mechanism)
Stream of Revision works by embedding special action tokens that enable the model to detect, localize, and patch vulnerabilities during autoregressive generation. This internalization of revision allows the model to self-correct within a single forward pass, avoiding the latency and token overhead of external repair agents. The minimal alignment on real-world CVE pairs teaches the model structured security revision patterns, which it can generalize to new code contexts. By maintaining syntactic validity and enabling runtime revision, the approach balances security improvements with practical usability.

## Foundational Learning
- **Autoregressive Code Generation**: Why needed - Enables step-by-step code synthesis; Quick check - Model generates syntactically correct code sequences.
- **Vulnerability Detection and Patching**: Why needed - Core to secure code generation; Quick check - Model identifies and fixes known CVE patterns.
- **Special Action Tokens**: Why needed - Allow the model to trigger revision actions; Quick check - Tokens are correctly embedded and recognized during generation.
- **Alignment on CVE Pairs**: Why needed - Teaches the model structured security revision; Quick check - Model learns to apply patches to similar vulnerabilities.
- **Zero-shot Transfer**: Why needed - Enables generalization to new languages; Quick check - Model maintains performance on unseen programming languages.
- **Syntactic Validity**: Why needed - Ensures generated code is usable; Quick check - Generated code passes basic syntax checks.

## Architecture Onboarding

### Component Map
- **Autoregressive Generator** -> **Vulnerability Detector** -> **Localizer** -> **Patcher** -> **Revised Output**

### Critical Path
The critical path is: Autoregressive Generator -> Vulnerability Detector -> Localizer -> Patcher -> Revised Output. The model generates code, detects vulnerabilities, localizes them, applies patches, and outputs the revised codeâ€”all within a single forward pass.

### Design Tradeoffs
- **Efficiency vs. Security**: Internalizing revision reduces latency but may limit the depth of analysis compared to external agents.
- **Minimal Alignment vs. Coverage**: Using ~1,000 CVE pairs is efficient but may miss rare or complex vulnerabilities.
- **Zero-shot Transfer vs. Language-specific Nuances**: Generalization to new languages is valuable but may overlook language-specific security idioms.

### Failure Signatures
- **False Negatives**: Missed vulnerabilities due to limited training data or subtle patterns.
- **Syntactic Errors**: Introduced by over-aggressive or incorrect patching.
- **Inefficient Revision**: Excessive backtracking or token usage in complex cases.

### Exactly 3 First Experiments
1. Test Stream of Revision on a larger, more diverse dataset of CVE pairs to assess robustness across vulnerability types.
2. Compare the model's performance against external repair agents on complex, multi-line vulnerability fixes.
3. Evaluate the approach on codebases with strict syntactic and semantic constraints to verify maintained code quality.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on a small set of ~1,000 labeled CVE pairs, which may not capture the full diversity of real-world vulnerabilities.
- Performance gains are primarily demonstrated for C/C++, with limited discussion of robustness across other languages.
- The model's ability to handle subtle or context-dependent vulnerabilities without external tools remains uncertain.
- Scalability and robustness for large, production-grade codebases are not fully explored.

## Confidence
- High confidence in the effectiveness of Stream of Revision for improving secure code generation in controlled benchmarks, particularly for C/C++.
- Medium confidence in its zero-shot transfer capabilities across programming languages and in maintaining general coding utility.
- Low confidence in its scalability and robustness for complex, real-world codebases with nuanced vulnerabilities.

## Next Checks
1. Test Stream of Revision on a larger, more diverse dataset of CVE pairs and real-world codebases to assess robustness across vulnerability types and programming languages.
2. Compare the model's performance against external repair agents on complex, multi-line vulnerability fixes to quantify the efficiency gains in realistic settings.
3. Evaluate the approach on codebases with strict syntactic and semantic constraints to verify that runtime revision does not degrade overall code quality or introduce new errors.