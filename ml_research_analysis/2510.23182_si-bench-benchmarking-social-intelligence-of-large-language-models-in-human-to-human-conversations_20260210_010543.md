---
ver: rpa2
title: 'SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human
  Conversations'
arxiv_id: '2510.23182'
source_url: https://arxiv.org/abs/2510.23182
tags:
- social
- reply
- human
- process
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SI-Bench is a new benchmark for evaluating social intelligence
  in large language models (LLMs) using real human-to-human dialogues. Unlike previous
  benchmarks relying on simulated or scripted interactions, SI-Bench contains 2,221
  authentic multi-turn conversations from a social networking platform, covering 12
  complex social situations based on social science theories.
---

# SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations

## Quick Facts
- arXiv ID: 2510.23182
- Source URL: https://arxiv.org/abs/2510.23182
- Reference count: 40
- Key outcome: SI-Bench benchmark shows LLMs surpass humans in social reasoning but lag in generating socially appropriate replies

## Executive Summary
SI-Bench is a novel benchmark for evaluating social intelligence in large language models using real human-to-human dialogues from a Chinese social networking platform. Unlike previous benchmarks relying on simulated interactions, SI-Bench contains 2,221 authentic multi-turn conversations covering 12 complex social situations based on social science theories. The benchmark evaluates both process reasoning (motivation, cause, emotional attitude, social intent, communication strategy) and final reply quality across 8 major models. Results reveal a significant "thought-action gap" where models can accurately reason about social contexts but struggle to translate this understanding into appropriate responses.

## Method Summary
The benchmark uses a three-stage evaluation framework: contextual understanding, response strategy, and reply generation. Each dialogue is annotated by three human experts using a 0-5-10 rubric for five process dimensions and reply quality. Eight LLMs are evaluated under two conditions: with Chain-of-Thought (CoT) reasoning and direct reply generation. The dataset consists of 2,221 authentic Chinese dialogues, with 312 manually annotated samples covering 12 social situation types. Inter-annotator agreement is measured using Krippendorff's Alpha (0.712 reported).

## Key Results
- Top LLMs surpass human experts in process reasoning scores but generate lower-quality replies
- CoT reasoning consistently degrades reply quality compared to direct generation
- Models show significant contextual bias, performing better on low-context (explicit intent) than high-context (implicit meaning) social situations

## Why This Works (Mechanism)

### Mechanism 1: Decoupling Process Reasoning from Reply Generation Reveals a Thought-Action Gap
Current LLMs exhibit a "thought-action gap"â€”they can accurately reason about social contexts (surpassing human experts in process reasoning) but still produce lower-quality replies than humans. The evaluation framework separates social intelligence into process reasoning and reply generation, where LLMs excel at analytical decomposition but fail to effectively bridge understanding a situation and enacting an appropriate social response.

### Mechanism 2: Chain-of-Thought Reasoning Can Degrade Social Reply Quality by Over-Literalizing Strategy
Explicit CoT prompting, while improving process reasoning, consistently degrades the quality of final social replies compared to direct generation. CoT prompts cause models to focus too heavily on literal execution of chosen strategies at a surface level, resulting in replies that are logically correct but socially awkward. Direct replies appear to leverage more implicit, holistic processing that better preserves conversational nuance.

### Mechanism 3: LLM Performance is Biased Toward Low-Context Social Situations
LLMs demonstrate a contextual bias, performing significantly better in reasoning about low-context social situations (where intent is explicit) than high-context ones (requiring inference of implicit cues). Current LLMs primarily rely on surface-level semantics and excel in situations like value tests, but struggle with high-context situations like euphemism and irony that require inferring unstated meaning and cultural norms.

## Foundational Learning

- **Concept: Social Intelligence Components (Social Awareness & Social Facility)**
  - Why needed here: The SI-Bench framework is explicitly built on this theory from social science (Goleman, 2006). Understanding that social intelligence involves both perception (understanding others) and action (acting wisely in relations) is fundamental to interpreting the paper's decoupled evaluation.
  - Quick check question: Can you distinguish between a model's ability to identify a speaker's motivation (social awareness) and its ability to choose an effective communicative strategy in response (social facility)?

- **Concept: High-Context vs. Low-Context Communication**
  - Why needed here: The paper's taxonomy of complex social situations is built on this core tension. Understanding the difference is crucial for interpreting experimental results showing model bias toward low-context tasks.
  - Quick check question: In a conversation, how would you distinguish a "hint" (high-context, implied meaning) from a "value test" (low-context, explicit challenge)? Which would you expect an LLM to handle better based on this paper?

- **Concept: Theory of Mind (ToM) and Pragmatic Reasoning**
  - Why needed here: The paper positions SI-Bench as a benchmark that goes beyond standard ToM tests, which often use static scenarios. Understanding ToM as the foundational capacity to infer others' beliefs and intentions is necessary to appreciate SI-Bench's interactive, multi-turn, real-dialogue approach.
  - Quick check question: How does evaluating a model's social intelligence in a multi-turn dialogue differ from evaluating its ability to answer a static multiple-choice question about a character's belief (a classic ToM task)?

## Architecture Onboarding

- **Component map:** Raw dialogues -> Sample Collection (critical turns) -> Cross-Validation (two pairs of annotators) -> Quality Control (minimal rewriting) -> Taxonomy Classification -> Model Inference (CoT & Direct) -> Human Annotation (3 annotators) -> Metric Calculation

- **Critical path:**
    1. Data Ingestion & Filtering: Raw dialogues ingested and filtered to find challenging "critical moments"
    2. Taxonomy Classification: Annotators label each dialogue with one of 12 situation types
    3. Model Inference (CoT & Direct): For each dialogue, models generate both a CoT analysis and two replies
    4. Human Annotation: Expert annotators score the CoT process (5 dimensions) and two replies blindly, with forced-preference choice between replies
    5. Metric Calculation: Calculate mean scores for process dimensions, reply quality, and CoT win rates

- **Design tradeoffs:**
    - Ecological Validity vs. Scale: Study prioritizes high ecological validity (real human dialogues) over scale, limiting sample size to 312 annotated dialogues
    - Model-Centric vs. Human-Like Reasoning: CoT prompt structure is designed as explicit, sequential scaffold for models, differing from human social reasoning
    - Single-Culture Focus vs. Generalizability: Dataset is exclusively Chinese, limiting cross-cultural validity despite capturing rich linguistic diversity

- **Failure signatures:**
    - Thought-Action Gap: Model scores highly on all process reasoning dimensions (e.g., 10/10) but generates socially awkward or inappropriate reply (e.g., 0/10)
    - CoT Over-Literalization: Model's CoT reply is a poor translation of its own good reasoning, clumsily executing named strategies
    - High-Context Inference Failure: Model performs well on direct challenges but fails to detect nuance in euphemisms, sarcasm, or vague statements

- **First 3 experiments:**
    1. Baseline Comparison: Run the direct (non-CoT) prompt on all 312 annotated dialogues and compare models' reply scores to human expert's direct reply scores
    2. CoT Ablation Study: For a subset of dialogues and models, run the full CoT prompt and versions with each of five process dimensions removed, comparing resulting reply scores
    3. High vs. Low Context Analysis: Segment dataset into high-context and low-context situations, comparing process reasoning scores of a top model across these groups

## Open Questions the Paper Calls Out
None

## Limitations
- The ecological validity of the benchmark comes at the cost of scale - with only 312 manually annotated dialogues, results may not generalize across all social situations
- The Chinese-only dataset limits cross-cultural applicability of findings
- The forced comparison between CoT-structured model reasoning and human implicit reasoning may disadvantage human experts

## Confidence

**Major uncertainties and limitations:**
- Medium confidence in the CoT degradation mechanism (while consistently observed, the over-literalization hypothesis needs further validation)
- Medium confidence in the high-context/low-context bias (the taxonomy is theoretically sound but may not capture all nuances)

**Confidence labels:**
- High confidence in the thought-action gap finding (both process reasoning and reply generation are separately measured and compared to human baselines)
- Medium confidence in the CoT degradation mechanism (while consistently observed, the over-literalization hypothesis needs further validation across different model families)
- Medium confidence in the high-context/low-context bias (the taxonomy is theoretically sound, but the 12-category division may not capture all nuances of contextual complexity)

## Next Checks
1. Run a cross-cultural validation using an equivalent English dataset to test whether the contextual bias toward low-context situations holds across languages
2. Conduct an ablation study varying CoT prompt structure (sequential vs parallel reasoning) to determine if the degradation is due to format rather than the reasoning process itself
3. Test the thought-action gap hypothesis by training a model to explicitly optimize for coherence between its CoT reasoning and final reply quality