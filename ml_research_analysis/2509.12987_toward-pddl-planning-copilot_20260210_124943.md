---
ver: rpa2
title: Toward PDDL Planning Copilot
arxiv_id: '2509.12987'
source_url: https://arxiv.org/abs/2509.12987
tags:
- planning
- tools
- tasks
- tool
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Planning Copilot is a chatbot that integrates planning tools
  and allows users to invoke them through natural language instructions, using the
  Model Context Protocol (MCP) to connect LLMs with external tools without domain-specific
  fine-tuning. It supports common planning tasks such as syntax checking, planner
  selection, plan validation, and simulation.
---

# Toward PDDL Planning Copilot
## Quick Facts
- arXiv ID: 2509.12987
- Source URL: https://arxiv.org/abs/2509.12987
- Authors: Yarin Benyamin; Argaman Mordoch; Shahaf S. Shperberg; Roni Stern
- Reference count: 8
- Primary result: Planning Copilot significantly outperforms LLMs without planning tools in planning tasks

## Executive Summary
The Planning Copilot introduces an innovative approach to integrating planning tools with large language models (LLMs) through a chatbot interface that uses the Model Context Protocol (MCP). This system allows users to perform planning tasks like syntax checking, planner selection, plan validation, and simulation through natural language instructions without requiring domain-specific fine-tuning. The architecture connects LLMs with external planning tools, creating a more capable planning assistant that can leverage specialized functionality while maintaining conversational interaction.

## Method Summary
The Planning Copilot employs the Model Context Protocol (MCP) to create a bridge between LLMs and external planning tools, eliminating the need for domain-specific fine-tuning. Users interact with the system through a chatbot interface that accepts natural language instructions for common planning tasks. The system integrates with PDDL (Planning Domain Definition Language) tools for syntax checking, planner selection mechanisms, plan validation routines, and simulation capabilities. Three open-source LLMs were evaluated against the same models without tool integration, with additional qualitative comparison to GPT-5 to demonstrate the effectiveness of the approach.

## Key Results
- The Planning Copilot significantly outperforms LLMs without planning tools in planning tasks
- The system achieves better performance than GPT-5 despite using much smaller LLMs
- Natural language interface successfully handles common planning tasks like syntax checking and plan validation

## Why This Works (Mechanism)
The Planning Copilot works by leveraging the Model Context Protocol to connect LLMs with specialized planning tools, creating a hybrid system that combines the language understanding capabilities of LLMs with the precision of domain-specific planning algorithms. By avoiding domain-specific fine-tuning, the system maintains the general reasoning capabilities of LLMs while extending their functionality through tool integration. The natural language interface lowers the barrier to entry for users who may not be experts in planning languages or tool usage, while the tool integration provides reliable, deterministic outputs for tasks that require computational precision.

## Foundational Learning
- **Model Context Protocol (MCP)**: A protocol for connecting LLMs with external tools - needed to enable seamless tool integration without custom implementations, quick check: verify MCP handles tool failures gracefully
- **PDDL (Planning Domain Definition Language)**: Standard language for expressing planning problems - needed for interoperability with existing planning tools, quick check: validate PDDL syntax checking accuracy
- **Tool-Augmented LLMs**: Integration of LLMs with external computational tools - needed to overcome LLM limitations in precise calculations and domain-specific reasoning, quick check: measure performance gap between tool-augmented and base models
- **Natural Language Interface Design**: Converting user instructions to tool invocations - needed to make planning accessible to non-experts, quick check: test interface with ambiguous or incomplete instructions
- **LLM Planning Evaluation**: Methods for assessing planning system performance - needed to quantify improvements from tool integration, quick check: establish baseline performance metrics
- **Hybrid AI Systems**: Combining different AI approaches for enhanced capabilities - needed to leverage strengths of both LLMs and specialized planning algorithms, quick check: analyze error propagation between components

## Architecture Onboarding
Component map: User -> Chatbot Interface -> MCP Layer -> LLM -> Planning Tools -> Results -> User
Critical path: Natural language instruction flows from user through chatbot to MCP, which invokes LLM reasoning and tool execution, returning results through the same path
Design tradeoffs: Avoided domain-specific fine-tuning for generality but introduced tool integration complexity and potential latency issues
Failure signatures: Tool invocation failures, LLM misinterpretation of natural language, tool conflicts, latency in multi-tool workflows
First experiments: 1) Test basic tool invocation with simple planning tasks, 2) Evaluate system response to ambiguous natural language instructions, 3) Measure latency impact of tool integration on response times

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology lacks comprehensive baseline comparisons and standardized benchmarks
- MCP integration scalability and robustness in production environments unproven
- Limited exploration of edge cases, complex scenarios, and real-world deployment challenges

## Confidence
- High confidence: The basic premise that tool integration can enhance LLM planning capabilities
- Medium confidence: The specific performance improvements reported, given limited evaluation scope  
- Low confidence: Generalizability to production environments and complex planning scenarios

## Next Checks
1. Conduct head-to-head comparisons between the Planning Copilot and simpler tool-augmented LLM interfaces to isolate the specific contribution of the MCP-based architecture
2. Perform systematic stress testing with edge cases, conflicting tool requirements, and ambiguous natural language inputs to assess robustness
3. Deploy the system in a controlled real-world planning scenario over extended periods to evaluate latency, error handling, and user experience under realistic conditions