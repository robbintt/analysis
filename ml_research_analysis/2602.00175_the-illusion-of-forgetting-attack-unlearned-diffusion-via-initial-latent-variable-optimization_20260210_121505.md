---
ver: rpa2
title: 'The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent
  Variable Optimization'
arxiv_id: '2602.00175'
source_url: https://arxiv.org/abs/2602.00175
tags:
- attack
- latent
- prompt
- optimization
- unlearned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper exposes a fundamental flaw in current diffusion model
  unlearning defenses by demonstrating that "forgotten" NSFW concepts persist as dormant
  memories within model weights. The authors introduce IVO (Initial Latent Variable
  Optimization), a framework that bypasses unlearning by optimizing initial latent
  variables in the image latent space.
---

# The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization

## Quick Facts
- arXiv ID: 2602.00175
- Source URL: https://arxiv.org/abs/2602.00175
- Reference count: 24
- Demonstrates IVO framework that successfully recovers "forgotten" NSFW concepts from unlearned diffusion models with over 70% attack success rates

## Executive Summary
This paper exposes a fundamental vulnerability in current diffusion model unlearning defenses by demonstrating that unlearned NSFW concepts persist as dormant memories within model weights. The authors introduce IVO (Initial Latent Variable Optimization), a framework that bypasses traditional unlearning defenses by optimizing initial latent variables in the image latent space rather than attempting to decode through the unlearned model's disrupted pathways. IVO uses DDIM inversion to initialize from NSFW images, then applies a dual-loss optimization combining distribution matching and direction calibration to reconstruct broken symbol-to-knowledge mappings.

Extensive experiments across 8 unlearning techniques show IVO achieves superior attack success rates while maintaining high semantic consistency compared to baseline methods. The work demonstrates IVO's effectiveness in both white-box and black-box settings, as well as in image-to-image generation scenarios. The authors argue their findings reveal that current unlearning methods merely suppress rather than eliminate unsafe knowledge, calling for a fundamental shift in unlearning research toward true weight-level erasure of unsafe concepts.

## Method Summary
IVO (Initial Latent Variable Optimization) is a framework designed to attack diffusion model unlearning defenses by reconstructing unsafe concepts from unlearned models. The method works by first initializing from NSFW images through DDIM inversion, then optimizing initial latent variables to recover the broken symbol-to-knowledge mappings. IVO employs a dual-loss optimization strategy: distribution matching loss that aligns the latent distribution with that of a standard model, and direction calibration loss that ensures semantic consistency during reconstruction. The framework is specifically designed to bypass the disrupted mappings created by unlearning defenses by working directly in the latent space rather than through the model's standard generation pathway.

## Key Results
- IVO achieves over 70% attack success rates across 8 unlearning techniques, outperforming baseline methods
- The framework maintains high semantic consistency in recovered NSFW content while bypassing unlearned defenses
- IVO demonstrates effectiveness in both white-box and black-box attack scenarios, as well as in image-to-image generation contexts

## Why This Works (Mechanism)
IVO succeeds because current unlearning methods only disrupt the symbol-to-knowledge mapping in diffusion models rather than eliminating the underlying knowledge. When unlearning techniques like ESD or AdvU are applied, they break the connection between the model's learned representations and their semantic interpretations, but the actual knowledge remains dormant in the model weights. IVO bypasses these disrupted pathways by optimizing initial latent variables directly, effectively rediscovering the original knowledge through distribution matching with standard models and direction calibration.

## Foundational Learning
- Diffusion models: Why needed - Core target of attack; quick check - Understand how diffusion models generate images through iterative denoising
- DDIM inversion: Why needed - Provides initialization for IVO; quick check - Know how to invert images back to latent space
- Unlearning techniques: Why needed - Attack target; quick check - Understand how methods like ESD and AdvU disrupt model knowledge
- MMD (Maximum Mean Discrepancy): Why needed - Used as distribution matching metric; quick check - Know how to measure distribution similarity
- Symbol-to-knowledge mapping: Why needed - Central vulnerability; quick check - Understand how models connect latent representations to semantic concepts
- Distribution matching loss: Why needed - Core optimization component; quick check - Know how to minimize distribution divergence

## Architecture Onboarding

Component map:
Standard Model -> DDIM Inversion -> Initial Latent Variable -> Distribution Matching Loss -> Direction Calibration Loss -> Generated Image

Critical path: DDIM Inversion -> Initial Latent Variable Optimization -> Dual-loss Optimization -> Image Generation

Design tradeoffs:
- Distribution matching vs. direction calibration balance
- White-box vs. black-box attack scenarios
- Computational cost vs. attack success rate
- Semantic consistency vs. reconstruction fidelity

Failure signatures:
- Low attack success rates despite optimization
- Generated images lack semantic consistency
- Distribution matching fails to converge
- Direction calibration produces unstable results

3 first experiments:
1. Verify DDIM inversion correctly recovers latent variables from NSFW images
2. Test distribution matching loss convergence on standard models
3. Validate direction calibration maintains semantic consistency during optimization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can unlearning methods be redesigned to achieve fundamental weight-level erasure of unsafe knowledge rather than just disrupting symbol-to-knowledge mappings?
- Basis in paper: The Impact Statement explicitly urges the community to move beyond suppression toward "fundamental unsafe knowledge elimination."
- Why unresolved: IVO demonstrates that current methods (ESD, AdvU, etc.) leave knowledge intact as "dormant memories," allowing attacks to bypass the disrupted mappings.
- What evidence would resolve it: Development of a defense method where IVO fails to reactivate concepts (0% ASR) and the model's noise distribution diverges significantly from the standard model.

### Open Question 2
- Question: Can the Maximum Mean Discrepancy (MMD) between standard and unlearned models be effectively utilized as a direct optimization constraint to ensure robust unlearning?
- Basis in paper: Section 4 states that distributional discrepancy "serves as a measurable indicator... reflecting the strength of unlearning."
- Why unresolved: The paper uses MMD post-hoc to explain attack success; it does not validate MMD as a proactive loss component in the unlearning training process.
- What evidence would resolve it: Experiments where MMD is minimized during unlearning training, resulting in a model that resists IVO's distribution matching loss.

### Open Question 3
- Question: Is IVO's effectiveness constrained by the availability of a standard surrogate model that closely matches the target model's pre-unlearned distribution?
- Basis in paper: Section 3 defines the surrogate assumption, and Appendix D.4 shows ASR degradation when using an unlearned surrogate instead of a standard one.
- Why unresolved: While effective on public models, it is unclear if the "distribution matching" logic holds if the attacker lacks a clean surrogate reference for the target.
- What evidence would resolve it: Attack evaluations on target models where no corresponding standard surrogate model exists or where the surrogate differs significantly in architecture.

## Limitations
- Uncertainty about whether dormant knowledge represents true model memory or optimization artifacts
- Evaluation focuses primarily on white-box scenarios with full model access
- Computational overhead compared to baseline attacks not thoroughly characterized
- Scalability to larger models or datasets requires further investigation

## Confidence
- High confidence: IVO framework design and implementation
- Medium confidence: Attack success rates and comparison with baselines
- Medium confidence: Black-box attack effectiveness
- Low confidence: Generalization to other unlearning targets beyond NSFW content

## Next Checks
1. Conduct ablation studies to isolate whether IVO success stems from stored model weights versus optimization artifacts
2. Evaluate IVO's effectiveness across diverse unlearning targets (e.g., copyrighted content, sensitive information) beyond NSFW concepts
3. Test IVO under realistic deployment constraints including query limits, partial model access, and defensive countermeasures like query detection or rate limiting