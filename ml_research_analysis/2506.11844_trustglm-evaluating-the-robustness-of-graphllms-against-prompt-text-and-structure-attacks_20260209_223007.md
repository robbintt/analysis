---
ver: rpa2
title: 'TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and
  Structure Attacks'
arxiv_id: '2506.11844'
source_url: https://arxiv.org/abs/2506.11844
tags:
- graph
- attack
- attacks
- graphllms
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces TrustGLM, a benchmark designed to evaluate\
  \ the robustness of Graph Large Language Models (GraphLLMs) against adversarial\
  \ attacks. We systematically examined vulnerabilities from three perspectives\u2014\
  text, graph structure, and prompt label attacks\u2014and found that these threats\
  \ significantly degrade performance across multiple datasets."
---

# TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks

## Quick Facts
- arXiv ID: 2506.11844
- Source URL: https://arxiv.org/abs/2506.11844
- Reference count: 40
- Key outcome: GraphLLMs are vulnerable to text, structure, and prompt label attacks; adversarial training and data augmentation improve robustness but don't eliminate risks.

## Executive Summary
This work introduces TrustGLM, a benchmark designed to evaluate the robustness of Graph Large Language Models (GraphLLMs) against adversarial attacks. We systematically examined vulnerabilities from three perspectives—text, graph structure, and prompt label attacks—and found that these threats significantly degrade performance across multiple datasets. To counter these vulnerabilities, we investigated defense techniques including adversarial data augmentation and adversarial training, which improved robustness yet did not completely eliminate the risks. Our findings highlight the urgent need for more effective defenses and lay a solid foundation for future research in securing GraphLLMs.

## Method Summary
TrustGLM evaluates GraphLLMs against three attack vectors: raw text perturbations (HLBB, TextHoaxer), graph structure modifications (Nettack, PRBCD), and prompt label manipulations (shuffle, in-domain noise, cross-domain noise). The benchmark uses six TAG datasets (Cora, PubMed, OGB-Arxiv, OGB-Products, Amazon-Computers, Amazon-Sports) and three victim GraphLLMs (LLaGA, GraphPrompter, GraphTranslator). Attacks are generated via surrogate GCN for structure and black-box queries for text. Defenses include FGSM, PGD adversarial training, data augmentation, and label shuffle training.

## Key Results
- GraphLLMs are highly susceptible to text attacks, with accuracy drops of 26-50% across datasets
- Structure attacks degrade performance by 14-43%, with LLaGA most affected
- Prompt label noise attacks cause accuracy drops up to 57%, with cross-domain noise most disruptive
- Adversarial training improves robustness but doesn't eliminate vulnerabilities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GraphLLMs are highly susceptible to text attacks that merely replace a few semantically similar words in a node's textual attribute.
- **Mechanism:** An attacker replaces words in a node's textual attribute with semantically similar synonyms, altering token embeddings and causing the model to misinterpret context without changing ground-truth meaning significantly.
- **Core assumption:** The attack success depends on the assumption that the GraphLLM's reasoning layer prioritizes token-level semantics over structural priors for classification.
- **Evidence anchors:**
  - [abstract]: "GraphLLMs are highly susceptible to text attacks that merely replace a few semantically similar words in a node’s textual attribute."
  - [section 5.2.1]: "Models that rely more on textual features for classification tend to be more vulnerable... accuracy drops from 87.82% to 46.74%."
  - [corpus]: "Robustness in Text-Attributed Graph Learning" confirms textual and structural perturbations have distinct effects, supporting the separation of attack vectors.
- **Break condition:** This mechanism likely fails if the GraphLLM uses a robust text encoder that projects synonyms to identical vectors or if the model ignores text features entirely.

### Mechanism 2
- **Claim:** Modifying graph topology (edge insertion/deletion) degrades performance by distorting the structural embeddings injected into the LLM.
- **Mechanism:** Attacks like Nettack or PRBCD modify the adjacency matrix to reshape a node's neighborhood, causing the GNN encoder to generate "poisoned" structural embeddings that lead to incorrect inference when projected into the LLM's token space.
- **Core assumption:** The mechanism assumes the GraphLLM utilizes a GNN or sampling method to encode structural context into the prompt, and this encoder is sensitive to topology changes.
- **Evidence anchors:**
  - [section 4.1.1]: "Adversaries reshape neighborhoods in ways that mislead the model’s inference... distorting relational dependencies."
  - [section 5.2.2]: "LLaGA is the most affected... accuracy drops of 43.07% on Cora under Nettack."
  - [corpus]: "Unveiling the Vulnerability of Graph-LLMs" further supports that combining LLMs with GNNs introduces specific structural vulnerabilities.
- **Break condition:** This mechanism is less effective against models with strong "graph priors" or those that rely less on local neighborhood fidelity.

### Mechanism 3
- **Claim:** Prompt manipulation attacks (specifically cross-domain noise) function by diluting the attention mechanism and disrupting label association.
- **Mechanism:** By injecting irrelevant labels (noise) into the prompt's candidate list, the attack expands the output space and distracts the LLM, making it struggle to associate correct input features with valid labels among "foreign" or "noisy" options.
- **Core assumption:** The LLM's ability to select the correct label is contingent on a clean, relevant set of candidate options in the prompt context.
- **Evidence anchors:**
  - [section 4.3.1]: "Altered prompt... disrupts how the model associates labels with the correct answer."
  - [section 5.2.3]: "Cross-domain noise is the most disruptive... GraphPrompter's performance drops by 57.42%."
  - [corpus]: Evidence regarding prompt robustness in general LLM literature exists, but specific mechanism for GraphLLMs is defined in this paper.
- **Break condition:** The attack fails if the model is trained with "Label Noise Training" to become invariant to label set expansion or reordering.

## Foundational Learning

- **Concept: Text-Attributed Graphs (TAGs)**
  - **Why needed here:** This is the fundamental data structure. You cannot understand the attack vectors without grasping that nodes possess both textual features (vulnerable to text attacks) and edges (vulnerable to structure attacks).
  - **Quick check question:** How does a TAG differ from a standard citation graph?

- **Concept: GraphLLM Architecture (Encoder + Projector + Frozen LLM)**
  - **Why needed here:** The paper targets the integration points. You must understand how the "Projector" aligns graph embeddings with the LLM to see where the "poisoned" data enters the system.
  - **Quick check question:** In the GraphLLM pipeline, where does the structural information get converted into a format the LLM can process?

- **Concept: Adversarial Training (FGSM & PGD)**
  - **Why needed here:** This is the primary defense strategy. Understanding the difference between single-step (FGSM) and iterative (PGD) perturbation generation is key to replicating the defense experiments.
  - **Quick check question:** Why might PGD provide better robustness than FGSM, even if it is computationally more expensive?

## Architecture Onboarding

- **Component map:** Victim Models (LLaGA, GraphPrompter, GraphTranslator) -> Attack Surface (Text, Structure, Prompt) -> Defense Module (FGSM/PGD, Data Augmentation)
- **Critical path:**
  1. **Surrogate Attack:** Generate perturbations (text/structure) using a smaller surrogate model or black-box access
  2. **Inference Attack:** Inject perturbations into the GraphLLM input (prompt or adjacency matrix)
  3. **Defense Application:** Train the GraphLLM projector using a mix of clean and perturbed samples
- **Design tradeoffs:**
  - **Robustness vs. Accuracy:** Defenses like FGSM improve robustness but sometimes cause minor performance degradation on clean data
  - **Model Dependency:** Text-reliant models are more robust to structure attacks but vulnerable to text attacks, whereas structure-reliant models show the inverse
- **Failure signatures:**
  - **High Sensitivity:** A sudden drop in accuracy (>20%) on Cora dataset when attacked by TextHoaxer indicates failure to robustly align text embeddings
  - **Prompt Confusion:** Drastic accuracy loss when "Cross-Domain Noise" is added indicates the model relies on superficial label matching rather than deep semantic reasoning
- **First 3 experiments:**
  1. **Reproduce TextHoaxer Attack:** Run TextHoaxer attack on Cora dataset against GraphPrompter to verify reported ~26% accuracy drop
  2. **Test Prompt Label Noise:** Implement "Cross-Domain Noise" attack on Amazon-Computers dataset to observe 57% drop in GraphPrompter performance
  3. **Validate FGSM Defense:** Train LLaGA using FGSM on Cora dataset and measure change in robustness against Nettack structure attack

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What defense mechanisms beyond adversarial training can fully secure GraphLLMs against combined text, structure, and prompt attacks without degrading clean performance?
- Basis in paper: [explicit] The authors state in the Conclusion that investigated defenses "improved robustness yet did not completely eliminate the risks," and Section 5.4.1 notes that data augmentation alone is "insufficient to completely mitigate adversarial effects."
- Why unresolved: Current defenses result in model- and dataset-specific trade-offs, failing to restore original accuracy in many scenarios.
- What evidence would resolve it: A novel defense strategy that maintains or exceeds baseline accuracy on clean data while achieving near-zero Attack Success Rates (ASR) across all three attack dimensions.

### Open Question 2
- Question: Can graph structure learning (GSL) be effectively adapted for GraphLLMs to defend against structural attacks despite existing scalability limitations?
- Basis in paper: [explicit] Section 4.1.2 mentions that while GSL is a potential defense, it was excluded from the benchmark because existing methods "often suffer from scalability issue."
- Why unresolved: Integrating structure learning into the already computationally expensive GraphLLM pipeline remains a challenge for large-scale graphs.
- What evidence would resolve it: A scalable GSL algorithm that operates efficiently within the GraphLLM framework and demonstrates superior robustness compared to adversarial training.

### Open Question 3
- Question: Do white-box attacks optimized directly on the GraphLLM yield higher success rates than the surrogate-based gray-box attacks used in this study?
- Basis in paper: [inferred] Section 4.1.1 notes that structural perturbations were generated using a surrogate GCN model because querying the LLM is "computationally expensive," creating a potential gap in attack transferability.
- Why unresolved: The reliance on surrogate gradients may underestimate the true vulnerability of the GraphLLM's specific reasoning process.
- What evidence would resolve it: A comparative analysis of Attack Success Rates (ASR) between surrogate-based attacks and white-box attacks utilizing gradients from the frozen LLM components.

## Limitations
- **Surrogate Model Architecture:** Exact GCN configuration used for generating structure attacks is unspecified, potentially impacting attack effectiveness replication
- **Random Seed Dependencies:** Lack of documented random seeds introduces variability in results, particularly for sampling-based attacks and data splits
- **Neighbor Sampling Strategy:** k-value for graph sampling in each GraphLLM is not specified, potentially affecting structural embedding quality and attack susceptibility

## Confidence
- **High Confidence:** The core finding that GraphLLMs are vulnerable to text, structure, and prompt attacks is well-supported by consistent results across multiple datasets and attack types
- **Medium Confidence:** The relative vulnerability rankings between models are plausible but may depend on implementation details not fully specified
- **Low Confidence:** The absolute accuracy numbers for specific attack-defense combinations may vary significantly with implementation choices not documented in the paper

## Next Checks
1. **Surrogate Model Verification:** Reproduce the surrogate GCN architecture and validate that generated structure attacks achieve similar success rates on Cora as reported
2. **Defense Effectiveness Replication:** Implement FGSM defense on LLaGA and measure robustness improvement against Nettack, comparing to Figure 3 results
3. **Cross-Domain Noise Sensitivity:** Test the prompt label noise attack on Amazon-Computers to verify the reported 57% performance drop in GraphPrompter