---
ver: rpa2
title: Performance Prediction for Large Systems via Text-to-Text Regression
arxiv_id: '2506.21718'
source_url: https://arxiv.org/abs/2506.21718
tags:
- figure
- regression
- prediction
- performance
- resource
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes text-to-text regression as a scalable alternative
  to traditional tabular regression for predicting outcomes in complex systems like
  Google's Borg compute cluster. The method uses a 60M parameter encoder-decoder language
  model trained from random initialization to directly predict floating-point efficiency
  metrics from rich, non-tabular system logs and configuration data represented as
  text.
---

# Performance Prediction for Large Systems via Text-to-Text Regression

## Quick Facts
- arXiv ID: 2506.21718
- Source URL: https://arxiv.org/abs/2506.21718
- Reference count: 40
- Primary result: Text-to-text regression achieves up to 0.99 rank correlation and 100x lower MSE than tabular approaches for predicting Google Borg compute cluster efficiency metrics

## Executive Summary
This paper introduces text-to-text regression as a scalable alternative to traditional tabular regression for predicting outcomes in complex systems like Google's Borg compute cluster. The method uses a 60M parameter encoder-decoder language model trained from random initialization to directly predict floating-point efficiency metrics from rich, non-tabular system logs and configuration data represented as text. The model achieves state-of-the-art performance by maximizing feature observability and leveraging large-scale pretraining, while also demonstrating strong few-shot adaptation capabilities requiring only 500 examples for new tasks.

## Method Summary
The approach uses a 60M parameter encoder-decoder transformer trained from random initialization to predict numeric performance metrics from YAML-formatted system logs and configuration data. The model employs cross-entropy loss over next tokens rather than MSE loss, treating numeric output as a sequence prediction problem. Training uses Adafactor optimizer with specific hyperparameters, and inference involves sampling 128 completions per input and aggregating results. The methodology demonstrates significant improvements over tabular approaches by preserving richer feature representations and naturally capturing complex outcome distributions through density estimation.

## Key Results
- Achieves up to 0.99 Spearman rank correlation on efficiency predictions
- Demonstrates 100x lower mean squared error compared to tabular regression baselines
- Requires only 500 examples for effective few-shot adaptation to new tasks
- Captures complex outcome distributions through density estimation, showing strong calibration properties

## Why This Works (Mechanism)

### Mechanism 1: Feature Observability Reduces Epistemic Uncertainty
By representing nested, variable-cardinality features directly as strings, the model avoids information loss from manual feature engineering, lowering the irreducible error floor through reduced epistemic uncertainty.

### Mechanism 2: Encoder-Decoder Separation Handles Complex Inputs
Encoder layers with bidirectional attention aggregate information across long input sequences, while decoders generate numeric tokens autoregressively. This separation is crucial for processing high-dimensional, structured inputs.

### Mechanism 3: Cross-Entropy Over Tokens Enables Multi-Task Density Estimation
Training with next-token cross-entropy produces both accurate point predictions and calibrated uncertainty by learning p(y|x) as a full density, treating numeric output as a sequence prediction problem.

## Foundational Learning

- **Aleatoric vs. Epistemic Uncertainty**: Understanding the distinction between irreducible noise (aleatoric) and reducible uncertainty (epistemic) is essential for interpreting error bounds and ablation results. Quick check: If you observe more features and error decreases, which type of uncertainty was reduced?

- **Bias-Variance Decomposition**: The theoretical justification for maximizing feature observability comes from the formal derivation showing that any regressor's MSE is lower-bounded by Var(y|φ(x)). Quick check: What happens to the optimal achievable MSE if two distinct states x, x' map to the same observed φ(x)?

- **P10 Tokenization for Numeric Outputs**: Using sign/mantissa/exponent tokens (e.g., <+><7><2><5><E-1>) rather than embedding each integer separately keeps vocabulary small while representing any float. Quick check: Why would tokenizing "72.5" as a single token <72.5> harm generalization?

## Architecture Onboarding

- **Component map**: Input YAML string -> SentencePiece tokenizer + P10 numeric tokens -> 2-layer Encoder (bidirectional attention) -> 2-layer Decoder (causal attention) -> Sampled y-values -> Aggregate for point estimate + variance

- **Critical path**: Format raw logs/configs as YAML strings -> Tokenize with P10 for y-values -> Train with cross-entropy on (x, y) pairs -> At inference: sample 128 completions per x, aggregate for point estimate + variance

- **Design tradeoffs**: Sequence length vs. compute (diminishing returns beyond ~3K tokens), encoder depth vs. decoder depth (2E2D outperforms alternatives), pretraining diversity vs. few-shot adaptation (more tasks improve OOD transfer but may cause meta-overfitting)

- **Failure signatures**: High validation loss with short sequences (truncation cutting critical features), good rank correlation but high MSE (miscalibration, check tokenization), OOD fine-tuning fails to improve (overfitted checkpoint, use earlier), predictions cluster at single value (decoder collapsed, verify learning rate)

- **First 3 experiments**: 1) Baseline sanity check: Train on single cell with full features vs. ablated features, 2) Architecture sweep: Compare 0E4D vs. 2E2D on same data, 3) Few-shot transfer test: Pretrain on 4 cells, fine-tune on 64 examples from held-out cell

## Open Questions the Paper Calls Out

### Open Question 1
Why do encoder-decoder architectures significantly outperform decoder-only models in processing complex nested feature representations for regression? The paper demonstrates the performance gap empirically but offers only a hypothesis regarding decoder-only attention limitations for dense feature processing.

### Open Question 2
Can text-to-text regressors be effectively integrated as "world models" or reward functions to accelerate reinforcement learning for system optimization? The paper frames this as future research but doesn't test the model in a closed-loop control setting.

### Open Question 3
Does pre-training on general-purpose human language data improve or impair the convergence and accuracy of regression models on highly specialized technical logs? The authors chose random initialization but leave the potential benefits or negative transfer of language pre-training unexplored.

## Limitations
- Proprietary Borg dataset prevents independent validation and reproduction of claimed results
- Exact feature preprocessing pipeline converting raw logs to YAML format is unspecified
- Few-shot adaptation claims depend critically on checkpoint selection and learning rate scheduling not fully specified

## Confidence

- **High Confidence**: Architectural insights about encoder-decoder superiority over decoder-only for complex inputs are well-supported by ablation studies and theoretically grounded
- **Medium Confidence**: Cross-entropy training methodology's ability to produce calibrated uncertainty estimates is demonstrated internally but lacks external validation
- **Low Confidence**: Few-shot adaptation capability claims depend critically on unspecified fine-tuning protocol details

## Next Checks

1. Apply methodology to open-source cluster monitoring dataset to verify 100x MSE improvement on independently verifiable data

2. Generate synthetic data with known aleatoric noise distributions, train the model, and measure whether predicted variance correlates with actual prediction error

3. Systematically vary sequence length and feature complexity in controlled synthetic dataset to confirm exact point where encoder-decoder architecture becomes necessary