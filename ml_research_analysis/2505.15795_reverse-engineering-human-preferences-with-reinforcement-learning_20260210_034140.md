---
ver: rpa2
title: Reverse Engineering Human Preferences with Reinforcement Learning
arxiv_id: '2505.15795'
source_url: https://arxiv.org/abs/2505.15795
tags:
- attack
- preambles
- command
- specific
- preamble
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates adversarial attacks on LLM-as-a-judge evaluation
  frameworks. The authors propose Reinforcement Learning for Reverse Engineering (RLRE),
  a method that adversarially tunes a preamble generator to optimize upstream text
  instructions that maximize downstream evaluation scores from judge-LLMs.
---

# Reverse Engineering Human Preferences with Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2505.15795
- **Source URL:** https://arxiv.org/abs/2505.15795
- **Reference count:** 40
- **Primary result:** Adversarial tuning of preamble generators substantially increases LLM-as-a-judge scores while remaining undetectable via perplexity analysis.

## Executive Summary
This study introduces Reinforcement Learning for Reverse Engineering (RLRE), a method that adversarially tunes a preamble generator to optimize upstream text instructions that maximize downstream evaluation scores from judge-LLMs. Unlike post-hoc response editing attacks, RLRE generates natural language preambles injected into frozen candidate-LLMs before inference. Experiments demonstrate that pipelining candidate-LLMs with adversarially tuned preambles significantly increases judge-LLM scores (e.g., +0.64 points on MT-Bench for Command R7B) while remaining virtually undetectable via perplexity analysis and human inspection. The attack successfully transfers across candidate-LLMs, judge-LLMs, and evaluation benchmarks, revealing significant vulnerabilities in LLM-as-a-judge frameworks.

## Method Summary
The method treats the pipeline as a bi-level optimization problem where a preamble generator learns to output token sequences that shift the conditional probability distribution of a frozen candidate-LLM. Using Contrastive Policy Gradient (CoPG), the generator compares rewards of two sampled preambles and adjusts its policy to prefer the token sequence that led to the higher-scoring response. The generator is trained to maximize scores from a frozen judge-LLM while penalizing divergence from a reference distribution using a low KL-divergence coefficient (β=0.03). Training uses high sampling temperature (T=4.0) for exploration, while inference uses lower temperature (T=0.5).

## Key Results
- RLRE achieves +0.64 points improvement on MT-Bench for Command R7B when pipelined with adversarially tuned preambles
- The attack remains virtually undetectable via perplexity analysis with false negative rates exceeding 0.90
- Generated preambles successfully transfer across candidate-LLMs, judge-LLMs, and evaluation benchmarks
- Analysis reveals optimal preamble style varies significantly across models, suggesting natural language constraints may limit performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing upstream instructions (preambles) via reinforcement learning allows a frozen policy (Candidate-LLM) to maximize rewards defined by a downstream evaluator (Judge-LLM).
- **Mechanism:** The method treats the pipeline as a bi-level optimization problem. The **Preamble Generator** learns to output token sequences that shift the conditional probability distribution of the **Candidate-LLM**. By using **Contrastive Policy Gradient (CoPG)**, the generator compares the rewards of two sampled preambles and adjusts its policy to prefer the token sequence that led to the higher-scoring response. Because the Candidate-LLM is frozen, the learning signal must implicitly model the candidate's response behavior to optimize the input context.
- **Core assumption:** The gradient signal from the reward model is sufficiently dense and accurate to guide the generator through the discrete latent space of the Candidate-LLM's behavior.
- **Evidence anchors:**
  - [abstract] "use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance."
  - [section 3] Details the loss function $L(p_j, p'_j; \pi)$ and the expectation $E[R(q, c)]$.
  - [corpus] "RLAF: Reinforcement Learning from Automaton Feedback" supports the general viability of using structured feedback for RL in LLMs, though this paper focuses specifically on judge scores.
- **Break condition:** If the Judge-LLM provides a flat or highly noisy reward landscape (e.g., random scoring), the contrastive loss would fail to identify advantageous preamble features.

### Mechanism 2
- **Claim:** The generated preambles exploit superficial heuristics in the Judge-LLM (such as formatting preferences) rather than improving objective correctness.
- **Mechanism:** The paper posits that "human preferences" learned by Judge-LLMs often reduce to simpler proxies like "structured reasoning chains" or "length." The RLRE generator discovers these proxies (e.g., adding distinct paragraph headers) without being explicitly programmed to do so. It effectively performs a "reverse engineering" of the judge's hidden decision boundary, identifying "soft spots" where the judge over-weights presentation over substance.
- **Core assumption:** Judge-LLMs rely on systematic, exploitable biases (e.g., authority, verbosity, structure) that can be approximated by a generator model.
- **Evidence anchors:**
  - [abstract] "These findings demonstrate effective reverse engineering of human preferences..."
  - [section 5.4] "We thus postulate that the improved layout may be solely responsible for the higher scores assigned by the judge-LLM, regardless of correctness."
  - [corpus] "Black-Box Guardrail Reverse-engineering Attack" parallels this by discussing how attackers exploit observable patterns in model guardrails, analogous to exploiting judge preferences.
- **Break condition:** If the Judge-LLM is robust against length/formatting biases or strictly evaluates semantic truth, the preamble attack would converge to a null result (no score improvement).

### Mechanism 3
- **Claim:** Constraining preambles to fluent natural language may limit attack effectiveness; optimal preambles can appear as "gibberish" or highly repetitive text to a human reader.
- **Mechanism:** The authors use a low KL-divergence coefficient ($\beta=0.03$), allowing the generator to deviate significantly from its reference distribution (fluency). The resulting preambles often contain repetitive or non-sensical token sequences that nonetheless trigger high-reward behaviors in the Candidate-LLM. This suggests that the internal representations of LLMs respond to token patterns (syntactic or frequency-based) that are not interpretable by humans.
- **Core assumption:** The Candidate-LLM is sensitive to non-semantic token patterns that the Judge-LLM rewards, and these patterns do not need to be human-readable.
- **Evidence anchors:**
  - [section 5.4] "Analysis reveals high variability in optimal preamble style... constraining conditioning tokens to natural language may limit model performance."
  - [appendix g] Shows representative preambles where Llama 3.1 70B preambles devolve into repetitive or non-fluent strings.
  - [corpus] Weak or missing support; corpus neighbors generally focus on semantic/functional RE rather than non-natural language optimization.
- **Break condition:** If the Candidate-LLM has a strong semantic filter that ignores incoherent inputs (instruction-following collapse), the "gibberish" preambles would fail to influence the output.

## Foundational Learning

- **Concept: Contrastive Policy Gradient (CoPG)**
  - **Why needed here:** This is the core optimizer used (adapted from [12]). Unlike PPO which requires a critic model, CoPG compares pairs of outputs $(p, p')$ to compute a loss based on the reward delta. You cannot understand the training loop without this.
  - **Quick check question:** How does CoPG differ from standard Reinforcement Learning (PPO) in terms of memory overhead and reward requirements? (Answer: No critic model; handles non-preference/absolute rewards).

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The entire reward signal comes from a frozen LLM acting as a judge (e.g., Command R+, GPT-4). Understanding how these judges assign scores (single vs. pairwise, verbosity bias) is critical to understanding *what* the attack optimizes for.
  - **Quick check question:** Does the attack edit the judge model or the candidate model? (Answer: Neither; it edits the input to the candidate to fool the judge).

- **Concept: Perplexity (PPL) as a Defense**
  - **Why needed here:** The paper claims success because the attack is "undetectable." You must understand why standard defenses (checking if text looks "likely" or "fluent" via perplexity) fail here.
  - **Quick check question:** Why does a post-hoc attack (appending text) trigger high perplexity, while the RLRE preamble attack does not? (Answer: Post-hoc text alters the *response* distribution; RLRE alters the *input*, keeping the response fluent).

## Architecture Onboarding

- **Component map:** Preamble Generator -> Candidate-LLM -> Judge-LLM

- **Critical path:**
  1. Sample a question $q$.
  2. Generator creates two preambles $p, p'$.
  3. Candidate-LLM generates responses $c, c'$ for both preambles.
  4. Judge-LLM assigns scalar rewards $R(q, c), R(q, c')$.
  5. Calculate CoPG Loss using the reward difference and log-probabilities relative to the reference model.
  6. Update Generator weights.

- **Design tradeoffs:**
  - **Fluency vs. Reward:** A low $\beta$ (KL coefficient, e.g., 0.03) yields high rewards but "gibberish" preambles. A high $\beta$ yields fluent instructions but potentially lower scores. The paper argues for low $\beta$ to maximize attack effectiveness.
  - **Temperature:** High sampling temperature ($t=4.0$) is used during training to ensure diversity (exploration), while lower temp ($t=0.5$) is used at inference.

- **Failure signatures:**
  - **High False Positive Rate (FPR) in PPL:** If candidate responses start showing high perplexity, the generator may be drifting too far from natural language distributions (over-optimization).
  - **Reward Hacking (Formatting):** The generator might learn to produce valid XML/markdown structures that the judge "likes" but the user hates (if used in non-adversarial contexts).
  - **Collapsed Preambles:** If the loss converges to a single repetitive phrase (mode collapse), the exploration temperature or batch size may be insufficient.

- **First 3 experiments:**
  1. **Baseline & Hyperparameter Tuning:** Train the Generator (Command R7B) on UltraFeedback. Vary $\beta$ (e.g., 0.01 vs 0.03 vs 0.1) and measure MT-Bench score vs. Preamble Fluency (human review).
  2. **Detectability Audit:** Run a PPL-W (Perplexity Window) test on the generated responses. Verify that the False Negative Rate (FNR) remains high (>0.90) compared to baseline "Universal Attack" text injections.
  3. **Transferability Stress Test:** Train the Generator on Pipeline A (Command R). Test the *same* generator on Pipeline B (Llama 3.1). Measure the score drop to confirm if the attack is model-specific or generalizes across architectures.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can RLRE be successfully adapted for beneficial applications such as reducing toxicity or mitigating bias in LLM outputs, rather than adversarial manipulation?
  - **Basis in paper:** [explicit] The conclusion states the approach "could be paired with different downstream rewards to optimise preambles for a variety of applications beyond just adversarial attacks—including but not limited to meaningful tasks such as reducing toxicity or mitigating bias."
  - **Why unresolved:** The authors only demonstrated RLRE in the adversarial attack setting; no experiments were conducted with alternative reward functions targeting prosocial outcomes.
  - **What evidence would resolve it:** Experiments applying RLRE with reward signals based on toxicity scores or bias metrics, showing improved performance on these dimensions without degrading task utility.

- **Open Question 2:** Does constraining conditioning tokens (e.g., preambles, reasoning traces) to the manifold of natural language inadvertently limit model performance?
  - **Basis in paper:** [explicit] Section 5.4 states that successful preambles showed "high variability in their fluency across different model pipelines" and that "constraining conditioning tokens—such as preambles or reasoning tokens—to the manifold of natural language may inadvertently limit model capabilities."
  - **Why unresolved:** The observation that non-fluent preambles (even apparently meaningless character strings for Llama models) remained effective suggests a tension between interpretability and performance, but no controlled study was conducted.
  - **What evidence would resolve it:** Controlled experiments comparing natural language constraints vs. relaxed token constraints on downstream task performance, measuring trade-offs between interpretability and effectiveness.

- **Open Question 3:** Would training with a jury of multiple judge-LLMs improve the transferability of RLRE attacks to unseen judges?
  - **Basis in paper:** [explicit] Appendix B.4 states: "We postulate that our training strategy could similarly benefit from an ensemble of multiple judge-LLMs, potentially improving the transferability of RLRE to new, unseen LLM-judges at inference... this remains an avenue worth exploring in future work."
  - **Why unresolved:** The authors trained with a single judge (Command R+) due to API cost and throttling constraints; jury-based training was not tested.
  - **What evidence would resolve it:** Experiments training preamble generators with rewards aggregated from multiple judge-LLMs, then evaluating transferability to held-out judges not in the ensemble.

- **Open Question 4:** Can RLRE effectively optimize tokens at different positions within an input sequence, such as post-query instructions (suffixes) rather than pre-query preambles?
  - **Basis in paper:** [explicit] The conclusion explicitly identifies as future work "the optimisation of tokens at different positions within an input sequence (e.g., post-query instructions instead of pre-query preambles)."
  - **Why unresolved:** The current study only optimized preambles injected before the question; suffix-based optimization was not investigated.
  - **What evidence would resolve it:** Experiments applying RLRE to generate post-query instructions appended after the question, comparing effectiveness and detectability to preamble-based approaches.

## Limitations
- The attack's effectiveness may diminish when applied to judge-LLMs with stronger robustness mechanisms or fundamentally different reward functions
- The use of contrastive rewards rather than absolute rewards introduces uncertainty about whether the generator truly learns optimal strategies
- Focus on UltraFeedback and MT-Bench datasets may not capture the full spectrum of real-world evaluation scenarios

## Confidence
- **High Confidence:** The core methodology of using CoPG to optimize preamble generators is technically sound and well-supported by experimental results
- **Medium Confidence:** The claim that the attack "successfully transfers across candidate-LLMs, judge-LLMs, and evaluation benchmarks" is supported but may overstate robustness
- **Medium Confidence:** The assertion that constraining preambles to natural language limits performance is plausible but not definitively proven
- **Low Confidence:** The interpretation that the attack reveals fundamental vulnerabilities in LLM-as-a-judge frameworks may overstate practical significance

## Next Checks
1. **Robustness to Countermeasures:** Test whether simple post-processing defenses (like perplexity filtering combined with semantic coherence checks) can reliably detect and block the attack while maintaining low false positive rates on legitimate prompts.

2. **Cross-Domain Transferability:** Evaluate the attack's effectiveness when applied to judge-LLMs trained on different preference datasets (e.g., RewardBench, Direct Preference Optimization data) to assess whether the discovered vulnerabilities generalize beyond the specific judge models used in the study.

3. **Resource Cost Analysis:** Quantify the computational overhead and API cost implications of implementing this attack at scale, including the marginal benefit of using higher-capacity generator models versus the diminishing returns in score improvement.