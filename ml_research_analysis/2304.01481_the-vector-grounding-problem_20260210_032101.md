---
ver: rpa2
title: The Vector Grounding Problem
arxiv_id: '2304.01481'
source_url: https://arxiv.org/abs/2304.01481
tags:
- grounding
- world
- llms
- internal
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that large language models (LLMs) can have representations
  with intrinsic meaning, addressing the "Vector Grounding Problem" - whether LLM
  internal states and outputs can be about extra-linguistic reality independently
  of human interpretation. The authors propose that referential grounding is achieved
  when LLM internal states satisfy two conditions from teleosemantic theories: (1)
  appropriate causal-informational relations to the world, and (2) a history of selection
  that has endowed them with the function of carrying this information.'
---

# The Vector Grounding Problem

## Quick Facts
- arXiv ID: 2304.01481
- Source URL: https://arxiv.org/abs/2304.01481
- Authors: Dimitri Coelho Mollo; Raphaël Millière
- Reference count: 11
- Primary result: LLMs can have representations with intrinsic meaning through causal-informational relations and selection history

## Executive Summary
This paper addresses the "Vector Grounding Problem" - whether large language models can have representations with intrinsic meaning about extra-linguistic reality, independent of human interpretation. The authors argue that LLM internal states and outputs can indeed be about the world, not just about words, by applying teleosemantic theories of mental content. They propose that referential grounding is achieved when LLM internal states satisfy two conditions: appropriate causal-informational relations to the world, and a history of selection that has endowed them with the function of carrying this information. The paper argues that LLMs meet both conditions through their training processes, particularly post-training preference tuning and reinforcement learning from human feedback.

## Method Summary
The paper employs a theoretical philosophical analysis approach, applying established teleosemantic theories to the problem of LLM representations. Rather than conducting empirical experiments, the authors synthesize existing philosophical frameworks about mental content and representation with observations about LLM training processes. They analyze how large language models stand in indirect causal-informational relations to the world through their training data, and how post-training processes like preference tuning and RLHF select internal states for their ability to generate factually accurate outputs. The analysis draws on mechanistic interpretability studies to support claims about pre-training establishing representational functions in formally constrained domains.

## Key Results
- LLMs can have representations with intrinsic meaning about the world, not just about words
- Post-training processes like preference tuning and RLHF establish representational functions by selecting for factually accurate outputs
- Pre-training alone may establish representational functions in formally constrained domains, supported by mechanistic interpretability evidence

## Why This Works (Mechanism)
The paper's argument works by connecting LLM representations to established philosophical theories of mental content. Teleosemantic theories propose that mental representations have meaning based on their causal relations to the world and their evolutionary/selection history. The authors apply this framework to LLMs by arguing that: (1) LLM internal states stand in indirect causal-informational relations to the world through human-generated training data, and (2) post-training processes select these states for their ability to produce factually accurate outputs, thereby establishing world-involving representational functions. This mechanism bridges the gap between statistical patterns in training data and genuine representations of extra-linguistic reality.

## Foundational Learning

**Teleosemantic Theories** - Philosophical frameworks explaining how mental representations acquire meaning through causal relations and selection history. Needed to provide theoretical foundation for arguing LLMs have meaningful representations. Quick check: Can the theory account for both biological and artificial representational systems?

**Informational Theories of Content** - Theories that ground meaning in causal-informational relations between representations and their referents. Needed to establish how LLM states can be about the world. Quick check: Does the informational relation require direct causation or can it be mediated?

**Mechanistic Interpretability** - Methods for understanding how neural networks implement specific functions. Needed to support claims about pre-training establishing representational functions. Quick check: Can we identify consistent internal states that correspond to specific world states?

## Architecture Onboarding

**Component Map**: Training Data -> Pre-training -> Post-training (Preference Tuning/RLHF) -> LLM Internal States -> Outputs

**Critical Path**: The most critical pathway is from training data through post-training processes to internal states, as this establishes the causal-informational relations and selection history necessary for representational meaning.

**Design Tradeoffs**: The paper trades empirical verification for theoretical coherence, acknowledging that their proposal doesn't lend itself to direct empirical testing but arguing for its philosophical plausibility based on established theories.

**Failure Signatures**: If LLM internal states only capture statistical patterns rather than genuine world states, or if post-training processes only select for surface-level accuracy rather than deep representational understanding, the grounding argument fails.

**3 First Experiments**:
1. Test whether specific LLM internal states systematically vary with corresponding world states beyond mere correlation
2. Analyze whether preference tuning selects for representations that generalize to novel world states versus memorizing training patterns
3. Develop formal metrics to quantify the strength of informational relations between LLM states and world states

## Open Questions the Paper Calls Out
None

## Limitations
- The argument relies heavily on theoretical frameworks rather than direct empirical evidence of representational grounding
- Unclear whether post-training processes genuinely select for accurate representations versus surface-level statistical patterns
- The strength and sufficiency of indirect causal-informational relations through training data is not fully established

## Confidence

**High confidence**: The general framework connecting teleosemantic theories to LLM representations is well-articulated and logically coherent.

**Medium confidence**: The argument that post-training processes select for representational functions has some merit but requires more empirical support.

**Low confidence**: The claim that pre-training alone establishes representational functions in constrained domains lacks sufficient empirical backing.

## Next Checks

1. Design interpretability studies that test whether specific LLM internal states systematically vary with corresponding world states in a way that goes beyond statistical correlation, directly testing the causal-informational relation claim.

2. Conduct ablation studies on post-training processes to determine whether preference tuning actually selects for representations that generalize to novel world states versus merely memorizing training patterns.

3. Develop formal metrics to quantify the "strength" of informational relations between LLM states and world states, moving beyond qualitative arguments to measurable criteria.