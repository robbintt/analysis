---
ver: rpa2
title: "A Proof of Learning Rate Transfer under $\u03BC$P"
arxiv_id: '2511.01734'
source_url: https://arxiv.org/abs/2511.01734
tags:
- learning
- rate
- transfer
- convergence
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides the first formal proof of learning rate transfer\
  \ in the infinite-width limit of deep neural networks. The key insight is that under\
  \ the maximal update parametrization (\u03BCP), the optimal learning rate converges\
  \ to a non-zero constant as network width increases, enabling stable hyperparameter\
  \ transfer across scales."
---

# A Proof of Learning Rate Transfer under $Î¼$P

## Quick Facts
- arXiv ID: 2511.01734
- Source URL: https://arxiv.org/abs/2511.01734
- Authors: Soufiane Hayou
- Reference count: 40
- Primary result: First formal proof that optimal learning rate converges to non-zero constant under maximal update parametrization ($\mu$P) as network width increases

## Executive Summary
This paper provides the first formal proof of learning rate transfer in deep neural networks. Under the maximal update parametrization ($\mu$P), the optimal learning rate converges to a non-zero constant as network width increases, enabling stable hyperparameter transfer across scales. The key insight is that the training loss can be expressed as a polynomial in the learning rate whose coefficients converge to deterministic limits. This contrasts with standard parametrization where the optimal learning rate vanishes as width increases. The proof analyzes deep linear MLPs and shows that with $\mu$P scaling, the linear coefficient of the loss polynomial converges to a non-zero constant while higher-order terms vanish.

## Method Summary
The method analyzes deep linear MLPs with gradient descent optimization. Under $\mu$P, weight matrices $W_\ell$ are initialized with variance $n^{-1}$ and the readout layer $V$ with variance $n^{-2}$, contrasting with standard parametrization where $V$ has variance $n^{-1}$. The loss function after $t$ training steps is expressed as a polynomial in the learning rate $\eta$, with coefficients depending on initialization and network width. The proof shows these coefficients converge almost surely to deterministic limits as width increases. Synthetic data with $d=100$ features and $N=1000$ samples is used, with linear and non-linear targets. Widths range from $2^7$ to $2^{13}$ and depths from 3 to 27.

## Key Results
- Optimal learning rate converges to non-zero constant under $\mu$P as width increases
- Under standard parametrization, optimal learning rate converges to zero
- Loss function can be expressed as polynomial in learning rate with convergent coefficients
- Proof limited to linear networks, but empirical results validate transfer for ReLU activations and Adam optimizer

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The training loss in deep linear networks can be modeled as a polynomial function of the learning rate $\eta$.
- **Mechanism:** By expanding the network output after $t$ steps of gradient descent, the loss function $L_n^{(t)}(\eta)$ takes the form of a polynomial. As width $n$ increases, the coefficients of this polynomial converge to deterministic constants.
- **Core assumption:** The network is a Multi-Layer Perceptron (MLP) with linear activations (deep linear network).
- **Break condition:** Introducing non-linearities makes exact polynomial expression intractable.

### Mechanism 2
- **Claim:** The $\mu$P scaling of the readout layer $V$ (specifically $\alpha_V=2$) stabilizes the optimal learning rate at a non-zero constant.
- **Mechanism:** $\mu$P initializes $V$ with variance $n^{-2}$, which counteracts the width scaling in hidden layers, keeping feature learning dynamics $\Theta(1)$ and maintaining stable optimal $\eta$.
- **Core assumption:** The optimizer is Gradient Descent (GD).
- **Break condition:** If the readout layer is not scaled down sufficiently (e.g., SP), the optimal learning rate collapses to zero.

### Mechanism 3
- **Claim:** The stability of the optimal learning rate results from uniform convergence of the loss landscape's minimizer.
- **Mechanism:** Since loss polynomial coefficients converge almost surely to deterministic limits, the finite-width loss $L_n(\eta)$ converges uniformly to a limiting loss $L_\infty(\eta)$. If $L_\infty(\eta)$ has a unique minimizer $\eta_\infty > 0$, then $\eta_n$ converges to $\eta_\infty$.
- **Core assumption:** The limiting loss function $L_\infty(\eta)$ has a unique minimizer.
- **Break condition:** If the limiting loss $L_\infty(\eta)$ is flat or non-convex with multiple local minima, convergence to a single point is not guaranteed.

## Foundational Learning

- **Concept: Maximal Update Parametrization ($\mu$P)**
  - **Why needed here:** $\mu$P is the specific set of scaling rules that enables the "feature learning" regime in infinite width.
  - **Quick check question:** How does the initialization variance of the readout layer $V$ differ between Standard Parametrization (SP) and $\mu$P in a width-$n$ network?

- **Concept: Neural Tangent Kernel (NTK) / Kernel Regime**
  - **Why needed here:** The paper contrasts $\mu$P with NTK parametrization where features don't move during training.
  - **Quick check question:** Why does the "kernel regime" prevent meaningful feature learning during training?

- **Concept: Asymptotic Analysis (Big-O/Theta)**
  - **Why needed here:** The proof relies on analyzing how coefficients behave as $n \to \infty$.
  - **Quick check question:** In the loss polynomial $L(\eta)$, why is it critical that the linear coefficient $\phi_1$ converges to a non-zero constant ($\Theta(1)$) rather than vanishing?

## Architecture Onboarding

- **Component map:** Input $x \in \mathbb{R}^d$ -> Hidden layers with $W_\ell \in \mathbb{R}^{n \times n}$ (init variance $n^{-1}$) -> Readout layer $V \in \mathbb{R}^n$ (init variance $n^{-2}$ for $\mu$P) -> Output
- **Critical path:**
  1. Initialize weights using $\mu$P scaling
  2. Run Gradient Descent
  3. Express the loss $L(\eta)$ as a polynomial
  4. Analyze the limit of the linear coefficient $\phi_1$ to find theoretical limit $\eta_\infty$
- **Design tradeoffs:**
  - **Proof vs. Generality:** The proof is rigorous for *linear* networks only; empirical results suggest it holds for ReLU/Adam but not formally proven
  - **$\mu$P vs SP:** $\mu$P requires careful implementation of scaling rules but enables zero-shot HP transfer; SP is standard but requires retuning $\eta$ for every width
- **Failure signatures:**
  - **Vanishing LR:** If optimal LR drops sharply as width increases, check if $V$ was initialized with $1/n$ variance (SP) instead of $1/n^2$ ($\mu$P)
  - **Exploding LR:** If using Adam with $\mu$P, scale the LR by $1/n$
- **First 3 experiments:**
  1. **Width Sweep (Linear):** Train a linear MLP with $\mu$P across widths $n \in \{128, 256, \dots, 8192\}$. Plot training loss vs. $\eta$ to verify the "valley" stays at same $\eta$ value.
  2. **Baseline Comparison (SP):** Repeat width sweep using Standard Parametrization ($V \sim N(0, 1/n)$). Plot optimal $\eta$ vs. width to verify shift toward 0.
  3. **Depth Scaling:** Fix width and vary depth $L$. Check if optimal $\eta$ decreases as depth increases, consistent with theoretical limit $\eta_\infty \propto 1/L$.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the proof of learning rate transfer under $\mu$P be rigorously extended to non-linear neural networks?
- **Basis in paper:** The Discussion section states results are limited to linear networks and explicitly leaves extension to non-linear MLPs for future work.
- **Why unresolved:** The current proof relies on the property that loss is a polynomial in learning rate, a structure that becomes significantly more complex with non-linear activations.
- **What evidence would resolve it:** A theoretical proof showing optimal learning rate converges to non-zero constant for MLPs with non-linear activation functions (e.g., ReLU).

### Open Question 2
- **Question:** Does learning rate transfer theoretically hold for adaptive optimizers like Adam in the infinite-width limit?
- **Basis in paper:** The paper notes extending results to different optimizers will likely require different proof machinery, despite empirical results suggesting transfer occurs with Adam.
- **Why unresolved:** The provided proofs are derived specifically for Gradient Descent; Adam's dynamics involve moments and normalization that complicate the polynomial analysis.
- **What evidence would resolve it:** A rigorous convergence proof for $\mu$P tailored to Adam's update rules.

### Open Question 3
- **Question:** Is the $O(n^{-1/2})$ convergence rate bound tight, and what explains the empirically observed acceleration at large widths?
- **Basis in paper:** The paper observes empirical convergence rate matches $n^{-1/2}$ only up to a certain width before becoming faster, noting "we currently do not have an explanation for this sudden change."
- **Why unresolved:** The analysis provides an upper bound, but the precise asymptotic behavior and cause of faster empirical convergence remain uncharacterized.
- **What evidence would resolve it:** A refined theoretical analysis providing a tighter convergence rate or identifying the asymptotic phase transition.

## Limitations
- Proof applies only to linear networks, not general non-linear neural networks
- Convergence guarantees depend on the limiting loss function having a unique minimizer
- Extension to adaptive optimizers like Adam requires different proof machinery
- The relationship between feature learning and optimal LR stability lacks formal theoretical justification

## Confidence

- **Linear network proof:** High
- **Non-linear empirical results:** Medium
- **Feature learning mechanism:** Medium
- **General convergence guarantees:** Medium

## Next Checks

1. Verify the limiting loss function's convexity for different architectures to ensure unique minimizer conditions
2. Test transfer stability across different optimizers (SGD, AdamW) and learning rate schedules
3. Investigate cases where optimal LR appears to converge to zero despite $\mu$P scaling to identify hidden break conditions