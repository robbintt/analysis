---
ver: rpa2
title: 'EvolKV: Evolutionary KV Cache Compression for LLM Inference'
arxiv_id: '2509.08315'
source_url: https://arxiv.org/abs/2509.08315
tags:
- cache
- evolkv
- uni00000013
- budget
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EvolKV is an evolutionary framework that optimizes layer-wise KV
  cache budgets in large language models by directly maximizing downstream task performance.
  Unlike heuristic methods that apply fixed or pyramidal cache allocation across layers,
  EvolKV treats cache budgets as learnable parameters and uses evolutionary search
  to discover task-aware, non-uniform distributions.
---

# EvolKV: Evolutionary KV Cache Compression for LLM Inference

## Quick Facts
- arXiv ID: 2509.08315
- Source URL: https://arxiv.org/abs/2509.08315
- Reference count: 40
- Primary result: Evolutionary search discovers task-aware KV cache budgets, achieving up to 7 percentage point gains on GSM8K and superior code completion performance using only 1.5% of the cache budget.

## Executive Summary
EvolKV introduces an evolutionary framework for optimizing layer-wise KV cache budgets in large language model inference. Unlike heuristic methods that apply fixed or pyramidal cache allocation across layers, EvolKV treats cache budgets as learnable parameters and uses evolutionary search to discover task-aware, non-uniform distributions. The approach directly maximizes downstream task performance while respecting memory constraints, achieving consistent improvements across 11 tasks with models like Mistral-7B-Instruct and Llama-3-8B-Instruct.

The key innovation lies in using CMA-ES optimization to sequentially determine cache budgets for groups of layers, starting from the bottom of the network. This evolutionary approach discovers cache distributions that are specifically tuned to each task's characteristics, outperforming static heuristic methods that apply the same allocation strategy across all tasks. The framework demonstrates that learned, adaptive allocation can provide significant performance gains while reducing memory usage.

## Method Summary
EvolKV employs a group-wise evolutionary optimization strategy where KV cache budgets are treated as learnable parameters optimized directly for task performance. The method groups layers (typically 8 per group for 32-layer models) and uses CMA-ES to optimize per-group budgets sequentially from bottom to top, with each optimization fixing previous groups to their best-found values. The fitness function combines task-specific metrics with a cache budget adherence score, using SnapKV as the base compression technique with window size 32 and kernel size 7. Budget completion through proportional redistribution allows the method to scale learned distributions to different total cache sizes.

## Key Results
- Achieved up to 7 percentage point gains on GSM8K compared to baseline methods
- Outperformed full cache settings on code completion using only 1.5% of the cache budget
- Demonstrated consistent improvements across 11 tasks with Mistral-7B-Instruct and Llama-3-8B-Instruct models
- Showed superior performance to static heuristic methods like PyramidKV and StreamingLLM

## Why This Works (Mechanism)
The evolutionary optimization discovers non-uniform cache distributions that are specifically tuned to each task's characteristics, rather than applying fixed heuristics across all tasks. By directly optimizing for downstream performance metrics while incorporating budget constraints, the method finds cache allocations that balance the trade-off between information retention and memory efficiency. The sequential group-wise optimization allows the method to build optimal cache distributions layer by layer, leveraging the learned patterns from earlier layers when optimizing later ones.

## Foundational Learning

**KV cache compression**: Why needed - Reduces memory footprint during LLM inference by selectively retaining key-value pairs. Quick check - Verify cache size reduction while maintaining inference quality.

**CMA-ES optimization**: Why needed - Handles continuous, non-convex optimization problems with adaptive step sizes. Quick check - Monitor convergence behavior and fitness improvement over iterations.

**Group-wise optimization**: Why needed - Balances optimization granularity with computational efficiency. Quick check - Test different group sizes to find optimal trade-off between performance and runtime.

**Budget constraint enforcement**: Why needed - Ensures memory limits are respected during optimization. Quick check - Verify CACHE_SCORE consistently penalizes budget violations.

## Architecture Onboarding

**Component map**: Task data -> CMA-ES optimizer -> Layer budget parameters -> SnapKV compression -> Model inference -> Fitness evaluation -> Budget update

**Critical path**: CMA-ES optimization loop (evaluate fitness → update parameters → enforce constraints) with sequential group processing from bottom to top layers.

**Design tradeoffs**: Sequential group optimization vs. joint optimization (computational efficiency vs. global optimality), small population size vs. convergence stability, fixed vs. adaptive budget completion methods.

**Failure signatures**: Performance degradation indicates poor convergence or overfitting to optimization samples; budget violations suggest insufficient penalty weighting or inappropriate step sizes; inconsistent results across seeds indicate optimization instability.

**First experiments**: 1) Test CMA-ES convergence with ng=8 groups on synthetic fitness landscapes, 2) Validate SnapKV compression quality with varying window/kernel sizes, 3) Verify budget completion scaling maintains relative layer distributions.

## Open Questions the Paper Calls Out
None

## Limitations
- Sequential group-wise optimization may converge to local optima rather than globally optimal cache distributions
- Method's effectiveness depends on the base compression technique (SnapKV), with results potentially varying across different compression strategies
- Small population size and unspecified number of CMA-ES iterations per group raise concerns about convergence reliability and reproducibility

## Confidence

**High confidence**: General evolutionary optimization framework is sound and well-documented; sequential group-wise optimization approach is clearly specified.

**Medium confidence**: Performance improvements are likely real but may be sensitive to hyperparameter choices and base compression method; generalization to models beyond tested ones remains unverified.

**Low confidence**: Exact budget extrapolation method for scaling beyond c=128 is unclear; sensitivity to random seeds and optimization stability are not thoroughly characterized.

## Next Checks

1. **Convergence analysis**: Run CMA-ES optimization for 5+ random seeds and track fitness evolution across iterations to verify stability and identify convergence patterns.

2. **Budget completion validation**: Implement and test the exact budget extrapolation method used to scale c=128 results to larger budgets (256, 512, 1024, 2048).

3. **Base method sensitivity**: Repeat the entire EvoKV pipeline using alternative base compression methods (e.g., LAVa, StreamingLLM) to confirm consistent benefits across different compression strategies.