---
ver: rpa2
title: Graph-based Event Log Repair
arxiv_id: '2508.05145'
source_url: https://arxiv.org/abs/2508.05145
tags:
- event
- attributes
- missing
- graph
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of repairing event logs with missing
  information in process mining. The authors introduce SANAGRAPH, a Heterogeneous
  Graph Neural Network (HGNN) model that encodes traces as heterogeneous graphs and
  reconstructs missing event attributes by treating the problem as a node classification
  task.
---

# Graph-based Event Log Repair

## Quick Facts
- arXiv ID: 2508.05145
- Source URL: https://arxiv.org/abs/2508.05145
- Reference count: 28
- Primary result: SANAGRAPH outperforms autoencoder in activity reconstruction (75.7% vs 19.48% accuracy on ODD masking) and matches MAE for timestamp repair

## Executive Summary
This paper addresses the problem of repairing event logs with missing information in process mining. The authors introduce SANAGRAPH, a Heterogeneous Graph Neural Network (HGNN) model that encodes traces as heterogeneous graphs and reconstructs missing event attributes by treating the problem as a node classification task. The model leverages SAGEConv operators and is trained end-to-end using a combination of CrossEntropy and L1 losses.

The evaluation compares SANAGRAPH against a state-of-the-art autoencoder approach on two synthetic and four real-world event logs using four masking strategies (ODD, EVEN, WINDOW, RANDOM). Results show that SANAGRAPH significantly outperforms the autoencoder in reconstructing missing activity labels, achieving up to 75.7% accuracy versus 19.48% for the autoencoder on the ODD masking experiment. For timestamp reconstruction, SANAGRAPH achieves comparable Mean Absolute Error (MAE) to the autoencoder, with differences typically below 0.2 seconds. When considering all event attributes (not just activities and timestamps), SANAGRAPH maintains high accuracy (>80% for most categorical attributes) and only slightly degrades activity/timestamp reconstruction performance.

## Method Summary
SANAGRAPH encodes traces as heterogeneous graphs where each event attribute becomes a distinct node type, with sequential edges linking same-type nodes and activity nodes connecting to all other attribute nodes within the same event. Missing attributes are marked as "empty" nodes and reconstructed using SAGEConv operators that aggregate neighborhood information. The model uses separate linear heads for each attribute type (softmax for categorical, raw output for numerical) and is trained end-to-end with a combined CrossEntropy and L1 loss. The approach frames log repair as a node classification task, allowing the GNN to leverage contextual information from neighboring events and attributes to fill missing values.

## Key Results
- SANAGRAPH achieves 75.7% activity reconstruction accuracy versus 19.48% for the autoencoder on ODD masking experiment
- Timestamp reconstruction MAE is comparable between SANAGRAPH and autoencoder (differences typically <0.2 seconds)
- Including all attributes (FULL version) maintains or slightly improves activity reconstruction compared to only using activity/timestamp (AT version)
- Performance degrades when contiguous missing sequences exceed twice the number of convolutional layers

## Why This Works (Mechanism)

### Mechanism 1: Heterogeneous Graph Encoding Preserves Attribute-Specific Semantics
- **Claim:** Encoding traces as heterogeneous graphs, where each event attribute type becomes a distinct node type, allows the model to capture attribute-specific patterns and inter-attribute dependencies more naturally than homogeneous or sequential encodings.
- **Mechanism:** The paper transforms each event's attributes into separate nodes (e.g., Activity node, Timestamp node, Resource node) connected by edges. Same-type nodes link sequentially (Activity_i → Activity_{i+1}), while activity nodes also connect to all other attribute nodes within the same event. The heterogeneous structure enables the GNN to learn specialized parameters for each edge type during message passing.
- **Core assumption:** Treating different attributes as distinct node types improves reconstruction compared to concatenating them into a single feature vector, as this preserves semantic distinctions and allows information to flow differently based on attribute relationships.

### Mechanism 2: Node Classification via Neighborhood Aggregation for Missing Value Imputation
- **Claim:** Framing repair as node classification, using SAGEConv operators to aggregate neighborhood information into empty nodes, enables reconstruction of missing attributes by propagating contextual signals from known neighbors.
- **Mechanism:** Spatial-based graph convolution (SAGEConv) updates each node's representation by combining its own features with the mean of its neighbors' features. For nodes marked as empty (missing), this aggregation brings in information from surrounding known nodes. After multiple convolutional layers, the enriched empty node representation is passed to an attribute-specific linear head for classification or regression.
- **Core assumption:** Missing attribute values can be inferred from neighboring nodes (in time and across attributes) within a limited receptive field. The receptive field—controlled by the number of layers—is assumed sufficient to capture relevant dependencies for most missing nodes.

### Mechanism 3: Multi-Task Loss Balances Categorical and Numerical Reconstruction
- **Claim:** Combining CrossEntropy loss for categorical attributes with L1 (MAE) loss for numerical attributes in a single end-to-end objective enables simultaneous reconstruction of all event attributes without requiring separate models.
- **Mechanism:** The total loss is a sum of CrossEntropy losses over all categorical attributes and L1 losses over all numerical ones. This forces the shared GNN backbone to learn representations useful for all attribute types, while attribute-specific linear heads specialize for each output.
- **Core assumption:** A single shared graph representation can support accurate reconstruction across diverse attribute types without catastrophic interference or domination by one attribute's loss scale.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) and Message Passing**
  - **Why needed here:** SANAGRAPH is built on the GNN paradigm. Understanding how nodes exchange and aggregate messages is essential to grasp why the model can reconstruct missing values from neighbors and why performance is bounded by layer count.
  - **Quick check question:** In a 2-layer GNN with bidirectional edges, from how many hops away can a node receive information?

- **Concept: Heterogeneous Graphs and Typed Edges**
  - **Why needed here:** The core architectural choice is to represent traces as heterogeneous graphs. Knowing why different node/edge types matter—and that they require separate convolution operators—is key to understanding the model's expressivity.
  - **Quick check question:** What is the difference between a homogeneous and a heterogeneous graph, and why might heterogeneous be better for multi-attribute event data?

- **Concept: Node Classification vs. Graph Classification**
  - **Why needed here:** The paper frames repair as a node-level task (classifying or regressing values for individual nodes), not a graph-level task. This determines how outputs are produced and how loss is computed.
  - **Quick check question:** In node classification, where is the prediction head applied—per node, per edge, or per graph?

## Architecture Onboarding

- **Component map:**
  1. Trace-to-Graph Encoder: Converts each trace into a heterogeneous graph. Each event attribute becomes a typed node. Sequential edges link same-type nodes; activity nodes connect to all other attribute nodes at same event.
  2. Node Feature Initialization: Categorical attributes → one-hot encoding (plus MISSING VALUE class). Numerical attributes → log-normalization (-1 for missing).
  3. Heterogeneous GNN Backbone: Multiple SAGEConv layers, with separate learnable parameters per edge type. Aggregates neighbor information to update node embeddings.
  4. Attribute-Specific Heads: One linear layer per attribute type. Categorical heads → softmax; numerical heads → raw output.
  5. Multi-Task Loss: Sum of CrossEntropy (categorical) and L1 (numerical) losses for all target attributes.

- **Critical path:**
  1. Encode trace → heterogeneous graph (nodes, edges, types, masks for empty nodes).
  2. Initialize node features (one-hot or log-normalized; mark missing).
  3. Apply K SAGEConv layers to propagate information across the graph.
  4. Extract embeddings for masked (empty) nodes.
  5. Pass each through its attribute-specific linear head.
  6. Compute combined loss and backpropagate.

- **Design tradeoffs:**
  - **Number of layers:** More layers increase receptive field (helpful for long missing sequences) but slow training and may hit diminishing returns. Paper shows 4 layers improve RANDOM masking over 2 layers.
  - **SAGEConv vs. GAT:** SAGEConv is chosen because attention mechanisms struggle with many empty nodes. This trades potential expressiveness for stability.
  - **Multi-task vs. separate models:** Multi-task is efficient but risks interference. Results suggest minimal degradation when adding attributes.
  - **Training masking strategy:** Training on all four masks (ODD, EVEN, WINDOW, RANDOM) jointly improves generalization but may underfit specific patterns.

- **Failure signatures:**
  - **Long missing sequences:** If a contiguous block of missing events exceeds 2× the number of layers, middle nodes receive no useful information → near-random predictions.
  - **RANDOM masking underperformance vs. autoencoder:** Due to receptive field limits; autoencoder's global encoding is less sensitive to masking pattern.
  - **Timestamp MAE spikes on some logs:** SANAGRAPH (AT) loses to autoencoder on BPI12, BPI13, BPI20RFP for timestamps, indicating numerical reconstruction is not universally superior.

- **First 3 experiments:**
  1. **Reproduce ODD/EVEN masking on Small Log:** Verify SANAGRAPH (AT) activity accuracy vs. autoencoder (target ~75–88% vs. 11–37%). Confirm 60/20/20 split and hyperparameters.
  2. **Vary convolutional layers on RANDOM masking:** Test 2, 4, 6 layers on Small Log with RANDOM masking. Expect activity accuracy to increase and timestamp MAE to decrease as layers grow, up to a threshold.
  3. **Compare FULL vs. AT on a real log:** Run SANAGRAPH with all attributes vs. only activity+timestamp on BPI12 or SP2020. Measure activity accuracy and timestamp MAE deltas to confirm that adding attributes does not harm primary task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the inclusion of the number of convolutional layers in the hyperparameter search, or the use of adaptive network depth, mitigate the performance degradation caused by sequences of missing events that exceed the model's receptive field?
- **Basis in paper:** [explicit] Section VI states, "We leave the study of this phenomenon, as well as the possibility to include the number of layers in the hyperparameter search, for future work."
- **Why unresolved:** The current fixed-depth model fails to propagate information when a sequence of empty nodes is longer than twice the number of convolutional layers, limiting reconstruction capabilities in scenarios like the RANDOM masking experiment.
- **What evidence would resolve it:** A comparative study on logs with varying lengths of consecutive missing events, evaluating a model where layer depth is dynamically adjusted or optimized during training.

### Open Question 2
- **Question:** How does enriching the heterogeneous graph encoding with global features extracted from the event log impact the reconstruction accuracy of SANAGRAPH?
- **Basis in paper:** [explicit] Section VII lists "a richer encoding of the graph in input, by leveraging global features that can be extracted from the event log" as a future direction.
- **Why unresolved:** The current implementation relies primarily on local node features and connectivity, potentially missing broader contextual patterns found in global log statistics that could aid in repairing sparse traces.
- **What evidence would resolve it:** An ablation study comparing the current model against a version utilizing graph-level attributes (e.g., process-wide statistics) as additional node or edge features during training.

### Open Question 3
- **Question:** What specific explainability mechanisms can be integrated into the model to provide justifications for the reconstructed trace values?
- **Basis in paper:** [explicit] Section VII notes, "Moreover, we plan to provide some form of explanation together with the reconstructed trace."
- **Why unresolved:** While the graph structure is semantically rich, the neural network acts as a black box, offering no justification for why a specific activity or timestamp was chosen to fill a missing event.
- **What evidence would resolve it:** The implementation of an explainability layer (e.g., GNNExplainer) that outputs attention weights or feature importance scores indicating which neighboring events influenced the repair.

## Limitations

- Evaluation based on only 6 event logs (2 synthetic, 4 real-world), limiting generalizability
- Masking strategies may not reflect realistic missing data patterns in real-world logs
- Performance degrades significantly when contiguous missing sequences exceed twice the number of convolutional layers
- Timestamp reconstruction MAE differences between SANAGRAPH and autoencoder are often small in absolute terms

## Confidence

- **High confidence:** SANAGRAPH's superior performance in reconstructing missing activity labels compared to the autoencoder baseline across multiple masking strategies and logs
- **Medium confidence:** The claim that heterogeneous graph encoding is the key driver of performance, given limited ablation studies comparing homogeneous vs. heterogeneous encoding directly
- **Medium confidence:** The effectiveness of the multi-task loss in maintaining or improving primary task performance when adding more attributes, based on Table V comparisons between AT and FULL versions
- **Medium confidence:** The generalizability of results across real-world logs, given the small sample size and limited diversity of logs used in evaluation

## Next Checks

1. **Receptive Field Experiment:** Systematically vary the number of SAGEConv layers (2, 4, 6, 8) and measure activity accuracy on RANDOM masking for Small Log. Plot accuracy vs. layers to quantify the receptive field limit and identify the point of diminishing returns.

2. **Ablation on Graph Structure:** Create a homogeneous version of SANAGRAPH where all attributes are concatenated into a single node type, removing attribute-specific semantics. Compare activity accuracy on ODD/EVEN masking to the original heterogeneous model to isolate the impact of heterogeneous encoding.

3. **Stress Test on Long Missing Sequences:** Generate synthetic traces with controlled long missing blocks (e.g., 5, 10, 15 consecutive missing events) and evaluate SANAGRAPH's ability to reconstruct activities. Compare against a sliding window baseline to assess performance degradation and identify the breaking point.