---
ver: rpa2
title: 'Rewarding the Journey, Not Just the Destination: A Composite Path and Answer
  Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning'
arxiv_id: '2510.17923'
source_url: https://arxiv.org/abs/2510.17923
tags:
- arxiv
- reasoning
- reward
- learning
- compass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the scalability challenge in reinforcement
  learning for large language models (LLMs) by proposing a method to enable self-improvement
  on unlabeled data without external supervision. The core contribution is COMPASS,
  a novel reward mechanism that combines two complementary components: the Dual-Calibration
  Answer Reward (DCAR), which establishes trustworthy pseudo-labels through confidence
  and credibility calibration, and the Decisive Path Reward (DPR), which optimizes
  reasoning process quality beyond outcome supervision.'
---

# Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.17923
- **Source URL**: https://arxiv.org/abs/2510.17923
- **Reference count**: 5
- **Primary result**: COMPASS achieves up to 15.8% improvement on GPQA and 12.3% on AIME through path-based reinforcement learning

## Executive Summary
This paper addresses the scalability challenge in reinforcement learning for large language models (LLMs) by proposing COMPASS, a novel reward mechanism that enables self-improvement on unlabeled data without external supervision. The method combines two complementary components: the Dual-Calibration Answer Reward (DCAR), which establishes trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which optimizes reasoning process quality beyond outcome supervision. Extensive experiments demonstrate COMPASS's effectiveness across diverse reasoning tasks (AIME, AMC, MATH, GPQA) and model architectures, achieving significant performance gains over baseline methods like TTRL.

## Method Summary
COMPASS introduces a composite reward mechanism that evaluates both the reasoning path and final answer quality during test-time reinforcement learning. The Dual-Calibration Answer Reward (DCAR) refines consensus answers by weighting responses based on their confidence scores and assessing the credibility of the consensus relative to the most confident response. The Decisive Path Reward (DPR) provides dense, step-wise rewards that encourage decisive actions at high-uncertainty reasoning junctures through an entropy-weighted mechanism. This approach enables LLMs to improve their reasoning capabilities without requiring labeled training data, addressing a key bottleneck in reinforcement learning scalability.

## Key Results
- COMPASS achieves up to 15.8% improvement on GPQA and 12.3% on AIME compared to baseline methods
- The method shows consistent scaling behavior with model size, from 1.5B to 32B parameters
- Performance gains are demonstrated across diverse reasoning tasks including AIME, AMC, MATH, and GPQA

## Why This Works (Mechanism)
The effectiveness of COMPASS stems from its dual focus on both the reasoning path and final answer quality. DCAR addresses the challenge of unreliable pseudo-labels in self-supervised learning by incorporating confidence calibration, ensuring that consensus answers are weighted appropriately based on the reliability of individual responses. DPR tackles the sparsity of reward signals in traditional RL by providing dense feedback at each reasoning step, particularly emphasizing critical decision points where uncertainty is highest. This combination allows the model to learn from both the quality of intermediate reasoning steps and the correctness of final answers, creating a more comprehensive learning signal than outcome-only supervision.

## Foundational Learning
- **Test-Time Reinforcement Learning**: Needed because traditional supervised learning doesn't capture the dynamic reasoning process; quick check - can the model improve during inference without retraining
- **Confidence Calibration**: Required to establish trustworthy pseudo-labels in self-supervised settings; quick check - does confidence correlate with actual correctness
- **Entropy-Weighted Rewards**: Essential for identifying critical reasoning junctures; quick check - are high-entropy steps actually where the model makes important decisions
- **Consensus-Based Evaluation**: Necessary for establishing ground truth without external labels; quick check - does consensus accuracy correlate with individual model performance
- **Path-Based vs Outcome-Based Rewards**: Critical distinction for dense vs sparse reward signals; quick check - does path-based reward lead to faster convergence

## Architecture Onboarding

**Component Map**: Input Problem → DPR (Path Scoring) → DCAR (Answer Scoring) → Composite Reward → LLM Policy Update

**Critical Path**: Problem → Reasoning Steps → DPR Evaluation → Answer Generation → DCAR Evaluation → Reward Computation → Policy Gradient Update

**Design Tradeoffs**: Dense path rewards provide better learning signals but increase computational overhead; confidence-based weighting improves pseudo-label quality but may introduce bias toward overconfident responses; consensus mechanisms require multiple model samples but enable self-supervision

**Failure Signatures**: Poor performance when confidence calibration fails (overconfident wrong answers dominate); training instability when DPR entropy thresholds are poorly tuned; limited generalization when path rewards don't align with actual reasoning quality

**First Experiments**: 1) Ablation study comparing DCAR vs DPR components individually; 2) Sensitivity analysis of entropy thresholds in DPR; 3) Cross-task transfer validation to test generalization

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Performance gains are primarily evaluated on mathematical and reasoning tasks, raising questions about generalization to other domains
- The method shows significant improvements but absolute performance on harder problems remains relatively low (e.g., Qwen2.5-Math-1.5B improves from 11.8% to 23.5% on GPQA-hard)
- Scalability claims are based on models up to 32B parameters, without testing whether gains continue with frontier models

## Confidence
- **High Confidence**: The core architectural contributions of COMPASS (DCAR and DPR mechanisms) are technically sound and well-motivated
- **Medium Confidence**: The claim that COMPASS achieves "significant performance gains" is supported by experimental results, though magnitude varies across tasks
- **Medium Confidence**: The analysis of training dynamics provides useful insights, but causal relationships could be more clearly established

## Next Checks
1. **Cross-Domain Generalization**: Evaluate COMPASS on non-mathematical reasoning tasks such as commonsense reasoning, multi-step planning, or code generation to assess whether the path and answer self-scoring mechanism generalizes beyond quantitative reasoning problems.

2. **Ablation Studies on Calibration Components**: Conduct systematic ablation studies to quantify the individual contributions of confidence weighting versus credibility assessment in DCAR, and the impact of different entropy thresholds in DPR on both performance and training stability.

3. **Scaling Analysis with Frontier Models**: Test COMPASS with state-of-the-art models (e.g., GPT-4 class or beyond 32B parameters) to validate whether the reported scaling behavior continues and to measure computational overhead relative to performance gains at larger scales.