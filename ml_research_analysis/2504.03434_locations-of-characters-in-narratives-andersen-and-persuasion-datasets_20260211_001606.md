---
ver: rpa2
title: 'Locations of Characters in Narratives: Andersen and Persuasion Datasets'
arxiv_id: '2504.03434'
source_url: https://arxiv.org/abs/2504.03434
tags:
- character
- andersen
- dataset
- context
- location
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new datasets, Andersen and Persuasion,
  for evaluating AI models' ability to understand the spatial relationships between
  characters and locations in narratives. The Andersen dataset consists of 249 annotations
  from fifteen children's stories by Hans Christian Andersen, while the Persuasion
  dataset contains 264 annotations from Jane Austen's novel "Persuasion." Both datasets
  were manually annotated to map characters to their respective locations within the
  narrative.
---

# Locations of Characters in Narratives: Andersen and Persuasion Datasets

## Quick Facts
- **arXiv ID:** 2504.03434
- **Source URL:** https://arxiv.org/abs/2504.03434
- **Reference count:** 0
- **Primary result:** LLMs achieve only 61.85% (Andersen) and 56.06% (Persuasion) accuracy on character location prediction, marginally outperforming simple baselines

## Executive Summary
This paper introduces two novel datasets (Andersen and Persuasion) for evaluating AI models' ability to track character locations in narratives. The datasets contain 249 and 264 manually annotated character-location mappings from Hans Christian Andersen's fairy tales and Jane Austen's "Persuasion," respectively. The authors evaluate five large language models on this task using 23 different prompt templates, finding that even the best-performing models only slightly outperform simple distance-based heuristics. The study reveals that current LLMs struggle with implicit spatial reasoning, are vulnerable to distraction sentences, and show inconsistent performance across prompt templates.

## Method Summary
The authors created two annotated datasets by manually mapping characters to their locations in narrative passages from Andersen's fairy tales (249 annotations) and Austen's "Persuasion" (264 annotations). They evaluated five LLMs (T0++, FLAN-T5-XXL, GPT-J 6B, LLaMA-2-13B-Chat, Mistral-7B-Instruct) using 23 prompt templates that asked "Where is [character]?" based on story excerpts. Context was extracted by going backward from the character's location annotation point up to the model's maximum context length. Performance was measured using exact matching and fuzzy matching (partial_ratio â‰¥ 90 after preprocessing). A simple baseline method that assigned locations based on minimum distance to character mentions achieved 55.84% (Andersen) and 46.34% (Persuasion) accuracy.

## Key Results
- Best LLM (LLaMA-2-13B-Chat) achieved 61.85% fuzzy matching accuracy on Andersen and 56.06% on Persuasion
- Performance dropped significantly when distraction sentences were appended to prompts
- In-context learning with dataset examples degraded performance due to context truncation, while short artificial examples improved results
- A simple distance-based baseline achieved 55.84% (Andersen) and 46.34% (Persuasion), only slightly below LLM performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs rely on lexical proximity heuristics rather than robust narrative state-tracking, making them fragile against distractors and only marginally better than simple distance-based baselines.
- **Mechanism:** The model's attention mechanism disproportionately weights recent or explicitly stated location tokens, failing to maintain a persistent "world state" separate from the immediate text stream. When distractors are added, attention is hijacked by recency.
- **Core assumption:** The transformer attention mechanism in tested LLMs disproportionately weights the most recent or explicitly stated location tokens, failing to maintain a persistent "world state" separate from the immediate text stream.
- **Evidence anchors:** Best LLM only slightly outperformed simple baseline (55.84% vs 61.85%); adding distraction reduced accuracy from 57.03% to 52.21%; corpus evidence shows models without structured representations suffer from inconsistency.
- **Break condition:** When narrative requires synthesizing non-local information while ignoring nearby distractors.

### Mechanism 2
- **Claim:** In-context learning effectiveness is constrained by "context dilution," where providing long, complex examples reduces the token budget available for the actual query context, degrading performance.
- **Mechanism:** Providing k-shot examples from the dataset consumes the context window. If k examples take up 800 tokens, only 224 tokens remain for the actual story snippet, stripping necessary context for the model to answer correctly. Short artificial examples demonstrate task format without crowding out target content.
- **Core assumption:** The models tested have fixed context windows and lack effective mechanisms to prioritize query context over few-shot example context when they compete for space.
- **Evidence anchors:** Dataset examples worsened performance compared to zero-shot; authors calculated 8-shot learning with dataset examples leaves only ~114 tokens for query; artificial one-sentence examples (11 tokens) improved performance.
- **Break condition:** If model architecture supports context window significantly larger than combined length of examples and full narrative text.

### Mechanism 3
- **Claim:** Spatial understanding requires resolving implicit pronominal references and temporal sequencing, capabilities that degrade with linguistic complexity and archaic language.
- **Mechanism:** Locating a character often requires resolving "that way" or "walked there" and linking to locations through complex semantic bridges rather than single named entities. The model fails to connect character to location when the semantic bridge is more complex than simple mappings seen in pre-training data.
- **Core assumption:** LLMs primarily map explicit location entities to characters and struggle when location is a composite concept derived from multiple clues rather than a single named entity.
- **Evidence anchors:** Performance dropped from 61.85% (Andersen) to 56.06% (Persuasion); Persuasion uses "heavier language" requiring processing longer contexts; bABI uses artificial text while this task involves natural, complex narrative.
- **Break condition:** When text explicitly states location in same sentence as character without requiring resolution of pronouns or temporal ordering.

## Foundational Learning

- **Concept:** Fuzzy String Matching (Partial Ratio)
  - **Why needed here:** LLMs often hallucinate slightly or use synonyms (e.g., "home" vs. "his castle"). Exact matching would unfairly penalize these correct semantic answers, so fuzzy match (threshold 90) grades the "spirit" of the answer.
  - **Quick check question:** If ground truth is "the garden," would "in the beautiful garden" pass exact match? How does fuzzywuzzy partial ratio handle this?

- **Concept:** Annotation Consensus (Inter-Annotator Agreement)
  - **Why needed here:** Paper reports only 52.7% agreement between human annotators, revealing ground truth ambiguity. Engineer must understand model accuracy is capped not just by ability but by dataset's inherent subjectivity.
  - **Quick check question:** If two humans disagree on whether character is "by the pigsty" or "outside the door," how should we interpret a model that outputs "near the prince"?

- **Concept:** Context Window fragmentation
  - **Why needed here:** Crucial for designing prompts. Paper shows adding examples can hurt performance if it forces model to truncate actual input text it needs to reason over.
  - **Quick check question:** In 1024-token window, if you use 4 examples of 200 tokens each, how many tokens remain for actual context passage?

## Architecture Onboarding

- **Component map:** Raw text (Story/Novel) -> Annotation Engine (Manual/Heuristic indexing) -> Prompt Constructor (Extracts context + Injects Question + Examples) -> Model Interface (LLM) -> Evaluator (Pre-processing -> Matcher)
- **Critical path:** Definition of `char_no` (index where character's location is certain) is most critical data structure. It dictates exactly where context window is cut off for prompt. Wrong indexing means model is asked about point in time it hasn't "seen" yet or has incomplete context for.
- **Design tradeoffs:**
  - **Dataset Source:** Andersen (Simple/Explicit) vs. Persuasion (Complex/Implicit). *Tradeoff:* Volume vs. Difficulty.
  - **Prompting Strategy:** Zero-shot (preserves context length) vs. Few-shot (demonstrates task format). *Tradeoff:* Guidance vs. Information Dilution.
  - **Evaluation:** Exact Match (strict/unfair) vs. Fuzzy Match (generous/noisy).
- **Failure signatures:**
  - **The "John" Effect:** Accuracy drops significantly when distraction sentence is appended, indicating model attends to recency bias rather than deep semantic logic.
  - **The "Unanswerable" Bias:** Some prompt templates allowing "unanswerable" might trigger false negatives if model is uncertain.
  - **Parameter Count Sensitivity:** Smaller models (GPT-J 6B, Mistral 7B) performed significantly worse than 11B+ models, suggesting parameter threshold exists for this spatial reasoning task.
- **First 3 experiments:**
  1. **Baseline Replication:** Implement distance-based baseline algorithm (Section 5) on sample story to verify "floor" for performance (~55%).
  2. **Distraction Injection:** Take high-performing prompt setup and append specific distractor sentence "John went into the kitchen" to context. Measure drop in Fuzzy Match accuracy to quantify model fragility.
  3. **Context-Length ICL Test:** Run exact same In-Context Learning experiment (Section 8) comparing "Dataset Examples" vs. "Artificial Examples" to verify long examples degrade performance due to context truncation while short examples improve it.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does reduction of context length for target passage in k-shot prompting cause observed performance degradation when using dataset-specific examples?
- **Basis:** Authors speculate in Section 8 that context length difference causes dataset examples not to increase accuracy, but need further experiments to validate this hypothesis.
- **Why unresolved:** Experiment compared dataset examples (long context) against artificial examples (short context) but did not control for length of target passage itself, leaving causality of performance drop ambiguous.
- **What evidence would resolve it:** Experiment where token length of target passage is kept constant while varying source of in-context examples.

### Open Question 2
- **Question:** To what extent does moderate inter-annotator agreement (52.7%) reflect inherent ambiguity in task that limits upper bound of model performance?
- **Basis:** Section 3.2 reports only 52.7% agreement between human annotators, suggesting task is subjective, yet paper does not analyze if model failures correlate with human disagreement.
- **Why unresolved:** Unclear if LLMs' errors are purely reasoning failures or if they struggle with same subjectivity that caused human annotators to disagree.
- **What evidence would resolve it:** Comparison of model predictions against specific annotation instances where human annotators disagreed.

### Open Question 3
- **Question:** Can LLMs be optimized to significantly outperform simple distance-based heuristic baseline on this task?
- **Basis:** Section 6 notes best LLM accuracy (61.85%) was only marginally better than non-machine learning baseline (55.84%), indicating current models fail to effectively leverage semantic narrative understanding over spatial proximity heuristics.
- **Why unresolved:** Paper only evaluates standard prompting strategies; does not explore if gap between baseline and models can be widened through architectural changes or more complex reasoning techniques.
- **What evidence would resolve it:** Applying advanced reasoning methods (e.g., Chain-of-Thought) or fine-tuning to determine if semantic understanding can decisively beat distance-based baseline.

## Limitations

- **Dataset quality issues:** Low inter-annotator agreement (52.7%) indicates substantial subjectivity in ground truth labels, creating fundamental ceiling for model performance evaluation.
- **Implementation underspecification:** Critical details like inference hyperparameters, model versions, framework implementations, and random seeds are not documented.
- **Limited model scope:** Only five LLMs from 2022-2023 are tested without exploring larger frontier models or alternative architectures.

## Confidence

**High Confidence (80-100%)**
- LLMs perform only marginally better than simple distance-based baselines on both datasets
- Adding distraction sentences significantly reduces model accuracy
- In-context learning with dataset examples degrades performance due to context truncation

**Medium Confidence (50-80%)**
- Performance gap between Andersen and Persuasion reflects true difficulty differences
- 11B+ parameter models show threshold performance for this spatial reasoning task
- Template 1 generally provides best performance across models and datasets

**Low Confidence (0-50%)**
- Specific mechanisms by which attention weights location tokens over narrative context
- Whether observed performance represents fundamental limitations or can be overcome with architectural modifications
- Generalizability to other narrative genres or languages

## Next Checks

**Validation Check 1: Inter-Annotator Agreement Analysis**
Re-analyze annotation data to compute per-story and per-character agreement rates. Test whether disagreement correlates with narrative complexity, implicit location references, or archaic language usage. This will determine whether 52.7% overall agreement represents uniform uncertainty or systematic difficulty with certain narrative types.

**Validation Check 2: Ablation Study on Context Window Usage**
Systematically vary context window size and measure impact on performance with and without in-context examples. Specifically, test whether degradation from dataset examples persists when model has sufficient context length to accommodate both examples and full passage. This will validate whether context dilution is true mechanism behind poor ICL performance.

**Validation Check 3: Expanded Model and Template Evaluation**
Test additional contemporary LLMs (including larger frontier models) and additional prompt templates beyond the 23 tested. Include models with different architectural approaches (e.g., retrieval-augmented, structured reasoning) to determine whether observed limitations are fundamental or can be overcome with current technology.