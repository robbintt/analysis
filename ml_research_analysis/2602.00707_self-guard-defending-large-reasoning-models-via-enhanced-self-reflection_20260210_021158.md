---
ver: rpa2
title: 'Self-Guard: Defending Large Reasoning Models via enhanced self-reflection'
arxiv_id: '2602.00707'
source_url: https://arxiv.org/abs/2602.00707
tags:
- safety
- self-guard
- reasoning
- awareness
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Self-Guard, a lightweight defense framework
  for Large Reasoning Models (LRMs) that addresses the awareness-compliance gap in
  model safety. The framework operates in two stages: safety-oriented prompting to
  activate the model''s latent safety awareness, followed by safety activation steering
  that amplifies directional shifts in the hidden state space to reinforce safety
  compliance.'
---

# Self-Guard: Defending Large Reasoning Models via enhanced self-reflection

## Quick Facts
- arXiv ID: 2602.00707
- Source URL: https://arxiv.org/abs/2602.00707
- Reference count: 40
- Primary result: Lightweight framework bridging awareness-compliance gap in LRM safety with minimal utility loss

## Executive Summary
Self-Guard introduces a two-stage defense framework for Large Reasoning Models that addresses the critical gap between safety awareness and compliance. The framework employs safety-oriented prompting to activate latent safety awareness, followed by safety activation steering that amplifies directional shifts in the hidden state space to reinforce safety compliance. This approach achieves competitive or superior safety performance across harmful and jailbreak benchmarks while maintaining high precision without over-refusal. The method demonstrates strong generalization across different model scales and unseen risks while preserving general reasoning utility with minimal degradation, offering a cost-efficient solution for LRM safety alignment.

## Method Summary
Self-Guard operates through a two-stage process: first, safety-oriented prompting activates the model's latent safety awareness by prompting the model to consider potential harms in its responses. Second, safety activation steering amplifies directional shifts in the hidden state space to reinforce safety compliance. This approach differs from traditional steering-based methods by focusing on enhancing the model's self-reflection capabilities rather than imposing external constraints. The framework is designed to be lightweight and adaptable across different LRM scales while maintaining general reasoning capabilities. The method specifically targets the awareness-compliance gap where models recognize safety principles but fail to consistently apply them in practice.

## Key Results
- Effectively bridges the awareness-compliance gap in LRM safety
- Achieves competitive or superior safety performance across harmful and jailbreak benchmarks
- Maintains high defense precision without over-refusal while preserving general reasoning utility

## Why This Works (Mechanism)
The framework works by first activating latent safety awareness through targeted prompting that encourages the model to consider potential harms in its responses. This initial stage ensures the model recognizes safety principles relevant to the task. The second stage employs safety activation steering that operates on the hidden state space, amplifying directional shifts that reinforce safety-compliant behavior. This two-stage approach allows the model to maintain its reasoning capabilities while strengthening its safety compliance mechanisms, creating a more robust defense against harmful outputs and jailbreak attempts.

## Foundational Learning
- **Safety Awareness vs Compliance Gap**: The difference between a model's ability to recognize safety principles and its consistent application of them; critical for understanding why models fail despite having safety knowledge
- **Hidden State Space Manipulation**: The technique of modifying internal model representations to influence output behavior; essential for implementing steering-based defenses
- **Self-Reflection in LLMs**: The model's ability to analyze and modify its own reasoning process; necessary for creating autonomous safety mechanisms
- **Jailbreak Resistance**: The model's ability to maintain safety constraints under adversarial prompting; fundamental for real-world deployment
- **Multi-Stage Defense Frameworks**: Sequential safety mechanisms that build upon each other; important for creating robust defense systems
- **Utility Preservation Metrics**: Methods for measuring reasoning capability retention during safety interventions; critical for practical deployment

Quick check: Verify each concept by examining how Self-Guard specifically addresses it through its two-stage approach.

## Architecture Onboarding

**Component Map:** Safety-oriented prompting -> Safety activation steering -> Output filtering

**Critical Path:** Prompt injection → Safety awareness activation → Hidden state modification → Safety-compliant output generation

**Design Tradeoffs:** The framework prioritizes minimal utility degradation over maximum safety enforcement, accepting some residual risk to maintain reasoning capabilities. This contrasts with steering-based approaches that achieve higher safety at the cost of catastrophic utility loss.

**Failure Signatures:** Potential over-refusal in ambiguous contexts, incomplete safety activation in complex scenarios, and reduced effectiveness against adaptive jailbreak techniques that bypass initial safety awareness.

**First Experiments:**
1. Baseline safety evaluation without Self-Guard intervention across standard safety benchmarks
2. Safety-oriented prompting alone to measure awareness activation effectiveness
3. Combined two-stage implementation testing against known jailbreak patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on English-language safety benchmarks, leaving multilingual safety performance unexplored
- Effectiveness against adaptive, state-of-the-art jailbreak techniques remains uncertain
- Limited evaluation of general reasoning capabilities to specific benchmark tasks

## Confidence

**High Confidence:** The core claim that Self-Guard effectively bridges the awareness-compliance gap is well-supported by experimental results across multiple safety benchmarks.

**Medium Confidence:** The assertion of minimal utility degradation is supported but limited by the scope of reasoning benchmarks tested.

**Low Confidence:** The claim about cost-efficiency relative to other steering-based approaches lacks comprehensive benchmarking against all relevant defense mechanisms.

## Next Checks
1. Conduct multilingual safety evaluation across at least 5 languages to verify cross-lingual safety alignment effectiveness
2. Test against adaptive, state-of-the-art jailbreak techniques including chain-of-thought jailbreaks and gradient-based adversarial attacks
3. Perform comprehensive computational overhead analysis including latency measurements and memory usage across different deployment scales and hardware configurations