---
ver: rpa2
title: 'Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis'
arxiv_id: '2509.08483'
source_url: https://arxiv.org/abs/2509.08483
tags:
- proof
- where
- lemma
- page
- times
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a detailed theoretical analysis of gradient
  descent with Polyak heavy-ball momentum (HB), a widely used optimization algorithm
  in machine learning. The key contribution is establishing that on an exponentially
  attractive invariant manifold, HB is exactly equivalent to plain gradient descent
  with a modified loss function, provided the step size is sufficiently small.
---

# Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis

## Quick Facts
- arXiv ID: 2509.08483
- Source URL: https://arxiv.org/abs/2509.08483
- Reference count: 40
- One-line primary result: Proves Polyak Heavy-Ball momentum is exactly equivalent to gradient descent on a modified loss on an invariant manifold for small step sizes.

## Executive Summary
This paper provides a detailed theoretical analysis of gradient descent with Polyak heavy-ball momentum (HB), a widely used optimization algorithm in machine learning. The key contribution is establishing that on an exponentially attractive invariant manifold, HB is exactly equivalent to plain gradient descent with a modified loss function, provided the step size is sufficiently small. The authors prove a global approximation bound of O(h^R) for any finite order R≥2, showing that HB can be approximated by a memoryless iteration with arbitrary precision.

A significant finding is the identification of implicit regularization terms introduced by the momentum. For example, in the full-batch case, the modified loss includes rescaled squared gradient norm and directional derivative terms. In the mini-batch case, additional regularization from stochasticity is identified. The analysis also reveals a rich family of polynomials in the momentum parameter β, including Eulerian and Narayana polynomials, hidden within the algorithm's structure.

## Method Summary
The method centers on proving that HB momentum dynamics converge to an exponentially attractive invariant manifold where the algorithm is exactly equivalent to gradient descent on a modified loss. The approach involves showing that for small step sizes, the velocity component can be expressed as a function of position alone, effectively removing the algorithm's memory. This transformation allows HB to be approximated by a memoryless iteration with arbitrary precision O(h^R) using Taylor expansions and rooted tree combinatorics. The framework also derives continuous modified equations of arbitrary approximation order and introduces the concept of principal flow to capture the algorithm's dynamics.

## Key Results
- Proves HB is exactly equivalent to GD on a modified loss on an exponentially attractive invariant manifold for small step sizes
- Establishes global approximation bound O(h^R) for any finite order R≥2, enabling arbitrary precision memoryless approximation
- Identifies implicit regularization terms including squared gradient norm and gradient covariance terms
- Reveals Eulerian and Narayana polynomials hidden in the algorithm's structure
- Derives continuous modified equations of arbitrary approximation order

## Why This Works (Mechanism)

### Mechanism 1: Invariant Manifold Projection
- Claim: Polyak Heavy-Ball (HB) momentum dynamics converge to an exponentially attractive invariant manifold where the algorithm is exactly equivalent to Gradient Descent (GD) on a modified loss.
- Mechanism: The algorithm maintains a velocity $v$. The authors prove that for a small step size $h$, the velocity decays exponentially to a function of the position $v \approx -(1-\beta)^{-1}\nabla L(\theta) + h g_h(\theta)$. On this manifold, the "memory" of past gradients is absorbed into a correction term $G_h$ (the antiderivative of $g_h$), transforming the second-order HB iteration into a first-order GD iteration $\theta_{n+1} = \theta_n - h(1-\beta)^{-1} \nabla(L - h\beta(1-\beta)G_h)$.
- Core assumption: The step size $h$ is sufficiently small to ensure the contraction mapping (defined in Eq 12) holds; loss $L$ is $C^3$ with bounded derivatives.
- Evidence anchors:
  - [Section 2, Theorem 2.1]: Establishes $g_h$ is a gradient and the manifold is exponentially attractive.
  - [Abstract]: "prove that on an exponentially attractive invariant manifold the algorithm is exactly plain gradient descent with a modified loss."
  - [Corpus]: [Weak/Missing] Corpus neighbors focus on continuous-time approximations rather than the specific invariant manifold reduction.

### Mechanism 2: Memory Removal via Taylor Expansion
- Claim: The history-dependent HB iteration can be approximated by a memoryless iteration with arbitrary precision $O(h^R)$.
- Mechanism: The algorithm expands the history of gradients $\nabla L(\theta_{n-k})$ using Taylor series around the current point $\tilde{\theta}^{(n)}$. By summing these geometrically weighted terms (controlled by $\beta$), the dependency on past iterates $\theta_{n-k}$ is collapsed into a single memoryless update rule with coefficients $d_j^{(n)}$ (defined via rooted trees).
- Core assumption: The time horizon $T$ is finite; the error terms $O(k^R h^R)$ from Taylor expansions are controlled by the exponential decay of the momentum parameter $\beta$.
- Evidence anchors:
  - [Section 3, Theorem 3.1]: Provides the global approximation guarantee.
  - [Section 1.2]: Shows the explicit form of the memoryless approximation coefficients.
  - [Corpus]: Neighbor "First and Second Order Approximations..." aligns with the general approximation approach but lacks the arbitrary-order guarantees.

### Mechanism 3: Implicit Regularization via Modified Loss
- Claim: Momentum implicitly modifies the loss function by adding terms proportional to the squared gradient norm and, in stochastic settings, the trace of the gradient covariance.
- Mechanism: The modified loss term $G_h$ contains "implicit regularization" terms. In the full-batch case, this includes rescaled squared gradient norms $\|\nabla L\|^2$. In the mini-batch case, the expectation over permutations reveals an additional term involving the empirical covariance matrix $\Sigma$, effectively regularizing the dynamics based on gradient noise.
- Core assumption: Mini-batch indices are sampled via random permutation (without replacement).
- Evidence anchors:
  - [Section 1.2]: Explicitly derives the regularization terms $\frac{\beta}{2(1-\beta)^2}\|\nabla L\|^2$ and $\frac{\beta}{2(1-\beta)(1+\beta)}\frac{tr\Sigma}{B}$.
  - [Section 5.1, Remark 5.2]: Notes that for $R=3$, the modified equation is no longer a gradient flow.
  - [Corpus]: [Weak/Missing] Corpus does not explicitly detail the specific covariance-based regularization derived here.

## Foundational Learning

- Concept: **Invariant Manifolds in Dynamical Systems**
  - Why needed here: The core theoretical contribution relies on proving the existence of a manifold where the system dynamics simplify.
  - Quick check question: Can you explain why the condition $v = -(1-\beta)^{-1}\nabla L + hg_h$ implies the system has lost its dependence on past velocities?

- Concept: **Backward Error Analysis (Modified Equations)**
  - Why needed here: The paper uses this to find a continuous ODE that the discrete HB algorithm follows approximately.
  - Quick check question: How does finding a modified ODE help in understanding the "implicit bias" of an optimization algorithm?

- Concept: **Combinatorial Rooted Trees (Butcher Series)**
  - Why needed here: The coefficients $d_j^{(n)}$ of the memoryless iteration are characterized using sums over rooted trees, managing the complexity of high-order derivatives.
  - Quick check question: How does the symmetry coefficient $\sigma(\tau)$ of a tree relate to the multiplicity of terms in the Taylor expansion of the loss?

## Architecture Onboarding

- Component map: HB Iterator -> Manifold Projector -> Modified Loss Calculator
- Critical path: **The Fixed-Point Equation (Eq 11)**. This equation defines the mapping $T$ which must be a contraction. If this holds, $g_h$ exists and is a gradient, validating the entire "GD with modified loss" view.
- Design tradeoffs:
  - **Approximation Order $R$ vs. Complexity**: Increasing the order $R$ of the memoryless approximation improves precision ($O(h^R)$) but exponentially increases the combinatorial complexity of computing the coefficients $d_j$.
  - **Step Size $h$**: The "memoryless" property only holds for "small enough" $h$. Large $h$ invalidates the manifold approximation.
- Failure signatures:
  - **Large $h$**: The contraction mapping condition fails; the velocity $v$ does not settle onto the manifold, and the memoryless approximation diverges from true HB.
  - **Non-smooth Loss**: If $L$ lacks $C^3$ smoothness, the Taylor expansions used to remove memory are invalid.
- First 3 experiments:
  1. **Manifold Validation**: Verify Theorem 2.1 numerically by plotting the distance $||v^{(n)} - (-(1-\beta)^{-1}\nabla L(\theta^{(n)}) + hg_h(\theta^{(n)}))||$ over time to confirm exponential decay.
  2. **Trajectory Comparison**: Compare the trajectory of true HB against the "memoryless iteration" (Eq 5) for increasing orders $R=2, 3, 4$ to visualize the $O(h^R)$ convergence on a simple quadratic loss.
  3. **Regularization Effect**: Train a model using standard GD with the explicit regularization terms derived in Section 1.2 (squared gradient norm) and compare generalization performance to standard HB.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the invariant manifold and memoryless approximation framework be rigorously extended to Adam or Shampoo, which have more complex memory structures (exponential moving averages of both gradients and squared gradients)?
- Basis in paper: [explicit] The authors state their techniques "could be used to study other (ubiquitous) numerical methods with memory such as Adam [34] or Shampoo [26]" and "outline a road-map for similar analysis of other optimization algorithms."
- Why unresolved: Adam has two coupled EMA terms with different decay rates, and Shampoo has preconditioning matrices, both of which make the invariant manifold structure significantly more complex than HB's single momentum term.
- What evidence would resolve it: A theorem showing Adam's dynamics can be approximated by a modified gradient descent on an invariant manifold, with explicit forms for the modified loss and regularization terms analogous to HB.

### Open Question 2
- Question: Does the implicit regularization by stochasticity (tr Σ(θ)/B term) interact with the implicit regularization by memory (∥∇L∥² term) in a way that explains why certain mini-batch sizes and momentum combinations generalize better?
- Basis in paper: [explicit] The paper identifies two separate regularization terms for mini-batch HB but notes these insights "can be of practical importance... for proposing new ones, e.g. by introducing (or strengthening) similar regularization explicitly."
- Why unresolved: The paper quantifies each term separately but does not analyze their joint effect on generalization or how they depend on hyperparameters β, h, and B together.
- What evidence would resolve it: Empirical or theoretical analysis correlating the magnitudes of both regularization terms with test generalization across different β, h, B settings, or a new algorithm that explicitly adds these terms and shows improved generalization.

### Open Question 3
- Question: When h is not small enough for the manifold to be attractive, what determines the algorithm's dynamics, and how sharp is the threshold?
- Basis in paper: [inferred] The main theorems require "h is small enough" and Section 5.4 shows that for "large step sizes" there exists "another attractive manifold of importance" with different dynamics.
- Why unresolved: The paper characterizes the small-h regime rigorously but only briefly notes the existence of a different attractive manifold for large h, without providing approximation bounds or a complete characterization of the transition.
- What evidence would resolve it: A theorem characterizing the critical h threshold, or a modified approximation result with explicit h-dependent error bounds that remain meaningful as h approaches the stability boundary.

## Limitations
- The framework requires the loss function to be C³ with bounded derivatives, limiting applicability to non-smooth problems
- The "small enough h" condition is critical but not quantified explicitly in terms of problem parameters
- The complexity of computing higher-order coefficients d_j grows combinatorially, making high-order approximations computationally challenging

## Confidence
High confidence in the theoretical framework and main theorems based on rigorous mathematical proofs. Medium confidence in the practical applicability due to the complexity of implementing higher-order approximations. Low confidence in extending the framework to more complex algorithms like Adam without significant additional work.

## Next Checks
1. Implement the memoryless iteration for order R=2 and verify the O(h²) approximation error on a quadratic loss
2. Numerically validate the exponential attractiveness of the invariant manifold by plotting velocity convergence
3. Compute and analyze the Eulerian and Narayana polynomials revealed in the HB structure for different β values