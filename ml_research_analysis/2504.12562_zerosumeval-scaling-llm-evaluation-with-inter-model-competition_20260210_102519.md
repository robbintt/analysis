---
ver: rpa2
title: 'ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition'
arxiv_id: '2504.12562'
source_url: https://arxiv.org/abs/2504.12562
tags:
- https
- arxiv
- games
- language
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ZeroSumEval, a novel competition-based evaluation\
  \ protocol for Large Language Models (LLMs) that leverages zero-sum games to provide\
  \ dynamic, scalable, and robust benchmarks resistant to saturation. Unlike traditional\
  \ static benchmarks that often suffer from data contamination, overfitting, and\
  \ high costs, ZeroSumEval employs a diverse suite of games\u2014including Chess,\
  \ Poker, MathQuiz, Gandalf, Liar's Dice, Debate, and PyJail\u2014to evaluate various\
  \ AI capabilities such as strategic reasoning, knowledge application, and creativity."
---

# ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition

## Quick Facts
- arXiv ID: 2504.12562
- Source URL: https://arxiv.org/abs/2504.12562
- Reference count: 26
- Primary result: Novel competition-based evaluation protocol using zero-sum games to create dynamic, scalable benchmarks resistant to saturation

## Executive Summary
This paper introduces ZeroSumEval, a novel competition-based evaluation protocol for Large Language Models (LLMs) that leverages zero-sum games to provide dynamic, scalable, and robust benchmarks resistant to saturation. Unlike traditional static benchmarks that often suffer from data contamination, overfitting, and high costs, ZeroSumEval employs a diverse suite of games—including Chess, Poker, MathQuiz, Gandalf, Liar's Dice, Debate, and PyJail—to evaluate various AI capabilities such as strategic reasoning, knowledge application, and creativity. The framework uses automated verification and prompt abstraction to ensure scalable and unbiased evaluation.

Extensive experiments with over 7,000 simulations across 13 models show that while frontier models like GPT-4o and Claude-3.7-sonnet perform well in common games, they struggle with tasks requiring novel question generation, creativity, and jailbreaking. The study also highlights the variability in performance between "thinking" models and their standard counterparts, demonstrating ZeroSumEval's effectiveness in revealing nuanced model strengths and weaknesses. The code is publicly available at https://github.com/facebookresearch/ZeroSumEval.

## Method Summary
ZeroSumEval employs a competition-based evaluation framework where models compete in zero-sum games to assess capabilities like strategic reasoning, planning, and creativity. The system uses automated verification with target abstraction to generate and validate challenges without human labeling, and DSPy abstractions to reduce prompt-sensitivity artifacts. Models compete in round-robin tournaments across seven games (Chess, Poker, Liar's Dice, MathQuiz, PyJail, Gandalf, Debate) with outcomes used to compute Bradley-Terry ratings. The framework includes retry mechanisms for invalid moves and a jury-based scoring system for Debate.

## Key Results
- Frontier models (GPT-4o, Claude-3.7-sonnet) excel in common games but struggle with novel question generation and creativity tasks
- Thinking models (o3-mini-high, deepseek-r1) underperform compared to instruct models with Chain-of-Thought in most games, with Debate as the exception
- PyJail sandbox-break success rates are below 2%, indicating significant challenges in creative problem-solving
- Chain-of-Thought prompting shows mixed or negligible benefits in competitive game settings, contrary to established advantages on static benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Zero-sum competition creates self-adjusting difficulty that resists benchmark saturation. In zero-sum games, any improvement in one player directly increases challenge for opponents. As models improve, the evaluation difficulty scales automatically without human intervention—the "opponent" becomes stronger. Each game trace explores unique move trajectories, and memorization is penalized by exploitation risk.

Core assumption: Models' strategic weaknesses will be exposed when facing adversarial opponents rather than static test cases.
Evidence anchors:
- [abstract] "dynamic benchmarks that resist saturation by leveraging game-based evaluations"
- [Section 1] "Regurgitation of memorized information is penalized naturally by threat of exploitation from opposing players"
- [corpus] Related work (lmgame-Bench, Orak) confirms games test perception, memory, and planning, but ZeroSumEval uniquely uses adversarial competition for scaling.

### Mechanism 2
Automated verification with target-abstraction enables scalable challenge generation without human labeling. The Manager generates a target (e.g., random integer answer), the Generator creates a challenge, then must solve it without seeing the target. Success proves validity. This prevents target leakage while ensuring solvability. The 1/1000 random-guess probability creates strong signal for valid generation.

Core assumption: Models can verify their own generated challenges when forced to solve them without answer access.
Evidence anchors:
- [Section 2.2] Detailed verification process with target generation, challenge creation, and blind solving
- [Section 2.2] "Without access to the answer, there is a 1/1000 chance the teacher will produce the target answer by randomly guessing"
- [corpus] No direct corpus comparison; this appears novel to ZeroSumEval.

### Mechanism 3
Prompt abstraction via DSPy with feedback loops reduces prompt-sensitivity artifacts. Rather than hand-crafting detailed prompts for each model, DSPy abstracts to input/output specifications. Invalid moves trigger automatic retries with feedback (max 3-5 attempts), allowing models to self-correct format errors without human intervention or prompt engineering per model.

Core assumption: Format and instruction-following errors can be corrected through feedback, isolating capability from prompt-compliance issues.
Evidence anchors:
- [Section 2.3] "DSPy provides abstractions that allow easily swapping and modifying strategies without depending on manually-written prompt templating"
- [Section 2.3] "simulates interactivity between the models and the game environment by allowing a number of retries (with feedback)"
- [corpus] No corpus comparison found; prompt-sensitivity mitigation appears underexplored in related game benchmarks.

## Foundational Learning

- **Concept: Zero-sum game theory (Elo/Bradley-Terry rating systems)**
  - Why needed here: The entire rating system depends on understanding pairwise win-probability modeling and why BT was chosen over Elo for fixed-weight LLMs.
  - Quick check question: Why does the paper use Bradley-Terry instead of Elo for rating LLMs?

- **Concept: DSPy assertions and declarative LM programming**
  - Why needed here: Understanding how DSPy abstracts prompts and implements retry logic is essential for extending or debugging the framework.
  - Quick check question: What role do DSPy Assertions play in handling invalid model moves?

- **Concept: Sandbox escape challenges (PyJail CTF paradigm)**
  - Why needed here: PyJail games require understanding restricted Python execution and creative escape techniques—core to security evaluation games.
  - Quick check question: In PyJail, what must the defender prove before the attacker attempts to escape?

## Architecture Onboarding

- **Component map:**
  Manager -> Players -> Game Engines -> Rating Calculator -> Verification Loop

- **Critical path:** Game selection → Player assignment → Move loop (generate → validate → retry/accept) → Outcome determination → Rating update. For challenge games (MathQuiz, PyJail), verification of generated challenge precedes opponent play.

- **Design tradeoffs:**
  - Retry limits (3-5): Higher values reduce invalid-move losses but increase cost and may mask instruction-following failures
  - Game diversity vs. comparability: More games increase coverage but make cross-game rating aggregation noisier
  - Thinking models vs. CoT prompting: Native thinking doesn't consistently outperform CoT—choice is model-family dependent

- **Failure signatures:**
  - Models exhausting retries in Chess (llama-3.1-8b fails by move 10; gpt-4o reaches 50 moves)
  - MathQuiz student correct >90%: teachers failing to create challenging problems
  - PyJail <2% sandbox-break success: models cannot solve their own challenges
  - Thinking models (o3-mini-high) underperforming instruct models with CoT in most games

- **First 3 experiments:**
  1. Run round-robin tournament on 2-3 games (Chess, MathQuiz) with 4-5 models to validate rating system produces expected ordering (stronger models rank higher).
  2. Ablate retry counts (1 vs. 3 vs. 5) on a weak model to measure how much invalid-move handling masks vs. reveals capability gaps.
  3. Compare CoT vs. Predict prompting on MathQuiz and Chess to replicate the mixed-results finding and identify which games benefit from explicit reasoning.

## Open Questions the Paper Calls Out

### Open Question 1
Why does Chain-of-Thought (CoT) prompting show mixed or negligible benefits in competitive game settings, contrary to its established advantages on static benchmarks? The paper reports that CoT produced mixed results in Chess and insignificant changes in MathQuiz, but does not investigate whether game dynamics, time pressure, or opponent adaptation counteract CoT benefits.

### Open Question 2
What mechanisms enable models to create novel, challenging, and verifiable problems in games like MathQuiz and PyJail, and why do current frontier models fail at this? The paper identifies the failure but does not diagnose whether it stems from limited creativity, insufficient domain knowledge, or inability to model opponent capabilities.

### Open Question 3
Why do thinking models (e.g., o3-mini-high, deepseek-r1) underperform compared to instruct models with CoT in most games, and under what conditions might thinking models excel? The paper speculates about tradeoffs between creativity and instruction following but does not systematically test whether thinking models' extended reasoning is misaligned with competitive game dynamics.

### Open Question 4
Does performance in zero-sum competitive games correlate with real-world task performance, and can ZeroSumEval predict downstream application success? Without correlation analysis, it remains unclear whether competitive game performance is a proxy for general capability or captures a narrow skill set.

## Limitations

- PyJail sandbox-break success rates are below 2%, suggesting the challenge generation mechanism may be overly conservative
- DSPy prompt abstractions lack complete transparency in module signatures and prompt structures
- Rating system aggregation across different games with varying difficulty levels is not clearly addressed

## Confidence

**High Confidence:** The core mechanism of using zero-sum competition to create self-adjusting difficulty is well-supported by game theory principles and the experimental results. The observation that frontier models struggle with novel question generation and creativity tasks is consistently demonstrated across multiple experiments.

**Medium Confidence:** The claim that automated verification with target-abstraction enables scalable challenge generation is supported by the methodology description, but the extremely low PyJail success rates suggest potential issues with challenge difficulty calibration.

**Medium Confidence:** The assertion that DSPy abstractions reduce prompt-sensitivity artifacts is plausible given the reported retry mechanisms, but the lack of detailed prompt specifications makes independent verification difficult.

## Next Checks

1. **Challenge Generation Calibration:** Systematically vary the difficulty parameters in MathQuiz and PyJail to identify the sweet spot where models can generate valid challenges at rates between 20-80%, rather than the current <2% PyJail success rate.

2. **Cross-Game Rating Aggregation:** Develop and validate a method for combining Bradley-Terry ratings across different games, accounting for varying difficulty levels and win-rate distributions, to produce meaningful overall model rankings.

3. **Prompt Sensitivity Analysis:** Conduct controlled experiments comparing DSPy-abstracted prompts against hand-crafted prompts for Chess and Poker to quantify the actual reduction in prompt-sensitivity artifacts.