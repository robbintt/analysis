---
ver: rpa2
title: How Quantization Shapes Bias in Large Language Models
arxiv_id: '2508.18088'
source_url: https://arxiv.org/abs/2508.18088
tags:
- quantization
- bias
- gptq
- toxicity
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how quantization affects social bias in
  large language models, focusing on stereotypes, fairness, toxicity, and sentiment.
  We evaluate multiple quantization strategies (GPTQ, AWQ, SmoothQuant) across different
  bit-widths, model architectures (LLaMA vs.
---

# How Quantization Shapes Bias in Large Language Models

## Quick Facts
- **arXiv ID:** 2508.18088
- **Source URL:** https://arxiv.org/abs/2508.18088
- **Authors:** Federico Marcuzzi; Xuefei Ning; Roy Schwartz; Iryna Gurevych
- **Reference count:** 40
- **Primary result:** Quantization increases stereotypes and unfairness in generative tasks while reducing raw toxicity, with effects varying by bit-width and strategy.

## Executive Summary
This work investigates how quantization affects social bias in large language models, focusing on stereotypes, fairness, toxicity, and sentiment. We evaluate multiple quantization strategies (GPTQ, AWQ, SmoothQuant) across different bit-widths, model architectures (LLaMA vs. Qwen), and reasoning capabilities (Non-reasoning vs. Reasoning). Using 13 benchmarks and both probability- and generated text-based metrics, we find that quantization increases stereotypes and unfairness in generative tasks, while significantly reducing raw toxicity. The impact on sentiment is minor, generally shifting it toward neutral. These trends are largely consistent across demographic categories and subgroups, though their magnitude depends on the specific setting. Stronger quantization, particularly W3 and W4A8, tends to amplify bias and degrade model capabilities, with SQ showing the strongest overall impact. Reasoning models are generally less biased than non-reasoning models, but quantization affects both similarly. Overall, quantization's effect on bias is nuanced, highlighting the need for careful balancing of efficiency and ethical considerations.

## Method Summary
The study applies post-training quantization (PTQ) to various LLM architectures using three strategies: GPTQ (sequential weight quantization with error compensation), AWQ (activation-aware weight scaling), and SmoothQuant (weight-activation quantization with outlier handling). Models are quantized at different bit-widths including W8, W4, W3 for weight-only methods and W8A8, W4A8 for weight-activation methods. The evaluation uses 13 benchmarks covering stereotypes (StereoSet, RedditBias, WinoBias, BBQ), fairness (DiscrimEval, DiscrimEvalGen, DT-Fairness), toxicity/sentiment (BOLD, DT-Toxicity), and capabilities (MMLU). Both probability-based metrics (StereotypeScore, effect size via Cohen's d) and generated text-based metrics (accuracy, Toxic-BERT scores ≥0.5, VADER sentiment) are computed. Inference uses greedy decoding via vLLM with max_tokens=2000 for reasoning tasks and max_tokens=250 for toxicity tasks.

## Key Results
- Quantization increases stereotypes and unfairness in generative tasks, with stronger effects at lower bit-widths (W3, W4A8)
- Raw toxicity is significantly reduced across all quantization settings, with W3 and W4A8 showing the strongest reduction
- Probability-based bias metrics can misleadingly suggest reduced bias due to increased model uncertainty, while generated text metrics reveal actual bias amplification
- Reasoning models show lower baseline bias than non-reasoning models, but quantization affects both similarly
- SmoothQuant (SQ) has the strongest overall impact on bias dimensions compared to GPTQ and AWQ

## Why This Works (Mechanism)

### Mechanism 1: Quantization-Induced Uncertainty Masks Bias in Probability Metrics
- Claim: Probability-based bias metrics appear to show reduced bias under quantization, but this reflects increased model uncertainty rather than genuine bias mitigation.
- Mechanism: Lower bit precision → symmetric reduction in log-likelihood for both pro- and anti-stereotypical sentences → probability-based metrics converge toward neutral values → generated text-based metrics reveal actual bias amplification.
- Core assumption: Quantization reduces confidence uniformly across sentence types without preferentially targeting biased representations.
- Evidence anchors: [abstract] "quantization increases stereotypes and unfairness in generative tasks"; [section D.4] log-likelihood decreases symmetrically across categories and sentence types; [corpus] Weak/missing—no corpus papers directly address the quantization-uncertainty relationship in bias metrics.

### Mechanism 2: Toxicity Reduction via Representation Disruption, Not Output Truncation
- Claim: Quantization reduces raw toxicity by disrupting internal representations required for generating harmful content, independent of output length effects.
- Mechanism: Precision loss → selective degradation of neural pathways encoding toxic language patterns → reduced toxicity that persists even when controlling for generation length.
- Core assumption: Toxic content generation relies on higher-precision representations than neutral or benign content.
- Evidence anchors: [abstract] "while it can reduce model toxicity"; [section D.2] Figure D.1 shows original model with constrained generation lengths has higher toxicity than quantized models with comparable lengths; [corpus] Weak/missing—corpus papers do not address quantization-toxicity mechanisms.

### Mechanism 3: Bias Amplification Through Reasoning Chain Corruption
- Claim: In generative tasks requiring chain-of-thought reasoning, quantization amplifies stereotypes by corrupting early reasoning steps, causing stereotypical associations to propagate through subsequent reasoning.
- Mechanism: Precision loss → errors in initial reasoning steps (e.g., incorrect gender-occupation associations) → stereotypical biases compound through chain-of-thought → incorrect final answers that reinforce stereotypes.
- Core assumption: Reasoning models use sequential chain-of-thought where early errors cascade.
- Evidence anchors: [abstract] "quantization increases stereotypes and unfairness in generative tasks, especially under aggressive compression"; [section D.3] Generation 4 shows quantized model reasoning with incorrect gender-occupation association; [corpus] "Veracity Bias and Beyond" discusses demographic associations in reasoning but not quantization effects.

## Foundational Learning

- Concept: Post-Training Quantization (PTQ) Methods
  - Why needed here: The paper compares GPTQ (sequential weight quantization with error compensation), AWQ (activation-aware weight scaling), and SmoothQuant (weight-activation quantization with outlier handling)—each has distinct bias implications.
  - Quick check question: Why does SmoothQuant (weight-activation) have stronger effects on social dimensions than weight-only methods (GPTQ, AWQ)?

- Concept: Probability-Based vs. Generated Text-Based Bias Metrics
  - Why needed here: The paper shows these metric types can yield contradictory results—probability metrics may show reduced bias due to uncertainty while text metrics show amplified bias.
  - Quick check question: If a model's StereotypeScore moves from 73% to 68% after quantization, does this indicate reduced bias or increased uncertainty?

- Concept: Demographic Categories vs. Subgroups
  - Why needed here: The paper distinguishes category-level analysis (gender, race, religion) from subgroup-level analysis (male/female, Black/White/Asian, etc.) to detect whether quantization introduces inter-subgroup disparities.
  - Quick check question: What does it mean if quantization reduces toxicity uniformly across subgroups but increases the toxicity gap between specific subgroups?

## Architecture Onboarding

- Component map: Quantization strategies (GPTQ → AWQ → SmoothQuant) → Bit-width configurations (W8 → W4 → W3, W8A8 → W4A8) → Evaluation benchmarks (stereotypes → fairness → toxicity/sentiment → capabilities) → Metric types (probability-based → generated text-based)
- Critical path: 1) Select quantization strategy and bit-width based on acceptable bias-capability tradeoff 2) Run evaluation suite across all demographic categories before deployment 3) Prioritize generated text-based metrics over probability-based metrics for bias assessment 4) Verify useful-answer rate has not degraded significantly (Table G.10 provides thresholds)
- Design tradeoffs: W8/W8A8: Minimal bias impact and capability loss—safe default; W4: Moderate toxicity reduction, mixed fairness effects—acceptable when efficiency is critical; W3/W4A8: Strongest toxicity reduction but significant bias amplification and capability degradation—high risk; GPTQ vs AWQ: GPTQ better for toxicity reduction; AWQ slightly better for capabilities and fairness; SmoothQuant: Strongest effects across all dimensions, worst generation quality—avoid for sensitive applications
- Failure signatures: Probability metrics showing bias improvement while text metrics show degradation → model uncertainty, not bias mitigation; Useful-answer rate below ~50% on toxicity benchmarks → instruction-following failure (see Table G.10); No-answer rate exceeding ~20% on decision tasks → capability collapse (Table G.3); Unbiased-answer rate dropping sharply on DiscrimEvalGen → fairness degradation
- First 3 experiments: 1) Baseline characterization: Evaluate your model architecture on MMLU, BOLD (toxicity), and BBQ (stereotypes) across all three demographic categories to establish pre-quantization bias profile 2) Bit-width sweep: At W8, W4, and W3 (or W8A8, W4A8), measure MMLU accuracy drop, toxicity reduction, and stereotype amplification to identify your acceptable operating point 3) Strategy comparison: At your target bit-width, compare GPTQ vs AWQ on your most sensitive bias dimension to select strategy with favorable bias-efficiency tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does quantization-aware training (QAT) or mixed-precision quantization mitigate the bias amplification effects observed with post-training quantization methods?
- Basis in paper: [explicit] The Limitations section states: "Our study focuses on the most commonly used post-training quantization (PTQ) methods with uniform bit-width settings across layers. We do not consider quantization-aware training (QAT) or mixed-precision strategies."
- Why unresolved: The study only evaluated PTQ methods (GPTQ, AWQ, SmoothQuant) with uniform bit-widths; alternative quantization paradigms may interact differently with model representations of bias.
- What evidence would resolve it: Comparative evaluation of QAT and mixed-precision strategies on the same bias benchmarks (stereotypes, fairness, toxicity, sentiment) used in this study.

### Open Question 2
- Question: What are the causal mechanisms by which quantization reduces toxicity and amplifies stereotypes in LLMs?
- Basis in paper: [explicit] The Limitations section states: "Although we observe trends in how quantization affects model bias, our study does not establish causal mechanisms underlying these changes. Future work could leverage interpretability methods or representation probing to shed light on how quantization alters internal representations of sensitive concepts."
- Why unresolved: The paper demonstrates effects (e.g., toxicity reduction, stereotype amplification in generated text) but does not identify which internal representations or circuit components are altered by quantization to produce these outcomes.
- What evidence would resolve it: Probing studies on layer-wise representations of bias-related concepts before and after quantization; causal tracing or activation patching to identify critical circuits.

### Open Question 3
- Question: How does quantization affect social bias in multilingual LLMs and across diverse cultural contexts beyond English?
- Basis in paper: [explicit] The Limitations section states: "our analysis primarily focuses on English and commonly studied demographic categories... This leaves out other important aspects of social identity, such as disability or nationality, as well as underexplored linguistic and cultural contexts."
- Why unresolved: All experiments used English-language benchmarks and Western-centric demographic categories (gender, race, religion as defined in US contexts).
- What evidence would resolve it: Evaluation of quantized multilingual models on bias benchmarks in multiple languages and cultural contexts.

### Open Question 4
- Question: Do stochastic decoding strategies (e.g., temperature sampling) produce different bias patterns under quantization compared to greedy decoding?
- Basis in paper: [explicit] The Limitations section states: "Our work uses greedy decoding for all model generations. We do not explore stochastic decoding strategies (e.g., temperature sampling), which are frequently employed in real-world applications and may yield different behavioral patterns."
- Why unresolved: Real-world deployments often use temperature-based sampling, which could interact with quantization's effects on probability distributions differently than greedy decoding.
- What evidence would resolve it: Systematic comparison of bias metrics across different temperature settings and sampling strategies for quantized models.

## Limitations

- The study focuses on post-training quantization methods (GPTQ, AWQ, SmoothQuant) and does not explore quantization-aware training or mixed-precision strategies that might have different bias effects
- All experiments use English-language benchmarks and Western-centric demographic categories, leaving out multilingual and culturally diverse contexts
- The paper does not establish causal mechanisms for how quantization alters internal representations of bias-related concepts
- The study uses greedy decoding exclusively and does not explore how stochastic decoding strategies might interact with quantization differently

## Confidence

**High Confidence:** The finding that quantization reduces raw toxicity while increasing stereotypes and unfairness in generative tasks is well-supported by multiple benchmarks and consistent across different quantization strategies and model architectures. The systematic evaluation design and statistical significance testing (approximate randomization test) provide strong evidence for this core claim.

**Medium Confidence:** The claim that probability-based metrics become unreliable under quantization due to uncertainty masking effects is supported by the evidence but requires additional validation. While the paper shows log-likelihood decreases symmetrically across sentence types, it does not directly measure model uncertainty or test whether uncertainty affects different demographic subgroups differently.

**Low Confidence:** The mechanism explanation for why SmoothQuant has stronger effects on social dimensions than weight-only methods (GPTQ, AWQ) remains speculative. The paper does not provide detailed analysis of how weight-activation quantization specifically disrupts bias representations compared to weight-only approaches.

## Next Checks

**Check 1:** Design an experiment to directly measure model uncertainty under different quantization levels by comparing entropy distributions across demographic subgroups. This would validate whether uncertainty affects pro- and anti-stereotypical content asymmetrically, potentially explaining why probability metrics become unreliable.

**Check 2:** Conduct ablation studies isolating different quantization components (weight quantization vs. activation quantization) to determine which aspects of SmoothQuant specifically contribute to stronger bias effects. This would test the mechanism hypothesis about weight-activation quantization disrupting bias representations more severely.

**Check 3:** Implement controlled generation experiments where models generate longer reasoning chains under different quantization levels, then analyze how early reasoning errors propagate to final answers. This would validate the chain-of-thought corruption mechanism and quantify the relationship between precision loss and stereotype amplification in reasoning tasks.