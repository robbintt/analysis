---
ver: rpa2
title: Closing the Sim2Real Performance Gap in RL
arxiv_id: '2510.17709'
source_url: https://arxiv.org/abs/2510.17709
tags:
- policy
- real-world
- parameters
- in-sim
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a bi-level RL framework to close the Sim2Real
  performance gap by directly optimizing simulator parameters to maximize real-world
  policy performance, rather than using simulator accuracy as a proxy. The method
  derives sensitivity gradients for in-simulation policy training via Stochastic Policy
  Gradient methods, enabling outer-level RL to adapt simulation models and reward
  functions using real-world data.
---

# Closing the Sim2Real Performance Gap in RL

## Quick Facts
- **arXiv ID:** 2510.17709
- **Source URL:** https://arxiv.org/abs/2510.17709
- **Reference count:** 29
- **Primary result:** Introduces bi-level RL framework that directly optimizes simulator parameters to maximize real-world policy performance, validated on discrete and continuous MDPs

## Executive Summary
This paper addresses the Sim2Real gap by proposing a bi-level reinforcement learning framework that directly optimizes simulator parameters to maximize real-world policy performance, rather than using simulation accuracy as a proxy. The approach uses an outer-loop RL process to adapt simulation models based on real-world data, while an inner-loop RL agent trains policies within the simulation. By deriving sensitivity gradients through Implicit Differentiation and Markov Chain Sensitivity analysis, the framework theoretically achieves Sim2Real optimality and overcomes objective mismatch issues in model-based RL. The method shows promise for applications in policy-based differentiable planning and offline RL, though it faces computational challenges and requires differentiable simulators.

## Method Summary
The framework treats simulator parameters as learnable entities to be optimized via outer-loop RL based on real-world performance, rather than fixed approximations of reality. It employs Implicit Function Theorem to compute how policy parameters change with simulator parameters, enabling gradient-based optimization. The method derives sensitivity gradients for in-simulation policy training via Stochastic Policy Gradient methods, allowing the outer loop to adapt simulation models and reward functions using real-world data. This creates a theoretically optimal solution where the simulator parameters are tuned to produce policies that perform best in the real world, not necessarily to perfectly replicate real-world physics.

## Key Results
- The bi-level RL framework successfully converges in five randomized trials on both discrete and continuous MDPs
- The method theoretically achieves Sim2Real optimality by directly optimizing for real-world performance
- Sensitivity gradients computed via Implicit Differentiation enable effective outer-loop parameter adaptation
- Framework overcomes objective mismatch issues present in traditional model-based reinforcement learning approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Optimizing simulator parameters directly for real-world performance closes the Sim2Real gap more effectively than optimizing for simulation accuracy.
- **Mechanism:** The framework treats the simulator as a parameterized environment rather than a fixed truth, using outer-loop RL to adjust parameters such that policies trained in simulation yield maximum return when deployed in the real world.
- **Core assumption:** Real-world performance data provides reliable gradient signals despite noise.
- **Evidence anchors:** Abstract states direct adaptation based on real-world performance; Section 4 describes outer-level RL adapting simulator dynamics and rewards.

### Mechanism 2
- **Claim:** Policy sensitivity to simulator parameters can be computed via Implicit Differentiation.
- **Mechanism:** The inner-loop training is viewed as an equilibrium condition, allowing IFT to derive how policy parameters change when simulator parameters change without needing explicit analytical solutions.
- **Core assumption:** Inner-level RL policy converges to a local optimum with invertible Jacobian.
- **Evidence anchors:** Section 4.1 provides the IFT derivation; Section 6 discusses sensitivity analysis contributions.

### Mechanism 3
- **Claim:** Full gradient estimation requires accounting for state visitation distribution changes.
- **Mechanism:** Uses Markov Chain Sensitivity to capture derivative of discounted state distribution with respect to parameters, ensuring outer loop optimizes for long-term distribution shifts.
- **Core assumption:** Simulator dynamics are differentiable with respect to parameters.
- **Evidence anchors:** Section 4.1 includes distribution gradient terms in Theorem 1; Appendix details Markov Chain Sensitivity derivation.

## Foundational Learning

- **Concept: Bi-level Optimization**
  - **Why needed here:** The core architecture is bi-level with inner policy optimization nested inside outer simulator optimization. Understanding nested gradients is essential.
  - **Quick check question:** Can you explain why we cannot simply train the policy and simulator parameters simultaneously in a single joint optimization loop?

- **Concept: Implicit Function Theorem (IFT)**
  - **Why needed here:** This allows calculation of how the trained policy changes with simulator parameters without unrolling the entire training history.
  - **Quick check question:** In the equation ∇θφ = -(∇φφ̂)^{-1}∇θφ̂, what does the term ∇φφ̂ represent physically?

- **Concept: Stochastic Policy Gradient (SPG)**
  - **Why needed here:** The paper assumes the inner policy is trained via SPG, and the derivation relies on differentiating the specific form of the policy gradient update.
  - **Quick check question:** Why does the framework require differentiating the log-probability logπφ(a|s) with respect to both policy parameters φ and simulator parameters θ?

## Architecture Onboarding

- **Component map:**
  Simulation Environment (θ) -> Inner Agent (φ) -> Real World Interface -> Sensitivity Estimator -> Outer Update

- **Critical path:**
  1. **Initialization:** Start with reasonably accurate simulator θ₀
  2. **Inner Loop:** Train πφ in simulation until convergence (φ̂≈0)
  3. **Sensitivity Calculation:** Compute ∇θφ using stored trajectories
  4. **Real Rollout:** Deploy πφ to real world to get performance metrics
  5. **Outer Update:** Update θ using real-world gradient weighted by sensitivity

- **Design tradeoffs:**
  - **Computational Cost vs. Accuracy:** Computing inverse Hessian-vector products is expensive; approximations needed for high-dimensional φ
  - **Simulator Fidelity vs. Performance:** Framework explicitly trades physical realism for policy performance

- **Failure signatures:**
  - **Gradient Instability:** Near saddle points or flat regions, Jacobian may be singular causing outer gradient explosion
  - **Non-Stationarity:** If real world changes faster than outer loop adapts, target becomes moving goalpost

- **First 3 experiments:**
  1. **LQR Validation (Toy):** Implement continuous MDP example to verify sensitivity estimation matches finite-difference approximations
  2. **Tabular MDP (Discrete):** Implement discrete 3-state example to debug Markov Chain Sensitivity logic
  3. **Sensitivity Approximation Scaling:** Replace exact inverse Jacobian with Neumann series approximation or conjugate gradient for larger policy networks

## Open Questions the Paper Calls Out

- **Question:** Can the computational expense of estimating critic sensitivities be reduced for large-scale RL problems?
  - **Basis in paper:** Section 6.1 states efficient approximations are required for scalability
  - **Why unresolved:** Significant computation needed to estimate sensitivities via DNNs increases with policy parameter size
  - **What evidence would resolve it:** Development of approximation techniques working on high-dimensional continuous control tasks without prohibitive training time

- **Question:** How can epistemic uncertainty be integrated into the bi-level RL framework?
  - **Basis in paper:** Section 6.1 notes the framework doesn't account for epistemic uncertainties from limited data
  - **Why unresolved:** Current formulation doesn't model uncertainty from limited data which can degrade decision-making
  - **What evidence would resolve it:** Modified algorithm with Bayesian models or regularization terms that stabilize value estimation in low-data regimes

- **Question:** Can the framework work directly with observation spaces rather than requiring same state representation?
  - **Basis in paper:** Section 6.1 lists as limitation that real-world environments often provide only partial or noisy observations
  - **Why unresolved:** Current derivation assumes simulation and real world share exact same state space
  - **What evidence would resolve it:** Theoretical extension or empirical validation using pixel-based observations without ground-truth states

## Limitations
- Computational expense of calculating inverse Jacobians and Hessian-vector products scales poorly with policy complexity
- Requires reliable real-world performance data to provide meaningful gradients for outer loop
- Performance heavily depends on differentiability of simulator and convergence of inner policy

## Confidence

**High Confidence:** Theoretical derivation of bi-level optimization framework and use of Implicit Function Theorem are mathematically sound and well-grounded.

**Medium Confidence:** Empirical validation demonstrates concept but limited scope (5 trials, simple environments) leaves questions about scalability and robustness to complex problems.

**Low Confidence:** Claims about direct application to policy-based differentiable planning and offline RL without modifications are speculative and unsupported by presented experiments.

## Next Checks

1. **Scale Test:** Apply framework to slightly more complex environment (CartPole or Acrobot) to evaluate computational feasibility of sensitivity estimation with larger policy networks.

2. **Noise Sensitivity Analysis:** Systematically vary real-world performance data noise and measure impact on convergence and robustness of outer-level RL.

3. **Non-Differentiable Dynamics Test:** Implement simulator with non-differentiable component and test whether framework can be adapted with gradient estimation techniques like score functions or smoothing.