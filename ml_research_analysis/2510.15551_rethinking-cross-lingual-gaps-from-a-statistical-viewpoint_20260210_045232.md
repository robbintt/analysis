---
ver: rpa2
title: Rethinking Cross-lingual Gaps from a Statistical Viewpoint
arxiv_id: '2510.15551'
source_url: https://arxiv.org/abs/2510.15551
tags:
- source
- target
- variance
- response
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual knowledge transfer in large language models suffers
  from performance gaps due to increased response variance in target languages, not
  from knowledge barriers. By formalizing this gap using bias-variance decomposition,
  the authors show that ensembling responses and targeted inference-time interventions
  can reduce cross-lingual divergence by 20-25%.
---

# Rethinking Cross-lingual Gaps from a Statistical Viewpoint

## Quick Facts
- **arXiv ID:** 2510.15551
- **Source URL:** https://arxiv.org/abs/2510.15551
- **Reference count:** 40
- **Primary result:** Cross-lingual gaps are primarily caused by variance in target-language responses, not knowledge barriers, and can be reduced by 20-25% through ensembling and inference-time interventions.

## Executive Summary
This paper challenges the conventional view that cross-lingual accuracy gaps in large language models stem from knowledge barriers. Instead, the authors argue that increased response variance in target languages is the primary culprit. By formalizing this gap using bias-variance decomposition, they show that ensembling responses and targeted inference-time interventions can significantly reduce cross-lingual divergence. Experiments across five state-of-the-art models on ECLeKTic and MMLU benchmarks confirm that unbiased noise in target responses is the dominant cause of the gap, enabling simple mitigation strategies without retraining.

## Method Summary
The paper investigates cross-lingual knowledge transfer in LLMs by formalizing the gap using bias-variance decomposition. The primary approach involves inference-time interventions: response ensembling (sampling multiple responses per query), Translation Ensemble (TrEn-k), and Translate-then-Answer (TTA-k). The authors analyze variance and bias components by comparing source and target language responses across five different LLMs on ECLeKTic and MMLU benchmarks, with a focus on factual recall tasks. They use Year-ECLeKTic subset for cleaner evaluation and employ metrics like transfer score, source-target agreement rate, and L2 distance of response embeddings.

## Key Results
- Cross-lingual accuracy gaps are primarily caused by higher response variance in target languages, not knowledge barriers
- Ensembling responses can reduce cross-lingual divergence by 20-25% across different models
- Response variance in source and target languages are proportional, with higher source confidence leading to better target performance
- Simple inference-time interventions like TTA-1 consistently improve transfer scores across model families

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-lingual accuracy gaps in LLMs are predominantly caused by higher response variance in target languages, not by knowledge transfer failure.
- **Mechanism:** The paper models target-language responses as a mixture of a "variance" component (same mean as source, higher variance) and a "bias" component (different mean). Using bias-variance decomposition, the authors show that ensembling multiple responses reduces variance. If gaps were due to bias, ensembling would widen the source-target disagreement; instead, ensembling consistently reduces it, proving variance is the dominant factor.
- **Core assumption:** Floating-point errors and MoE routing noise in a single model's forward passes sufficiently approximate the response variance that would otherwise require training multiple full LLMs.
- **Evidence anchors:**
  - [abstract] "hypothesize that the variance of responses in the target language is the main cause of this gap... formalize the cross-lingual gap in terms of bias-variance decomposition."
  - [Section 4.1.1, Figure 4] Shows a steady decrease in source-target divergence with increasing ensemble size for both ECLeKTic and MMLU (with mixup), with estimated π ≈ 0.9–0.95, indicating unbiased noise (κ=1) for 90-95% of examples.
  - [corpus] The corpus neighbors focus on transfer methods (e.g., fine-tuning, translation) but do not directly test or confirm the variance-dominated mechanism proposed here.
- **Break condition:** The mechanism assumes access to multiple stochastic forward passes or semantically equivalent query transformations. It may not hold for fully deterministic inference or for languages with systemic factual inconsistencies in the pretraining data.

### Mechanism 2
- **Claim:** Response variance in source and target languages is proportional; therefore, cross-lingual gaps diminish when the model is confident (low variance) in the source language.
- **Mechanism:** The paper provides a theoretical lower bound on the probability of the mode response in source and target. When the source confidence is high (large margin between top two logits relative to variance), the target confidence must also be high. Empirically, this translates to higher source-target agreement on questions where the model is confident in the source language.
- **Core assumption:** The relationship between source and target variance holds across the evaluated benchmarks and model families.
- **Evidence anchors:**
  - [Section 4.2, Figure 6] Plots show a consistent, near-linear increase in source-target agreement as source confidence increases across five different LLMs on ECLeKTic and MMLU (with mixup).
  - [Appendix F, Figure 10] Directly demonstrates that high confidence in source leads to high confidence in target, supporting the proportional variance claim.
  - [corpus] No corpus evidence directly addresses or contradicts this proportional variance relationship.
- **Break condition:** This proportionality may break for languages or scripts that are extremely underrepresented in the pretraining data, where the model has not learned stable representations.

### Mechanism 3
- **Claim:** Inference-time interventions that implicitly ensemble across semantic variants (e.g., multiple translations) can reduce variance and significantly close the cross-lingual gap.
- **Mechanism:** Two prompt-based methods are introduced: 1) **Translation Ensemble (TrEn-k):** Presents the original question plus *k* translations in a single prompt. 2) **Translate-then-Answer (TTA-k):** Requires the model to first generate *k* translations of the question, then answer. Both methods force the model to reconcile information across multiple linguistic variants, effectively performing an internal ensemble that averages out variance.
- **Core assumption:** The model can reliably translate or recognize semantically equivalent questions across languages within the prompt context.
- **Evidence anchors:**
  - [Section 4.1.2, Table 1] TTA-1 consistently improved transfer scores across Gemini and GPT models on ECLeKTic. TrEn-k showed progressive improvement from k=1 to k=5.
  - [abstract] "demonstrate a simple prompt instruction to reduce the response variance, which improved target accuracy by 20-25% across different models."
  - [corpus] Corpus neighbors like "AdaMCoT" and "Bridging Language Gaps" propose complex methods (CoT, fine-tuning) for transfer, whereas this mechanism shows simple inference-time prompting can be highly effective.
- **Break condition:** Effectiveness depends on the model's instruction-following ability (e.g., TTA failed for GPT-5-mini and DeepSeek due to instruction misinterpretation). It is also less applicable to benchmarks like MMLU (with mixup) that already contain mixed-language content.

## Foundational Learning

- **Concept: Bias-Variance Decomposition in Classification**
  - **Why needed here:** The paper's core argument reframes a problem typically attributed to "bias" (knowledge not transferring) as one of "variance" (inconsistent expression of transferred knowledge). Understanding how ensembling reduces variance is essential.
  - **Quick check question:** In a classification setting, if you average predictions from multiple high-variance models that are all centered around the correct answer, would you expect the ensemble's accuracy to improve, worsen, or stay the same? Why?

- **Concept: Cross-Lingual Transfer in LLMs**
  - **Why needed here:** The problem being studied is the performance drop when querying knowledge in a language different from the source. Contextualizing this as a specific instance of domain adaptation (where domain = language) is key.
  - **Quick check question:** An LLM is trained mostly on English text but can answer factual questions in Hindi, though with lower accuracy. According to this paper, is the primary issue that the knowledge itself isn't stored in a language-agnostic way, or something else?

- **Concept: Test-Time Ensembling and Augmentation**
  - **Why needed here:** The proposed solutions (TrEn, TTA, response ensembling) are all forms of test-time intervention that do not require retraining. This is a practical paradigm for improving model outputs post-hoc.
  - **Quick check question:** Instead of retraining a model, you pass a single input image to a vision model in ten different rotated orientations and average the predictions. What statistical property of the model's outputs are you leveraging to potentially improve accuracy?

## Architecture Onboarding

- **Component map:** Variance Diagnosis Pipeline -> Inference-Time Ensemble Router -> Bias-Variance Analyzer
- **Critical path:** Start by implementing the **Variance Diagnosis Pipeline** on a small held-out set. Run it for a target language pair (e.g., English source, Hindi target) to confirm high variance is the issue. Then, implement the **TTA-1** prompt strategy, which showed the most consistent gains in the paper, and integrate it into the serving path for cross-lingual queries.
- **Design tradeoffs:**
  - **Ensembling cost vs. accuracy gain:** Response ensembling (Mechanism 1) requires multiple forward passes (10 in the paper), increasing latency and compute cost linearly. Input ensembling (TTA/TrEn) costs one longer-context forward pass but depends on the model's ability to handle and reconcile the provided translations.
  - **Generality vs. specificity:** The methods are model-agnostic but may require prompt tuning per model family (e.g., DeepSeek failed with TTA). A production system would need a fallback strategy.
  - **Applicability:** These are inference-time fixes; they do not improve the model's fundamental multilingual representations.
- **Failure signatures:**
  - **No improvement from ensembling:** Suggests the gap is bias-dominated for that language/task (π is low), requiring retraining or data curation, not post-hoc fixes.
  - **TTA/TrEn degrades performance:** Likely indicates the model cannot reliably translate or is confused by the mixed-language prompt. Check for instruction-following failures.
  - **High variance even in source language:** Indicates the model is inherently uncertain about the knowledge, regardless of language. Addressing this requires improving the source-domain knowledge, not just cross-lingual transfer.
- **First 3 experiments:**
  1. **Replicate the core finding:** On a slice of your cross-lingual evaluation set (e.g., 100 queries), implement 10x response ensembling. Measure the change in average source-target L2 distance (for text) or agreement rate. A decrease confirms variance as a primary factor.
  2. **Test TTA-1 effectiveness:** Implement the Translate-then-Answer (k=1) prompt template for your most common source-target language pair. Compare accuracy and transfer score against the baseline single-language prompt.
  3. **Analyze the π coefficient:** Using the data from Experiment 1, compute the fraction of queries where ensembling reduced source-target disagreement. This π value estimates the proportion of errors attributable to variance vs. bias for your specific model and domain, guiding whether to invest in inference-time fixes or seek training-time solutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the observed increase in response variance in target languages a functional coping mechanism for LLMs to manage perplexity loss resulting from cross-language factual inconsistencies in pretraining data?
- Basis in paper: [explicit] Section 6 (Discussion): "Is increased variance in target a coping mechanism of LLMs to keep perplexity loss from exploding due to cross-language factual inconsistencies in pretraining data? We leave such analysis also for future work."
- Why unresolved: The paper identifies variance as the cause of the gap but does not investigate the underlying mechanism generating this variance.
- What evidence would resolve it: A causal analysis linking specific cross-language inconsistencies in the pretraining corpus to activation patterns that induce high-variance outputs during inference.

### Open Question 2
- Question: Can specific training objectives (post-training or fine-tuning) effectively minimize this target response variance?
- Basis in paper: [explicit] Section 6 (Discussion): "We championed some inference-time mitigation strategies... and leave mitigation through training approaches for future work."
- Why unresolved: The authors demonstrated successful inference-time interventions (ensembling) but did not test if the model can learn to maintain low variance through weight updates.
- What evidence would resolve it: Experimental results showing that fine-tuning on a variance-penalizing objective closes the cross-lingual gap comparably to inference-time ensembling.

### Open Question 3
- Question: Does the bias-variance framework for cross-lingual gaps generalize to cross-modal gaps (e.g., text vs. audio inputs)?
- Basis in paper: [explicit] Section 6 (Discussion): "Our insights may also explain cross-modal (for e.g., performance disparity between text as input vs audio as input) gaps. We leave such generalizations for future work."
- Why unresolved: The hypothesis is proposed as a generalization but is validated only on textual language tasks.
- What evidence would resolve it: Replication of the paper's variance-reduction experiments on multimodal benchmarks, showing that ensembling reduces performance gaps between text and audio inputs.

### Open Question 4
- Question: Is the model's confidence in the source language determined by knowledge consistency or duplication rather than the entity's popularity across multiple languages?
- Basis in paper: [explicit] Appendix A: "If not their multilingual multiplicity in pretraining data, what then determines confidence in source? Is knowledge consistency or duplication important?"
- Why unresolved: The authors found a poor correlation between an entity's multilingual frequency on the web and the model's ability to answer correctly, leaving the root cause of "source confidence" unidentified.
- What evidence would resolve it: Controlled experiments varying the duplication rate and factual consistency of entities in a synthetic pretraining corpus to observe the impact on source confidence and target transfer.

## Limitations
- Findings primarily apply to factual recall tasks and may not generalize to complex reasoning or subjective judgment tasks
- Analysis relies on exact-match evaluations, which may not capture nuanced multilingual content where bias-dominated cases could emerge
- Proportional variance relationship may break for extremely low-resource languages with sparse pretraining coverage

## Confidence
- **High confidence** in the variance-dominance hypothesis and the effectiveness of ensembling for reducing cross-lingual gaps, supported by consistent empirical results across five models and two benchmarks
- **Medium confidence** in the generalizability of the proportional variance relationship to all language pairs and model architectures, as it was primarily tested on Indo-European and major East Asian languages
- **Medium confidence** in the TTA/TrEn methods as broadly applicable inference-time interventions, given observed failures for specific model families

## Next Checks
1. Test the TTA-1 prompt template on a wider range of model families (e.g., open-weight models like Llama, Qwen) to assess the robustness of instruction-following as a universal mitigation strategy
2. Apply the variance diagnosis pipeline to a non-factual task benchmark (e.g., multilingual commonsense reasoning or sentiment analysis) to determine if the variance-dominance finding extends beyond knowledge recall
3. Conduct ablation studies varying temperature and decoding strategies within ensembling to identify optimal settings for maximizing variance reduction while minimizing computational cost