---
ver: rpa2
title: Episodic Novelty Through Temporal Distance
arxiv_id: '2501.15418'
source_url: https://arxiv.org/abs/2501.15418
tags:
- learning
- reward
- distance
- intrinsic
- episodic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of exploration in sparse reward
  Contextual Markov Decision Processes (CMDPs), where environments differ across episodes.
  Existing episodic intrinsic motivation methods struggle due to ineffective count-based
  approaches in large state spaces and similarity-based methods lacking appropriate
  metrics for state comparison.
---

# Episodic Novelty Through Temporal Distance

## Quick Facts
- **arXiv ID:** 2501.15418
- **Source URL:** https://arxiv.org/abs/2501.15418
- **Reference count:** 40
- **Primary result:** ETD achieves near-optimal performance in ObstructedMaze-Full within 20 million steps and improves sample efficiency by a factor of two compared to strongest baseline.

## Executive Summary
This paper addresses exploration in sparse reward Contextual Markov Decision Processes (CMDPs) where environments differ across episodes. Existing episodic intrinsic motivation methods struggle due to ineffective count-based approaches in large state spaces and similarity-based methods lacking appropriate metrics for state comparison. The proposed method, Episodic Novelty Through Temporal Distance (ETD), introduces temporal distance as a robust metric for state similarity and intrinsic reward computation. ETD employs contrastive learning to accurately estimate temporal distances and derives intrinsic rewards based on the novelty of states within the current episode. Extensive experiments on various benchmark tasks demonstrate that ETD significantly outperforms state-of-the-art methods.

## Method Summary
ETD is an episodic intrinsic motivation method built on PPO for CMDPs. It learns a temporal distance metric through contrastive learning, where positive pairs are constructed by sampling geometrically-distributed future states. The intrinsic reward is computed as the minimum temporal distance to any state in the current episode's memory. This aggressive exploration signal penalizes revisiting similar states while maintaining robustness in noisy environments. The method uses a CNN+RNN backbone with a quasimetric network (MRN) for asymmetric temporal distance and a potential network (MLP) that is discarded after training.

## Key Results
- Achieves near-optimal performance in ObstructedMaze-Full within 20 million steps
- Improves sample efficiency by a factor of two compared to the strongest baseline
- Demonstrates robustness in noisy environments where other methods fail
- Outperforms state-of-the-art methods on MiniGrid, Crafter, and MiniWorld benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Temporal Distance as Quasimetric State Similarity
- **Claim:** Temporal distance (expected steps between states) provides a more robust similarity metric than Euclidean distance or likelihood-based approaches for episodic novelty computation.
- **Mechanism:** The successor distance formulation satisfies quasimetric properties (positivity, identity, triangle inequality) even in stochastic MDPs, learned via contrastive learning where positive pairs are (current state, geometrically-distributed future state).
- **Core assumption:** States reachable in fewer steps under the current policy are more similar; the environment is sufficiently ergodic that temporal distance remains finite.
- **Evidence anchors:** [abstract] introduces temporal distance; [section 4.1] defines successor distance satisfying triangle inequality and quasimetric properties; limited direct evidence in corpus papers.
- **Break condition:** In highly non-ergodic environments or when policy changes drastically, learned temporal distances may become unreliable.

### Mechanism 2: Episodic Intrinsic Reward via Minimum Temporal Distance
- **Claim:** Computing intrinsic reward as the minimum temporal distance to any state in episodic memory provides an aggressive exploration signal that avoids revisiting similar states.
- **Mechanism:** At time t, reward = min_{k∈[0,t)} d_φ(s_k, s_t). If current state matches any memory state, reward → 0. This aggressively penalizes revisitation.
- **Core assumption:** The minimum (vs. k-nearest-neighbor average) provides the most useful exploration signal; episodic memory captures the relevant exploration history.
- **Evidence anchors:** [section 4.2] defines the episodic temporal distance bonus formally; [section 5.3] ablation shows minimum outperforms other aggregate functions; consistent with corpus papers on episodic intrinsic rewards.
- **Break condition:** When episodic memory grows large, computational cost increases (though paper notes single batch inference mitigates this).

### Mechanism 3: Contrastive Learning with Potential-Quasimetric Decomposition
- **Claim:** Decomposing the energy function f(x,y) = c_ψ(y) - d_φ(x,y) enables accurate temporal distance recovery from contrastive learning.
- **Mechanism:** The symmetrized InfoNCE loss trains f to assign high values to temporally close (x,y) pairs. The unique solution decomposes into potential c(y) minus quasimetric d(x,y), where d approximates successor distance. The potential is discarded after training.
- **Core assumption:** Sufficient batch size and sample diversity for contrastive learning convergence; MRN architecture can represent asymmetric distances.
- **Evidence anchors:** [section 4.1] defines the loss and parameterization; [section 5.3] ablation shows MRN+POT outperforms alternatives; corpus papers use contrastive learning but not this specific decomposition.
- **Break condition:** With insufficient positive pair diversity or inappropriate time horizon sampling, learned distances may not generalize.

## Foundational Learning

- **Contextual Markov Decision Processes (CMDPs):**
  - **Why needed here:** ETD targets CMDPs where each episode presents a different environment (procedurally generated). Standard intrinsic motivation methods fail because cross-episode experience doesn't generalize.
  - **Quick check question:** Can you explain why count-based exploration fails when each episode has a unique environment configuration?

- **Contrastive Learning & InfoNCE Loss:**
  - **Why needed here:** ETD uses contrastive learning to estimate temporal distances. Understanding how positive/negative pairs are constructed and how the loss shapes representations is essential.
  - **Quick check question:** In ETD's contrastive setup, what makes (x_i, y_i) a positive pair, and what role does the geometric distribution play?

- **Quasimetric Properties (Triangle Inequality):**
  - **Why needed here:** The paper proves successor distance satisfies quasimetric properties. Understanding why this matters for reliable state comparison is key.
  - **Quick check question:** Why does satisfying triangle inequality make a distance metric more suitable for comparing state similarities across a trajectory?

## Architecture Onboarding

- **Component map:**
  CNN feature extractor → shared backbone for policy/value/quasimetric networks → Quasimetric network d_φ (MRN) outputs asymmetric temporal distance, Potential network c_ψ (MLP) trained jointly, discarded after contrastive learning → Episodic memory stores CNN embeddings of visited states → PPO base RL algorithm receiving augmented rewards

- **Critical path:**
  1. Sample (x_i, y_i) pairs from rollout buffer where y_i = s_{t+j}, j ~ Geom(1-γ)
  2. Train f_{φ,ψ} = c_ψ - d_φ via symmetrized InfoNCE loss
  3. At each environment step, compute b_t = min_k d_φ(s_k, s_t) against episodic memory
  4. Augment reward: r_total = r_extrinsic + β × b_t
  5. Update PPO policy with augmented rewards

- **Design tradeoffs:**
  - Minimum vs. k-NN aggregation: Minimum is most aggressive but potentially noisier; paper shows minimum works best
  - Symmetric vs. asymmetric distance: Paper tests both; asymmetric is default but symmetric performs similarly in tested environments
  - Episodic-only vs. global+episodic: ETD is purely episodic (no global bonus), which may limit performance in singleton MDPs

- **Failure signatures:**
  - If intrinsic reward remains uniformly high: temporal distance learning may have failed (check contrastive loss convergence)
  - If agent revisits states despite high bonus: episodic memory may not be updating, or distance metric is not discriminating
  - If training is unstable with noise: check batch/layer normalization settings (paper notes sensitivity here)

- **First 3 experiments:**
  1. **Sanity check:** Train on MiniGrid-DoorKey-8x8; verify temporal distance visualization matches expected geometry
  2. **Ablation on aggregate function:** Compare min, knn10, quantile10 on DoorKey-16x16 to reproduce Fig. 8 results
  3. **Noise robustness test:** Add Gaussian noise (μ=0, σ²=0.1) to MiniGrid states; verify ETD maintains performance while NovelD fails

## Open Questions the Paper Calls Out

- **Can a unified intrinsic reward be designed to combine temporal distance for both global and episodic bonuses?**
  - The current purely episodic approach limits performance in singleton MDPs; combining temporal distance for both global and episodic bonuses is a promising direction.

- **How does the method behave in non-ergodic environments where the successor distance theoretically approaches infinity?**
  - The successor distance may be infinite in non-ergodic settings, implying an assumption that we're dealing with ergodic MDP problems.

- **How can the bias introduced by violating the Markov property through episodic bonuses be theoretically or empirically mitigated?**
  - Episodic bonuses transform the task into a POMDP, and further research is needed to mitigate the impact of the POMDP transformation.

## Limitations

- The method's effectiveness in non-episodic or singleton MDPs remains unclear
- Claims about noise robustness are based on limited evidence beyond MiniWorld experiments
- The core innovation relies on temporal distance quasimetric, which is validated primarily through ablation studies rather than theoretical guarantees

## Confidence

- **High:** ETD's superior performance on benchmark tasks (MiniGrid, Crafter, MiniWorld) compared to state-of-the-art baselines
- **Medium:** The mechanism by which temporal distance provides more robust state similarity than alternatives
- **Low:** The claim of noise robustness, as the paper provides limited evidence beyond MiniWorld experiments

## Next Checks

1. Test ETD on singleton MDPs (fixed environment across episodes) to verify performance claims beyond procedural generation
2. Implement ablation comparing temporal distance to alternative metrics (Euclidean, likelihood-based) on a simplified environment to isolate the quasimetric advantage
3. Conduct sensitivity analysis on the geometric distribution parameter p in contrastive learning to determine robustness to temporal horizon choices