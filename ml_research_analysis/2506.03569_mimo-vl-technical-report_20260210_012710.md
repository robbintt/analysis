---
ver: rpa2
title: MiMo-VL Technical Report
arxiv_id: '2506.03569'
source_url: https://arxiv.org/abs/2506.03569
tags:
- reasoning
- data
- arxiv
- zhang
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MiMo-VL-7B-RL, a vision-language model, outperforms Qwen2.5-VL-7B
  on 35 out of 40 evaluated tasks and achieves state-of-the-art performance among
  open-source VLMs of comparable scale. The model delivers strong general visual understanding,
  scoring 66.7 on MMMU, and demonstrates exceptional multimodal reasoning with a 59.4
  on OlympiadBench, surpassing models with up to 78B parameters.
---

# MiMo-VL Technical Report

## Quick Facts
- arXiv ID: 2506.03569
- Source URL: https://arxiv.org/abs/2506.03569
- Reference count: 40
- Primary result: MiMo-VL-7B-RL achieves state-of-the-art performance among open-source VLMs of comparable scale, scoring 59.4 on OlympiadBench and 56.1 on OSWorld-G

## Executive Summary
MiMo-VL-7B-RL is a vision-language model that achieves state-of-the-art performance among open-source VLMs of comparable scale. The model delivers strong general visual understanding (66.7 on MMMU), exceptional multimodal reasoning (59.4 on OlympiadBench), and sets a new standard for GUI grounding (56.1 on OSWorld-G). These results are achieved through a four-stage pre-training process incorporating high-quality synthetic reasoning data with long Chain-of-Thought, combined with a Mixed On-policy Reinforcement Learning framework that integrates diverse reward signals across reasoning, perception, grounding, and human preference alignment.

## Method Summary
The method involves building MiMo-VL-7B, a 7B vision-language model, through a four-stage pre-training curriculum followed by Mixed On-policy Reinforcement Learning. The architecture combines Qwen2.5-ViT with an MLP projector and MiMo-7B-Base LLM. Pre-training stages progress from projector warmup (300B tokens) to full multimodal training (2.4T total tokens). Post-training applies MORL with verifiable rewards (GIoU, Math-Verify) and learned reward models via a Reward-as-a-Service framework using on-policy GRPO updates.

## Key Results
- Achieves 59.4 on OlympiadBench, surpassing models with up to 78B parameters
- Sets new standard for GUI grounding with 56.1 on OSWorld-G, outperforming specialized models like UI-TARS
- Delivers strong general visual understanding with 66.7 on MMMU
- Outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1
Incorporating long Chain-of-Thought reasoning data into late-stage pre-training yields continued performance gains without saturation. Synthetic reasoning data with explicit reasoning steps provides richer supervision signals than short-answer QA patterns, enabling the model to learn generalizable reasoning patterns rather than surface-level pattern matching.

### Mechanism 2
Mixed On-policy Reinforcement Learning integrating multiple reward signals improves performance across diverse tasks, though simultaneous multi-domain optimization remains unstable. The unified Reward-as-a-Service framework routes queries to appropriate reward functions while on-policy GRPO performs single-step policy updates, avoiding the early plateau observed in vanilla GRPO.

### Mechanism 3
A four-stage pre-training curriculum with progressive component unfreezing stabilizes vision-language alignment. Stage 1 freezes ViT/LLM to warm up projector, Stage 2 unfreezes ViT with interleaved data, Stage 3 trains all parameters on diverse multimodal data, and Stage 4 extends to 32K context. This prevents noisy gradients from poorly aligned projectors from destabilizing early learning.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The post-training RL algorithm underlying MORL. Understanding how advantages are computed relative to group means (not absolute rewards) is essential for debugging training dynamics.
  - Quick check question: Given rewards [0.8, 0.4, 0.6] for three sampled responses, what are the normalized advantages?

- **Vision-Language Alignment via Projector Warmup**
  - Why needed here: Explains why Stage 1 exists and why skipping it causes instability. The MLP projector must learn to map ViT embeddings to LLM token space before joint training.
  - Quick check question: What would happen if you initialized the projector randomly and immediately trained all components together?

- **Reward Hacking in Multi-objective RL**
  - Why needed here: The paper explicitly notes this risk (Section 3.1 filters easy questions, removes image-independent questions). Understanding how models exploit weak reward signals is critical for MORL stability.
  - Quick check question: Why might a model learn to produce longer responses regardless of correctness if only accuracy rewards are used?

## Architecture Onboarding

- Component map: ViT output → Projector → LLM input (concatenated with text tokens) → Autoregressive generation → Response sampled → Reward computed → Policy update via on-policy GRPO

- Critical path: Vision Encoder (Qwen2.5-ViT) outputs embeddings → MLP projector maps to LLM token space → MiMo-7B-Base LLM processes concatenated vision-text tokens → Autoregressive generation produces response → Response routed through Reward-as-a-Service → Policy updated via on-policy GRPO

- Design tradeoffs:
  - Native resolution ViT preserves detail but increases compute; fixed-resolution would be faster but lose fine-grained information
  - On-policy GRPO vs. vanilla GRPO: Slower (requires fresh rollouts per step) but avoids early plateau
  - Separate reward models for text vs. multimodal: Higher maintenance but better specialization than a single unified model

- Failure signatures:
  - Stage 1 undertrained: Projector produces noisy embeddings, Stage 2 loss spikes
  - Reward hacking in counting tasks: Model outputs repetitive "1, 2, 3..." regardless of image content
  - Length divergence in MORL: Reasoning tasks grow responses indefinitely while grounding tasks collapse to single tokens

- First 3 experiments:
  1. Ablate Stage 4 reasoning data: Train Stage 4 without synthetic CoT data, measure gap on OlympiadBench/MMMU to quantify the reasoning boost
  2. Single-task RL baselines: Train separate models on reasoning-only and grounding-only RL, compare to MORL to measure interference magnitude
  3. Projector warmup duration sensitivity: Vary Stage 1 token count (100B/300B/500B) and measure Stage 2 loss stability and final benchmark scores

## Open Questions the Paper Calls Out

### Open Question 1
How can interference between reasoning tasks and perception/grounding tasks be effectively mitigated during Mixed On-policy Reinforcement Learning (MORL)? The paper notes that opposing growth trends in response length (reasoning encourages long CoT, grounding encourages short outputs) hinder stable simultaneous improvements, and the authors are actively investigating the causes.

### Open Question 2
Does the superior scaling behavior of the fully on-policy RL recipe persist indefinitely with increased compute and data? While the paper demonstrates no saturation within the observed training window, it remains unconfirmed if this positive correlation continues at much larger scales without hitting new bottlenecks.

### Open Question 3
What are the saturation limits for incorporating synthetic long-form Chain-of-Thought (CoT) data into pre-training stages? The authors highlight the lack of saturation during Stage 4 pre-training, but do not define the upper bound of data volume or quality required before diminishing returns set in.

## Limitations
- The claim of state-of-the-art performance relies heavily on the curated evaluation suite and may not generalize to all vision-language tasks
- The MORL framework's effectiveness across multi-domain optimization lacks direct ablation studies comparing it to simpler single-task RL approaches
- Stage 4 reasoning data quality and synthetic Chain-of-Thought generation processes are not fully validated against human-annotated reasoning chains

## Confidence

- **High Confidence**: The four-stage pre-training curriculum and its component unfreezing strategy (Stage 1 projector warmup prevents noisy gradients)
- **Medium Confidence**: MORL framework benefits from diverse reward signals, though multi-domain stability remains partially untested
- **Medium Confidence**: Synthetic reasoning data integration improves reasoning benchmarks, but quality assurance processes need more transparency

## Next Checks

1. **Ablation study on MORL vs. single-task RL**: Train separate reasoning-only and grounding-only RL models, then compare their combined performance against the full MORL model to quantify interference and benefits

2. **Projector warmup sensitivity analysis**: Systematically vary Stage 1 token count (100B, 300B, 500B) and measure Stage 2 loss stability, then track downstream benchmark performance differences

3. **Synthetic reasoning data quality audit**: Generate synthetic CoT data with varying quality thresholds (low/medium/high), train models with each subset, and measure correlation between synthetic data quality scores and OlympiadBench/MMMU performance improvements