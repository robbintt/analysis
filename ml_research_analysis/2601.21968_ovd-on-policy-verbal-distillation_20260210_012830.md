---
ver: rpa2
title: 'OVD: On-policy Verbal Distillation'
arxiv_id: '2601.21968'
source_url: https://arxiv.org/abs/2601.21968
tags:
- training
- verbal
- distillation
- reasoning
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces On-policy Verbal Distillation (OVD), a memory-efficient\
  \ knowledge distillation framework that replaces token-level probability matching\
  \ with trajectory matching using discrete verbal scores (0\u20139) from teacher\
  \ models. The approach addresses severe memory bottlenecks in reinforcement learning\
  \ by dramatically reducing memory consumption while enabling on-policy distillation\
  \ from teacher models with verbal feedback, and avoiding token-level alignment to\
  \ allow the student model to freely explore the output space."
---

# OVD: On-policy Verbal Distillation

## Quick Facts
- **arXiv ID:** 2601.21968
- **Source URL:** https://arxiv.org/abs/2601.21968
- **Reference count:** 40
- **Primary result:** OVD achieves up to +12.9% absolute improvement in average EM on Web Q&A tasks and up to +25.7% gain on math benchmarks when trained with only one random sample

## Executive Summary
OVD introduces a memory-efficient knowledge distillation framework that replaces token-level probability matching with trajectory matching using discrete verbal scores (0–9) from teacher models. This approach dramatically reduces memory consumption while enabling on-policy distillation from teacher models with verbal feedback, and avoids token-level alignment to allow the student model to freely explore the output space. The framework provides unbiased gradient estimates through verbal rejection sampling and theoretical guarantees of convergence under the mixture training distribution.

## Method Summary
OVD implements on-policy verbal distillation where the student model generates trajectories that are scored by a teacher model using discrete verbal scores (0-9) rather than full token-level logits. The system uses verbal rejection sampling: trajectories scoring below a threshold θ are rejected and replaced with teacher demonstrations, creating a mixture training distribution. This enables unbiased gradient estimates while dramatically reducing memory consumption from O(N·L·V) to O(N·K·v) where K is reasoning steps and v=10. The framework is trained using GRPO with advantages normalized within problem groups and KL regularization to maintain policy stability.

## Key Results
- **Web Q&A:** Up to +12.9% absolute improvement in average EM across 8 benchmarks
- **Mathematical reasoning:** Up to +25.7% gain on math benchmarks when trained with only one random sample
- **Memory efficiency:** ~48,000× reduction in memory consumption compared to token-level distillation

## Why This Works (Mechanism)

### Mechanism 1: Verbal Score Compression Replaces Token-Level Logits
Replacing full-vocabulary token-level logits with discrete verbal scores (0–9) reduces memory by ~48,000× while preserving gradient signal quality. Token-level distillation stores logits over vocabulary V (~152K) for each token in trajectory length L (~8192), consuming O(N·L·V) memory. OVD compresses this to O(N·K·v) where K is reasoning steps (K≪L) and v=10 (verbal vocabulary). The teacher outputs a single score token per step/trajectory rather than probability distributions over 152K tokens.

### Mechanism 2: Verbal Rejection Sampling Creates On-Policy Mixture Distribution
Threshold-based acceptance/rejection on verbal scores yields unbiased gradient estimates while filtering low-quality trajectories. Student samples trajectory y ∼ π_S. Teacher assigns score S(y) ∈ {0,…,9}. If S(y) < θ, reject and replace with teacher demonstration y' ∼ π_T. The resulting training distribution is: p_train(y) = α_t · π_S(y)·1[S(y)≥θ] + (1-α_t)·π_T(y). Theorem 3.1 proves this produces unbiased gradients.

### Mechanism 3: Implicit Curriculum via Acceptance Rate Evolution
As student improves, acceptance rate α_t naturally increases, creating automatic curriculum from teacher-guided to autonomous learning. Early training: student generates poor trajectories → low α_t → more teacher demonstrations. Later training: student improves → higher acceptance → α_t → 1 → student explores autonomously. Proposition 3.3 shows expected reward converges as quality gap δ_t → 0.

## Foundational Learning

- **Concept: Policy Gradient / REINFORCE**
  - Why needed here: OVD uses GRPO (PPO-style) policy optimization; understanding ∇J = E[R·∇log π] is essential for Theorem 3.1
  - Quick check question: Can you explain why the gradient of expected reward involves ∇log π_S rather than ∇π_S directly?

- **Concept: Knowledge Distillation (KL Divergence)**
  - Why needed here: OVD explicitly replaces token-level KL matching with trajectory-level verbal scoring; understanding what's being replaced clarifies the tradeoff
  - Quick check question: What information is lost when replacing soft teacher logits (152K-dim distribution) with a single discrete score (0–9)?

- **Concept: On-Policy vs Off-Policy Learning**
  - Why needed here: OVD's core innovation is enabling on-policy distillation without token-level alignment; the mixture distribution combines on-policy student samples with off-policy teacher demonstrations
  - Quick check question: Why does on-policy sampling mitigate distribution shift compared to training on pre-generated teacher outputs?

## Architecture Onboarding

- **Component map:**
  Student Model (π_S) -> Generate N trajectories per problem -> Verbal Scoring Interface -> Teacher Model (π_T) -> Verbal Rejection Sampling -> GRPO Optimizer -> Policy Update

- **Critical path:**
  1. Student generates trajectories -> Teacher scores -> Apply rejection threshold
  2. Construct mixture batch (accepted student + teacher demos) -> Compute rewards
  3. Normalize rewards within problem group (GRPO advantage) -> PPO-clip update

- **Design tradeoffs:**
  - Score granularity (v): Higher v (e.g., 100) improves precision but harder to calibrate; paper uses v=10 as balance
  - Rejection threshold (θ): Paper ablates θ∈{5,7,10}; θ=5 (lenient) allows more exploration, θ=10 (strict) relies heavily on teacher
  - Step-level vs trajectory-level scoring: Step-level provides denser feedback but more teacher calls; paper supports both

- **Failure signatures:**
  - Memory still high: Check if accidentally storing full logits somewhere (e.g., legacy KL computation)
  - Acceptance rate stuck at 0: Threshold too high or teacher scoring misconfigured
  - No improvement over baseline: Teacher may not be scoring discriminatively; verify score distribution has variance
  - Training instability: KL coefficient too low or advantage normalization failing

- **First 3 experiments:**
  1. Validate verbal scoring calibration: Run teacher on held-out trajectories, check score distribution spans 0–9 and correlates with ground-truth correctness; if collapsed to single value, adjust prompt or temperature
  2. Ablate rejection threshold: Train with θ∈{3,5,7,10} on small subset; plot acceptance rate α_t over training steps and final performance to find sweet spot
  3. Memory profiling: Measure peak memory with OVD vs token-level distillation on identical batch/sequence settings; verify ~40,000× reduction holds for your setup

## Open Questions the Paper Calls Out

### Open Question 1
What specific mechanisms enable environment agents, fine-tuned solely to mimic search outputs, to develop emergent discriminative capabilities for evaluating reasoning trajectories? The authors identify the phenomenon but do not provide an analysis of the learned representations or attention mechanisms that allow a simulator to perform discrimination.

### Open Question 2
To what extent can the verbal vocabulary size (v) be expanded before teacher calibration issues negate the theoretical benefits of finer-grained quality approximation? The paper utilizes a fixed vocabulary size of 10 (0-9) based on practical heuristics, leaving the upper bounds of granularity unexplored.

### Open Question 3
Can a dynamic rejection threshold schedule further optimize performance by better managing the trade-off between student exploration diversity and teacher guidance? The paper tests static thresholds, but the theoretical analysis suggests the mixture ratio α_t evolves; the optimal strategy for adapting θ to this evolution is not defined.

## Limitations

- **Verbal Scoring Quality and Calibration:** The effectiveness of OVD fundamentally depends on teacher verbal scores being both discriminative and well-calibrated, but the calibration process is not fully specified
- **Theoretical Convergence Conditions:** The convergence guarantees assume consistent teacher scores and monotonic student improvement, but these may not hold in practice with stochastic rewards
- **Memory Savings Validation:** While ~48,000× memory reduction is claimed, actual savings may vary depending on implementation details and sequence lengths

## Confidence

- **High Confidence:** Memory efficiency gains from verbal score compression are technically sound and verified through explicit calculation; unbiased gradient estimation through rejection sampling follows from importance sampling theory; empirical performance improvements over baselines are supported by extensive experiments
- **Medium Confidence:** The curriculum effect via acceptance rate evolution is theoretically supported but limited empirical validation; the general framework of verbal rejection sampling can be applied to other RL-based distillation tasks; the 0-9 verbal score discretization provides optimal balance between granularity and calibration difficulty
- **Low Confidence:** Long-term stability and convergence behavior under varying rejection thresholds; performance robustness across different teacher model capabilities and domains; scalability to much larger student models beyond 3B-7B parameters

## Next Checks

1. **Score Calibration Validation:** Implement systematic evaluation of teacher verbal score quality by comparing score distributions against ground-truth correctness across multiple datasets. Measure correlation between verbal scores and actual trajectory quality, and test whether score calibration issues significantly impact OVD performance.

2. **Convergence Behavior Analysis:** Conduct detailed analysis of acceptance rate evolution and its relationship to student performance over training time. Track how different rejection thresholds affect curriculum dynamics, and whether theoretical convergence guarantees hold in practice.

3. **Memory vs Performance Tradeoff:** Perform systematic ablation studies varying verbal vocabulary size (v) from 5 to 100 to quantify exact tradeoff between memory savings and performance. Measure point at which additional granularity provides diminishing returns versus increased memory consumption and teacher scoring difficulty.