---
ver: rpa2
title: 'RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback'
arxiv_id: '2507.15024'
source_url: https://arxiv.org/abs/2507.15024
tags:
- critic
- refcritic
- critique
- performance
- refinement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RefCritic, a critic model trained via reinforcement
  learning with dual rewards to improve both critique quality and policy model refinement.
  It addresses the limitation of supervised fine-tuning in producing deep, actionable
  critiques for complex reasoning tasks.
---

# RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback

## Quick Facts
- arXiv ID: 2507.15024
- Source URL: https://arxiv.org/abs/2507.15024
- Reference count: 13
- Key outcome: RefCritic achieves 6.8–7.2% gains in pass@1 after refinement and 3.6% gains in majority voting with 64 samples on AIME24/25 and Olympiad.

## Executive Summary
RefCritic introduces a novel training approach for critic models that goes beyond supervised fine-tuning to produce actionable, deep critiques for complex reasoning tasks. By leveraging reinforcement learning with dual rewards—one for judgment accuracy and another for the downstream utility of refinement—it forces the critic to generate feedback that not only identifies errors but also enables the policy model to correct them. Evaluated across mathematical reasoning benchmarks, RefCritic demonstrates substantial improvements in both critique quality and policy model performance, while also generalizing to step-level error identification without explicit supervision.

## Method Summary
RefCritic uses a two-stage training process: cold-start supervised fine-tuning (SFT) on curated seed critiques, followed by reinforcement learning with group relative policy optimization (GRPO) using dual rewards. The first reward optimizes judgment accuracy (R_j), while the second reward (R_r) is based on whether the policy model successfully refines its answer using the critic's feedback. Training data is filtered from NuminaMath-1.5, balancing correctness and difficulty, with at most two sampled responses per problem. The RL phase runs λ=0 (judgment only) for 600 steps, then λ=1 (full dual reward) for 300 steps, using 8 refinement rollouts per critique to calculate R_r.

## Key Results
- 6.8–7.2% gains in pass@1 after refinement on AIME24/25 and Olympiad benchmarks
- 3.6% gains in majority voting with 64 samples
- Outperforms step-level supervised approaches on ProcessBench for step-level error identification
- Generalizes to coding and science tasks with strong scaling and cross-model supervision

## Why This Works (Mechanism)

### Mechanism 1
Rewarding downstream refinement success forces the generation of actionable feedback rather than superficial error identification. The dual-reward system during RL creates selection pressure for critiques that contain sufficient reasoning and specific guidance to repair the policy model's logic.

### Mechanism 2
RL mitigates the "superficial reasoning" failure mode observed in SFT, where models learn to mimic critique style without verifying logic. The verifiable outcome reward (correctness of the judgment) forces the model to explore reasoning chains that reliably predict the ground truth.

### Mechanism 3
Training on solution-level refinement feedback induces step-level error localization capabilities without explicit step-level supervision. The requirement to generate a successful "repair" forces the model to attend to fine-grained error steps during its long Chain-of-Thought.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Understanding how on-policy sampling and reward normalization work is key to replicating the training loop. Quick check: Do you understand how to calculate a group-relative advantage score for a batch of sampled critiques?

- **The "Critic" vs. "Policy" Distinction in LLMs**: The architecture relies on a distinct separation where the Policy model generates solutions and the Critic model evaluates them. Quick check: Can you explain why training the Critic requires a fixed (or frozen) Policy model to ensure stable reward signals?

- **Chain-of-Thought (CoT) Scaling**: The paper emphasizes "Long CoT" and measures success by the growth of output token length. Quick check: How does temperature sampling affect the diversity and depth of CoT reasoning during the RL exploration phase?

## Architecture Onboarding

- **Component map**: Policy Model (P_φ) -> Critic Model (C_θ) -> Reward Calculator (R_j + λR_r) -> GRPO Trainer (updates C_θ)

- **Critical path**: The Refinement Reward Calculation is the bottleneck. For every critique generated during training, you must run the Policy Model m times (paper uses m=8) to see if it can fix the error.

- **Design tradeoffs**: Lambda (λ) Scheduling uses λ=0 initially for speed and stability, then switches to λ=1 to induce refinement capabilities. SFT Cold Start cannot be skipped as the critic needs correct formatting for the reward calculator.

- **Failure signatures**: "Right Judgment, Wrong Reasoning" (addressed by RL), Vague Feedback (addressed by Refinement Reward), Reward Hacking (paper mentions filtering this in data construction).

- **First 3 experiments**:
  1. SFT vs. RL Baseline: Train a critic with only SFT and verify the claim that it produces high judgment accuracy but low refinement gains
  2. Ablation on λ: Run the full RefCritic pipeline with λ=0 and measure the drop in "Pass@1 after Refinement" metrics
  3. Cross-Model Supervision: Test if RefCritic-14B can effectively critique and refine a stronger model (e.g., Qwen2.5-72B) to verify generalization

## Open Questions the Paper Calls Out

### Open Question 1
Can the RefCritic framework remain computationally tractable when scaling to models significantly larger than 14B parameters? The dual-reward RL framework requires significant computational resources, potentially limiting scalability for very large models.

### Open Question 2
To what extent does the refinement feedback mechanism generalize to domains outside of mathematical reasoning, such as commonsense or professional knowledge tasks? Generalizability to domains like commonsense reasoning or specialized professional contexts remains to be thoroughly investigated.

### Open Question 3
Why does training on solution-level supervision enable RefCritic to outperform step-level supervised baselines on fine-grained error localization (ProcessBench)? The mechanism behind this transfer is unclear and needs further investigation.

## Limitations
- Computational expense due to m=8 refinement rollouts per critique during RL training
- Reliance on specific data curation pipeline (NuminaMath-1.5 with strict filtering)
- Assumption that policy model has baseline capacity to understand and execute refinement instructions

## Confidence
- **High Confidence**: Empirical gains on AIME24/25 (6.8-7.2% Pass@1 improvement) and ProcessBench step-identification
- **Medium Confidence**: Claim that SFT alone produces "superficial reasoning" based on ablation study
- **Medium Confidence**: Cross-model generalization demonstrated but only for a single stronger model pair

## Next Checks
1. Ablation on Refinement Reward Weight (λ): Systematically test λ values in {0.25, 0.5, 0.75, 1.0} to quantify marginal benefit
2. Cross-Dataset Generalization: Evaluate RefCritic-trained models on a different mathematical reasoning dataset (e.g., GSM8K or MATH)
3. Step-Level Supervision Comparison: Train a control model with explicit step-level error labels and compare its ProcessBench performance against RefCritic's solution-level approach