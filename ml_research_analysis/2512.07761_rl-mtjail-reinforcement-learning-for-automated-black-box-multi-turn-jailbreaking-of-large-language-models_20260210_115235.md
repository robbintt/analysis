---
ver: rpa2
title: 'RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking
  of Large Language Models'
arxiv_id: '2512.07761'
source_url: https://arxiv.org/abs/2512.07761
tags:
- harmful
- multi-turn
- prompts
- attack
- jailbreak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents TROJail, a reinforcement learning framework
  for training automated attackers to perform black-box multi-turn jailbreak attacks
  on large language models. TROJail formulates the problem as multi-turn RL, directly
  optimizing the harmfulness of the final response while mitigating sparse supervision
  through two process rewards: over-harm penalization (avoiding triggers for model
  refusal) and semantic relevance progression (ensuring responses gradually align
  with the original harmful intent).'
---

# RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models
## Quick Facts
- arXiv ID: 2512.07761
- Source URL: https://arxiv.org/abs/2512.07761
- Reference count: 40
- RL-based framework that achieves significantly higher black-box jailbreak success rates than single-turn or turn-level optimization methods

## Executive Summary
RL-MTJail introduces a reinforcement learning framework for automating black-box multi-turn jailbreak attacks on large language models. The method formulates jailbreaking as a multi-turn RL problem, optimizing directly for the harmfulness of the final response while using process rewards to address sparse supervision. Two key process rewards are employed: over-harm penalization to avoid model refusal triggers, and semantic relevance progression to ensure responses gradually align with the original harmful intent. Experiments demonstrate that RL-MTJail significantly outperforms existing methods across multiple victim models and benchmarks, with improved attack success rates and robustness against increasing prompt difficulty.

## Method Summary
RL-MTJail frames multi-turn jailbreaking as a reinforcement learning problem where the attacker learns to generate a sequence of turns that ultimately elicits a harmful response from the victim model. The framework uses a transformer-based policy network that generates jailbreak prompts turn by turn. The reward function combines a final harmfulness reward with two process rewards: over-harm penalization (penalizing prompts likely to trigger model refusal) and semantic relevance progression (encouraging responses to gradually align with the harmful intent). The policy is trained using Proximal Policy Optimization (PPO) with entropy regularization to maintain exploration. The method operates in a black-box setting, requiring only the victim model's outputs without access to gradients or internal representations.

## Key Results
- RL-MTJail achieves significantly higher attack success rates than single-turn and turn-level optimization methods across multiple victim models
- The proposed process rewards (over-harm penalization and semantic relevance progression) are essential for performance, with ablations showing substantial drops without them
- Training on robust victim models leads to improved cross-model transferability, suggesting the attacker learns more generalizable strategies
- RL-MTJail maintains effectiveness even as prompt difficulty increases, demonstrating robustness to stronger safety measures

## Why This Works (Mechanism)
RL-MTJail succeeds by directly optimizing the multi-turn jailbreak trajectory rather than treating each turn independently. The RL framework allows the attacker to learn long-term strategies that gradually manipulate the conversation toward harmful outcomes while avoiding immediate triggers for model refusal. The process rewards provide dense feedback during training, enabling the policy to learn effective intermediate strategies despite the sparsity of successful jailbreaks. The entropy regularization prevents policy collapse and maintains exploration of diverse attack strategies.

## Foundational Learning
- **Reinforcement Learning**: Framework for sequential decision making where actions (prompt generations) lead to states (conversation progression) with associated rewards (harmfulness)
  - *Why needed*: Multi-turn jailbreaking requires optimizing sequences of actions rather than single decisions
  - *Quick check*: Verify that the policy network can learn to maximize cumulative reward over multiple turns

- **Proximal Policy Optimization (PPO)**: On-policy RL algorithm that optimizes policy parameters while constraining policy updates to prevent instability
  - *Why needed*: Provides stable training for the jailbreak policy while maintaining exploration
  - *Quick check*: Monitor policy entropy and loss stability during training

- **Process Rewards**: Intermediate rewards provided during trajectory execution to guide learning when final rewards are sparse
  - *Why needed*: Jailbreak success is rare, making pure final reward optimization inefficient
  - *Quick check*: Compare learning curves with and without process rewards

- **Semantic Relevance Progression**: Reward mechanism encouraging gradual alignment of responses with harmful intent
  - *Why needed*: Enables the attacker to build toward harmful outcomes without triggering immediate refusal
  - *Quick check*: Measure semantic drift between consecutive turns in successful attacks

- **Over-Harm Penalization**: Reward mechanism that penalizes prompts likely to trigger model refusal
  - *Why needed*: Prevents the policy from learning counterproductive strategies that immediately fail
  - *Quick check*: Track refusal rates across different training stages

- **Cross-Model Transferability**: Ability of attack strategies learned on one model to succeed on different models
  - *Why needed*: Enables attack generalization across diverse LLM architectures and safety implementations
  - *Quick check*: Test attack success rates on held-out victim models

## Architecture Onboarding
- **Component Map**: Policy Network (transformer) -> Turn Generator -> Victim Model -> Reward Calculator -> PPO Optimizer
- **Critical Path**: Policy generates turn → Victim responds → Rewards calculated → PPO updates policy → Repeat
- **Design Tradeoffs**: Black-box approach sacrifices gradient efficiency for broader applicability; multi-turn optimization increases attack success but requires more computation
- **Failure Signatures**: High refusal rates indicate over-harm penalization needs adjustment; low semantic progression suggests relevance reward is too weak; poor transfer indicates strategies are too model-specific
- **3 First Experiments**: 1) Single-turn vs multi-turn baseline comparison, 2) Ablation study removing each process reward, 3) Transferability test across different victim model families

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can multi-objective reinforcement learning jointly optimize attack effectiveness and diversity in multi-turn jailbreaks?
- Basis in paper: [explicit] The authors state in the Limitations section that future work could address the lack of explicit diversity optimization by incorporating multi-objective RL.
- Why unresolved: Current entropy regularization mitigates policy collapse but does not explicitly optimize for a diverse set of attack strategies.
- What evidence would resolve it: Experiments comparing current single-objective success rates against a multi-objective framework that quantifies distinct strategy clusters.

### Open Question 2
- Question: How do learned attack strategies perform against adversarially trained safety policies or adaptive defenses?
- Basis in paper: [explicit] The Limitations section notes that TROJail does not incorporate defensive mechanisms or adversarially trained safety policies.
- Why unresolved: Current experiments evaluate attacks against static victim models without co-evolutionary or adaptive defense training loops.
- What evidence would resolve it: Evaluation of attack success rates against victims fine-tuned specifically to detect and refuse TROJail-generated trajectories.

### Open Question 3
- Question: What specific linguistic or semantic patterns learned from robust victim models drive the improved cross-model transferability?
- Basis in paper: [inferred] The transferability analysis suggests training on robust models compels the attacker to learn generalizable strategies, but the underlying features of these strategies remain uncharacterized.
- Why unresolved: The paper observes higher out-of-domain performance but attributes it to a hypothesis without mechanistic proof.
- What evidence would resolve it: An interpretability analysis identifying prompt features that correlate with high transferability across diverse victim architectures.

## Limitations
- The evaluation focuses primarily on success rate metrics without deeper analysis of the semantic quality or coherence of generated jailbreaks
- The attack methodology requires significant computational resources for RL training, limiting practical deployment
- The paper does not address potential defenses or detection mechanisms that victim models might employ

## Confidence
- **High**: Core technical contribution and experimental results showing TROJail's superiority over baselines
- **Medium**: Claims about generalization across different model families and prompt categories
- **Medium**: Assertion that the proposed process rewards are necessary and sufficient for the observed performance gains

## Next Checks
1. Test TROJail's performance against models with active jailbreak detection mechanisms, including those that analyze conversation patterns across multiple turns
2. Evaluate whether the learned attack strategies transfer to unseen prompt categories and domains not included in the original training distribution
3. Assess the computational cost-benefit tradeoff by comparing attack success rates against the number of RL training iterations required