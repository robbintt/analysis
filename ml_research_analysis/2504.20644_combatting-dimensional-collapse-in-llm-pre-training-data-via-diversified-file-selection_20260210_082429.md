---
ver: rpa2
title: Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File
  Selection
arxiv_id: '2504.20644'
source_url: https://arxiv.org/abs/2504.20644
tags:
- selection
- training
- performance
- data
- disf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dimensional collapse in large
  language model (LLM) pre-training data selection, where methods focusing on specific
  domains improve task performance in those areas but cause overall degradation across
  diverse tasks. The authors propose a Diversified File Selection (DiSF) method that
  selects the most decorrelated text files by minimizing the Frobenius norm of the
  feature covariance matrix, achieving more uniform eigenvalues and enhanced diversity.
---

# Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection

## Quick Facts
- arXiv ID: 2504.20644
- Source URL: https://arxiv.org/abs/2504.20644
- Reference count: 40
- This paper addresses dimensional collapse in LLM pre-training by proposing a Diversified File Selection method that improves task performance across diverse domains

## Executive Summary
This paper addresses the problem of dimensional collapse in large language model pre-training data selection, where methods focusing on specific domains improve task performance in those areas but cause overall degradation across diverse tasks. The authors propose a Diversified File Selection (DiSF) method that selects the most decorrelated text files by minimizing the Frobenius norm of the feature covariance matrix, achieving more uniform eigenvalues and enhanced diversity. They use a classical greedy algorithm and analyze its approximation under γ-weakly submodular optimization.

## Method Summary
The paper proposes a Diversified File Selection (DiSF) method to combat dimensional collapse in LLM pre-training data selection. The approach selects text files by maximizing decorrelation, measured through the Frobenius norm of the feature covariance matrix. A greedy algorithm iteratively selects files that minimize this norm, ensuring diverse feature coverage. The method is theoretically analyzed under γ-weakly submodular optimization, providing approximation guarantees. The authors evaluate DiSF on TinyLlama architectures (120M to 1.1B parameters) across nine tasks from the Harness framework, demonstrating improved efficiency and performance compared to traditional pre-training approaches.

## Key Results
- DiSF selects only 1.5% of 590M training files in SlimPajama
- Achieves about 1.5x training efficiency and 5x data efficiency compared to full-data pre-training under a 50B token budget
- Outperforms baselines in overall task performance across diverse domains

## Why This Works (Mechanism)
The method works by addressing the fundamental problem of feature redundancy in pre-training data. By selecting files that minimize the Frobenius norm of the feature covariance matrix, DiSF ensures that the selected subset has maximum decorrelation between features. This prevents over-representation of certain feature directions while under-representing others, which is the core issue in dimensional collapse. The greedy algorithm provides a computationally efficient way to approximate the optimal solution while maintaining theoretical guarantees through γ-weakly submodularity analysis.

## Foundational Learning

**γ-weakly submodular optimization**: A framework for analyzing greedy algorithms' approximation guarantees. Why needed: To provide theoretical justification for the greedy selection approach. Quick check: Verify that the objective function satisfies the γ-weakly submodular property for the theoretical bounds to hold.

**Frobenius norm of covariance matrix**: A mathematical measure of feature correlation and redundancy. Why needed: Serves as the diversity metric to identify and eliminate redundant information. Quick check: Ensure the covariance matrix is well-conditioned and numerically stable during computation.

**Greedy algorithm approximation**: Iterative selection method with provable performance bounds. Why needed: Provides efficient computation while maintaining theoretical guarantees. Quick check: Monitor the marginal gain of each selection to ensure convergence behavior.

## Architecture Onboarding

**Component map**: Raw corpus -> Feature extraction -> Covariance matrix computation -> Greedy selection -> Diverse subset

**Critical path**: Feature extraction → Covariance matrix computation → Greedy selection loop → Model training

**Design tradeoffs**: Computational efficiency vs. selection quality; theoretical guarantees vs. practical performance; diversity vs. domain-specific performance

**Failure signatures**: Over-selection of redundant files (covariance matrix eigenvalues remain skewed); under-selection leading to poor coverage (eigenvalues too uniform); greedy algorithm getting stuck in local optima

**First experiments**: 1) Verify feature extraction produces meaningful representations; 2) Test greedy algorithm on synthetic data with known correlation structure; 3) Compare selected subset diversity against random selection using eigenvalue analysis

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, focusing instead on presenting its solution to dimensional collapse in LLM pre-training data selection.

## Limitations

- Evaluation focuses on narrow parameter range (120M-1.1B) on TinyLlama architectures, raising scalability concerns
- Claims of training and data efficiency are tied to specific 50B token budget that may not generalize
- Use of Frobenius norm may not fully capture semantic and functional diversity needed for optimal pre-training
- Theoretical guarantees rely on γ-weakly submodularity assumptions not empirically validated across different datasets

## Confidence

**High**: The mathematical formulation of the Diversified File Selection method and its theoretical analysis under γ-weakly submodularity

**Medium**: The empirical results showing improved performance over baselines within the tested parameter range

**Low**: Generalization claims regarding training efficiency and data efficiency across different model scales and training budgets

## Next Checks

1. Test DiSF's effectiveness on larger model sizes (7B-70B parameters) to verify scalability and identify any threshold effects
2. Evaluate the method's performance across different corpus compositions and sizes to assess robustness
3. Conduct ablation studies comparing the Frobenius norm-based diversity metric against alternative diversity measures in the context of downstream task performance