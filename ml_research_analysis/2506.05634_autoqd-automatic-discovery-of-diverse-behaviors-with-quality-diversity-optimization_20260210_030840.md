---
ver: rpa2
title: 'AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization'
arxiv_id: '2506.05634'
source_url: https://arxiv.org/abs/2506.05634
tags:
- policy
- policies
- behavioral
- learning
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoQD automatically generates behavioral descriptors for quality-diversity
  optimization in sequential decision-making tasks. The method embeds policy occupancy
  measures using random Fourier features to approximate Maximum Mean Discrepance distances,
  then extracts low-dimensional behavioral descriptors through calibrated weighted
  PCA.
---

# AutoQD: Automatic Discovery of Diverse Behaviors with Quality-Diversity Optimization

## Quick Facts
- arXiv ID: 2506.05634
- Source URL: https://arxiv.org/abs/2506.05634
- Authors: Saeed Hedayatian; Stefanos Nikolaidis
- Reference count: 40
- Primary result: AutoQD automatically generates behavioral descriptors for quality-diversity optimization in sequential decision-making tasks without hand-crafted descriptors.

## Executive Summary
AutoQD is a method for automatically generating behavioral descriptors in quality-diversity optimization for reinforcement learning. The method embeds policy occupancy measures using random Fourier features to approximate Maximum Mean Discrepancy distances, then extracts low-dimensional behavioral descriptors through calibrated weighted PCA. By eliminating the need for hand-crafted behavioral descriptors, AutoQD discovers diverse and high-performing policies without domain-specific knowledge. The approach was evaluated on six continuous control tasks and demonstrated superior performance compared to baselines, particularly in terms of quality-weighted Vendi scores and adaptability to changing dynamics.

## Method Summary
AutoQD generates behavioral descriptors by first embedding policy occupancy measures using Random Fourier Features (RFF) with a Gaussian kernel. These embeddings are then projected to low-dimensional descriptors using calibrated weighted PCA (cwPCA), which weights each policy's embedding by its normalized fitness before computing principal components. The method is designed to work with existing quality-diversity algorithms like CMA-MAE by providing the behavioral descriptors that define the archive grid. The entire process requires no hand-crafted behavioral descriptors and adapts to the discovered behaviors throughout training.

## Key Results
- AutoQD achieved higher quality-weighted Vendi scores than baselines in most environments (Ant, HalfCheetah, Hopper, Swimmer, Walker2d, BipedalWalker)
- Demonstrated superior adaptability to changing dynamics with friction and mass variations
- Eliminated the need for hand-crafted behavioral descriptors while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Occupancy measures provide a complete, theoretically grounded representation of policy behavior.
- Mechanism: The occupancy measure ρπ(s,a) = (1-γ)ΣγᵗP(St=s, At=a|π) captures the expected discounted visitation frequency of all state-action pairs under a policy. Because there exists a one-to-one correspondence between policies and their occupancy measures, two policies with identical occupancy measures must exhibit identical behaviors.
- Core assumption: The MDP's state-action space adequately captures behaviorally relevant features.
- Evidence anchors:
  - [Section 2.1]: "Importantly, there exists a one-to-one correspondence between policies and their occupancy measures, making occupancy measures a complete characterization of policy behavior."
  - [Section 3.1]: "occupancy measures fully characterize policy behavior, and MMD with a Gaussian kernel provides a metric over them"
  - [corpus] Related work on policy representations (arXiv:2601.22350) similarly leverages occupancy measures for policy representation.

### Mechanism 2
- Claim: Random Fourier Features (RFF) embeddings approximate true MMD distances between occupancy measures with bounded error that decreases exponentially in samples and embedding dimensions.
- Mechanism: RFF maps infinite-dimensional Gaussian kernel feature space to D-dimensional vectors via ϕ(s,a) = √(2/D)[cos(w₁ᵀ[s;a]+b₁), ..., cos(w_Dᵀ[s;a]+b_D)] where wᵢ ~ N(0, σ⁻²I), bᵢ ~ U(0,2π). The ℓ₂ distance between policy embeddings converges to true MMD as n (samples) and D (features) increase.
- Core assumption: The Gaussian kernel bandwidth σ appropriately captures the scale of meaningful behavioral differences.
- Evidence anchors:
  - [Theorem 1]: "Pr[|∥ϕ₁-ϕ₂∥₂ - MMD(ρ₁,ρ₂)| ≥ ¾ε] ≤ 2e^{-ncε²} + O(1/ε² exp(-Dε²/64(d+2))) + 6e^{-nε²/8]"
  - [Section 3.1]: "By increasing the number of samples (n) and the number of random features (D), we can arbitrarily shrink the error."

### Mechanism 3
- Claim: Fitness-weighted PCA on embeddings produces behavioral descriptors that prioritize meaningful variation among high-performing policies.
- Mechanism: Rather than standard PCA, cwPCA weights each policy's embedding by its normalized fitness before computing principal components. This biases the discovered behavioral axes toward variations that matter for task performance, then calibrates output to [-1,1]ᵏ for stable archive bounds.
- Core assumption: Behavioral variations among high-performing policies are more valuable to capture than variations among poor performers.
- Evidence anchors:
  - [Section 3.2]: "weighting them by their fitness, so that high-performing policies have greater influence on the principal directions. This biases the resulting components toward capturing behavior variation among strong policies"
  - [Appendix B]: "The 1/m term in Eq. 38 ensures a minimum contribution from each policy...while prioritizing those with higher fitness."

## Foundational Learning

- Concept: Occupancy Measures in MDPs
  - Why needed here: Core theoretical object AutoQD uses to represent behavior; understanding the (1-γ) normalization and discounted visitation is essential.
  - Quick check question: If γ=0.99, how does the occupancy measure weight a state-action pair visited at t=0 vs t=100?

- Concept: Maximum Mean Discrepancy (MMD)
  - Why needed here: AutoQD's embeddings are designed to approximate MMD distances; understanding kernel-based distribution comparison clarifies what "behavioral similarity" means mathematically.
  - Quick check question: Why does MMD with a Gaussian kernel define a proper metric (zero iff distributions identical)?

- Concept: Quality-Diversity Optimization (MAP-Elites, CMA-MAE)
  - Why needed here: AutoQD is a drop-in behavioral descriptor generator for existing QD algorithms; the archive structure and QD score objective contextualize why low-dimensional descriptors matter.
  - Quick check question: What happens to archive coverage if behavioral descriptor dimensionality increases from 4 to 20?

## Architecture Onboarding

- Component map:
Environment → Policy (θ) → Trajectories → Random Fourier Features (fixed w_i, b_i) → Policy Embedding ψ_π (Eq. 6) → Calibrated Weighted PCA (A, b) → Behavioral Descriptor desc(π) ∈ R^k → CMA-MAE Archive [grid over [-1.2, 1.2]^k]

- Critical path:
1. Initialize RFF parameters (w_i, b_i) once—these never update
2. Collect trajectories, compute ψ_π via discounted RFF averaging
3. Apply current affine map (Aψ_π + b) to get descriptor
4. At scheduled iterations (20, 50, 100, 200, 300), recompute A, b via cwPCA on current archive

- Design tradeoffs:
- Higher embedding dimension D → better MMD approximation but more memory/compute
- Higher descriptor dimension k → more behavioral resolution but exponentially larger archive (10^k cells with default settings)
- More trajectories per policy → more stable embeddings but lower sample efficiency
- Assumption: Paper uses D=100, k=4, γ=0.999 as defaults

- Failure signatures:
- Archive not filling: Check if calibration is collapsing all descriptors to narrow range
- Low diversity despite high QD score: cwPCA may be over-weighting a single high-performing policy's variation
- Embedding distances not correlating with behavioral similarity: Kernel bandwidth σ may be mismatched to state-action scale

- First 3 experiments:
1. **Sanity check**: On a simple environment (e.g., Swimmer), visualize embeddings of hand-crafted diverse policies (e.g., forward vs. backward locomotion) to verify embedding distances capture behavioral difference.
2. **Ablation on k**: Run AutoQD with k∈{1,2,3,4} on BipedalWalker, plotting QD score, Vendi score, and mean objective separately (see Appendix E Figure 5) to understand the quality-diversity tradeoff.
3. **Robustness test**: Evaluate whether discovered policies transfer to modified dynamics (vary friction/mass as in Section 4.4) to validate that discovered diversity corresponds to functional adaptability.

## Open Questions the Paper Calls Out

- **Open Question 1**: Does integrating AutoQD with gradient-based QD algorithms improve sample efficiency compared to the current evolutionary CMA-MAE backbone?
  - Basis in paper: [explicit] "Future directions include integrating AutoQD with gradient-based QD methods to improve sample efficiency..."
  - Why unresolved: The current implementation relies solely on CMA-MAE, and gradient-based methods were not evaluated.
  - What evidence would resolve it: Comparative experiments showing sample complexity and performance of AutoQD when combined with algorithms like PGA-MAP-Elites.

- **Open Question 2**: Can AutoQD effectively generate behavioral descriptors from high-dimensional image observations?
  - Basis in paper: [explicit] "Future directions include... extending the approach to environments with image-based observations."
  - Why unresolved: The method currently computes embeddings using low-dimensional state vectors, and it is unclear how random Fourier features would scale to raw pixels.
  - What evidence would resolve it: Demonstration of AutoQD discovering diverse policies in visual control tasks (e.g., Atari or vision-based robotics) without ground-truth state access.

- **Open Question 3**: Would dynamically adapting the kernel bandwidth during training result in more meaningful behavioral descriptors than the fixed bandwidth approach?
  - Basis in paper: [explicit] "The choice of kernel bandwidth affects the sensitivity... dynamically adapting it during training could allow the embeddings to better capture behavioral distinctions."
  - Why unresolved: The authors used a fixed bandwidth hyperparameter throughout the experiments.
  - What evidence would resolve it: An ablation study comparing fixed vs. learned/adaptive bandwidth schedules on the Quality-Weighted Vendi Score.

## Limitations

- The method assumes state-action spaces capture behaviorally relevant features, which may fail in tasks with partial observability or irrelevant state dimensions.
- The fixed kernel bandwidth heuristic may not generalize across environments with different scales.
- cwPCA's fitness weighting could prematurely collapse behavioral diversity if early high-performing policies dominate.

## Confidence

- **High**: Occupancy measure theory as a complete policy representation (Theorem 1's MMD approximation bound is mathematically rigorous)
- **Medium**: RFF embedding quality in practice (exponential convergence in theory but sample efficiency in stochastic environments untested)
- **Low**: Fitness-weighted PCA consistently prioritizing meaningful behavioral variation (empirical evidence shows qVS improvements but the mechanism's robustness to initial conditions unclear)

## Next Checks

1. Test AutoQD on a partially observable task (e.g., navigation with partial vision) to evaluate the occupancy measure assumption.
2. Compare fixed vs. adaptive kernel bandwidth selection across environments to assess sensitivity to σ choice.
3. Evaluate cwPCA behavior when initialized with policies from different seeds to measure dependence on early archive composition.