---
ver: rpa2
title: Including local feature interactions in deep non-negative matrix factorization
  networks improves performance
arxiv_id: '2503.20398'
source_url: https://arxiv.org/abs/2503.20398
tags:
- neural
- layer
- while
- page
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a deep learning architecture that integrates\
  \ non-negative matrix factorization (NMF) with convolutional neural networks (CNNs)\
  \ to improve classification performance while maintaining biological plausibility.\
  \ The key innovation is combining NMF modules, which capture biologically realistic\
  \ positive interactions, with 1\xD71 convolutional layers that provide local feature\
  \ mixing analogous to cortical inhibitory processing."
---

# Including local feature interactions in deep non-negative matrix factorization networks improves performance

## Quick Facts
- arXiv ID: 2503.20398
- Source URL: https://arxiv.org/abs/2503.20398
- Reference count: 13
- This paper presents a deep learning architecture that integrates non-negative matrix factorization (NMF) with convolutional neural networks (CNNs) to improve classification performance while maintaining biological plausibility.

## Executive Summary
This paper introduces Convolutional NMF (CNMF), a hybrid deep learning architecture that combines non-negative matrix factorization layers with 1×1 convolutional layers to achieve better image classification performance than standard CNNs while maintaining biological plausibility. The key innovation is integrating NMF modules, which capture biologically realistic positive interactions, with 1×1 convolutions that provide local feature mixing analogous to cortical inhibitory processing. The proposed architecture achieves 85.1% accuracy on CIFAR-10, outperforming both vanilla CNNs (81%) and CNMF alone (81.5%) while using similar parameter counts.

## Method Summary
The CNMF architecture consists of four repeating blocks, each containing an NMF layer followed by a 1×1 convolutional layer, batch normalization, and ReLU activation. The NMF layers enforce non-negative weights and activations to create parts-based representations through excitatory interactions, while the 1×1 convolutions allow for local feature mixing through both excitatory and inhibitory effects. Training uses a composite loss function combining cross-entropy and reconstruction error, with an efficient approximate back-propagation method to reduce computational overhead from the iterative NMF updates.

## Key Results
- CNMF with 1×1 convolutions achieves 85.1% accuracy on CIFAR-10
- Outperforms vanilla CNN baseline (81.0%) and CNMF alone (81.5%)
- Uses similar parameter counts to baseline models
- Demonstrates that biologically inspired constraints can enhance deep network performance when properly implemented

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Enforcing non-negativity in long-range connections (via NMF) creates parts-based representations, but limits the network to purely additive (excitatory) feature interactions.
- **Mechanism:** The Convolutional NMF (CNMF) layer constrains weights and activations to be non-negative ($W, H \ge 0$). This forces the network to decompose inputs into additive "parts" rather than allowing features to cancel each other out via negative weights.
- **Core assumption:** Visual features can be effectively constructed through the superposition of positive, sparse components.
- **Evidence anchors:** [Abstract] "NMF captures the biological constraint of positive long-range interactions... realized by excitatory synapses." [Page 3] "NMF... satisfies two key biological constraints: the positivity of long-range neural interactions and the tendency toward sparse representations."
- **Break condition:** If the target features require significant contrast normalization or subtractive encoding (e.g., defining an edge by the *absence* of surrounding light) in a single layer, a pure CNMF layer may fail to converge or require excessive complexity.

### Mechanism 2
- **Claim:** Inserting 1×1 convolutions after CNMF layers recovers computational expressiveness by re-introducing local inhibition (negative weights), bridging the gap between biological constraints and deep learning performance.
- **Mechanism:** The 1×1 convolution layers allow for both positive and negative weights. They take the purely positive outputs of the CNMF layers and apply local "mixing" or modulation. This mimics cortical circuits where long-range excitatory projections are modulated by local inhibitory interneurons.
- **Core assumption:** The brain combines long-range excitatory signaling with local inhibitory microcircuits to process complex information.
- **Evidence anchors:** [Page 4] "Missing in current networks using NMF are local interactions that include inhibition... subsequent 1x1 convolutional layers... realize general local interactions among the features."
- **Break condition:** If the 1×1 convolution layer is removed, the architecture theoretically reverts to the performance of a pure CNMF network, which the paper notes struggles to match vanilla CNNs on clean data (Page 14).

### Mechanism 3
- **Claim:** Optimizing NMF layers via supervised back-propagation (rather than unsupervised reconstruction) preserves "non-dominant" features critical for classification.
- **Mechanism:** Standard NMF updates weights to minimize reconstruction error, often ignoring class-relevant features that are statistically non-dominant. By treating NMF weights as learnable parameters $W = |U|$ updated via gradient descent on classification loss, the network retains features useful for the task even if they are not the most prominent statistical components.
- **Core assumption:** Classification-relevant features are not always the most statistically dominant features in the raw pixel space.
- **Evidence anchors:** [Page 22] "NMF's unsupervised learning rule... primarily captures dominant features... non-dominant but class-relevant features (CN) are progressively filtered out."
- **Break condition:** If the training objective is switched to pure reconstruction (unsupervised), the performance gain over vanilla CNNs should vanish, and accuracy may drop significantly.

## Foundational Learning

- **Concept:** **Non-negative Matrix Factorization (NMF)**
  - **Why needed here:** This is the fundamental building block of the paper's "biologically plausible" layer. You must understand that NMF decomposes data into two positive matrices ($V \approx WH$), resulting in additive, parts-based features (e.g., detecting a "nose" and "eyes" separately rather than a holistic "face").
  - **Quick check question:** If you feed an image of a face into NMF, would it likely learn a single "face" template or distinct "eye," "nose," and "mouth" templates?

- **Concept:** **Implicit Layers / Fixed-Point Iteration**
  - **Why needed here:** Unlike standard CNN layers which compute output in one pass, the CNMF layer iterates ($N$ times) until the activation converges (Eq 6). This models the "settling" dynamics of biological neurons but complicates back-propagation.
  - **Quick check question:** Why does iterating a layer $N$ times make standard back-propagation computationally expensive (memory-wise)?

- **Concept:** **Dale’s Principle**
  - **Why needed here:** This biological constraint states that a neuron is either excitatory or inhibitory, not both. The paper uses this to justify splitting the architecture: CNMF handles the excitatory (positive) part, and 1×1 Conv handles the mixing/inhibition.
  - **Quick check question:** How does the 1×1 convolution layer technically violate Dale's Principle if viewed as a single neuron, but satisfy it if viewed as a microcircuit?

## Architecture Onboarding

- **Component map:** Input Image → CNMF Layer → 1×1 Conv → Batch Norm → ReLU → (repeat 4x) → Flatten → Linear Classifier
- **CNMF Layer Internals:**
  - Forward: Iterates Eq. 6 ($h_i$) for $N$ steps
  - Weights: Stored as auxiliary matrix $U$, transformed to $W$ via $W_{s,i} = |U_{s,i}|$ to enforce non-negativity

- **Critical path:** The transformation $U \to W$ (Eq 20) is the single most critical implementation detail. If you update $W$ directly with gradient descent, it will eventually become negative, breaking the NMF mechanism. The "Approximated Back-propagation" (Eq 18) is necessary for speed. Without it, the authors note the network is ~29x slower than a CNN.

- **Design tradeoffs:**
  - **Biological Fidelity vs. Speed:** The iterative NMF forward pass is slower than a standard Conv layer
  - **Accuracy vs. Plasticity:** The paper claims superior accuracy (85.1% vs 81%), but this relies on a specific composite loss function (CrossEntropy + MSE). Standard loss setups may not yield the same gains

- **Failure signatures:**
  - **Symptom:** Accuracy stagnates around 30-32%
  - **Diagnosis:** The CNMF weights are likely updating via the unsupervised "local learning rule" (Eq 4) instead of back-propagation, or the 1×1 mixing layer is missing
  - **Symptom:** Memory explodes during training
  - **Diagnosis:** The approximate back-prop was not implemented, and the system is trying to store the graph for $20+$ iterations per layer

- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run the CNMF network *without* the 1×1 Conv layers on CIFAR-10. Verify that performance drops to roughly the "CNMF alone" baseline (81.5%) or lower
  2. **Approximation Validation:** Compare the validation loss of the "Approximate Backprop" model against a model using standard Autograd (on a smaller dataset/subset). Confirm the approximation does not diverge from the true gradient
  3. **Hyperparameter Sensitivity:** Vary the number of iterations $N$ in the CNMF forward pass (Eq 6). Determine if $N=20$ is sufficient or if performance degrades with fewer iterations

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can local, biologically plausible learning rules be developed for NMF modules that preserve class-relevant non-dominant features (CN) without relying on global back-propagation?
- **Basis in paper:** [explicit] Section 4.2 explicitly analyzes that local NMF learning rules filter out non-dominant class-relevant features (CN), causing a drop in accuracy to 32%, whereas back-propagation successfully retains them.
- **Why unresolved:** The paper relies on back-propagation to achieve high performance (85.1%), effectively bypassing the biological constraint of local learning to solve the feature selection problem.
- **What evidence would resolve it:** A demonstration of a purely local update rule achieving comparable accuracy to the back-propagation baseline by retaining CN features.

### Open Question 2
- **Question:** Does the performance advantage of CNMF with local mixing persist on high-resolution datasets (e.g., ImageNet) or deeper architectures?
- **Basis in paper:** [inferred] The experimental scope is limited to the CIFAR-10 dataset and a specific shallow architecture (4 blocks), leaving scalability unverified.
- **Why unresolved:** The efficiency of the proposed approximate back-propagation is demonstrated on small-scale images; it is unknown if the memory/time savings or accuracy gains translate to larger, more complex data distributions.
- **What evidence would resolve it:** Benchmarking the CNMF + 1x1 Conv architecture against standard ResNets or Transformers on ImageNet.

### Open Question 3
- **Question:** What is the sensitivity of classification accuracy to the number of NMF iterations ($N$) performed during the forward pass?
- **Basis in paper:** [inferred] The paper notes that $N$ usually ranges from 20 to 100, but the reported results rely on an approximate back-propagation method that decouples the backward pass from these iterations.
- **Why unresolved:** While the paper shows approximate back-prop is faster, it does not extensively explore how varying $N$ (convergence depth) affects the final classification error in the combined CNMF+CNN setup.
- **What evidence would resolve it:** Ablation studies showing test accuracy curves as a function of $N$ for both the proposed architecture and baseline CNNs.

## Limitations

- The biological plausibility claims rely heavily on analogical reasoning rather than direct experimental validation in neural systems
- The performance improvement (85.1% vs 81% baseline) is modest and may not generalize across different architectures or datasets
- The approximate back-propagation method, while efficient, lacks formal error bounds or validation of gradient accuracy

## Confidence

- **High confidence:** The core mathematical framework of CNMF layers and their implementation via W = |U| transformation is well-specified and reproducible
- **Medium confidence:** The architectural design choices (layer configurations, hyperparameters) are detailed enough for reproduction but contain some ambiguities
- **Low confidence:** The biological plausibility claims and their direct connection to the performance gains are more speculative than demonstrated

## Next Checks

1. **Ablation study validation:** Reproduce the CNMF-only network (without 1×1 convolutions) to verify the claimed 81.5% baseline performance
2. **Gradient approximation accuracy:** Compare the approximate back-propagation method against exact gradients on a small-scale problem to quantify approximation error
3. **Biological plausibility test:** Implement the architecture on simpler datasets where parts-based representations are known to work (e.g., MNIST) to verify the NMF mechanism functions as intended before scaling to CIFAR-10