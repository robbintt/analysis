---
ver: rpa2
title: Process-based Self-Rewarding Language Models
arxiv_id: '2503.03746'
source_url: https://arxiv.org/abs/2503.03746
tags:
- reasoning
- step
- data
- llms
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a process-based self-rewarding method for language
  models that introduces step-wise LLM-as-a-judge and step-wise preference optimization
  into the self-rewarding framework. The method addresses the limitations of existing
  self-rewarding algorithms in mathematical reasoning scenarios by enabling models
  to conduct fine-grained step-by-step reasoning and evaluation.
---

# Process-based Self-Rewarding Language Models

## Quick Facts
- **arXiv ID:** 2503.03746
- **Source URL:** https://arxiv.org/abs/2503.03746
- **Reference count:** 17
- **Primary result:** Process-based self-rewarding method achieves up to 60.6% accuracy on 72B models across mathematical benchmarks

## Executive Summary
This paper introduces a process-based self-rewarding method for language models that incorporates step-wise LLM-as-a-judge and step-wise preference optimization. The approach addresses limitations in existing self-rewarding algorithms by enabling fine-grained step-by-step reasoning and evaluation for mathematical problems. Experiments on Qwen2.5-Math models show significant performance improvements across multiple math benchmarks, with accuracy increasing from 51.9% to 55.7% on 7B models and from 48.6% to 60.6% on 72B models. The results demonstrate that LLMs can effectively improve their mathematical reasoning capabilities through iterative self-rewarding at the step level.

## Method Summary
The framework trains a single model to perform both reasoning and evaluation roles. It starts with cold-start initialization using EFT (evaluation fine-tuning) and IFT (instruction fine-tuning) data to establish judge capability. The model then generates step-wise preference pairs through iterative self-rewarding: for each problem, it generates multiple candidate steps, uses step-wise pairwise comparison to score them, and selects the best and worst examples for preference optimization. This process repeats across multiple iterations, with each iteration providing increasingly refined preference data for training.

## Key Results
- 72B model achieves 60.6% average accuracy across benchmarks after four iterations (up from 48.6%)
- 7B model improves from 51.9% to 55.7% average accuracy
- Step-wise pairwise comparison shows 0.84 consistency and 0.88 human agreement vs. 0.72 and 0.32 for solution scoring
- Judge accuracy remains high (92.2% for 7B, 95.6% for 72B at M4) despite no additional EFT data after initialization

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Step-wise pairwise comparison provides more reliable reward signals than holistic solution scoring for multi-step mathematical reasoning.
- **Mechanism:** Breaking evaluation into individual reasoning steps reduces cognitive load on the judge model. Instead of assessing an entire solution's quality (which conflates multiple error types), the model compares two candidate next-steps given the same prefix. This pairwise framing is more stable than absolute scoring because relative judgments are easier than assigning calibrated numerical scores.
- **Core assumption:** Errors in mathematical reasoning are localized—individual step quality is identifiable independent of full-solution correctness.
- **Evidence anchors:** Appendix B (Table 6) shows step-wise pairwise comparison achieves 0.84 consistency and 0.88 human agreement vs. 0.72 and 0.32 for solution scoring.

### Mechanism 2
- **Claim:** Iterative self-rewarding creates compounding improvements because better reasoning enables better evaluation, and better evaluation produces higher-quality preference data.
- **Mechanism:** The pipeline trains a single model to perform two roles—reasoner and judge. After initialization (M1), each iteration generates new preference pairs using the current model's judgment, then applies step-wise DPO. As reasoning improves, the model encounters more diverse and challenging solution paths. Simultaneously, improved judgment quality means preference pairs more accurately reflect true step quality, providing cleaner training signal.
- **Core assumption:** The model's self-generated preference data maintains sufficient quality to avoid reward hacking or distribution collapse.
- **Evidence anchors:** Table 2 shows 72B model gains +1.1% to +12.5% across benchmarks from M1→M4.

### Mechanism 3
- **Claim:** Cold-start initialization with carefully constructed EFT+IFT data establishes sufficient judge accuracy to bootstrap the self-rewarding loop.
- **Mechanism:** EFT data (4,679 samples) trains the model on step-wise comparison via PRM-filtered pairs verified by GPT-o1. IFT data (28,889 samples) teaches the "Step n:" output format. This dual training creates distinct task patterns, reducing interference. High initial judge accuracy (~93-96%) prevents early-iteration collapse.
- **Core assumption:** A small external supervision set can establish robust enough evaluation capability that subsequent iterations require no additional human/RM guidance.
- **Evidence anchors:** Table 3 shows M1 achieves 92.8% (7B) and 95.6% (72B) judge accuracy after initialization.

## Foundational Learning

- **Direct Preference Optimization (DPO)**
  - Why needed here: Core training algorithm for preference learning. Understanding how DPO converts pairwise preferences into policy updates without an explicit reward model is essential.
  - Quick check question: Can you explain why DPO eliminates the need for a separate reward model during training?

- **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The framework operates on step-by-step reasoning traces. Understanding how CoT decomposes problems and why intermediate steps matter for verifier training.
  - Quick check question: How does step-level supervision differ from outcome-level supervision for mathematical reasoning?

- **LLM-as-a-Judge**
  - Why needed here: The model evaluates its own outputs. Understanding prompt design for evaluation, pairwise comparison formats, and known biases (position, length).
  - Quick check question: What biases should you control for when using an LLM to compare two candidate responses?

## Architecture Onboarding

- **Component map:**
  ```
  M0 (Base Model: Qwen2.5-Math)
       ↓ [EFT + IFT initialization]
  M1 (Cold-start model: reasoning + judge)
       ↓ [Search + Self-Judge → Preference Pairs → Step-DPO]
  M2 → M3 → M4 (Iterative improvement)
  ```

- **Critical path:**
  1. EFT data quality → M1 judge accuracy → PPD quality → iteration effectiveness
  2. Search width (w=6) and temperature (T=0.5) → candidate diversity → preference pair informativeness

- **Design tradeoffs:**
  - Wider search (more candidates) improves best/worst discrimination but increases compute
  - More iterations may yield diminishing returns—paper stops at M4 due to resource constraints
  - Step granularity: too fine = trivial comparisons; too coarse = loses granularity benefit

- **Failure signatures:**
  - Judge accuracy drops significantly between iterations (sign of capability forgetting)
  - Preference pairs show max(Score)=min(Score) frequently (judge cannot distinguish quality)
  - Performance plateaus or declines after M2 (common in original self-rewarding on math)

- **First 3 experiments:**
  1. **Reproduce M1 initialization**: Train Qwen2.5-Math-7B on provided EFT+IFT data, verify judge accuracy reaches ~92% on held-out test set.
  2. **Single-iteration sanity check**: Generate PPD(M1) for 100 problems, train to get M2, compare GSM8k accuracy. Should see modest improvement over M1.
  3. **Ablate search width**: Compare w=2 vs w=6 on preference pair quality (measured by human annotation on a sample). Confirm wider search produces more discriminative pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does model performance and stability evolve beyond four self-rewarding iterations?
- **Basis in paper:** The authors state in the Limitations section that due to "limited resources," they only conducted experiments from M1 to M4, and suggest exploring the "impact of iteration count" in future work.
- **Why unresolved:** It remains unclear if the upward trend in math accuracy continues indefinitely, plateaus, or eventually succumbs to reward hacking/model collapse at scale.
- **What evidence would resolve it:** An ablation study extending the iterative training pipeline to 10 or 20 iterations while tracking benchmark accuracy and LLM-as-a-Judge consistency.

### Open Question 2
- **Question:** To what extent does the quality of the cold-start initialization data determine the success of the self-rewarding loop?
- **Basis in paper:** The authors note that "basic capabilities of initialized M1 model directly influence the effectiveness" and suggest that "utilizing more high-quality data to initialize LLMs more adequately may lead to stronger performance."
- **Why unresolved:** The framework currently relies on a specific PRM and GPT-o1 for initialization (M1), making it difficult to separate the model's self-improvement potential from the quality of its starting conditions.
- **What evidence would resolve it:** Comparisons of the M4 model performance when initialized with varying volumes and qualities of EFT and IFT data.

### Open Question 3
- **Question:** Can the process-based self-rewarding framework generalize to non-mathematical domains requiring complex reasoning, such as code generation or agentic planning?
- **Basis in paper:** The experimental setup and the specific design of the step-wise LLM-as-a-Judge prompts are tailored exclusively to mathematical reasoning benchmarks.
- **Why unresolved:** While the original Self-Rewarding paradigm applied to instruction following, this process-based variant depends on distinct "step-wise" logic that may not map cleanly to domains where steps are not strictly sequential or numerical.
- **What evidence would resolve it:** Applying the identical training pipeline to coding benchmarks (e.g., HumanEval, MBPP) or multi-turn dialogue tasks without modifying the core algorithm.

## Limitations

- The method's success critically depends on the quality of cold-start initialization data, with basic capabilities of the initialized M1 model directly influencing effectiveness.
- Computational resource constraints limited experiments to only four iterations, leaving uncertainty about performance evolution at larger scales.
- The framework is specifically designed for mathematical reasoning and may not generalize to domains with more complex step interdependencies.

## Confidence

- **High confidence**: Step-wise pairwise comparison mechanism - directly supported by quantitative consistency metrics (0.84 vs 0.72) and human agreement data (0.88 vs 0.32).
- **Medium confidence**: Iterative self-rewarding compounding effects - supported by performance gains but theoretical justification is limited.
- **Medium confidence**: Cold-start initialization sufficiency - supported by M1 judge accuracy but lacks systematic ablation studies on initialization quality.

## Next Checks

1. **Initialization sensitivity test**: Train multiple M1 models with varying EFT data quality/quality thresholds and measure how M2 performance varies across initialization quality levels.
2. **Step granularity ablation**: Systematically vary step segmentation granularity (e.g., 2-step vs 4-step vs full-step) and measure impact on judge consistency and downstream reasoning performance.
3. **Cross-domain validation**: Apply the same framework to non-mathematical reasoning tasks (e.g., scientific question answering or logical reasoning) and measure whether step-wise pairwise comparison still outperforms holistic scoring.