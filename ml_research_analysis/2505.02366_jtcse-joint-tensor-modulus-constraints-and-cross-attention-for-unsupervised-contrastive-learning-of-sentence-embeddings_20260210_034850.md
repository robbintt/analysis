---
ver: rpa2
title: 'JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised
  Contrastive Learning of Sentence Embeddings'
arxiv_id: '2505.02366'
source_url: https://arxiv.org/abs/2505.02366
tags:
- uni00000048
- uni00000003
- uni00000051
- uni0000004c
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitations of unsupervised contrastive
  learning for sentence embeddings, specifically the neglect of modulus features in
  semantic representations and insufficient attention to CLS tokens in BERT-like models.
  The authors propose a novel framework, JTCSE, which incorporates two key innovations:
  a modulus-constrained training objective to strengthen alignment between positive
  samples and a cross-attention mechanism to enhance the model''s attention to CLS
  tokens and optimize CLS pooling.'
---

# JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings

## Quick Facts
- **arXiv ID**: 2505.02366
- **Source URL**: https://arxiv.org/abs/2505.02366
- **Reference count**: 40
- **Primary result**: Outperforms existing methods on STS tasks with novel modulus constraints and cross-attention mechanism

## Executive Summary
JTCSE addresses key limitations in unsupervised contrastive learning for sentence embeddings by incorporating modulus features and enhancing CLS token attention. The framework introduces a modulus-constrained training objective that strengthens alignment between positive samples while maintaining semantic fidelity. Additionally, a cross-attention mechanism is implemented to address the "attention sinking" problem in BERT-like models, optimizing CLS pooling for better semantic representation.

The method demonstrates superior performance across seven semantic textual similarity benchmarks and shows strong generalization in zero-shot evaluations across more than 130 downstream tasks. JTCSE achieves state-of-the-art results compared to baselines like EDFSE and RankCSE, validating the effectiveness of its dual approach to improving unsupervised sentence embedding learning.

## Method Summary
The JTCSE framework combines tensor-modulus constraints with cross-attention mechanisms to enhance unsupervised contrastive learning of sentence embeddings. The modulus constraint strengthens the alignment between positive samples by incorporating magnitude information into the contrastive loss, addressing the limitation of existing methods that focus solely on directional similarity. The cross-attention mechanism enhances the model's attention to CLS tokens, mitigating the attention sinking problem in BERT-like architectures and optimizing CLS pooling for semantic representation. The framework leverages pre-trained BERT-like models as base encoders and introduces joint training objectives that balance alignment, uniformity, and modulus preservation.

## Key Results
- Achieves state-of-the-art performance on seven semantic textual similarity tasks
- Demonstrates strong zero-shot generalization across 130+ downstream tasks
- Shows consistent performance improvements over baselines like EDFSE and RankCSE

## Why This Works (Mechanism)
The framework's effectiveness stems from addressing two fundamental limitations in unsupervised contrastive learning: the neglect of modulus information in semantic representations and insufficient attention to CLS tokens in BERT-like models. By incorporating modulus constraints, JTCSE strengthens the alignment between positive samples while preserving semantic magnitude information. The cross-attention mechanism enhances the model's focus on CLS tokens, optimizing CLS pooling and mitigating the attention sinking problem. These innovations work synergistically to produce more discriminative and semantically rich sentence embeddings.

## Foundational Learning
- **Contrastive Learning**: Self-supervised learning paradigm that pulls positive pairs together and pushes negative pairs apart. Needed to understand the baseline approach being improved.
- **Modulus Constraints**: Incorporation of magnitude information into embedding space optimization. Needed to grasp how JTCSE preserves semantic magnitude.
- **Cross-Attention Mechanism**: Attention mechanism that allows tokens to attend to each other bidirectionally. Needed to understand how CLS token attention is enhanced.
- **Attention Sinking Problem**: Tendency of BERT-like models to allocate disproportionate attention to lower layers. Needed to appreciate why CLS token attention needs enhancement.
- **CLS Pooling**: Method of extracting sentence representations using the CLS token. Needed to understand how sentence embeddings are generated.

## Architecture Onboarding
- **Component Map**: Input sentences -> BERT-like encoder -> Cross-attention module -> Modulus-constrained contrastive loss -> Sentence embeddings
- **Critical Path**: Input -> Encoder -> Cross-attention -> Modulus constraint application -> Output embeddings
- **Design Tradeoffs**: Balance between alignment and uniformity vs. modulus preservation; computational overhead of cross-attention vs. performance gains
- **Failure Signatures**: Degraded performance on STS tasks without modulus constraints; attention sinking issues without cross-attention mechanism
- **First Experiments**: 1) Ablation study removing modulus constraints, 2) Evaluation with standard CLS pooling only, 3) Testing on single STS benchmark without zero-shot evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation on out-of-domain datasets beyond standard STS benchmarks
- Computational overhead of cross-attention mechanism not thoroughly analyzed
- Potential limitations when applying to transformer architectures beyond BERT-like models

## Confidence
- **Modulus-constrained training effectiveness**: High - consistent performance improvements across multiple metrics
- **Cross-attention mechanism contribution**: High - positive correlation with CLS energy weights and downstream performance
- **Generalization robustness**: Low - not explicitly tested under domain shifts or adversarial conditions

## Next Checks
1. Extensive testing on out-of-domain datasets to assess generalization beyond STS benchmarks
2. Evaluation of computational efficiency and memory requirements for large-scale deployment
3. Investigation of framework performance under adversarial conditions and noisy inputs