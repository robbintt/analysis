---
ver: rpa2
title: Generative Value Conflicts Reveal LLM Priorities
arxiv_id: '2509.25369'
source_url: https://arxiv.org/abs/2509.25369
tags:
- value
- values
- user
- scenario
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONFLICTSCOPE, an automated pipeline to evaluate
  how large language models (LLMs) prioritize different values under conflict. The
  method automatically generates scenarios where models face conflicts between pairs
  of user-defined values, evaluates their responses in both multiple-choice and open-ended
  settings, and fits a Bradley-Terry model to elicit value rankings.
---

# Generative Value Conflicts Reveal LLM Priorities

## Quick Facts
- arXiv ID: 2509.25369
- Source URL: https://arxiv.org/abs/2509.25369
- Reference count: 40
- Key outcome: CONFLICTSCOPE pipeline automatically generates value conflict scenarios, evaluates LLM responses, and elicits value rankings, revealing that models prioritize protective values in MCQ evaluation but user-facing values in open-ended evaluation

## Executive Summary
This paper introduces CONFLICTSCOPE, an automated pipeline to evaluate how large language models prioritize different values under conflict. The method generates scenarios where models face conflicts between pairs of user-defined values, evaluates their responses in both multiple-choice and open-ended settings, and fits a Bradley-Terry model to elicit value rankings. Experiments across three value sets show that CONFLICTSCOPE outperforms existing moral decision-making datasets at eliciting inter-model disagreement, indicating more morally challenging scenarios. The study reveals that models shift from supporting protective values like harmlessness in multiple-choice evaluation to supporting personal values like user autonomy in open-ended evaluation, and that system prompting with detailed value rankings can moderately steer LLM behavior.

## Method Summary
CONFLICTSCOPE is a three-stage pipeline that generates value conflict scenarios, filters them using LLM judges, and evaluates target models' responses to elicit value rankings. The process begins with two-stage scenario generation using Claude 3.5 Sonnet with four prompt templates that vary benefit/harm severity. Generated scenarios undergo six-dimensional filtering (Realism, Specificity, Feasibility, Impossibility, Value-Guidedness, Genuine Dilemma) using GPT-4.1 as judge. Target models are then evaluated in both multiple-choice (binary action selection) and open-ended (simulated user interaction) settings. Finally, a Bradley-Terry model fits pairwise preferences from the responses to produce a complete ranking of values for each model.

## Key Results
- CONFLICTSCOPE achieves 35.2% Observed Agreement (inter-model agreement), significantly lower than existing moral datasets (43.8-60.8%), indicating more challenging scenarios
- All models except Claude show substantial shifts toward protective values when moving from open-ended to multiple-choice evaluation
- System prompting with explicit value hierarchies improves alignment with target rankings by 14% on average across models and value sets
- Models consistently prioritize personal values like user autonomy over protective values like harmlessness in open-ended evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Top-down generation from value pairs produces more morally challenging scenarios than post-hoc labeling of existing data
- Mechanism: The pipeline generates scenarios by explicitly sampling value pairs and prompting an LLM to create conflicts where each value recommends a different action, rather than collecting scenarios first and labeling values afterward
- Core assumption: Explicitly targeting value pairs creates harder tradeoffs than naturally-occurring scenarios where values may not genuinely conflict
- Evidence anchors:
  - [abstract] "Given a user-defined value set, CONFLICTSCOPE automatically generates scenarios in which a language model faces a conflict between two values sampled from the set."
  - [Section 3.1] Describes staged generation with four prompt templates varying benefit/harm severity to mitigate biases
  - [corpus] Weak direct corpus support; related work (AIRiskDilemmas) similarly generates dilemmas but with different methodology

### Mechanism 2
- Claim: Open-ended evaluation reveals different value prioritization than multiple-choice evaluation
- Mechanism: In MCQ, models explicitly select an action, but in open-ended settings, a simulated user prompts the model which then generates a free-text response. The judge LLM determines which action the response more closely resembles
- Core assumption: The judge LLM accurately classifies which value the model's open-ended response supports
- Evidence anchors:
  - [Section 5.2] "All models except Claude show substantial shifts toward protective values when moving to open-ended evaluation."
  - [Figure 4] Shows MCQ rankings prioritize protective values (avg rank 1.7) while interactive rankings deprioritize them (avg rank 4.5)
  - [corpus] Related work notes MCQ evaluation limitations but doesn't compare formats directly

### Mechanism 3
- Claim: System prompting with explicit value hierarchies moderately steers model behavior under conflict
- Mechanism: Providing detailed value orderings with definitions, conflict resolution rules, and decision frameworks in the system prompt shifts the model's revealed preferences toward the target ranking
- Core assumption: Models can reliably follow hierarchical instructions when values conflict
- Evidence anchors:
  - [Section 5.3] "Using a system prompt to steer models toward a target ranking leads to moderate but consistent gains in alignment."
  - [Figure 5] Shows average normalized effect size of 0.145 (14.5% improvement) across models and value sets
  - [corpus] Related work on steerability (Sorensen et al., 2024) calls for designs enabling easier value steering; this provides empirical evidence

## Foundational Learning

- Concept: **Bradley-Terry preference modeling**
  - Why needed here: Used to aggregate pairwise value comparisons across many scenarios into a complete ranking of values
  - Quick check question: Can you explain how pairwise win probabilities map to a global ranking?

- Concept: **LLM-as-judge validation**
  - Why needed here: The pipeline relies on LLM judgments for filtering and open-ended evaluation; understanding inter-annotator agreement metrics (Cohen's Kappa) is essential
  - Quick check question: What does a Kappa of 0.62 indicate about judge-model agreement?

- Concept: **Expressed vs revealed preferences**
  - Why needed here: The key finding—that models say one thing in MCQ but do another in open-ended settings—requires understanding this distinction
  - Quick check question: Why might a model's MCQ choice differ from its behavior in conversation?

## Architecture Onboarding

- Component map: Value set definition -> Scenario generation -> Filtering -> Evaluation (MCQ + open-ended) -> Bradley-Terry ranking

- Critical path: Value set definition → Scenario generation → Filtering → Evaluation (MCQ + open-ended) → Bradley-Terry ranking

- Design tradeoffs:
  - Filtering stringency vs scenario coverage (3.8% agreement improvement but fewer scenarios)
  - User-LLM choice (GPT-4.1) balances performance vs cost; Claude performs better on some metrics
  - Single-turn vs multi-turn interaction (paper restricts to single turn for tractability)

- Failure signatures:
  - Low inter-model agreement on scenarios may indicate scenarios lack genuine conflict
  - High disagreement between judge-LLM and human annotations signals evaluation drift
  - Inconsistent rankings across domains suggests value prioritization is context-dependent

- First 3 experiments:
  1. Run the pipeline on a custom value pair relevant to your application to verify scenario quality
  2. Compare MCQ vs open-ended rankings on your target model to measure preference gap
  3. Test steering with a system prompt targeting your desired hierarchy to establish baseline steerability

## Open Questions the Paper Calls Out

- **Question:** How do multi-turn interactions alter elicited value rankings compared to the single-turn simulations used in this study?
  - Basis in paper: [explicit] The authors note, "We restrict our simulations to a single user-LLM turn; future work could consider multi-turn interaction in a more realistic chat environment."
  - Why unresolved: The current setup cannot determine if models maintain consistent value prioritization over extended dialogue or if initial preferences shift
  - What evidence would resolve it: Extending ConflictScope to support multi-turn dialogues and comparing the resulting value rankings against the single-turn baseline

- **Question:** Can Item Response Theory (IRT) or iterative matchmaking improve the efficiency or informativeness of the scenario generation process?
  - Basis in paper: [explicit] "Future work could also consider using targeted generation... such as by applying Item Response Theory... or iterative matchmaking to the scenario generation process."
  - Why unresolved: The current pipeline generates a fixed number of scenarios for all value pairs without adaptively targeting the model's specific decision boundaries
  - What evidence would resolve it: Implementing an adaptive generator and measuring the number of scenarios required to reach stable value rankings

- **Question:** Do sophisticated interventions like fine-tuning outperform system prompting in steering model behavior toward target value rankings?
  - Basis in paper: [explicit] The authors state that the evaluation metric "could be used to benchmark the effectiveness of more sophisticated interventions, such as fine-tuning, at steering LLM behavior under value conflict."
  - Why unresolved: The current study only tested system prompting, which achieved only moderate success (14% improvement), leaving the potential of stronger interventions unexplored
  - What evidence would resolve it: Applying fine-tuning with the target rankings and comparing alignment scores against the system prompting baseline on ConflictScope scenarios

## Limitations

- The LLM-as-judge pipeline has moderate reliability with Cohen's Kappa ~0.62, indicating substantial disagreement in ~38% of cases
- Steering effectiveness shows only moderate gains (14% average improvement), suggesting limited control over LLM value prioritization
- The study does not specify hyperparameters for LLM generation or filtering, which could significantly impact reproducibility

## Confidence

- **High Confidence:** The core finding that models show different value priorities in MCQ versus open-ended evaluation is well-supported by multiple experiments and consistent across value sets
- **Medium Confidence:** The claim that CONFLICTSCOPE outperforms existing moral datasets at eliciting inter-model disagreement is supported by observed agreement metrics, but depends on assumptions about dataset quality
- **Low Confidence:** The generalizability of the steering effectiveness findings is limited by the relatively modest effect size and testing with only one target ranking per value set

## Next Checks

1. **Evaluate model consistency:** Test the same models on both CONFLICTSCOPE scenarios and AIRiskDilemmas to directly compare the moral difficulty and inter-model disagreement metrics, validating the claim that CONFLICTSCOPE elicits more genuine value conflicts

2. **Validate judge reliability:** Conduct human validation of a subset of judge-LLM classifications in open-ended evaluation to quantify the impact of potential evaluation drift on observed preference shifts between MCQ and open-ended formats

3. **Test steering robustness:** Apply system prompts targeting multiple different value hierarchies on the same model and value set to establish whether steering effects are consistent across different target rankings or depend heavily on the specific values chosen