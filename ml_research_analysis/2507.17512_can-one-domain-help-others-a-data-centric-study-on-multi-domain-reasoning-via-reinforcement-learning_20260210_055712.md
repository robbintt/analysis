---
ver: rpa2
title: Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning
  via Reinforcement Learning
arxiv_id: '2507.17512'
source_url: https://arxiv.org/abs/2507.17512
tags:
- performance
- training
- reasoning
- code
- puzzle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how training on multiple reasoning domains
  affects the performance of large language models under reinforcement learning with
  verifiable rewards. Using the GRPO algorithm and Qwen-2.5-7B models, the authors
  systematically evaluate single-domain training (math, code, and puzzle), dual and
  triple-domain combinations, curriculum learning, reward design variations, and the
  impact of training language.
---

# Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning

## Quick Facts
- arXiv ID: 2507.17512
- Source URL: https://arxiv.org/abs/2507.17512
- Reference count: 40
- Primary result: Multi-domain training improves overall reasoning performance and task balance in RLVR

## Executive Summary
This study investigates how training on multiple reasoning domains affects the performance of large language models under reinforcement learning with verifiable rewards. Using the GRPO algorithm and Qwen-2.5-7B models, the authors systematically evaluate single-domain training (math, code, and puzzle), dual and triple-domain combinations, curriculum learning, reward design variations, and the impact of training language. They find that puzzle and math data provide mutual benefits, code reasoning has mixed cross-domain effects, and multi-domain training improves overall performance and task balance. Template consistency is critical, and SFT before RL substantially boosts performance. Curriculum learning with periodic policy refresh accelerates convergence, and reward design should match task complexity. RLVR is sensitive to language, with Chinese training underperforming English. Overall, the study highlights the importance of domain combination, reward customization, and template alignment in developing robust multi-domain reasoning capabilities.

## Method Summary
The authors employ reinforcement learning with verifiable rewards (RLVR) using the GRPO algorithm to train Qwen-2.5-7B models across three reasoning domains: math, code, and puzzles. They conduct systematic experiments comparing single-domain, dual-domain, and triple-domain training approaches. The study evaluates curriculum learning strategies, reward design variations, and the impact of training language (English vs Chinese). Experiments are conducted on standardized datasets including GSM8K, MATH, LiveCodeBench, and puzzle datasets, with performance measured using domain-specific metrics and aggregated scores.

## Key Results
- Puzzle and math domains provide mutual benefits when trained together, with cross-domain transfer observed in both directions
- Multi-domain training improves overall performance and achieves better task balance compared to single-domain approaches
- Template consistency is critical for RLVR success, with format mismatches leading to performance degradation
- SFT pre-training before RLVR substantially improves performance across all domains
- Curriculum learning with periodic policy refresh accelerates convergence compared to standard approaches
- RLVR shows significant language sensitivity, with Chinese-trained models underperforming English-trained models

## Why This Works (Mechanism)
The study demonstrates that different reasoning domains capture complementary cognitive processes - mathematical reasoning emphasizes logical deduction, code reasoning involves algorithmic thinking, and puzzles require creative problem-solving. When these domains are combined, the model develops more flexible reasoning strategies that transfer across domains. The mutual benefits between math and puzzles suggest shared underlying reasoning structures, while code reasoning's mixed effects indicate domain-specific computational requirements. Curriculum learning accelerates convergence by gradually exposing the model to increasing complexity while periodic policy refresh prevents catastrophic forgetting of earlier learned patterns.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Why needed - enables training on tasks with automatic evaluation; Quick check - verify reward functions produce consistent scores across domains
- **GRPO algorithm**: Why needed - provides stable policy optimization without value function estimation; Quick check - monitor KL divergence to ensure stable training
- **Curriculum Learning**: Why needed - manages complexity progression to prevent learning plateaus; Quick check - track performance improvements across curriculum stages
- **Template Consistency**: Why needed - ensures uniform reward signal interpretation across tasks; Quick check - validate all prompts follow standardized formats
- **Cross-Domain Transfer**: Why needed - enables knowledge sharing between related reasoning tasks; Quick check - measure performance gains when combining domains

## Architecture Onboarding

**Component Map:**
Qwen-2.5-7B -> GRPO Algorithm -> Verifiable Rewards -> Domain Datasets

**Critical Path:**
Input prompts → Model inference → Verifiable reward calculation → Policy gradient update → Model checkpoint

**Design Tradeoffs:**
- Domain diversity vs. specialization: More domains provide broader reasoning capabilities but may dilute domain-specific expertise
- Template standardization vs. flexibility: Consistent formats enable reliable RLVR but limit real-world applicability
- Curriculum complexity vs. training efficiency: Gradual progression improves learning but extends training time

**Failure Signatures:**
- Performance degradation when switching between domains suggests insufficient cross-domain adaptation
- Template inconsistency leading to unreliable reward signals indicates poor prompt standardization
- Language-specific underperformance may reveal cultural or linguistic biases in reward design

**First 3 Experiments:**
1. Single-domain RLVR on each domain (math, code, puzzle) to establish baselines
2. Dual-domain combinations to test cross-domain transfer effects
3. Curriculum learning with math→code→puzzle progression to evaluate learning acceleration

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Findings based on a single 7B parameter model, limiting generalizability to larger architectures
- Template consistency requirements may not reflect real-world reasoning task diversity
- Focus on short-chain reasoning tasks may not capture complex multi-step reasoning scenarios
- Language sensitivity findings lack detailed exploration of underlying causes

## Confidence

**High confidence:**
- Single-domain performance differences (math > code > puzzle for Qwen-2.5-7B)
- Cross-domain transfer benefits (puzzle → math, math → puzzle)
- Curriculum learning acceleration effects

**Medium confidence:**
- Cross-domain transfer benefits (puzzle → math, math → puzzle)
- Curriculum learning acceleration effects
- Template consistency requirements

**Low confidence:**
- Language-specific performance differences and their causes

## Next Checks

1. Replicate experiments across multiple model sizes (including frontier models) to test whether observed domain combination benefits are model-agnostic

2. Test the curriculum learning approach on more diverse reasoning task combinations and evaluate long-term retention effects

3. Conduct ablation studies on template flexibility to quantify the trade-off between standardization and real-world applicability