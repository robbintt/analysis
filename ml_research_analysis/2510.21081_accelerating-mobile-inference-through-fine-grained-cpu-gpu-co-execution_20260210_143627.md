---
ver: rpa2
title: Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution
arxiv_id: '2510.21081'
source_url: https://arxiv.org/abs/2510.21081
tags:
- latency
- linear
- mobile
- co-execution
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reducing inference latency
  for deep neural networks on mobile devices by co-executing computation across CPU
  and GPU. The main obstacles are synchronization overhead and the difficulty of predicting
  GPU kernel performance, which varies due to dynamic implementation choices and workgroup
  configurations.
---

# Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution

## Quick Facts
- arXiv ID: 2510.21081
- Source URL: https://arxiv.org/abs/2510.21081
- Reference count: 22
- Key result: Achieves up to 1.89x speedup for linear layers and 1.75x speedup for convolutional layers on mobile devices

## Executive Summary
This paper addresses the challenge of reducing inference latency for deep neural networks on mobile devices by co-executing computation across CPU and GPU. The main obstacles are synchronization overhead and the difficulty of predicting GPU kernel performance, which varies due to dynamic implementation choices and workgroup configurations. The authors propose a lightweight synchronization mechanism using OpenCL fine-grained shared virtual memory and develop accurate machine learning models that incorporate GPU kernel dispatch details and workgroup characteristics. Experimental results on four mobile platforms show that their approach achieves significant speedups close to the maximum possible values found by exhaustive search.

## Method Summary
The approach partitions linear and convolutional layers along output channels, with CPU and GPU processing disjoint subsets independently. It uses a lightweight synchronization mechanism based on OpenCL fine-grained shared virtual memory with active polling to reduce overhead from hundreds of microseconds to single-digit microseconds. Accurate latency prediction is achieved through gradient-boosted decision trees trained on operation parameters combined with GPU kernel dispatch features (workgroup size, count, and implementation type). The method balances CPU-GPU workloads while minimizing synchronization costs, enabling efficient parallel inference.

## Key Results
- Achieves up to 1.89x speedup for linear layers and 1.75x speedup for convolutional layers
- Synchronization overhead reduced by ~95%, from 162µs to 7µs for linear layers
- Latency prediction MAPE improved from 9.3% to 4.4% for linear layers using feature augmentation

## Why This Works (Mechanism)

### Mechanism 1: Feature-Augmented Latency Prediction for Workload Partitioning
- Claim: Augmenting ML latency predictors with kernel implementation details and workgroup characteristics enables more accurate workload partitioning decisions
- Core assumption: TFLite's kernel selection and workgroup heuristics are deterministic and can be statically computed from operation parameters
- Evidence: GBDT-aug reduces prediction MAPE from 9.3% to 4.4% (linear) and 14.1% to 9.3% (convolution)

### Mechanism 2: Fine-Grained SVM with Active Polling Synchronization
- Claim: OpenCL fine-grained shared virtual memory combined with active polling reduces CPU-GPU synchronization overhead by ~95%
- Core assumption: Target hardware supports fine-grained SVM with automatic cache coherence; busy waiting is acceptable for short durations
- Evidence: Mean synchronization overhead reduced from 162µs to 7µs (linear) and 141µs to 5.4µs (convolution)

### Mechanism 3: Output-Channel Partitioning for Independent Co-Execution
- Claim: Partitioning linear and convolution operations along output channels enables independent CPU and GPU computation without weight duplication
- Core assumption: Output channels are independent (no cross-channel dependencies like batch norm fused into the same kernel)
- Evidence: Achieves up to 1.89x speedup for linear layers and 1.75x speedup for convolutional layers

## Foundational Learning

### Concept: OpenCL Execution Model (Workgroups, Kernels, SVM)
- Why needed here: Understanding workgroups, kernel dispatch, and SVM types is essential to grasp how latency spikes arise and how fine-grained synchronization works
- Quick check question: Explain the difference between coarse-grained and fine-grained SVM in OpenCL, and why fine-grained SVM eliminates map/unmap overhead

### Concept: Mobile SoC Unified Memory Architecture
- Why needed here: Mobile platforms differ from discrete GPUs—CPU and GPU share main memory with hardware cache coherence, enabling the low-overhead co-execution strategy
- Quick check question: Why does unified memory architecture on mobile enable CPU-GPU co-execution more efficiently than on discrete desktop/server GPUs?

### Concept: Gradient-Boosted Decision Trees (GBDT) for Regression
- Why needed here: Latency predictors use LightGBM GBDTs; understanding feature importance, MAPE metrics, and non-linear modeling helps interpret evaluation results
- Quick check question: Why would a GBDT capture sudden latency spikes better than a linear model using only matrix dimensions?

## Architecture Onboarding

### Component Map:
Feature Extraction Module -> Latency Predictor Training Pipeline -> Partitioning Decision Engine -> Synchronization Layer -> Execution Runtime

### Critical Path:
1. Extract workgroup calculation logic and kernel heuristics from TFLite source
2. Collect training data on each target device (12,500 configs, structured random sampling)
3. Train and validate device-specific GBDT predictors
4. Integrate SVM synchronization into TFLite execution pipeline
5. Generate offline partitioning decisions for each layer in target models

### Design Tradeoffs:
- **Accuracy vs. Portability**: Feature augmentation is TFLite-specific; may not transfer to PyTorch Mobile or ONNX Runtime
- **Latency vs. Power**: Active polling minimizes sync delay but burns cycles during imbalance; accurate load balancing mitigates this
- **Offline vs. Online**: Partitioning decisions (3–4ms per op) can be pre-computed, avoiding runtime overhead but requiring device-specific profiling
- **CPU Thread Count**: More threads increase potential speedup but raise prediction complexity and contention

### Failure Signatures:
- **Unexpected Latency Spikes**: If predictions miss spikes at new C_out values, check if TFLite version changed kernel heuristics—re-extract feature logic
- **Synchronization Hang**: If polling loops indefinitely, verify fine-grained SVM support via clGetDeviceInfo; check for GPU queue starvation
- **Numerical Errors**: If outputs differ from baseline, ensure weight splits are disjoint and output regions don't overlap
- **Regression vs. GPU-Only**: If co-execution is slower, synchronization overhead may dominate—measure T_overhead and verify device compatibility

### First 3 Experiments:
1. **GPU Latency Characterization**: On target device, measure latency for 50 linear ops with C_out from 256–2560 (fixed input 50×768). Plot latency vs. C_out and workgroup count to identify spikes and validate feature extraction.
2. **Synchronization Overhead Comparison**: Implement both clWaitForEvents and fine-grained SVM polling. Measure overhead for 100 linear ops; compute reduction ratio and verify ~20x improvement.
3. **Single-Layer Partitioning Validation**: For 10 representative linear ops, run GBDT-based partitioning and measure speedup. Compare to grid search (step=8) baseline; verify speedup within 10% of exhaustive search.

## Open Questions the Paper Calls Out
- Can the co-execution framework be extended to efficiently include NPUs (Neural Processing Units) alongside CPUs and GPUs?
- How does model quantization affect the accuracy of the latency predictors and the efficiency of the synchronization mechanism?
- What is the energy consumption trade-off of the proposed busy-wait synchronization mechanism compared to event-based methods?

## Limitations
- The approach is tightly coupled to TFLite's deterministic kernel selection heuristics, which may change across versions
- Feature extraction requires white-box analysis of TFLite source code, creating portability barriers
- Active polling may become inefficient for highly unbalanced workloads or on devices with limited fine-grained SVM support

## Confidence
- **High Confidence**: Synchronization overhead measurements and achieved speedups (1.89x linear, 1.75x convolution) are well-supported by experimental data across multiple devices
- **Medium Confidence**: The feature-augmentation approach for latency prediction is theoretically sound, but validation is limited to TFLite's specific heuristics without broader framework testing
- **Medium Confidence**: The output-channel partitioning strategy is effective for independent channels, but potential limitations with fused operations or cross-channel dependencies are not fully explored

## Next Checks
1. **Cross-Framework Portability Test**: Implement the feature extraction and prediction pipeline for PyTorch Mobile to verify whether the approach generalizes beyond TFLite's specific kernel heuristics and workgroup calculation logic.
2. **Workload Imbalance Stress Test**: Systematically evaluate performance degradation under highly unbalanced CPU-GPU workloads (e.g., partition ratios 90:10 vs 50:50) to quantify the limits of the active polling synchronization strategy.
3. **Real-World Model Integration**: Deploy the approach on a complete mobile inference pipeline using a larger, more complex model like MobileBERT or a full vision transformer, measuring end-to-end latency improvements and energy consumption compared to pure GPU execution.