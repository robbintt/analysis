---
ver: rpa2
title: 'About Time: Model-free Reinforcement Learning with Timed Reward Machines'
arxiv_id: '2512.17637'
source_url: https://arxiv.org/abs/2512.17637
tags:
- reward
- timed
- machines
- learning
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Timed Reward Machines (TRMs), extending reward
  machines to incorporate timing constraints for reinforcement learning tasks. TRMs
  allow specification of time-sensitive objectives, such as imposing costs for delays
  and granting rewards for timely actions, by integrating timing constraints into
  the reward structure.
---

# About Time: Model-free Reinforcement Learning with Timed Reward Machines

## Quick Facts
- arXiv ID: 2512.17637
- Source URL: https://arxiv.org/abs/2512.17637
- Reference count: 40
- One-line primary result: Timed Reward Machines (TRMs) extend reward machines to incorporate timing constraints, enabling reinforcement learning of time-sensitive objectives with digital and real-time semantics.

## Executive Summary
This paper introduces Timed Reward Machines (TRMs) to extend reward machines with timing constraints for reinforcement learning. TRMs allow specification of time-sensitive objectives, such as imposing costs for delays and granting rewards for timely actions. The authors develop model-free reinforcement learning frameworks using tabular Q-learning for optimal policy learning with TRMs under both digital and real-time semantics. Their approach integrates TRMs into learning via abstractions of timed automata and employs counterfactual-imagining heuristics to improve search efficiency. Experiments on standard RL benchmarks demonstrate that TRMs effectively learn policies achieving high rewards while satisfying timing constraints.

## Method Summary
The method converts non-Markovian timed problems into standard MDPs by constructing a cross-product MDP that augments environment states with TRM states and clock valuations. For digital semantics, the agent learns in the expanded state space using tabular Q-learning with delay actions. For real-time semantics, the paper employs corner-point abstractions based on timed automata region theory, reducing infinite clock valuations to finite equivalence classes. A counterfactual-imagining heuristic synthesizes alternative timing experiences during learning to accelerate convergence. The approach is evaluated on Taxi and Frozen Lake benchmarks, comparing digital vs real-time interpretations and with/without counterfactual augmentation.

## Key Results
- TRMs enable learning of time-sensitive policies that standard reward machines cannot capture
- Corner-point abstraction in real-time setting often yields better reward returns than uniform discretization, particularly for tasks requiring substantial delays
- Counterfactual-imagining heuristics consistently improve performance across different settings
- Clear performance differences between product constructions for digital and real-time interpretations
- TRMs successfully learn policies that satisfy timing constraints while maximizing cumulative reward

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Converting the non-Markovian timed problem into a standard MDP allows existing tabular Q-learning algorithms to find optimal policies.
- **Mechanism:** The system constructs a "cross-product" MDP ($M^\otimes$) that augments the environment state $s$ with the Timed Reward Machine (TRM) state $u$ and the clock valuation $v$. This triplet $(s, u, v)$ serves as a single, Markovian state input for Q-learning.
- **Core assumption:** The optimal policy is positional within this expanded state space, and the state space remains tractable despite the Cartesian product.
- **Evidence anchors:** [abstract] "Our algorithms integrate the TRM into learning via abstractions of timed automata..."; [section 4] "The cross product MDP $M^\otimes = M \otimes A$ is defined below... optimal positional deterministic delay-discounted policy exists..."
- **Break condition:** If the maximum time constant $M$ is large, the clock valuation space explodes, making the Q-table too large to store or learn efficiently.

### Mechanism 2
- **Claim:** Finite "corner-point" abstractions allow agents to approximate optimal dense-time rewards without enumerating infinite real-valued delays.
- **Mechanism:** Instead of uniformly discretizing time, the algorithm uses region abstraction to group clock valuations into equivalence classes and identifies "corner points" (boundary integer valuations). The agent learns to pick delays that drive the system state toward these corners.
- **Core assumption:** Lemma 3 states that for any trajectory, there exists a region-equivalent corner trajectory with equal or higher reward; thus, searching only corners is sufficient for near-optimality.
- **Evidence anchors:** [abstract] "...employ counterfactual-imagining heuristics that exploit the structure of the TRM..."; [section 5.1] "For any trajectory $\zeta$, there exists a region-equivalent corner trajectory $\hat{\zeta} \in C_\epsilon(\zeta)$ such that $G_{\hat{\zeta}} \geq G_\zeta$."
- **Break condition:** If the reward function is not continuous or monotonic with respect to time within regions, the approximation fails to capture the true optimum.

### Mechanism 3
- **Claim:** Counterfactual imagining accelerates learning by simulating alternative timing outcomes without environment interaction.
- **Mechanism:** When the agent experiences a transition, the heuristic synthesizes "imagined" transitions for nearby clock valuations and alternative valid delays. It updates the Q-table for these counterfactual states.
- **Core assumption:** The TRM structure (guards and resets) is known and can be simulated deterministically to generate these synthetic rewards.
- **Evidence anchors:** [abstract] "...employ counterfactual-imagining heuristics that exploit the structure of the TRM to improve the search."; [section 4.1] "We synthesize counterfactual experiences by varying the TRM states, clock valuations, and delays..."
- **Break condition:** If the radius of counterfactual exploration ($r_{crm}$) is set too high, it may generate invalid or noisy updates that destabilize learning.

## Foundational Learning

- **Concept: Timed Automata & Region Abstraction**
  - **Why needed here:** The "Corner-Point Abstraction" relies on the theory of timed automata. You must understand how clocks, guards ($x \leq c$), and resets partition the infinite time domain into finite "regions" to comprehend the state space reduction.
  - **Quick check question:** Can you explain why two clock valuations $v_1(x)=1.1$ and $v_2(x)=1.9$ might belong to the same "region" if the maximum constant is 2, and why this matters for state reduction?

- **Concept: Reward Machines (RM)**
  - **Why needed here:** TRMs extend RMs. You need to understand how RMs decompose a task into a finite state machine (states $u_i$) to capture history-dependent rewards, which is the scaffold upon which timing constraints are built.
  - **Quick check question:** If a standard Reward Machine relies only on logical propositions (e.g., `has_key`), what additional input does a Timed Reward Machine require to determine the next state and reward?

- **Concept: Q-Learning in Expanded State Spaces**
  - **Why needed here:** The paper applies tabular Q-learning to a product MDP. Understanding the trade-off between tabular methods (guarantees convergence) and the exponential state growth caused by adding clocks is vital.
  - **Quick check question:** In the cross-product MDP $M^\otimes$, if the environment has 10 states and the TRM has 3 states and 1 clock (max value 5), what is the approximate maximum size of the Q-table?

## Architecture Onboarding

- **Component map:** Environment Wrapper -> TRM Specification -> Abstraction Layer -> Product Builder -> Learner (with Counterfactual Generator)
- **Critical path:** The implementation of the **Corner-Point Abstraction** (Section 5.1). Correctly calculating the successor region $(R, \alpha) \oplus (d, \sigma)$ is the most complex logic piece; errors here will cause the agent to misinterpret if a timing constraint was met.
- **Design tradeoffs:**
  - **Digital vs. Real-time:** Digital clock semantics (integer time) is easier to implement but limits the agent to coarse-grained timing. Real-time (Corner-point) enables precise optimal timing but significantly increases code complexity (region calculation).
  - **Max Constant ($M$):** Lower $M$ reduces Q-table size but may clip the time horizon, making some tasks unsolvable if the agent cannot wait long enough.
- **Failure signatures:**
  - **"Rushing" behavior:** The agent completes the task but violates delay constraints (e.g., picking up a passenger too fast), resulting in negative total reward. This suggests the reward shaping (state costs) isn't penalizing speed enough or the TRM guard logic is buggy.
  - **State Explosion:** Memory exhaustion. This happens if the maximum constant $M$ is set too high or if multiple clocks are used without bounds.
  - **Deadlocks:** The agent enters a state where no delay $d \in [0, M]$ satisfies any TRM guard, effectively trapping the policy.
- **First 3 experiments:**
  1. **Sanity Check (Taxi):** Replicate Figure 1. Run the digital-clock agent. Verify it learns to wait (self-loop) to maximize reward, distinct from a standard RM agent.
  2. **Abstraction Comparison:** Run the same task (Figure 4 example) using both Uniform Discretization and Corner-Point Abstraction. Plot cumulative reward to confirm Corner-Point achieves higher returns (approximation quality) or learns faster.
  3. **Ablation (Counterfactuals):** Disable the counterfactual imagining module. Measure the "time to convergence." The paper predicts a significant drop in sample efficiency (RQ1).

## Open Questions the Paper Calls Out

- **Question:** Can deep continuous reinforcement learning algorithms (e.g., TD3) be effectively adapted to handle the continuous action spaces required by Timed Reward Machines?
  - **Basis in paper:** [explicit] The authors explicitly list "adapt deep continuous RL (e.g., TD3 [20]) to continuous-time" as a future avenue in the Conclusion.
  - **Why unresolved:** The current work relies exclusively on tabular Q-learning, which does not scale to high-dimensional or continuous state/action spaces typical in deep RL.
  - **What evidence would resolve it:** Successful application of the TRM framework on a continuous control benchmark (e.g., MuJoCo) using a deep actor-critic architecture.

- **Question:** How can Timed Reward Machines be interpreted over continuous-time Markov models (CTMDPs) to better capture rate-based timing?
  - **Basis in paper:** [explicit] The Conclusion suggests applying TRMs to "continuous-time Markov models [19], which better capture rate-based timing."
  - **Why unresolved:** The current paper interprets TRMs over discrete-time MDPs augmented with delay actions, rather than models where transitions are governed by exponential distributions (rates).
  - **What evidence would resolve it:** A formal semantics for TRMs over CTMDPs and a corresponding learning algorithm that optimizes for timing constraints defined by rates.

- **Question:** Does incorporating guidance from priced zones improve the sample efficiency of exploration in TRM-based learning?
  - **Basis in paper:** [explicit] The Conclusion proposes "incorporate guidance from priced zones [6] to improve exploration of TRM objectives."
  - **Why unresolved:** The current approach uses corner-point abstractions and counterfactual imagining, but does not utilize the more refined symbolic data structures used in timed automata verification.
  - **What evidence would resolve it:** Empirical results comparing the sample complexity of Q-learning using priced zones versus the current corner-point abstraction.

- **Question:** Does the guarantee that delay-bounded trajectories yield optimal rewards hold when state-based rewards are positive rather than negative?
  - **Basis in paper:** [inferred] Lemma 2 proves that bounded delays improve discounted returns based on the assumption that state rewards are negative (costs).
  - **Why unresolved:** If waiting (state duration) granted positive rewards, the incentive structure would change, potentially invalidating the strategy of bounding delays to the maximum constant $M$.
  - **What evidence would resolve it:** A counter-example or a revised theorem demonstrating the relationship between delay bounding and positive state rewards.

## Limitations
- The scalability of tabular Q-learning with expanded state spaces (S×U×V) is not thoroughly addressed, and the impact of increasing clock constants M on learning efficiency is not quantified.
- Empirical validation is limited to small benchmark tasks (Taxi, Frozen Lake), with no evaluation on more complex environments.
- The counterfactual imagining heuristic's performance depends heavily on the radius r_crm and selection criteria, which are not fully specified in the paper.

## Confidence

- **High confidence:** The mechanism of converting non-Markovian timed problems into standard MDPs via product construction (Mechanism 1) is well-established and theoretically sound.
- **Medium confidence:** The corner-point abstraction (Mechanism 2) and counterfactual imagining (Mechanism 3) are innovative contributions, but their effectiveness depends on implementation details and task complexity not fully explored in the paper.
- **Low confidence:** The paper does not provide sufficient empirical evidence to conclusively claim superiority of corner-point abstraction over uniform discretization across diverse scenarios, nor does it address potential failure modes in more complex environments.

## Next Checks
1. **Scalability test:** Evaluate the approach on larger MDPs (e.g., 16x16 Frozen Lake or more complex Taxi variants) to assess state space explosion and learning efficiency with increased clocks/constants.
2. **Ablation study:** Systematically vary the counterfactual radius r_crm and number of alternatives (currently "top 15") to quantify their impact on sample efficiency and final reward.
3. **Robustness check:** Test the corner-point abstraction's approximation quality by comparing its learned policy's reward against a dense-time baseline (e.g., discretized with very small step size) on a task with tight timing constraints.