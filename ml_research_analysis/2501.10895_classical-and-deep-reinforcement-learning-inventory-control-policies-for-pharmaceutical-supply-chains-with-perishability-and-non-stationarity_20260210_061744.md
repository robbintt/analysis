---
ver: rpa2
title: Classical and Deep Reinforcement Learning Inventory Control Policies for Pharmaceutical
  Supply Chains with Perishability and Non-Stationarity
arxiv_id: '2501.10895'
source_url: https://arxiv.org/abs/2501.10895
tags:
- policy
- inventory
- policies
- cost
- lost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies inventory control policies for pharmaceutical
  supply chains, addressing perishability, yield uncertainty, and non-stationary demand.
  The authors develop a realistic case study in collaboration with Bristol-Myers Squibb,
  benchmark three policies (order-up-to, projected inventory level, and deep reinforcement
  learning using PPO) against a human-driven baseline, and derive bounds-based procedures
  for optimizing OUT and PIL policy parameters.
---

# Classical and Deep Reinforcement Learning Inventory Control Policies for Pharmaceutical Supply Chains with Perishability and Non-Stationarity

## Quick Facts
- arXiv ID: 2501.10895
- Source URL: https://arxiv.org/abs/2501.10895
- Reference count: 9
- No single policy consistently outperforms the others across all experiments

## Executive Summary
This paper investigates inventory control policies for pharmaceutical supply chains, addressing challenges of perishability, yield uncertainty, and non-stationary demand. Through a realistic case study with Bristol-Myers Squibb, the authors benchmark three policies (order-up-to, projected inventory level, and deep reinforcement learning using PPO) against a human-driven baseline. While all three implemented policies achieve lower average costs than human-driven approaches, they exhibit higher cost variability. The study reveals that no single policy consistently outperforms others, with PIL showing robust performance, OUT struggling under high lost sales costs, and PPO excelling in complex scenarios but requiring significant computational effort.

## Method Summary
The paper develops a simulation environment for a pharmaceutical supply chain with periodic review, lead time L=12, product lifetime m=12, and batch ordering. Three policies are implemented: Order-Up-To (OUT) using bounds-based search with Monte Carlo simulation, Projected Inventory Level (PIL) requiring nested simulation to estimate expected expired and lost sales, and Deep Reinforcement Learning (PPO) using a novel state representation combining projected inventory levels with demand forecasts. The policies are evaluated over 2000 simulated episodes against a human-expert baseline, measuring average total cost and cost variability.

## Key Results
- All three implemented policies (OUT, PIL, PPO) achieve lower average costs than the human-driven baseline
- No single policy consistently outperforms the others across all experimental scenarios
- PIL demonstrates robust and consistent performance across different cost regimes
- OUT policy exhibits significant variability, particularly under high lost sales costs
- PPO excels in complex and variable scenarios but requires substantial computational resources

## Why This Works (Mechanism)

### Mechanism 1
Integrating projected inventory levels and demand forecasts as input features enables Deep Reinforcement Learning (PPO) to manage non-stationarity and perishability more effectively than raw inventory vectors. Standard inventory vectors are high-dimensional and difficult for neural networks to map directly to optimal order quantities during non-stationary life-cycles. By pre-computing the Projected Inventory Level (PIL) and condensing future inventory states into a statistic that accounts for lead times and expirations, the model input is reduced, lowering the learning burden on the agent.

### Mechanism 2
Classical policies (OUT and PIL) remain competitive when optimized via derived lower and upper cost bounds. Instead of unguided search, the paper derives theoretical cost bounds based on simplified assumptions, then constrains the optimal safety stock parameter within this interval using Monte Carlo simulation. This approach ensures computational efficiency and prevents divergence by bounding the search space for policy parameters.

### Mechanism 3
The "No Free Lunch" theorem applies to these policies; performance is strictly conditional on the cost regime. OUT fails under high lost sales costs because it ignores age-distribution of stock, leading to unnecessary expirations or stockouts in systems with long lead times. PIL excels in balanced settings by accounting for expirations but can underperform if expected lost sales terms fluctuate too much during non-stationary periods. PPO handles complexity (yield + non-stationarity) best but has high variance.

## Foundational Learning

### Inventory Policy Concepts
- **Why needed**: Understanding the fundamental approaches to inventory control
- **Quick check**: Can identify differences between OUT, PIL, and DRL approaches

### Perishability Modeling
- **Why needed**: Pharmaceutical products have limited shelf life affecting inventory decisions
- **Quick check**: Can explain how FIFO issuing interacts with lead times and demand

### Cost Structure Components
- **Why needed**: Multiple cost types (ordering, holding, lost sales, expiration) create complex trade-offs
- **Quick check**: Can identify which cost dominates under different scenarios

### Forecast Error Integration
- **Why needed**: Demand forecasts are imperfect and must be incorporated into policy decisions
- **Quick check**: Can describe how forecast errors propagate through inventory calculations

### Bounds-Based Optimization
- **Why needed**: Provides computationally efficient way to optimize classical policies
- **Quick check**: Can explain derivation of upper and lower bounds for safety stock

## Architecture Onboarding

### Component Map
- Simulation Environment -> Classical Policies (OUT, PIL) -> Bounds-Based Optimization
- Simulation Environment -> PPO Agent -> Training/Evaluation Loop
- All Policies -> Cost Evaluation -> Performance Comparison

### Critical Path
State Update (FIFO) -> Cost Calculation (Ordering, Holding, Lost Sales, Expiration) -> Policy Decision (OUT/PIL/PPO) -> Order Placement -> Next Period

### Design Tradeoffs
- OUT: Simple to implement, struggles with perishability and long lead times
- PIL: Accounts for expirations, requires complex nested simulation
- PPO: Handles complexity best, computationally expensive, high variance

### Failure Signatures
- High lost sales despite positive inventory: OUT policy not accounting for age distribution
- Computational slowness: PIL nested simulation inefficiency
- Poor PPO performance: Incorrect state representation or insufficient training

### First Experiments
1. Test OUT policy with different safety stock values to verify bounds-based optimization
2. Validate PIL policy with perfect forecasts to isolate perishability effects
3. Compare PPO performance with different state representations (raw inventory vs projected)

## Open Questions the Paper Calls Out

### Open Question 1
Can the upper bound for the order-up-to (OUT) policy be formally proven to act as a cap for the optimal policy level in perishable inventory systems with positive lead times and lost sales? While the paper expects the upper bound to act as a cap, providing formal proof in the context of perishable products remains a challenging open problem.

### Open Question 2
How can classical policies (OUT, PIL) and Deep Reinforcement Learning (DRL) be effectively integrated into a hybrid framework to leverage their respective strengths across different demand and cost regimes? The paper demonstrates that different policies excel in specific scenarios but does not propose a mechanism to switch or combine them dynamically during operation.

### Open Question 3
Can inventory policies be modified to minimize cost variability to match the robustness of human-driven policies without sacrificing the average cost reductions achieved by automated methods? The current optimization procedures minimize expected total cost but do not explicitly account for variance or risk constraints, which are prioritized by human planners.

## Limitations
- Case-study specificity limits generalizability to other perishable products and supply chains
- Heavy dependence on forecast quality, with robustness to forecast errors not thoroughly tested
- PPO requires extensive training and evaluation episodes, making it less practical for real-time deployment
- Short planning horizons (T=60/240 timesteps) may not capture long-term policy effects

## Confidence

### High Confidence
- Classical policies with bounds-based optimization are robust and computationally efficient
- Finding that no single policy dominates is well-supported by experimental results

### Medium Confidence
- PPO's superiority in complex, variable scenarios, though computational expense and variance suggest caution
- Projected inventory level state representation meaningfully improves DRL performance

### Low Confidence
- Generalizability to other pharmaceutical products and supply chains
- Long-term effects of policy choices beyond the 5-20 year simulation horizon

## Next Checks
1. Systematically degrade forecast accuracy and re-evaluate all three policies to quantify sensitivity
2. Apply the same policy framework to a different perishable product with distinct lifecycle dynamics and lead times
3. Compare total computational cost (training + evaluation) of PPO versus bounds-based classical policies across all scenarios