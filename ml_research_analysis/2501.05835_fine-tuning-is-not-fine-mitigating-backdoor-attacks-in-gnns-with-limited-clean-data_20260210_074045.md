---
ver: rpa2
title: 'Fine-tuning is Not Fine: Mitigating Backdoor Attacks in GNNs with Limited
  Clean Data'
arxiv_id: '2501.05835'
source_url: https://arxiv.org/abs/2501.05835
tags:
- backdoor
- graph
- attention
- data
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GraphNAD, a backdoor defense framework for
  graph neural networks (GNNs) that achieves effective mitigation using only 3% clean
  data. The method introduces a graph-specific attention representation that captures
  both feature and topological information, and aligns intermediate-layer attention
  between backdoored and fine-tuned models through graph attention transfer.
---

# Fine-tuning is Not Fine: Mitigating Backdoor Attacks in GNNs with Limited Clean Data

## Quick Facts
- **arXiv ID:** 2501.05835
- **Source URL:** https://arxiv.org/abs/2501.05835
- **Reference count:** 40
- **Primary result:** Reduces backdoor attack success rates to below 5% while maintaining negligible accuracy loss using only 3% clean data.

## Executive Summary
This paper introduces GraphNAD, a novel backdoor defense framework for Graph Neural Networks (GNNs) that achieves effective mitigation using only 3% clean data. The method leverages a graph-specific attention representation that captures both feature and topological information, aligning intermediate-layer attention between backdoored and fine-tuned models through graph attention transfer. By enforcing attention relation congruence between layers, GraphNAD ensures model accuracy while significantly reducing backdoor influence. Extensive experiments demonstrate superior performance compared to existing defense methods across various attack settings and datasets.

## Method Summary
GraphNAD operates by fine-tuning a backdoored model on a small clean data subset to create a teacher model, then distilling this knowledge back to the student (original backdoored model) using a combination of graph attention transfer and attention relation congruence. The framework computes a degree-weighted attention representation that captures both node activations and graph topology, aligning these representations between teacher and student through L2 distance minimization. Additionally, it enforces consistency between inter-layer relation maps using sliced Wasserstein-2 distance to preserve main-task accuracy while amplifying backdoor mitigation signals.

## Key Results
- Reduces attack success rates to below 5% while maintaining negligible accuracy loss
- Outperforms existing defense methods significantly across multiple datasets and attack types
- Effective even with auxiliary datasets of different distributions (though with 5-10% ACC penalty)
- Achieves mitigation with as little as 1% clean data in some settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning intermediate-layer attention representations between a fine-tuned teacher and backdoored student forces backdoor neurons to behave like benign neurons.
- **Mechanism:** The framework computes a graph-aware attention representation that incorporates node degree information. By minimizing the L2-distance between normalized attention maps of teacher and student models, neurons that activate strongly on trigger patterns are pressured to shift their behavior toward benign activation patterns.
- **Core assumption:** Backdoor neurons exhibit distinguishable attention patterns compared to benign neurons when processing clean data, even with limited samples.
- **Evidence anchors:** Attention shifting away from trigger regions post-purification is visualized in Figure 6; equation definitions provided in Section 4.3.

### Mechanism 2
- **Claim:** Enforcing consistency between inter-layer relation maps preserves main-task accuracy while amplifying backdoor mitigation signals.
- **Mechanism:** The relation map captures how representations transform between layers. By minimizing the sliced Wasserstein-2 distance between teacher and student relation maps across layer pairs, the framework transfers additional structural knowledge about normal feature propagation.
- **Core assumption:** The transformation dynamics between layers encode task-relevant knowledge that is distinct from backdoor-triggered pathways.
- **Evidence anchors:** Table 7 shows L_RC alone preserves ACC but cannot fully remove backdoors—both L_AD and L_RC are needed together.

### Mechanism 3
- **Claim:** Limited clean data (3%) suffices when attention-based distillation amplifies representational differences that vanilla feature-matching misses.
- **Mechanism:** Standard distillation compares raw activations, which show minimal differences between backdoored and clean models on clean inputs. The attention operator with p > 1 amplifies high-magnitude activations, making dormant backdoor neurons more detectable.
- **Core assumption:** The attention exponent p can be tuned to amplify backdoor-specific signals without overly suppressing benign feature knowledge.
- **Evidence anchors:** Table 5 shows p=2 yields best defense across datasets; Figure 4 shows ASR drops below 10% with only 1% clean data.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student Framework)**
  - Why needed here: GraphNAD treats the fine-tuned clean model as teacher and the original backdoored model as student; understanding how soft-label and feature-based distillation work is prerequisite.
  - Quick check question: Can you explain why matching intermediate-layer features differs from matching output logits?

- **Backdoor Attacks on GNNs (Trigger Injection Types)**
  - Why needed here: Defense mechanisms must account for node-feature tampering, topology disruption, or combined triggers; the attention operator must capture both feature and structural anomalies.
  - Quick check question: How does a subgraph trigger differ from a node-feature trigger in terms of detectability?

- **Attention Mechanisms in GNNs**
  - Why needed here: GraphNAD's attention representation is not standard self-attention; it's a degree-weighted activation aggregation designed specifically for distillation, not for message-passing.
  - Quick check question: Why does the attention formula include node degree normalization?

## Architecture Onboarding

- **Component map:** Teacher Fine-tuning Module -> Graph Attention Transfer (L_AD) -> Attention Relation Congruence (L_RC) -> Combined Optimization
- **Critical path:**
  1. Verify backdoored model ASR is high (>80%) before defense
  2. Fine-tune teacher on clean subset (verify ACC doesn't collapse)
  3. Run distillation with both L_AD and L_RC; monitor ASR and ACC concurrently
  4. If ASR stalls, tune attention exponent p (try 1→4); if ACC degrades, reduce β or check data quality
- **Design tradeoffs:**
  - Higher p amplifies backdoor detection but risks suppressing benign features
  - Using all layer pairs maximizes knowledge transfer but increases compute
  - Auxiliary datasets enable defense without original data but incur 5-10% ACC penalty
- **Failure signatures:**
  - ASR remains high (>20%) + ACC preserved → Teacher fine-tuning likely failed
  - ASR low + ACC severely degraded → Over-aggressive attention alignment
  - High variance across runs → Clean data subset too small or non-representative
- **First 3 experiments:**
  1. Baseline replication on PROTEINS dataset with GTA attack using default settings
  2. Ablation study: Disable L_RC and observe ASR/ACC to confirm Table 7 pattern
  3. Holding rate sensitivity: Test clean data ratios of 1%, 3%, 5%, 7%, 9% on AIDS dataset

## Open Questions the Paper Calls Out
- How to transplant existing defense strategies to node prediction scenarios remains an open problem
- GraphNAD is designed for neural networks with a multi-layer structure (3 layers), and its performance may be affected when faced with networks with fewer layers
- Future work goals include developing a general backdoor defense framework in scenarios where there is no dataset interaction

## Limitations
- The method's effectiveness may degrade if backdoor triggers are designed to evade attention-based detection
- Reliance on a fine-tuned teacher model trained on only 3% clean data introduces sensitivity to data quality and representativeness
- The framework is designed for multi-layer GNNs and may be affected when faced with networks with fewer layers

## Confidence
- **High Confidence:** The core claim that GraphNAD reduces ASR to <5% while maintaining ACC is well-supported by extensive experiments
- **Medium Confidence:** The mechanism explaining how attention alignment forces backdoor neuron transformation is plausible and supported by visualizations and ablation studies
- **Medium Confidence:** The claim of effectiveness with auxiliary datasets of different distributions is demonstrated but with noted ACC penalty

## Next Checks
1. Reproduce results using GCN and GAT instead of GIN as the base architecture to confirm generalizability
2. Test GraphNAD with <1% clean data on AIDS dataset to determine absolute minimum data requirement
3. Design a backdoor trigger that minimally perturbs attention maps and evaluate whether GraphNAD can still detect and mitigate it