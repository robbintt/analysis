---
ver: rpa2
title: An Explainable Framework for Misinformation Identification via Critical Question
  Answering
arxiv_id: '2503.14626'
source_url: https://arxiv.org/abs/2503.14626
tags:
- argument
- critical
- argumentation
- scheme
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an explainable framework for misinformation
  detection using argumentation schemes and critical questions. The authors create
  and release NLAS-CQ, a corpus of 3,566 textbook-like argumentation schemes with
  4,687 critical question annotations.
---

# An Explainable Framework for Misinformation Identification via Critical Question Answering

## Quick Facts
- arXiv ID: 2503.14626
- Source URL: https://arxiv.org/abs/2503.14626
- Reference count: 20
- Introduces NLAS-CQ corpus with 3,566 argumentation schemes and 4,687 critical question annotations

## Executive Summary
This paper presents a novel explainable framework for misinformation detection that leverages argumentation schemes and critical questions. The framework consists of two modules: an Argumentation Scheme Classification (ASC) module that identifies the reasoning pattern in an argument, and a Critical Question Answering (CQA) module that evaluates the argument's validity by answering scheme-specific critical questions. By providing transparent explanations in the form of critical questions and their answers, the framework addresses the explainability gap in traditional misinformation detection systems. The authors create and release the NLAS-CQ corpus, establishing a new benchmark for the critical question answering task.

## Method Summary
The framework implements a two-stage pipeline where arguments are first classified into argumentation schemes using a RoBERTa-large classifier, then evaluated using critical questions answered by another fine-tuned RoBERTa model. The ASC module maps input arguments to one of 20 scheme classes, while the CQA module answers binary yes/no questions about the argument's validity. Contextual information can be provided to improve the CQA performance. The system was trained on a synthetic "textbook-like" corpus of 3,566 argumentation schemes and validated on both the same corpus and a dialogue corpus.

## Key Results
- ASC module achieves 99.2% F1-score on textbook-like arguments in NLAS-CQ corpus
- CQA module achieves 70.7% F1-score without contextual information, improving to 73.9% with it
- Performance drops significantly on real dialogue data (44.8% F1), highlighting limitations with enthymematic arguments
- RoBERTa-CQA outperforms GPT-4 and Mixtral on the binary critical question answering task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A two-stage pipeline of scheme classification followed by targeted questioning enables explainable misinformation analysis.
- Mechanism: The framework first classifies an input argument into one of several stereotyped reasoning patterns (argumentation schemes) using the ASC module. This classification determines which set of predefined Critical Questions (CQs) are retrieved. A second module (CQA) then answers these questions, and any unsatisfactory answer serves as a transparent reason for flagging the argument.
- Core assumption: The validity of an argument can be systematically challenged by a fixed set of critical questions associated with its reasoning pattern.
- Evidence anchors:
  - [abstract] "...we implement and validate our new framework which combines classification with question answering..."
  - [section 3] "...the ASC enriches the analysis with additional argumentative information, and the CQA estimates the most likely answers to the critical questions..."
  - [corpus] Corpus shows `status: ok`. No directly comparable two-stage mechanisms in neighbor papers, which typically use single-model classification for detection tasks.
- Break condition: If the ASC module misclassifies the argument's scheme, the retrieved critical questions will be irrelevant, rendering the subsequent analysis invalid.

### Mechanism 2
- Claim: Critical Questions function as inherent, human-readable explanations for misinformation detection.
- Mechanism: Instead of a black-box "misinformation" label, the system outputs the critical questions themselves (e.g., "Is the source in a position to know?") along with their answers. A negative answer directly points to a factual or logical flaw, providing the "reason why" an argument is suspect.
- Core assumption: The critical questions, as defined by argumentation theory, are comprehensive enough to expose both factual and rational flaws and are interpretable by end-users.
- Evidence anchors:
  - [abstract] "...provides the explanations in form of critical questions to the human user."
  - [section 1] "Being unable to provide a good answer for all the critical questions will result in a potential piece of misinformation, and we will know the exact reasons why..."
  - [corpus] Weak corpus support for this specific explainability mechanism. Neighbor 'TraceRAG' and 'ESGBench' aim for explainability but via traceable evidence paths or grounded answers, not theoretical questioning.
- Break condition: This mechanism fails if the user lacks the background to understand the argumentation theory or if the questions are too abstract, making the explanation unhelpful.

### Mechanism 3
- Claim: A fine-tuned encoder model can answer critical questions as a binary classification task, augmented by retrieved context.
- Mechanism: The CQA module is implemented by fine-tuning a RoBERTa-large model. For each argument and its associated critical question, the model is trained to output a binary "yes" or "no" answer. Performance is improved by adding contextual information (supporting evidence) to the input, which helped the model achieve a 73.94 F1-score.
- Core assumption: The complex task of judging an argument's validity can be reduced to a binary classification problem solvable by patterns learned from a textbook-like corpus.
- Evidence anchors:
  - [section 5.1] "...we fine-tuned a RoBERTa-large model on a Binary Question Answering task... where the natural language input contained the argument, the contextual information... and the question..."
  - [table 4] Shows RoBERTa-CQA [CI] outperforming GPT-4 and Mixtral.
  - [corpus] No direct corpus evidence for this specific CQA mechanism.
- Break condition: This mechanism is brittle when answers require strong common sense or subjective judgment, as seen in the error analysis for questions like "Is the generalization strong?".

## Foundational Learning

- Concept: **Argumentation Schemes**
  - Why needed here: This is the core theoretical unit. An engineer must understand that an argumentation scheme is a reusable pattern of reasoning (e.g., "Argument from Expert Opinion") with defined premises and a conclusion, not just arbitrary text.
  - Quick check question: What are the three abstract variables (s, f, p) in the "Argument from Position to Know" scheme described in the introduction?

- Concept: **Critical Questions (CQs)**
  - Why needed here: CQs are the system's primary analytical and explanatory tool. They are not open-ended queries but predefined questions specific to each scheme that test its logical and factual soundness.
  - Quick check question: According to the paper, what is the final step in the proposed framework after an argumentation scheme is identified?

- Concept: **Enthymemes**
  - Why needed here: This is the key to understanding the paper's primary limitation. Real-world arguments are often enthymemes (incomplete arguments with unstated premises), unlike the "textbook-like" data used for training. This concept explains the performance gap.
  - Quick check question: Why does the paper's error analysis suggest that argument classification is harder on the QT30 dialogue corpus compared to the NLAS-CQ corpus?

## Architecture Onboarding

- Component map:
  1.  **ASC Module**: A RoBERTa-large classifier that maps an input argument `x` to a scheme class `c`.
  2.  **Critical Question Database**: A static lookup that maps a scheme class `c` to a set of critical questions `{q1, q2, ...}`.
  3.  **CQA Module**: A second RoBERTa-large model fine-tuned for binary QA. It takes `(argument, context, question)` and outputs an answer `a` ("yes" or "no").

- Critical path:
  1.  Input Argument -> **ASC Module** -> Predicted Scheme Class (e.g., 'Argument from Analogy').
  2.  Predicted Scheme Class -> **Critical Question Database** -> Retrieve relevant CQs.
  3.  For each CQ: (Argument + CQ + optional Context) -> **CQA Module** -> Predicted Answer.
  4.  Final Output -> Argument's scheme, CQs, and answers (the explanation).

- Design tradeoffs:
  - **Interpretability vs. Performance**: The framework provides clear, theory-backed explanations via CQs but is limited by the performance of its two-stage pipeline, which currently struggles with real-world dialogue.
  - **Data Source Tradeoff**: The decision to train on synthetic, "textbook-like" arguments enables high performance (99.2 F1) on structured data but creates a domain gap, leading to poor generalization on enthymematic (real-world) arguments.
  - **Model Choice**: Using a fine-tuned encoder (RoBERTa) for the CQA task trades the general knowledge of a large generative model (GPT-4) for higher, more reliable accuracy on this specific binary task.

- Failure signatures:
  - **ASC Generalization Failure**: Dramatic performance drop (F1 from 99.2 to ~1.0-44.8) when processing real dialogue, where arguments are incomplete (enthymemes) and lack the clear structural cues of textbook examples.
  - **CQA Subjectivity Failure**: Incorrect answers on critical questions that require subjective judgment (e.g., "Is the generalization strong?") or extensive world knowledge not present in the context.
  - **Generative QA Bias**: The error analysis shows that generative models like GPT-4 have a strong bias towards answering "yes", failing to critically challenge the arguments.

- First 3 experiments:
  1.  **Reproduce ASC Baseline**: Fine-tune a RoBERTa-large model on the NLAS-CQ corpus for the 20-class scheme classification task. Verify you achieve the reported ~99% F1-score to ensure data and model setup are correct.
  2.  **Ablate Contextual Information**: Train the CQA module both with and without the provided contextual information (`RoBERTa-CQA` vs `RoBERTa-CQA[CI]`) to quantify the performance gain from external evidence.
  3.  **Test on Dialogue Data**: Take the trained ASC model and evaluate it directly on the QT30 dialogue corpus provided in the paper's validation set. Observe the performance collapse to understand the real-world limitations discussed in the error analysis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can an automated information retrieval module be optimized to provide high-quality contextual evidence for the Critical Question Answering (CQA) task?
- Basis in paper: [explicit] The conclusion states that future work includes "improving the information retrieval process that complements our natural language input."
- Why unresolved: The current experimental setup assumed contextual information was already available (provided by annotators) rather than automatically retrieving it.
- What evidence would resolve it: Implementation of a retrieval component that improves CQA performance metrics beyond the baselines established using static annotated context.

### Open Question 2
- Question: How can the framework effectively address the subjectivity inherent in answering certain critical questions?
- Basis in paper: [explicit] The conclusion identifies "exploring effective ways of dealing with question subjectivity" as a direction for future work.
- Why unresolved: The error analysis revealed that questions such as "Is the generalisation strong?" caused significant errors because they rely on subjective judgment rather than objective facts.
- What evidence would resolve it: A methodology or model update that increases accuracy on questions previously identified as highly subjective in the error analysis.

### Open Question 3
- Question: How can the Argumentation Scheme Classification (ASC) module be adapted to handle enthymematic arguments in natural dialogue where structural premises are missing?
- Basis in paper: [inferred] The Limitations section notes a significant performance drop when classifying real dialogue arguments, as "enthymemes" make the ASC task "significantly harder."
- Why unresolved: The high performance (99.2 F1) relies on "textbook-like" arguments with clear structures, which are absent in the "heterogeneous" nature of human communication.
- What evidence would resolve it: A modified framework that maintains high classification accuracy on incomplete, natural dialogue datasets like QT30.

## Limitations

- The framework shows significant performance degradation on real-world dialogue arguments (44.8% F1) compared to textbook-like arguments (99.2% F1), highlighting limitations with enthymematic reasoning patterns.
- Binary critical question answering may oversimplify complex validity judgments, particularly for subjective questions requiring nuanced evaluation beyond simple yes/no answers.
- The predefined argumentation schemes and critical questions may not capture all forms of misinformation, potentially creating blind spots for novel argumentative patterns not covered by the existing theoretical framework.

## Confidence

- **High Confidence**: The theoretical foundation of combining argumentation schemes with critical questions for explainable analysis is well-established in argumentation theory literature.
- **Medium Confidence**: The experimental results on the textbook-like NLAS-CQ corpus are robust and reproducible, but their applicability to real-world scenarios remains uncertain.
- **Medium Confidence**: The performance advantage of the fine-tuned RoBERTa model over generative models like GPT-4 for the CQA task is demonstrated but may depend on the specific dataset characteristics.

## Next Checks

1. Test the ASC module on a small sample of real-world arguments (news articles, social media posts) to quantify the enthymeme-related performance gap beyond the QT30 corpus.
2. Conduct a user study to evaluate whether the critical questions and their answers provide meaningful explanations to non-expert users, validating the interpretability assumption.
3. Extend the framework to include an additional module that attempts to identify and reconstruct missing premises in enthymematic arguments before scheme classification.