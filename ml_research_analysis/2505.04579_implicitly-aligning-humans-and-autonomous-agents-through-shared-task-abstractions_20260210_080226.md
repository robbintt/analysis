---
ver: rpa2
title: Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions
arxiv_id: '2505.04579'
source_url: https://arxiv.org/abs/2505.04579
tags:
- agents
- agent
- humans
- each
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of zero-shot coordination in
  human-agent teaming, where autonomous agents struggle to quickly adapt to new and
  unfamiliar teammates. The authors propose that the lack of shared task abstractions
  is a key limiting factor and introduce HA2 (Hierarchical Ad Hoc Agents), a framework
  leveraging hierarchical reinforcement learning to mimic the structured approach
  humans use in collaboration.
---

# Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions

## Quick Facts
- arXiv ID: 2505.04579
- Source URL: https://arxiv.org/abs/2505.04579
- Reference count: 12
- Agents trained on human data were significantly preferred by human partners for fluency, trustworthiness, and cooperativeness

## Executive Summary
This paper addresses the challenge of zero-shot coordination in human-agent teaming, where autonomous agents struggle to quickly adapt to new and unfamiliar teammates. The authors propose that the lack of shared task abstractions is a key limiting factor and introduce HA2 (Hierarchical Ad Hoc Agents), a framework leveraging hierarchical reinforcement learning to mimic the structured approach humans use in collaboration. HA2 consists of a Manager that focuses on high-level task synchronization and a Worker that executes low-level sub-tasks. Experiments in the Overcooked environment demonstrate that HA2 significantly outperforms existing baselines when paired with both unseen agents and humans, achieving improvements of over 18% in mean round score. HA2 also shows better resilience to environmental changes and is significantly preferred by humans for its fluency, trustworthiness, and cooperativeness.

## Method Summary
The method employs a hierarchical reinforcement learning framework where a Manager agent selects from 12 possible sub-tasks (e.g., "pick up onion," "put onion in pot") while a Worker agent executes primitive actions to complete these sub-tasks. The Manager uses a PPO agent with action masking to prevent selection of impossible sub-tasks, while the Worker uses a PPO agent trained on sparse rewards for sub-task completion. The framework is trained in the Overcooked environment with various partner distributions including Behavioral Cloning partners (trained on human data) and Fictitious Co-Play populations (diverse PPO agents). Training involves separate optimization of the Worker on sub-task completion, followed by end-to-end training of the Manager with the frozen Worker.

## Key Results
- HA2 achieves an 18.5% improvement in mean round score compared to baselines when paired with unseen agents
- HA2 trained on human data (BCP) is significantly preferred by human partners for fluency, trustworthiness, and cooperativeness
- HA2 demonstrates superior generalization to new environmental layouts, outperforming baselines by more than 10.5x
- HA2 shows reduced sensitivity to random initialization noise compared to flat architectures

## Why This Works (Mechanism)

### Mechanism 1: Abstraction-Level Overfitting Prevention
Decoupling high-level strategy from low-level execution prevents the agent from overfitting to spurious correlations in training partners' micro-behaviors. In flat architectures, agents may learn brittle policies like "wait for teammate to face left before acting." By separating a Manager (strategy) from a Worker (execution), the Worker is not rewarded based on teammate behaviors, only on sub-task completion. The Manager, in turn, has no control over low-level movement, forcing it to learn robust, state-based coordination signals rather than trajectory-based ones.

### Mechanism 2: Cognitive Alignment via Human-Interpretable Sub-Tasks
Aligning the agent's action space with human-interpretable "sub-tasks" (e.g., "get onion") improves human-agent fluency by making the agent's intent legible. Humans naturally decompose tasks hierarchically. When the agent operates at this abstract level (Manager outputs sub-tasks), its behavior becomes more predictable and understandable to a human partner, facilitating implicit coordination.

### Mechanism 3: Compositional Generalization
Hierarchical structures enable better generalization to environmental shifts (layout changes) because low-level skills are learned as reusable, goal-conditioned primitives. The Worker learns general motor skills (e.g., "navigate to dispenser") that function regardless of layout, while the Manager adapts the sequence of these skills. This allows the system to handle layout changes that would break a flat policy trained on specific paths.

## Foundational Learning

- **Concept**: Hierarchical Reinforcement Learning (HRL)
  - **Why needed here**: The core of HA2 relies on a two-tier hierarchy (Manager/Worker). You must understand temporal abstraction—how a high-level policy selects goals over longer time scales while a low-level policy executes actions.
  - **Quick check question**: Can you explain the difference between a "primitive action" (e.g., move left) and a "temporal abstract action" (e.g., put onion in pot)?

- **Concept**: Zero-Shot Coordination (ZSC)
  - **Why needed here**: The problem is not just learning to cooperate, but cooperating with *unseen* partners (humans or new AI) without retraining.
  - **Quick check question**: Why does standard "self-play" (training an agent against itself) often fail when deploying to a human partner?

- **Concept**: Action Masking
  - **Why needed here**: The HA2 Manager selects from 12 possible sub-tasks. The system uses masking to prevent the Manager from selecting impossible sub-tasks (e.g., "place onion" when holding nothing), which is critical for sample efficiency.
  - **Quick check question**: How does masking invalid actions simplify the learning problem for the Manager?

## Architecture Onboarding

- **Component map**: Environment -> Manager (PPO, 12 sub-tasks with masking) -> Worker (PPO, primitive actions) -> Environment
- **Critical path**:
  1. Environment generates observation.
  2. **Manager** filters valid sub-tasks via mask and selects one (e.g., "Put onion in pot").
  3. **Worker** receives observation augmented with the location of valid "pot" interact points.
  4. **Worker** executes atomic actions until "Interact" is triggered or timeout occurs.
  5. Reward flows back: +20 to Manager for soup served; +1/-1 to Worker for sub-task success/failure.
- **Design tradeoffs**:
  - **Hardcoded vs. Learned Abstractions**: HA2 uses hand-defined sub-tasks (Section 3.2). This guarantees human interpretability but requires domain knowledge.
  - **Manager Frequency**: The paper selects sub-tasks at every timestep (Section 3.3) rather than per episode. This improves sample efficiency but adds compute overhead.
- **Failure signatures**:
  - **Aliasing**: If two distinct states look identical in the Manager's observation but require different sub-tasks, the Manager will fail.
  - **Worker Timeout**: If the Worker cannot complete the sub-task (e.g., path blocked), the loop may stall if not handled by a timeout reset.
  - **Over-conservatism**: HA2_BCP sometimes underperforms in "Forced Coordination" layouts (Section 5.2) if the training partner distribution (e.g., untrained agents) skews the Manager toward safe, low-reward strategies.
- **First 3 experiments**:
  1. **Worker Validation**: Train the Worker in isolation in the modified environment. Verify it can achieve >95% success rate on "Pick up onion" and "Place onion in pot" sub-tasks in a static layout.
  2. **Manager Overfitting Test**: Train the Manager with a single deterministic teammate. Verify if the team score drops significantly when swapping that teammate for a random agent (testing the overfitting hypothesis).
  3. **Layout Shift Robustness**: Train on the "Cramped Room" layout and test immediately on the "Asymmetric Advantages" layout without retraining. Compare the drop in score between HA2 and a flat PPO baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can explicit mental models of teammate sub-tasks be integrated into HA2 to improve planning?
- Basis in paper: [explicit] Section 6.3 states as future work: "incorporate explicit mental models of teammate sub-tasks into agent planning."
- Why unresolved: The current Manager selects sub-tasks based on shared abstractions for alignment but lacks an explicit model of the teammate's internal state or future intentions.
- What evidence would resolve it: Integrating a theory-of-mind module into the Manager that predicts teammate sub-tasks, resulting in statistically significant performance gains over the baseline HA2.

### Open Question 2
- Question: Does utilizing human behavior data in training provide more fluid gameplay than population-based training?
- Basis in paper: [explicit] Section 5.2 notes BCP (trained on humans) was perceived better than FCP (trained on agents) despite lower scores, stating "we leave a more thorough investigation... to future work."
- Why unresolved: It is unclear if the subjective preference for the BCP baseline stems from the training data distribution (human vs. agent) or other architectural factors.
- What evidence would resolve it: Ablation studies isolating training data origin while normalizing the architecture, correlated with human subjective ratings of fluency and trust.

### Open Question 3
- Question: Can the HA2 framework facilitate more effective explicit communication in collaborative games?
- Basis in paper: [explicit] Section 6.3 suggests "HA2 shows promise as a framework to investigate human-agent communication... it is much easier to communicate at sub-task-level."
- Why unresolved: The current study evaluates implicit coordination only; the utility of the Manager's abstractions for explicit communication channels remains untested.
- What evidence would resolve it: Implementing a communication protocol based on Manager outputs and measuring improvements in human-agent team performance and objective understanding.

## Limitations
- The hierarchical structure relies on hand-crafted sub-tasks specific to Overcooked, raising questions about scalability to more complex domains
- The study uses a limited number of human participants (10), which constrains statistical power for human preference claims
- Performance improvements may partly reflect the specific training distributions used rather than inherent architectural advantages

## Confidence
**High Confidence**: The core mechanism of decoupling high-level strategy from low-level execution (Mechanism 1) is well-supported by both theoretical reasoning and empirical results showing reduced variance and improved generalization.

**Medium Confidence**: The cognitive alignment claims (Mechanism 2) are moderately supported by human preference data, though the small sample size limits generalizability. The improvement in human-rated fluency and cooperativeness is consistent with the mechanism but requires larger-scale validation.

**Medium Confidence**: The compositional generalization claims (Mechanism 3) are demonstrated on the Overcooked layouts tested, but the extent to which this generalizes to substantially different task domains remains uncertain.

## Next Checks
1. **Sample Efficiency Validation**: Measure the number of samples required for HA2 to match the performance of flat baselines, controlling for computational resources used during training.

2. **Domain Transfer Test**: Implement HA2 with automatically discovered sub-tasks (rather than hand-crafted ones) in a different collaborative environment to assess scalability.

3. **Human Scale-Up Study**: Replicate the human-agent teaming experiments with a larger, more diverse participant pool (n≥50) and measure objective coordination metrics alongside subjective preferences.