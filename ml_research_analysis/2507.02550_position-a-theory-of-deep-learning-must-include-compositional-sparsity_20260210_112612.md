---
ver: rpa2
title: 'Position: A Theory of Deep Learning Must Include Compositional Sparsity'
arxiv_id: '2507.02550'
source_url: https://arxiv.org/abs/2507.02550
tags:
- functions
- learning
- sparse
- compositional
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that compositional sparsity is a fundamental
  property underlying the success of deep neural networks (DNNs) in high-dimensional
  learning tasks. The authors show that all efficiently Turing-computable functions
  can be decomposed into a small set of constituent functions, each depending on only
  a low-dimensional subset of inputs.
---

# Position: A Theory of Deep Learning Must Include Compositional Sparsity

## Quick Facts
- arXiv ID: 2507.02550
- Source URL: https://arxiv.org/abs/2507.02550
- Reference count: 29
- Primary result: Compositional sparsity explains why deep networks avoid the curse of dimensionality

## Executive Summary
This paper argues that compositional sparsity is a fundamental property underlying the success of deep neural networks in high-dimensional learning tasks. The authors show that all efficiently Turing-computable functions can be decomposed into a small set of constituent functions, each depending on only a low-dimensional subset of inputs. This property explains why DNNs can avoid the curse of dimensionality in approximation: while shallow networks require exponentially many parameters to approximate such functions, deep networks can mimic the compositional structure with polynomial complexity. However, the paper also highlights that this property alone does not guarantee efficient learnability, as learning general compositionally sparse functions is computationally hard in the worst case.

## Method Summary
This is a theoretical position paper with proofs and conjectures rather than empirical experiments. The paper establishes that efficiently Turing-computable functions are compositionally sparse (Theorem 3.2), demonstrates that deep networks can achieve polynomial approximation complexity while shallow networks require exponential complexity (Theorem 3.3), and proposes conjectures about how chain-of-thought reasoning exploits compositional structure. The theoretical framework connects compositional sparsity to approximation, optimization, and generalization in deep learning.

## Key Results
- All efficiently Turing-computable functions are compositionally sparse (Theorem 3.2)
- Deep networks achieve polynomial complexity O(dε⁻²) while shallow networks require exponential complexity O(ε⁻ᵈ) for approximating compositionally sparse functions (Theorem 3.3)
- Chain-of-thought reasoning may improve learnability by decomposing dense problems into sparse sub-tasks (Conjecture 1)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Dimensionality Reduction
- Deep Neural Networks (DNNs) avoid the curse of dimensionality in approximation by mirroring the directed acyclic graph (DAG) structure of compositionally sparse functions.
- The paper argues that efficiently Turing-computable functions decompose into constituents relying on small subsets of inputs (Definition 3.1). Deep networks exploit this by assigning layers to approximate these constituents sequentially, reducing complexity from exponential O(ε⁻ᵈ) in shallow networks to polynomial O(dε⁻²) in deep networks (Theorem 3.3).
- Core assumption: The target function is efficiently Turing-computable (in complexity class FP) and thus compositionally sparse.
- Evidence anchors: [Section 3.2] Theorem 3.3 explicitly contrasts the exponential complexity of shallow networks with the polynomial complexity of deep networks for compositionally sparse functions.

### Mechanism 2: Chain-of-Thought as Sparse Sub-tasking
- Chain-of-Thought (CoT) reasoning improves learnability by decomposing a dense, high-dimensional prediction problem into a sequence of sparse, low-dimensional prediction steps.
- CoT does not solve a single dense mapping f(x) but rather a sequence f_θ(…f_θ(x)…) (Eq. 5). The paper suggests this exploits the "universality of auto-regressive predictors" (Theorem 4.2), where each step solves a simpler constituent function with limited dependencies (c << d), circumventing the hardness of learning general compositional functions.
- Core assumption: The intermediate reasoning steps (tokens) are available or can be inferred, effectively providing supervision for the sparse constituents.
- Evidence anchors: [Section 4.3] Conjecture 1 posits that CoT explicitly decomposes sparse sub-problems to overcome the complexity of one-shot learning.

### Mechanism 3: Symmetry Reduction via Architectural Sparsity
- Architectural biases (like CNNs) improve optimization and generalization by reducing the symmetry group of the loss landscape.
- Dense networks suffer from complex loss landscapes due to permutation symmetries. By enforcing local connectivity (as in CNNs), the network restricts the parameter space to sparse interactions, reducing degenerate minima and allowing tighter generalization bounds based on weight sparsity rather than just convolution.
- Core assumption: The problem domain (e.g., images) exhibits the locality required by the sparse architectural prior.
- Evidence anchors: [Section 4.2] "CNNs address compositional sparsity... because the filters are constrained to local patches... ensuring the network computes a compositionally sparse function."

## Foundational Learning

- **Concept: Curse of Dimensionality (Approximation)**
  - Why needed here: This is the central barrier the paper attempts to break. Understanding that classical methods require exponentially infinite parameters to approximate high-dimensional functions is necessary to value the "compositional sparsity" solution.
  - Quick check question: Why does a shallow network fail to approximate a generic high-dimensional function with a polynomial number of parameters?

- **Concept: Complexity Class FP (Function Problems)**
  - Why needed here: The paper restricts its claims to "efficiently Turing-computable" functions (FP). One must grasp that this excludes functions that are computationally intractable to even compute, let alone learn.
  - Quick check question: Does the theory claim all functions are compositionally sparse, or only those computable in polynomial time?

- **Concept: Directed Acyclic Graphs (DAGs)**
  - Why needed here: DAGs are the formal representation of compositional sparsity used throughout the paper (Section 3.1). The depth of the DAG corresponds to network depth, and node in-degree corresponds to the "sparsity" constant c.
  - Quick check question: In the context of the paper, what does the "in-degree" of a node in a DAG represent regarding the function's inputs?

## Architecture Onboarding

- **Component map:** Input high-dimensional vector X -> Deep Network (depth L approximates DAG depth, layers enforce local connectivity/sparsity) -> Output prediction Y -> Optimizer SGD (potentially with intermediate supervision/CoT)

- **Critical path:**
  1. Verify Computability: Confirm the target function is in FP (theoretical assumption)
  2. Match Depth: Ensure network depth is sufficient to represent the hierarchical composition of the DAG (Theorem 3.3)
  3. Enforce Sparsity: Use architectural priors (Convolutions, Attention) or training methods (CoT) to ensure layers focus on low-dimensional subsets of features

- **Design tradeoffs:**
  - Dense vs. Sparse Weights: Dense weights are universal but hard to optimize due to symmetries; Sparse weights (CNNs) optimize better but require domain-specific locality
  - End-to-End vs. CoT: End-to-end learning is "dense" and computationally hard in the worst case; CoT is "sparse" and easier to learn but requires intermediate data or generation steps

- **Failure signatures:**
  - Exponential Scaling: Parameter count explodes as input dimension increases (indicates failure to leverage sparsity)
  - Symmetry Stalls: Optimization stuck in degenerate minima (common in dense, unstructured networks)
  - Amortized Inference Failure: Model memorizes training data but fails to generalize on compositional shifts (suggests it learned a dense lookup rather than a sparse DAG)

- **First 3 experiments:**
  1. Depth vs. Width Efficiency: Train a shallow (wide) network and a deep (narrow) network on a known compositionally sparse task (e.g., hierarchical parity). Verify if the deep network converges with significantly fewer parameters.
  2. CoT Necessity: Compare direct answer prediction vs. Chain-of-Thought on a multi-hop reasoning task. Measure sample complexity to see if CoT reduces the data required for learning.
  3. Sparsity Generalization: Train a dense CNN and a sparse "Compositional" network (or a network with L1 regularization on weights) and compare generalization bounds on a high-dimensional image classification task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimal amount and type of intermediate supervision (such as Chain-of-Thought steps) required to efficiently learn compositionally sparse functions without exponential sample complexity?
- Basis in paper: [explicit] Section 4.4 explicitly asks "How Much Supervision Is Needed for Efficient Learning?" noting that while intermediate supervision helps, the necessary thresholds are undefined.
- Why unresolved: While the paper proves these functions are representable, learning them is computationally hard; the precise "supervision budget" needed to make learning tractable remains unknown.
- What evidence would resolve it: Theoretical bounds or empirical scaling laws linking the number of intermediate supervision steps to sample complexity for specific compositional DAGs.

### Open Question 2
- Question: Is the implicit bias of Stochastic Gradient Descent (SGD) alone sufficient to discover sparse constituent functions, or are additional architectural biases or explicit constraints necessary?
- Basis in paper: [explicit] Section 4.4 asks "Can SGD Alone Discover Compositional Structure?" distinguishing between recovering input support and recovering the full hierarchical decomposition.
- Why unresolved: Recent work shows SGD recovers input support, but it remains unproven whether this extends to recovering the internal sparse structure of the DAG without manual sparsity induction.
- What evidence would resolve it: Proofs demonstrating that SGD reliably converges to the correct sparse constituent functions on standard architectures, or evidence that it fails without explicit structural priors.

### Open Question 3
- Question: When a target function admits multiple valid compositional sparse decompositions, which specific hierarchical structure does a neural network select, and what drives this preference?
- Basis in paper: [explicit] Section 4.4 asks "How Do Neural Networks Choose Among Multiple Decompositions?" regarding the factors influencing the learned hierarchy.
- Why unresolved: A function can often be composed in different ways (different DAGs); understanding if networks favor certain topologies due to optimization dynamics is critical for interpretability.
- What evidence would resolve it: Mechanistic interpretability studies mapping learned internal features to specific DAG topologies, correlated with variations in initialization and architecture.

## Limitations

- The theory applies only to efficiently Turing-computable functions (complexity class FP), leaving unclear whether compositional sparsity extends to intractable or chaotic functions
- While the paper proves polynomial approximation complexity for deep networks on compositionally sparse functions, it does not establish polynomial learnability - learning such functions remains computationally hard in the worst case
- The connection between theoretical compositional sparsity and practical architectural choices (CNNs, Transformers) requires empirical validation

## Confidence

- High: Theorem 3.2 (efficiently computable functions are compositionally sparse) - this is a formal proof
- Medium: Theorem 3.3 (deep networks achieve polynomial complexity) - the proof is sound but assumes the DAG structure exists
- Low: Conjecture 1 (CoT reasoning exploits compositional sparsity) - this is a hypothesis requiring empirical validation

## Next Checks

1. Construct synthetic compositional tasks with varying DAG depths and verify whether deep networks consistently outperform shallow networks in parameter efficiency
2. Design experiments comparing direct prediction vs. Chain-of-Thought approaches on compositional reasoning tasks, measuring sample complexity and generalization
3. Test whether imposing architectural sparsity constraints (e.g., L1 regularization, local connectivity) improves optimization convergence and generalization bounds in high-dimensional settings