---
ver: rpa2
title: 'OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs'
arxiv_id: '2510.24663'
source_url: https://arxiv.org/abs/2510.24663
tags:
- tool
- wang
- zhang
- data
- tools
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OrchDAG, a synthetic data generation pipeline
  for training models to perform complex multi-turn tool orchestration in agentic
  systems. The pipeline models tool execution as directed acyclic graphs (DAGs) with
  controllable complexity, enabling the creation of challenging multi-turn scenarios
  where tools have dependencies, outputs feed into subsequent tools, and failures
  must be handled gracefully.
---

# OrchDAG: Complex Tool Orchestration in Multi-Turn Interactions with Plan DAGs

## Quick Facts
- arXiv ID: 2510.24663
- Source URL: https://arxiv.org/abs/2510.24663
- Reference count: 39
- Primary result: Introduces a synthetic data generation pipeline for training models on complex multi-turn tool orchestration, achieving 40% accuracy on single-turn tasks with graph-based rewards.

## Executive Summary
This paper addresses the challenge of multi-turn tool orchestration in agentic systems by introducing OrchDAG, a synthetic data generation pipeline that models tool execution as Directed Acyclic Graphs (DAGs). The framework enables controlled generation of complex multi-turn scenarios with explicit dependencies between tools, outputs feeding into subsequent tools, and error recovery mechanisms. The authors propose a graph-based reward using weighted Graph Edit Distance to guide reinforcement learning with verifiable rewards (RLVR), improving training efficiency compared to coarse binary rewards.

## Method Summary
OrchDAG generates synthetic data by sampling DAG templates with controllable height and width parameters, then synthesizing tools layer-by-layer with explicit input-output dependencies. The pipeline uses LangGraph to create DAGs, seeds tools from APIGEN and TOOLACE datasets, and generates user queries conditioned on the DAG structure. Training employs GRPO (Group Relative Policy Optimization) with a hybrid reward combining format verification and GED-based graph similarity. The method handles multi-turn scenarios by appending three scenario types (irrelevant query, dependent follow-up, error recovery) to the final DAG node.

## Key Results
- Closed-source models like GPT-4o achieve ~24% accuracy on zero-shot DAG prediction, while open-source models lag significantly
- Graph-based GED rewards improve RLVR training, with Qwen2.5-7B achieving up to 40% accuracy on single-turn tasks and 35% on multi-turn scenarios
- Coarse-grained binary rewards fail to guide learning, achieving 0% accuracy even after 15 training steps
- Performance collapses around step 51 with GRPO due to entropy collapse, mitigated by DAPO optimizer

## Why This Works (Mechanism)

### Mechanism 1
Representing tool execution as a DAG provides a high-level planning blueprint that improves coordination of dependent, parallel, and sequential tool calls. The DAG explicitly encodes dependencies between tools, enabling the model to reason about execution order before making any calls. This decouples planning from execution, avoiding reactive interleaving.

### Mechanism 2
Graph Edit Distance (GED)-based rewards provide denser, structure-aware credit assignment than coarse binary rewards. Instead of only rewarding exact DAG matches, GED measures similarity through node/edge operations, giving partial credit for correct substructures and creating a smoother learning signal.

### Mechanism 3
Controllable DAG complexity via hyperparameters (height, width) enables curriculum-style difficulty scaling for training. Height controls sequential depth while width controls parallel fan-out, allowing the model to gradually learn from simpler to more complex orchestration patterns.

## Foundational Learning

- **Concept: Directed Acyclic Graphs (DAGs) and Topological Sorting**
  - Why needed here: Core representation for tool execution plans. Understanding that DAGs enforce acyclicity and topological order determines valid execution sequences.
  - Quick check question: Given tools A→B, A→C, B→D, C→D, list two valid topological orders.

- **Concept: Graph Edit Distance (GED)**
  - Why needed here: Foundation for the reward mechanism. GED quantifies graph similarity via edit operations (insert/delete nodes/edges).
  - Quick check question: What is the GED between a single-node graph and an empty graph?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: Training paradigm used (GRPO-style). Understanding sparse vs. dense rewards, rollout exploration, and KL divergence constraints is essential for interpreting results.
  - Quick check question: Why might a sparse binary reward (0/1 for exact match) fail to guide learning in complex multi-step tasks?

## Architecture Onboarding

- **Component map:** Data Generation Pipeline (LangGraph) -> DAG Template Generation -> Tool Synthesis with Dependencies -> User Query Generation -> Verification Layer -> Training Loop (GRPO) -> Inference (DAG Prediction)
- **Critical path:** DAG template generation → tool synthesis with correct dependency wiring → verification → reward computation during GRPO training
- **Design tradeoffs:**
  - Synthetic vs. real data: Synthetic enables controllable complexity and guaranteed ground-truth DAGs, but may not capture implicit real-world dependencies
  - Rollout count vs. compute cost: n=8 rollouts improve exploration but increase GPU hours 2×
  - KL penalty vs. entropy regularization: DAPO maintains higher entropy and avoids performance collapse
- **Failure signatures:**
  - Entropy collapse: Performance drops sharply around step 51 with GRPO
  - Zero query-level accuracy with step-level rewards: Indicates local correctness without global coherence
  - Verification failures during generation: 30-40% of samples require regeneration
- **First 3 experiments:**
  1. Baseline difficulty probe: Run GPT-4o, Claude 4, Qwen2.5-7B on test set with 0/1/3 shots
  2. Reward ablation: Train with coarse binary reward vs. GED-based reward
  3. Entropy monitoring: Track entropy during GRPO training and test DAPO as alternative optimizer

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the OrchDAG framework be extended to capture and manage implicit dependencies in multi-turn interactions, such as state changes in file operations or computer-use tasks?
- **Open Question 2:** How does model performance and training stability scale when extending the evaluation from two-turn interactions to longer conversational horizons?
- **Open Question 3:** Does training on OrchDAG's synthetic queries, which are generated conditioned on a specific graph structure, generalize effectively to the ambiguity and phrasing of real-world user queries?

## Limitations

- The synthetic dataset may not capture nuanced dependencies and implicit state changes present in real-world scenarios
- GED-based rewards assume node equivalence based solely on tool name and parameters, potentially missing semantic similarities
- Performance gap between closed-source and open-source models suggests fundamental architectural limitations
- 30-40% verification failure rate during data generation indicates potential brittleness in the synthetic pipeline

## Confidence

- **High Confidence:** DAG representation mechanism for explicit dependency modeling; controlled complexity design via height/width hyperparameters
- **Medium Confidence:** Effectiveness of GED-based rewards for learning improvement; 40% accuracy achievement on single-turn tasks
- **Low Confidence:** Scalability to cyclic dependencies, implicit state management, or truly open-ended multi-turn conversations

## Next Checks

1. **Real-World Transfer Test:** Evaluate trained Qwen2.5-7B model on a small real-world multi-turn tool orchestration dataset to assess synthetic-to-real generalization
2. **GED Reward Sensitivity:** Systematically vary the α weighting parameter between format reward and GED reward to identify optimal balance
3. **Implicit Dependency Handling:** Design synthetic test cases with implicit dependencies (e.g., file system state changes) to probe limits of DAG assumption