---
ver: rpa2
title: 'Integrating Cognitive Processing Signals into Language Models: A Review of
  Advances, Applications and Future Directions'
arxiv_id: '2504.06843'
source_url: https://arxiv.org/abs/2504.06843
tags:
- language
- data
- attention
- cognitive
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews recent advancements in integrating cognitive
  processing signals, particularly eye-tracking (ET) data, into Language Models (LMs)
  and Multimodal Large Language Models (MLLMs). It highlights how incorporating user-centric
  cognitive signals can address challenges like data scarcity, reduce environmental
  costs of training large-scale models, enable efficient data augmentation, and improve
  human alignment and reduce hallucinations.
---

# Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions

## Quick Facts
- **arXiv ID:** 2504.06843
- **Source URL:** https://arxiv.org/abs/2504.06843
- **Reference count:** 40
- **Primary result:** This review paper surveys methods integrating eye-tracking data into LMs and MLLMs, showing cognitive signals can reduce data needs, improve human alignment, and mitigate hallucinations.

## Executive Summary
This paper reviews recent advancements in integrating cognitive processing signals, particularly eye-tracking (ET) data, into Language Models (LMs) and Multimodal Large Language Models (MLLMs). The review highlights how incorporating user-centric cognitive signals can address challenges like data scarcity, reduce environmental costs of training large-scale models, enable efficient data augmentation, and improve human alignment while reducing hallucinations. The paper emphasizes the potential of ET data in tasks like Visual Question Answering (VQA) and mitigating hallucinations in MLLMs, while also discussing emerging challenges and research trends.

## Method Summary
This is a comprehensive review paper that categorizes approaches for integrating cognitive signals into LMs by application (sentiment classification, QA, VQA, hallucination detection, human alignment), integration method (attention modification, input concatenation, multi-task learning, etc.), and data source. The paper identifies seven main integration approaches and evaluates their effectiveness across various benchmarks including GLUE, SQuAD, and VQA tasks. It discusses both organic eye-tracking data and synthetic gaze generation methods, while identifying key challenges around data availability and hardware requirements.

## Key Results
- Eye-tracking signals can guide attention mechanisms to reduce data requirements and improve learning efficiency
- Cognitive signals enable efficient data augmentation and faster convergence compared to standard training
- ET data shows promise for hallucination mitigation in MLLMs and improving human alignment through RLHF/DPO frameworks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating eye-tracking (ET) signals may guide Language Models (LMs) to allocate computational attention similarly to humans, potentially reducing the data volume required to learn semantic relationships.
- **Mechanism:** ET features (e.g., fixation duration, scanpaths) act as a supervisory signal for the Transformer's self-attention mechanism. By biasing the attention weights or masking irrelevant tokens based on human gaze, the model is constrained to focus on high-information tokens, effectively regularizing the learning process.
- **Core assumption:** Human visual attention during reading correlates with semantic importance and syntactic structure, providing a stronger learning signal than raw text frequency alone.
- **Evidence anchors:**
  - [section V.B]: Modifying Transformer attention via gaze representations in key vectors.
  - [abstract]: "Cognitive signals enable efficient data augmentation, faster convergence, and improved human alignment."
  - [corpus]: Weak direct evidence in provided corpus; neighbors focus on multimodal integration broadly rather than ET-specific attention mechanisms.
- **Break condition:** If the downstream task requires attending to "obvious" distractors or low-frequency information that humans typically skip (e.g., detecting subtle sarcasm missed by skimming), gaze-guided attention may degrade performance.

### Mechanism 2
- **Claim:** Gaze behavior serves as a proxy for implicit human feedback, offering a dense signal for aligning LLMs with user intent without relying solely on sparse explicit ratings.
- **Mechanism:** Reading measures (e.g., Total Reading Time, re-fixations) quantify user engagement or confusion. These metrics are integrated into the Reward Model (RM) of a Reinforcement Learning from Human Feedback (RLHF) pipeline or used to generate preference datasets for Direct Preference Optimization (DPO).
- **Core assumption:** Fixation duration and re-reading patterns reliably indicate user preference or perceived error (uncertainty), which can be mapped to scalar rewards.
- **Evidence anchors:**
  - [section IV.C]: Cites [22] for integrating ET measures in RLHF rewards and [47] for gaze-based DPO.
  - [section I]: "Oculomotor behavior reflects precision-weighted prediction error minimization."
  - [corpus]: Weak evidence; corpus focuses on biosensors in learning contexts but not specifically on reward modeling.
- **Break condition:** If users exhibit "stare-and-compare" behaviors or pause on errors for reasons other than preference (e.g., confusion), the reward signal becomes noisy.

### Mechanism 3
- **Claim:** Synthetic gaze data generation allows models to reap the benefits of cognitive augmentation without the inference-time latency or hardware costs of live eye-tracking.
- **Mechanism:** A secondary "gaze predictor" model (e.g., Eyettention) is trained to generate synthetic scanpaths or attention heatmaps from text/images. The primary LM is then trained on this synthetic cognitive data, decoupling the dependency on organic eye-trackers during deployment.
- **Core assumption:** Predictive models (e.g., TPP-Gaze, UniAR) can approximate human attention distributions with sufficient fidelity to transfer cognitive inductive biases to the LM.
- **Evidence anchors:**
  - [section III.A]: Discusses generative models for predicting scanpaths to address data scarcity and runtime unavailability.
  - [section V]: "Studies employ pre-trained models [22, 38-40]... to predict cognitive signals."
  - [corpus]: No direct corpus support for this specific synthesis mechanism.
- **Break condition:** If the gaze predictor systematically hallucinates attention on invisible or non-semantic features, it propagates bias to the primary LM.

## Foundational Learning

- **Concept:** **Transformer Self-Attention Mechanics**
  - **Why needed here:** The primary integration points for cognitive signals are the $Q, K, V$ matrices and attention masks within the Transformer architecture.
  - **Quick check question:** Can you explain how modifying the Key vector ($K$) in the self-attention formula changes how the Query ($Q$) attends to the sequence?

- **Concept:** **Eye-Tracking Metrics (Fixations vs. Saccades)**
  - **Why needed here:** To select the correct input features (e.g., First Fixation Duration vs. Total Reading Time) for the model, one must understand which cognitive process they represent.
  - **Quick check question:** Why might "Total Reading Time" be a better feature for sentiment analysis than "First Fixation Duration"?

- **Concept:** **Multi-Task Learning (MTL)**
  - **Why needed here:** MTL is a key strategy for using gaze data as an auxiliary loss to improve the primary LM, allowing inference without gaze hardware.
  - **Quick check question:** How does adding an auxiliary loss for "gaze prediction" prevent the model from overfitting to the specific linguistic patterns of a small dataset?

## Architecture Onboarding

- **Component map:** Tokenizer + Gaze Feature Encoder -> Transformer layers (Standard vs. Gaze-Infused Attention) -> Fusion Module (Cross-attention or Gating) -> Optional Gaze Predictor head

- **Critical path:**
  1. **Alignment:** Aligning temporal gaze points to sub-word tokens (handling tokenization mismatch)
  2. **Signal Conversion:** Converting raw coordinates to reading measures (FFD, TRT)
  3. **Integration:** Injecting these measures into the attention matrix or embedding layer

- **Design tradeoffs:**
  - **Inference dependency:** Models using gaze as **Input** (Fig 2.B) require live eye-tracking or a separate predictor during inference; models using gaze as **Supervision/MTL** (Fig 2.G) do not
  - **Data fidelity:** Organic gaze is accurate but scarce; synthetic gaze is scalable but risks amplifying predictor bias

- **Failure signatures:**
  - **Token Misalignment:** Gaze applies to words, but models use sub-words; mapping errors cause noisy gradients
  - **Attention Collapse:** Over-weighting gaze constraints might prevent the model from learning long-range dependencies that humans missed

- **First 3 experiments:**
  1. **Sanity Check (Input Injection):** Fine-tune a BERT model on a sentiment task by concatenating random noise vs. organic gaze features to verify the signal contains unique information
  2. **Ablation (Attention vs. Input):** Compare performance when integrating gaze as a simple input embedding vs. modifying the self-attention mask (Section V.B) on a GLUE task
  3. **Synthetic Transfer:** Train a gaze predictor on the ZuCo dataset, generate synthetic gaze for a larger corpus, and fine-tune the LM on this synthetic data to test scalability

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can low-fidelity data acquisition methods, such as standard webcams or mouse cursor trajectories, effectively replace high-precision eye-tracking hardware for training robust cognitive-augmented Language Models?
- **Basis in paper:** [explicit] Section VII identifies data scarcity and hardware reliance as major challenges, explicitly proposing "paradigm shifts" using webcams (e.g., WebQAmGaze) or mouse clicks as proxies for visual attention.
- **Why unresolved:** While the paper identifies these as "promising solutions" for simplifying acquisition, it does not provide evidence that these low-fidelity proxies achieve parity with specialized equipment in terms of signal quality or model performance improvement.
- **What evidence would resolve it:** Comparative benchmarks showing that models trained on webcam or mouse-proxy data perform equivalently to those trained on high-fidelity eye-tracking data across standard NLP tasks.

### Open Question 2
- **Question:** To what extent can the integration of eye-tracking signals specifically mitigate the problem of hallucinations in Multimodal Large Language Models (MLLMs)?
- **Basis in paper:** [explicit] Section VIII states that ET appears to be a "promising technique" for addressing hallucinations, noting that recent integration into multimodal frameworks "paves the way for further advancements" beyond text-based models.
- **Why unresolved:** The paper notes that most existing successful applications of hallucination detection (e.g., [53]) are text-based, and the application to multimodal frameworks is currently limited and requires further validation.
- **What evidence would resolve it:** Studies demonstrating a statistically significant reduction in factual error rates within MLLMs when utilizing gaze-augmented attention mechanisms compared to standard training paradigms.

### Open Question 3
- **Question:** What data generation or augmentation strategies are required to overcome the current bottleneck in training sophisticated models that predict human gaze behavior?
- **Basis in paper:** [explicit] Section VII highlights that while progress has been made in architectures (e.g., UniAR), "the primary limitation remains the lack of sufficient (or sufficiently rich) training data" for predicting gaze behavior.
- **Why unresolved:** Current datasets are insufficient to train predictive models that can generalize well, limiting the availability of synthetic cognitive signals during inference.
- **What evidence would resolve it:** The creation of large-scale, richly annotated datasets that enable predictive models to generalize across diverse textual and visual stimuli without task-specific configurations.

## Limitations
- Limited empirical validation across diverse tasks and datasets for comparative effectiveness of different integration approaches
- Lack of quantitative estimates for computational savings from gaze-guided training compared to standard approaches
- Insufficient evaluation of synthetic gaze data quality and how predictor errors propagate to LM performance

## Confidence
- **High Confidence:** The fundamental premise that eye-tracking data captures meaningful cognitive processing signals during reading is well-established in psycholinguistics literature
- **Medium Confidence:** The mechanisms described for integrating gaze signals into Transformer architectures are technically sound, though empirical validation would strengthen these claims
- **Low Confidence:** Claims about synthetic gaze generation scalability and specific performance improvements for hallucination mitigation in MLLMs lack direct empirical support

## Next Checks
1. **Cross-Modal Alignment Validation:** Implement a controlled experiment comparing token-to-gaze alignment accuracy between word-level and subword-level tokenization schemes across multiple eye-tracking datasets to quantify mapping error rates
2. **Synthetic vs. Organic Performance Gap:** Train an identical LM architecture on identical text data, once with organic gaze features and once with synthetic gaze features generated by a pre-trained gaze predictor, then measure performance deltas on downstream tasks
3. **Attention Mask Ablation Study:** Systematically vary the strength of gaze-guided attention masking in Transformer layers (0% to 100% gaze influence) on a single GLUE task to identify the optimal integration point and detect potential attention collapse thresholds