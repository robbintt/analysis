---
ver: rpa2
title: 'Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable
  Language Representations'
arxiv_id: '2509.05060'
source_url: https://arxiv.org/abs/2509.05060
tags:
- language
- languages
- uriel
- typological
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ENTROPY2VEC, a novel framework that derives
  cross-lingual language representations by leveraging the entropy of monolingual
  language models. Unlike traditional typological inventories that suffer from feature
  sparsity and static snapshots, ENTROPY2VEC uses the inherent uncertainty in language
  models to capture typological relationships between languages.
---

# Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations

## Quick Facts
- **arXiv ID:** 2509.05060
- **Source URL:** https://arxiv.org/abs/2509.05060
- **Reference count:** 20
- **Primary result:** Introduces ENTROPY2VEC framework that derives cross-lingual language representations from monolingual LM entropy, achieving competitive performance in downstream multilingual NLP tasks

## Executive Summary
This paper presents ENTROPY2VEC, a novel framework that generates cross-lingual language representations by leveraging the entropy of monolingual language models. Unlike traditional typological inventories that suffer from feature sparsity and static snapshots, ENTROPY2VEC uses the inherent uncertainty in language models to capture typological relationships between languages. By training a language model on a single language, the entropy of its predictions reflects its structural similarity to other languages. This approach yields dense, non-sparse language embeddings that are adaptable to different timeframes and free from missing values. Empirical evaluations demonstrate that ENTROPY2VEC embeddings align with established typological categories and achieve competitive performance in downstream multilingual NLP tasks, such as those addressed by the LinguAlchemy framework.

## Method Summary
ENTROPY2VEC derives language embeddings by training separate monolingual GPT-2 models (4 layers, 512 dim, 8 heads) on 33 languages from the Glot500c corpus, then computing cross-entropy of each model on all other languages' test sets. The resulting cross-entropy matrix forms dense language vectors, which are clustered using DBSCAN to produce typological trees. These vectors are integrated with the LinguAlchemy regularization framework for downstream multilingual tasks, evaluated on SIB-200 and MASSIVE benchmarks. The approach eliminates the sparsity issues of traditional typological inventories while capturing structural relationships through statistical patterns learned by language models.

## Key Results
- ENTROPY2VEC achieves Robinson-Foulds distance of 17.0 vs. Glottolog, demonstrating reasonable alignment with established typological trees
- In LinguAlchemy framework, ENTROPY2VEC achieves 81.6 average accuracy on SIB-200, competitive with best URIEL+ baseline (81.8)
- Provides dense language representations without missing values, solving sparsity issues inherent in traditional typological databases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cross-entropy of a monolingual LM on another language's corpus reflects structural similarity between languages.
- **Mechanism:** When a language model trained on language Li evaluates corpus Dj, lower perplexity indicates Dj shares statistical patterns with Li (similar syntax, morphology, vocabulary distributions). The model's "surprise" quantifies linguistic distance without explicit feature engineering.
- **Core assumption:** The statistical patterns learned by LMs encode typological properties (phonological, morphological, syntactic) that transfer predictably across related languages.
- **Evidence anchors:** [abstract] "Low entropy indicates high similarity, while high entropy suggests greater divergence." [section 3.2] Uses monolingual LMs to build language vectors. Related work suggests bilingual models acquire shared grammatical structures.
- **Break condition:** If monolingual LMs primarily capture surface-level statistics rather than deeper structural properties, entropy signals would reflect corpus similarity rather than true typological relationships.

### Mechanism 2
- **Claim:** Dense language vectors derived from cross-lingual entropy outperform sparse typological inventories for low-resource languages.
- **Mechanism:** Traditional databases like WALS have missing values for underrepresented languages. ENTROPY2VEC produces complete n-dimensional vectors regardless of resource level, as long as sufficient text exists to train a basic LM. This eliminates sparsity-induced noise in downstream tasks.
- **Core assumption:** Even limited monolingual corpora can train LMs that capture enough statistical structure to produce meaningful entropy patterns.
- **Evidence anchors:** [abstract] "This approach yields dense, non-sparse language embeddings that are adaptable to different timeframes and free from missing values." [section 4.1] Dataset includes low-resource languages like Acehnese (29,495 sentences) and Madurese (9,055 sentences). [table 5] Shows ENTROPY2VEC produces dense representations without missing features.
- **Break condition:** If very low-resource corpora produce unstable or noisy entropy measurements, vector quality would degrade rather than provide consistent signals.

### Mechanism 3
- **Claim:** Entropy-based vectors integrate with regularization frameworks (LinguAlchemy) to improve cross-lingual transfer, particularly for unseen languages.
- **Mechanism:** Language vectors serve as regularization signals during fine-tuning, guiding models toward linguistically-informed parameter spaces. ENTROPY2VEC provides these signals without requiring expert-curated features, enabling transfer to languages absent from training data.
- **Core assumption:** The regularization framework can effectively utilize entropy-derived vectors as proxies for typological knowledge.
- **Evidence anchors:** [section 5.1] "LinguAlchemy utilize language vectors to bring better cross-lingual generalization for low-resource and unseen languages." [table 3] XLM-R with ENTROPY2VEC achieves 81.6 average accuracy vs. 81.8 for best URIEL+ variant on SIB-200. [section 5.2] Notes ENTROPY2VEC shows "competitive accuracy (81.3) compared to URIEL+ (81.5, the best baseline)."
- **Break condition:** If regularization benefits stem primarily from having any consistent language-specific signal rather than entropy-derived properties specifically, simpler language identifiers might achieve similar results.

## Foundational Learning

- **Concept: Cross-entropy and perplexity as uncertainty measures**
  - **Why needed here:** The entire framework relies on interpreting LM entropy as a similarity signal. Understanding that perplexity = exp(cross-entropy) and that lower values indicate the model is "less surprised" by the input is essential for interpreting results.
  - **Quick check question:** If a model trained on English achieves perplexity 50 on Spanish text and 500 on Japanese text, what does this suggest about the structural relationships?

- **Concept: Autoregressive language modeling**
  - **Why needed here:** ENTROPY2VEC uses GPT-2 style causal LMs. Understanding that these models predict P(xt|x1,...,xt-1) through sequential conditioning explains why they capture syntactic and morphological patterns.
  - **Quick check question:** Why would an autoregressive model trained on one language struggle with a typologically distant language—what specific predictions would fail?

- **Concept: Hierarchical clustering and tree distance metrics**
  - **Why needed here:** Evaluation uses DBSCAN clustering to form typological trees, compared against Glottolog using Robinson-Foulds and LCA metrics. Understanding these metrics is necessary to interpret Table 1 results.
  - **Quick check question:** If Robinson-Foulds distance is 17.0 (as in ENTROPY2VEC vs. Glottolog), does this indicate strong or weak alignment—what's the scale and baseline?

## Architecture Onboarding

- **Component map:** Monolingual LM Training Pipeline -> Cross-Entropy Computation Module -> Clustering Module -> Integration Layer -> Evaluation Interface
- **Critical path:**
  1. Prepare monolingual corpora (7:2:1 splits, cap at 1M sentences per language)
  2. Train 33 monolingual LMs (up to 150 epochs, early stopping patience=3)
  3. Compute cross-entropy matrix: each cell = model θi evaluating corpus Dj
  4. Extract language vectors Z_Li from matrix rows
  5. Apply DBSCAN clustering or integrate with downstream frameworks
- **Design tradeoffs:**
  - Small LM architecture vs. representational power: 4-layer GPT-2 is computationally efficient but may not capture complex grammatical phenomena that larger models would
  - Character-level tokenization vs. subword: Handles scripts consistently but may lose morphological boundaries that BPE would preserve
  - DBSCAN vs. linkage-based clustering: DBSCAN handles varying cluster densities but requires epsilon/min_samples tuning; paper notes linkage methods failed (Appendix C)
  - Standalone vs. concatenated vectors: Results show concatenation improves mBERT but degrades XLM-R performance, suggesting model-specific optimization needed
- **Failure signatures:**
  - Similar languages separated by script differences (Thai/Lao split despite linguistic similarity)
  - Languages grouped by shared vocabulary rather than genetic relationships (Malayic languages dispersed across branches)
  - Concatenation with URIEL+ degrades XLM-R performance (from 79.07 to 78.78 accuracy), indicating potential signal conflict
  - Low-resource languages (Madurese: 9,055 sentences) may produce unstable entropy estimates
- **First 3 experiments:**
  1. **Sanity check:** Train monolingual LMs on 3-5 closely related languages (e.g., Indonesian, Malay, Minangkabau) and 3 distant ones. Verify that cross-entropy within language families is consistently lower than across families.
  2. **Scale test:** Compare entropy vectors from the 4-layer GPT-2 against a larger model (e.g., 12-layer) on a subset of languages. Check whether tree alignment improves or if computational cost increases without benefit.
  3. **Ablation on tokenizer choice:** Train models with character-level vs. BPE tokenization on languages with complex morphology (e.g., Finnish, Turkish if available, or Javanese from dataset). Compare whether morphological boundaries affect entropy patterns and downstream task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can integrating shared encoding structures or transliteration mitigate the unwanted splitting of typologically similar languages that use different scripts?
- **Basis in paper:** [Explicit] "Future work could integrate additional linguistic features or shared encoding structures to better capture underlying etymological relationships."
- **Why unresolved:** The paper notes that the current representation splits similar languages like Thai and Lao because the model is influenced by differing character encodings (scripts) rather than just structural features.
- **What evidence would resolve it:** A version of Entropy2Vec trained on transliterated data or using a shared sub-word vocabulary across specific families, resulting in clustering that aligns with genealogical trees rather than script boundaries.

### Open Question 2
- **Question:** Why does concatenating Entropy2Vec with static typological features harm performance in strong models (XLM-R) while aiding weaker ones (mBERT)?
- **Basis in paper:** [Inferred] "These results suggest that in XLM-R, combining vectors may introduce redundancy... In contrast, concatenation improves the performance in mBERT... to compensate for the weaker language understanding."
- **Why unresolved:** The paper identifies the divergent behavior (overfitting/redundancy in XLM-R vs. enrichment in mBERT) but does not isolate the specific model capacity or pre-training dynamics that cause strong models to reject the auxiliary signal.
- **What evidence would resolve it:** An ablation study varying model capacity (small to large) and pre-training data size to identify the exact threshold where feature concatenation shifts from beneficial to detrimental.

### Open Question 3
- **Question:** To what extent does the quality of Entropy2Vec degrade when applied to extremely low-resource languages with limited training data?
- **Basis in paper:** [Explicit] "For languages with limited textual resources, the resulting embeddings may be less accurate or informative."
- **Why unresolved:** The study relies on Glot500-c and limits experiments to 33 languages, leaving the lower bound of data required for a meaningful entropy signal undefined.
- **What evidence would resolve it:** Experiments measuring the correlation with typological gold standards while systematically reducing the training corpus size for specific low-resource languages.

## Limitations

- **Fundamental uncertainty:** Cross-entropy may capture corpus similarity rather than true typological relationships, lacking external validation of this core assumption
- **DBSCAN dependency:** Performance heavily depends on clustering parameters (epsilon=0.1, min_samples=0.3) that may not generalize across different language sets
- **Marginal improvements:** Competitive but not superior results in downstream tasks suggest entropy vectors may provide redundant rather than complementary information

## Confidence

- **High Confidence:** The core mechanism of deriving language vectors from cross-entropy matrices is technically sound and reproducible. DBSCAN clustering produces consistent tree structures that align with linguistic expectations for many language families.
- **Medium Confidence:** The claim that ENTROPY2VEC vectors outperform traditional typological inventories is partially supported but not conclusively proven. While the framework avoids sparsity issues, it's unclear whether dense representations actually improve downstream performance beyond what simple language identifiers would achieve.
- **Low Confidence:** The assertion that ENTROPY2VEC provides unique regularization benefits for unseen languages lacks strong empirical support. Marginal improvements over URIEL+ suggest the approach may be capturing general linguistic patterns rather than specific typological relationships.

## Next Checks

1. **Correlation Analysis with Gold Typological Features:** Compute Pearson/Spearman correlation between ENTROPY2VEC vector distances and known typological distances from WALS or AUTOTYP for languages with complete feature annotations. This would directly test whether entropy captures structural similarity.

2. **Resource Sensitivity Study:** Systematically vary training corpus sizes (10K, 100K, 500K, 1M sentences) for 3-5 representative languages and measure the stability of resulting entropy vectors and downstream performance. This would identify the minimum viable corpus size.

3. **Ablation with Simple Language Identifiers:** Replace ENTROPY2VEC vectors with one-hot or embedding-based language identifiers in the LinguAlchemy framework. If performance remains similar, this would suggest the regularization benefits stem from having any language signal rather than entropy-derived properties specifically.