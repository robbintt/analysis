---
ver: rpa2
title: MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation
arxiv_id: '2509.06389'
source_url: https://arxiv.org/abs/2509.06389
tags:
- generation
- synthesis
- audio
- one-step
- mf-mjt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving inference efficiency
  in multimodal video-to-audio synthesis without compromising audio quality, semantic
  alignment, or temporal synchronization. Existing methods, particularly those based
  on flow matching, require iterative sampling steps, resulting in slow inference.
---

# MeanFlow-Accelerated Multimodal Video-to-Audio Synthesis via One-Step Generation

## Quick Facts
- arXiv ID: 2509.06389
- Source URL: https://arxiv.org/abs/2509.06389
- Reference count: 0
- One-line primary result: MF-MJT achieves over 2× speedup (RTF=0.007) while maintaining comparable or superior performance across multiple metrics

## Executive Summary
This paper addresses the inference efficiency bottleneck in multimodal video-to-audio synthesis by introducing MeanFlow-accelerated multimodal joint training (MF-MJT). The method models average velocity instead of instantaneous velocity, enabling native one-step generation without iterative sampling. A scalar rescaling mechanism (CFG-scaled) is also proposed to stabilize classifier-free guidance in the one-step setting. Experiments demonstrate over 2× speedup while maintaining or improving performance across FAD, FD, IS, IB, DeSync, and CLAP metrics on VGGSound and AudioCaps test sets.

## Method Summary
MF-MJT is a multimodal video-to-audio synthesis model built on a Multimodal Diffusion Transformer (MM-DiT) backbone. The key innovations are: (1) MeanFlow formulation that learns average velocity fields instead of instantaneous velocities, enabling one-step generation by directly mapping prior latents to data space; (2) CFG-scaled mechanism that rescales the unconditional component of classifier-free guidance to prevent overshooting artifacts in one-step generation; (3) Multimodal joint training from scratch on video, audio, and text data to establish unified semantic spaces. The model uses CLIP visual/text encoders, Synchformer for temporal synchronization, and audio-only DiT blocks for refinement before final velocity prediction.

## Key Results
- Achieves over 2× speedup (RTF=0.007) compared to baseline methods
- Maintains comparable or superior performance across FAD, FD, IS, IB, DeSync, and CLAP metrics
- Demonstrates strong generalization for both video-to-audio and text-to-audio synthesis tasks
- Performs well in multi-step settings while optimized for one-step generation

## Why This Works (Mechanism)

### Mechanism 1
Modeling the average velocity field enables native one-step generation by bypassing iterative integration required by standard flow matching. Standard Flow Matching learns instantaneous velocity `v(z_t, t)` requiring ODE solves, while MeanFlow models average velocity `u(z_t, r, t)` over interval `[r,t]`. This satisfies `u(z_t, r, t) = v(z_t, t) - (t-r)(d/dt)u(z_t, r, t)`, enabling direct mapping `z_r = z_t - (t-r)u(z_t, r, t)` for one-step generation.

### Mechanism 2
A scalar rescaling mechanism stabilizes CFG in one-step generation by preventing update overshooting. Standard CFG blends conditional and unconditional velocities `u_cfg = ω·u_cond + (1-ω)·u_uncond`. In one-step generation, high `ω` can cause overshooting. The CFG-scaled method introduces scalar `s = (u_cond · u_uncond) / ||u_uncond||^2` to re-weight unconditional term, projecting conditional update toward unconditional direction.

### Mechanism 3
Multimodal joint training on video, audio, and text from scratch establishes unified semantic space for cross-modal alignment. The MM-DiT backbone uses joint-attention layers with video (`F_v`), text (`F_t`), and audio (`x`) embeddings, plus Synchformer temporal features (`F_sync`). Unified training on combined datasets enables scalable data utilization and facilitates cross-modal understanding for both VTA and TTA tasks.

## Foundational Learning

**Concept: Flow Matching (FM) and ODE-based Generation**
- Why needed: Core innovation modifies standard Flow Matching; understanding FM's ODE formulation is prerequisite
- Quick check: In standard Flow Matching, what does velocity field `v(z_t, t)` represent, and why does its use necessitate iterative sampling process?

**Concept: Classifier-Free Guidance (CFG)**
- Why needed: Paper proposes specific CFG modification for one-step models; standard CFG formulation is prerequisite
- Quick check: How is conditional velocity `u_cfg` typically constructed from conditional `u_cond` and unconditional `u_uncond`, and what role does guidance scale `ω` play?

**Concept: Transformer Architectures (DiT / MM-DiT)**
- Why needed: Model backbone is Diffusion Transformer extended to Multimodal DiT; understanding attention mechanisms for fusing sequences is required
- Quick check: In MM-DiT blocks, how are video, text, and audio tokens fused, and what purpose do audio-only DiT blocks serve before final prediction head?

## Architecture Onboarding

**Component map**: CLIP visual encoder `F_v` -> CLIP text encoder `F_t` -> Synchformer encoder `F_sync` -> VAE encoder for audio latents `x` -> Projection layer -> MM-DiT blocks (`N1=4`) -> DiT blocks (`N2=8`) -> Output head

**Critical path**: Input Encoders -> Projection Layer -> MM-DiT Blocks (`N1=4`) -> DiT Blocks (`N2=8`) -> Output Head. The critical interaction happens in MM-DiT blocks where multimodal tokens attend to each other.

**Design tradeoffs**:
- One-step vs. Multi-step: Optimized for one-step generation (RTF=0.007); multi-step generation possible but ~14x slower (RTF=0.101)
- Training Ratio: MeanFlow training uses low `r≠t` ratio (10%) to improve alignment, trading broader trajectory coverage for more direct supervision

**Failure signatures**:
- Audio artifacts/distortion: May indicate `u_theta` not well-trained or CFG guidance scale `ω` too high for one-step generation
- Poor temporal sync: Could be issue with Synchformer features (`F_sync`) not integrated correctly
- Low semantic alignment: Check if CLIP encoders frozen correctly and projection layers functioning

**First 3 experiments**:
1. Ablate CFG Rescaling: Run one-step generation with standard CFG vs CFG-scaled; plot IS/IB scores against `ω` to confirm rescaling prevents degradation at higher guidance strengths
2. Vary MeanFlow Training Ratio: Train with different `r≠t` ratios (10%, 30%, 60%); evaluate on IB and DeSync scores to reproduce finding that lower ratios improve alignment
3. Step-wise Quality vs Efficiency: Generate samples using 1, 2, 4, 8, and 25 steps; plot FAD/FD against RTF to demonstrate model's flexibility and quality-efficiency trade-off curve

## Open Questions the Paper Calls Out

**Open Question 1**: Is the optimal ratio of `r≠t` sampling pairs (found to be 10%) dependent on dataset scale or temporal resolution, or is it a universal constant for MeanFlow? The paper only tests VGGSound dataset and doesn't investigate if this hyperparameter requires tuning under different data regimes or for different modalities.

**Open Question 2**: Can the scalar rescaling mechanism (CFG-scaled) effectively stabilize classifier-free guidance in other one-step generation architectures, such as consistency models or standard rectified flow? The paper proposes CFG-scaled specifically for MeanFlow but doesn't demonstrate its utility as general-purpose tool for other accelerated generative paradigms.

**Open Question 3**: Does removal of absolute positional encodings limit model's ability to maintain temporal structure in long-form video generation exceeding training sequence length? The paper evaluates on standard 8s-10s test sets but doesn't analyze failure cases in long-form scenarios where absolute temporal context might be required to prevent looping or drifting.

## Limitations
- Theoretical guarantees for MeanFlow formulation and scalar rescaling mechanism are limited, relying primarily on empirical validation
- Claims about joint training superiority lack rigorous comparison against adapted pre-trained models
- Evaluation focuses on automated metrics without extensive perceptual studies to validate subjective quality improvements

## Confidence
**High Confidence**: Efficiency claims (RTF=0.007) and basic functionality of MeanFlow formulation are well-supported by experimental setup and metric definitions
**Medium Confidence**: Semantic alignment improvements (IB and CLAP scores) and temporal synchronization (DeSync) claims are supported by metric comparisons but need qualitative analysis and user studies
**Low Confidence**: Claim that joint training from scratch is superior to adapting pre-trained models lacks rigorous comparison; theoretical justification for scalar rescaling mechanism is weak

## Next Checks
1. **Perceptual Validation Study**: Conduct human evaluation comparing one-step MF-MJT outputs against multi-step baselines across different guidance scales to validate metric improvements translate to perceptible quality gains
2. **CFG Overshooting Analysis**: Systematically visualize and analyze conditional vs unconditional velocity vectors during one-step generation with and without CFG-scaled mechanism to understand when and why overshooting occurs
3. **Joint Training Ablation**: Train alternative version using pre-trained text-to-audio model (e.g., AudioGen) with frozen encoders then fine-tune on video data; compare to proposed joint training approach on cross-modal alignment metrics to rigorously test superiority claims