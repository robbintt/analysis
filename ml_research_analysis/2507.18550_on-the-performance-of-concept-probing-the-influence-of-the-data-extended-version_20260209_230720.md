---
ver: rpa2
title: 'On the Performance of Concept Probing: The Influence of the Data (Extended
  Version)'
arxiv_id: '2507.18550'
source_url: https://arxiv.org/abs/2507.18550
tags:
- probing
- concept
- data
- hassymbol
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how the data used to train concept probing
  models affects their performance in image classification tasks. Concept probing
  helps interpret neural networks by training additional classifiers to map internal
  model representations to human-defined concepts of interest.
---

# On the Performance of Concept Probing: The Influence of the Data (Extended Version)

## Quick Facts
- **arXiv ID**: 2507.18550
- **Source URL**: https://arxiv.org/abs/2507.18550
- **Authors**: Manuel de Sousa Ribeiro; Afonso Leote; João Leite
- **Reference count**: 40
- **Primary result**: Concept probing achieves high performance with limited data and shows robustness to noise and data reuse

## Executive Summary
This paper investigates how data characteristics affect concept probing performance in image classification models. The authors examine four key dimensions: training data size, original model size, data reuse, and data quality. They find that relevant concepts require as few as 250 training samples to reach 97.3% of maximum accuracy, and that probes show surprising robustness to moderate label noise (up to 20%). The study reveals that increasing original model size slightly improves probe performance, contrary to expectations, and that reusing the original model's training data has negligible negative impact. These findings provide practical guidance for practitioners on efficiently implementing concept probing with limited resources.

## Method Summary
The study uses pre-trained image classification models (ViT, ResNet variants, MobileNet, VGGNet) across six datasets to extract internal activations from specific layers. These activations serve as inputs to lightweight probe classifiers (Logistic Regression, Ridge, LightGBM, Neural Networks, and MapNN) that map representations to human-defined concept labels. The experimental protocol involves balanced datasets (typically 500 samples), 25% validation splits, and 5 repetitions per condition. The authors systematically vary data size, model scale, data source (reused vs. fresh), and label noise levels to evaluate probe performance across different conditions.

## Key Results
- Probes achieve 97.3% of maximum performance with just 250 training samples for relevant concepts
- Increasing original model size slightly improves probe performance rather than degrading it
- Reusing training data from the original model has negligible impact on probe accuracy
- Probes tolerate up to 20% label noise with only 9.3% performance drop

## Why This Works (Mechanism)

### Mechanism 1: Task-Relevant Representation Distillation
- **Claim**: Probing relevant concepts requires significantly less data because the original model has already compressed necessary information into its activations
- **Mechanism**: The original model acts as a feature extractor. When a concept is relevant to the model's task, internal representations naturally cluster based on that concept, allowing probes to learn shallow mappings rather than learning concepts from scratch
- **Core assumption**: The probed concept is functionally useful to the original model's trained task
- **Evidence anchors**: "relevant concepts require as few as 250 training samples to reach 97.3% of maximum accuracy"; accuracy stabilizes at ~200 samples for relevant concepts but requires more data for non-relevant ones
- **Break condition**: Probing "non-relevant" concepts where performance is significantly lower and data requirements are higher

### Mechanism 2: Noise Tolerance via Feature Quality
- **Claim**: Probes exhibit moderate robustness to training label noise because high-quality input representations provide a strong signal that overrides some labeling errors
- **Mechanism**: Probe inputs are processed activations from trained DNNs, which are often linearly separable. This high-quality feature space allows probes to fit the underlying true concept distribution even with flipped labels
- **Core assumption**: The activation space provides a cleaner basis for classification than raw input space
- **Evidence anchors**: "probes tolerate up to 20% label noise with only 9.3% performance drop"; 20% noise led to 9.3% relative performance reduction on average
- **Break condition**: Systematic or non-random noise (e.g., specific subclasses always mislabeled) causes greater degradation than random flipping

### Mechanism 3: Activation Consistency
- **Claim**: Reusing the original model's training data for probing doesn't degrade performance because activation patterns remain consistent between seen and unseen data
- **Mechanism**: If a model generalizes well, its internal activations for training inputs are structurally similar to activations for novel inputs, allowing probes trained on training set activations to learn general concept mappings
- **Core assumption**: The original model is not severely overfitted, causing distinct activation distributions for training samples
- **Evidence anchors**: "reusing training data from the original model has negligible impact"; slope is slightly negative (-4.1×10^-3) with no statistically significant effect
- **Break condition**: This might fail if the original model is significantly overfitted, causing distinct activation distributions for training samples

## Foundational Learning

- **Concept: Internal Representations (Activations)**
  - **Why needed here**: Concept probing operates on vectors generated by specific layers of a neural network, not raw inputs. Understanding these as the probe's "features" is critical
  - **Quick check question**: If you probe a layer immediately after input embedding vs. the layer before output head, which would capture higher-level concepts like "texture" or "shape"?

- **Concept: Concept Relevance**
  - **Why needed here**: The paper explicitly distinguishes between concepts that aid the model's task (relevant) and those that don't, determining probe feasibility
  - **Quick check question**: If probing a bird classification model for the concept "is_daytime", would this be classified as relevant or non-relevant?

- **Concept: Regularization (L1/L2)**
  - **Why needed here**: The paper compares architectures and finds regularized probes often perform better. MapNN uses L1 to select sparse subsets of units
  - **Quick check question**: Why might L1 regularization be preferable to L2 if you want to identify exactly which neurons encode a concept?

## Architecture Onboarding

- **Component map**: Original Model ($f$) -> Probe ($g$) -> Probe Dataset ($D_g$)
- **Critical path**:
  1. **Extract**: Pass images through $f$ and extract activations from target layers
  2. **Construct**: Create balanced dataset of activations and concept labels (start with 500 samples)
  3. **Train**: Fit probe $g$ using early stopping (patience=15) and 75/25 train/validation split

- **Design tradeoffs**:
  - **Linear vs. Non-Linear**: Linear probes (Logistic/Ridge) are simpler and prevent overfitting on small data. MapNN (non-linear with feature selection) achieves higher accuracy on larger datasets but adds complexity
  - **Model Scaling**: Scaling up original model size slightly improves probe accuracy, so using larger backbones is acceptable

- **Failure signatures**:
  - **High Variance/Low Accuracy**: Indicates the concept is likely "non-relevant" to the model's task, or the probed layer is too early/late
  - **Drastic Drop with Noise**: If performance collapses with <10% noise, check data integrity; probes should be robust up to 20%

- **First 3 experiments**:
  1. **Minimum Data Test**: Train Ridge probe on [8, 125, 250, 500] samples to verify if concept saturates at ~250 samples
  2. **Architecture Comparison**: Compare Logistic vs. MapNN on fixed 500-sample set to see if concept benefits from sparse feature selection
  3. **Noise Injection**: Artificially flip 20% of labels to ensure probe maintains >90% of original performance

## Open Questions the Paper Calls Out
- **Open Question 1**: Do the observed data efficiency and robustness findings generalize to non-vision domains like NLP?
- **Open Question 2**: Does reusing the probed model's training data induce overfitting effects not detectable by standard accuracy metrics?
- **Open Question 3**: Does the observed improvement in probe performance with model scaling hold for diverse architectural families?

## Limitations
- Findings are based on binary concept probes in image classification and may not generalize to multi-class concepts or other domains
- The exact implementation details of MapNN's "Input Reduce" procedure remain unspecified
- Limited computational resources constrained scaling experiments to only two architectures

## Confidence
- **High Confidence**: Inverse relationship between model size and probe performance; data reuse has negligible impact (supported by t-tests)
- **Medium Confidence**: Specific thresholds (250 samples, 20% noise) are empirically derived but may be domain-dependent
- **Low Confidence**: Generalization to non-image domains or concepts requiring fine-grained spatial reasoning

## Next Checks
1. **Concept Relevance Classification**: Systematically define and validate concepts as "relevant" vs. "non-relevant" to test data efficiency differences
2. **Noise Distribution Analysis**: Compare probe performance under random vs. structured noise to validate robustness mechanisms
3. **Multi-Class Extension**: Test whether the 250-sample threshold holds for multi-class concepts by extending probe architectures