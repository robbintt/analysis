---
ver: rpa2
title: 'Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive
  Fiction Games'
arxiv_id: '2505.12439'
source_url: https://arxiv.org/abs/2505.12439
tags:
- action
- games
- lplh
- game
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Learning to Play Like Humans (LPLH)
  framework, which guides Large Language Models (LLMs) to play interactive fiction
  (IF) games in a human-like manner. LPLH incorporates three main components: a dynamic
  knowledge graph for spatial and narrative mapping, action space learning to identify
  valid commands, and an experience reflection module that summarizes past successes
  and failures.'
---

# Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games

## Quick Facts
- **arXiv ID:** 2505.12439
- **Source URL:** https://arxiv.org/abs/2505.12439
- **Reference count:** 40
- **Primary result:** LPLH achieves higher scores than baselines on 9 IF games by combining dynamic knowledge graphs, action space learning, and experience reflection.

## Executive Summary
This paper introduces the Learning to Play Like Humans (LPLH) framework, which guides Large Language Models (LLMs) to play interactive fiction (IF) games in a human-like manner. LPLH incorporates three main components: a dynamic knowledge graph for spatial and narrative mapping, action space learning to identify valid commands, and an experience reflection module that summarizes past successes and failures. By integrating these elements, the framework enables LLMs to adapt their decision-making over time and make contextually informed choices without relying on external pre-training or task-specific fine-tuning. The approach was tested across nine IF games, where it achieved notable performance improvements, with some agents reaching maximum scores and consistently outperforming baseline models in raw score gains. LPLH thus offers a novel, interpretable path toward more adaptive and human-aligned gameplay in complex text-based environments.

## Method Summary
LPLH operates through three modules: (1) a dynamic knowledge graph (KG-map) that updates the known world state via fine-tuned relation extraction, (2) an action space learner that tracks valid verb-object pairs from successful actions, and (3) an experience library (Experience Lib) that summarizes and retrieves past gameplay experiences via GPT-o3-mini and RAG when scores change. The main agent (LLMa) is a zero-shot LLM that, at each step, receives the current observation, KG-map, valid action candidates, and relevant past experiences as context, then generates the next command. The auxiliary fine-tuned model (f_m) handles relation extraction, action validation, and verb-object splitting. The system is trained-free for the main agent but uses LoRA fine-tuning on f_m with data from three held-out games.

## Key Results
- LPLH outperforms baseline RL and LLM models on 8 of 9 tested IF games.
- The framework achieves maximum scores on several games (e.g., omniquest, detective) and shows consistent improvement over time.
- Individual modules (KG-map, Action Space, Experience Lib) each contribute to performance gains in ablation tests.

## Why This Works (Mechanism)
LPLH mimics human learning by building an internal model of the game world (KG-map), remembering what actions work (Action Space), and reflecting on past experiences to inform future decisions (Experience Lib). Instead of static pre-training, the agent learns in-context by updating its prompt with the current game state, valid commands, and relevant memories retrieved via RAG. This allows the LLM to adapt its reasoning dynamically without changing its weights, combining exploration with structured memory to overcome the partial observability of IF games.

## Foundational Learning
- **Concept: Partially Observable Markov Decision Process (POMDP)**
  - **Why needed here:** IF games are formally defined as a POMDP tuple `(S, T, A, O, R)`. The agent's core challenge is that the full game state `s ∈ S` is hidden; it only receives a partial, textual observation `o ∈ O`. The LPLH framework’s design is a direct response to this problem, using the KG-map and Experience Lib to build an internal model of the hidden state from sequential observations.
  - **Quick check question:** Why is the agent unable to simply query the game engine for a complete list of all object states at the beginning of the game?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** The Experience Lib implements RAG. Instead of relying on the LLM's fixed weights or limited context window, the system dynamically retrieves relevant "experiences" (summarized past trajectories) from a vector database and injects them into the LLM's prompt at decision time. Understanding RAG is essential to grasp how the agent simulates long-term, cross-episode learning without weight updates.
  - **Quick check question:** What event triggers the creation of a new experience summary, and how is that summary later used to influence a future action?

- **Concept: In-Context Learning vs. Weight Fine-Tuning**
  - **Why needed here:** The paper claims a "training-free" approach for the main agent (`LLMa`). The agent's "learning" is achieved entirely through in-context learning: updating the prompt's context with the current KG-map, valid action candidates, and retrieved experiences. This is distinct from the fine-tuning done on the smaller `f_m` model for relation extraction. Recognizing this distinction is critical for debugging and optimizing the system.
  - **Quick check question:** Does the main agent (`LLMa`) update its internal parameters after successfully solving a puzzle? If not, what structure within the system changes to reflect that new knowledge?

## Architecture Onboarding
- **Component map:** Game Environment (Jericho) -> Fine-tuned Model (`f_m`) -> Dynamic Knowledge Graph (KG-map) -> Action Space (`AS`) -> Experience Lib (`E`) -> Main Agent (`LLMa`)

- **Critical path:**
  1. **Perceive:** Game emits observation `o`.
  2. **Process:** `f_m` extracts relations from `o` and updates the **KG-map** (`G`).
  3. **Recall:** If score changed, **Experience Lib** is updated. RAG retrieves relevant past experiences (`E_q`).
  4. **Propose:** **Action Space** module combines current room objects from `G` with known verbs from `AS` to form candidate actions (`objv_loc`).
  5. **Decide:** The full context (`G`, `objv_loc`, `E_q`) is assembled into a prompt for the **Main Agent** (`LLMa`), which generates the next command `a`.
  6. **Act:** Command `a` is sent to the game. If valid, `f_m` decomposes it to update **Action Space** (`AS`).

- **Design tradeoffs:**
  - **Interpretability vs. Complexity:** The modular design (KG, Action Space, Experience Lib) makes the agent's reasoning highly interpretable, as each component's state can be inspected. The tradeoff is architectural complexity and reliance on multiple interacting components (`f_m`, `LLMes`, `LLMa`).
  - **Training-Free vs. Performance:** By using a training-free main agent, the system is flexible and generalizable. The tradeoff is that performance is capped by the LLM's base reasoning ability and the quality of the auxiliary modules, rather than being directly optimized for game reward via RL.
  - **Experience Trigger:** Experience summarization is *only* triggered by score changes. This is computationally efficient but means the agent may fail to learn from important non-scoring events (e.g., a dead-end that doesn't kill the player).

- **Failure signatures:**
  - **"Hallucinated State":** Agent repeatedly tries to interact with an object not present. *Diagnostic:* Check the KG-map (`G`) for incorrect entries, likely from a faulty relation extraction by `f_m`.
  - **"Catastrophic Forgetting":** Agent makes a mistake it had previously "learned" to avoid. *Diagnostic:* Check if the relevant experience is being retrieved from the Experience Lib. The RAG query may be failing to match the current context.
  - **"Procedural Bottleneck":** Agent is stuck on a known difficult puzzle (e.g., "echo" in Loud Room). *Diagnostic:* This is an expected failure mode noted in the paper's error analysis; the system cannot infer commands that are highly unconventional or lack contextual cues.

- **First 3 experiments:**
  1. **Module Ablation:** Run the agent on a single game (e.g., Zork1) in four configurations: full LPLH, LPLH without KG-map, LPLH without Experience Lib, and LPLH without Action Space. Compare scores and step efficiency to validate each component's contribution as per Table 2.
  2. **Auxiliary Model Stress Test:** Evaluate the fine-tuned `f_m` model's accuracy on its three tasks (relation extraction, action validation, action splitting) using a held-out set of game logs. High error rates here will directly propagate and corrupt the entire system's "memory" and "vocabulary."
  3. **RAG Retrieval Inspection:** Manually inspect the experience summaries generated by `LLMes` and the experiences retrieved by the RAG system for a few key game moments. Determine if summaries are accurate and if retrieval is contextually relevant, as the entire learning mechanism hinges on this loop.

## Open Questions the Paper Calls Out
- **How can LLM agents overcome "known puzzle bottlenecks" that require unconventional, domain-specific commands (e.g., "echo") not inferable from standard exploration?**
  - **Basis in paper:** [explicit] Section 7 identifies "known puzzle bottlenecks" where progress requires "highly specific actions" lacking inductive bias, such as typing "echo" in the "Loud Room."
  - **Why unresolved:** Current structural guidance (KG-maps) and standard exploration fail to generate these obscure commands, lacking the "intuition and imagination" human players use.
  - **What evidence would resolve it:** A mechanism enabling agents to infer or "imagine" valid but low-probability commands without explicit training data or external hints.

- **Does implementing a dynamic, continuous experience summarization strategy improve performance over the current point-triggered approach?**
  - **Basis in paper:** [explicit] The "Limitation" section states the current summarization is simple and only triggered by score changes, whereas humans "integrate relevant information... at any point."
  - **Why unresolved:** The authors suggest a more dynamic strategy could yield better results but have not yet implemented or tested a continuous memory integration method.
  - **What evidence would resolve it:** A comparative study showing performance gains when the Experience Lib updates continuously or based on semantic shifts rather than just reward signals.

- **Can the LPLH framework generalize effectively to wider text-based environments like TextWorld given its current reliance on specific IF game structures?**
  - **Basis in paper:** [explicit] The "Limitation" section notes the framework is currently only applied to IF games and lists fitting LPLH for "wider text-based environments such as TextWorld" as future work.
  - **Why unresolved:** The framework was tested on a limited set of games (9 titles), and it is unclear if the specific KG-map and action-parsing modules transfer to generation-based environments.
  - **What evidence would resolve it:** Successful application and maintenance of performance metrics when deploying LPLH on the TextWorld platform or other text simulation domains.

## Limitations
- The framework struggles with "known puzzle bottlenecks" requiring unconventional commands (e.g., "echo" in Loud Room) that lack contextual cues.
- Experience summarization is only triggered by score changes, missing opportunities to learn from non-rewarding but informative events.
- The approach is currently limited to IF games and its generalization to other text-based environments (e.g., TextWorld) is unproven.

## Confidence
- **High Confidence:** The core architectural design of LPLH and its three main components (KG-map, Action Space, Experience Lib) are well-documented and logically sound for addressing the POMDP nature of IF games. The experimental setup and baseline comparisons are clearly specified.
- **Medium Confidence:** The reported performance improvements over baselines are supported by the experimental data, but the exact mechanisms of the RAG retrieval and the precise prompt formatting for the LLM agent contain some unspecified details that could affect reproducibility.
- **Low Confidence:** The long-term generalization capabilities of the framework beyond the nine tested games and the robustness of the experience learning mechanism across diverse game types remain uncertain due to limited testing scope.

## Next Checks
1. **Module Ablation Study:** Run the agent on a single game (e.g., Zork1) in four configurations: full LPLH, LPLH without KG-map, LPLH without Experience Lib, and LPLH without Action Space. Compare scores and step efficiency to validate each component's contribution as per Table 2.

2. **Auxiliary Model Stress Test:** Evaluate the fine-tuned `f_m` model's accuracy on its three tasks (relation extraction, action validation, action splitting) using a held-out set of game logs. High error rates here will directly propagate and corrupt the entire system's "memory" and "vocabulary."

3. **RAG Retrieval Inspection:** Manually inspect the experience summaries generated by `LLMes` and the experiences retrieved by the RAG system for a few key game moments. Determine if summaries are accurate and if retrieval is contextually relevant, as the entire learning mechanism hinges on this loop.