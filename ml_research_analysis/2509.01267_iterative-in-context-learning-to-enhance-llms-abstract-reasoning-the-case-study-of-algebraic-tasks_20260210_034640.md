---
ver: rpa2
title: 'Iterative In-Context Learning to Enhance LLMs Abstract Reasoning: The Case-Study
  of Algebraic Tasks'
arxiv_id: '2509.01267'
source_url: https://arxiv.org/abs/2509.01267
tags:
- shot
- llms
- prompt
- shots
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving systematic generalization
  in LLMs for abstract reasoning tasks, particularly in mathematical domains. It introduces
  an iterative in-context learning methodology that constructs tailored few-shot examples
  to enhance model performance on algebraic tasks with non-standard simplification
  rules.
---

# Iterative In-Context Learning to Enhance LLMs Abstract Reasoning: The Case-Study of Algebraic Tasks

## Quick Facts
- arXiv ID: 2509.01267
- Source URL: https://arxiv.org/abs/2509.01267
- Reference count: 10
- LLMs struggle with systematic generalization on algebraic tasks with non-standard simplification rules

## Executive Summary
This paper introduces an iterative in-context learning methodology to improve LLMs' performance on abstract reasoning tasks, specifically focusing on algebraic problems with non-standard simplification rules. The approach constructs tailored few-shot examples by querying LLMs, analyzing incorrect answers, and iteratively adding representative examples to the prompt set. Experiments on five synthetic datasets demonstrate that LLMs struggle with these reasoning tasks, but the iterative shot selection strategy significantly improves performance. The study reveals that simpler examples often yield better generalization than complex ones, and that models with reasoning modules like GMN2.0-R perform best, though simpler models like GMN2.0 can match their performance through effective shot selection.

## Method Summary
The methodology employs an iterative in-context learning approach that constructs tailored few-shot examples to enhance model performance on algebraic tasks. The process begins by querying LLMs to solve tasks, then analyzing incorrect answers to identify patterns and gaps in reasoning. Based on this analysis, representative examples are iteratively added to the prompt set, with the hypothesis that well-chosen examples can guide models toward better abstract reasoning. The approach emphasizes selecting simpler, more representative examples over complex ones, as experiments show that simpler examples often yield better generalization. The method was tested on five synthetic algebraic datasets featuring non-standard simplification rules, comparing models with and without reasoning modules.

## Key Results
- LLMs struggle with systematic generalization on algebraic tasks featuring non-standard simplification rules
- Iterative shot selection strategy improves performance by adding representative examples to prompts
- Simpler examples often yield better generalization than complex ones
- Models with reasoning modules (GMN2.0-R) perform best, but simpler models (GMN2.0) can match their performance through effective shot selection

## Why This Works (Mechanism)
The iterative shot selection methodology works by identifying and addressing specific reasoning gaps through targeted example selection. When LLMs fail on algebraic tasks with non-standard rules, the analysis of incorrect answers reveals patterns in their reasoning failures. By iteratively adding examples that specifically address these failure modes, the approach helps models develop better abstract reasoning patterns. The preference for simpler examples suggests that models benefit more from clear, focused demonstrations of core principles rather than complex examples that may introduce additional noise or obscure the underlying patterns.

## Foundational Learning
- **Systematic generalization**: The ability of models to apply learned rules to novel instances. Why needed: Algebraic tasks with non-standard rules require models to understand and apply rules consistently rather than memorizing patterns. Quick check: Test whether models can correctly simplify expressions following rules they haven't explicitly seen before.
- **In-context learning mechanisms**: How models use examples in prompts to guide their reasoning. Why needed: The methodology relies on selecting examples that effectively guide model behavior. Quick check: Analyze how different example selection strategies affect model performance on unseen tasks.
- **Abstract reasoning patterns**: The cognitive processes underlying mathematical reasoning and rule application. Why needed: Understanding these patterns helps identify which examples will best teach the desired reasoning capabilities. Quick check: Compare human and model reasoning patterns on the same algebraic tasks.

## Architecture Onboarding

**Component Map**: LLMs (GMN2.0, GMN2.0-R) -> Algebraic Task Datasets -> Iterative Shot Selection -> Performance Evaluation

**Critical Path**: Model Input -> Task Processing -> Error Analysis -> Example Selection -> Prompt Update -> Performance Improvement

**Design Tradeoffs**: The methodology trades computational overhead from iterative analysis for improved generalization performance. Simpler examples provide better generalization but may require more examples to cover the full task space, while complex examples might capture more ground but introduce noise.

**Failure Signatures**: Models fail to generalize when examples are too complex, when they don't capture the core abstract principles, or when the iterative selection process overfits to specific patterns rather than developing genuine reasoning capabilities.

**First Experiments**:
1. Test whether the iterative approach improves performance on a held-out subset of algebraic tasks not used during the shot selection process
2. Compare performance using examples selected by the iterative method versus randomly selected examples of similar complexity
3. Evaluate whether simpler examples consistently outperform complex ones across different model architectures and task types

## Open Questions the Paper Calls Out
None

## Limitations
- The synthetic nature of algebraic tasks may not fully capture the complexity of real-world mathematical reasoning problems
- The methodology's reliance on LLM-generated incorrect answers introduces potential circularity in the example selection process
- The performance improvement may represent pattern matching optimization rather than fundamental enhancement of abstract reasoning capabilities

## Confidence

**High**: LLMs struggle with systematic generalization on algebraic tasks with non-standard simplification rules - well-supported by experimental evidence and aligned with established literature on LLM reasoning limitations.

**Medium**: Iterative shot selection strategy effectiveness demonstrated within controlled synthetic environment, but generalizability to diverse reasoning tasks and real-world applications remains uncertain.

**Low**: The approach fundamentally enhances abstract reasoning capabilities rather than merely improving pattern matching through strategic example selection - lacks rigorous validation beyond specific task domains tested.

## Next Checks

1. **Cross-domain generalization testing**: Apply the iterative shot selection methodology to non-algebraic abstract reasoning tasks (e.g., logical puzzles, symbolic manipulation in different mathematical domains) to assess whether simpler examples consistently yield better generalization across diverse problem types.

2. **Human evaluation of example quality**: Conduct expert human assessment of the automatically selected examples to determine whether they genuinely capture abstract principles of the tasks or merely exploit superficial patterns that happen to work for specific LLM architectures tested.

3. **Long-term performance stability**: Implement a longitudinal study where models trained with iterative shot selection are periodically retested on both original and novel task variations to measure whether learned reasoning patterns demonstrate robust generalization or degrade when faced with task modifications.