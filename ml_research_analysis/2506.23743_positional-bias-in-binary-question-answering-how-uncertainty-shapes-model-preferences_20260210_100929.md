---
ver: rpa2
title: 'Positional Bias in Binary Question Answering: How Uncertainty Shapes Model
  Preferences'
arxiv_id: '2506.23743'
source_url: https://arxiv.org/abs/2506.23743
tags:
- answer
- bias
- uncertainty
- positional
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates positional bias in large language models
  (LLMs) when performing binary question answering under varying levels of answer
  uncertainty. The authors created SQuAD-it-2, a benchmark derived from the Italian
  SQuAD-it dataset, with three uncertainty levels: low (context provided), medium
  (context removed), and high (two out-of-context distractors).'
---

# Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences

## Quick Facts
- arXiv ID: 2506.23743
- Source URL: https://arxiv.org/abs/2506.23743
- Reference count: 28
- Primary result: Positional bias in LLMs grows exponentially with uncertainty, absent under low uncertainty but pronounced in high-uncertainty and argumentative contexts

## Executive Summary
This study investigates how uncertainty modulates positional bias in large language models during binary question answering. The authors created SQuAD-it-2, a benchmark with three uncertainty levels (low: context provided, medium: context removed, high: two distractors), plus two subjective datasets (WebGPT, Winning Arguments). Five LLMs were tested using a two-pass evaluation protocol with systematic answer order swaps. Results show positional bias is nearly absent under low uncertainty but grows exponentially as uncertainty increases, with models consistently preferring the second answer in persuasive argumentation tasks regardless of content.

## Method Summary
The study used a two-pass evaluation protocol: Pass 1 presents preferred answer first, Pass 2 swaps the order. This systematic position swapping measures positional bias using Preference Fairness (PF) and Position Consistency (PC) metrics. SQuAD-it-2 provides 7,609 test samples across three uncertainty variants, while WebGPT (14,346 pairs) and Winning Arguments (807 pairs) test subjective quality assessment. Five LLMs were evaluated: Llama-3.1-8B, Gemma-3-12B-Q, Gemini-1.5, Gemini-2, and Phi4-14B-Q. Quantized models were tested via Ollama. The protocol records model choice in each pass to compute bias metrics.

## Key Results
- Positional bias is nearly absent under low-uncertainty conditions but grows exponentially as uncertainty increases
- All models consistently preferred the second answer in the Winning Arguments dataset regardless of content
- Quantized models showed minimal accuracy degradation while maintaining similar bias patterns
- High invalid response rates occurred in high-uncertainty settings, particularly for Llama (1,897 invalid responses)

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-modulated heuristic fallback
Models employ positional heuristics as a secondary inference strategy when semantic discrimination becomes difficult, not as a fixed architectural bias. When uncertainty is low (clear context, unambiguous answers), models prioritize semantic reasoning pathways. As uncertainty increases (context removed, equally plausible distractors), models progressively shift to positional priors as a fallback decision mechanism. This reflects a learned computational tradeoff rather than an inherent architectural limitation.

### Mechanism 2: Discourse-structure recency bias
In argumentative contexts, models systematically prefer the second option, possibly reflecting internalized discourse norms from training data. In persuasive/argumentative settings, stronger arguments often follow weaker ones for rebuttal. Models trained on such discourse may encode recency bias as a domain-specific heuristic. Training corpora likely contain discourse patterns where later responses are more persuasive in debate/dialogue contexts.

### Mechanism 3: Context-gated semantic processing
Explicit supporting context gates access to reliable semantic reasoning, suppressing positional heuristics. With context present (Low Uncertainty), models ground decisions in explicit evidence. Context removal degrades grounding, forcing reliance on world knowledge and positional priors. Models prioritize context over positional priors when both are available.

## Foundational Learning

- **Concept: Binary QA under controlled uncertainty**
  - Why needed here: Understanding how to construct datasets that systematically vary epistemic uncertainty is essential for diagnosing model robustness and bias thresholds
  - Quick check question: How does SQuAD-it-2 create three uncertainty levels, and what does each condition test?

- **Concept: Preference Fairness (PF) vs. Position Consistency (PC)**
  - Why needed here: PF measures directional bias magnitude (preference for position 1 or 2); PC measures robustness to order perturbation. They capture orthogonal phenomena
  - Quick check question: If a model always selects whichever answer is in position 2, what would |PF| and PC values be?

- **Concept: Two-pass evaluation protocol**
  - Why needed here: Systematic position swapping is the core diagnostic for detecting positional bias; understanding this design pattern is prerequisite for replication
  - Quick check question: Why must prompts in Pass 1 and Pass 2 be structurally identical except for answer order?

## Architecture Onboarding

- **Component map:** Dataset layer (SQuAD-it-2 variants, WebGPT, Winning Arguments) -> Evaluation layer (two-pass protocol with order swaps) -> Metrics layer (PF, PC) -> Model layer (5 LLMs including quantized variants)

- **Critical path:**
  1. Prepare binary-choice instances with designated preferred/distractor answers
  2. Run Pass 1 (preferred in position 1), record selection C^(1)
  3. Run Pass 2 (swapped order), record selection C^(2)
  4. Compute PF and PC from paired selections across N instances
  5. Compare metrics across uncertainty levels to identify bias amplification

- **Design tradeoffs:**
  - Quantized models trade precision for efficiency; preliminary tests show minimal accuracy impact, but generalization is uncertain
  - Italian language choice reduces linguistic prior anchoring; findings may differ for higher-resource languages
  - Binary format simplifies analysis but may miss multi-option complexity

- **Failure signatures:**
  - High invalid response counts in ambiguous conditions (Llama: 1897 invalid in High Uncertainty) indicate format degradation
  - |PF| → 1.0 and PC → 0.5 signals maximum positional bias with near-random selection
  - Direction reversals across models (some prefer position 1, others position 2) suggest model-specific heuristics

- **First 3 experiments:**
  1. Replicate Low Uncertainty baseline to establish PF/PC when semantic cues are strong
  2. Incrementally remove context to determine bias onset threshold
  3. Test domain-specific argumentative tasks to assess recency bias generalization

## Open Questions the Paper Calls Out

- What specific mechanistic or training-data factors drive the systematic recency bias (preference for the second option) observed in persuasive argumentation tasks? The authors note this "warrants further investigation" to understand if it stems from discourse norms where stronger arguments follow weaker ones, or other heuristics.

- Can debiasing interventions effectively mitigate positional bias when semantic cues are weak or absent (high uncertainty) without degrading task performance? The conclusion highlights the "need for robust evaluation and debiasing strategies" and explicitly states that "Future work should explore... developing more interpretable, trustworthy, and bias-resilient models."

- Are the observed exponential relationships between uncertainty and positional bias consistent across languages with different typological features? The methodology relies on SQuAD-it-2 (Italian) to address underrepresentation, but does not verify if the exponential growth is universal or influenced by specific morphological/syntactic properties.

## Limitations

- The Italian-language SQuAD-it-2 benchmark may not reflect behaviors in higher-resource languages
- The binary-choice format simplifies the more complex multi-option scenarios common in real applications
- Quantized models introduce approximation uncertainties that may affect bias patterns differently than full-precision variants

## Confidence

- **High confidence:** The core finding that positional bias increases exponentially with uncertainty is well-supported by systematic experimental evidence across five diverse models and three dataset types
- **Medium confidence:** The mechanism that models "fall back" to positional heuristics when semantic discrimination becomes difficult is strongly suggested by the data but requires additional validation
- **Low confidence:** The discourse-structure recency bias hypothesis lacks direct evidence about training data discourse patterns and remains speculative

## Next Checks

1. Replicate the core experiment using English SQuAD or another high-resource language to determine if the uncertainty-bias relationship holds across linguistic contexts

2. Expand the experimental design beyond binary choices to test whether positional bias scales predictably with the number of options

3. Conduct a systematic examination of model training corpora to identify the prevalence of positionally-ordered argumentative structures, directly testing the discourse-recency hypothesis