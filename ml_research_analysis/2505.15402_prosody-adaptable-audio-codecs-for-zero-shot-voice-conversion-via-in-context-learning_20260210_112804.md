---
ver: rpa2
title: Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context
  Learning
arxiv_id: '2505.15402'
source_url: https://arxiv.org/abs/2505.15402
tags:
- speech
- prosody
- voice
- conversion
- codec
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses voice conversion (VC) by proposing a method
  to disentangle and control prosody while preserving speaker identity, leveraging
  in-context learning (ICL) for zero-shot adaptation. The core contribution is the
  Prosody-Aware Codec Encoder (PACE), which conditions audio representations on prosody
  features and uses mutual information loss to disentangle prosody from other speech
  attributes.
---

# Prosody-Adaptable Audio Codecs for Zero-Shot Voice Conversion via In-Context Learning

## Quick Facts
- **arXiv ID**: 2505.15402
- **Source URL**: https://arxiv.org/abs/2505.15402
- **Reference count**: 0
- **Primary result**: Proposed PACE module achieves 0.9078 ASV score, 0.1010 ASR-WER, and 2.6988 F0 distance for prosody alignment on LibriTTS

## Executive Summary
This paper presents PACE (Prosody-Aware Codec Encoder), a method for zero-shot voice conversion that disentangles and controls prosody while preserving speaker identity. The approach leverages in-context learning through VALLE-X, enabling adaptation to unseen speakers without pretrained speaker encoders. PACE conditions audio representations on prosody features and uses mutual information loss to explicitly separate prosody from content and timbre, then re-injects it as conditioning for controllable manipulation.

## Method Summary
The method extracts prosody features (F0 and unvoiced/voiced indicators) using pyworld's harvest function, then processes audio through a modified SoundStream encoder to create embeddings. Mutual information minimization via CLUB loss disentangles prosody from content and timbre embeddings. Prosody information is re-injected through explicit conditioning before residual vector quantization. The quantized codes are consumed by VALLE-X's codec language model, which performs autoregressive generation conditioned on both content phonemes and target speaker prompts. The system is trained in three stages: reconstruction-only, MI disentanglement addition, and joint training.

## Key Results
- Achieves 0.9078 ASV score (speaker similarity) and 0.1010 ASR-WER (intelligibility) on LibriTTS
- Outperforms VALLE-X, TriAAN-VC, and ProsoVC in speaker similarity, intelligibility, and prosody alignment
- F0 distance metrics show effective prosody transfer: 2.6988 for prosody from prompt vs 2.8239 for prosody from source
- Ablation studies confirm mutual information loss is critical for prosody disentanglement (F0 distance degrades to 3.2751 without LMI)

## Why This Works (Mechanism)

### Mechanism 1: Mutual Information Disentanglement
Minimizing mutual information between audio embeddings and prosody features via CLUB loss enables explicit disentanglement of prosody from content and timbre. This forces the speech embedding to become invariant to prosody features by contrasting positive pairs against sampled negatives, then re-injects prosody as explicit conditioning.

### Mechanism 2: Explicit Prosody Conditioning
Conditioning codec codes on extracted prosody features enables fine-grained, controllable prosody manipulation. The method extracts f0 and uv, embeds them, subtracts their influence via MI minimization, then re-injects via summation before RVQ, creating prosody-aware codes.

### Mechanism 3: In-Context Learning Adaptation
VALLE-X's emergent in-context learning enables zero-shot speaker adaptation by conditioning on target prompt codes. The pretrained neural codec language model learns speaker timbre through attention over prompt tokens rather than explicit speaker embeddings, generalizing to unseen speakers within VALLE-X's training distribution.

## Foundational Learning

- **Residual Vector Quantization (RVQ)**: Required for converting continuous PACE embeddings into discrete codes that VALLE-X consumes. Critical for understanding hierarchical codebook structure and debugging codec alignment.
  - Quick check: Why does RVQ use multiple codebooks rather than a single large codebook, and how does residual coding affect reconstruction quality?

- **Mutual Information Estimation via CLUB**: Essential for the disentanglement mechanism; incorrect implementation or tuning will cause prosody leakage.
  - Quick check: In equations 1-2, why does the contrastive term use negative samples from the batch, and what happens if the batch size is too small?

- **In-Context Learning in Autoregressive Language Models**: Critical for understanding how VALLE-X's zero-shot adaptation works through conditioning on prompt tokens.
  - Quick check: How does conditioning on acoustic prompt codes differ from conditioning on text tokens in a standard language model?

## Architecture Onboarding

- **Component map**: Source speech → Whisper ASR → phoneme sequence → VALLE-X conditioning; Prosody prompt → pyworld harvest → f0/uv → PACE → prosody-conditioned codes; Target prompt + prosody → PACE → RVQ → VALLE-X → output codes → decoder → converted speech

- **Critical path**: Source speech (speaker A) → Whisper ASR → phoneme sequence; Prosody prompt (speaker X) → harvest → f0, uv; Target prompt (speaker B) + f0/uv → PACE → prosody-conditioned codes; VALLE-X LM (phonemes + target codes) → output codes; Decoder → converted speech (content from A, timbre from B, prosody from X)

- **Design tradeoffs**: PACE vs. baseline EnCodec shows slight quality drop (NISQA 4.1662→3.9805, MOSNET 3.8755→3.6592) but improved intelligibility (ASR-WER 0.1239→0.1104). Three-stage training stabilizes learning but increases complexity. λMI weighting controls disentanglement strength vs. reconstruction quality.

- **Failure signatures**: ASV score drops sharply → scale layer or reconstruction loss failing to align with VALLE-X; High F0 distance despite training → MI loss not effectively disentangling; Poor ASR-WER → ASR transcription errors propagating; NISQA drops significantly → PACE encoder degrading audio quality.

- **First 3 experiments**: 1) Validate PACE reconstruction quality: Run PACE encode-decode without VALLE-X, compare NISQA/MOSNET/ASR-WER against baseline EnCodec. 2) Ablate mutual information loss: Train PACE without LMI, measure F0 distance and ASV score. 3) Prosody transfer visualization: Generate samples with prosody from speaker A, content/timbre from speaker B; plot F0 contours against reference.

## Open Questions the Paper Calls Out

### Open Question 1
Can the reconstruction fidelity of the PACE module be improved to match or exceed the baseline EnCodec's perceptual quality metrics (NISQA/MOSNET) without sacrificing intelligibility? The authors note a slight gap in performance between the original EnCodec and PACE, with lower NISQA (3.9805 vs 4.1662) and MOSNET scores.

### Open Question 2
Does the explicit conditioning on F0 and UV features impede the model's ability to perform cross-lingual voice conversion, a capability inherent to the VALLE-X backbone? The backbone possesses "strong generalization... particularly in zero-shot cross-lingual TTS," but evaluation is restricted to English.

### Open Question 3
Can the scope of disentangled prosody control be expanded to include duration and energy, which are currently omitted from the feature set? The methodology explicitly limits extracted features to "fundamental frequency (f0) and unvoiced/voiced (uv) indicators."

### Open Question 4
Does the performance of the PACE module scale effectively when trained on significantly larger datasets (e.g., 60K hours) typically used for training large codec language models? The model is trained on a "54-hour LibriTTS-clean-100 dataset" while VALLE-X is generally trained on tens of thousands of hours.

## Limitations

- **Technical limitations**: pyworld-based F0/UV extraction is sensitive to noise and may struggle with breathy or whispery speech; frame shift alignment is critical and may cause misalignment artifacts.
- **Distributional constraints**: Zero-shot capabilities depend entirely on VALLE-X's pretraining distribution; extreme vocal characteristics may fall outside the implicit speaker distribution.
- **Evaluation gaps**: Subjective MOS/SMOS scores lack statistical significance testing; F0 distance doesn't account for other prosodic dimensions like rhythm or stress.

## Confidence

**High Confidence Claims**: Mutual information minimization successfully disentangles prosody (evidenced by F0 distance improvements); PACE improves speaker similarity metrics while maintaining intelligibility; three-stage training procedure is technically sound.

**Medium Confidence Claims**: Superiority over baseline methods could be influenced by implementation differences beyond proposed contributions; prosody controllability claims are supported by F0 metrics but not comprehensively validated across all prosodic dimensions.

**Low Confidence Claims**: NISQA/MOSNET quality scores show degradation suggesting quality-reconstruction tradeoff not fully explored; "lightweight" claim lacks computational complexity analysis; exact impact of CLUB hyperparameters on disentanglement quality is not empirically validated.

## Next Checks

**Validation 1**: Implement PACE encode-decode pipeline without VALLE-X integration and measure quality degradation using NISQA, MOSNET, and ASR-WER. Compare against baseline EnCodec to quantify the reconstruction quality tradeoff introduced by prosody disentanglement.

**Validation 2**: Conduct controlled prosody transfer experiments with known prosodic variations (emotional speech, singing, whispered speech) to test the robustness of F0-based prosody conditioning beyond standard audiobook narration in LibriTTS.

**Validation 3**: Evaluate the zero-shot capabilities on out-of-distribution speakers including children's voices, accented speech, and non-standard prosody to empirically test the claimed generalization limits of the VALLE-X ICL adaptation.