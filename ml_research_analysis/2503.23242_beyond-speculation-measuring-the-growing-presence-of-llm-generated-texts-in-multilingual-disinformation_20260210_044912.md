---
ver: rpa2
title: 'Beyond speculation: Measuring the growing presence of LLM-generated texts
  in multilingual disinformation'
arxiv_id: '2503.23242'
source_url: https://arxiv.org/abs/2503.23242
tags:
- texts
- machine-generated
- datasets
- dataset
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first empirical evidence of large language
  model (LLM)-generated content in real-world multilingual disinformation datasets.
  The authors developed a detection framework using two fine-tuned multilingual detectors,
  GemmaMultiDomain and GemmaGenAI, which were evaluated on benchmark datasets showing
  precision of 0.94-0.9985 and true positive rates of 0.5-0.74.
---

# Beyond speculation: Measuring the growing presence of LLM-generated texts in multilingual disinformation

## Quick Facts
- arXiv ID: 2503.23242
- Source URL: https://arxiv.org/abs/2503.23242
- Reference count: 24
- First empirical evidence of LLM-generated content in real-world multilingual disinformation datasets

## Executive Summary
This study provides the first empirical evidence of large language model (LLM)-generated content in real-world multilingual disinformation datasets. The authors developed a detection framework using two fine-tuned multilingual detectors, Gemma_MultiDomain and Gemma_GenAI, which were evaluated on benchmark datasets showing precision of 0.94-0.9985 and true positive rates of 0.5-0.74. The framework was applied to four real-world datasets: MultiClaim (fact-checked social media posts), FakeNews (English election articles), USC-X (election-related Twitter posts), and FIGNEWS (news media narratives on the Israel-Gaza conflict). Results showed increasing prevalence of LLM-generated content over time, from 0.93% in 2021 to 1.85% in 2023 in MultiClaim, with significant variation across languages (highest in Polish at 4.7% and French at 4.2%) and platforms. The study documents machine-generated content in both "true" and "false" labeled texts, confirming concerns about LLM misuse for disinformation campaigns.

## Method Summary
The authors developed a dual-detector framework using Gemma-2-9b-it fine-tuned with QLoRA on two complementary datasets: Gemma_GenAI (trained on GenAI dataset) and Gemma_MultiDomain (trained on MULTITuDE + MultiSocial). The combined detection approach uses conservative probability thresholds (1.0 for positive, >0.0 for negative) with language-specific disqualification rules (Arabic, German, Italian, Russian disqualified due to FPR >0.1). The framework was evaluated on five benchmark datasets (MULTITuDE, MultiSocial, SemEval, GenAI, MIX) showing precision of 0.94-0.9985, then applied to four real-world disinformation datasets (MultiClaim, FakeNews, USC-X, FIGNEWS) to measure LLM-generated content prevalence over time and across languages/platforms using both binary predictions and Mean Score aggregation.

## Key Results
- LLM-generated content prevalence increased from 0.93% in 2021 to 1.85% in 2023 in MultiClaim dataset
- Significant variation across languages: highest in Polish (4.7%) and French (4.2%), lowest in Hebrew, Hindi, Arabic (<0.5%)
- High precision (0.94-0.9985) but moderate recall (0.5-0.74) due to conservative detection thresholds
- Machine-generated content found in both "true" and "false" labeled texts across all datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual detectors trained on complementary datasets provide more robust out-of-distribution detection than single detectors.
- Mechanism: Gemma_MultiDomain (trained on MULTITuDE + MultiSocial) and Gemma_GenAI (trained on GenAI dataset) produce independent probability scores. Their different training distributions create complementary coverageâ€”where one detector may struggle with certain domains or generators, the other may succeed.
- Core assumption: The diversity of training data translates to improved generalization on real-world disinformation content; errors between detectors are not perfectly correlated.
- Evidence anchors:
  - [abstract] "two fine-tuned multilingual detectors, Gemma_MultiDomain and Gemma_GenAI" evaluated with precision 0.94-0.9985
  - [Methods] "Since the detectors are trained on different data, their detection capabilities complement each other."
  - [corpus] Related work on LLM detection (e.g., "Human Texts Are Outliers") suggests out-of-distribution detection remains challenging; complementary approaches are an active research direction.
- Break condition: If real-world disinformation uses novel LLM generators or adversarial techniques outside both training distributions, both detectors may fail simultaneously.

### Mechanism 2
- Claim: Combined confident detection using probability thresholds reduces false positives at the cost of reduced recall.
- Mechanism: A positive prediction requires one detector to output probability 1.0 (maximal confidence) while the other outputs >0.0 (not fully confident in negative). Additionally, Gemma_GenAI positive predictions are disqualified for languages exceeding 0.1 FPR on benchmarks (Arabic, German, Italian, Russian).
- Core assumption: False positives are more costly than false negatives in prevalence estimation; near-maximal confidence indicates genuinely different text characteristics rather than borderline cases.
- Evidence anchors:
  - [Methods] "The prediction is positive if one of the detectors predicts the positive class with a probability of 1.0 and the other detector is not fully confident with the negative prediction"
  - [Results] Precision of 0.9358-0.9985 across benchmark datasets (Table 2), supporting conservative positive predictions
  - [corpus] Limited direct corpus evidence on this specific thresholding strategy; related work focuses on detector architecture rather than confidence calibration.
- Break condition: If adversarial actors learn to produce text that avoids triggering maximal-confidence predictions, detection rates could drop significantly while maintaining low false positives.

### Mechanism 3
- Claim: Mean Score aggregation across time periods reveals LLM-usage trends independent of threshold calibration.
- Mechanism: Rather than relying on binary predictions (which require calibrated thresholds), Mean Score averages all probability scores for texts in a given time period. This provides a distribution-robust signal: as LLM usage increases, the average probability of machine-generation rises even if individual classifications are uncertain.
- Core assumption: Mean probability scores shift monotonically with actual LLM prevalence; confounding factors (e.g., changes in human writing style, platform policies) do not systematically bias scores over time.
- Evidence anchors:
  - [Results] "Mean Score has increased from 0.04 and 0.27 in 2021 to 0.05 and 0.36 in 2023" representing ~30% increase
  - [Methods] "For measuring the prevalence of LLM-generated texts independently from distribution-specific calibrations, we use the Mean Score metric"
  - [corpus] Corpus papers on LLM detection do not extensively validate Mean Score methodology; this appears to be a study-specific approach.
- Break condition: If detector calibration drifts over time (e.g., due to shifting LLM output styles), Mean Score trends could reflect detector degradation rather than actual prevalence changes.

## Foundational Learning

- Concept: **QLoRA (Quantized Low-Rank Adaptation)**
  - Why needed here: The detectors are fine-tuned using QLoRA for efficient adaptation of the 9B-parameter Gemma-2 model. Understanding parameter-efficient fine-tuning is essential for reproducing or modifying the detection architecture.
  - Quick check question: Can you explain why QLoRA might be preferred over full fine-tuning for a multilingual detection task with limited labeled data?

- Concept: **Out-of-Distribution (OOD) Generalization**
  - Why needed here: The paper explicitly addresses that "fine-tuned detectors are known to not generalize well to out-of-distribution data." This motivates both the dual-detector approach and the conservative confidence thresholds.
  - Quick check question: If a detector trained on GPT-3.5 outputs is applied to text from an unreleased future model, would you expect precision to increase, decrease, or stay the same? Why?

- Concept: **False Positive Rate (FPR) vs. True Positive Rate (TPR) Tradeoffs**
  - Why needed here: The detection framework explicitly trades TPR (0.5-0.74) for high precision (0.94-0.9985). Understanding this tradeoff is critical for interpreting prevalence estimates as lower bounds rather than exact counts.
  - Quick check question: In disinformation detection, why might a researcher prefer 95% precision with 50% recall over 70% precision with 90% recall?

## Architecture Onboarding

- Component map: Gemma-2-9b-it -> Gemma_GenAI (fine-tuned on GenAI) + Gemma_MultiDomain (fine-tuned on MULTITuDE + MultiSocial) -> Combined Confident Detection (probability thresholds) -> Mean Score Aggregation

- Critical path:
  1. Load both fine-tuned detector weights
  2. For each input text, run inference through both detectors to obtain probability scores
  3. Apply language-specific disqualification rules (check FPR thresholds from benchmark evaluation)
  4. Apply combined confident detection logic for binary prediction
  5. Aggregate scores by time/language/platform for prevalence estimation

- Design tradeoffs:
  - **High precision vs. high recall**: Conservative thresholds (probability 1.0 requirement) yield reliable positive identifications but likely underestimate true prevalence (TPR 0.5-0.74 means many machine-generated texts are not flagged)
  - **Dual detectors vs. single detector**: Added complexity and inference cost in exchange for improved OOD robustness
  - **Mean Score vs. binary prediction**: Distribution-agnostic trend tracking vs. concrete instance-level classification

- Failure signatures:
  - **Sudden precision drop**: May indicate emergence of new LLM generators or adversarial techniques not represented in training data
  - **Language-specific anomalies**: Unexpectedly high detection rates in specific languages may indicate detector miscalibration (check against benchmark FPR)
  - **Temporal non-monotonicity**: Decreasing Mean Scores over time could indicate model degradation or changes in disinformation tactics

- First 3 experiments:
  1. **Benchmark validation**: Reproduce Table 2 results on held-out test splits to confirm detector performance before applying to real-world data
  2. **Threshold sensitivity analysis**: Test alternative confidence thresholds (e.g., 0.95, 0.99 instead of 1.0) to quantify the precision-recall tradeoff curve
  3. **Language-specific calibration**: For each target language, evaluate FPR on human-written benchmark data to verify disqualification rules remain appropriate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What factors drive the varying prevalence of LLM-generated content across different languages, platforms, and contexts?
- Basis in paper: [explicit] The conclusion states: "Future research should examine factors driving varying prevalence across contexts and investigate correlations with information ecosystem vulnerabilities."
- Why unresolved: The study documents significant variation (e.g., Polish at 4.7%, French at 4.2%, vs. under 0.5% for Hebrew, Hindi, Arabic) but does not investigate causal mechanisms behind these differences.
- What evidence would resolve it: Correlational studies linking prevalence rates to language-specific factors (e.g., native speaker population, LLM capabilities in that language), platform policies, and geopolitical contexts.

### Open Question 2
- Question: How can detection capabilities be maintained and improved as AI technology evolves, particularly for vulnerable languages and platforms?
- Basis in paper: [explicit] The conclusion explicitly calls for: "Maintaining and improving detection capabilities will be crucial as AI technology evolves, especially for vulnerable languages and platforms."
- Why unresolved: Current detectors show significant performance gaps across languages (e.g., Gemma_GenAI was disqualified for Arabic, German, Italian, and Russian due to FPR > 0.1), and models trained on current LLM outputs may not generalize to future models.
- What evidence would resolve it: Longitudinal studies tracking detector performance on outputs from newer LLM versions, plus development of language-specific detection benchmarks and training data.

### Open Question 3
- Question: What is the true prevalence of LLM-generated disinformation in the broader information ecosystem compared to measured prevalence in fact-checked datasets?
- Basis in paper: [inferred] The paper notes: "The MultiClaim dataset consists of fact-checked claims, but fact-checkers can only verify a small sample of the most viral or suspicious claims due to the vast daily influx of information. This creates a significant imbalance between true and false claims, complicating systematic comparisons."
- Why unresolved: Fact-checked datasets represent a biased sample of the information ecosystem, potentially missing both low-visibility LLM-generated content and sophisticated campaigns that evade fact-checker attention.
- What evidence would resolve it: Random sampling studies across broader social media corpora combined with detection analysis, comparing prevalence rates against fact-checked subsets.

### Open Question 4
- Question: How does the conservative combined confident detection threshold affect the accuracy of prevalence estimates?
- Basis in paper: [inferred] The detection method requires one detector to predict with probability 1.0 while the other does not fully reject, with TPRs ranging from 0.50 to 0.74 on benchmark datasets. The paper acknowledges: "Since the detectors do not always provide a probability of 1.0 for each machine-generated text, the prevalence of LLM-generated texts between 2021 and 2023 has most probably increased even more."
- Why unresolved: The high-precision, lower-recall approach likely underestimates true prevalence, but the magnitude of underestimation remains unknown.
- What evidence would resolve it: Calibration studies comparing confident detection against human annotation on stratified samples, and analysis of detection probability distributions to estimate missed cases.

## Limitations
- Conservative detection thresholds yield high precision (0.94-0.9985) but moderate recall (0.5-0.74), meaning true prevalence is likely underestimated
- Mean Score methodology for tracking temporal trends lacks extensive validation in broader corpus literature
- Language-specific detection performance varies significantly with unclear reasons for disparities

## Confidence

- **High confidence**: Detector precision (0.94-0.9985) on benchmark datasets; increasing Mean Score trends over time; documented variation across languages and platforms
- **Medium confidence**: Absolute prevalence percentages (0.93-1.85%) as lower bounds; dual-detector complementarity for OOD detection
- **Low confidence**: Mean Score methodology for trend analysis; precise interpretation of why certain languages show higher detection rates

## Next Checks

1. **Temporal consistency test**: Validate Mean Score trends by applying the framework to historical data with known LLM presence (e.g., ChatGPT release timeline) to confirm monotonic relationships between Mean Score and actual LLM usage

2. **Adversarial robustness evaluation**: Test detectors against adversarially crafted text designed to minimize probability scores while maintaining human-like characteristics to quantify potential evasion scenarios

3. **Cross-platform calibration**: Apply the framework to multiple platforms simultaneously using identical thresholds to identify platform-specific detector biases versus true prevalence differences