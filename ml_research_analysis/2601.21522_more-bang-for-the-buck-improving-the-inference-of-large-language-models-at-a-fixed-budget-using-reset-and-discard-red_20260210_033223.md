---
ver: rpa2
title: 'More Bang for the Buck: Improving the Inference of Large Language Models at
  a Fixed Budget using Reset and Discard (ReD)'
arxiv_id: '2601.21522'
source_url: https://arxiv.org/abs/2601.21522
tags:
- cost
- coverage
- pass
- https
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reset-and-Discard (ReD), a method to improve
  the efficiency of large language models (LLMs) on verifiable tasks under a fixed
  budget. The key problem is that standard solve-to-completion allocation leads to
  sublinear growth in the number of unique problems solved as budget increases, due
  to diminishing returns when some tasks are much harder than others.
---

# More Bang for the Buck: Improving the Inference of Large Language Models at a Fixed Budget using Reset and Discard (ReD)

## Quick Facts
- arXiv ID: 2601.21522
- Source URL: https://arxiv.org/abs/2601.21522
- Reference count: 40
- Primary result: ReD algorithm strictly improves coverage@cost (unique problems solved under fixed budget) by cycling through problems rather than solving each to completion

## Executive Summary
This paper addresses the fundamental inefficiency in large language model inference where solve-to-completion allocation leads to sublinear growth in unique problems solved as budget increases. The authors introduce Reset-and-Discard (ReD), a breadth-first approach that resets to the next task after a fixed number of attempts and discards solved tasks. Theoretical analysis proves ReD strictly improves coverage@cost for any budget and difficulty distribution, with τ=1 (resetting every attempt) being optimal. Experiments on HumanEval demonstrate substantial improvements across three model sizes when measured in attempts, tokens, or cost.

## Method Summary
ReD implements a breadth-first search strategy for verifiable tasks by cycling through all questions in rounds, attempting each once per round, and immediately discarding solved problems. This contrasts with solve-to-completion, which exhaustively attempts each question until solved before moving to the next. The theoretical framework uses renewal theory to map pass@k distributions to coverage@cost curves, showing that when pass@k decays as a power law with exponent α<1, solve-to-completion yields sublinear growth while ReD enables linear growth. The method can predict savings when pass@k is known or estimate the power-law exponent when it is not.

## Key Results
- ReD strictly improves coverage@cost for any budget and difficulty distribution compared to solve-to-completion
- τ=1 (resetting every attempt) is proven to be the optimal ReD parameter
- Under power-law pass@k decay with α<1, ReD achieves linear coverage@cost growth versus sublinear growth for solve-to-completion
- On HumanEval, ReD increases problems solved by 30-50% at fixed budgets across three model sizes
- For smaller llama-3.1-8b, ReD outperforms larger llama-3.3-70b up to certain coverage levels when measured in tokens or cost

## Why This Works (Mechanism)
ReD works by avoiding the diminishing returns trap of solve-to-completion. When some tasks are much harder than others, spending excessive attempts on difficult problems yields diminishing returns while easy problems remain unsolved. By cycling through all problems (breadth-first), ReD ensures that the marginal benefit of each attempt is maximized across the entire problem set rather than being concentrated on already-solved or intractable problems. This approach exploits the fact that most problems in verifiable benchmarks have relatively similar difficulty distributions, making breadth-first exploration more efficient than depth-first completion.

## Foundational Learning
- **Coverage@cost**: The ratio of unique problems solved to cumulative attempts/resources used; needed to measure inference efficiency under budget constraints; quick check: plot solved vs attempts and verify sublinear vs linear growth
- **Power-law decay**: When pass@k ~ k^(-α), determining the exponent α is crucial for predicting ReD benefits; quick check: fit log(1-pass@k) vs log(k) and verify α<1
- **Renewal theory**: Mathematical framework for analyzing stochastic processes with resets; needed to prove ReD optimality; quick check: verify that expected attempts to solve additional problems decrease under ReD
- **Breadth-first vs depth-first allocation**: Strategic choice between attempting many problems shallowly versus few problems deeply; quick check: compare coverage curves under different τ values
- **Verifiable tasks**: Problems with automated correctness checking (like HumanEval); needed because ReD relies on immediate discard of solved problems; quick check: ensure perfect verifier implementation
- **Pass@k metric**: Probability of solving a problem within k attempts; needed to characterize difficulty distributions; quick check: compute pass@k for each problem and verify decay pattern

## Architecture Onboarding
**Component Map:** HumanEval questions -> ReD scheduler -> LLM API calls -> Verifier -> Coverage tracker -> Budget monitor
**Critical Path:** Question selection → LLM inference → Output verification → Success/failure recording → Next question selection
**Design Tradeoffs:** τ=1 maximizes coverage but increases API call overhead; larger τ reduces overhead but sacrifices some efficiency; ReD requires perfect verifier whereas solve-to-completion can tolerate noisy verification
**Failure Signatures:** Sublinear coverage growth under solve-to-completion not observed (check α>1); exponent inference from ReD fails (check Rn≫1 for fit rounds); coverage gains smaller than predicted (check API pricing accuracy)
**First Experiments:** 1) Implement ReD with τ=1 and baseline solve-to-completion on HumanEval; 2) Compute coverage@cost curves and verify ReD improvement; 3) Fit power-law decay and predict savings using theoretical equations

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims rely on idealized assumptions about pass@k decay and perfect verifier behavior
- Empirical validation limited to three model sizes on single benchmark (HumanEval)
- Uncertainty about generalization to other task distributions, longer-form generation, or multi-turn reasoning tasks
- Claims about τ=1 optimality assume stationarity in difficulty distribution and pass@k decay rate

## Confidence
- High confidence in ReD's empirical performance gains on HumanEval at fixed attempt budgets
- Medium confidence in token and cost-based savings due to API pricing variability
- Medium confidence in theoretical optimality claims, pending validation across diverse difficulty distributions
- Low confidence in ReD's performance when pass@k exhibits non-power-law behavior or heavy-tailed difficulty distributions

## Next Checks
1. Test ReD on non-power-law pass@k decays (exponential or step-function) to verify theoretical robustness
2. Implement ReD on multi-turn reasoning benchmark (e.g., GSM8K with step-by-step solutions) for sequential generation tasks
3. Compare ReD against adaptive round-robin strategies that dynamically adjust τ based on observed success rates