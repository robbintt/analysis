---
ver: rpa2
title: 'Multi-Agent Reinforcement Learning for Market Making: Competition without
  Collusion'
arxiv_id: '2510.25929'
source_url: https://arxiv.org/abs/2510.25929
tags:
- agent
- market
- agents
- execution
- behavior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a hierarchical multi-agent reinforcement learning\
  \ framework for studying competitive behavior in market making without collusion.\
  \ The method introduces three types of agents: a self-interested market maker (Agent\
  \ A), a competitive agent (B2) trained to minimize opponents' profits, and a hybrid\
  \ agent (B\u2605) that dynamically modulates between self-interested and competitive\
  \ behaviors using a learnable parameter."
---

# Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion

## Quick Facts
- arXiv ID: 2510.25929
- Source URL: https://arxiv.org/abs/2510.25929
- Reference count: 21
- Primary result: Hybrid agent achieves market dominance with milder adverse effects through adaptive modulation

## Executive Summary
This paper presents a hierarchical multi-agent reinforcement learning framework for studying competitive behavior in market making without collusion. The method introduces three types of agents: a self-interested market maker (Agent A), a competitive agent (B2) trained to minimize opponents' profits, and a hybrid agent (B★) that dynamically modulates between self-interested and competitive behaviors using a learnable parameter. Results show that Agent B2 achieves dominant performance through aggressive quoting and high market share, tightening spreads and improving execution efficiency, but at the cost of systemic pressure on competitors. In contrast, the hybrid agent B★ exhibits adaptive self-interested behavior when co-existing with profit-seeking agents, securing dominant market share while exerting milder adverse effects on overall market dynamics. These findings suggest that adaptive incentive control supports more sustainable strategic coexistence in heterogeneous agent environments.

## Method Summary
The paper develops a three-stage hierarchical training pipeline using PPO via Stable-Baselines3. First, an adversary perturbs market parameters (λ, σ) to create stressful conditions. Second, Agent A trains under these adversarial conditions to develop robustness. Third, three B-type agents train independently against frozen Agent A: B1 (self-interested), B2 (zero-sum competitive with r_B2 = -r_A), and B★ (hybrid with modulation parameter ω). The synthetic market environment simulates mid-price evolution via Brownian motion, Poisson order flow, and probabilistic fills based on quote distance (p_fill(d) = exp(-αd)). Evaluation uses agent-level metrics (PnL, Sharpe, inventory volatility, market share), market-level metrics (avg spread, fill ratio), and interaction-level metrics (joint drawdown, herding ratio, inventory divergence, fill overlap).

## Key Results
- Agent B2 captures 83.6% market share with aggressive quoting (aggressiveness = 1.482) while reducing competitor A's PnL from 506.49 to 81.48
- Hybrid agent B★ achieves 67% market share against A with ω = 0.697, exerting milder systemic pressure (joint drawdown ratio = 0.107 vs B2's 0.153)
- Adversarial training improves Agent A robustness: 24% higher PnL under adversarial conditions (626.42 vs 506.49 fixed) with stable Sharpe ratio

## Why This Works (Mechanism)

### Mechanism 1: Competitive Reward Shaping Drives Aggressive Market Capture
Competitive reward (r_B2 = -r_A) → aggressive quoting (lower bid/ask offsets) → higher fill probability (exponential decay p_fill(d) = exp(-αd)) → market share concentration → suppressed competitor profits. B2 posts tighter quotes (aggressiveness = 1.482 vs A's 2.524), capturing 83.6% market share while reducing A's PnL from 506.49 (solo) to 81.48 (joint).

### Mechanism 2: Hybrid Modulation Enables Adaptive Strategic Coexistence
r_B★ = ω·r_self - (1-ω)·r_A - penalty·(ω-0.5)² → B★ learns ω ≈ 0.696 during evaluation → favors self-interested behavior while retaining competitive edge → secures 67% market share against A with less severe Sharpe ratio degradation (1.742 vs B2's impact reducing A to 1.639).

### Mechanism 3: Adversarial Environment Training Induces Robustness with Partial Transfer
Adversary perturbs λ ∈ [300,500] and σ ∈ [0.2,2.0] → Agent A learns under volatility stress → A achieves 24% higher PnL under adversarial conditions (626.42 vs 506.49 fixed) with stable Sharpe. B1, trained with A but not adversary, shows partial robustness transfer (zero-fill steps drop from 27 to 16.87) but 16% lower PnL than A under adversarial evaluation.

## Foundational Learning

- **Proximal Policy Optimization (PPO) with Actor-Critic Architecture**
  - Why needed here: All agents use PPO via Stable-Baselines3; understanding clipping, advantage estimation, and policy updates is essential for debugging training instability.
  - Quick check question: Can you explain why PPO's clipped objective prevents excessive policy updates compared to vanilla policy gradient?

- **Market Microstructure: Limit Order Books, Spreads, and Fill Probability**
  - Why needed here: Environment simulates mid-price evolution, order flow (Poisson), and probabilistic execution (exp(-αd)); quoting strategy directly maps to market-making fundamentals.
  - Quick check question: If two agents quote at offsets d=1 and d=2 with α=0.5, what are their relative fill probabilities?

- **Multi-Agent Reward Shaping and Zero-Sum Game Formulation**
  - Why needed here: Agent behaviors diverge entirely based on reward design (self-interested vs competitive vs hybrid); interpreting results requires understanding reward-rational behavior.
  - Quick check question: What behavioral difference would you expect if B2's reward were r_B2 = -r_A + 0.1·r_self instead of pure zero-sum?

## Architecture Onboarding

- **Component map:**
  Adversary (environment-level actions: λ_t, σ_t) → perturbs market dynamics
  Agent A (PPO actor-critic; obs: price, time, inventory, cash; actions: d_bid, d_ask) → self-interested MM trained under adversarial stress
  B1 (self-interested), B2 (zero-sum), B★ (hybrid with extended action space [d_bid, d_ask, ω])
  Environment: Synthetic HFT simulator with mid-price Brownian motion, Poisson order flow, multinomial fill allocation, inventory constraints

- **Critical path:**
  1. Pretrain adversary to maximize r_adv = -(ΔPnL_benchmark - ζI²_t - η·terminal·I²_t)
  2. Train Agent A under fixed adversary policy (robustness via adversarial exposure)
  3. Train B-type agents independently against frozen Agent A
  4. Evaluate pairwise agent interactions with interaction-level metrics (herding, fill overlap, inventory divergence)

- **Design tradeoffs:**
  - B2's aggressive quoting tightens spreads (market efficiency ↑) but suppresses competitors (systemic concentration ↑)
  - B★'s modulation parameter adds flexibility but requires penalty regularization to prevent collapse
  - Adversarial training improves robustness but may overfit to specific perturbation patterns
  - Assumption: No communication between agents; asymmetric information environment

- **Failure signatures:**
  - ω collapsing to 0 or 1 early in training (penalty coefficient too low)
  - B1/B2 showing near-identical behavior (insufficient reward differentiation)
  - Agent A PnL degrading under fixed market after adversarial training (overfitting to stress)
  - Zero-fill steps remaining high despite aggressive quoting (environment fill probability mismatch)

- **First 3 experiments:**
  1. Replicate single-agent evaluation: Train Agent A with adversary; evaluate in both fixed (λ=400, σ=1.1) and adversarial settings; verify PnL and Sharpe stability.
  2. Ablate modulation penalty: Train B★ with penalty_coeff ∈ {0, 0.01, 0.1, 1.0}; measure ω distribution and behavioral collapse rates.
  3. Test B★ generalization: After training B★ against Agent A, evaluate against unseen B1; compare ω values and market share vs A-B★ baseline to assess opponent adaptation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a meta-learning formulation enable agents to dynamically adapt objective preferences based on environmental conditions more effectively than the current hybrid architecture?
- Basis in paper: [explicit] The authors state in the conclusion that "one could pursue a meta-learning formulation where agents learn to adapt their objective preferences based on environmental conditions or opponent behavior."
- Why unresolved: The current hybrid agent ($B^\star$) optimizes the modulation parameter $\omega$ directly as part of the action vector, which couples it tightly to the immediate reward and may limit strategic flexibility compared to a higher-level meta-controller.
- What evidence would resolve it: A comparative study where a meta-learning agent demonstrates superior adaptation speed and stability compared to Agent $B^\star$ when facing novel adversarial environments or unseen opponent strategies.

### Open Question 2
- Question: Do the observed competitive behaviors and lack of collusion persist in a Limit Order Book (LOB) environment where price formation is endogenous?
- Basis in paper: [inferred] The paper notes that the lack of collusion contrasts with retail pricing literature, attributing this partially to the "exogenous stochastic process" of the simulated mid-price. It leaves open whether these results hold in more realistic microstructures.
- Why unresolved: The current synthetic environment relies on exogenous price dynamics (Brownian motion) and probabilistic fills, abstracting away the complex queue dynamics and endogenous price impact found in real markets.
- What evidence would resolve it: Replicating the framework in a high-fidelity LOB simulator to observe if the hybrid agent maintains dominance or if agents develop tacit collusion strategies absent exogenous price anchors.

### Open Question 3
- Question: How do interaction-level metrics and systemic stability scale when the population of heterogeneous agents increases beyond pairwise interactions?
- Basis in paper: [inferred] The study evaluates interactions primarily between pairs or trios of agents (A vs B1, A vs B2, etc.). While it analyzes systemic pressure, it does not test how these dynamics evolve in a crowded market with $N$ agents.
- Why unresolved: It is unclear if the "milder adverse impact" of the hybrid agent scales to prevent market failure in a dense population, or if the aggressive dominance of Agent B2 leads to systemic instability in larger systems.
- What evidence would resolve it: Simulating a multi-agent environment with $N > 10$ agents of mixed types and analyzing the aggregate levels of inventory divergence and joint drawdowns.

## Limitations
- Fill probability formula (p_fill(d) = exp(-αd)) oversimplifies real market microstructure where queue position and volume imbalances matter
- Adversarial perturbation ranges (λ ∈ [300,500], σ ∈ [0.2,2.0]) appear arbitrary and may not reflect realistic market stress
- Hybrid agent's modulation penalty coefficient (0.01) was fixed without sensitivity analysis, raising concerns about overfitting to training opponent

## Confidence

**High Confidence**: The core mechanism that competitive reward shaping (r_B2 = -r_A) drives aggressive quoting and market share concentration is well-supported by direct empirical evidence (B2 captures 83.6% market share with PnL of 306.53 vs A's 81.48 in joint setting).

**Medium Confidence**: The hybrid agent's adaptive behavior through ω modulation shows promising results (B★ achieves 67% market share with milder adverse effects), but the generalization claim to unseen opponents lacks direct validation.

**Low Confidence**: The interaction-level metrics (herding ratio, fill overlap, inventory divergence) provide novel analytical tools, but their interpretation and threshold values lack grounding in established market microstructure theory.

## Next Checks

1. **Cross-opponent generalization test**: After training B★ against Agent A, evaluate it against both Agent B1 and a randomly initialized Agent C. Compare ω values, market share distribution, and interaction metrics across all pairings to quantify generalization versus overfitting.

2. **Adversarial perturbation sensitivity**: Vary the adversary's perturbation ranges systematically (e.g., λ ∈ [350,450], [200,600]; σ ∈ [0.5,1.5], [0.1,3.0]) and measure the impact on Agent A's robustness metrics (PnL, Sharpe under adversarial evaluation).

3. **Statistical significance verification**: Re-run all pairwise agent interactions with 30+ independent seeds. Report mean and standard error for all metrics (PnL, Sharpe, market share, interaction metrics). Apply paired t-tests to determine which performance differences are statistically significant versus random variation.