---
ver: rpa2
title: Targeted Learning for Data Fairness
arxiv_id: '2502.04309'
source_url: https://arxiv.org/abs/2502.04309
tags:
- fairness
- data
- inference
- estimator
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a targeted learning approach to perform statistical
  inference on data fairness, moving beyond algorithmic fairness to assess the data-generating
  process itself. The authors derive estimators for demographic parity, equal opportunity,
  and conditional mutual information using the efficient influence function framework.
---

# Targeted Learning for Data Fairness

## Quick Facts
- arXiv ID: 2502.04309
- Source URL: https://arxiv.org/abs/2502.04309
- Reference count: 24
- Primary result: Introduces targeted learning approach for statistical inference on data fairness metrics with double robustness guarantees

## Executive Summary
This paper presents a targeted learning framework for performing statistical inference on data fairness, shifting focus from algorithmic fairness to assessing fairness at the data-generating process level. The authors develop estimators for demographic parity, equal opportunity, and conditional mutual information using the efficient influence function framework, demonstrating that their probabilistic fairness estimators achieve double robustness. Their approach provides valid confidence intervals even when one of the two required models is misspecified, and can identify which variables contribute to fairness violations in real-world datasets.

## Method Summary
The authors derive efficient influence functions for three fairness metrics—demographic parity, equal opportunity, and conditional mutual information—and construct doubly robust estimators by combining outcome and propensity models through estimating equations. They use sample splitting to control empirical process terms and enable inference with flexible machine learning models via SuperLearner ensembles. The method distinguishes between traditional fairness (binary predictions) and probabilistic fairness (soft predictions), with the latter showing better variance properties. Inference is performed using Wald-type confidence intervals with variance estimated from the efficient influence function.

## Key Results
- Probabilistic fairness estimators achieve double robustness, remaining consistent when either outcome or propensity model is correctly specified
- Targeted learning variance estimates exceed naive t-test variance, appropriately accounting for estimation uncertainty in conditional distributions
- Simulation shows >95% coverage for demographic parity and equal opportunity estimators across sample sizes 500-5000, with bias typically below 0.01
- CMI estimator exhibits bias near zero and poor coverage, limiting its utility for inference despite capturing dependence structure
- Real-world applications identify fairness violations in Adult-Income and Law School datasets, with variable importance analysis showing race/gender as major contributors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic fairness metric estimators achieve double robustness—valid inference if either the outcome model or propensity model is correctly specified.
- Mechanism: The estimator structure combines an outcome model D̂(x) and propensity model π̂(x) through the efficient influence function. When taking expectations, cross-terms cancel if either model matches truth: E[π̂(X)D(X)] + E[π(X)D̂(X)] - E[π̂(X)D̂(X)] → E[π(X)D(X)] when either π̂=π or D̂=D.
- Core assumption: At least one nuisance model converges to truth at rate o_P(n^(-1/4)); product of convergence rates is o_P(n^(-1/2)).
- Evidence anchors:
  - [abstract] "our estimators for probabilistic fairness metrics are doubly robust"
  - [section 3.1.3] "if either π̂(x) = π(x) or D̂(x) = D(x), then E[Ψ̂(P̂)] ≈ Ψ(P)"
  - [appendix C] Full proof showing E[Ψ̂₁(P̂)] - Ψ₁(P) ≤ |E[1/p̂ - 1/p]| → 0
  - [corpus] "Rescuing double robustness: safe estimation under complete misspecification" discusses limitations when both models fail—relevant caveat.
- Break condition: Both π̂ and D̂ are severely misspecified simultaneously; product of errors exceeds o_P(n^(-1/2)).

### Mechanism 2
- Claim: Targeted learning produces valid confidence intervals by correcting plug-in bias and controlling empirical process variance through sample splitting.
- Mechanism: The EIF identifies bias as -P_n[φ(O, P̂)]. Setting sample mean of EIF to zero yields bias-corrected estimator. Sample splitting breaks dependence between P̂ estimation and inference evaluation, ensuring (P_n - P)[φ(O, P̂) - φ(O, P)] = o_P(n^(-1/2)).
- Core assumption: L²-consistency of φ(O, P̂); IID data; model class complexity controlled or decoupled via splitting.
- Evidence anchors:
  - [section 2] "the plug-in bias turns out to be exactly -1/n Σφ(Oᵢ, P̂)"
  - [appendix D] "We can either only consider fitting functions that are 'simple enough' (i.e. Donsker) or simply decouple our estimation and inference by sample splitting"
  - [corpus] Weak direct support—neighbors focus on estimation, not specifically on TL inference guarantees.
- Break condition: Adaptive data collection violating IID; overfitting P̂ on training set that leaks to evaluation set (insufficient split); non-Donsker model classes without splitting.

### Mechanism 3
- Claim: TL variance estimates exceed naive t-test variance for fairness metrics because TL accounts for conditional distribution estimation uncertainty.
- Mechanism: Naive approaches treat predictions as fixed samples, ignoring that D̂(x) itself is estimated from data with sampling variability. The EIF propagates this uncertainty through φ(O, P̂), inflating standard errors appropriately.
- Core assumption: The conditional distribution Y|X must be estimated (data fairness setting), not given (model fairness setting).
- Evidence anchors:
  - [section 4.1.1] "TL estimates of variance are significantly larger than the t-test standard errors. This is desirable since we not only need to estimate the mean of a variable, but the conditional distribution as well."
  - [section 3.1.3] "data fairness requires additional estimation steps and further consideration of the relationships between X, Y, and G"
  - [corpus] No direct corpus support for this specific claim.
- Break condition: When outcome model is known ex-ante (true model fairness setting), TL and naive variance converge.

## Foundational Learning

- Concept: **Efficient Influence Function (EIF)**
  - Why needed here: EIF encodes how estimand changes under distribution perturbations; enables bias correction and efficient estimator construction.
  - Quick check question: Can you explain why EIF for population mean is (X - E[X]), and what this implies for constructing an unbiased estimator?

- Concept: **Double Robustness**
  - Why needed here: Core theoretical guarantee for probabilistic fairness estimators; justifies ensemble modeling strategies.
  - Quick check question: If propensity model π̂ achieves 15% error and outcome model D̂ achieves 20% error, what must hold for double robustness to provide valid inference?

- Concept: **Sample Splitting for Inference**
  - Why needed here: Controls empirical process terms when using flexible ML models; avoids Donsker class restrictions.
  - Quick check question: Why does fitting and evaluating on the same data invalidate Wald confidence intervals for nonparametric estimators?

## Architecture Onboarding

- Component map:
  - Estimand Layer -> EIF Derivation -> Nuisance Estimators -> Bias Correction -> Inference Engine

- Critical path: Estimand definition -> EIF derivation (analytical, one-time) -> Nuisance model training (computational bottleneck) -> Bias-corrected estimation -> Confidence interval construction

- Design tradeoffs:
  - Traditional vs probabilistic metrics: Traditional uses binary D_c(x) thresholding; probabilistic uses soft D(x). Probabilistic has lower variance per simulations but different substantive meaning.
  - Single vs separate CMI approach: Single joint model for (Y,G)|X vs separate marginals. Simulations show single approach has lower bias.
  - Ensemble complexity vs convergence rate: Larger ensembles increase chance of capturing truth but computational cost grows; Highly Adaptive Lasso guarantees o_P(n^(-1/4)) but generates O(n·2^d) features.

- Failure signatures:
  - Coverage < 90% with large n: Likely model misspecification in both nuisance models; check residual diagnostics
  - CMI estimates ≈ 0 with zero coverage: Known bias near zero per Section 4.2; do not use CMI for inference
  - Variance estimates suspiciously small: Check that train/test split is enforced; verify EIF implementation includes all terms
  - Confidence intervals never contain true value in simulation: EIF derivation error; verify against Appendix B/C formulas

- First 3 experiments:
  1. **Validate on simulation Setting 1**: Implement probabilistic parity estimator; verify >95% coverage and bias < 0.01 with n=5000. Compare TL variance to t-test variance—should be larger.
  2. **Test double robustness via misspecification**: Use Setting 3; fit logistic model (misspecified for squared-X truth) for D̂ only, then π̂ only, then both. Coverage should remain adequate when exactly one model is misspecified.
  3. **Identify CMI failure modes**: Run CMI estimator on simulation varying c parameter. Confirm bias near zero for small c; document that inference is unreliable per paper's explicit limitation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can targeted learning estimators be generalized to accommodate categorical and continuous sensitive attributes and outcomes rather than just binary ones?
- Basis in paper: [explicit] Section 6.2 states that focusing only on binary group status is limiting and calls for future work to develop tools for categorical and continuous attributes.
- Why unresolved: The derivations for demographic parity and equal opportunity in Section 3 rely explicitly on the binary nature of the outcome $Y$ and group $G$.
- What evidence would resolve it: Derivation of efficient influence functions for non-binary variables and simulations confirming the error rates and coverage of the resulting estimators.

### Open Question 2
- Question: Can a hypothesis testing framework effectively assess conditional independence in data fairness given the poor inferential properties of the Conditional Mutual Information (CMI) estimator?
- Basis in paper: [explicit] Section 6.2 notes that the CMI estimator has poor coverage and suggests that "hypothesis testing may be a more appropriate approach" for future work.
- Why unresolved: The current CMI estimator is biased and exhibits coverage near zero for many scenarios, making it invalid for standard confidence interval inference.
- What evidence would resolve it: The development of a conditional independence test with controlled Type I error rates that outperforms the estimation-based approach.

### Open Question 3
- Question: What approaches can be developed to remedy or correct violations of data fairness in the data-generating process?
- Basis in paper: [explicit] Section 6.2 highlights that while the paper provides methods for assessment, it offers "no guidance on correcting for unfair decisions" and solicits remediation methods.
- Why unresolved: The paper focuses exclusively on the detection and statistical inference of fairness violations, not on the intervention required to fix them.
- What evidence would resolve it: Algorithms or procedural interventions that successfully alter the data-generating process to reduce specific fairness metric violations.

## Limitations

- Nuisance model specification uncertainty: SuperLearner library composition, CV folds, and hyperparameters are not specified, potentially impacting performance
- CMI estimator reliability issues: Acknowledged bias near zero and poor coverage limit interpretability and utility for inference
- Real-world applicability concerns: Detected fairness violations in datasets may reflect measurement artifacts rather than genuine societal inequities

## Confidence

- High confidence: Probabilistic fairness estimators achieving double robustness; TL variance exceeding naive approaches; simulation results for demographic parity and equal opportunity
- Medium confidence: EIF derivation and bias correction mechanism; variable importance methodology; interpretation of real-world results
- Low confidence: CMI estimator performance and interpretability; optimal SuperLearner configuration for fairness applications

## Next Checks

1. **Systematic SuperLearner benchmarking**: Test multiple library compositions (logistic, RF, XGBoost with varying hyperparameters) across simulations to identify configurations that maximize double robustness
2. **CMI estimator comparison**: Implement and compare alternative CMI estimators (e.g., KNN-based) to quantify whether observed bias is inherent to the CMI estimand or the TL approach
3. **Real-world sensitivity analysis**: Apply the method to additional datasets with known fairness properties to calibrate expectations about when detected violations reflect meaningful inequities versus statistical artifacts