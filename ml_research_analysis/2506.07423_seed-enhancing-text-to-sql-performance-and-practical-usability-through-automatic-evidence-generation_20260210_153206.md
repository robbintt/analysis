---
ver: rpa2
title: 'SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic
  Evidence Generation'
arxiv_id: '2506.07423'
source_url: https://arxiv.org/abs/2506.07423
tags:
- evidence
- seed
- text-to-sql
- bird
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SEED addresses the gap between academic text-to-SQL research and
  real-world deployment by proposing an automatic evidence generation system. It systematically
  analyzes database schema, description files, and sampled values to generate domain
  knowledge, synonyms, and value illustrations without relying on human-provided evidence.
---

# SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation

## Quick Facts
- arXiv ID: 2506.07423
- Source URL: https://arxiv.org/abs/2506.07423
- Reference count: 40
- SEED automatically generates evidence from database schema, descriptions, and sampled values, achieving up to 17.73% improvement in exact match rate on BIRD dataset.

## Executive Summary
SEED addresses the gap between academic text-to-SQL research and real-world deployment by proposing an automatic evidence generation system. It systematically analyzes database schema, description files, and sampled values to generate domain knowledge, synonyms, and value illustrations without relying on human-provided evidence. Evaluated on BIRD and Spider datasets, SEED significantly improves SQL generation accuracy in no-evidence scenarios, with up to 17.73% improvement in exact match rate (EX%) on BIRD and up to 4.6% on Spider. In some cases, SEED even outperforms settings using human-generated BIRD evidence, demonstrating enhanced adaptability and robustness of text-to-SQL models.

## Method Summary
SEED employs two architectures: SEED_gpt using GPT-4o with full schema context, and SEED_deepseek using DeepSeek-R1 with schema summarization for 8K token limits. The system extracts keywords from questions, generates sample SQL queries, executes them against databases to retrieve values, and uses embedding similarity to find relevant training examples. These components feed into an evidence generation module that produces domain knowledge, synonym mappings, and value illustrations. The evidence is then passed to downstream text-to-SQL models. SEED was evaluated on BIRD (12,751 pairs, 95 databases) and Spider (200 databases, 10,181 questions) datasets.

## Key Results
- SEED achieves up to 17.73% improvement in exact match rate (EX%) on BIRD dataset in no-evidence scenarios
- SEED improves execution accuracy by up to 4.6% on Spider dataset
- SEED sometimes outperforms settings using human-generated BIRD evidence, showing enhanced adaptability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Executing sample SQL queries against the database reveals schema-to-value mappings that clarify ambiguous natural language terms.
- Mechanism: SEED extracts keywords from the question representing potential columns/values, generates sample SQL queries, and executes them to retrieve actual data. This reveals whether "Fremont" refers to a county, district, or city by inspecting where the value appears.
- Core assumption: Database values contain sufficient signal to disambiguate natural language terms when sampled appropriately.
- Evidence anchors: [section III-B] "SEED emulates this human process by systematically executing sample SQL queries to generate domain knowledge... pairs the extracted columns with their corresponding values and generates and executes sample SQL queries for each pair."
- Break condition: Databases with sparse or encoded values (e.g., IDs without descriptive names) may yield insufficient signal for disambiguation.

### Mechanism 2
- Claim: Retrieving similar training examples via embedding similarity improves evidence generation by providing in-context exemplars.
- Mechanism: SEED uses all-mpnet-base-v2 embeddings and cosine similarity to find the most similar question from training data, then retrieves four additional questions from the same database. These form few-shot examples in the evidence generation prompt.
- Core assumption: Similar questions benefit from structurally similar evidence patterns (synonym mappings, value illustrations).
- Evidence anchors: [section III-C] "SEED identifies the question most similar to the given query from the training set and then retrieves four more related questions from the same database. We use all-mpnet-base-v2 as the embedding model."
- Break condition: Novel domains with no training analogs will provide weak few-shot guidance; embedding similarity may surface superficially similar but structurally different SQL patterns.

### Mechanism 3
- Claim: Preserving full schema context (vs. pruning) improves evidence quality when using high-capacity reasoning models.
- Mechanism: SEED-gpt passes the entire schema without summarization, based on findings that schema pruning degrades performance with strong reasoning LLMs. SEED-deepseek only summarizes when context limits (8,192 tokens) require it.
- Core assumption: Strong reasoning models can attend to relevant schema elements without explicit filtering; pruning removes potentially useful signals.
- Evidence anchors: [section III-A] "A recent study has highlighted that performing schema pruning as a preprocessing step when leveraging LLMs with strong reasoning capabilities for text-to-SQL tasks can actually degrade SQL generation performance."
- Break condition: Very large schemas exceeding even extended context windows will force summarization, potentially losing relevant signals.

## Foundational Learning

- Concept: **Evidence types in text-to-SQL (Domain Knowledge, Synonym Knowledge, Value Illustration)**
  - Why needed here: SEED generates these three evidence categories automatically. Understanding what each provides helps debug evidence quality.
  - Quick check question: Given "How many female clients opened accounts?", what evidence type maps "female" to `gender = 'F'`?

- Concept: **Schema linking vs. full schema approaches**
  - Why needed here: SEED deliberately avoids schema linking/pruning. Understanding this tradeoff explains why two architectures exist.
  - Quick check question: Why might schema pruning help small models but hurt GPT-4-class models?

- Concept: **Execution accuracy (EX) vs. exact string match**
  - Why needed here: Paper reports EX% improvements. Semantically equivalent SQL with different syntax should score as correct.
  - Quick check question: If SEED generates `SELECT COUNT(*)` but ground truth is `SELECT COUNT(id)`, should EX count this as correct?

## Architecture Onboarding

- Component map:
  Sample SQL Execution Module -> Evidence Generation Module -> Downstream Text-to-SQL Model
  (gpt-4o-mini) (gpt-4o/DeepSeek-R1) (CodeS, DAIL-SQL, etc.)

- Critical path: Question → keyword extraction → sample SQL execution → evidence generation → pass evidence to downstream text-to-SQL model

- Design tradeoffs:
  - **SEED-gpt vs. SEED-deepseek**: Full schema preserves information but requires 128K+ context; summarized schema fits 8K limit but may lose relevant tables
  - **Join information inclusion**: SEED-deepseek adds join guidance not in BIRD evidence format; helps CodeS but slightly hurts CHESS (optimized for BIRD format)
  - **Assumption**: Evidence format tuning per downstream model may yield additional gains (Table VII shows format matters)

- Failure signatures:
  - **Negative improvement** (e.g., CHESS IR+CG+UT with SEED-deepseek: -0.58% EX): Evidence format mismatch with model's expected structure
  - **Over-specified evidence**: Including join information when model already handles linking
  - **Sample query failure**: No results returned when value doesn't exist or column mismatches

- First 3 experiments:
  1. Run SEED on BIRD dev set with a simple text-to-SQL model (DAIL-SQL or CodeS) comparing: no evidence, BIRD evidence, SEED evidence. Verify the ~17% EX gap closes.
  2. Ablate sample SQL execution: generate evidence without sample results vs. with sample results on 50 questions containing ambiguous value references. Measure EX delta.
  3. Test format sensitivity: take SEED-deepseek evidence, strip join-related lines, run on CHESS. Verify improvement per Table VII pattern.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automatic evidence formats be optimized or personalized to align with the specific prompt engineering and internal reasoning processes of different text-to-SQL model architectures?
- Basis in paper: [explicit] The authors note in Section E.2 that while CodeS benefited from SEED's additional join information, CHESS performed worse, leading to the conclusion: "These findings highlight the need for future research on optimizing evidence formats based on how models utilize evidence."
- Why unresolved: The study shows a "one-size-fits-all" evidence format can negatively impact agent-based models that are prompt-optimized for human-generated evidence styles (e.g., CHESS), whereas simpler models benefit from the extra context.
- What evidence would resolve it: A systematic study measuring performance when evidence content (e.g., inclusion of join logic) is dynamically modulated based on the target model's known prompt preferences or architecture.

### Open Question 2
- Question: To what extent can systems like SEED autonomously generate "numeric reasoning knowledge" (e.g., complex mathematical formulas) purely from schema and value analysis without relying on external knowledge bases?
- Basis in paper: [inferred] In Section II.A, the authors categorize BIRD evidence into four types and state that three (domain, synonym, value) can be derived from the database, implicitly excluding "Numeric reasoning knowledge" (requiring expertise) from the core derivation logic described.
- Why unresolved: The paper demonstrates strong results in domain and synonym extraction, but remains vague on whether automatic value analysis is sufficient to replace the expert-provided mathematical logic required for complex calculations in the BIRD dataset.
- What evidence would resolve it: An ablation study isolating performance on questions requiring "numeric reasoning knowledge" (as defined by BIRD) to compare SEED's generated evidence against human gold standards specifically for mathematical queries.

### Open Question 3
- Question: How robust is the SEED pipeline when applied to databases that completely lack description files or contain only raw, noisy metadata?
- Basis in paper: [inferred] The methodology in Section III relies on "description files" as a key input source, and the Spider experiment (Section IV.E.3) required generating synthetic description files using DeepSeek-V3 because they were missing.
- Why unresolved: The reliance on description files is a stated dependency; if these files are missing in real-world scenarios, the system currently relies on another LLM to hallucinate or synthesize them, which introduces a potential point of failure not thoroughly evaluated for accuracy.
- What evidence would resolve it: Experiments on a modified version of the BIRD dataset where description files are progressively degraded or removed, measuring the resulting drop in evidence quality and SQL accuracy.

## Limitations
- Unknown exact prompt templates for evidence generation, making faithful reproduction challenging
- Unclear keyword extraction method (LLM-based, regex, or NER approach not specified)
- Sample SQL query generation logic from extracted keyword pairs not detailed
- Performance varies significantly depending on target model's expected evidence format
- Reliance on description files as key input source creates potential failure point

## Confidence

- High confidence: The core contribution of SEED in automatically generating evidence from database schema, descriptions, and sampled values is well-supported by the experimental results showing significant improvements on BIRD and Spider datasets.
- Medium confidence: The mechanisms for schema summarization (DeepSeek-R1) and sample SQL execution are described but lack implementation details that would enable precise reproduction.
- Medium confidence: The claim that SEED outperforms human-generated evidence in some cases is supported by the data but depends heavily on the specific baseline models and evidence formats used.

## Next Checks
1. Ablation study of sample SQL execution: Generate evidence with and without sample results on 50 questions containing ambiguous value references and measure the exact match difference.
2. Format sensitivity test: Strip join-related lines from SEED-deepseek evidence and run on CHESS to verify the pattern observed in Table VII where format mismatches affect different models differently.
3. Novel domain evaluation: Test SEED on a database with no training analogs to assess how well the embedding-based few-shot retrieval performs when similar questions are unavailable.