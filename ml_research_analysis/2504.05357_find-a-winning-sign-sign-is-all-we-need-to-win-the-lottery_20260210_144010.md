---
ver: rpa2
title: 'Find A Winning Sign: Sign Is All We Need to Win the Lottery'
arxiv_id: '2504.05357'
source_url: https://arxiv.org/abs/2504.05357
tags:
- network
- init
- parameters
- initialized
- subnetwork
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of finding sparse subnetworks
  (winning tickets) in neural networks that can match the performance of their dense
  counterparts when trained from scratch. While iterative pruning methods like IMP
  and LRR have been successful in small-scale settings, they struggle to generalize
  to larger architectures and datasets.
---

# Find A Winning Sign: Sign Is All We Need to Win the Lottery

## Quick Facts
- arXiv ID: 2504.05357
- Source URL: https://arxiv.org/abs/2504.05357
- Reference count: 12
- Primary result: AWS (A Winning Sign) improves sparse subnetwork training by randomly interpolating normalization parameters during pruning, enabling transfer of generalization potential via signed masks

## Executive Summary
This paper addresses the challenge of finding sparse subnetworks (winning tickets) in neural networks that can match the performance of their dense counterparts when trained from scratch. While iterative pruning methods like IMP and LRR have been successful in small-scale settings, they struggle to generalize to larger architectures and datasets. The key insight is that parameter sign information plays a crucial role in preserving generalization potential, and the original methods fail because they neglect the adverse effects of initializing normalization layer parameters. To overcome this, the authors propose AWS (A Winning Sign), a variant of LRR that randomly interpolates normalization parameters with their initial values during training, preventing high error barriers between the sparse subnetwork and its initialized counterpart.

## Method Summary
AWS is a variant of Learning Rate Rewinding (LRR) that addresses the normalization parameter dependency problem. During each training iteration, AWS samples α ~ U(0,1) and interpolates normalization layer parameters between their current and initial values: ψ_forward = α·ψ + (1-α)·ψ_init. This trains the network to perform well across the entire interpolation path. After T iterations of iterative magnitude pruning, the signed mask (sparsity mask + sign information) is extracted from the final subnetwork. This signed mask can then be applied to any randomly initialized network using abs(θ_init) ⊙ s, transferring the generalization potential of the sparse subnetwork.

## Key Results
- AWS achieves performance comparable to the dense network at non-trivial sparsity levels (up to ~90%) on CIFAR-100, Tiny-ImageNet, and ImageNet
- The signed mask transfer method enables any randomly initialized network to converge to solutions comparable to the dense network
- AWS outperforms standard LRR by addressing the normalization parameter dependency problem through interpolation
- Linear Mode Connectivity diagnostics confirm basin of attraction preservation for AWS solutions

## Why This Works (Mechanism)

### Mechanism 1
Parameter sign configurations encode transferable generalization potential that can be applied to any random initialization when combined with appropriate sparsity masks. LRR learns and preserves effective sign configurations during iterative pruning, unlike IMP which loses sign information during parameter rewinding. When the signed mask is transferred to a randomly initialized network, the resulting network abs(θ_init) ⊙ s can converge to solutions comparable to the dense network, provided normalization layer complications are addressed.

### Mechanism 2
Preserving basin of attraction membership via linear mode connectivity enables generalization potential transfer. The subnetwork maintains its basin of attraction if it exhibits low error barriers along linear interpolation paths to related configurations. When signs are preserved but magnitudes randomized, SGD-noise stability and LMC with the original solution determine whether the network converges back to the same basin.

### Mechanism 3
Random interpolation of normalization layer parameters during training eliminates the dependency on trained normalization parameters for transfer. AWS samples α ~ U(0,1) during each forward pass and computes the interpolated value (ψ_t, ψ_init)_α = α·ψ_t + (1-α)·ψ_init. This trains the network to perform well across the entire interpolation path between trained and initialized normalization parameters, preventing high error barriers that would otherwise break basin membership.

## Foundational Learning

- **Lottery Ticket Hypothesis (LTH)**: The entire paper frames AWS as progress toward LTH's goal—finding sparse subnetworks at initialization that match dense network performance. Without understanding LTH, the motivation for transferring signed masks to random initializations is unclear.
- **Linear Mode Connectivity (LMC)**: The paper uses LMC as the primary diagnostic tool to verify basin of attraction preservation. Understanding error barriers along interpolation paths is essential for interpreting Figures 2 and 3.
- **Learning Rate Rewinding (LRR) vs Iterative Magnitude Pruning (IMP)**: AWS is a variant of LRR; understanding why LRR preserves sign configurations while IMP loses them explains the baseline comparison and why AWS builds on LRR specifically.

## Architecture Onboarding

- **Component map**: Pruning backbone -> Normalization interpolation module -> Signed mask extractor -> Transfer protocol
- **Critical path**: 1. Warm-up training (10-20 epochs) → 2. Iterative AWS training with normalization interpolation → 3. Extract signed mask → 4. Apply to fresh random initialization → 5. Standard training to convergence
- **Design tradeoffs**: Negligible interpolation overhead; reduced training epochs vs standard IMP; effectiveness up to ~90% sparsity on CIFAR-100; works with BatchNorm and LayerNorm
- **Failure signatures**: Random initialization + signed mask underperforms LRR solution (check normalization randomization); high SGD-noise instability after transfer (verify AWS interpolation was active); signed mask transfer fails on VGG-style networks
- **First 3 experiments**: 1. Reproduce motivational experiment (Figure 2) to verify normalization parameter dependency; 2. AWS vs LRR transfer comparison to measure test accuracy gap; 3. LMC diagnostic to verify basin preservation claims

## Open Questions the Paper Calls Out
- Can the AWS-derived signed mask transfer generalization potential across different tasks in a transfer learning setting?
- Does the "sign is all we need" hypothesis hold for Transformer-based architectures?
- What is the theoretical mechanism by which random linear interpolation of normalization parameters prevents error barriers?

## Limitations
- The exact mechanism by which signs encode functional structure remains underspecified
- The assumption that linear interpolation is sufficient for complex normalization layer behaviors may not hold for all architectures
- The paper does not explore fundamental limits of when signed mask transfer breaks down at extreme sparsity levels

## Confidence
- **High Confidence**: AWS improves upon LRR by addressing normalization parameter dependency; empirical results on CIFAR-100 and Tiny-ImageNet are reproducible
- **Medium Confidence**: Sign information is crucial for transfer; AWS works across multiple architectures; sparsity limits exist but are not fully characterized
- **Low Confidence**: Theoretical explanation for why signs encode generalization potential; mechanism by which interpolation preserves basin membership; performance guarantees at extreme sparsity levels

## Next Checks
1. **Ablation Study on Interpolation Range**: Systematically vary the interpolation range (e.g., α ~ U(0.5, 1.0) vs U(0,1)) to determine the minimum interpolation needed for basin preservation and identify the sensitivity of AWS to this hyperparameter.

2. **Error Barrier Analysis at Different Sparsities**: For AWS-transferred networks at multiple sparsity levels (50%, 70%, 90%, 95%), compute and compare error barriers along linear interpolation paths to their source solutions to quantify how basin preservation degrades with sparsity.

3. **Sign vs Magnitude Contribution**: Design an experiment that separately tests the importance of sign preservation vs magnitude patterns by comparing transfer performance when using (1) signed masks (AWS), (2) magnitude masks with fixed signs, and (3) random sign assignment with magnitude preservation.