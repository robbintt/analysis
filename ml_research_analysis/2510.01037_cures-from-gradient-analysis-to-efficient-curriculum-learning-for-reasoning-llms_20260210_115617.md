---
ver: rpa2
title: 'CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning
  LLMs'
arxiv_id: '2510.01037'
source_url: https://arxiv.org/abs/2510.01037
tags:
- training
- cures
- gradient
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of reinforcement learning
  with verifiable rewards (RLVR) in training large language models (LLMs) for reasoning
  tasks, where existing methods treat all training prompts equally and waste computational
  resources on prompts of varying difficulty. The authors propose CurES, a curriculum
  learning method that leverages gradient analysis to dynamically allocate sampling
  probabilities and rollout quantities across prompts based on their estimated difficulty.
---

# CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs

## Quick Facts
- arXiv ID: 2510.01037
- Source URL: https://arxiv.org/abs/2510.01037
- Authors: Yongcheng Zeng; Zexu Sun; Bokai Ji; Erxue Min; Hengyi Cai; Shuaiqiang Wang; Dawei Yin; Haifeng Zhang; Xu Chen; Jun Wang
- Reference count: 40
- Primary result: +3.30 and +4.82 accuracy points on 1.5B and 7B models, converging 1.75×-4.3× faster than GRPO and GVM

## Executive Summary
CurES addresses the inefficiency of reinforcement learning with verifiable rewards (RLVR) by proposing a curriculum learning method that dynamically allocates training resources based on prompt difficulty. The method uses gradient analysis to estimate the optimal sampling distribution and rollout allocation, achieving significantly faster convergence and higher accuracy on mathematical reasoning tasks compared to strong baselines like GRPO and GVM.

## Method Summary
CurES implements a curriculum learning framework for RLVR that models prompt difficulty using Beta distributions and dynamically adjusts sampling probabilities and rollout quantities. The method maintains Beta distribution parameters (α, β) for each prompt to estimate difficulty, computes optimal sampling distributions based on gradient efficiency, and allocates rollouts to minimize gradient variance. The approach integrates with standard RL optimizers like GRPO or REINFORCE++ and uses a cold-start mechanism with initial rollouts to seed difficulty estimates.

## Key Results
- Achieves +3.30 accuracy points on Qwen2.5-Math-1.5B compared to GRPO
- Achieves +4.82 accuracy points on Qwen2.5-Math-7B compared to GVM
- Converges 1.75× to 4.3× faster across multiple benchmarks (MATH500, AIME, AMC, GSM8K)
- Outperforms GRPO and GVM on all tested mathematical reasoning datasets

## Why This Works (Mechanism)

### Mechanism 1: Accuracy-Guided Gradient Optimization
- **Claim:** Prioritizing prompts with intermediate estimated accuracy maximizes per-step optimization efficiency within a trust region.
- **Mechanism:** The authors derive that the theoretical upper bound of the loss function update is proportional to $\sqrt{p_{\theta_{old}}(x)(1 - p_{\theta_{old}}(x))}$, where $p$ is the model's accuracy on a prompt. This term is maximized when accuracy is near 0.5 (intermediate difficulty). CurES samples prompts proportional to this gradient efficiency potential rather than uniformly.
- **Core assumption:** The model's current success rate on a prompt is a reliable proxy for the prompt's "training utility" (gradient magnitude) at that specific training step.
- **Evidence anchors:** [Section 4.1] Equation 11 establishes the bound $|L| \le \sqrt{2\delta p(1-p)}$. [Section 4.1] Equation 14 derives the optimal sampling distribution $\rho^*$ based on this efficiency term.
- **Break condition:** If the reward signal is sparse or the model plateaus (accuracy $\approx 1.0$ for most prompts), the gradient magnitude becomes negligible, and the curriculum provides no signal.

### Mechanism 2: Variance-Adjusted Rollout Allocation
- **Claim:** Dynamically allocating the number of rollouts per prompt based on gradient variance reduces gradient estimator noise and stabilizes updates.
- **Mechanism:** Instead of a fixed number of rollouts (e.g., always 8), CurES minimizes the trace of the gradient estimator variance. It assigns more rollouts to prompts where the gradient variance is high (often intermediate difficulty) to get a stable estimate, and fewer to low-variance prompts.
- **Core assumption:** The computational cost of generating additional rollouts is justified by the reduction in gradient variance; specifically, that high-variance prompts are not outliers to be discarded, but signals to be better measured.
- **Evidence anchors:** [Section 4.2] Equation 20 derives the optimal rollout quantity $n_i \propto \sigma_i$, where $\sigma_i$ is the standard deviation of the gradient contribution. [Figure 4] Shows the "bell-shaped" allocation curve where intermediate accuracy prompts receive the highest rollout quantities.
- **Break condition:** If the budget constraint is too tight, the algorithm may allocate 0 rollouts to easy/hard prompts, potentially missing out on "easy wins" or out-of-distribution generalization.

### Mechanism 3: Bayesian Difficulty Tracking
- **Claim:** Modeling prompt accuracy as a Beta distribution allows for robust, low-overhead estimation of prompt difficulty using historical sampling data.
- **Mechanism:** Rather than re-evaluating prompt difficulty from scratch every iteration (expensive), CurES updates a Beta distribution prior ($\alpha, \beta$ counts of correct/incorrect answers) as rollouts occur. This posterior estimation refines the difficulty score used for Mechanisms 1 & 2.
- **Core assumption:** The "difficulty" of a prompt changes slowly enough relative to the update frequency that a simple accumulating count does not suffer catastrophic lag due to distribution shift.
- **Evidence anchors:** [Section 4.3] Equation 22-24 details the Beta-Binomial updating process. [Algorithm 1] Lines 5-6 and 16-18 show the incremental update of counts $\alpha_t, \beta_t$.
- **Break condition:** Under rapid model capability shifts (catastrophic forgetting or rapid overfitting), the historical counts $\alpha, \beta$ may become stale, misrepresenting current difficulty.

## Foundational Learning

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - **Why needed here:** CurES operates specifically within the RLVR paradigm (common for math/code) where rewards are binary (correct/incorrect). Understanding that the reward is sparse and deterministic is key to understanding why variance minimization is critical.
  - **Quick check question:** Does the reward function rely on a separate reward model (human preference) or a deterministic check?

- **Concept: Natural Gradient & Fisher Information Matrix (FIM)**
  - **Why needed here:** The theoretical justification for CurES relies on optimizing within a "trust region" defined by KL divergence. The authors use the FIM to transform the parameter space, which leads to the $\sqrt{p(1-p)}$ efficiency term.
  - **Quick check question:** Why does standard gradient descent fail to capture the "distance" between policy distributions effectively?

- **Concept: Curriculum Learning (Self-Paced Learning)**
  - **Why needed here:** CurES is a form of curriculum learning. While traditional methods might use hand-crafted stages, CurES automates the "pacing" by treating accuracy as the difficulty metric.
  - **Quick check question:** How does CurES define "difficulty" differently than a static word-length or syntax-complexity metric?

## Architecture Onboarding

- **Component map:** Estimator -> Scheduler -> Rollout Engine -> Verifier -> Updater
- **Critical path:**
  1. **Initialization:** Cold-start with small pre-rollout ($N'=4$) to seed $\alpha, \beta$ counts.
  2. **Scheduling:** Calculate $n_i$ (rollouts) and $\rho_i$ (sampling weights).
  3. **Collection:** Sample batch, run rollouts, update Beta counts immediately with new success/failure data.
  4. **Optimization:** Perform policy update.

- **Design tradeoffs:**
  - **Overhead vs. Freshness:** The paper resets estimation every $T$ iterations to handle distribution shift. Setting $T$ too long risks stale difficulty estimates; setting it too short wastes compute on re-estimation.
  - **Memory:** Requires storing two floats ($\alpha, \beta$) per prompt in the dataset, which is negligible but strictly more state than standard GRPO.

- **Failure signatures:**
  - **Curriculum Collapse:** If the model overfits, the accuracy distribution becomes bimodal (all 0s or 1s), driving $\sqrt{p(1-p)} \to 0$. The optimizer may stall as it struggles to find "informative" prompts.
  - **High Variance in Estimates:** With low pre-rollout ($N'$), the Beta distribution has high variance, potentially causing erratic sampling probabilities that destabilize training.

- **First 3 experiments:**
  1. **Ablation of Allocation:** Run CurES with fixed sampling but dynamic rollout allocation vs. dynamic sampling but fixed rollouts to isolate the contribution of each component.
  2. **Sensitivity to $\tau$:** Sweep the temperature parameter $\tau$ in Equation 14 to verify how sensitive the convergence speed is to the "sharpness" of the curriculum.
  3. **Cold-Start Analysis:** Vary the initial pre-rollout count $N'$ (e.g., 1, 4, 8) to determine the minimal overhead required to establish a stable difficulty distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the prompt difficulty estimation mechanism be adapted to avoid the "reset" between iterations without suffering from severe distribution shift?
- **Basis in paper:** [inferred] Section 4.3 states that difficulty estimations are reset when a new iteration begins to mitigate distribution shift, potentially discarding useful historical data.
- **Why unresolved:** The current reset heuristic ensures stability but may waste accumulated posterior knowledge as the model evolves.
- **What evidence would resolve it:** A comparative analysis of a continuous Bayesian update strategy versus the reset strategy on convergence stability and speed.

### Open Question 2
- **Question:** Does the relationship between prompt difficulty and gradient efficiency transfer to non-mathematical reasoning domains like code generation?
- **Basis in paper:** [inferred] While the introduction mentions code generation as a suitable RLVR domain, Section 5 experiments are limited strictly to mathematical reasoning benchmarks.
- **Why unresolved:** The binary reward structure and gradient variance properties may differ in syntax-constrained tasks like coding compared to mathematical derivation.
- **What evidence would resolve it:** Applying CurES to code generation benchmarks (e.g., HumanEval) and comparing convergence speed against GRPO baselines.

### Open Question 3
- **Question:** Does the theoretical bound linking convergence to prompt difficulty hold for models significantly larger than 7B parameters?
- **Basis in paper:** [inferred] The paper validates the method only on 1.5B and 7B parameter models (Section 5), leaving the scaling behavior for frontier-scale models unexplored.
- **Why unresolved:** Gradient dynamics and the signal-to-noise ratio in difficulty estimation may change non-linearly with increased model capacity.
- **What evidence would resolve it:** Scaling experiments on 70B+ models demonstrating similar relative performance gains over GRPO.

## Limitations
- The theoretical derivations rely on assumptions about gradient variance that may not generalize to all reasoning tasks
- Beta distribution approximation for prompt difficulty assumes stationary difficulty, which may break down during rapid learning phases
- Computational overhead of variance estimation for rollout allocation is acknowledged but not fully quantified against wall-clock time gains

## Confidence
- **High Confidence:** The empirical performance improvements (+3.30 and +4.82 accuracy points, 1.75×-4.3× faster convergence) are well-documented with multiple baselines and benchmarks
- **Medium Confidence:** The theoretical justification via natural gradient analysis is sound, but the practical approximation of gradient variance terms may introduce implementation-dependent variance
- **Medium Confidence:** The Beta distribution modeling is efficient but assumes difficulty stability; the reset mechanism (every T iterations) is a heuristic fix whose optimal value is not explored

## Next Checks
1. **Variance Estimation Overhead:** Profile the actual computational cost of calculating Eq. 21 versus using a fixed rollout count, particularly for the 7B model where gradients are expensive
2. **Curriculum Collapse Stress Test:** Design an experiment where the model rapidly plateaus (e.g., by reducing training steps) to verify the method fails gracefully when accuracy distributions become bimodal
3. **Distribution Shift Sensitivity:** Vary the reset interval T (e.g., 5, 10, 20 iterations) to measure the tradeoff between stale estimates and re-computation overhead, ensuring the hyperparameter is not tuned specifically to the Numina-Math dataset