---
ver: rpa2
title: Optimizing Retrieval Augmented Generation for Object Constraint Language
arxiv_id: '2505.13129'
source_url: https://arxiv.org/abs/2505.13129
tags:
- retrieval
- generation
- language
- performance
- chunks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates optimizing retrieval-augmented generation\
  \ for automating Object Constraint Language (OCL) rule generation in Model-Based\
  \ Systems Engineering. The research evaluates three retrieval approaches\u2014BM25\
  \ (lexical), BERT-based (semantic dense), and SPLADE (semantic sparse)\u2014to determine\
  \ their impact on generation accuracy when integrated with a large language model."
---

# Optimizing Retrieval Augmented Generation for Object Constraint Language

## Quick Facts
- arXiv ID: 2505.13129
- Source URL: https://arxiv.org/abs/2505.13129
- Authors: Kevin Chenhao Li; Vahid Zolfaghari; Nenad Petrovic; Fengjunjie Pan; Alois Knoll
- Reference count: 23
- Key outcome: Semantic retrieval (BERT/SPLADE) significantly outperforms lexical retrieval (BM25) for OCL generation, with SPLADE excelling at lower k values but degrading with excessive retrieval due to noise.

## Executive Summary
This study investigates retrieval-augmented generation (RAG) for automating Object Constraint Language (OCL) rule generation from natural language specifications in Model-Based Systems Engineering. The research evaluates three retrieval approaches—BM25 (lexical), BERT-based (semantic dense), and SPLADE (semantic sparse)—to determine their impact on generation accuracy when integrated with a large language model. Results show that while retrieval can improve generation quality, performance is highly dependent on the retrieval method and number of retrieved chunks (k). BM25 underperforms the baseline, whereas BERT and SPLADE achieve better results, with SPLADE excelling at lower k values. However, excessive retrieval (high k) degrades performance by introducing irrelevant context. The study highlights the importance of optimizing retrieval configurations to balance context relevance and output consistency.

## Method Summary
The study preprocesses PlantUML meta-models into atomic chunks (classes, enums, associations) and uses three retrieval methods to fetch relevant context based on natural language specifications. These retrieved chunks are then injected into prompts for Llama-3-8B-Instruct to generate OCL constraints. The system evaluates performance using BERT-based cosine similarity and Euclidean distance metrics. The dataset consists of 72 "hard samples" from the text-to-ocl-from-ecore dataset, filtered for meta-models with more than 50 chunks. Retrieval performance is tested across k values of 10, 20, 30, 40, and 50.

## Key Results
- Semantic retrieval methods (BERT and SPLADE) significantly outperform lexical retrieval (BM25) for OCL generation tasks
- SPLADE achieves the highest peak performance at k=10 but degrades rapidly as k increases due to noise
- BERT retrieval provides more stable performance across different k values compared to SPLADE
- Excessive retrieval (k>30) degrades performance for all methods by introducing irrelevant context

## Why This Works (Mechanism)

### Mechanism 1
BM25 relies on exact term overlap, which fails when natural language specifications use different terminology than meta-model attributes. Semantic retrievers like SPLADE and BERT bridge this gap by generating vector representations that capture semantic equivalence, allowing retrieval of relevant classes and associations even without keyword matches.

### Mechanism 2
SPLADE's sparse expansion allows it to find highly relevant chunks immediately at low k values by expanding queries to include synonyms and related concepts. However, this same expansion mechanism becomes a liability at higher k values, where it retrieves semantically similar but contextually irrelevant "distractor" chunks that introduce noise into the LLM's context window.

### Mechanism 3
RAG mitigates LLM context window limitations for MBSE by dynamically injecting only the relevant subset of the meta-model. Instead of processing entire large meta-models, the system retrieves specific classes and associations needed for each constraint, focusing the LLM's attention and preventing context dilution.

## Foundational Learning

- **Concept: Object Constraint Language (OCL) & Meta-models**
  - Why needed here: OCL rules are tightly bound to specific meta-model structures; you cannot generate valid OCL without knowing the types involved
  - Quick check question: If a user asks for a constraint on "Vehicle" but the meta-model only defines "Car", will lexical retrieval find the context? (Answer: Likely not, but semantic retrieval might)

- **Concept: Sparse vs. Dense Retrieval**
  - Why needed here: The paper's core findings hinge on the performance gap between BM25 (lexical), BERT (dense), and SPLADE (sparse)
  - Quick check question: Which method expands the query vocabulary to include synonyms, potentially retrieving more relevant results at the cost of potential noise?

- **Concept: Chunking Granularity**
  - Why needed here: The system splits PlantUML descriptions into atomic units to balance context limits and noise reduction
  - Quick check question: Why chunk by "class" or "enum" rather than keeping the entire PlantUML file as one document? (Answer: To fit within context limits and reduce noise)

## Architecture Onboarding

- **Component map:** Input Interface -> Pre-processor -> Index -> Retriever -> Generator
- **Critical path:** The Retriever is the bottleneck; if it fails to fetch the specific class defining the constrained attribute, the Generator will hallucinate or fail regardless of LLM capability
- **Design tradeoffs:** Choose SPLADE for precision in data-sparse scenarios (low k) if latency allows; choose BERT for stability when needing larger context windows (high k) to avoid noise
- **Failure signatures:** BM25 returns empty/irrelevant results due to lexical mismatch; high k values cause noise hallucination where model references correct classes but incorrect attributes; strict prompt formatting prevents syntax drift
- **First 3 experiments:** 1) Implement BM25 retrieval to verify it underperforms baseline; 2) Run SPLADE retrieval sweeping k from 5-50 and plot similarity vs k to locate performance cliff; 3) Compare Variance of Cosine Similarity for BERT vs SPLADE to confirm BERT's lower variance and higher stability

## Open Questions the Paper Calls Out

- **Open Question 1:** Do optimized retrieval strategies improve the formal syntactic validity and semantic correctness of OCL rules, or merely their textual similarity to reference solutions?
  - Basis: Current metrics don't capture functional correctness; future work should include OCL parser validation and expert review
  - Evidence needed: OCL parser checks for syntactic validity and manual expert review for semantic correctness

- **Open Question 2:** Can hybrid retrieval approaches or multi-stage retrieval pipelines outperform single-method approaches like SPLADE?
  - Basis: Paper notes further research needed on hybrid approaches like multi-stage retrieval
  - Evidence needed: Experimental results comparing single-stage retrievers against hybrid retrievers (e.g., BM25 + SPLADE re-ranking)

- **Open Question 3:** Does refining the chunking strategy to group semantically connected elements improve generation performance?
  - Basis: Authors identify refining chunking strategy as underexplored, suggesting grouping semantically connected chunks
  - Evidence needed: Comparative study evaluating atomic chunking against semantic or relationship-aware chunking strategies

## Limitations

- Unspecified implementation details block faithful reproduction: exact BERT model for retrieval and evaluation, specific SPLADE variant, BM25 hyperparameters, and random seed for sampling
- Study focuses exclusively on "hard samples" (>50 chunks) and small meta-models, leaving scalability to larger enterprise systems untested
- Uses greedy decoding (do_sample=false) which may underrepresent LLM's full capability compared to sampling approaches

## Confidence

- **High Confidence:** Semantic retrieval outperforms lexical retrieval; performance degradation at high k values for SPLADE
- **Medium Confidence:** Specific peak performance values and absolute performance rankings
- **Low Confidence:** SPLADE's noise sensitivity mechanism and superiority claims over graph-based PathOCL

## Next Checks

1. **Model Variant Sensitivity Test:** Re-run experiments with different BERT variants and SPLADE checkpoints to determine sensitivity to specific model choices

2. **Noise Characterization Analysis:** Systematically measure relevance of retrieved chunks at increasing k values through manual annotation to quantify SPLADE's noise sensitivity

3. **Hard vs. Easy Sample Comparison:** Replicate experiment on full dataset including "easy samples" (<50 chunks) to validate performance differences across complete problem space