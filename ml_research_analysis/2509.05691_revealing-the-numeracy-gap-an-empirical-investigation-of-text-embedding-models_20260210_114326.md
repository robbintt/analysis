---
ver: rpa2
title: 'Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models'
arxiv_id: '2509.05691'
source_url: https://arxiv.org/abs/2509.05691
tags:
- embedding
- numbers
- text
- numerical
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether modern text embedding models can accurately
  encode nuanced numerical information, such as subtle differences between values
  (e.g., 2% vs. 20%).
---

# Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models

## Quick Facts
- arXiv ID: 2509.05691
- Source URL: https://arxiv.org/abs/2509.05691
- Reference count: 21
- Primary result: Modern text embedding models achieve only 54% accuracy on numerical comparison tasks, barely above random guessing.

## Executive Summary
This paper reveals a critical limitation in modern text embedding models: their inability to accurately encode and compare numerical information. Through a carefully designed synthetic financial dataset, the authors demonstrate that widely used embedding models struggle to distinguish between subtle numerical differences (e.g., 2% vs. 20%). Despite variations in architecture and training data, no model excels at numerical reasoning, with LLM-based models showing only marginal improvements over encoder-based alternatives. The findings suggest that current embedding practices, optimized for semantic similarity, are insufficient for applications requiring precise numerical understanding in domains like finance and healthcare.

## Method Summary
The study evaluates 13 embedding models using a synthetic financial dataset (EmbedNum-1K) containing 1,000 question-answer pairs where correct and incorrect answers differ only in numeric values. Models are tested on their ability to rank correct answers closer to questions than incorrect ones using cosine similarity. The dataset includes 18 variants testing different numeric formats (integers, decimals, percentages, written form) and number ranges (1-999). No model training occurs—the evaluation measures existing embedding quality. Questions are filtered from the BULL dataset and augmented using DeepSeek-V3.1 to generate controlled A+/A- pairs with single numeric differences.

## Key Results
- Overall accuracy across 13 models is only 54%, barely above random guessing (50%)
- LLM-based embedders (e.g., Qwen3-Embedding-8B) outperform encoder-based models by approximately 5 percentage points
- Models show significant sensitivity to numeric formats, with decimals and high-precision numbers causing greater performance drops
- Adding context to questions reduces accuracy by 5-10% as semantic features mask numerical signals
- No model demonstrates superior numeracy despite varying amounts of numerical data in training corpora

## Why This Works (Mechanism)

### Mechanism 1: Semantic Saturation and Signal Masking
Standard embedding training optimizes for semantic similarity, causing rich linguistic context to overwhelm fine-grained numerical differences in the vector space. When encoding sentences like "Stock A rose by 2%" vs. "Stock A rose by 20%", dense semantic features dominate the fixed-dimensional vector, resulting in nearly identical embeddings for numerically distinct sentences. The numerical token contributes a signal too weak relative to semantic noise. This mechanism breaks when models are trained with explicit contrastive loss on numerically distinct pairs or when context is stripped, improving accuracy.

### Mechanism 2: Magnitude Disruption via Token Fragmentation
Numeracy degrades when numbers are tokenized into multiple sub-word units (OOV), disrupting the model's ability to perceive magnitude. Numbers are often split into multiple tokens (e.g., "1,800" → "1", ",", "800"), and standard transformers lack specific induction heads for arithmetic aggregation, treating fragments as independent semantic units rather than components of a unified value. This mechanism breaks if tokenization preserves numbers as single tokens or if models are explicitly trained to attend to digit position.

### Mechanism 3: Scale-Dependent Feature Transfer
LLM-based embedders outperform encoder-only models because massive pre-training creates a more robust "superposition" of features where numerical signals are less likely to be annihilated by semantic layers. Large models possess higher dimensional hidden states and have seen more arithmetic data during pre-training, retaining residual streams where numerical magnitude is somewhat preserved. This advantage is a side-effect of general capability scaling rather than targeted numerical reasoning, and disappears if fine-tuning catastrophically forgets pre-training numeracy.

## Foundational Learning

**Concept:** Cosine Similarity in Vector Space
- Why needed: The entire evaluation relies on the geometric assumption that "correct" answers reside closer to the query in vector space than incorrect ones.
- Quick check: If vector A is `[1, 0]` and vector B is `[0.9, 0.1]`, are they semantically similar? (Yes, cosine similarity is high).

**Concept:** Out-of-Vocabulary (OOV) Tokenization
- Why needed: The paper identifies OOV as a primary failure mode—models do not natively "see" `1800` as a value but often as shards.
- Quick check: Why might a model struggle to compare `1,000` and `1000` if the tokenizer treats the comma as a break?

**Concept:** Semantic vs. Numerical Granularity
- Why needed: This is the core trade-off—standard NLP treats "revenue is $10M" and "revenue is $12M" as semantically equivalent, but this paper treats them as distinct.
- Quick check: In standard semantic search, should "Apple revenue 2020" match "Apple revenue 2021"? (Usually yes, but this paper requires No).

## Architecture Onboarding

**Component map:** Dataset -> Tokenizer -> Encoder (Transformer) -> Metric (Cosine Similarity)

**Critical path:** The failure signature is observed at the Similarity Calculation step, but the causal root is upstream in the Encoder's attention mechanism. Trace attention weights: does the model attend to number tokens, or primarily to linguistic context?

**Design tradeoffs:**
- Context vs. Precision: Adding linguistic context helps semantic grounding but appears to "drown out" the numerical signal in the embedding norm.
- Synthetic vs. Real Data: Using synthetic data allows controlled isolation of the numeric variable but may lack the noise of real financial text.

**Failure signatures:**
- Random Baseline: ~50% accuracy on "Greater-than" vs "Less-than" tasks
- Context Collapse: Similarity scores for pairs like `200` vs `220` jump to ~0.95 when wrapped in identical sentences
- OOV Drop-off: Performance degrades significantly for numbers with >3 significant figures or decimal places

**First 3 experiments:**
1. Context Ablation: Run same model on `Q: Which number is > 200? A: 220` vs. `Q: Which stock P/E > 200?` Quantify accuracy drop attributable to semantic "noise."
2. Tokenization Audit: Group test set numbers by token count (1 token vs. 3 tokens). Plot accuracy vs. token count to verify OOV hypothesis.
3. Format Sensitivity: Convert all numbers to written text ("twenty percent") vs. digit-symbol format ("20%"). Compare if linguistic grounding consistently boosts numeracy across different architectures.

## Open Questions the Paper Calls Out

**Open Question 1:** Through what specific mechanisms does the superior natural language understanding of LLM-based embedding models translate into better numerical encoding compared to encoder-based models? The study identifies the performance gap but only speculates it stems from pre-existing NLU capabilities without pinpointing architectural or attention mechanisms responsible.

**Open Question 2:** Do the findings regarding magnitude comparison generalize to complex numerical reasoning tasks such as arithmetic operations, ratios, or temporal comparisons? The current dataset is restricted to magnitude comparisons, leaving models' ability to synthesize or compute with numbers untested.

**Open Question 3:** How can the "granularity dilemma" be resolved to preserve fine-grained numerical signals without losing necessary semantic context? The study demonstrates that adding context reduces numerical distinctness, but a standard embedding vector must carry both semantic and numerical data simultaneously.

## Limitations
- The evaluation relies entirely on synthetic data rather than naturally occurring financial text, which may not reflect real-world numerical usage
- The study focuses exclusively on cosine similarity-based retrieval, not representing all use cases where numerical understanding matters
- Dataset construction depends on model-generated content and filtered SQL queries, potentially introducing formulation biases

## Confidence
- **High Confidence:** Embeddings struggle with numerical comparisons (accuracy near random) across 13 models and multiple formats
- **Medium Confidence:** LLM-based embedders outperform encoder-based ones due to pre-training advantages, though this may conflate general language capability with specific numerical reasoning
- **Medium Confidence:** Context masks numerical signals through attention dynamics, though exact mechanisms remain unmeasured
- **Low Confidence:** Tokenization fragmentation is the primary cause of numeracy gaps lacks direct causal evidence

## Next Checks
1. Test the same embedding models on naturally occurring financial documents containing numerical comparisons to determine if synthetic data results generalize to real-world scenarios.
2. Use attention visualization tools to empirically verify whether models attend differently to numeric tokens versus semantic context in high-accuracy versus low-accuracy samples.
3. Evaluate whether incorporating numerical features (e.g., through special token embeddings or numeric preprocessing) can close the performance gap, testing whether the limitation is fundamental to current architectures or addressable through simple modifications.