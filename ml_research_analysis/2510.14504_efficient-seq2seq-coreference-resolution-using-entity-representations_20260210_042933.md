---
ver: rpa2
title: Efficient Seq2seq Coreference Resolution Using Entity Representations
arxiv_id: '2510.14504'
source_url: https://arxiv.org/abs/2510.14504
tags:
- coreference
- incremental
- ontonotes
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an efficient incremental coreference resolution
  method using compressed entity representations. The method extracts and re-organizes
  entity-level tokens while discarding most other input tokens, enabling faster processing
  in incremental settings like dialogue.
---

# Efficient Seq2seq Coreference Resolution Using Entity Representations

## Quick Facts
- arXiv ID: 2510.14504
- Source URL: https://arxiv.org/abs/2510.14504
- Reference count: 21
- Key outcome: 0.6 F1 points below baseline while compressing input by 1.8x

## Executive Summary
This paper proposes an efficient incremental coreference resolution method that uses compressed entity representations instead of full document context. The approach extracts and re-organizes entity-level tokens while discarding most other input tokens, enabling faster processing in incremental settings like dialogue. On OntoNotes, the best model achieves 0.6 F1 points below a full-prefix incremental baseline while compressing input length by 1.8x. On LitBank, the model surpasses contemporary methods and matches state-of-the-art performance. The approach reduces GPU memory usage by 1.9x compared to non-incremental models.

## Method Summary
The method processes text incrementally, maintaining entity representations by extracting tokens corresponding to entity mentions and organizing them into a structured format. Unlike traditional approaches that process full document context, this method discards most non-entity tokens while preserving essential information needed for coreference resolution. The model is trained on these compressed representations, allowing it to make coreference decisions based on entity-centric context rather than full document history. This approach is particularly suited for incremental processing scenarios like dialogue where memory efficiency and speed are critical.

## Key Results
- Best model achieves 0.6 F1 points below full-prefix incremental baseline on OntoNotes
- Input compression rate of 1.8x compared to baseline
- 1.9x reduction in GPU memory usage compared to non-incremental models
- Surpasses contemporary methods on LitBank while matching state-of-the-art performance

## Why This Works (Mechanism)
The method works by leveraging the observation that coreference resolution primarily depends on entity mentions rather than full contextual information. By extracting and reorganizing only the tokens corresponding to entity mentions, the model creates a compressed representation that retains the essential information needed for coreference decisions while eliminating redundant or irrelevant content. This compression enables faster processing and reduced memory requirements without significantly compromising accuracy, as the coreference relationships are preserved in the entity-centric representation.

## Foundational Learning
- Incremental processing: Why needed - Enables real-time coreference resolution in streaming scenarios like dialogue; Quick check - Verify model can process tokens one-by-one without revisiting previous decisions
- Entity representation compression: Why needed - Reduces computational overhead while preserving essential coreference information; Quick check - Confirm compressed representations still contain all mention spans
- Sequence-to-sequence modeling: Why needed - Allows generation of coreference predictions from compressed entity representations; Quick check - Validate model can map compressed input to correct mention clusters

## Architecture Onboarding

Component map: Input tokens -> Entity extractor -> Compressed representation -> Seq2seq model -> Coreference predictions

Critical path: The core processing pipeline extracts entity mentions, compresses them into a structured representation, and feeds this to a sequence-to-sequence model that generates coreference clusters. The critical path is the entity extraction and compression step, as errors here propagate through the entire system.

Design tradeoffs: The primary tradeoff is between compression efficiency and information preservation. The method aggressively discards non-entity tokens to achieve speed and memory benefits, but this may lose contextual cues important for resolving certain types of coreference (e.g., when narrative context disambiguates pronoun references).

Failure signatures: The model particularly struggles with named entities and definite noun phrases in OntoNotes, likely due to the dataset's lack of singleton annotation. Errors also occur when narrative context is needed to disambiguate references (e.g., confusing "us" with named entities).

First experiments: 1) Test entity extraction accuracy on gold-standard mention spans; 2) Evaluate compression ratio vs. accuracy tradeoff across different entity densities; 3) Compare performance on documents with varying proportions of named entities vs. pronouns

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does the presence of singleton annotations intrinsically reduce the performance gap between incremental and non-incremental coreference models?
- Basis in paper: The authors hypothesize that the lack of singleton annotation in OntoNotes causes specific difficulties for incremental processing (Section 6.4), noting that this effect disappears on LitBank which includes such annotations.
- Why unresolved: While results differ between OntoNotes and LitBank, the datasets differ in domain (news vs. literature) and size, making it difficult to isolate singleton annotation as the sole cause of the performance disparity.
- What evidence would resolve it: A controlled ablation study on a dataset with gold singleton annotations (like LitBank), comparing model performance when training with and without the singleton labels included.

### Open Question 2
- Question: Can incorporating explicit speaker or dialogue role tags into the Entity-Centric representation mitigate errors in resolving first-person plural pronouns?
- Basis in paper: Appendix A describes a specific error where the model confuses "us" and "CNN" because the narrative context was discarded. The authors suggest "adding speaker tags... may offer a simple solution."
- Why unresolved: The current model relies solely on text spans for entity representation; it does not explicitly encode metadata about the speaker or narrative voice, leading to ambiguity when context is compressed.
- What evidence would resolve it: An experiment augmenting the Entity-Centric input representation with speaker IDs (e.g., from OntoNotes dialogue or broadcast news metadata) and measuring the change in recall for first and second-person pronouns.

### Open Question 3
- Question: How does the Entity-Centric compression strategy affect generalization to out-of-domain genres compared to full-prefix baselines?
- Basis in paper: The Limitations section notes that models trained on OntoNotes (7 genres) may not generalize to out-of-domain examples, and the paper observes that the method struggles with named entities in the OntoNotes test set.
- Why unresolved: The aggressive token discarding strategy creates a "memory" of the document state; it is unclear if this state is more brittle or more robust than full-context models when facing unseen vocabulary or discourse structures in new domains.
- What evidence would resolve it: A cross-domain evaluation (e.g., training on OntoNotes, testing on biomedical or scientific coreference datasets) comparing the performance drop of the Entity-Centric model against the Full-Prefix baseline.

## Limitations
- Performance gap of 0.6 F1 points compared to full-prefix incremental baseline
- Particular struggle with named entities and definite noun phrases in OntoNotes
- Uncertainty about generalizability across different discourse types and entity categories
- Limited validation across diverse text domains beyond OntoNotes and LitBank

## Confidence
- High: Efficiency improvements and computational benefits
- Medium: Performance comparisons between OntoNotes and LitBank datasets
- Low: Generalizability claims across different discourse types and entity categories

## Next Checks
1. Conduct ablation studies specifically isolating the impact of singleton exclusion in OntoNotes versus LitBank annotation schemes
2. Test the method on longer dialogue sequences and documents with varying entity density to verify the claimed 1.8x compression holds across diverse input types
3. Evaluate performance differences between named entities, pronouns, and common noun phrases to better understand which entity types the compression strategy handles least effectively