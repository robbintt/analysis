---
ver: rpa2
title: In-Context Learning of Temporal Point Processes with Foundation Inference Models
arxiv_id: '2509.24762'
source_url: https://arxiv.org/abs/2509.24762
tags:
- processes
- event
- intensity
- fim-pp
- sequences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIM-PP, the first foundation model for inference
  of marked temporal point processes (MTPPs). Instead of training separate models
  for each dataset, it pretrains a transformer to estimate conditional intensity functions
  in-context from sets of event sequences, using a synthetic corpus of Hawkes processes
  with diverse base intensities and interaction kernels.
---

# In-Context Learning of Temporal Point Processes with Foundation Inference Models

## Quick Facts
- **arXiv ID:** 2509.24762
- **Source URL:** https://arxiv.org/abs/2509.24762
- **Reference count:** 40
- **One-line primary result:** FIM-PP is the first foundation model for marked temporal point processes that achieves state-of-the-art results through in-context learning from synthetic pretraining.

## Executive Summary
This paper introduces FIM-PP, the first foundation model for inference of marked temporal point processes (MTPPs). Instead of training separate models for each dataset, it pretrains a transformer to estimate conditional intensity functions in-context from sets of event sequences, using a synthetic corpus of Hawkes processes with diverse base intensities and interaction kernels. Experiments show that zero-shot FIM-PP matches or exceeds specialized models on real-world datasets, and fine-tuning further improves performance, achieving state-of-the-art results for long-horizon prediction and strong accuracy on next-event prediction. The approach offers a unified, transferable framework for MTPP inference, reducing the need for dataset-specific training while maintaining competitive predictive performance.

## Method Summary
FIM-PP pretrains a transformer encoder-decoder architecture on a synthetic corpus of 72,000 Hawkes processes (14.4M events) with randomized base intensities, kernels, and interaction types. The model learns to infer conditional intensity functions from sets of context sequences and apply them to target histories through cross-attention. During inference, it processes a context of sequences from the training split and a target history from the test split, outputting intensity parameters for each potential event mark. The model can operate in zero-shot mode or be fine-tuned on specific datasets to improve performance.

## Key Results
- Zero-shot FIM-PP achieves competitive performance on real-world datasets (TAXI, TAOBAO, STACKOVERFLOW, AMAZON, RETWEET), matching or exceeding specialized models
- Fine-tuned FIM-PP consistently outperforms zero-shot version and achieves state-of-the-art results for long-horizon prediction
- The model successfully handles both next-event prediction and multi-event sequence generation through autoregressive simulation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-context learning via transformer attention enables FIM-PP to extract dynamical rules from a set of context sequences and apply them to a target history.
- Mechanism: The transformer encoder builds representations from multiple context sequences ($C$), which are then attended to by a decoder processing the target history ($H_t$). This cross-attention mechanism allows the model to infer intensity parameters ($\hat{\alpha}, \hat{\beta}, \hat{\mu}$) that are conditioned on the patterns observed across the entire context, effectively amortizing inference across heterogeneous processes.
- Core assumption: The underlying dynamics of the target sequence share statistical structure with the processes seen in the context.
- Evidence anchors:
  - [abstract] "pretrain a deep neural network to infer, in-context, the conditional intensity functions of event histories from a context defined by sets of event sequences."
  - [section 4.2] "A context of marked event sequences $S_j$ is encoded by a self-attentive transformer encoder. The result is further processed by a transformer decoder, using a history $H_t$ of marked events before time $t$ as queries."
  - [corpus] Corpus evidence on this specific mechanism is weak; related papers focus on disentangled dynamics or adversarial attacks, not the in-context foundation model architecture.
- Break condition: The mechanism may fail if the target sequence's dynamics are outside the distribution represented by the context (e.g., novel inhibitory patterns not present in the provided context).

### Mechanism 2
- Claim: Pretraining on a diverse synthetic Hawkes process prior induces a strong inductive bias, enabling zero-shot generalization to real-world data.
- Mechanism: By sampling from a broad distribution over base intensities ($\mu_k(t)$), interaction kernels ($\gamma_{\kappa\kappa'}(t-t')$), and influence types (excitatory, inhibitory, neutral), the model's weights are shaped to recognize a wide variety of temporal excitation and inhibition patterns. This acts as a universal functional prior for intensity estimation.
- Core assumption: Real-world event sequences can be well-approximated by (or lie within the span of) the synthetic Hawkes process family used for pretraining.
- Evidence anchors:
  - [abstract] "Pretraining is performed on a large synthetic dataset of MTPPs sampled from a broad distribution of Hawkes processes."
  - [section 4.1] Describes the synthetic dataset generation with "randomized base intensities, kernels, and interaction types (excitatory, inhibitory, neutral)."
  - [corpus] The related paper "On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery" similarly advocates for foundation models in TPPs, supporting the general approach.
- Break condition: The assumption breaks for real datasets exhibiting patterns not well-captured by the Hawkes process family or the specific synthetic distributions used (e.g., the strong alternating mark patterns in TAXI or single-mark dominance in TAOBAO).

### Mechanism 3
- Claim: Fine-tuning adapts the pretrained prior to target-specific distributions by reweighting context information and adjusting intensity parameters.
- Mechanism: Further training on the negative log-likelihood (LNLL) of a real dataset allows the model to shift probability mass from the generic Hawkes prior to the empirically observed patterns in the target data. The model retains its in-context inference capability but learns to leverage the provided context sequences more effectively for that specific domain.
- Core assumption: The pretrained weights provide a sensible initialization that can be efficiently adapted with relatively few gradient steps.
- Evidence anchors:
  - [section 4.2] "FIM-PP can be finetuned on the train split C of an evaluation dataset, minimizing LNLL."
  - [section 5] "Finetuning progress is monitored by processing target sequences from the validation split, given the train split context." Results show FIM-PP(f) consistently outperforms FIM-PP(zs).
  - [corpus] Not directly supported by corpus; related work focuses on model interpretability or attacks.
- Break condition: Fine-tuning may overfit to small datasets or fail if the pretraining distribution is too dissimilar from the target, requiring extensive data to overcome the initial bias.

## Foundational Learning

- **Marked Temporal Point Processes (MTPPs)**
  - Why needed here: FIM-PP's core objective is to estimate the conditional intensity function $\lambda(t, \kappa | \mathcal{H}_t)$, which defines the instantaneous rate of events for each mark $\kappa$ given history. Understanding this function is essential to interpreting the model's output.
  - Quick check question: Can you write the likelihood for a sequence of events $\{(t_i, \kappa_i)\}_{i=1}^n$ in terms of its conditional intensity?

- **Hawkes Processes**
  - Why needed here: The entire synthetic pretraining corpus is built upon Hawkes processes. Understanding their self- and mutually-exciting formulation ($\lambda(t,\kappa) = \mu_\kappa + \sum z_{\kappa\kappa'}\gamma_{\kappa\kappa'}(t-t')$) is key to analyzing the model's learned prior and its potential biases.
  - Quick check question: How does the `max(0, ...)` operation in the Hawkes intensity definition relate to inhibitory effects?

- **In-Context Learning with Transformers**
  - Why needed here: The model's architecture is a specific instantiation of in-context learning, where the context sequences provide "keys" and "values" for the "query" from the target history. Familiarity with encoder-decoder attention is crucial.
  - Quick check question: In the FIM-PP architecture, what roles do the fixed query $q_{cont}$ and the learnable projections ($\phi_t, \phi_\kappa, \phi_{\Delta t}$) play?

## Architecture Onboarding

- **Component map:** Input Processing -> Context Encoder -> Context Combiner -> History Decoder -> Intensity Head -> Output
- **Critical path:**
  1. **Synthetic Data Generation:** Sample Hawkes processes with diverse configurations (Table 3), simulate sequences via Ogata's thinning.
  2. **Pretraining:** Train FIM-PP to minimize LNLL on synthetic data, learning to infer $\hat{\lambda}$ from context + history.
  3. **Deployment:** (a) **Zero-shot:** Feed real dataset's training split as context, test split as history. (b) **Finetune:** Continue training on real dataset's LNLL, using its train split for context.

- **Design tradeoffs:**
  - **Interpretability vs. Flexibility:** The parametric intensity form (Eq. 11) offers interpretability (jump size $\hat{\alpha}$, decay rate $\hat{\beta}$, asymptote $\hat{\mu}$) but may be less flexible than fully neural intensity estimators.
  - **Fixed Prior vs. Adaptability:** Pretraining on Hawkes processes creates a strong but potentially biased prior. Fine-tuning is required for optimal performance on out-of-distribution real data (e.g., TAXI, TAOBAO).
  - **Context Size vs. Compute:** Performance depends on the number of context sequences provided (up to 2000), trading off memory/compute for better inference.

- **Failure signatures:**
  - **Poor Zero-Shot Mark Prediction:** On datasets like TAXI (alternating marks) or TAOBAO (single-mark dominance), FIM-PP(zs) shows low accuracy (Acc) and high sMAPE$_{\Delta t}$. This indicates the synthetic prior does not cover these specific temporal/mark dependencies.
  - **Out-of-Memory Errors:** If the number of context sequences or sequence lengths exceeds training limits, the model cannot process all available information.
  - **Instability in Long-Horizon Simulation:** Autoregressive simulation using the thinning algorithm on the predicted intensity $\hat{\lambda}$ can accumulate errors, leading to divergent event counts over long horizons.

- **First 3 experiments:**
  1. **Reproduce Synthetic Generalization:** Train a fresh FIM-PP on a small subset of the synthetic corpus (e.g., 10K processes) and evaluate its zero-shot next-event prediction accuracy on a held-out synthetic test set. This validates the training loop and in-distribution learning.
  2. **Ablate Context Quantity:** Take the pretrained FIM-PP and evaluate its zero-shot performance on a real dataset (e.g., RETWEET) while systematically varying the number of context sequences provided (e.g., 100, 500, 1000, 2000). This quantifies the impact of context richness.
  3. **Finetune and Compare:** Finetune FIM-PP on the TAXI dataset for a fixed number of steps and compare its next-event mark accuracy (Acc) against the zero-shot baseline and a standard neural Hawkes process baseline. This demonstrates the adaptation capability and closes the performance gap identified in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can intensity-free inference methods be effectively integrated into the amortized in-context learning framework to improve predictive accuracy?
- Basis in paper: [explicit] The Conclusion states: "We will explore incorporating such intensity-free methods into our amortized in-context learning approach."
- Why unresolved: Current FIM-PP relies on conditional intensity function estimation, which preserves interpretability but may lag behind generative, intensity-free methods in predictive performance.
- What evidence would resolve it: A modified architecture that successfully leverages intensity-free sampling within the in-context framework, demonstrating improved sMAPE and OTD metrics on benchmark datasets.

### Open Question 2
- Question: To what extent does broadening the synthetic pretraining distribution beyond Hawkes processes improve zero-shot performance on datasets with oscillating or sparse patterns?
- Basis in paper: [explicit] The Conclusion notes: "Future work will broaden the pretraining distribution beyond Hawkes processes, to capture more real-world patterns in zero-shot mode."
- Why unresolved: The current Hawkes-based prior fails to capture specific real-world patterns (e.g., the alternating mark behavior in the Taxi dataset), resulting in poor zero-shot mark accuracy.
- What evidence would resolve it: Empirical results showing that a model pretrained on a diverse mix of processes (e.g., self-correcting or intermittent) handles oscillating patterns without fine-tuning.

### Open Question 3
- Question: How can the model architecture be adapted to support datasets where the number of distinct event marks exceeds the fixed upper bound used during pretraining?
- Basis in paper: [inferred] The Limitations section states the model is "constrained by a fixed maximum number of marks |K|... When these limits are exceeded, the model may fail to exploit all available context."
- Why unresolved: The fixed embedding size limits the model's generalizability to systems with vast numbers of event types without architectural restructuring or retraining.
- What evidence would resolve it: A mechanism (such as dynamic embedding scaling) that allows the model to process datasets with |K| > 22 in a zero-shot manner while maintaining inference quality.

## Limitations

- **Synthetic prior bias:** The model's strong performance on real-world data relies heavily on the synthetic Hawkes processes covering the true data dynamics, limiting zero-shot performance on datasets with specific mark-level dependencies.
- **Generalization to non-Hawkes dynamics:** Real-world event sequences may exhibit patterns (e.g., self-modulating intensities, external covariates) that fall outside the synthetic prior, requiring extensive fine-tuning or architectural modifications.
- **Scalability and memory constraints:** Processing up to 2000 context sequences with variable lengths requires significant memory, and the fixed architecture may not scale efficiently to very long sequences or extremely large context sets.

## Confidence

- **High Confidence:** The core in-context learning mechanism (transformer encoder-decoder with cross-attention) is well-specified and theoretically sound. The pretraining methodology (synthetic Hawkes processes + NLL objective) is clearly defined and reproducible.
- **Medium Confidence:** The empirical results showing competitive performance on real-world datasets are convincing, but the analysis of failure modes (e.g., specific mark prediction errors on TAXI/TAOBAO) could be more detailed. The claimed interpretability benefits of the parametric intensity form are plausible but not rigorously validated.
- **Low Confidence:** The paper's assertion that the model can generalize to *any* MTPP distribution via in-context learning is overstated. The pretraining distribution is finite and Hawkes-specific, creating a functional prior that may not cover all real-world dynamics.

## Next Checks

1. **Synthetic Distribution Coverage Analysis:** Systematically sample the synthetic Hawkes process prior and evaluate whether it captures the mark-level dependencies observed in real datasets (e.g., TAXI's alternating marks, TAOBAO's single-mark dominance). This validates the pretraining distribution's adequacy.
2. **Ablation of Pretraining Diversity:** Train FIM-PP variants on synthetic corpora with reduced diversity (e.g., only excitatory effects, only exponential kernels) and compare their zero-shot performance on real datasets. This quantifies the importance of the broad synthetic prior.
3. **Long-Horizon Simulation Stability:** Evaluate the autoregressive simulation performance on real datasets beyond the 1000-event horizon reported. Measure event count divergence and mark distribution drift to assess the model's ability to maintain coherent dynamics over extended sequences.