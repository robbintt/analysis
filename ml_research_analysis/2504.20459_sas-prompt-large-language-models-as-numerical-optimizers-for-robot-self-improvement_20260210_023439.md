---
ver: rpa2
title: 'SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement'
arxiv_id: '2504.20459'
source_url: https://arxiv.org/abs/2504.20459
tags:
- robot
- table
- ball
- prompt
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SAS-Prompt, a method enabling large language
  models (LLMs) to perform iterative self-improvement of robot policies through explainable
  numerical optimization. The key insight is that LLMs possess emergent capabilities
  for stochastic numerical optimization, which can be leveraged for policy search
  in robotics.
---

# SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement

## Quick Facts
- arXiv ID: 2504.20459
- Source URL: https://arxiv.org/abs/2504.20459
- Reference count: 38
- Key result: LLM-based optimizer achieves 39.4% Top-1, 68.89% Top-5, 83.70% Top-10 retrieval accuracy for table tennis parameter tuning

## Executive Summary
This paper presents SAS-Prompt, a method enabling large language models (LLMs) to perform iterative self-improvement of robot policies through explainable numerical optimization. The key insight is that LLMs possess emergent capabilities for stochastic numerical optimization, which can be leveraged for policy search in robotics. SAS-Prompt implements a three-step process within a single LLM prompt: summarize (extract features from execution traces), analyze (reason about parameter effects), and synthesize (generate improved parameters). The approach is evaluated in a robot table tennis task both in simulation and on a real robot, demonstrating successful adaptation to user-specified objectives through natural language instructions.

## Method Summary
SAS-Prompt uses an LLM as a numerical optimizer for robot policy search without gradient information. The system operates through a four-step process: (1) Summarize execution traces in a structured table, (2) Identify parameters closest to the objective, (3) Analyze parameter effects through step-by-step reasoning, and (4) Synthesize new parameters. The method uses in-context learning with a cache of execution traces, each containing 8 attenuation parameters and corresponding landing positions. Natural language objectives replace mathematical reward functions, and the LLM generates parameter updates through emergent optimization capabilities demonstrated on benchmark functions like Ackley and Rastrigin.

## Key Results
- Retrieval experiments achieve 39.4% (Top-1), 68.89% (Top-5), and 83.70% (Top-10) accuracy
- Self-improvement successfully adapts robot behavior to hit balls to different table positions as specified by user objectives
- LLM outperforms Adam and Nelder-Mead optimizers on 2D and 8D numerical optimization benchmarks
- Robot successfully translates natural language objectives into improved control parameters in both simulation and real-world experiments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large Language Models (LLMs) can function as gradient-free stochastic optimizers capable of minimizing numerical error through in-context pattern matching.
- **Mechanism**: The LLM processes a history of input-output pairs (execution traces) and iteratively proposes new inputs. By leveraging pre-trained reasoning capabilities, it identifies trends in the data to minimize a cost signal without explicit gradient calculations.
- **Core assumption**: The pre-training data of the LLM contains sufficient mathematical logic and optimization heuristics to allow the model to "solve" for better inputs via next-token prediction.
- **Evidence anchors**: [abstract]: The paper states LLMs have a "built-in ability to perform (stochastic) numerical optimization." [section]: Table I shows the LLM outperforming Adam and Nelder-Mead on 2D and 8D optimization benchmarks (Ackley and Rastrigin). [corpus]: "Large Language Model is Secretly a Protein Sequence Optimizer" supports the cross-domain generality of LLMs as optimizers.

### Mechanism 2
- **Claim**: Natural language objectives can substitute for mathematical reward functions by grounding spatial reasoning in domain descriptions.
- **Mechanism**: The prompt translates user intent (e.g., "hit far right") into a semantic constraint. The LLM then filters the execution cache for traces that maximize this semantic similarity, effectively performing a form of retrieval-based value estimation.
- **Core assumption**: The LLM possesses robust spatial reasoning and can reliably map qualitative instructions ("far right," "shallow") to quantitative coordinate data in the execution traces.
- **Evidence anchors**: [section]: Fig. 4 details the "Domain Description" and "Objective" components replacing standard fitness functions. [section]: Fig. 5 and Fig. 7 show the robot successfully retrieving parameters for "Hit the ball as far right as possible," though performance drops on ambiguous targets like "middle of table." [corpus]: "Critique-GRPO" suggests that combining natural language feedback with numerical signals can advance reasoning, though this paper relies primarily on the language signal.

### Mechanism 3
- **Claim**: Explicit step-by-step reasoning (Summarize, Analyze) generates a "textual gradient" that guides parameter synthesis.
- **Mechanism**: The "Analyze" step forces the LLM to articulate correlations (e.g., "Increasing parameter $g$ shifts landing right"). This textual summary acts as an explicit update rule, allowing the model to synthesize new parameters by applying these inferred "derivatives."
- **Core assumption**: The model can accurately infer causal relationships between control parameters and outcomes from limited, noisy samples in the context window.
- **Evidence anchors**: [section]: Section IV.C explicitly compares the analysis to a "textual formulation of a gradient." [section]: Fig. 8 provides qualitative evidence where the LLM identifies correlations ($g$ and $h$) to propose new parameters. [corpus]: "In-Context Iterative Policy Improvement" highlights similar capabilities in attention-based architectures for dynamic manipulation.

## Foundational Learning

- **Concept**: **Gradient-Free Optimization**
  - **Why needed here**: The SAS-Prompt treats the LLM as a black-box optimizer (similar to Nelder-Mead or CMA-ES). Understanding how optimization works without derivatives is essential to diagnose why the LLM might converge slowly or get stuck in local minima.
  - **Quick check question**: How would you find the peak of a hill blindfolded using only a walking stick and an altimeter, without seeing the slope?

- **Concept**: **In-Context Learning (ICL)**
  - **Why needed here**: The system does not train the LLM; it relies entirely on the context window (the cache of execution traces). Engineers must understand that the "learning" is transient and limited by token limits.
  - **Quick check question**: If you clear the context window, does the robot retain any learned skills? (Answer: No, unless the final parameters were saved externally).

- **Concept**: **Residual Policy Learning**
  - **Why needed here**: The LLM does not generate raw joint angles; it generates "attenuation parameters" (scaling factors) for a lower-level controller (LLC). This constrains the search space but limits the robot to behaviors within the LLC's manifold.
  - **Quick check question**: What happens to the robot's behavior if the attenuation parameters are all set to 1.0?

## Architecture Onboarding

- **Component map**: User Objective (Text) + Domain Description (Coordinate System) -> Memory (In-Context Cache) -> SAS Prompt (Summarize → Analyze → Synthesize) -> Lower-Level Controller (LLC) + Robot Environment -> Execution Trace -> Memory

- **Critical path**: The loop moves from **Execution** → **Logging** → **Analysis (LLM)** → **Synthesis (LLM)** → **Execution**. The most fragile step is the **Analysis**, where the LLM must correctly interpret the trace data.

- **Design tradeoffs**:
  - **Explainability vs. Precision**: The system provides natural language reasons for changes (high explainability) but lacks the numerical precision of gradient-based optimizers.
  - **Cache Size vs. Latency**: A larger history of traces improves the LLM's ability to find trends but increases inference cost and latency.

- **Failure signatures**:
  - **Oscillation**: The LLM proposes parameter set A, then B, then A again, without convergence.
  - **Hallucinated Gradients**: The Analysis step claims "Parameter $a$ moves the ball Left," but the data shows no correlation or a negative correlation.
  - **Context Saturation**: The robot stops improving or errors out because the token limit of the prompt is reached.

- **First 3 experiments**:
  1. **Unit Test the Optimizer**: Isolate the LLM using the prompt in Fig. 3 on a 2D mathematical function (e.g., Ackley). Verify it can actually find the minimum without robot hardware.
  2. **Retrieval Ablation**: Populate the cache with 20 fixed examples. Run the "Retrieval" steps (1 & 2) only, and measure if the LLM selects the ground-truth best example for specific queries (e.g., "hit to coordinate [x,y]").
  3. **Sim-to-Real Transfer**: Run the full self-improvement loop in simulation for 20 iterations to find a target, then take the final parameters and test them on the physical robot to check for reality gap issues.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The approach relies on in-context learning without fine-tuning, making long-term learning impractical without external storage mechanisms.
- Performance degrades on ambiguous natural language objectives that lack clear semantic boundaries.
- Scalability to higher-dimensional parameter spaces and more complex robotic tasks remains speculative and untested.

## Confidence
**High Confidence**: The LLM's capability to function as a gradient-free optimizer for well-defined numerical functions (Ackley, Rastrigin benchmarks) is well-supported by quantitative comparisons showing superior performance to traditional methods.

**Medium Confidence**: The effectiveness of natural language objectives as substitutes for mathematical reward functions shows mixed results. While clear directional objectives ("far right") work well, ambiguous targets reveal limitations in the LLM's spatial reasoning capabilities.

**Low Confidence**: The scalability of this approach to more complex robotic tasks remains speculative. The paper does not address how the method would perform with larger parameter spaces, continuous state-action spaces, or tasks requiring long-term planning and memory beyond the context window.

## Next Checks
1. **Cross-Domain Optimizer Validation**: Apply the SAS prompt to a diverse set of mathematical optimization problems (including high-dimensional functions like Rosenbrock and Griewank) to verify the LLM's optimization capabilities extend beyond the tested Ackley and Rastrigin functions.

2. **Robustness to Ambiguous Objectives**: Systematically test retrieval and self-improvement performance across a spectrum of objective specificity, from clear directional commands to progressively more ambiguous targets, to quantify the limits of natural language grounding.

3. **Context Window Stress Test**: Evaluate the system's performance as the in-context cache grows from 24 to 100+ examples to determine whether the LLM maintains optimization effectiveness or experiences degradation due to context saturation and increased computational overhead.