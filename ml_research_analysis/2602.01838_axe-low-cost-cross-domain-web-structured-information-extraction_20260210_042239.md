---
ver: rpa2
title: 'AXE: Low-Cost Cross-Domain Web Structured Information Extraction'
arxiv_id: '2602.01838'
source_url: https://arxiv.org/abs/2602.01838
tags:
- html
- text
- page
- context
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AXE introduces a low-cost, zero-shot web information extraction
  pipeline that treats the HTML DOM as a tree to be pruned rather than a wall of text
  to be read. By stripping away boilerplate and irrelevant nodes, it distills the
  page into a high-density context that a tiny 0.6B LLM can process efficiently.
---

# AXE: Low-Cost Cross-Domain Web Structured Information Extraction

## Quick Facts
- arXiv ID: 2602.01838
- Source URL: https://arxiv.org/abs/2602.01838
- Reference count: 40
- AXE achieves 88.1% zero-shot F1 on SWDE, outperforming larger models by pruning DOM to ~350 tokens

## Executive Summary
AXE introduces a low-cost, zero-shot web information extraction pipeline that treats the HTML DOM as a tree to be pruned rather than a wall of text to be read. By stripping away boilerplate and irrelevant nodes, it distills the page into a high-density context that a tiny 0.6B LLM can process efficiently. Grounded XPath Resolution ensures every extracted value is traceable to its source node, preventing hallucinations. On the SWDE dataset, AXE achieves state-of-the-art zero-shot F1 of 88.1%, outperforming much larger, fully-trained models while reducing average token context by 97.9% (from ~16.6K to ~350 tokens). Ablation studies confirm the critical role of pruning, fine-tuning, and grounding in its performance.

## Method Summary
AXE operates through a three-stage pipeline: DOM tree pruning, fine-tuning a compact LLM, and grounded XPath resolution. The pruning stage uses iterative rules to remove boilerplate (ads, headers, footers) and nodes lacking relevant attributes, retaining only information-dense subtrees. A 0.6B parameter Llama model is fine-tuned on synthetic HTML-data pairs generated via GPT-4 to recognize extraction patterns. During extraction, the pruned DOM is converted to JSON, and the LLM predicts attribute values along with their XPath locations. An XPath grounding module verifies each prediction by locating the actual node, ensuring traceability and reducing hallucinations.

## Key Results
- Achieves 88.1% zero-shot F1 on SWDE, surpassing larger, fully-trained models
- Reduces average token context by 97.9% (from ~16.6K to ~350 tokens)
- Outperforms existing zero-shot methods by 2.5% F1 while using a 0.6B parameter model

## Why This Works (Mechanism)
AXE works by fundamentally rethinking how LLMs process web pages: instead of treating HTML as unstructured text, it exploits the DOM tree structure to prune irrelevant content before any LLM interaction. This drastically reduces the context window, enabling a small model to focus only on extraction-relevant nodes. Fine-tuning on synthetic HTML-data pairs teaches the model to extract directly from HTML rather than unstructured text. The XPath grounding step ensures that every extraction is verifiable and anchored to a real DOM node, preventing hallucinations. Together, these innovations enable high performance with minimal computational overhead.

## Foundational Learning
- **DOM Tree Structure**: HTML pages are represented as trees of nodes (elements, text, attributes). Understanding this structure is essential for effective pruning and navigation.
- **XPath Navigation**: XPath is a query language for selecting nodes in an XML/HTML tree. It enables precise location of elements for grounding predictions.
- **Context Window Efficiency**: Reducing the number of tokens an LLM processes directly lowers inference cost and improves speed. AXE achieves 97.9% reduction by pruning irrelevant nodes.
- **Fine-Tuning on Synthetic Data**: Generating synthetic HTML-data pairs allows small models to learn extraction patterns without expensive manual annotation.
- **Hallucination Prevention**: Grounding predictions to actual DOM nodes via XPath verification ensures that extracted values are traceable and verifiable, reducing model hallucinations.

## Architecture Onboarding

**Component Map**: HTML Input -> DOM Parser -> Iterative Pruner -> Fine-Tuned LLM -> XPath Grounding -> Structured Output

**Critical Path**: The core pipeline is DOM pruning → fine-tuned extraction → XPath verification. Pruning is the most impactful step, as it directly determines model context size and extraction accuracy.

**Design Tradeoffs**: AXE trades off some extraction coverage (by pruning nodes) for massive efficiency gains and hallucination reduction. Using a tiny LLM instead of a large one saves cost but requires careful fine-tuning and grounding to maintain accuracy.

**Failure Signatures**: 
- Over-aggressive pruning may remove relevant nodes, lowering recall
- Poor fine-tuning can cause the model to miss extraction patterns
- XPath grounding may fail on dynamically generated or malformed HTML, leading to unverifiable predictions

**First 3 Experiments to Run**:
1. Measure token reduction and accuracy after each pruning iteration to identify the optimal pruning depth
2. Compare zero-shot extraction F1 with and without XPath grounding to quantify hallucination reduction
3. Test fine-tuning with different amounts of synthetic data to find the minimum effective training set size

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Evaluation is limited to the SWDE dataset (from 2010), raising questions about performance on modern, dynamic websites
- No detailed runtime or infrastructure cost analysis is provided for different deployment scenarios
- XPath grounding may fail on JavaScript-heavy sites where DOM structures change after initial load

## Confidence

**High confidence**: The core technical approach of DOM pruning and the resulting token efficiency improvements are well-supported by ablation studies and quantitative metrics

**Medium confidence**: The zero-shot F1 score of 88.1% is impressive, but the reliance on a single benchmark dataset and lack of cross-dataset validation reduces confidence in real-world applicability

**Medium confidence**: Claims about cost-effectiveness relative to larger models are reasonable given the context reduction, but lack comprehensive cost modeling

## Next Checks

1. Test AXE's performance on more recent, diverse web extraction datasets (e.g., contemporary e-commerce sites, news portals) to assess robustness to modern web architectures
2. Conduct a cost analysis comparing AXE's total inference costs (including preprocessing overhead) against both fine-tuned small models and zero-shot large models across different hardware configurations
3. Evaluate AXE's performance on dynamic, JavaScript-heavy websites where DOM structures change post-load to identify potential failure modes and robustness limitations