---
ver: rpa2
title: 'How Language Models Conflate Logical Validity with Plausibility: A Representational
  Analysis of Content Effects'
arxiv_id: '2510.06700'
source_url: https://arxiv.org/abs/2510.06700
tags:
- validity
- plausibility
- content
- vectors
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how large language models (LLMs) conflate
  logical validity with plausibility in reasoning tasks, a phenomenon known as content
  effects. Researchers examined ten different LLMs and found that both validity and
  plausibility are linearly represented in model representations, with high geometric
  similarity between these concepts.
---

# How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects

## Quick Facts
- **arXiv ID:** 2510.06700
- **Source URL:** https://arxiv.org/abs/2510.06700
- **Reference count:** 19
- **Primary result:** Large language models conflate logical validity with plausibility due to entangled representations, but targeted steering vectors can disentangle these concepts and reduce reasoning biases.

## Executive Summary
This study investigates how large language models conflate logical validity with semantic plausibility—a cognitive bias known as content effects. Researchers found that both validity and plausibility are linearly represented in model activations with high geometric similarity, explaining why plausible but invalid arguments are often judged as valid. Using steering vectors, they demonstrated causal interactions between these representations and developed debiasing vectors that reduce content effects while improving reasoning accuracy. The work provides both theoretical insight into model reasoning failures and practical interventions for mitigating them without model retraining.

## Method Summary
The researchers constructed a controlled dataset of 1,280 syllogisms covering 64 types × 10 taxonomic term triples, with validity (valid/invalid) and plausibility (true/false) factors crossed. They extracted hidden states at the last token position from multiple LLM layers and computed difference-in-means vectors for validity and plausibility concepts using model predictions rather than ground truth. Steering interventions applied these vectors (scaled by α=1.5) to test causal effects, measuring Steering Power (SP) and Content Effect (CE). Debiasing vectors were constructed as the difference between validity and plausibility task representations to disentangle the concepts.

## Key Results
- Both validity and plausibility concepts are linearly represented in model activations with cosine similarities of 0.48-0.64
- Geometric alignment between validity and plausibility vectors predicts the magnitude of behavioral content effects
- Steering plausibility vectors into validity tasks causally biases judgments, demonstrating interactive representations
- Debiasing vectors reduce content effects from 0.348 to 0.072 while improving reasoning accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Logical validity and semantic plausibility are linearly represented in the model's residual stream, and their geometric alignment correlates with behavioral reasoning biases.
- **Mechanism:** The model encodes "valid vs. invalid" and "true vs. false" as distinct vector directions in late layers. When these directions have high cosine similarity (are "aligned"), the model conflates the two concepts, treating logically invalid but plausible arguments as valid.
- **Core assumption:** The difference-in-means vector captures the causal direction of the concept, and geometric proximity implies functional confusion for the model.
- **Evidence anchors:**
  - [abstract] "...both concepts are linearly represented and strongly aligned in representational geometry... degree of alignment... predicts the magnitude of behavioral content effects."
  - [section 4.2] "Validity and plausibility vectors are similar... average cosine similarities... 0.48 to 0.64."
  - [corpus] The corpus generally supports linear representation hypotheses, but specific vector alignment for validity/plausibility is unique to this paper's analysis.
- **Break condition:** If concepts require non-linear (multi-dimensional) manifolds rather than single directions, this linear steering approach would fail to alter behavior significantly.

### Mechanism 2
- **Claim:** Representations of validity and plausibility are causally interactive; intervening on one dimension shifts judgments on the other.
- **Mechanism:** A "plausibility vector" (derived from true/false classification tasks) is added to the hidden state activations during a logical reasoning task. This addition shifts the model's output logits, causing it to classify an argument as valid simply because the steering vector made the content "feel" true.
- **Core assumption:** The steering vector applies a sufficient perturbation to overcome the model's trained weights without pushing activations into out-of-distribution regions.
- **Evidence anchors:**
  - [abstract] "Using steering vectors, we demonstrate that plausibility vectors can causally bias validity judgements."
  - [section 4.3] "For both models... we observe high SP [Steering Power] in a subset of late layers in both directions."
  - [corpus] Neighbor "Abstract Activation Spaces for Content-Invariant Reasoning" supports the feasibility of disentangling content from reasoning structure in activation space.
- **Break condition:** If the model relies on separate, non-interacting circuits for logic and knowledge retrieval, cross-task steering (plausibility → validity) would result in negligible Steering Power (SP).

### Mechanism 3
- **Claim:** A "task-difference" steering vector can disentangle validity from plausibility, reducing bias without weight updates.
- **Mechanism:** By computing the difference between the mean activation for the validity task (μ_V) and the plausibility task (μ_P), researchers isolate a vector representing "pure logic." Adding this μ_V-P vector during inference suppresses the "truthfulness" dimension, forcing the model to rely on structural validity.
- **Core assumption:** The "logic" features are linearly separable from the "truth" features in the activation space.
- **Evidence anchors:**
  - [abstract] "...construct debiasing vectors that disentangle these concepts, reducing content effects..."
  - [section 4.4] "We define the task-difference vector: μ^l_V-P = μ^l_V - μ^l_P... reducing CE [Content Effect] from 0.348 to 0.072."
  - [corpus] "Mitigating Content Effects..." (Valentino et al., 2025) is cited as concurrent/prior work confirming that activation steering can mitigate such biases.
- **Break condition:** If the scaling factor α is too high, the intervention destroys semantic coherence, causing the model to output invalid labels or gibberish.

## Foundational Learning

- **Concept:** **Linear Representation Hypothesis**
  - **Why needed here:** The entire intervention strategy relies on the premise that high-level concepts (like "validity") are encoded as directions (vectors) in the latent space, rather than complex non-linear patterns.
  - **Quick check question:** If concepts were non-linearly encoded, would adding a simple vector effectively change the model's reasoning process? (Answer: No).

- **Concept:** **Difference-in-Means (Contrastive Activation)**
  - **Why needed here:** This is the method used to extract the "concept vectors." You must understand that the vector is defined as the average activation for Class A minus the average for Class B.
  - **Quick check question:** Why do we subtract the mean of "invalid" activations from "valid" activations? (Answer: To isolate the direction that specifically encodes the difference/transition from invalid to valid).

- **Concept:** **Content Effects (Cognitive Bias)**
  - **Why needed here:** To diagnose the failure mode, you must understand the distinction between an argument's structural validity (does it follow?) and its semantic plausibility (is it true?).
  - **Quick check question:** In the syllogism "All cats are dogs; All dogs are birds; Therefore all cats are birds," is the conclusion valid, plausible, or both? (Answer: Valid structurally, but implausible semantically).

## Architecture Onboarding

- **Component map:** Dataset Generator → Activation Collector → Vector Computer → Steering Engine
- **Critical path:** The layer selection is critical. The paper identifies late layers (e.g., 40-60 out of 64) as the "steerable" region where concepts are crystallized but before the final output projection.
- **Design tradeoffs:**
  - **Scaling Factor (α):** Too low (<1.0) and the bias persists; too high (>2.0) and the model outputs incoherent text. The paper found α=1.5 optimal.
  - **Token Position:** The paper operates on the last token before generation. Steering earlier tokens might affect premise comprehension rather than the logical judgment itself.
- **Failure signatures:**
  - **Low Steering Power (SP ≈ 0):** You are likely steering an early layer (where concepts aren't formed) or the model architecture does not linearly encode the concept.
  - **Success Rate Drop:** If α is too aggressive, the model may fail to generate "valid" or "invalid" tokens at all, outputting "I cannot determine..." or gibberish.
- **First 3 experiments:**
  1. **Sanity Check:** Compute cosine similarity between the Validity vector and a random vector. It should be near 0. Then compare Validity vs. Plausibility vectors to confirm high similarity (entanglement).
  2. **Steering Power Sweep:** For layers 1 to N, apply the Validity vector to the Validity task. Plot SP vs. Layer to find the "causal" layers (where SP peaks).
  3. **Debiasing Run:** Using the identified peak layer, apply the task-difference vector (μ_V-P) to the test set and measure the delta in Accuracy and Content Effect (CE).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the representational entanglement of validity and plausibility shift or resolve across the full sequence of Chain-of-Thought (CoT) tokens, rather than just at the final prediction token?
- **Basis in paper:** [explicit] The authors explicitly state in the Limitations that analyzing the full sequence of generated tokens is necessary to explain why "CoT models show lower content effects than zero-shot models, despite exhibiting similar patterns" in their single-token analysis.
- **Why unresolved:** The current study focuses only on dense representations extracted at the last token position before prediction, missing the dynamics of the reasoning process.
- **What evidence would resolve it:** An analysis of hidden states across every generated token in a CoT trace to see if the alignment between validity and plausibility directions decreases during intermediate reasoning steps.

### Open Question 2
- **Question:** Do statistical properties of the pre-training data (specifically the correlation between validity and soundness) cause the representational entanglement of validity and plausibility?
- **Basis in paper:** [explicit] The authors conjecture in the Limitations that "conflation stems from properties of the training data" where "valid arguments are also sound," causing models to "associate validity with plausibility."
- **Why unresolved:** The paper demonstrates the existence of this entanglement in model weights but does not investigate the source of the error in the training corpus.
- **What evidence would resolve it:** Training models on datasets where logical validity is strictly decorrelated from truth/soundness, followed by testing for reduced cosine similarity between validity and plausibility vectors.

### Open Question 3
- **Question:** Does the mechanism of linear representational entanglement underlie other types of cognitive biases or reasoning fallacies in LLMs?
- **Basis in paper:** [explicit] The Conclusion suggests future work should "extend this framework to other cognitive biases" and "investigate whether similar mechanisms underlie them."
- **Why unresolved:** The study is restricted to syllogistic reasoning and the specific interplay of validity/plausibility; it is unknown if this is a general principle of model bias.
- **What evidence would resolve it:** Extracting steering vectors for distinct bias dimensions (e.g., confirmation bias, anchoring) and testing for geometric alignment similar to that found in this study.

## Limitations

- Findings are limited to syllogistic reasoning tasks using a specific vocabulary of 10 taxonomic term triples
- The linear representation hypothesis may not generalize to more complex logical structures or non-taxonomic domains
- The study focuses on late-layer representations without exploring whether earlier layers encode complementary or contradictory representations

## Confidence

**High Confidence:**
- The existence of content effects in LLMs (validity judged by plausibility) is well-established through controlled experiments
- Linear representation of both concepts in activation space is empirically verified across multiple models
- Steering interventions causally affect model behavior as measured by SP metrics

**Medium Confidence:**
- The geometric alignment between validity and plausibility vectors fully explains behavioral content effects
- The debiasing vector approach generalizes beyond the specific syllogism domain
- α=1.5 is optimal across all model scales and reasoning tasks

**Low Confidence:**
- The mechanism by which entangled representations lead to content effects is completely understood
- Steering vectors extracted from one model transfer effectively to other architectures
- The intervention improves reasoning on naturally occurring text rather than controlled datasets

## Next Checks

1. **Cross-Domain Transfer Test:** Apply the debiasing vectors trained on syllogisms to natural language reasoning tasks (common sense reasoning, mathematical proofs) and measure whether content effects persist and whether accuracy improves.

2. **Architecture Transfer Experiment:** Extract steering vectors from Qwen models and apply them to GPT, Claude, or Llama architectures to determine if representational geometry is model-specific or universal.

3. **Temporal Stability Analysis:** Track how validity-plausibility entanglement evolves during model training (early checkpoints vs. final weights) to determine whether this is an emergent property of optimization or present from initialization.