---
ver: rpa2
title: Keyword-Oriented Multimodal Modeling for Euphemism Identification
arxiv_id: '2503.21504'
source_url: https://arxiv.org/abs/2503.21504
tags:
- multimodal
- euphemism
- euphemisms
- identification
- kom-ei
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles euphemism identification in multimodal contexts,
  where euphemisms must be linked to their target keywords using text, images, and
  speech. The authors create KOM-Euph, a multimodal dataset with 86K text-image-speech
  triplets across Drug, Weapon, and Sexuality domains.
---

# Keyword-Oriented Multimodal Modeling for Euphemism Identification

## Quick Facts
- arXiv ID: 2503.21504
- Source URL: https://arxiv.org/abs/2503.21504
- Reference count: 40
- Key outcome: KOM-EI achieves 45-60% higher top-1 accuracy than state-of-the-art baselines for euphemism identification

## Executive Summary
This paper tackles euphemism identification in multimodal contexts, where euphemisms must be linked to their target keywords using text, images, and speech. The authors create KOM-Euph, a multimodal dataset with 86K text-image-speech triplets across Drug, Weapon, and Sexuality domains. They propose KOM-EI, a keyword-oriented model that uses cross-modal feature alignment and dynamic fusion to leverage visual and audio cues for accurate identification. Experiments show KOM-EI achieves 45-60% higher top-1 accuracy than state-of-the-art baselines and outperforms large language models and multimodal models in both accuracy and efficiency. The method demonstrates the importance of multimodal data for euphemism identification.

## Method Summary
KOM-EI employs a multimodal architecture with BERT-base (text), CLIP ViT-L/14 (image), and Wav2Vec2 (speech) encoders, each projecting to 768D. The model uses cross-modal contrastive alignment to align semantic spaces, text-anchored cross-attention to extract relevant modality features, gated units to filter noise, and self-attention for refinement. Training is self-supervised by masking target keywords in sentences. The model optimizes a combined loss of prediction, text-image, and text-speech contrastive components. Hardware: 2× Tesla V100 32G; Optimizer: AdamW (lr=5e-5, 1000 warm-up steps).

## Key Results
- KOM-EI achieves 45-60% higher top-1 accuracy than state-of-the-art baselines
- Outperforms large language models and multimodal models in both accuracy and efficiency
- Ablation shows gated units add 5-6% top-1 accuracy improvement

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Cross-modal feature alignment via contrastive learning bridges the semantic gap between text, images, and speech representations.
- **Mechanism**: The CFA module uses contrastive loss to pull matching keyword-modality pairs closer in embedding space while pushing mismatched pairs apart.
- **Core assumption**: Euphemisms' literal meanings share visual or phonetic semantic connections with their implied meanings.
- **Evidence anchors**:
  - [abstract]: "uses cross-modal feature alignment and dynamic fusion modules to explicitly utilize the visual and audio features"
  - [section D]: "CFA mitigates these gaps by aligning heterogeneous features... The cross-modal contrastive loss promotes semantic alignment and discourages irrelevant associations"
- **Break condition**: When euphemisms have no semantic relationship across modalities, alignment fails to provide discriminative signal.

### Mechanism 2
- **Claim**: Text-anchored cross-attention extracts complementary visual and audio signals that disambiguate contextually similar euphemisms.
- **Mechanism**: Text embeddings serve as queries while image/audio embeddings provide keys and values, highlighting modality-specific features relevant to the textual context.
- **Core assumption**: Some euphemisms are indistinguishable from text context alone but can be disambiguated when cross-modal cues are selectively attended to.
- **Evidence anchors**:
  - [section D]: "we anchor on textual features and dynamically incorporate relevant elements from images and audio"
  - [section C]: "distinguishing 'weed' from 'coke' is challenging with only sentence-level context"
- **Break condition**: When visual/audio features are irrelevant to the euphemism meaning, attention weights may amplify noise rather than signal.

### Mechanism 3
- **Claim**: Gated units filter modality-specific noise, preventing extraneous visual or audio features from degrading identification.
- **Mechanism**: After cross-attention, gated units learn to suppress attention outputs that don't contribute to disambiguation.
- **Core assumption**: Cross-modal attention captures both useful signal and noise; not all attended features are beneficial for the target prediction.
- **Evidence anchors**:
  - [section D]: "GU filters the noise from visual or audio features, learning dynamic text-image and text-speech co-attention"
  - [Table VI]: Ablation shows GU adds 5-6% top-1 accuracy improvement
- **Break condition**: If gating learns incorrect suppression patterns, performance degrades.

## Foundational Learning

- **Concept**: Contrastive learning for cross-modal alignment
  - **Why needed here**: Understanding how L_TI and L_TS losses create shared embedding spaces where matching keyword-modality pairs cluster
  - **Quick check question**: Given a batch with 4 text-image pairs, can you sketch which pairs are positive vs. negative samples for the contrastive loss?

- **Concept**: Cross-attention with heterogeneous modalities
  - **Why needed here**: Understanding why text is Q and image/audio are K, V—and what the attention output represents semantically
  - **Quick check question**: If text embedding T queries image embedding I, what does a high attention weight on a specific image region imply?

- **Concept**: Self-supervised learning via masking
  - **Why needed here**: Understanding how masking target keywords creates supervision without manual labels, and why this creates a train-test distribution gap
  - **Quick check question**: Why might a model trained on masked target keywords struggle when tested on masked euphemisms?

## Architecture Onboarding

- **Component map**: Text Encoder (BERT-base-uncased) -> Projection -> CFA -> CA -> GU -> SA -> Prediction
- **Critical path**: Text features are aligned with image/audio via contrastive learning, then refined through cross-attention, gating, and self-attention before prediction
- **Design tradeoffs**: Using pre-trained encoders (BERT, CLIP, Wav2Vec2) provides strong initialization but requires careful alignment; gating adds complexity but improves robustness
- **Failure signatures**: Poor performance on euphemism-masked test data indicates train-test distribution gap; degraded accuracy with irrelevant modalities suggests gating failure
- **First experiments**:
  1. Train with masked target keywords and evaluate on masked euphemisms to confirm distribution gap
  2. Test with randomly assigned images/speech to validate gating mechanism effectiveness
  3. Ablate contrastive loss components (L_TI, L_TS) to assess their individual contributions

## Open Questions the Paper Calls Out

1. **Distribution Gap**: How can the distribution gap between training (keyword-masked sentences) and testing (euphemism-masked sentences) be effectively reduced to improve model robustness?
2. **Synthetic Audio Limitations**: To what extent does the use of synthetic, prosody-free audio limit the model's ability to identify euphemisms in realistic, noisy environments?
3. **Generalization to Emerging Euphemisms**: Can the keyword-oriented multimodal approach effectively generalize to emerging euphemisms that evolve after the dataset is constructed?

## Limitations
- The multimodal dataset may not accurately capture intended euphemistic meanings due to synthetic speech and curated images
- Train-test distribution gap (masking keywords vs. euphemisms) could explain performance gains rather than genuine multimodal benefit
- Efficiency comparison with LLMs lacks quantified metrics and systematic evaluation

## Confidence
**High Confidence**: Multimodal approach improves accuracy vs text-only baselines (45-60% top-1 gain)
**Medium Confidence**: Proposed mechanisms are theoretically sound with ablation benefits, but implementations lack full detail
**Low Confidence**: Claims about outperforming LLMs/MLLMs in efficiency lack sufficient empirical support

## Next Checks
1. **Distribution Gap Validation**: Compare KOM-EI trained and tested with consistent masking (either both target keywords or both euphemisms) versus mismatched masking approach
2. **Modality Ablation with Controlled Data**: Create subset where images/speech are deliberately irrelevant to euphemism meanings and test performance degradation
3. **Cross-Domain Transfer Analysis**: Evaluate KOM-EI on held-out domain (e.g., political euphemism identification) to test domain-general learning vs memorization