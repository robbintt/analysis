---
ver: rpa2
title: Dynamic Feature Selection based on Rule-based Learning for Explainable Classification
  with Uncertainty Quantification
arxiv_id: '2508.02566'
source_url: https://arxiv.org/abs/2508.02566
tags:
- uni00000048
- feature
- uni00000055
- uni00000003
- uni00000046
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of dynamic feature selection (DFS)
  with uncertainty quantification and interpretability. Existing DFS methods rely
  on opaque neural models and lack uncertainty quantification, limiting their use
  in high-stakes domains.
---

# Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification

## Quick Facts
- arXiv ID: 2508.02566
- Source URL: https://arxiv.org/abs/2508.02566
- Reference count: 14
- Primary result: Rule-based DFS achieves 69.43% average accuracy across 5 datasets while providing interpretable predictions

## Executive Summary
This paper addresses dynamic feature selection (DFS) for classification by proposing a rule-based framework that maintains interpretability while quantifying uncertainty. The approach adapts global rule-based classifiers to arbitrary feature subsets through condition removal and confidence re-estimation, avoiding the black-box nature of neural DFS methods. A greedy feature selection policy minimizes prediction divergence from the global model while explicitly accounting for epistemic uncertainty from model adaptation. Experiments on five tabular datasets show the method achieves competitive accuracy while providing interpretable predictions and uncertainty estimates.

## Method Summary
The framework trains a global rule-based classifier (CART or FuzzyCART) on full features, then adapts it to arbitrary feature subsets by removing conditions on unobserved features and re-estimating rule confidences using cached sub-rule statistics. A greedy selection policy queries features that minimize KL divergence from the global model predictions, incorporating an epistemic uncertainty penalty. Neural estimators predict both predictive improvement and epistemic uncertainty reduction for each potential feature query. The method uses a 2-layer MLP with dual heads, trained to predict value function components from feature subset observations.

## Key Results
- Achieves 69.43% average accuracy across 5 datasets, competitive with state-of-the-art methods
- Rule-based adaptation preserves interpretability while maintaining performance
- Prediction entropy decreases with feature acquisition regardless of selection strategy
- No correlation found between budget size and calibration (ECE) - models become more confident without becoming more reliable
- Tree-based methods benefit most from intelligent feature selection, while other methods' performance stems from predictor robustness

## Why This Works (Mechanism)

### Mechanism 1: Rule-Based Model Adaptation via Condition Removal
- Claim: A rule-based classifier trained on full features can be adapted to arbitrary feature subsets while preserving interpretability by removing conditions on unobserved features and re-estimating rule confidences.
- Mechanism: Given a global rule model, for any subset S of observed features, remove antecedent conditions involving features not in S. Re-estimate each rule's class prediction using only remaining conditions by computing confidence from training data where those conditions hold. Cache sub-rule confidences for antecedents with ≤4 conditions to avoid inference overhead.
- Core assumption: The global model's rules generalize sufficiently to partial feature sets when conditions are removed (not imputed), and sub-rule statistics from training data approximate true conditional probabilities.
- Evidence anchors:
  - [abstract] "adapts interpretable rule-based classifiers to arbitrary feature subsets through condition removal and confidence re-estimation"
  - [section 3.1] "we can easily do that by removing the conditions regarding the missing features, and removing rules that have no active conditions. Then, we re-estimate the predictions of each rule for each class"
  - [corpus] Limited corpus support; Paper 27930 discusses crisp vs. fuzzy rule complexity but not DFS-specific adaptation
- Break condition: When removed conditions are discriminatively critical, causing Δ_S = R_S(f_θ*) - R_S(f_θS*) to be large (high epistemic uncertainty from subset adaptation)

### Mechanism 2: Divergence Minimization with Epistemic Uncertainty Penalty
- Claim: A greedy policy minimizing KL divergence from the global model approximates conditional mutual information (CMI) maximization, with an explicit epistemic uncertainty penalty for robustness.
- Mechanism: Define value function v_q(i, x^S) = E_{p(x_i|x^S)}[q(x_i, x^S)] where q = u(x^S ∪ {x_i}) - λe(x^S ∪ {x_i}). The prediction improvement u uses KL divergence: u(x^S) = D_KL(p(ŷ|x) || p(ŷ|x^S)). Query feature maximizing v_q per unit cost. Train neural estimator (2-head MLP) to predict both u and e_R reductions.
- Core assumption: Proportionality holds between global and sub-model predictions (Proposition 1): if p(ŷ|x) ∝ p(ŷ|x^S, x_i), then minimizing divergence equals maximizing CMI.
- Evidence anchors:
  - [section 3.2.1] "We can show that this expression is minimized by the same x_i that maximizes the CMI, as long as the following proportionality holds: I(x_i; ŷ|x^S) ∝ E_{p(x_i|x^S)}[u(x^S) - u(x^S ∪ {x_i})]"
  - [section 3.2.2] "function q(x_i, x^S) combines two components: the predictive improvement from adding feature i... and the epistemic uncertainty"
  - [corpus] Paper 21745 addresses dynamic feature selection to reduce measurement costs but uses different selection criteria
- Break condition: When proportionality assumption fails; when global model poorly calibrated; when neural estimator inaccurate for unseen feature combinations

### Mechanism 3: Dual-Path Epistemic Uncertainty Quantification
- Claim: Epistemic uncertainty from model adaptation across feature subsets can be quantified using ensemble disagreement (crisp rules) or membership-weighted rule divergence (fuzzy rules).
- Mechanism: For crisp rules (CART): train auxiliary classifiers, compute e_R(x^S) = (1/|I|) Σ_i D_KL(p_i^r*(ŷ|x^S) || p_r*(ŷ|x^S)). For fuzzy rules: use firing-weighted divergence e_R(x^S) = Σ_r μ_r(x^S) D_KL(p_r(ŷ|x^S) || p_r*(ŷ|x^S)) where μ_r is membership degree and r* is winning rule.
- Core assumption: Variation across models/rules correlates with epistemic uncertainty from adapting to arbitrary feature subsets S.
- Evidence anchors:
  - [section 3.2.2] "For crisp rules, we quantify epistemic uncertainty using auxiliary models... This approach involves computing the predictions of a set of auxiliary classifiers"
  - [section 3.2.2] "For fuzzy rule-based models... we define epistemic uncertainty as the membership-weighted divergence between all activated rules and the winning rule"
  - [corpus] Paper 33679 discusses fuzzy rough feature selection for uncertainty characterization; Paper 10098 uses fuzzy rules for diffusion guidance
- Break condition: When auxiliary models insufficiently diverse; when fuzzy membership poorly calibrated for intermediate values (Appendix C discussion)

## Foundational Learning

- **Concept: Aleatoric vs. Epistemic Uncertainty in Sequential Acquisition**
  - Why needed here: DFS introduces epistemic uncertainty from model adaptation across 2^M feature subsets—distinct from static settings. The paper formalizes how imputation bias and non-monotonic confidence affect uncertainty estimation.
  - Quick check question: Explain why acquiring more features can increase model uncertainty in DFS, and why entropy-based stopping criteria are unreliable.

- **Concept: KL Divergence for Distribution Comparison**
  - Why needed here: The policy uses D_KL(p(ŷ|x) || p(ŷ|x^S)) to measure prediction quality. Understanding asymmetry and interpretation is essential.
  - Quick check question: What does D_KL(p||q) → 0 indicate? Why might minimizing D_KL(global||subset) differ from minimizing D_KL(subset||global)?

- **Concept: Rule-Based Classifier Structure (Antecedent → Consequent)**
  - Why needed here: The adaptation mechanism requires understanding how conjunctive conditions combine and how confidence is computed from rule support/coverage.
  - Quick check question: Given rule "IF x_1 > 5 AND x_2 < 10 THEN class A" with confidence 0.8, how should prediction change if x_2 is unobserved and only x_1 > 5 is known?

## Architecture Onboarding

- **Component map:**
  1. **Global Rule Model**: Pre-trained CART/FuzzyCART on full features (max depth 10, min 5 samples/split)
  2. **Sub-Rule Cache**: Pre-computed confidences for all rule antecedents (length ≤4)
  3. **Rule Adapter**: Runtime condition removal and confidence lookup
  4. **Neural Value Estimator**: 2-layer MLP (hidden: min(64, 2×features)), SELU+dropout(0.5), two heads predicting Δu and Δe_R
  5. **Greedy Selector**: Iteratively queries argmax_i v_q(i, x^S)/cost_i until budget exhausted
  6. **Uncertainty Module**: Ensemble classifiers (crisp) or firing-strength calculator (fuzzy)

- **Critical path:**
  1. Train global model → 2. Extract rules and cache all sub-rule confidences → 3. Generate training pairs (x^S, u_target, e_R_target) via simulation → 4. Train neural estimator (100 epochs comparison, 10 epochs deployment) → 5. At inference: start with empty S, iteratively adapt rules → estimate v_q → query max feature → repeat

- **Design tradeoffs:**
  - CART vs. FuzzyCART backbone: CART achieves 69.43% accuracy (best rank 3.3); FuzzyCART 65.91% but smoother uncertainty curves
  - λ parameter: Higher values prioritize low-uncertainty subsets over potentially informative features; paper uses L1 regularization grid [0.0, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0]
  - Condition removal vs. imputation: Paper argues removal avoids bias from mean/zero imputation (Section 2.2)
  - Neural estimator complexity: 2-layer MLP vs. deeper networks—shallower chosen for faster inference

- **Failure signatures:**
  - **Accuracy degradation at high budget**: FuzzyCART on Diabetes peaks at 35% budget (60.66%) then drops to 56.45% at 100% (Figure 1)
  - **Poor calibration despite confidence**: ECE shows no correlation with budget; models "become more confident without necessarily becoming more reliable" (Section 4.2)
  - **Selector redundancy**: DIME shows ~0 gain from trained policy vs. random selection (Figure 4)—robustness from base model, not policy
  - **Non-monotonic entropy**: Confidence decreases regardless of feature utility (random vs. learned selection show similar patterns)

- **First 3 experiments:**
  1. **Sanity check on Heart dataset (13 features, 303 samples)**: Train CART-DFS with budget=5, compare against Static CMI. Expected: CART-DFS achieves ~78-80% accuracy, matching or exceeding baselines. Monitor rule complexity and entropy reduction curve.
  2. **λ sensitivity analysis on Cirrhosis (17 features)**: Test λ ∈ {0, 0.5, 1.0, 2.0} with variable costs [1,4]. Track accuracy vs. epistemic uncertainty tradeoff. Expected: Higher λ reduces e_R but may sacrifice accuracy on informative features.
  3. **Calibration audit on Wine (13 features, 178 samples)**: Compute ECE at 20%/50%/80%/100% budget for CART-DFS vs. DIME. Expected: No clear calibration improvement with budget; confirm paper's finding that "uncertainty reduction is primarily driven by information accumulation rather than selection strategy."

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can robust stopping criteria be developed for DFS when prediction entropy is demonstrably unreliable?
- **Basis in paper:** [explicit] Section 2.3 states that using "metrics of prediction entropy or thresholds in class scores to stop querying is not a reliable policy" because confidence often increases even when querying non-informative features.
- **Why unresolved:** The paper shows theoretically and empirically that entropy reduction is driven by information accumulation regardless of feature value, decoupling confidence from actual utility.
- **What evidence would resolve it:** A stopping metric that correlates strictly with ground-truth risk reduction rather than model confidence.

### Open Question 2
- **Question:** How can DFS frameworks be modified to ensure that increased confidence leads to improved calibration?
- **Basis in paper:** [explicit] The Conclusion highlights that "DFS models become more confident without necessarily becoming more reliable," noting that ECE did not correlate with budget size in experiments.
- **Why unresolved:** Current methods successfully reduce predictive entropy but fail to align confidence probabilities with actual accuracy, posing risks for safety-critical applications.
- **What evidence would resolve it:** A training objective that enforces monotonic calibration improvements as the feature budget increases.

### Open Question 3
- **Question:** Under what conditions does a learned feature selection policy provide significant benefits over training a robust base model?
- **Basis in paper:** [inferred] The Conclusion discusses the "redundancy" of the feature selector in methods like DIME, where performance was attributed to base model robustness rather than the selection policy.
- **Why unresolved:** It remains unclear if the computational cost of complex selection policies is justified when a robust model can perform equally well on arbitrary subsets.
- **What evidence would resolve it:** A theoretical or empirical characterization of the "selector utility gap" relative to model capacity and dataset noise.

## Limitations
- Rule adaptation breaks down for high-complexity datasets where specific feature interactions are discriminatively critical
- Neural estimator performance depends heavily on training data coverage across feature subsets
- Fundamental calibration issue: prediction entropy decreases with feature acquisition regardless of selection strategy, decoupling confidence from reliability
- Framework tested only on 5 tabular datasets with relatively low dimensionality

## Confidence
- Rule adaptation mechanism (Medium): Supported by paper methodology but limited empirical validation on diverse rule structures
- Epistemic uncertainty quantification (High): Clear theoretical foundation and implementation details provided
- Performance claims (Medium): Competitive results shown but narrow dataset scope (5 tabular datasets)
- Calibration findings (High): Well-documented but concerning limitation

## Next Checks
1. **Robustness to Rule Complexity**: Test adaptation performance on datasets requiring deeper trees (depth >10) to identify breaking points where condition removal fails
2. **Calibration Improvement Strategies**: Evaluate whether ensembling subset-adapted models or incorporating temperature scaling improves ECE without sacrificing accuracy
3. **Generalization to High-Dimensional Data**: Validate framework on datasets with 50+ features to assess neural estimator scalability and rule cache memory constraints