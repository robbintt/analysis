---
ver: rpa2
title: 'Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory'
arxiv_id: '2504.07952'
source_url: https://arxiv.org/abs/2504.07952
tags:
- memory
- cheatsheet
- learning
- aime
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamic Cheatsheet (DC) is a lightweight framework that gives black-box
  language models a persistent, evolving memory for test-time learning. Unlike static
  retrieval or fine-tuning, DC lets models store and reuse problem-solving strategies,
  code snippets, and heuristics at inference time without modifying their weights.
---

# Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory

## Quick Facts
- arXiv ID: 2504.07952
- Source URL: https://arxiv.org/abs/2504.07952
- Reference count: 40
- Primary result: Lightweight framework enables black-box LMs to improve reasoning accuracy at inference time via persistent adaptive memory

## Executive Summary
Dynamic Cheatsheet (DC) is a lightweight framework that enables black-box language models to perform test-time learning by maintaining a persistent, evolving memory of problem-solving strategies, code snippets, and heuristics. Unlike static retrieval or fine-tuning, DC allows models to store and reuse effective approaches during inference without modifying their weights. The framework demonstrated significant performance improvements across challenging reasoning tasks, with Claude 3.5 Sonnet's accuracy more than doubling on AIME math exams and GPT-4o achieving 99% success on Game of 24 after discovering and reusing a Python brute-force solver.

## Method Summary
DC operates by maintaining a persistent memory store that grows as the model encounters and solves problems during inference. The framework curates and stores only concise, transferable knowledge rather than raw conversation transcripts, focusing on effective strategies and validated solutions. At each inference step, the model can retrieve relevant stored knowledge to apply to new problems. The approach is particularly effective for tasks prone to arithmetic mistakes or requiring repeated application of similar strategies, as models can recall previously validated code or methods. DC's memory-centric design allows continuous refinement of problem-solving capabilities without requiring model retraining or fine-tuning.

## Key Results
- Claude 3.5 Sonnet's accuracy more than doubled on AIME math exams
- GPT-4o's success rate on Game of 24 jumped from 10% to 99% after discovering and reusing a Python brute-force solver
- Near-perfect accuracy achieved on equation balancing tasks through recall of validated code
- GPQA-Diamond accuracy increased by 9% and MMLU-Pro by 8%

## Why This Works (Mechanism)
DC leverages the observation that language models often make repeated mistakes on similar problems during inference. By maintaining a persistent memory of successful strategies and validated solutions, the framework allows models to learn from their own problem-solving experiences in real-time. The curated approach focuses on storing transferable knowledge rather than raw conversations, enabling models to avoid repetitive errors and continuously refine their approaches. The framework's lightweight design means it can be applied to black-box models without requiring access to model weights or extensive retraining infrastructure.

## Foundational Learning
- **Test-time learning**: Models improve during inference by learning from problem-solving experiences
  - Why needed: Traditional inference doesn't allow models to adapt or improve based on encountered problems
  - Quick check: Can the model solve a new problem using previously stored strategies?
- **Memory-based reasoning**: Persistent storage of effective problem-solving approaches
  - Why needed: Models often forget successful strategies between different inference sessions
  - Quick check: Does the model retrieve relevant stored knowledge when encountering similar problems?
- **Knowledge curation**: Selective storage of concise, transferable knowledge over raw transcripts
  - Why needed: Raw conversations are noisy and contain many irrelevant details
  - Quick check: Are stored entries focused on actionable strategies rather than conversational context?

## Architecture Onboarding

### Component Map
Memory Store -> Retrieval Engine -> Model Input -> Solution Generator -> Knowledge Curator -> Memory Store

### Critical Path
1. Problem input arrives at model
2. Retrieval engine searches memory for relevant stored strategies.
3. Retrieved knowledge augments model input.
4. Model generates solution.
5. Knowledge curator evaluates and stores effective strategies.
6. Updated memory store available for future queries.

### Design Tradeoffs
- Memory size vs. retrieval efficiency: Larger memories provide more coverage but slower retrieval
- Curation strictness vs. storage growth: Strict curation maintains quality but may miss useful patterns
- Retrieval scope vs. relevance: Broader searches find more potential matches but increase noise

### Failure Signatures
- Poor retrieval: Model receives irrelevant stored knowledge, leading to confusion
- Inadequate curation: Memory becomes cluttered with ineffective strategies
- Memory bloat: Retrieval slows significantly as memory grows without pruning

### First 3 Experiments
1. Baseline: Model performance without any memory assistance
2. Static retrieval: Model uses fixed set of example solutions
3. DC adaptive: Model uses evolving memory with continuous curation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hierarchical or modular memory architectures improve DC's effectiveness compared to a unified memory store?
- Basis in paper: The authors state specialized domains may benefit from subdividing or hierarchically organizing memory to reduce load on a unified memory store and isolate errors within respective domains.
- Why unresolved: Only single unified memory was tested; no hierarchical variants were evaluated.
- What evidence would resolve it: Experiments comparing unified vs. hierarchical memory on multi-domain benchmarks, measuring accuracy and error propagation rates.

### Open Question 2
- Question: Does curriculum-style ordering of test examples significantly accelerate DC's test-time learning and final performance?
- Basis in paper: The authors note curriculum-style learning may bootstrap performance and that tasks arranged to present related questions early may accelerate and improve the model's test-time learning.
- Why unresolved: All experiments used fixed dataset orderings; no curriculum-based experiments were conducted.
- What evidence would resolve it: Controlled experiments with randomized vs. curriculum-ordered test sequences on tasks like AIME, measuring learning curve speed and peak accuracy.

### Open Question 3
- Question: Can more sophisticated retrieval methods reduce retrieval-induced errors in DC-RS?
- Basis in paper: The authors report poorly filtered retrieval mechanisms can introduce confusion and that GPT-4o's performance occasionally dipped in GPQA-Diamond due to suboptimal retrieval choices.
- Why unresolved: Only simple cosine similarity retrieval with k=3 was tested; no alternative retrieval architectures were evaluated.
- What evidence would resolve it: Ablation studies comparing different retrieval methods on diverse benchmarks, tracking accuracy changes and false-positive retrieval rates.

### Open Question 4
- Question: What minimum base model capacity is required for DC to provide meaningful gains, and can auxiliary mechanisms compensate for limited generative ability?
- Basis in paper: Smaller models benefit minimally from DC, struggling to refine stored content and generate too few correct solutions, leaving memory populated with flawed strategies.
- Why unresolved: Only four model sizes were tested; no systematic capacity analysis or auxiliary mechanism experiments were conducted.
- What evidence would resolve it: Systematic experiments across model scales with and without external verification modules, correlating base model accuracy with DC improvement magnitude.

## Limitations

- Task-specific performance gains may not generalize to open-ended or creative tasks
- Memory curation overhead could limit scalability in practical applications
- No detailed analysis of how retrieval performance scales with memory size
- Lacks direct comparisons with other test-time learning or memory-augmented approaches

## Confidence

- High confidence in reported accuracy gains within tested reasoning and knowledge-intensive domains
- Medium confidence in efficiency claims pending clarification of curation process
- Low confidence in long-term scalability without additional technical details on memory management
- Medium confidence in relative advantage over baselines without direct comparison to alternative approaches

## Next Checks

1. **Cross-Domain Generalization**: Evaluate DC on open-ended generation, multi-modal reasoning, and long-term memory tasks to test adaptability limits.

2. **Memory Scalability and Retrieval Efficiency**: Measure how retrieval accuracy, memory insertion time, and inference latency scale as memory grows, including ablation studies on memory size impact.

3. **Ablation on Memory Curation Strategy**: Compare DC's performance with automatically vs. manually curated memories, and assess different memory formats on downstream task accuracy.