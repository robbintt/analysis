---
ver: rpa2
title: An extension of linear self-attention for in-context learning
arxiv_id: '2503.23814'
source_url: https://arxiv.org/abs/2503.23814
tags:
- matrix
- which
- linear
- have
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends linear self-attention to better support in-context
  learning by introducing bias matrices in addition to weight matrices. The extended
  linear self-attention (ELSA) can output any constant matrix, the input matrix (enabling
  skip connections), and multiplications of two or three matrices in the input.
---

# An extension of linear self-attention for in-context learning

## Quick Facts
- arXiv ID: 2503.23814
- Source URL: https://arxiv.org/abs/2503.23814
- Authors: Katsuyuki Hagiwara
- Reference count: 40
- This paper extends linear self-attention to better support in-context learning by introducing bias matrices in addition to weight matrices.

## Executive Summary
This paper proposes Extended Linear Self-Attention (ELSA), an enhancement to linear self-attention that incorporates bias matrices alongside weight matrices. ELSA can output constant matrices, pass input through directly (skip connections), and perform flexible multiplications of two or three matrices in the input. These capabilities address limitations of standard linear self-attention, which can only compute products involving H^TH. The paper demonstrates ELSA's utility by heuristically constructing a batch-type gradient descent algorithm for ridge regression, showing that ELSA can implement matrix manipulations required for in-context learning beyond what standard self-attention provides.

## Method Summary
ELSA extends linear self-attention by computing (HW₃ + B₃)(HW₁ + B₁)^T(HW₂ + B₂) instead of the standard (HW₃)(HW₁)^T(HW₂). This addition of bias matrices enables three fundamental operations: constant matrix output (by setting weights to zero and configuring biases), skip connections (by passing input through), and flexible two-matrix multiplication (by extracting matrices from H via the mask-and-move operation on H^TH). The paper applies this to implement a batch-type gradient descent algorithm for ridge regression by structuring the input as a block matrix where variables occupy specific positions, then using ELSA heads to extract and combine gradient terms iteratively.

## Key Results
- ELSA can output any constant matrix, input matrix (enabling skip connections), and multiplications of two or three matrices in the input
- The extended formulation relaxes input formatting requirements compared to standard linear self-attention for algorithmic tasks
- ELSA successfully implements batch-type gradient descent for ridge regression using a naturally structured input form

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adding bias matrices to linear self-attention enables three fundamental operations: constant matrix output, skip connections, and flexible two-matrix multiplication.
- **Mechanism:** Standard LSA computes (HW₃)(HW₁)^T(HW₂), which restricts computation to products involving H^TH. ELSA computes (HW₃ + B₃)(HW₁ + B₁)^T(HW₂ + B₂). By setting weights to zero and biases appropriately, ELSA can: (1) output constant C via B₃B₁^TB₂ = C; (2) pass input through via HW₃B₁^TB₂ = H; (3) compute AB by extracting A^T and B from H and multiplying via the MskMov operation on H^TH.
- **Core assumption:** The target computations can be decomposed into operations extractable via the mask-and-move primitive on H^TH or direct bias-based injection.
- **Evidence anchors:**
  - [abstract] "ELSA can output any constant matrix, input matrix and multiplications of two or three matrices in the input"
  - [Section 2.4, Properties 2-4] Formal constructions for constant output, skip connection, and AB multiplication
  - [corpus] Related work [1, 2, 5, 15] show LSA can implement gradient descent but requires specific input formatting; ELSA relaxes this
- **Break condition:** If task requires more than three-matrix products in a single ELSA layer, or requires operations not expressible as masked submatrix extractions from H^TH, mechanism fails.

### Mechanism 2
- **Claim:** The mask-and-move operation enables submatrix extraction and repositioning, which is essential for accessing task-relevant data structures within the unified input matrix.
- **Mechanism:** Given input H containing multiple data blocks (e.g., X^T, y, w, λ), MskMov_{i,j,k,l}^{a,b}(A) = WAV extracts submatrix A[i:j, k:l] and places it at position [i+a:j+a, k+b:l+b] in a zero matrix. This is implemented via sparse W and V matrices with 1s at specific (row, col) index pairs.
- **Core assumption:** Input data can be organized in a block matrix format where relevant correlations appear as submatrices of H^TH.
- **Evidence anchors:**
  - [Section 2.2, Property 1] Formal definition and proof of MskMov = WAV
  - [Section 2.3] "MskMov helps for extracting submatrices of H^TH, which may represent correlation structures among variables"
  - [corpus] Neighbor papers on linear attention variants do not explicitly address this extraction bottleneck
- **Break condition:** If input format cannot be designed to place correlated quantities in H^TH submatrices, or if dynamic reconfiguration is needed beyond fixed W, V patterns.

### Mechanism 3
- **Claim:** Stacking ELSA modules with skip connections enables iterative algorithms like gradient descent by accumulating updates across layers.
- **Mechanism:** Each module M_t computes H_t = P_t + H_{t-1} where P_t contains the gradient update term -ηΔw_{t-1}. For ridge regression, Δw = -X^Ty + X^TXw + λw. ELSA heads extract X^TXw, λw, and X^Ty via different (W, B) configurations, sum them, and add via skip connection. T iterations approximate the ridge solution.
- **Core assumption:** Assumption: The gradient descent trajectory converges within T steps; weight matrices can be shared across all t=1,...,T (parameter tying).
- **Evidence anchors:**
  - [Section 3.4] Full LSA implementation with H_{t-1} form and update equations (35)-(36)
  - [Section 3.5] ELSA implementation using two sequential multi-head blocks per iteration
  - [corpus] Papers [1, 14, 15] establish gradient descent ↔ attention correspondence; [22] shows trained transformers learn linear functions
- **Break condition:** If algorithm requires non-linear operations (e.g., exact division for closed-form solutions), or if weight sharing across iterations is inappropriate for the task.

## Foundational Learning

- **Concept: Ridge Regression and Gradient Descent**
  - **Why needed here:** The paper's demonstration requires understanding how ridge regression works (X^TX + λI)^{-1}X^Ty and how gradient descent approximates this iteratively.
  - **Quick check question:** Can you derive the gradient ∂ℓ(w)/∂w for ℓ(w) = (1/2)||y - Xw||² + (λ/2)||w||²?

- **Concept: Linear Self-Attention Formulation**
  - **Why needed here:** ELSA is defined as an extension of LSA; understanding LSA(H) = (HW₃)(W₁^TH^THW₂) is prerequisite.
  - **Quick check question:** Explain why LSA inherently computes products involving H^TH and why this is restrictive.

- **Concept: Block Matrix Input Design**
  - **Why needed here:** Both LSA and ELSA implementations require carefully structured input matrices where variables occupy specific blocks.
  - **Quick check question:** For H = [X^T Y; 0 λI; √ηI u w], what does H^TH[1:d, 1:d] represent?

## Architecture Onboarding

- **Component map:**
  - Input Layer -> ELSA Module (2 sequential multi-head blocks) -> Skip Connection -> Iteration Stack (T copies) -> Output Module

- **Critical path:**
  1. Design input format → determines what appears in H^TH submatrices
  2. Configure W₁, W₂ for MskMov extraction of needed terms (X^TXw, λw, X^Ty)
  3. Configure W₃, B₃ for combining extracted terms into update vector
  4. Stack T modules with skip connections accumulating w updates
  5. Final extraction of prediction via output module

- **Design tradeoffs:**
  - **Input format vs. module complexity:** Section 3.4 LSA requires √η scaling embedded in input; Section 3.5 ELSA accepts "natural" enumeration but needs two sequential ELSA blocks per iteration
  - **LSA vs. ELSA:** LSA is simpler (no biases) but requires more artificial input formatting; ELSA is more flexible but has 2× parameters (weights + biases)
  - **Fixed vs. learned weights:** Paper uses heuristically constructed weights; whether these emerge from training is untested

- **Failure signatures:**
  - Prediction doesn't update across layers → check skip connection implementation (B matrices may not satisfy Property 3 conditions)
  - Wrong update direction → verify MskMov indices extract correct H^TH submatrices
  - Gradient explosion/vanishing → check η scaling and λ regularization placement in input

- **First 3 experiments:**
  1. **Validate ELSA primitives:** Construct single ELSA heads for constant output, identity (skip), and AB multiplication; verify against direct computation on random matrices.
  2. **Single-step gradient check:** Implement one M_t module for ridge regression; compare computed Δw against analytical gradient on synthetic (X, y, w).
  3. **Convergence study:** Stack T=1,5,10,20 modules; plot ||w_T - ŵ_λ|| vs. T to verify gradient descent trajectory matches expected convergence rate for varying condition numbers of X^TX.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper demonstrates ELSA's capabilities heuristically but does not empirically validate whether these constructions emerge from training transformers on relevant tasks
- The flexibility of ELSA over standard LSA for in-context learning remains speculative without empirical evidence from trained models
- Fixed weight/bias constructions rely heavily on specific input formatting and may be brittle to data distribution variations

## Confidence

- **High Confidence:** The mathematical validity of ELSA's extended operations (constant output, skip connections, two-matrix multiplication) is sound and well-supported by formal proofs in Properties 2-4. The MskMov mechanism for submatrix extraction is rigorously defined and proven.
- **Medium Confidence:** The equivalence between gradient descent and attention mechanisms, established in related work, provides a solid foundation for the ridge regression application. The heuristic construction for implementing gradient descent with ELSA is logically consistent with this literature.
- **Low Confidence:** The claim about ELSA's broader importance for in-context learning beyond language modeling lacks empirical support. Whether the flexibility of ELSA over standard LSA translates to practical performance gains in trained models remains an open question.

## Next Checks

1. **Empirical Training Validation:** Train transformer models with ELSA attention on tasks requiring algorithmic reasoning (e.g., synthetic algorithmic datasets, or modified benchmarks like listOps or SCAN). Compare performance against standard LSA transformers to verify if ELSA's additional flexibility leads to measurable gains in in-context learning ability.

2. **Gradient Descent Convergence Analysis:** Implement the ELSA ridge regression construction and empirically verify that the sequence of weight updates {w_t} converges to the true ridge solution ŵ_λ = (X^TX + λI)^{-1}X^Ty. Measure convergence rate across different condition numbers of X^TX and compare against theoretical gradient descent bounds.

3. **Ablation on Input Formatting Requirements:** Systematically vary the block matrix input structure while keeping the ELSA module fixed. Test whether the heuristic weight/bias constructions remain effective under perturbations to the input format, or if they are overly brittle to changes in data organization.