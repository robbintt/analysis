---
ver: rpa2
title: Automated Detection of Clinical Entities in Lung and Breast Cancer Reports
  Using NLP Techniques
arxiv_id: '2505.09794'
source_url: https://arxiv.org/abs/2505.09794
tags:
- cancer
- clinical
- entities
- validation
- lung
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates the application of Named Entity Recognition\
  \ (NER) using a fine-tuned RoBERTa-based model (bsc-bio-ehr-en3) for automated extraction\
  \ of clinical entities from lung and breast cancer reports. The methodology leveraged\
  \ GMV\u2019s NLP tool uQuery, integrating a text pre-processing layer to enhance\
  \ entity detection."
---

# Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques

## Quick Facts
- arXiv ID: 2505.09794
- Source URL: https://arxiv.org/abs/2505.09794
- Reference count: 12
- Primary result: Fine-tuned RoBERTa-based model achieves 95.91% accuracy and F1-scores of 0.7489–0.7684 for NER on Spanish clinical cancer reports

## Executive Summary
This study presents an automated system for Named Entity Recognition (NER) in lung and breast cancer clinical reports using a fine-tuned RoBERTa-based model (bsc-bio-ehr-en3) integrated with a text pre-processing layer. The methodology, implemented through GMV's NLP tool uQuery, standardizes and cleans clinical text to improve entity detection accuracy. Using a dataset of 600 annotated reports from IIS La Fe, the system achieved strong overall performance with high precision and recall for most entity types, particularly for method of diagnosis (MET) and pathology (PAT). The approach demonstrates the effectiveness of combining domain-specific pre-training with careful text pre-processing for clinical NLP tasks.

## Method Summary
The study employed a fine-tuned bsc-bio-ehr-en3 RoBERTa model for NER, using 600 manually annotated Spanish clinical reports (200 breast cancer, 400 lung cancer) from IIS La Fe. The model was configured with 9 labels (8 clinical entities plus 'O') and trained using Hugging Face Transformers. A key innovation was the integration of a text pre-processing layer (uQuery) that standardized clinical text before model application. The dataset was split 50% train, 25% validation, 25% test, and performance was evaluated using accuracy, precision, recall, and F1-score metrics.

## Key Results
- Overall accuracy reached 95.91% with F1-scores of 0.7489–0.7684 across validation and test sets
- Model excelled at identifying common entities like MET (method of diagnosis) and PAT (pathology) with high F1-scores
- EVOL (evolution) entity posed significant challenges, achieving F1=0 in breast cancer validation due to absence from training data
- Pre-processing layer significantly improved entity detection accuracy by standardizing clinical text formatting and handling domain-specific notations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A text pre-processing layer substantially improves entity detection accuracy in clinical NER
- Mechanism: Pre-processing standardizes clinical text (cleaning abbreviations, normalizing formatting, handling pTNM staging notations) before the model sees it, enabling the tokenizer to produce more meaningful subword splits and the model to match learned patterns
- Core assumption: Clinical text noise (inconsistent formatting, domain-specific abbreviations) is a primary cause of missed entities; removing it does not discard discriminative signals
- Evidence anchors:
  - [abstract] "A key contribution was the text pre-processing layer, which significantly improved entity detection accuracy by standardizing and cleaning clinical text before model application."
  - [section 4.1.2] Figures 2 and 3 show that raw text misses entities (e.g., pTNM staging) while pre-processed text captures all relevant entities including partial entities like "Mama derecha."
  - [corpus] Limited direct corpus evidence on pre-processing effectiveness; related work focuses on model architectures rather than pre-processing pipelines
- Break condition: If pre-processing removes or distorts token patterns the model was trained on (e.g., aggressive normalization of medical abbreviations), entity detection may degrade

### Mechanism 2
- Claim: Fine-tuning a domain-specific, language-matched pre-trained model (bsc-bio-ehr-en3) enables effective transfer to clinical NER tasks
- Mechanism: The RoBERTa-based model, pre-trained on Spanish biomedical text, already encodes linguistic patterns and domain vocabulary. Fine-tuning on labeled cancer reports adjusts the token classification head to map contextual embeddings to the 8 entity types
- Core assumption: The pre-training distribution overlaps sufficiently with clinical report language; the labeled fine-tuning set is representative of target documents
- Evidence anchors:
  - [abstract] "fine-tuned the bsc-bio-ehr-en3 RoBERTa model on a Spanish biomedical dataset... achieving strong performance with overall accuracy of 95-94%, F1-scores of 0.75-0.77"
  - [section 3.3] "The model used for this task is bsc-bio-ehr-en, a biomedical linguistic model based on the RoBERTa architecture, which is pre-trained in Spanish and suitable for medical records and EHRs."
  - [corpus] Similar approaches (Zhang et al., Paolo et al.) report success with BERT-based fine-tuning for clinical NER in other languages/domains, supporting transferability
- Break condition: If target reports contain terminology or dialect patterns absent from pre-training, fine-tuning may fail to generalize without additional domain adaptation

### Mechanism 3
- Claim: Entity detection performance is strongly conditioned on label frequency in the training set
- Mechanism: The model learns statistical patterns from labeled examples; frequent entities (MET, PAT) provide sufficient positive examples for robust classification, while rare entities (EVOL with 0 training instances in breast cancer set) leave the model unable to learn their patterns
- Core assumption: Label distribution in the annotated sample reflects true entity importance; class imbalance is not addressed through resampling or loss weighting
- Evidence anchors:
  - [abstract] "The model excelled at identifying common entities like MET and PAT, while less frequent entities like EVOL posed challenges."
  - [section 4.1, Table 1] EVOL had 0 instances in the training set, resulting in F1=0 in validation/test. MET had 1,237 training instances and achieved F1=0.83
  - [corpus] Corpus papers do not explicitly address class imbalance remediation strategies for clinical NER
- Break condition: If rare entities are critical for downstream use (e.g., EVOL for tracking patient outcomes), the system will produce systematic false negatives regardless of overall metrics

## Foundational Learning

- Concept: **Named Entity Recognition (NER)**
  - Why needed here: This is the core task—token-level classification assigning each span to one of 8 clinical entity types or "O" (outside)
  - Quick check question: Can you explain why NER is formulated as token classification rather than text classification?

- Concept: **Transformers and Fine-Tuning**
  - Why needed here: The model is RoBERTa-based; understanding self-attention, tokenization, and the fine-tuning paradigm is essential for debugging and extending the system
  - Quick check question: What happens to a pre-trained model's weights during fine-tuning versus during inference-only use?

- Concept: **Clinical Text Pre-Processing**
  - Why needed here: The paper identifies pre-processing as a key contribution; this includes normalization, abbreviation expansion, and structured field extraction before model input
  - Quick check question: Give one example where aggressive lowercasing could harm clinical NER performance

## Architecture Onboarding

- Component map:
  - Data Source (IIS La Fe EHR) → Pseudonymization → uQuery Pre-processing Layer → Tokenizer (AutoTokenizer) → Fine-tuned bsc-bio-ehr-en3 (AutoModelForTokenClassification) → NER Pipeline → Entity Output (with aggregation_strategy="average")

- Critical path:
  1. Pre-processing (without this, entities like pTNM are missed)
  2. Tokenization alignment with pre-trained model vocabulary
  3. Fine-tuning data quality (Doccano annotations must be consistent)
  4. Inference pipeline configuration (aggregation strategy affects span boundaries)

- Design tradeoffs:
  - Pre-processing aggressiveness: More cleaning improves detection but risks removing informative variation
  - Training set size vs. annotation cost: 600 labeled reports achieved acceptable performance; expanding annotations for rare entities (EVOL) would likely improve recall but requires expert time
  - Language specificity: Using a Spanish biomedical pre-trained model improves performance but limits transfer to other languages

- Failure signatures:
  - F1=0 for an entity class → Check training label count; if zero, the model cannot learn that class
  - Overshoot (model predicts more entities than labeled) → May indicate overfitting or pre-processing artifacts; verify with manual review
  - Inconsistent span boundaries → Check aggregation_strategy and tokenizer settings

- First 3 experiments:
  1. **Ablate pre-processing**: Run the same model on raw vs. pre-processed text; quantify F1 delta per entity type to validate the pre-processing contribution
  2. **Address class imbalance**: For EVOL, add oversampling or class-weighted loss; measure recall improvement without degrading precision on frequent entities
  3. **Cross-cancer validation**: Train on breast cancer only, test on lung cancer (and vice versa) to assess domain transfer and identify entity-specific generalization gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the detection of underrepresented clinical entities, such as EVOL (Evolution), be improved in imbalanced datasets?
- Basis in paper: [explicit] The authors state that "challenges were encountered with less frequent entities, such as EVOL" and that "further refinement is necessary to improve detection accuracy for less frequent labels."
- Why unresolved: The model yielded zero F1-scores for EVOL in breast cancer validation because the training set contained zero instances of this entity, and it scored low in the combined dataset
- What evidence would resolve it: Results from experiments utilizing data augmentation, oversampling techniques, or few-shot learning approaches that demonstrate a non-zero recall for the EVOL entity

### Open Question 2
- Question: What is the quantitative contribution of the uQuery text pre-processing layer to the model's performance compared to raw text?
- Basis in paper: [inferred] The authors claim the pre-processing layer "significantly improved the accuracy" and show visual examples (Figures 2 and 3), but they do not provide a comparative metrics table (e.g., F1-scores) isolating the model's performance on raw versus pre-processed text
- Why unresolved: While the final results are reported, the specific performance gain attributable solely to the pre-processing cleaning step versus the RoBERTa fine-tuning remains unquantified in the results section
- What evidence would resolve it: An ablation study reporting F1-scores and precision for the model run on raw text versus pre-processed text using the same test set

### Open Question 3
- Question: Can the fine-tuned model generalize effectively to Spanish clinical reports from different hospital systems with distinct documentation practices?
- Basis in paper: [inferred] The study uses a dataset from a single center (IIS La Fe) and notes that extracting insights is difficult due to "unique documentation practices."
- Why unresolved: The model was trained and tested exclusively on data from the La Fe Health Department (EMRAM stage 6), potentially overfitting to the specific structure and terminology of that institution's EHRs
- What evidence would resolve it: Evaluation of the model's accuracy and recall when applied to an external validation dataset from a different Spanish hospital

## Limitations

- Dataset access restricted: The 600 annotated clinical reports from IIS La Fe are not publicly available, preventing direct validation of absolute performance metrics
- Pre-processing opacity: While critical to performance, the specific text cleaning and normalization rules in the uQuery pre-processing layer are not detailed, limiting reproducibility
- Class imbalance unaddressed: The model achieved strong results for frequent entities but failed entirely on rare entities like EVOL (F1=0) due to absence from the training set, with no reported mitigation strategies

## Confidence

- **High confidence** in the mechanism that pre-processing substantially improves entity detection accuracy. This is directly supported by comparative results showing raw text misses entities that pre-processed text captures
- **Medium confidence** in the transfer learning approach using bsc-bio-ehr-en3. While the pre-training on Spanish biomedical text is documented and supported by similar approaches, the specific performance gains relative to alternative architectures are not benchmarked
- **Medium confidence** in the overall accuracy metrics (95.91% accuracy, F1-scores 0.7489-0.7684). These are internally validated on the same hospital's data but cannot be externally verified due to data access restrictions

## Next Checks

1. **Pre-processing ablation study**: Run the model on identical raw and pre-processed text to quantify the exact F1 improvement per entity type, validating the pre-processing contribution
2. **Class imbalance remediation**: For entities like EVOL, implement oversampling or class-weighted loss, then measure recall improvement without degrading precision on frequent entities
3. **Cross-cancer domain transfer**: Train on breast cancer reports only, then test on lung cancer reports (and vice versa) to assess whether entity detection generalizes across cancer types or requires cancer-specific adaptation