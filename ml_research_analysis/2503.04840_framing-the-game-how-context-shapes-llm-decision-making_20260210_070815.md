---
ver: rpa2
title: 'Framing the Game: How Context Shapes LLM Decision-Making'
arxiv_id: '2503.04840'
source_url: https://arxiv.org/abs/2503.04840
tags:
- decision
- game
- decision-making
- both
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a procedurally generated evaluation framework
  for analyzing Large Language Model (LLM) decision-making in game-theoretic scenarios,
  focusing on the Prisoner's Dilemma. The method dynamically generates diverse vignettes
  by varying topics, world types, and actor relationships, then systematically evaluates
  LLM choices across these contexts.
---

# Framing the Game: How Context Shapes LLM Decision-Making

## Quick Facts
- arXiv ID: 2503.04840
- Source URL: https://arxiv.org/abs/2503.04840
- Reference count: 36
- Key outcome: A procedurally generated framework reveals that LLM decision-making in game-theoretic scenarios is highly sensitive to contextual framing, even when models recognize underlying game structures.

## Executive Summary
This paper introduces a novel evaluation framework that systematically examines how context influences LLM decision-making in game-theoretic scenarios, specifically the Prisoner's Dilemma. The framework generates diverse vignettes by varying topics, world types, and actor relationships, then evaluates LLM choices across these contexts. The study reveals that LLMs exhibit significant context-dependent variability in cooperation/defection decisions, with patterns largely predictable but still showing inherent stochasticity. This approach offers a more robust methodology for real-world LLM deployment by accounting for contextual variability in decision-making.

## Method Summary
The framework procedurally generates vignettes for Prisoner's Dilemma scenarios by combining topic categories (sports, politics, technology), world types (real, fantasy, historical), and relationship types (friends, enemies, strangers). Each vignette maintains the core payoff structure while varying contextual details. The evaluation uses zero-shot prompting to assess LLM choices across systematically varied contexts. The framework employs four prompting strategies: direct choice, payoff explanation, role-play, and mixed. Results are analyzed for both choice selection and choice explanations, with inter-rater reliability established for evaluating explanations.

## Key Results
- LLMs show significant context-dependent variability in cooperation/defection decisions, with predictable patterns but inherent stochasticity
- Framework effectively mitigates benchmark contamination risks through dynamic vignette generation
- Even when models recognize game structures, their choices remain highly sensitive to framing effects
- Claude-3 demonstrated notably different behavior patterns compared to GPT-4, suggesting model-specific contextual sensitivities

## Why This Works (Mechanism)
The framework's effectiveness stems from its systematic approach to isolating and varying contextual elements while maintaining consistent game-theoretic structures. By procedurally generating vignettes, the method creates a controlled environment to study how different contextual cues influence decision-making. The zero-shot prompting approach ensures that evaluations reflect genuine contextual influences rather than learned patterns from training data.

## Foundational Learning
- **Game-theoretic decision-making**: Understanding strategic interactions is crucial for evaluating LLM behavior in structured scenarios
- **Contextual framing effects**: Recognizing how environmental factors influence decision-making is essential for real-world LLM deployment
- **Procedural content generation**: Creating diverse test scenarios helps mitigate benchmark contamination and ensures robust evaluation
- **Zero-shot prompting**: Testing models without prior exposure to specific scenarios provides more generalizable insights
- **Inter-rater reliability**: Establishing consistent evaluation criteria is critical for analyzing qualitative outputs

## Architecture Onboarding

**Component Map**: Vignette Generator -> Prompt Engine -> LLM Interface -> Response Analyzer -> Context Classifier

**Critical Path**: Procedural vignette generation → Prompt delivery → Response collection → Choice classification → Explanation analysis

**Design Tradeoffs**: Dynamic generation versus controlled consistency; comprehensive context coverage versus evaluation efficiency; zero-shot prompting versus potential performance limitations

**Failure Signatures**: Inconsistent choice patterns across similar contexts; failure to recognize game structure; explanation quality variations

**3 First Experiments**:
1. Generate 100 diverse vignettes and verify consistent game structure preservation
2. Test single-prompt consistency across 50 random vignettes
3. Validate inter-rater reliability on 30 response explanations

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Exclusive focus on Prisoner's Dilemma may not generalize to other game-theoretic scenarios
- Limited to two LLM models (GPT-4 and Claude-3), constraining generalizability
- Static model versions may miss performance variations across iterations
- English-language scenarios only, limiting multilingual applicability

## Confidence

- **High Confidence**: Context-dependent variability in LLM decision-making is well-supported by systematic evidence
- **Medium Confidence**: Claims about benchmark contamination mitigation require broader validation
- **Medium Confidence**: Real-world applicability predictions need additional empirical support

## Next Checks

1. Test the framework across multiple game-theoretic scenarios beyond Prisoner's Dilemma to assess generalizability
2. Evaluate additional LLM models, including open-source alternatives, to determine if observed patterns hold
3. Conduct longitudinal studies with evolving model versions to track contextual sensitivity changes