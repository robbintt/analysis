---
ver: rpa2
title: 'Echoing: Identity Failures when LLM Agents Talk to Each Other'
arxiv_id: '2511.09710'
source_url: https://arxiv.org/abs/2511.09710
tags:
- echoing
- agent
- hotel
- customer
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies echoing, a critical failure mode in agent-agent
  (AxA) interactions where LLMs abandon their assigned identities and mirror their
  conversational partners. Through experiments across 66 configurations, 3 transactional
  domains, and 2500+ conversations, the authors demonstrate echoing rates ranging
  from 5% to 70% across major LLM providers.
---

# Echoing: Identity Failures when LLM Agents Talk to Each Other

## Quick Facts
- arXiv ID: 2511.09710
- Source URL: https://arxiv.org/abs/2511.09710
- Authors: Sarath Shekkizhar; Romain Cosentino; Adam Earle; Silvio Savarese
- Reference count: 38
- Primary result: Echoing rates of 5-70% across major LLM providers in agent-agent interactions, persisting despite prompt engineering and reasoning effort

## Executive Summary
This paper identifies "echoing" as a critical failure mode in agent-agent interactions where LLMs abandon their assigned identities and mirror their conversational partners. Through experiments across 66 configurations, 3 transactional domains, and 2500+ conversations, the authors demonstrate echoing rates ranging from 5% to 70% across major LLM providers. Even advanced reasoning models show substantial echoing (32.8%) that persists regardless of reasoning effort levels. The phenomenon is domain-sensitive, with lower rates in advisory settings, and typically emerges after 7+ conversation turns. While prompt engineering and structured responses provide partial mitigation (reducing echoing to 9%), the persistence of echoing across different mitigation strategies suggests fundamental model limitations rather than implementation artifacts.

## Method Summary
The study simulates Agent-to-Agent (AxA) negotiations between Customer and Seller agents using system prompts defining identity, objectives, and private utilities. Agents interact in turn-based conversations (max 12 turns) with access to domain-specific tools while maintaining information asymmetry. Echoing is detected using a specialized LLM judge (EchoEvalLM) that analyzes conversation history for identity inconsistency. The experimental setup includes 66 configurations across 5 model families (GPT-4o, Gemini-2.5-Flash, Sonnet-4, Llama-3.1-8B, GPT-5) in hotel booking, car sales, and supply chain domains, with 2500+ conversations evaluated for echoing rates, task completion, and utility outcomes.

## Key Results
- Echoing rates range from 5% to 70% across different models and domains
- Advanced reasoning models show only modest improvement (32.8% echoing vs 37.7% for non-reasoning)
- Echoing typically emerges after 7+ conversation turns and increases with conversation length
- Structured responses reduce echoing to ~9% but constrain conversational flexibility
- 93% of conversations complete successfully despite echoing, making completion metrics unreliable

## Why This Works (Mechanism)

### Mechanism 1: Training Distribution Mismatch
- Claim: Models optimized for human-facing service roles fail to maintain identity when assigned consumer-facing roles in AxA settings
- Mechanism: Post-training alignment procedures over-represent enterprise/service-provider roles in training data. When deployed in AxA with inverted role assignments, learned priors override explicit system instructions
- Evidence: Advanced reasoning models still show 32.8% echoing; customer agents systematically exhibit higher echoing rates than seller agents
- Break condition: Fine-tuning on balanced AxA role distributions with identity-consistency objectives should substantially decrease echoing rates

### Mechanism 2: Contextual Role Inference
- Claim: Models perform contextual role inference from conversational patterns rather than strictly adhering to system prompt identity specifications
- Mechanism: LLMs process conversation history as implicit context for role identification. When partner agent's language patterns are strongly role-characteristic, the model infers its own role through complementary/imitative pattern matching
- Evidence: Explicit anti-echoing instructions fail to eliminate echoing in already susceptible models; models frequently infer role from conversational context rather than explicit instructions
- Break condition: Persistent architectural constraints (not prompt-based) should reduce contextual override

### Mechanism 3: Attention/Context Decay
- Claim: Echoing likelihood increases with conversation length due to attention/context decay that weakens initial identity specification salience
- Mechanism: As conversation history grows, relative attention weight on initial system prompts decreases while recent conversational context dominates. The model's implicit identity derived from recent turns progressively displaces the explicitly assigned role
- Evidence: Echoing typically emerges after 7+ conversation turns; conversations exhibiting echoing are longer (9.6 vs 8.7 turns) across all domains
- Break condition: Sustained elevated attention weights for identity-relevant tokens or periodic identity re-injection should flatten turn-dependent echoing

## Foundational Learning

- **Partially Observable Stochastic Games (POSGs)**: The paper formalizes AxA as a POSG where agents have private states and interact through natural language without access to each other's full state. Understanding POSG fundamentals (information asymmetry, turn-based interaction, private utility functions) is prerequisite to grasping why AxA differs from both single-agent and multi-agent coordination settings.
  - Quick check: Can you explain why AxA with private utilities differs fundamentally from multi-agent systems with shared objectives and centralized orchestration?

- **Alignment Training Distribution Effects**: Echoing prevalence is attributed to models being "optimized for human-facing use" with training data that over-represents service-provider roles. Understanding how RLHF/DPO creates implicit behavioral priors—and how these priors generalize (or fail to generalize) to out-of-distribution deployment contexts—is essential for interpreting echoing as a training artifact rather than a prompt design issue.
  - Quick check: If you fine-tuned a model exclusively on symmetrical role-switching conversations (50% buyer, 50% seller roles), would you expect echoing rates to increase, decrease, or stay the same for customer-agent assignments?

- **Reasoning Model Limitations in Identity Maintenance**: The paper shows reasoning models achieve only modest echoing reduction (32.8% vs 37.7% for non-reasoning), and reasoning effort levels show near-zero difference. Understanding what reasoning models actually do—chain-of-thought for task completion versus active identity monitoring—explains why increased reasoning compute doesn't address the root cause.
  - Quick check: What explicit identity-consistency checks would you need to add to a reasoning trace to detect role drift before it manifests in output?

## Architecture Onboarding

- **Component map**: Agent Configuration (Ii, Oi, Ti, Ui, πi) -> System Prompt Template -> Environment Layer -> EchoEvalLM -> Structured Response Format
- **Critical path**: Initialize two agents with distinct identities → turn-based interaction loop → tool execution → response generation → conversation history analysis → success/failure determination
- **Design tradeoffs**: 
  - Prompt complexity vs. effectiveness: Identity boundary prompts reduce echoing modestly but add token overhead
  - Structured responses vs. natural dialogue: Forces explicit role declaration each turn cuts echoing to ~9% but constrains flexibility
  - Reasoning effort vs. latency: High reasoning effort shows no echoing reduction over low effort but increases latency and cost
  - Evaluation depth vs. scalability: Manual review has 91.1% agreement with LLM judge but doesn't scale
- **Failure signatures**:
  - Early warning: Role-inappropriate language appearing around turn 7-8
  - Customer-specific: Customer agent using service-provider language ("I've reserved the room for you")
  - Seller echo: Less common but occurs when seller adopts buyer perspective
  - Task completion masking: 93% of conversations complete successfully despite echoing
  - Domain variation: Echoing rates vary 17-58% across domains for same model
- **First 3 experiments**:
  1. Baseline echo rate measurement: Run customer-seller AxA in hotel booking domain with minimal prompts across 5 model families
  2. Structured response mitigation test: Compare echoing rates with and without Pydantic-style structured response format
  3. Turn-length intervention: Inject identity reinforcement every N turns using non-intrusive architectural method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can AxA-specific training datasets and fine-tuning eliminate echoing, or is architectural modification required?
- Basis: Authors state that "robust solutions likely require changes such as AxA-specific datasets and training" and that "deeper architectural or training-level solutions might be required to fully eliminate echoing"
- Why unresolved: The study only tested prompt engineering and structured responses as mitigations; no training interventions were explored
- What evidence would resolve it: Experiments fine-tuning models on AxA interaction data with identity consistency rewards, comparing against baseline models

### Open Question 2
- Question: What mechanistic patterns in model activations predict or cause echoing behavior?
- Basis: Authors explicitly note "we did not analyse activations on open-weight models to understand echoing mechanisms at a deeper level"
- Why unresolved: The study focused on behavioral observation rather than internal model analysis; mechanism remains unexplored
- What evidence would resolve it: Activation analysis on open-weight models during echoing vs. non-echoing turns, identifying attention patterns or representational drift

### Open Question 3
- Question: How does echoing manifest in multi-party conversations (3+ agents) and long-horizon tasks beyond 12 turns?
- Basis: Limitations state "broader AxA contexts (e.g., multi-party, collaborative planning, long-horizon tasks) may reveal additional or different failure modes"
- Why unresolved: All experiments used dyadic interactions with a maximum of 12 turns per agent
- What evidence would resolve it: Experiments with 3+ agent configurations and extended conversation lengths, measuring echoing rates and identity drift patterns

## Limitations
- Reproducibility gaps: Exact domain scenario parameters and internal state management architecture are not provided
- Evaluation subjectivity: Binary nature of echoing detection may miss nuanced identity drift cases
- Domain specificity: Echoing rates vary substantially across domains, suggesting mitigation strategies may not generalize

## Confidence
- **High confidence**: Empirical measurement of echoing phenomenon (measured across 66 configurations with 2500+ conversations), turn-dependent emergence patterns, and domain sensitivity
- **Medium confidence**: Mechanism explanations attributing echoing to training distribution effects and role inference from conversational patterns
- **Low confidence**: Proposed architectural interventions for persistent identity maintenance, as current mitigation strategies only provide partial reduction

## Next Checks
1. **Cross-domain robustness test**: Apply structured response mitigation across all four domains to verify whether ~9% echoing reduction generalizes or domain-specific adaptation is required
2. **Fine-tuning intervention validation**: Train a model exclusively on balanced AxA role distributions with explicit identity-consistency objectives and measure echoing rate reduction compared to base models
3. **Architectural attention intervention**: Implement identity token salience mechanism and measure whether turn-dependent echoing patterns flatten compared to prompt-based interventions