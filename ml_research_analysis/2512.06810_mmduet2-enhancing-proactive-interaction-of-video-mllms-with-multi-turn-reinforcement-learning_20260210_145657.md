---
ver: rpa2
title: 'MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement
  Learning'
arxiv_id: '2512.06810'
source_url: https://arxiv.org/abs/2512.06810
tags:
- video
- reply
- proactive
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MMDuet2, a method for enhancing proactive interaction
  in video multimodal large language models (Video MLLMs) using multi-turn reinforcement
  learning. The key idea is to use a text-based approach where the model autonomously
  decides whether to respond or remain silent at each turn based on dialogue history
  and visual context.
---

# MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning

## Quick Facts
- arXiv ID: 2512.06810
- Source URL: https://arxiv.org/abs/2512.06810
- Reference count: 9
- Primary result: MMDuet2 achieves state-of-the-art performance on ProactiveVideoQA benchmark for proactive video interaction

## Executive Summary
MMDuet2 introduces a novel approach to enhance proactive interaction in video multimodal large language models (Video MLLMs) through multi-turn reinforcement learning. The framework enables autonomous decision-making about when to respond or remain silent during video dialogues, addressing the challenge of real-time video comprehension and interaction. By training on a dataset of 52k videos with two types of dialogues, the method demonstrates significant improvements in both response timing and quality while maintaining performance on offline video understanding tasks.

## Method Summary
The MMDuet2 framework employs a text-based approach where the model autonomously decides whether to respond or remain silent at each turn based on dialogue history and visual context. Rather than manually tuning response thresholds or requiring precise reply timestamps, the authors use reinforcement learning with a reward function that encourages timely and accurate responses while penalizing incorrect or delayed ones. The training process involves supervised fine-tuning followed by reinforcement learning optimization, allowing the model to learn optimal response strategies through iterative improvement.

## Key Results
- Achieves state-of-the-art performance on ProactiveVideoQA benchmark
- Demonstrates superior response timing and quality compared to existing baselines
- Maintains comparable performance on offline video understanding tasks while excelling in proactive interaction
- Successfully handles both 1QnA and nQnA dialogue types in the constructed dataset

## Why This Works (Mechanism)
The reinforcement learning approach allows the model to learn optimal response strategies through trial and error, rather than relying on predefined thresholds or fixed rules. By incorporating a reward function that balances multiple objectives (timeliness, accuracy, and silence appropriateness), the model can dynamically adjust its behavior based on the specific context of each interaction. This adaptive approach enables more natural and effective proactive responses compared to rule-based systems.

## Foundational Learning

1. **Reinforcement Learning for Dialogue Management**
   - Why needed: Enables autonomous decision-making about when to respond or remain silent
   - Quick check: Model learns to maximize cumulative reward across multiple dialogue turns

2. **Multimodal Video Understanding**
   - Why needed: Integrates visual context with dialogue history for informed response decisions
   - Quick check: Model successfully processes both video frames and text input simultaneously

3. **Reward Function Design**
   - Why needed: Balances competing objectives of timeliness, accuracy, and appropriate silence
   - Quick check: Different reward components contribute appropriately to overall performance

## Architecture Onboarding

**Component Map:** Video Encoder -> Text Encoder -> Decision Module -> Response Generator

**Critical Path:** Visual input → multimodal fusion → decision making → response generation → reward calculation → policy update

**Design Tradeoffs:** Text-based approach vs. timestamp-based methods; supervised fine-tuning vs. pure RL; single-turn vs. multi-turn learning

**Failure Signatures:** Premature responses to irrelevant content; delayed responses to critical information; inappropriate silence during meaningful dialogue

**First Experiments:**
1. Compare response timing accuracy with baseline timestamp-based methods
2. Evaluate performance on extended dialogues beyond standard benchmark length
3. Test generalization across different video genres and dialogue patterns

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation relies heavily on ProactiveVideoQA benchmark, which may not fully capture real-world streaming scenarios
- Reinforcement learning depends on manually designed reward function that may introduce bias
- Performance on extended multi-turn dialogues beyond the constructed dataset remains unverified
- Scalability to diverse video content types and complex dialogue patterns requires further validation

## Confidence

- **High Confidence**: Core technical contribution of using reinforcement learning for proactive response timing is well-supported
- **Medium Confidence**: State-of-the-art performance claims based on existing baseline comparisons
- **Medium Confidence**: Scalability and generalization to complex scenarios requires further validation

## Next Checks

1. Test MMDuet2's performance on extended dialogues with more than 10 interaction turns to evaluate long-term context retention and response coherence
2. Conduct ablation studies on the reward function components to quantify their individual contributions to model performance and identify potential biases in the learning process
3. Evaluate the model's response timing and quality on a diverse set of video genres (e.g., sports, news, entertainment) to assess generalization across different visual contexts and dialogue patterns