---
ver: rpa2
title: Apriel-1.5-15b-Thinker
arxiv_id: '2510.01141'
source_url: https://arxiv.org/abs/2510.01141
tags:
- reasoning
- arxiv
- multimodal
- performance
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Apriel-1.5-15B-Thinker achieves frontier-level multimodal reasoning
  by prioritizing mid-training design over model scale. Starting from Pixtral-12B,
  it uses a three-stage pipeline: depth upscaling to expand reasoning capacity, staged
  continual pretraining with synthetic visual data for spatial and compositional understanding,
  and high-quality supervised fine-tuning with explicit reasoning traces.'
---

# Apriel-1.5-15b-Thinker

## Quick Facts
- arXiv ID: 2510.01141
- Source URL: https://arxiv.org/abs/2510.01141
- Reference count: 38
- Achieves 52 on Artificial Analysis Intelligence Index, matching DeepSeek-R1-0528

## Executive Summary
Apriel-1.5-15b-Thinker achieves frontier-level multimodal reasoning by prioritizing mid-training design over model scale. Starting from Pixtral-12B, it uses a three-stage pipeline: depth upscaling to expand reasoning capacity, staged continual pretraining with synthetic visual data for spatial and compositional understanding, and high-quality supervised fine-tuning with explicit reasoning traces. This approach yields a 52 score on the Artificial Analysis Intelligence Index, matching DeepSeek-R1-0528 while operating within single-GPU constraints. On ten image benchmarks, it averages within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, demonstrating that thoughtful data-centric training can close capability gaps without massive scale. The model is released under MIT license to advance open research.

## Method Summary
The model builds on Pixtral-12B-Base-2409 through a four-stage pipeline: (1) depth upscaling from 40 to 48 decoder layers with replay data and checkpoint averaging, (2) projection realignment with frozen vision encoder and decoder, (3) CPT Stage 1 with mixed text and multimodal data, and (4) CPT Stage 2 using synthetic visual tasks with frozen vision encoder. Final supervised fine-tuning uses text-only data with explicit reasoning traces across multiple phases and weight averaging. The approach avoids reinforcement learning and preference optimization, focusing on data quality and training efficiency.

## Key Results
- Achieves 52 on Artificial Analysis Intelligence Index, matching DeepSeek-R1-0528
- Averages within 5 points of Gemini-2.5-Flash and Claude Sonnet-3.7 on ten image benchmarks
- Reaches 87-88% on AIME'25, 60-62% on IFBench, and 58-68% on τ²-Bench Telecom
- Operates within single-GPU constraints while maintaining frontier-level performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Depth upscaling expands reasoning capacity without full pretraining from scratch.
- **Mechanism:** Increasing decoder layers from 40 to 48 (using Pixtral-12B as base) with 50% replay data prevents catastrophic forgetting while adding capacity. Checkpoint averaging across 6 equispaced intermediates stabilizes the expanded model before projection realignment.
- **Core assumption:** The replay data distribution sufficiently covers prior knowledge to preserve multimodal alignment during depth expansion.
- **Evidence anchors:**
  - [abstract]: "depth upscaling to expand reasoning capacity without pretraining from scratch"
  - [Section 2, PAGE 3-4]: Describes 40→48 layer upscaling with checkpoint averaging before projection realignment
  - [corpus]: Weak direct evidence—no corpus papers replicate this exact depth upscaling approach on Pixtral
- **Break condition:** If replay data is under-sampled or domain-coverage is narrow, upscaling may degrade multimodal alignment rather than enhance reasoning.

### Mechanism 2
- **Claim:** Synthetic visual data generation in staged CPT improves spatial, compositional, and fine-grained perception.
- **Mechanism:** CPT Stage 2 applies task-centric synthetic pipelines (image reconstruction, visual matching, object detection, counting) to raw images, transforming natural distributions into curriculum signals. Freezing the vision encoder while updating projection + decoder focuses learning on visual reasoning rather than low-level features.
- **Core assumption:** Synthetic task difficulty modulation transfers to downstream reasoning without overfitting to augmentation artifacts.
- **Evidence anchors:**
  - [abstract]: "enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception"
  - [Table 1, PAGE 5]: SFT on CPT Stage 2 improves MathVerse Vision-Dominant by +9.65 points over Stage 1
  - [corpus]: OpenMMReasoner (2511.16334) reports similar gains from systematic data curation for multimodal reasoning
- **Break condition:** If synthetic tasks are too narrow or augmentation artifacts dominate, gains may not transfer to complex visual reasoning benchmarks.

### Mechanism 3
- **Claim:** Text-only SFT with explicit reasoning traces yields frontier-level reasoning without RL or preference optimization.
- **Mechanism:** High-quality instruction-response pairs (math, coding, science, tool use) with explicit reasoning traces are curated via LLM-as-Judge and execution verification. Multi-stage SFT (full dataset → 25% stratified subset + longer-context run) with weight averaging captures both breadth and long-context performance.
- **Core assumption:** Reasoning traces in text-only SFT generalize to multimodal contexts given a strong multimodal base.
- **Evidence anchors:**
  - [abstract]: "notably, our model achieves competitive results without reinforcement learning or preference optimization"
  - [Section 4, PAGE 5-6]: Describes multi-step SFT with reasoning traces and weight merging
  - [corpus]: rStar-Coder (2505.21297) shows verified reasoning data improves code reasoning; limited direct evidence for text→multimodal transfer
- **Break condition:** If reasoning trace quality is inconsistent or multimodal grounding is weak, text-only SFT may not transfer effectively.

## Foundational Learning

- **Concept: Depth upscaling (layer expansion with replay)**
  - Why needed here: Understanding how to add capacity to a pre-trained multimodal model without losing alignment is critical for efficiently scaling reasoning.
  - Quick check question: Can you explain why replay data is mixed with new domain data during upscaling?

- **Concept: Synthetic data generation for visual reasoning**
  - Why needed here: The paper's CPT Stage 2 relies on task-centric synthetic pipelines; understanding these augmentation strategies is essential for replicating visual reasoning gains.
  - Quick check question: How does masking image regions for reconstruction support part-whole reasoning?

- **Concept: Checkpoint averaging and multi-run weight merging**
  - Why needed here: Stability across long training runs and merging specialized SFT runs (stratified subset vs. long-context) are used to produce the final model.
  - Quick check question: What are the trade-offs of averaging intermediate checkpoints versus selecting the final checkpoint?

## Architecture Onboarding

- **Component map:**
  - Pixtral-12B-Base-2409 → Depth upscaling (40→48 layers) → Projection realignment → CPT Stage 1 → CPT Stage 2 → Multi-phase SFT → Final checkpoint

- **Critical path:**
  1. Start from Pixtral-12B-Base-2409
  2. Upscale decoder to 48 layers with replay + diverse text (8192 seq, lr=5e-5)
  3. Average 6 intermediate checkpoints
  4. Realign projection network (vision encoder + decoder frozen)
  5. CPT Stage 1: 50% text, 20% replay, 30% multimodal (32768 seq, all components unfrozen)
  6. CPT Stage 2: synthetic visual reasoning tasks (16384 seq, vision encoder frozen, loss on response only)
  7. SFT: multi-phase text-only training with reasoning traces, merge final checkpoints

- **Design tradeoffs:**
  - Freezing vision encoder in CPT Stage 2 improves stability but may limit visual representation refinement
  - Text-only SFT simplifies data curation but assumes transfer from text reasoning to multimodal
  - No RL/preference optimization reduces cost but may limit alignment depth

- **Failure signatures:**
  - CPT Stage 2 shows no gain: synthetic tasks may be misaligned or augmentation artifacts dominate
  - SFT degrades multimodal performance: text-only reasoning traces may not transfer
  - Upscaling reduces multimodal alignment: insufficient replay data or projection realignment failure

- **First 3 experiments:**
  1. **Reproduce CPT Stage 2 effect:** Train SFT from CPT Stage 1 vs. Stage 2 checkpoints on a small reasoning set; compare on MathVerse and CharXiv to validate Table 1 gains.
  2. **Ablate synthetic task categories:** Remove one synthetic task (e.g., visual matching) and measure impact on spatial reasoning benchmarks to identify critical signals.
  3. **Test text→multimodal transfer:** Evaluate text-only SFT checkpoint vs. multimodal SFT on vision benchmarks to quantify transfer assumptions.

## Open Questions the Paper Calls Out

- **Question:** Would adding reinforcement learning or preference optimization on top of the current mid-training pipeline yield significant gains, and in which capability dimensions?
  - **Basis in paper:** [explicit] "Given computational constraints, the current release focuses on maximizing the potential of the base model through mid-training, without employing reinforcement learning or preference optimization."
  - **Why unresolved:** The authors deliberately excluded RL/PO to isolate mid-training contributions, leaving the marginal benefit of these techniques untested on this architecture.
  - **What evidence would resolve it:** Train Apriel with identical mid-training plus RL/PO and compare against the current checkpoint on the full benchmark suite.

- **Question:** Can performance on vision-dominant tasks (e.g., MMMU-PRO Vision at 48.21%) be substantially improved by using multimodal SFT data instead of text-only SFT?
  - **Basis in paper:** [explicit] The paper notes the model "shows room for improvement" on vision-dominant tasks, while stating SFT was "high-quality text-only supervised fine-tuning."
  - **Why unresolved:** Text-only SFT was chosen for efficiency, but whether multimodal instruction data during SFT would close the vision-dominant gap remains untested.
  - **What evidence would resolve it:** Ablation comparing text-only SFT vs. multimodal SFT on MMMU-PRO Vision and MathVerse Vision-Dominant, holding other factors constant.

- **Question:** What is the impact of freezing the vision encoder during CPT Stage 2 on downstream visual reasoning capabilities?
  - **Basis in paper:** [inferred] The methodology states "the vision encoder was frozen, with just the projection network and decoder updated during training" in CPT Stage 2, but no ablation compares this against an unfrozen encoder.
  - **Why unresolved:** Freezing may limit adaptation of visual representations for complex reasoning tasks, but the trade-off was not evaluated.
  - **What evidence would resolve it:** Compare CPT Stage 2 with frozen vs. unfrozen vision encoder on visual reasoning benchmarks like LogicVista and MathVerse.

- **Question:** How do more rigorous safety mitigations affect reasoning capabilities, and what is the optimal balance?
  - **Basis in paper:** [explicit] "This release prioritized performance, with safety mitigations included but not pursued to the same depth."
  - **Why unresolved:** The performance-safety trade-off was not systematically evaluated, leaving uncertainty about costs of stronger alignment.
  - **What evidence would resolve it:** Compare benchmark performance across model variants with varying levels of safety training.

## Limitations

- Dataset sizes and exact synthetic data generation pipelines are underspecified, making faithful reproduction difficult.
- The assumption that replay data sufficiently preserves multimodal alignment during upscaling is not directly tested.
- Text-to-multimodal transfer via SFT lacks empirical validation on multimodal benchmarks before and after each stage.

## Confidence

- **High confidence:** The technical design is well-articulated and internally consistent, with strong benchmark results supporting the central claim about mid-training efficiency.
- **Medium confidence:** The claim that text-only SFT with reasoning traces generalizes to multimodal reasoning, as this transfer is assumed rather than experimentally validated.
- **Low confidence:** The specific contribution of individual design choices (synthetic task composition, replay data quality) due to lack of ablation studies.

## Next Checks

1. Replicate CPT-Stage 2 effect by training SFT from both CPT-Stage 1 and Stage 2 checkpoints on a small reasoning set; compare performance on MathVerse and CharXiv to confirm the +9.65 point gain.
2. Ablate synthetic task categories by removing one (e.g., visual matching) and measure impact on spatial reasoning benchmarks to identify critical signals.
3. Test text-only SFT transfer by evaluating the text-only SFT checkpoint vs. a multimodal SFT variant on vision benchmarks to quantify the assumed generalization.