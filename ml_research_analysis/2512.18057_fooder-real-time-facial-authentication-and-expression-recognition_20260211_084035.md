---
ver: rpa2
title: 'FOODER: Real-time Facial Authentication and Expression Recognition'
arxiv_id: '2512.18057'
source_url: https://arxiv.org/abs/2512.18057
tags:
- facial
- expression
- recognition
- detection
- radar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FOODER, a real-time privacy-preserving radar-based
  framework that integrates facial authentication and expression recognition. The
  system uses low-cost FMCW radar and leverages range-Doppler and micro range-Doppler
  representations.
---

# FOODER: Real-time Facial Authentication and Expression Recognition

## Quick Facts
- **arXiv ID:** 2512.18057
- **Source URL:** https://arxiv.org/abs/2512.18057
- **Reference count:** 0
- **Primary result:** 94.13% AUROC authentication, 94.70% FER accuracy using 60 GHz FMCW radar

## Executive Summary
FOODER is a privacy-preserving radar-based system that combines facial authentication with expression recognition. The system uses low-cost FMCW radar to capture facial signatures without visual imagery, preserving user privacy. It employs a one-class reconstruction approach for authentication, trained exclusively on a single enrolled user, and a hierarchical expression recognition pipeline that routes samples based on motion characteristics. The system achieves state-of-the-art performance while maintaining real-time operation.

## Method Summary
FOODER processes 60 GHz FMCW radar signals into Range-Doppler and micro Range-Doppler images through FFT-based preprocessing with E-RESPD enhancement. The authentication module (RFOOD) uses a multi-encoder multi-decoder architecture with Body Part and Intermediate Linear Encoder-Decoder components, trained only on the single enrolled user's data. Expression recognition uses a frozen RFOOD backbone, with a ResNet18 router classifying samples as dynamic or static, followed by specialized MobileViT networks for fine-grained classification. The system operates without visual imagery, preserving privacy while achieving real-time performance.

## Key Results
- Authentication AUROC: 94.13% with FPR95 of 18.12%
- Expression recognition accuracy: 94.70% average across 4 classes
- Outperforms state-of-the-art OOD detection methods and transformer-based architectures
- Real-time inference capability maintained throughout

## Why This Works (Mechanism)

### Mechanism 1: One-Class Reconstruction Error for Authentication
The RFOOD module is trained exclusively on data from the single enrolled user, learning to compress and reconstruct only that user's facial radar signature with low error. During inference, inputs from non-enrolled users result in statistically higher Mean Squared Error because the network never learned their distinct structural or dynamic features. The ILED component forces feature maps into a compressed latent vector, acting as the primary OOD scoring mechanism.

### Mechanism 2: Hierarchical Decomposition of Expression Tasks
The FER module first uses a ResNet18 block to separate "Dynamic" (high motion) from "Static" (low motion) expressions, then routes samples to specialized lightweight transformers for fine-grained classification. This reduces complexity burden on any single classifier, with the root classifier achieving >98% accuracy in routing decisions.

### Mechanism 3: Dual-Modality Radar Fusion
Combining Range-Doppler Images (RDIs) with micro-Range-Doppler Images (micro-RDIs) provides complementary signals. RDIs capture gross motion and range, while micro-RDIs capture subtle muscle vibrations. The concatenation of these inputs allows the network to correlate gross facial movement with fine muscle engagement, with ablation studies showing significant performance drops when using single modalities.

## Foundational Learning

- **FMCW Radar Signal Processing:** Understanding "Fast-time" (range) vs. "Slow-time" (Doppler/velocity) is required to debug data pipelines. *Quick check:* Can you explain why a "smile" creates a different Doppler signature than a "neutral" face in the slow-time dimension?
- **Out-of-Distribution (OOD) Detection:** The authentication system relies on the principle that the network should produce high error on data it never saw during training. *Quick check:* Why is setting the decision threshold for OOD detection a trade-off between security and usability?
- **Vision Transformers (ViT) for Time-Series/Radar:** The expression module uses MobileViT, which treats input patches as sequences. *Quick check:* How does the self-attention mechanism in MobileViT help in focusing on the specific region of the face generating the micro-Doppler signature?

## Architecture Onboarding

- **Component map:** Sensor -> FFT preprocessing -> E-RESPD enhancement -> RFOOD authentication -> ResNet18 router -> MobileViT-v2/MobileViT expression classifiers
- **Critical path:** The ILED (Intermediate Linear Encoder-Decoder) component sits before the final upsampling layer in the decoder, forcing feature maps into a compressed latent vector and reconstructing them. This acts as a regularizer and the primary OOD scoring mechanism.
- **Design tradeoffs:** Real-time vs. Accuracy (MobileViT over standard ViT for speed), Privacy vs. Granularity (radar avoids visual data but loses high-resolution texture details), and Memory efficiency (one-class learning vs multi-user scalability).
- **Failure signatures:** High False Positive Rate if MSE threshold is too low, Motion Confusion if head movement occurs during expression, and Slow inference if lightweight variants aren't used.
- **First 3 experiments:** 1) Pipeline Sanity Check: Plot reconstruction error distributions between ID and OOD classes. 2) Ablation on Modality: Disable micro-RDI branch and measure AUROC drop. 3) Router Accuracy Test: Compute confusion matrix for dynamic/static binary task.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several limitations are implied regarding multi-user scalability, cross-subject generalization, and robustness to real-world variations.

## Limitations
- Single-user enrollment model doesn't scale to multiple users without separate models
- No cross-subject generalization testing for expression recognition
- Limited evaluation under varying facial occlusions and environmental conditions
- Fixed 25cm distance and chin-supported setup not representative of natural usage

## Confidence

- **High Confidence:** Hierarchical decomposition mechanism for expression recognition (94.70% accuracy supported by ablation studies)
- **Medium Confidence:** OOD detection via reconstruction error (theoretically sound but limited comparative analysis)
- **Low Confidence:** Real-time performance claims (7s for full test set lacks hardware context)

## Next Checks

1. **Threshold Calibration Validation:** Plot ROC curves for authentication using multiple MSE thresholds to verify claimed 94.13% AUROC and 18.12% FPR95 metrics
2. **Cross-User Generalization Test:** Evaluate authentication accuracy with natural variations (facial hair, glasses, expression intensity) in the enrolled user
3. **End-to-End Latency Profiling:** Measure per-frame inference time for each module on target hardware to confirm real-time capability