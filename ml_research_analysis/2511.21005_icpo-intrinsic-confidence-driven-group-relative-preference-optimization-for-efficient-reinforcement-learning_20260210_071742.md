---
ver: rpa2
title: 'ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for
  Efficient Reinforcement Learning'
arxiv_id: '2511.21005'
source_url: https://arxiv.org/abs/2511.21005
tags:
- icpo
- reward
- responses
- training
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ICPO addresses challenges in RLVR such as sparse rewards, reward
  noise, and entropy collapse by leveraging the intrinsic probabilities of model-generated
  responses as a self-assessment signal. The method computes a preference advantage
  score for each response by comparing relative generation probabilities within a
  group, then integrates these scores with external rewards through multi-stage weight
  adjustment.
---

# ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2511.21005
- **Source URL:** https://arxiv.org/abs/2511.21005
- **Reference count:** 40
- **Primary result:** ICPO consistently outperforms GRPO baselines on seven reasoning benchmarks across multiple model architectures by leveraging intrinsic generation probabilities for self-assessment.

## Executive Summary
ICPO addresses fundamental challenges in RLVR including sparse rewards, reward noise, and entropy collapse by using the intrinsic probabilities of model-generated responses as a self-assessment signal. The method computes preference advantage scores by comparing relative generation probabilities within response groups, then integrates these scores with external rewards through multi-stage weight adjustment. This approach suppresses overconfident errors, enhances undervalued high-quality responses, and sustains exploration. Comprehensive experiments demonstrate consistent improvements over GRPO across diverse reasoning tasks.

## Method Summary
ICPO modifies GRPO by adding intrinsic confidence signals derived from model generation probabilities. For each prompt, the method samples 5 responses and computes their normalized sequence-level probabilities. Responses are ranked and all valid preference pairs are constructed to calculate a preference advantage score that quantifies relative merit within the group. This score is added to the verifiable reward with clipping to prevent dominance. A multi-stage weight adjustment schedule gradually introduces the intrinsic signal during early training (warmup) then reduces its influence as training progresses (decay), matching exploration needs to training phases.

## Key Results
- ICPO consistently outperforms GRPO baselines across seven benchmarks including MMLU-Pro, GPQA, and MATH-500
- The method maintains higher policy entropy throughout training, preventing premature convergence
- ICPO demonstrates robustness to reward noise up to 60% while still providing meaningful performance gains
- Scale-adaptive improvements: smaller gains on 3B/14B models, larger gains on 7B models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Generation probabilities encode fine-grained self-assessment that compensates for sparse or noisy external rewards.
- **Mechanism:** For each prompt, sample G responses and compute their normalized sequence-level probabilities. Rank responses by probability, construct all valid preference pairs, then calculate a preference advantage score Sp_k that quantifies relative merit within the group. This score is added to the verifiable reward with clipping to prevent dominance.
- **Core assumption:** The model's probability distribution reflects meaningful uncertainty about reasoning correctness, not just exposure frequency.
- **Evidence anchors:** [abstract]: "probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process"

### Mechanism 2
- **Claim:** Preference advantage scores suppress overconfident errors while amplifying undervalued correct responses.
- **Mechanism:** The score formula asymmetrically rewards lower-confidence correct responses more than higher-confidence ones, and penalizes high-confidence errors more heavily. A clipping mechanism (min(Sp_k, |Rverif_k|/τ)) prevents extreme intrinsic signals from overriding external verification.
- **Core assumption:** There exists a learnable mapping between model confidence and response quality that can be exploited for policy guidance.
- **Evidence anchors:** [abstract]: "suppresses overconfident errors, enhances undervalued high-quality responses"

### Mechanism 3
- **Claim:** Multi-stage weight adjustment (warmup then decay) matches intrinsic signal strength to exploration needs across training phases.
- **Mechanism:** Early training uses inverse cosine warmup to gradually introduce preference signals as entropy decreases and informative low-probability responses emerge. Later training applies linear decay to reduce intrinsic influence as remaining low-probability responses become mostly noise.
- **Core assumption:** The distribution of "correct but underexplored" responses follows a predictable trajectory: rare initially, peaks mid-training, diminishes as policy converges.
- **Evidence anchors:** [Section 3.4]: "In early training, high entropy makes it challenging to determine whether low-probability responses are worth learning"

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** ICPO modifies GRPO's advantage calculation by adding preference scores. Understanding baseline GRPO is essential to see what changes.
  - **Quick check question:** Can you explain how GRPO computes advantages within a sampled group and why this creates vulnerability to sparse rewards?

- **Concept: Preference Modeling in DPO**
  - **Why needed here:** ICPO borrows pairwise preference comparison logic from DPO but applies it to self-generated probabilities rather than human annotations.
  - **Quick check question:** How does ranking responses by probability and forming preference pairs differ from traditional preference learning with labeled data?

- **Concept: Entropy Collapse in RL**
  - **Why needed here:** A core motivation for ICPO is preventing premature policy convergence. Understanding entropy dynamics is critical.
  - **Quick check question:** What behavioral symptoms indicate entropy collapse in an LLM policy, and why does sparse reward exacerbate it?

## Architecture Onboarding

- **Component map:**
  - Sampling module → G responses per prompt with probabilities
  - Ranking module → Sort by normalized log-probability
  - Preference pair constructor → Generate all valid pairs
  - Advantage calculator → Sp_k via Eq. 9, merged reward via Eq. 11
  - Weight scheduler → Warmup decay curve with configurable turning point
  - Policy optimizer → Standard GRPO-style objective with modified advantages

- **Critical path:**
  1. Verify probability normalization handles response length correctly
  2. Check preference pair construction handles edge cases (ties, extreme outliers)
  3. Validate clipping threshold τ prevents runaway intrinsic rewards
  4. Monitor entropy trajectory to confirm warmup decay timing matches exploration phase

- **Design tradeoffs:**
  - **δ parameter:** Higher values amplify preference signal strength but risk over-reliance on model self-assessment. Paper recommends δ=0.5 (<4B), 0.4 (4-8B), 0.3 (>8B) based on calibration analysis.
  - **τ parameter:** Lower values more aggressively clip intrinsic rewards, protecting against noise but potentially losing signal. τ=2.0 performed best in experiments.
  - **Warmup turning point:** 0.4 worked best for Qwen2.5-7B; adjust based on when correct low-probability responses peak in your model.

- **Failure signatures:**
  - Early entropy drop with no accuracy gain → intrinsic rewards too strong, reduce ω or increase τ
  - No accuracy improvement despite stable entropy → warmup too slow or preference signal too weak
  - Performance collapse under noisy rewards → noise threshold exceeded (~0.6 in experiments); ICPO cannot fully compensate

- **First 3 experiments:**
  1. **Baseline comparison:** Replicate GRPO vs ICPO on Qwen2.5-7B with MMLU-Pro subset. Monitor both accuracy and entropy curves to verify entropy maintenance benefit.
  2. **Sparse reward stress test:** Filter training data to only include groups where all responses have identical rewards. Compare GRPO (should fail) vs ICPO (should provide differentiation via preference scores).
  3. **Noise injection robustness:** Add random ±0.3 noise to 40% of rewards. Compare accuracy degradation curves to quantify ICPO's noise resistance claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ICPO be modified to maintain stability in scenarios where external rewards are completely adversarial or reversed, rather than merely sparse or noisy?
- Basis in paper: [explicit] The authors state in the Limitations section that when rewards "completely deviate from the true task objectives," the mechanism reverses, causing policy failure, and they are "actively exploring solutions."

### Open Question 2
- Question: Can the preference advantage score calculation be calibrated to address the non-linear relationship between model scale and confidence calibration?
- Basis in paper: [inferred] Section 4.4 notes that ICPO shows smaller improvements on 3B and 14B models compared to 7B models, hypothesizing that small models "lack confidence" while large models are "overconfident."

### Open Question 3
- Question: How can the method differentiate between correct high-confidence responses and overconfident errors to avoid penalizing already-optimal behaviors?
- Basis in paper: [inferred] Appendix E.3 identifies failure modes where ICPO cannot distinguish trajectories that are "correct yet possess high confidence" from incorrect ones.

## Limitations
- ICPO's performance degrades sharply when reward noise exceeds 60% threshold
- The method cannot fully compensate for systematically inverted or adversarial rewards
- All experiments use Qwen2.5 models, limiting generalizability to other architectures

## Confidence

- **High:** ICPO improves accuracy on tested benchmarks vs GRPO
- **Medium:** ICPO maintains higher entropy throughout training
- **Low:** The proposed multi-stage weight adjustment schedule universally optimizes exploration across different model sizes

## Next Checks

1. **Cross-architecture validation:** Test ICPO on 1.8B and 14B models using identical training procedures to verify δ parameter scaling recommendations.

2. **Zero-shot transfer test:** Apply ICPO-trained models to completely unseen reasoning domains (e.g., medical reasoning if trained on math/science) to assess generalization.

3. **Ablation on weight schedule:** Systematically vary the warmup turning point (0.2-0.8) and decay rates to identify optimal schedules for different entropy trajectories.