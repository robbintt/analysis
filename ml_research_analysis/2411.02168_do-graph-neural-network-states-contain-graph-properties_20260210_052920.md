---
ver: rpa2
title: Do graph neural network states contain graph properties?
arxiv_id: '2411.02168'
source_url: https://arxiv.org/abs/2411.02168
tags:
- graph
- properties
- layers
- node
- probing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a model-agnostic pipeline to probe Graph Neural
  Networks (GNNs) for graph-theoretic properties using diagnostic classifiers. The
  authors propose probing embeddings with linear classifiers to measure how well specific
  graph properties (like number of squares, average degree, spectral radius) are linearly
  separable.
---

# Do graph neural network states contain graph properties?

## Quick Facts
- arXiv ID: 2411.02168
- Source URL: https://arxiv.org/abs/2411.02168
- Reference count: 40
- Key outcome: Model-agnostic probing pipeline reveals task-relevant graph properties are linearly decodable in GNN representations

## Executive Summary
This work introduces a model-agnostic pipeline to probe Graph Neural Networks (GNNs) for graph-theoretic properties using diagnostic classifiers. The authors propose probing embeddings with linear classifiers to measure how well specific graph properties (like number of squares, average degree, spectral radius) are linearly separable. Experiments on Grid-House and ClinTox datasets show that properties most useful for the classification task (e.g., #squares for Grid-House, average degree for ClinTox) consistently achieve the highest R² scores across layers. GIN models perform best, with regularization methods affecting property separation differently. The findings validate that GNNs leverage specific structural graph properties for classification, offering interpretability into their decision-making process.

## Method Summary
The method trains GNNs on graph classification tasks, then extracts hidden layer representations to probe for graph-theoretic properties using linear regression classifiers. For each property, a linear probe predicts property values from GNN embeddings, and R² scores measure linear separability. Embeddings are sorted by norm to handle permutation invariance. The pipeline tests multiple GNN architectures (GCN, GIN, GAT) and regularization types (L2, dropout) across synthetic (Grid-House) and molecular (ClinTox) datasets.

## Key Results
- Task-relevant properties (e.g., #squares for Grid-House, avg degree for ClinTox) consistently achieve highest R² scores across layers
- GIN models show highest linear separability due to greater expressivity (WL-equivalent)
- L2 regularization improves property separation while dropout reduces it
- Early layers encode local properties while deeper layers capture global properties

## Why This Works (Mechanism)

### Mechanism 1: Linear Representability of Graph Properties
During supervised training, gradient updates reinforce specific activation directions that correlate with informative properties. If a linear probe achieves high R², the model has organized representations such that a hyperplane separates inputs by that property.

### Mechanism 2: Progressive Receptive Field Expansion
GNN layers progressively aggregate local neighborhood information, causing early layers to encode local properties (degree, clustering) while deeper layers capture global properties (diameter, spectral radius).

### Mechanism 3: Regularization-Driven Representation Sharpening
L2 regularization improves linear separability by reducing weight sensitivity and making embeddings more selective to discriminative properties. Dropout degrades separability by forcing distributed representations across redundant pathways.

## Foundational Learning

- **Message Passing and Receptive Fields**: Understanding that layer k aggregates information from k-hop neighborhoods explains why global properties require deeper layers.
  - Quick check: At layer 3 of a GCN, what is the maximum graph distance from which a node can receive information?

- **Linear Separability in High Dimensions**: The core diagnostic relies on whether a hyperplane can partition embeddings by property value.
  - Quick check: If R² = 0.95 for "number of squares" at layer 5, what does this imply about how the GNN represents this property?

- **Probing Classifier Limitations**: A probe can detect information the GNN stores but doesn't use; high R² doesn't prove causality.
  - Quick check: If a probe achieves R² = 0.9 for a property but removing that property from the data doesn't affect accuracy, what conclusion should you draw?

## Architecture Onboarding

- **Component map**: GNN encoder (GCN/GIN/GAT layers) → pooling (mean/sum/max) → dense layers → classification → probing pipeline extracts hidden states → linear classifiers predict graph properties

- **Critical path**: 1) Train GNN on target task until convergence 2) Extract representations at each layer (pre- and post-pooling) 3) For each property z, train linear probe g: f_l(x) → ẑ 4) Compute R² on held-out set; track across layers 5) Identify which properties show increasing/decreasing R² with depth

- **Design tradeoffs**: Linear vs nonlinear probes (linear avoids detecting unused features but may miss nonlinear encodings), pre-pooling vs post-pooling (fixed vs variable size handling), GIN vs GCN vs GAT (expressivity vs simplicity)

- **Failure signatures**: R² consistently near 0 across all properties (model not learning structure), high R² in layer 1 for global properties (data leakage), negative R² (probe worse than mean), properties appearing only at final MLP layer (information post-pooling)

- **First 3 experiments**: 1) Baseline validation: Replicate Grid-House results—confirm #squares achieves highest R² and increases with depth 2) Architecture comparison: Run identical probing on GCN vs GIN for same dataset 3) Regularization ablation: Train with L2, dropout, and neither; plot R² trajectories

## Open Questions the Paper Calls Out

### Open Question 1
How does the nature of the supervision signal (unsupervised, self-supervised, vs. supervised) influence the emergence and linear separability of graph-theoretic properties in GNN representations?

### Open Question 2
Why do early GNN layers often contain predictive information for global graph features, despite message-passing mechanisms theoretically requiring multiple layers to aggregate information globally?

### Open Question 3
To what extent do higher-order GNN architectures (equivalent to 2-WL and 3-WL tests) encode complex structural properties differently or more effectively than standard 1-WL equivalent models?

## Limitations
- Probing framework shows linear separability but doesn't prove causal relevance to model decisions
- Expressivity claims assume property requirements match architectural capabilities
- Regularization effects could stem from overfitting prevention vs specific property concentration mechanisms

## Confidence
- High confidence: Linear probing methodology and core empirical finding about task-relevant properties
- Medium confidence: Progressive receptive field expansion and regularization effects mechanisms
- Low confidence: Claims about causal property importance and universality of expressivity-performance relationship

## Next Checks
1. **Ablation validation**: Remove the top-ranked property and retrain the GNN. If accuracy drops significantly, this confirms causal relevance.

2. **Nonlinear probe comparison**: Run identical experiments with nonlinear probing classifiers. If nonlinear probes show no consistent improvement for task-relevant properties, this strengthens the linear representation hypothesis.

3. **Cross-dataset property generalization**: Test whether properties that rank highly in Grid-House also rank highly in ClinTox when both are used for the same classification task.