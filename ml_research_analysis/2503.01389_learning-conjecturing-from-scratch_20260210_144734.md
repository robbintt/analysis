---
ver: rpa2
title: Learning Conjecturing from Scratch
arxiv_id: '2503.01389'
source_url: https://arxiv.org/abs/2503.01389
tags:
- loop
- problems
- predicates
- induction
- divf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a self-learning approach for conjecturing induction
  predicates to prove program equalities in the OEIS benchmark, which requires combining
  inductive and arithmetical reasoning. The method iterates between training a neural
  translator on previously discovered problem-solution pairs, generating new candidates,
  and evaluating them with Z3.
---

# Learning Conjecturing from Scratch

## Quick Facts
- arXiv ID: 2503.01389
- Source URL: https://arxiv.org/abs/2503.01389
- Reference count: 40
- Primary result: Self-learning NMT+ATP system solves 5,565/16,197 OEIS program equalities by discovering useful induction predicates, compared to 2,265 solved by CVC5/Vampire/Z3 in 60s.

## Executive Summary
This paper presents a self-learning approach for conjecturing induction predicates to prove program equalities in the OEIS benchmark. The method iterates between training a neural translator on previously discovered problem-solution pairs, generating new candidates, and evaluating them with Z3. Starting from scratch with brute-force enumeration, the algorithm discovers many useful induction predicates, solving 5,565 of 16,197 problems. A trained NMT+ATP system achieves 5,372 solutions in at most 48 seconds, and the union of manual heuristics and the self-learning approach solves 6,351 problems.

## Method Summary
The system maintains a growing corpus of problem–predicate pairs and iterates between training a neural machine translation model on this corpus, generating candidate predicates for unsolved problems, validating them with Z3 (200ms timeout), and adding newly solved pairs back to training data with minimization and selection. Initial predicate generation enumerates formulas by size with aggressive pruning using fingerprinting and semantic evaluation, yielding ~1,000 initial candidates per problem. The NMT (2-layer BiLSTM with attention, 512 units) translates problem encodings to predicate sequences, with beam search (width 240) generating candidates. Data augmentation includes rare index shifting and definition expansion.

## Key Results
- Self-learning approach solves 5,565/16,197 OEIS problems versus 2,265 solved by CVC5/Vampire/Z3 in 60s
- Trained NMT+ATP system achieves 5,372 solutions in at most 48 seconds
- Union of manual heuristics and self-learning approach solves 6,351 problems
- The system produces a large dataset of interesting induction predicates, some analyzed for generality

## Why This Works (Mechanism)

### Mechanism 1
Iterative self-learning expands provable problems by accumulating useful induction patterns. The system maintains a growing corpus of problem–predicate pairs. Each iteration trains an NMT model on this corpus, generates candidate predicates for unsolved problems, validates them with Z3 (200ms timeout), and adds newly solved pairs back to training data with minimization and selection. Core assumption: Solutions discovered for one subset of problems generalize to structurally similar unsolved problems via the NMT's learned distribution. Evidence: Figure 1 shows cumulative solved problems increasing across iterations with sudden jumps, suggesting discovery of generalizable patterns.

### Mechanism 2
Brute-force enumeration with aggressive pruning produces a sufficient initial training signal to bootstrap neural guidance. Initial predicate generation enumerates formulas by size, applying four filters: restrict to loop functions and `s` functions, discard arithmetically equivalent terms via fingerprinting, discard formulas independent of induction variable `x`, and keep only formulas true on all test values. This yields ~1,000 initial candidates per problem, solving 2,506 problems in ~9 hours. Core assumption: The pruned search space still contains enough correct induction predicates to create a non-trivial training set. Evidence: The initial data were produced by the initial (brute-force) run using 1000 candidates per problem, resulting in 2,506 solved problems.

### Mechanism 3
Neural-guided generation with beam search and data augmentation efficiently explores the combinatorial predicate space beyond brute-force limits. The NMT (2-layer BiLSTM with attention, 512 units) is trained to translate problem encodings to predicate sequences. At inference, beam search (width 240) generates candidates. Data augmentation includes rare index shifting and definition expansion. Two inference modes: whole (generate coordinated predicate lists) and split (generate independent predicates, randomly combine). Core assumption: The NMT learns a distribution that prioritizes syntactically valid and semantically useful predicates, reducing the search space for Z3 evaluation. Evidence: The record is 5,372 problems solved in iteration 98 of Run-4, meaning the final trained NMT+ATP system outperforms the union of all manual heuristics.

## Foundational Learning

- **Second-order induction axioms**
  - Why needed here: The core proof mechanism instantiates the second-order axiom `∀P. ((∀y. P(0,y)) ∧ (∀xy. P(x,y) ⇒ P(x+1,y))) ⇒ (∀xy. 0≤x ⇒ P(x,y))` with concrete predicates `Q`. Understanding this is essential to see why predicate synthesis is the key bottleneck.
  - Quick check question: Given the induction axiom above, what predicate `Q` would prove `∀x≥0. v(x) = x(x+1)/2` for `v(0)=0, v(n+1)=v(n)+n+1`?

- **Neural Machine Translation (sequence-to-sequence with attention)**
  - Why needed here: The NMT model treats conjecturing as translation from problem strings to predicate strings. Beam search outputs multiple candidates for Z3 validation. Without this background, the training/inference pipeline is opaque.
  - Quick check question: In a seq2seq model with beam width 5, if the top 5 partial sequences at step 10 have scores [-0.2, -0.5, -0.8, -1.1, -1.4], which sequences are retained for step 11?

- **SMT solving (Z3) and unsatisfiability**
  - Why needed here: Problems are posed as `∃c. c≥0 ∧ ¬(fSmall(c) = fFast(c))`. Z3 proving unsatisfiability establishes program equality. The 200ms timeout is a critical design choice balancing coverage vs. throughput.
  - Quick check question: If Z3 returns "unsat" on an SMT problem with the assertion `∃c. c≥0 ∧ small(c) ≠ fast(c)`, what does this imply about `∀x≥0. small(x) = fast(x)`?

## Architecture Onboarding

- **Component map:**
  Predicate Generator -> NMT Trainer -> NMT Inference -> Z3 Evaluator -> Selector/Minimizer -> updated training corpus

- **Critical path:**
  1. Run initial brute-force generation → 2,506 solved problems (training seed)
  2. Train NMT on seed corpus (2,000–14,000 steps, 15–130 min on GTX1080 Ti)
  3. Run NMT inference (75 min for 16,197 problems)
  4. Z3 evaluation (parallelized, 1–3 hours per iteration)
  5. Minimization + selection → updated training corpus
  6. Repeat from step 2

- **Design tradeoffs:**
  - Timeout per Z3 call (200ms): Lower timeouts increase iteration speed but miss harder proofs; 200ms is empirically tuned
  - Beam width (240): Wider beams explore more candidates but increase Z3 calls linearly; 240 balances coverage and throughput
  - Whole vs. split mode: Whole mode coordinates multi-predicate solutions (better for complex problems); split mode allows longer solutions via random combination (better for coverage)
  - Predicate language restriction: Excluding helper functions in initial generation reduces search space but may miss solutions; data augmentation via definition expansion compensates

- **Failure signatures:**
  - Plateau in solved problems across iterations: Indicates NMT overfitting or exploration exhaustion
  - High predicate redundancy (minimization removes most): Suggests NMT is generating syntactic variations without semantic diversity
  - Z3 timeout rate >90%: Candidates are irrelevant or axioms insufficient

- **First 3 experiments:**
  1. Reproduce initial brute-force baseline: Enumerate predicates with fingerprinting on a 100-problem subset. Verify ~15% solve rate aligns with 2,506/16,197.
  2. Ablate data augmentation: Train NMT on seed corpus with and without rare-index shifting. Compare solved problems after 10 iterations.
  3. Profile Z3 bottleneck: Measure Z3 call time distribution (should be bimodal: fast proofs vs. timeouts).

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: Can the self-learning approach scale efficiently to millions of problems, specifically the full output of the OEIS synthesis run?
  - Basis in paper: Section 10 states the intent to "scale our approach to much larger sets of problems such as the millions of programs coming from our latest OEIS synthesis run."
  - Why unresolved: The current experiments required months to process 16,197 problems; scaling by two orders of magnitude introduces significant computational and convergence challenges.
  - What evidence would resolve it: Successful execution of the training and inference loop on the full dataset within a feasible time frame.

- **Open Question 2**
  - Question: Is the methodology transferable to verifying loop invariants in industrial programs, which are typically more complex than OEIS programs?
  - Basis in paper: Section 10 expresses hope to "apply the methodology... to verification of loop invariants in industrial programs which are typically much longer."
  - Why unresolved: The system is currently tuned for the compact syntax of OEIS programs; industrial code involves larger state spaces and more verbose definitions.
  - What evidence would resolve it: Adaptation of the system to prove invariants in industrial benchmarks (e.g., SV-COMP) with comparable success rates.

- **Open Question 3**
  - Question: Can the feedback loop successfully generate intermediate lemmas (cuts) or instantiate first-order quantifiers to simplify large proofs?
  - Basis in paper: Section 10 proposes "apply[ing] the approach to other forms of conjecturing such as generating instantiations of first-order quantifiers and synthesizing intermediate lemmas."
  - Why unresolved: The current implementation focuses on second-order induction axioms; generalizing to arbitrary lemma synthesis requires a new grammar and learning target.
  - What evidence would resolve it: A modified system that improves proof automation in large theory libraries (e.g., Mizar or Isabelle) via lemma synthesis.

## Limitations

- Predicate Generalization: The paper does not rigorously quantify how many discovered predicates are truly novel versus rediscoveries of manual heuristics.
- Scalability to New Domains: While the system excels on OEIS programs, there is no evidence it transfers to different problem classes (e.g., bitvector arithmetic, quantifier-heavy problems).
- Predicate Language Expressiveness: The grammar excludes helper functions and restricts to loop functions and `s` functions, potentially missing valid induction predicates.

## Confidence

- **High Confidence**: Iterative self-learning mechanism is well-supported by iteration curves showing continued improvement across hundreds of iterations. The 5,565 problems solved via self-learning vs. 2,265 by state-of-the-art ATPs is directly measured.
- **Medium Confidence**: Initial brute-force generation is supported by 2,506 solved problems, but the fingerprinting method's correctness for loop terms is not independently verified.
- **Medium Confidence**: Neural-guided exploration is supported by the final 5,372 solutions, but the specific contribution of NMT vs. other factors (data augmentation, beam width) is not isolated.

## Next Checks

1. **Predicate Novelty Analysis**: Manually examine a sample of 100 discovered predicates to verify they are not derivable from manual heuristics. Quantify the fraction of truly novel patterns.
2. **Cross-Domain Transfer**: Apply the trained NMT+ATP system to a benchmark with different induction structures (e.g., bitvector programs from SV-COMP). Measure solve rate to assess domain generalization.
3. **Ablation on Grammar Expressiveness**: Extend the predicate grammar to include helper functions and quantifiers. Compare solve rates with the restricted grammar to determine if expressiveness limitations are binding.