---
ver: rpa2
title: Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced
  Abstract MDPs
arxiv_id: '2507.16473'
source_url: https://arxiv.org/abs/2507.16473
tags:
- option
- learning
- policy
- variational
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VMOC, a variational off-policy algorithm
  for learning hierarchical options in reinforcement learning. The core innovation
  is formulating option learning as a structured variational inference problem, which
  naturally incorporates maximum entropy regularization to improve exploration and
  prevent option degradation.
---

# Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs

## Quick Facts
- arXiv ID: 2507.16473
- Source URL: https://arxiv.org/abs/2507.16473
- Reference count: 40
- This paper introduces VMOC, a variational off-policy algorithm for learning hierarchical options in reinforcement learning.

## Executive Summary
This paper presents VMOC, a variational off-policy algorithm for learning hierarchical options in reinforcement learning. The core innovation is formulating option learning as a structured variational inference problem, which naturally incorporates maximum entropy regularization to improve exploration and prevent option degradation. VMOC learns options as low-cost embeddings rather than complex triples, and uses soft policy iteration to optimize both option and action policies. Theoretically, the paper extends continuous MDP homomorphisms to HiT-MDPs, proving that learning in abstract option spaces preserves optimality. Empirically, VMOC significantly outperforms strong baselines on challenging Mujoco locomotion tasks and achieves competitive results on logical reasoning benchmarks. Additionally, the authors propose a cold-start supervised fine-tuning procedure that distills explicit reasoning chains into latent option embeddings, enabling efficient implicit reasoning in language models.

## Method Summary
VMOC learns hierarchical options through variational inference by maximizing the ELBO of optimal trajectory probabilities. The algorithm uses a continuous HiT-MDP framework where options are represented as low-dimensional embeddings in a vector bundle structure. The method employs soft actor-critic updates for both option and action policies, with automatic temperature adaptation to maintain entropy regularization. For language models, VMOC incorporates a cold-start supervised fine-tuning phase that distills explicit reasoning chains into latent option embeddings before RL refinement. The approach addresses sample inefficiency and option collapse issues that plague previous hierarchical reinforcement learning methods.

## Key Results
- VMOC significantly outperforms strong baselines on challenging Mujoco locomotion tasks, achieving higher returns with better sample efficiency
- The algorithm successfully learns temporally extended options that prevent option collapse through entropy regularization
- VMOC-SFT achieves competitive performance on LLM reasoning benchmarks by distilling explicit reasoning chains into latent option embeddings
- The continuous HiT-MDP homomorphism framework theoretically preserves optimality when learning in abstract option spaces

## Why This Works (Mechanism)

### Mechanism 1: Entropy-Regularized Variational Inference
The algorithm maximizes the Evidence Lower Bound (ELBO) of optimal trajectory probability, which naturally incorporates entropy terms for both option and action policies. This encourages diverse option usage rather than collapsing to a single high-reward option. The core assumption is that optimality can be modeled as a latent variable problem where the variational distribution can approximate the posterior without controlling environment dynamics. If temperature parameters are poorly tuned, the entropy bonus may be overwhelmed by reward scale, leading to premature convergence.

### Mechanism 2: Continuous HiT-MDP Homomorphisms
The authors extend MDP homomorphisms to continuous HiT-MDPs by modeling the state-option space as a vector bundle. By ensuring reward invariance and transition equivariance between original and abstract spaces, they prove that optimal policies in the abstract space lift back to optimal policies in the original space. The core assumption is that the state-option space forms a topological manifold where the abstraction map is surjective and continuous. If the learned abstraction violates equivariance properties, the optimality guarantee is lost.

### Mechanism 3: Cold-Start Latent Distillation
Supervised fine-tuning on explicit reasoning chains initializes latent option embeddings to encode high-level reasoning primitives. The model learns a posterior distribution over discrete latent options conditioned on prompt, reasoning, and answer, optimizing a variational objective to reconstruct reasoning and answer from sampled latent options. The core assumption is that the discrete latent space has sufficient capacity to capture variance in logical reasoning steps. If the KL weight is too high, the posterior collapses to the prior and the model ignores the latent option.

## Foundational Learning

- **Concept: Variational Inference (ELBO)**
  - Why needed: This is the mathematical engine of VMOC, explaining why entropy regularization appears and how the algorithm balances reconstruction against regularization
  - Quick check: In the ELBO derivation, does maximizing the entropy term H(q) increase or decrease the lower bound on the log-likelihood of the optimal trajectory?

- **Concept: Semi-Markov Decision Processes (SMDP) vs. HiT-MDP**
  - Why needed: Understanding the shift from SMDP options to Hidden Temporal MDPs (HiT-MDP) is crucial for implementing off-policy updates effectively
  - Quick check: Why does the HiT-MDP formulation allow for the use of a standard replay buffer more effectively than the traditional SMDP option framework?

- **Concept: Vector Bundles (Topology)**
  - Why needed: This theoretical tool defines the homomorphism structure, helping interpret the "abstract space" the paper claims to learn
  - Quick check: In this paper, what does the "fiber" of the vector bundle represent: the primitive action, the environment state, or the option vector space?

## Architecture Onboarding

- **Component map:** State/Prompt → Encoder → Option Module (Embedding Matrix + Option Policy) → Action Module (Action Policy + Action Critic) → Option Critic + Latent Decoder (for LLMs)

- **Critical path:**
  1. Cold Start (if LLM): Train Encoder, Option Embeddings, and Decoder using SFT data to minimize Reconstruction Loss + KL Divergence
  2. RL Phase: Initialize embedding matrix, sample option, condition Action Policy, execute action
  3. Update: Perform Soft Actor-Critic updates for Q_A/Q_O (Critics) and π_A/π_O (Policies) using ELBO gradients

- **Design tradeoffs:** Uses discrete option indices mapped to continuous embeddings for LLM token vocabulary compatibility, limiting expressiveness compared to purely continuous spaces. Off-policy implementation solves sample inefficiency but requires careful replay buffer handling.

- **Failure signatures:**
  - Option Collapse: π_O assigns probability ≈ 1 to a single option index for all states
  - Posterior Collapse (LLM): Decoder ignores sampled latent option and relies solely on prompt/encoder state
  - Trivial Termination: Options switch every timestep, failing to learn temporal abstractions

- **First 3 experiments:**
  1. Run VMOC on Humanoid-v2 against PPO and DAC to verify sample efficiency and stability
  2. Disable automatic temperature adjustment to verify if option degradation occurs
  3. Train VMOC-SFT on GSM8k-Aug and test on GSM-HARD to evaluate latent reasoning generalization

## Open Questions the Paper Calls Out

### Open Question 1
Can the variational homomorphism framework be extended to multi-task settings where option embeddings must generalize across distinct reward functions? The paper focuses on single-task environments and does not address how the homomorphism behaves when task distributions change. Empirical evaluation in multi-task RL benchmarks would resolve this.

### Open Question 2
Does the RL fine-tuning phase after cold-start training yield significant improvements in reasoning capability compared to supervised-only baseline? Section 4.5 proposes RL refinement but experiments only evaluate the static VMOC-SFT model. Ablation studies showing performance before and after online RL fine-tuning would provide clarity.

### Open Question 3
How does approximation error of the neural homomorphism mapping quantitatively impact the optimality gap in continuous control tasks? Theorem 11 assumes exact homomorphisms but practical VMOC uses neural networks introducing errors. Analysis correlating reconstruction error with policy sub-optimality would address this gap.

## Limitations

- The optimality preservation theorem assumes continuous and surjective abstraction maps that may not hold for finite-sample approximations
- Key hyperparameters remain unspecified including replay buffer capacity, target network update rates, and target entropy values
- The cold-start SFT approach requires high-quality CoT data, limiting applicability to domains where explicit reasoning traces are scarce

## Confidence

**High confidence:** The variational inference formulation and maximum entropy regularization mechanism are mathematically sound and well-supported by ELBO derivation. Off-policy implementation using soft actor-critic updates is standard and reproducible.

**Medium confidence:** The optimality preservation theorem is valid under stated assumptions, but empirical validation of homomorphism properties is limited. Ablation showing entropy regularization prevents option collapse is suggestive but not conclusive.

**Low confidence:** Cold-start SFT claims for LLM reasoning rely on unpublished architectural details and hyperparameter settings. Claims about latent options generalizing better than explicit CoT are based on limited out-of-distribution tests without controlled comparisons.

## Next Checks

1. **Homomorphism verification:** After training VMOC on a simple gridworld, measure whether learned option embeddings preserve transition dynamics by testing if distinct optimal state-option pairs map to distinct abstract states with consistent reward structures.

2. **Temperature sensitivity analysis:** Run ablation experiments systematically varying α_O and α_A across multiple orders of magnitude (0.001 to 0.5) to identify the exact threshold where option collapse occurs.

3. **Generalization stress test:** Evaluate cold-start VMOC-SFT on LLM reasoning tasks where training CoT data contains logical fallacies or incomplete reasoning, testing whether latent options can correct for noisy supervision or simply memorize flawed patterns.