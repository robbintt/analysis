---
ver: rpa2
title: 'Foundation of Intelligence: Review of Math Word Problems from Human Cognition
  Perspective'
arxiv_id: '2510.21999'
source_url: https://arxiv.org/abs/2510.21999
tags:
- reasoning
- problem
- language
- learning
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides the first comprehensive survey of math word
  problem (MWP) research from a human cognitive perspective. The authors organize
  MWP solving methods around five key cognitive abilities: Problem Understanding,
  Logical Organization, Associative Memory, Critical Thinking, and Knowledge Learning.'
---

# Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective

## Quick Facts
- arXiv ID: 2510.21999
- Source URL: https://arxiv.org/abs/2510.21999
- Reference count: 40
- This paper provides the first comprehensive survey of math word problem (MWP) research from a human cognitive perspective.

## Executive Summary
This paper presents the first systematic survey of math word problem (MWP) solving research through the lens of human cognitive abilities. The authors organize MWP solving methods around five key cognitive capabilities: Problem Understanding, Logical Organization, Associative Memory, Critical Thinking, and Knowledge Learning. They review both traditional neural network solvers and large language model approaches, analyzing how each simulates different cognitive processes. The survey includes experimental comparisons across 14 small models, 4 LLMs, and 10 LLM-based methods on 5 mainstream benchmarks, revealing that while methods excel at basic cognitive abilities, higher-order capabilities like critical thinking and knowledge learning remain underdeveloped.

## Method Summary
The paper conducts a comprehensive survey of MWP solving methods, organizing them according to a cognitive framework derived from human problem-solving psychology. The authors categorize approaches based on five cognitive abilities and analyze how different model architectures simulate these capabilities. They perform experimental comparisons across multiple model types and benchmarks, examining performance on various cognitive dimensions. The analysis combines qualitative assessment of model architectures with quantitative evaluation of their outputs on standard MWP benchmarks.

## Key Results
- Methods excel at problem understanding and logical organization but struggle with higher-order cognitive abilities like critical thinking and knowledge learning
- Large language models show improved performance on MWP tasks compared to traditional neural network solvers
- The survey identifies a significant research gap in incorporating human-like reasoning capabilities into MWP solving systems

## Why This Works (Mechanism)
The cognitive framework provides a structured approach to understanding MWP solving by mapping computational methods to human cognitive processes. By analyzing how different models simulate specific cognitive abilities, the paper reveals which aspects of human-like reasoning are well-captured by current AI approaches and which remain challenging. This mechanism helps identify gaps in current methods and suggests directions for developing more human-like reasoning systems.

## Foundational Learning
- **Problem Understanding**: Converting textual problem descriptions into mathematical representations - needed to bridge natural language and formal mathematics; quick check: can the model accurately identify quantities, entities, and relationships in word problems
- **Logical Organization**: Structuring problem information into solvable mathematical expressions - needed to transform unstructured text into formal problem-solving steps; quick check: can the model generate correct equation structures from problem descriptions
- **Associative Memory**: Retrieving relevant mathematical concepts and solution patterns - needed to apply learned knowledge to new problem types; quick check: can the model recall and apply appropriate mathematical operations for similar problem patterns
- **Critical Thinking**: Analyzing problem requirements and selecting appropriate solution strategies - needed to handle complex, multi-step problems requiring strategic reasoning; quick check: can the model handle problems requiring multiple solution paths or non-obvious approaches
- **Knowledge Learning**: Acquiring and applying mathematical concepts beyond training data - needed for generalization to novel problem types; quick check: can the model solve problems requiring concepts not explicitly present in training data

## Architecture Onboarding

**Component Map:** Input Text -> Problem Understanding Module -> Logical Organization Module -> Solution Generation Module -> Output

**Critical Path:** The most critical path is from Problem Understanding through Logical Organization to Solution Generation, as errors in early stages propagate to final answers. The framework emphasizes that success depends on accurate comprehension of problem structure before mathematical formulation.

**Design Tradeoffs:** The survey reveals tradeoffs between model complexity and cognitive capability simulation. Traditional neural networks excel at pattern recognition but struggle with higher-order reasoning, while LLMs show better generalization but may lack precise mathematical formulation capabilities. The choice between approaches depends on whether the priority is accurate computation or flexible reasoning.

**Failure Signatures:** Common failure modes include misinterpreting problem context (Problem Understanding failure), generating incorrect equation structures (Logical Organization failure), and applying inappropriate solution strategies (Critical Thinking failure). The analysis shows that models often fail on problems requiring integration of multiple cognitive abilities simultaneously.

**First Experiments:**
1. Test model performance on problems requiring only single cognitive abilities (baseline assessment)
2. Evaluate performance degradation when combining multiple cognitive abilities in complex problems
3. Compare model explanations for correct vs. incorrect answers to identify which cognitive abilities are most frequently missed

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- The cognitive framework primarily draws from Western cognitive psychology literature, potentially overlooking culturally diverse problem-solving approaches
- Experimental comparisons cover 14 small models, 4 LLMs, and 10 LLM-based methods but may not represent all MWP solving approaches
- Focus on English-language benchmarks may limit applicability to multilingual or culturally-specific problem types
- Analysis relies heavily on qualitative assessment of model architectures and outputs rather than direct measurement of cognitive processes

## Confidence

**High confidence in:**
- Systematic organization of MWP research by cognitive abilities
- Experimental results comparing model performance across benchmarks

**Medium confidence in:**
- Identification of research gaps regarding higher-order cognitive abilities
- Analysis of which cognitive capabilities are most challenging for current methods

## Next Checks

1. Conduct cross-cultural validation by testing survey framework against MWP solving approaches from non-Western educational traditions
2. Perform quantitative cognitive process analysis using established human problem-solving metrics to validate model performance assessments
3. Expand experimental scope to include additional benchmarks covering diverse problem types and difficulty levels beyond the current 5 mainstream datasets