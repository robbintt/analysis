---
ver: rpa2
title: Optimization of Activity Batching Policies in Business Processes
arxiv_id: '2507.15457'
source_url: https://arxiv.org/abs/2507.15457
tags:
- batch
- batching
- time
- activity
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an approach to optimize activity batching policies
  in business processes, addressing the trade-off between cost, processing effort,
  and waiting time. The method uses 19 heuristic interventions to identify and adjust
  batching policies based on scenarios affecting waiting time, processing time, cost
  efficiency, and resource utilization.
---

# Optimization of Activity Batching Policies in Business Processes

## Quick Facts
- **arXiv ID:** 2507.15457
- **Source URL:** https://arxiv.org/abs/2507.15457
- **Reference count:** 18
- **Primary result:** Heuristic-guided meta-heuristics achieved up to 80% improvement in convergence and reduced cycle time by up to 721 hours in real-life business processes.

## Executive Summary
This paper presents a Pareto optimization approach for activity batching policies in business processes, addressing the trade-off between cost, processing effort, and waiting time. The method uses 19 heuristic interventions to identify and adjust batching policies based on specific inefficiency scenarios. These heuristics are embedded within three meta-heuristics (hill-climbing, simulated annealing, and reinforcement learning) to explore the policy space. Evaluation on 10 real-life business processes demonstrates that heuristic-guided approaches outperform non-guided meta-heuristics in convergence, diversity, and cycle time reduction.

## Method Summary
The approach formulates activity batching optimization as a multi-objective problem minimizing cumulative cycle time and cost. The method uses three meta-heuristics—Hill-Climbing, Simulated Annealing, and Reinforcement Learning (PPO)—guided by 19 domain-specific intervention heuristics that identify and address batching inefficiencies. Each candidate policy is evaluated through simulation using discovered process models from event logs. The optimization maintains a Pareto front of non-dominated solutions, iteratively improving the trade-off between cycle time and cost through targeted policy mutations rather than random exploration.

## Key Results
- Heuristic-guided approaches achieved up to 80% improvement in convergence compared to non-guided variants
- Cycle time was reduced by up to 721 hours in some processes
- Reinforcement Learning showed the highest adaptability but required the longest runtime (averaging 11.3 hours)
- Simulated Annealing+ outperformed non-guided SA- in 55% of diversity cases

## Why This Works (Mechanism)

### Mechanism 1: Domain-Constrained Search Space Reduction
The system achieves higher quality Pareto fronts by restricting policy mutations to 19 domain-specific intervention heuristics rather than random perturbation. Instead of randomly adjusting batch sizes or timeouts, the system identifies specific inefficiency scenarios and applies targeted logical adjustments, directing the search toward high-probability improvements. This assumes the 19 scenarios cover most inefficiency causes in business process batching.

### Mechanism 2: Multi-Objective Trade-off via Pareto Dominance
The architecture balances conflicting metrics (cycle time vs. cost) by maintaining a set of non-dominated solutions rather than seeking a single optimum. The simulation evaluates a candidate policy against two objectives (cumulative cycle time and cumulative cost). A solution is added to the Pareto front only if it improves at least one objective without degrading the other, forcing exploration of the efficient trade-off boundary.

### Mechanism 3: Adaptive Exploration Strategy (RL)
Reinforcement Learning (PPO) adapts the intervention selection policy based on simulation feedback, outperforming static search strategies in complex environments. The RL agent treats intervention selection as a Markov Decision Process, taking the process state as input and selecting a heuristic action. The reward function incentivizes dominance or improvement, allowing continuous adaptation over time.

## Foundational Learning

- **Batch Activation Rules (Size vs. Time):** Understanding complex predicates like "daily at 8 AM" or "after 10 items" is required to interpret how interventions modify the policy. Quick check: Can you distinguish between a "time-to-live" threshold and an "inactivity" threshold in a batch activation rule?

- **Pareto Dominance:** The output is a front of trade-offs, not a single policy. You must understand why a solution with higher cost but lower waiting time is kept alongside a cheaper, slower one. Quick check: Given two solutions A (Cost: 100, Time: 10) and B (Cost: 90, Time: 12), which one(s) belong on the Pareto front?

- **Meta-heuristics (Hill-Climbing vs. Simulated Annealing):** Understanding the difference between local radius search (HC) and stochastic acceptance (SA) explains the convergence results. Quick check: Why might Simulated Annealing escape a local optimum that traps Hill-Climbing?

## Architecture Onboarding

- **Component map:** Event Log -> Simulation Model -> (Loop: Intervene -> Simulate -> Evaluate Dominance)
- **Critical path:** Event Log -> Simulation Model -> (Loop: Intervene -> Simulate -> Evaluate Dominance). The reliability of the simulation model is the primary dependency.
- **Design tradeoffs:**
  - Hill-Climbing: Low latency, low compute; high risk of local optima
  - Simulated Annealing: Better global search; requires tuning cooling factor/temperature
  - RL (PPO): Highest adaptation; highest compute cost and implementation complexity
- **Failure signatures:**
  - Stagnation: Pareto front stops updating early (likely in HC without reheating or diverse heuristics)
  - Dominance Collapse: All new solutions are dominated; metrics do not improve
  - High Variance: Significant jitter in results; check simulation stability
- **First 3 experiments:**
  1. Baseline Sanity Check: Run HC- (random) vs. HC+ (heuristic) on a simple process to verify heuristics speed up convergence
  2. Isolate a Heuristic: Manually disable all heuristics except "Waiting Time" on a process with known delays to observe specific impact on cycle time
  3. Stress Test RL Reward: Modify the reward function weights to see if the RL agent becomes too conservative or too aggressive

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the batching policy optimization approach be extended to incorporate data-aware activation conditions (e.g., order type, urgency level, or production constraints)?
**Basis:** The conclusion explicitly states this as a future work avenue.
**Why unresolved:** Current activation rules only support size-based, time-based, and scheduled-based conditions, not data attributes of activity instances.
**What evidence would resolve it:** An extended batching model with formalized data-aware activation conditions, implemented heuristics that identify data-driven batching opportunities, and experimental evaluation showing performance improvements.

### Open Question 2
**Question:** Can batching policy optimization be integrated with activity prioritization and multitasking policy optimization in a unified framework?
**Basis:** The conclusion states this as another future work avenue.
**Why unresolved:** The current approach optimizes only batching policies while treating prioritization and multitasking as fixed, though these mechanisms interact.
**What evidence would resolve it:** A unified optimization framework covering all three policy types with experimental results demonstrating whether joint optimization yields Pareto fronts that dominate independently optimized policies.

### Open Question 3
**Question:** How does the heuristic-guided approach compare against other established optimization methods for batching, such as genetic algorithms or analytical queuing theory models?
**Basis:** Related work mentions genetic algorithms and queuing theory as alternatives, but the experimental evaluation only compares against non-guided variants of the same meta-heuristics.
**Why unresolved:** Without comparison to alternative optimization methods, it remains unclear whether the proposed intervention heuristics provide advantages specific to the chosen meta-heuristics or would also enhance other search strategies.
**What evidence would resolve it:** Comparative experiments benchmarking the proposed heuristic-guided meta-heuristics against genetic algorithms, particle swarm optimization, and queuing theory-based analytical methods.

### Open Question 4
**Question:** Are the 19 intervention heuristics equally effective across different process characteristics (e.g., highly unstructured vs. structured workflows, high vs. low arrival rate variability)?
**Basis:** The evaluation covers 10 diverse processes, but the paper does not analyze which heuristics contribute most to improvements in each process type.
**Why unresolved:** The heuristics encode domain assumptions that may not hold universally across all process types.
**What evidence would resolve it:** An ablation study isolating individual heuristic contributions across process types with varying structural complexity, coupled with regression analysis identifying process features that predict heuristic effectiveness.

## Limitations
- The approach's effectiveness depends on the 19 heuristics comprehensively covering real-world inefficiency scenarios, which is not empirically validated beyond the studied processes
- The simulation model's fidelity to reality is a critical dependency; any drift could render optimized policies suboptimal
- The high computational cost of RL (averaging 11.3 hours) may limit scalability for complex processes

## Confidence
- **High Confidence:** The core mechanism of using domain-specific heuristics to guide meta-heuristic search is well-supported by results showing improved convergence and cycle time reduction
- **Medium Confidence:** The claim that RL adapts intervention selection based on simulation feedback is supported, but learning efficiency and generalizability require further validation
- **Low Confidence:** The assertion that the 19 identified scenarios cover the majority of inefficiency causes is not empirically validated beyond the 10 studied processes

## Next Checks
1. **Heuristics Coverage Test:** Manually analyze a process outside the studied set to identify inefficiencies not covered by the 19 heuristics, confirming the break condition
2. **Simulation Fidelity Assessment:** Compare simulation outputs with actual process executions to quantify model drift and its impact on optimized policies
3. **Scalability Benchmark:** Measure the runtime and solution quality of RL optimization on processes with significantly higher complexity than those studied to assess practical limits