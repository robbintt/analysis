---
ver: rpa2
title: Enhancing Generalization in Evolutionary Feature Construction for Symbolic
  Regression through Vicinal Jensen Gap Minimization
arxiv_id: '2602.01510'
source_url: https://arxiv.org/abs/2602.01510
tags:
- uni00000013
- uni00000048
- uni00000051
- vicinal
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of overfitting in genetic programming-based
  feature construction for symbolic regression. The core method idea involves decomposing
  vicinal risk into empirical loss and regularization terms (either vicinal Jensen
  gap or finite difference) to enable flexible control over model complexity.
---

# Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization

## Quick Facts
- **arXiv ID:** 2602.01510
- **Source URL:** https://arxiv.org/abs/2602.01510
- **Reference count:** 40
- **Primary result:** Vicinal Jensen gap minimization significantly outperforms 7 complexity measures and 15 ML baselines on 58 datasets, achieving higher test R² on 36 datasets while underperforming on only 5 compared to standard GP.

## Executive Summary
This paper addresses overfitting in genetic programming-based feature construction for symbolic regression by introducing vicinal Jensen gap minimization. The method decomposes vicinal risk into empirical loss and regularization terms, enabling flexible control over model complexity. Through noise estimation and manifold intrusion detection, the approach dynamically adjusts regularization strength and avoids unrealistic synthesized samples. Experiments on 58 real-world datasets demonstrate superior generalization performance compared to both traditional complexity measures and standard machine learning algorithms.

## Method Summary
The approach combines multi-tree genetic programming with vicinal risk minimization to control overfitting in feature construction. The method uses mixup-based vicinal data synthesis with label-based kernel sampling to generate realistic training samples. A two-objective optimization framework minimizes both cross-validation loss and vicinal Jensen gap, with noise estimation dynamically adjusting regularization strength based on dataset characteristics. Manifold intrusion detection prevents the generation of unrealistic samples by checking bounds using Extra Trees. The final model selection incorporates both objectives with a noise-aware weight adjustment.

## Key Results
- Vicinal Jensen gap minimization outperforms 7 other complexity measures across 58 datasets
- Achieves higher test R² scores on 36 datasets compared to standard GP
- Underperforms on only 5 datasets while matching or exceeding performance on the remaining 17
- Outperforms 15 baseline machine learning algorithms in terms of generalization capability

## Why This Works (Mechanism)
The vicinal Jensen gap minimization works by explicitly accounting for the model's behavior on vicinal (neighboring) data points rather than just the empirical training set. By decomposing vicinal risk into empirical loss plus regularization terms, the method can flexibly control model complexity based on the estimated noise level in the data. The dynamic adjustment of regularization strength through noise estimation ensures appropriate complexity control for both clean and noisy datasets. Manifold intrusion detection further improves generalization by preventing the model from learning from unrealistic synthetic samples that fall outside the data manifold.

## Foundational Learning

**Genetic Programming with Multi-Trees:** Needed to construct complex features from multiple interacting subtrees; check by verifying tree depth grows from 3 to 10 and multiple trees per individual are properly combined.

**Vicinal Risk Minimization:** Required to optimize model performance on neighboring data points rather than just training samples; check by implementing mixup synthesis and verifying vicinal Jensen gap computation.

**Lexicase Selection:** Essential for maintaining population diversity in multi-objective optimization; check by implementing ε-lexicase with median absolute deviation threshold and verifying parent selection diversity.

**NSGA-II Environmental Selection:** Needed to maintain Pareto-optimal solutions across generations; check by verifying non-dominated sorting and crowding distance calculations work correctly.

**Noise Estimation via Extra Trees:** Required to dynamically adjust regularization strength based on dataset characteristics; check by computing 5-fold CV R² and verifying τ adjustment threshold at R² = 0.5.

## Architecture Onboarding

**Component Map:** GP Individual -> Vicinal Synthesis -> Jensen Gap Computation -> Two-Objective Optimization -> Noise Estimation -> Manifold Intrusion Detection -> Final Model Selection

**Critical Path:** Population initialization → Lexicase parent selection → Vicinal data synthesis → Vicinal Jensen gap computation → NSGA-II survival selection → Noise estimation → Manifold intrusion detection → Final model selection

**Design Tradeoffs:** The method trades computational complexity (extra vicinal computations) for improved generalization, with the noise estimation mechanism providing adaptive complexity control that outperforms fixed regularization approaches.

**Failure Signatures:** Poor generalization occurs when manifold intrusion detection fails (synthetic samples fall outside realistic bounds) or when noise estimation incorrectly sets τ (over/under-regularization for the dataset's noise level).

**First Experiments:**
1. Implement basic GP with multi-tree individuals and verify tree depth and function set parameters
2. Test vicinal data synthesis with mixup parameters and verify label-based kernel sampling
3. Validate two-objective optimization by computing cross-validation loss and vicinal Jensen gap separately

## Open Questions the Paper Calls Out
None

## Limitations
- Ridge regularization coefficient α in the linear model weights is unspecified, requiring empirical determination
- The R² = 0.5 threshold for noise estimation is heuristic and may not be optimal across all problem domains
- Manifold intrusion detection relies on Extra Trees bounds, which may not capture complex manifolds effectively, especially with categorical variables

## Confidence
- **High confidence:** The overall framework combining vicinal Jensen gap minimization with noise-aware regularization is well-specified and reproducible
- **Medium confidence:** The empirical results showing superior generalization compared to 15 baseline algorithms, though exact implementation details may affect reproducibility
- **Medium confidence:** The mixup-based vicinal data synthesis approach, as hyperparameters may require tuning for different domains

## Next Checks
1. Implement noise estimation verification: Test the R²-based threshold for τ adjustment across datasets with known noise levels to confirm the heuristic works as intended
2. Benchmark manifold intrusion detection: Compare the Extra Trees-based bounds approach against alternative manifold learning methods (e.g., t-SNE or UMAP) on datasets with known non-linear manifolds
3. Statistical significance analysis: Perform Wilcoxon signed-rank tests on the 36 datasets where the method outperforms standard GP to confirm results are statistically significant at α = 0.05