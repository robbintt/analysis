---
ver: rpa2
title: 'State of the Art in Text Classification for South Slavic Languages: Fine-Tuning
  or Prompting?'
arxiv_id: '2511.07989'
source_url: https://arxiv.org/abs/2511.07989
tags:
- classification
- llms
- topic
- languages
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares fine-tuned BERT-like models with large language
  models (LLMs) for text classification in South Slavic languages across three tasks:
  sentiment analysis, topic classification, and genre identification. LLMs, used in
  zero-shot prompting, achieved performance comparable to or better than fine-tuned
  models, with closed-source models like GPT-4o and Gemini 2.5 Flash performing best.'
---

# State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?

## Quick Facts
- arXiv ID: 2511.07989
- Source URL: https://arxiv.org/abs/2511.07989
- Reference count: 0
- Large language models achieve comparable or superior performance to fine-tuned BERT-like models for text classification in South Slavic languages across sentiment, topic, and genre tasks.

## Executive Summary
This study compares zero-shot large language model (LLM) prompting with fine-tuned BERT-like models for text classification tasks in South Slavic languages. The research evaluates three classification tasks - sentiment analysis, topic classification, and genre identification - across multiple LLM and fine-tuned model variants. Results show that LLMs, particularly closed-source models like GPT-4o and Gemini 2.5 Flash, achieve performance comparable to or better than fine-tuned models, with open-source models like Gemma 3 also showing strong results. The study demonstrates that LLMs perform similarly on South Slavic languages as on English, with only minor performance drops.

## Method Summary
The study employs a comparative evaluation framework testing both zero-shot LLM prompting and fine-tuned BERT-like models across three South Slavic languages. LLMs are evaluated using zero-shot prompting without task-specific fine-tuning, while BERT-like models undergo traditional fine-tuning on task-specific datasets. The evaluation covers sentiment analysis, topic classification, and genre identification tasks, measuring performance metrics and computational costs. The study specifically compares closed-source models (GPT-4o, Gemini 2.5 Flash) against open-source alternatives (Gemma 3) and traditional fine-tuned approaches.

## Key Results
- LLMs achieved performance comparable to or better than fine-tuned models across all three classification tasks
- Closed-source models (GPT-4o, Gemini 2.5 Flash) performed best, with open-source models like Gemma 3 also showing strong results
- LLMs demonstrated similar performance on South Slavic languages as on English with only minor drops
- Fine-tuned BERT-like models remained more efficient and practical for large-scale annotation due to lower computational costs

## Why This Works (Mechanism)
LLMs leverage their broad pretraining on diverse web-scale data, which includes multilingual content and various text genres. Their transformer-based architecture with attention mechanisms enables effective cross-lingual transfer, allowing them to generalize from high-resource languages to low-resource South Slavic languages. The zero-shot prompting capability means these models can adapt to new tasks through natural language instructions without requiring task-specific fine-tuning, making them immediately applicable across different classification scenarios.

## Foundational Learning
- **Transformer architecture**: The self-attention mechanism allows models to capture long-range dependencies and contextual relationships in text, essential for understanding nuanced sentiment, topics, and genres.
- **Cross-lingual transfer learning**: Models pretrained on multilingual data can transfer knowledge from high-resource languages to low-resource languages, enabling effective classification without language-specific fine-tuning.
- **Prompt engineering**: The ability to formulate effective task descriptions in natural language is crucial for zero-shot performance, as it guides the model's reasoning process.
- **Fine-tuning vs. zero-shot trade-offs**: Understanding when to use computationally expensive fine-tuning versus zero-shot approaches based on task requirements and resource constraints.
- **Multilingual representation learning**: Models learn shared semantic spaces across languages, enabling transfer learning and reducing the need for language-specific resources.

## Architecture Onboarding
- **Component map**: Input text -> LLM backbone (GPT-4o/Gemini 2.5/Gemma 3) -> Prompt processing -> Classification output
- **Critical path**: Text input → Prompt encoding → Model inference → Output classification
- **Design tradeoffs**: Zero-shot prompting offers immediate applicability and generalization but lacks task-specific optimization; fine-tuning provides better efficiency and reliability but requires labeled data and computational resources.
- **Failure signatures**: Poor prompt formulation, domain mismatch between pretraining data and target task, insufficient model capacity for complex classification boundaries.
- **First experiments**: 1) Compare zero-shot prompting with different prompt formulations on same task, 2) Measure inference latency and cost across model variants, 3) Test cross-lingual generalization by evaluating English-trained models on South Slavic languages.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on narrow classification tasks (sentiment, topic, genre) limiting generalizability to other domains
- Zero-shot prompting quality depends on prompt engineering, which introduces performance variability not quantified
- Computational cost comparison lacks systematic benchmarking across different hardware and batch configurations
- Comparison between closed-source and open-source models confounds architecture differences with training data and optimization variations
- South Slavic language focus limits generalizability to other low-resource language families

## Confidence
- **High confidence**: LLMs achieve comparable performance to fine-tuned models on South Slavic languages in zero-shot settings
- **Medium confidence**: Relative model rankings and observation that closed-source models outperform open-source alternatives
- **Medium confidence**: LLMs perform similarly on South Slavic languages as on English with minor performance drops

## Next Checks
1. Conduct systematic prompt engineering experiments to quantify impact of different prompting strategies on LLM performance variability
2. Expand evaluation to include additional classification tasks beyond sentiment, topic, and genre to test generalizability
3. Perform controlled benchmarking of computational costs across different hardware setups and batch processing configurations