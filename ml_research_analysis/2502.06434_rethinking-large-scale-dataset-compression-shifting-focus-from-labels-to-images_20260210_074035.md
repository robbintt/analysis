---
ver: rpa2
title: 'Rethinking Large-scale Dataset Compression: Shifting Focus From Labels to
  Images'
arxiv_id: '2502.06434'
source_url: https://arxiv.org/abs/2502.06434
tags:
- dataset
- images
- pruning
- labels
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies critical flaws in large-scale dataset compression
  evaluation, showing that existing dataset distillation methods relying on soft labels
  often underperform simple random baselines. The authors establish a fair benchmark
  by unifying evaluation protocols and demonstrate that an overemphasis on soft labels
  diverts focus from the intrinsic value of image data.
---

# Rethinking Large-scale Dataset Compression: Shifting Focus From Labels to Images

## Quick Facts
- arXiv ID: 2502.06434
- Source URL: https://arxiv.org/abs/2502.06434
- Reference count: 40
- Primary result: PCA (Prune, Combine, Augment) framework achieves up to 45.5% accuracy on ImageNet-1K with only 100 images per class using hard labels, surpassing state-of-the-art methods.

## Executive Summary
This paper identifies critical flaws in large-scale dataset compression evaluation, showing that existing dataset distillation methods relying on soft labels often underperform simple random baselines. The authors establish a fair benchmark by unifying evaluation protocols and demonstrate that an overemphasis on soft labels diverts focus from the intrinsic value of image data. They propose the Prune, Combine, and Augment (PCA) framework, which exclusively uses hard labels and applies pruning insights to select representative, balanced, and "easy" images. PCA combines images and applies scaling-law-aware augmentation to enhance performance without requiring pretrained models or storing soft labels. Extensive experiments on ImageNet-1K show that PCA consistently surpasses both random baselines and state-of-the-art methods across various model architectures, achieving up to 45.5% accuracy with only 100 images per class. The framework reduces storage and computational costs while shifting research focus back to the images themselves, paving the way for more balanced and accessible dataset compression techniques.

## Method Summary
The PCA framework consists of three stages: Prune, Combine, and Augment. First, the Prune stage uses EL2N (Early Learning to Noise) scores computed from a pretrained model's early training to select the easiest, most representative images per class while maintaining strict balance. Second, the Combine stage merges selected images into composite mosaics (e.g., 2x2 grids) without irreversible cropping to preserve information. Third, the Augment stage applies scaling-law-aware patch extraction during training, using RandomResizedCrop and Cutout while avoiding label-mixing augmentations like Mixup or CutMix that can introduce complexity. The framework exclusively uses hard labels, eliminating the need for soft label storage and enabling fair evaluation across dataset compression methods.

## Key Results
- PCA achieves 45.5% accuracy on ImageNet-1K with 100 images per class using hard labels, significantly outperforming random baselines (31.7%) and state-of-the-art methods.
- Under hard-label evaluation, many SOTA methods that rely on soft labels fail to surpass random baselines, revealing a fundamental flaw in current evaluation protocols.
- The Augment component (patch extraction) is the most impactful, followed by Combine and Prune, as shown in ablation studies.
- PCA demonstrates strong generalization across different model architectures, though transformers like Swin-V2-Tiny remain challenging due to data-hunger.

## Why This Works (Mechanism)

### Mechanism 1: Soft-label exploitation can mask insufficient image quality
Soft labels encode rich teacher knowledge; training with them can enable learning from weak or uninformative images by directly optimizing to match teacher outputs. When evaluations use soft labels from pretrained models, even randomly selected or noised images can appear competitive with state-of-the-art methods. This suggests that an overemphasis on soft labels may be diverting attention from the intrinsic value of the image data.

### Mechanism 2: PCA (Prune, Combine, Augment) shifts focus to image quality via selection and scaling-aware augmentation
PCA's three-stage pipeline addresses this by: (1) Prune uses low-EL2N (easy) images with class balance → lower dataset entropy; (2) Combine merges whole images without irreversible cropping → preserves information; (3) Patch-extraction augmentation during training keeps samples simple and scaling-law aware. This hard-label-only approach demonstrates that careful image selection and appropriate augmentation can outperform soft-label-dependent methods.

### Mechanism 3: Training-time augmentation quality is critical under extreme compression
When dataset size is tiny, augmentation strategy must preserve sample simplicity; label-mixing or aggressive cross-sample augmentation can degrade performance. Small datasets are easily overfit or confused; augmentation should increase diversity without introducing complexity or label noise. The framework's use of patch extraction and Cutout maintains simplicity while avoiding the pitfalls of Mixup or CutMix in low-data regimes.

## Foundational Learning

- **Concept: Dataset Distillation vs. Dataset Pruning**
  - **Why needed here:** The paper unifies evaluation across these two historically separate approaches; understanding their tradeoffs (synthetic vs. real images, soft vs. hard labels) is essential.
  - **Quick check question:** Can you explain why distillation methods often use soft labels while pruning methods typically use hard labels?

- **Concept: Soft Labels vs. Hard Labels in Evaluation**
  - **Why needed here:** A core finding is that soft-label evaluation can hide poor image quality; hard-label evaluation reveals true compression quality.
  - **Quick check question:** What storage and implementation burdens do soft labels introduce (per Section 3)?

- **Concept: Scaling Laws for Dataset Size (Small-data Regime)**
  - **Why needed here:** PCA's design choices (easy samples, class balance, simplicity-preserving augmentation) are justified by scaling-law insights.
  - **Quick check question:** According to the paper, what happens to optimal image difficulty as dataset size shrinks?

## Architecture Onboarding

- **Component map:** EL2N scores -> Prune (select easy, balanced images) -> Combine (create mosaics) -> Augment (patch extraction + Cutout) -> Train with hard labels

- **Critical path:**
  1. Compute EL2N scores on full dataset using pretrained model (early training only)
  2. Select top-k lowest-scoring images per class (k = IPC)
  3. Combine selected images into mosaics (2x2 grids)
  4. Store combined images with hard labels
  5. Apply patch extraction augmentation during training

- **Design tradeoffs:**
  - EL2N vs. AUM: EL2N faster (early training only), AUM slightly better at larger IPCs
  - Combine vs. Mosaic: Combine saves storage, Mosaic requires 4x I/O
  - Patch extraction vs. shuffling: Extraction preserves simplicity, shuffling may introduce complexity

- **Failure signatures:**
  - High variance across runs with small IPC → insufficient class balance or too-hard samples
  - Hard-label accuracy near random baseline → soft labels leaked or augmentation too aggressive
  - Poor cross-architecture transfer (e.g., Swin-V2-Tiny) → transformer data-hunger

- **First 3 experiments:**
  1. Baseline consistency check: Implement Random + hard-label training to verify results (~4.6% at IPC10, ResNet-18)
  2. Ablation of PCA components: Test Prune-only, Prune+Combine, and full PCA to confirm each step adds improvement
  3. Soft vs. hard label comparison: Evaluate SOTA method (e.g., SRe2L) with both label types to observe performance drop

## Open Questions the Paper Calls Out

### Open Question 1
Can the PCA framework be effectively applied to non-original data sources, such as high-performing distilled datasets or diffusion-generated synthetic datasets, to yield further performance improvements? The paper suggests significant value in considering pruning on potentially high-performing distilled datasets (e.g., YOCO) or generated datasets (e.g., diffusion-based DD methods) rather than limiting pruning to the original dataset.

### Open Question 2
How can the augmentation component of the PCA framework be theoretically optimized rather than relying on heuristics? While the paper proposes "patch extraction" to align with scaling laws, the specific choices are empirically determined without a theoretical guarantee of optimality for maximizing information density.

### Open Question 3
How can dataset compression methods be adapted to maintain performance on data-hungry architectures like Vision Transformers (ViTs), which currently underperform compared to CNNs in this hard-label setting? The paper notes that the framework scales well with model capacity "with one exception on the transformer-based Swin-V2-Tiny model," attributing this to the model being "extremely data-hungry."

### Open Question 4
Can the framework be modified to jointly optimize for metrics beyond accuracy, such as robustness or fairness, while maintaining the benefits of hard-label-only storage? The paper suggests that future frameworks might also jointly optimize additional metrics, such as robustness, fairness, or interpretability, while maintaining the same compressed dataset constraint.

## Limitations

- The "Combine" stage implementation details are not fully specified, creating barriers to faithful reproduction
- The framework's performance on non-ImageNet datasets and different class distributions remains untested
- Transformers like Swin-V2-Tiny underperform significantly, suggesting the framework may not generalize well to data-hungry architectures

## Confidence

- **High confidence:** The core finding that soft-label evaluation can mask poor image quality is well-supported by controlled experiments and the observation that random baselines with soft labels achieve surprisingly competitive performance.
- **Medium confidence:** The PCA framework's individual components show consistent improvements in ablation studies, though exact implementation details leave some uncertainty about reproducibility.
- **Medium confidence:** The claim that PCA shifts research focus back to image data is compelling but requires validation across more datasets and compression ratios beyond ImageNet-1K.

## Next Checks

1. Reproduce the hard-label baseline: Implement Random + hard-label training with identical hyperparameters to PCA and verify the reported performance (e.g., ~4.6% at IPC=10 with ResNet-18).

2. Test across architectures: Evaluate PCA on architectures beyond ResNet-18 (e.g., Swin-V2-Tiny or ConvNeXt-Tiny) to assess cross-architecture transfer and identify if transformers require modified approaches due to data-hunger.

3. Validate on non-ImageNet datasets: Apply PCA to datasets with different characteristics (e.g., CIFAR-100 for smaller scale, or long-tailed distributions) to test the framework's robustness and generalization beyond the controlled ImageNet-1K setting.