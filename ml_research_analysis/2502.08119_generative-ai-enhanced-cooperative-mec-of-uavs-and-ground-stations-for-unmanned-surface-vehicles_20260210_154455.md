---
ver: rpa2
title: Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned
  Surface Vehicles
arxiv_id: '2502.08119'
source_url: https://arxiv.org/abs/2502.08119
tags:
- usvs
- task
- time
- gai-happo
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of computational support for
  unmanned surface vehicles (USVs) in maritime applications through a cooperative
  framework combining unmanned aerial vehicles (UAVs) and ground stations (GSs) within
  a multi-access edge computing (MEC) environment. The key problem involves task offloading
  and UAV trajectory optimization under uncertainties, heterogeneous devices, and
  limited resources.
---

# Generative AI-Enhanced Cooperative MEC of UAVs and Ground Stations for Unmanned Surface Vehicles

## Quick Facts
- arXiv ID: 2502.08119
- Source URL: https://arxiv.org/abs/2502.08119
- Reference count: 16
- Performance: 22.8% improvement over four benchmark methods

## Executive Summary
This paper addresses computational support challenges for unmanned surface vehicles (USVs) in maritime environments through a cooperative framework combining UAVs and ground stations within multi-access edge computing. The authors propose GAI-HAPPO, a generative AI-enhanced heterogeneous agent proximal policy optimization algorithm that integrates transformer-based actor networks for complex environment modeling and GAN-based critic networks for training stability. The method demonstrates significant performance gains over baseline approaches in optimizing task offloading decisions and UAV trajectory planning under dynamic conditions.

## Method Summary
The approach combines transformer-based actor networks with GAN-based critic networks in a sequential heterogeneous agent update framework. USV agents optimize task offloading ratios and associations, while UAV agents plan trajectories. The system uses self-attention layers to capture dependencies across states and adversarial training to stabilize value estimation. The method builds on proximal policy optimization with sequential agent updates, where each agent's policy is updated considering previously updated agents' new policies.

## Key Results
- Achieves 22.8% performance improvement over four benchmark methods
- Successfully handles dynamic conditions and cross-domain challenges in MEC scenarios
- Demonstrates effective coordination between heterogeneous agents (USVs and UAVs) with different capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformer-based actor networks improve feature extraction and environment modeling for heterogeneous multi-agent systems under uncertainty.
- Mechanism: Self-attention layers compute Query, Key, Value matrices from input states, capturing dependencies across spatial-temporal dimensions. The attention output is refined through fully connected layers to predict task ratios, associations, and trajectory parameters.
- Core assumption: Attention mechanisms can capture meaningful cross-agent and temporal dependencies in state representations.
- Evidence anchors: [abstract] "integrates transformer-based actor networks for complex environment modeling" [section III-B] "allowing the model to capture complex dependencies across states"

### Mechanism 2
- Claim: GAN-based critic networks stabilize value estimation in multi-agent reinforcement learning by adversarial refinement.
- Mechanism: Generator predicts state values; discriminator learns to distinguish true values from predictions through adversarial loss functions.
- Core assumption: True value function can be approximated or sampled during training.
- Evidence anchors: [abstract] "GAN-based critic networks for training stability" [section III-B] Equations 35-36 define discriminator/generator losses explicitly

### Mechanism 3
- Claim: Sequential policy update with monotonic improvement objective enables stable heterogeneous agent coordination.
- Mechanism: Agents update policies in random permutation order, with each agent's update conditioning on others' new policies through advantage re-weighting.
- Core assumption: Sequential updates approximate joint policy improvement without requiring centralized training at execution.
- Evidence anchors: [section III-B] Algorithm 1 lines 6-11 describe sequential update; Equation 37-38 formalize the objective

## Foundational Learning

- **Multi-Agent Reinforcement Learning (MARL) with Heterogeneous Agents**: Why needed here: USVs and UAVs have different action spaces, observation access, and capabilities. Standard homogeneous MARL fails.
  - Quick check question: Can you explain why parameter sharing between USV and UAV agents would harm performance?

- **Proximal Policy Optimization (PPO) Clipping**: Why needed here: Prevents destructively large policy updates in non-stationary multi-agent settings. The clip parameter constrains policy ratio deviations.
  - Quick check question: What happens to training stability if the clip parameter is set too large (>0.5) in this multi-agent context?

- **Generative Adversarial Training Dynamics**: Why needed here: The critic network uses GAN loss for value estimation. Understanding generator-discriminator equilibrium is essential for debugging training collapse.
  - Quick check question: If discriminator accuracy reaches 100% early in training, what does this indicate about generator learning?

## Architecture Onboarding

- **Component map**: Actor Networks (Transformer-based) -> Self-attention layers -> FC layers -> Action outputs; Critic Networks (GAN-based) -> Generator predicts V(s) -> Discriminator refines via adversarial loss; HAPPO Core -> Sequential agent update loop -> Advantage re-weighting

- **Critical path**: 1. Collect trajectories under joint policy 2. Compute GAE advantage 3. For each agent in random order: update actor via clipped objective, recompute advantage 4. Update GAN generator/discriminator

- **Design tradeoffs**: Transformer depth vs. inference latency; GAN training frequency vs. value estimate freshness; Sequential vs. parallel agent updates for training speed

- **Failure signatures**: Discriminator collapse (100% accuracy early), attention degradation (near-uniform outputs), policy oscillation (high-frequency reward curve variations)

- **First 3 experiments**: 1. Ablation: GAN-only vs. Transformer-only vs. Full GAI-HAPPO 2. Scalability stress test: Vary USVs (2→10) and UAVs (2→6) independently 3. Uncertainty robustness check: Increase task arrival rate variance and USV velocity memory parameter

## Open Questions the Paper Calls Out

- **Open Question 1**: How does inclusion of strict UAV energy consumption constraints impact joint optimization of task offloading and trajectory planning?
- **Open Question 2**: Does the GAI-enhanced architecture introduce prohibitive communication overhead or training latency in large-scale swarm scenarios?
- **Open Question 3**: How robust is the learned policy to non-stationary, real-world maritime conditions that deviate from assumed statistical models?

## Limitations

- Neural network architectures remain underspecified (transformer depth, attention heads, GAN critic topology)
- Critical hyperparameters (batch size, learning rates, clipping threshold, GAN update frequency) are not reported
- Computational resources and communication parameters are missing from problem formulation

## Confidence

- **High**: 22.8% performance improvement claim is supported by explicit comparison to four baselines and convergence plots
- **Medium**: Transformer-based actor network effectiveness—mechanism is theoretically sound but corpus evidence is limited
- **Medium**: GAN-based critic stability—training dynamics are well-defined but GAN-critic integration lacks direct corpus validation
- **Low**: Real-time applicability—no latency or inference time analysis provided for transformer architecture

## Next Checks

1. **Ablation study**: Implement and compare Transformer-HAPPO, GAN-HAPPO, and standard HAPPO baselines using identical hyperparameters to isolate each GAI component's contribution

2. **Scalability testing**: Systematically vary USV (2→10) and UAV (2→6) counts independently; measure reward degradation and identify computational bottlenecks

3. **Uncertainty robustness**: Introduce task arrival rate variance and mobility parameter perturbations; measure reward stability and convergence recovery time across episodes