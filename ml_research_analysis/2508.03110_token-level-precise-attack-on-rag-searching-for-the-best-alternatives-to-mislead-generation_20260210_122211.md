---
ver: rpa2
title: 'Token-Level Precise Attack on RAG: Searching for the Best Alternatives to
  Mislead Generation'
arxiv_id: '2508.03110'
source_url: https://arxiv.org/abs/2508.03110
tags:
- attack
- tparag
- arxiv
- generation
- malicious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses vulnerabilities in Retrieval-Augmented Generation
  (RAG) systems, specifically the risk of malicious content in external knowledge
  databases being retrieved and used to manipulate model outputs. The authors propose
  Token-level Precise Attack on RAG (TPARAG), a novel framework that targets both
  black-box and white-box RAG systems by leveraging a lightweight white-box LLM to
  generate and iteratively optimize malicious passages at the token level.
---

# Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation

## Quick Facts
- arXiv ID: 2508.03110
- Source URL: https://arxiv.org/abs/2508.03110
- Authors: Zizhong Li; Haopeng Zhang; Jiawei Zhang
- Reference count: 7
- Primary result: TPARAG framework achieves state-of-the-art attack success rates by targeting both retrieval and generation stages of RAG systems

## Executive Summary
This paper introduces Token-level Precise Attack on RAG (TPARAG), a novel framework that systematically identifies vulnerabilities in Retrieval-Augmented Generation systems by injecting malicious content into external knowledge databases. The approach leverages a lightweight white-box LLM to generate and iteratively optimize malicious passages at the token level, ensuring both retrievability and high attack success in generation. Extensive experiments on open-domain QA datasets demonstrate that TPARAG consistently outperforms previous approaches in both retrieval-stage and end-to-end attack effectiveness, revealing critical vulnerabilities in RAG pipelines and offering new insights into improving their robustness.

## Method Summary
TPARAG employs a two-stage attack strategy: first, a Generation Attack stage uses a lightweight white-box LLM to generate malicious passages while recording top-k token logits per position, and second, an Optimization Attack stage applies NER-based token filtering, substitutes tokens from recorded candidates, and iteratively refines passages over 5 rounds to maximize attack success. The framework targets both black-box and white-box RAG systems by ensuring malicious content remains retrievable while effectively misleading the generation model, with experiments conducted on NaturalQuestions, TriviaQA, and PopQA using Wikipedia corpus with ATLAS embeddings.

## Key Results
- TPARAG achieves superior Attack Success Rates (ASR_R, ASR_L, ASR_T) compared to previous approaches across three open-domain QA datasets
- The framework demonstrates effectiveness in both black-box and white-box RAG settings
- Experimental results reveal critical vulnerabilities in RAG pipelines that could compromise generation outputs

## Why This Works (Mechanism)
TPARAG exploits the dual-stage nature of RAG systems by simultaneously targeting both retrieval and generation components. The generation attack creates malicious content that remains retrievable by the retriever while the optimization stage fine-tunes token-level perturbations to maximize generation deception. By leveraging a lightweight white-box LLM for attack generation and optimization, the framework can systematically explore the token space to find the most effective substitutions that maintain retrieval relevance while manipulating generation outputs.

## Foundational Learning
- RAG pipeline vulnerabilities: Understanding how malicious content can be injected at the retrieval stage to affect generation outputs is crucial for both attackers and defenders
- Token-level optimization: The approach demonstrates how fine-grained token substitutions can achieve high attack success while maintaining retrieval relevance
- Black-box vs white-box attack transferability: The use of a lightweight white-box LLM to attack black-box systems reveals important insights about model behavior generalization
- NER-based token filtering: This technique helps identify optimal attack positions by focusing on entities and key terms in passages
- Retrieval-relevance vs generation deception tradeoff: The framework balances these competing objectives to maximize overall attack effectiveness
- Iterative optimization strategies: The 5-round optimization process shows how systematic refinement can improve attack success rates

## Architecture Onboarding
Component Map: RAG pipeline (Retriever -> Reader/Generator) -> TPARAG attack (Generation Attack -> Optimization Attack -> Malicious Passage Injection)
Critical Path: Generation Attack creates initial malicious passages → Optimization Attack refines token-level perturbations → Malicious passages are injected into knowledge database → Retrieved by retriever → Used by reader to generate misleading outputs
Design Tradeoffs: Lightweight white-box LLM for attack generation vs computational cost; token substitution rate (0.2) vs attack effectiveness; similarity threshold tuning for black-box vs white-box settings
Failure Signatures: Low ASR_R despite optimization suggests similarity computation mismatch; high ASR_L but low end-to-end success indicates transferability issues between attacker and reader models
Three First Experiments: 1) Validate generation attack with k=10 candidates per position, 2) Test optimization attack with different token substitution rates (0.1, 0.2, 0.3), 3) Compare black-box vs white-box attack success rates using different lightweight LLMs

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does TPARAG's attack effectiveness vary across retrievers with different training objectives (e.g., sparse retrievers like BM25, dense retrievers trained with contrastive learning vs. multi-vector approaches)?
- Basis in paper: [explicit] The Limitations section states: "the current experiments are limited by the choice of retriever, as only Contriever is used for both black-box and white-box RAG settings. Future work could extend the evaluation to a broader range of retrievers with diverse training objectives and architectures."
- Why unresolved: The study restricted evaluation to a single dense retriever architecture, leaving unknown whether token-level perturbation strategies transfer to retrievers with fundamentally different similarity mechanisms.
- What evidence would resolve it: Attack success rates (ASR_R, ASR_L, ASR_T) across at least 3-5 diverse retriever architectures including sparse, dense, and hybrid approaches.

### Open Question 2
- Question: Does TPARAG maintain consistent attack effectiveness when scaled to datasets with thousands of queries, or does performance degrade due to increased query diversity?
- Basis in paper: [explicit] The Limitations section notes: "due to computational constraints, our main experiments are conducted on datasets with hundreds of instances. Scaling the evaluation to larger datasets (e.g., thousands of queries) would further validate the robustness and generalizability of TPARAG."
- Why unresolved: The 100-query sample per dataset may not capture the full variance in query types, answer structures, and passage characteristics that emerge at larger scales.
- What evidence would resolve it: Experiments on full training sets of NQ, TriviaQA, and PopQA with statistical analysis of performance variance across query subgroups.

### Open Question 3
- Question: How can RAG systems be explicitly hardened against token-level attacks that exploit both retrievability and generation vulnerability simultaneously?
- Basis in paper: [inferred] The abstract states the results "offer new insights into improving their robustness" and the conclusion mentions TPARAG "reveals critical vulnerabilities," but no defensive mechanisms are proposed or tested.
- Why unresolved: The paper focuses entirely on attack methodology without investigating whether detection, filtering, or adversarial training approaches could mitigate the identified vulnerabilities.
- What evidence would resolve it: Evaluation of defense strategies (e.g., passage verification, adversarial training, anomaly detection) against TPARAG-generated malicious passages measuring attack success rate reduction.

## Limitations
- Limited retriever evaluation: Experiments only use Contriever, leaving unknown how TPARAG performs against diverse retriever architectures
- Computational constraints: Main experiments limited to hundreds of instances rather than thousands, potentially affecting generalizability
- Missing defense mechanisms: Paper identifies vulnerabilities but does not propose or evaluate defensive strategies against the identified attacks

## Confidence
- Attack effectiveness metrics (ASR_R, ASR_L, ASR_T): **High** - clearly defined and consistently measured across experiments
- TPARAG framework design: **Medium** - well-described conceptually but missing critical implementation parameters
- Comparative advantage over baselines: **Medium** - experimental results show improvement but limited by unspecified parameters and potential transferability issues

## Next Checks
1. Implement the full pipeline with k=10 (reasonable default) and verify attack success rates match or exceed those reported in the paper
2. Test attack transferability by comparing ASR_L when using different attacker LLMs (Qwen2.5-3B, Mistral-7B, Llama-3-8B) against the same black-box GPT-4o reader
3. Validate the similarity filtering mechanism by testing alternative similarity metrics (cosine, Euclidean) against Sentence-BERT to determine impact on retrieval success rates