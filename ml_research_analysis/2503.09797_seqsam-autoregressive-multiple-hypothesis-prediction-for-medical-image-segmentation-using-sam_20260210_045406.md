---
ver: rpa2
title: 'SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image Segmentation
  using SAM'
arxiv_id: '2503.09797'
source_url: https://arxiv.org/abs/2503.09797
tags:
- masks
- image
- multiple
- each
- mask
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEQSAM, a method for generating multiple
  plausible segmentation masks for medical images, addressing the challenge of inherent
  uncertainty in medical image annotation. SEQSAM builds on the Segment Anything Model
  (SAM) by introducing a recurrent module that sequentially generates masks using
  a single prediction head, rather than multiple parallel heads as in standard Multiple
  Choice Learning.
---

# SeqSAM: Autoregressive Multiple Hypothesis Prediction for Medical Image Segmentation using SAM

## Quick Facts
- arXiv ID: 2503.09797
- Source URL: https://arxiv.org/abs/2503.09797
- Reference count: 0
- Primary result: Autoregressive method generates multiple plausible medical segmentation masks with improved Dice scores and Generalized Energy Distance metrics compared to SAM-based baselines

## Executive Summary
This paper introduces SEQSAM, a method for generating multiple plausible segmentation masks for medical images, addressing the challenge of inherent uncertainty in medical image annotation. SEQSAM builds on the Segment Anything Model (SAM) by introducing a recurrent module that sequentially generates masks using a single prediction head, rather than multiple parallel heads as in standard Multiple Choice Learning. A key innovation is the use of bipartite matching for credit assignment during training, ensuring each generated mask captures clinically relevant variations. The method can generate an arbitrary number of masks through randomized strided pooling, making it more flexible than previous approaches.

## Method Summary
SEQSAM modifies SAM for multiple hypothesis prediction by adding a recurrent module that sequentially generates masks with a single prediction head. The method uses bipartite matching for credit assignment during training, ensuring each prediction captures distinct annotation modes. For scenarios where the number of requested masks exceeds available labels, randomized strided pooling enables flexible generation. The approach is evaluated on LIDC-IDRI and QUBIQ Kidney datasets with multiple annotators, showing improved performance over SAM-based baselines while maintaining quality when generating up to 10 masks.

## Key Results
- SEQSAM achieves notable improvements in Dice score compared to SAM-MCL baselines (e.g., 83.8 vs 82.4 on LIDC-IDRI)
- The method demonstrates better distribution metrics, with Generalized Energy Distance improving from 0.245 to 0.227 on LIDC-IDRI
- SEQSAM maintains quality when generating up to 10 masks, showing robustness of the autoregressive approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sequential generation via a recurrent module produces more diverse, clinically relevant masks than parallel multi-head prediction.
- **Mechanism:** A hidden state H(m) is updated at each timestep via `H(m+1) = conv-2([H(m), Z(m)])`, maintaining memory of previously generated masks. This state conditions the next prediction through `Z(m)+ = conv-1([H(m), Z(m)])`, creating dependency between sequential outputs rather than independent predictions.
- **Core assumption:** The model can learn to canonicalize mask ordering despite labels lacking inherent sequence structure.
- **Evidence anchors:** [abstract] "SEQSAM...uses a bipartite matching loss for ensuring the clinical relevancy of each mask, and can produce an arbitrary number of masks"; [section 2.3] "H(m+1) = conv-2([H(m), Z(m)])...preventing SAM from tracking long-term information about which masks have been generated"
- **Break condition:** If hidden state dimensionality is insufficient for the number of masks, or if backpropagation-through-time is disabled, diversity collapses (Table 1B shows performance drop without BPTT).

### Mechanism 2
- **Claim:** Bipartite matching loss enforces that each prediction captures a distinct mode from the label distribution.
- **Mechanism:** Rather than fixed assignment, the Hungarian algorithm (O(N³)) finds the optimal permutation π minimizing `Σ L(ŷ(m), y_π(m))`. This guarantees one-to-one assignment, preventing any single prediction from dominating and forcing coverage of all annotation modes.
- **Core assumption:** The loss landscape permits the model to learn a stable canonical ordering through sequential training.
- **Evidence anchors:** [abstract] "sparse, winner-takes-all loss function makes it easy for one prediction head to become overly dominant"; [section 2.4] "each prediction is assigned to exactly one label...enforcing that the model captures the range of possible modes"
- **Break condition:** If M ≠ K without proper handling, or if dice loss is replaced with non-permutation-invariant alternatives, the credit assignment fails.

### Mechanism 3
- **Claim:** Randomized strided pooling enables generation of arbitrary mask counts beyond training label counts.
- **Mechanism:** For M > K, predictions are divided into K chunks of size ⌈M/K⌉. During training, one mask per chunk is randomly sampled for loss computation. Each chunk "specializes" for a particular label, allowing inference to use all M predictions.
- **Core assumption:** Chunk-wise specialization emerges from training, and intra-chunk diversity captures sub-modes.
- **Evidence anchors:** [section 2.5] "each chunk can become specialised for a particular role"; [Table 1A] SEQSAM-10 achieves 83.5/89.4 Dice vs. SEQSAM-3's 83.8/90.0, showing maintained quality at higher counts
- **Break condition:** Ablation (Table 1B) shows "First K" sampling causes large accuracy drops (78.7 vs 83.0 Dice), indicating alignment between training and inference sequence length is critical.

## Foundational Learning

- **Concept: Multiple Choice Learning (MCL)**
  - Why needed here: SEQSAM is positioned as a solution to MCL's fixed-head limitation and winner-takes-all collapse problem.
  - Quick check question: Can you explain why sparse winner-takes-all loss causes prediction head dominance?

- **Concept: Backpropagation Through Time (BPTT)**
  - Why needed here: The recurrent module requires gradients to flow through sequential timesteps; ablation shows removing BPTT from logits reduces performance.
  - Quick check question: How does BPTT differ from standard backpropagation in sequential models?

- **Concept: Bipartite Matching / Hungarian Algorithm**
  - Why needed here: Core to the set-based optimization; the O(N³) assignment ensures clinically relevant outputs.
  - Quick check question: Why is fixed label assignment problematic when labels lack canonical ordering?

## Architecture Onboarding

- **Component map:** Image → SAMenc → E → SAMdec (with H(m) conditioning) → Z(m) → sigmoid → ŷ(m) → Hungarian matching → dice loss → BPTT through logits and H(m)
- **Critical path:** Image → SAMenc (frozen) → embedding E → SAMdec (with hidden state H(m) conditioning) → mask logits Z(m) → sigmoid → ŷ(m) → bipartite matching assignment → dice loss → BPTT through logits and H(m)
- **Design tradeoffs:**
  - Frozen encoder limits adaptability but reduces trainable parameters significantly
  - Single prediction head with sequential generation vs. multi-head MCL: slower inference for M masks but more flexible
  - Chunk-based training for M > K: enables flexibility but requires assumption of chunk specialization
- **Failure signatures:**
  - Dominant single output (mode collapse): indicates bipartite matching not functioning or recurrent module not tracking history
  - Identical masks in sequence: hidden state update may be broken
  - Performance degradation at higher M: strided pooling not properly chunking
- **First 3 experiments:**
  1. Reproduce baseline comparison: SAM-MCL vs SEQSAM with M=K=3 on LIDC-IDRI, verify ~1.4 Dice improvement
  2. Ablate recurrent module: replace with stateless prompting, confirm diversity loss matches "No Backprop" results
  3. Test mask count generalization: train with K=3 labels, evaluate at M=10, verify GED remains stable (should achieve ~0.234 vs 0.245 baseline)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SEQSAM framework be effectively adapted for the more common scenario of medical datasets with only single annotations per image?
- Basis in paper: [explicit] Section 3.1 states that a limitation of the approach is that "it requires a dataset with multiple annotations per image, which are much rarer than single annotator datasets."
- Why unresolved: The current methodology relies on bipartite matching between the set of predictions and the set of ground truth labels, which necessitates multiple annotations for training.
- What evidence would resolve it: A modification of the loss function or training regime (e.g., using synthetic augmentation) that achieves similar uncertainty capture on single-annotator datasets.

### Open Question 2
- Question: Does the autoregressive nature of the model implicitly learn a predictable, canonical ordering for the generated masks?
- Basis in paper: [inferred] Section 2.4 mentions that due to the sequential nature, the model is "encouraged to canonicalise its generated masks," but the paper does not analyze if this order corresponds to semantic features like size or confidence.
- Why unresolved: The loss function optimizes for set membership via bipartite matching rather than enforcing a specific sequence property (e.g., predicting the largest mask first).
- What evidence would resolve it: Analysis of the correlation between the generation timestep $m$ and mask properties such as voxel volume or prediction confidence across the test set.

### Open Question 3
- Question: How does the diversity and clinical relevance of predictions degrade as the requested number of masks ($M$) scales far beyond the number of available labels ($K$)?
- Basis in paper: [inferred] Section 2.5 introduces randomized strided pooling to handle $M > K$, but results are only provided for $M=10$ with $K=3$ or $K=4$.
- Why unresolved: It is unclear if the "chunking" method maintains distinct, plausible hypotheses or collapses into redundant predictions when the chunk sizes become very large.
- What evidence would resolve it: Evaluating the Generalized Energy Distance (GED) and inter-mask diversity metrics on significantly larger $M$ values (e.g., $M=20, 50$).

## Limitations
- Requires datasets with multiple annotations per image, which are rarer than single-annotator datasets
- The autoregressive approach is slower than parallel multi-head methods for generating M masks
- The assumption of chunk specialization for M > K scenarios lacks direct empirical validation

## Confidence

**High Confidence:**
- Bipartite matching framework for set-based prediction is well-established
- Claim that sequential generation provides flexibility beyond fixed-head MCL is well-supported

**Medium Confidence:**
- Recurrent module maintains diversity through hidden state updates is plausible but relies on assumed capacity
- Claim that this outperforms parallel multi-head approaches is supported but not definitively proven

**Low Confidence:**
- Randomized strided pooling enables robust generalization to arbitrary mask counts is theoretically sound but assumption of chunk specialization lacks direct validation
- Claim that all generated masks are "clinically relevant" is difficult to verify without radiologist evaluation

## Next Checks
1. **Mechanism isolation:** Train with frozen hidden state (no BPTT) and verify the ~4-point Davg drop matches the ablation, confirming recurrent memory is essential for diversity
2. **Chunk specialization test:** For M>K, visualize mask diversity within each chunk vs across chunks to empirically verify the specialization assumption underlying strided pooling
3. **Clinical relevance validation:** Conduct blinded radiologist review comparing SEQSAM-generated masks against SAM-MCL outputs to assess whether the increased diversity captures meaningful anatomical variations rather than artifacts