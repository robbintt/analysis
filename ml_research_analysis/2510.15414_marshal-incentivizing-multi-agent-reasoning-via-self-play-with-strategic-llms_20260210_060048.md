---
ver: rpa2
title: 'MARSHAL: Incentivizing Multi-Agent Reasoning via Self-Play with Strategic
  LLMs'
arxiv_id: '2510.15414'
source_url: https://arxiv.org/abs/2510.15414
tags:
- marshal
- game
- player
- games
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of developing multi-agent reasoning
  capabilities in large language models (LLMs) by extending reinforcement learning
  to multi-turn, multi-agent scenarios. The authors propose MARSHAL, an end-to-end
  RL framework that trains LLMs through self-play in strategic games using two key
  innovations: a turn-level advantage estimator for fine-grained credit assignment
  and agent-specific advantage normalization to stabilize training across heterogeneous
  roles.'
---

# MARSHAL: Incentivizing Multi-Agent Reasoning via Self-Play with Strategic LLMs

## Quick Facts
- arXiv ID: 2510.15414
- Source URL: https://arxiv.org/abs/2510.15414
- Reference count: 40
- Primary result: Qwen3-4B trained with MARSHAL shows up to 28.7% improvement in held-out games and generalizes to 10.0% gains on AIME and 6.6% on GPQA-Diamond when integrated into multi-agent systems

## Executive Summary
This paper addresses the challenge of developing multi-agent reasoning capabilities in large language models (LLMs) by extending reinforcement learning to multi-turn, multi-agent scenarios. The authors propose MARSHAL, an end-to-end RL framework that trains LLMs through self-play in strategic games using two key innovations: a turn-level advantage estimator for fine-grained credit assignment and agent-specific advantage normalization to stabilize training across heterogeneous roles. When Qwen3-4B is trained with MARSHAL, it achieves up to 28.7% performance improvements in held-out games and demonstrates strong generalization, achieving up to 10.0% gains on AIME and 6.6% on GPQA-Diamond when integrated into multi-agent systems like MAD and AutoGen.

## Method Summary
MARSHAL is an end-to-end RL framework that trains LLMs through self-play in strategic games. The method extends traditional RL to multi-turn, multi-agent scenarios by introducing a turn-level advantage estimator for fine-grained credit assignment and agent-specific advantage normalization to stabilize training across heterogeneous roles. The framework trains Qwen3-4B models by having them play strategic games against themselves, learning optimal policies for different agent roles. The approach aims to develop transferable multi-agent reasoning capabilities that can generalize beyond the training games to downstream reasoning tasks.

## Key Results
- Qwen3-4B trained with MARSHAL achieves up to 28.7% performance improvements in held-out strategic games
- Strong generalization demonstrated with up to 10.0% gains on AIME and 6.6% on GPQA-Diamond when integrated into multi-agent systems
- MARSHAL establishes self-play in strategic games as a powerful approach for developing transferable multi-agent reasoning capabilities

## Why This Works (Mechanism)
MARSHAL leverages self-play in strategic games to develop multi-agent reasoning capabilities by exposing LLMs to complex, sequential decision-making scenarios with multiple interacting agents. The framework's key innovations - turn-level advantage estimation and agent-specific advantage normalization - address the credit assignment problem in multi-agent settings by providing fine-grained reward signals and stabilizing training across heterogeneous agent roles. This creates a rich learning environment where models must reason about both their own actions and the actions of other agents across multiple turns, building transferable reasoning skills that generalize to downstream tasks.

## Foundational Learning
- **Multi-agent reinforcement learning**: Understanding how to extend RL to scenarios with multiple interacting agents is crucial, as traditional RL assumes single-agent environments. Quick check: Can the framework handle both cooperative and competitive multi-agent scenarios?
- **Credit assignment in sequential games**: The turn-level advantage estimator addresses the challenge of attributing rewards to specific actions in multi-turn games. Quick check: Does the estimator correctly handle delayed rewards and complex game dynamics?
- **Agent-specific normalization**: Stabilizing training across heterogeneous roles requires understanding how different agent types receive and process reward signals. Quick check: How does the normalization handle extreme differences in reward scales between agent types?

## Architecture Onboarding
**Component Map**: LLM model -> MARSHAL RL framework -> Strategic game environment -> Turn-level advantage estimator -> Agent-specific normalization -> Policy update

**Critical Path**: Strategic game environment generates multi-turn game data -> MARSHAL framework processes game data with turn-level advantage estimation -> Agent-specific normalization stabilizes training signals -> LLM policy is updated based on normalized advantages

**Design Tradeoffs**: The framework trades computational complexity for richer learning signals - self-play requires generating many game instances but provides diverse, realistic training scenarios. The turn-level advantage estimator adds complexity but enables more precise credit assignment compared to episode-level rewards.

**Failure Signatures**: Poor generalization to downstream tasks suggests the self-play environment lacks sufficient diversity or the advantage estimation fails to capture relevant reasoning patterns. Training instability or collapse indicates issues with the agent-specific normalization or learning rate scheduling.

**First Experiments**:
1. Verify that the turn-level advantage estimator correctly attributes rewards across multiple turns in simple game scenarios
2. Test agent-specific normalization on heterogeneous agents with known reward scale differences
3. Validate that self-play training improves performance on held-out game instances before testing generalization to downstream tasks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, though the limitations section implicitly raises questions about the relative contributions of different technical innovations and the nature of the generalization improvements.

## Limitations
- The evaluation relies heavily on single-game holds-out tests and downstream integration scenarios without systematic ablation studies isolating the contributions of each technical innovation
- Sample efficiency compared to other multi-agent RL approaches is not thoroughly characterized, leaving questions about scalability
- The claim of "strong generalization" conflates improvements in multi-agent collaboration with improvements in individual reasoning capability, which remain distinct phenomena

## Confidence
- **High confidence** in the technical implementation of MARSHAL and the core experimental methodology
- **Medium confidence** in the interpretation of generalization results, particularly regarding what aspects of reasoning are being improved
- **Medium confidence** in the claim that MARSHAL's specific innovations (vs. self-play alone) drive the improvements

## Next Checks
1. Conduct systematic ablation studies removing either the turn-level advantage estimator or agent-specific normalization to quantify their individual contributions to performance gains
2. Evaluate sample efficiency by measuring learning curves across different numbers of training episodes and comparing against alternative multi-agent RL baselines
3. Design targeted experiments to distinguish whether performance gains on AIME/GPQA stem from improved individual reasoning or improved multi-agent collaboration dynamics