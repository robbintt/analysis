---
ver: rpa2
title: Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks
arxiv_id: '2510.18195'
source_url: https://arxiv.org/abs/2510.18195
tags:
- control
- system
- optimal
- ensemble
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a multistage ensemble framework for solving
  the Hamilton-Jacobi-Bellman (HJB) equation using physics-informed neural networks
  (PINNs) to enable optimal control of nonlinear systems. The approach addresses the
  computational challenges of traditional numerical solutions to the HJB equation
  by leveraging ensemble learning and automatic differentiation to approximate the
  optimal cost-to-go function and derive corresponding control signals.
---

# Ensemble based Closed-Loop Optimal Control using Physics-Informed Neural Networks

## Quick Facts
- **arXiv ID:** 2510.18195
- **Source URL:** https://arxiv.org/abs/2510.18195
- **Reference count:** 40
- **Key outcome:** This work presents a multistage ensemble framework for solving the Hamilton-Jacobi-Bellman (HJB) equation using physics-informed neural networks (PINNs) to enable optimal control of nonlinear systems.

## Executive Summary
This paper introduces a multistage ensemble framework that uses physics-informed neural networks to solve the Hamilton-Jacobi-Bellman equation for optimal control of nonlinear systems. The approach combines warm-start initialization with HJB-PINN loss optimization, training an ensemble of networks that can handle perturbed system states and varying initial conditions. The method successfully demonstrates closed-loop control of a two-state continuous nonlinear system, achieving stability even with noisy, perturbed inputs. Three control strategies are evaluated: individual control for each ensemble member, mean control for all members, and outlier-excluding mean control, with the latter showing particularly strong performance.

## Method Summary
The framework employs a two-stage training process: warm-start initialization using suboptimal analytical data followed by HJB-PINN loss optimization. An ensemble of 20 PINNs is trained on both boundary data and HJB equation residuals, with each network consisting of two hidden layers with 10 nodes each using Tanh activation. The control strategies include individual control per network, mean control across all networks, and outlier-excluding mean control that filters out unstable ensemble members using a multivariate Chauvenet criterion. The method handles infinite-horizon optimal control without requiring stabilizer terms or finite-horizon approximations, leveraging automatic differentiation to compute costates and derive control signals.

## Key Results
- The ensemble approach successfully stabilizes system dynamics to zero even with perturbed initial conditions
- MSE between analytical and learned solutions approaches zero in most regions, with elevated error at boundaries as expected
- Outlier-excluding mean control policy maintains stability while excluding control signals from systems that become uncontrollable
- The method achieves optimal control without relying on stabilizer terms or finite-horizon approximations

## Why This Works (Mechanism)

### Mechanism 1: Multistage Training with Warm-Start Initialization
The warm-start phase initializes network weights using suboptimal analytical data, placing the network in a favorable region of the loss landscape. The subsequent HJB-PINN loss then refines weights by minimizing the HJB equation residual while maintaining boundary condition constraints. This two-stage approach avoids local minima that would trap randomly initialized networks.

### Mechanism 2: Ensemble Aggregation with Outlier Exclusion
By averaging control signals across 20 networks and using multivariate Chauvenet criterion to detect outliers, the framework filters out control signals from systems drifting outside admissible trajectories. Outliers are controlled individually with hope of reintegration into the ensemble mean.

### Mechanism 3: Physics-Informed Loss with Boundary Constraints
The composite loss function balances two objectives: matching known cost-to-go values at domain boundaries and minimizing the HJB PDE residual throughout the domain. The α weighting coefficient controls the tradeoff between constraint satisfaction and optimality enforcement.

## Foundational Learning

- **Concept: Hamilton-Jacobi-Bellman (HJB) Equation**
  - **Why needed here:** The HJB equation defines optimal control for nonlinear systems. Understanding its structure—the relationship between value function V, system dynamics f, running cost L, and control u—is essential for interpreting how the PINN loss enforces optimality.
  - **Quick check question:** Why does the steady-state HJB equation lack a time derivative term, and how does this relate to infinite-horizon problems?

- **Concept: Costate Variables via Automatic Differentiation**
  - **Why needed here:** The costate λ = ∇_x J is derived by differentiating the network's cost-to-go output with respect to state input. This gradient feeds directly into control signal computation. Understanding this computational chain is critical for debugging.
  - **Quick check question:** How would you verify that automatic differentiation correctly computes the costate λ from the network output?

- **Concept: Bellman's Principle of Optimality**
  - **Why needed here:** This principle justifies why solving the HJB equation locally at each time step produces globally optimal trajectories, forming the theoretical basis for the closed-loop control strategy.
  - **Quick check question:** In the closed-loop strategy, why is optimal control recomputed at each discrete time step rather than once for the entire horizon?

## Architecture Onboarding

- **Component map:** State Input Layer -> Hidden Layers (2×10 nodes, Tanh) -> Output Layer (scalar cost-to-go) -> Automatic Differentiation Module (computes λ = ∇_x Ĵ) -> Control Computation (u = -0.5·R⁻¹·f2(x)ᵀ·λᵀ) -> Loss Function (Boundary MSE + HJB residual) -> Ensemble Manager (coordinates N=20 networks, implements outlier detection) -> Control Aggregator (three policies)

- **Critical path:** 1. Warm-start: MSE loss on analytical data (100 epochs, batch=100) 2. HJB-PINN training: Composite loss (60 epochs per network, batch=400) 3. Closed-loop deployment: State perturbation → control computation → dynamics update

- **Design tradeoffs:** Network size vs. generalization (small network prevents overfitting but may limit expressiveness); learning rate cyclic schedule helps escape local minima but introduces instability; ensemble size N=20 provides robustness at 20× computational cost; Tanh activation required for smooth gradients (ReLU fails).

- **Failure signatures:** 1. Loss explosion during HJB training (insufficient warm-start or α too aggressive) 2. MSE concentrated at boundaries (expected pattern, verify Fig. 2) 3. Ensemble divergence (perturbation σ too high or initial conditions outside training domain) 4. Control oscillation (learning rate too high or non-smooth activation)

- **First 3 experiments:** 1. Reproduce warm-start + HJB training on eq. 39; verify MSE approaches zero in interior regions with elevated boundary error 2. Ablate control policies on perturbed initial conditions [10,10], [0,0], [-10,-10] with σ=0.01; confirm outlier-excluding mean outperforms 3. Sweep noise σ from 0.01 to 0.05; document threshold where ensemble fails to stabilize

## Open Questions the Paper Calls Out

- **Question 1:** Can the ensemble PINN framework be successfully extended to more complex control problems such as the cart pole or inverted pendulum?
- **Question 2:** To what extent can the three control policies handle increasing amounts of noise in the system state data?
- **Question 3:** Does the ensemble framework effectively scale to higher-dimensional systems to overcome the curse of dimensionality associated with HJB equations?

## Limitations

- The framework validates only on a two-state nonlinear system, limiting generalizability to higher-dimensional problems
- The choice of α weighting coefficient in the HJB-PINN loss function is not specified, representing a critical hyperparameter
- The method assumes the system remains within the training domain; performance degrades when perturbations push states beyond boundaries
- The ensemble approach requires 20× computational resources compared to single-network solutions

## Confidence

- **High Confidence:** The core mechanism of using warm-start initialization followed by HJB-PINN loss optimization works as described. The improvement in convergence speed and stability compared to random initialization is well-supported by experimental results.
- **Medium Confidence:** The ensemble outlier-exclusion strategy provides meaningful robustness benefits. While results demonstrate improved stability under perturbations, the specific choice of N=20 networks and parameters could be optimized further.
- **Low Confidence:** The framework's scalability to higher-dimensional systems and more complex dynamics. The paper validates only a two-state system, and computational complexity of maintaining 20 networks may become prohibitive for larger state spaces.

## Next Checks

1. **Hyperparameter Sensitivity Analysis:** Systematically vary α in the HJB-PINN loss function across orders of magnitude (0.01 to 100) and document the impact on convergence speed, final MSE, and control stability.
2. **Dimensionality Stress Test:** Apply the framework to a three-state nonlinear system with known analytical solution. Measure training time, ensemble size requirements, and control accuracy as a function of state dimension.
3. **Boundary Condition Robustness:** Generate training data with systematically reduced boundary coverage. Test whether the PINN can still approximate the HJB solution in regions far from actual boundaries, quantifying sensitivity to boundary data availability.