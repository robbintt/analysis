---
ver: rpa2
title: 'BOOM: Beyond Only One Modality KIT''s Multimodal Multilingual Lecture Companion'
arxiv_id: '2512.02817'
source_url: https://arxiv.org/abs/2512.02817
tags:
- translation
- text
- multimodal
- lecture
- slides
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BOOM, a multimodal multilingual lecture
  companion that addresses the challenge of localizing educational content for global
  audiences. BOOM combines three synchronized outputs: translated text, localized
  slides with preserved visual elements, and synthesized speech.'
---

# BOOM: Beyond Only One Modality KIT's Multimodal Multilingual Lecture Companion

## Quick Facts
- arXiv ID: 2512.02817
- Source URL: https://arxiv.org/abs/2512.02817
- Reference count: 19
- One-line primary result: BOOM achieves BLEU scores of up to 18.4 on summarization and 35.7 on question answering for English by incorporating slide screenshots into multimodal speech translation

## Executive Summary
This paper introduces BOOM, a multimodal multilingual lecture companion that addresses the challenge of localizing educational content for global audiences. BOOM combines three synchronized outputs: translated text, localized slides with preserved visual elements, and synthesized speech. The system leverages multimodal speech translation by incorporating slide screenshots into the translation pipeline, using the OmniFusion model to improve accuracy through visual context. Experiments demonstrate that slide-aware transcripts enhance downstream tasks like summarization and question answering across multiple languages.

## Method Summary
BOOM employs OmniFusion for multimodal speech translation, using slide screenshots as visual context to improve transcript quality for technical content. The system features an open-source image translation pipeline that detects, recognizes, translates, and re-renders text within slide images while preserving layout and visual coherence. For downstream tasks, the system uses LLM-based summarization and RAG-based question answering on the translated transcripts. The architecture processes streaming lecture audio with synchronized PDF slides, producing text transcripts, localized slide decks, and synthesized speech output.

## Key Results
- Slide-aware transcripts improve downstream summarization BERTScore from 18.4 to 20.5 for English
- Question answering accuracy improves from 31.5 to 34.5 for English with multimodal input
- The system achieves 13.6 BLEU (de) vs. 9.2 with line-level segmentation, confirming block-level segmentation superiority

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating slide screenshots into speech translation improves transcript quality for technical lecture content.
- Mechanism: The OmniFusion model uses slide images as visual context during speech translation, providing domain-specific terminology, formulas, and definitions that disambiguate spoken content. A PDF viewer tracks which slide corresponds to each audio segment, enabling extraction of temporally-aligned screenshots.
- Core assumption: Slides contain terminology and concepts that are verbally referenced during the corresponding audio segment, and the visual-textual grounding reduces transcription/translation ambiguity.
- Evidence anchors:
  - [abstract]: "core approach uses OmniFusion for multimodal speech translation, incorporating slide screenshots to improve translation quality"
  - [section 2.1]: "Accurate visual context is crucial for effective translation. The PDF viewer tracks the slide displayed during each audio segment, allowing us to extract a screenshot from the middle of the segment and feed it to the ST model."
  - [corpus]: Weak direct validation. Neighbor papers (e.g., "From Slides to Chatbots") support slides as LLM context but don't validate multimodal ST improvements.
- Break condition: If slide-audio alignment is poor (>3 second offset) or slides lack text (image-only), visual context may introduce noise rather than signal.

### Mechanism 2
- Claim: Block-level text segmentation improves image-embedded translation quality over line-level approaches.
- Mechanism: Layout analysis using Hi-SAM groups OCR-detected text into semantic blocks (e.g., bullet lists, diagram labels) before translation. This preserves sentence-like units and structural cues, whereas line-level segmentation fragments context.
- Core assumption: Coherent semantic units produce better translations than fragmented text, and visual layout correlates with semantic grouping.
- Evidence anchors:
  - [section 3.2]: "Block-level segmentation improves translation over line-level segmentation, confirming that coherent sentence-like units are critical for high-quality output."
  - [table 2]: OmniFusion with layout-level segmentation achieves 13.6 BLEU (de) vs. 9.2 with line-level.
  - [corpus]: No direct corpus validation for this specific segmentation finding.
- Break condition: If Hi-SAM misclassifies layout regions (e.g., merges unrelated text blocks), translation quality degrades. Ground-truth OCR+segmentation in Table 2 shows 18.4 BLEU, indicating OCR errors compound.

### Mechanism 3
- Claim: Slide-aware transcripts cascade to improved downstream task performance (summarization, QA) even when downstream models are text-only.
- Mechanism: Multimodal ST produces richer, more accurate transcripts containing terminology from slides. These transcripts serve as context for LLM-based summarization and RAG-based QA, improving output quality without modifying downstream models.
- Core assumption: Transcript quality (not direct image access) drives downstream gains; the LLM benefits from better text, not from visual reasoning.
- Evidence anchors:
  - [abstract]: "slide-aware transcripts also yield cascading benefits for downstream tasks such as summarization and question answering"
  - [table 3]: English summarization BERTScore improves from 18.4 (audio-only) to 20.5 (audio+image); QA improves 31.5→34.5.
  - [section 3.3]: "summaries generated from audio+image input consistently outperform those based on audio-only across most languages and models, even though the summarization models are text-only."
  - [corpus]: "From Slides to Chatbots" shows LLMs benefit from course materials but doesn't validate transcript-mediated gains.
- Break condition: QA improvements are inconsistent (regression in 4/12 language-model combinations). Assumption: gains require terminology overlap between slides and questions; Chinese shows degradation, possibly due to lexical distance from English terminology.

## Foundational Learning

- **Multimodal Fusion in Sequence-to-Sequence Models**
  - Why needed here: OmniFusion integrates visual encoders with speech-text transformers. Understanding cross-attention between modalities is essential for debugging alignment failures.
  - Quick check question: Given an audio segment mentioning "the gradient here" and a slide showing ∇f(x), how would cross-attention weight the visual token?

- **Streaming Speech Translation Policies**
  - Why needed here: Live lectures require low-latency translation. The system uses voice-activity detection + Local-Agreement to determine when to emit partial translations.
  - Quick check question: If VAD detects 500ms of silence, should the system commit the current hypothesis or wait for more context?

- **OCR + Layout Analysis Pipeline Design**
  - Why needed here: Slide translation requires coordinated OCR (PaddleOCR v5) and layout segmentation (Hi-SAM). Errors propagate through the pipeline.
  - Quick check question: OCR outputs word-level bounding boxes; how do you group them into translatable blocks without semantic understanding?

## Architecture Onboarding

- **Component map:**
  Input Layer -> Multimodal ST -> Slide Translation -> Downstream Tasks -> Output
  (PDF viewer + Audio) -> (OmniFusion) -> (OCR + Hi-SAM + translation + inpainting) -> (LLM summarization + RAG QA) -> (Text + Slides + TTS)

- **Critical path:**
  1. Slide-to-audio alignment (PDF viewer timestamp matching)
  2. OCR accuracy (CER 11.31% with PaddleOCR v4, 13.48% with v5)
  3. Layout analysis (2.93s latency, slowest component after translation at 3.10s)
  4. Inpainting quality (background preservation affects re-rendered text readability)

- **Design tradeoffs:**
  - Unimodal MT for editable slide text (faster) vs. multimodal MT for embedded images (context-aware but 3.10s latency)
  - Line-level segmentation (simpler, lower context) vs. block-level (better quality, layout-dependent)
  - Diffusion-based rendering (high quality but degradation on repeated edits) vs. heuristic drawing (stable but less flexible)

- **Failure signatures:**
  - Misaligned slides: Transcript contains terminology not present in current slide → disambiguation fails
  - OCR cascade errors: CER 13% compounds through translation → Ground-truth OCR shows 18.4 BLEU vs. 13.6 with predicted OCR
  - Inpainting artifacts: Simple-LaMa leaves text remnants → translated text overlaps original
  - QA regression: Chinese QA drops 35.8→32.4 with multimodal input (lexical distance assumption unverified)

- **First 3 experiments:**
  1. **Ablate slide timing offset**: Test translation quality with ±1s, ±3s, ±5s slide misalignment to characterize robustness bounds.
  2. **OCR error injection**: Synthesize OCR noise at 5%, 10%, 20% CER to measure translation degradation rates and identify breaking points.
  3. **Block vs. line segmentation on dense diagrams**: Manually annotate 20 slides with complex diagrams; compare BLEU/COMET for each segmentation strategy to validate layout analysis contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the heuristic drawing module compare to neural rendering baselines in preserving visual coherence during slide translation when assessed via human evaluation?
- Basis in paper: [explicit] The authors report that "Fully automatic diffusion-based methods proved unsuitable" due to clarity degradation and explicitly call for "Human evaluation... to assess the rendering quality of translated slides."
- Why unresolved: Current evaluation relies on the VISTRA benchmark (street signs) and automatic metrics, which do not capture layout preservation or the visual artifacts introduced by repeated edits in lectures.
- What evidence would resolve it: A user study comparing heuristic re-rendering against diffusion baselines, rating layout fidelity and text clarity.

### Open Question 2
- Question: Does the inclusion of visual context improve Question Answering accuracy in a streaming scenario with timeline-aligned benchmarks?
- Basis in paper: [explicit] The authors note that current evaluation occurs post-hoc and that "Benchmarks with questions aligned to the lecture timeline would provide more realistic and informative evaluations for our use-case."
- Why unresolved: The current setup evaluates QA only after the full talk is translated, ignoring the "live lecture scenario" constraints where context is built incrementally.
- What evidence would resolve it: Performance evaluation on a dataset where questions are timestamped and the system must answer using only the available sliding context window.

### Open Question 3
- Question: Does providing direct visual input to the downstream LLM (rather than just the text transcript) yield significant improvements in Question Answering performance?
- Basis in paper: [inferred] The authors assume the lack of significant QA improvement is because "the LLM does not receive the image data itself," suggesting this architectural limitation hinders the model.
- Why unresolved: The system currently passes only the translated text to the LLM; it remains untested whether multimodal LLMs could leverage the raw slide images for better reasoning on this specific task.
- What evidence would resolve it: An ablation study comparing text-only LLM inputs against multimodal (text + image) inputs for the QA task using the same translation context.

## Limitations

- Evaluation relies on synthetic test data rather than real-world lecture recordings, which may not capture full complexity of live lectures with spontaneous speech and varying slide densities
- System performance depends critically on perfect slide-audio synchronization, with timing mismatches potentially breaking the visual context mechanism
- Translation quality metrics show moderate improvements but remain relatively low for production use, with inconsistent results across language pairs

## Confidence

- **High Confidence**: The multimodal speech translation framework using OmniFusion is technically sound and builds on established models
- **Medium Confidence**: The claimed improvements in translation quality and downstream task performance are supported by benchmark results, though absolute scores remain modest
- **Low Confidence**: The practical effectiveness in real lecture scenarios, particularly robustness to timing misalignments and consistency across all language pairs, cannot be fully assessed from current evaluation

## Next Checks

1. **Ablate slide timing offset**: Test translation quality with ±1s, ±3s, ±5s slide misalignment to characterize robustness bounds and identify practical limits of the visual context mechanism

2. **OCR error injection**: Synthesize OCR noise at 5%, 10%, 20% CER to measure translation degradation rates and identify breaking points, validating the claim that OCR errors cascade through the pipeline

3. **Real lecture deployment**: Apply the system to actual recorded university lectures with varying presentation styles, slide densities, and spontaneous content to validate performance beyond synthetic benchmarks and assess practical utility