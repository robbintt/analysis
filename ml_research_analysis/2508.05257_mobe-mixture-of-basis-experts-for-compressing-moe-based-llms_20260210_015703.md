---
ver: rpa2
title: 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs'
arxiv_id: '2508.05257'
source_url: https://arxiv.org/abs/2508.05257
tags:
- mobe
- matrices
- rank
- expert
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs
  Large Mixture-of-Experts (MoE) models like DeepSeek-V3 and Kimi-K2 suffer from prohibitive
  memory requirements during deployment. While existing MoE compression methods achieve
  parameter reduction, they incur significant accuracy drops (7-14% relative) even
  at modest compression rates.'
---

# MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs

## Quick Facts
- arXiv ID: 2508.05257
- Source URL: https://arxiv.org/abs/2508.05257
- Authors: Xiaodong Chen; Mingming Ha; Zhenzhong Lan; Jing Zhang; Jianguo Li
- Reference count: 40
- Key outcome: MoBE achieves 24%-30% parameter reduction while retaining 98% of original performance on MoE models.

## Executive Summary
MoBE (Mixture-of-Basis-Experts) addresses the prohibitive memory requirements of large MoE models like DeepSeek-V3 and Kimi-K2. The method factorizes each expert's weight matrix using rank decomposition, where a transformation matrix is expert-specific but the second matrix is re-parameterized as a linear combination of shared basis matrices. This approach significantly reduces reconstruction error compared to prior methods while achieving substantial compression rates. Experiments on diverse MoE models demonstrate that MoBE reduces parameter counts by 24%-30% with only 1%-2% absolute accuracy drop, enabling more efficient deployment of trillion-parameter models.

## Method Summary
MoBE compresses MoE models by factorizing expert weight matrices W=AB, where A is expert-specific and B is a linear combination of basis matrices shared across all experts within each MoE layer. The method applies Z-score normalization to stabilize optimization, uses bipolar non-linear activations (SiLU/Tanh) to avoid destructive sparsity, and keeps down-projection matrices unchanged to retain critical knowledge. The factorization is learned by minimizing reconstruction error via Adam optimization. The approach reduces total parameters while potentially increasing activation parameters, with a MoBE† variant adjusting the number of activated experts to balance this tradeoff.

## Key Results
- Achieves 24%-30% parameter reduction across diverse MoE models including DeepSeek-V3 and Kimi-K2
- Reduces reconstruction error by over 50% compared to prior methods (MoLAE, D2-MoE)
- Retains 98% of original performance (only 1%-2% absolute accuracy drop)
- Particularly effective for large-scale MoE models, enabling efficient deployment of trillion-parameter models

## Why This Works (Mechanism)

### Mechanism 1: Shared Basis Factorization
Re-parameterizing expert weights as a unique transformation combined with a shared set of basis matrices significantly reduces reconstruction error compared to per-expert SVD. The factorization W=AB uses a shared library of basis matrices {Bj} across all experts, forcing the model to find common "vocabulary" of basis features rather than fitting noise individually.

### Mechanism 2: Non-linear Activation in Basis Combination
Applying bipolar non-linear activation (SiLU, Tanh) to the linear combination of basis matrices enhances representational power. ReLU induces destructive sparsity by zeroing negative pre-activations that the smaller transformation matrix A cannot recover, resulting in an order-of-magnitude higher MSE loss.

### Mechanism 3: Z-score Normalization for Optimization Stability
Normalizing expert weight matrices to zero mean and unit variance stabilizes the gradient descent process. This smooths the optimization landscape, preventing instability caused by wide value ranges in raw LLM weights. The scaling factor is later folded into the transformation matrix A for zero overhead.

## Foundational Learning

- **Mixture-of-Experts (MoE) Architecture**
  - Why needed here: MoBE specifically targets the FFN experts within MoE models where compression is applied
  - Quick check question: In a standard MoE layer, which weights are typically modified by MoBE: the Router weights, the Expert up/gate weights, or the Attention weights?

- **Matrix Rank Decomposition (SVD/Low-Rank)**
  - Why needed here: MoBE relies on factorizing large matrix W into two smaller matrices A and B, requiring understanding of rank vs information retention tradeoff
  - Quick check question: If a matrix W is 4096×1536 and we decompose it to rank r=512, what are the dimensions of the resulting factors A and B?

- **Reconstruction Error (MSE) vs. Downstream Accuracy**
  - Why needed here: The paper optimizes explicitly for Mean Squared Error between original and reconstructed weights as proxy for performance
  - Quick check question: Why does the paper argue that minimizing the Frobenius norm (MSE) of the weight matrix is a valid proxy for maintaining model performance?

## Architecture Onboarding

- **Component map:**
  - Input: Hidden states x
  - Router: Selects top-k experts (standard MoE behavior)
  - MoBE Layer: Shared Basis + Expert Head + Reconstruction + Down Projection (unchanged)

- **Critical path:**
  1. Conversion (Training): Load pre-trained MoE → Initialize A, B, α → Optimize via Adam to minimize reconstruction loss
  2. Folding: Fold Z-score normalization stats (σ) into A
  3. Inference: Load compressed parameters → Pre-calculate W or compute factorization directly

- **Design tradeoffs:**
  - Total vs. Activation Parameters: Reduces storage but may increase compute per token; MoBE† variant reduces activated experts to balance
  - Basis Count (m): Higher m yields better accuracy but lower compression; paper uses m=4 for smaller models up to m=128 for 1T parameter models

- **Failure signatures:**
  - High MSE (>1e-4): Check Z-score normalization; verify activation function is Tanh/SiLU not ReLU
  - Accuracy Crash in Math/Coding: Ensure Wdown was not compressed as it stores factual knowledge

- **First 3 experiments:**
  1. Verify MSE Reduction: Compare SVD vs MoBE reconstruction MSE on single layer from Qwen3-30B
  2. Activation Function Ablation: Swap f(·) between ReLU, Sigmoid, SiLU to confirm destructive sparsity
  3. E2E Conversion Pilot: Run MoBE conversion on Ling-Lite-Chat, verify ~16% parameter drop and evaluate on BBH benchmark

## Open Questions the Paper Calls Out
- One potential direction is to employ full network knowledge distillation between the original and compressed models to close the remaining 1-2% accuracy gap
- MoBE requires multiple calls to current optimized kernel fused-MoE, which is relatively inefficient and requires implementing a specific mega-kernel
- The optimal number of basis matrices (m) appears to be tuned per model without systematic guidelines for selection

## Limitations
- Model-specific tuning required for optimal basis count (m) and rank (r=p) without clear generalizability rules
- Method optimizes for MSE reconstruction loss without rigorous quantitative correlation to downstream accuracy established
- Claims about ReLU being "10x worse" lack statistical significance testing or deeper analysis of weight distribution

## Confidence
- **High Confidence:** Core mechanism of factorizing expert matrices into shared basis matrices is well-specified with robust empirical evidence
- **Medium Confidence:** Superiority to prior methods supported but limited to subset of models without comprehensive ablation
- **Low Confidence:** Assertion that ReLU is "10x worse" based on single figure without deeper analysis

## Next Checks
1. **Ablation on Basis Count (m):** Run MoBE on Qwen3-30B with m=2, 4, 8, 16 and plot Pareto frontier of (compression ratio, accuracy)
2. **Activation Function Distribution Analysis:** Compute histograms of pre/post-activation values for ReLU, SiLU, Tanh to quantify percentage zeroed out by ReLU
3. **MSE vs. Accuracy Correlation Study:** Systematically increase reconstruction error by adding noise to basis matrices and measure resulting change in downstream accuracy on BBH benchmark