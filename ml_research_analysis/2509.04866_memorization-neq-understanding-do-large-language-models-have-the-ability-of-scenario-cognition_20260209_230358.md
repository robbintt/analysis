---
ver: rpa2
title: 'Memorization $\neq$ Understanding: Do Large Language Models Have the Ability
  of Scenario Cognition?'
arxiv_id: '2509.04866'
source_url: https://arxiv.org/abs/2509.04866
tags:
- scenario
- llms
- cognition
- knowledge
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates large language models\u2019 ability to link\
  \ scenario elements with their arguments\u2014a cognitive skill termed scenario\
  \ cognition\u2014by introducing a novel bi-perspective framework. The framework\
  \ combines a scenario-based dataset of fictional facts annotated with semantic roles\
  \ and two evaluation approaches: model output performance and internal representation\
  \ probing."
---

# Memorization $\neq$ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?

## Quick Facts
- arXiv ID: 2509.04866
- Source URL: https://arxiv.org/abs/2509.04866
- Authors: Boxiang Ma; Ru Li; Yuanlong Wang; Hongye Tan; Xiaoli Li
- Reference count: 8
- Primary result: LLMs show high memorization but poor scenario-based understanding, with exact match scores below 0.31 even for largest models

## Executive Summary
This study investigates whether large language models possess scenario cognition—the ability to link semantic scenario elements with their arguments—using a novel bi-perspective evaluation framework. The framework combines a scenario-based dataset of fictional facts annotated with semantic roles and two evaluation approaches: model output performance and internal representation probing. Results reveal a critical gap between surface-level memorization and true semantic understanding in current LLMs. While models excel at memorizing training data, their performance on scenario-based understanding tasks remains poor, with exact match scores below 0.31 even for the largest models. Probing experiments further reveal that internal representations fail to encode clear associations between scenario elements and arguments.

## Method Summary
The paper introduces a bi-perspective framework for evaluating LLMs' scenario cognition using a novel dataset of fictional facts with semantic role annotations. The dataset includes 500 atomic knowledge statements, 5,000 Memory Set descriptions, and 1,581 Understanding Set questions. The study employs full-parameter supervised fine-tuning on the Memory Set and evaluates models using exact match, BLEU, and ROUGE metrics. Internal representation probing uses linear classifiers on frozen hidden states to test for element-argument associations. The evaluation spans multiple model families (Qwen2.5, LLaMA3.x, Gemma2) across different scales (0.5B-14B parameters).

## Key Results
- Model output performance on scenario understanding tasks remains poor, with exact match scores below 0.31 across all model sizes
- Performance on memorization tasks (Memory Set) improves significantly with training, while scenario understanding (Understanding Set) stagnates
- Linear probing of internal representations shows F1 scores near or below the 0.5 random baseline, indicating weak encoding of element-argument associations
- Case studies reveal hallucinations that appear grammatically correct but are factually wrong, suggesting limited semantic grounding

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Supervised Fine-Tuning (SFT) optimizes for surface-level co-occurrence patterns ("data" memory) rather than structural role binding ("knowledge" memory), leading to a divergence between high memorization scores and low scenario cognition.
- **Mechanism:** The training objective minimizes next-token prediction loss on the Memory Set. This incentivizes the model to learn statistical likelihoods of phrase sequences (e.g., "Film director Paxton...") rather than the abstract relational graph connecting "Director" (element) to "Paxton" (argument). Consequently, performance on the Memory Set rises with epochs, while performance on the Understanding Set stagnates.
- **Core assumption:** The model lacks an inherent inductive bias or pre-existing module that forces token representations to bind strictly based on semantic roles during standard autoregressive training.
- **Evidence anchors:**
  - [Page 6, Figure 3]: Shows solid lines (Memory metrics) rising consistently while dashed lines (Understanding metrics) remain flat across epochs.
  - [Page 2, Introduction]: Distinguishes "data" memory (formal linguistic competence) from "knowledge" memory (functional linguistic competence).
  - [corpus]: Related work *Protoknowledge Shapes Behaviour of LLMs* suggests LLMs internalize token sequences during pretraining, supporting the idea that behavior is shaped by sequence memorization rather than abstract reasoning.
- **Break condition:** If the model were explicitly trained with a loss function penalizing role mismatches (e.g., a structural constraint), this divergence would likely disappear.

### Mechanism 2
- **Claim:** LLMs fail to encode linearly separable associations between scenario elements and arguments in their internal hidden states.
- **Mechanism:** As the model processes input, the hidden states $H^{(k)}$ at specific layers do not organize the vector space such that the representation of an element $h_e$ (e.g., "director") and its argument $h_a$ (e.g., "Paxton") are geometrically aligned or easily distinguishable from non-matching pairs.
- **Core assumption:** Assumption: Valid scenario cognition *requires* a distinct geometric signature in the hidden state space that a linear probe can detect.
- **Evidence anchors:**
  - [Page 7, Figure 4]: Probing results show F1 scores often near or below the 0.5 random baseline, with low Recall, indicating the model struggles to retrieve matched pairs from internal states.
  - [Page 4, Methods]: Describes the probe $ \hat{y} = \sigma(w^\top z + b) $ designed to detect these associations.
- **Break condition:** If scenario cognition were encoded via highly non-linear manifolds or distributed across attention heads in a way linear probes miss, this mechanism would be under-detected (though the paper tested non-linear probes in Appendix B with similar results).

### Mechanism 3
- **Claim:** Hallucinations in LLMs may be a direct byproduct of high formal competence (fluency) coupled with low functional competence (scenario verification).
- **Mechanism:** The model generates text by maximizing the probability of plausible continuations derived from training data. Without robust scenario cognition (the "knowledge" memory to verify facts against roles), the model creates "plausible-sounding" fabrications that violate the semantic constraints of the specific scenario.
- **Core assumption:** Hallucination rates are not just sampling errors but symptoms of missing semantic grounding.
- **Evidence anchors:**
  - [Page 8, Section 3.3]: The case study shows the model generating content never present in training data that looks grammatically correct but is factually wrong.
  - [Page 8, Section 3.3]: "Most errors do not display obvious grammatical or pragmatic flaws, yet they deviate substantially from factual correctness."
  - [corpus]: Sparse direct evidence in the provided corpus neighbors regarding hallucination specifically; general consensus in the field (implied by titles like *Memorization $\neq$ Understanding*) aligns with this gap.
- **Break condition:** If hallucinations were purely a function of temperature scaling or decoding strategy, reducing temperature would solve them, which this paper implies is insufficient.

## Foundational Learning
- **Concept:** **Frame Semantics (Scenario Elements vs. Arguments)**
  - **Why needed here:** To understand the definition of "Scenario Cognition" used in the paper. You must distinguish the *role* (e.g., "Director") from the *filler* (e.g., "Paxton").
  - **Quick check question:** In the sentence "Alice sold the car to Bob," what is the scenario element and what is the argument for the role 'Buyer'?
- **Concept:** **Linear Probing**
  - **Why needed here:** To interpret the paper's internal evaluation method. The authors use a linear classifier on frozen hidden states to test if the model "knows" a relationship internally.
  - **Quick check question:** If a linear probe trained on hidden states achieves 50% accuracy on a binary classification task, what does that imply about the encoding of that information in those layers?
- **Concept:** **Formal vs. Functional Linguistic Competence**
  - **Why needed here:** To grasp the theoretical distinction between "memorization" (grammar/fluency) and "understanding" (reasoning/fact verification).
  - **Quick check question:** Why can a model have high formal competence (perfect grammar) while having low functional competence (logical inconsistency)?

## Architecture Onboarding
- **Component map:** Input: Scenario-based Dataset (Atomic Knowledge + Expanded Descriptions) -> Model: Open-source LLMs (LLaMA3.x, Qwen2.5, Gemma2) -> Training: Full-parameter Supervised Fine-Tuning (SFT) on the "Memory Set" -> Evaluation 1 (Output): Exact Match (EM) / BLEU / ROUGE on the "Understanding Set" (Scenario Questions) -> Evaluation 2 (Internal): Linear Probe (MLP) trained on frozen hidden states ($H_e, H_a$) to predict pair matches
- **Critical path:** The experiment relies on the model failing the "Understanding Set" *despite* succeeding on the "Memory Set". If the Understanding Set is too easy or looks exactly like the Memory Set, the mechanism breaks.
- **Design tradeoffs:**
  - **Synthetic vs. Real Data:** The authors use fictional facts to ensure "memorization" is isolated from pre-training knowledge, but this may limit ecological validity compared to real-world scenarios.
  - **Linear vs. Non-linear Probes:** The main paper uses linear probes for interpretability, risking under-detection of complex encodings (though Appendix B addresses this).
- **Failure signatures:**
  - **High BLEU, Low EM:** Indicates the model is producing relevant-sounding text (fluency) but missing the specific factual entity required.
  - **Probe Recall < 0.5:** Indicates the model cannot retrieve the correct argument from the element representation; it suggests the association isn't stored locally.
- **First 3 experiments:**
  1. **Reproduce the Divergence:** Fine-tune a small model (e.g., LLaMA3.2-1B) on the Memory Set and plot the gap between Memory ROUGE and Understanding EM over 5 epochs to confirm the stagnation of scenario cognition.
  2. **Run the Probe:** Extract hidden states from the fine-tuned model for element-argument pairs. Train the linear probe to see if it beats the 0.5 baseline. If it does not, confirm the lack of internal encoding.
  3. **Format Control:** Run the "Format Adaptation" experiment (Table 2) by mixing 30% of the Understanding Set into training to verify that the low scores aren't just due to the model not understanding the prompt format.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a causal relationship exist between deficits in scenario cognition and the frequency of hallucinations in LLMs?
- Basis in paper: [explicit] The Discussion section identifies a "potential correlation" between limited scenario cognition and hallucinations, noting that insufficient "knowledge" memory may lead to plausible but factually incorrect outputs.
- Why unresolved: The paper establishes a correlation through error analysis but does not conduct experiments to verify if enhancing scenario cognition directly reduces hallucination rates.
- What evidence would resolve it: Interventional experiments demonstrating that models specifically trained for high scenario cognition show a statistically significant reduction in hallucinations compared to baseline models.

### Open Question 2
- Question: What specific architectural mechanisms or training objectives are required to transform surface-level "data" memory into functional "knowledge" memory?
- Basis in paper: [explicit] The Conclusion states that current architectures "may lack mechanisms... necessary to effectively encode" scenario cognition, and simply scaling parameters or extending training is insufficient.
- Why unresolved: The study showed that standard supervised fine-tuning improves memorization but fails to improve semantic understanding, indicating a fundamental methodological gap.
- What evidence would resolve it: The development of a model architecture or training paradigm that achieves high performance on the Understanding Set (scenario cognition) without merely overfitting to the Memory Set.

### Open Question 3
- Question: Do the observed limitations in scenario cognition persist in complex, real-world scenarios outside of the synthetic fictional dataset used in this study?
- Basis in paper: [inferred] The Limitations section notes that the synthetic dataset of fictional facts "may not fully capture the complexity and variability of real-world language scenarios."
- Why unresolved: The study restricted evaluation to fictional atomic facts to isolate the models' reasoning from pre-training memorization, leaving real-world performance unverified.
- What evidence would resolve it: Evaluation results from the proposed bi-perspective framework applied to naturalistic, non-fictional corpora (e.g., news articles or complex dialogues) showing similar deficits in element-argument association.

## Limitations
- The paper's claim that LLMs lack "scenario cognition" rests on synthetic fictional data, which may not generalize to real-world factual recall tasks where pretraining provides stronger contextual grounding.
- Linear probing results suggest weak internal encoding of scenario-element associations, but the paper acknowledges non-linear probes (Appendix B) yield similar results, leaving open the possibility of missed complex representations.
- The Memory Set/Understanding Set split, while designed to isolate memorization from reasoning, could still contain overlapping linguistic patterns that confound the evaluation.

## Confidence
- **High Confidence:** The divergence between Memory Set performance and Understanding Set performance across fine-tuning epochs (Mechanism 1) is directly observable in Figure 3 and well-supported by the experimental design.
- **Medium Confidence:** The failure of linear probes to detect clear element-argument associations (Mechanism 2) is supported by F1 scores near random baseline, but alternative encoding schemes cannot be fully ruled out.
- **Medium Confidence:** The link between low functional competence and hallucination tendencies (Mechanism 3) is plausible given the case study, but broader hallucination evaluation across diverse domains is not provided.

## Next Checks
1. **Probe Architecture Sweep:** Test non-linear probe architectures (MLP with varying depths, attention-based probes) on the same frozen hidden states to verify whether complex encoding schemes are systematically missed by linear methods.
2. **Cross-Domain Transfer:** Apply the same scenario cognition evaluation to real-world factual recall tasks (e.g., Wikidata triples) to test whether the memorization-understanding gap persists outside synthetic data.
3. **Intervention Study:** Fine-tune a model with explicit structural constraints (e.g., loss penalizing role mismatches) and measure whether the Memory-Understanding performance gap narrows, validating the mechanism of SFT's surface-level optimization.