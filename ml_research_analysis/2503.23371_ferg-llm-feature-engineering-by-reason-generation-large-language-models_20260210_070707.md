---
ver: rpa2
title: 'FeRG-LLM : Feature Engineering by Reason Generation Large Language Models'
arxiv_id: '2503.23371'
source_url: https://arxiv.org/abs/2503.23371
tags:
- features
- data
- dataset
- learning
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FeRG-LLM, a novel framework for automated feature
  engineering using large language models. The method employs a two-stage conversational
  dialogue approach to enable LLMs to understand machine learning tasks and discover
  new features through Chain-of-Thought reasoning.
---

# FeRG-LLM : Feature Engineering by Reason Generation Large Language Models

## Quick Facts
- arXiv ID: 2503.23371
- Source URL: https://arxiv.org/abs/2503.23371
- Reference count: 27
- Primary result: FeRG-LLM achieves performance comparable to or better than Llama 3.1 70B on most classification datasets using fewer resources and reduced inference time

## Executive Summary
FeRG-LLM is a novel framework for automated feature engineering that leverages large language models to generate new features for machine learning tasks. The method employs a two-stage conversational dialogue approach where LLMs first generate reasoning rationales for feature ideas, then produce executable Python code to implement those features. The framework fine-tunes a smaller Llama 3.1 8B model on structured dialogues extracted from ML papers and further optimizes it using Direct Preference Optimization (DPO) based on downstream model performance. Experiments demonstrate that FeRG-LLM achieves performance comparable to or better than much larger 70B models while being more resource-efficient and deployable locally.

## Method Summary
The FeRG-LLM framework consists of three main stages: First, GPT-4o-mini extracts structured dialogues from ~7,190 ML documents, capturing domain information, task types, variable descriptions, and feature engineering rationales. Second, these dialogues are formatted into two-turn conversations and used to fine-tune Llama 3.1 8B with LoRA (rank 8, alpha 16). Third, DPO alignment optimizes the model using preference pairs constructed from XGBoost AUC evaluations of generated features. During inference, the model generates rationales and Python code at temperature 1.4, which are executed to produce enriched datasets for downstream ML models.

## Key Results
- Achieves AUC comparable to or better than Llama 3.1 70B on most classification datasets while using fewer resources
- Reduces inference time compared to larger models while maintaining feature quality
- Demonstrates effectiveness on both classification and regression tasks
- Can be deployed locally, addressing security concerns associated with cloud-hosted LLMs

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Dialogue Enforces Reasoning Before Action
The framework's two-turn structure (rationale generation followed by code implementation) forces intermediate representation before action, leveraging CoT capabilities even in 8B-scale models. This structured approach ensures that feature generation is grounded in analytical reasoning rather than direct implementation.

### Mechanism 2: AUC-Grounded DPO Aligns Model Toward Effective Reasoning Patterns
DPO uses downstream model performance (AUC) as feedback to shape the reasoning space of the LLM. By constructing preference pairs based on AUC comparisons and optimizing the model to prefer rationales that led to higher AUC, the framework aligns the model's reasoning patterns with feature quality.

### Mechanism 3: Domain Knowledge Extraction via External LLM Creates Specialized Training Data
The framework uses GPT-4o-mini to extract structured feature engineering dialogues from ML papers, transferring domain reasoning to a smaller model through fine-tuning. This distillation process captures transferable reasoning patterns from academic literature.

## Foundational Learning

- **Chain-of-Thought (CoT) Prompting**: Why needed: The framework is built on the premise that explicit reasoning steps improve feature generation. Quick check: Can you design a two-turn structure that first elicits reasoning about relevant features before requesting predictions for a house price prediction task?
- **Direct Preference Optimization (DPO)**: Why needed: DPO replaces RLHF-style reward model training. Quick check: If you have 3 model outputs with scores [0.72, 0.68, 0.61], how many preference pairs can you form, and which direction does each pair point?
- **LoRA Fine-Tuning**: Why needed: FeRG-LLM uses Low-Rank Adaptation for efficient fine-tuning of the 8B model. Quick check: If LoRA rank=8 and alpha=16, what is the effective scaling factor applied to the low-rank update?

## Architecture Onboarding

- **Component map**: Data Extraction Pipeline (GPT-4o-mini API) -> Dialogue Reformatter -> SFT Stage (Llama 3.1 8B + LoRA) -> Preference Data Builder -> DPO Stage -> Inference Engine
- **Critical path**: Dialogue quality -> SFT coverage -> Preference pair diversity -> DPO stability -> Inference temperature settings
- **Design tradeoffs**: Temperature 1.4 encourages exploration but risks incoherent code; LoRA rank 8 prioritizes memory efficiency over expressive capacity; AUC as sole feedback metric is simple but ignores interpretability and computational cost
- **Failure signatures**: Generated Python code raises runtime errors; rationale is plausible but code implements unrelated transformations; AUC gains cluster in only 1-2 of K=3 required improvements
- **First 3 experiments**:
  1. Reproduce single-dataset pipeline: Take one binary classification dataset, construct 10 rationale-code samples at temperature 1.4, evaluate AUC for each, verify >50% exceed baseline
  2. Ablate dialogue structure: Compare two-stage vs. single-stage prompting on the same dataset, measure mean AUC difference and code executability rate
  3. Validate DPO signal strength: Compare πSFT vs. πDPO on held-out dataset, if difference <0.2% AUC inspect whether preference pairs are dominated by near-tie AUC values

## Open Questions the Paper Calls Out
1. What is the quantitative trade-off between computational overhead and performance gains in the training process?
2. Would incorporating iterative refinement loops or increasing the number of reasoning steps improve feature quality and stability?
3. To what extent does reliance on XGBoost for preference optimization restrict effectiveness for deep learning tabular models?

## Limitations
- Insufficient analysis of trade-off between computational overhead and performance gains in the proposed training process
- Model generates features directly from initial reasoning output without further refinement
- Potential overfitting to the 7,190 training documents with unknown generalization beyond specific domains

## Confidence

- **High confidence**: Two-stage dialogue structure improves code executability and feature diversity compared to single-step prompting
- **Medium confidence**: DPO alignment using AUC-based preference pairs provides meaningful performance gains, though improvement magnitude is modest
- **Low confidence**: Claim that smaller models can approximate CoT reasoning when structured appropriately, as evidence is primarily comparative

## Next Checks

1. **Causal attribution test**: On held-out dataset, generate 50 rationale-code pairs using both πSFT and πDPO, have three ML practitioners independently rate rationale quality, correlate ratings with AUC outcomes
2. **Signal strength analysis**: For each dataset in preference pair construction, plot distribution of AUC differences between chosen and rejected pairs, if >30% have differences <0.5% DPO signal is too weak
3. **Generalization stress test**: Apply FeRG-LLM to datasets with different variable naming conventions (medical vs. financial), measure performance degradation, if AUC drops >10% model has not learned transferable reasoning