---
ver: rpa2
title: Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional
  Encodings
arxiv_id: '2505.13087'
source_url: https://arxiv.org/abs/2505.13087
tags:
- graph
- alignment
- dataset
- datasets
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmarking methodology for Graph
  Neural Networks (GNNs) based on the Graph Alignment Problem, which generalizes graph
  isomorphism by aligning two unlabeled graphs to maximize overlapping edges. The
  authors propose a siamese neural network architecture to solve this problem and
  generate graph alignment datasets from both synthetic and real-world graph data
  with varying difficulty levels.
---

# Graph Alignment for Benchmarking Graph Neural Networks and Learning Positional Encodings

## Quick Facts
- **arXiv ID:** 2505.13087
- **Source URL:** https://arxiv.org/abs/2505.13087
- **Authors:** Adrien Lagesse; Marc Lelarge
- **Reference count:** 40
- **Primary result:** Anisotropic GNNs outperform isotropic GNNs on graph alignment tasks, and learned positional encodings from this task achieve SOTA on molecular property prediction.

## Executive Summary
This paper introduces Graph Alignment as a novel benchmarking framework for Graph Neural Networks (GNNs) that generalizes graph isomorphism by aligning two unlabeled graphs to maximize overlapping edges. The authors propose a siamese neural network architecture to solve this problem and generate alignment datasets from both synthetic and real-world graph data. Experimental results demonstrate that anisotropic GNNs (GAT/GATv2) outperform standard convolutional architectures on this task. Additionally, the paper shows that node embeddings learned through the Graph Alignment Task can serve as effective positional encodings for transformer-based models, achieving state-of-the-art results on the PCQM4Mv2 dataset while using significantly fewer parameters than competing methods.

## Method Summary
The paper proposes a Graph Alignment framework where the goal is to find a permutation matrix π that maximizes the overlap between edges of two graphs G and G̃ (a perturbed, permuted version of G). A siamese neural network architecture is used, with two identical GNN encoders processing the original and perturbed graphs separately. The encoders output node embeddings, which are combined via dot product to form a similarity matrix. The model is trained to predict the identity matrix (shifted by the true permutation π) using binary cross-entropy loss. For inference, the Hungarian algorithm solves the linear assignment problem to extract the discrete permutation from the similarity matrix.

## Key Results
- Anisotropic GNNs (GAT/GATv2) consistently outperform isotropic GNNs (GCN/GIN) on graph alignment tasks across multiple datasets
- Node embeddings learned via Graph Alignment serve as effective positional encodings (GAPE) for transformers, achieving SOTA results on PCQM4Mv2
- The learned GAPE requires significantly fewer parameters than competing methods while maintaining superior performance
- Optimal alignment performance occurs at noise levels of 12-18%, where model ranking is most stable

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aligning a graph with a perturbed version of itself forces a GNN to learn node embeddings that are robust to noise and structurally unique.
- **Mechanism:** The siamese architecture processes graph $G$ and a perturbed, permuted version $\tilde{G}$. The model maximizes the similarity $X_i \cdot \tilde{X}_{\pi(i)}$ for corresponding nodes while minimizing it for non-aligned nodes. To solve this, the GNN must learn embeddings that act as unique structural "fingerprints" for nodes, capable of surviving edge additions/deletions.
- **Core assumption:** The noise level $\eta$ is low enough that the graph topology retains a signal strong enough to correlate $G$ and $\tilde{G}$, but high enough to require learning generalizable features rather than memorizing exact adjacency.
- **Evidence anchors:**
  - [abstract]: "generalizes graph isomorphism by aligning two unlabeled graphs to maximize overlapping edges"
  - [Section 3]: "The siamese module outputs a similarity matrix... indicating a higher probability of them being aligned."
  - [corpus]: Corpus papers (e.g., "Benchmarking Positional Encodings") discuss similar needs for robust structural injection, but this specific noise-injection alignment mechanism is unique to this paper.
- **Break condition:** If the noise level $\eta$ is too high, the structural correlation breaks down, and the problem becomes unsolvable for 1-WL GNNs, reducing accuracy to random guessing.

### Mechanism 2
- **Claim:** Anisotropic GNNs (like GAT/GATv2) outperform isotropic GCNs on alignment because attention mechanisms can selectively focus on stable sub-structures amidst noise.
- **Mechanism:** Isotropic GCNs aggregate neighbors uniformly, potentially diluting the unique structural signature of a node when edges are added or removed randomly. Anisotropic mechanisms (attention) can theoretically down-weight noisy or non-essential neighbors, preserving the core topological signature required for successful alignment.
- **Core assumption:** The graph contains heterophilic or complex local structures where uniform neighbor aggregation fails to disambiguate nodes.
- **Evidence anchors:**
  - [Section 5]: "Our results prove that in the class of 1-WL GNNs, anisotropic architectures enable a finer understanding of structural features..."
  - [Figure 3]: Visualizes the "Accuracy Gap" where GAT/GATv2 consistently outperform GCN/GIN.
- **Break condition:** On purely homophilic, regular graphs where neighbor averaging is sufficient, the computational overhead of attention may not yield significant alignment gains over simpler convolutional baselines.

### Mechanism 3
- **Claim:** Node embeddings learned via Graph Alignment act as high-quality Positional Encodings (GAPE) for Transformers because they encode relative structural roles.
- **Mechanism:** Standard Transformers treat nodes as sets (permutation invariant) and lack spatial inductive bias. By training on the alignment task, the GNN learns embeddings $X$ where $X_i$ encodes the "position" of node $i$ relative to the global structure. Using these as PEs allows the Transformer to distinguish nodes based on topology without expensive pre-computations like Laplacian eigenvectors.
- **Core assumption:** The structural understanding required for alignment overlaps significantly with the structural context needed for molecular property prediction.
- **Evidence anchors:**
  - [Section 6]: "...node embeddings learned through the Graph Alignment Task can serve as effective positional encodings..."
  - [Table 1]: Shows GAPE outperforming Laplacian and Random Walk PEs on ZINC and PCQM4Mv2.
  - [corpus]: Neighbors like "Learning Efficient Positional Encodings" validate the importance of learnable PEs, supporting this mechanism.
- **Break condition:** If the downstream task relies heavily on continuous spatial coordinates (3D conformation) rather than 2D topology, the 2D-structural bias of GAPE may be insufficient compared to 3D-aware encodings.

## Foundational Learning

- **Concept: Graph Isomorphism vs. Alignment**
  - **Why needed here:** The paper frames alignment as a generalization of isomorphism. You must understand that isomorphism is a binary check (are they the same?), whereas alignment is an optimization (how do we best fit them together?).
  - **Quick check question:** If two graphs have different numbers of nodes, can you solve the Graph Alignment Problem as defined in Equation (1)?

- **Concept: Anisotropic vs. Isotropic GNNs**
  - **Why needed here:** The paper’s benchmarking conclusion relies on this distinction. Isotropic means treating all neighbors equally (GCN); Anisotropic means weighting them differently (GAT).
  - **Quick check question:** Which mechanism allows a GNN to ignore a "noisy" neighbor during aggregation?

- **Concept: Linear Assignment Problem (LAP)**
  - **Why needed here:** The model predicts a similarity matrix $\Sigma$, but inference requires extracting a discrete permutation. The Hungarian algorithm (LAP solver) bridges this gap.
  - **Quick check question:** Why can't we simply take the `argmax` of the similarity matrix row-by-row to find the alignment?

## Architecture Onboarding

- **Component map:** Base graph -> Noise generator -> Siamese encoders (GNN1, GNN2) -> Similarity head (dot product) -> Similarity matrix $\Sigma$ -> Loss (BCE) -> Hungarian algorithm for inference
- **Critical path:** The Correlated Graphs generation (Section 4). The logic for handling sparse vs. dense graphs (setting `p_remove=0` for sparse graphs to prevent disconnection) is the most fragile implementation detail.
- **Design tradeoffs:**
  - **Noise $\eta$:** Low noise makes the task trivial (accuracy ceiling); high noise destroys the signal (accuracy floor). The "optimal noise" ($\approx 12\%-18\%$) is where model ranking is most stable.
  - **Dataset Topology:** Pre-training GAPE on a topology mismatched to the downstream task (e.g., training on Social Networks, applying to Molecules) degrades performance (Table E.5).
- **Failure signatures:**
  - **Disconnected Components:** If noise removal disconnects the graph, the GNN cannot pass messages between components, making alignment impossible for those nodes.
  - **Memory Explosion:** Computing the $N \times N$ similarity matrix $\Sigma$ is $O(N^2)$. The paper warns against using $N > 10,000$ without subsampling (BFS sampling).
- **First 3 experiments:**
  1. **Sanity Check:** Generate an Erdös-Rényi alignment dataset with $\eta=0$. The model should achieve 100% accuracy immediately.
  2. **Ranking Verification:** Train GCN vs. GAT on the AQSOL alignment dataset at $\eta=15\%$. Verify that GAT wins.
  3. **Transfer Check:** Train a Transformer on PCQM4Mv2 (subset) using GAPE generated from step 2. Compare MAE against a Transformer with no PE.

## Open Questions the Paper Calls Out
None

## Limitations
- The Graph Alignment framework is limited to unlabeled graphs without node/edge attributes, restricting its applicability to many real-world datasets that contain rich feature information.
- The computational complexity of generating similarity matrices (O(N²)) severely limits scalability, with the paper explicitly warning against using graphs with more than 10,000 nodes without aggressive subsampling.
- Performance degradation when training and testing on mismatched graph topologies suggests limited domain transfer capability of the learned positional encodings.

## Confidence
- **High Confidence:** Anisotropic GNNs outperforming isotropic GNNs on alignment tasks (supported by systematic experiments across multiple datasets)
- **Medium Confidence:** GAPE positional encodings providing state-of-the-art results on PCQM4Mv2 (limited to one dataset, though results are significant)
- **Medium Confidence:** The optimal noise level range (12-18%) for stable model ranking (empirically observed but not theoretically justified)

## Next Checks
1. **Transfer Learning Test:** Train GAPE on social network graphs and evaluate its effectiveness as positional encodings on molecular datasets beyond PCQM4Mv2 (e.g., ZINC, OGB-MOL* benchmarks)
2. **Attribute Extension Test:** Modify the siamese architecture to handle node/edge attributes and validate whether alignment performance improves on attributed graph benchmarks
3. **Scalability Test:** Implement memory-efficient approximations (e.g., low-rank similarity matrices, hierarchical clustering) to enable alignment on graphs with 50K+ nodes and measure accuracy retention