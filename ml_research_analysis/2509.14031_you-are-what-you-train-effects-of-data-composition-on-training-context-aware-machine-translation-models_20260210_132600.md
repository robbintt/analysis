---
ver: rpa2
title: 'You Are What You Train: Effects of Data Composition on Training Context-aware
  Machine Translation Models'
arxiv_id: '2509.14031'
source_url: https://arxiv.org/abs/2509.14031
tags:
- language
- translation
- examples
- training
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of context-aware machine translation,
  particularly focusing on how the composition of training data affects a model's
  ability to handle context-dependent linguistic phenomena such as gender, formality,
  and auxiliary verbs. The authors systematically construct training datasets with
  controlled proportions of contextually rich examples and demonstrate a strong correlation
  between the density of such examples and the model's performance on context-sensitive
  tasks.
---

# You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models

## Quick Facts
- arXiv ID: 2509.14031
- Source URL: https://arxiv.org/abs/2509.14031
- Reference count: 40
- Primary result: Training data composition strongly affects context-aware MT performance; proposed methods improve ctxPro accuracy by 6-8 percentage points

## Executive Summary
This paper investigates how the composition of training data affects context-aware machine translation models' ability to handle context-dependent linguistic phenomena. The authors systematically construct training datasets with controlled proportions of contextually-rich examples and demonstrate that models improve on phenomena they are explicitly trained on, but this improvement does not generalize to other linguistic phenomena. The study reveals limited cross-lingual transfer of context utilization capabilities and proposes two novel training strategies—token-level loss weighting and metric-based example selection—that significantly enhance model performance on context-sensitive tasks without requiring additional annotated data.

## Method Summary
The study employs a two-stage training approach using pre-trained models (OPUS-MT en-de, NLLB-200 600M, TowerBase 7B) that are fine-tuned on context-aware datasets. Training data is constructed by extracting contextually-rich examples using the ctxPro toolset, which identifies sentences containing gender, formality, auxiliary verb, inflection, and animacy phenomena. The authors create controlled datasets with varying densities of these examples (0%, 10%, 50%, 100%) by combining IWSLT 2017 en-de and OpenSubtitles 2018 corpora. Two novel training strategies are proposed: token-level loss weighting that emphasizes contextually-dependent tokens during training, and metric-based example selection that prioritizes examples most beneficial for context utilization using the MaxPCXMI metric. Models are evaluated using BLEU, COMET, ctxPro accuracy, and ContraPro contrastive accuracy.

## Key Results
- Models trained on higher proportions of contextually-rich examples show significantly improved performance on corresponding phenomena (ctxPro accuracy gains of 3-6 percentage points)
- Improvements in handling one linguistic phenomenon do not generalize to others, demonstrating task-specific learning
- Cross-lingual transfer of context utilization is limited and not significantly stronger within the same language sub-family
- Proposed training strategies improve context utilization by up to 6 percentage points (single language) and 8 percentage points (multilingual setting) on ctxPro evaluation

## Why This Works (Mechanism)
The paper demonstrates that context-aware machine translation performance is directly influenced by the composition of training data, with models learning to handle specific linguistic phenomena based on the examples they encounter during training. The token-level loss weighting approach works by emphasizing the importance of context-dependent tokens during training, forcing the model to pay more attention to resolving ambiguities that require contextual information. The metric-based example selection method identifies and prioritizes training examples that are most beneficial for developing context utilization capabilities, effectively curating a more efficient training set for context-aware translation.

## Foundational Learning
- **Context-aware machine translation**: Translation that considers surrounding sentences to resolve ambiguities and maintain coherence; needed because many translation decisions depend on context beyond individual sentences; quick check: model concatenates current sentence with up to 3 previous sentences using `[SEP]` token
- **ctxPro toolset**: Framework for extracting contextually-rich examples across five linguistic phenomena; needed to systematically identify and annotate training data with context-dependent elements; quick check: identifies gender, formality, auxiliary, inflection, and animacy phenomena
- **Token-level loss weighting**: Training technique that multiplies loss by a factor for contextually-dependent tokens; needed to emphasize importance of context resolution during training; quick check: λ∈{2,5,10} applied to annotated tokens
- **MaxPCXMI metric**: Selection metric that identifies most beneficial examples for context utilization; needed to efficiently curate training data without requiring additional annotations; quick check: selects top-k examples based on mutual information with context phenomena
- **Contrastive evaluation**: Method comparing model outputs on ambiguous contexts to measure context utilization; needed to provide fine-grained assessment beyond standard metrics; quick check: ContraPro accuracy measures correct resolution of context-dependent ambiguities

## Architecture Onboarding
- **Component map**: Pre-trained model → Context-aware fine-tuning (concatenated sentences) → Token-level loss weighting / Metric-based selection → Evaluation (BLEU/COMET/ctxPro/ContraPro)
- **Critical path**: Data annotation with ctxPro → Controlled dataset construction → Context-aware fine-tuning with proposed methods → Evaluation on context-sensitive tasks
- **Design tradeoffs**: Maximum context window of 3 sentences balances computational efficiency with coverage of longer-distance dependencies; synthetic data construction enables controlled experiments but may not reflect natural distribution
- **Failure signatures**: BLEU drops >1% while ctxPro accuracy improves indicates excessive emphasis on context phenomena; no improvement on target phenomenon suggests annotation quality issues or insufficient context window
- **3 first experiments**: 1) Implement token-level loss weighting with λ=2 and verify ctxPro accuracy improvement on target phenomenon; 2) Test metric-based selection by comparing top-1000 vs random examples for fine-tuning; 3) Evaluate cross-lingual transfer by training on en-de and testing on en-fr for same phenomena

## Open Questions the Paper Calls Out
None

## Limitations
- Study focuses on limited set of linguistic phenomena (gender, formality, auxiliary verbs) in English-German pair, limiting generalizability
- Maximum context window of 3 sentences may miss longer-distance dependencies, with 9-29% of examples having antecedent distances exceeding this window
- Token-level loss weighting assumes accurate alignment between ctxPro word-level annotations and subword token boundaries, which may introduce implementation challenges

## Confidence
- **High Confidence**: Positive correlation between contextually-rich example density and model performance on specific phenomena
- **Medium Confidence**: Limited cross-lingual transfer of context utilization capabilities
- **Medium Confidence**: Effectiveness of proposed training strategies (token-level loss weighting and metric-based selection)

## Next Checks
1. Test token-level loss weighting implementation across multiple tokenization schemes to verify robustness when ctxPro annotations don't perfectly align with subword tokens
2. Evaluate proposed methods on naturally occurring rather than synthetically constructed datasets to assess real-world applicability
3. Conduct ablation studies removing the 3-sentence context window limit to quantify impact of longer-distance dependencies currently inaccessible to models