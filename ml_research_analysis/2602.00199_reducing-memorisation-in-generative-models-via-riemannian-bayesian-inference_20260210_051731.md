---
ver: rpa2
title: Reducing Memorisation in Generative Models via Riemannian Bayesian Inference
arxiv_id: '2602.00199'
source_url: https://arxiv.org/abs/2602.00199
tags:
- memorisation
- riemannian
- generative
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of excessive memorization in generative
  models, which can undermine generalization and raise privacy concerns. The authors
  propose a Bayesian approach using Riemannian geometry to sample from the parameter
  space of generative models, specifically flow matching and diffusion models.
---

# Reducing Memorisation in Generative Models via Riemannian Bayesian Inference

## Quick Facts
- arXiv ID: 2602.00199
- Source URL: https://arxiv.org/abs/2602.00199
- Reference count: 40
- Primary result: Riemannian Laplace approximation reduces memorization in generative models while preserving generalization better than Euclidean perturbations

## Executive Summary
This paper addresses excessive memorization in generative models by introducing a Riemannian Bayesian inference approach. The authors construct a posterior distribution that captures the geometry of the loss landscape, allowing generation of models that reduce memorization while maintaining generalization. The key innovation is using geodesic paths on the parameter manifold rather than straight-line perturbations, which keeps sampled models in low-loss regions and preserves functional symmetries.

## Method Summary
The method involves training a flow matching or diffusion model to obtain a MAP estimate, then computing a low-rank approximation of the Hessian using Lanczos iterations. Instead of sampling directly in parameter space (Euclidean Laplace), the approach samples from a Gaussian in the tangent space and maps these samples to the parameter space using geodesics - the shortest paths on the Riemannian manifold defined by the loss landscape. This Riemannian Laplace approximation respects the local geometry of the loss manifold, keeping perturbed parameters in high-probability regions and reducing memorization.

## Key Results
- Riemannian perturbations stay closer to low-loss regions than Euclidean perturbations (Theorem 4.2)
- The method reduces memorization ratio across multiple threshold values in both toy and CIFAR-10 experiments
- Riemannian samples preserve functional symmetry (e.g., odd/even properties) that Euclidean samples break
- KL divergence to target distribution is lower for Riemannian approach compared to Euclidean in 1D toy experiments

## Why This Works (Mechanism)

### Mechanism 1: Riemannian Perturbations Stay in Low-Loss Regions
Parameter samples from the Riemannian Laplace approximation remain in high-probability regions of the true posterior better than Euclidean samples. The Riemannian approach solves geodesic equations on the loss manifold (Eq. 14), which bends paths away from high-loss directions. The correction term in Eq. 26 explicitly opposes motion toward directions where the Hessian indicates sharp loss increase: `κ ∝ ∇θL / (1 + ||∇θL||²)`, which shrinks steps toward high-curvature (high-loss) directions.

### Mechanism 2: Perturbations Disrupt Memorization by Moving Boundary Points
Parameter perturbations selectively shift generated samples that lie near the memorization boundary into non-memorized regions. For memorization threshold `c`, a point `x̂` is memorized if `||x̂ - x^(1)|| ≤ c||x̂ - x^(2)||`. Perturbations change the endpoint `x̂' = g_θ'(x₀)` by modifying the velocity field (Eq. 24). Theorem 4.1 shows that when `x̂` lies between training points, a perturbation of magnitude `||δ||` in the right direction pushes it into a "green zone" where it fails the memorization criterion.

### Mechanism 3: Geodesic Sampling Preserves Functional Symmetry
Riemannian samples maintain inductive biases (e.g., odd/even symmetry in velocity fields) that Euclidean samples break. The Riemannian metric `G(θ) = I_K + ∇θL·∇θL^T` (Eq. 18) couples perturbation directions to the loss gradient. This constrains sampled models to remain on the loss manifold's "fiber" of similar-performing functions.

## Foundational Learning

- **Concept: Laplace Approximation**
  - Why needed here: The method builds on standard Bayesian LA (Gaussian posterior at MAP with inverse Hessian covariance) before extending to Riemannian geometry
  - Quick check question: Can you explain why the Hessian must be positive-definite for LA sampling?

- **Concept: Geodesics and Riemannian Metrics**
  - Why needed here: The core innovation replaces straight-line perturbations with manifold-aware paths
  - Quick check question: Given metric G(θ), how do Christoffel symbols Γ^k_ij affect geodesic curvature?

- **Concept: Flow Matching / Diffusion Models**
  - Why needed here: The method operates on velocity fields u_θ(x,t) learned via flow matching
  - Quick check question: What is the relationship between the learned velocity field and generated samples x(1)?

## Architecture Onboarding

- **Component map:** MAP Training -> Hessian Computation -> Initial Velocity Sampling -> Geodesic Integration -> Generation

- **Critical path:** Hessian quality -> valid eigendecomposition -> geodesic solver stability. The paper notes Euler/fixed-step solvers fail (produce noise images); adaptive RK is required.

- **Design tradeoffs:**
  - Subset of parameters: Full-network Hessian is intractable; paper uses first layer only for CIFAR-10. Trade-off: cheaper but incomplete posterior.
  - Lanczos iterations k: More iterations capture more eigenvectors but memory scales as O(k·K).
  - Perturbation scale η: Larger η reduces memorization but risks leaving the valid manifold region.

- **Failure signatures:**
  - Psychadelic/noise images: Indicates geodesic solver divergence (use adaptive RK45, reduce η)
  - No memorization reduction: MAP already generalizes well; perturbations have little effect
  - High KL divergence: Perturbations too aggressive; Euclidean path taken (check if geodesic solver converged)

- **First 3 experiments:**
  1. 1D toy problem replication: Train flow matching on 2-point dataset, compute LA vs Riemannian LA, plot velocity field uncertainty and KL to true GMM
  2. Eigenspectrum analysis: Run Lanczos on your model's Hessian (single batch). Confirm few large eigenvalues dominate
  3. Memorization sweep: For fixed initial noise x₀, generate images under increasing η for both Euclidean and Riemannian. Plot memorization ratio vs η

## Open Questions the Paper Calls Out
1. How can the Riemannian posterior sampling be modified to generate structured semantic variations rather than unstructured pixel noise?
2. What mechanisms cause the "sink" behavior observed when perturbing along top eigenvectors, and can it be analytically avoided?
3. Does the Riemannian Laplace approximation maintain its benefits when applied to the full parameter space of large-scale generative models?

## Limitations
- Computational constraints limited experiments to first-layer Hessian approximation rather than full network
- Memorization metric may not capture all privacy-relevant aspects of memorization
- Performance on larger-scale models and more complex datasets remains untested

## Confidence
- Core claims about Riemannian perturbations staying closer to MAP: **High confidence** (proven in Theorems 4.1-4.2)
- Practical memorization reduction benefits: **Medium confidence** (limited scale experiments)
- Scalability to full networks: **Low confidence** (not validated beyond first layer)

## Next Checks
1. Apply full Hessian approximation (not just first layer) on a moderately-sized generative model to verify computational advantages and benefits persist
2. Systematically vary perturbation magnitude η across multiple datasets and model scales to quantify Pareto frontier between memorization reduction and generation quality
3. Construct or identify loss landscapes with known curvature pathologies to test whether geodesic computation fails or produces invalid samples in these regimes