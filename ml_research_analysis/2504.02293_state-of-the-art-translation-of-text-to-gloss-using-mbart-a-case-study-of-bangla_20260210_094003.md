---
ver: rpa2
title: 'State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of
  Bangla'
arxiv_id: '2504.02293'
source_url: https://arxiv.org/abs/2504.02293
tags:
- gloss
- data
- bangla
- language
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla

## Quick Facts
- arXiv ID: 2504.02293
- Source URL: https://arxiv.org/abs/2504.02293
- Authors: Sharif Md. Abdullah; Abhijit Paul; Shebuti Rayana; Ahmedul Kabir; Zarif Masud
- Reference count: 40
- Primary result: Achieves SOTA SacreBLEU=63.89 on PHOENIX-14T benchmark using mBART-50 fine-tuning

## Executive Summary
This paper introduces the first text-to-gloss translation system for Bangla Sign Language (BdSL), achieving state-of-the-art results on both the BdSL and German PHOENIX-14T benchmarks. The authors propose using mBART-50 fine-tuning combined with rule-based synthetic data generation to overcome the low-resource challenge in BdSL. The key insight is that mBART's pre-training on shuffled text aligns well with gloss translation's word reordering requirements. The system generates large-scale synthetic training data using deterministic linguistic rules, enabling effective fine-tuning of the multilingual model despite limited annotated data.

## Method Summary
The authors fine-tune mBART-large-50 with a learning rate of 2e-5 for 3 epochs on a synthetic Bangla text-to-gloss dataset. They construct training data through three approaches: (1) rule-based gloss generation using SOV reordering and content-word filtering (Algorithm 2), (2) GPT-4o synthetic data generation, and (3) multilingual augmentation using PHOENIX-14T German data. Evaluation uses SacreBLEU, BLEU-1 through BLEU-4, and COMET metrics. The paper also establishes baselines using mBERT-multiclass, RNN, GRU, and Seq2Seq models trained from scratch.

## Key Results
- mBART-50 achieves SacreBLEU=63.89 on PHOENIX-14T, establishing SOTA on this benchmark
- Rule-based synthetic data augmentation enables training on BdSL despite low-resource constraints
- Fine-tuning mBART-50 outperforms training Seq2Seq from scratch (BLEU-1: 85.1 vs 20.75 on PHOENIX-14T)
- mBART-50 performs significantly better than BERT-multiclass, RNN, and GRU baselines on BdSL text-to-gloss

## Why This Works (Mechanism)

### Mechanism 1
- Claim: mBART's pre-training on shuffled text aligns with gloss translation requirements
- Mechanism: mBART was pre-trained with sentence shuffling and token masking as denoising objectives. Since gloss generation inherently involves word reordering (Algorithm 1 applies permutations where tokens move ≤4 positions), mBART's pre-training creates an inductive bias suited for this task.
- Core assumption: Text-to-gloss translation difficulty is primarily driven by word reordering rather than semantic transformation.
- Evidence anchors:
  - [abstract]: "We soon notice an interesting property of mBART -- it was trained on shuffled and masked text data. And as we know, gloss form has shuffling property. So we hypothesize that mBART is inherently good at text-to-gloss tasks."
  - [section]: Algorithm 1 specifies "Apply random permutation σ such that ∀i ∈ {1, |S|}, |σ(i) − i| ≤ 4"
  - [corpus]: No direct corpus validation of this hypothesis beyond this paper
- Break condition: If gloss semantics require transformations beyond word-order changes (e.g., aspectual modifications, classifier predicates), the pre-training advantage may not transfer.

### Mechanism 2
- Claim: Rule-based synthetic data augmentation enables low-resource text-to-gloss training
- Mechanism: Apply deterministic linguistic rules (SOV reordering, lemmatization, filtering non-content words, handling negation/locations) to existing text corpora, generating large-scale noisy but useful training pairs without expert annotation.
- Core assumption: Rule-generated glosses are sufficiently similar to human annotations that they improve rather than degrade model learning.
- Evidence anchors:
  - [abstract]: "our results show that BdSL text-to-gloss task can greatly benefit from rule-based synthetic dataset"
  - [section]: Algorithm 2 details language-specific rules; Table III shows Bangla-gloss achieving SacreBLEU=79.53
  - [corpus]: Weak—neighboring papers discuss Bangla sign language datasets but don't validate rule-based augmentation for text-to-gloss
- Break condition: If target sign language grammar diverges significantly from the assumed rules (e.g., non-SOV languages, different negation placement)

### Mechanism 3
- Claim: Multilingual pre-training provides cross-lingual transfer for text-to-gloss
- Mechanism: mBART-50's multilingual representations allow the model to leverage patterns from German DGS text-to-gloss (PHOENIX-14T) when fine-tuned on Bangla, even with limited Bangla-specific data.
- Core assumption: Text-to-gloss transformation patterns share structural similarities across sign languages that transfer via multilingual representations.
- Evidence anchors:
  - [abstract]: mBART-50 achieved "State-of-the-Art performance on PHOENIX-14T benchmark" (SacreBLEU=63.89)
  - [section]: "considering high-quality multilingual dataset can improve T2G performance [17]"
  - [corpus]: No independent validation—this is the first text-to-gloss work for BdSL
- Break condition: If sign language typological differences (e.g., simultaneous vs. sequential morphology) create fundamentally different gloss patterns

## Foundational Learning

- Concept: **Gloss as intermediate representation**
  - Why needed here: The entire pipeline depends on understanding that gloss is a word-by-word annotated form bridging spoken text and sign language production.
  - Quick check question: Why use gloss as an intermediate step rather than direct text-to-sign video generation?

- Concept: **BART denoising pre-training**
  - Why needed here: The paper's core hypothesis rests on understanding BART's shuffled-text reconstruction objective differs from BERT's masked token prediction.
  - Quick check question: How does autoregressive decoding in BART differ from bidirectional encoding in BERT?

- Concept: **BLEU variants and COMET**
  - Why needed here: The paper reports 6 evaluation metrics; understanding what each measures is essential for interpreting claims.
  - Quick check question: Why might SacreBLEU standardization matter for reproducibility compared to raw BLEU?

## Architecture Onboarding

- Component map:
  - Text corpora -> Rule-based gloss generation (Algorithm 1-2) -> GPT-4o synthetic data -> mBART-50 fine-tuning -> Evaluation metrics

- Critical path:
  1. Construct dataset via Algorithm 1 (general rules) + Algorithm 2 (language-specific SOV handling)
  2. Fine-tune mBART-50 for 3 epochs with learning rate 2e-5
  3. Evaluate using SacreBLEU, BLEU-1-4, COMET

- Design tradeoffs:
  - Rule-based generation scales to 176K samples but fails on complex vocabulary (BdSL has only ~1,200 words vs. 100K in Bangla)
  - Fine-tuning mBART outperforms training Seq2Seq from scratch (BLEU-1: 85.1 vs 20.75)
  - Conversational data produces better rule-based glosses than book reviews

- Failure signatures:
  - Low performance on RNN/GRU indicates insufficient data for training from scratch
  - Book-review corpus produces poor glosses due to complex sentence structures and vocabulary mismatch
  - COMET dropping to 0.624 on PHOENIX-14T suggests semantic drift on held-out benchmarks

- First 3 experiments:
  1. Replicate PHOENIX-14T fine-tuning to validate SOTA claims against reported baselines (Mansueto et al., Stoll et al., Moryossef et al.)
  2. Ablate data sources: train mBART on only rule-based vs. only LLM-generated vs. combined to isolate contribution
  3. Test cross-lingual zero-shot: evaluate Bangla fine-tuned model on German PHOENIX test set to probe transfer limits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is mBART's superior performance on text-to-gloss tasks specifically attributable to its pre-training on shuffled text?
- Basis in paper: [explicit] The authors state, "We soon notice an interesting property of mBART -- it was trained on shuffled and masked text data... So we hypothesize that mBART is inherently good at text-to-gloss tasks," but concede that "further review of literature is needed to solidify this hypothesis."
- Why unresolved: The observed high performance supports the hypothesis, but the paper does not include ablation studies or architectural comparisons to prove the "shuffling" capability is the distinct causal factor.
- What evidence would resolve it: A comparative analysis of mBART against other pre-trained models that lack shuffled text training, or an ablation study isolating the shuffling component's contribution to the BLEU scores.

### Open Question 2
- Question: Can native, intuition-based gloss generation rules for Bangla outperform the currently adapted German-DGS rules?
- Basis in paper: [explicit] The authors mention using adapted German rules and state, "We aim to explore more intuitive and native gloss generation rules for Bangla in future."
- Why unresolved: The current methodology relies on adapting German grammatical rule sets (Algorithm 2) because native Bangla text-to-gloss rules were previously undeveloped.
- What evidence would resolve it: Developing a rule set derived specifically from Bangla linguistic properties and comparing the translation quality of models trained on this data against the current German-adapted baseline.

### Open Question 3
- Question: To what extent can explicit linguistic insights improve the algorithmic generation of Bangla glosses?
- Basis in paper: [explicit] The authors identify a lack of expert evaluation for rule-based data and state, "Additionally, we want to incorporate linguists insights into bangla gloss generation algorithm to generate better glosses."
- Why unresolved: The current rule-based approach (Algorithm 1 & 2) is heuristic and programmatic, lacking the nuanced syntactic and semantic validation that a linguistic expert would provide.
- What evidence would resolve it: A study comparing the semantic accuracy and model performance of heuristically generated glosses versus glosses generated by an algorithm explicitly encoded with linguistic expert insights.

## Limitations
- No independent validation of the claimed SOTA performance on PHOENIX-14T benchmark
- Rule-based gloss generation assumes SOV word order and simple vocabulary, limiting generalizability
- GPT-4o synthetic data generation uses unknown prompt templates, hindering exact reproduction

## Confidence
- **High Confidence**: mBART-50 outperforms training from scratch on small datasets (85.1 vs 20.75 BLEU-1 on PHOENIX-14T), and fine-tuning multilingual models shows clear advantage over monolingual baselines
- **Medium Confidence**: Claims about mBART's pre-training being inherently suited for gloss translation due to shuffled-text training. No ablation studies specifically test this hypothesis
- **Low Confidence**: The state-of-the-art claim on PHOENIX-14T, as no independent verification exists

## Next Checks
1. **Independent Replication**: Fine-tune mBART-50 on PHOENIX-14T using the same hyperparameters (lr=2e-5, 3 epochs) and compare SacreBLEU scores against the reported 63.89
2. **Data Source Ablation**: Train mBART-50 separately on only rule-based glosses, only GPT-4o synthetic glosses, and the combined dataset to isolate data contribution
3. **Cross-Lingual Transfer Boundary**: Evaluate the Bangla fine-tuned mBART-50 on the PHOENIX-14T test set (zero-shot transfer) and vice versa to test multilingual pre-training transfer