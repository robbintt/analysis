---
ver: rpa2
title: Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for
  Summary Evaluation
arxiv_id: '2502.08514'
source_url: https://arxiv.org/abs/2502.08514
tags:
- summary
- document
- source
- sentence
- ambiguity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MADISSE, a multi-agent debate framework for
  evaluating the faithfulness of summaries using large language models (LLMs). The
  key innovation is assigning opposing initial stances to LLM-based agents and having
  them engage in multi-round debates to reach a consensus on whether a summary is
  faithful or not.
---

# Faithful, Unfaithful or Ambiguous? Multi-Agent Debate with Initial Stance for Summary Evaluation

## Quick Facts
- arXiv ID: 2502.08514
- Source URL: https://arxiv.org/abs/2502.08514
- Reference count: 40
- Introduces MADISSE, a multi-agent debate framework that improves faithfulness evaluation of summaries using opposing LLM stances and achieves 75.1% balanced accuracy on MeetingBank

## Executive Summary
This paper introduces MADISSE, a multi-agent debate framework for evaluating the faithfulness of summaries using large language models (LLMs). The key innovation is assigning opposing initial stances to LLM-based agents and having them engage in multi-round debates to reach a consensus on whether a summary is faithful or not. The paper also introduces a new dimension, "ambiguity," and a detailed taxonomy to identify summaries that can be interpreted in multiple correct ways, affecting their faithfulness evaluation. MADISSE outperforms single and multi-LLM baselines on faithfulness evaluation, particularly on non-ambiguous summaries, and helps identify ambiguous cases. The framework shows improved balanced accuracy and Krippendorff's alpha, demonstrating better alignment with human judgments.

## Method Summary
MADISSE uses four LLM evaluator agents with opposing initial stances (2 faithful, 2 unfaithful) to debate summary faithfulness for up to 3 rounds. Agents provide arguments following structured guidelines, with chat history injected into subsequent rounds. If no consensus emerges, k adjudicator agents vote on the final label. The framework can run m simultaneous debate sessions with aggregated voting. An optional ambiguity detection module analyzes debate arguments using a taxonomy to flag summaries interpretable in multiple correct ways.

## Key Results
- MADISSE achieves 75.1% balanced accuracy on MeetingBank vs 69.1% for MADISSE without initialization
- Debate-arguments method achieves 71.4% balanced accuracy for ambiguity detection vs 59.3% for taxonomy-only baseline
- Filtering ambiguous cases increases Krippendorff's alpha from ~0.52 to ~0.71 for the debate method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Assigning opposing initial stances to LLM agents increases error detection in summary faithfulness evaluation.
- Mechanism: When forced to justify an imposed belief (regardless of actual assessment), LLMs engage in deeper semantic analysis. The paper states: "when told that a given summary is unfaithful, LLMs can come up with correct reasoning and arguments that they couldn't otherwise." Uniform stance distribution ensures balanced debate pressure from both sides.
- Core assumption: LLMs possess latent error-detection capability that is underutilized in standard zero-shot evaluation due to fluency bias.
- Evidence anchors:
  - [abstract] "uniformly distributed initial assignments result in greater diversity of stances leading to more meaningful debates and ultimately more errors identified"
  - [section] Table 3 shows MADISSE (75.1 BAcc) significantly outperforms MADISSE without initialization (69.1 BAcc) on MeetingBank
  - [corpus] Limited corpus validation; related work focuses on faithfulness improvement rather than debate-based evaluation mechanisms
- Break condition: If the underlying LLM lacks sufficient reasoning capacity to construct coherent justifications for arbitrary stances, debate quality degrades.

### Mechanism 2
- Claim: Multi-round structured debate surfaces ambiguous summaries that have multiple valid interpretations.
- Mechanism: When agents with opposing stances both produce plausible arguments following guidelines, this signals potential ambiguity. The disagreement pattern becomes a detection signal rather than noise.
- Core assumption: Ambiguous summaries (those interpretable as both faithful and unfaithful) exist systematically and contribute to low inter-annotator agreement.
- Evidence anchors:
  - [section 4.3] "if there are sound arguments both supporting the faithfulness of the summary as well as some sound arguments arguing for the unfaithfulness... the summary will be deemed ambiguous"
  - [section 6] Table 4 shows debate-arguments method achieves 71.4 BAcc for ambiguity detection vs. 59.3 for taxonomy-only baseline
  - [corpus] No direct corpus validation of the ambiguity taxonomy; this is a novel contribution requiring further validation
- Break condition: If guidelines are misinterpreted or agents produce superficial arguments, false-positive ambiguity detection increases.

### Mechanism 3
- Claim: Filtering ambiguous summaries improves evaluator accuracy and alignment with human judgments.
- Mechanism: Ambiguous cases introduce noise in evaluation; removing them allows clearer assessment of actual faithfulness detection capability.
- Core assumption: Ambiguity is a property of the summary-document pair, not a flaw in evaluation methodology.
- Evidence anchors:
  - [section 6] Figure 3 shows K-alpha increases from ~0.52 to ~0.71 for debate method after filtering ambiguous cases
  - [section 4.2] Annotated dataset has Cohen's Kappa ≈ 0.73 on binary ambiguity labels
  - [corpus] Weak corpus support; related work does not address ambiguity as an evaluability dimension
- Break condition: If ambiguity annotation is inconsistent or the taxonomy is incomplete, filtering may remove legitimate test cases.

## Foundational Learning

- Concept: **Faithfulness vs. Factuality in Summarization**
  - Why needed here: Faithfulness requires attribution to the source document specifically, while factuality allows attribution to world knowledge. This distinction is foundational to the evaluation task definition.
  - Quick check question: If a summary contains a true statement not mentioned in the source document, is it faithful?

- Concept: **Balanced Accuracy and Inter-Annotator Agreement Metrics**
  - Why needed here: The paper uses Balanced Accuracy (accounts for class imbalance) and Krippendorff's alpha (measures alignment with human judgments). Understanding these is essential for interpreting results.
  - Quick check question: Why would accuracy alone be insufficient when evaluating on datasets with 10% vs. 49% unfaithful summaries?

- Concept: **Prompt Engineering for Multi-Agent Systems**
  - Why needed here: The framework relies on structured prompts with guidelines, chat history injection, and stance assignment. Prompt design directly affects debate quality.
  - Quick check question: What happens if the same guidelines are not provided to both evaluator and adjudicator agents?

## Architecture Onboarding

- Component map:
  Initialization Module -> Debate Engine -> Adjudication Module -> Final Label

- Critical path: Document + Summary → Stance Assignment → Debate (up to n rounds) → Adjudication (if no consensus) → Final Label. Ambiguity detection runs as parallel analysis on debate artifacts.

- Design tradeoffs:
  - More agents/rounds increases context length (may hit LLM limits) but improves robustness
  - Higher FPR accompanies lower FNR (MADISSE more sensitive to potential errors); tradeoff adjustable via stance distribution
  - Simultaneous sessions reduce error propagation but multiply compute cost

- Failure signatures:
  - Agents converge too quickly on incorrect label → check stance initialization, guideline clarity
  - Persistent disagreement on non-ambiguous summaries → may indicate guideline conflicts or insufficient rounds
  - High FPR on ambiguous cases → expected; consider ambiguity pre-filtering
  - Adjudicator bias toward argument order → verify random permutation is applied

- First 3 experiments:
  1. Replicate Table 3 comparison on held-out subset: Zero-shot vs. Chain-of-Thought vs. Self-consistency vs. MADISSE (with/without initialization) using same LLM
  2. Ablate stance distribution: test 3+1, 2+2, 1+3 configurations to validate uniform distribution claim (see Table 34)
  3. Measure ambiguity detection precision/recall: run debate-arguments method on annotated MeetingBank subset, compare against self-consistency variation baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the MADISSE framework be adapted to provide a continuous faithfulness score rather than a binary label, and would this improve evaluation granularity?
- Basis in paper: [explicit] The Limitations section states, "MADISSE can be modified to ask for a faithfulness rating rather than a binary label. This can further improve the evaluation... This can be a direction for future work."
- Why unresolved: The current implementation aggregates agent debates into a binary "faithful" or "unfaithful" decision, which may lose nuance regarding the degree of error.
- What evidence would resolve it: Experiments modifying the adjudicator prompt to output a probability or Likert scale, comparing the correlation of these scores with human judgments against the baseline binary accuracy.

### Open Question 2
- Question: Does the concept of ambiguity scale from sentences to full summaries, and do ambiguities span across sentence boundaries?
- Basis in paper: [explicit] The Limitations section notes, "ambiguity annotation is only done on sentence-level. More analysis is required to see whether ambiguities can span over a sentence."
- Why unresolved: The current ambiguity taxonomy and dataset (MeetingBank) focus on sentence-level evaluation; it is unknown if complex, cross-sentence dependencies introduce distinct ambiguity types.
- What evidence would resolve it: An extension of the ambiguity annotation task to document-level summaries, followed by an analysis of inter-sentence ambiguity phenomena.

### Open Question 3
- Question: Can the proposed ambiguity taxonomy be utilized as a feedback mechanism to train summarizers to avoid generating ambiguous text?
- Basis in paper: [explicit] Appendix B.2 states, "A summarizer can additionally be evaluated on ambiguity dimension and be provided with feedback to avoid generating ambiguous summaries. This can be seen as a future direction."
- Why unresolved: The current work focuses on *identifying* ambiguity for evaluation purposes, not on correcting the summarization model itself.
- What evidence would resolve it: Reinforcement Learning (RL) experiments where the MADISSE ambiguity detector serves as a reward signal to penalize ambiguous generations.

### Open Question 4
- Question: How can the accuracy of the ambiguity detection module be improved beyond the current 71.4% baseline?
- Basis in paper: [inferred] While the authors explicitly note "room for improvement," the methodological constraint is that the current detector relies on checking if arguments from the debate support opposing views.
- Why unresolved: The current method depends on the quality of the generated debate; if agents fail to generate sound opposing arguments, the ambiguity is missed.
- What evidence would resolve it: Implementing specialized fine-tuning for ambiguity detection or using the generated debate arguments as synthetic training data for a smaller, dedicated classifier.

## Limitations

- Ambiguity detection methodology relies heavily on debate quality and may miss ambiguities if agents fail to generate opposing arguments
- Framework computational cost is not quantified, making practical deployment considerations unclear
- Results generalization beyond Llama3-70B-instruct is limited, with only one alternative LLM tested

## Confidence

**High Confidence**:
- MADISSE outperforms single-agent baselines on balanced accuracy for faithfulness evaluation
- Stance initialization improves performance over uniform initial beliefs
- Filtering ambiguous cases improves Krippendorff's alpha alignment with human judgments

**Medium Confidence**:
- The ambiguity detection mechanism reliably identifies ambiguous summaries (based on limited MeetingBank validation)
- Debate format inherently improves error detection through deeper semantic analysis
- Results generalise across different LLMs (limited evidence with GPT-4o-mini)

**Low Confidence**:
- Ambiguity is the primary driver of low inter-annotator agreement in summarization evaluation
- The framework's performance is robust to different debate configurations and stopping criteria
- Computational cost is justified by accuracy improvements

## Next Checks

1. **Cross-Model Validation**: Replicate MADISSE performance using three different LLMs (e.g., GPT-4, Claude-3, Mistral-7B) on the same dataset to test mechanism generalisability beyond Llama3-70B-instruct.

2. **Ambiguity Taxonomy External Validation**: Apply the ambiguity detection framework to an independently annotated summarization dataset with known inter-annotator disagreement patterns to test whether detected ambiguity correlates with disagreement.

3. **Cost-Performance Analysis**: Measure the relationship between number of debate rounds, number of adjudicator agents, and final accuracy to identify optimal resource allocation and quantify the computational overhead relative to accuracy gains.