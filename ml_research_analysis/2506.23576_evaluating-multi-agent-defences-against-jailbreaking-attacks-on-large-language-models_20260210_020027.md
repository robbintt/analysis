---
ver: rpa2
title: Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language
  Models
arxiv_id: '2506.23576'
source_url: https://arxiv.org/abs/2506.23576
tags:
- agent
- original
- response
- multi-agent
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates multi-agent LLM systems as a defense against
  jailbreak attacks. It compares 1-, 2-, and 3-agent configurations against three
  jailbreak strategies: BetterDan, JB, and the original attack.'
---

# Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models

## Quick Facts
- arXiv ID: 2506.23576
- Source URL: https://arxiv.org/abs/2506.23576
- Authors: Maria Carolina Cornelia Wit; Jun Pang
- Reference count: 40
- Primary result: Multi-agent LLM systems reduce false negatives and improve ASR against jailbreak attacks, but increase false positives

## Executive Summary
This paper evaluates multi-agent large language model (LLM) systems as a defense against jailbreak attacks, comparing 1-, 2-, and 3-agent configurations against three jailbreak strategies: BetterDan, JB, and the original attack. The study demonstrates that multi-agent systems significantly reduce false negative rates and attack success rates compared to single-agent defenses, with the 3-agent configuration showing optimal performance for BetterDan attacks (ASR 10.51%, FNR 0.51%). However, the research also reveals a critical trade-off: while multi-agent defenses improve security metrics, they substantially increase false positive rates, particularly for the JB attack template where responses contain layered refusal prefixes followed by harmful content. The paper highlights the need for context-aware evaluation methods and suggests that optimal agent configuration varies by attack type, with 3 agents best for BetterDan, 2 agents for JB, and 1 agent for the original attack.

## Method Summary
The study evaluates 1-, 2-, and 3-agent LLM defense systems using the AutoDefense framework against three jailbreak attacks. Researchers generated 390 response samples per jailbreak template (total 1170) by querying GPT-3.5 with templates from Appendix A. Ground truth labels were created using GPT-4.1 with the AttackEval prompt. The defense systems classify responses as "VALID" (unethical/harmful) or "INVALID" (ethical/benign). The 1-agent system uses direct judgment, the 2-agent system employs an Analyzer (Intent + Prompt) and Judge, while the 3-agent system adds an Intention Analyzer. Performance metrics include Attack Success Rate (ASR), False Negative Rate (FNR), False Positive Rate (FPR), Accuracy, Precision, Recall, F1, and P4-score.

## Key Results
- 3-agent configuration achieves lowest ASR for BetterDan attack (10.51%) and lowest FNR (0.51%)
- JB attack shows highest ASR at 55.13% with significant false positive rates due to layered response format
- Multi-agent systems improve ASR and FNR but substantially increase false positive rates across all configurations
- Optimal agent count varies by attack type: 3 agents for BetterDan, 2 agents for JB, 1 agent for Original Attack

## Why This Works (Mechanism)
The multi-agent defense framework reduces jailbreak attack success by implementing a layered verification approach where specialized agents perform distinct analytical roles. The Intention Analyzer examines the underlying harmful intent, the Prompt Analyzer evaluates the request structure, and the Judge makes final determinations based on combined assessments. This distributed responsibility reduces the likelihood that any single agent's bias or blind spot allows a jailbreak to succeed. The system's effectiveness varies by attack type because different jailbreak strategies exploit different vulnerabilities - BetterDan's sophisticated approach requires comprehensive analysis, while JB's layered format creates parsing challenges that even multi-agent systems struggle to handle correctly.

## Foundational Learning
**LLM Jailbreak Attacks**: Techniques that manipulate prompt engineering to bypass safety filters and elicit harmful responses - needed to understand attack vectors and defense requirements; quick check: can you explain the difference between BetterDan and JB attack strategies?
**Multi-Agent System Architecture**: Distributed AI systems where multiple specialized agents collaborate to solve problems - needed to grasp how defense components interact; quick check: can you diagram the 3-agent workflow from Intention Analyzer to Judge?
**Attack Success Rate (ASR) vs False Positive Rate (FPR) Trade-off**: Security metrics that often conflict, where improving one degrades the other - needed to interpret the paper's core finding about accuracy vs. false alarms; quick check: can you calculate FPR given TP, FP, TN, FN values?
**Prompt Engineering for Safety**: Crafting prompts that guide LLMs toward ethical behavior while maintaining functionality - needed to understand both attack and defense mechanisms; quick check: can you write a prompt template that asks an LLM to identify harmful content in a response?

## Architecture Onboarding

**Component Map**: Jailbreak Template -> GPT-3.5 Target -> Multi-Agent System (Intention Analyzer -> Prompt Analyzer -> Judge) -> Classification (VALID/INVALID) -> Ground Truth Comparison

**Critical Path**: Input response → Intention Analyzer assessment → Prompt Analyzer evaluation → Judge final determination → Output classification

**Design Tradeoffs**: The paper trades computational efficiency for improved security, as multi-agent systems require more API calls and processing time compared to single-agent defenses. The 3-agent configuration provides optimal security but at highest computational cost, while 1-agent offers fastest response but poorest security metrics.

**Failure Signatures**: High false positive rates indicate the system is overly cautious, misclassifying benign content as harmful. The JB attack specifically fails due to the dual-layer response format where refusal prefixes followed by harmful content confuse the layered analysis. Low FNR but high FPR suggests the system prioritizes catching threats over minimizing false alarms.

**First 3 Experiments**:
1. Test the 3-agent system on BetterDan attack samples to verify the reported ASR of 10.51% and FNR of 0.51%
2. Evaluate JB attack responses with 2-agent configuration to confirm the 55.13% ASR and identify false positive patterns
3. Run the Original Attack through 1-agent system to validate baseline performance against multi-agent improvements

## Open Questions the Paper Calls Out

**Cross-Cultural LLM Safety Alignment**: How do non-Western LLMs (e.g., DeepSea k) compare to Western models in detecting jailbreak attempts and aligning on ethical assessments? The paper suggests extending evaluation to non-Western language models to examine whether such models detect more or fewer jailbreak attempts and to identify where misalignments in ethical assessments may arise. Cultural variability in ethical judgments (e.g., around sexual content) remains untested since current evaluation uses only ChatGPT trained predominantly on Western norms.

**Adaptive Agent Selection Strategies**: Can dynamic agent selection strategies optimize the false positive/false negative trade-off across varying attack types? The optimal agent count varies by attack type (3-agent for BetterDan, 2-agent for JB, 1-agent for Original Attack), and a fixed configuration cannot be optimal for real-world systems handling diverse inputs. Developing a meta-classifier that predicts the optimal agent configuration per input could improve performance.

**Human-in-the-Loop Ground Truth Verification**: Can human annotators with consensus voting produce more reliable ground truth labels than LLM-based evaluators? The AttackEval LLM evaluator misclassified clearly unsafe content (e.g., listing pornography websites) as benign and flagged harmless content (general descriptions of intimacy) as harmful. Human-annotated benchmarks with multiple annotators per sample could reduce evaluator errors.

## Limitations
- Missing critical implementation details including exact 390 base prompts and inference parameters (temperature, top_p, seed) that impact result determinism
- High false positive rates, particularly for JB attack template where dual-layer response format confuses multi-agent systems
- Computational cost increases substantially with more agents, though not quantified in the paper
- Evaluation framework assumes static jailbreak templates and does not address adaptive adversaries

## Confidence

**High Confidence**: Core finding that multi-agent systems reduce false negatives and attack success rates compared to single-agent defenses is well-supported by reported metrics (e.g., 3-agent ASR of 10.51% for BetterDan vs higher rates for 1-agent).

**Medium Confidence**: Relative performance differences between attack types appear robust, but high FPR for JB attacks suggests systematic biases when handling layered response formats. Trade-off between accuracy and computational cost is noted but not quantified.

**Low Confidence**: Exact numerical results difficult to verify without base prompts and inference parameters. Ground truth generation using GPT-4.1 introduces potential subjectivity not fully addressed.

## Next Checks

1. **Prompt Source Verification**: Test the defense system using a publicly available harmful prompt dataset (such as AdvBench) with the three jailbreak templates to verify whether core findings about reduced ASR hold across different prompt sources.

2. **Parameter Sensitivity Analysis**: Reproduce the evaluation with varying temperature and top_p values for both target and agent responses to assess impact on result stability and false positive rates.

3. **Response Format Robustness Test**: Create controlled test cases with layered responses (refusal prefix followed by harmful content) to validate whether the 3-agent system consistently misclassifies JB-style attacks as the paper suggests.