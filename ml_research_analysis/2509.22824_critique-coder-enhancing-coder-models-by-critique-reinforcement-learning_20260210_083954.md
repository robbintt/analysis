---
ver: rpa2
title: 'Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning'
arxiv_id: '2509.22824'
source_url: https://arxiv.org/abs/2509.22824
tags:
- arxiv
- reasoning
- data
- critique
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Critique Reinforcement Learning (CRL) is introduced to address
  the lack of critique and reflection mechanisms in standard RL-based code generation.
  CRL trains models to generate critiques for (question, solution) pairs, with rewards
  based on the correctness of the critique's judgment.
---

# Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.22824
- Source URL: https://arxiv.org/abs/2509.22824
- Reference count: 6
- Key outcome: Critique-Coder 4B model achieves 59.0 accuracy on LiveCodeBench v5 (+4.8 over baseline), outperforming DeepCoder-14B and GPT-o1

## Executive Summary
Critique-Coder introduces Critique Reinforcement Learning (CRL) to address the lack of critique and reflection mechanisms in standard RL-based code generation. The method trains models to judge whether given solutions are correct for programming questions, with binary rewards based on critique accuracy. By integrating 20% CRL data with standard RL training, Critique-Coder achieves significant improvements across coding and reasoning benchmarks, demonstrating that critique-based training enhances both generation quality and general reasoning ability.

## Method Summary
Critique-Coder uses a hybrid training approach combining standard RL with Critique Reinforcement Learning. CRL tasks the model with judging (True/False) whether a solution is correct for a given question. The training uses GRPO algorithm with asymmetric clipping (0.2, 0.3) on a filtered rStar-Coder dataset. The hybrid approach uses 20% CRL data (generated by prompting QWEN3-CODER-30B-A3B-INSTRUCT for candidate solutions) and 80% RL data. Training runs in two phases: 16k → 32k context length once rewards stabilize, with CRL reward scaled by 0.8 in the initial phase. The model is trained for one epoch and selected based on LiveCodeBench v5 validation performance.

## Key Results
- Critique-Coder 4B achieves 59.0 accuracy on LiveCodeBench v5 (+4.8 over baseline)
- Outperforms larger models including DeepCoder-14B and GPT-o1
- Demonstrates strong transferable reasoning ability on BBEH logic tasks (+6.1 points over base model)
- Confirms CRL enhances both coding performance and general reasoning

## Why This Works (Mechanism)
CRL works by training models to evaluate solution correctness, creating a reflection mechanism that improves both generation and judgment capabilities. The binary reward signal for critique accuracy provides clearer feedback than pass-rate signals alone, helping the model develop better reasoning about code correctness. The hybrid approach ensures the model maintains strong generation skills while developing critical evaluation abilities.

## Foundational Learning
- **Reinforcement Learning with GRPO**: Proximal Policy Optimization variant for training code generation models; needed for stable policy updates; quick check: verify reward scaling implementation
- **Binary vs Continuous Rewards**: Binary critique rewards provide sharper learning signals than pass-rate rewards; needed for effective CRL; quick check: compare training curves with different reward types
- **Context Length Scaling**: Two-phase training (16k → 32k) to balance efficiency and performance; needed for managing computational resources; quick check: monitor validation performance during length transition
- **Hybrid Data Mixing**: 20% CRL / 80% RL ratio balances generation and critique training; needed to prevent performance degradation; quick check: ablate CRL ratio percentages
- **Solution Execution and Validation**: Test case execution infrastructure to generate ground truth labels; needed for CRL data creation; quick check: verify execution accuracy on sampled solutions
- **Model Self-Critique Limitations**: Models can critique others' solutions but struggle with self-critique; needed for understanding current method boundaries; quick check: test self-critique performance on generated candidates

## Architecture Onboarding

**Component Map**: Question/Solution Pairs -> Critique Module -> Binary Reward -> GRPO Update -> Improved Model

**Critical Path**: Input question → generate solution → execute test cases → compute pass rate → compare to threshold → generate True/False judgment → binary reward → policy gradient update

**Design Tradeoffs**: Binary critique rewards provide clearer signals but may oversimplify correctness assessment compared to continuous pass-rate rewards. The 20% CRL ratio balances new capabilities with maintaining generation performance, though the optimal ratio may vary by task.

**Failure Signatures**: Over-reliance on CRL causes performance degradation (ablation shows 100% CRL performs worse than hybrid). Poor context length timing causes unstable training. Incorrect test case execution leads to wrong ground truth labels and corrupts CRL training.

**Three First Experiments**:
1. Verify binary reward computation by testing with known correct/incorrect solutions
2. Check hybrid data sampling ensures proper 20/80 CRL/RL ratio distribution
3. Monitor reward curves to identify optimal timing for context length transition

## Open Questions the Paper Calls Out
- Why does the Critique-Coder model fail to utilize its learned critique abilities for effective self-critique during parallel test-time scaling? Despite CRL training, the model "lacks genuine self-critical ability" and attempts to score solutions based on self-generated critiques "did not yield performance improvements."
- Can the performance drop observed when training exclusively on CRL data be mitigated? The ablation study shows that using 100% CRL data causes performance degradation compared to hybrid training, which the authors attribute to a "mismatch between training outputs and inference behavior."
- Does the cross-domain transfer of reasoning abilities function bidirectionally? The paper demonstrates transfer from code CRL to logic tasks (BBEH) but does not investigate if training on logic-only CRL datasets would yield similar improvements in code generation.

## Limitations
- Unclear timing criterion for context length switch ("once rewards stabilize") creates reproducibility challenges
- Limited investigation of optimal CRL ratio beyond the 20% used in experiments
- Self-critique limitations prevent effective use of learned abilities during inference

## Confidence
- **High Confidence**: Core methodology and GRPO hyperparameters are well-documented
- **Medium Confidence**: Transferability claims require independent validation across different reasoning tasks
- **Low Confidence**: Exact implementation details of hybrid data sampling and reward scaling may have been optimized for specific conditions

## Next Checks
1. Run ablation studies varying when the context length switch occurs (based on reward plateau vs. fixed steps) to determine sensitivity and quantify impact on final performance
2. Train models with varying proportions of CRL data (10%, 20%, 30%, 40%) while keeping other hyperparameters constant to identify optimal ratio
3. Evaluate Critique-Coder on a held-out set of coding problems from different distribution than rStar-Coder to verify general reasoning improvements