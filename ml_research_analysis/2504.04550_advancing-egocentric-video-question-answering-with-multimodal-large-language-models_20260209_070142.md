---
ver: rpa2
title: Advancing Egocentric Video Question Answering with Multimodal Large Language
  Models
arxiv_id: '2504.04550'
source_url: https://arxiv.org/abs/2504.04550
tags:
- video
- answer
- question
- dataset
- egocentric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates four Multimodal Large Language Models (GPT-4o,
  Gemini-1.5-Pro, Video-LLaVa-7B, and Qwen2-VL-7B-Instruct) on a refined egocentric
  video QA dataset, QaEgo4Dv2. The authors systematically compare zero-shot and fine-tuned
  approaches, analyzing the impact of frame sampling, supervised fine-tuning, contrastive
  learning, curriculum learning, and multi-task training.
---

# Advancing Egocentric Video Question Answering with Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2504.04550
- Source URL: https://arxiv.org/abs/2504.04550
- Authors: Alkesh Patel; Vibhav Chitalia; Yinfei Yang
- Reference count: 40
- Primary result: Fine-tuned Video-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art performance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (for OpenQA) and +13% accuracy (for CloseQA)

## Executive Summary
This paper systematically evaluates four Multimodal Large Language Models (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B, and Qwen2-VL-7B-Instruct) on a refined egocentric video QA dataset, QaEgo4Dv2. The authors compare zero-shot and fine-tuned approaches while analyzing the impact of frame sampling, supervised fine-tuning, contrastive learning, curriculum learning, and multi-task training. Their fine-tuned models establish new state-of-the-art results, demonstrating significant improvements over previous benchmarks. The study also identifies persistent challenges in spatial reasoning and fine-grained object recognition, providing clear directions for future research.

## Method Summary
The study evaluates four MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B, and Qwen2-VL-7B-Instruct) on a refined egocentric video QA dataset (QaEgo4Dv2) containing 100 videos and 2,095 QA pairs. The authors employ zero-shot and fine-tuned approaches, with fine-tuning conducted on cleaned annotations. They systematically analyze the effects of frame sampling (8-32 frames), supervised fine-tuning, contrastive learning, curriculum learning, and multi-task training. Performance is measured using accuracy for CloseQA and ROUGE/METEOR/LLM-as-judge for OpenQA.

## Key Results
- Fine-tuned Video-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art performance, surpassing previous benchmarks by up to +2.6% ROUGE/METEOR (OpenQA) and +13% accuracy (CloseQA)
- Increasing frame count from 8 to 24 generally boosts performance by 4-14%, with diminishing returns beyond 24 frames
- Removing annotation errors (1.2-3.2% of data) yields measurable performance gains, with fine-tuned models improving by 0.6-2.8% accuracy
- Spatial localization errors account for 57% of failures, followed by object recognition (21%), color (10%), and counting (5%) errors

## Why This Works (Mechanism)

### Mechanism 1: Supervised Fine-Tuning for Domain Alignment
- Pre-trained MLLMs encode general video-language knowledge; supervised fine-tuning on (video, question → answer) triples aligns output distributions with egocentric task requirements
- Core assumption: The base model has sufficient visual-linguistic grounding that can be redirected through gradient updates on task-specific data
- Evidence: Table 2 shows Video-LLaVa SFT improves GPT accuracy from 22.96% to 32.11%; Qwen2-VL from 29.98% to 31.40%

### Mechanism 2: Frame Sampling Density Affects Temporal Reasoning
- More frames provide denser temporal coverage, increasing probability of capturing relevant moments in long videos (8+ minutes average)
- Core assumption: Relevant information is approximately uniformly distributed across the video
- Evidence: Table 5 shows Qwen2-VL improves from 54.40% CloseQA accuracy (8 frames) to 55.04% (32 frames)

### Mechanism 3: Annotation Noise Creates Performance Ceiling
- Noisy labels create conflicting gradient signals during fine-tuning; models waste capacity learning to predict incorrect answers
- Core assumption: The remaining annotations are largely correct; noise is not systematic
- Evidence: Section 3.1 documents fixed template errors affecting 1.2-4.3% of instances; Table 6 shows Video-LLaVa trained on cleaned data improves from 30.55% to 31.68% GPT accuracy

## Foundational Learning

- **Egocentric Video Characteristics**: First-person videos exhibit unique challenges—frequent camera motion, variable viewpoints, and long temporal horizons (8+ minutes)—that differ from third-person video QA benchmarks
  - Why needed: Standard video QA models trained on ActivityNet-style videos might fail on head-mounted camera footage
  - Quick check: Can you explain why standard video QA models trained on ActivityNet-style videos might fail on head-mounted camera footage?

- **MLLM Architecture Components**: Understanding the vision encoder, projection layer, and LLM backbone is necessary to reason about where fine-tuning updates occur
  - Why needed: Understanding how Video-LLaVa differs from Qwen2-VL in processing multiple video frames
  - Quick check: Where does Video-LLaVa differ from Qwen2-VL in how it processes multiple video frames?

- **Evaluation Metrics for Generative QA**: OpenQA requires semantic similarity metrics (ROUGE, METEOR, LLM-as-judge) rather than exact match
  - Why needed: Understanding these tradeoffs is critical for interpreting results
  - Quick check: Why might ROUGE/METEOR underperform compared to GPT-based evaluation for this task?

## Architecture Onboarding

- **Component map**: Video input → frame sampling (8-32 evenly spaced) → vision encoder → projection → LLM → text output
- **Critical path**: Frame sampling determines temporal coverage; vision encoder extracts visual features; projection aligns to LLM space; LLM generates answers
- **Design tradeoffs**:
  - Frame count vs. compute: 32 frames improves accuracy but increases memory 4× over 8 frames
  - Resolution vs. performance: Appendix D shows 512×512 underperforms 336×336 for GPT-4o (54.36 vs 55.06 Sim)
  - OpenQA vs. CloseQA training: Multi-task training did not outperform task-specific SFT (Table 4)
- **Failure signatures**:
  - Spatial localization errors (57%): Predicts generic locations instead of specific ones
  - Object recognition errors (21%): Confusing visually similar items (cabbage vs. spring onions)
  - Color/counting errors (15% combined): Blur and motion degrade fine-grained perception
- **First 3 experiments**:
  1. Baseline zero-shot evaluation: Run GPT-4o/Gemini on 100 samples from QaEgo4Dv2 test set using provided prompts; measure accuracy to confirm reported numbers
  2. Frame ablation: Fine-tune Video-LLaVa on 8, 16, and 24 frames; plot CloseQA accuracy vs. frame count to verify diminishing returns pattern
  3. Error taxonomy validation: Sample 50 incorrect predictions from fine-tuned Qwen2-VL; manually classify into spatial/object/color/counting/other to validate error distribution claims

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can incorporating explicit 3D spatial maps or region-based attention mechanisms reduce the high error rate in spatial localization within egocentric video QA?
- **Basis**: Section 5.8 identifies spatial localization as the most frequent error type (57%), and Section 6 explicitly suggests "Advanced Spatial Reasoning" via 3D maps as a future direction
- **Why unresolved**: Current models rely on generic visual features, leading to predictions like "on the floor" instead of precise locations like "at the top of cabinet"
- **Evidence**: Improved accuracy on "Where" questions in QaEgo4Dv2 compared to the fine-tuned Qwen2-VL baseline

### Open Question 2
- **Question**: Does a dynamic, data-driven curriculum learning strategy yield significant performance gains over standard supervised fine-tuning?
- **Basis**: Section 6 states the static curriculum approach used in experiments did not yield significant improvements, suggesting a need for "more adaptive scheduling or rigorous difficulty metrics"
- **Why unresolved**: The hypothesis that gradual exposure to difficulty helps remains unproven because the static method of estimating difficulty was insufficient
- **Evidence**: Higher accuracy scores on QaEgo4Dv2 using an adaptive scheduler compared to the static curriculum results reported in Table 4

### Open Question 3
- **Question**: Can integrating auxiliary tasks like moment localization or action forecasting improve MLLM generalization where multi-task training with OpenQA and CloseQA failed?
- **Basis**: Section 5.3 shows standard multi-task training did not outperform SFT, yet Section 6 proposes "integrating moment localization, dense captioning, or action forecasting" to unlock richer understanding
- **Why unresolved**: It is unclear if the failure of multi-task learning was due to the specific pairing of OpenQA/CloseQA or a fundamental limitation of the multi-task paradigm in this context
- **Evidence**: Performance comparison of a model jointly trained on QA and moment localization versus the single-task SFT baseline

## Limitations

- The study relies on a refined but still imperfect dataset with residual annotation ambiguities, particularly around spatial ground-truth intervals and answer acceptability
- All reported improvements come from models fine-tuned on the same dataset used for evaluation, raising potential overfitting concerns
- The analysis of error patterns is based on a relatively small sample (200 incorrect predictions), which may not capture the full distribution of failure modes

## Confidence

- **High Confidence**: The performance improvements from supervised fine-tuning (+2.6% OpenQA, +13% CloseQA) are well-supported by systematic ablation studies across multiple frame counts and annotation conditions
- **Medium Confidence**: The error taxonomy analysis showing 57% spatial, 21% object, and 15% color/counting errors is plausible but based on limited manual inspection
- **Low Confidence**: Claims about diminishing returns beyond 24 frames and the relative ineffectiveness of contrastive/multi-task learning are less thoroughly validated

## Next Checks

1. **Cross-Domain Generalization**: Evaluate fine-tuned models on EgoCross or other cross-domain egocentric benchmarks to assess whether performance gains transfer beyond the QaEgo4Dv2 distribution, particularly for spatial reasoning and object recognition errors

2. **Annotation Noise Quantification**: Implement a blinded annotation study where 3-5 independent annotators relabel a random 200-sample subset of the test set to empirically measure residual annotation error rates

3. **Alternative Frame Sampling Evaluation**: Replace uniform frame sampling with an adaptive strategy that prioritizes visually distinct or temporally important frames (e.g., using motion detection or semantic change detection) and compare QA accuracy against the best-performing uniform sampling configuration (24-32 frames)