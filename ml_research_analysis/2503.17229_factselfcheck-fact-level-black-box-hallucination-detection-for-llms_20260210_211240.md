---
ver: rpa2
title: 'FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs'
arxiv_id: '2503.17229'
source_url: https://arxiv.org/abs/2503.17229
tags:
- hallucination
- detection
- fact-level
- methods
- sentence-level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FactSelfCheck introduces a novel zero-resource black-box sampling-based
  method for fact-level hallucination detection in LLMs, operating at the granularity
  of knowledge graph triples rather than sentences. The method leverages multiple
  LLM generations to assess factual consistency without requiring external knowledge
  bases or training data.
---

# FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs

## Quick Facts
- arXiv ID: 2503.17229
- Source URL: https://arxiv.org/abs/2503.17229
- Authors: Albert Sawczyn; Jakub Binkowski; Denis Janiak; Bogdan Gabrys; Tomasz Kajdanowicz
- Reference count: 25
- Key outcome: FactSelfCheck introduces a novel zero-resource black-box sampling-based method for fact-level hallucination detection in LLMs, operating at the granularity of knowledge graph triples rather than sentences.

## Executive Summary
FactSelfCheck presents a novel approach to detecting hallucinations in LLM-generated text by operating at the fact-level rather than sentence-level. The method uses multiple stochastic generations of the same input to identify factual inconsistencies, leveraging the observation that true facts appear consistently across samples while hallucinations vary. By representing text as knowledge graphs of triples and analyzing consistency across samples, FactSelfCheck achieves competitive performance with sentence-level methods while providing more granular, interpretable insights into hallucination locations.

## Method Summary
FactSelfCheck employs a three-step pipeline for fact-level hallucination detection. First, it extracts entities and relations from the input passage using an LLM, then builds a knowledge graph by extracting facts from each sentence. Second, it generates N stochastic samples of the input at non-zero temperature and extracts facts from each sample using the same entity/relation schema. Third, it computes hallucination scores for each fact based on its consistency across samples, using either frequency-based scoring or LLM-based comparison. The method can aggregate fact-level scores to sentence-level using max or mean functions. The approach requires only black-box LLM access without external knowledge bases or training data.

## Key Results
- Fact-level detection achieved 93.41 AUC-PR on WikiBio dataset
- FactSelfCheck enabled 35.5% increase in factual content during hallucination correction compared to baseline methods
- The method shows competitive performance with sentence-level sampling-based methods while providing more interpretable insights
- Sample size shows dramatic performance improvement up to 5 samples, with diminishing returns after

## Why This Works (Mechanism)

### Mechanism 1: Sampling-Based Consistency as a Truth Proxy
Factual information exhibits higher consistency across stochastic LLM generations than hallucinated content. By generating N samples at non-zero temperature and measuring the frequency of each fact's appearance, the method exploits the observation that hallucinations vary unpredictably while true knowledge is reinforced through model parameters. The probability of a fact being hallucinated is inversely proportional to the fraction of samples containing the same fact.

### Mechanism 2: Triple-Level Granularity Reduces Information Loss
Decomposing sentences into (head, relation, tail) triples enables more precise hallucination localization than sentence-level classification. A single sentence may contain 3-5 facts with independent truth values. Aggregating to sentence level conflates correct and incorrect facts, obscuring which specific claims need correction. This granularity enables targeted correction of individual hallucinated facts rather than wholesale rejection of entire sentences.

### Mechanism 3: Constrained Schema Improves Cross-Sample Comparability
Reusing extracted entities (ES) and relations (RS) from the original response when extracting KGs from samples reduces alignment failures. Instead of freeform extraction per sample (which introduces naming variance), the pipeline constrains sample KGs to use entities/relations already identified, making frequency counting meaningful. This eliminates the need for entity alignment and ensures that the KG is built using the same schema as the original response.

## Foundational Learning

- **Knowledge Graph Triple Representation**: Understanding that "(Robert Smith, member of, The Cure)" is an atomic, verifiable unit distinct from the sentence containing it. Quick check: Given "Paris is the capital of France and has 2.1M residents," what are two distinct triples?

- **Black-Box vs White-Box Detection**: FactSelfCheck requires only API access to generate samples, unlike AttentionScore which needs internal states. Quick check: If you only have access to GPT-4 via API, which detection methods are available to you?

- **Aggregation Functions (Max vs Mean)**: Converting fact-level scores to sentence-level requires choosing whether one hallucinated fact invalidates a sentence (max) or dilutes it (mean). Quick check: If a sentence contains three facts with scores [0.9, 0.1, 0.2], what would max vs mean aggregation produce?

## Architecture Onboarding

- **Component map**: Input Passage → Entity Extraction (LLM) → Relation Extraction (LLM) → Sentence KG Extraction (LLM per sentence) → Response KG (KGp) → N Stochastic Samples → Sample KG Extraction (constrained schema) → Fact Consistency Scoring (Frequency or LLM-based) → Hallucination Scores → Optional: Sentence Aggregation (max/mean)

- **Critical path**: Entity/relation extraction quality cascades through all downstream steps. If the initial extraction misses key entities, no subsequent correction is possible.

- **Design tradeoffs**:
  - FactSelfCheck-Text: Fewer LLM calls (no sample KG extraction), but may miss implicit relationships
  - FactSelfCheck-KG (Frequency): Cheapest scoring, but requires exact fact matches
  - FactSelfCheck-KG (LLM): Enables semantic matching, but multiplies LLM calls by |KGp| × |S|
  - Paper shows FSC-Text performs best on WikiBio (93.41 AUC-PR), FSC-KG (Frequency) best on FavaMultiSamples (48.52)

- **Failure signatures**:
  - Short sentences with no extractable facts → default score 0.5 (watch for 6%+ rates)
  - High-refusal corrections (5%) when model lacks knowledge to fix flagged facts
  - Sample size < 5: Paper shows dramatic performance improvement up to 5 samples, diminishing returns after

- **First 3 experiments**:
  1. **Baseline replication**: Run FactSelfCheck-Text on WikiBio with 20 samples, verify AUC-PR ≈ 92-93 using provided code
  2. **Sample size ablation**: Plot AUC-PR with N ∈ {1, 3, 5, 10, 20} to confirm inflection point at 5 samples (Figure 4 pattern)
  3. **Extraction failure analysis**: Measure percentage of sentences yielding zero facts on your target domain; if >5%, consider prompt optimization before deployment

## Open Questions the Paper Calls Out

### Open Question 1
Would fact-level annotated datasets reveal superior performance for FactSelfCheck compared to sentence-level methods, given that current evaluation requires score aggregation which disadvantages the method? The lack of datasets with long generated passages and annotations at the fact-level forced evaluation through aggregation rather than directly assessing fact-level detection capabilities.

### Open Question 2
What computational efficiency gains can be achieved through step merging and prompt optimization without degrading detection performance? Future work could focus on reducing prompt lengths and merging steps, such as merging the KG extraction steps or simultaneously assessing support for multiple facts.

### Open Question 3
What dataset characteristics determine whether FSC-Text or FSC-KG variants perform better, given that optimal variant selection is dataset-dependent? Results show FSC-Text performs best on WikiBio while FSC-KG (Frequency-based) outperforms on FavaMultiSamples, with authors suggesting "fact density, sentence length, and text style" as potential factors but not testing these hypotheses.

## Limitations

- The method's reliance on KG extraction quality creates a fragile dependency chain - if the initial LLM fails to identify entities or relations, hallucination detection becomes impossible for those sentences.
- The 6.08% failure rate on FavaMultiSamples (defaulting to 0.5 score) represents significant noise injection into the detection pipeline.
- The constrained schema approach assumes all samples can be expressed within the original response's vocabulary, which may not hold for novel or contextually shifted facts.

## Confidence

- **High confidence**: Fact-level granularity provides more interpretable insights than sentence-level detection (supported by AUC-PR improvements and 35.5% factual content increase during correction)
- **Medium confidence**: Sampling-based consistency effectively distinguishes hallucinations from facts (mechanism well-supported by equations and experimental results, but break conditions under systematic model bias are not extensively tested)
- **Medium confidence**: Constrained schema approach reduces alignment failures (theoretically sound and supported by improved results, but no ablation comparing freeform vs constrained extraction directly)

## Next Checks

1. **Robustness to extraction failure**: Test FactSelfCheck on a domain with high entity/relation extraction complexity (e.g., scientific text) and measure the percentage of sentences receiving default 0.5 scores to quantify noise injection.

2. **Schema completeness validation**: For a subset of samples, perform freeform KG extraction and compare the entity/relation vocabulary growth to test whether the original response schema is indeed sufficient for expressing all sample facts.

3. **Bias sensitivity analysis**: Evaluate FactSelfCheck on a dataset with known LLM biases (e.g., consistently wrong but confident outputs) to test whether sampling-based consistency breaks down under systematic hallucination patterns.