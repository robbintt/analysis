---
ver: rpa2
title: 'SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation'
arxiv_id: '2502.05539'
source_url: https://arxiv.org/abs/2502.05539
tags:
- arxiv
- learning
- lora
- tasks
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SSH introduces a parameter-efficient fine-tuning method based on
  Discrete Hartley Transform (DHT) that selects and updates the most informative spectral
  components while maintaining strong model performance. By leveraging DHT's real-valued
  and symmetric properties, SSH eliminates the computational overhead of complex arithmetic
  present in Fourier-based approaches.
---

# SSH: Sparse Spectrum Adaptation via Discrete Hartley Transformation

## Quick Facts
- **arXiv ID**: 2502.05539
- **Source URL**: https://arxiv.org/abs/2502.05539
- **Reference count**: 21
- **Primary result**: SSH achieves state-of-the-art parameter-efficient fine-tuning with 7.93 GPT-4 score on LLaMA3.1-8B using <0.1% parameters

## Executive Summary
SSH introduces a parameter-efficient fine-tuning method based on Discrete Hartley Transform (DHT) that selects and updates the most informative spectral components while maintaining strong model performance. By leveraging DHT's real-valued and symmetric properties, SSH eliminates the computational overhead of complex arithmetic present in Fourier-based approaches. The method achieves state-of-the-art results across diverse NLP tasks, image classification, and multi-modal applications while using significantly fewer parameters than existing methods like LoRA and FourierFT.

## Method Summary
SSH performs parameter-efficient fine-tuning by transforming weight matrices into the spectral domain using Discrete Hartley Transform, then selectively updating the most informative spectral components. The method exploits DHT's real-valued nature to avoid complex arithmetic overhead while maintaining the benefits of frequency-based parameter selection. During fine-tuning, SSH identifies and updates only the most critical spectral components, leaving the majority of parameters frozen. This approach combines the advantages of frequency-based methods with computational efficiency, achieving superior performance compared to traditional PEFT techniques across multiple domains and model sizes.

## Key Results
- Achieves 7.93 GPT-4 score on LLaMA3.1-8B with less than 0.1% of parameters
- Matches full fine-tuning performance (77.4% accuracy) on vision tasks
- Outperforms LoRA and FourierFT across diverse NLP, vision, and multi-modal benchmarks

## Why This Works (Mechanism)
SSH leverages the Discrete Hartley Transform to decompose weight matrices into spectral components, where each component represents a specific frequency pattern in the weight space. The method identifies that low-frequency components typically capture general patterns while high-frequency components encode task-specific information. By selectively updating only the most informative spectral components rather than all parameters, SSH achieves parameter efficiency while maintaining performance. The use of DHT instead of Fourier Transform eliminates complex number operations, reducing computational overhead while preserving the benefits of frequency-based adaptation.

## Foundational Learning

**Discrete Hartley Transform**: A real-valued transform that converts signals from time domain to frequency domain without using complex numbers. Needed to enable efficient spectral decomposition while avoiding complex arithmetic overhead. Quick check: Verify DHT is unitary and invertible for rectangular matrices.

**Parameter-Efficient Fine-Tuning**: Methods that adapt pre-trained models using a small subset of parameters to achieve task-specific performance. Needed to reduce computational costs and memory requirements for model adaptation. Quick check: Confirm parameter count reduction compared to full fine-tuning.

**Spectral Component Selection**: The process of identifying which frequency components contain the most task-relevant information. Needed to determine which parameters to update during fine-tuning. Quick check: Validate selection criteria through ablation studies.

## Architecture Onboarding

**Component Map**: Input weights -> DHT transformation -> Spectral component selection -> Parameter update -> Output weights

**Critical Path**: DHT forward pass → component ranking → selective parameter update → inverse DHT (implicit in weight application)

**Design Tradeoffs**: Uses DHT instead of Fourier Transform to eliminate complex arithmetic at the cost of potentially different spectral characteristics. Selects components based on magnitude rather than learned importance to avoid additional parameters.

**Failure Signatures**: Performance degradation when selected spectral components don't capture task-relevant information; increased computational overhead if too many components are selected; potential instability if component selection is too aggressive.

**First Experiments**:
1. Verify DHT correctly decomposes weight matrices on a small transformer block
2. Test component selection sensitivity on a simple classification task
3. Compare computational efficiency against LoRA on identical hardware

## Open Questions the Paper Calls Out

None

## Limitations

- Limited exploration of out-of-distribution scenarios and long-tail tasks
- Spectral selection mechanism relies on fixed heuristics that may not generalize optimally
- Computational efficiency claims would benefit from more extensive ablation studies across different hardware configurations

## Confidence

**High**: Core methodological contribution (DHT-based spectral decomposition), empirical results on standard benchmarks, parameter efficiency comparisons

**Medium**: Cross-domain generalization claims, spectral component selection strategy, computational overhead reductions

**Low**: Real-world deployment scenarios, robustness to distribution shifts, scalability to extremely large models (beyond 8B parameters)

## Next Checks

1. **Robustness Testing**: Evaluate SSH performance on adversarial examples and domain-shifted datasets to assess generalization beyond standard benchmarks

2. **Spectral Sensitivity Analysis**: Conduct systematic ablation studies varying the number of selected spectral components across different model scales and task types

3. **Resource Profiling**: Measure actual memory consumption and training time on diverse hardware setups (CPU, GPU, TPU) with varying batch sizes and sequence lengths