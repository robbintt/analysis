---
ver: rpa2
title: 'Information Gain-based Policy Optimization: A Simple and Effective Approach
  for Multi-Turn LLM Agents'
arxiv_id: '2510.14967'
source_url: https://arxiv.org/abs/2510.14967
tags:
- igpo
- reward
- arxiv
- answer
- tool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "IGPO introduces an information gain-based reward design that provides\
  \ dense, turn-level supervision by measuring the marginal increase in the policy\u2019\
  s probability of producing the correct answer at each turn. This addresses the challenge\
  \ of sparse outcome rewards in multi-turn agent training, which leads to advantage\
  \ collapse and poor credit assignment."
---

# Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents

## Quick Facts
- **arXiv ID**: 2510.14967
- **Source URL**: https://arxiv.org/abs/2510.14967
- **Reference count**: 38
- **Key outcome**: IGPO achieves 58.7 average F1 score across 7 datasets, outperforming baselines especially for smaller models (+15.3 points on 3B).

## Executive Summary
IGPO introduces a novel information gain-based reward design that provides dense, turn-level supervision for training multi-turn LLM agents. By measuring the marginal increase in policy probability of producing the correct answer at each turn, IGPO addresses the challenge of sparse outcome rewards that lead to advantage collapse in group-relative policy optimization. The method integrates turn-level information gain rewards with outcome rewards to form a dense trajectory, enabling stable optimization via a GRPO-style objective with discounted cumulative advantages. Experiments demonstrate consistent improvements across in-domain and out-of-domain benchmarks, with particularly large gains for smaller models and improved token efficiency.

## Method Summary
IGPO computes turn-level rewards as the marginal increase in the policy's probability of generating the ground-truth answer at each turn, using teacher forcing for stable probability estimation. These information gain rewards are combined with outcome rewards at the final turn, then normalized across all turns in a group. The method applies discounted cumulative advantage estimation to propagate future signals backward to earlier turns, addressing long-horizon credit assignment challenges. The policy is optimized using a GRPO-style clipped surrogate objective with KL regularization, using these turn-level advantages instead of sparse outcome rewards alone.

## Key Results
- IGPO achieves 58.7 average F1 score across 7 datasets (NQ, TQ, HotpotQA, 2Wiki, MusiQue, Bamboogle, PopQA)
- Particularly strong performance on smaller models (+15.3 F1 points on 3B vs baselines)
- Improves token efficiency and convergence speed compared to outcome-only methods
- Ablation shows combined IG+F1 rewards outperform either reward type alone (52.8 vs 51.9 vs 58.7)

## Why This Works (Mechanism)

### Mechanism 1
Turn-level information gain rewards provide dense supervision that mitigates advantage collapse in multi-turn agent training. At each turn t, IGPO computes the policy's probability of generating the ground-truth answer under teacher forcing, with the turn-level reward being the marginal increase. This creates non-zero signals for every sample, even when no rollout produces the correct final answer. The core assumption is that ground-truth answer probability is a meaningful proxy for "information acquisition toward correct reasoning."

### Mechanism 2
Combining information gain rewards with outcome rewards provides both fine-grained credit assignment and task alignment. IGPO constructs a length-T reward vector where t<T uses information gain and t=T uses outcome F1 score. This hybrid approach prevents reward collapse while maintaining goal-directed behavior. The core assumption is that information gain rewards alone may not fully align with final task objectives, so outcome supervision remains necessary.

### Mechanism 3
Discounted cumulative advantage estimation captures long-horizon dependencies by propagating future signals backward to earlier turns. After z-normalizing turn-level rewards, IGPO computes cumulative advantages that assign future-aware credit to each turn's decision tokens. The core assumption is that discounted accumulation appropriately balances immediate vs. future rewards in multi-turn reasoning.

## Foundational Learning

- **Advantage collapse in GRPO**: Understanding why GRPO fails with sparse rewards—when all rollouts receive identical outcome rewards, normalized advantages approach zero, yielding no learning signal. Quick check: Given 16 rollouts where 15 produce incorrect answers and 1 produces correct answer, does advantage collapse occur? (No, because rewards differ. Collapse occurs when all rewards are identical.)

- **Teacher forcing for reward computation**: IGPO computes ground-truth answer probability using teacher forcing (conditioning on ground-truth answer tokens), not autoregressive sampling. This is critical for stable gradient signals. Quick check: Why does teacher forcing matter for information gain computation? (It provides a consistent, differentiable estimate of "how likely is the correct answer given current context" without sampling variance.)

- **Stop-gradient on reward terms**: Information gain rewards receive stop-gradient treatment to prevent the reward model from being optimized directly. Quick check: What happens if you forget stop-gradient on r_{i,t}? (Gradients would flow through the reward computation into the probability estimator, potentially destabilizing training by creating a moving target.)

## Architecture Onboarding

- **Component map**: Rollout Generator -> Information Gain Calculator -> Reward Aggregator -> Advantage Computer -> Policy Optimizer

- **Critical path**: 1. Sample rollouts → 2. Compute per-turn ground-truth probabilities (requires separate forward passes per turn) → 3. Calculate IG rewards as differences → 4. Combine with outcome reward → 5. Z-normalize → 6. Compute cumulative advantages → 7. Update policy

- **Design tradeoffs**: Computational overhead requires T forward passes per rollout vs 1 for standard GRPO. Ground-truth dependency limits applicability to open-ended tasks. Discount factor sensitivity requires careful tuning.

- **Failure signatures**: Zero information gain across all turns may indicate ground-truth probability saturation. Negative cumulative IG rewards indicate policy moving away from ground-truth. Advantage still collapsing suggests group size issues.

- **First 3 experiments**: 1. Apply IGPO to single-turn QA dataset to verify it reduces to outcome reward. 2. Test discount factor γ ∈ {0.5, 0.8, 0.9, 1.0} on validation set. 3. Replicate advantage collapse analysis on target model/dataset.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can IGPO be adapted for open-ended tasks without ground-truth answers? The current mechanism relies on exact ground-truth strings, limiting applicability to open-ended generation.

- **Open Question 2**: Does teacher forcing during probability estimation introduce exposure bias or limit exploration of alternative reasoning paths? The method may penalize valid reasoning trajectories that diverge from specific ground-truth phrasing.

- **Open Question 3**: How effective is IGPO for non-search agentic tools like code execution or API manipulation? Search tools provide textual context, but non-linguistic tool outputs may not update answer probability as directly.

## Limitations
- **Ground-truth dependency**: Requires access to ground-truth answers for every training sample, limiting applicability to open-ended tasks
- **Computational overhead**: T forward passes per rollout for information gain computation vs single-pass outcome rewards
- **Discount factor sensitivity**: Uses γ=1 without tuning or sensitivity analysis across different task structures

## Confidence
- **High confidence**: Empirical improvements over baselines are well-documented across multiple datasets and model sizes
- **Medium confidence**: Theoretical mechanism connecting information gain to credit assignment is sound but lacks extensive validation against alternatives
- **Medium confidence**: Advantage collapse mitigation is supported but conditions could be more precisely characterized

## Next Checks
1. **Ablation on discount factor γ**: Test IGPO with γ ∈ {0.5, 0.8, 0.9, 1.0} across different trajectory lengths to verify γ=1 choice is robust
2. **Cross-dataset collapse analysis**: Replicate advantage collapse analysis on target model/dataset to determine if collapse rates are already low (<15%) or high (>40%)
3. **Single-turn sanity check**: Apply IGPO to single-turn QA dataset to verify it reduces to outcome reward and reveals implementation correctness