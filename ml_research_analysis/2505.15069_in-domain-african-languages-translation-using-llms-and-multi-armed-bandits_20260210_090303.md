---
ver: rpa2
title: In-Domain African Languages Translation Using LLMs and Multi-armed Bandits
arxiv_id: '2505.15069'
source_url: https://arxiv.org/abs/2505.15069
tags:
- translation
- machine
- data
- domain
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of domain adaptation in low-resource
  Neural Machine Translation (NMT) by proposing a bandit-based model selection approach.
  It uses contextual bandit algorithms (UCB, LinUCB, Neural Linear Bandit, and Thompson
  Sampling) to dynamically select the best NMT model for specific domains using limited
  in-domain data.
---

# In-Domain African Languages Translation Using LLMs and Multi-armed Bandits

## Quick Facts
- arXiv ID: 2505.15069
- Source URL: https://arxiv.org/abs/2505.15069
- Reference count: 18
- Primary result: Bandit-based model selection achieves competitive or superior performance to best individual NMT models for African language translation across domains

## Executive Summary
This paper addresses domain adaptation challenges in low-resource Neural Machine Translation by proposing a bandit-based model selection approach. The method dynamically selects the best NMT model for specific domains using contextual bandit algorithms (UCB, LinUCB, Neural Linear Bandit, and Thompson Sampling) with limited in-domain data. Experiments on three African languages (Yoruba, Swahili, Igbo) across News, Movies, and Religious domains demonstrate that bandit-based methods achieve an average 2.68% BLEU score improvement over the best individual models, with particular effectiveness in target-free scenarios using CometKiwi for reference-less quality estimation.

## Method Summary
The approach uses a pool of five frozen LLMs (Aya101, NLLB, Madlad, Gemma2, Llama3.1) as arms in a contextual bandit framework. Source sentences are encoded via LaBSE into language-agnostic embeddings, which serve as context vectors for contextual bandits. The system selects models online based on bandit policy, translates source sentences, and receives rewards computed from BLEU and COMET metrics (when references available) or CometKiwi (reference-free). The method operates without model fine-tuning, making it practical for low-resource scenarios where adaptation is infeasible.

## Key Results
- UCB bandit achieves 2.68% average BLEU score improvement over best individual model
- Bandit methods show robust performance in both reference-available and reference-free scenarios
- Contextual bandits (LinUCB, Neural Linear) leverage sentence embeddings for domain-aware selection
- Thompson Sampling and Neural Linear Bandit demonstrate competitive performance across domains

## Why This Works (Mechanism)

### Mechanism 1
- Multi-armed bandit algorithms can identify near-optimal NMT models with significantly fewer samples than exhaustive evaluation requires by formalizing exploration-exploitation tradeoff and maintaining uncertainty estimates per model.
- Core assumption: Reward distributions per arm are stationary and bounded, enabling convergence guarantees to hold.
- Evidence anchors: [abstract] "demonstrating its robustness and effectiveness in both scenarios where target data is available and where it is absent"; [section 1] "by leveraging these techniques, it is possible to make more informed decisions about model selection, even when In-domain data availability is limited".
- Break condition: Non-stationary domains where optimal model shifts mid-deployment, or reward signals that are poorly calibrated to human judgment.

### Mechanism 2
- Contextual features (LaBSE embeddings) enable bandit algorithms to generalize model selection across semantically similar inputs by treating embeddings as context vectors and learning linear or neural mappings from context to expected reward per arm.
- Core assumption: Reward is approximately linear in the embedding space (for LinUCB) or can be learned via shallow neural projection (Neural Linear).
- Evidence anchors: [section 3] "This vector x acts as the context vector in the contextual bandit algorithms"; [section 3] "LinUCB extends UCB to contextual bandits, assuming that rewards follow a linear function of the context/feature vector".
- Break condition: Embeddings that fail to capture domain-relevant distinctions, or contexts where no single model dominates consistently.

### Mechanism 3
- Reference-less quality estimation (CometKiwi) can substitute for BLEU/COMET as a reward signal when target translations are unavailable by providing scalar quality estimates that guide selection even without references.
- Core assumption: CometKiwi scores correlate sufficiently with human judgment to guide selection; noise is zero-mean or bounded.
- Evidence anchors: [section 3] "when only the source sentence is present... we use a reference-less MT metric like CometKiwi"; [section 6] "demonstrating its effectiveness both in the presence and absence of target domain data".
- Break condition: Quality estimator with systematic bias toward certain model outputs, or domains where QE fails (e.g., creative or culturally-bound text).

## Foundational Learning

- Concept: Multi-armed bandits (stochastic, contextual)
  - Why needed here: Core algorithmic framework; must understand regret, UCB confidence bounds, Thompson Sampling posteriors, and contextual extensions to implement correctly.
  - Quick check question: Can you derive the UCB selection rule and explain why α√(log t / N_a(t)) encourages exploration?

- Concept: LaBSE (Language-agnostic BERT Sentence Embeddings)
  - Why needed here: Provides the context vector for contextual bandits; understanding its multilingual properties helps diagnose when embeddings may fail.
  - Quick check question: What does "language-agnostic" mean for a sentence embedding, and how might this fail for truly low-resource languages?

- Concept: MT evaluation metrics (BLEU, COMET, CometKiwi)
  - Why needed here: Reward design is critical; BLEU captures n-gram overlap, COMET captures semantic similarity, CometKiwi is reference-less QE.
  - Quick check question: Why might BLEU be a poor reward signal for morphologically rich African languages?

## Architecture Onboarding

- Component map:
  - Source sentence -> LaBSE encoder -> d-dimensional embedding
  - Embedding + Bandit policy -> Model selection
  - Selected model -> Translation -> Hypothesis
  - Hypothesis + Reference/CometKiwi -> Reward computation
  - Reward -> Bandit policy update

- Critical path:
  1. Source sentence → LaBSE embedding
  2. Bandit policy receives context, selects model
  3. Model produces translation
  4. Reward computed (may require reference)
  5. Policy updates internal state; repeat

- Design tradeoffs:
  - UCB: Simple, no context; best when one model globally dominates
  - LinUCB / Neural Linear: Leverages context; more data-efficient but requires embedding quality
  - Thompson Sampling: Bayesian, naturally stochastic; good for non-stationary regimes (not tested here)
  - λ in reward: Controls BLEU vs COMET tradeoff; λ=0.4 worked best but may need tuning per language/domain

- Failure signatures:
  - Bandit collapses to single arm early (exploration insufficient; increase α or prior variance)
  - Reward variance too high (normalize metrics, increase exploration budget)
  - Contextual methods underperform UCB (embeddings not discriminative; try alternative encoders)
  - Target-free reward diverges from BLEU (CometKiwi miscalibrated; audit correlation on held-out data)

- First 3 experiments:
  1. Baseline calibration: Run all 5 models on 1K held-out samples per language/domain; compute BLEU variance to confirm high variance motivation. Identify "oracle best" per slice.
  2. UCB sanity check: Implement UCB with synthetic rewards (e.g., known means + Gaussian noise). Verify regret bounds hold before plugging in real MT rewards.
  3. Ablation on reward type: Compare λ∈{0.0, 0.4, 1.0} and BLEU-only vs COMET-only vs CometKiwi. Measure both final BLEU and regret convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bandit-based model selection approach scale to a broader range of low-resource African and non-African languages beyond the three evaluated?
- Basis in paper: [inferred] The evaluation is limited to only three African languages (Igbo, Yoruba, Swahili) and three domains, despite the authors noting that "domain adaptation techniques remain underexplored and challenging specifically for low-resource languages."
- Why unresolved: No systematic evaluation across diverse language families or morphological typologies is provided to establish generalizability.
- What evidence would resolve it: Comprehensive evaluation across additional African language families and other low-resource languages, demonstrating consistent performance improvements.

### Open Question 2
- Question: Can bandit-based selection be effectively combined with fine-tuning or other domain adaptation techniques for further performance gains?
- Basis in paper: [inferred] The paper positions bandit-based selection as an alternative to fine-tuning "where fine-tuning is not feasible or practical," but does not explore whether the approaches could be complementary rather than mutually exclusive.
- Why unresolved: The hybrid paradigm of bandit selection plus lightweight adaptation remains unexplored.
- What evidence would resolve it: Experiments combining bandit-based model selection with techniques like adapter-based fine-tuning, measuring whether hybrid approaches yield additive improvements.

### Open Question 3
- Question: How sensitive are the bandit algorithm performance and hyper-parameter choices (λ, α) to different domains and language pairs?
- Basis in paper: [inferred] The authors state hyper-parameters were selected "based on hit and trail method, where initial few sentences of validations were used for convergence," without providing systematic sensitivity analysis.
- Why unresolved: No ablation or sensitivity analysis is conducted to determine whether optimal hyper-parameters transfer across domains or require per-domain tuning.
- What evidence would resolve it: Sensitivity analysis across hyper-parameter ranges for all language-domain combinations, establishing robustness or transferability of settings.

### Open Question 4
- Question: Would alternative context representations beyond LaBSE embeddings improve contextual bandit performance for domain-specific model selection?
- Basis in paper: [inferred] The methodology relies exclusively on LaBSE embeddings as context vectors for LinUCB and Neural LinUCB, without exploring whether domain-aware or multilingual-specific embeddings might better capture selection-relevant signals.
- Why unresolved: No comparison of embedding choices is provided.
- What evidence would resolve it: Ablation studies comparing LaBSE against domain-specialized encoders or combined semantic-domain features for context representation.

## Limitations
- Evaluation limited to three African languages and three specific domains, constraining generalizability claims
- Performance depends on quality of external embeddings (LaBSE) and quality estimators (CometKiwi), which may not be robust across all language pairs
- Does not address computational costs or scalability challenges for larger model pools

## Confidence
- **High confidence** in theoretical soundness of bandit algorithms for model selection, given established convergence guarantees and clear motivation from high BLEU variance
- **Medium confidence** in practical effectiveness of contextual bandits, as results show improvements but are limited to three languages and specific domains
- **Low confidence** in scalability and generalization to other language pairs or more diverse domains, due to narrow experimental scope

## Next Checks
1. Validate the approach on additional African language pairs and domains not covered in the current study to assess robustness and scalability
2. Experiment with alternative encoders (e.g., multilingual BERT, XLM-R) to evaluate the impact of embedding quality on contextual bandit performance
3. Conduct systematic analysis of CometKiwi's correlation with human judgment across different domains and languages to ensure reliable reward signals