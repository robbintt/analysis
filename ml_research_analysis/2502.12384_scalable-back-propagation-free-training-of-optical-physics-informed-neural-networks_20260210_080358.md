---
ver: rpa2
title: Scalable Back-Propagation-Free Training of Optical Physics-Informed Neural
  Networks
arxiv_id: '2502.12384'
source_url: https://arxiv.org/abs/2502.12384
tags:
- training
- photonic
- neural
- error
- optical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of training physics-informed
  neural networks (PINNs) in real-time on energy-constrained edge devices by leveraging
  photonic computing accelerators. The authors propose a completely back-propagation-free
  (BP-free) framework to enable scalable training of real-size PINNs on integrated
  photonic platforms.
---

# Scalable Back-Propagation-Free Training of Optical Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2502.12384
- Source URL: https://arxiv.org/abs/2502.12384
- Reference count: 40
- Authors propose a back-propagation-free framework for training physics-informed neural networks on photonic accelerators

## Executive Summary
This work addresses the challenge of training physics-informed neural networks (PINNs) in real-time on energy-constrained edge devices by leveraging photonic computing accelerators. The authors propose a completely back-propagation-free (BP-free) framework to enable scalable training of real-size PINNs on integrated photonic platforms. The method combines a sparse-grid Stein derivative estimator to avoid BP in loss evaluation and a tensor-train compressed zeroth-order optimization to improve scalability and convergence.

## Method Summary
The authors present a novel framework for training PINNs without back-propagation, combining sparse-grid Stein derivative estimators with tensor-train compressed zeroth-order optimization. The approach enables photonic implementation of PINN training by reducing the number of required photonic devices while maintaining competitive accuracy. The method leverages stochastic optimization techniques adapted for photonic hardware constraints, with the tensor-train decomposition providing efficient parameter representation for large-scale networks.

## Key Results
- 42.7× reduction in device count for photonic implementation
- 1.64-second training time for solving the Black-Scholes equation
- Competitive accuracy compared to standard BP-based training across various PDEs

## Why This Works (Mechanism)
The framework works by eliminating the need for gradient-based back-propagation through two key innovations: a sparse-grid Stein derivative estimator that approximates gradients using random sampling rather than exact derivatives, and a tensor-train compressed representation that reduces the parameter space while maintaining expressiveness. This combination allows the optimization to proceed without computing exact gradients, which is computationally expensive on photonic hardware.

## Foundational Learning
- **Physics-Informed Neural Networks (PINNs)**: Neural networks trained to solve PDEs by incorporating physical laws as constraints in the loss function. Needed to understand the target application domain; quick check: can the network satisfy both data fitting and physical constraints simultaneously?
- **Back-Propagation-Free Optimization**: Optimization methods that don't require gradient computation through the entire network. Needed to understand the core innovation; quick check: can the method converge without exact gradient information?
- **Stein Derivative Estimator**: A statistical method for approximating gradients using random sampling. Needed to understand how gradients are approximated; quick check: does the estimator provide sufficient accuracy for convergence?
- **Tensor-Train Decomposition**: A low-rank tensor representation that compresses high-dimensional data. Needed to understand the scalability solution; quick check: does compression preserve sufficient information for accurate solutions?
- **Zeroth-Order Optimization**: Optimization methods that only require function evaluations, not gradients. Needed to understand the optimization framework; quick check: how does convergence rate compare to first-order methods?
- **Photonic Computing Accelerators**: Hardware that performs computation using light instead of electricity. Needed to understand the target platform; quick check: what are the fundamental limitations of photonic computation?

## Architecture Onboarding
**Component Map**: Stein Estimator -> Tensor-Train Compression -> Zeroth-Order Optimizer -> Photonic Hardware
**Critical Path**: Input PDE parameters → Stein derivative estimation → Tensor-train optimization update → Photonic device control → Solution output
**Design Tradeoffs**: Accuracy vs. device count reduction, convergence speed vs. hardware complexity, generalization vs. PDE-specific tuning
**Failure Signatures**: Divergence due to poor gradient approximation, stagnation from insufficient device resolution, accuracy loss from over-compression
**First Experiments**: 1) Validate Stein estimator accuracy on simple analytical functions, 2) Test tensor-train compression limits on known PDE solutions, 3) Benchmark convergence against standard BP-based training on benchmark PDEs

## Open Questions the Paper Calls Out
The paper acknowledges several open questions including the need for experimental validation on real photonic hardware, the performance of the method on more complex, multi-scale PDEs, and the potential for extending the approach to other physics-informed learning tasks beyond traditional PDEs.

## Limitations
- Performance on real photonic hardware not experimentally validated, only simulated
- Theoretical convergence guarantees for high-dimensional problems remain unproven
- Tensor-train decomposition may introduce approximation errors that accumulate during training

## Confidence
- High confidence in theoretical framework and simulation methodology
- Medium confidence in device count reduction claims (validated via simulation)
- Medium confidence in convergence and accuracy compared to BP-based training
- Low confidence in real-world photonic hardware performance and energy efficiency

## Next Checks
1. Experimental validation on integrated photonic platforms to verify simulation results and measure actual energy consumption
2. Benchmarking against BP-based training across diverse PDE types and solution regimes
3. Assessment of numerical stability and error propagation in long-time integration scenarios