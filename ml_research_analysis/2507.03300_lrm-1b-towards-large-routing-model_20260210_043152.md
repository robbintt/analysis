---
ver: rpa2
title: 'LRM-1B: Towards Large Routing Model'
arxiv_id: '2507.03300'
source_url: https://arxiv.org/abs/2507.03300
tags:
- performance
- routing
- scaling
- size
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LRM-1B, a 1-billion-parameter neural routing
  model for solving vehicle routing problems (VRPs). Inspired by scaling laws in large
  language models, LRM-1B is trained on diverse VRP variants, scales, and distributions
  using a transformer-based architecture with spectral normalization for stability.
---

# LRM-1B: Towards Large Routing Model

## Quick Facts
- **arXiv ID:** 2507.03300
- **Source URL:** https://arxiv.org/abs/2507.03300
- **Authors:** Han Li; Fei Liu; Zhenkun Wang; Qingfu Zhang
- **Reference count:** 40
- **One-line primary result:** 1-billion-parameter neural routing model achieves state-of-the-art performance on 16 VRP variants through scaling laws.

## Executive Summary
This paper introduces LRM-1B, a 1-billion-parameter neural routing model trained on diverse Vehicle Routing Problem (VRP) variants using a transformer-based architecture with spectral normalization. The model demonstrates scaling laws where performance improvements follow predictable power-law relationships with model size and inference trajectory count. LRM-1B achieves state-of-the-art results across 16 VRP variants, outperforming existing multi-task routing models like MTPOMO, MVMoE, and RouteFinder, particularly on challenging out-of-distribution scenarios.

## Method Summary
LRM-1B is a transformer-based neural routing model trained using Policy Optimization with Multiple Optima (POMO) on 16 VRP variants with diverse scales (50-200 nodes) and distributions. The architecture employs RMSNorm normalization, SwiGLU layers, and spectral normalization on all linear layers to prevent gradient explosion during reinforcement learning. Training uses REINFORCE with shared baseline, Adam for models ≤40M parameters and Adafactor for the 1B model, with dynamic batch sizing based on instance scale. Instances are generated on-the-fly from Mixed Gaussian and Uniform distributions.

## Key Results
- LRM-1B achieves state-of-the-art performance across 16 VRP variants, outperforming MTPOMO, MVMoE, and RouteFinder
- Performance scales predictably with model size following a power law (αN ≈ 0.066), where doubling model size reduces performance gap by ~5%
- Inference-time performance scales with trajectory count following a power law, with ~7% improvement when doubling trajectory count for 100-node graphs
- Model demonstrates strong generalization to out-of-distribution scenarios, though performance degrades on real-world instances with high demand-to-capacity ratios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing model parameter count enables consolidation of distinct combinatorial heuristics into a single unified policy following predictable scaling laws.
- **Mechanism:** The 1.1B parameter Transformer provides sufficient capacity to approximate complex, conditional mappings for 16 different VRP variants simultaneously. Instead of overfitting, increased scale allows learning shared structural representations that transfer across problem types.
- **Core assumption:** Training data distribution is sufficiently diverse and optimization landscape is smooth enough for the model to converge on shared features rather than memorizing instance-specific noise.
- **Evidence anchors:** [abstract] scaling up neural routing models yields consistent improvements; [Section 5.1] average scaling exponent αN = 0.066; [corpus] URS paper corroborates difficulty of unified solvers.
- **Break condition:** Scaling benefits may diminish if training distribution is narrow, causing overfitting to specific structural biases.

### Mechanism 2
- **Claim:** Spectral normalization is critical for stabilizing reinforcement learning training dynamics in high-capacity routing models.
- **Mechanism:** Large Transformers are prone to gradient explosion during RL policy updates. Spectral normalization constrains the spectral norm of weight matrices, bounding the Lipschitz constant and dampening exploding gradients.
- **Core assumption:** Instability is primarily driven by unbounded weight matrix norms rather than inherent RL reward variance.
- **Evidence anchors:** [Section 3.2] Without spectral norm regularization, severe gradient fluctuations lead to explosion; [Figure 2] demonstrates divergence without regularization.
- **Break condition:** If model scale is reduced significantly, this regularization may become redundant or overly restrictive.

### Mechanism 3
- **Claim:** Inference-time performance scales predictably with the number of generated trajectories and compute budget.
- **Mechanism:** The model generates multiple candidate solutions by varying starting nodes and applying geometric augmentations. Because the base policy is robust due to scale, these trajectories are diverse and high-quality.
- **Core assumption:** Base policy generates solutions that are locally optimal and distinct enough that sampling yields meaningful variance.
- **Evidence anchors:** [Section 5.2] Doubling trajectory count leads to ~7% improvement for 100-node graphs; [Figure 5] visualizes linear relationship between ln(T) and ln(G).
- **Break condition:** If model is undertrained or too small, increasing trajectory count may yield diminishing returns as policy collapses to repetitive, low-quality solutions.

## Foundational Learning

- **Concept:** Transformer Attention Masks & Context Embeddings
  - **Why needed here:** The model processes graphs of nodes, not sequences. Understanding how the decoder constructs context embeddings based on current partial route and dynamically masks infeasible nodes is crucial.
  - **Quick check question:** If a vehicle has remaining capacity 5, and nodes A (demand 10) and B (demand 3) are available, which node should the attention mechanism assign a probability of ~0 via masking?

- **Concept:** Spectral Normalization
  - **Why needed here:** This specific technique is the "glue" holding the 1B model together. Standard normalization was insufficient.
  - **Quick check question:** Does spectral normalization scale weights based on the mean of the batch or the singular values of the weight matrix?

- **Concept:** POMO (Policy Optimization with Multiple Optima)
  - **Why needed here:** Training framework relies on POMO, forcing the model to learn multiple solution trajectories simultaneously during a single forward pass.
  - **Quick check question:** How does POMO differ from standard Reinforcement Learning regarding the starting state of an episode?

## Architecture Onboarding

- **Component map:** Node coordinates, demands, time windows → Linear Projection → Encoder (MHA + SwiGLU + RMSNorm + Spectral Norm) → Decoder (Context Embedding → MHA Query → Compatibility Calculation → Softmax) → Probability distribution over next nodes

- **Critical path:**
  1. Instance Generation: Batch creation mixing 16 variants and diverse distributions
  2. Forward Pass: Encoder processes nodes; Decoder autoregressively builds routes
  3. Masking: Hard constraints (capacity, time windows) zero out invalid logits
  4. Optimization: Adafactor (for 1B) with Spectral Norm on linear layers

- **Design tradeoffs:**
  - 1B vs. 40M: While 1B sets SOTA on synthetic benchmarks, 40M model often performs better on real-world datasets (CVRPLib) due to overfitting
  - Augmentation vs. Width: Using ×8 aug with fewer trajectories is often more compute-efficient than wider multi-start without augmentation

- **Failure signatures:**
  - Gradient Explosion: If loss suddenly spikes and diverges after ~1000 epochs, check if spectral normalization is disabled
  - Real-World Degradation: If 1B model underperforms smaller models on specific real instances, check the demand-to-capacity ratio R

- **First 3 experiments:**
  1. Stability Ablation: Train 40M model with and without spectral normalization, plot gradient norms to confirm stability mechanisms
  2. Scaling Law Verification: Train 1M, 5M, and 40M models, plot performance gap vs. model size on log-log scale to verify power-law exponent
  3. Capacity Overfitting Check: Compare 40M vs. 1B on CVRPLib "X" dataset, verify if 1B model's performance degrades on instances with high demand-to-capacity ratios (R > 0.2)

## Open Questions the Paper Calls Out
- Can the LRM-1B framework be extended to a unified foundation model capable of solving diverse combinatorial optimization tasks beyond vehicle routing?
- Does a compute-optimal scaling law exist for the training phase of neural routing models, similar to those found in Large Language Models?
- How can large neural routing models be regularized or trained to prevent overfitting to specific instance parameters (like the demand-capacity ratio) when generalizing to real-world data?

## Limitations
- Model shows degradation on real-world instances with demand-to-capacity ratios outside training distribution
- Learning rate decay schedule for 1B model remains underspecified beyond being "time-dependent"
- Scaling benefits may diminish or reverse for narrow training distributions

## Confidence
- **High Confidence (80-100%):** Architectural innovations (RMSNorm, SwiGLU, Spectral Normalization) and their documented effects on training stability are well-supported
- **Medium Confidence (50-80%):** Power-law scaling relationships demonstrated on synthetic benchmarks may not translate directly to real-world instances
- **Low Confidence (0-50%):** Specific optimal training configurations for 1B model may be sensitive to implementation details not fully specified

## Next Checks
1. **Scaling Law Generalization Test:** Train LRM models of 5M, 40M, and 1B parameters on a real-world VRP dataset and verify whether performance gap follows predicted power law with αN ≈ 0.066
2. **Spectral Normalization Necessity Verification:** Conduct ablation study across model sizes to determine whether spectral normalization remains critical for stability as model capacity increases
3. **Distribution Robustness Analysis:** Systematically vary the demand-to-capacity ratio R in test instances to identify threshold beyond which 1B model's performance degrades relative to smaller models