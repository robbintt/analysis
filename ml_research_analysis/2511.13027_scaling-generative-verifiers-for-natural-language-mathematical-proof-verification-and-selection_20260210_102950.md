---
ver: rpa2
title: Scaling Generative Verifiers For Natural Language Mathematical Proof Verification
  And Selection
arxiv_id: '2511.13027'
source_url: https://arxiv.org/abs/2511.13027
tags:
- proof
- verification
- mathematical
- judgement
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the challenge of verifying and selecting
  natural-language mathematical proofs using large language models (LLMs). The authors
  develop a unified test-time scaling approach that combines GenSelect tournaments
  with LLM-as-a-Judge evaluation, demonstrating that this hybrid method outperforms
  individual techniques in proof selection tasks.
---

# Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection

## Quick Facts
- arXiv ID: 2511.13027
- Source URL: https://arxiv.org/abs/2511.13027
- Reference count: 40
- SOTA hybrid method (GenSelect tournaments + LLM-as-a-Judge) achieves 96.05% accuracy on Challenge-19

## Executive Summary
This paper tackles the challenge of verifying and selecting natural-language mathematical proofs using large language models. The authors develop a unified test-time scaling approach that combines GenSelect tournaments with LLM-as-a-Judge evaluation, demonstrating that this hybrid method outperforms individual techniques in proof selection tasks. Through extensive experiments, they show that reinforcement learning can improve proof-level metrics and reduce prompt sensitivity in LLM-as-a-Judge methods, though it does not enhance final-answer precision, suggesting current approaches rely more on recognizing stylistic features than deep mathematical understanding.

## Method Summary
The method combines three key components: (1) GenSelect tournaments that efficiently filter candidate proofs through pairwise comparisons, (2) LLM-as-a-Judge evaluation that scores survivors with multiple sampled judgments, and (3) optional reinforcement learning fine-tuning to improve judge calibration. The hybrid approach generates $n_p$ candidate proofs, runs a knockout tournament to reduce to $n_s$ survivors, then applies $n_j$ LLM judgments per survivor to select the best proof. The system addresses the fundamental challenge that proof verification requires step-by-step reasoning rather than simple final-answer matching, necessitating new evaluation methodologies and scaling strategies.

## Key Results
- Hybrid GenSelect (128, 16, 32) achieves 96.05% accuracy on Challenge-19 vs 91.4% for pure knockout tournaments
- AIME 2025 benchmark reaches 100% accuracy with the hybrid approach
- Reinforcement learning improves proof-level metrics (6-8% accuracy gain) but does not improve final-answer precision, indicating models learn stylistic features rather than mathematical reasoning
- Simple MLP and heuristic methods achieve 75.3% and 65% accuracy on VerOPC by exploiting dataset imbalances, highlighting the need for balanced evaluation sets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hybrid approach combining GenSelect knockout tournaments with LLM-as-a-Judge evaluation outperforms either method alone for proof selection at scale.
- Mechanism: GenSelect tournaments efficiently filter $n_p$ candidates to $n_s$ survivors via pairwise comparisons ($n_p - 1$ comparisons), then LLM-as-a-Judge samples $n_j$ judgments per survivor to estimate correctness scores. This balances compute (quadratic pairwise costs avoided) with accuracy (scoring provides finer-grained selection than single-elimination).
- Core assumption: Judges provide independent, approximately calibrated scores; tournament winners are more likely correct than random candidates.
- Evidence anchors:
  - [abstract]: "identify their combination as the most effective framework for solution verification and selection"
  - [Section 7, Figure 3]: Hybrid (128, 16, 32) achieves 96.05% on Challenge-19 vs 91.4% for pure knockout; AIME 2025 reaches 100% accuracy
  - [corpus]: Weak direct support—neighbor papers focus on generative verification broadly, not this specific hybrid
- Break condition: When $n_s$ is too small (filters out correct proofs early) or judge calibration is poor (scores uncorrelated with correctness).

### Mechanism 2
- Claim: Reinforcement learning fine-tuning improves proof-level judgment metrics and reduces prompt sensitivity without improving final-answer precision.
- Mechanism: GRPO/DAPO training on labeled proof data teaches models to match ground-truth judgments, converging different prompts (OPC, GIMO) to similar performance. The model learns to recognize stylistic markers of incorrect proofs (missing justifications, format issues) rather than deeper mathematical validity.
- Core assumption: Training labels are sufficiently accurate; stylistic features correlate with incorrectness in the training distribution.
- Evidence anchors:
  - [Section 5, Figure 1]: RL closes performance gap between OPC and GIMO prompts; validation accuracy improves 6-8%
  - [Section 5]: "RL significantly improves proof-level metrics, but the performance gain mainly serves as a calibration process rather than improved mathematical reasoning"
  - [corpus]: DeepSeekMath-V2 and related work show RLVR success on final-answer tasks, contrasting with proof-level limitations here
- Break condition: When test distribution differs significantly from training (out-of-distribution problems) or when correct proofs share stylistic features with incorrect ones.

### Mechanism 3
- Claim: Multi-benchmark evaluation combining proof-level and final-answer metrics reveals whether verifiers assess mathematical content versus exploiting spurious correlations.
- Mechanism: Final-answer precision on balanced datasets (equal correct/incorrect final answers per problem) acts as a proxy for genuine reasoning assessment. If a judge labels a proof "correct," precision measures whether the final answer is actually right—exposing style-over-substance learning.
- Core assumption: Correct proofs should have correct final answers; stylistic features can be decorrelated from correctness through balanced construction.
- Evidence anchors:
  - [Section 4]: Simple MLP and trivial heuristic achieve 75.3% and 65% on VerOPC by exploiting dataset imbalances (geometry = incorrect, "---" marker = correct)
  - [Section 4, Takeaway 1]: "Incorporating a balanced final-answer evaluation set can further help better distinguish judges that assess genuine mathematical content"
  - [corpus]: Hard2Verify (2510.13744) similarly emphasizes step-level verification to avoid superficial correctness signals
- Break condition: When proof correctness doesn't imply final-answer correctness (partial credit scenarios) or when balanced datasets don't cover the problem distribution.

## Foundational Learning

- Concept: **LLM-as-a-Judge paradigm** (model assigns correctness scores to candidate solutions)
  - Why needed here: Core verification method; understanding prompt sensitivity, aggregation strategies (majority voting, averaging), and binary vs. graded scoring is essential for configuring the system.
  - Quick check question: Given 32 sampled judgments for a proof with scores averaging 0.7, what aggregation method would you use to make a binary decision?

- Concept: **GenSelect tournament structures** (knockout vs. pairwise comparison)
  - Why needed here: Determines compute-accuracy tradeoffs; knockout requires $O(n)$ comparisons but is sequential, pairwise requires $O(n^2)$ but is parallelizable.
  - Quick check question: For selecting among 256 candidate proofs, how many LLM calls does a knockout tournament require versus a full pairwise tournament?

- Concept: **Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed here: Explains why final-answer tasks scale well (easy verification) while proof tasks struggle; frames the calibration-vs-reasoning distinction.
  - Quick check question: Why does binary reward (match/mismatch ground truth) improve proof-judgment accuracy but not final-answer precision?

## Architecture Onboarding

- Component map:
  - Proof Generator -> GenSelect Tournament -> LLM-as-a-Judge -> Aggregator -> Selected proof

- Critical path: Generator $\to$ Tournament $\to$ Judge $\to$ Aggregator $\to$ Selected proof
  - Bottleneck: Judge inference ($n_s \times n_j$ calls); parallelizable across proofs and judgment samples

- Design tradeoffs:
  - Higher $n_p$: More coverage but more compute
  - Higher $n_s$: Better tournament filtering retention but more judge calls
  - Higher $n_j$: More stable scores but diminishing returns (saturates ~32)
  - With rubric/ground-truth: Slight final-answer precision gain, minimal proof-level improvement

- Failure signatures:
  - **High recall, low precision** (General Summary prompt): Judge accepts too many incorrect proofs
  - **High precision, low recall** (GIMO prompt): Judge rejects correct proofs as incomplete
  - **Dataset exploitation**: Accuracy high on benchmark but low on balanced final-answer set (stylistic shortcuts)
  - **RL without reasoning gain**: Proof accuracy improves but final-answer precision flat

- First 3 experiments:
  1. **Baseline comparison**: Run pure knockout, pure LLM-as-a-Judge, and hybrid ($n_p=128, n_s=16, n_j=32$) on Challenge-19; measure accuracy vs. compute (LLM calls)
  2. **Prompt sensitivity audit**: Test OPC, GIMO, and General Summary prompts with and without RL fine-tuning; plot proof-level F1 and final-answer precision
  3. **Dataset balance check**: Train a simple classifier on (problem, proof) embeddings; if it exceeds 70% accuracy without reasoning, audit for spurious correlations (model/source markers, problem-type bias)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can step-level judgment methods, which supervise intermediate reasoning steps, significantly outperform whole-proof evaluation in proof verification accuracy?
- Basis in paper: [explicit] In the Future Directions section, the authors state they plan to "explore other scaling methods such as step-level judgement, where intermediate reasoning steps are supervised rather than the final outcome."
- Why unresolved: The paper implemented step-based judgment (Section 5) but found it had higher precision at the cost of much lower recall, indicating the aggregation method and supervision strategy remain open problems.
- What evidence would resolve it: Empirical results on benchmarks like VerProofArena showing step-level methods achieving higher F1 scores than whole-proof LLM-as-a-Judge approaches, along with ablations on aggregation strategies.

### Open Question 2
- Question: How can reinforcement learning be designed to improve genuine mathematical reasoning in verifiers, rather than primarily rewarding stylistic or procedural features?
- Basis in paper: [explicit] The paper observes that "RL significantly improves proof-level metrics, but the performance gain mainly serves as a calibration process rather than improved mathematical reasoning," and hypothesizes that gains come from identifying stylistic flaws.
- Why unresolved: Current RL training improved proof-level metrics but not final-answer precision, suggesting a fundamental misalignment in the reward signal or data composition.
- What evidence would resolve it: Demonstration of RL-trained verifiers that improve both proof-level metrics and final-answer precision, particularly on challenging out-of-distribution datasets like VerProofArena.

### Open Question 3
- Question: What methodologies are required to scale up LLM-based judges and solvers to handle frontier mathematical problems reliably?
- Basis in paper: [explicit] The Future Directions section states the intent to "leverage RL to further scale up the performance of LLM-based judges and solvers."
- Why unresolved: The paper shows current SOTA LLMs (e.g., GPT-5) still hallucinate correctness on difficult proofs (Section 9), and dataset imbalances limit training.
- What evidence would resolve it: Creation of new benchmarks composed of problems at the frontier of LLM capabilities (e.g., USAMO/IMO problems where current models have low solve rates) and models that achieve high precision on these sets after scaled RL training.

### Open Question 4
- Question: How can large-scale, balanced, and low-noise evaluation sets for proof verification be constructed to avoid exploitable correlations?
- Basis in paper: [inferred] The paper reveals that simple MLP classifiers and heuristics outperform many LLMs by exploiting dataset imbalances (Challenge 2), and that small test sets (e.g., SelProofBench with 29 problems) limit the reliability of method comparisons.
- Why unresolved: Existing benchmarks suffer from human label noise (Challenge 1) and distributional imbalances that inflate accuracy without testing true reasoning.
- What evidence would resolve it: Introduction of new benchmarks with statistically significant sample sizes, balanced across problem types and generator models, validated through procedures that prevent trivial solutions from achieving non-random performance.

## Limitations
- The effectiveness of the hybrid GenSelect + LLM-as-a-Judge approach depends heavily on dataset balance and judge calibration, which are not fully characterized across problem types
- The paper identifies significant dataset imbalances (e.g., Simple MLP achieving 75.3% on VerOPC by exploiting geometry problem correlations) that limit generalizability
- Current LLMs, despite high accuracy on proof judgment tasks, cannot yet be fully trusted as reliable mathematical judges in high-stakes scenarios

## Confidence
- **High confidence**: Hybrid GenSelect + LLM-as-a-Judge outperforms individual methods (supported by Challenge-19 and AIME 2025 results)
- **Medium confidence**: RL training improves proof-level metrics but not final-answer precision (consistent with RLVR limitations on proof tasks vs. final-answer tasks)
- **Low confidence**: Current LLMs cannot be trusted as reliable mathematical judges in high-stakes scenarios (this is a general claim not fully supported by the specific experiments)

## Next Checks
1. Audit the VerOPC dataset for spurious correlations by training a simple classifier on problem embeddings; if accuracy exceeds 70% without reasoning, investigate geometry problem biases
2. Test the hybrid selection pipeline on out-of-distribution problems (e.g., IMO 2025 problems) to verify robustness beyond Challenge-19
3. Compare RL-trained vs. non-RL judges on a balanced final-answer dataset where correct proofs have correct final answers and incorrect proofs have incorrect final answers; measure whether style-over-substance learning persists