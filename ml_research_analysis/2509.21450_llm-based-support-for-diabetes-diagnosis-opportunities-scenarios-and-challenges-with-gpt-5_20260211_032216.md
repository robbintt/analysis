---
ver: rpa2
title: 'LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges
  with GPT-5'
arxiv_id: '2509.21450'
source_url: https://arxiv.org/abs/2509.21450
tags:
- diabetes
- gpt-5
- patient
- cases
- hba1c
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluated GPT-5 for diabetes diagnosis using synthetic\
  \ patient cases aligned with ADA 2025 guidelines. Across five scenarios\u2014symptom\
  \ recognition, laboratory interpretation, gestational diabetes screening, remote\
  \ monitoring, and multimodal complication detection\u2014GPT-5 consistently classified\
  \ cases correctly and produced both clinical rationales and patient-friendly explanations."
---

# LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5

## Quick Facts
- **arXiv ID**: 2509.21450
- **Source URL**: https://arxiv.org/abs/2509.21450
- **Reference count**: 14
- **Primary result**: GPT-5 correctly classified synthetic diabetes cases across five clinical scenarios using ADA 2025 thresholds and generated both clinical rationales and patient-friendly explanations.

## Executive Summary
This study evaluates GPT-5 for diabetes diagnosis support across five clinical scenarios aligned with ADA 2025 guidelines. Using synthetic patient cases, GPT-5 demonstrated accurate classification of diabetes, prediabetes, and gestational diabetes cases based on laboratory thresholds. The model produced structured JSON outputs alongside clinical rationales and patient-friendly explanations, showing potential as a dual-purpose decision-support tool. However, findings are based on synthetic data and require real-world validation before clinical deployment.

## Method Summary
The study used synthetic patient cases constructed from ADA 2025 diagnostic thresholds and public dataset distributions (NHANES, Pima Indians, EyePACS, MIMIC-IV). GPT-5 was prompted with standardized templates for five scenarios: symptom recognition, laboratory interpretation, gestational diabetes screening, remote monitoring, and multimodal complication detection. The model generated classifications, clinical rationales, patient messages, and JSON-structured outputs. Performance was evaluated against ADA-defined ground truth labels without model fine-tuning.

## Key Results
- GPT-5 correctly classified all synthetic cases according to ADA 2025 thresholds across all five scenarios
- Model generated both technical clinical rationales and simplified patient explanations within single responses
- Structured JSON outputs (classification, criteria_triggered, next_steps) were produced consistently for system integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-5 maps clinical inputs to guideline-based diagnostic classifications when provided with structured prompts and explicit threshold definitions
- Mechanism: The model receives synthetic patient cases with ADA 2025 thresholds embedded in system prompts and applies conditional logic to classify cases
- Core assumption: GPT-5's pre-trained medical knowledge combined with in-context threshold definitions enables reliable rule application
- Evidence anchors: Strong alignment with ADA-defined criteria; correct classification of discordant case P009 based on HbA1c≥6.5% despite FPG<126 mg/dL
- Break condition: Adversarial or ambiguous cases not tested; real-world heterogeneity may exceed synthetic case distributions

### Mechanism 2
- Claim: GPT-5 can generate parallel outputs—clinical rationales and patient-friendly explanations—within a single response
- Mechanism: Prompt instructs GPT-5 to produce both clinical rationale using medical terminology and simplified patient message
- Core assumption: Model maintains accuracy while code-switching between clinical and lay registers
- Evidence anchors: Results showed combination of guideline adherence with accessible communication; GDM outputs include both technical rationale and patient message
- Break condition: Complex cases where simplification introduces inaccuracy

### Mechanism 3
- Claim: GPT-5 demonstrates capacity to reason across longitudinal time-series data and multimodal input descriptors
- Mechanism: For remote monitoring, weekly glucose logs are provided as tabular text; for multimodal assessment, textual retinal finding descriptors are paired with HbA1c values
- Core assumption: Text-based representations approximate true multimodal processing capabilities
- Evidence anchors: Three-week glucose progression correctly classified with escalating alerts; retinal descriptors + HbA1c correctly staged retinopathy severity
- Break condition: Raw image inputs not tested; true multimodal fusion performance unknown

## Foundational Learning

- Concept: ADA 2025 Diagnostic Thresholds (FPG ≥126 mg/dL, HbA1c ≥6.5%, OGTT GDM criteria: fasting ≥92, 1h ≥180, 2h ≥153 mg/dL)
  - Why needed here: All GPT-5 classifications are evaluated against these thresholds
  - Quick check question: Given FPG=118 mg/dL and HbA1c=6.6%, what is the ADA 2025 classification? (Answer: Diabetes—HbA1c meets threshold even though FPG does not)

- Concept: Structured Prompt Engineering (system instructions + user input + explicit task specification)
  - Why needed here: GPT-5 performance depends on standardized prompt templates that embed guidelines and constrain output format
  - Quick check question: What three elements does the general prompt template require for each scenario? (Answer: System role with ADA thresholds, patient case input, explicit task list)

- Concept: Synthetic Case Generation for AI Evaluation
  - Why needed here: This study's claims are entirely based on synthetic data
  - Quick check question: Why were synthetic cases used instead of real EHR data? (Answer: Ethical compliance, reproducibility, avoiding identifiable health data—but limits heterogeneity)

## Architecture Onboarding

- Component map: Synthetic Case Generator -> Prompt Templates -> GPT-5 API Layer -> Output Parser -> Ground Truth Evaluator
- Critical path: Case construction → Prompt formatting → GPT-5 inference → JSON parsing → Threshold alignment evaluation
- Design tradeoffs: Synthetic vs. real data enables reproducibility but may not capture real-world complexity; textual multimodal descriptors test reasoning logic but don't validate true image processing
- Failure signatures: Classification mismatch with ADA thresholds (indicates prompt failure or model error); missing JSON fields (integration blocker); patient message contradicting clinical rationale (consistency failure)
- First 3 experiments: 1) Replicate Scenario 2 with additional borderline cases around FPG 100–126 and HbA1c 6.4–6.5% to stress-test threshold boundaries; 2) Extend Scenario 4 to 8–12 weeks of simulated data to evaluate alert fatigue; 3) Introduce adversarial cases with conflicting symptoms to identify breakdown points in reasoning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-5 performance on diabetes diagnosis compare when using raw retinal images versus textual descriptions of retinal findings?
- Basis in paper: The authors state: "No raw multimodal inputs: Retinal images were described textually, not processed as raw image files, though GPT-5 supports multimodal input."
- Why unresolved: Scenario 5 tested multimodal reasoning using only textual descriptors, not actual image files. Performance degradation with raw images is unknown.

### Open Question 2
- Question: How does GPT-5 perform on adversarial or clinically ambiguous diabetes cases that do not align cleanly with ADA thresholds?
- Basis in paper: The discussion notes: "This work did not evaluate the model's behavior in adversarial or ambiguous cases, which are common in real-world practice."
- Why unresolved: All synthetic cases were constructed with clear threshold boundaries; no edge cases with conflicting symptoms were tested.

### Open Question 3
- Question: Do patients understand and trust GPT-5-generated explanations of diabetes diagnoses at levels comparable to clinician-provided explanations?
- Basis in paper: The authors state future studies "should also assess patient comprehension and trust in AI-generated explanations."
- Why unresolved: The study demonstrated that GPT-5 can generate patient-friendly messages but did not empirically evaluate whether actual patients find them comprehensible or trustworthy.

### Open Question 4
- Question: How does GPT-5 diagnostic performance compare to practicing clinicians across the five diabetes care scenarios?
- Basis in paper: The conclusion states: "Future research should... compare model outputs directly with clinician performance."
- Why unresolved: Ground truth was defined by ADA guideline thresholds, not by independent clinician adjudication; no human benchmarking was conducted.

## Limitations
- Findings based entirely on synthetic patient cases, which may not capture real-world complexity and heterogeneity
- Multimodal reasoning tested only with textual descriptions of retinal findings, not raw image data
- Paper does not specify total number of test cases beyond representative examples shown

## Confidence

- **High Confidence**: GPT-5's ability to classify synthetic diabetes cases according to ADA 2025 thresholds when provided with structured prompts and explicit criteria
- **Medium Confidence**: GPT-5's capacity to generate both clinical rationales and patient-friendly explanations within a single response
- **Low Confidence**: GPT-5's multimodal reasoning with raw imaging data and performance with real-world EHR data

## Next Checks
1. Test GPT-5 with additional borderline cases around FPG 100-126 and HbA1c 5.7-6.5% thresholds to stress-test classification consistency and prompt sensitivity
2. Extend the remote monitoring scenario to 8-12 weeks of simulated glucose data to evaluate trend detection, alert fatigue, and long-term pattern recognition
3. Introduce adversarial cases with conflicting symptoms and lab values to identify breakdown points in GPT-5's reasoning and document any classification errors or rationale inconsistencies