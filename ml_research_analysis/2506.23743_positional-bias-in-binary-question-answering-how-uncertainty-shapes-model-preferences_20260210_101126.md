---
ver: rpa2
title: 'Positional Bias in Binary Question Answering: How Uncertainty Shapes Model
  Preferences'
arxiv_id: '2506.23743'
source_url: https://arxiv.org/abs/2506.23743
tags:
- answer
- bias
- uncertainty
- positional
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates positional bias in large language models
  (LLMs) when performing binary question answering under varying levels of answer
  uncertainty. The authors created SQuAD-it-2, a benchmark derived from the Italian
  SQuAD-it dataset, with three uncertainty levels: low (context provided), medium
  (context removed), and high (two out-of-context distractors).'
---

# Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences

## Quick Facts
- arXiv ID: 2506.23743
- Source URL: https://arxiv.org/abs/2506.23743
- Reference count: 28
- Primary result: Positional bias grows exponentially as answer uncertainty increases, from nearly absent under low uncertainty to strong reliance on position-based heuristics under high uncertainty.

## Executive Summary
This study investigates how answer uncertainty influences positional bias in large language models performing binary question answering. Using the SQuAD-it-2 benchmark with three uncertainty levels (low: context provided, medium: context removed, high: two out-of-context distractors) plus subjective datasets (WebGPT and Winning Arguments), the authors measure positional bias using Preference Fairness and Position Consistency metrics across systematic answer order swaps. Results show that positional bias is negligible under low uncertainty but grows dramatically as uncertainty increases, with models increasingly defaulting to position-based heuristics. Notably, in persuasive argument tasks, all models consistently preferred the second option regardless of content, suggesting discourse patterns in training data create strong recency biases.

## Method Summary
The study employs a two-pass evaluation design where each binary question is presented twice: once with the correct answer as Option 1 and once with it as Option 2, keeping all other prompt elements constant. Positional bias is quantified using Preference Fairness (|PF|, measuring directional bias) and Position Consistency (PC, measuring stability of choices). Five LLMs were tested across three datasets: SQuAD-it-2 (7,609 samples × 3 uncertainty variants), WebGPT (14,346 samples), and Winning Arguments (807 samples). The methodology systematically isolates position-based effects from semantic content by swapping answer positions while maintaining identical prompts.

## Key Results
- Positional bias is nearly absent under low uncertainty conditions but grows exponentially as uncertainty increases
- Under high uncertainty, models frequently default to order-based heuristics and show increased invalid response rates
- In the Winning Arguments dataset, all models consistently preferred the second option regardless of content
- Gemma-3-12B and Phi4-14B (quantized) showed lower positional bias compared to other models
- Invalid responses spike significantly in high-uncertainty settings, especially for Llama and Gemini models

## Why This Works (Mechanism)

### Mechanism 1: Uncertainty-Driven Heuristic Activation
Model reliance on positional heuristics emerges conditionally as answer uncertainty increases. Under low uncertainty, strong semantic cues allow confident identification of correct answers, suppressing positional influence. As these cues are degraded or removed, internal uncertainty rises, triggering a switch from semantic-evaluation mode to heuristic-based mode. This mechanism assumes models possess an internal uncertainty signal that, when elevated, activates fallback strategies.

### Mechanism 2: Discourse-Derived Recency Bias in Persuasion Tasks
In subjective argumentative domains, models systematically prefer the second-presented option due to training on discourse where effective arguments often follow initial claims as rebuttals. The model may have internalized discourse patterns from training data (e.g., Reddit, forums) where persuasive counter-arguments are commonly presented after initial claims, creating a prior that the second message is implicitly treated as a refinement or rebuttal.

### Mechanism 3: Output Instability Under Maximum Ambiguity
When presented with two equally implausible or incorrect options, model decision-making becomes unstable, manifesting as both strong positional bias and failure to follow output instructions. In high-uncertainty conditions lacking semantically valid answers, the model's inability to ground decisions leads to degradation of instruction-following capability, resulting in higher rates of invalid (non-A/B) outputs alongside volatile, position-driven choices.

## Foundational Learning

- **Positional Bias (vs. Token Bias)**: Bias stemming from an option's ordinal position (first/second) rather than its label (A/B). Why needed: Critical for proper experimental design and interpretation. Quick check: If you swap positions of options "A" and "B," are you testing for token bias or positional bias? What if you keep positions but relabel them "B" and "A"?

- **Answer Uncertainty as a Continuous Variable**: The study's core manipulation is a spectrum controlled by context availability and distractor plausibility, not binary certain vs. uncertain. Why needed: Essential for applying findings. Quick check: Which creates higher uncertainty: removing a context passage or replacing a correct answer with an out-of-context distractor?

- **Preference Fairness (PF) vs. Position Consistency (PC)**: PF measures direction of bias (favoring 1st or 2nd), while PC measures stability of choice irrespective of position. Why needed: These capture distinct aspects of model behavior. Quick check: A model always chooses the first option. Would it have high or low PC score? What would its PF score indicate?

## Architecture Onboarding

- **Component map**: Input (Q, C, A_correct, A_distractor) → Processing (Two-pass evaluation with position swap) → Output (Raw selection tokens) → Evaluation Logic (PF and PC metric calculation) → Reporting (Aggregate scores)

- **Critical path**: 1) Data Loading: Ingest single instance (Q, C, A_correct, A_distractor) 2) First Pass: Construct prompt (A_correct as Option 1), get response C^(1) 3) Second Pass: Construct prompt (A_correct as Option 2), get response C^(2) 4) Metric Calculation: Compare C^(1) and C^(2) to update PF and PC aggregate scores 5) Reporting: After running across dataset, report PF (absolute value) and PC

- **Design tradeoffs**: Metric simplicity vs. depth (straightforward counts don't capture confidence or reasoning), quantization efficiency vs. potential variance (4-bit quantization tested but may introduce variability)

- **Failure signatures**: High invalid response rates in high-uncertainty conditions (models output text not matching required "A"/"B" format), PC << 1.0 and |PF| > 0 (defining signature of positional bias)

- **First 3 experiments**: 1) Sanity Check: Run full evaluation on SQuAD-it-2 Low Uncertainty set, expect PC ≈ 1.0 and |PF| ≈ 0 2) Mechanism Probe: Run on SQuAD-it-2 High Uncertainty, expect dramatic drop in PC and spike in invalid responses 3) Domain Transfer Test: Evaluate on Winning Arguments, expect strong consistent bias toward second option

## Open Questions the Paper Calls Out

### Open Question 1
What specific training data features or discourse structures mechanistically cause the consistent preference for the second option in subjective persuasion tasks? The paper identifies the behavioral pattern but doesn't isolate specific causal factors within model architecture or training corpus. Resolution would require causal mediation analysis or ablation studies.

### Open Question 2
Can uncertainty-aware debiasing strategies effectively mitigate positional bias in scenarios where semantic cues are entirely absent? While the paper quantifies correlation between uncertainty and bias, it doesn't test interventions to suppress positional heuristics when models lack information for semantically grounded decisions. Resolution would require applying debiasing techniques to high-uncertainty datasets.

### Open Question 3
Does the high rate of invalid responses in high-uncertainty settings indicate failure of instruction-following or emergent capability to express ambiguity? It's unclear if these invalid responses are random errors or if models are implicitly "refusing" to guess when semantic void makes task logically impossible. Resolution would require qualitative analysis categorizing outputs as format errors versus expressions of uncertainty.

## Limitations

- Data construction transparency: SQuAD-it-2 derivation process not fully specified, limiting reproducibility and assessment of uncertainty manipulation control
- Model-specific behavior gaps: Study aggregates results without deep analysis of why certain models show much higher invalid response rates under high uncertainty
- Generalizability boundary: Findings may not extend to multiple-choice settings, natural conversation, or visual presentation formats

## Confidence

**High confidence**: The core empirical finding that positional bias increases with answer uncertainty is well-supported by data across multiple models and datasets with clean causal evidence from two-pass design.

**Medium confidence**: The mechanism explaining uncertainty-triggered heuristic reliance is plausible but not directly tested - shows correlation but doesn't measure internal uncertainty signals.

**Medium confidence**: The discourse-based explanation for second-option preference in persuasive tasks is speculative - consistent directional bias observed but alternative explanations not ruled out.

**Low confidence**: The link between high uncertainty and output instability is weakly supported - paper notes increased invalid responses but doesn't establish whether this reflects genuine instability versus model confusion about task format.

## Next Checks

1. **Direct uncertainty measurement**: Modify experimental pipeline to capture model confidence scores for each option and test whether confidence drops correlate with positional bias emergence across uncertainty spectrum.

2. **Controlled discourse analysis**: Create persuasive argument dataset where second option is deliberately weaker, matching first in all other aspects. If second-option bias persists, it would falsify the discourse-prior hypothesis.

3. **Task format generalization**: Replicate two-pass evaluation with three-option multiple choice (to test if bias scales with position), natural dialogue datasets (where options aren't explicitly ordered), and visual presentation of options (to separate sequence from spatial position).