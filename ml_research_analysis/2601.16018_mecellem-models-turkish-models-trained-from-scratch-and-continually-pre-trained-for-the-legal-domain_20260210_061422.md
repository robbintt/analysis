---
ver: rpa2
title: 'Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained
  for the Legal Domain'
arxiv_id: '2601.16018'
source_url: https://arxiv.org/abs/2601.16018
tags:
- legal
- training
- turkish
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mecellem, a framework for developing specialized
  Turkish legal language models through domain adaptation. The authors address the
  challenge of adapting large language models to the Turkish legal domain, which requires
  handling complex morphological structures and domain-specific terminology.
---

# Mecellem Models: Turkish Models Trained from Scratch and Continually Pre-trained for the Legal Domain

## Quick Facts
- arXiv ID: 2601.16018
- Source URL: https://arxiv.org/abs/2601.16018
- Reference count: 40
- Primary result: 36.2% perplexity reduction on Turkish legal text through four-phase continual pre-training with curriculum learning

## Executive Summary
This paper introduces Mecellem, a framework for developing specialized Turkish legal language models through domain adaptation. The authors address the challenge of adapting large language models to the Turkish legal domain, which requires handling complex morphological structures and domain-specific terminology. They employ two complementary approaches: (1) pre-training ModernBERT-based encoder models from scratch on a Turkish-dominant corpus of 112.7 billion tokens, and (2) continual pre-training Qwen3 decoder models with curriculum learning. The encoder models achieve competitive performance on Turkish legal retrieval benchmarks, ranking among the top models despite using fewer parameters. The decoder models show 36.2% perplexity reduction on Turkish legal text. The work demonstrates that domain-specific pre-training combined with effective post-training techniques can produce high-quality legal language models while maintaining computational efficiency.

## Method Summary
Mecellem employs two parallel approaches to Turkish legal language modeling. For encoder models, it pre-trains ModernBERT architectures from scratch on a 112.7B token Turkish-dominant corpus using 30% masking with 80-10-10 masking strategy, decoupled StableAdamW optimizer, and warmup_stable_decay learning rate schedule. Crucially, it evaluates checkpoints on downstream retrieval tasks rather than relying on MLM loss, finding optimal performance before loss minimization. Post-training uses GISTEmbed false-negative filtering with guide models for contrastive learning. For decoder models, it applies four-phase continual pre-training curriculum: (1) general Turkish texts for language adaptation, (2) legal content with domain terminology, (3) long normative documents, and (4) mixed-complexity refinement. The decoder models use AdamW optimizer with max 5e-5 and min 5e-6 learning rates, cosine schedule, and seq_len=4096.

## Key Results
- Encoder models rank among top performers on Turkish legal retrieval benchmarks despite fewer parameters than competing models
- Decoder models achieve 36.2% perplexity reduction on Turkish legal text through four-phase curriculum learning
- GISTEmbed false-negative filtering improves contrastive learning by 4.6% overall and 15.9% in legal domain
- Checkpoint selection based on downstream retrieval (not MLM loss) prevents performance degradation in morphologically rich languages

## Why This Works (Mechanism)

### Mechanism 1: Downstream-Aware Checkpoint Selection
The paper demonstrates that selecting checkpoints based on downstream retrieval performance rather than minimum pre-training loss yields better embedding models for morphologically rich languages. This occurs because embedding utility diverges from MLM optimization as linguistic complexity increases in agglutinative languages. The authors implement checkpoint evaluation throughout training, revealing a non-linear relationship between MLM loss and embedding quality.

### Mechanism 2: Four-Phase Curriculum Learning for CPT
Progressive domain adaptation through curriculum learning achieves comparable perplexity reduction to single-phase training while preserving general capabilities. Training progresses through four phases: general Turkish texts, legal content with domain terminology, long normative documents, and mixed-complexity refinement. A replay buffer from Phase 1 reinforces earlier knowledge.

### Mechanism 3: GISTEmbed False-Negative Filtering
Guided contrastive learning with cached guide models outperforms standard InfoNCE for Turkish legal retrieval. A guide model pre-filters in-batch negatives, removing false negatives where similarity exceeds a margin threshold. Pre-computed embeddings avoid repeated forward passes.

## Foundational Learning

- **Masked Language Modeling (MLM)**
  - Why needed here: Foundation objective for encoder pre-training. Paper uses 30% masking (ModernBERT/RobertA-style) not 15% (BERT-style). Understanding this distinction is critical for interpreting checkpoint dynamics.
  - Quick check question: Can you explain why 80-10-10 masking strategy reduces pre-training/fine-tuning discrepancy?

- **Contrastive Learning / InfoNCE Loss**
  - Why needed here: Core mechanism for post-training encoders into embedding models. Understanding temperature scaling, negative sampling, and false-negative filtering is essential for reproducing results.
  - Quick check question: What is the role of the temperature parameter τ in InfoNCE loss, and how does GISTEmbed modify the negative selection process?

- **Catastrophic Forgetting**
  - Why needed here: Primary risk in continual pre-training. Paper mitigates via curriculum learning and replay buffers; understanding failure modes prevents wasted training runs.
  - Quick check question: How does replay buffer usage in Phase 2 differ from standard EWC-based forgetting prevention?

## Architecture Onboarding

- **Component map:**
  Custom BPE tokenizer (59K vocab, Llama pre-tokenization) → ModernBERT-base/large (155M/403M params, RoPE, sliding window attention) → Post-training with GISTEmbed → Embedding model
  Qwen3-1.7B/4B (GQA, SwiGLU, RoPE) → Four-phase CPT curriculum → Optional decoder-to-encoder conversion

- **Critical path:**
  1. Tokenizer training on legal + web corpus (morphological integrity critical for Turkish)
  2. MLM pre-training with periodic downstream evaluation (not just loss monitoring)
  3. Post-training sequence length must match target corpus (256 tokens caused 23.6% regulation retrieval degradation)

- **Design tradeoffs:**
  Single-stage pre-training (cheaper) vs. multi-stage SOTA pipelines (better absolute performance)
  Curriculum learning (essential for small models) vs. single-phase (viable for large models)
  Decoder-to-encoder conversion: Resource-efficient but underperforms (4B model ranked 9th vs. 155M encoder ranked 2nd)

- **Failure signatures:**
  MLM loss decreasing but retrieval scores degrading → checkpoint too late, switch to earlier checkpoint
  Legal domain perplexity not improving after Phase 2 → check curriculum phase boundaries and data ratios
  Decoder conversion models underperforming → likely insufficient post-training data (need more than 920K samples)

- **First 3 experiments:**
  1. Reproduce tokenizer benchmark: Compare Turkish token count and purity metrics across tokenizers using the provided methodology
  2. Ablate sequence length: Train post-training with seq_len=256 vs. 2048 on legal retrieval subset to validate paper's 8.5% degradation finding
  3. Test curriculum phase boundaries: Skip Phase 1 (general Turkish) and compare perplexity trajectory to validate gradual adaptation hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can advanced multi-stage decoder-to-encoder conversion pipelines close the performance gap with purpose-built encoder models for Turkish legal retrieval?
- Basis in paper: [explicit] The conclusion states future work will "explore advanced decoder-to-encoder conversion techniques." The discussion notes converted models underperformed due to lacking the "sophisticated training infrastructure... unavailable in our implementation."
- Why unresolved: The current converted models (rank 9) trailed smaller encoder models (rank 2) because they relied on single-stage training on limited data (920K samples) versus SOTA approaches using large-scale synthetic data (150M+ samples).
- What evidence would resolve it: A study implementing synthetic data generation and multi-stage pipelines for the decoder models, followed by evaluation on the MTEB-Turkish benchmark.

### Open Question 2
- Question: Why does minimizing Masked Language Modeling (MLM) loss lead to performance degradation in downstream retrieval tasks for morphologically rich languages like Turkish?
- Basis in paper: [inferred] Section 4.1 highlights a "critical finding" where the v6 checkpoint achieved lower MLM loss than v5 but suffered significant regression in retrieval scores (-5.4% MTEB, -11.9% Legal).
- Why unresolved: The paper observes this non-linear relationship and suggests overfitting to pre-training objectives, but does not identify the precise theoretical mechanism or linguistic features driving the divergence.
- What evidence would resolve it: An analysis mapping specific morphological features or structural representations that degrade when MLM loss is minimized past an optimal threshold in agglutinative languages.

### Open Question 3
- Question: To what extent can increased model capacity compensate for the absence of complex curriculum learning strategies during domain adaptation?
- Basis in paper: [inferred] Section 4.4.3 notes the 4B model achieved comparable perplexity improvements with single-phase training as the 1.7B model with four-phase curriculum learning, suggesting "larger model capacity can compensate for the absence of curriculum structure."
- Why unresolved: It remains unclear if this efficiency trade-off holds for complex semantic tasks or reasoning capabilities without the gradual transition provided by curriculum learning phases.
- What evidence would resolve it: A comparative study evaluating single-phase vs. curriculum learning on the 4B scale using the Muhakim reward model to assess legal reasoning depth.

## Limitations

- The decoder-to-encoder conversion approach shows substantial performance gaps compared to purpose-built encoder models, with a 13-point Legal Score difference despite using fewer parameters
- The effectiveness of checkpoint selection may be specific to Turkish's agglutinative morphology rather than representing a general principle for morphologically rich languages
- Curriculum learning effectiveness varies significantly with model size, with unclear thresholds for when it becomes unnecessary versus essential

## Confidence

**High Confidence Claims:**
- Mecellem corpus construction methodology is well-documented and reproducible
- 36.2% perplexity reduction achieved through four-phase CPT on Qwen3-1.7B is directly measurable
- GISTEmbed false-negative filtering's 4.6% overall and 15.9% legal domain improvements are empirically demonstrated
- Checkpoint selection finding (optimal checkpoints before minimum loss) is supported by direct ablation

**Medium Confidence Claims:**
- Curriculum learning superiority over single-phase training for small models (parameter-size dependency not fully characterized)
- General applicability of checkpoint selection beyond Turkish legal retrieval (limited to one domain and language)
- Sufficiency of the 920K sample post-training corpus for achieving competitive performance

**Low Confidence Claims:**
- Mechanism's transferability to non-agglutinative languages (untested)
- Curriculum learning effectiveness for parameter sizes between 1.7B and 4B (gap in experimental validation)
- Long-term stability of decoder-to-encoder conversion models (no extended evaluation period reported)

## Next Checks

**Validation Check 1: Cross-Lingual Morphology Transfer**
Test the checkpoint selection mechanism on another morphologically rich language (Finnish or Hungarian) using the same ModernBERT architecture and downstream retrieval evaluation protocol. Compare whether the non-linear relationship between MLM loss and embedding quality persists, or if the mechanism is specific to Turkish agglutinative structures.

**Validation Check 2: Curriculum Learning Parameter Sweep**
Systematically vary the phase boundaries and data mixing ratios in the four-phase curriculum to identify optimal configurations for different parameter sizes (1B, 2B, 3B). Measure the point at which curriculum learning provides diminishing returns versus single-phase training, establishing clear guidelines for when this approach is necessary versus optional.

**Validation Check 3: Multi-Stage Decoder Conversion**
Implement a multi-stage decoder-to-encoder conversion pipeline matching the official Qwen3 methodology (150M synthetic samples + 7M labeled samples) and compare performance against the single-stage approach. Measure the performance gap reduction and determine whether the resource efficiency of conversion can be maintained while achieving encoder-level performance.