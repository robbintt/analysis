---
ver: rpa2
title: 'SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced
  Safety in LLM-based Robotic Task Planning'
arxiv_id: '2503.06892'
source_url: https://arxiv.org/abs/2503.06892
tags:
- task
- safety
- tasks
- safeplan
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafePlan, a multi-component framework that
  combines formal logic and chain-of-thought reasoning to enhance safety in LLM-based
  robotic task planning. The core idea is to use a Prompt Sanity Check COT Reasoner
  to filter unsafe natural language prompts, followed by Invariant COT Reasoners that
  generate and verify preconditions, postconditions, and invariants for task plans
  and code.
---

# SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning

## Quick Facts
- arXiv ID: 2503.06892
- Source URL: https://arxiv.org/abs/2503.06892
- Reference count: 21
- SafePlan achieves 90.5% reduction in harmful task acceptance while maintaining reasonable acceptance of safe tasks

## Executive Summary
SafePlan introduces a multi-component framework that enhances safety in LLM-based robotic task planning by combining formal logic verification with chain-of-thought reasoning. The system filters unsafe natural language prompts through a Prompt Sanity Check COT Reasoner before generating and verifying task plans and code using Invariant COT Reasoners. Evaluation using a benchmark of 621 expert-curated task prompts and AI2-THOR simulations demonstrates significant improvements in safety performance compared to baseline models.

## Method Summary
SafePlan employs a multi-stage approach where natural language prompts first pass through a Prompt Sanity Check COT Reasoner that filters unsafe instructions. Subsequently, Invariant COT Reasoners generate and verify preconditions, postconditions, and invariants for task plans and executable code. The framework integrates formal logic verification into the reasoning process, ensuring that generated plans adhere to safety constraints before execution. The system was evaluated using AI2-THOR simulations against a benchmark of 621 expert-curated task prompts.

## Key Results
- 90.5% reduction in harmful task acceptance compared to baseline models
- Maintained reasonable acceptance rate for safe tasks
- Outperformed baseline models in safety-critical task filtering

## Why This Works (Mechanism)
SafePlan's effectiveness stems from its multi-layered safety approach that combines prompt filtering with formal verification. The Prompt Sanity Check COT Reasoner acts as the first line of defense by identifying and rejecting unsafe natural language instructions before they can generate harmful plans. The Invariant COT Reasoners then apply formal logic to verify that generated task plans and executable code maintain necessary safety properties throughout execution. This combination addresses both input-level and execution-level safety concerns in LLM-based robotic planning systems.

## Foundational Learning
**Chain-of-Thought Reasoning**: Sequential reasoning approach that breaks down complex problems into intermediate steps, needed to ensure thorough safety analysis; quick check: verify reasoning traces for logical consistency
**Formal Logic Verification**: Mathematical methods for proving correctness of systems against specified properties, needed to provide rigorous safety guarantees; quick check: validate invariant generation against safety specifications
**Invariant Generation**: Automated creation of conditions that must hold true throughout system execution, needed to capture safety constraints; quick check: test invariants under various execution scenarios
**Prompt Filtering**: Initial screening of natural language instructions, needed to prevent unsafe tasks from entering the planning pipeline; quick check: measure false positive/negative rates
**Robotic Task Planning**: Process of converting high-level goals into executable action sequences, needed as the core application domain; quick check: validate plan executability in simulation

## Architecture Onboarding

**Component Map**: Prompt Sanity Check COT Reasoner -> Invariant COT Reasoners -> Task Plan Generation -> Code Generation -> Execution

**Critical Path**: User Prompt → Prompt Sanity Check COT Reasoner → Invariant COT Reasoners → Verified Task Plan → Safe Code Generation → Robotic Execution

**Design Tradeoffs**: Safety vs. task completion rate (conservative filtering may reject some safe tasks), computational overhead from multiple reasoning passes vs. safety guarantees, formal verification rigor vs. practical executability

**Failure Signatures**: False negatives (safe tasks rejected), false positives (unsafe tasks accepted), invariant generation failures, COT reasoning loops or inconsistencies, execution mismatches between verified plan and actual robot behavior

**3 First Experiments**:
1. Test prompt filtering with edge-case safety scenarios to measure false positive/negative rates
2. Verify invariant generation with known safety properties to assess formal logic correctness
3. Evaluate end-to-end latency with varying task complexity to understand performance impact

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to simulated environments (AI2-THOR) without real-world robotics deployment
- Safety improvements measured against relatively small benchmark of 621 curated prompts
- Latency implications from multiple COT reasoning passes not discussed
- Formal logic verification soundness depends on quality of LLM-generated invariants

## Confidence

**High Confidence**: Experimental results showing 90.5% reduction in harmful task acceptance are well-supported by reported benchmark and simulation setup

**Medium Confidence**: Claims about maintaining "reasonable acceptance of safe tasks" lack quantitative specificity regarding safety-completion trade-offs

**Medium Confidence**: Assertion that SafePlan "significantly outperforms baseline models" is supported but could benefit from more diverse baseline comparisons

## Next Checks
1. Deploy SafePlan on physical robot platforms in controlled real-world environments to assess performance under sensor noise and execution uncertainty
2. Expand evaluation to a more diverse task corpus (minimum 2,000 prompts) covering edge cases and safety-critical scenarios
3. Measure end-to-end latency and computational overhead introduced by the multi-stage COT reasoning pipeline for real-time applications