---
ver: rpa2
title: 'CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion'
arxiv_id: '2512.19535'
source_url: https://arxiv.org/abs/2512.19535
tags:
- casa
- image
- tokens
- text
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of token insertion
  in vision-language models (VLMs) when processing high-resolution images or long
  videos. The authors propose CASA (Cross-Attention via Self-Attention), a fusion
  mechanism that injects visual information through dedicated attention layers where
  text tokens attend to both image and text tokens in local windows.
---

# CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion

## Quick Facts
- **arXiv ID:** 2512.19535
- **Source URL:** https://arxiv.org/abs/2512.19535
- **Authors:** Moritz Böhle, Amélie Royer, Juliette Marrie, Edouard Grave, Patrick Pérez
- **Reference count:** 40
- **Primary result:** Proposes CASA fusion mechanism that matches token-insertion performance on VQA tasks while outperforming cross-attention, particularly for fine-grained visual understanding

## Executive Summary
This paper addresses the computational inefficiency of token insertion in vision-language models when processing high-resolution images or long videos. The authors propose CASA (Cross-Attention via Self-Attention), a fusion mechanism that injects visual information through dedicated attention layers where text tokens attend to both image and text tokens in local windows. CASA preserves text-to-text interactions, enabling implicit gating and avoiding the high memory costs of token insertion. When trained from scratch, CASA matches token-insertion models on general VQA tasks while outperforming cross-attention approaches, particularly on fine-grained visual tasks like chart and document understanding. When adapted from pretrained VLMs, CASA achieves similar performance with reduced memory usage. The method also enables real-time streaming video captioning with low latency and constant memory cost, outperforming token-insertion methods in long-horizon inference.

## Method Summary
CASA introduces a fusion mechanism that injects visual information into language models through dedicated attention layers without adding visual tokens to the LLM's vocabulary. Text tokens attend to both image and text tokens within local windows defined by image insertion points. The method supports three placement strategies: CASA⊕ (parallel with self-attention), CASA→ (before self-attention), and CASA∨ (replacing self-attention in subset of layers). CASA preserves text-to-text interactions, enabling implicit gating through attention softmax normalization. The asymmetric attention design uses image tokens only as keys and values, eliminating their propagation through FFN layers and KV cache. This enables constant memory for streaming inference while maintaining performance on both general VQA and fine-grained visual understanding tasks.

## Key Results
- CASA matches token-insertion performance on general VQA tasks while outperforming cross-attention by ~20 points
- On fine-grained visual tasks (ChartQA, DocVQA, InfographicVQA), CASA significantly outperforms cross-attention with only modest gaps to token insertion
- For streaming video captioning, CASA achieves constant memory usage and lower latency compared to token insertion methods
- When adapting from pretrained VLMs, CASA⊕ performs best, while CASA∨ requires careful layer placement (every 4 layers)

## Why This Works (Mechanism)

### Mechanism 1: Implicit Gating via Self-Attention in Cross-Attention Layers
Adding text-to-text self-attention within cross-attention layers provides an implicit gating mechanism that outperforms explicit gated cross-attention designs. In CASA layers, text tokens attend to both image tokens and other text tokens (including themselves). The attention softmax naturally balances contributions: query tokens show "attention-to-self" several orders of magnitude larger than attention to individual image tokens, regulating how visual information influences the textual stream without explicit gating parameters. The core assumption is that softmax normalization in attention provides sufficient capacity for the model to learn when to prioritize text context vs. visual information. Evidence shows ablating self-attention causes severe performance degradation across all benchmarks, far worse than masking random tokens, and visualizations confirm dominant attention-to-self patterns.

### Mechanism 2: Local Window Attention Bounded by Image Insertion Points
Restricting cross-attention to local windows (delimited by image occurrences) maintains text coherence while avoiding the quadratic cost of full token insertion. Each CASA window contains one or more consecutive images followed by associated text. Text tokens only attend to images and text within their window, reducing attention complexity from O(T+N)² to O(max(T_window + N, N²)) where T_window ≪ T. The core assumption is that visual grounding for a text segment primarily depends on nearby images, not distant images in the conversation. This design choice enables efficient streaming but may struggle with tasks requiring cross-image reasoning.

### Mechanism 3: Asymmetric Attention with Image Tokens as Keys/Values Only
Using image tokens only as keys and values (never queries) eliminates their propagation through FFNs and KV cache, enabling constant memory for streaming. Unlike token insertion where image tokens become queries in self-attention, CASA's cross-attention only uses text tokens as queries. This means image tokens skip FFN layers entirely and don't accumulate in the LLM's KV cache during autoregressive generation. The core assumption is that visual representations from the encoder are sufficiently rich that they don't require iterative refinement through the LLM's FFN layers. Memory cost comparisons show CASA⊕ at 40.1GB vs. insertion at 52.4GB, and streaming tests show memory and latency remain nearly constant as frame count increases.

## Foundational Learning

- **Cross-Attention vs. Token Insertion Fusion**: CASA bridges the performance gap between these two paradigms; understanding their tradeoffs is essential. Quick check: In token insertion, where do image tokens appear in the attention computation? In cross-attention, what are the queries and what are the keys/values?

- **Attention Softmax as a Normalization/Gating Mechanism**: CASA's core insight is that the softmax over concatenated [image, text] tokens provides implicit gating without learnable gate parameters. Quick check: If a query token has attention scores [0.1, 0.1, 0.8] to [image_1, image_2, itself], what is the effective weight given to visual information?

- **Blockwise/Windowed Attention (FlashAttention)**: CASA's training efficiency relies on FlashAttention's blockwise implementation to handle local windows without materializing full attention matrices. Quick check: How does blockwise attention reduce memory compared to standard attention, and what constraint does it place on attention patterns?

## Architecture Onboarding

- **Component map:** Input: Text tokens + Image tokens (from Qwen2.5-VL encoder) → For each transformer layer: Standard Self-Attention (text-only, global causal) → CASA Layer (one of three placements: CASA⊕ parallel, CASA→ before, CASA∨ replaces) → FFN (text tokens only) → Output: Text embeddings with fused visual information

- **Critical path:** 1) Initialize CASA layers from backbone's self-attention weights 2) Define attention windows using image insertion points as delimiters 3) Implement asymmetric attention: text queries attend to [images + text in window], but images are never queries 4) Use FlashAttention-2's blockwise attention with bottom-right aligned masks (image must be at window start)

- **Design tradeoffs:** CASA⊕ vs. CASA→ vs. CASA∨: ⊕ performs best for adapting frozen VLMs; ∨ is most efficient but requires careful layer placement (every 4th layer works best, uniformly distributed). Window scope: larger windows = more context but higher compute; paper uses image occurrence as natural boundary. Image encoder finetuning: finetuning last 4 encoder layers helps but increases memory; freezing is viable for video.

- **Failure signatures:** Performance degrades to standard cross-attention levels if text-to-text attention is ablated in CASA layers. CASA∨ performs poorly if placed in too many layers (every 2 blocks drops to 53.5 avg). Training breaks causality if images are placed mid-window (FlashAttention-2 alignment constraint).

- **First 3 experiments:** 1) Sanity check: Train minimal CASA⊕ model (2-3 layers) on small image-text dataset. Verify attention patterns show dominant self-attention scores vs. image attention scores. 2) Ablation: Compare vanilla cross-attention vs. CASA on fine-grained benchmark (ChartQA or DocVQA). Expect 20-30 point gap. 3) Streaming stress test: Run inference on 60-second video at 2fps. Plot memory usage over time. CASA should show near-constant memory; if memory grows linearly, image tokens may be leaking into KV cache.

## Open Questions the Paper Calls Out

### Open Question 1
Can combining CASA with explicit token compression techniques yield further efficiency gains without sacrificing performance on fine-grained visual tasks? The authors note that token compression is orthogonal to CASA and both could be combined for particularly constrained memory or compute budgets. This remains unresolved as the paper explores token compression and CASA independently without investigating their joint application.

### Open Question 2
What specific mechanisms cause the remaining performance gap between CASA and full token insertion on chart and infographic understanding tasks? While CASA reduces the gap, the paper does not fully explain why chart/infographic tasks remain more challenging, reporting an average drop of 7 percent for CASA → relative to token insertion on InfoVQA and ChartVQA.

### Open Question 3
Does the implicit gating mechanism in CASA generalize effectively to multi-image and interleaved image-text scenarios with varying image placements? The current training implementation assumes images at specific positions with a footnote mentioning causality constraints when images are not at window beginnings, leaving behavior with arbitrary image placements unclear.

## Limitations

- **Local window constraint**: CASA's reliance on local windows introduces a fundamental tradeoff between efficiency and global context, potentially struggling with tasks requiring cross-image reasoning or long-range visual dependencies.

- **Uneven performance gains**: While substantial on general VQA tasks, performance gains over cross-attention are more modest on specialized benchmarks like ChartQA and DocVQA (typically 5-15 points), suggesting incomplete resolution of architectural limitations.

- **Implicit gating transparency**: The mechanism lacks transparency - the paper shows text-to-self attention dominates but doesn't prove nuanced gating versus simple token ignoring, with ablation studies showing necessity but not functional mechanism.

## Confidence

- **High Confidence**: Memory efficiency claims for streaming inference - the asymmetric attention design directly eliminates image token propagation through FFN layers and KV cache, which is a concrete architectural constraint with measurable effects.

- **Medium Confidence**: Core hypothesis that self-attention in cross-attention layers provides implicit gating - while ablation studies show self-attention is necessary for good performance, evidence for it functioning as "implicit gating mechanism" versus providing additional representational capacity is circumstantial.

- **Low Confidence**: Claim that CASA "outperforms cross-attention approaches" on general VQA tasks - the comparison is primarily against a single baseline and the 20-point gap may reflect implementation differences rather than fundamental architectural advantages.

## Next Checks

1. **Cross-image reasoning evaluation**: Design a benchmark requiring visual comparisons across non-adjacent images (e.g., "Compare the number of people in frame 1 versus frame 5"). Test whether CASA's local window approach degrades performance compared to token insertion, and quantify the tradeoff between efficiency and reasoning capability.

2. **Attention pattern interpretability study**: Beyond aggregate statistics, visualize attention distributions for specific queries that should and should not attend to visual information. Verify that CASA's implicit gating produces meaningful patterns (image tokens receive attention when relevant to the text query, and self-attention dominates otherwise) rather than random or uniform distributions.

3. **Layer-wise contribution analysis**: Conduct a systematic study varying CASA∨ placement frequency and measuring both performance and memory impact to clarify whether benefits come from specific layers chosen or cumulative effect of multiple CASA layers throughout the network.