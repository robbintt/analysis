---
ver: rpa2
title: 'Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces'
arxiv_id: '2506.06557'
source_url: https://arxiv.org/abs/2506.06557
tags:
- search
- nearest
- neighbor
- spaces
- q-metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Infinity Search proposes a novel method for approximate nearest
  neighbor search in high-dimensional vector spaces. The key insight is that ultrametric
  spaces, which satisfy the strong triangle inequality, enable logarithmic search
  complexity with VP trees.
---

# Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces

## Quick Facts
- arXiv ID: 2506.06557
- Source URL: https://arxiv.org/abs/2506.06557
- Reference count: 40
- Primary result: Achieves logarithmic search complexity for ANN by projecting into q-metric spaces while maintaining high recall (often >0.9) across diverse datasets.

## Executive Summary
Infinity Search proposes a novel method for approximate nearest neighbor search in high-dimensional vector spaces by exploiting ultrametric properties. The key insight is that ultrametric spaces enable logarithmic search complexity with VP trees, but real-world data rarely satisfies this property. The authors develop a projection operator that maps arbitrary dissimilarity functions into q-metric spaces while preserving nearest neighbors, and learn an approximation that embeds vectors into Euclidean space where distances estimate q-metric distances. Experiments demonstrate competitive performance against state-of-the-art methods on multiple datasets including text and image embeddings, with speedups of up to three orders of magnitude while maintaining high recall.

## Method Summary
Infinity Search operates in three stages: (1) Offline, it computes a canonical projection that transforms the original distance matrix into a q-metric space using shortest-path algorithms, optionally restricted to a k-NN graph to reduce complexity; (2) It trains an MLP to approximate these projected distances in Euclidean space, minimizing stress between learned and true q-metric distances; (3) Online, it embeds query vectors using the trained MLP and performs VP-tree search with q-metric-specific pruning rules. The method can optionally use a two-stage approach where broad search uses the q-metric approximation followed by re-ranking with the original dissimilarity function.

## Key Results
- Achieves logarithmic search complexity through ultrametric properties when q is sufficiently high
- Maintains recall above 0.9 for moderate q values (q=10) while significantly reducing comparisons
- Demonstrates state-of-the-art performance on high-dimensional sparse data with non-Euclidean dissimilarities like Jaccard similarity
- Provides up to three orders of magnitude speedup compared to exact nearest neighbor search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Search complexity becomes logarithmic when dissimilarities satisfy the strong triangle inequality (ultrametric).
- Mechanism: In ∞-metric spaces, VP-tree pruning conditions become mutually exclusive—every query satisfies exactly one of (∞-CI) or (∞-CO)—guaranteeing single-branch traversal per level.
- Core assumption: The tree is approximately balanced (median-based radius selection).
- Evidence anchors: [Theorem 1]: "The number of comparisons c(x₀) needed to find a nearest neighbor... is bounded by the depth h(T(X)) of the VP tree." [Section 2.3]: Conditions (8) are mutually exclusive; empirical depth ~⌈log₂ m⌉ (Figure 2).

### Mechanism 2
- Claim: The canonical projection P★_q preserves nearest neighbors while enforcing q-triangle inequality.
- Mechanism: P★_q computes shortest q-norm paths; by Axioms (A1) and (A2), this uniquely produces a q-metric that cannot reduce any dissimilarity below the original nearest-neighbor distance.
- Core assumption: Dissimilarities are nonnegative and symmetric.
- Evidence anchors: [Proposition 1]: "ˆx₀ ≡ argmin_x E(x, x₀) ⊆ argmin_x E_q(x, x₀)" with equality for q < ∞. [Section 3]: Canonical projection defined via q-norm shortest paths; satisfies axioms by Theorem 2.

### Mechanism 3
- Claim: Learned embeddings approximate q-metric distances efficiently for queries without computing all-pairs shortest paths.
- Mechanism: An MLP minimizes stress ℓ_D between ‖Φ(x;θ) − Φ(y;θ)‖₂ and true projected distances D_q(x,y); trained on dataset samples, it generalizes inductively to queries.
- Core assumption: The learned map transfers to out-of-sample query points.
- Evidence anchors: [Section 4]: "θ* = argmin_θ Σ_{x,y∈X} ℓ_D(x,y)" with ℓ_D as squared error. [Figure 4]: Learned approximation reduces comparisons monotonically with q while maintaining recall > 0.9 for moderate q.

## Foundational Learning

- Concept: **Triangle inequality variants (standard, q-metric, ultrametric)**
  - Why needed here: The entire method exploits progressively stronger inequalities for better pruning.
  - Quick check question: For d_q(x,y) ≤ d_q(x,z) + d_q(y,z), what happens as q → ∞?

- Concept: **VP-tree construction and search**
  - Why needed here: The index structure; vantage points partition space into inside/outside sets.
  - Quick check question: Why does median radius selection promote balanced trees?

- Concept: **Shortest-path matrix algorithms (Floyd-Warshall style)**
  - Why needed here: Canonical projection computes all-pairs q-norm shortest paths.
  - Quick check question: What is the O(n³) bottleneck that motivates learned approximations?

## Architecture Onboarding

- Component map: Sample subset → Canonical projection P★_q (Algorithms 4-7) → Train MLP Φ(·;θ) → Transform full dataset → Build VP-tree (Algorithm 1) → Query x₀ → Φ(x₀; θ*) → VP-tree search (Algorithm 2/3) → Return candidate(s)

- Critical path: The MLP training quality determines whether theoretical speedups materialize; projection approximation error directly impacts recall.

- Design tradeoffs:
  - Higher q → fewer comparisons but lower recall (spurious neighbors at q=∞)
  - Larger K in two-stage → better accuracy but slower (breaks log bound)
  - k-NN graph sparsity (parameter k in Algorithms 6-7) → faster projection but potential distortion

- Failure signatures:
  - Recall collapses at q=∞ → spurious equidistant neighbors; use two-stage search
  - Build time explodes → restrict to k-nearest-neighbor graph for projection
  - Poor generalization to queries → training set too small or not representative

- First 3 experiments:
  1. Validate canonical projection: On 1K points, sweep q ∈ {1, 2, 5, 10, ∞}, measure comparisons vs. recall (replicate Figure 3).
  2. Test learned approximation: Train MLP on 10K Fashion-MNIST, evaluate stress ℓ_D and q-triangle violation ℓ_T (replicate Figure 16).
  3. Benchmark end-to-end: Run ANN-Benchmarks comparison on Fashion-MNIST and Kosarak at k=1,10; confirm Pareto dominance (replicate Figure 6).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the canonical projection be modified to prevent the introduction of spurious equidistant neighbors at $q=\infty$ while retaining the logarithmic search bound?
- Basis in paper: [explicit] Page 7 notes that at $q=\infty$, "the projection may introduce spurious optima not present in the original nearest neighbor set, thereby affecting accuracy." Example 1 (Page 19) demonstrates this phenomenon.
- Why unresolved: The authors currently rely on a two-stage search (Appendix F.5) to filter these spurious candidates, addressing the symptom rather than solving the projection's tendency to distort the uniqueness of the nearest neighbor.
- What evidence would resolve it: A formal proof identifying conditions under which the projection preserves nearest neighbor uniqueness, or a modified projection operator that strictly enforces inequality among distinct points.

### Open Question 2
- Question: What are the theoretical generalization bounds for the learned embedding $\Phi_q$ when the distribution of query vectors diverges from the fixed training set used for the projection?
- Basis in paper: [explicit] Appendix F.6 observes "degradation when applying a model trained on 100K points to 1M–5M points, consistent with inference mismatch," and identifies "enabling lightweight incremental updates" as future work.
- Why unresolved: While the inductive setting (fixed training set) is convenient, the paper does not analyze how domain shift or density variations in larger datasets impact the accuracy of the approximated q-metric distances.
- What evidence would resolve it: Empirical validation on billion-scale datasets where the training set is a small fraction of the total, or theoretical bounds on the error of the learned projection relative to the canonical projection.

### Open Question 3
- Question: Is minimizing the stress loss ($\ell_D$) sufficient to guarantee efficient VP-tree pruning, or is there a threshold of $q$-triangle inequality violation ($\ell_T$) that significantly degrades search complexity?
- Basis in paper: [inferred] Appendix F.3 states that the stress $\ell_D$ is more correlated with performance than the explicit violation $\ell_T$, raising the question of whether enforcing the geometric constraint is redundant or strictly necessary for the tree's structural integrity.
- Why unresolved: The paper demonstrates that the learned approximation works empirically, but does not isolate whether the failure to strictly satisfy the $q$-triangle inequality limits the theoretical logarithmic complexity in practice.
- What evidence would resolve it: An ablation study comparing the number of node visits (comparisons) for embeddings trained solely with $\ell_D$ versus those trained with strict $q$-metric constraints.

## Limitations
- Canonical projection introduces spurious equidistant neighbors at q=∞ that require two-stage search to resolve
- Computational cost of all-pairs shortest paths motivates sparse approximations that may distort distances
- No empirical validation of nearest neighbor preservation under canonical projection across diverse datasets
- Generalization of learned MLP approximations to out-of-sample queries is assumed rather than rigorously established

## Confidence
- High confidence: The core mechanism of using ultrametric properties for logarithmic VP-tree search is well-established (Theorem 1, Section 2.3).
- Medium confidence: The canonical projection preserves nearest neighbors (Proposition 1) but lacks empirical validation across diverse datasets.
- Medium confidence: Learned approximations work effectively (Figure 4 shows monotonic improvement), though generalization to queries is assumed.
- Low confidence: The claim of "state-of-the-art" performance requires context—the paper dominates on high-dimensional sparse data but not necessarily on all metric spaces.

## Next Checks
1. **NN Preservation Validation:** For synthetic and real datasets (e.g., 1K Fashion-MNIST points), sweep q values and empirically verify that P★_q preserves nearest neighbors compared to original distances.
2. **Query Generalization Test:** Train MLP on 10K points, then measure stress ℓ_D and q-triangle violation ℓ_T on held-out query points (not in training set) across q ∈ {2, 5, 10, 20}.
3. **Failure Mode Analysis:** Systematically test recall degradation at high q by varying dataset density and dimensionality—identify when two-stage search becomes necessary versus single-stage.