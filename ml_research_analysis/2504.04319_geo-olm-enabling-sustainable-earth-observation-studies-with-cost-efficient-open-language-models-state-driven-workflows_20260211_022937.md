---
ver: rpa2
title: 'Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient
  Open Language Models & State-Driven Workflows'
arxiv_id: '2504.04319'
source_url: https://arxiv.org/abs/2504.04319
tags:
- geospatial
- geo-olm
- arxiv
- agentic
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the unsustainable cost of using large-scale
  proprietary models like GPT-4o for geospatial AI Copilots, which are often employed
  in Earth observation and climate studies. The authors propose Geo-OLM, a geospatial
  agent framework that leverages state-driven LLM reasoning to decouple task progression
  from tool calling, enabling smaller open language models to perform effectively.
---

# Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient Open Language Models & State-Driven Workflows

## Quick Facts
- **arXiv ID**: 2504.04319
- **Source URL**: https://arxiv.org/abs/2504.04319
- **Reference count**: 40
- **Key outcome**: Geo-OLM achieves performance within 10% of GPT-4o while reducing inference costs by two orders of magnitude, from $500-$1000 to under $10 for 2K queries.

## Executive Summary
This paper addresses the unsustainable costs of using proprietary models like GPT-4o for geospatial AI Copilots in Earth observation and climate studies. The authors propose Geo-OLM, a geospatial agent framework that leverages state-driven LLM reasoning to decouple task progression from tool calling. By structuring workflows as discrete states and transitions, Geo-OLM reduces the reasoning burden on low-resource open language models. The results show that Geo-OLM achieves performance within 10% of GPT-4o while reducing inference costs by two orders of magnitude, from $500-$1000 to under $10. On benchmarks, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates when downsizing to models below 7B parameters.

## Method Summary
Geo-OLM is a geospatial agent framework that implements state-driven LLM reasoning for Earth observation workflows. The method structures tasks as a finite state machine with states including Init, Load, Filter, Detect, Map, and End, plus an Error state with self-reflection. The framework uses explicit state transitions to reduce decision complexity for smaller models, requiring specific outputs like `CURRENT_STAGE` and `TERMINATE` tokens with validation. It employs AutoGen for conversation management and Ollama for local inference of quantized open language models (Qwen-2.5-7B/14B by default). The system processes geospatial data including satellite imagery (xView1, Sentinel-2) and climate datasets (MODIS Terra), evaluating performance on GeoLLM-Engine and GeoLLM-Squad benchmarks across 2K queries.

## Key Results
- Geo-OLM achieves performance within 10% of GPT-4o while reducing inference costs by two orders of magnitude, from $500-$1000 to under $10 for 2K queries.
- Outperforms strongest prior geospatial baselines by 32.8% in successful query completion rates when downsizing to models below 7B parameters.
- Success rates range from 63.3% (Qwen-2.5-7B) to 73.6% (GPT-4o) on the GeoLLM-Engine benchmark.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Constraining the decision space via explicit states may reduce tool-selection errors in smaller models.
- **Mechanism**: The architecture structures workflows as a sequence of discrete states (e.g., Init → Load → Filter). By binding specific tools to specific states (e.g., only offering database tools during the "Load" state), the system reduces the number of choices the OLM must evaluate at any single inference step.
- **Core assumption**: Geospatial tasks possess an inherent sequential dependency that can be pre-mapped to a static state graph.
- **Evidence anchors**:
  - [abstract]: "structur[ing] workflows as discrete states... reduces the reasoning burden on low-resource OLMs."
  - [section 3.2]: "This selective prompting significantly reduces decision complexity... beneficial for smaller OLMs which might struggle when presented with a large set of tool choices."
  - [corpus]: Corpus papers confirm that geospatial workflows often suffer from complexity in tool coordination, but do not validate the specific state-reduction mechanism of Geo-OLM directly.
- **Break condition**: Tasks requiring non-linear reasoning or parallel tool usage that violate the pre-defined sequential state graph.

### Mechanism 2
- **Claim**: Decoupling error recovery from the main generation loop likely prevents premature task termination.
- **Mechanism**: Instead of relying on the OLM to self-correct within a continuous chat history, the system detects execution errors programmatically and forces a transition to a dedicated "Error" state. This triggers a standalone "SELF-REFLECT" prompt, isolating the debugging logic from the flow of the main task.
- **Core assumption**: Small models struggle to distinguish between task completion and error states when both appear in the same context window.
- **Evidence anchors**:
  - [section 3.2]: "state-driven reasoning enables programmatic transitions to the Error state... prompt[ing] the model to reconsider its previous actions."
  - [section 6]: "lower success rates in existing works primarily result from models exiting prematurely... [Geo-OLM] helps prevent premature exits."
  - [corpus]: Related work (GeoLLM-Squad) highlights orchestration challenges, but specific error-state isolation data is unique to this paper.
- **Break condition**: Cascading errors where the "Self-Reflect" context window lacks sufficient information to debug the root cause.

### Mechanism 3
- **Claim**: Explicit "termination safeguards" may reduce hallucinated task completion.
- **Mechanism**: The system requires the LLM to output a specific `TERMINATE` token, which triggers a secondary validation step where the model must summarize and confirm completion. This forces a "double-check" mechanism that counters the tendency of small models to hallucinate success.
- **Core assumption**: OLMs are prone to claiming success prematurely to minimize generation effort.
- **Evidence anchors**:
  - [section 3.2]: "This mechanism helps prevent premature exits or infinite loops due to incorrect state reasoning."
  - [results]: Geo-OLM improves success rates by 32.8% over baselines when downsized to <7B parameters.
  - [corpus]: N/A (Mechanism specific to this architecture).
- **Break condition**: User queries that are genuinely simple and short, where the validation step adds unnecessary latency.

## Foundational Learning

- **Concept**: **Finite State Machines (FSM)**
  - **Why needed here**: The core Geo-OLM architecture models the agent lifecycle as a tuple $\langle S, \delta, \Gamma \rangle$ (States, Transitions, Context). You cannot debug the prompt flow without understanding state transitions.
  - **Quick check question**: If an SQL query fails in the "Load" state, does the system transition to "Filter" or "Error"?

- **Concept**: **Tool-Augmented LLMs (Function Calling)**
  - **Why needed here**: The paper optimizes how an LLM selects external APIs (tools). You need to understand the difference between the LLM generating text vs. generating a structured JSON function call.
  - **Quick check question**: How does the agent know if a tool execution failed vs. returned a null result?

- **Concept**: **Model Quantization & Inference Costs**
  - **Why needed here**: The paper's value proposition rests on running 7B-14B parameter models (OLMs) instead of proprietary ones. Understanding Q4 quantization (used in Ollama) vs. FP16 is critical for replicating their cost savings.
  - **Quick check question**: Why might a Q4 quantized model fail at complex JSON formatting more often than a full-precision API model?

## Architecture Onboarding

- **Component map**: User Query → **Init State** (LLM classifies intent) → **Tool Selection** (LLM picks tool based on state) → **Tool Execution** (Sandbox) → **Transition Logic** (Check for Error/Success) → **Next State Prompt**

- **Critical path**: User Query → **Init State** (LLM classifies intent) → **Tool Selection** (LLM picks tool based on state) → **Tool Execution** (Sandbox) → **Transition Logic** (Check for Error/Success) → **Next State Prompt**

- **Design tradeoffs**:
  - **Rigidity vs. Reliability**: Defining strict states prevents the model from "freestyling," which helps small models but might fail on ambiguous queries.
  - **Cost vs. Correctness**: The paper notes that lower correctness rates increase token usage due to retries; optimizing for the cheapest model might increase latency.

- **Failure signatures**:
  - **Infinite Loops**: The model repeats `CURRENT_STAGE = Filter` without progressing.
  - **Syntax Errors**: The OLM generates malformed JSON for tool calls (e.g., `startdate` vs `start_date`).
  - **Premature Termination**: The model outputs `TERMINATE` before executing the final "Map" visualization.

- **First 3 experiments**:
  1. **Baseline Ablation**: Run the GeoLLM-Engine benchmark with a standard ReAct prompt on Qwen-7B vs. the State-Driven prompt to verify the 30%+ performance gap.
  2. **Error State Validation**: Intentionally inject a tool error (e.g., invalid SQL credentials) and verify if the agent successfully transitions to the "Error" state and attempts a self-correction.
  3. **Token Analysis**: Profile a 100-query batch to compare token usage of the "Orchestrator" baseline vs. Geo-OLM to confirm the cost reduction claims.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does improving OLM performance through state-driven prompting come at the cost of generalizability beyond predefined benchmarks?
- **Basis in paper**: [explicit] The authors explicitly raise this in Section 7: "A possible limitation of state-driven prompting is its reliance on explicitly defined workflow states, raising an important question: does improving OLM performance come at the cost of generalizability?"
- **Why unresolved**: Only one real-world case study (Turkey earthquake) was tested, and workflows were manually adapted to match that study's methodology. It remains unclear whether the approach generalizes to novel workflows without manual state definition.
- **What evidence would resolve it**: Evaluation across diverse, previously unseen EO workflows without hand-tuning state definitions, or demonstrating automated state discovery from task descriptions.

### Open Question 2
- **Question**: Why does the self-reflection mechanism improve some models but cause performance degradation in others (e.g., LLaMA-70B abandoning tasks prematurely)?
- **Basis in paper**: [inferred] The paper observes: "for LLaMA 70B, we observe that adding more logic sometimes has the opposite effect, especially with the self-reflection mechanism which surprisingly causes the model to question itself and abandon tasks prematurely." This counterintuitive finding is reported but not explained.
- **Why unresolved**: The authors note this paradox but do not investigate whether it stems from model architecture, training data, or interaction between chat-based APIs and specific model families.
- **What evidence would resolve it**: Ablation studies isolating self-reflection across model families with analysis of the generated reasoning chains to identify when and why models self-abort.

### Open Question 3
- **Question**: Can improving tool-selection correctness directly reduce token overhead and inference latency?
- **Basis in paper**: [inferred] The paper states: "We observe a correlation between lower correctness rates and higher per-task token usage. As motivation for future work, we hypothesize that improving reasoning for better correctness rates (e.g., tool selection) would reduce token overhead."
- **Why unresolved**: Correlation is observed, but causation is not established. It is unclear whether better tool selection would reduce retries enough to meaningfully lower costs and latencies.
- **What evidence would resolve it**: Controlled experiments comparing token usage before and after targeted improvements to tool-selection accuracy, measuring both retry reduction and overall cost savings.

## Limitations

- **State rigidity limitations**: The state-driven approach may struggle with tasks requiring parallel tool execution or non-linear reasoning paths that violate the pre-defined sequential state graph.
- **Model family sensitivity**: SELF-REFLECT mechanism shows variable effectiveness across model families, with LLaMA models performing significantly worse when error recovery is enabled.
- **Domain generalizability**: Evaluation focuses on specific geospatial benchmarks, leaving open questions about generalizability to other domains or more complex multi-modal tasks.

## Confidence

- **High confidence**: The core observation that structuring workflows as discrete states reduces decision complexity for smaller OLMs, supported by direct performance improvements and clear mechanistic explanation.
- **Medium confidence**: The absolute cost savings figures, as these depend on specific quantization settings and local inference setups that may vary in practice.
- **Low confidence**: The robustness of error recovery across all model families, given the significant performance degradation observed with LLaMA models under the SELF-REFLECT mechanism.

## Next Checks

1. Test Geo-OLM with models requiring parallel tool execution (e.g., simultaneous data loading and preprocessing) to assess state-graph rigidity limitations.

2. Implement cost benchmarking across different quantization levels (Q3, Q4, Q5) to verify sensitivity of cost savings to quantization settings.

3. Conduct cross-domain validation by applying Geo-OLM architecture to non-geospatial sequential workflows (e.g., document processing) to test generalizability of the state-driven approach.