---
ver: rpa2
title: Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM
arxiv_id: '2505.05772'
source_url: https://arxiv.org/abs/2505.05772
tags:
- attention
- tokens
- memory
- decoding
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes STARC, a clustering-based data mapping scheme
  for efficient LLM decoding on PIM architectures. The key idea is to cluster KV pairs
  by semantic similarity and map them to contiguous memory regions, enabling selective
  attention and parallel processing without frequent reclustering.
---

# Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM

## Quick Facts
- arXiv ID: 2505.05772
- Source URL: https://arxiv.org/abs/2505.05772
- Reference count: 40
- Key outcome: STARC reduces attention-layer latency by 19%-31% and energy by 19%-27% vs. token-wise sparsity methods on HBM-PIM

## Executive Summary
This paper proposes STARC, a clustering-based data remapping scheme for efficient LLM decoding on Processing-in-Memory (PIM) architectures. The key insight is that by clustering semantically similar KV pairs and mapping them to contiguous memory regions aligned with PIM row granularity, STARC enables selective attention with minimal over-fetch and reduced row activations. Experiments on HBM-PIM using the AttAcc simulator show STARC achieves significant latency and energy reductions while maintaining model accuracy comparable to state-of-the-art sparse attention methods.

## Method Summary
STARC introduces a clustering-based remapping mechanism that groups KV pairs by semantic similarity using online K-means clustering on key vectors. After initial clustering during prefill, newly generated tokens are incrementally clustered every 128 decoding steps without modifying existing clusters. The query vector matches against precomputed cluster centroids to select top-ranked clusters for attention computation. This approach aligns sparse attention patterns with PIM row-level access granularity, reducing over-fetch from irrelevant tokens scattered across memory rows.

## Key Results
- Reduces attention-layer latency by 19%-31% and energy consumption by 19%-27% compared to token-wise sparsity methods
- Achieves up to 54%-74% latency reduction and 45%-67% energy reduction compared to full KV cache retrieval
- Maintains model accuracy comparable to state-of-the-art sparse attention methods on LongBench benchmark

## Why This Works (Mechanism)

### Mechanism 1
Clustering semantically similar KV pairs improves hardware utilization under sparse attention on PIM architectures. STARC groups key vectors by cosine similarity using K-means clustering and co-locates each cluster's KV pairs in contiguous memory rows. This alignment means when a row is activated, most or all fetched tokens contribute meaningfully to attention, reducing over-fetch from irrelevant tokens scattered across rows. The core assumption is that key vectors of semantically related tokens exhibit sufficient clustering structure that can be captured by online K-means, and this structure persists across decoding steps without frequent reclustering.

### Mechanism 2
Query-to-centroid matching enables selective attention at cluster granularity with bounded retrieval cost. During decoding, the query vector is compared to precomputed cluster centroids via dot product. Top-ranked clusters are selected until the token budget is met, and the final cluster may be partially truncated. This avoids per-token scoring overhead while maintaining retrieval quality. The core assumption is that centroid similarity correlates sufficiently with individual token relevance to achieve recall comparable to token-wise sparsity methods.

### Mechanism 3
Incremental clustering with fixed clusters avoids data movement overhead while accommodating distribution shift between prefill and decoding. Initial clustering occurs after prefill. New decoding tokens are accumulated and clustered every 128 steps without modifying existing clusters. This avoids costly row-level data movement in PIM while handling the observation that prefill and decoding key distributions diverge. The core assumption is that distribution shift is gradual enough that fixed clusters remain useful, and newly generated tokens exhibit local semantic coherence.

## Foundational Learning

- **PIM row-level access granularity**: Why needed here - PIM units activate entire memory rows; understanding this constraint is essential to see why token-wise sparsity causes over-fetching and why STARC's clustering alignment matters. Quick check: If a PIM row stores 8 KV vectors and only 2 are relevant, what fraction of fetched data is wasted?

- **KV cache sparsity in LLM decoding**: Why needed here - Sparse attention methods assume only a subset of past tokens contribute significantly; STARC exploits this but requires understanding the trade-off between retrieval granularity and accuracy. Quick check: Under a budget of 1024 tokens and context length 32K, what sparsity ratio does this represent?

- **K-means clustering with cosine similarity**: Why needed here - STARC uses K-means with cosine distance on key vectors; understanding clustering quality and convergence informs why 15 iterations suffice and how cluster count (sequence_length/32) balances granularity vs. overhead. Quick check: Why might cosine similarity be preferred over Euclidean distance for clustering key vectors in attention?

## Architecture Onboarding

- **Component map**: Prefill stage → initial K-means clustering → write clusters to contiguous PIM rows → Decoding stage: new tokens accumulate in buffer → every 128 steps cluster buffer and append → query compares against all centroids to select clusters → activate corresponding rows → compute attention

- **Critical path**: 1) Prefill → initial K-means clustering → write clusters to contiguous PIM rows 2) Each decoding step: generate new KV pair → buffer → (every 128 steps) cluster buffer and append 3) Query → dot product with all centroids → select top clusters → activate corresponding rows → compute attention

- **Design tradeoffs**: Cluster size (~32 tokens) - larger clusters reduce centroid comparison overhead but increase over-fetch within selected clusters; smaller clusters improve retrieval precision but raise clustering cost. Reclustering interval (128 steps) - more frequent reclustering captures distribution drift but adds latency; less frequent risks stale clusters. Prefill vs. decoding separation - clustering them separately handles distribution shift but may fragment semantic clusters spanning both phases.

- **Failure signatures**: Accuracy drop below baselines - centroid similarity may not correlate with attention weights; check recall rates (Fig. 10) and adjust cluster count or distance metric. No latency improvement - clusters may not align with row boundaries; verify cluster-to-row mapping and bank-level workload balance. High clustering overhead - K-means iterations or cluster count may be too high for online inference; profile clustering time relative to attention computation.

- **First 3 experiments**: 1) Cluster granularity sweep: Vary cluster size (16, 32, 64, 128 tokens) on LongBench datasets; measure accuracy vs. latency trade-off to validate the sequence_length/32 heuristic. 2) Recall vs. budget curve: Plot recall rate (Fig. 10) across KV cache budgets (256-2048) for STARC vs. Quest, InfiniGen, SparQ; confirm STARC bridges token-wise and page-wise recall. 3) Row activation profiling: Using AttAcc simulator, measure row activations per decoding step for STARC vs. token-wise sparsity; quantify over-fetch reduction and correlate with energy savings (Fig. 12).

## Open Questions the Paper Calls Out

- **Open Question 1**: How does STARC scale to context lengths beyond 8K tokens, particularly for 128K+ contexts that modern LLMs increasingly support? The evaluation only covers decoding lengths up to 8192 tokens, while the paper claims to target "long-context LLM inference" and the base model (LongChat-7B-v1.5-32K) supports 32K context.

- **Open Question 2**: What is the optimal cluster size/number, and how does it vary across model architectures, tasks, and sparsity levels? The cluster count is set to sequence_length/32 as a practical tradeoff, but the paper states "increasing the number of clusters can enhance attention quality...but also significantly increases the cost of clustering" without systematic exploration.

- **Open Question 3**: Can STARC be effectively combined with other LLM optimization techniques such as quantization, KV cache compression, or speculative decoding? The conclusion states: "We hope that our work inspires further integration of PIM architectures with emerging LLM optimization techniques."

## Limitations
- Performance gains depend critically on stable semantic clustering of key vectors, which may not transfer to larger models or highly diverse contexts
- Clustering overhead and incremental reclustering strategy may introduce latency spikes in latency-sensitive streaming applications
- Evaluation is simulation-based using AttAcc, and actual PIM hardware may exhibit different access patterns and overheads

## Confidence
- **High Confidence**: STARC's core mechanism of clustering KV pairs by semantic similarity and aligning them with PIM row granularity is sound and well-supported by evidence
- **Medium Confidence**: The accuracy preservation claim is supported by recall rate comparisons, but exact F1/ROUGE numbers are not provided for all datasets
- **Low Confidence**: Claims about scalability to much larger models and extremely long contexts (>32K) are speculative, as the paper only validates on the 32K variant of LongChat-7B

## Next Checks
1. **Model and Context Scaling Test**: Evaluate STARC on LLaMA-70B (4096-dim embeddings) and contexts exceeding 32K tokens. Measure accuracy, latency, and energy to identify scaling bottlenecks or accuracy degradation patterns.

2. **Retrieval Quality Analysis**: For each query, compute the actual attention weights of retrieved tokens versus the top-k most relevant tokens by true attention scores. Quantify recall precision and analyze failure cases where centroid similarity mispredicts relevance.

3. **Hardware Overhead Characterization**: Profile the clustering computation time (K-means iterations, centroid dot products) relative to attention computation time across different cluster counts and reclustering intervals. Determine the break-even point where clustering overhead negates retrieval benefits.