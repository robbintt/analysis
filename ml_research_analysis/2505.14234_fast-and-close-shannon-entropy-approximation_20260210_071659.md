---
ver: rpa2
title: Fast and close Shannon entropy approximation
arxiv_id: '2505.14234'
source_url: https://arxiv.org/abs/2505.14234
tags:
- entropy
- approximation
- learning
- gradient
- shannon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational cost and numerical instability
  associated with Shannon entropy (SE) calculations, particularly due to the singularity
  of SE's gradient at zero probabilities. The authors propose Fast Entropy Approximation
  (FEA), a non-singular rational approximation of Shannon entropy and its gradient.
---

# Fast and close Shannon entropy approximation

## Quick Facts
- **arXiv ID:** 2505.14234
- **Source URL:** https://arxiv.org/abs/2505.14234
- **Authors:** Illia Horenko; Davide Bassetti; Lukáš Pospíšil
- **Reference count:** 17
- **Key outcome:** FEA achieves 10^-3 MAE with 50% speedup and enables faster feature selection

## Executive Summary
This paper addresses the computational bottleneck of Shannon entropy (SE) calculations in optimization tasks, particularly due to the singularity of SE's gradient at zero probabilities. The authors propose Fast Entropy Approximation (FEA), a non-singular rational approximation that achieves significantly lower approximation error (10^-3 MAE, ~20× better than state-of-the-art) while requiring only 5-6 elementary operations compared to tens for existing methods. FEA is Lipschitz-continuous with a small Lipschitz constant, enabling faster convergence in optimization algorithms like Spectral Projected Gradient (SPG). Implementation tests demonstrate approximately 50% faster computation across Python, Julia, and MATLAB platforms, with mean squared approximation error of 3 × 10^-6. When integrated into machine learning feature selection tasks, FEA enables model reduction that is two to three orders of magnitude faster than existing methods while maintaining comparable model quality.

## Method Summary
The paper proposes a rational approximation function for Shannon entropy that avoids the singularity at zero probabilities. The FEA function is defined as h_FEA(x) = x · (0.6648/(x + 0.2086) - 0.5754x) + 0.0206, with an analytical derivative that is non-singular across the domain [0,1]. This enables use with gradient-based optimizers like SPG for entropy-based optimization tasks. The method is evaluated on synthetic regression benchmarks based on Tibshirani's LASSO methodology, where it demonstrates faster convergence and comparable model quality to standard methods while requiring fewer computational operations.

## Key Results
- Achieves MAE ≈ 10^-3, approximately 20× better than state-of-the-art fast entropy methods
- Requires only 5-6 elementary computational operations vs. tens for existing fast algorithms
- Lipschitz-continuous with L ≤ 31.616, enabling faster optimization convergence
- Computes entropy approximately 50% faster than standard methods across Python, Julia, and MATLAB
- Mean squared approximation error of 3 × 10^-6
- Enables feature selection that is 2-3 orders of magnitude faster while maintaining model quality

## Why This Works (Mechanism)
FEA works by replacing the logarithmic Shannon entropy function with a rational approximation that is mathematically smooth across the entire probability domain, including zero. The rational form h(x) = x · (a/(x+b) - cx) + d eliminates the singularity in the derivative that occurs with standard -x log(x) at x=0, making it compatible with gradient-based optimization methods. The carefully chosen coefficients (0.6648, 0.2086, 0.5754, 0.0206) ensure the approximation stays within 10^-3 MAE while maintaining computational efficiency. The resulting Lipschitz-continuous gradient with a relatively small constant enables faster convergence in optimization algorithms like SPG, as step sizes can be chosen more aggressively without violating convergence criteria.

## Foundational Learning

**Shannon Entropy** - Information measure -x log x used in optimization
*Why needed:* Core concept being approximated for faster computation
*Quick check:* Verify approximation error stays below 10^-3 across [0,1]

**Lipschitz Continuity** - Property where function gradients are bounded
*Why needed:* Enables faster convergence in gradient-based optimization
*Quick check:* Confirm L ≤ 31.616 for FEA gradient

**Spectral Projected Gradient (SPG)** - Optimization algorithm for constrained problems
*Why needed:* Method used to demonstrate FEA's practical benefits
*Quick check:* Verify SPG convergence speed improvement with FEA

**Rational Approximation** - Function approximation using ratios of polynomials
*Why needed:* Mathematical foundation enabling non-singular gradient
*Quick check:* Confirm FEA avoids division by zero at x=0

## Architecture Onboarding

**Component Map:** FEA function → Analytical gradient → SPG optimizer → Application (feature selection)

**Critical Path:** The FEA approximation and its analytical gradient form the critical path, as they must be evaluated repeatedly during optimization iterations. The non-singularity of the gradient is essential for SPG convergence.

**Design Tradeoffs:** FEA trades minimal approximation error (10^-3 MAE) for significant computational speedup and numerical stability. The rational form sacrifices the strict concavity of true Shannon entropy but maintains sufficient curvature for optimization purposes.

**Failure Signatures:** Speed-up not observed when using high-level library calls that are already highly optimized; optimization divergence if Lipschitz constant is too large for chosen step sizes; approximation error exceeding tolerance if coefficients are modified.

**First Experiments:**
1. Implement FEA scalar function and derivative in Python/NumPy
2. Generate synthetic sparse regression data and benchmark FEA vs. standard entropy in gradient descent
3. Test FEA integration with SPG optimizer on simple constrained optimization problem

## Open Questions the Paper Calls Out

**Open Question 1:** Can the scalar FEA formula be extended to approximate von Neumann entropy for quantum computing applications without negating the computational speed gains? The current formulation operates on scalar probabilities x_i, whereas quantum applications require handling density matrices.

**Open Question 2:** Does the Lipschitz-continuous gradient of FEA improve convergence speed and model quality in deep neural network pruning compared to standard methods? The optimization landscape of deep neural networks is non-convex, meaning the convergence benefits observed in regression may not transfer directly to backpropagation.

**Open Question 3:** Does the rational FEA function preserve the strict concavity of Shannon entropy over the interval [0, 1]? Rational approximations can introduce inflection points not present in the logarithmic function, potentially creating spurious local optima in optimization landscapes.

## Limitations

- Limited benchmarking against recent fast entropy approximation methods
- Empirical validation restricted to linear regression tasks, not deep learning applications
- Does not verify preservation of strict concavity property of Shannon entropy
- Coefficient optimization details for the rational approximation are not fully specified

## Confidence

- **Computational Efficiency:** High - FEA requires only 5-6 operations vs. tens for existing methods
- **Speed-up Claims:** Medium - 50% speedup supported by multi-language tests but baseline implementations unspecified
- **Non-singularity:** High - Rational function design explicitly avoids division by zero
- **Optimality Comparison:** Low - Claims 20× better MAE but limited comparative analysis against state-of-the-art
- **Lipschitz Property:** High - Theoretically sound with L ≤ 31.616

## Next Checks

1. Benchmark FEA against recent fast entropy algorithms like BinCounting or other polynomial approximations on identical synthetic regression tasks
2. Profile FEA performance in high-dimensional feature selection problems (D > 1000) to verify scalability claims
3. Test FEA robustness to edge cases including near-zero probabilities and extreme sparsity patterns in real-world datasets