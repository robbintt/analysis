---
ver: rpa2
title: Emergent Stack Representations in Modeling Counter Languages Using Transformers
arxiv_id: '2502.01432'
source_url: https://arxiv.org/abs/2502.01432
tags:
- languages
- counter
- probing
- these
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether transformer models trained on counter
  languages implicitly learn stack-like representations by probing their internal
  activations. The authors train transformer models on Dyck-1 and Shuffle-k languages
  (k=2,4,6), which can be modeled using counter variables or equivalently stacks.
---

# Emergent Stack Representations in Modeling Counter Languages Using Transformers

## Quick Facts
- arXiv ID: 2502.01432
- Source URL: https://arxiv.org/abs/2502.01432
- Reference count: 26
- Primary result: Transformers trained on counter languages learn stack-like representations detectable via probing classifiers

## Executive Summary
This paper investigates whether transformer models trained on counter languages implicitly learn stack-like representations by probing their internal activations. The authors train transformer models on Dyck-1 and Shuffle-k languages (k=2,4,6), which can be modeled using counter variables or equivalently stacks. They then train probing classifiers to predict stack depths at each token position from the model's internal representations. Results show high probing accuracy (significantly above random baselines and control tasks) for all tested languages, with accuracy increasing with k values. This demonstrates that transformers learn stack-like structures when trained on counter languages, bringing insights into how these models process hierarchical patterns and supporting circuit discovery efforts in mechanistic interpretability.

## Method Summary
The paper trains encoder-only transformers (1 layer, 4 heads, d_model=64) on counter languages (Dyck-1 and Shuffle-k) using next-token prediction with MSE loss against k-hot valid-next-token vectors. After training, the authors extract embeddings from the final encoder layer and train probing classifiers (FFN with 1-6 layers) to predict stack depth values at each token position. They evaluate probing accuracy and compare against control tasks where probing targets are randomized. The model uses causal masking, absolute positional encodings, and RMSProp optimization with lr=5e-3 for 25 epochs.

## Key Results
- Probing classifiers achieve high accuracy in predicting stack depths across all tested languages (Dyck-1, Shuffle-2, Shuffle-4, Shuffle-6)
- Accuracy is higher for Shuffle languages compared to Dyck-1, likely because stacks in Shuffle languages are updated less frequently
- Probing accuracy remains significantly above random baselines even with linear probes, suggesting stack depth is linearly recoverable from activations
- Small transformers (1 layer, 4 heads) can learn counter representations without explicit stack mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformers trained on counter languages encode stack depth information in their internal representations, detectable via probing classifiers.
- **Mechanism:** During next-token prediction training on counter languages, the model learns embeddings that correlate with counter/stack state. A probing classifier g: fl(x) → ẑ maps layer-l embeddings to stack depth values with high accuracy, indicating the property is linearly recoverable from activations.
- **Core assumption:** High probing accuracy implies the model encodes the probed property in its representations (correlational, not causal).
- **Evidence anchors:** "probing results show high accuracy in predicting stack depths across all tested languages"; "accuracies on the control task are near random baselines it suggests that the probing models are not learning spurious connections"

### Mechanism 2
- **Claim:** Stack depth representations are more easily learned when counter updates occur less frequently.
- **Mechanism:** In Shuffle-k languages, each of k stacks updates on average every k-th token, versus every token for Dyck-1. This reduced update frequency appears to make counter representations easier for the model to encode and probes to recover.
- **Core assumption:** Update frequency directly affects representational quality/extractability.
- **Evidence anchors:** "accuracy is higher for Shuffle languages compared to Dyck-1, likely because stacks in Shuffle languages are updated less frequently"; "as the number of stacks required increase the frequency at which their depths change decreases"

### Mechanism 3
- **Claim:** Small transformers (1 layer, 4 heads) can learn counter representations without explicit stack mechanisms.
- **Mechanism:** Self-attention with causal masking enables the model to track opening/closing symbol pairs across sequences, effectively simulating counter behavior through learned attention patterns rather than architectural inductive biases.
- **Core assumption:** Attention patterns can implement counting behavior; no explicit external memory required.
- **Evidence anchors:** Model has "1 layer deep transformer and 4 attention heads" with hidden dimension 64; "these models when trained as next token predictors learn stack-like representations"

## Foundational Learning

- **Counter Languages (Dyck-1, Shuffle-k)**
  - Why needed here: The paper uses these as well-defined formal languages with precise stack/counter semantics for probing.
  - Quick check question: Can you explain why "(())" ∈ Dyck-1 but "())(" ∉ Dyck-1?

- **Probing Classifiers**
  - Why needed here: Core methodology for detecting whether properties (stack depth) are encoded in activations.
  - Quick check question: Why does high probing accuracy not prove a model *uses* the probed property causally?

- **Stack/Counter Equivalence**
  - Why needed here: Shuffle-k can be modeled by k counters OR k stacks; depth = counter value.
  - Quick check question: For Shuffle-2 with vocabularies {(,)} and {[,]}, what are the stack depths at each position in "[(])"?

## Architecture Onboarding

- **Component map:** Token IDs → Embedding layer (dim 32) + Positional encoding → 1 transformer layer with 4-head causal self-attention (hidden 64) → Linear decoder head (|V| × d_model) → Sigmoid → k-hot prediction → Probe training (post-hoc)

- **Critical path:** Token → Embedding → Attention → FFN → LayerNorm → Output → Probe training (post-hoc)

- **Design tradeoffs:**
  - Small model (1 layer) chosen for interpretability vs. capacity
  - MSE loss on k-hot targets (valid next tokens) vs. cross-entropy on single tokens
  - Encoder-only architecture vs. decoder-only for autoregressive modeling

- **Failure signatures:**
  - Probing accuracy ≈ random baseline → model failed to learn counter structure
  - Control task accuracy ≈ main task → probe learning spurious correlations
  - Long-sequence performance degradation → counter capacity exceeded

- **First 3 experiments:**
  1. Replicate Dyck-1 probing with varying probe depths (linear to 6-layer) to confirm selectivity gap.
  2. Test on sequences longer than 50 tokens to find representational capacity limits.
  3. Ablate attention heads individually to identify if specific heads track specific counters in Shuffle-k.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified stack-like representations play a causal role in the model's computation, or are they merely correlated epiphenomena?
- Basis in paper: The authors explicitly state they "leave to future work... to determine whether these structures play a causal role in determining the model's output."
- Why unresolved: Probing classifiers can only identify the presence of information in activations; they cannot determine if the model actually uses that information to generate predictions.
- What evidence would resolve it: Causal intervention experiments (e.g., activation patching or ablation) that alter the stack representation within the model and result in a predictable change in the output token predictions.

### Open Question 2
- Question: What specific algorithms operate within the transformer to update and read from these emergent stack structures?
- Basis in paper: The paper distinguishes between identifying representations and identifying algorithms, noting that "understanding of how these stack structures is updated is an important area of research."
- Why unresolved: This work focused on verifying the *existence* of the stack depth representation, not tracing the computational steps (circuits) that modify this depth token-by-token.
- What evidence would resolve it: Circuit analysis identifying specific attention heads or MLPs responsible for incrementing and decrementing the stack depth values upon seeing specific tokens.

### Open Question 3
- Question: Do these models learn a single stack representation or multiple redundant representations to solve the task?
- Basis in paper: The authors suggest that "It is also possible for the LM to learn multiple representations and algorithms, in which case exploring the causality of these becomes important."
- Why unresolved: A probing classifier may find one interpretable representation while missing other polysemantic or distributed representations that also encode the counter state.
- What evidence would resolve it: Applying distributed interpretation methods or ablating the discovered stack direction to see if the model retains performance via backup mechanisms.

## Limitations
- The paper demonstrates correlation between stack depth and activations but cannot establish causation - high probing accuracy doesn't prove the model uses this information for computation
- The small model size (1 layer, 4 heads) and short sequences (50 tokens max) may not generalize to more complex or longer sequences where stack operations become more challenging
- The control task methodology, while showing selectivity above random baselines, doesn't fully rule out alternative explanations such as position-based heuristics or other spurious correlations

## Confidence
- **High confidence**: The core finding that transformers trained on counter languages develop representations from which stack depth can be linearly extracted
- **Medium confidence**: The interpretation that lower update frequency in Shuffle-k languages makes counter representations easier to learn
- **Low confidence**: That the specific probing architecture captures all relevant aspects of how transformers encode stack information

## Next Checks
1. Perform causal intervention validation through activation patching or feature ablation experiments to test whether the model causally uses the probed representations for computation
2. Train and probe models on sequences longer than 50 tokens (e.g., up to 200-500 tokens) to identify representational capacity limits
3. Apply more sophisticated probing techniques such as minimum description length probing or causal mediation analysis to verify the completeness of linear probes