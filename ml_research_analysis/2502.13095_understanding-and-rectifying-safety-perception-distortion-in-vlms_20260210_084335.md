---
ver: rpa2
title: Understanding and Rectifying Safety Perception Distortion in VLMs
arxiv_id: '2502.13095'
source_url: https://arxiv.org/abs/2502.13095
tags:
- safety
- vlms
- shift
- activation
- unsafe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes why vision-language models (VLMs) are more
  vulnerable to safety issues compared to their text-only LLM backbones. The authors
  identify that incorporating images into inputs causes an activation shift in VLMs
  that systematically pushes activations toward a "safer" direction, making the model
  perceive harmful requests as less risky than they actually are.
---

# Understanding and Rectifying Safety Perception Distortion in VLMs

## Quick Facts
- arXiv ID: 2502.13095
- Source URL: https://arxiv.org/abs/2502.13095
- Reference count: 40
- Primary result: Activation Shift Disentanglement and Calibration (ShiftDC) reduces VLM attack success rates by 5-10% while preserving visual reasoning

## Executive Summary
Vision-language models (VLMs) are systematically more vulnerable to safety attacks than their text-only LLM backbones, exhibiting a 28% higher attack success rate due to a safety perception distortion. This distortion arises from modality-induced activation shifts that push representations toward a "safer" direction, causing harmful inputs to be misclassified as benign. The proposed training-free method, ShiftDC, addresses this by removing the safety-relevant component of the activation shift while preserving visual semantics. Experimental results demonstrate significant improvements in safety without degrading visual reasoning capabilities.

## Method Summary
ShiftDC is a training-free inference method that calibrates VLMs by disentangling the safety-relevant component of modality-induced activation shifts. It first computes safety direction vectors using contrastive text-only datasets, then at inference generates captions for input images to create text-only counterparts. The method calculates the modality shift between VL and text-only activations, projects out the safety-relevant component, and calibrates the VL activation. This restoration of the LLM backbone's inherent safety mechanisms is achieved without retraining, addressing the systematic overestimation of input safety in VLMs.

## Key Results
- ShiftDC reduces attack success rates across safety benchmarks by 5-10% compared to baseline defense methods
- The method maintains visual reasoning capabilities on utility benchmarks (MME and MM-Vet) while improving safety
- Effectiveness varies by model: LLaVA-1.6-34B shows only marginal improvement, while others achieve substantial reductions in ASR

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual modality induces an activation shift that pushes representations toward the "safe" direction, causing VLMs to systematically overestimate input safety.
- Mechanism: When images are paired with text inputs, the visual embedding pathway introduces a modality-induced shift in the residual stream activations. This shift contains a component aligned with the safety direction (from unsafe→safe contrastive pairs), effectively moving unsafe inputs across the decision boundary into perceived-safe territory.
- Core assumption: Safety-relevant features in LLM backbones are encoded as approximately linear directions in activation space.
- Evidence anchors:
  - [abstract] "multimodal inputs introduce an modality-induced activation shift toward a 'safer' direction compared to their text-only counterparts"
  - [section 4, Observation 3] Figure 6 shows positive correlation (cosine similarity >0.7 for successful attacks) between modality-induced shift alignment with safety direction and attack success rate
  - [corpus] VLM-Guard (arXiv:2502.10486) attributes similar vulnerability to "safety alignment gap" from vision integration
- Break condition: If the LLM backbone has poor safety alignment on text-only inputs (e.g., scenarios 10-13 in MM-SafetyBench where even text ASR is high), calibrating the modality shift cannot restore safety.

### Mechanism 2
- Claim: The modality-induced shift can be decomposed into safety-relevant and safety-irrelevant components via projection onto the safety direction.
- Mechanism: Given the safety direction vector sℓ (computed as difference-in-means between safe/unsafe text-only activations) and the modality shift m, the safety-relevant component is isolated via projection: proj_s(m) = (m·s/||s||²)·s. This exploits orthogonality assumptions between semantic visual features and safety-related features.
- Core assumption: Safety-relevant and modality-specific visual features occupy approximately orthogonal subspaces.
- Evidence anchors:
  - [section 5] Equations 5-7 formalize the decomposition and calibration
  - [section 6.4, Figure 8] After ShiftDC, unsafe VL activations become linearly separable from safe VL activations (matching text-only separability)
  - [corpus] Internal Activation Revision (arXiv:2501.16378) reports similar findings about activation distribution shifts from image integration
- Break condition: If safety and visual semantics share significant directional overlap, removing the safety projection will degrade visual reasoning capability.

### Mechanism 3
- Claim: Calibrated activations restore the LLM backbone's inherent safety mechanisms without requiring retraining.
- Mechanism: By subtracting the safety-relevant projection from the VL activation (̂x = x - proj_s(m)), the intervention repositions the activation to where it would approximately be if processed as text-only. This allows pre-trained refusal circuits in middle layers to function as intended.
- Core assumption: The LLM backbone's safety mechanisms remain intact after VL fine-tuning and are simply misactivated due to shifted input distributions.
- Evidence anchors:
  - [section 6.2] Table 1 shows ShiftDC reduces ASR below text-only baselines on some models (e.g., LLaVA-1.6-34B: 35.2% text → 30.1% SD)
  - [section 6.5] Table 5 shows minimal increase in false alarm rates on benign datasets
  - [corpus] Bootstrapping LLM Robustness (arXiv:2505.24208) confirms blank images alone can trigger harmful responses, suggesting mechanism is modality-agnostic
- Break condition: If the backbone's safety alignment was degraded during VL fine-tuning (catastrophic forgetting), calibration alone is insufficient.

## Foundational Learning

- Concept: **Residual stream activations**
  - Why needed here: ShiftDC operates on xℓ(t), the last-token residual stream at layer ℓ; understanding that this represents cumulative processed information is essential.
  - Quick check question: At layer 15, what does the last-token residual stream encode about a harmful query paired with an image?

- Concept: **Difference-in-means vectors as concept directions**
  - Why needed here: The safety direction sℓ is computed by averaging activations from contrastive datasets (safe vs. unsafe) and subtracting; this assumes concepts are linearly represented.
  - Quick check question: If safe and unsafe activations have high variance within classes, will the difference-in-means vector still capture a meaningful safety direction?

- Concept: **Vector projection for component isolation**
  - Why needed here: ShiftDC isolates the safety component via proj_s(m); understanding that this extracts the portion of m parallel to s while discarding orthogonal components is critical.
  - Quick check question: If m and s are orthogonal (cos = 0), what happens to the calibrated activation?

## Architecture Onboarding

- Component map:
  Vision encoder (e.g., CLIP) -> vision-language projector -> LLM backbone

- Critical path:
  1. Pre-compute safety direction sℓ using contrastive text-only datasets (Eq. 4)
  2. At inference: generate caption for input image -> compute text-only activation xℓ(ttt)
  3. Compute VL activation xℓ(tvl) -> calculate modality shift m = xℓ(tvl) - xℓ(ttt)
  4. Project safety component: proj_s(mℓ) = (mℓ·sℓ/||sℓ||²)·sℓ
  5. Calibrate: x̂ℓ(tvl) = xℓ(tvl) - proj_s(mℓ) -> continue forward pass

- Design tradeoffs:
  - Layer range selection: Appendix F shows middle layers (5-20) are optimal; early layers lack linear structure, late layers have insufficient downstream effect
  - Computational overhead: ~2 additional forward passes per query (Table 10: +0.26s for LLaVA-1.5-7B)
  - Caption quality vs. efficiency: Poor captions may misrepresent text-only baseline; dense captions add latency

- Failure signatures:
  - High ASR persists: safety direction not properly extracted (check text-only classification accuracy first)
  - Visual utility drops: over-correction into unsafe direction (verify projection sign)
  - False alarms spike: calibration pushing benign inputs toward unsafe region (check if defensive prompt mechanism is active per Figure 9)

- First 3 experiments:
  1. Validate safety direction extraction: Train linear probe on text-only Dtt activations; confirm ~90% classification accuracy at middle layers (replicate Figure 3 baseline)
  2. Layer ablation: Apply ShiftDC starting at layers 1, 5, 10, 15, 20; plot ASR to identify optimal calibration range (replicate Figure 10)
  3. Utility preservation check: Run ShiftDC on MME/MM-Vet; verify scores remain within 1% of baseline (replicate Table 4 pattern)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the visual modality inherently induce an activation shift toward the "safer" direction, even with semantically minimal inputs?
- Basis in paper: [explicit] The authors observe that even blank images push activations toward the safe side, suggesting the shift originates from the visual modality itself rather than image content (Page 5).
- Why unresolved: While the paper identifies the correlation between the modality and the safety shift, it does not isolate the specific architectural component (e.g., vision encoder, projector) or training dynamic responsible for this directional bias.
- What evidence would resolve it: Ablation studies swapping vision encoders or analyzing pre-training datasets to identify the source of the "safe" direction bias.

### Open Question 2
- Question: Is safety perception distortion a universal phenomenon in multimodal models, or is it specific to vision-language architectures?
- Basis in paper: [inferred] The paper frames the issue as "modality-induced," implying a potential generalization, but restricts experiments to vision-language models (VLMs).
- Why unresolved: The theoretical framework suggests adding any non-text modality might distort safety perception, but this has not been empirically verified for audio or other modalities.
- What evidence would resolve it: Experiments replicating the activation analysis on Audio-Language Models or 3D-language models to see if similar safety-relevant shifts occur.

### Open Question 3
- Question: Is ShiftDC robust to adaptive attacks specifically designed to bypass the linear disentanglement mechanism?
- Basis in paper: [inferred] The method relies on removing a specific linear safety component; however, the paper evaluates only existing benchmarks rather than adaptive adversaries.
- Why unresolved: An attacker with knowledge of the calibration vector could potentially craft inputs where the harmful component is orthogonal to the safety direction, rendering the projection ineffective.
- What evidence would resolve it: Evaluation using white-box adaptive attacks optimized to maximize attack success rate while accounting for the ShiftDC projection.

## Limitations
- The observed safety distortion may stem from multiple sources beyond modality-induced activation shifts, including captioning quality and defensive prompt tuning.
- Safety direction vectors computed from text-only data may not generalize to VL inputs if safety-relevant feature spaces differ between modalities.
- The calibration is training-free and doesn't adapt to adaptive attackers who might probe the safety direction.

## Confidence
- **High confidence** in the core observation that VLMs are more vulnerable than their LLM backbones due to safety perception distortion. The comparative ASR analysis across multiple models and benchmarks provides strong empirical support.
- **Medium confidence** in the mechanism that modality-induced activation shifts systematically push representations toward the "safe" direction. While the cosine similarity correlation (>0.7 for successful attacks) supports this, alternative explanations like caption-induced semantic changes cannot be ruled out.
- **Medium confidence** in ShiftDC's effectiveness. The 5-10% ASR reduction is promising but shows model-dependent variability. LLaVA-1.6-34B shows only marginal improvement, suggesting the method doesn't universally restore safety.

## Next Checks
1. **Caption ablation study**: Evaluate ShiftDC performance using ground truth captions versus generated captions on the same dataset. This isolates whether caption quality or the modality shift itself drives safety distortion.

2. **Transferability test**: Train safety directions on one VLM backbone (e.g., LLaVA-1.5) and apply ShiftDC to another (e.g., MiniGPT-4). This validates whether safety directions are model-agnostic or require backbone-specific computation.

3. **Adaptive attack robustness**: Design attacks that explicitly target the safety direction vector (e.g., by optimizing perturbations in the projected space). Test whether ShiftDC maintains its defense efficacy against these specialized attacks.