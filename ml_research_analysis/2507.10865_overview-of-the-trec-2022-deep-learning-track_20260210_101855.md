---
ver: rpa2
title: Overview of the TREC 2022 deep learning track
arxiv_id: '2507.10865'
source_url: https://arxiv.org/abs/2507.10865
tags:
- uni00000013
- uni00000015
- uni00000014
- uni00000016
- uni00000019
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The TREC 2022 Deep Learning Track continued benchmarking neural
  ranking methods on the MS MARCO passage and document collections, focusing on constructing
  a reusable test collection. This year's key change was using test queries that never
  contributed to the MS MARCO corpus, along with only judging passages and inferring
  document labels, and deduplicating passages to improve reusability and reduce saturation.
---

# Overview of the TREC 2022 deep learning track

## Quick Facts
- arXiv ID: 2507.10865
- Source URL: https://arxiv.org/abs/2507.10865
- Reference count: 9
- Primary result: Single-stage dense retrieval significantly underperformed compared to 2021, with the best dense run 23% worse than the best multi-stage neural run on NDCG@10.

## Executive Summary
The TREC 2022 Deep Learning Track benchmarked neural ranking methods on the MS MARCO v2 corpus, introducing key changes to improve evaluation reusability and discriminative power. The track used 76 judged queries selected from 500 candidates, ensuring relevance density below 0.4 for all topics. The evaluation focused on passage ranking with document labels inferred from passage judgments, and implemented near-duplicate passage deduplication to reduce redundancy and judgment costs. Results showed neural methods with large-scale pretraining continued to dominate traditional approaches, with the best neural run improving NDCG@10 by 125% over the best traditional run for passage ranking and 76% for document ranking.

## Method Summary
The track used MS MARCO v2 (138M passages, 11.9M documents) with 500 candidate queries, selecting 76 for final judging based on relevance density constraints. Passages were judged on a 4-point scale (0-3), with document labels inferred using maximum passage judgment per document. Near-duplicate passages were clustered and deduplicated during the judging process. Evaluation used NDCG@10, NCG@100, and MAP metrics, with binary relevance at levels 2+ for passages and 1+ for documents. Pyserini BM25 served as the baseline sparse retrieval method, while neural methods included both single-stage dense retrieval and multi-stage reranking approaches.

## Key Results
- Single-stage dense retrieval was 23% worse on NDCG@10 compared to the best multi-stage neural run, a significant drop from 2021.
- The best neural run (nnlm) improved NDCG@10 by 125% over the best traditional run for passage ranking and 76% for document ranking.
- Deduplication reduced redundant judgments while maintaining evaluation stability, allowing 76 queries to be judged versus a potential 64 without deduplication.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Using test queries that never contributed to the MS MARCO corpus increases task difficulty and widens the performance gap between neural and traditional methods.
- **Mechanism:** Removing the guarantee that "Bing top-10" passages are present forces systems to rely on generalized semantic matching rather than memorized retrieval paths, where neural models with pretraining show greater robustness.
- **Core assumption:** Performance shift is driven by corpus construction bias removal rather than traditional algorithm degradation.
- **Evidence anchors:** Abstract states key change was using queries never in corpus; section 4 shows 125% improvement gap between neural and traditional methods.

### Mechanism 2
- **Claim:** Single-stage dense retrieval struggles to maintain competitiveness on larger, refreshed corpora with deduplication relative to multi-stage pipelines.
- **Mechanism:** Without first-stage sparse filtering or sophisticated reranking, single-stage dense methods lose precision when searching the expanded MS MARCO v2 collection compared to v1.
- **Core assumption:** Performance drop is structural rather than lack of participant effort.
- **Evidence anchors:** Abstract notes single-stage dense runs were less competitive; section 4 shows 23% worse NDCG@10 performance.

### Mechanism 3
- **Claim:** Concentrating judgment resources on deduplicated passages while inferring document labels maximizes evaluation discriminative power for fixed budget.
- **Mechanism:** Judging documents suffers from saturation; deduplicating passages and propagating labels reduces noise and judgment cost while maintaining ability to distinguish high-performing systems.
- **Core assumption:** Passage relevance perfectly indicates document relevance, and near-duplicates provide no additional utility.
- **Evidence anchors:** Abstract confirms deduplication reduced redundant judgments while maintaining stability; section 5 states eliminating document judging was crucial for complete judgments.

## Foundational Learning

**Cranfield Paradigm & Test Collection Reusability**
- Why needed: Paper obsesses over "reusability" and "saturation" - understanding that saturated test sets (rating all top systems identically) are useless is key to decoding design choices.
- Quick check: Why does having "too many relevant results" per query make a benchmark worse for evaluating top-tier systems?

**Sparse vs. Dense Retrieval**
- Why needed: Paper draws sharp line between "trad" (BM25/sparse), "nnlm" (Neural Language Models), and "single-stage dense" - must distinguish exact term matching from embedding-based vector search.
- Quick check: Why would a dense retrieval model potentially miss a relevant document containing exact answer keywords but lacking semantic context?

**Label Propagation (Passage -> Document)**
- Why needed: Track uses passage labels to grade documents.
- Quick check: If a document contains one "Perfectly Relevant" passage (Level 3) but the rest is garbage, does the document get a score of 3? (Yes, per paper's "max judgment" rule).

## Architecture Onboarding

**Component map:**
- Corpus: MS MARCO v2 (138M passages, 11.9M docs)
- Index: Pyserini (Baseline Sparse) + Neural Indexes (Dense/SPLADE)
- Pipeline: Sparse Retrieval (BM25) -> Neural Reranking (nnlm) OR Single-Stage Dense Retrieval
- Evaluation: NIST Judgments (Passage) -> Propagation -> Document Labels -> NDCG@10

**Critical path:** Passage Ranking is the source of truth; Document metrics are downstream inferences. Do not optimize for Document Ranking directly without validating underlying Passage signals.

**Design tradeoffs:**
- Deduplication: Saves judging budget but removes redundancy that might be useful for answer diversity
- Inferred Document Labels: Faster/Cheaper but risks labeling documents relevant for "bad" passages if rest is high quality (though paper uses "max", implying per-passage quality dictates doc quality)

**Failure signatures:**
- Saturation: Achieving P@10 = 1.0 (indicates test set is too easy or queries are biased)
- Dense Drift: Single-stage dense models retrieving semantically similar but actually irrelevant passages

**First 3 experiments:**
1. Baseline Sparse Check: Run BM25 on v2 corpus with 2022 queries to confirm floor performance (approx 0.26-0.32 NDCG@10)
2. Reranker Lift: Apply BERT-based reranker to top-100 BM25 results to observe nnlm lift (expect significant gain)
3. Dense Retrieval Stress Test: Implement single-stage dense retrieval run (e.g., ColBERT/ANCE) to verify underperformance relative to multi-stage systems

## Open Questions the Paper Calls Out

**Open Question 1:** Why did single-stage dense retrieval methods significantly underperform relative to other methods in TREC 2022 compared to their 2021 performance? The paper identifies this as a "surprise" but doesn't determine if cause is query sampling change, corpus refresh, or stagnation in dense retrieval techniques.

**Open Question 2:** Is the community's near-total convergence on pre-trained language models ("nnlm") and abandonment of other neural methods ("nn") a healthy evolution or premature narrowing of research scope? The authors note the "nn" category disappeared and question whether this homogenization is healthy.

**Open Question 3:** To what extent does the new query sampling method—using queries that never contributed to the MS MARCO corpus—affect the discriminative power and reusability of the test collection? The authors posit this may have contributed to increased gap between nnlm and trad performance but don't confirm causal link.

**Open Question 4:** How much extra statistical power is gained by deduplicating near-duplicate passages during the judgment process? The authors state more analysis is needed to understand how much extra statistical power was achieved by avoiding duplicate judgments.

## Limitations
- Evaluation design choices may not generalize to real-world retrieval scenarios
- Significant performance gaps could reflect corpus-specific artifacts rather than universal retrieval principles
- Report lacks detailed implementation specifics for top-performing systems, limiting reproducibility

## Confidence

**High Confidence:** Claims about 2022 evaluation setup (unseen queries, deduplication process, 76 judged queries) - directly stated and verifiable from track documentation

**Medium Confidence:** Claims about single-stage dense retrieval underperformance - clearly stated but mechanism behind degradation isn't fully explained and may vary with different corpus versions or indexing approaches

**Low Confidence:** Claims about general superiority of neural methods over traditional approaches - results are specific to MS MARCO v2 corpus and particular query set, may not generalize to other domains

## Next Checks

1. **Cross-Corpus Validation:** Test whether observed performance gaps between neural and traditional methods persist when using different corpus (e.g., ClueWeb, Robust04) with same 2022 query set

2. **Query Difficulty Analysis:** Analyze the 76 judged queries to determine if they systematically differ from 2021 queries in query length, ambiguity, or topical complexity that could explain performance shifts

3. **Alternative Deduplication Strategy:** Implement different deduplication approach (e.g., semantic clustering vs. n-gram overlap) to verify if observed stability in system rankings holds under alternative redundancy reduction methods