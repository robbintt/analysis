---
ver: rpa2
title: 'Refine Drugs, Don''t Complete Them: Uniform-Source Discrete Flows for Fragment-Based
  Drug Discovery'
arxiv_id: '2509.26405'
source_url: https://arxiv.org/abs/2509.26405
tags:
- sampling
- invirtuogen
- sequence
- optimization
- molecules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InVirtuoGen introduces a discrete flow generative model for fragmented
  SMILES that shifts from completion-style generation to simultaneous refinement of
  all sequence positions, decoupling sampling steps from sequence length. Unlike masked
  diffusion models, it updates all tokens at every denoising step starting from a
  uniform source distribution, enabling coordinated molecular refinement.
---

# Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery

## Quick Facts
- **arXiv ID**: 2509.26405
- **Source URL**: https://arxiv.org/abs/2509.26405
- **Authors**: Benno Kaech; Luis Wyss; Karsten Borgwardt; Gianvito Grasso
- **Reference count**: 40
- **Primary result**: InVirtuoGen achieves state-of-the-art performance in target property optimization on PMO benchmark and provides a versatile foundation for drug discovery across multiple stages.

## Executive Summary
InVirtuoGen introduces a discrete flow generative model for fragmented SMILES that shifts from completion-style generation to simultaneous refinement of all sequence positions, decoupling sampling steps from sequence length. Unlike masked diffusion models, it updates all tokens at every denoising step starting from a uniform source distribution, enabling coordinated molecular refinement. The model uses a diffusion transformer backbone and achieves a stronger quality-diversity Pareto frontier in de novo generation, outperforming prior fragment-based models. For fragment-constrained tasks, it delivers competitive quality and diversity. In target property optimization on the PMO benchmark, a hybrid GA-PPO framework with discrete flow adaptation achieves state-of-the-art performance measured by top-10 AUC. In lead optimization, it yields higher docking scores under similarity, synthesizability, and drug-likeness constraints than baselines.

## Method Summary
InVirtuoGen is a discrete flow generative model that processes fragmented SMILES sequences using a diffusion transformer backbone. The model initializes with uniform random tokens and learns to predict all token positions simultaneously at every denoising step, rather than unmasking tokens sequentially. It uses a time-weighted cross-entropy loss with linear interpolation between uniform noise and target tokens. Sampling is performed by directly drawing from the predicted token-wise distribution rather than using standard flow velocity updates. For optimization tasks, a hybrid genetic algorithm and proximal policy optimization framework fine-tunes the pretrained model to maximize property-specific rewards while maintaining chemical validity.

## Key Results
- Achieves stronger quality-diversity Pareto frontier in de novo generation compared to prior fragment-based models
- Delivers state-of-the-art top-10 AUC performance on the PMO benchmark using hybrid GA-PPO optimization
- Produces higher docking scores under similarity, synthesizability, and drug-likeness constraints in lead optimization tasks
- Maintains competitive quality and diversity in fragment-constrained generation despite validity challenges with prompt token overwriting

## Why This Works (Mechanism)

### Mechanism 1: Uniform-Source Refinement Over Completion
The model initializes with a uniform distribution over tokens rather than a [MASK] token, learning a probability path to the data distribution that predicts target tokens for all positions at every step. This enables bidirectional attention to resolve global dependencies immediately, allowing bidirectional attention (via a Transformer) to resolve global dependencies immediately, rather than locking in early tokens that might constrain later ones. The core assumption is that molecular validity and drug-likeness benefit from global coordination of fragments during the entire generative process.

### Mechanism 2: Empirical Sampling with Direct Distribution Prediction
Sampling directly from the predicted token-wise distribution with annealed noise outperforms the standard probability velocity update, provided specific loss weighting is used. This heuristic encourages rapid confidence saturation early in the trajectory, reducing the rate of token changes as time approaches 1. The core assumption is that a "refinement" trajectory - where the number of token changes decays exponentially - is superior to the linear change rate of standard discrete flows.

### Mechanism 3: Hybrid GA-PPO for Optimization
A hybrid framework combining Genetic Algorithms for exploration and PPO for refinement achieves state-of-the-art property optimization by leveraging the flow model's ability to process arbitrary full-sequence inputs. The GA generates candidate offspring via fragment crossover and mutation, while the discrete flow model acts as a refiner fine-tuned via PPO to maximize reward signals. The core assumption is that the flow model can approximate log-probabilities for whole sequences sufficiently well to provide useful gradients for PPO.

## Foundational Learning

- **Discrete Flow Matching**: This is the core mathematical engine that defines transport paths directly, unlike diffusion which uses noise-to-data paths. You must understand the probability path to understand how the model blends noise and data.
  - *Quick check*: How does the source distribution in this paper differ from the [MASK] token distribution in BERT-style generative models?

- **Fragment-based SMILES (SAFE)**: The model operates on "fragmented SMILES," not atoms. Understanding that a molecule is a sequence of disconnected fragments with attachment points is crucial for interpreting the tokenization and the "crossover" logic in the GA.
  - *Quick check*: If a model generates "c1ccccc1 [1*] C(C)C", how does the rBRICS decomposition ensure this represents a valid chemical assembly instruction?

- **PPO for Non-Autoregressive Models**: Standard PPO uses the sum of log-probs of tokens. This model is non-autoregressive. You need to understand how the authors approximate the policy gradient to implement the optimization loop.
  - *Quick check*: Why can't we calculate log p(x) exactly for a discrete flow model in closed form, necessitating the Monte Carlo approach?

## Architecture Onboarding

- **Component map**: Uniform random tokens + Sequence Length n -> DiT backbone with Rotary Embeddings -> Probability distribution over vocabulary for every token position -> Controller with Time embedding t conditioning the DiT

- **Critical path**: 
  1. Preprocessing: Convert SMILES -> rBRICS fragments -> Random shuffle -> Tokenize
  2. Training: Sample t, interpolate between uniform noise and target tokens, compute Cross-Entropy loss on all positions (weighted by 1/(1-t^2))
  3. Sampling: Initialize X_0 ~ Uniform. Loop t=0 -> 1: Predict logits -> Sample X_{t+h} (using Eq. 6 heuristic)

- **Design tradeoffs**:
  - Sequence Length: The model requires length n as input, using an empirical prior p(n) or a bandit to guess length, unlike autoregressive models that stop at an EOS token
  - Speed vs. Quality: The heuristic sampling allows fewer steps (h=0.01) than masked models (steps proportional to length), but the theoretical tradeoff is stability

- **Failure signatures**:
  - "Runaway" Refinement: If noise damping r is too high, the sequence oscillates and never settles into a valid SMILES string
  - Invalid Constraints: In fragment-constrained tasks, "naive overwriting" of prompt tokens disrupts the flow dynamics, leading to lower validity
  - PPO Collapse: If the KL-divergence constraint in PPO is too loose, the model generates high-reward but invalid strings

- **First 3 experiments**:
  1. De Novo Validity Curve: Generate 10k molecules with varying temperature T and noise r. Plot Quality (QED>=0.6, SA<=4) vs Diversity to confirm the Pareto frontier
  2. Sampling Ablation: Compare sampling using the standard velocity update (Eq. 2) vs. the direct distribution sampling (Eq. 6). Verify that Eq. 6 leads to "refinement" behavior (decaying token changes)
  3. Scaffold Retention: Run the fragment-constrained task (e.g., Scaffold Decoration). Measure the drop in Validity to quantify the cost of "naive overwriting"

## Open Questions the Paper Calls Out
None

## Limitations
- Fragment-constrained generation shows notable validity drops when prompt tokens are naively overwritten, suggesting the model struggles with forced modifications
- The heuristic sampling approach lacks theoretical grounding compared to standard flow velocity updates
- Experimental comparisons often use different baseline model sizes (12 vs 36 layers), making performance differences difficult to attribute solely to architectural improvements
- The method's dependence on length prediction through a bandit approach for arbitrary-length generation introduces potential failure points

## Confidence
- **Uniform-source refinement improves quality-diversity frontier**: High
- **Direct distribution sampling outperforms standard flow updates**: Medium
- **Hybrid GA-PPO achieves SOTA on PMO benchmark**: Medium
- **Lead optimization outperforms baselines**: Medium
- **Fragment-constrained generation validity**: Low-Medium

## Next Checks
1. **Robustness on Complex Ring Systems**: Generate molecules with multiple fused rings and bridgehead structures to test whether the uniform-source refinement can handle highly structured molecular scaffolds without the guidance of a specific prefix.

2. **Sampling Stability Ablation**: Implement both the standard velocity update (Eq. 2) and direct distribution sampling (Eq. 6) with identical training conditions and loss weighting. Measure not just final quality but also the stability and reproducibility of sampling across multiple runs.

3. **Fragment Constraint Validity Optimization**: Systematically vary the degree of prompt token modification in fragment-constrained tasks (from 0% to 100% changes) and measure the corresponding validity drop. Test whether alternative refinement strategies can maintain higher validity while still achieving the desired chemical modifications.