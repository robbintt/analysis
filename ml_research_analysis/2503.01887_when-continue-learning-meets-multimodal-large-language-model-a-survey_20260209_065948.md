---
ver: rpa2
title: 'When Continue Learning Meets Multimodal Large Language Model: A Survey'
arxiv_id: '2503.01887'
source_url: https://arxiv.org/abs/2503.01887
tags:
- learning
- arxiv
- continual
- multimodal
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This survey systematically reviews the integration of continual\
  \ learning (CL) and multimodal large language models (MLLMs), analyzing 440 research\
  \ papers. The key problem addressed is how to adapt pre-trained MLLMs to dynamic\
  \ data distributions and tasks without catastrophic forgetting\u2014where fine-tuning\
  \ for new tasks degrades performance on previously learned knowledge."
---

# When Continue Learning Meets Multimodal Large Language Model: A Survey

## Quick Facts
- **arXiv ID**: 2503.01887
- **Source URL**: https://arxiv.org/abs/2503.01887
- **Reference count**: 40
- **Primary result**: Systematic survey of 440 papers on continual learning for MLLMs, addressing catastrophic forgetting and proposing benchmarks and future directions.

## Executive Summary
This survey systematically reviews the integration of continual learning (CL) and multimodal large language models (MLLMs), analyzing 440 research papers. The key problem addressed is how to adapt pre-trained MLLMs to dynamic data distributions and tasks without catastrophic forgettingâ€”where fine-tuning for new tasks degrades performance on previously learned knowledge. The survey categorizes innovations into framework, method, and module levels, highlighting techniques like expert routing, parameter-efficient tuning, and knowledge distillation. It also introduces new benchmarks like CoIN and ViLCo-Bench to evaluate continual learning in MLLMs, demonstrating performance improvements in task adaptation while minimizing forgetting.

## Method Summary
The survey reviews multiple continual learning methods for MLLMs, focusing on parameter-efficient fine-tuning strategies like LoRA, prompt tuning, and knowledge distillation. Representative approaches include DualLoRA (orthogonal + residual low-rank adapters with dynamic memory), C-LoRA (continual LoRA in cross-attention layers), NoRGa (non-linear residual gate), and ZAF (zero-shot stability regularization with EMA-LoRA). The sequential fine-tuning paradigm is applied across task streams, with most methods using parameter-efficient tuning rather than full fine-tuning. Evaluation is conducted on benchmarks like CoIN, ViLCo-Bench, CLiMB, and COAST, measuring mean average accuracy (MAA), backward transfer (BWT), and forgetting rate (Fgt).

## Key Results
- Novel benchmarks like CoIN and ViLCo-Bench evaluate continual learning in MLLMs across 8 tasks and 3 video-language tasks respectively.
- Methods like DualLoRA and C-LoRA demonstrate performance improvements in task adaptation while minimizing catastrophic forgetting.
- The survey identifies challenges including catastrophic forgetting, benchmark standardization, and interpretability, highlighting future trends toward more efficient, scalable, and interpretable continual learning solutions.

## Why This Works (Mechanism)

### Mechanism 1: Modular Expert Routing for Task Isolation
- **Claim**: Routing inputs through specialized sub-networks (experts) can reduce interference between new and old tasks in MLLMs.
- **Mechanism**: Architectures like MoVA use a gating network to dynamically select which expert (sub-network) processes a given input. This allows different experts to specialize on different task types or data domains, so updates for a new task primarily affect one expert, reducing overwriting of parameters critical for previous tasks.
- **Core assumption**: Tasks are learnable by distinct specialized functions, and the routing mechanism can reliably assign inputs to the correct expert.
- **Evidence anchors**:
  - [abstract] "framework innovations like MoVA's expert routing"
  - [section] "MoVA incorporates coarse-grained context-aware expert routing and fine-grained expert fusion" (Table 1, Appendix 7.1.1)
  - [corpus] Related work on Mixture-of-Experts for MLLMs supports modular specialization, though specific continual learning benefits are not directly proven in this survey.
- **Break condition**: Fails if tasks are highly overlapping or if the router consistently misassigns inputs, causing all experts to be updated for every task, negating isolation.

### Mechanism 2: Parameter-Efficient Tuning with Prompt Reservoirs
- **Claim**: Storing task-specific knowledge in small, tunable prompt parameters or low-rank adapters preserves the bulk of the pre-trained MLLM, mitigating catastrophic forgetting.
- **Mechanism**: Methods like DualLoRA or prompt pools freeze the main model weights and only train a tiny set of parameters (e.g., prompts, LoRA adapters) per task. During inference, the relevant prompt/adapter for the current task is retrieved and appended. Since the core model is unchanged, knowledge from prior tasks remains accessible via their corresponding prompts.
- **Core assumption**: The pre-trained MLLM provides a sufficiently general feature space that task-specific adjustments can be captured in low-dimensional parameter spaces without cross-interference.
- **Evidence anchors**:
  - [abstract] "method innovations such as gradient calibration and prompt tuning"
  - [section] "DualLoRA utilizes orthogonal and residual low-rank adapters along with a dynamic memory mechanism" (Table 8)
  - [corpus] The MCITlib paper confirms prompt-based and adapter-based methods are central to MLLM continual learning benchmarks.
- **Break condition**: Fails when tasks require fundamental changes to the pre-trained model's representations that cannot be expressed via prompts/low-rank adapters, or when the number of tasks grows beyond the manageable size of the prompt/adapter pool.

### Mechanism 3: Regularization and Replay via Knowledge Distillation
- **Claim**: Constraining updates on important weights or replaying synthetic data from past tasks can anchor the model to its previous state during new task learning.
- **Mechanism**: Regularization techniques (e.g., EWC) calculate importance weights for parameters and penalize changes to them. Replay methods generate synthetic examples (e.g., via diffusion models or LLMs) from previous task distributions and interleave them with new task data during training. This acts as a form of rehearsal, explicitly reminding the model of old knowledge.
- **Core assumption**: Importance of parameters for old tasks can be accurately estimated, and synthetic or replayed data sufficiently approximates the original task distribution to prevent drift.
- **Evidence anchors**:
  - [abstract] "techniques like knowledge distillation, parameter-efficient tuning, and dynamic memory management"
  - [section] "DSSP leverages domain sharing and task-specific prompt learning... reduces catastrophic forgetting" (Table 4); "ZAF preserves knowledge through zero-shot stability regularization" (Table 8)
  - [corpus] Weak direct evidence; corpus papers focus on benchmarking, not validating specific distillation/replay mechanisms for MLLM-CL.
- **Break condition**: Fails if importance estimation is noisy, leading to under-regularization (forgetting) or over-regularization (rigidity), or if synthetic replay data introduces bias.

## Foundational Learning

- **Concept: Catastrophic Forgetting**
  - **Why needed here**: It is the central problem the entire survey addresses. All proposed methods are attempts to mitigate it.
  - **Quick check question**: Can you explain why a neural network forgets a previously learned task when trained on a new one with standard gradient descent?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT)**
  - **Why needed here**: Methods like LoRA, adapters, and prompt tuning are core building blocks of the most popular MLLM continual learning approaches (e.g., DualLoRA, C-LoRA, Prompt-based CL).
  - **Quick check question**: How does freezing most of a model's weights and only training a small set of adapter matrices help in a sequential learning setting?

- **Concept: Multimodal Large Language Model (MLLM) Architecture**
  - **Why needed here**: Understanding the fusion of a vision encoder, a projection layer, and a frozen LLM is critical to seeing where continual learning interventions are applied (e.g., tuning the projection layer, prompts, or adapters).
  - **Quick check question**: In a typical MLLM, which components are usually frozen and which are fine-tuned for downstream tasks?

## Architecture Onboarding

- **Component map**: The MLLM-CL ecosystem consists of:
  1. **Base MLLM**: Pre-trained vision encoder, connector, and LLM.
  2. **CL Strategy Module**: Implements expert routing, prompt pools, LoRA adapters, or regularization losses.
  3. **Memory/Buffer**: Stores exemplars, synthetic data, or prompt/adapter indices for past tasks.
  4. **Benchmark & Evaluator**: Datasets like CoIN, COAST, ViLCo-Bench and metrics like Average Accuracy and Backward Transfer.

- **Critical path**: Start by implementing a rehearsal-free method like a prompt pool on a simple benchmark (e.g., CoIN) using a small MLLM (e.g., LLaVA-7B). This requires minimal memory and provides a baseline. Then, integrate a simple regularization loss (EWC) to observe its effect.

- **Design tradeoffs**:
  - **Replay vs. Replay-free**: Replay (using stored or synthetic data) is often more effective but raises privacy, storage, and fairness concerns. Replay-free methods are more elegant but can be less robust.
  - **Parameter Growth vs. Fixed Capacity**: Growing models (e.g., adding experts/adapters per task) avoid interference but increase inference cost. Fixed-capacity models must carefully manage interference within the same parameters.

- **Failure signatures**:
  - **Catastrophic forgetting** observed as a sharp drop in accuracy on all previous tasks after learning a new one.
  - **Rigidity/intransigence** where the model fails to learn new tasks well because it's overly constrained by preservation mechanisms.
  - **Task confusion** where the model cannot correctly identify which task it should solve for a given input.

- **First 3 experiments**:
  1. **Establish a Baseline**: Sequentially fine-tune a small MLLM (e.g., LLaVA-7B) on tasks from the CoIN benchmark without any CL strategy. Measure the catastrophic forgetting curve.
  2. **Implement a Prompt Pool**: Add a simple prompt pool where each task gets a learnable prompt. Train prompts sequentially while freezing the MLLM. Compare against baseline.
  3. **Integrate Knowledge Distillation**: Add a distillation loss using a teacher model (a copy of the student before new task training) to the prompt pool setup. Measure if it improves backward transfer (reduces forgetting) compared to prompt pool alone.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can algorithms effectively balance forgetting management with learning efficiency in MLLMs as the number of tasks increases?
- Basis in paper: [explicit] Section 5.1 states that balancing forgetting management with learning efficiency is a "complex optimization challenge" and a key future trend.
- Why unresolved: Existing methods often struggle with the trade-off between retaining old knowledge and rapidly adapting to new tasks without significant computational overhead.
- What evidence would resolve it: Development of training frameworks that maintain constant or logarithmic efficiency scaling while preserving knowledge over long sequences of diverse tasks.

### Open Question 2
- Question: What standardized metrics and evaluation frameworks are necessary to assess multi-task learning and cross-modal consistency in MLLM continual learning?
- Basis in paper: [explicit] Section 5.2 notes that current benchmarks are scarce and highlights the urgent need for "comprehensive and systematic evaluation benchmarks" with standardization.
- Why unresolved: Evaluation currently lacks unified metrics for assessing cross-modal consistency and knowledge transfer across dynamic tasks.
- What evidence would resolve it: The establishment of a unified benchmark suite that includes specific metrics for cross-modal consistency and long-term knowledge transfer.

### Open Question 3
- Question: How can architectural designs be improved to enhance the transparency and traceability of decision-making processes in continually learning MLLMs?
- Basis in paper: [explicit] Section 5.3 suggests that future research should design "more transparent and traceable architectures" to address the complexity of modality integration.
- Why unresolved: The integration of information across modalities and tasks in continual learning environments makes model decisions opaque and difficult to interpret.
- What evidence would resolve it: Architectures incorporating Explainable AI (XAI) features that allow clear tracking and analysis of decision rationales during cross-modal tasks.

## Limitations
- The survey does not provide implementation-level details or code for the methods discussed, making direct reproduction challenging.
- Key hyperparameters, architectural specifics, and training configurations remain unspecified, relying on related work for validation.
- Evaluation relies heavily on benchmark papers rather than direct validation of the surveyed methods' effectiveness.

## Confidence
- **High confidence**: The existence of catastrophic forgetting as a central problem in MLLM continual learning; the identification of benchmark datasets and evaluation metrics; the general categorization of methods (framework, method, module innovations).
- **Medium confidence**: The effectiveness of specific mechanisms like expert routing and prompt reservoirs, as these are supported by related work but not directly validated in this survey.
- **Low confidence**: The exact implementation details and hyperparameter choices for reproducing any specific method, as these are not provided.

## Next Checks
1. **Implementation verification**: Select one method (e.g., C-LoRA) and attempt to implement it on a simple benchmark (e.g., CoIN with LLaVA-7B) to confirm the survey's claims about its architecture and effectiveness.
2. **Mechanism isolation**: Conduct ablation studies on key mechanisms (e.g., expert routing vs. prompt tuning) to determine which components contribute most to reducing catastrophic forgetting.
3. **Generalization testing**: Evaluate the best-performing method across multiple benchmarks (e.g., CoIN, ViLCo-Bench, CLiMB) to verify its robustness and identify any overfitting to specific datasets.