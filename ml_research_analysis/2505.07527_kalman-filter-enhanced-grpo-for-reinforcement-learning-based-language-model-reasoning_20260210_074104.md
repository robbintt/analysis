---
ver: rpa2
title: Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model
  Reasoning
arxiv_id: '2505.07527'
source_url: https://arxiv.org/abs/2505.07527
tags:
- krpo
- policy
- grpo
- group
- kalman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes KRPO, a Kalman filter-enhanced version of\
  \ Group Relative Policy Optimization (GRPO) for improving language model reasoning.\
  \ The method replaces GRPO\u2019s group mean baseline with a Kalman-filtered baseline\
  \ and uncertainty estimate, providing more accurate and adaptive advantage estimation\
  \ without adding new model parameters."
---

# Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning

## Quick Facts
- arXiv ID: 2505.07527
- Source URL: https://arxiv.org/abs/2505.07527
- Authors: Hu Wang; Congbo Ma; Ian Reid; Mohammad Yaqub
- Reference count: 31
- Primary result: KRPO improves LLM reasoning accuracy by 2.3–5.4% on arithmetic and 4.6–17.9% on OpenMath-Instruct over GRPO baseline

## Executive Summary
This paper introduces KRPO, a Kalman filter-enhanced version of Group Relative Policy Optimization for improving language model reasoning through better advantage estimation. The method replaces GRPO's group mean baseline with a Kalman-filtered baseline and uncertainty estimate, providing more accurate and adaptive advantage computation without additional model parameters. Evaluated on math reasoning datasets using Llama-3.2-1B and Qwen2.5-0.5B/1.5B models, KRPO consistently outperforms GRPO and other baselines while adding negligible computational overhead.

## Method Summary
KRPO enhances GRPO by replacing the group mean baseline with a Kalman filter-based estimation. The filter recursively maintains a posterior estimate of the latent reward baseline and its uncertainty via prediction and update steps. The advantage is computed as the normalized difference between observed reward and the filtered baseline estimate, scaled by the estimated variance. This approach provides more adaptive advantage normalization without requiring additional learned parameters, making it computationally efficient compared to traditional value network baselines.

## Key Results
- On Arithmetic dataset: KRPO improves accuracy by 2.3–5.4% across difficulty levels (5.1% easy, 5.4% normal, 2.3% hard)
- On OpenMath-Instruct: Improvements range from 4.6% (Algebra) to 17.9% (Probability) over GRPO baseline
- Training curves show KRPO converges faster and reaches higher rewards, especially on harder tasks

## Why This Works (Mechanism)

### Mechanism 1
Replacing the static group-mean baseline with a Kalman-filtered baseline reduces variance in advantage estimation. The Kalman filter recursively maintains a posterior estimate of the latent reward baseline (x̂_{i|i}) and its uncertainty (P_{i|i}) via prediction and update steps. New observations are weighted by the Kalman gain K_i = P_{i|i-1} / (P_{i|i-1} + R), which adaptively balances prior belief against incoming noisy rewards rather than treating all samples equally. Core assumption: Observed rewards are noisy measurements of a latent, slowly-varying reward baseline with Gaussian noise characteristics.

### Mechanism 2
Normalizing advantages by the Kalman-estimated variance (not just mean-centering) stabilizes policy gradient scale. The advantage is computed as A_i = (r_i - x̂_{i|i}) / √(P_{i|i} + ε). The denominator scales the advantage by estimated uncertainty, shrinking updates when the baseline estimate is uncertain and amplifying signal when confident. Core assumption: The posterior variance P_{i|i} meaningfully captures epistemic uncertainty about the baseline.

### Mechanism 3
KRPO provides these benefits without additional learned parameters, incurring negligible computational overhead. The Kalman filter is a non-parametric, closed-form recursive estimator. It adds only two scalar hyperparameters (Q, R) and O(1) operations per reward observation, avoiding the need for a value network. Core assumption: A scalar univariate Kalman filter suffices for group-level baseline tracking; per-token or per-state value estimation is unnecessary.

## Foundational Learning

- Concept: Advantage functions and baselines in policy gradient methods
  - Why needed here: KRPO's entire contribution is a better baseline for advantage estimation; understanding why baselines reduce variance is prerequisite.
  - Quick check question: Why does subtracting a baseline from returns not introduce bias in policy gradient estimates?

- Concept: Group Relative Policy Optimization (GRPO) for LLMs
  - Why needed here: KRPO is a drop-in modification to GRPO's advantage computation; you must understand GRPO's group sampling and mean-baseline approach.
  - Quick check question: In GRPO, how is the baseline computed for a group of n sampled responses to the same prompt?

- Concept: Kalman filter fundamentals (state estimation, process/measurement noise)
  - Why needed here: The core innovation is applying a 1D Kalman filter to reward baseline tracking; understanding prediction/update steps and Q/R tuning is essential.
  - Quick check question: What happens to the Kalman gain K_i if measurement noise R is set very low relative to prior variance P?

## Architecture Onboarding

- Component map: Sample n responses → Compute rewards → Update Kalman filter → Compute variance-normalized advantages → Policy loss with KL penalty

- Critical path:
  1. Sample n responses per prompt (group size typically 12)
  2. Compute sparse rewards (exact match=1.0, partial=0.5, else=0)
  3. Update Kalman filter with each reward in group
  4. Compute A_i = (r_i - x̂) / √(P + ε) for each trajectory
  5. Feed advantages into GRPO's clipped policy loss with KL term

- Design tradeoffs:
  - Q (process noise): Higher Q → faster adaptation to shifting baselines but noisier estimates; default 1e-5
  - R (measurement noise): Higher R → smoother estimates but slower response; default 1e-2. Paper warns R=1e-3 causes instability
  - Group size: Smaller groups → noisier mean baselines; KRPO gap widens (Fig. 7a), suggesting KRPO helps more in sparse-sample regimes

- Failure signatures:
  - Rewards stagnate or diverge: Check if R is too low (<1e-3), causing overconfident updates
  - No improvement over GRPO: May indicate rewards are already low-variance (easy tasks, strong base models); KRPO provides less marginal benefit
  - KL divergence explodes: Monitor α (KL weight); paper finds α=0.01 optimal, α=0 or 0.05 underperform

- First 3 experiments:
  1. Replicate GRPO vs KRPO on Arithmetic-Normal with group size 12, Llama-3.2-1B-Instruct; plot reward curves to verify faster convergence and higher asymptote
  2. Ablate Q and R: sweep Q∈{1e-5, 1e-4, 1e-3}, R∈{1e-3, 1e-2, 1e-1} on a held-out subset; confirm robustness band (R≥1e-2 is safe)
  3. Test on a harder dataset (MATH500 or AIME) to verify that KRPO's advantage scales with task difficulty and reward variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance gain of KRPO over GRPO transfer to tasks with inherently noisy and subjective reward signals, such as creative writing or summarization?
- Basis in paper: The Conclusion states: "In future work, we plan to evaluate KRPO on tasks such as creative writing and summarization, where reward signals are inherently noisier and more subjective."
- Why unresolved: The current study exclusively evaluates the method on mathematical reasoning datasets where rewards are deterministic and objective (verifiable answers).
- Evidence: Empirical results comparing KRPO against baselines on standard creative writing or summarization benchmarks using reward models trained on human preferences.

### Open Question 2
- Question: Can the Kalman filter's noise hyperparameters (process noise $Q$ and measurement noise $R$) be adapted online to remove the need for manual tuning?
- Basis in paper: Section 5.3.2 demonstrates that performance is sensitive to the measurement noise $R$, noting that "overly small $R$ leads to unstable or poor learning signals," and different tasks may require different settings.
- Why unresolved: The current implementation relies on fixed hyperparameters ($Q=1e-5, R=1e-2$), which may not be optimal for all reward landscapes.
- Evidence: An adaptive version of KRPO that automatically adjusts $Q$ and $R$ based on observed reward variance, showing stable performance across distinct datasets without manual re-tuning.

### Open Question 3
- Question: Does the sequential application of the Kalman filter introduce bias when observations are not perfectly randomized or when reward distributions shift drastically?
- Basis in paper: Section 3.3 notes that "observations are completely randomly sampled" to avoid temporal dependence, but acknowledges the filter may introduce "a small bias" if the arbitrary order is not carefully managed.
- Why unresolved: The paper asserts robustness due to random shuffling but does not quantify the bias or performance degradation if temporal correlations exist in the data sampling.
- Evidence: An ablation study analyzing KRPO performance when processing batches in sorted order (e.g., by difficulty) versus random order, measuring the divergence in policy convergence.

## Limitations
- The method assumes reward baselines are well-modeled as noisy observations of a slowly-varying latent process, which may not hold for tasks with high variance in response quality or non-stationary reward distributions
- Evaluation is limited to mathematical reasoning tasks with binary/sparse rewards, leaving uncertainty about generalization to more nuanced reward structures
- The method relies on somewhat arbitrary hyperparameter choices (Q=1e-5, R=1e-2) that may require task-specific tuning

## Confidence
- **High confidence**: Claims about computational efficiency (negligible overhead) and hyperparameter robustness (Q=1e-5, R=1e-2) are well-supported by ablation studies and timing comparisons
- **Medium confidence**: Claims about improved convergence speed and accuracy gains are moderately supported but could benefit from more extensive statistical analysis and larger-scale experiments
- **Medium confidence**: Claims about the Kalman filter mechanism's superiority over group mean baselines are theoretically sound but could be more rigorously validated with ablation of individual components

## Next Checks
1. Evaluate KRPO on non-mathematical reasoning tasks (e.g., commonsense reasoning, code generation) with continuous reward signals to assess generalizability beyond the current evaluation scope

2. Implement variants of KRPO with (a) Kalman mean estimation only (no variance normalization), (b) variance normalization only (no Kalman filtering), and (c) traditional value network baseline to isolate which mechanism drives the improvements

3. Test KRPO with larger language models (7B+ parameters) on the same tasks to verify that the method scales and maintains its advantages as model capacity increases