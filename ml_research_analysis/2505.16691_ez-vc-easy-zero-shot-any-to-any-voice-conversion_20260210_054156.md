---
ver: rpa2
title: 'EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion'
arxiv_id: '2505.16691'
source_url: https://arxiv.org/abs/2505.16691
tags:
- speech
- voice
- speaker
- arxiv
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EZ-VC introduces a simple zero-shot any-to-any voice conversion
  architecture that achieves state-of-the-art results by combining discrete speech
  representations from a multilingual self-supervised encoder with a non-autoregressive
  flow-matching speech decoder. Unlike existing methods, it does not require multiple
  encoders or complex feature disentanglement, enabling it to generalize across unseen
  languages, accents, and speakers.
---

# EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion

## Quick Facts
- arXiv ID: 2505.16691
- Source URL: https://arxiv.org/abs/2505.16691
- Reference count: 3
- Primary result: Achieves SOTA naturalness (NMOS 3.91) and speaker similarity (SSIM 0.71) in zero-shot any-to-any voice conversion across 12,840 hours of multilingual training data

## Executive Summary
EZ-VC introduces a simplified architecture for zero-shot voice conversion that achieves state-of-the-art results without requiring multiple encoders or complex feature disentanglement. The system combines discrete speech representations from a multilingual self-supervised encoder with a non-autoregressive flow-matching speech decoder. Unlike prior methods that use multiple encoders, adapters, or explicit disentanglement modules, EZ-VC relies on k-means quantization of SSL embeddings to implicitly separate content from speaker characteristics. Trained on 12,840 hours of multilingual data, it significantly outperforms baselines on both seen and unseen languages while maintaining architectural simplicity.

## Method Summary
EZ-VC uses a single SSL encoder (Xeus) to extract frame-level embeddings from the 14th layer, which are then discretized using 500-cluster k-means quantization. The resulting discrete units, combined with target reference mel-spectrograms, are fed to an F5-TTS decoder that performs conditional flow matching with an infilling task to reconstruct speech. A BigVGAN vocoder converts the generated mel-spectrograms to waveforms. The key innovation is using quantization as implicit disentanglement, eliminating the need for multiple encoders or adapters found in prior methods.

## Key Results
- Achieves NMOS of 3.91 (vs. 3.67 for best baseline) on naturalness
- Achieves SSIM of 0.71 (vs. 0.69 for best baseline) on speaker similarity
- Shows strong cross-lingual performance with UTMOS scores of 3.71 (German) and 3.49 (Spanish) versus 2.83 and 3.24 for the best baseline

## Why This Works (Mechanism)

### Mechanism 1: Discrete Speech Units for Content Preservation
Quantized representations from SSL encoders preserve linguistic content while discarding speaker-specific details, enabling cross-lingual transfer. The Xeus SSL encoder produces frame-level embeddings from its 14th layer, which are clustered into 500 discrete units. These units represent phonetic content while removing speaker information through quantization.

### Mechanism 2: Flow Matching Decoder with Infilling Task
Non-autoregressive conditional flow matching enables high-quality speech synthesis with speaker conditioning from reference audio. The F5-TTS architecture reconstructs mel-spectrograms from discrete units using an infilling task where speaker attributes come from unmasked mel-spectrogram portions while content comes from input units.

### Mechanism 3: Unified Architecture Avoiding Explicit Disentanglement
A single encoder + decoder suffices when discrete units implicitly separate content from speaker characteristics. Unlike prior works using multiple encoders or adapters, EZ-VC uses only Xeus + k-means for encoding and F5-TTS for decoding, with quantization acting as implicit disentanglement.

## Foundational Learning

**Self-Supervised Speech Representations (SSL)**
- Why needed here: Understanding how Xeus encodes speech across 4,000 languages explains cross-lingual generalization
- Quick check question: Can you explain why SSL models trained on multilingual data produce more language-agnostic representations than monolingual models?

**Conditional Flow Matching (CFM)**
- Why needed here: The decoder uses CFM for speech generation; understanding its training/inference helps debug synthesis quality
- Quick check question: How does CFM differ from standard diffusion in terms of sampling efficiency and conditioning?

**K-Means Quantization for Speech**
- Why needed here: The 500-cluster k-means converts continuous embeddings to discrete units; vocabulary size affects content fidelity
- Quick check question: What tradeoffs exist between k-means vocabulary size (e.g., 100 vs. 500 vs. 1000 clusters) for speech content preservation?

## Architecture Onboarding

**Component map:**
Xeus SSL encoder (frozen) → K-means quantizer (trained) → F5-TTS decoder (trained) → BigVGAN vocoder (trained)

**Critical path:**
1. Audio → Xeus encoder → layer 14 embeddings → k-means → discrete units (deduplicated)
2. Discrete units + target mel reference → F5-TTS (infilling) → generated mel
3. Generated mel → BigVGAN → output audio

**Design tradeoffs:**
- Single encoder vs. multiple: Simpler but relies on quantization quality for disentanglement
- 500-cluster k-means: Compact but may lose rare phonetic distinctions in low-resource languages
- Non-autoregressive decoder: Faster inference than autoregressive LMs but reduced prosody control

**Failure signatures:**
- Robotic/artificial output: Check Xeus layer selection (14th layer assumed optimal)
- Poor speaker similarity: Verify target reference mel quality and length
- Cross-lingual degradation: Examine k-means coverage for unseen language phonemes

**First 3 experiments:**
1. Ablate k-means vocabulary size (300, 500, 1000 clusters) on seen vs. unseen languages to measure content/speaker tradeoffs
2. Compare Xeus layers (12th, 14th, 16th) for quantization to validate layer-14 assumption
3. Test reference audio length sensitivity (1s, 3s, 5s, 10s) to determine minimum viable conditioning signal

## Open Questions the Paper Calls Out
None

## Limitations
- Cross-lingual generalization scope limited to only 2 languages (German, Spanish) with limited speaker diversity
- Quantization-induced information loss may degrade rare phoneme representation in unseen languages
- Speaker disentanglement reliability unproven when source and target speakers have similar acoustic profiles

## Confidence

**High Confidence:**
- Architectural simplification compared to multi-encoder baselines is correctly characterized
- CFM decoder architecture can perform infilling-based speech reconstruction
- 12,840 hours multilingual training data provides reasonable coverage for most common languages

**Medium Confidence:**
- 500-cluster k-means provides optimal tradeoff between compression and content preservation
- Layer 14 of Xeus captures maximally language-agnostic phonetic content
- Cross-lingual performance on German/Spanish generalizes to other unseen languages

**Low Confidence:**
- Architecture works equally well for all 4,000 languages Xeus was trained on
- No explicit disentanglement is needed for robust voice conversion across diverse speaker pairs

## Next Checks
1. Conduct systematic evaluation of k-means vocabulary sizes (100, 300, 500, 1000, 2000 clusters) on both seen and unseen languages to quantify the tradeoff between content preservation and model complexity
2. Compare Xeus layer 12, 14, and 16 for quantization quality by training identical models with each layer's embeddings and evaluating on cross-lingual conversion tasks
3. Systematically test reference audio lengths from 1 second to 10 seconds in 1-second increments to determine the minimum viable conditioning signal while maintaining conversion quality