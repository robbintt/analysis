---
ver: rpa2
title: Can Language Models Compose Skills In-Context?
arxiv_id: '2510.22993'
source_url: https://arxiv.org/abs/2510.22993
tags:
- examples
- simple
- task
- accuracy
- composite
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Can Language Models Compose Skills In-Context?

## Quick Facts
- **arXiv ID**: 2510.22993
- **Source URL**: https://arxiv.org/abs/2510.22993
- **Reference count**: 40
- **Key outcome**: Language models struggle to compose multiple skills from in-context examples, but Expanded Chain-of-Thought (ExpCoT) improves performance by 5-15%.

## Executive Summary
This paper investigates whether language models can compose multiple skills from in-context examples. The authors find that while models can perform individual simple tasks well, they struggle to combine these skills for composite tasks. Surprisingly, providing more examples of simple tasks can harm performance due to the model's inability to recognize compositional structure. The proposed solution, Expanded Chain-of-Thought (ExpCoT), marks which compositional step each example belongs to, significantly improving composition accuracy across multiple model sizes.

## Method Summary
The paper evaluates in-context composition using synthetic operator-based tasks (e.g., opposition+swap, pastTense+capitalization). Prompts contain k examples from simple task 1, k examples from simple task 2, kc composite task examples, and a composite query. The ExpCoT method converts simple task examples to CoT format with explicit step markers (Step1:, Step2:, ???). Experiments test models from 7B to 70B parameters across varying k and kc values, with accuracy and correspondence metrics measuring performance.

## Key Results
- Simple task examples can negatively impact composite task performance due to models matching queries to examples based on surface-level operators
- ExpCoT significantly improves composition accuracy by 5-15% across models by explicitly aligning examples to compositional steps
- Attention analysis reveals models don't distinguish simple vs. composite queries, explaining composition failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Simple task examples can harm composite task performance when models fail to recognize compositional structure.
- Mechanism: Models match composite queries to in-context examples based on surface-level cues (operators), with probability proportional to example counts per task type. More simple task examples → higher probability of misapplying a simple task to a composite query.
- Core assumption: Models treat all in-context examples as potentially relevant to the query without distinguishing their compositional role.
- Evidence anchors:
  - [abstract]: "results reveal that simple task examples can have a surprising negative impact on the performance, because the models generally struggle to recognize and assemble the skills correctly"
  - [Section 3.2]: "the model may ignore the compositional structure: it may match the composite query to examples from any task and perform only the matched task"
  - [corpus]: Corpus evidence is limited—neighbor papers focus on multimodal composition and visual skills but do not directly replicate this negative-example effect.
- Break condition: When composite task examples dominate (high kc) or tasks are trivial, the negative effect is attenuated.

### Mechanism 2
- Claim: Models rely primarily on operators (syntactic markers) rather than semantic content when aligning queries to examples.
- Mechanism: Ablation shows that replacing operators eliminates the negative effect of simple task examples, while replacing content preserves it. Operators serve as task-type identifiers; without distinct operators, models cannot route examples to correct reasoning steps.
- Core assumption: The attention mechanism attends to surface tokens (operators) more than to the semantic transformation demonstrated in content.
- Evidence anchors:
  - [Section 3.3]: "in the irrelevant operator setting, increasing k has little impact after ablating the operators... the utilization of the examples is largely based on the operators"
  - [Figure 4]: Performance change curves flatten after operator ablation
  - [corpus]: No direct corpus confirmation; related work on compositional reasoning does not isolate operator vs. content effects.
- Break condition: If operators are semantically meaningful (e.g., natural language "opposite of"), models may attend more to content.

### Mechanism 3
- Claim: Explicit step alignment via Expanded Chain-of-Thought (ExpCoT) improves composition by marking which compositional step each example belongs to.
- Mechanism: ExpCoT converts simple task examples into CoT format with missing steps marked by "???". This signals to the model: "this example applies to Step t only." The model can then attend selectively to appropriate examples per step.
- Core assumption: Models can use explicit structural markers (Step1:, ???) to constrain attention and routing decisions.
- Evidence anchors:
  - [Section 4.1]: "This will explicitly align the examples for better utilization... ExpCoT leads to significant improvement"
  - [Table 3]: Accuracy improvements of 5–15% across models with ExpCoT vs. vanilla
  - [corpus]: Neighbor paper "Multimodal LLMs Do Not Compose Skills Optimally Across Modalities" finds similar composition failures in multimodal settings, supporting generalizability of the alignment problem.
- Break condition: Small models may still lack capacity to exploit markers; very complex compositions (T > 3) may exceed working context.

## Foundational Learning

- Concept: In-Context Learning (ICL)
  - Why needed here: The entire paper assumes models can learn tasks from in-context examples without parameter updates. Without understanding ICL basics, the composition problem is unmotivated.
  - Quick check question: Can you explain why ICL enables task adaptation at inference time, and what factors affect ICL sensitivity?

- Concept: Chain-of-Thought (CoT) Reasoning
  - Why needed here: The proposed solution (ExpCoT) extends CoT by adding step markers. Understanding standard CoT is prerequisite to seeing why naive CoT fails here.
  - Quick check question: How does CoT change the effective computational depth of a transformer for serial reasoning tasks?

- Concept: Attention Mechanisms in Transformers
  - Why needed here: The paper uses attention similarity analysis to show models don't distinguish simple vs. composite queries. Interpreting these results requires attention literacy.
  - Quick check question: What does high cosine similarity between attention outputs for different query types imply about representational separation?

## Architecture Onboarding

- Component map:
  - Prompt constructor -> ExpCoT formatter -> Model backbone -> Evaluation harness

- Critical path:
  1. Generate or load simple/composite task examples
  2. Apply ExpCoT transformation to all examples
  3. Shuffle examples (to control order effects)
  4. Run inference on composite queries
  5. Compute accuracy and correspondence metrics

- Design tradeoffs:
  - **More composite examples (kc)**: Improves performance but increases prompt length; diminishing returns after kc ≈ 5
  - **More simple examples (k)**: With vanilla prompting, hurts performance; with ExpCoT, helps in larger models but adds token cost
  - **Step markers**: ExpCoT adds ~2–3 tokens per example; for k=30 per simple task, this is ~180 extra tokens—acceptable for 4K+ context models

- Failure signatures:
  - **Task confusion**: Output matches a simple task (t1 or t2) instead of composite (t0)—check correspondence metrics
  - **CoT misalignment**: Model generates CoT steps but applies wrong operation at a step (e.g., both tasks in step 2)
  - **Attention collapse**: High similarity between attention patterns for simple and composite queries (Figure 5)—indicates model hasn't learned compositional structure

- First 3 experiments:
  1. Replicate Figure 2 on a single model: vary k (2–30) with fixed kc=5 on opposition+swap task; confirm negative slope in accuracy vs. k
  2. Apply ExpCoT to the same setup; verify accuracy improvement and reduced negative slope (replicate Figure 7)
  3. Run attention similarity analysis (Section 3.4): compare cosine similarity matrices for a high-accuracy task (e.g., opposition+pastTense) vs. low-accuracy task (opposition+swap); confirm higher separation in high-accuracy case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the limitations in in-context composition identified in open-source models persist in state-of-the-art proprietary models (e.g., GPT-5) and complex, real-world tasks?
- Basis in paper: [explicit] The authors state that due to resource limitations, empirical studies did not include the most powerful models or complex tasks targeted by AI assistants.
- Why unresolved: The experiments were restricted to specific open-source models (Llama, Mistral) on synthetic linguistic and logical tasks.
- What evidence would resolve it: Replicating the "negative impact" and ExpCoT experiments on proprietary models using complex, non-synthetic benchmarks.

### Open Question 2
- Question: Can LLMs effectively self-annotate data with skill tags to bootstrap their own compositional reasoning without human supervision?
- Basis in paper: [explicit] The paper suggests using LLMs to generate annotations for self-boosting as an alternative to expensive human labeling, leaving this as an interesting direction for future work.
- Why unresolved: The ExpCoT method requires explicit step annotations (e.g., "Step1:", "Step2:"), which the paper notes are expensive to produce via human supervision at scale.
- What evidence would resolve it: Demonstrating that models fine-tuned on LLM-generated synthetic annotations achieve comparable performance to those trained on human-annotated data.

### Open Question 3
- Question: Does the Expanded Chain-of-Thought (ExpCoT) method remain effective as the number of compositional steps ($T$) increases significantly?
- Basis in paper: [inferred] Theoretical analysis covers a general $T$-step composition, but empirical experiments focused primarily on 2-step composite tasks.
- Why unresolved: The complexity of aligning examples and filling missing steps may degrade or lead to context window issues in deeper reasoning chains.
- What evidence would resolve it: Performance evaluation of ExpCoT on tasks requiring 3, 4, or more sequential skills.

## Limitations

- Dataset availability and reproducibility: Primary dataset from external paper with unclear current availability and incomplete specification of task generation
- Generalizability beyond synthetic tasks: All evaluation tasks use synthetic operator-based transformations, limiting real-world applicability
- Model capacity assumptions: Scaling behavior and minimum model size requirements for ExpCoT effectiveness aren't fully characterized

## Confidence

- **High confidence** in: The existence of a composition problem in ICL - multiple models consistently fail to combine simple task examples into composite task solutions
- **Medium confidence** in: The mechanism explanation - operator-attention hypothesis is plausible but may involve additional factors
- **Medium confidence** in: ExpCoT effectiveness - shows consistent improvements but magnitude varies significantly
- **Low confidence** in: Generalization to real-world tasks - synthetic operator-based tasks may not capture natural language complexity

## Next Checks

1. **Replicate the core composition failure**: Using any available dataset with synthetic operator tasks, construct prompts with varying numbers of simple task examples (k=2 to 30) and fixed composite examples (kc=5). Verify the negative correlation between simple task examples and composite task accuracy on at least one model.

2. **Validate the operator-attention mechanism**: Implement the operator ablation experiment by replacing operators with random tokens or natural language equivalents. Confirm that removing operators eliminates the negative effect of simple task examples while preserving content.

3. **Test ExpCoT on natural language tasks**: Adapt the ExpCoT approach to a non-synthetic compositional task (e.g., combining summarization with sentiment analysis). Measure whether explicit step markers improve composition performance compared to vanilla CoT or no-CoT baselines.