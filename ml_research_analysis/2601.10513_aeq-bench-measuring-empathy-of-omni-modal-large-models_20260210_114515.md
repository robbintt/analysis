---
ver: rpa2
title: 'AEQ-Bench: Measuring Empathy of Omni-Modal Large Models'
arxiv_id: '2601.10513'
source_url: https://arxiv.org/abs/2601.10513
tags:
- audio
- human
- empathy
- response
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AEQ-Bench, a benchmark for evaluating empathy
  in omni-modal large models (OLMs). The benchmark addresses the challenge of assessing
  empathy in OLMs by jointly examining linguistic and paralinguistic features through
  two novel settings: context variation and tone variation.'
---

# AEQ-Bench: Measuring Empathy of Omni-Modal Large Models

## Quick Facts
- **arXiv ID**: 2601.10513
- **Source URL**: https://arxiv.org/abs/2601.10513
- **Reference count**: 40
- **Primary result**: OLMs with audio output capabilities generally outperform text-only models, but remain unreliable for fine-grained paralinguistic expressiveness evaluation.

## Executive Summary
AEQ-Bench introduces a novel benchmark for evaluating empathy in omni-modal large models (OLMs) by jointly examining linguistic and paralinguistic features. The benchmark addresses the challenge of assessing empathy through two novel settings: context variation and tone variation. OLMs with audio output capabilities generally outperform text-only models, and while they align with human judgments for coarse-grained tasks, they remain unreliable for fine-grained paralinguistic expressiveness. The best-performing models include GPT and Qwen's Omni series.

## Method Summary
AEQ-Bench evaluates empathy in OLMs using 1,885 English instances from MELD, GIGASPEECH, and EMOV-DB datasets, each pairing human audio with textual context. The benchmark employs two settings: Context Variation (same utterance, different contexts) and Tone Variation (same context, different emotional tones). Evaluation metrics include linguistic measures (Modality Reliance, Naturalness 1-4, Coherence/Supportiveness/Delivery Good/Fair/Poor, Discrimination 1-6) and paralinguistic measures (Delivery). Models are evaluated using both text-based judges (GPT-5-mini) and audio-capable OLMs, with human annotators providing ground truth.

## Key Results
- OLMs with audio output capabilities generally outperform text-only models on empathy tasks
- Models align with human judgments for coarse-grained quality assessment but not fine-grained paralinguistic expressiveness
- GPT and Qwen's Omni series achieve the highest multimodal integration scores (0.91, 0.87)
- Context variation successfully tests cognitive empathy by requiring perspective-taking from background information

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Integration for Empathy Learning
- Claim: OLMs with audio output capabilities outperform text-only models because joint audio-text training captures paralinguistic cues essential for empathy.
- Mechanism: End-to-end audio-text training creates shared representations linking linguistic content with prosodic features (pitch, pacing, pauses), enabling models to both comprehend and generate empathetic delivery—not just empathetic text.
- Core assumption: Audio-native representations encode emotional information that text transcriptions discard.
- Evidence anchors: [abstract] "OLMs trained with audio output capabilities generally outperformed models with text-only outputs"; [section 5] Table 4 shows GPT and Qwen-Omni achieve highest multimodal integration scores (0.91, 0.87) vs. text-only models.

### Mechanism 2: Context-Dependent Perspective Taking
- Claim: Empathic responses require inferring unstated user concerns from the combination of audio utterance and background context, not explicit instructions.
- Mechanism: The A–C (Audio-to-Context) variation tests whether models can shift interpretation when identical utterances are paired with different contexts (e.g., "give him some money" implies family support in one context, political donation in another).
- Core assumption: Cognitive empathy involves reading between the lines—detecting latent goals the user never explicitly states.
- Evidence anchors: [section 3.1] "Same utterance, different contexts... where background framing shifts its interpretation"; [appendix D] explicitly contrasts how models must infer wheelchair accessibility concerns vs. investment scale from the same utterance.

### Mechanism 3: Paralinguistic Judgment Gap
- Claim: Text-based audio captions cannot substitute for direct audio perception in fine-grained empathy evaluation.
- Mechanism: When OLMs evaluate audio via textual captions (e.g., "calm, gentle, empathetic voice"), captions introduce bias and miss nuanced emotional prosody, causing divergence from human judgments at fine granularity.
- Core assumption: Paralinguistic cues are implicit and easily excluded from captions, making text intermediaries unreliable proxies.
- Evidence anchors: [abstract] "OLMs align with human judgments for coarse-grained quality assessment, they remain unreliable for evaluating fine-grained paralinguistic expressiveness"; [section 7] Direct audio judging achieves only 43% consistency with humans; captions introduce positive or negative bias depending on prompting.

### Mechanism 4: Discrimination–Naturalness Tradeoff
- Claim: Sensitivity to emotional variations (Discrimination) and conversational naturalness (Naturalness) may be decoupled abilities requiring different training signals.
- Mechanism: Models optimized for audio analysis (e.g., Qwen-Audio) produce varied responses across tones but default to analytical descriptions rather than conversational replies, while models optimized for dialogue produce natural but less differentiated responses.
- Core assumption: High discrimination requires attending to acoustic features; high naturalness requires conversational-style training.
- Evidence anchors: [section 5] "Models' performance varies on Discrimination and Naturalness, indicating they may be decoupled abilities"; [section 5] Qwen-Audio ranks #1 in Discrimination (0.77) but #8 in Naturalness (0.46); it "often providing analytical or descriptive content rather than natural, conversational replies."

## Foundational Learning

- **Cognitive vs. Affective Empathy (Davis, 1983)**
  - Why needed here: AEQ-Bench evaluates both—cognitive empathy requires perspective-taking from context; affective empathy requires emotional resonance with speaker tone. Confusing these leads to misinterpreting results.
  - Quick check question: If a user says "I'm fine" with an angry tone, should the model address the literal statement (cognitive) or the emotional state (affective)?

- **Paralinguistic Features (Prosody, Tone, Pacing)**
  - Why needed here: The benchmark's key contribution is evaluating *how* responses sound, not just what they say. Without understanding MFCCs, pitch variation, and speech rate, you cannot interpret Delivery scores.
  - Quick check question: What paralinguistic cue distinguishes a supportive "I understand" from a dismissive one?

- **Empathic Accuracy (Ickes et al., 2000)**
  - Why needed here: The Coherence and Discrimination metrics are grounded in empathic accuracy research—the ability to infer another's thoughts and feelings. This frames why context-variation is a valid empathy test.
  - Quick check question: Why would two people hearing the same utterance in different contexts require different empathic responses?

## Architecture Onboarding

- **Component map:** Audio encoder (processes raw speech for prosodic features) + Text encoder (processes context summary) -> Fusion Layer (combines audio and text representations) -> Generation Layer (produces synchronized text + audio) -> Evaluation Layer (OLM-as-judge assessing Coherence, Supportiveness, Delivery on generated audio)

- **Critical path:**
  1. Audio encoding must preserve paralinguistic features (not just ASR transcription)
  2. Context + audio fusion must enable perspective inference (A–C tasks)
  3. Audio output must support expressive prosody (currently a gap—models produce "flat speech")
  4. Judge model must process audio directly, not via captions (43% consistency vs. human)

- **Design tradeoffs:**
  - Analysis-optimized vs. Dialogue-optimized: Qwen-Audio excels at discrimination but fails naturalness; GPT-4o excels at naturalness but shows moderate discrimination
  - Coarse vs. Fine-grained evaluation: 3-point Likert aligns with human judgment; 5-point scales expose OLM-human divergence
  - Caption-based vs. Audio-native judging: Captions introduce bias; direct audio is underdeveloped

- **Failure signatures:**
  - Polarized scoring: Qwen2-Audio assigns extreme scores (1 or 3) with bias toward Qwen-family models
  - Template responses: Qwen-Omni appends formulaic sentences ("but you know"), reducing discrimination
  - Flat delivery: All tested OLMs lack expressive prosody; synthesized speech remains mostly neutral
  - Modality conflict: Earlier models (Flamingo, SALMONN) show 7–13% failure rates when audio and text cues conflict

- **First 3 experiments:**
  1. **Modality Ablation:** Run AEQ-Bench with audio-only input (no context text) vs. text-only input (ASR transcription, no audio) to quantify each modality's contribution to empathy scores.
  2. **Judge Consistency Baseline:** Compare GPT-4o-audio, Qwen2.5-Omni, and Qwen2-Audio as judges on the same 50 response pairs; measure inter-judge agreement and alignment with human annotations (target: >80% consistency for coarse-grained).
  3. **Tone-Conditioned Generation:** Prompt audio-capable OLMs with explicit tone instructions ("respond with warm, supportive tone" vs. "respond neutrally") on EMOV-DB subset; evaluate whether Delivery scores improve or remain flat (tests if models can modulate prosody on demand).

## Open Questions the Paper Calls Out
None

## Limitations

- The evaluation of fine-grained paralinguistic expressiveness remains unreliable due to the fundamental challenge of using text-based captions as proxies for audio perception
- The benchmark's reliance on proprietary models (GPT-5-mini, GPT-5) for evaluation creates reproducibility concerns, as these models are not publicly accessible
- The audio generation quality across all tested OLMs remains underdeveloped, with synthesized speech consistently described as "flat" and lacking expressive prosody

## Confidence

**High Confidence**: The finding that OLMs with audio output capabilities generally outperform text-only models is well-supported by systematic evaluation across 1,885 instances.

**Medium Confidence**: The claim that OLMs align with human judgments for coarse-grained quality assessment but not fine-grained paralinguistic expressiveness is supported by quantitative metrics (72% vs 43% alignment), but the exact causes of this gap require further investigation.

**Low Confidence**: The mechanism explaining why context-dependent perspective taking requires cognitive empathy inference is theoretically sound but lacks direct empirical validation within the benchmark itself.

## Next Checks

1. **Modality Ablation Study**: Run AEQ-Bench with audio-only input (no context text) vs. text-only input (ASR transcription, no audio) to quantify each modality's contribution to empathy scores and isolate the specific value of audio information beyond transcription.

2. **Judge Consistency Baseline**: Compare GPT-4o-audio, Qwen2.5-Omni, and Qwen2-Audio as judges on the same 50 response pairs; measure inter-judge agreement and alignment with human annotations (target: >80% consistency for coarse-grained tasks) to establish whether current judge models are sufficiently reliable.

3. **Tone-Conditioned Generation**: Prompt audio-capable OLMs with explicit tone instructions ("respond with warm, supportive tone" vs. "respond neutrally") on EMOV-DB subset; evaluate whether Delivery scores improve or remain flat to test if models can modulate prosody on demand rather than defaulting to flat speech.