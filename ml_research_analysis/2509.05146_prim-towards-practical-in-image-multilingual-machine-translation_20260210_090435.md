---
ver: rpa2
title: 'PRIM: Towards Practical In-Image Multilingual Machine Translation'
arxiv_id: '2509.05146'
source_url: https://arxiv.org/abs/2509.05146
tags:
- text
- translation
- images
- image
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of In-Image Machine Translation
  (IIMT) under practical conditions, where current research relies on synthetic data
  with simple backgrounds, single fonts, fixed text positions, and bilingual translation,
  creating a significant gap with real-world scenarios. To tackle this, the authors
  introduce the PRIM dataset, a new benchmark containing real-world captured one-line
  text images with complex backgrounds, various fonts, diverse text positions, and
  support for five multilingual translation directions.
---

# PRIM: Towards Practical In-Image Multilingual Machine Translation

## Quick Facts
- arXiv ID: 2509.05146
- Source URL: https://arxiv.org/abs/2509.05146
- Reference count: 28
- Key outcome: VisTrans achieves 12.6 BLEU for En-De vs 10.4 for PEIT-Render and 28.8 FID vs 69.1 for TranslatotronV on PRIM dataset

## Executive Summary
This paper addresses the significant gap between synthetic and real-world in-image machine translation by introducing the PRIM dataset and VisTrans model. Current IIMT research relies heavily on synthetic data with simplified conditions, creating a disconnect from practical applications. The authors propose a new benchmark containing real-world captured one-line text images with complex backgrounds, various fonts, and diverse text positions supporting five multilingual translation directions. Their VisTrans model processes visual text and background information separately using a two-stage training and multi-task learning strategy, demonstrating superior performance in both translation quality and visual preservation compared to existing approaches.

## Method Summary
The authors introduce PRIM, a new benchmark dataset containing 20,000 real-world captured one-line text images with complex backgrounds, various fonts, and diverse text positions supporting five multilingual translation directions (En→De, En→Fr, En→Es, En→Zh, and De→En). They propose VisTrans, an end-to-end model that processes visual text and background information separately through a two-stage training approach with multi-task learning. The model first learns to translate text content while preserving background information, then refines the visual rendering to maintain image integrity. This approach addresses the limitations of previous synthetic datasets and models that struggle with real-world complexity, achieving higher BLEU scores and lower FID scores while maintaining multilingual capability.

## Key Results
- VisTrans achieves 12.6 BLEU for En→De translation compared to 10.4 for PEIT-Render
- VisTrans achieves 28.8 FID score compared to 69.1 for TranslatotronV in visual preservation
- Outperforms existing models in both translation quality (BLEU, COMET) and visual effect while supporting five multilingual translation directions

## Why This Works (Mechanism)
The VisTrans model works by decoupling the translation and visual rendering tasks through a two-stage training process. In the first stage, the model learns to accurately translate text content while preserving background context. In the second stage, it refines the visual rendering to ensure the translated text integrates naturally with the original image background. This separation allows the model to focus on translation accuracy without being overwhelmed by visual complexity, then separately optimize for visual quality. The multi-task learning strategy enables the model to leverage shared representations across different translation directions while maintaining language-specific nuances.

## Foundational Learning

**Image-Text Processing**: Required to understand how to extract and manipulate text within images while preserving visual context. Quick check: Verify the model can accurately detect and segment text regions in complex backgrounds.

**Multilingual Machine Translation**: Essential for handling the five translation directions supported by PRIM. Quick check: Test translation quality across all language pairs, not just high-resource ones.

**Visual Quality Metrics (FID)**: Needed to quantify how well the translated image preserves visual fidelity. Quick check: Compare FID scores with other image quality metrics like SSIM or PSNR.

**Multi-task Learning**: Required to train the model on both translation and visual rendering tasks simultaneously. Quick check: Verify that performance on one task doesn't degrade when training on multiple tasks.

## Architecture Onboarding

**Component Map**: Text Extractor -> Translator -> Background Processor -> Visual Renderer -> Output Image

**Critical Path**: The translation path (Text Extractor → Translator) is most critical for maintaining translation quality, while the visual path (Background Processor → Visual Renderer) is critical for image integrity.

**Design Tradeoffs**: The two-stage training approach sacrifices end-to-end optimization for better separation of concerns, potentially limiting the model's ability to learn complex interdependencies between translation and visual rendering.

**Failure Signatures**: Poor translation quality when text regions are too small or complex, visual artifacts when background complexity exceeds training distribution, degraded performance on languages with limited training data.

**First 3 Experiments**: 1) Test translation quality on held-out PRIM test set, 2) Evaluate visual preservation using FID metric on same test set, 3) Cross-validate on external real-world images not in PRIM dataset.

## Open Questions the Paper Calls Out
None

## Limitations
- PRIM dataset contains only 20,000 training samples, potentially insufficient for full real-world diversity
- Limited to one-line text images, excluding common multi-line text scenarios like street signs and posters
- Two-stage training approach may introduce optimization challenges and lacks comprehensive ablation studies

## Confidence

**High Confidence Claims:**
- The gap between synthetic and real-world IIMT data is well-established
- Performance improvements over existing models are methodologically sound
- The necessity of multilingual support in IIMT is valid

**Medium Confidence Claims:**
- PRIM being the first real-world captured dataset requires verification
- The "end-to-end" characterization given the two-stage approach
- Relative importance of visual vs background processing components

**Low Confidence Claims:**
- Generalizability to truly diverse real-world scenarios beyond PRIM
- Scalability to complex text layouts and longer sequences
- Practical deployment feasibility given computational constraints

## Next Checks
1. Conduct comprehensive statistical analysis comparing PRIM's background complexity, font diversity, and text positioning against other image datasets and real-world samples.

2. Test VisTrans on external IIMT datasets or real-world collected images not seen during training to assess true generalization capability.

3. Perform detailed ablation experiments to isolate the contribution of two-stage training, multi-task learning, and visual processing components.