---
ver: rpa2
title: 'PentestJudge: Judging Agent Behavior Against Operational Requirements'
arxiv_id: '2508.02921'
source_url: https://arxiv.org/abs/2508.02921
tags:
- agent
- arxiv
- penetration
- requirements
- pentestjudge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PentestJudge introduces an LLM-as-judge framework for evaluating
  the process quality of AI penetration testing agents beyond simple success/failure
  metrics. It uses hierarchical rubrics to decompose complex security tasks into gradable
  criteria covering operational objectives, security, and tradecraft, with judges
  scoring agent trajectories via tool access.
---

# PentestJudge: Judging Agent Behavior Against Operational Requirements

## Quick Facts
- arXiv ID: 2508.02921
- Source URL: https://arxiv.org/abs/2508.02921
- Authors: Shane Caldwell; Max Harley; Michael Kouremetis; Vincent Abruzzo; Will Pearce
- Reference count: 40
- Primary result: LLM-as-judge framework achieves F1 up to 0.83 vs human experts for evaluating AI penetration testing agent trajectories

## Executive Summary
PentestJudge introduces an LLM-as-judge framework for evaluating the process quality of AI penetration testing agents beyond simple success/failure metrics. It uses hierarchical rubrics to decompose complex security tasks into gradable criteria covering operational objectives, security, and tradecraft, with judges scoring agent trajectories via tool access. When tested against human expert judgments, top models achieved F1 scores up to 0.83, with specialized performance across task categories. Budget models provided cost-effective verification, suggesting that evaluation can be easier than generation for security tasks. The approach enables scalable, holistic assessment of agent behavior, advancing production-ready deployment of AI security tools.

## Method Summary
PentestJudge evaluates penetration testing agent trajectories using hierarchical rubrics that break down complex tasks into binary yes/no criteria. A pentest agent (GPT-4.1) with Kali Linux tool access generates trajectories in a GOAD AD environment, while LLM judges use specialized tools to navigate trajectories, search for relevant evidence, and store findings in memory before making binary judgments. Each trajectory is judged multiple times by different models against human expert ground truth, measuring F1 scores per rubric leaf node and aggregating by task category (Operational Objectives, Operational Security, Tradecraft).

## Key Results
- Top models achieved F1 scores up to 0.83 when judging GPT-4.1-generated trajectories against human experts
- Budget models (Claude 3.5 Haiku, $0.82/trajectory) achieved F1 0.72-0.74, demonstrating cost-effective verification
- Open models struggled with tool navigation, often checking fewer trajectory steps before concluding
- Different model families excelled at different categories: Anthropic models at Operational Objectives, OpenAI models at Operational Security

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical rubric decomposition enables evaluation of complex, non-programmatically-verifiable security criteria
- Mechanism: Tree-structured rubrics break penetration testing tasks into progressively smaller sub-tasks until each leaf node represents a binary yes/no criterion
- Core assumption: Security domain experts can reliably encode tacit operational knowledge into explicit, granular rubric criteria
- Evidence anchors: [abstract] "We develop rubrics that use a tree structure to hierarchically collapse the penetration testing task" [Section 3.4] "Rubrics dissect a complicated evaluation task... into subtrees of specific requirements"
- Break condition: If rubric criteria require contextual interpretation beyond explicit specification, leaf node ambiguity may cause inconsistent grading

### Mechanism 2
- Claim: Agentic tool access enables judgment over trajectories exceeding model context windows
- Mechanism: PentestJudge receives tools to search trajectory tool calls, inspect tool definitions, and store intermediate findings in memory
- Core assumption: Models with stronger tool-calling capabilities will more effectively navigate and extract relevant evidence
- Evidence anchors: [abstract] "LLM-as-judge with access to tools that allow it to consume arbitrary trajectories" [Section 4.4] "PentestJudge is able to look at the tool definitions... search through those tool calls for specific inputs or outputs"
- Break condition: If models lack sufficient tool-calling training or default to shallow exploration, they will miss critical evidence

### Mechanism 3
- Claim: Verification is computationally easier than generation for penetration testing tasks
- Mechanism: Budget-tier models achieved F1=0.72-0.74 when judging GPT-4.1-generated trajectories
- Core assumption: The cognitive demands of evaluating evidence exceed by less than those of generating successful attack chains
- Evidence anchors: [abstract] "Weaker and cheaper models can judge the trajectories of pentests performed by stronger and more expensive models" [Section 5.1] Kimi-k2-instruct "placing 3rd... with open weights and a fraction of the cost"
- Break condition: If task complexity scales beyond evaluated scope, verification difficulty may converge with generation

## Foundational Learning

- Concept: **LLM-as-Judge evaluation paradigm**
  - Why needed here: PentestJudge relies on LLMs making binary classification judgments against rubric criteria
  - Quick check question: Can you explain why LLM-as-judge is preferred over programmatic metrics for "operational security" criteria like stealth or scope compliance?

- Concept: **Process-supervised evaluation vs. outcome-only metrics**
  - Why needed here: The paper explicitly positions itself against outcome-focused benchmarks by evaluating intermediate trajectories
  - Quick check question: What information is lost when evaluating a penetration test solely by whether domain admin was achieved?

- Concept: **Tool-calling agent architectures**
  - Why needed here: Both the penetration testing agent and PentestJudge are tool-using agents
  - Quick check question: Why would a model's ability to call tools affect its performance as a judge, given it's not executing attacks?

## Architecture Onboarding

- Component map: Penetration Testing Agent -> GOAD Environment -> Trajectory Capture -> Rubric Tree -> PentestJudge Harness -> Judge Model -> Binary Judgment -> Human Ground Truth Comparison
- Critical path: 1. Define environment-specific rubric 2. Run pentest agent; capture trajectory 3. For each leaf node: instantiate PentestJudge with criterion + trajectory tools 4. Judge searches trajectory, stores findings, outputs judgment 5. Compare judge outputs to human ground truth; compute F1
- Design tradeoffs: Specificity vs. coverage (vague criteria cause inconsistency), Model cost vs. accuracy (frontier models achieve highest F1), Portfolio judging (different models excel at different categories)
- Failure signatures: Shallow tool exploration (open models check few tool calls), Hallucinated requirements (judges add unstated constraints), Terminology mismatch (judges misinterpret security jargon), OpSec blind spots (models score 0.07-0.50 on Operational Security)
- First 3 experiments: 1. Baseline calibration: Run random judge baseline against your rubric; confirm test models exceed F1â‰ˆ0.49 random baseline 2. Category stratification: Evaluate same trajectories across all three task categories; identify which category your chosen model struggles with most 3. Requirement specificity ablation: Take 5 leaf nodes where judge disagrees with human; rephrase with additional context; measure F1 delta

## Open Questions the Paper Calls Out

- Can the binary outputs of PentestJudge function as effective reward signals for reinforcement learning algorithms (e.g., GRPO) to align security agents with non-programmable operational requirements?
- Is it feasible to generate accurate evaluation rubrics dynamically at test-time based on broad human specifications, rather than relying on manually constructed, static rubrics?
- Can the "shallow tool calling" deficiency observed in open-source models be overcome through specific fine-tuning?
- What specific mechanisms (knowledge injection vs. tool augmentation) most effectively improve model judgment accuracy for "Operational Security" categories?

## Limitations

- Rubric construction remains artisanal and may not generalize to all security contexts
- Performance gap between frontier and open models suggests current judging capabilities may be bottlenecked by model architecture
- 0.83 F1 score represents performance on specific AD environment; scaling to more diverse contexts could impact accuracy
- Observed performance patterns may reflect task distribution rather than fundamental capabilities

## Confidence

- **High Confidence**: The core mechanism of hierarchical rubric decomposition enabling LLM judging works as described
- **Medium Confidence**: Cost-effectiveness findings hold within tested scope but may not generalize to all security domains
- **Medium Confidence**: Model-specific performance patterns are well-documented but may reflect task distribution

## Next Checks

1. Apply identical judging methodology to a non-AD environment (e.g., web application security) and measure rubric construction time vs accuracy retention
2. Train a fine-tuned open model specifically for trajectory navigation tools and compare F1 improvement against raw models
3. Create a judge ensemble combining top-performing models per category and measure whether aggregation improves overall F1 beyond individual model peaks