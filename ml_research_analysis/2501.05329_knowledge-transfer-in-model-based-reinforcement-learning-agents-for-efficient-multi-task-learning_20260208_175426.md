---
ver: rpa2
title: Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient
  Multi-Task Learning
arxiv_id: '2501.05329'
source_url: https://arxiv.org/abs/2501.05329
tags:
- distillation
- learning
- teacher
- loss
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of deploying large model-based
  reinforcement learning agents in resource-constrained environments by developing
  an efficient knowledge transfer approach. The authors distill a 317M parameter TD-MPC2
  model into a compact 1M parameter model using reward-based knowledge distillation,
  achieving a state-of-the-art normalized score of 28.45 on the MT30 benchmark, representing
  a 48.5% improvement over the original 1M parameter model and a 2.77% improvement
  over models trained from scratch.
---

# Knowledge Transfer in Model-Based Reinforcement Learning Agents for Efficient Multi-Task Learning

## Quick Facts
- **arXiv ID**: 2501.05329
- **Source URL**: https://arxiv.org/abs/2501.05329
- **Reference count**: 9
- **Primary result**: Achieves 28.45 normalized score on MT30 benchmark, 48.5% improvement over 1M parameter baseline

## Executive Summary
This paper addresses the challenge of deploying large model-based reinforcement learning agents in resource-constrained environments by developing an efficient knowledge transfer approach. The authors distill a 317M parameter TD-MPC2 model into a compact 1M parameter model using reward-based knowledge distillation, achieving state-of-the-art performance on the MT30 benchmark. The approach also incorporates FP16 post-training quantization, reducing model size by 50% while maintaining performance. Key findings include the effectiveness of extended distillation periods (up to 1M steps), optimal batch sizes (256), and the importance of using high-capacity teacher models for knowledge transfer.

## Method Summary
The authors develop a knowledge distillation pipeline for TD-MPC2 model-based RL agents. They freeze a pretrained 317M parameter teacher model and train a 1M parameter student model using a combined loss function that includes the original TD-MPC2 losses (consistency, reward, and value losses) plus a distillation loss based on MSE between teacher and student reward predictions. The distillation coefficient is set to 0.4 for optimal results. The approach also includes FP16 post-training quantization for deployment efficiency. The student model is trained for 200K-1M steps with batch size 256 to achieve optimal knowledge transfer.

## Key Results
- Achieves state-of-the-art 28.45 normalized score on MT30 benchmark
- 48.5% improvement over original 1M parameter model (18.93 score)
- 2.77% improvement over models trained from scratch (27.36 score)
- FP16 quantization reduces model size by 50% with minimal performance impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward-based distillation loss enables compact student models to capture task-relevant knowledge from high-capacity teachers more effectively than training from scratch.
- Mechanism: The student model minimizes MSE between its reward predictions and the teacher's reward predictions for the same state-action pairs. This loss is combined with the original TD-MPC2 loss (consistency + reward + value losses) via a distillation coefficient (d_coef â‰ˆ 0.4). The teacher's frozen weights provide stable supervision signals that guide the student toward better policy representations without requiring environmental interaction.
- Core assumption: The teacher's reward predictions encode compressed, transferable task knowledge that is more informative for learning than environmental reward signals alone.
- Evidence anchors:
  - [abstract] "distills a high-capacity multi-task agent (317M parameters) into a compact 1M parameter model, achieving state-of-the-art performance...28.45 normalized score, a substantial improvement over the original 1M parameter model's score of 18.93"
  - [section 2] "ð¿distill = MSE(ð‘…teacher (ð‘ , ð‘Ž), ð‘…student (ð‘ , ð‘Ž))" and "values close to 0.5 yield the best results, with 0.4 being optimal"
  - [corpus] Related work "TD-MPC-Opt" (FMR=0.637) confirms similar distillation approaches for model-based RL, suggesting reproducibility across implementations.

### Mechanism 2
- Claim: Extended distillation periods with smaller batch sizes yield better knowledge consolidation than shorter training with larger batches.
- Mechanism: Longer exposure (up to 1M steps) allows the student to gradually align its reward predictions with the teacher's across diverse state-action distributions. Batch size 256 provides optimal gradient noise for generalization while fitting within 12GB VRAM constraints.
- Core assumption: Knowledge transfer requires sufficient iterations to cover the multi-task state space; smaller batches may provide regularization benefits.
- Evidence anchors:
  - [section 3, Table 2] "distill 256 1M 28.12" vs "from scratch 256 1M 27.36" shows 2.77% improvement; "distill 256 200K 17.85" vs "distill 128 200K 17.37" shows batch size impact.
  - [corpus] Weak direct evidenceâ€”corpus papers focus on LLM distillation rather than batch size effects in RL distillation.

### Mechanism 3
- Claim: High-capacity teacher models (317M) provide substantially more transferable knowledge than medium-capacity teachers (48M).
- Mechanism: Larger models encode richer representations of multi-task dynamics and reward structures. When distilled, this compressed knowledge exceeds what students can learn from smaller teachers or from scratch.
- Core assumption: Model capacity correlates with knowledge quality that survives distillation compression.
- Evidence anchors:
  - [section 3] "Distillation from the 48M parameter teacher resulted in a score of 13.61, while distillation from the 317M parameter teacher achieved 17.85, representing a 31.2% relative improvement"
  - [corpus] "Mixture-of-World Models" paper suggests modular scaling helps multi-task RL, indirectly supporting capacity-value relationships.

## Foundational Learning

- Concept: **Model-Based Reinforcement Learning (TD-MPC2 specifically)**
  - Why needed here: The distillation pipeline modifies TD-MPC2's loss functions; understanding world models, latent planning, and the consistency/reward/value losses is prerequisite to implementing the distillation modifications.
  - Quick check question: Can you explain why TD-MPC2 uses a latent dynamics model rather than predicting raw observations?

- Concept: **Knowledge Distillation (Teacher-Student Framework)**
  - Why needed here: The core contribution adapts distillation to model-based RL. You need to understand how frozen teacher weights provide supervision, how distillation coefficients balance losses, and why MSE is appropriate for regression targets.
  - Quick check question: Why might distillation work better than direct training even when both use the same dataset?

- Concept: **Post-Training Quantization (FP16)**
  - Why needed here: The deployment pipeline applies FP16 quantization for 50% size reduction. Understanding precision tradeoffs is necessary for real-world deployment decisions.
  - Quick check question: What types of operations are most sensitive to FP16 precision loss, and how might you detect degradation?

## Architecture Onboarding

- Component map:
  - **Teacher Model**: Pretrained 317M TD-MPC2 checkpoint (frozen weights)
  - **Student Model**: 1M TD-MPC2 backbone (trainable)
  - **Original Loss**: L_orig = Î±_cÂ·L_consistency + Î±_rÂ·L_reward + Î±_vÂ·L_value
  - **Distillation Loss**: L_distill = MSE(R_teacher(s,a), R_student(s,a))
  - **Total Loss**: L_total = L_orig + d_coef Ã— L_distill
  - **Quantization**: Post-training FP16 conversion

- Critical path:
  1. Load pretrained 317M teacher checkpoint (frozen)
  2. Initialize 1M student from existing checkpoint
  3. For each batch: compute original TD-MPC2 losses + distillation loss
  4. Backprop through student only (teacher weights frozen)
  5. Train for 200K-1M steps with batch size 256
  6. Apply FP16 quantization for deployment

- Design tradeoffs:
  - **d_coef selection**: 0.4 optimal in experiments, but 0.45 better for extended training. Higher values over-regularize to teacher; lower values underutilize distillation.
  - **Batch size**: 256 optimal for distillation, but 1024 better for training from scratch. Assumption: smaller batches provide beneficial noise during distillation.
  - **Latent vs reward distillation**: Paper attempted next-state latent distillation but found dimensional mismatch (1376-dim teacher vs 128-dim student) caused information loss. Reward-only distillation is recommended.

- Failure signatures:
  - **Score ~7-9 with latent distillation**: Dimensional mismatch between teacher/student latent spaces; use reward-only distillation instead.
  - **Score ~13-14 with d_coef > 0.6**: Over-regularization to teacher; reduce d_coef to 0.4-0.5.
  - **Score ~13-14 with 48M teacher**: Teacher capacity insufficient; use 317M teacher.
  - **OOM on 12GB GPU**: Batch size too large; reduce to 256 or lower.

- First 3 experiments:
  1. **Baseline validation**: Train 1M student from scratch on MT30 (batch 256, 200K steps) to verify baseline score â‰ˆ14.04. This confirms your TD-MPC2 implementation is correct.
  2. **Distillation coefficient sweep**: Using 317M teacher, batch 256, 200K steps, sweep d_coef âˆˆ {0.25, 0.4, 0.55}. Target: 0.4 should yield â‰ˆ17.85.
  3. **Extended training comparison**: Run both (a) distillation and (b) from-scratch training for 1M steps, batch 256. Target: distillation â‰ˆ28.12, from-scratch â‰ˆ27.36, confirming 2.77% improvement.

## Open Questions the Paper Calls Out
None

## Limitations
- **Computational efficiency**: Extended training (1M steps) required for optimal performance, with no wall-clock time or efficiency metrics reported.
- **Architecture constraints**: Approach is specific to TD-MPC2 model-based RL agents; generalization to other RL architectures untested.
- **Teacher quality assumptions**: No validation of teacher model calibration or generalization, which could limit distillation effectiveness.

## Confidence

**High confidence**: The reward-based distillation mechanism and FP16 quantization effects are well-supported by experimental evidence and align with established ML principles.

**Medium confidence**: The batch size and distillation duration recommendations are supported by ablation studies but lack theoretical justification. The 2.77% improvement over from-scratch training is statistically significant but may not generalize across all benchmarks.

**Low confidence**: The assumption that larger teacher capacity always yields better distillation results is supported by one comparison (48M vs 317M) but requires more systematic capacity scaling studies.

## Next Checks

1. **Teacher quality assessment**: Evaluate the 317M teacher model's performance and reward prediction accuracy on held-out tasks to verify it provides reliable supervision signals.

2. **Cross-benchmark generalization**: Test the distillation approach on additional multi-task RL benchmarks (e.g., Meta-World, RoboNet) to assess robustness beyond MT30.

3. **Efficiency benchmarking**: Measure training wall-clock time and GPU memory usage for both distillation and from-scratch approaches to quantify the computational cost of the 2.77% performance improvement.