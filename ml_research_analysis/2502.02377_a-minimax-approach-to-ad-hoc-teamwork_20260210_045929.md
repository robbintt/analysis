---
ver: rpa2
title: A Minimax Approach to Ad Hoc Teamwork
arxiv_id: '2502.02377'
source_url: https://arxiv.org/abs/2502.02377
tags:
- policy
- learning
- utility
- policies
- scenarios
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of robust ad hoc teamwork, where
  an agent must cooperate effectively with unknown teammates without prior coordination.
  The core contribution is adapting minimax-Bayes reinforcement learning to the multi-agent
  setting, where policies are trained against an adversarial prior over teammates
  rather than a fixed distribution.
---

# A Minimax Approach to Ad Hoc Teamwork

## Quick Facts
- arXiv ID: 2502.02377
- Source URL: https://arxiv.org/abs/2502.02377
- Authors: Victor Villin; Thomas Kleine Buening; Christos Dimitrakakis
- Reference count: 40
- Key outcome: Minimax-utility approach achieves 266.9±4.3 average utility and 225.3±11.5 worst-case utility on training scenarios, outperforming baselines including self-play, fictitious play, and population best response

## Executive Summary
This paper addresses the challenge of robust ad hoc teamwork (AHT), where an agent must cooperate effectively with unknown teammates without prior coordination. The core contribution is adapting minimax-Bayes reinforcement learning to the multi-agent setting, where policies are trained against an adversarial prior over teammates rather than a fixed distribution. This approach optimizes for worst-case performance guarantees rather than performance on a specific distribution. The method is evaluated on both a fully observable Iterated Prisoner's Dilemma and a partially observable collaborative cooking task (Overcooked) from the Melting Pot suite. Results show that policies trained using the minimax framework achieve superior worst-case utility and regret compared to baselines including self-play, fictitious play, and population best response.

## Method Summary
The method formulates AHT as a two-player zero-sum game between the focal agent and an adversarial "nature" that selects the worst-case distribution over training scenarios. Training proceeds via gradient descent-ascent (GDA) where the policy parameters are updated to maximize expected utility while the distribution parameters are updated to minimize it. For regret-based training, the adversary instead maximizes expected regret (gap to scenario-optimal performance). The framework uses delayed policy copies to approximate fixed background policies in multi-agent settings. Implementation uses PPO with CNN+LSTM policies, and training employs stochastic sampling of scenarios with minimum exploration probability to prevent distribution collapse.

## Key Results
- Minimax-utility (MU) policies achieve highest worst-case utility on training scenarios (U_min = 225.3±11.5 on Overcooked)
- MU outperforms baselines on average utility (266.9±4.3) and demonstrates faster learning curves
- MU achieves better worst-case regret than all baselines except MR, while MR provides no average utility improvement
- The method generalizes to held-out Melting Pot scenarios with moderate performance degradation

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Prior Optimization
Training against a dynamically-computed worst-case distribution over teammates yields policies with guaranteed worst-case performance bounds. By solving max_π min_β U(π, β), the resulting policy achieves bounded utility on any distribution over the training set. This works because the training scenarios form an ε-net of the true scenario space, providing distribution-free guarantees.

### Mechanism 2: Regret-Based Curriculum Effect
Minimax-regret training implicitly creates a curriculum that focuses learning on scenarios where improvement is most possible. A regret-maximizing adversary prioritizes scenarios where the current policy has highest regret—where learning is most informative. This avoids degenerate scenarios and creates curriculum-like learning dynamics that accelerate sample efficiency.

### Mechanism 3: Gradient Descent-Ascent Equilibrium Finding
Simultaneous gradient-based optimization of policy parameters (ascent) and scenario distribution (descent for utility, ascent for regret) converges to equilibrium solutions. The method alternates between computing utilities/regrets, updating the distribution via projected gradient steps, and updating policy parameters via policy gradient. Delayed policy copies smooth multi-agent POMGs to single-agent learning.

## Foundational Learning

- **Minimax/Bayesian Decision Theory**: The framework builds on minimax-Bayes RL theory. Understanding why worst-case priors provide distribution-free guarantees and how minimax theorem applies to bilinear utility functions is essential.
  - Quick check: Explain why max_π min_β U(π, β) = min_β max_π U(π, β) holds for this setting and what it implies about solution optimality.

- **Partially Observable Markov Games (POMGs)**: The formalism uses POMGs with c focal players and m-c background players. Understanding scenario marginalization—how fixing background policies induces a c-player POMG—is essential for implementation.
  - Quick check: Given a scenario σ = (c, π_b), derive the induced transition function P_σ(s'|s, a_f) when c < m.

- **Gradient Descent-Ascent for Minimax Optimization**: The core training loop is GDA. Understanding convergence conditions, simplex projection for distribution updates, and why alternating updates stabilize training is critical.
  - Quick check: In Algorithm 1, why does β update use descent for utility (line 11) but ascent for regret (line 13)?

## Architecture Onboarding

- **Component map**: Background Population Generator -> Scenario Generator -> Utility/Regret Estimator -> Distribution Learner -> Policy Learner -> Delayed Copy Buffer
- **Critical path**: 1) Generate diverse B_train via population play with heterogeneous preferences (one-time cost). 2) Pre-compute U*(σ) for all training scenarios via BR training (required for MR; skip for MU). 3) Initialize π_θ randomly, β_0 = Uniform(Σ(B)). 4) For each GDA iteration: sample scenarios, update β via gradient step, update θ via policy gradient. 5) Return uniform average of iterate parameters.
- **Design tradeoffs**: MU is computationally cheaper than MR (no BR needed) and achieved best worst-case utility in experiments. Exact GDA requires computing all utilities each iteration (prohibitive for large sets) while stochastic sampling introduces variance. Larger background populations improve coverage but increase computational cost.
- **Failure signatures**: Policy collapse to narrow scenarios (symptom: high U_min but poor U_avg; mitigation: enforce minimum sampling probability), regret approximation errors (symptom: MR underperforms MU; mitigation: accurate BR training), GDA non-convergence (symptom: oscillating β or diverging θ; mitigation: smaller learning rates).
- **First 3 experiments**: 1) Tabular validation on 3-round IPD with 9 background policies to verify MU achieves highest U_min and MR achieves lowest R_max. 2) Ablation on distribution learning rate η_β on simple Overcooked layout to find optimal convergence rate. 3) Comparison of MU vs. PBR on held-out partners to confirm MU's robustness advantage.

## Open Questions the Paper Calls Out

### Open Question 1
Can a curriculum learning framework over partner distributions improve sample efficiency and asymptotic performance while maintaining minimax robustness guarantees? The paper sees great potential in extending the approach to curriculum learning based on partner distributions to improve sample efficiency and AHT robustness.

### Open Question 2
Can minimax-utility policies be constrained or regularized to avoid the observed "narrow subset" problem while preserving worst-case guarantees? The paper notes that MU's policy may be forced to face pure defecting opponents entirely cutting exposure to cooperative strategies, suggesting it's possible to have equal worst-case utility with improved performance.

### Open Question 3
How can the computational burden of minimax-regret optimization be reduced when best-response utilities must be approximated? The paper states regret proved computationally expensive when best-response utilities are not readily available, demanding calculation of best responses for each scenario.

### Open Question 4
How does the choice of background population construction method affect the learned minimax distribution and resulting robustness? The paper states the chosen construction remains arbitrary and is not the main focus, yet diversity of training partners is critical for robustness guarantees.

## Limitations
- The method's robustness guarantees depend on training scenarios forming an ε-net over the true scenario space—poor representation degrades worst-case performance
- Computational burden of minimax-regret optimization requires expensive best-response utility estimation
- Theoretical convergence guarantees assume single-agent settings and may not hold for multi-agent scenarios without careful handling
- Experimental results limited to two cooperative tasks, leaving generalization to competitive scenarios unclear

## Confidence

- **High**: Worst-case utility maximization (MU) achieves superior worst-case performance on training scenarios
- **Medium**: MU outperforms baselines on average utility and sample efficiency due to curriculum-like dynamics  
- **Medium**: The method's robustness extends to held-out scenarios (Melting Pot) but with degradation from training-set performance
- **Low**: Regret-based training (MR) provides meaningful improvements over utility-based training given computational costs

## Next Checks

1. **ε-net coverage validation**: Systematically vary background population size and diversity to quantify the relationship between ε and worst-case utility degradation on held-out scenarios.
2. **Scalability stress test**: Implement exact Algorithm 1 on small finite scenario sets (e.g., IPD with 9 scenarios) to verify convergence behavior and compare against stochastic sampling approach.
3. **Adversarial scenario analysis**: For MU policies that achieve high U_min, identify specific training scenarios that dominate β_t and analyze whether these scenarios are representative of realistic teammates or pathological edge cases.