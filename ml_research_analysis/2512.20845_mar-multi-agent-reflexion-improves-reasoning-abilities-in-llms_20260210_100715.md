---
ver: rpa2
title: MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs
arxiv_id: '2512.20845'
source_url: https://arxiv.org/abs/2512.20845
tags:
- reflexion
- reasoning
- multi-agent
- hotpotqa
- debate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper identifies a key failure mode in single-agent Reflexion:\
  \ the model repeatedly reinforces its own flawed reasoning, leading to \u201Cdegeneration\
  \ of thought\u201D and mode collapse. To address this, the authors introduce Multi-Agent\
  \ Reflexion (MAR), which replaces single-agent self-reflection with a structured\
  \ debate among diverse persona-driven critics (e.g., Verifier, Skeptic, Logician,\
  \ Creative)."
---

# MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs
## Quick Facts
- arXiv ID: 2512.20845
- Source URL: https://arxiv.org/abs/2512.20845
- Reference count: 4
- Single-agent Reflexion leads to reasoning collapse; MAR introduces multi-agent debate to correct this, improving HotPotQA EM from 44% to 47% and HumanEval pass@1 from 76.4% to 82.6%.

## Executive Summary
Single-agent Reflexion often degenerates into reinforcing flawed reasoning, a problem the authors call "degeneration of thought." MAR addresses this by replacing self-reflection with a structured debate among diverse persona-driven critics. These critics analyze failed reasoning from different angles, and a judge synthesizes their critiques into a unified reflection. Experiments show MAR improves reasoning accuracy on HotPotQA and HumanEval, though at higher computational cost.

## Method Summary
MAR replaces single-agent self-reflection with a debate among multiple critics, each embodying a distinct persona (e.g., Verifier, Skeptic, Logician, Creative). Critics independently analyze the model's failed attempts, and a judge synthesizes their feedback into a corrected reflection. This multi-agent structure introduces diverse perspectives and mitigates the confirmation bias and mode collapse inherent in single-agent Reflexion.

## Key Results
- HotPotQA Exact Match accuracy improves from 44% to 47%.
- HumanEval pass@1 increases from 76.4% to 82.6%.
- Gains attributed to reduced confirmation bias and more diverse corrective reasoning.

## Why This Works (Mechanism)
Single-agent Reflexion tends to reinforce its own errors through repeated self-reflection, leading to reasoning collapse. MAR introduces multiple critics with distinct perspectives, forcing the model to confront diverse critiques rather than its own biases. This debate structure diversifies corrective feedback and reduces mode collapse, resulting in more robust reasoning.

## Foundational Learning
- **Mode collapse**: When repeated self-reflection narrows reasoning to a flawed pattern; check by monitoring diversity of reasoning steps over iterations.
- **Confirmation bias**: Tendency to favor information that confirms existing beliefs; check by measuring variance in critic feedback.
- **Persona-driven debate**: Assigning distinct roles to critics to diversify perspectives; check by evaluating performance under different persona sets.
- **Synthesis by judge**: Aggregating diverse critiques into a unified reflection; check by comparing judge-merged reflections to single-agent outputs.
- **Computational overhead**: Trade-off between improved accuracy and increased resource use; check by measuring wall-clock and token costs.
- **Ablation studies**: Isolating contributions of persona diversity versus debate structure; check by running variants with single critic or no judge.

## Architecture Onboarding
- **Component map**: Input -> (Multiple Critics) -> Judge -> Synthesized Reflection -> Output
- **Critical path**: Failed attempt → Critic analysis → Judge synthesis → Reflection update
- **Design tradeoffs**: Diversity vs. coherence; accuracy vs. computational cost; fixed personas vs. adaptive roles
- **Failure signatures**: Degeneracy in single critic, judge bias, or over-reliance on specific personas
- **First experiments**:
  1. Run MAR with randomized critic assignments to test robustness of gains.
  2. Measure wall-clock and token costs versus single-agent Reflexion.
  3. Test MAR on a multilingual benchmark to assess generalizability.

## Open Questions the Paper Calls Out
None

## Limitations
- Results rely on a single proprietary model (GPT-4) and fixed prompt templates.
- Computational overhead is not quantified.
- Ablation studies do not isolate individual contributions of persona diversity versus debate structure.
- No exploration of performance under alternative critic assignments or judge strategies.
- Lack of cross-linguistic or non-English dataset validation.

## Confidence
- **High** confidence in empirical observation that MAR outperforms single-agent Reflexion on tested benchmarks.
- **Medium** confidence in attribution of gains to reduced confirmation bias and mode collapse, as these are inferred rather than directly measured.
- **Low** confidence in generalizability of persona diversity benefits without cross-model or cross-language validation.

## Next Checks
1. Run MAR with randomized critic assignments and compare variance in performance to assess robustness of gains.
2. Measure wall-clock and token costs of MAR versus single-agent Reflexion to quantify computational overhead.
3. Test MAR on a multilingual reasoning benchmark to evaluate cross-linguistic generalizability.