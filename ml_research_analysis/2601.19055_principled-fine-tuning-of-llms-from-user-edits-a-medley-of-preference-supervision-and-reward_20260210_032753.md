---
ver: rpa2
title: 'Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision,
  and Reward'
arxiv_id: '2601.19055'
source_url: https://arxiv.org/abs/2601.19055
tags:
- user
- learning
- edits
- where
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a theoretical framework for fine-tuning\
  \ large language models (LLMs) using user-edit deployment data. The setup unifies\
  \ three feedback types\u2014supervised labels, preferences, and cost\u2014into a\
  \ single learning problem, reflecting how real users naturally provide feedback\
  \ by editing LLM responses."
---

# Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward

## Quick Facts
- arXiv ID: 2601.19055
- Source URL: https://arxiv.org/abs/2601.19055
- Reference count: 40
- Key outcome: Unifies supervised, preference, and reward feedback from user edits into a single theoretical framework, proving different sample complexity trade-offs for each method and proposing a late-ensembling bandit strategy that adaptively selects the best policy at test time

## Executive Summary
This paper introduces a theoretical framework for fine-tuning large language models (LLMs) using user-edit deployment data. The setup unifies three feedback types—supervised labels, preferences, and cost—into a single learning problem, reflecting how real users naturally provide feedback by editing LLM responses. The authors analyze three basic learning algorithms—supervised fine-tuning, direct preference optimization, and reinforcement learning from edit costs—proving each has different sample complexity trade-offs depending on user distribution and policy class. They propose a late-ensembling bandit strategy that combines these methods and adaptively selects the best-performing policy at test time.

Experiments on email writing and summarization tasks show that late-ensembling outperforms single-method approaches, especially under varying user distributions. The method also generalizes well to different user LLMs, achieving the lowest worst-case suboptimality. The work lays theoretical groundwork for principled user-edit-based LLM fine-tuning and offers a practical algorithm for real-world personalization.

## Method Summary
The method collects user-edit deployment logs (context, agent response, user edits) and extracts three feedback signals: supervised labels (user edits), preference pairs (edited vs original), and edit costs (Levenshtein distance). Three policies are trained offline: SFT on user edits, DPO on preference pairs, and EarlyEnsemble combining both losses. At test time, a UCB bandit selects among these policies based on observed edit costs, adapting to different user distributions without additional fine-tuning.

## Key Results
- Late-ensembling achieves lowest worst-case suboptimality (0.1586) across all train/test user combinations
- SFT outperforms DPO with strong users (high γ_min) while DPO excels with weak users (low γ_min), confirming theoretical sample complexity predictions
- Transfer learning degrades all methods, but ensemble maintains lowest worst-case suboptimality across user LLMs
- The unified framework extracts supervised, preference, and cost signals from single user edits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User edits simultaneously encode three complementary feedback signals—supervision, preference, and cost—enabling a unified learning framework
- Mechanism: When a user edits response y to y', this single interaction yields: (1) a supervised label y' for fine-tuning, (2) a preference pair (y' ≻ y) for DPO, and (3) an edit distance cost ∆edit(y, y') for RL. The paper proves each signal has different sample complexity trade-offs depending on user behavior and policy class
- Core assumption: User edits reflect genuine preferences rather than random corrections; the balance equation (Assumption 1) holds linking edit probability to policy optimality ratios
- Evidence anchors:
  - [abstract] "In this setup, there emerges a unification of various feedback types namely preferences, supervised labels, and cost"
  - [section 3.1] Equations 3-6 formalize SFT, DPO, and RL losses from the same edit data
  - [corpus] Related work on fine-grained feedback (arxiv 2512.23693) supports multi-signal supervision
- Break condition: When users make minimal edits (empty edits), preference signal weakens; when edits are extensive, supervision signal may overfit to suboptimal intermediate states

### Mechanism 2
- Claim: The user edit distribution satisfies a contraction property that drives iterative convergence toward the optimal policy
- Mechanism: Lemma 1 proves ||q∘π - π*β||_TV ≤ (1-γ_min(x)) ||π - π*β||_TV, meaning applying the user edit distribution contracts distance to optimal by factor (1-γ_min). Higher γ_min (users more likely to produce optimal edits) yields faster contraction, explaining why "strong users" (high γ_min) favor SFT while "weak users" favor DPO
- Core assumption: Assumption 1's balance equation and non-zero γ_min probability of optimal edits
- Evidence anchors:
  - [section 4.1] Lemma 1 and its proof establish the contraction bound
  - [section 5] "SFT is able to learn and performs much better when trained and tested on strong user... DPO performs better than SFT when we train on weak user"
  - [corpus] No direct corpus support for this specific theoretical contribution
- Break condition: If γ_min → 0 (users never produce optimal edits), contraction stalls and SFT fails to converge regardless of sample size

### Mechanism 3
- Claim: Late-ensembling via bandit selection outperforms single-method approaches by adaptively choosing policies at test time
- Mechanism: Algorithm 1 trains multiple policies (SFT, DPO, EarlyEnsemble) offline, then uses UCB bandit during online deployment to select which policy to query. The edit cost serves as bandit feedback. This handles distribution shift between training and test users without requiring additional fine-tuning
- Core assumption: T online episodes are insufficient for fine-tuning but sufficient for bandit exploration; the best policy varies by user/domain context
- Evidence anchors:
  - [abstract] "propose a late-ensembling bandit strategy that combines these methods and adaptively selects the best-performing policy at test time"
  - [section 3.2] Algorithm 1 specification with UCB selection rule
  - [section 5, Table 1] LateEnsemble achieves lowest worst-case SubOpt (0.1586) across all settings
  - [corpus] T-POP (arxiv 2509.24696) similarly uses test-time adaptation but requires online preference collection
- Break condition: If one policy dominates universally, UCB exploration overhead causes regret; if T is too small, bandit cannot identify best policy

## Foundational Learning

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: The paper treats user edits as implicit preference data (edited response preferred over original). Understanding DPO's objective—optimizing policy directly from preference pairs without reward modeling—is essential to grasp why preference concentrability (C_PREF) affects sample complexity differently than SFT
  - Quick check question: Can you explain why DPO avoids learning an explicit reward model and how that changes its data coverage requirements compared to RL-based approaches?

- Concept: KL-Regularized Policy Optimization
  - Why needed here: The paper defines suboptimality against the KL-regularized optimal policy π*β (Equation 1). All theoretical bounds and the balance equation are formulated in this regularized framework, which prevents policies from deviating too far from the reference π_ref
  - Quick check question: What is the closed-form solution for the optimal KL-regularized policy given cost function c, and why does this formulation appear in both RLHF and this paper's theoretical setup?

- Concept: Upper Confidence Bound (UCB) Bandits
  - Why needed here: Late-ensembling uses UCB to balance exploration (trying policies to estimate their costs) and exploitation (using the apparent best policy). The α√(log(t)/N(π)) term encourages trying under-explored policies
  - Quick check question: In Algorithm 1, why does UCB subtract the confidence term rather than add it, and what does this imply about the relationship between edit cost and policy quality?

## Architecture Onboarding

- Component map:
  ```
  Offline Phase:
  Dataset D → [SFT trainer] → π_SUP
           → [DPO trainer] → π_PREF  
           → [SFT+DPO combined] → π_EF
  
  Online Phase (LateEnsemble):
  Context x_t → [Policy selector (UCB)] → Selected π_t → Response y_t
              ↑                              |
              └── Edit cost c_t ←── User edit y'_t
  ```

- Critical path:
  1. Collect deployment logs: (context, agent_response, user_edit) tuples
  2. Compute edit costs using Levenshtein distance (or domain-appropriate metric)
  3. Train π_SUP via supervised fine-tuning on user edits
  4. Train π_PREF via DPO treating (agent_response, user_edit) as preference pairs
  5. Train π_EF by combining SFT and DPO losses with λ weighting
  6. At deployment, initialize UCB bandit with all three policies
  7. Select policy via argmin(C(π)/N(π) - α√(log(t)/N(π))), observe edit cost, update statistics

- Design tradeoffs:
  - **SFT vs DPO**: SFT converges faster with high γ_min users (strong editors) but fails when edits are suboptimal; DPO is more robust to weak editing but requires preference concentrability
  - **Early vs Late ensemble**: Early ensembling (Equation 7) fixes trade-off weights at train time via λ; late ensembling adapts at test time but incurs exploration overhead
  - **Bandit α parameter**: Higher α increases exploration (more conservative worst-case performance); lower α exploits faster but may miss optimal policy

- Failure signatures:
  - SFT generates repetitive text when trained on weak user edits (noted in experiments, addressed via post-generation trimming)
  - DPO underperforms when preference concentrability C_PREF is high (limited coverage of preference space)
  - LateEnsemble shows high variance early in test phase (Figure 2) before converging to best policy
  - Transfer learning (Table 2) degrades all methods, but ensemble maintains lowest worst-case SubOpt

- First 3 experiments:
  1. **Validate SFT/DPO trade-off**: Replicate the strong vs weak user comparison from Section 5. Train on weak user data, test on strong user—expect DPO to outperform SFT, confirming Theorem 1 vs Theorem 2 trade-offs
  2. **Ablate ensemble composition**: Run LateEnsemble with only {π_SUP, π_PREF} vs full {π_SUP, π_PREF, π_EF} to determine if EarlyEnsemble policy adds value beyond banditing over SFT+DPO directly
  3. **Stress test γ_min sensitivity**: Synthesize user simulators with varying γ_min (probability of optimal edit). Plot SFT suboptimality vs sample size for γ_min ∈ {0.1, 0.3, 0.5, 0.7} to verify Theorem 1's predicted dependence on (1-γ_min) factor

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed methods perform with real human users compared to the LLM-simulated users used in experiments?
- Basis in paper: [explicit] The authors explicitly state "Future works can investigate limitations of our work such as...evaluating these algorithms in a human study" and note all experiments use simulated users (Qwen 3 32B, Llama-3.3-70B)
- Why unresolved: LLM-simulated users may not capture real human editing behavior, preferences, or the diversity of actual user interactions
- What evidence would resolve it: Experiments with human participants editing LLM outputs across multiple domains, comparing edit patterns and algorithm performance against simulated user baselines

### Open Question 2
- Question: How robust are the theoretical guarantees when the balance equation assumption (Assumption 1) is violated?
- Basis in paper: [explicit] The authors state future work should address "relaxing different assumptions." Assumption 1 requires a specific relationship between user edit probabilities and the optimal policy, which may not hold for all real user behaviors
- Why unresolved: The contraction property (Lemma 1) and convergence results depend critically on this assumption. Real users may not edit according to the balance equation
- What evidence would resolve it: Empirical analysis measuring how actual user edit distributions deviate from the balance equation, and theoretical analysis extending results under relaxed assumptions

### Open Question 3
- Question: What is the optimal algorithm for combining SFT, DPO, and RL feedback, and is the late-ensembling approach provably optimal?
- Basis in paper: [explicit] The authors explicitly mention "deriving optimal algorithms" as future work. The current UCB-based late-ensemble is heuristic and the paper notes "late ensembling isn't guaranteed to do better in practice"
- Why unresolved: The paper proves different sample complexity trade-offs for each method but does not establish optimality of any combination strategy
- What evidence would resolve it: Theoretical lower bounds on regret for this learning setting and algorithms that achieve them, or empirical optimization over combination strategies

### Open Question 4
- Question: How can the pessimistic RL method (Equation 6) be implemented efficiently while preserving its theoretical guarantees?
- Basis in paper: [inferred] The authors state "We do not evaluate Equation 6 given challenges in implementing it," noting the pessimistic estimator \bar{f} is "computationally impractical in the general setting"
- Why unresolved: The theoretical analysis relies on the pessimistic RL approach, but practical implementation remains infeasible, creating a gap between theory and practice
- What evidence would resolve it: Efficient approximations or relaxation methods for the pessimistic cost estimator with provable approximation guarantees

## Limitations
- The theoretical analysis relies on Assumption 1 (balance equation linking edit probabilities to policy optimality ratios), which may not hold for real user behavior
- The RL-based method (Equation 6) was not empirically validated, leaving the late-ensemble bandit's third policy component unproven
- Transfer learning experiments show significant performance degradation across user models, suggesting limited generalization

## Confidence
- **High confidence**: The unified framework for extracting supervised, preference, and cost signals from user edits is well-established and theoretically sound
- **Medium confidence**: Late-ensembling with UCB bandit improves worst-case suboptimality, though the benefit depends heavily on the diversity of user distributions
- **Low confidence**: The sample complexity bounds and their practical implications, particularly the specific hyperparameter recommendations (λ values, α=150) without broader validation

## Next Checks
1. **Validate Assumption 1 empirically**: Collect real user edit data and test whether the balance equation holds across different user types and tasks
2. **Ablate RL policy contribution**: Run LateEnsemble without the RL-trained policy to quantify whether this component adds value beyond SFT/DPO/EarlyEnsemble
3. **Stress test transfer learning**: Evaluate LateEnsemble across a wider range of user LLMs (beyond Qwen3-32B) and document performance degradation patterns