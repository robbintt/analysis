---
ver: rpa2
title: Not All Documents Are What You Need for Extracting Instruction Tuning Data
arxiv_id: '2505.12250'
source_url: https://arxiv.org/abs/2505.12250
tags:
- data
- pairs
- documents
- arxiv
- equal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EQUAL, a framework that efficiently extracts
  instruction-tuning data from web documents by iteratively selecting document clusters
  and extracting high-quality QA pairs using multi-armed bandits and optimal transport
  scoring. It addresses the challenge of prohibitive computational costs and irrelevant
  data in large-scale document processing.
---

# Not All Documents Are What You Need for Extracting Instruction Tuning Data

## Quick Facts
- **arXiv ID**: 2505.12250
- **Source URL**: https://arxiv.org/abs/2505.12250
- **Reference count**: 39
- **Primary result**: Reduces computational costs by 5-10x while improving downstream task accuracy by 2.5%

## Executive Summary
This paper introduces EQUAL, a framework that efficiently extracts instruction-tuning data from web documents by iteratively selecting document clusters and extracting high-quality QA pairs. The method addresses prohibitive computational costs and irrelevant data in large-scale document processing using multi-armed bandits and optimal transport scoring. Experiments on AutoMathText and StackOverflow with LLaMA-3.1-8B and Mistral-7B models demonstrate significant efficiency gains while maintaining or improving model performance.

## Method Summary
EQUAL processes large web document corpora by first clustering documents based on embeddings derived from contrastive learning, then using a multi-armed bandit strategy to identify clusters likely to contain valuable QA pairs. The framework iteratively selects clusters, extracts QA pairs, and computes optimal transport scores to measure distributional similarity with reference datasets. This approach reduces computational costs by 5-10x compared to traditional methods while improving downstream task accuracy by 2.5%.

## Key Results
- Reduces computational costs by 5-10x through efficient cluster selection
- Improves downstream task accuracy by 2.5% compared to baseline methods
- Successfully extracts high-quality QA pairs from 1.45M AutoMathText and 1.22M StackOverflow documents

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Learning for Document-QA Alignment
The framework aligns document and QA pair feature spaces via contrastive learning to improve document clustering quality. A small random sample of documents is used to extract QA pairs, which serve as positive examples for contrastive learning. The embedding model is fine-tuned to maximize similarity between documents and their contained QA pairs while minimizing similarity with other pairs. This creates a shared embedding space where documents are close if they contain similar QA content, enabling meaningful clustering based on instructional value. The core assumption is that QA pair distributions in a small random sample are sufficiently representative to learn generalizable alignment.

### Mechanism 2: Multi-Armed Bandit for Cluster Selection
Each document cluster is treated as an "arm" in a multi-armed bandit problem with an Upper Confidence Bound strategy. The algorithm iteratively selects clusters based on a Document Sampling score that combines current quality estimates (exploitation) with uncertainty bonuses (exploration). This balances exploiting known good clusters while exploring under-sampled ones to avoid local optima and ensure data diversity. The core assumption is that cluster quality can be iteratively estimated from small samples and fits the stochastic bandit framework.

### Mechanism 3: Optimal Transport for Distributional Scoring
The framework uses optimal transport score to provide a robust distribution-level measure of cluster relevance to downstream tasks. Instead of individual QA pair evaluation, OT measures the transportation cost between QA pair distributions in a cluster and reference datasets. Lower costs indicate closer distributional matches, serving as reward signals for the MAB algorithm. The core assumption is that OT cost between extracted QA distributions and reference distributions reliably proxies downstream performance of models fine-tuned on that data.

## Foundational Learning

**Concept: Contrastive Learning**
- Why needed here: Creates unified embedding space for documents and QA pairs, prerequisite for meaningful clustering
- Quick check question: Can you explain how positive and negative pairs are constructed to train an embedding model to distinguish between similar and dissimilar items?

**Concept: Multi-Armed Bandit (MAB) Problem**
- Why needed here: Understands the iterative cluster selection strategy balancing exploitation and exploration
- Quick check question: In a recommendation system, how does an MAB algorithm decide whether to show a user an item they've liked before (exploitation) or a new, untested item (exploration)?

**Concept: Optimal Transport (OT)**
- Why needed here: Grasps how EQUAL measures cluster quality by comparing entire distributions of extracted QA pairs to target distributions
- Quick check question: How does the OT score differ from a simple average similarity score when comparing two sets of data points?

## Architecture Onboarding

**Component map**: Warm-up/Contrastive Learning -> Document Clustering -> Iterative Selection (MAB) -> QA Extraction & Model Tuning

**Critical path**: The quality of the final fine-tuned model is most critically dependent on the Iterative Selection loop, which hinges on OT score accuracy and MAB's ability to use it effectively.

**Design tradeoffs**: Cluster granularity (k value) creates tradeoffs between variance within clusters and selection overhead. Small k leads to high variance making samples unrepresentative; large k creates many similar clusters hindering exploration. The reference dataset choice for OT scoring directly biases data selection.

**Failure signatures**: Sudden drop in diversity of selected QA pairs indicates MAB over-exploitation. Stagnant or noisy OT scores suggest samples are too small to be representative or embedding alignment was poor.

**First 3 experiments**:
1. Ablation Study on Contrastive Learning: Compare EQUAL with and without warm-up step to validate alignment mechanism
2. MAB vs. Greedy Selection: Compare MAB-based selection against simple greedy strategy to quantify exploration value
3. Sensitivity Analysis on Cluster Count (k): Test varying k values on smaller dataset slice to observe impact on selection diversity and task accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can EQUAL effectively generalize to non-structured domains like creative writing or open-domain dialogue where optimal transport scoring may be less definitive than in math or code?
- Basis: Experiments restricted to AutoMathText and StackOverflow with distinct logical structures
- Why unresolved: OT correlation with task performance uncertain in subjective or highly diverse domains
- What evidence would resolve it: Application to general-purpose instruction tuning datasets with open-ended benchmark evaluation

### Open Question 2
- Question: How robust is EQUAL to size and quality of reference dataset used for scoring?
- Basis: Method relies on high-quality reference set for OT reward calculation
- Why unresolved: Small or noisy reference sets could provide misleading reward signals
- What evidence would resolve it: Ablation studies varying reference dataset volume and noise levels

### Open Question 3
- Question: Does improving QA extraction quality within the loop amplify EQUAL's effectiveness?
- Basis: Authors state extraction quality improvement is orthogonal to this work
- Why unresolved: Noisy extraction could degrade embedding alignment and limit clustering/selection accuracy
- What evidence would resolve it: Experiments integrating higher-quality extraction models or self-correction mechanisms

## Limitations
- Computational barrier: Reliance on 72B parameter model for QA extraction presents significant cost for many research groups
- Domain specificity: Results only validated on math and code domains, generalizability to other instruction-tuning scenarios unproven
- Specification gaps: Key hyperparameters like sampling batch size, OT solver configuration, and negative sampling strategy not fully detailed

## Confidence

- **High Confidence**: Core iterative selection framework combining MAB and OT scoring is well-specified and theoretically sound; ablation study showing 2.5% performance drop without contrastive learning is compelling
- **Medium Confidence**: Computational cost reduction claims supported by FLOPs measurements but actual wall-clock savings may vary; parameter choices appear reasonable but may not be optimal
- **Low Confidence**: Exact impact of unspecified hyperparameters on final performance unclear; no reported variance across multiple runs or sensitivity analyses for these parameters

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Run EQUAL with varying cluster counts (k=500, 1000, 2000) and sampling batch sizes (10, 50, 100 documents) on smaller dataset slice to quantify impact on convergence, diversity, and accuracy

2. **OT Score Stability Test**: Log and analyze OT scores over time during MAB selection; compute variance across multiple small samples from same cluster to assess noise levels and reward signal stability

3. **Cross-Domain Generalization**: Apply EQUAL to different domain (legal documents, scientific literature) with different downstream task (summarization, question answering) to evaluate efficiency and performance benefits outside math and code domains