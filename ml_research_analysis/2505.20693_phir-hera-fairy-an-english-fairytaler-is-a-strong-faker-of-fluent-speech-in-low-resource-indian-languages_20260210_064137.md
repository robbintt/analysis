---
ver: rpa2
title: 'Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech
  in Low-Resource Indian Languages'
arxiv_id: '2505.20693'
source_url: https://arxiv.org/abs/2505.20693
tags:
- speech
- languages
- indian
- in-f5
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Phir Hera Fairy demonstrates that large-scale English TTS models
  can be effectively adapted to Indian languages through fine-tuning, achieving human-level
  speech synthesis and unlocking emergent abilities like polyglot fluency, voice cloning,
  and code-mixing. Direct fine-tuning on Indian languages without English data yields
  the best results, with MUSHRA scores reaching 73.4 overall and near-human naturalness
  for seen speakers.
---

# Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages

## Quick Facts
- arXiv ID: 2505.20693
- Source URL: https://arxiv.org/abs/2505.20693
- Reference count: 0
- This paper demonstrates that large-scale English TTS models can be effectively adapted to Indian languages through fine-tuning, achieving human-level speech synthesis and unlocking emergent abilities like polyglot fluency, voice cloning, and code-mixing.

## Executive Summary
Phir Hera Fairy demonstrates that large-scale English TTS models can be effectively adapted to Indian languages through fine-tuning, achieving human-level speech synthesis and unlocking emergent abilities like polyglot fluency, voice cloning, and code-mixing. Direct fine-tuning on Indian languages without English data yields the best results, with MUSHRA scores reaching 73.4 overall and near-human naturalness for seen speakers. The approach also enables zero-resource TTS for languages like Bhojpuri and Tulu through transfer learning and synthetic data generation, achieving MUSHRA scores of 82.0 and 93.6, respectively. This method provides a scalable, compute-efficient solution for building inclusive, high-quality TTS systems for low-resource languages.

## Method Summary
The method involves fine-tuning an English F5-TTS model on 1417 hours of Indian language data across 11 languages. The approach uses vocabulary expansion with 685 Indic character tokens, initializing new embeddings by sampling from the English checkpoint's embedding space. Direct fine-tuning on Indian languages only (without English replay) achieves the best results, with self-training enabling zero-resource synthesis for unseen languages sharing scripts with training languages.

## Key Results
- Direct fine-tuning achieves MUSHRA score of 73.4 overall, surpassing mixed-language fine-tuning (66.2) and training from scratch (43.2)
- Near-human naturalness achieved for seen speakers (78.0 vs 75.9 for human recordings)
- Zero-resource TTS for Bhojpuri and Tulu achieves MUSHRA scores of 82.0 and 93.6 respectively
- Code-mixing synthesis maintains intelligibility scores above 80 across language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: English TTS pretraining provides transferable acoustic representations that enable human-quality synthesis with 1.4% of original data.
- Mechanism: The English F5-TTS model, pretrained on ~100K hours, learns general speech synthesis capabilities (prosody, acoustic coherence, speaker representations) that transfer across languages. Fine-tuning only on IN11 (1417 hours total across 11 languages) preserves these learned priors while adapting to Indian phonology.
- Core assumption: Speech synthesis involves language-independent acoustic modeling that transfers across typologically distant languages.
- Evidence anchors: [abstract] "Fine-tuning with only Indian data proves most effective and the resultant IN-F5 is a near-human polyglot"; [section 4.1] Training from scratch achieves 43.2 MUSHRA vs. 73.4 for direct fine-tuning; "simply initializing from a large pretrained English model and fine-tuning on a small set of IN11 data (1.4% of EN data) is sufficient to reach human-level synthesis"

### Mechanism 2
- Claim: Direct fine-tuning without English replay achieves higher naturalness than mixed-language fine-tuning.
- Mechanism: Fine-tuning exclusively on IN11 allows full optimization capacity for Indian language distributions without gradient interference from English. The original English checkpoint remains available separately for English synthesis.
- Core assumption: Catastrophic forgetting of English is acceptable when the goal is Indian-language-only deployment; English-Indian gradient mixing introduces harmful interference.
- Evidence anchors: [section 4.1] EN→IN achieves 73.4 overall MUSHRA vs. 66.2 for EN→EN+IN; "direct fine-tuning (78.0) slightly surpasses human recordings (75.9)"; [section 2.2] Three strategies explicitly compared; "fine-tuning without English yields the most natural adaptation"

### Mechanism 3
- Claim: Zero-resource TTS for unseen languages is achievable via script-sharing transfer, synthetic data generation, and human-in-the-loop validation.
- Mechanism: Languages sharing scripts with training languages inherit learned grapheme representations. IN-F5 generates synthetic speech → native speakers validate 1 hour of samples → self-training on validated data refines the model.
- Core assumption: High phonetic orthography of Indian languages means script knowledge transfers to phonological knowledge; human validation filters out synthesis errors.
- Evidence anchors: [abstract] "IN-F5 can synthesize unseen languages like Bhojpuri and Tulu using a human-in-the-loop approach for zero-resource TTS via synthetic data generation"; [section 2.4] Tulu shares Kannada script; Bhojpuri uses Devanagari like Hindi; MUSHRA scores: Bhojpuri 82.0, Tulu 93.6

## Foundational Learning

- Concept: Flow matching / diffusion-based TTS
  - Why needed here: F5-TTS uses flow matching for speech generation; understanding this helps debug synthesis artifacts and training stability issues.
  - Quick check question: Can you explain why flow matching enables zero-shot voice cloning by conditioning on reference audio?

- Concept: Transfer learning vs. multi-task learning
  - Why needed here: The paper compares fine-tuning strategies; understanding gradient interference explains why English+IN mixing underperforms.
  - Quick check question: Why might mixed-language batches harm adaptation compared to single-language fine-tuning?

- Concept: Grapheme-to-phoneme (G2P) vs. character-level modeling
  - Why needed here: IN-F5 uses character tokens directly (685 tokens) rather than phonemes; this bypasses underdeveloped G2P for Indian languages.
  - Quick check question: What properties of Indian writing systems make character-level modeling viable?

## Architecture Onboarding

- Component map: English F5-TTS checkpoint → Vocabulary expansion (685 Indic character tokens, embeddings initialized via random sampling from English embedding space) → Fine-tuning on IN11 (1417 hours) → IN-F5 model → Optional self-training for zero-resource languages.

- Critical path: Vocabulary initialization quality → fine-tuning data diversity (studio + ASR-restored + crowdsourced) → emergent ability preservation (voice cloning, polyglot, code-mixing). Incorrect vocabulary initialization causes token embedding misalignment and poor synthesis.

- Design tradeoffs: (1) EN→IN vs. EN→EN+IN: higher Indian quality vs. English retention; (2) 10h/L vs. 100h/L: compute efficiency vs. marginal quality gains (0.8% average improvement); (3) character-level vs. phoneme: simplicity vs. potential pronunciation errors on loanwords.

- Failure signatures: (1) MUSHRA <50 suggests insufficient pretraining transfer or vocabulary issues; (2) speaker similarity <80 with training speakers indicates embedding initialization failure; (3) code-mixing intelligibility <60 suggests cross-lingual transfer breakdown.

- First 3 experiments:
  1. Validate vocabulary expansion: Generate speech for all 11 languages with training speakers; verify MUSHRA >70 and speaker similarity >85.
  2. Test polyglot capability: Cross-lingual synthesis (e.g., Odia speaker generating Hindi); expect naturalness 65-70.
  3. Zero-resource pilot: Generate 1 hour of synthetic speech for a script-sharing unseen language; have native speaker validate intelligibility before self-training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does direct fine-tuning on Indian data (EN→IN) outperform mixed fine-tuning (EN→EN+IN) for naturalness, contrary to the expectation that retaining English data prevents catastrophic forgetting and aids generalization?
- Basis in paper: [explicit] The authors note a "surprising trend" in Section 4.1 where direct fine-tuning achieves the highest MUSHRA score (73.4), surpassing the mixed-data strategy (66.2).
- Why unresolved: The paper demonstrates the result but does not provide a mechanistic explanation for why retaining the source language data in the fine-tuning mix degrades performance on the target languages compared to direct adaptation.
- What evidence would resolve it: Ablation studies varying the ratio of English-to-Indian data during fine-tuning, or analysis of the loss landscape/gradient interference between the two datasets.

### Open Question 2
- Question: Does large-scale English pre-training specifically enable the model to learn universal speaker identity embeddings that transfer robustly to typologically distant Indian languages?
- Basis in paper: [inferred] In Section 4.3, the authors hypothesize that "pretraining on English may perhaps benefit models in implicitly learning speaker characteristics that transfer well during multilingual adaptation" based on high speaker similarity scores even in data-scarce (1h/L) settings.
- Why unresolved: While the results show high speaker similarity, the paper does not isolate whether this is an inherent property of the Flow Matching architecture or a specific cross-lingual transfer benefit derived from the English pre-training data.
- What evidence would resolve it: Probing experiments on the model's latent space to visualize and quantify the disentanglement of speaker identity from language-specific phonetic features.

### Open Question 3
- Question: Why does the human-in-the-loop self-training method yield improvements for Bhojpuri but result in no positive impact for Tulu?
- Basis in paper: [inferred] Section 4.4 reports that self-training improved Bhojpuri scores slightly but "has no positive impact on Tulu," leaving the inconsistency in the proposed zero-resource recipe unexplained.
- Why unresolved: The paper attributes the general success to transfer learning from related scripts, but does not explain why the self-training loop fails to refine the model for Tulu specifically.
- What evidence would resolve it: A comparative analysis of the phoneme distribution and expressiveness of the proxy speakers (Kannada for Tulu vs. Maithili for Bhojpuri) relative to the synthesized outputs.

## Limitations
- The evaluation relies entirely on MUSHRA subjective ratings without objective intelligibility metrics or word-level alignment quality measures.
- Zero-resource approach assumes script-sharing implies phonetic similarity, but Indian languages exhibit significant phonological variation even within the same script family.
- The paper does not address computational requirements for production deployment or model size constraints for edge devices.

## Confidence
- High confidence: English pretraining provides effective transfer for Indian languages (MUSHRA scores 73.4 vs. 43.2 baseline, direct empirical evidence)
- Medium confidence: Zero-resource TTS works via script-sharing transfer (scores 82.0/93.6 are promising but based on limited validation scope)
- Medium confidence: Code-mixing synthesis quality (methodology described but limited systematic evaluation across code-mixing types)
- Low confidence: Claims about computational efficiency compared to training from scratch (no ablation on compute costs or training time)

## Next Checks
1. **Objective intelligibility verification**: Conduct forced alignment tests between synthetic and reference transcripts using native speaker transcriptions to verify that high MUSHRA naturalness correlates with actual word-level accuracy across all 11 languages.
2. **Zero-resource robustness testing**: Evaluate the zero-resource approach on a language with shared script but distinct phonology (e.g., Sanskrit/Devanagari vs. Hindi) to quantify the limits of script-sharing transfer assumptions.
3. **Production readiness assessment**: Benchmark the IN-F5 model's inference latency and memory footprint against commercial TTS systems on standard edge hardware to validate claimed computational efficiency.