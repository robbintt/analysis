---
ver: rpa2
title: 'Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction'
arxiv_id: '2506.07976'
source_url: https://arxiv.org/abs/2506.07976
tags:
- interaction
- agents
- scaling
- task
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces interaction scaling as a new dimension of
  test-time scaling for interactive agents, focusing on increasing the number of interaction
  steps rather than just reasoning depth. The core method, TTI, uses curriculum-based
  online reinforcement learning to adaptively scale interaction horizons, training
  agents to explore and adapt dynamically.
---

# Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction

## Quick Facts
- arXiv ID: 2506.07976
- Source URL: https://arxiv.org/abs/2506.07976
- Reference count: 40
- The paper introduces interaction scaling as a new dimension of test-time scaling for interactive agents, focusing on increasing the number of interaction steps rather than just reasoning depth. The core method, TTI, uses curriculum-based online reinforcement learning to adaptively scale interaction horizons, training agents to explore and adapt dynamically. Using a Gemma 3 12B model, TTI achieves state-of-the-art performance among open-source agents on WebVoyager and WebArena benchmarks, improving task success rates by 9% and 8% respectively. The approach demonstrates that scaling interaction enables agents to balance exploration and exploitation effectively, outperforming fixed-horizon baselines and conventional test-time compute scaling methods.

## Executive Summary
The paper argues that scaling test-time interaction—the number of environment steps an agent takes—is a distinct and powerful axis for improving interactive agents, complementary to traditional reasoning depth scaling. The proposed TTI method trains agents using a curriculum that gradually increases allowed interaction horizons, enabling them to adaptively explore and exploit based on task complexity. Experiments show that this approach yields state-of-the-art results on web navigation benchmarks, with agents learning to balance exploration (e.g., search, backtracking) and exploitation (direct task completion) more effectively than fixed-horizon or reasoning-only baselines.

## Method Summary
TTI (Test-Time Interaction) is a curriculum-based online reinforcement learning method that scales the number of environment interaction steps during training. It uses a Gemma 3 12B model as the policy, which takes screenshots, accessibility trees, and action history as input and outputs one of six discrete actions (click, type, scroll, goback, bing, answer). The method employs a multiplicative curriculum schedule that gradually increases the maximum allowed interaction horizon (e.g., 10→20→30 steps). Training uses filtered behavior cloning: only successful trajectories are kept (via a 27B verifier) and used to update the policy via supervised learning. Synthetic tasks are generated via an exploratory agent, and training is online with a replay buffer storing successful rollouts.

## Key Results
- TTI achieves 62.5% success rate on WebVoyager, improving state-of-the-art by 9% among open-source agents
- TTI achieves 39.2% success rate on WebArena, improving state-of-the-art by 8% among open-source agents
- Interaction scaling outperforms per-step reasoning scaling under fixed compute budgets, as shown by steeper success rate improvements in Figure 3

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling interaction steps yields higher task success than scaling per-step reasoning under fixed compute budgets.
- Mechanism: Each additional environment step provides new observations (information gain), whereas extended per-step reasoning only reorganizes existing information. The agent can explore alternatives, backtrack, and dynamically re-plan within a single rollout.
- Core assumption: Tasks are partially observable or non-stationary, requiring information-gathering through action.
- Evidence anchors:
  - [abstract] "this process does not allow agents to acquire new information from the environment or adapt their behavior over time"
  - [Section 4.2] Figure 3 shows interaction scaling achieves the steepest success rate improvement versus total tokens; best-of-n yields diminishing returns despite highest cost.
  - [corpus] Neighbor paper "Budget-Aware Tool-Use Enables Effective Agent Scaling" similarly argues scaling "acting" via tool calls is a distinct and valuable axis, supporting the general principle.
- Break condition: If environments are fully observable and deterministic, per-step reasoning may suffice; interaction scaling offers marginal benefit.

### Mechanism 2
- Claim: Curriculum-based horizon scaling prevents the learning instability observed with fixed long horizons.
- Mechanism: Starting with short rollouts allows the agent to learn exploitative "atomic" skills first; gradually increasing the horizon introduces exploration behaviors (e.g., GoBack, Bing search) once basic competence is established. This avoids noisy credit assignment from long trajectories early in training.
- Core assumption: Skills learned at short horizons transfer compositionally to longer horizons.
- Evidence anchors:
  - [Section 5.1] "agents trained at longer horizons generally learn policies that are quite stochastic and learn significantly more slowly due to higher variance of the loss and credit assignment challenges"
  - [Section 5.2] Table 2: multiplicative curriculum achieves 32.25% vs additive 29.50% success rate; "exposes the agent to longer horizons early on and helps prevent it from overfitting prematurely to shortcut behaviors"
  - [corpus] No direct corpus validation for curriculum-over-horizon specifically; related work (WebRL, AutoWebGLM) uses difficulty-based curricula, not horizon-based.
- Break condition: If tasks require exploration from the outset (no exploitable short-path solutions), curriculum may delay necessary learning.

### Mechanism 3
- Claim: TTI-trained agents adaptively balance exploration and exploitation based on task context.
- Mechanism: The multiplicative curriculum encourages the policy to learn when to extend interaction (complex tasks require search/backtracking) versus when to terminate early (simple tasks have direct paths). The agent's trajectory length and GoBack/Bing action usage increase as the allowed horizon expands during training.
- Core assumption: Task complexity correlates with optimal interaction length, and the policy can infer this from observations.
- Evidence anchors:
  - [Section 6.1] Figure 6: TTI agents show increasing GoBack and Bing actions once maximum horizon reaches 30 (green shaded area), while fixed h=10 agents continuously decrease these behaviors.
  - [Section 6.2] Case studies show TTI exploring multiple recipes when initial attempts fail, versus early-stage agents that "stick to the first recipe" and hallucinate alternatives.
  - [corpus] TIDE paper notes test-time improvement mechanisms remain poorly understood; does not contradict but offers no direct support.
- Break condition: If tasks uniformly require fixed-length solutions, adaptive behavior provides no advantage.

## Foundational Learning

- Concept: **Reinforcement Learning with Sparse Rewards**
  - Why needed here: TTI uses binary task success (0/1) as the sole reward signal; understanding credit assignment over long horizons is essential.
  - Quick check question: Can you explain why sparse rewards cause high variance in policy gradient estimates for long trajectories?

- Concept: **Curriculum Learning**
  - Why needed here: The core innovation is structuring training by gradually increasing interaction horizon rather than task difficulty.
  - Quick check question: What is the difference between additive and multiplicative curriculum schedules, and when might each fail?

- Concept: **Test-Time Compute Scaling**
  - Why needed here: TTI positions itself as a new axis (interaction scaling) complementary to existing methods like chain-of-thought extension and best-of-n sampling.
  - Quick check question: Why does best-of-n scale poorly for multi-step interactive tasks compared to single-turn reasoning tasks?

## Architecture Onboarding

- Component map: Synthetic Task Generator -> Web Environment -> Policy Network (Gemma 3 12B) -> Actions -> Evaluator (Gemma 3 27B) -> Replay Buffer -> Policy Update

- Critical path:
  1. Generate synthetic tasks via exploration
  2. For each training iteration: sample tasks, collect rollouts at current horizon h_i
  3. Filter successful trajectories via evaluator
  4. Update policy via filtered behavior cloning (maximize log-likelihood of actions in successful rollouts)
  5. Increase h_i per multiplicative schedule

- Design tradeoffs:
  - Multiplicative vs additive curriculum: multiplicative exposes longer horizons earlier but risks more early failures
  - Filtered BC vs full RL: simpler and stable, but cannot leverage negative examples; authors note GRPO/PPO as future work
  - Observation truncation (last 3 steps) vs full history: reduces context window pressure but may lose relevant history

- Failure signatures:
  - **Over-reliance on resets:** Agent returns to Bing search rather than recovering within-domain (see Appendix E.6)
  - **Limited self-verification:** Agent identifies mismatch (e.g., wrong year) but submits incorrect answer anyway (Appendix E.7)
  - **Fixed short horizon overfitting:** Agent terminates prematurely even when longer interaction allowed
  - **Fixed long horizon drift:** Agent executes aimless exploration without progressing toward goal

- First 3 experiments:
  1. **Baseline replication:** Implement prompting-based "check-again" mechanism on WebArena subset (62 tasks); verify 23%→28% improvement curve matches Figure 2.
  2. **Ablate curriculum schedule:** Compare fixed h=10, fixed h=30, additive, and multiplicative schedules on same task set; expect multiplicative ~3% higher than fixed h=20 per Table 2.
  3. **Scale to WebVoyager:** Train TTI on synthetic task data with horizon schedule [10,20,20,30,...]; evaluate on held-out validation set and track trajectory length, GoBack/Bing usage, and success rate per Figure 6 patterns.

## Open Questions the Paper Calls Out

- **Question:** How should reinforcement learning training incentivize the optimal balance between generating longer per-step reasoning traces ("thinking") and increasing the number of interaction steps ("doing")?
  - Basis in paper: [explicit] Section 7 asks, "how we should incentivize RL training to best balance thinking and acting," noting that TTI currently causes per-step reasoning tokens to decrease in favor of longer interaction.
  - Why unresolved: The paper demonstrates that interaction scaling is effective, but it does not propose a mechanism to dynamically allocate a compute budget between reasoning depth and interaction length; the current training simply prefers acting.
  - What evidence would resolve it: A study analyzing performance when explicitly regularizing for per-step reasoning length versus interaction horizon, identifying the Pareto frontier for compute-efficient task success.

- **Question:** Does interaction scaling via curriculum learning transfer effectively to high-stochasticity domains like robotic control or open-world compute use?
  - Basis in paper: [explicit] Section 7 lists "Extension to other agentic problem domains" as future work, hypothesizing the method "is likely going to be even more effective in domains that admit more stochasticity and uncertainty."
  - Why unresolved: The empirical validation is confined to web navigation benchmarks (WebVoyager, WebArena), which have specific state dynamics and error modes that may differ from physical or continuous control environments.
  - What evidence would resolve it: Evaluating the TTI curriculum approach on a robotic simulation benchmark (e.g., requiring information gathering before manipulation) and comparing it against fixed-horizon RL baselines.

- **Question:** Can value-based RL algorithms (e.g., PPO, GRPO) improve credit assignment and sample efficiency for interaction scaling compared to the filtered behavior cloning used in this study?
  - Basis in paper: [explicit] Section 7 states, "RL algorithms that train value functions are likely to be more successful at effective credit assignment as the task horizon is scaled further," suggesting the current filtered BC approach is limited.
  - Why unresolved: The authors utilized filtered BC for stability, but credit assignment becomes noisy as horizons scale, suggesting more sophisticated RL objective functions are required for further scaling.
  - What evidence would resolve it: A comparative analysis of TTI trained with PPO/GRPO versus filtered BC on complex, long-horizon tasks, measuring both final success rates and training sample efficiency.

## Limitations

- The work relies heavily on synthetic task generation, which may not fully capture real-world complexity.
- The curriculum schedule appears effective but lacks ablation studies on optimal horizon increments.
- The filtered BC approach cannot learn from failures, potentially limiting robustness.
- Results are benchmark-specific (WebVoyager/WebArena) without broader generalization testing.

## Confidence

- **High confidence:** Interaction scaling outperforms per-step reasoning scaling (supported by Figure 3 and multiple baselines)
- **Medium confidence:** Curriculum-based horizon scaling improves stability (supported by training dynamics but limited ablation)
- **Medium confidence:** Adaptive exploration/exploitation emerges from multiplicative schedule (supported by trajectory analysis but not exhaustive behavioral testing)

## Next Checks

1. **Replicate the WebArena prompting baseline:** Implement the "check-again" prompting method on the 62-task subset and verify the claimed 23%→28% improvement matches Figure 2.

2. **Curriculum schedule ablation study:** Train fixed-horizon (h=10, h=30) and additive curriculum agents alongside multiplicative TTI on identical task sets; measure success rates and trajectory quality to confirm the ~3% advantage claimed in Table 2.

3. **Failure mode diagnostic test:** Run agents on tasks known to require backtracking or search; instrument to track GoBack/Bing usage patterns and compare against fixed-horizon agents to validate the exploration- exploitation balance claims in Figure 6.