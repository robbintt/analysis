---
ver: rpa2
title: Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large
  Language Models
arxiv_id: '2511.06793'
source_url: https://arxiv.org/abs/2511.06793
tags:
- influential
- unlearning
- knowledge
- neuron
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of machine unlearning in multimodal
  large language models (MLLMs), focusing on the need to selectively forget specific
  knowledge while preserving general model performance. Existing approaches struggle
  with inconsistent forgetting across modalities and disruption of general reasoning
  paths.
---

# Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2511.06793
- Source URL: https://arxiv.org/abs/2511.06793
- Reference count: 40
- Primary result: Up to 87.75% forgetting rate and 54.26% improvement in general knowledge retention on multimodal tasks

## Executive Summary
This paper introduces MIP-Editor, a method for machine unlearning in multimodal large language models that addresses the challenge of selectively forgetting specific knowledge while preserving general model performance. The approach uses a dual-branch influential neuron path localization framework with inter-layer gradient and Fisher integration scores to capture modality-specific information flow, followed by path-aware editing via representation misdirection unlearning. Experimental results demonstrate superior unlearning performance compared to existing approaches, achieving high forgetting rates while maintaining strong general knowledge retention across both textual and visual modalities.

## Method Summary
MIP-Editor addresses machine unlearning in MLLMs through a three-stage process: (1) Locate influential neuron paths using inter-layer gradient integration (IGI) for textual pathways and Fisher integration (IFI) for visual pathways, (2) Prune identified path neurons by zeroing their activations, and (3) Apply representation misdirection unlearning (RMisU) that steers forget-set representations toward random unit vectors while preserving retain-set representations through separate loss terms. The method uses LoRA fine-tuning with Adam optimizer, learning rate 2e-5, 4 epochs, and batch size 4.

## Key Results
- Achieves up to 87.75% forgetting rate and 54.26% improvement in general knowledge retention on multimodal tasks
- On textual tasks, achieves 80.65% forgetting with 77.9% retention of general performance
- Demonstrates superior performance compared to baselines including GA, KL, NPO, and MANU across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Inter-layer Gradient Integration (IGI) for Textual Influential Paths
- Coherent neuron paths across layers encode structured knowledge better than point-wise neurons
- Scales text neuron activations from 0 to original values across N layers, accumulating gradients via Riemann approximation
- Information flows through layer-wise neuron dependencies that can be approximated by gradient integration

### Mechanism 2: Inter-layer Fisher Integration (IFI) for Visual Influential Paths
- Fisher-based second-order signals better capture visual neuron importance than first-order gradients
- Interpolates visual neuron activations and sums squared gradients across layers to approximate diagonal Fisher Information
- FIM diagonal captures sufficient neuron importance for visual pathways due to spatial correlation and redundancy

### Mechanism 3: Representation Misdirection Unlearning (RMisU) for Path-Specific Editing
- Targeted representation steering decouples forget-set from retain-set knowledge
- After pruning identified path neurons, steers forget-set representations toward random unit vectors while preserving retain-set representations
- Random directional perturbation erases semantics without reintroducing forgotten content

## Foundational Learning

- Concept: Machine Unlearning (MU) in MLLMs
  - Why needed: Core problem—selectively forgetting targeted visual/textual knowledge while preserving model utility
  - Quick check: Can you explain why MU in MLLMs is harder than in LLMs (hint: modality inconsistency)?

- Concept: Gradient Integration (Integrated Gradients)
  - Why needed: Underpins IGI for attributing neuron importance across layers
  - Quick check: How does scaling activations from 0 to original and integrating gradients help attribute output contributions?

- Concept: Fisher Information Matrix (FIM)
  - Why needed: Provides second-order curvature signal for visual neuron importance estimation
  - Quick check: Why might squared gradients approximate FIM diagonal efficiently for high-dimensional visual features?

## Architecture Onboarding

- Component map: Input (I, T) image–text pairs → dual-branch path locator (IGI + IFI) → P_t, P_v → Pruning (zero activations) → RMisU Editor → Output: Unlearned MLLM M_θ*

- Critical path: Forward pass on (I, T) → greedy layer-wise IGI/IFI scoring → prune selected neurons → RMisU training with forget misdirection + retain preservation losses → update only path neurons

- Design tradeoffs: Path-based vs. point-wise localization (paths capture structured flow but increase computation), pruning vs. full fine-tuning (targeted pruning reduces over-forgetting but may overlap with retain-set), random vs. learned misdirection (random vectors avoid reintroduction but may be suboptimal)

- Failure signatures: Modality-inconsistent forgetting (textual modality retains forget knowledge), general knowledge collapse (retain-set accuracy drops below 15%), model instability (gradient or FIM divergence on CLEAR benchmark)

- First 3 experiments: (1) Reproduce MIP-Editor on MLLMU-Bench with Qwen2.5-VL at 5% forget ratio, (2) Ablate dual-path: run IGI-only and IFI-only variants, (3) Visualize activation residuals comparing MIP-Editor vs. baselines on forget/retain sets

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational overhead of inter-layer gradient and Fisher integration scale when applied to MLLMs with significantly larger parameter counts (>30B parameters) or deeper architectures? The complexity analysis indicates quadratic time in layers, but empirical evaluation is restricted to 3B and 7B models.

### Open Question 2
How robust is MIP-Editor against adversarial attempts to recover forgotten knowledge, such as fine-tuning attacks or specific membership inference attacks? The paper evaluates unlearning via standard accuracy metrics but does not test against adversarial extraction techniques.

### Open Question 3
Does the random sampling of the unit vector in RMisU introduce variance in the semantic consistency of the unlearned outputs? The paper does not analyze sensitivity to random initialization or specific direction chosen.

## Limitations

- Computational complexity scales quadratically with the number of layers, limiting applicability to very large models
- Sensitivity to hyperparameter choices (learning rate, batch size, forget ratio) can cause model collapse
- Reliance on random perturbation for misdirection may introduce semantic inconsistency in outputs

## Confidence

- Superior modality coordination through dual-path editing: High confidence (well-supported by ablation results)
- Effectiveness of representation misdirection unlearning: Medium confidence (empirical demonstration but assumptions about random perturbation sufficiency untested)
- Reported stability on CLEAR benchmark: Medium confidence (paper admits sensitivity to small hyperparameter changes)

## Next Checks

1. Implement a learned misdirection baseline (e.g., adversarial perturbation) and compare forgetting/retention performance against random vector steering to test sufficiency assumptions
2. Conduct cross-modal ablation studies varying the number of shared forget-set samples between text and vision modalities to quantify inter-modal influence strength
3. Perform extended stability testing on CLEAR with learning rate and batch size sweeps to establish robust hyperparameter bounds and identify failure conditions