---
ver: rpa2
title: Automated Detection of Clinical Entities in Lung and Breast Cancer Reports
  Using NLP Techniques
arxiv_id: '2505.09794'
source_url: https://arxiv.org/abs/2505.09794
tags:
- cancer
- clinical
- entities
- validation
- lung
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study demonstrates the application of Named Entity Recognition\
  \ (NER) using a fine-tuned RoBERTa-based model (bsc-bio-ehr-en3) for automated extraction\
  \ of clinical entities from lung and breast cancer reports. The methodology leveraged\
  \ GMV\u2019s NLP tool uQuery, integrating a text pre-processing layer to enhance\
  \ entity detection."
---

# Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques

## Quick Facts
- arXiv ID: 2505.09794
- Source URL: https://arxiv.org/abs/2505.09794
- Reference count: 12
- Primary result: Fine-tuned RoBERTa-based NER model achieves 95.91% accuracy and F1-scores of 0.7684-0.7489 on clinical entity extraction

## Executive Summary
This study presents a Named Entity Recognition system for automated extraction of clinical entities from Spanish lung and breast cancer reports. The approach leverages a fine-tuned bsc-bio-ehr-en3 RoBERTa model integrated with GMV's uQuery NLP tool, incorporating a text pre-processing layer. Using 600 annotated reports from IIS La Fe, the system demonstrates strong performance on frequent entities like diagnosis methods and pathology findings, though rare entities like disease evolution remain challenging. The methodology shows promise for clinical data extraction while highlighting the importance of pre-processing and training data balance.

## Method Summary
The methodology employs a fine-tuned bsc-bio-ehr-en3 RoBERTa-based model for token classification of 8 clinical entity types in Spanish cancer reports. The approach integrates uQuery's text pre-processing layer to normalize clinical text before model inference. A dataset of 600 manually annotated reports (200 breast, 400 lung) is split 50/25/25 for training, validation, and testing. The model uses standard Transformer NER architecture with class-weighted learning to address entity frequency imbalances. Post-processing associates contextual information like negation and temporality before standardizing outputs to SNOMED/OMOP formats.

## Key Results
- Overall accuracy reaches 95.91% across validation and test sets
- Strong F1-scores for frequent entities: MET (0.8289) and PAT (0.6578) in training set
- Pre-processing layer significantly improves detection of partial entities and formatting variations
- EVOL entity performance remains poor (F1=0) due to zero training examples in breast cancer subset

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a Spanish biomedical pre-trained model on cancer-specific clinical reports enables accurate entity recognition for domain-specific NER tasks. The bsc-bio-ehr-en3 model (RoBERTa-based, pre-trained on Spanish biomedical corpora) is adapted via fine-tuning on 600 manually annotated cancer reports. This transfers general biomedical linguistic knowledge to the specialized task of identifying 8 clinical entity types in lung and breast cancer reports.

### Mechanism 2
Text pre-processing via uQuery's normalization layer measurably improves entity detection accuracy compared to raw clinical text input. Clinical reports contain formatting artifacts, abbreviations, and structural inconsistencies. The pre-processing layer cleans and normalizes text before model inference, reducing noise and enabling more consistent tokenization and entity boundary detection.

### Mechanism 3
Transformer-based token classification with class-weighted learning achieves high performance on frequent clinical entities but remains sensitive to training data imbalance. The model uses self-attention to capture contextual relationships and classifies tokens into 8 entity categories. Performance scales with training data availability per class, with frequent entities showing strong results while rare entities struggle.

## Foundational Learning

- **Named Entity Recognition (NER) as Token Classification**: Why needed here: The entire methodology hinges on understanding NER as a sequence labeling task where each token is classified into entity categories or "O" (outside). Quick check: Given the sentence "Por el hallazgo de múltiples fracturas," would the model classify tokens independently or consider surrounding context? Why does this matter?

- **Transfer Learning in Biomedical NLP**: Why needed here: The approach depends on adapting a pre-trained Spanish biomedical model rather than training from scratch—a standard paradigm requiring understanding of what knowledge transfers and what requires task-specific fine-tuning. Quick check: If you applied the fine-tuned model to Spanish cardiology reports without additional training, would you expect comparable performance? What assumptions would this require?

- **Class Imbalance Effects on Sequence Labeling**: Why needed here: The dramatic performance gap between MET (1237 training instances) and EVOL (0 training instances) directly determines where the model succeeds and fails. Quick check: A new entity type has only 5 annotated examples across 600 reports. What performance would you predict, and what intervention might help?

## Architecture Onboarding

- Component map:
EHR System (IIS La Fe) -> Annotation Layer (Doccano) -> Pre-processing (uQuery) -> Model (bsc-bio-ehr-en3 / RoBERTa) -> Post-processing (uQuery) -> Standardization (SNOMED/OMOP output)

- Critical path:
1. Data preparation: 600 annotated reports split 50/25/25 (train/test/validation) with stratified entity distribution
2. Entity dictionary creation: Hierarchical term lists per entity type, sourced from clinical ontologies and literature
3. Model fine-tuning: 3-5 epochs on training set with standard Transformers NER pipeline
4. Pre-processing integration: uQuery layer applied before inference to normalize clinical text
5. Manual validation loop: Expert review of model outputs to identify systematic errors

- Design tradeoffs:
| Tradeoff | Choice Made | Implication |
|----------|-------------|-------------|
| Dataset size | 600 annotated reports (subset of 6000 available) | Faster annotation but limited generalizability |
| Language model | Spanish-specific (bsc-bio-ehr-en3) | Better Spanish clinical text performance; no cross-lingual transfer |
| Entity granularity | 8 entity types | Coarser than some schemas but simpler annotation |
| Pre-processing | uQuery proprietary layer | Strong results but creates tool dependency |

- Failure signatures:
1. Zero-shot entity failure: Any entity with 0 training instances will have F1=0 (observed: EVOL in breast cancer subset)
2. Low-frequency entity degradation: Entities with <50 training instances show high variance (FACTR, SINT)
3. Pre-processing dependency: Raw text input causes missed entities like staging information (pTNM)
4. Overshoot on high-frequency entities: Model may over-predict MET compared to ground truth

- First 3 experiments:
1. Stratified performance audit: Run inference on held-out test set, compute precision/recall/F1 per entity type. Flag any entity with <50 training examples for potential data augmentation.
2. Pre-processing ablation: Process same 50 reports with and without uQuery pre-processing. Manually count missed entities to quantify the pre-processing contribution.
3. Error analysis on EVOL/low-frequency entities: Extract all false negatives for EVOL and FACTR. Determine if errors stem from insufficient training data, annotation inconsistency, or lexical variation.

## Open Questions the Paper Calls Out
1. How can the detection of underrepresented clinical entities, specifically EVOL (Evolution), be improved in imbalanced datasets?
2. To what extent does the model generalize to clinical reports from different Spanish healthcare institutions with distinct documentation practices?
3. What is the specific quantitative contribution of the uQuery pre-processing layer to the model's overall accuracy independent of the RoBERTa architecture?

## Limitations
- Critical failure on rare entities (EVOL) with zero training examples, rendering the system incomplete for comprehensive clinical documentation
- Proprietary uQuery pre-processing creates tool dependency and prevents independent verification of performance contributions
- Single-center dataset limits generalizability to different institutions with varying documentation practices

## Confidence
**High confidence** in the core mechanism: Fine-tuning Spanish biomedical models on clinical text demonstrably improves entity recognition performance for frequent entities (MET, PAT). **Medium confidence** in the pre-processing contribution: While Figures 2-3 clearly show improvement, the proprietary nature of uQuery prevents independent verification. **Low confidence** in real-world clinical utility: The system's inability to detect entities with <50 training examples represents a critical gap for comprehensive clinical documentation extraction.

## Next Checks
1. Generate synthetic training examples for EVOL and FACTR using clinical template expansion and synonym mapping. Measure whether performance improves from F1=0 to >0.3 with 50-100 augmented examples per entity.
2. Replace uQuery with standard clinical text normalization (e.g., cTAKES, scispaCy) and compare entity detection rates on 100 held-out reports. Quantify the performance gap attributable to proprietary pre-processing.
3. Apply the fine-tuned model to a separate Spanish clinical report corpus from a different institution without additional training. Compute performance drop and identify entity types most sensitive to reporting style variation.