---
ver: rpa2
title: Developing a Grounded View of AI
arxiv_id: '2511.14013'
source_url: https://arxiv.org/abs/2511.14013
tags:
- program
- human
- behavior
- thinking
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the nature and limits of artificial intelligence\
  \ from an engineering perspective, clarifying its fundamental differences from rule-based\
  \ software programs. It proposes a framework distinguishing AI behaviors (\u042F\
  -relations) from rule-based systems (\u03A6-relations), along with three types of\
  \ decision-making in AI systems."
---

# Developing a Grounded View of AI

## Quick Facts
- arXiv ID: 2511.14013
- Source URL: https://arxiv.org/abs/2511.14013
- Authors: Bifei Mao; Lanqing Hong
- Reference count: 26
- This paper proposes a framework distinguishing AI behaviors (Я-relations) from rule-based systems (Φ-relations), along with three types of decision-making in AI systems to enable responsible AI adoption.

## Executive Summary
This paper examines artificial intelligence from an engineering perspective, clarifying its fundamental differences from rule-based software programs. The authors propose a framework distinguishing AI behaviors (Я-relations) characterized by hidden or weakly observable rules from rule-based systems (Φ-relations) with fully specifiable rules. They introduce a three-tier decision framework and epistemic configuration mapping to help users evaluate AI system soundness and make informed adoption decisions while preserving human rule-based rationality.

## Method Summary
The paper develops a theoretical framework through conceptual analysis rather than empirical experimentation. It defines Program Φ (rule-based software with Φ-relations where rules are inherent and specifiable) versus Program Я (AI systems with Я-relations where rules are hidden or only weakly observable). The framework establishes three decision types (Type-I: truth verification, Type-II: input-output consistency, Type-III: rule-based predictability) and epistemic configurations mapping human subjects, reasoning methods, and phenomena types. The methodology provides qualitative guidance for understanding when and how to use AI responsibly.

## Key Results
- Introduces Я-relations as a distinct category of system behavior where rules are hidden or only weakly observable, fundamentally different from rule-based Φ-relations
- Proposes a three-tier decision framework (Type-I, Type-II, Type-III) for evaluating AI output verifiability and epistemic position
- Demonstrates that AI outputs always remain within the Я-relation domain regardless of input type through epistemic configuration analysis
- Provides practical guidance for users to evaluate AI system soundness and maintain human responsibility in AI adoption

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Distinguishing AI systems from rule-based software by analyzing the observability of their governing rules enables appropriate deployment decisions.
- **Mechanism:** The paper introduces a classification scheme where Program Φ operates through Φ-relations (rules inherent in use, fully specifiable in advance) while Program Я operates through Я-relations (stochastic relationships with hidden or weakly observable rules). This distinction determines whether traditional engineering quality assurance applies.
- **Core assumption:** Users can identify whether a system's behavior follows specifiable rules or exhibits fundamental unpredictability.
- **Evidence anchors:**
  - [abstract] "Although the rules of AI behaviors are still hidden or only weakly observable, the paper has proposed a methodology to make a sense of discrimination possible"
  - [Section 4, Notation #2] "Я-Relation: A stochastic or partially indeterminate relationship, in which rules are hidden or only weakly observable—especially in the behaviors of Program Я"
  - [corpus] Weak direct support; neighboring papers focus on extracting rules from transformers and LLM rationality benchmarks but do not validate this specific classification scheme.
- **Break condition:** If a system's rules become fully observable and specifiable post-hoc, it may no longer fit the Я-relation category.

### Mechanism 2
- **Claim:** A three-tier decision framework (Type-I, Type-II, Type-III) allows users to systematically evaluate AI output verifiability and their own epistemic position.
- **Mechanism:** Type-I decisions assess truth against real-world correspondence. Type-II decisions evaluate input-output consistency when both are Type-I-decidable. Type-III decisions assess predictability under preset rules. This hierarchy maps what the user (Jack), developer (David), and system can each know.
- **Core assumption:** Users possess sufficient domain knowledge to make Type-I decisions in their specific contexts.
- **Evidence anchors:**
  - [Section 5, Notation #5] Full specification of three decision types with decidability criteria
  - [Section 5, C6-C7] "It is Jack who has the opportunity to capture the significant value in the Я-relation, in the case that the output y is type-I-decidable for him"
  - [corpus] "Rationality Check! Benchmarking the Rationality of Large Language Models" examines LLM rationality deviations but uses different evaluation criteria.
- **Break condition:** If Type-I verification costs exceed practical limits, the decision hierarchy collapses into undecidability.

### Mechanism 3
- **Claim:** The epistemic configuration framework (ES→EM→EO) reveals that AI outputs always remain within the Я-relation domain regardless of input type.
- **Mechanism:** By mapping Epistemic Subject (human, AI, or hybrid), Epistemic Method (Φ-thinking, non-Φ-thinking, Я-thinking), and Epistemic Object (phenomena types), the framework shows that Я(Φ)=Я, Я(~Φ)=Я, Я(Я)=Я—AI processing always produces stochastic outputs.
- **Core assumption:** The categorical distinction between Φ-thinking (rule-based rationality) and non-Φ-thinking holds across contexts.
- **Evidence anchors:**
  - [Section 4] Full epistemic structure with ES1-ES4, EM1-EM3, EO1-EO3 mappings
  - [Section 4] "This indicates that, regardless of whether program Я emulates rational, non-rational, or itself reasoning, its epistemic output always remains within the Я-relation domain"
  - [corpus] No direct validation; neighboring work on human-machine interaction sociology addresses related themes without confirming this formalism.
- **Break condition:** If AI systems develop verifiable rule-following behavior, the Я-relation closure property would need revision.

## Foundational Learning

- **Concept: Stochastic vs. deterministic system behavior**
  - **Why needed here:** The entire framework depends on distinguishing systems where "given inputs A, you will always get B" from systems with inherent unpredictability.
  - **Quick check question:** Can you explain why a system that sometimes outputs 73, sometimes 37, sometimes 79 when asked to "remember 73" is fundamentally different from a database lookup?

- **Concept: Epistemic position and decidability**
  - **Why needed here:** The framework assigns different knowledge capabilities to David (researcher), Bob (engineer), and Jack (user)—understanding who can know what is essential.
  - **Quick check question:** For a medical diagnosis AI, who in the framework can make the Type-I decision about whether a diagnosis is correct?

- **Concept: Rule-based practical rationality (Φ-thinking)**
  - **Why needed here:** The paper argues this form of rationality "deserves to be valued and preserved" against erosion by AI adoption.
  - **Quick check question:** When you follow a recipe exactly versus improvising based on taste, which exemplifies Φ-thinking?

## Architecture Onboarding

- **Component map:**
  - Φ-Я Classifier: Determines if a system exhibits Φ-relations (specifiable rules) or Я-relations (hidden rules)
  - Decision-Type Evaluator: Assesses Type-I/II/III decidability for specific inputs and outputs
  - Epistemic Configuration Mapper: Identifies which ES-EM-EO combination applies to a use case
  - User Reflection Checklist: C7 questions (laugh it off? verify affordably? bear consequences?)

- **Critical path:**
  1. Classify the system (Φ or Я)
  2. Identify your epistemic position (what Type-I decisions can you make?)
  3. Determine output decidability before deployment
  4. Apply C7 reflection questions to accept/reject ambiguity

- **Design tradeoffs:**
  - High decidability use cases (Example #1: comparing 9.10 and 9.11) permit wider AI deployment
  - Low decidability use cases (Example #5: self-referential remarks) require human judgment or rejection
  - Domain expertise shifts Type-I decidability—what's decidable for a chemist may not be for a generalist

- **Failure signatures:**
  - Treating Я-relation outputs as Φ-relation (expecting deterministic correctness)
  - Deploying AI where Type-I verification is impossible and consequences are unbearable
  - Conflating David's knowledge (model internals) with Jack's knowledge (real-world verification)

- **First 3 experiments:**
  1. **Classification audit:** Take 5 systems your team uses; classify each as Program Φ or Program Я using the rule-observability criterion. Document what evidence supports each classification.
  2. **Decidability mapping:** For your primary AI use case, create a matrix: list 10 typical inputs, determine Type-I decidability for each, then assess whether you can make Type-II or Type-III decisions.
  3. **C7 reflection protocol:** For your highest-stakes AI output, explicitly answer: Can we laugh it off? Can we verify at affordable cost? Can we bear verification-failure consequences? Document the decision threshold this implies.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a rule-based autonomous artifact be meaningfully designed with the capability to understand and intervene in the Я-relation (ES3–EM1–EO3)?
- **Basis in paper:** [explicit] Section 4 asks, "Can ES3–EM1–EO3 exist meaningfully—could a rule-based autonomous artifact be designed with capability of understanding and intervening in the Я-relation itself?"
- **Why unresolved:** The authors identify this as a new epistemic configuration where the boundary between rule-based systems and stochastic AI behavior remains undefined.
- **What evidence would resolve it:** The successful engineering of a deterministic system (Program Φ) that can formally verify or correct the stochastic outputs of Program Я without human intervention.

### Open Question 2
- **Question:** What are the stable probabilistic fluctuations underlying the behavior of Program Я?
- **Basis in paper:** [explicit] Section 4 explicitly asks, "Yet, what are the stable probabilistic fluctuations underlying the behavior of Program Я?" and states, "At present, these remain unknown."
- **Why unresolved:** Unlike natural phenomena which follow known statistical laws (e.g., normal distribution), the "human-made stochastic phenomenon" of AI lacks a characterized mathematical regularity.
- **What evidence would resolve it:** Empirical identification of statistical laws or probability distributions that consistently describe the variance and temporal dependence of AI model outputs.

### Open Question 3
- **Question:** What is the nature of the cognitive interaction when human rule-based rationality (ES1) engages with AI-generated stochastic phenomena (EO3)?
- **Basis in paper:** [explicit] Section 4 lists "What would be the nature of ES1–EM1–EO3?" as one of the new epistemic configurations revealed by Program Я that requires further study.
- **Why unresolved:** The paper defines the components (human subject, rule-based method, AI object) but leaves the specific dynamics and outcomes of their interaction open.
- **What evidence would resolve it:** A theoretical model or empirical study detailing how rule-based reasoning methods (e.g., hypothesis testing) successfully process and validate AI-generated outputs.

## Limitations
- The framework relies on conceptual distinctions that lack operational definitions, making consistent application challenging.
- The distinction between Φ-relations and Я-relations is theoretically clear but practically ambiguous for real-world AI systems exhibiting both deterministic and stochastic behaviors.
- The Type-I/II/III decision framework assumes users have sufficient domain knowledge to make accurate truth assessments, which may not hold in specialized or emerging domains.

## Confidence
- **High confidence:** The distinction between rule-based software and AI systems exhibiting stochastic behavior is well-established and fundamental to understanding AI limitations.
- **Medium confidence:** The three-tier decision framework provides useful structure for thinking about AI output verification, though practical implementation may vary significantly by domain.
- **Low confidence:** The epistemic configuration formalism (ES→EM→EO) offers theoretical elegance but lacks empirical grounding or validation against real-world AI deployment scenarios.

## Next Checks
1. **Classification reliability test:** Have 5 independent evaluators apply the Φ/Я classification to 20 diverse AI behaviors (LLM outputs, computer vision predictions, robotics actions) and measure inter-rater agreement. Document boundary cases and resolution criteria.

2. **Decidability cost analysis:** For 3 representative AI use cases, measure actual costs (time, expertise, resources) required to make Type-I decisions. Compare against Type-II and Type-III decision costs to identify practical decision thresholds.

3. **Real-world framework application:** Partner with 3 organizations using AI in production to apply the full framework (classification → decidability assessment → C7 reflection) to their current deployments. Track decision outcomes and identify framework limitations in practice.