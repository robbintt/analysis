---
ver: rpa2
title: Language steering in latent space to mitigate unintended code-switching
arxiv_id: '2510.13849'
source_url: https://arxiv.org/abs/2510.13849
tags:
- uni00000013
- language
- uni00000051
- uni00000003
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces latent-space language steering, a lightweight
  inference-time method that mitigates unintended code-switching in multilingual LLMs
  by steering token embeddings along language-specific directions identified via PCA
  on parallel translations. The approach requires minimal parallel data and adds only
  a dot product and vector subtraction per token, achieving 95-99% language classification
  accuracy and reducing next-token distributional divergence by up to 42% across multiple
  language pairs on Qwen2.5 and Llama-3.2 models.
---

# Language steering in latent space to mitigate unintended code-switching

## Quick Facts
- arXiv ID: 2510.13849
- Source URL: https://arxiv.org/abs/2510.13849
- Authors: Andrey Goncharov; Nikolai Kondusov; Alexey Zaytsev
- Reference count: 8
- Primary result: 95-99% language classification accuracy with up to 42% reduction in next-token distributional divergence via lightweight inference-time steering

## Executive Summary
This paper introduces latent-space language steering, a lightweight inference-time method that mitigates unintended code-switching in multilingual LLMs by steering token embeddings along language-specific directions identified via PCA on parallel translations. The approach requires minimal parallel data and adds only a dot product and vector subtraction per token, achieving 95-99% language classification accuracy and reducing next-token distributional divergence by up to 42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. Empirical analysis reveals that language identity concentrates in final layers with near-perfect linear separability, enabling effective steering while preserving semantic coherence. The method demonstrates superior performance for typologically distant pairs and offers a practical alternative to expensive fine-tuning for production multilingual systems.

## Method Summary
The method extracts language-specific direction vectors via PCA on parallel translation embeddings, then applies inference-time steering by removing the projection of token embeddings onto these directions. Steering is applied only to final layers where language identity shows near-perfect linear separability. The approach requires 50-200 parallel samples for calibration and uses a grid-search coefficient (-1.4 to -2.9) to control steering strength per language pair.

## Key Results
- Achieves 95-99% language classification accuracy using single principal component
- Reduces next-token distributional divergence by up to 42% across language pairs
- Requires only 50-200 parallel samples and adds negligible computational overhead
- Works best for typologically distant languages (English-Chinese: 99% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
When semantic content is held constant across parallel translations, PCA on hidden states identifies language-encoding axes that are linearly separable. Parallel translations provide semantically equivalent text in different languages. When embeddings from these texts are centered and analyzed via PCA, language identity becomes the dominant variance source (since semantics is controlled), making the first principal component an interpretable "language direction" vector.

### Mechanism 2
Language identity occupies a subspace that is largely orthogonal to semantic content, enabling projection-based removal without degrading meaning. The steering operation computes h̃ = h − s(h·v)v, removing the projection onto the language direction v while preserving components orthogonal to it. This preserves semantic content because the model organizes identical content into tight language-specific clusters with geometrically precise boundaries.

### Mechanism 3
Language identity concentrates in final transformer layers with near-perfect linear separability, enabling targeted steering. Layer-wise analysis reveals that language clusters emerge loosely in early layers and sharpen dramatically in final layers. Explained variance analysis shows late-stage specialization for language identity, allowing steering to be applied only to layers ≥ ℓ_crit.

## Foundational Learning

- **Principal Component Analysis (PCA) for Direction Extraction**
  - Why needed here: PCA identifies the dominant axes of variance in embedding space; when semantics is controlled, these axes correspond to language identity.
  - Quick check question: If you run PCA on embeddings from parallel English-Chinese translations, what does the first principal component most likely encode?

- **Vector Projection and Orthogonal Subspaces**
  - Why needed here: Steering requires computing the projection of a hidden state onto a direction vector and removing it while preserving orthogonal components.
  - Quick check question: Given hidden state h ∈ R^d and unit direction v, what is the formula for the component of h along v versus orthogonal to v?

- **Code-Switching in Multilingual LLMs**
  - Why needed here: Understanding the failure mode being addressed—models generating tokens in unintended languages despite monolingual prompts.
  - Quick check question: Why might a multilingual model trained on imbalanced language data spontaneously switch languages during generation?

## Architecture Onboarding

- **Component map:**
  Parallel Corpus (50-200 samples) -> LLM Forward Pass → Hidden States h_t^(l) per layer -> PCA per Layer → Language Direction Vectors v^(l) -> Inference-Time Steering: h̃_t^(l) = h_t^(l) − s(h_t^(l)·v^(l))v^(l) -> Steered Logits → Reduced Code-Switching

- **Critical path:**
  1. Collect 50+ parallel translation pairs (Flores Plus or equivalent)
  2. Extract hidden states from each layer via forward pass
  3. Fit PCA per layer on merged embeddings; extract first PC as language direction
  4. Identify ℓ_crit (final layers with sharp clustering)
  5. Grid search steering strength s per language pair (paper uses -1.4 to -2.9)
  6. Apply steering only to layers ≥ ℓ_crit during inference

- **Design tradeoffs:**
  - Steering strength (s): Larger magnitude → stronger language shift but potential fluency degradation; requires per-pair calibration
  - Layer selection: Steering only final layers minimizes semantic disruption but may miss language signal in mid-layers
  - Parallel data size: Paper uses 50-200 samples; minimal viable calibration set not systematically determined
  - Language pair distance: Typologically distant pairs (En-Zh) show stronger results than similar pairs (En-Es)

- **Failure signatures:**
  - Hindi showed 0 improvement: Likely tokenization/data imbalance issues in base model
  - English-Spanish lower accuracy (0.95): Similar languages have less separable language directions
  - Average 20% KL reduction: Linear projections cannot fully reconstruct monolingual distributions; non-linear interactions remain
  - Incomplete semantic preservation: KL divergence measures distributional similarity, not semantic fidelity directly

- **First 3 experiments:**
  1. Visualize language clustering: Extract final-layer embeddings from parallel translations, project to 2D via PCA, color by language; expect tight, separable clusters
  2. Validate linear separability: Train logistic regression on single PC1 dimension to predict language; target 95%+ accuracy as sanity check
  3. Steering strength sweep: On artificial code-switched test set (half-translated TED talks), grid search s ∈ [-3, 0], measure KL divergence reduction and inspect top-token shifts

## Open Questions the Paper Calls Out

- **Does the linear separability of language identity persist in significantly larger models (e.g., 70B+ parameters), or does the representation geometry become more complex?**
  - Experiments limited to small models (1-1.5B parameters) and two architectures.

- **To what extent does latent steering degrade performance on downstream semantic tasks despite reducing distributional divergence?**
  - Primary reliance on KL divergence... not semantic preservation, fluency, or downstream task performance.

- **Can the method generalize to naturally occurring code-switching in conversational domains, or is it restricted to the artificial "hard-switch" scenarios tested?**
  - Artificial code-switching setup may not reflect real-world scenarios... Conversational data... are not tested.

## Limitations

- Linear projection approach cannot fully reconstruct monolingual distributions, achieving only 20% average KL reduction
- Method fails for Hindi (0% improvement), suggesting tokenization or data imbalance issues in base models
- Current evaluation uses artificial code-switching rather than real-world conversational data
- Requires per-language-pair calibration of steering strength via grid search

## Confidence

- **High Confidence (★★★):** PCA-based direction extraction works as described; steering formula correctly implements projection removal
- **Medium Confidence (★★☆):** Layer-wise language concentration and steering effectiveness generalize across tested models and language pairs
- **Low Confidence (★★★):** Generalization to all multilingual LLMs, particularly larger models, non-Latin scripts, or typologically similar languages

## Next Checks

1. Apply steered models to multilingual benchmarks (XNLI, PAWS-X) to verify semantic preservation claims beyond distributional KL divergence
2. Validate layer-wise language concentration pattern on larger models (7B+ parameters) and different architectures (Mistral, Gemma)
3. Test steering intervention on datasets containing natural, spontaneous code-switching rather than constructed parallel sentences