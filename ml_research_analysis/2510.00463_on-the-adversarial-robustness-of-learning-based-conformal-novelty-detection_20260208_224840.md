---
ver: rpa2
title: On the Adversarial Robustness of Learning-based Conformal Novelty Detection
arxiv_id: '2510.00463'
source_url: https://arxiv.org/abs/2510.00463
tags:
- attack
- data
- adversarial
- surrogate
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work examines the adversarial robustness of conformal novelty\
  \ detection methods that offer FDR control. We focus on two frameworks\u2014AdaDetect\
  \ and a one-class classifier approach\u2014and formulate an oracle attack that quantifies\
  \ worst-case FDR degradation, deriving an upper bound for such attacks."
---

# On the Adversarial Robustness of Learning-based Conformal Novelty Detection

## Quick Facts
- arXiv ID: 2510.00463
- Source URL: https://arxiv.org/abs/2510.00463
- Reference count: 40
- One-line primary result: Adversarial perturbations can significantly increase FDR in conformal novelty detection while maintaining high detection power, exposing fundamental limitations in current error-controlled methods.

## Executive Summary
This work examines the adversarial robustness of conformal novelty detection methods that offer False Discovery Rate (FDR) control. The authors focus on two frameworks—AdaDetect and a one-class classifier approach—and formulate an oracle attack that quantifies worst-case FDR degradation, deriving an upper bound for such attacks. This analysis motivates a practical surrogate decision-based attack scheme that only requires querying output labels. Coupling these with two black-box adversarial algorithms (HopSkipJump and Boundary Attack), the authors systematically evaluate both methods on synthetic and real-world datasets. Results show that adversarial perturbations can significantly increase FDR while maintaining high detection power, exposing fundamental limitations in current error-controlled novelty detection and highlighting the need for more robust designs.

## Method Summary
The paper evaluates adversarial robustness of two FDR-controlled novelty detection frameworks (AdaDetect and Bates et al.) under black-box decision-based attacks. The method involves implementing both detection pipelines with PU-learning score functions (AdaDetect) or one-class classifiers (Bates et al.), followed by conformal p-value computation and Benjamini-Hochberg (BH) procedure. Two attack strategies are evaluated: an oracle attack (requiring full knowledge of labels and model) and a surrogate attack (query-based pseudo-labels). The surrogate attack trains a separate model using binary decisions from the target system, then applies decision-based black-box attacks (HopSkipJump or Boundary Attack) to generate minimal perturbations that inflate FDR. Experiments run on synthetic data (Independent/Exchangeable Gaussian, Non-Gaussian; d=20) and four real-world datasets with varying attack configurations.

## Key Results
- Adversarial perturbations can significantly increase FDR while maintaining high detection power across multiple datasets
- The oracle attack provides an upper bound on worst-case FDR degradation, while the surrogate attack offers practical black-box vulnerability assessment
- Model architecture mismatch (e.g., RF-NN) still enables effective attacks, demonstrating transferability of adversarial examples
- Low initial detection power in victim systems degrades surrogate attack effectiveness due to unreliable pseudo-labels

## Why This Works (Mechanism)

### Mechanism 1
If an attacker has query access to the system's binary decisions, they can train a surrogate model to approximate the decision boundary and generate adversarial perturbations that inflate the False Discovery Rate (FDR). The attacker queries the detector to obtain "pseudo-labels" (rejected vs. not rejected) for the test set, trains a separate surrogate classifier, and applies decision-based attacks to flip classification of true null samples to outliers, thereby increasing false discoveries. Core assumption: The initial detector has reasonable power, ensuring pseudo-labels are accurate enough for the surrogate to learn a useful approximation. Break condition: If initial detector power is very low (< 0.5), pseudo-labels become unreliable, causing the surrogate to fail.

### Mechanism 2
If adversarial perturbations are applied to a fixed subset of test data, the FDR inflation is theoretically bounded by the sum of the target level α and a term proportional to the number of attacked samples. The analysis proves that unattacked nulls maintain conditional exchangeability even after the attack, ensuring their contribution to FDR remains below α. Total FDR inflation is therefore capped by the "statistical cost" of the attacked subset. Core assumption: The attack function must be order-invariant with respect to unattacked calibration samples. Break condition: If the attack violates order-invariance or the exchangeability assumption is not met, the theoretical bound dissolves.

### Mechanism 3
Adversarial perturbations generated for one model architecture (e.g., Neural Network) can successfully attack a target system using a different architecture (e.g., Random Forest). This relies on transferability of adversarial examples, where decision boundaries learned by both models for the novelty detection task are often sufficiently aligned. Perturbations that cross the surrogate's boundary will likely cross the target's boundary. Core assumption: The surrogate and target models have learned correlated feature representations or decision boundaries. Break condition: If model architectures are fundamentally incompatible or decision boundaries are orthogonal, transferability fails.

## Foundational Learning

**Concept: False Discovery Rate (FDR)**
Why needed here: This is the specific statistical metric the paper aims to break. Unlike standard accuracy, FDR measures the proportion of false positives among all rejections, critical in high-stakes testing.
Quick check question: How does FDR differ from the False Positive Rate (FPR), and why is controlling FDR at level α (e.g., 0.1) the guarantee provided by AdaDetect?

**Concept: Conformal Prediction & P-values**
Why needed here: The defense mechanisms (AdaDetect, Bates et al.) rely on transforming raw scores into conformal p-values using a calibration set. The attack indirectly manipulates these p-values by changing the raw input data.
Quick check question: Explain how a conformal p-value is calculated using a calibration set and why exchangeability is required for its validity?

**Concept: Decision-based Black-box Attacks**
Why needed here: The proposed "Surrogate Attack" operates in a black-box setting, relying only on hard labels (rejected/not rejected) rather than gradients or confidence scores.
Quick check question: Why are decision-based attacks (like HopSkipJump) considered more realistic for security scenarios than white-box gradient attacks (like FGSM)?

## Architecture Onboarding

**Component map:**
Training Data → Score Function (RF/NN) → Calibration → Conformal P-values → BH Procedure → Decisions (Victim System)
Test Data → Query Victim → Pseudo-labels → Surrogate Model → HSJA/Boundary Attack → Perturbed Data (Attack Pipeline)

**Critical path:** The interaction point is the Query. The attacker sends test data; the victim returns binary decisions. The attacker uses these decisions to estimate the boundary and shift data points (specifically true nulls near the boundary) into the rejection region.

**Design tradeoffs:**
- Oracle vs. Surrogate: Oracle attack gives worst-case upper bound but is unrealistic; surrogate attack is practical but depends on victim's initial detection power
- Model Mismatch: Using NN as surrogate for RF victim is faster but may lose attack precision compared to matched architectures

**Failure signatures:**
- Low Power Degradation: If dataset is difficult (e.g., Mammography), victim's low power results in poor pseudo-labels, causing surrogate attack to fail
- Bound Saturation: FDR inflation cannot exceed theoretical upper bound derived in Theorem 1

**First 3 experiments:**
1. Baseline Verification (RF-RF): Replicate "Surrogate + HopSkipJump" attack on Credit Card dataset using RF for both victim and surrogate
2. Architectural Transfer (RF-NN): Test attack where victim uses RF but attacker trains NN surrogate to verify black-box capability
3. Attack Intensity Scaling: Vary attack size mₐ (e.g., 50 to 200 samples) to observe linear relationship between perturbed samples and FDR inflation

## Open Questions the Paper Calls Out

**Open Question 1**
How can defense mechanisms like adversarial training or randomized smoothing be integrated into conformal novelty detection while preserving finite-sample FDR guarantees? The paper states exploring such defenses offers a promising direction for integrating adversarial robustness with principled error control, but does not propose or test any defense strategies.

**Open Question 2**
What is the impact on FDR control when the adversary targets the training or calibration data (null samples) rather than the test data? The paper proposes studying attacks on null samples including training and calibration data, but current analysis relies on the assumption that training data is highly secure and intact.

**Open Question 3**
Can the label-flipping strategy for the surrogate score function be theoretically optimized rather than relying on the heuristic 75% threshold? The paper acknowledges the optimal threshold may vary across datasets but does not provide a formal method to determine the optimal flipping probability for maximizing attack success.

## Limitations
- Theoretical FDR bound relies critically on exchangeability assumption and order-invariance that may not hold in practice for complex data distributions
- Surrogate attack's effectiveness depends heavily on victim's initial detection power, with low-power datasets potentially breaking the attack mechanism
- Hyperparameter sensitivity remains a concern, as performance could vary significantly with different model architectures, learning rates, or attack parameters not fully specified

## Confidence

**High Confidence**: Oracle attack framework and theoretical FDR upper bound (Theorem 1) are well-supported by mathematical proofs and clear mechanism descriptions. Demonstration that adversarial perturbations can significantly increase FDR while maintaining high detection power is robustly evidenced.

**Medium Confidence**: Surrogate attack's practical effectiveness shows variability across datasets, with success highly dependent on initial detection power. Transferability claims between different model architectures are demonstrated but may not generalize to all domain-specific scenarios.

**Low Confidence**: Exact implementation details for hyperparameter tuning and attack parameters introduce uncertainty in reproducibility. Generalizability of results to datasets with different characteristics (class imbalance, feature distributions) requires further validation.

## Next Checks

1. **Power Threshold Analysis**: Systematically vary the victim detector's power (via model architecture or training data size) to identify the minimum power threshold below which the surrogate attack consistently fails, confirming the mechanism's dependency.

2. **Transferability Stress Test**: Evaluate the surrogate attack across a wider range of model architecture pairs (e.g., SVM-NN, Linear Regression-RF) and feature space dimensionalities to quantify transferability limits and identify conditions where the attack degrades to random noise.

3. **Bound Validation Under Stress**: Design synthetic datasets where the exchangeability assumption is deliberately violated (e.g., temporal correlation, class imbalance) to test whether the theoretical FDR upper bound still provides meaningful guarantees or breaks down in realistic scenarios.