---
ver: rpa2
title: Local Regularizers Are Not Transductive Learners
arxiv_id: '2502.07187'
source_url: https://arxiv.org/abs/2502.07187
tags:
- learning
- local
- xtest
- which
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper resolves an open question from Asilis et al. (2024a)
  about the sufficiency of local regularization for multiclass learning.
---

# Local Regularizers Are Not Transductive Learners

## Quick Facts
- arXiv ID: 2502.07187
- Source URL: https://arxiv.org/abs/2502.07187
- Authors: Sky Jafar; Julian Asilis; Shaddin Dughmi
- Reference count: 10
- Key outcome: Local regularizers cannot learn certain hypothesis classes even when global regularizers can, resolving an open question from Asilis et al. (2024a)

## Executive Summary
This paper resolves an open question about whether local regularization is sufficient for multiclass learning. The authors construct a hypothesis class (Hotp) that is learnable via global regularization but provably unlearnable by any local regularizer in the transductive model. Hotp uses a cryptographic secret-sharing construction based on the one-time pad, where each hypothesis reveals only one "share" of information at each point, making local preferences fundamentally inconsistent across related learning tasks.

## Method Summary
The authors prove their impossibility result through a coupling argument. They first establish that Hotp is learnable via the Generalized Binary with Distinct Label Sets (GBDLS) property, which ensures learnability through global regularization. They then show that any local regularizer attempting to learn Hotp must fail due to a preference cycle over four carefully constructed learning instances. The cryptographic structure of Hotp ensures that observing one label reveals negligible information about other possible labels, preventing local regularizers from making consistent predictions across related tasks.

## Key Results
- Proved that Hotp is learnable via global regularization (satisfies GBDLS property with DS dimension ≤ 2)
- Demonstrated that no local regularizer can learn Hotp in the transductive model
- Constructed a preference cycle showing that any local regularizer must make contradictory ordering decisions across four coupled learning instances
- Left open the question of whether this separation extends to the PAC model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hypothesis class $H_{otp}$ remains learnable despite its cryptographic structure because it satisfies the GBDLS (Generalized Binary with Distinct Label Sets) property.
- Mechanism: Each hypothesis $h_{A,B}$ outputs exactly two labels $\{(0,A), (1,B)\}$, and distinct hypotheses have distinct label sets. This limits DS dimension to ≤2, ensuring learnability.
- Core assumption: The DS dimension characterization of multiclass learnability (Brukhim et al., 2022) holds.
- Evidence anchors:
  - [section] Definition 4.1 and Lemma 4.3 establish that $H_{otp}$ is GBDLS
  - [section] Proposition 3.5 proves GBDLS classes are learnable
  - [corpus] Related work "On the Computability of Multiclass PAC Learning" discusses multiclass learnability characterizations
- Break condition: If a hypothesis class allowed >2 labels per hypothesis OR had overlapping label sets between hypotheses, the GBDLS guarantee would fail.

### Mechanism 2
- Claim: Any local regularizer attempting to learn $H_{otp}$ must fail due to a preference cycle over four coupled learning instances.
- Mechanism: The proof constructs four transductive instances $(S_i, h_i^*)$ where success on each requires: $\psi(h_1^*, x_{test}) < \psi(h_2^*, x_{test}) < \psi(h_3^*, x_{test}) < \psi(h_4^*, x_{test}) < \psi(h_1^*, x_{test})$ — an impossible cycle.
- Core assumption: The local regularizer is locally injective (can be assumed WLOG for countable classes per Lemma 4.5).
- Evidence anchors:
  - [section] Theorem 4.6 proof and Lemma 4.7 establish the cycle argument
  - [section] Figure 1 depicts the four learning problems and resulting ordering constraints
  - [corpus] No direct corpus support for this specific cryptographic construction; mechanism is novel to this paper
- Break condition: If the regularizer could access information beyond local hypothesis-test-point pairs (e.g., unlabeled data statistics), the cycle might be avoided.

### Mechanism 3
- Claim: The one-time pad structure ensures that observing one "share" (label) reveals negligible information about the other share, preventing local regularizers from generalizing.
- Mechanism: Each $h_{A,B}$ reveals either $A$ or $B$ per point (via XOR mask $A \oplus B$). Training points showing $(0,A)$ provide no information about $B$'s identity, so local preferences at test points cannot be consistently informed.
- Core assumption: The domain points are uniformly distributed across the XOR mask positions for the probabilistic argument.
- Evidence anchors:
  - [section] Definition 4.1 constructs $h_{A,B}$ using XOR: output is $(0,A)$ when $(A \oplus B)(x) = 0$, else $(1,B)$
  - [abstract] Explicitly states the construction uses "cryptographic secret-sharing"
  - [corpus] "Private List Learnability" explores privacy-learning connections but doesn't address this specific construction
- Break condition: If training sets contained both label types revealing both $A$ and $B$, the ground truth would be uniquely identified, bypassing the regularizer's role.

## Foundational Learning

- Concept: **Transductive Learning Model**
  - Why needed here: The paper's main impossibility result is proven in the transductive setting, where the learner sees all unlabeled points upfront and predicts one held-out point.
  - Quick check question: Given unlabeled points $\{x_1, \ldots, x_n\}$ and labels for all but $x_j$, can you explain why error is averaged over the random choice of $j$?

- Concept: **Local vs Global Regularization**
  - Why needed here: Local regularizers $\psi(h, x)$ take the test point as input, enabling improper learning. This flexibility is both their power and (per this paper) their limitation.
  - Quick check question: How does $\psi(h, x)$ differ from classical regularization $\psi(h)$, and why does locality enable "stitching together" hypotheses?

- Concept: **One-Time Pad / Secret Sharing**
  - Why needed here: The hypothesis class $H_{otp}$ is constructed so each hypothesis encodes two "shares" — seeing one share reveals nothing about the other.
  - Quick check question: Given secret $C$ and random pad $A$, why does observing only $A$ or only $A \oplus C$ reveal no information about $C$?

## Architecture Onboarding

- Component map:
  - **Hypothesis class $H_{otp}$**: Parameterized by pairs $(A, B)$ of equal-length binary strings with balanced XOR. Defined in Section 4, Definition 4.1.
  - **Local regularizer $\psi$**: Function $H \times X \rightarrow \mathbb{R}_{\geq 0}$ inducing a learner via $\arg\min_{h: L_S(h)=0} \psi(h, x)$.
  - **Coupling construction**: Four instances $(S_i, h_i^*)$ related by bit-flips at strategically chosen positions ($x_{test}$, $x_{fool}$).

- Critical path: Understanding why the GBDLS property guarantees learnability → Grasping the one-time pad information-hiding structure → Following the coupling argument that produces the preference cycle.

- Design tradeoffs: The paper trades off generality for proof tractability — the result is proven for transductive learning, not PAC. Extending to PAC requires overcoming version space incomparability and error measurement differences (Section 5).

- Failure signatures:
  - A local regularizer that appears to work on $H_{otp}$ likely has non-zero probability of producing the preference cycle — check for inconsistent orderings across coupled instances.
  - If your regularizer accesses unlabeled data statistics beyond local $(h, x)$ pairs, it may bypass the impossibility (this would be a UL-SRM, not a pure local regularizer).

- First 3 experiments:
  1. **Implement $H_{otp}$ for small $d$** (e.g., $d=4$): Enumerate all valid $(A, B)$ pairs, verify GBDLS properties, and confirm each hypothesis has exactly two distinct labels.
  2. **Simulate the coupling argument**: For fixed $d$, sample random $(C, A, m_0, m_1)$, construct the four instances, and verify that any fixed preference ordering fails on at least one instance.
  3. **Test a candidate local regularizer**: Design a simple $\psi$ (e.g., based on string length or lexicographic order) and measure its transductive error on $H_{otp}$ — expect $\geq 1/4$ error rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the hypothesis class Hotp be shown to be unlearnable by local regularizers in the PAC model, as conjectured?
- Basis in paper: [explicit] "We conjecture therefore that our same hypothesis class is in fact not learnable by any local regularizer in the PAC model."
- Why unresolved: The proof techniques for the transductive setting do not transfer to PAC because PAC learning allows multiple zero-error hypotheses that are correct at the test point, version spaces are typically incomparable across training sets, and PAC error is averaged over the distribution rather than focused on a single test point.
- What evidence would resolve it: Either a proof extending the coupling argument to the PAC setting, or a construction of a local regularizer that successfully learns Hotp in the PAC model.

### Open Question 2
- Question: Does there exist a separation between transductive and PAC learnability with respect to local regularization?
- Basis in paper: [explicit] "leaving open the tantalizing possibility of a PAC/transductive separation with respect to local regularization."
- Why unresolved: The authors' conjecture that Hotp is also unlearnable by local regularizers in PAC is unproven; if false, this would constitute a PAC/transductive separation, but this possibility remains unexplored.
- What evidence would resolve it: Either showing Hotp is PAC-learnable by some local regularizer (establishing separation), or proving no such separation exists for any learnable class.

### Open Question 3
- Question: What is the optimal sample complexity of learning Hotp, both in the transductive and PAC models?
- Basis in paper: [inferred] The paper proves learnability of Hotp but does not analyze its sample complexity. Understanding this could inform whether local regularizers are inherently sample-inefficient even when they succeed.
- Why unresolved: The proof of learnability uses the DS dimension argument (DS(H) ≤ 2) but does not compute exact sample complexities or compare them to what local regularizers might achieve if they could learn the class.
- What evidence would resolve it: Tight upper and lower bounds on the transductive and PAC sample complexities of Hotp.

## Limitations

- The main result is proven only in the transductive setting, not for the more general PAC model.
- The cryptographic construction relies on uniform distribution assumptions about the XOR masks, which may not hold in all practical scenarios.
- The paper assumes countable hypothesis classes and locally injective regularizers, which may not capture all local regularizers of interest.

## Confidence

- Confidence in the transductive impossibility result: **High** - The proof is rigorous and well-structured
- Confidence in the PAC extension: **Medium** - The authors acknowledge substantial technical hurdles without providing a complete argument
- Confidence in the GBDLS characterization of learnability: **High** - Follows established results from Brukhim et al. (2022)

## Next Checks

1. Verify the transductive impossibility result by implementing $H_{otp}$ and attempting to construct a local regularizer that succeeds across all four coupled instances.
2. Investigate whether any natural local regularizer (e.g., based on hypothesis complexity or data-dependent features) can avoid the preference cycle identified in the proof.
3. Explore whether the GBDLS characterization of learnability can be extended to handle hypothesis classes where the XOR mask distribution is non-uniform.