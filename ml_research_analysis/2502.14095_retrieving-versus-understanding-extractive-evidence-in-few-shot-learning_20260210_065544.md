---
ver: rpa2
title: Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning
arxiv_id: '2502.14095'
source_url: https://arxiv.org/abs/2502.14095
tags:
- evidence
- label
- prediction
- given
- then
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reliability of large language models
  (LMs) in identifying within-document evidence to support their predictions in a
  few-shot learning setting. The authors analyze the correlation between label prediction
  errors and evidence retrieval errors using five datasets with gold-standard human-annotated
  evidence, focusing on two proprietary models (GPT-4 and Gemini-1.5).
---

# Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning

## Quick Facts
- arXiv ID: 2502.14095
- Source URL: https://arxiv.org/abs/2502.14095
- Authors: Karl Elbakian; Samuel Carton
- Reference count: 7
- Primary result: LMs can mostly reliably quote evidence from input documents, with prediction errors more commonly from misinterpreting relevant evidence rather than missing key evidence entirely

## Executive Summary
This paper investigates the reliability of large language models in identifying within-document evidence to support their predictions in few-shot learning settings. The authors analyze the correlation between label prediction errors and evidence retrieval errors using five datasets with gold-standard human-annotated evidence, focusing on two proprietary models (GPT-4 and Gemini-1.5). They find that LMs can mostly reliably quote evidence from input documents, and that prediction errors are more commonly associated with misinterpreting relevant evidence rather than missing key evidence entirely. Evidence retrieval errors are mostly not associated with evidence interpretation errors, which is a positive sign for downstream verification applications.

## Method Summary
The authors conducted empirical analysis on five datasets with gold-standard human-annotated evidence to evaluate large language models' ability to retrieve and interpret extractive evidence in few-shot learning settings. They focused on two proprietary models (GPT-4 and Gemini-1.5) and measured the correlation between prediction errors and evidence retrieval errors. The analysis distinguished between evidence retrieval failures (missing key evidence) and interpretation failures (misunderstanding retrieved evidence), examining how these error types relate to overall prediction accuracy.

## Key Results
- LMs can mostly reliably quote evidence from input documents when provided with gold-standard human-annotated evidence
- Prediction errors are more commonly associated with misinterpreting relevant evidence rather than missing key evidence entirely
- Evidence retrieval errors are mostly not associated with evidence interpretation errors, indicating potential for downstream verification applications

## Why This Works (Mechanism)
The mechanism behind this phenomenon relates to the inherent capabilities of large language models in processing and synthesizing information from input documents. When provided with clear, relevant evidence, these models demonstrate reliable extractive capabilities, suggesting that their architecture supports effective information retrieval from source material. The distinction between retrieval and interpretation errors indicates that the models possess robust document processing mechanisms, but may struggle with the higher-level reasoning required to correctly interpret the meaning and implications of the evidence they successfully retrieve.

## Foundational Learning

**Evidence Retrieval**: The ability to identify and extract relevant passages from source documents
- Why needed: Forms the foundation for model-based verification and fact-checking systems
- Quick check: Can the model correctly identify relevant passages when explicitly asked?

**Evidence Interpretation**: The capacity to understand and correctly reason about retrieved evidence
- Why needed: Critical for transforming raw information into accurate predictions
- Quick check: Does the model make correct predictions when provided with relevant evidence?

**Error Correlation Analysis**: Methodology for distinguishing between different types of model failures
- Why needed: Enables targeted improvements by identifying whether failures stem from retrieval or interpretation
- Quick check: Can we categorize errors into distinct types that map to different failure modes?

## Architecture Onboarding

**Component Map**: Input Document -> Evidence Retrieval Module -> Evidence Interpretation Module -> Prediction Output

**Critical Path**: The evidence retrieval and interpretation pipeline represents the critical path for model performance, where failures at any stage directly impact prediction accuracy.

**Design Tradeoffs**: The models balance between comprehensive evidence retrieval (which may introduce noise) and focused interpretation (which may miss relevant context). This tradeoff affects both retrieval precision and interpretation accuracy.

**Failure Signatures**: Evidence retrieval errors manifest as missing relevant passages, while interpretation errors appear as incorrect conclusions drawn from correctly retrieved evidence. The study finds these failure modes are largely independent.

**First Experiments**:
1. Test whether the same evidence retrieval reliability holds for open-source models
2. Evaluate model performance on abstractive evidence synthesis tasks
3. Apply the analysis framework to datasets from different domains

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis is constrained to two proprietary models (GPT-4 and Gemini-1.5), limiting generalizability
- Focus on extractive evidence retrieval may not extend to abstractive evidence synthesis tasks
- Proprietary nature of models prevents full transparency regarding architecture and training specifics

## Confidence

**High Confidence**: LMs can mostly reliably quote evidence from input documents when provided with gold-standard human-annotated evidence

**Medium Confidence**: Prediction errors are more commonly associated with misinterpreting relevant evidence rather than missing key evidence entirely

**Medium Confidence**: Evidence retrieval errors are mostly not associated with evidence interpretation errors

## Next Checks

1. Replicate findings with open-source models like LLaMA, Mistral, or Claude to assess generalizability beyond proprietary systems

2. Expand to abstractive evidence tasks to evaluate whether interpretation errors dominate over retrieval errors in tasks requiring synthesis rather than extraction

3. Apply the same analytical framework to datasets from different domains (scientific literature, legal documents, medical records) to determine if observed relationships are domain-specific or universal