---
ver: rpa2
title: 'Leveraging What''s Overfixed: Post-Correction via LLM Grammatical Error Overcorrection'
arxiv_id: '2509.20811'
source_url: https://arxiv.org/abs/2509.20811
tags:
- recall
- correction
- precision
- error
- overcorrection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the low recall issue in robust supervised
  fine-tuned small Language Models (sLMs) for grammatical error correction (GEC) by
  leveraging the overcorrection tendency of Large Language Models (LLMs). The proposed
  method, Post-Correction via Overcorrection (PoCO), uses a two-step approach: first,
  it intentionally triggers overcorrection using an LLM to maximize recall by allowing
  comprehensive revisions, then applies a targeted post-correction step via fine-tuning
  smaller models to identify and refine erroneous outputs.'
---

# Leveraging What's Overfixed: Post-Correction via LLM Grammatical Error Overcorrection

## Quick Facts
- arXiv ID: 2509.20811
- Source URL: https://arxiv.org/abs/2509.20811
- Authors: Taehee Park; Heejin Do; Gary Geunbae Lee
- Reference count: 17
- Primary result: Achieves highest recall among robust fine-tuned single models on BEA-19 and CoNLL-14 while maintaining competitive precision

## Executive Summary
This paper addresses the persistent low recall problem in robust supervised fine-tuned small Language Models (sLMs) for grammatical error correction (GEC). The authors propose Post-Correction via Overcorrection (PoCO), a two-stage method that leverages the overcorrection tendency of Large Language Models (LLMs) to first maximize recall through comprehensive revisions, then applies targeted post-correction via fine-tuning smaller models to identify and refine erroneous outputs. PoCO achieves the highest recall among robust fine-tuned single models on standard benchmarks while maintaining competitive precision.

## Method Summary
PoCO uses a two-stage approach: first, it intentionally triggers overcorrection using an LLM (GPT-3.5-CoT) by prompting with "find as many errors as you can" to maximize recall. This generates overcorrected outputs that contain more true corrections but also more false positives. Second, it applies a targeted post-correction step via fine-tuning smaller models (T5-large) to identify and refine erroneous outputs. The post-corrector is trained on a recovered target constructed by selectively incorporating correct edits from gold annotations within LLM-modified regions while reverting incorrect modifications back to the source. This double-target training (gold + recovered) teaches the model to both restore overcorrections and fix errors the LLM missed.

## Key Results
- PoCO achieves highest recall among robust fine-tuned single models on BEA-19 (F0.5: 75.3) and CoNLL-14 (F0.5: 65.1)
- Outperforms existing quality estimation-based filtering techniques
- Superior performance when integrated into ensemble systems
- PoCO-Mix (joint gold and recovered target training) consistently achieves best F0.5 scores

## Why This Works (Mechanism)

### Mechanism 1
Intentional overcorrection prompting increases recall by expanding the candidate correction space. The LLM is prompted with "find as many errors as you can" rather than conservative correction instructions, allowing broader modifications that capture errors missed by conservative sLMs.

### Mechanism 2
Recovered target training teaches the post-corrector to reverse invalid edits while preserving valid ones. The recovered target is constructed by taking only the correct portions of LLM output (validated against gold) and reverting incorrect modifications back to source.

### Mechanism 3
Double-target training (gold + recovered) balances recall and precision by teaching complementary objectives. Gold targets teach the model to restore overcorrections and fix errors the LLM missed, while recovered targets focus primarily on restoration.

## Foundational Learning

- **Concept: Precision-Recall Trade-off in GEC** - Understanding why sLMs achieve high P/low R and LLMs achieve low P/high R is essential to grasp why the two-stage design is necessary. Quick check: If you optimized only for F1 instead of F0.5, how would the optimal balance shift?

- **Concept: Encoder-Decoder Input Formatting for Multi-Source Inputs** - PoCO concatenates source + overcorrected output as encoder input. Understanding how T5 handles multi-part inputs determines whether you can modify this format. Quick check: What separator tokens would you use if adding a third input (e.g., error type hints)?

- **Concept: Reference-Free Evaluation with LLM Judges** - The paper supplements Errant/M2 scores with GPT-4.1 judging. Understanding when LLM evaluators disagree with n-gram metrics helps interpret conflicting results. Quick check: If GPT-4.1 rates fluency high but F0.5 is low, what does this suggest about the error distribution?

## Architecture Onboarding

- **Component map:** Source text → LLM overcorrection (GPT-3.5-CoT) → T5-large post-correction → Refined output
- **Critical path:**
  1. Generate overcorrections for training set (W&I+LOCNESS only)
  2. Construct recovered targets by aligning LLM output to gold, reverting incorrect regions
  3. Fine-tune T5 with double-target strategy
  4. At inference, run LLM overcorrection → T5 post-correction

- **Design tradeoffs:**
  - PoCO-Seq vs PoCO-Mix: Seq gives slightly higher precision; Mix gives slightly higher recall
  - LLM choice: GPT-3.5 preferred over GPT-4 for recall
  - Model size: T5-large preserves recall better than T5-base

- **Failure signatures:**
  - Recall drops to sLM baseline: Overcorrection prompt may not be triggering
  - Precision remains low: Recovered target construction may be incorrect
  - Meaning drift: LLM may be rewriting content beyond grammar

- **First 3 experiments:**
  1. Baseline sanity check: Run T5-large without PoCO on BEA-19 dev
  2. Overcorrection validation: Apply modified prompt to 50 samples, inspect edit rate
  3. Recovered target ablation: Train gold-only, recovered-only, and mixed models

## Open Questions the Paper Calls Out

### Open Question 1
Can advanced prompting strategies be developed to enable LLMs to autonomously correct overcorrection errors, removing the need for a separate fine-tuned sLM post-corrector? The authors note current LLMs fail at post-correction despite explicit error tags.

### Open Question 2
How can the "Triggering Overcorrection" prompt be systematically optimized to balance maximizing recall with minimizing semantic distortion? The current heuristic baseline may introduce noise that the post-corrector must filter out.

### Open Question 3
Is the "Recovered Target" training strategy effective for languages with rich morphology or non-Latin scripts where token alignment is more complex? The study evaluates exclusively on English datasets.

## Limitations

- Recovered target construction mechanism is underspecified with missing algorithmic details for alignment
- Double-target training schedule (mixing ratios, transition epochs) not explicitly defined
- Limited cross-domain validation - experiments confined to BEA-19 and CoNLL-14 datasets

## Confidence

- **High Confidence**: Core two-stage architecture, precision-recall trade-off mechanism, double-target training approach
- **Medium Confidence**: Recovered target construction method, superiority over quality estimation filtering
- **Low Confidence**: Claims about generalization across domains, assumption about semantic recoverability

## Next Checks

1. Implement recovered target construction algorithm on 100 BEA-19 dev samples and manually verify reverted spans contain invalid edits
2. Apply PoCO to 50 samples from domain-shifted corpus and measure semantic similarity between source and final output
3. Systematically vary F0.5 beta parameter (0.3, 0.5, 0.7) on BEA-19 dev to test relative advantage changes