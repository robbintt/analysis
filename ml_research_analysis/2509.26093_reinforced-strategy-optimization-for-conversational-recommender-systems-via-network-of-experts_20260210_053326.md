---
ver: rpa2
title: Reinforced Strategy Optimization for Conversational Recommender Systems via
  Network-of-Experts
arxiv_id: '2509.26093'
source_url: https://arxiv.org/abs/2509.26093
tags:
- strategy
- user
- conversational
- macro-level
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing conversational
  strategies in conversational recommender systems (CRSs) to improve user satisfaction
  and recommendation success. Existing LLM-based CRS methods rely on predefined prompts
  and lack explicit strategy optimization, leading to suboptimal interactions.
---

# Reinforced Strategy Optimization for Conversational Recommender Systems via Network-of-Experts

## Quick Facts
- arXiv ID: 2509.26093
- Source URL: https://arxiv.org/abs/2509.26093
- Reference count: 40
- Primary result: Hierarchical RL with entropy regularization and knowledge-grounded experts achieves state-of-the-art CRS performance

## Executive Summary
This paper addresses the challenge of optimizing conversational strategies in conversational recommender systems (CRSs) to improve user satisfaction and recommendation success. Existing LLM-based CRS methods rely on predefined prompts and lack explicit strategy optimization, leading to suboptimal interactions. To tackle this, the authors propose a Reinforced Strategy Optimization (RSO) method that decomposes strategy generation into macro-level planning and micro-level adaptation through a network-of-experts architecture. The macro-level Planner selects conversational strategies (e.g., recommend, explain, encourage), while micro-level experts (Preference Reasoner, Fact Retriever, and Actor) adapt these strategies to user context and factual grounding. Strategy learning is formulated as a reinforcement learning problem guided by an LLM-based reward model, enabling automatic strategy exploration. Extensive experiments on INSPIRED and REDIAL datasets show that RSO significantly improves interaction performance, achieving state-of-the-art results in metrics such as watching intention (WI), persuasiveness (PRS), credibility (Cred), and conversation success rate (Conv-SR).

## Method Summary
The authors propose Reinforced Strategy Optimization (RSO), a hierarchical RL framework for CRS that decomposes strategy optimization into macro-level planning and micro-level adaptation. The system uses a Network-of-Experts architecture: a Planner expert selects from 13 predefined conversational strategies, while auxiliary experts (Preference Reasoner, Fact Retriever, and Actor) adapt these strategies to user context and factual grounding. Strategy learning is formulated as an MDP with entropy-regularized RL to prevent strategy collapse and promote exploration. The method is trained in two stages: supervised fine-tuning on strategy-annotated data, followed by RL tuning with LLM-based rewards. Experiments on INSPIRED and REDIAL datasets demonstrate significant improvements over strong baselines.

## Key Results
- RSO achieves Conv-SR of 0.98 on INSPIRED, outperforming PCCRS baseline
- RSO improves WI from 3.65 to 3.80, PRS from 3.77 to 3.83, and Cred from 3.52 to 3.73 on INSPIRED
- RSO demonstrates superior strategy diversity with entropy-regularized RL compared to standard RL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical decomposition of strategy optimization into macro-level planning and micro-level adaptation makes learning tractable.
- Mechanism: By separating the decision of "what type of action to take" (Planner) from "how to instantiate it" (Actor + auxiliary experts), each sub-task operates within a well-defined, smaller decision scope. The Planner learns $\pi_{\theta_1}(h_t|s_t)$ for strategy selection, while the Actor learns $\pi_{\theta_2}(a_t|h_t, s_t)$ for response generation. This reduces the joint optimization problem from learning $\pi_\theta(a_t|s_t)$ directly—which must account for preference reasoning, factual grounding, and dialogue context simultaneously—to learning two simpler conditional policies.
- Core assumption: Strategy selection and strategy execution are sufficiently independent that they can be optimized separately without significant loss of coordination.
- Evidence anchors:
  - [abstract] "This hierarchical decomposition disentangles the optimization of different sub-tasks involved in CRS response generation, enabling more tractable learning at each level."
  - [Section III.A] "To simplify strategy optimization, we hierarchically decompose it into: 1) Macro-level Strategy Planning... 2) Micro-level Strategy Adaptation"
  - [corpus] Limited direct validation in neighbors; related work (CARE, arXiv:2508.13889) addresses contextual adaptation but does not test hierarchical decomposition specifically.
- Break condition: If strategy selection and execution are highly coupled—e.g., the best response depends on fine-grained interaction between strategy type and user context that cannot be captured by cascading—then hierarchical decomposition may introduce approximation error, limiting performance gains.

### Mechanism 2
- Claim: Entropy-regularized reinforcement learning prevents strategy collapse and promotes diverse, context-appropriate strategy exploration.
- Mechanism: The RL objective adds an entropy bonus term $\beta \nabla_\theta \frac{1}{T}\sum_t H(\pi_\theta(\cdot|s_t))$ to the standard policy gradient. This counteracts the tendency of RL