---
ver: rpa2
title: How Reliable are LLMs for Reasoning on the Re-ranking task?
arxiv_id: '2508.18444'
source_url: https://arxiv.org/abs/2508.18444
tags:
- re-ranking
- ranking
- llms
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of making large language models
  (LLMs) transparent and reliable when used for re-ranking tasks. The authors propose
  a method that combines SHAP explainability with LLM-generated textual reasoning
  to provide clear rationales behind ranked recommendations, particularly useful in
  systems with limited user data.
---

# How Reliable are LLMs for Reasoning on the Re-ranking task?

## Quick Facts
- arXiv ID: 2508.18444
- Source URL: https://arxiv.org/abs/2508.18444
- Reference count: 40
- Key result: DPO-trained LLMs with SHAP attribution significantly outperform BM25 baselines and provide more coherent explanations

## Executive Summary
This paper investigates the reliability of large language models for re-ranking tasks when transparency and explainability are required. The authors propose combining SHAP attribution with LLM-generated textual reasoning to provide clear rationales behind ranked recommendations, particularly valuable in systems with limited user data. They compare different training methods for LLMs and find that Direct Preference Optimization (DPO) produces the most effective re-ranking models with the best explainability. The approach is evaluated using multiple metrics including NDCG@5/10 for ranking quality and BERTScore, BLEU, Rouge-L, and Cosine Similarity for explanation alignment.

## Method Summary
The method combines SHAP-based token attribution with LLM-generated explanations for re-ranking tasks. Query-response pairs with human rankings are used to train LLM variants (LLaMa, Mistral, Phi-3, Qwen) with different methods (SFT, PPO, DPO, Reward Modeling). DPO performs best, achieving lowest loss (0.31-0.38) and highest accuracy (78.5%-85.2%). For inference, the trained model generates re-ranked lists, SHAP analyzer extracts top-10 token attributions per item, and GPT-4o generates natural language explanations using these attributions plus structured prompts. The approach is evaluated on the ENVRI-Hub-Next dataset using NDCG@5/10 and semantic similarity metrics.

## Key Results
- DPO-trained LLMs, especially Qwen-7B, significantly outperform BM25 baselines (NDCG@5: 0.73 vs 0.48)
- DPO achieves lowest loss (0.31-0.38) and highest accuracy (78.5%-85.2%) across all model variants
- Explanations with SHAP attribution scores are more coherent and relevant than those without
- DPO shows wider performance margins over PPO at larger model scales

## Why This Works (Mechanism)

### Mechanism 1: Direct Preference Optimization (DPO) for Re-ranking
DPO directly optimizes preference pairs without requiring a separate reward model, leading to better alignment with human ranking judgments through pairwise comparison learning. The approach works because it captures ranking quality more effectively than pointwise or RL approaches by training on comparative judgments rather than absolute scores.

### Mechanism 2: SHAP Attribution for Explainability Enhancement
SHAP quantifies token-level contributions to ranking decisions, enabling grounded, evidence-based explanation generation. By feeding numerical attributions alongside query and response to a general-purpose LLM, the system produces explanations that reference specific influential tokens rather than generic reasoning.

### Mechanism 3: LLM-based Semantic Re-ranking Over Sparse Data
Pre-trained LLMs bring transferable semantic understanding that adapts to domain-specific ranking preferences without extensive user interaction history. The approach leverages the semantic capabilities of LLMs to achieve effective re-ranking even with limited training data (~2,350 preference pairs).

## Foundational Learning

- **SHAP (SHapley Additive exPlanations)**: Why needed here - Core to the paper's explainability approach; calculates feature/token contribution scores for model predictions. Quick check: Can you explain how SHAP assigns attribution values to input tokens, and why these might differ from simple attention weights?

- **Direct Preference Optimization (DPO)**: Why needed here - Identified as the best-performing training method; requires understanding preference-based learning vs. standard supervised fine-tuning. Quick check: How does DPO differ from RLHF approaches like PPO, and what data format does it require for training?

- **Learning-to-Rank and NDCG Metric**: Why needed here - Core evaluation framework; NDCG@K measures ranking quality with position-weighted relevance. Quick check: Why does NDCG penalize relevant items appearing lower in the ranked list, and how is it computed?

## Architecture Onboarding

- **Component map**: Training Module (query-response pairs → LLM variants → training methods) → Reasoning Module (trained LLM generates rankings → SHAP analyzer extracts attributions → explanation LLM generates rationales)

- **Critical path**: 1) Prepare preference pairs (top-5 positive, bottom-5 negative for DPO/PPO), 2) Train LLM with DPO (single neuron output for ranking score), 3) Apply SHAP to identify top-10 attribution tokens, 4) Feed tokens + scores + query + item to explanation LLM with structured prompt

- **Design tradeoffs**: DPO vs PPO vs SFT (DPO requires preference pairs but achieves better accuracy), SHAP top-k selection (paper uses k=10; smaller k risks missing features, larger k adds noise), explanation LLM choice (GPT-4o for quality vs smaller model for cost), data augmentation (DPO expanded 940 to 2,350 pairs)

- **Failure signatures**: Low NDCG gap from BM25 (model hasn't learned meaningful preferences), generic explanations without token references (SHAP values not incorporated), high training loss after DPO epochs (preference signal may be noisy), split token artifacts in explanations (tokenization mismatch)

- **First 3 experiments**: 1) Reproduce training method comparison (train same LLM with SFT, PPO, DPO; compare loss curves and accuracy), 2) SHAP ablation for explanations (generate with k=5,10,15; have experts rate coherence), 3) Cross-domain validation (train on environmental data, test on different domain to assess generalization)

## Open Questions the Paper Calls Out

- **Generalization across domains**: The approach needs systematic evaluation across multiple domains (e-commerce, healthcare, legal) to validate consistent performance beyond environmental/Earth science.

- **Why DPO works better**: The paper doesn't investigate underlying mechanisms explaining why DPO leads to better semantic understanding compared to PPO and SFT for re-ranking tasks.

- **Correlation with human judgment**: Current evaluation relies solely on automated metrics without human assessment of explanation coherence and usefulness, raising questions about metric validity.

- **Optimal attribution token count**: The selection of top-10 tokens was arbitrary; no systematic ablation study was conducted to determine if this varies by query complexity, domain, or model size.

## Limitations

- Results are based on a single environmental/Earth science dataset with limited annotations (94 participants, 940 data points), raising generalizability concerns
- Critical SHAP implementation details are missing (background dataset size, approximation method, computational overhead)
- The approach combines multiple resource-intensive components (DPO training, SHAP computation, GPT-4o explanations) without addressing deployment costs or latency

## Confidence

- **High Confidence**: DPO training effectiveness (clear loss and accuracy improvements across model variants)
- **Medium Confidence**: SHAP attribution improving explanation quality (supported by qualitative comparisons but lacking rigorous human evaluation)
- **Medium Confidence**: LLM-based re-ranking outperforming BM25 (statistically significant but single dataset)
- **Low Confidence**: Generalizability to other domains and datasets (no cross-domain validation performed)

## Next Checks

1. **Cross-Domain Generalization Test**: Train the DPO model on environmental/Earth science data and evaluate on a different domain (e.g., legal or medical retrieval) to assess whether the performance advantage persists when query distributions and terminology change significantly.

2. **SHAP Stability Analysis**: Systematically vary the number of background samples and SHAP approximation method to measure how attribution stability affects explanation quality. Compare explanations generated with different SHAP configurations to identify the minimal reliable setup.

3. **Resource Cost-Benefit Analysis**: Implement the complete pipeline (DPO training + SHAP attribution + GPT-4o explanations) and measure end-to-end computational costs, inference latency, and token usage. Compare these costs against the performance gains over simpler baselines like BM25 to determine practical deployment thresholds.