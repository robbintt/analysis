---
ver: rpa2
title: A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic
  Signal Control
arxiv_id: '2503.23626'
source_url: https://arxiv.org/abs/2503.23626
tags:
- traffic
- learning
- constraints
- each
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of scalable traffic signal control
  in complex urban environments using constrained multi-agent reinforcement learning.
  The authors propose MAPPO-LCE, which extends MAPPO by incorporating a Lagrange multiplier
  method with a cost estimator to balance rewards and constraints.
---

# A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control

## Quick Facts
- arXiv ID: 2503.23626
- Source URL: https://arxiv.org/abs/2503.23626
- Authors: Anirudh Satheesh; Keenan Powell
- Reference count: 40
- Primary result: MAPPO-LCE outperforms MAPPO, IPPO, and QTRAN baselines on real-world traffic datasets with 12.60%, 10.29%, and 13.10% improvements respectively.

## Executive Summary
This paper addresses scalable traffic signal control in complex urban environments using constrained multi-agent reinforcement learning. The authors propose MAPPO-LCE, which extends MAPPO by incorporating a Lagrange multiplier method with a learned cost estimator to balance rewards and constraints. Three real-world-inspired constraints (GreenTime, GreenSkip, PhaseSkip) ensure realistic and fair traffic policies. Experiments on three real-world datasets demonstrate that MAPPO-LCE outperforms baseline MARL algorithms across all environments and constraints, achieving significant improvements in throughput and delay reduction while maintaining constraint satisfaction.

## Method Summary
MAPPO-LCE is a constrained multi-agent reinforcement learning approach for autonomous traffic signal control. It builds on MAPPO (Multi-Agent PPO) by adding a Lagrange multiplier method with a learned cost estimator to handle constraints. The method uses a shared actor-critic architecture where agents control intersections to maximize throughput and minimize delay while satisfying three real-world constraints: GreenTime (maximum green duration), PhaseSkip (consecutive phase skips), and GreenSkip (consecutive light skips). A cost estimator network predicts immediate costs, and the Lagrange multiplier is updated via gradient descent to balance reward maximization against constraint violations. The approach is evaluated on three real-world road network datasets using CityFlow simulator.

## Key Results
- MAPPO-LCE outperforms MAPPO baseline by 12.60% on throughput and reduces average delay
- MAPPO-LCE outperforms IPPO baseline by 10.29% across all environments and constraints
- MAPPO-LCE outperforms QTRAN baseline by 13.10% while maintaining lower constraint violations
- The method shows consistent performance across Hangzhou (16 intersections), Jinan (12 intersections), and New York (48 intersections) datasets

## Why This Works (Mechanism)

### Mechanism 1: Lagrange Cost Estimator for Stable Constraint Handling
- **Claim:** A learned cost estimator provides more stable constraint violation estimates than using cost advantage functions directly, enabling more reliable Lagrange multiplier updates.
- **Mechanism:** The cost estimator network θ_C is trained via MSE loss (L_θC = ||θ_C(s_t, a_t) - c_t||²) to predict immediate costs. The Lagrange multiplier λ is then updated using L_λ = E[-λ(θ_C(s_t, a_t) - c)], which penalizes violations relative to cost limit c. This decouples constraint estimation from policy learning dynamics.
- **Core assumption:** Assumption: The cost function can be learned sufficiently fast (within first few iterations) to provide accurate guidance before policy divergence occurs.
- **Evidence anchors:**
  - [Section 4.1]: "This cost estimator quickly learns the cost dynamics within the first few iterations to accurately predict the cost, and then updates λ."
  - [Section 4.1]: Describes Equation 15 and 16 for cost estimator training and λ update.
  - [corpus]: Weak direct validation. Neighbor papers address MARL for ATSC but do not evaluate Lagrange cost estimator approaches specifically.
- **Break condition:** If cost dynamics are highly non-stationary or depend on long-term state histories beyond immediate (s_t, a_t), the estimator may fail to converge.

### Mechanism 2: Lagrangian Objective Composition for Reward-Cost Tradeoff
- **Claim:** Combining reward and cost losses via L(π_θ) = L_r(π_θ) - λL_c(π_θ) allows adaptive balancing between performance maximization and constraint satisfaction.
- **Mechanism:** MAPPO-LCE maintains separate critics V^r_φr (reward) and V^c_φc (cost), computing advantage functions A^r_t and A^c_t. The policy is updated to maximize L_r while minimizing λL_c, with λ adaptively controlling the penalty weight for constraint violations. Clamping λ ≥ 0 ensures the policy is always penalized when constraints are violated.
- **Core assumption:** Assumption: The constrained optimization landscape admits solutions where reward maximization and constraint satisfaction are not fundamentally contradictory.
- **Evidence anchors:**
  - [Section 4.1]: Equation 10-14 define the loss composition and critic updates.
  - [Section 6.1]: MAPPO-LCE shows improved test reward while maintaining lower constraint violations compared to baselines.
  - [corpus]: Constrained RL for traffic is mentioned in Haydari et al. (single-agent with emissions constraint), but multi-agent Lagrangian approaches remain underexplored.
- **Break condition:** If constraints are over-specified or mutually incompatible with achieving any positive reward, λ may grow unbounded without finding feasible solutions.

### Mechanism 3: Domain-Specific Soft Constraints (GreenTime, PhaseSkip, GreenSkip)
- **Claim:** Penalty-based constraints derived from real-world traffic engineering guidelines produce policies that are both performant and practically deployable.
- **Mechanism:** Three constraint types penalize violations: (1) GreenTime limits maximum green duration per light (G_time(l) ≤ G_max_time), (2) PhaseSkip limits consecutive phase skips, (3) GreenSkip limits consecutive light skips. Constraint violations are averaged across all lights/phases per Equation 4, producing a normalized penalty signal.
- **Core assumption:** Assumption: These specific constraints adequately capture the essential safety and fairness requirements for real-world deployment.
- **Evidence anchors:**
  - [Section 3.2]: Defines all three constraints with mathematical formulations and Algorithm 1-3.
  - [Section 6.1]: "In almost all environment and constraint combinations, the value of the constraint violation is lowest for MAPPO-LCE."
  - [corpus]: Raeis and Leon-Garcia [33] propose fairness constraints but in single-agent settings; RiLSA guidelines [16] inform constraint design but empirical validation in MARL remains limited.
- **Break condition:** If real-world deployment requires additional constraints (e.g., emergency vehicle priority, pedestrian phases) not modeled, learned policies may be unsafe or unfair in practice.

## Foundational Learning

- **Concept: Constrained Markov Games (CMGs)**
  - **Why needed here:** The paper formulates ATSC as a CMG with joint cost constraints. Understanding the tuple M = ⟨N, S, {O_i}, {A_i}, T, r, Ω, C, c, γ⟩ is essential for following the problem setup.
  - **Quick check question:** Can you explain the difference between a standard Markov Game and a Constrained Markov Game in terms of what the policy must optimize?

- **Concept: Proximal Policy Optimization (PPO) with Clipping**
  - **Why needed here:** MAPPO-LCE builds on MAPPO, which uses PPO's clipped objective. Understanding ρ_t (importance sampling ratio), the clipping operation clip(ρ_t, 1±ε), and advantage estimation is required to follow the loss formulations.
  - **Quick check question:** Why does PPO clip the importance sampling ratio rather than using a hard KL constraint?

- **Concept: Centralized Training Decentralized Execution (CTDE)**
  - **Why needed here:** MAPPO-LCE follows CTDE where agents share actor/critic networks during training but execute independently. This addresses partial observability while maintaining scalability.
  - **Quick check question:** In CTDE, what information is available during training that is NOT available during execution?

## Architecture Onboarding

- **Component map:**
  - Actor network π_θ -> Policy network outputting phase selection for each agent's intersection
  - Reward critic V^r_φr -> Estimates discounted cumulative reward
  - Cost critic V^c_φc -> Estimates discounted cumulative cost for advantage computation
  - Cost estimator θ_C -> MLP (2 hidden layers, 128 units each) predicting immediate cost from (s_t, a_t)
  - Lagrange multiplier λ -> Scalar parameter updated via gradient descent on L_λ
  - Replay buffer D -> Stores rollouts (s_t, a_t, r_t, c_t, s_{t+1}) for batch training

- **Critical path:**
  1. Environment step -> collect (s, a, r, c, s') tuple -> store in buffer
  2. After B episodes: sample batch from buffer
  3. Update critics via TD error (Equations 13-14)
  4. Update cost estimator via MSE loss (Equation 15)
  5. Update λ via L_λ minimization (Equation 16), clamp λ ≥ 0
  6. Update policy via combined Lagrangian loss (Equation 10)
  7. Soft update: θ ← τθ + (1-τ)θ'

- **Design tradeoffs:**
  - Single shared actor/critic reduces memory/compute vs. independent networks per agent, but assumes homogeneous intersections
  - Cost estimator adds computational overhead but stabilizes λ updates vs. direct advantage-based updates (MAPPO-Lagrange)
  - Constraint thresholds (G_max_time=40, P_max_skips=16, G_max_skips=4) are hyperparameters; stricter values may over-constrain

- **Failure signatures:**
  - λ growing unbounded -> constraints may be infeasible; check cost limit c and constraint thresholds
  - Constraint violations high but λ not increasing -> cost estimator may be underfitting; increase estimator capacity or learning rate
  - Test reward degrading while training reward improves -> potential overfitting; reduce model capacity or increase batch diversity
  - Disparate performance across environments -> dataset-specific tuning needed; NY (48 agents) is most complex

- **First 3 experiments:**
  1. **Baseline sanity check:** Run MAPPO-LCE on HZ dataset (16 intersections) with GreenTime constraint only. Verify test reward improves vs. IPPO/MAPPO baselines and constraint violation decreases over training. Target: convergence within 500K timesteps.
  2. **Ablate cost estimator:** Compare MAPPO-LCE vs. MAPPO-Lagrange (advantage-based λ update) on JN dataset. Measure constraint violation stability and reward variance across seeds. Target: MAPPO-LCE should show lower variance in λ trajectories.
  3. **Scalability stress test:** Run on NY dataset (48 intersections) with all three constraints combined. Monitor throughput, average delay, and summed constraint violations. Compare against Table 5-7 baselines. Target: >30% throughput improvement over MAPPO.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can incorporating inter-agent communication mechanisms, such as Graph Neural Networks (GNNs) with message pruning, improve policy coordination without introducing prohibitive computational overhead?
- Basis in paper: [explicit] The authors state in Section 7 that future work could incorporate communication via connected vehicle technology and GNNs, noting the need to leverage pruning strategies to avoid redundant communication that hinders learning.
- Why unresolved: While the current centralized critic shares information, the agents lack explicit communication channels for exchanging local data like travel times, and it is unclear if the benefits of such communication outweigh the associated optimization costs.
- Evidence would resolve it: Experiments comparing the current MAPPO-LCE against a GNN-augmented version on the New York (NY) dataset, measuring convergence speed and reward performance relative to the communication volume.

### Open Question 2
- Question: How can the algorithm be modified to handle agent-specific constraint violations rather than relying on a globalized constraint value applied equally to all agents?
- Basis in paper: [explicit] Section 7 notes that the current architecture uses a globalized constraint value, meaning the system "cannot model each agent to correct any individual constraint violations."
- Why unresolved: Modeling fine-grained control requires distinct cost estimators for every agent, which the authors identify as a significant computational overhead that has not yet been balanced against potential performance gains.
- Evidence would resolve it: A modification of MAPPO-LCE utilizing agent-specific cost critics, demonstrating the ability to isolate and correct violations at specific intersections without degrading global training stability.

### Open Question 3
- Question: Does the inclusion of soft constraints (e.g., fairness variance) and heterogeneous vehicle types (e.g., emergency vehicles) destabilize the Lagrangian optimization or improve real-world applicability?
- Basis in paper: [explicit] Section 7 proposes expanding the environment with soft constraints and vehicle types like ambulances to create robust constraints that better reflect real-life scenarios.
- Why unresolved: Introducing competing objectives (soft constraints) and variable priority dynamics complicates the reward-cost balance, and it is uncertain if the current Lagrange multiplier method can adapt to these multi-faceted demands.
- Evidence would resolve it: Ablation studies in a modified CityFlow environment containing priority vehicles, showing that MAPPO-LCE can maintain stability (low variance in loss) while satisfying these new, complex constraints.

### Open Question 4
- Question: Can expectation alignment methods, such as ELIGN, be integrated to allow agents to predict neighboring traffic swells more effectively than the current observation space allows?
- Basis in paper: [explicit] The authors suggest in Section 7 that giving agents a "better idea of their surroundings through expectation alignment" could help predict traffic swells before they reach an intersection.
- Why unresolved: It is untested whether adding second-order theory of mind (predicting neighbors' actions) provides a significant advantage over the current method, or if it merely adds complexity without improving throughput.
- Evidence would resolve it: Comparative analysis of average delay metrics between the standard implementation and an implementation augmented with ELIGN-based intrinsic rewards.

## Limitations

- Evaluation relies on three datasets with fixed constraints that may not generalize to all urban scenarios
- Cost estimator performance depends heavily on the assumption that immediate (s_t, a_t) → c_t mappings capture all relevant constraint dynamics
- Shared actor/critic architecture assumes intersection homogeneity, potentially limiting applicability to heterogeneous urban layouts

## Confidence

- **High confidence:** The core MAPPO-LCE architecture and Lagrangian cost estimator mechanism are technically sound and clearly explained
- **Medium confidence:** Performance improvements over baselines are well-demonstrated, though the constrained MARL space lacks extensive direct comparisons
- **Low confidence:** Real-world deployment viability is assumed but not empirically validated beyond the specific constraint types tested

## Next Checks

1. **Constraint Generalization Test:** Apply MAPPO-LCE to a new constraint type (e.g., pedestrian phase priority) and evaluate whether the cost estimator adapts without retraining from scratch
2. **Heterogeneous Intersection Evaluation:** Modify the shared network architecture to handle intersections with different numbers of phases/lanes and measure performance degradation
3. **Long-term Stability Analysis:** Run training for 2M+ timesteps to assess whether constraint violations and λ stability persist over extended periods, checking for potential convergence issues