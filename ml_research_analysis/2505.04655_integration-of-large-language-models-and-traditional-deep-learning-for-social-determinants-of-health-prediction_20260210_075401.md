---
ver: rpa2
title: Integration of Large Language Models and Traditional Deep Learning for Social
  Determinants of Health Prediction
arxiv_id: '2505.04655'
source_url: https://arxiv.org/abs/2505.04655
tags:
- sdoh
- dataset
- classi
- learning
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically extracting
  Social Determinants of Health (SDoH) from clinical text using both traditional deep
  learning and Large Language Models (LLMs). The authors develop and evaluate multiple
  classification approaches, including supervised fine-tuned LLMs (Llama 3.1 and 3.3),
  traditional deep learning models (RoBERTa), and a novel two-step classifier that
  combines a binary traditional model for efficiency with an LLM for precision.
---

# Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction

## Quick Facts
- arXiv ID: 2505.04655
- Source URL: https://arxiv.org/abs/2505.04655
- Authors: Paul Landes; Jimeng Sun; Adam Cross
- Reference count: 39
- Primary result: Best model achieves macro F1 of 0.67 on MIMIC-III clinical text SDoH classification

## Executive Summary
This paper addresses the challenge of automatically extracting Social Determinants of Health (SDoH) from clinical text using both traditional deep learning and Large Language Models (LLMs). The authors develop and evaluate multiple classification approaches, including supervised fine-tuned LLMs (Llama 3.1 and 3.3), traditional deep learning models (RoBERTa), and a novel two-step classifier that combines a binary traditional model for efficiency with an LLM for precision. They train and evaluate these models on MIMIC-III clinical text data, both alone and supplemented with synthetic data. The best performing model achieves a macro F1 score of 0.67, outperforming a previous baseline by 10 points. The two-step classifier provides a favorable trade-off between accuracy (macro F1 0.62) and inference speed (12X faster than the smallest LLM).

## Method Summary
The authors develop a two-step classifier that first uses a binary RoBERTa model to identify sentences containing SDoH, then routes positive cases to a Llama 3.1 8B model for multi-label classification. They also train traditional RoBERTa models with concatenated linguistic and clinical concept features, and fine-tune LLMs with LoRA. The datasets include MIMIC-III (5,355 sentences) and synthetic data (588 sentences) generated to address severe class imbalance. Models are evaluated using macro F1 score, with cross-validation and ablation studies to understand performance drivers.

## Key Results
- Best model (Llama 3.1 8B) achieves macro F1 of 0.67 on MIMIC-III
- Two-step classifier achieves macro F1 of 0.62 with 12X faster inference than pure LLM
- RoBERTa with synthetic data augmentation improves macro F1 from 0.53 to 0.88
- Cross-validation shows stable performance across folds with low variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A cascaded two-step architecture can preserve LLM precision while achieving 12X faster inference than pure LLM approaches.
- Mechanism: A lightweight binary RoBERTa classifier (110M parameters) first filters sentences to identify those containing at least one SDoH. Only positive cases proceed to the computationally expensive Llama 3.1 8B model for multi-label classification. This exploits the observation that 92.6% of sentences contain no SDoH, allowing the binary model to eliminate most LLM processing.
- Core assumption: False negatives in the binary filtering stage will propagate and reduce overall recall; the binary classifier's recall must be sufficiently high to avoid losing true SDoH cases.
- Evidence anchors:
  - [abstract]: "The two-step classifier provides a favorable trade-off between accuracy (macro F1 0.62) and inference speed (12X faster than the smallest LLM)"
  - [Section 3.1, Table 6]: Binary RoBERTa achieves 371 sentences/second vs Llama 3.1 8B at 5.1 sentences/second
  - [Section 2.3]: Two-step achieves macro F1 0.62, within 5 points of best LLM (0.67)
  - [corpus]: Related work on LLM routing (arXiv:2405.19631) supports intelligent model selection for efficiency, though not directly comparable
- Break condition: If binary classifier recall drops significantly (e.g., <0.85), overall system recall degrades unacceptably. Current macro recall is 0.59 per Table 1.

### Mechanism 2
- Claim: Synthetic LLM-generated training data can improve minority class performance, particularly for rare SDoH categories with few training examples.
- Mechanism: The "Amended" dataset combines original MIMIC-III data (5,355 sentences) with synthetic data (588 sentences). The synthetic data is generated to contain at least one SDoH per sentence, addressing the severe class imbalance where Housing and Transportation labels had only 3 occurrences each in MIMIC-III.
- Core assumption: Synthetic sentences accurately reflect the linguistic patterns of real clinical text for each SDoH category; distribution shift between synthetic and real data does not introduce harmful artifacts.
- Evidence anchors:
  - [Section 2.1, Table 1]: RoBERTa macro F1 improves from 0.53 (MIMIC-III only) to 0.88 (Amended dataset)
  - [Section 2.1, Table 2]: Housing F1 improves from 0.00 to 0.92 on Amended dataset
  - [Table 8-9]: Housing occurrences increase from 3 to 72; Transportation from 3 to 64
  - [corpus]: No direct corpus evidence on synthetic data augmentation for SDoH; this mechanism is paper-specific
- Break condition: If synthetic data contains systematic biases or unrealistic patterns, models may overfit to synthetic artifacts that do not generalize to real clinical notes.

### Mechanism 3
- Claim: Concatenating linguistic and clinical concept features with transformer embeddings can improve traditional model performance on specialized classification tasks.
- Mechanism: RoBERTa base embeddings are concatenated with: (1) one-hot encoded linguistic features (POS tags, dependency depth, named entities, medical entities), (2) token-level SDoH predictions from a prior model, and (3) SBERT embeddings of UMLS concept definitions linked via MedCAT entity linker. This enriched representation passes through a 1D CNN before classification.
- Core assumption: The added features provide signal not captured in RoBERTa's pre-trained representations; entity linking quality is sufficient to connect clinical terms to UMLS concepts.
- Evidence anchors:
  - [Section 4.4.3, Figure 3]: Architecture diagram shows RoBERTa + MedCAT + linguistic taggers feeding concatenated embeddings to Conv1D
  - [Section 2.5, Figure 1]: Ablation shows feature combinations vary significantly; POS/dependency/named entities help on MIMIC-III, token-level SDoH helps on Amended
  - [Section 4.4.2, Figure 2]: 35% of MIMIC-III sentences have at least one CUI, supporting feasibility of concept features
  - [corpus]: Related corpus papers (arXiv:2506.00134) warn of spurious correlations in SDoH extraction, suggesting feature engineering requires careful validation
- Break condition: If entity linking recall is low or CUI embeddings are sparse, concept features add noise. Figure 2 shows 65% of sentences have zero CUIs extracted.

## Foundational Learning

- Concept: Multi-label classification with severe class imbalance
  - Why needed here: SDoH dataset has 92.6% "No SDoH" sentences; minority labels (Housing, Transportation) have <0.2% prevalence. Standard accuracy metrics are misleading; macro F1 averages across classes equally regardless of frequency.
  - Quick check question: Given 992 negative and 73 positive binary samples, what baseline accuracy would a naive "always predict negative" classifier achieve? (Answer: 93.1%)

- Concept: Encoder-only vs. decoder-only transformer architectures
  - Why needed here: RoBERTa (encoder-only, 110M params) excels at efficient classification but struggles with minority labels. Llama (decoder-only, 8B-70B params) handles few-shot learning and minority classes better but has 73X higher inference latency.
  - Quick check question: Why can encoder-only models use bidirectional context while decoder-only models cannot? (Answer: Encoder attention has no causal mask; decoder masks future tokens for autoregressive generation.)

- Concept: Low-Rank Adaptation (LoRA) for efficient fine-tuning
  - Why needed here: Full fine-tuning of 8B+ parameter models is computationally prohibitive. LoRA adds trainable rank-64 matrices to attention layers, reducing parameters while preserving base model knowledge.
  - Quick check question: If a model has 8B parameters and LoRA uses rank 64 on attention layers only, approximately what fraction of parameters are trainable? (Answer: ~0.1-1% depending on architecture; paper uses rank 64, lr=5e-5, dropout=0.1)

## Architecture Onboarding

- Component map:
  Data layer -> Preprocessing layer -> Traditional model OR LLM branch -> Two-step orchestrator

- Critical path:
  1. Sentence → binary RoBERTa → probability of "Has SDoH"
  2. If probability > threshold (default 0.5) → sentence → Llama 3.1 8B → multi-label output
  3. If probability ≤ threshold → return "No SDoH"
  
  Bottleneck: LLM inference (5.1 sentences/sec vs 371 for binary model)

- Design tradeoffs:
  - **Accuracy vs. latency**: Pure Llama 3.1 8B achieves MF1=0.67 but 5.1 sentences/sec; two-step achieves MF1=0.62 at 61.7 sentences/sec (5-point accuracy loss for 12X speedup)
  - **Feature complexity vs. robustness**: Ablation shows feature combinations have high variance across datasets; simpler models may generalize better
  - **Synthetic data vs. distribution shift**: Amended dataset improves MF1 from 0.53 to 0.88 for RoBERTa, but relies on synthetic data quality

- Failure signatures:
  - **Binary filter false negatives**: Check recall on "Has SDoH" class (currently 0.767 per Table 4); low values indicate lost SDoH cases
  - **Minority label collapse**: Housing/Transportation F1=0.00 on MIMIC-III-only training signals insufficient examples
  - **LLM output parsing failures**: Paper mentions "noisy" few-shot outputs requiring complex regex; fine-tuned models more consistent
  - **CUI sparsity**: 65% of sentences have zero CUIs (Figure 2); concept features provide no signal for majority of inputs

- First 3 experiments:
  1. **Binary classifier calibration**: Train binary RoBERTa on MIMIC-III, evaluate recall specifically on the 73 "Has SDoH" samples. Target: recall ≥0.90 before deploying two-step system.
  2. **Feature ablation on held-out fold**: Replicate Figure 1 ablation using cross-validation splits to identify which features (POS, CUI embeddings, token-level SDoH) provide consistent signal vs. dataset-specific noise.
  3. **Two-step end-to-end latency benchmark**: Measure wall-clock time for 1,000 sentences through two-step vs. pure Llama 3.1 8B. Verify 10-15X speedup claim holds in your inference environment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the two-step classifier architecture yield higher performance when trained and evaluated on the Amended (MIMIC-III + synthetic) dataset compared to the MIMIC-III only dataset?
- Basis in paper: [explicit] The authors state in Section 4.4.4: "We believe the results of the two-step on the Amended dataset would be higher, but leave this as a future work."
- Why unresolved: The paper only evaluates the two-step model on the original MIMIC-III data, despite showing that the Amended dataset significantly boosts the performance of single-stage models.
- What evidence would resolve it: Execution of the two-step pipeline (Binary RoBERTa -> Llama 3.1) on the Amended dataset splits with a comparison of Macro F1 scores against the current MIMIC-III baseline.

### Open Question 2
- Question: Does removing negative ("No SDoH") labels from the training data of the LLM in the second stage of the hybrid classifier improve its precision and recall?
- Basis in paper: [explicit] Section 2.4 hypothesizes: "The two-step classifier may perform better using a LLM trained without negative SDoH labels" since the binary classifier in step one theoretically filters out negative instances.
- Why unresolved: The current implementation trains the second-step LLM with negative labels, which may lead to error propagation or unnecessary label suppression.
- What evidence would resolve it: An ablation study comparing the current two-step model against a variant where the second-step LLM is fine-tuned exclusively on positive SDoH examples.

### Open Question 3
- Question: Can integrating `cui2vec` embeddings improve the performance of the traditional deep learning model over the SBERT-based concept encoding method employed in this study?
- Basis in paper: [explicit] Section 4.4.2 notes: "we opted for a middle-ground solution to encode the CUI properties and leave the implementation of the cui2vec features for future work."
- Why unresolved: The authors utilized a clinical SBERT model for concept embeddings but did not test `cui2vec` due to sparsity concerns, leaving the comparative efficacy of `cui2vec` unknown.
- What evidence would resolve it: A comparative evaluation replacing the SBERT CUI encoding component with `cui2vec` embeddings and measuring the change in Macro F1 on the test sets.

## Limitations
- Synthetic data dependency: Performance improvements rely on LLM-generated synthetic data that may not generalize to real clinical text
- Binary filter sensitivity: Two-step architecture performance depends critically on binary classifier recall, which currently allows 23% of SDoH cases to be missed
- Feature engineering variance: Ablation studies show feature combinations have high dataset-specific variance rather than consistent benefits

## Confidence
- Two-step architecture efficiency: **High** (clear empirical support, 12X speedup measured)
- Synthetic data minority class improvement: **Medium** (dramatic results but no real-data validation)
- Feature concatenation benefits: **Low-Medium** (ablation shows dataset-specific effects)

## Next Checks
1. **Synthetic Data Distribution Shift Test**: Take the best-performing model trained on Amended data and evaluate exclusively on MIMIC-III test set. Measure whether the minority class improvements persist when synthetic data is excluded from evaluation.

2. **Binary Filter Sensitivity Analysis**: Systematically vary the binary classifier threshold (0.3→0.7) and measure the trade-off between overall inference speed and system-level recall. Identify the threshold that maintains acceptable recall (≥0.85) while maximizing efficiency gains.

3. **Cross-Dataset Generalization**: Apply the two-step model to an independent clinical text corpus (e.g., eICU or clinical notes from another institution). Evaluate whether the 12X speedup and 0.62 MF1 performance hold when the data distribution shifts away from MIMIC-III.