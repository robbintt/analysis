---
ver: rpa2
title: Unlearning Algorithmic Biases over Graphs
arxiv_id: '2505.14945'
source_url: https://arxiv.org/abs/2505.14945
tags:
- unlearning
- graph
- latexit
- random
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a training-free post-processing framework that
  leverages graph unlearning to mitigate algorithmic bias in pre-trained graph ML
  models. The approach introduces a novel node feature unlearning setting with certified
  removal guarantees that scale sublinearly with training set size, along with degree-aware
  structural unlearning strategies for edges and nodes informed by formal bias analyses.
---

# Unlearning Algorithmic Biases over Graphs

## Quick Facts
- arXiv ID: 2505.14945
- Source URL: https://arxiv.org/abs/2505.14945
- Reference count: 40
- Key outcome: Training-free post-processing framework achieving up to 75% reduction in fairness metrics without sacrificing utility, 10-20x faster than retraining.

## Executive Summary
This paper introduces a training-free post-processing framework that leverages graph unlearning to mitigate algorithmic bias in pre-trained graph ML models. The approach introduces a novel node feature unlearning setting with certified removal guarantees that scale sublinearly with training set size, along with degree-aware structural unlearning strategies for edges and nodes informed by formal bias analyses. By identifying and unlearning bias-propagating graph components via single-step Newton updates, the method achieves significant fairness improvements while maintaining utility and offering substantial computational efficiency gains over retraining from scratch.

## Method Summary
The framework targets pre-trained SGC (linearized GCN) models and performs certified unlearning through three mechanisms: feature unlearning (removing highly correlated features), edge unlearning (removing intra-edges between same-sensitive-attribute nodes), and node unlearning (removing low-degree nodes). The method computes bias scores for graph components, selects top-k elements for removal, and applies a single-step Newton update using the Hessian at the optimal weights. This provides statistically indistinguishable results from retraining while offering 10-20x speedup. The approach requires pre-training via DP-SGD for theoretical guarantees and operates on attributed graphs with binary sensitive attributes.

## Key Results
- Up to 75% reduction in statistical parity bias metrics without sacrificing utility
- 10-20x faster runtime compared to retraining from scratch on benchmark datasets
- Certified (ε,δ)-removal guarantees for convex loss functions
- Validated on four real-world attributed graph datasets (Credit Defaulter, German Credit, Recidivism, Pokec-z)

## Why This Works (Mechanism)

### Mechanism 1: Correlation-Based Feature Erasure
If a feature correlates strongly with a sensitive attribute, removing it reduces statistical parity bias (ΔSP). The method identifies features using Pearson correlation ρ between features and sensitive attributes, then "unlearns" these features (setting to zero across all nodes) and updates model weights. This removes linear dependency between sensitive attribute and prediction. Core assumption: linear correlation captures relationship between features and sensitive attributes. Break condition: if model learns non-linear interactions where benign features act as proxies for sensitive attributes, linear correlation selection may fail.

### Mechanism 2: Homophily-Guided Structural Unlearning
Removing specific "intra-edges" (connections between nodes with same sensitive attribute) reduces bias by disrupting the "white/straight" effect where sensitive attributes leak through topology. The framework assigns bias score b_e(e_ij) to edges, prioritizing removal of edges connecting nodes of same sensitive attribute (intra-edges), specifically those incident to low-degree nodes. This alters propagation matrix P to balance inter-vs-intra-group information flow. Core assumption: bias partly caused by homophily where sensitive information amplifies during message-passing. Break condition: if graph is heterophilic or inter-edges are primary bias source, this strategy might degrade utility without improving fairness.

### Mechanism 3: Certified Newton Update
A single-step Newton update provides mathematically certified approximation of retraining model from scratch on modified data. Instead of retraining (10-20× slower), the method calculates gradient difference Δ caused by removed data and applies Hessian-based update: w̃ = w* + H^{-1}Δ. This projects weights to state statistically close to retrained optimum. Core assumption: loss function is convex (e.g., logistic loss), twice differentiable, and data removal request is small relative to training set that first-order approximation holds. Break condition: if loss landscape is highly non-convex, Hessian inverse may be unstable or single-step approximation may diverge from true retrained optimum.

## Foundational Learning

- **Concept: Simple Graph Convolution (SGC)**
  - Why needed here: Paper explicitly restricts theoretical guarantees to SGCs (linearized GCNs) where aggregation is Z = P^L X. Understanding this linear bottleneck is critical to understanding why Newton update works (convexity).
  - Quick check question: Can you explain why removing non-linearities (ReLU) from a GCN makes the model "linear" and why that enables the "certified" unlearning guarantees?

- **Concept: Group Fairness Metrics (Statistical Parity)**
  - Why needed here: Optimization target defined by ΔSP (Statistical Parity) and ΔEO (Equal Opportunity). Must understand goal is "Independence" (prediction independent of sensitive attribute) vs. "Separation" (prediction independent of sensitive attribute given label).
  - Quick check question: Does Theorem 3 guarantee that ΔSP will be zero, or simply that it is bounded by feature correlation ‖ρ‖?

- **Concept: Influence Functions & The Hessian**
  - Why needed here: Unlearning mechanism relies on inverse Hessian H^{-1} to measure how much weights should change when data points are removed.
  - Quick check question: Why is calculating H^{-1} generally considered computationally expensive, and does paper suggest method to mitigate this?

## Architecture Onboarding

- **Component map:**
  1. Input: Pre-trained SGC model weights (w*), Graph G=(V, E, X), Sensitive attribute s
  2. Selection Module: Feature Scorer computes correlation ρ_f for each feature f, selects top-k features Fu; Structure Scorer computes bias score b_e or b_n, selects top-k edges/nodes
  3. Unlearning Engine: Computes "Unlearning Gradient" Δ (gradient difference between original and modified data), computes/approximates Hessian Inverse H^{-1}, updates weights: w̃ = w* + H^{-1}Δ
  4. Output: Updated weights w̃ and modified data G̃

- **Critical path:** Calculation of selection criteria (e.g., correlation ρ) and subsequent Hessian inversion. If selection is wrong, unlearn "safe" data (hurting utility); if Hessian is singular, update fails.

- **Design tradeoffs:**
  - Utility vs. Fairness: Paper claims "up to 75% bias reduction without sacrificing utility," but empirical results show accuracy can fluctuate (e.g., Recidivism Random Unlearning drops to 79.65% vs 88% pre-trained)
  - Approximation vs. Exactness: Framework trades exact solution of "Retraining from Scratch" for speed of "Newton Update." Claims gap is negligible (ε-certified), but relies on convex assumptions

- **Failure signatures:**
  - High Bias Persistence: Occurs if selection budget k is too small or if bias encoded in non-linear interactions not captured by ρ
  - Utility Collapse: Occurs if k is too large or if Newton step is unstable (learning rate/Hessian issues)
  - Certification Failure: If loss is non-convex, εδ-guarantee (Eq 1) no longer holds strictly

- **First 3 experiments:**
  1. Validate Feature Unlearning (Tabular sanity check): Run feature unlearning on non-graph dataset to verify removing high-correlation features actually lowers ΔSP as predicted by Theorem 3
  2. Ablation on Edge Selection: Compare proposed "intra-edge" removal strategy vs "random edge" removal to confirm hypothesis that homophily drives bias
  3. Complexity Benchmark: Measure wall-clock time for Newton update vs Retraining from scratch on scale-up of "Credit Defaulter" dataset to verify claimed 10-20x speedup

## Open Questions the Paper Calls Out

- **Open Question 1:** How can certified bias mitigation guarantees be extended to models with non-convex loss functions or deep non-linear Graph Neural Networks?
  - Basis: Section 6 states limitations related to Newton updates are naturally inherited, such as loss of certified removal guarantees for non-convex losses
  - Why unresolved: Theoretical proofs for certified removal bounds rely on assumption of convex loss function with unique minimizer
  - What evidence would resolve it: Theoretical framework or approximate bound providing certified removal guarantees for non-convex objectives

- **Open Question 2:** Can proposed unlearning framework be adapted for pre-trained models that were not trained using DP-SGD?
  - Basis: Section 6 states framework requires graph ML model pre-trained in DP-SGD manner for theoretical unlearning guarantees to hold
  - Why unresolved: Theoretical mechanism utilizes noise vector b added to objective, specific property of DP-SGD training process
  - What evidence would resolve it: Modified certification mechanism that doesn't rely on specific properties of DP-SGD training data

- **Open Question 3:** How does fairness-aware structural unlearning strategy perform in heterophilic graphs where connectivity is not driven by similarity?
  - Basis: Section 4.2 states edge selection mechanism is designed based on "homophilic... nature" of real-world networks
  - Why unresolved: Bias score b_e explicitly prioritizes intra-edges based on assumption that they densify connectivity within sensitive groups
  - What evidence would resolve it: Experimental results on heterophilic benchmark datasets demonstrating unlearning intra-edges reduces bias without degrading utility

## Limitations
- Theoretical scope limited to convex logistic regression/SGC settings; extension to deep non-linear GNNs remains theoretical
- Feature unlearning relies on Pearson correlation which may miss non-linear or proxy bias effects
- Empirical validation limited to four datasets with binary sensitive attributes; robustness to multi-class sensitive attributes not explored

## Confidence
- **High Confidence:** Correlation-based feature selection mechanism and link to statistical parity bias well-supported by Theorem 3 and Table 1 results
- **Medium Confidence:** Intra-edge removal strategy grounded in homophily analysis but effectiveness may vary with graph structure
- **Low Confidence:** Claims about scalability and robustness to non-convex deep GNNs not directly validated; absence of multi-class sensitive attributes limits generalizability

## Next Checks
1. **Theoretical Extension Validation:** Test feature unlearning mechanism on non-graph tabular dataset (e.g., COMPAS) to confirm Theorem 3's correlation bounds hold outside graph setting
2. **Graph Structure Ablation:** Perform ablation comparing intra-edge vs random edge removal on heterophilic and balanced graphs to verify bias score consistently improves fairness without degrading utility
3. **Scalability and Non-Linear Extension:** Benchmark Newton update against retraining on larger, deeper GNN models (e.g., GCN with ReLU, GAT) to assess runtime gains and impact of non-convexity on certification guarantees