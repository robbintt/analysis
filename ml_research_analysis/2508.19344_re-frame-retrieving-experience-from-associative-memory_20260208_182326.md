---
ver: rpa2
title: Re:Frame -- Retrieving Experience From Associative Memory
arxiv_id: '2508.19344'
source_url: https://arxiv.org/abs/2508.19344
tags:
- expert
- memory
- associative
- frame
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of offline reinforcement learning
  (RL) from suboptimal datasets with limited expert demonstrations. The proposed Re:Frame
  method integrates a small Associative Memory Buffer (AMB) containing expert trajectories
  into a standard offline RL policy, enabling the agent to retrieve and incorporate
  expert knowledge during training and evaluation.
---

# Re:Frame -- Retrieving Experience From Associative Memory

## Quick Facts
- arXiv ID: 2508.19344
- Source URL: https://arxiv.org/abs/2508.19344
- Reference count: 40
- Primary result: Improves Decision Transformer baseline by up to +10.7 normalized points on D4RL MuJoCo using as few as 60 expert trajectories

## Executive Summary
Re:Frame introduces a simple method to improve offline RL performance on suboptimal datasets by incorporating a small Associative Memory Buffer (AMB) of expert trajectories. The method uses an autoencoder to compress expert data into a latent space, enabling content-based retrieval of contextually relevant expert actions during both training and evaluation. Experiments on D4RL MuJoCo benchmarks show consistent improvements over Decision Transformer baselines in three of four settings, demonstrating data-efficient integration of scarce expert knowledge into offline RL.

## Method Summary
Re:Frame works in two stages: first, an autoencoder compresses expert trajectories (returns, observations, actions) into a compact latent space; second, during training and evaluation, the policy retrieves contextually relevant expert actions via nearest-neighbor search in this latent space and fuses them with the main policy's output through additive correction. The method requires no environment interaction and no modifications to the backbone architecture, making it broadly applicable to existing offline RL algorithms.

## Key Results
- Re:Frame improves Decision Transformer by +10.7 normalized points on Hopper-MR with only 60 expert trajectories
- Performance degrades sharply as AMB size decreases below 60 trajectories on Hopper-MR
- Swapping to expert AMB only at evaluation time yields no improvement, confirming training-time retrieval learning is essential

## Why This Works (Mechanism)

### Mechanism 1
- Compressing expert trajectories via autoencoding preserves task-relevant information while enabling efficient nearest-neighbor retrieval
- Expert triplets (Rt, ot, at) are encoded separately, concatenated, projected through a linear bottleneck, and reconstructed by three independent decoders with separate optimizers
- Core assumption: The autoencoder bottleneck captures sufficient structure for similarity-based retrieval to match contextually relevant expert experiences
- Break condition: If the latent space fails to disentangle task-relevant features, nearest-neighbor retrieval returns irrelevant expert states

### Mechanism 2
- Content-based associative retrieval allows the policy to access contextually relevant expert guidance without modifying the backbone architecture
- At each timestep, the agent encodes current (Rt, ot) into a query vector, projects it into the AMB latent space, and retrieves the nearest expert embedding via L2 distance minimization
- Core assumption: L2 similarity in the learned latent space correlates with behavioral relevance—similar states should receive similar expert-informed corrections
- Break condition: If the projection layer misaligns query and memory latent spaces, retrieval becomes near-random

### Mechanism 3
- Additive fusion of dataset-driven and expert-derived action representations enables the policy to leverage expert knowledge while preserving learned behavioral patterns
- The DT backbone produces action embedding a*t from the trajectory sequence; the expert correction vector a"t is added: at = a*t + a"t
- Core assumption: Additive combination in embedding space preserves meaningful information from both sources without destructive interference
- Break condition: If expert corrections are systematically misaligned with DT embeddings, additive fusion introduces noise

## Foundational Learning

- Concept: Offline Reinforcement Learning and Decision Transformers
  - Why needed here: Re:Frame builds on DT's sequence-modeling formulation; understanding return-conditioned policy generation is essential to grasp where AMB augments the pipeline
  - Quick check question: Can you explain how DT conditions action prediction on returns-to-go and why this limits performance on suboptimal datasets?

- Concept: Autoencoder Representation Learning
  - Why needed here: The AMB is constructed via an autoencoder; understanding encoder-decoder dynamics, reconstruction loss, and bottleneck design clarifies memory quality
  - Quick check question: How does minimizing reconstruction loss ensure the latent space preserves information relevant for downstream retrieval?

- Concept: Nearest-Neighbor Retrieval in Latent Space
  - Why needed here: Retrieval is the core of Re:Frame's associative memory; understanding distance metrics and projection alignment is critical for debugging performance
  - Quick check question: Why might L2 distance in a learned latent space fail to capture behavioral similarity, and how could you diagnose this?

## Architecture Onboarding

- Component map: Autoencoder Encoders (3) -> Linear Projection -> AMB -> Query Projection -> Nearest-Neighbor Retrieval -> Autoencoder Decoders (3) -> Correction Linear Layer -> DT Backbone -> Additive Fusion -> Action Head

- Critical path:
  1. Train autoencoder on expert trajectories; freeze weights; populate AMB with latent vectors
  2. At each timestep, encode current (Rt, ot), project to query, retrieve nearest neighbor from AMB
  3. Decode retrieved vector to expert action candidate; transform to correction vector
  4. Process trajectory through DT to obtain action embedding; add correction vector
  5. Pass fused embedding through Action Head; compute loss against ground-truth action

- Design tradeoffs:
  - AMB size vs. coverage: Table 2 shows Hopper-MR collapses from 69.7 → 3.0 as AMB shrinks from 60 → 30, while Walker2d-M remains stable
  - Expert vs. dataset AMB: Table 1 shows "dataset AMB" matches DT baseline, confirming expert quality—not quantity—drives gains
  - Fusion strategy: Additive fusion preserves backbone but may underutilize expert signal if corrections are small

- Failure signatures:
  - Sharp performance drop as AMB size decreases (Table 2, Hopper-MR): indicates insufficient expert coverage in latent space
  - No improvement when swapping to expert AMB only at evaluation (Table 1): indicates policy hasn't learned to use retrieval signals
  - Performance matching baseline with dataset AMB: confirms expert quality is necessary

- First 3 experiments:
  1. Replicate Table 1 on Hopper-M and Walker2d-MR: Compare DT baseline, fine-tuned DT, Re:Frame (expert train+eval), and Re:Frame (swap expert @ eval)
  2. Replicate Table 2 AMB size ablation on Hopper-MR: Reduce AMB from 60 → 45 → 30 trajectories
  3. Probe retrieval alignment: Visualize query and AMB latent distributions (t-SNE or PCA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Re:Frame be effectively integrated with non-transformer offline RL algorithms, such as IQL or TD3+BC?
- Basis in paper: Section 6 states validation is "currently limited to DT" and applying to other algorithms would "better demonstrate generality"
- Why unresolved: Only evaluated with Decision Transformer architecture
- What evidence would resolve it: Successful implementation and benchmarking within IQL or TD3+BC on same D4RL tasks

### Open Question 2
- Question: Why does Re:Frame underperform the baseline on the Hopper Medium-Replay dataset?
- Basis in paper: Section 5 and Table 1 show lower scores on Hopper-MR; authors hypothesize "broader behavior-policy distribution" but don't confirm mechanism
- Why unresolved: Success on other tasks but specific failure mode on Medium-Replay datasets unaddressed
- What evidence would resolve it: Ablation study analyzing retrieval quality on heterogeneous datasets

### Open Question 3
- Question: Can the Associative Memory Buffer guide exploration and accelerate learning in online or hybrid RL settings?
- Basis in paper: Section 6 lists applying Re:Frame in "online or hybrid settings" as promising direction
- Why unresolved: Current work focuses exclusively on offline setting
- What evidence would resolve it: Experiments showing online agent querying AMB achieves higher sample efficiency

## Limitations
- Underperforms baseline on Hopper Medium-Replay dataset, suggesting retrieval mechanism struggles with highly heterogeneous data
- Performance sensitive to AMB size, with sharp drops below 60 trajectories on some tasks
- Only validated with Decision Transformer architecture, limiting generality claims

## Confidence
- Core claims (empirical improvements): High
- Autoencoder's role in preserving retrieval-relevant structure: Medium
- Additive fusion strategy's optimality: Low

## Next Checks
1. Visualize t-SNE/PCA of AMB latent space and query embeddings to verify retrieval relevance correlates with performance
2. Test alternative fusion strategies (gating, multiplicative) in place of additive correction to assess robustness
3. Conduct systematic sweep of AE latent dimension size to quantify tradeoff between compression and retrieval fidelity