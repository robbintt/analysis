---
ver: rpa2
title: Intent-Aware Neural Query Reformulation for Behavior-Aligned Product Search
arxiv_id: '2507.22213'
source_url: https://arxiv.org/abs/2507.22213
tags:
- query
- intent
- reformulation
- search
- buyer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the challenge of query reformulation in e-commerce
  search by developing a data-driven framework that captures and classifies buyer
  intent from query logs. Using sequence mining techniques, the authors extract intent-rich
  query pairs from in-session and cross-session behaviors, then train a neural machine
  translation model conditioned on three intent types: Same, Similar, and Inspired
  Intent.'
---

# Intent-Aware Neural Query Reformulation for Behavior-Aligned Product Search

## Quick Facts
- arXiv ID: 2507.22213
- Source URL: https://arxiv.org/abs/2507.22213
- Reference count: 13
- Primary result: A neural query reformulation framework conditioned on three intent types (Same, Similar, Inspired) that improves coverage, recall, and precision in e-commerce search by mining user behavior signals.

## Executive Summary
This work introduces a data-driven framework for intent-aware query reformulation in e-commerce search. By mining query reformulation pairs from user engagement logs using both in-session and cross-session behaviors, the authors develop a neural machine translation model conditioned on three explicit intent types. The approach captures fine-grained intent signals from behavioral cues and trains a unified model capable of handling different reformulation strategies. The framework demonstrates improved performance across standard metrics and domain-specific measures, particularly in handling diverse reformulation patterns and supporting multiple downstream applications.

## Method Summary
The framework extracts query reformulation pairs from 4 weeks of buyer search logs using three mining strategies: in-session n-hop reformulations where target queries lead to engagement, cross-session co-engaged queries (shared clicked items across sessions), and cross-session 1-hop co-clicked queries. Pairs are filtered based on categorical alignment, recall similarity, and query length compatibility. Intent labels (Same, Similar, Inspired) are assigned based on category alignment and lexical overlap. A sequence-to-sequence NMT model with intent-type tags prepended to input is trained to learn intent-specific reformulation strategies. The model is evaluated against baselines including token-drop, BiLSTM variants, and knowledge-graph approaches.

## Key Results
- The intent-aware model achieves higher coverage and token-level recall/precision compared to baselines across all rewrite types
- The lightweight Transformer architecture ($\theta_T$) demonstrates strong performance in RATS (Rewrite Type Agreement Score) and type-weighted metrics
- Cross-session mining helps mitigate contextual bias found in single-session data, improving coverage for sparse queries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grounding query reformulation in user engagement signals rather than lexical similarity may produce more relevant rewrites.
- **Mechanism:** The framework identifies "successful" query pairs by observing if a user transitioned from a source query to a target query that resulted in meaningful engagement (click or purchase). By training on these successful paths, the model learns to replicate transitions that historically led to results.
- **Core assumption:** Assumes that user query modifications leading to engagement are reliable proxies for intent satisfaction and semantic relevance.
- **Break condition:** If engagement signals are noisy (e.g., accidental clicks or bot traffic), the model may learn spurious correlations.

### Mechanism 2
- **Claim:** Conditioning a unified neural model on explicit intent tags allows it to learn distinct reformulation strategies within a single architecture.
- **Mechanism:** The architecture prepends intent-specific tokens (e.g., tags for *Same*, *Similar*, *Inspired*) to the source query during training. This forces the model to attend to the desired "transformation type" conditional on the tag.
- **Core assumption:** Assumes that the three defined intent buckets are sufficiently distinct and mutually exclusive to allow the model to separate their transformation logics.
- **Break condition:** If the input tags are misapplied during data mining, the model will conflate strategies.

### Mechanism 3
- **Claim:** Cross-session co-click mining helps mitigate "contextual bias" found in single-session data, improving coverage for sparse queries.
- **Mechanism:** In addition to tracking a single user's session, the system identifies query pairs where different users clicked the same items in separate sessions. This links queries that are semantically related but lexically different.
- **Core assumption:** Assumes that if two distinct queries lead to clicks on the same product, they share a high degree of semantic intent.
- **Break condition:** If items are generic, co-clicks may link unrelated intents, causing false-positive reformulations.

## Foundational Learning

- **Concept: Neural Machine Translation (NMT) for Query Reformulation**
  - **Why needed here:** The paper frames query rewriting as a translation task: translating a "failed" or "vague" query into a "successful" or "specific" one.
  - **Quick check question:** Can you explain why a sequence-to-sequence model is preferred over a simple synonym replacement dictionary for this task? (Hint: Context and sequence order).

- **Concept: Behavioral Sequence Mining**
  - **Why needed here:** The training data is not human-labeled but mined from logs. You must understand how to extract valid signal (clicks/purchases) from noise.
  - **Quick check question:** What is the difference between "in-session" behavior and "cross-session" behavior, and why might the latter help with cold-start problems?

- **Concept: Intent Taxonomies (Informational vs. Transactional)**
  - **Why needed here:** The system relies on a taxonomy (Same, Similar, Inspired) to function. Understanding standard IR intent types helps contextualize why these specific three buckets were chosen.
  - **Quick check question:** Which intent bucket (*Same*, *Similar*, or *Inspired*) is most likely to handle a "null result" recovery scenario where the original query was overly specific?

## Architecture Onboarding

- **Component map:** Data Mining Layer -> Post-Processing/Filtering -> Model Core (Seq2Seq NMT with intent tags) -> Inference Layer
- **Critical path:** The quality of the **Mining Layer** dictates model performance. If the co-click graph creates false edges, the NMT model will hallucinate bad rewrites regardless of its architecture.
- **Design tradeoffs:** Precision vs. Exploration (*Same Intent* for precision, *Inspired Intent* for discovery); Unified vs. Specialized Models (single model for multiple apps vs. specialized models).
- **Failure signatures:** Drift in *Inspired Intent* suggestions; Looping (model generating same query); Overspecification (adding too many specific attributes).
- **First 3 experiments:**
  1. Data Quality Audit: Sample 100 query pairs from mining output for each intent bucket and manually verify correctness.
  2. A/B Baseline Comparison: Deploy heuristic baseline against trained Transformer on known "null query" inputs and measure Recall Recovery Rate.
  3. Rewrite Type Analysis: Run RATS evaluation to check if model generates expected "Subset" and "Superset" behaviors.

## Open Questions the Paper Calls Out
- How can the model be refined to effectively handle ambiguous queries and rare reformulation types where training data is sparse?
- To what extent can Retrieval-Augmented Generation (RAG) systems enhance the proposed NMT model by leveraging external knowledge?
- How can the framework be extended to process multimodal inputs, such as images or voice search, to broaden its applicability?

## Limitations
- The framework depends critically on the quality of mined behavioral signals, with no quantification of noise level in engagement-based mining
- Cross-session co-click strategy may produce spurious edges for generic products, causing intent drift in the *Inspired* bucket
- The assumption that three intent categories are sufficient to capture full diversity of user reformulation behavior may not hold for edge cases

## Confidence
- **High Confidence**: Basic architecture (seq2seq + intent conditioning) and standard metric reporting are well supported
- **Medium Confidence**: Intent-specific conditioning leading to improved RATS and balanced rewrite type distribution are supported by internal metrics but lack external validation
- **Low Confidence**: Claims about robustness to generic co-clicks and sufficiency of three-bucket taxonomy are largely untested

## Next Checks
1. **Mining Quality Audit**: Sample and manually annotate 200 query pairs from each intent bucket (Same, Similar, Inspired). Compute precision and recall of intent classification to estimate pipeline noise before model training.

2. **Intent Drift Analysis**: Generate top-5 rewrites for 100 randomly selected *Inspired Intent* source queries. Categorize the semantic relationship between source and top-1 target (e.g., valid pivot, irrelevant drift, category mismatch). Measure the percentage of valid pivots.

3. **Cross-Session Noise Filter Test**: Simulate a stricter co-click filtering step (e.g., require at least 3 shared items and same top-level category). Retrain a small model and compare RATS and type-weighted recall/precision to quantify the impact of reducing spurious edges.