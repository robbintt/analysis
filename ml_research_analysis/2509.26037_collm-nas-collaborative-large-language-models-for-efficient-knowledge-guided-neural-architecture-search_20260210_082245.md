---
ver: rpa2
title: 'CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided
  Neural Architecture Search'
arxiv_id: '2509.26037'
source_url: https://arxiv.org/abs/2509.26037
tags:
- search
- architectures
- architecture
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoLLM-NAS, a novel collaborative framework
  that integrates Large Language Models (LLMs) with two-stage Neural Architecture
  Search (NAS). The framework employs two complementary LLMs - a stateful Navigator
  LLM for adaptive search strategy generation and a stateless Generator LLM for synthesizing
  high-quality architectures - coordinated by a dedicated Coordinator module.
---

# CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search

## Quick Facts
- arXiv ID: 2509.26037
- Source URL: https://arxiv.org/abs/2509.26037
- Authors: Zhe Li; Zhiwei Lin; Yongtao Wang
- Reference count: 40
- Primary result: CoLLM-NAS achieves new state-of-the-art results on ImageNet and NAS-Bench-201, outperforming existing NAS methods while reducing search costs by 3-10x.

## Executive Summary
This paper introduces CoLLM-NAS, a collaborative framework that integrates Large Language Models (LLMs) with two-stage Neural Architecture Search (NAS). The framework employs two complementary LLMs - a stateful Navigator LLM for adaptive search strategy generation and a stateless Generator LLM for synthesizing high-quality architectures - coordinated by a dedicated Coordinator module. By leveraging both the LLMs' inherent knowledge of neural architectures and progressive insights from iterative feedback and historical trajectories, CoLLM-NAS consistently enhances performance and efficiency across various two-stage NAS methods (OFA, SPOS, AutoFormer) in diverse search spaces while significantly reducing search costs compared to baselines.

## Method Summary
CoLLM-NAS decomposes the neural architecture search process into two complementary LLM roles: a stateful Navigator LLM that analyzes historical performance data to formulate natural language search strategies, and a stateless Generator LLM that translates these strategies into concrete architecture parameters. The Navigator maintains a history of trajectories to learn the performance landscape and avoid loops, while the Generator operates only on the current strategy without access to past failures to prevent overfitting. A Coordinator module orchestrates the process, holding the visited archive, checking legality, evaluating performance via a pre-trained supernet, and formatting history. This "trajectory → strategy → solution" pipeline creates an efficient exploration-exploitation balance while leveraging LLMs' inherent architectural priors for a warm start.

## Key Results
- CoLLM-NAS outperforms existing NAS methods and conventional search algorithms, achieving new state-of-the-art results on ImageNet and NAS-Bench-201
- The framework consistently enhances performance and efficiency across various two-stage NAS methods (OFA, SPOS, AutoFormer) in diverse search spaces
- CoLLM-NAS significantly reduces search costs by 3-10x compared to baselines while maintaining or improving accuracy

## Why This Works (Mechanism)

### Mechanism 1
Decomposing the search process into high-level strategic guidance and low-level architecture synthesis improves search efficiency and convergence compared to single-model approaches. A "Navigator" LLM analyzes historical performance data to formulate a natural language strategy (e.g., "increase depth in early stages"), while a separate "Generator" LLM translates this strategy into concrete architecture parameters. This creates a "trajectory → strategy → solution" pipeline. The search space is complex enough that directing the search via semantic strategy is more efficient than random or strictly syntactic mutation. Evidence: Figure 4(a) shows CoLLM-NAS consistently outperforms the single-LLM variant (SiLLM-NAS) across all datasets.

### Mechanism 2
Asymmetric memory retention (Stateful Navigator, Stateless Generator) optimizes the exploration-exploitation balance. The Navigator maintains a history of trajectories ($H$) to learn the performance landscape and avoid loops (Stateful). The Generator operates only on the current strategy $S_t$ without access to past failures (Stateless), preventing it from overfitting to specific syntax patterns or accumulating noise from invalid previous generations. LLMs suffer from "noise accumulation" or syntax overfitting when asked to generate based on a long history of specific architectural failures. Evidence: "Retaining the Generator LLM's memory induces progressive noise accumulation, leading to substantial invalid architectures..."

### Mechanism 3
LLMs possess inherent "architectural priors" that allow them to rank architectures better than random chance, providing a "warm start" for the search. Pre-trained on technical literature, LLMs internalize design principles (e.g., "residual connections usually help"). By querying the LLM to rank candidates before evaluation, the initial search distribution is biased toward high-potential regions without explicit training. The LLM's pre-training corpus contained sufficient high-quality information linking architecture patterns to performance. Evidence: Proof-of-concept experiment shows LLMs achieve a Mean Kendall's $\tau$ of 0.89 on ranking, demonstrating "non-trivial comprehension of neural architecture performance patterns."

## Foundational Learning

- **Concept: Two-Stage NAS & Supernets**
  - **Why needed here:** CoLLM-NAS acts as a drop-in replacement for the search phase in two-stage NAS. You cannot deploy this without understanding that a Supernet must be pre-trained (Stage 1) to allow for rapid weight inheritance during the LLM search (Stage 2).
  - **Quick check question:** Can you explain why evaluating an architecture in this framework is fast compared to training from scratch? (Answer: It inherits pre-trained weights from the supernet).

- **Concept: Exploration vs. Exploitation**
  - **Why needed here:** The "Stateful/Stateless" design is explicitly built to manage this trade-off. Understanding that the Navigator exploits the historical trajectory while the Generator explores based on current directives is key to debugging the system.
  - **Quick check question:** Which component is responsible for ensuring the search doesn't get stuck in a local optimum (exploration), and which ensures we refine good solutions (exploitation)?

- **Concept: In-Context Learning (ICL)**
  - **Why needed here:** The system relies on prompting, not fine-tuning. The Navigator "learns" solely by reading the history of previous results in its context window. If the context window fills or the history format is confusing, the mechanism fails.
  - **Quick check question:** How does the Navigator LLM improve its strategy over time without updating its model weights? (Answer: By conditioning on the historical trajectory provided in the prompt).

## Architecture Onboarding

- **Component map:**
  - User/Coordinator -> Navigator LLM (Stateful) -> Generator LLM (Stateless) -> Supernet (External)

- **Critical path:**
  1. **Prompt Engineering:** You must precisely define the search space encoding (e.g., MobileNet depth/kernel config) to the Generator so it doesn't hallucinate invalid layers.
  2. **History Formatting:** The Navigator needs a structured summary of (Arch, Params, Accuracy) to infer patterns. Poor summarization leads to poor strategies.

- **Design tradeoffs:**
  - **LLM Inference vs. Evaluation Cost:** The paper claims 3-10x reduction in search cost, but this relies on Supernet evaluation being significantly faster than LLM inference time.
  - **Invalidity Rate:** While better than code-generation methods, constrained decoding is not strictly enforced here; the Coordinator must be robust to filtering out invalid JSON or constraint-violating architectures.

- **Failure signatures:**
  - **"Mode Collapse":** Navigator keeps suggesting the same strategy; check if the history feedback is actually changing or if the prompt encourages repetitive behavior.
  - **High Invalidity Rate:** Generator produces architectures incompatible with the supernet; usually a prompt clarity issue regarding the search space boundaries.
  - **No Improvement over Random:** Suggests the LLM lacks the necessary "prior" knowledge for this specific domain, or the Supernet evaluation is too noisy.

- **First 3 experiments:**
  1. **Validation of Priors (Reproduce Section 3.1):** Before complex search, verify your chosen LLM can rank a set of 10 random architectures from NAS-Bench-201 with Kendall's $\tau > 0.5$. If it fails, the LLM is insufficient for the Navigator role.
  2. **Memory Ablation (Reproduce Section 4.4):** Run a simple search on CIFAR-100 with both LLMs Stateless vs. Navigator Stateful. Verify that Stateful Navigator improves results on complex datasets.
  3. **Budget Scaling:** Run a full search on ImageNet (MobileNet space) with a strict budget (e.g., 250 architectures) to verify if the system can beat the baseline (OFA) within a fixed, low budget.

## Open Questions the Paper Calls Out
None

## Limitations
- The claimed 3-10x efficiency gains rely heavily on the assumption that Supernet evaluation is orders of magnitude faster than LLM inference time, which is not explicitly validated
- Performance on more diverse or complex search spaces beyond ImageNet and NAS-Bench-201 remains untested
- The Coordinator module's filtering of invalid architectures introduces a potential bottleneck not fully characterized in terms of computational overhead or failure rates

## Confidence
- **High Confidence:** The dual-LLM decomposition (Navigator + Generator) improves search efficiency compared to single-model approaches, supported by ablation studies
- **Medium Confidence:** The asymmetric memory design (Stateful Navigator, Stateless Generator) optimizes exploration-exploitation balance, though the specific claim about "noise accumulation" is based on qualitative observation
- **Medium Confidence:** LLMs possess inherent architectural priors enabling effective ranking, but this is validated primarily on NAS-Bench-201 rather than diverse architecture types

## Next Checks
1. **Supernet vs. LLM Cost Validation:** Measure and report the actual inference time for both Supernet evaluation and LLM generation steps. Verify that the claimed 3-10x efficiency gains hold when accounting for total wall-clock time including LLM overhead.

2. **Robustness to Invalid Architectures:** Systematically characterize the rate and nature of invalid architectures produced by the Generator LLM across different search spaces. Test whether the Coordinator's filtering mechanism introduces significant computational overhead or search bias.

3. **Cross-Space Generalization:** Test CoLLM-NAS on at least two additional search spaces not covered in the current evaluation (e.g., NAS-Bench-301 or a different architecture family like EfficientNet variants). Verify whether the claimed "architectural priors" generalize beyond the MobileNet space or if performance degrades significantly.