---
ver: rpa2
title: Conditional Unigram Tokenization with Parallel Data
arxiv_id: '2507.07824'
source_url: https://arxiv.org/abs/2507.07824
tags:
- pairedsp
- sptgt
- pairedspm
- language
- tokenization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for learning a target-language tokenizer
  conditioned on source-language tokens from parallel data. The approach extends unigram
  tokenization by maximizing the probability of target tokens given aligned source
  tokens.
---

# Conditional Unigram Tokenization with Parallel Data

## Quick Facts
- arXiv ID: 2507.07824
- Source URL: https://arxiv.org/abs/2507.07824
- Reference count: 40
- Primary result: Conditional unigram tokenization with parallel data improves language modeling perplexity but not machine translation quality due to quadratic scaling data efficiency bottleneck.

## Executive Summary
This paper introduces a method for learning a target-language tokenizer conditioned on source-language tokens from parallel data. The approach extends unigram tokenization by maximizing the probability of target tokens given aligned source tokens. The method was evaluated on four language pairs across different families and resource levels using intrinsic metrics and downstream tasks including machine translation and language modeling. Intrinsic evaluations showed comparable statistical properties to standard unigram tokenizers, with no consistent improvements in machine translation quality. However, the conditional tokenizer achieved consistent perplexity reductions in language modeling tasks. The authors hypothesize that the quadratic scaling of conditional probability estimation with respect to vocabulary size creates a data efficiency bottleneck, suggesting that alternative parameterizations may be necessary for practical cross-lingual tokenization.

## Method Summary
The paper extends unigram tokenization by conditioning target token probabilities on aligned source tokens from parallel data. The method initializes a target vocabulary with all character spans, then iteratively prunes tokens based on mutual information with source vocabulary using co-occurrence counts from word alignments. Two inference modes are proposed: PairedSP requires source context at inference, while PairedSPM marginalizes over source tokens for source-free use. The approach aims to learn tokenizations that preserve cross-lingual semantic correspondence by favoring target tokens with reliable statistical alignments to source tokens rather than just high monolingual frequency.

## Key Results
- Conditional tokenizer achieved consistent perplexity reductions in language modeling tasks despite no MT improvements
- Intrinsic metrics (parity, fertility, one-to-one alignment) showed comparable performance to standard unigram tokenizers
- Quadratic scaling of conditional probability estimation with vocabulary size creates data efficiency bottleneck
- Approximately 28M parallel examples would be required to match standard unigram fertility performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditioning target token probabilities on aligned source tokens enables vocabulary selection that preserves cross-lingual semantic correspondence.
- Mechanism: The model replaces unconditional unigram probabilities p(t) with conditional probabilities p(t|S) computed from co-occurrence counts c(t, s) between target tokens and source tokens in parallel sentences. Vocabulary pruning retains tokens maximizing mutual information I(t, V_src) with the source vocabulary, favoring target tokens that have reliable statistical correspondences rather than just high monolingual frequency.
- Core assumption: Co-occurrence statistics from parallel data reflect meaningful semantic alignments between subword units across languages.
- Evidence anchors:
  - [abstract] "conditioning target token probabilities on source-language tokens from parallel data... learns a target tokenizer that maximizes cross-lingual semantic alignment"
  - [section 3, Equation 4] "We keep the subwords with the highest mutual information with the source tokens until the desired vocabulary size is reached"
  - [corpus] Limited direct evidence; corpus neighbors focus on morphological tokenization quality rather than cross-lingual conditioning mechanisms.
- Break condition: Parallel data is too sparse for reliable co-occurrence estimation, causing vocabulary to collapse toward character-level tokens.

### Mechanism 2
- Claim: Marginalization over source tokens enables inference without parallel context while preserving alignment-informed token statistics.
- Mechanism: By computing p(t) = Σ_s∈V_src p(t, s), the tokenizer derives unconditional probabilities from the conditional co-occurrence table. This allows deployment in monolingual settings while the vocabulary composition and probability estimates still reflect cross-lingual alignment signals from training.
- Core assumption: The co-occurrence table c(t, s) captures stable cross-lingual patterns that generalize when marginalized.
- Evidence anchors:
  - [section 3, Equation 6] "p(t) = Σ_si∈V_src c(t, si) / Σ_tj Σ_sk c(tj, sk)... resembles an unconditional Unigram tokenizer but with tokens counted differently"
  - [section 5.1] "PairedSPM outperforms PairedSP... PairedSPM's probability estimation more closely resembles that of SPtgt"
  - [corpus] No direct corpus validation of marginalization approach for cross-lingual tokenization.
- Break condition: Marginalized probabilities diverge significantly from true monolingual distributions when source vocabulary is small or alignment quality is poor.

### Mechanism 3
- Claim: Quadratic scaling of the co-occurrence table creates a data efficiency bottleneck that limits vocabulary quality.
- Mechanism: The conditional probability table c(t, s) has |V_tgt| × |V_src| entries. Doubling vocabulary size quadruples table entries while training data remains fixed, causing sparsity and unreliable estimates. This particularly harms low-resource scenarios where parallel data is scarce.
- Core assumption: Table sparsity directly translates to degraded vocabulary selection and tokenization quality.
- Evidence anchors:
  - [abstract] "quadratic scaling of conditional probability estimation with respect to the vocabulary size creates a data efficiency bottleneck"
  - [section 6] "approximately 28M examples would be required to match unigram fertility and 4M examples for comparable one-to-one alignment performance"
  - [corpus] Indirectly supported by corpus work on multilingual tokenization challenges for low-resource languages (Indian languages paper notes skew toward high-resource languages).
- Break condition: Very large parallel corpora (>28M examples) or alternative parameterizations that reduce the quadratic dependency.

## Foundational Learning

- Concept: Unigram tokenization (Kudo, 2018)
  - Why needed here: This paper extends the unigram framework by conditioning on source tokens. Understanding the base algorithm—vocabulary initialization with all character spans, iterative pruning based on probability, and Viterbi decoding for tokenization—is essential.
  - Quick check question: Can you explain how unigram tokenization differs from BPE in vocabulary learning?

- Concept: Word alignment and co-occurrence statistics
  - Why needed here: The method relies on Eflomal for word alignment and builds a co-occurrence table c(t, s). Understanding alignment quality's impact on downstream statistics is critical.
  - Quick check question: What happens to alignment quality when parallel data contains noisy or misaligned sentence pairs?

- Concept: Perplexity per byte for language model evaluation
  - Why needed here: The paper reports consistent perplexity reductions despite no MT improvement. Perplexity per byte normalizes across vocabulary sizes, enabling fair comparison.
  - Quick check question: Why is perplexity per byte preferred over token-level perplexity when comparing tokenizers with different vocabulary sizes?

## Architecture Onboarding

- Component map: Whitespace/punctuation preprocessing -> Eflomal alignment -> Co-occurrence table c(t,s) -> Mutual information pruning -> Viterbi tokenization
- Critical path: Data → Eflomal alignment → Table initialization → EM-style count updates → MI-based pruning → Final vocabulary → Tokenization (Viterbi with conditional scores)
- Design tradeoffs:
  - Data efficiency vs. alignment quality: Pre-aligning with Eflomal reduces memory but ties quality to alignment accuracy
  - PairedSP vs. PairedSPM: Source context at inference improves alignment metrics but requires parallel data; marginalization enables monolingual use
  - Vocabulary size: Larger vocabularies improve intrinsic metrics but exacerbate quadratic scaling
- Failure signatures:
  - Character-only tokenization: Indicates table sparsity collapsed vocabulary (observed in fra→ita 8k PairedSPM: fertility 5.61)
  - High unaligned ratio: Source tokens with no target correspondences suggest alignment or data quality issues
  - High variance in MT scores: Tokenizer instability across random seeds (noted in Section 5.2)
- First 3 experiments:
  1. Replicate PairedSP vs. SP_tgt on French-Italian (1M examples) measuring parity, fertility, and one-to-one alignment to validate implementation.
  2. Ablate vocabulary sizes (8k, 16k, 32k) on Czech-Ukrainian to observe quadratic scaling effects on low-resource pair.
  3. Compare PairedSPM inference (marginalized) vs. PairedSP (source-conditioned) on language modeling perplexity to confirm cross-lingual transfer benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can alternative parameterizations of the conditional probability estimation $p(t|s)$, such as neural network-based scorers or low-rank approximations, overcome the quadratic data efficiency bottleneck identified in count-based models?
- Basis in paper: [explicit] The authors hypothesize that "quadratic scaling of conditional probability estimation with respect to the vocabulary size creates a data efficiency bottleneck" and explicitly conclude that "alternative parameterizations may be necessary."
- Why unresolved: The current work relies on count-based co-occurrence tables which scale quadratically ($|V_{src}| \times |V_{tgt}|$). The paper did not test function approximators that might decouple model capacity from data sparsity.
- What evidence would resolve it: Demonstrating that a parameterized model (e.g., a learned embedding scorer) achieves higher fertility or alignment scores than the count-based PairedSP baseline on the same low-resource datasets (e.g., German-Upper Sorbian).

### Open Question 2
- Question: Does scaling parallel data to the estimated threshold of 28M examples enable conditional unigram tokenization to match the intrinsic quality (fertility/parity) of standard unigram tokenizers?
- Basis in paper: [explicit] The authors estimate that "approximately 28M examples would be required to match unigram fertility," but they only experimented with datasets up to 1M examples.
- Why unresolved: The experiments were limited by available data resources. It remains unknown if the method is fundamentally flawed or simply data-starved in the evaluated settings.
- What evidence would resolve it: Training the conditional tokenizer on a high-resource pair (e.g., English-French) with >28M aligned sentence pairs and comparing intrinsic metrics against the standard SentencePiece baseline.

### Open Question 3
- Question: Why does conditional tokenization reduce perplexity in language modeling while failing to improve machine translation quality?
- Basis in paper: [inferred] The authors note consistent perplexity reductions in LM but "no improvements in machine translation quality." This discrepancy suggests the tokenizer captures useful statistical regularities for encoding but fails to provide the semantic alignment benefits hypothesized to aid the translation task.
- Why unresolved: The paper identifies the phenomenon but does not investigate if the failure in MT is due to poor alignment propagation, optimization instability, or a mismatch between the tokenizer's training objective and the MT model's needs.
- What evidence would resolve it: An analysis of the cross-attention maps in the MT models to see if improved "one-to-one" token alignment (shown in intrinsic eval) actually correlates with better alignment distributions in the Transformer layers.

## Limitations

- Quadratic scaling of the co-occurrence table creates a fundamental data efficiency bottleneck that cannot be resolved within the current framework
- Method relies entirely on Eflomal word alignments, with alignment errors directly propagating to degraded vocabulary quality
- Evaluation shows intrinsic metric improvements but no MT quality gains, suggesting the approach may not provide semantic benefits for translation

## Confidence

- **High Confidence**: The quadratic scaling problem and its impact on data efficiency. This is mathematically straightforward and directly supported by empirical observations.
- **Medium Confidence**: The mechanism by which parallel conditioning improves cross-lingual semantic alignment. While the mutual information formulation is sound, semantic benefits are only indirectly measured through alignment metrics.
- **Low Confidence**: The claim that the approach would become competitive with 28M examples. This is based on extrapolation from current results and assumes linear scaling behavior.

## Next Checks

1. **Alignment Quality Ablation Study**: Systematically vary Eflomal hyperparameters and alignment quality on a single language pair (e.g., Czech-Ukrainian) to quantify the relationship between alignment F1 score and downstream tokenization quality. This would validate whether alignment errors are the primary failure mode.

2. **Sparse Representation Implementation**: Implement a sparse tensor or hash-based representation for the co-occurrence table to enable experiments with larger vocabulary sizes (>32k) without memory constraints. This would test whether the quadratic scaling limitation is truly fundamental or can be circumvented through computational optimization.

3. **Cross-lingual Transfer Experiment**: Train the conditional tokenizer on high-resource pairs (e.g., French-Italian) and evaluate zero-shot on low-resource pairs (e.g., German-Upper Sorbian). This would test whether the alignment-informed vocabulary generalizes across language families, providing evidence for the cross-lingual transfer hypothesis.