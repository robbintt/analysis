---
ver: rpa2
title: 'VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic
  Services through Natural Language'
arxiv_id: '2505.00989'
source_url: https://arxiv.org/abs/2505.00989
tags:
- query
- vessel
- language
- traffic
- maritime
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of real-time vessel risk identification
  in Vessel Traffic Services (VTS), where operators face cognitive overload from complex
  spatiotemporal data and domain-specific rules. The authors propose VTS-LLM, the
  first domain-adaptive LLM agent that reformulates vessel identification as a knowledge-augmented
  Text-to-SQL task, combining structured vessel databases with external maritime knowledge.
---

# VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language

## Quick Facts
- arXiv ID: 2505.00989
- Source URL: https://arxiv.org/abs/2505.00989
- Authors: Sijin Sun; Liangbin Zhao; Ming Deng; Xiuju Fu
- Reference count: 24
- Key outcome: VTS-LLM achieves 77.80% accuracy on operational-style queries, outperforming general-purpose and SQL-specialized baselines by 12-35% in vessel risk identification for Vessel Traffic Services.

## Executive Summary
This work addresses the challenge of real-time vessel risk identification in Vessel Traffic Services (VTS), where operators face cognitive overload from complex spatiotemporal data and domain-specific rules. The authors propose VTS-LLM, the first domain-adaptive LLM agent that reformulates vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. VTS-LLM introduces four key innovations: NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms. Experiments on a curated benchmark dataset show VTS-LLM achieves 77.80% accuracy under operational-style queries, outperforming both general-purpose and SQL-specialized baselines by 12-35%. Notably, the work provides the first empirical evidence that linguistic style variation significantly impacts Text-to-SQL performance, with VTS-LLM demonstrating superior robustness to concise, informal query styles representative of real-world VTS scenarios.

## Method Summary
VTS-LLM reformulates vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. The system processes natural language queries through a four-stage pipeline: hierarchical NER-based relational reasoning to resolve maritime entity ambiguities, agent-based domain knowledge injection using RAG to incorporate navigational rules, semantic algebra intermediate representation to decompose SQL generation into structured algebraic steps, and a query rethink validation loop to correct errors. The framework operates on a custom MySQL database with four tables (ship_ais, ship_ais_quarter, shp_data, warn_single) and includes specialized metrics that penalize over-selection in safety-critical contexts.

## Key Results
- VTS-LLM achieves 77.80% accuracy on operational-style queries, outperforming GPT-4o, Claude-3.7, and SQL-specialized baselines by 12-35%.
- The query rethink mechanism provides a 7.58 percentage point accuracy improvement over non-validated SQL generation.
- VTS-LLM demonstrates superior robustness to linguistic style variation, maintaining higher accuracy on command-style queries compared to formal queries where other models excel.

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical NER-based Relational Reasoning
- Claim: Hierarchical named entity recognition reduces ambiguity in maritime entity linking compared to flat NER schemes.
- Mechanism: The system identifies maritime entities (geographical regions, named types, navigational facilities) at multiple semantic granularities, then generates verification queries against the database to confirm entity-database mappings before SQL generation.
- Core assumption: Maritime queries contain ambiguous or overlapping entity references (e.g., ship names, area codes) that require iterative database validation to resolve correctly.
- Evidence anchors:
  - [abstract]: "Our framework incorporates NER-based relational reasoning... to enhance domain grounding and context-aware understanding."
  - [Section III.A]: "Such hierarchical annotation provides structured representations for entities, significantly mitigating ambiguity compared to traditional flat NER schemes."
  - [corpus]: Weak direct corpus support—neighbor papers focus on trajectory prediction rather than NER architectures.
- Break condition: If database schema lacks coverage for recognized entity types, or if entity names are genuinely ambiguous without disambiguation context, the verification queries will return unreliable matches.

### Mechanism 2: Semantic Algebra Intermediate Representation (SAIR)
- Claim: An intermediate relational algebra representation bridges the semantic gap between natural language queries and spatiotemporal SQL operations.
- Mechanism: The SAIR module decomposes query parsing into three substeps: (1) entity understanding with domain enhancement, (2) structured algebraic transformation using selection σ, projection π, and join ▷◁ operations, and (3) fusion with spatial functions like ST_Contains. This explicit intermediate step allows controlled semantic mapping before final SQL generation.
- Core assumption: Decomposing the natural-language-to-SQL translation into explicit algebraic steps improves accuracy over direct end-to-end generation, particularly for complex geospatial semantics.
- Evidence anchors:
  - [abstract]: "semantic algebra intermediate representation... enhance domain grounding and context-aware understanding"
  - [Section III.C]: "This design reflects the key improvement in our approach, which consists of decomposing the natural language parsing process into multiple substeps with clear structure and controllable semantics."
  - [corpus]: No direct corpus validation for SAIR approach specifically.
- Break condition: If the relational algebra representation cannot express certain maritime-specific operations (e.g., complex temporal aggregations), the intermediate representation becomes a bottleneck rather than a bridge.

### Mechanism 3: Query Rethink Validation Loop
- Claim: Post-hoc validation and correction of generated SQL through iterative reasoning improves accuracy for complex spatiotemporal queries.
- Mechanism: After initial SQL generation, the model re-examines the query for semantic coherence and logical consistency against domain-specific rules, dynamically correcting problematic elements before execution.
- Core assumption: Initial SQL drafts contain detectable errors (ambiguity, inaccuracy, grammatical issues) that can be identified and fixed through self-reflection without external feedback.
- Evidence anchors:
  - [Section III.D]: "the RT module identifies and dynamically corrects problematic query elements, ultimately generating refined and validated SQL statements."
  - [Section IV.D, Table VI]: Ablation shows removing RT drops accuracy from 77.80% to 70.22% (7.58 point reduction).
  - [corpus]: No corpus validation for query rethink mechanisms in maritime contexts.
- Break condition: If the model lacks sufficient domain knowledge to detect its own errors, or if errors are structurally similar to valid queries, the rethink mechanism may reinforce rather than correct mistakes.

## Foundational Learning

- Concept: **Text-to-SQL with Schema Linking**
  - Why needed here: VTS-LLM maps natural language to SQL over a custom maritime schema with 4 tables (ship_ais, ship_ais_quarter, shp_data, warn_single). Understanding how to link query terms to specific columns and tables is foundational.
  - Quick check question: Given a query "show me ships in the waterway," can you identify which table and column(s) would be involved?

- Concept: **Named Entity Recognition for Domain Adaptation**
  - Why needed here: Maritime terminology includes specialized entities (VLCC, deep-draught vessel, strait names) that require hierarchical categorization before query processing.
  - Quick check question: How would you distinguish between a geographic entity ("Singapore Port") and a vessel type entity ("VLCC") in a query?

- Concept: **Retrieval-Augmented Generation (RAG) for Knowledge Injection**
  - Why needed here: VTS queries require external domain knowledge (navigational rules, Notices to Mariners) not present in the structured database. The agent uses RAG to inject this context.
  - Quick check question: When a user asks about "vessels violating speed requirements," what external knowledge would need to be retrieved before generating SQL?

## Architecture Onboarding

- Component map:
  Input Layer -> NER-based Relational Reasoning -> Agent-based Domain Knowledge Injection -> Semantic Algebra IR -> SQL Generation -> Query Rethink -> Output

- Critical path: NER extraction -> domain knowledge retrieval -> SAIR construction -> SQL generation -> rethink validation. Errors propagate forward; the rethink module is the only correction point.

- Design tradeoffs:
  - Complexity vs. accuracy: Four-stage pipeline (NER + knowledge injection + SAIR + rethink) adds latency but improves accuracy by 12-35% over baselines.
  - Linguistic robustness vs. formal structure: Model optimized for operational/command styles (72.60-77.80%) trades some formal-language peak performance (89.72%) for real-world robustness.
  - Penalty-based metric: Custom Ms metric penalizes over-selection more heavily than under-selection, appropriate for safety-critical scenarios but may underreport recall issues.

- Failure signatures:
  - **Entity ambiguity cascade**: NER fails to disambiguate -> verification queries return multiple matches -> incorrect entity linking -> wrong SQL predicates.
  - **Knowledge retrieval mismatch**: RAG retrieves irrelevant regulations -> SQL includes incorrect rule filters -> false positives in vessel identification.
  - **Linguistic style degradation**: Command-style queries (fragmented, informal) cause 15-25% accuracy drop vs. formal queries for non-adapted models.
  - **Rethink reinforcement**: Validation loop confirms incorrect SQL when domain knowledge is insufficient to detect errors.

- First 3 experiments:
  1. **Baseline comparison**: Run VTS-LLM against GPT-4o, Claude-3.7, and SQL-specialized baselines (DAIL-SQL, SQLCoder) on the operational-style query set using the Ms metric to confirm the 12-35% improvement claim.
  2. **Ablation by component**: Systematically remove NER, SAIR, and RT modules (as in Table VI) to validate each component's contribution and identify which module provides the largest accuracy gain.
  3. **Linguistic style sensitivity**: Test the same queries across command/operational/formal variants to replicate the finding that linguistic style causes systematic performance variation (Table VII), and measure whether VTS-LLM's robustness holds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can joint optimization strategies for retrieval-augmented generation (RAG) components significantly improve query understanding and SQL generation accuracy in domain-adaptive Text-to-SQL systems?
- Basis in paper: [explicit] The authors state: "Future work will focus on... exploring joint optimization strategies about RAGs for enhanced query understanding."
- Why unresolved: Current RAG integration uses semantic matching independently; interactions between retrieval quality, domain knowledge injection, and SQL generation remain unoptimized.
- What evidence would resolve it: Comparative experiments measuring SQL accuracy under jointly-tuned vs. independently-tuned RAG components on the VTS-SQL benchmark.

### Open Question 2
- Question: What architectural or training modifications can close the 17-percentage-point performance gap between formal natural language queries (89.72%) and command-style queries (72.60%) in Text-to-SQL systems?
- Basis in paper: [explicit] The authors note: "Improving the model's understanding of concise and informal query expressions remains an important and application-relevant challenge." The sensitivity analysis reveals systematic performance degradation as queries become less formally structured.
- Why unresolved: Current LLMs are predominantly trained on well-formed text; command-style VTS queries feature omissions, shorthand, and fragmented syntax that standard models fail to robustly interpret.
- What evidence would resolve it: Demonstrating improved command-style accuracy (e.g., >85%) through targeted fine-tuning, prompt engineering, or intermediate representations designed for informal query patterns.

### Open Question 3
- Question: How can LLM-based agents be extended to reliably handle voice communication tasks (e.g., interpreting vessel calls, generating automated radio responses) in VTS operations?
- Basis in paper: [explicit] The authors propose: "More proactive VTS activities could be supported by an LLM-based agent. For instance, the agent could interpret vessel calls, generate automated voice responses, perform basic radio duties, maintain relevant communication logs, and reply to standard messages on behalf of the operator."
- Why unresolved: The current VTS-LLM system only addresses text-based querying; extending to multimodal voice input/output introduces speech recognition errors, timing constraints, and safety-critical response generation requirements.
- What evidence would resolve it: A prototype system demonstrating accurate transcription-to-query pipelines and appropriate automated voice response generation, evaluated against VTS operator communication protocols.

### Open Question 4
- Question: What failure modes and query characteristics account for the approximately 22% error rate in operational-style queries, and can these be systematically categorized and mitigated?
- Basis in paper: [inferred] While VTS-LLM achieves 77.80% on operational-style queries, ~22% remain incorrect. In safety-critical VTS domains, understanding and reducing this error rate is essential. The paper does not provide detailed error analysis.
- Why unresolved: The ablation study shows component contributions but does not categorize residual errors by type (e.g., spatiotemporal reasoning failures, entity linking errors, rule misinterpretation).
- What evidence would resolve it: Fine-grained error taxonomy with frequency analysis, followed by targeted interventions demonstrating error-rate reduction for each category.

## Limitations
- The evaluation relies on a custom benchmark dataset of 500 queries, which may not capture the full complexity of real-world VTS scenarios.
- The penalty-based Ms metric, while appropriate for safety-critical applications, may underreport recall issues that could be problematic in practice.
- The semantic algebra intermediate representation may struggle with increasingly complex maritime operations requiring novel SQL constructs.

## Confidence
- **High Confidence**: The 12-35% improvement over baselines on the operational-style query set is well-supported by ablation studies (Table VI) and consistent across multiple comparisons.
- **Medium Confidence**: The claim about linguistic style variation significantly impacting Text-to-SQL performance is supported by Table VII, but the underlying mechanism requires deeper investigation.
- **Low Confidence**: The hierarchical NER-based relational reasoning's superiority over flat NER schemes is asserted but lacks direct comparative validation.

## Next Checks
1. **Schema Coverage Stress Test**: Systematically evaluate VTS-LLM on queries requiring joins across all four database tables and operations beyond current spatial functions (e.g., temporal aggregations, conditional logic) to identify SAIR limitations.
2. **Linguistic Robustness Validation**: Test the system on real-world VTS communication logs containing typical operational noise (fragmentation, abbreviations, informal terminology) to measure degradation from the curated benchmark.
3. **Knowledge Source Reliability**: Conduct stress tests where the RAG system encounters incomplete, conflicting, or outdated maritime regulations to assess whether the query rethink mechanism can detect and handle knowledge quality issues.