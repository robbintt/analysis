---
ver: rpa2
title: 'InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback'
arxiv_id: '2505.23950'
source_url: https://arxiv.org/abs/2505.23950
tags:
- multi-turn
- human
- multimodal
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INTERMT, the first human preference dataset
  for multi-turn multimodal interaction, addressing the lack of long-horizon, interleaved
  multimodal understanding and generation data. It contains 15.6k prompts, 52.6k multi-turn
  QA instances, and 32.4k human-labeled preference pairs across nine dimensions at
  both local and global levels.
---

# InterMT: Multi-Turn Interleaved Preference Alignment with Human Feedback

## Quick Facts
- arXiv ID: 2505.23950
- Source URL: https://arxiv.org/abs/2505.23950
- Reference count: 40
- Introduces INTERMT, the first human preference dataset for multi-turn multimodal interaction with 15.6k prompts, 52.6k QA instances, and 32.4k preference pairs across 9 dimensions

## Executive Summary
This paper introduces INTERMT, the first human preference dataset for multi-turn multimodal interaction, addressing the lack of long-horizon, interleaved multimodal understanding and generation data. It contains 15.6k prompts, 52.6k multi-turn QA instances, and 32.4k human-labeled preference pairs across nine dimensions at both local and global levels. To construct this dataset, the authors develop an agentic workflow using tool-augmented MLLMs (including image generation, editing, and retrieval tools) to simulate high-quality multi-turn interactions. Experiments reveal that modeling fine-grained local preferences is more effective than direct global modeling, and demonstrate a multi-turn scaling law in judge moderation—accuracy improves with more training turns but generalizes less well to longer evaluations. The introduced INTERMT-Bench evaluates MLLMs' ability to judge multi-turn multimodal tasks, showing that while current models struggle with long-horizon alignment, they excel at recognizing crucial steps.

## Method Summary
The INTERMT dataset is constructed through a four-stage pipeline: (1) prompt collection and filtering of 100k seed image-text QA instances down to 15,604 high-quality seed questions, (2) tool-augmented agent-based multi-turn QA construction using GPT-4o, Qwen2-VL, and Gemini agents with integrated image generation (FLUX/SD), editing (Gemini), and retrieval tools, (3) human preference annotation across 9 dimensions (4 local, 5 global) with dual verification and language feedback, and (4) preference modeling training using Qwen2.5-VL-3B/7B backbones. The dataset targets multi-turn interleaved multimodal understanding and generation where both inputs and outputs can contain mixed text-image sequences, requiring cross-modal coherence and long-horizon preference modeling.

## Key Results
- Local (turn-level) preference modeling achieves ~0.735 accuracy vs ~0.595 for global training on the same test set
- Judge models exhibit a multi-turn scaling law where accuracy for turn k+1 improves as training turns increase from 1 to k-1
- INTERMT-Bench reveals current MLLMs achieve <60% agreement with humans on global evaluation tasks but excel at recognizing crucial steps
- Multi-turn scaling law shows diminishing generalization as evaluation horizon extends beyond training turns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling fine-grained local (turn-level) preferences is more effective than global (conversation-level) preferences for capturing human values in multi-turn multimodal interactions.
- **Mechanism:** The preference predictor is trained on per-turn preference pairs (prefix-preference approach), where the model learns to identify preferred responses given a conversation prefix z = (x₁, y₁, ..., xₖ). The training objective optimizes log σ(r_ϕ(y_w, z) - r_ϕ(y_l, z)) for preferred vs dispreferred responses at each turn, rather than comparing entire conversation trajectories.
- **Core assumption:** Human evaluators can reliably distinguish quality at the turn level, and these local judgments aggregate meaningfully to reflect overall conversation quality.
- **Evidence anchors:** Experiments show 3B model trained on local preferences achieves ~0.735 accuracy vs ~0.595 for global training when tested on local preferences.

### Mechanism 2
- **Claim:** Judge models exhibit a multi-turn scaling law where training on more turns improves predictive accuracy for future (unseen) preferences, but with diminishing generalization as evaluation horizons extend.
- **Mechanism:** Sequential exposure to k training turns enables the model to capture evolving user intent patterns. The model learns implicit preference dynamics that transfer to turns k+1 through N. However, as the gap between training and evaluation turns widens, contextual drift and preference evolution degrade predictions.
- **Core assumption:** User preferences follow learnable temporal patterns within conversations that partially persist across turns.
- **Evidence anchors:** Accuracy for evaluating turn k improves as training turns increase from 1 to k-1, but performance on turns k+1→T diminishes as evaluation horizon extends.

### Mechanism 3
- **Claim:** Tool-augmented agentic workflows can simulate realistic multi-turn interleaved multimodal conversations where current MLLMs lack native interleaved generation capabilities.
- **Mechanism:** Agent receives conversation history and generates follow-up questions via Socratic strategy (10 candidates, select M=1-3 via similarity ranking). For responses, the agent samples 10+ candidates, invokes tools (FLUX/SD for generation, Gemini for editing, web retrieval for sourcing), then selects N=2-4 responses based on relevance and multimodal quality. GPT-4o classifies intent and routes tool calls via structured tokens <Image, caption>.
- **Core assumption:** Current MLLMs can approximate human-like multi-turn reasoning when augmented with external tools, and the resulting dialogues are sufficiently realistic for preference annotation.
- **Evidence anchors:** The four-stage pipeline with tool integration at Stage II successfully generates 52.6k multi-turn QA instances with average 5.33 images per conversation.

## Foundational Learning

- **Concept: Bradley-Terry Preference Model**
  - Why needed here: Understanding how preference pairs (y_w ≻ y_l) convert to reward signals via the BT probability formulation p(y₁ ≻ y₂|x) = exp(r*(y₁,x)) / (exp(r*(y₁,x)) + exp(r*(y₂,x)))
  - Quick check question: Given two responses with rewards 2.0 and 1.0, what is the probability the first is preferred? (Answer: e²/(e²+e¹) ≈ 0.73)

- **Concept: Interleaved Multimodal Understanding and Generation**
  - Why needed here: INTERMT targets scenarios where both inputs AND outputs can contain mixed text-image sequences, not just text-to-image OR image-to-text
  - Quick check question: What distinguishes interleaved generation from standard text-to-image generation? (Answer: Interleaved allows arbitrary alternation of text and images within a single conversation, requiring cross-modal coherence)

- **Concept: Local vs Global Preference Decomposition**
  - Why needed here: The 9-dimension framework splits into 4 local (L1-L4: per-turn) and 5 global (G1-G5: conversation-level) metrics, each requiring different annotation protocols
  - Quick check question: Which dimensions assess turn-level quality vs overall conversation coherence? (Answer: L1-L4 assess single turns; G1-G5 assess full conversation including context awareness and style coherence)

## Architecture Onboarding

- **Component map:**
  Seed Questions (15.6k from open-source/web/human) -> Agent Workflow (GPT-4o/Qwen2-VL/Gemini + Tools) -> Multi-turn QA Trees (depth ≥5, branching M×N per turn) -> Quality Pruning (visual quality + text-image coherence + consistency filters) -> Human Annotation (30 annotators, 9 dimensions, dual verification) -> INTERMT Dataset (52.6k QA instances, 32.4k preference pairs) -> Judge Training (Qwen2.5-VL-3B/7B backbone)

- **Critical path:**
  1. Seed question quality determines multi-turn expansion potential (filter for "multi-turn potential" via GPT-4o classification)
  2. Tool call routing accuracy directly impacts image relevance (GPT-4o double-classifier determines tool selection)
  3. Annotation dimension definition determines preference signal quality (9-dimension framework requires ~2-week annotator training)

- **Design tradeoffs:**
  - AI-synthesized dialogs vs human-human: 10× cost reduction but potential distribution shift
  - Local vs global preference training: Local captures fine-grained signals but may miss long-horizon intent
  - Tool diversity vs consistency: Multiple image generators (FLUX, SD-3) increase diversity but risk style incoherence

- **Failure signatures:**
  - Position bias: Models favor responses in specific positions
  - High-score bias: Models inflate scores for entire conversations, reducing discriminability
  - Reasoning disconnect: Adding rationale generation does not improve accuracy
  - Tool over-reliance: Excessive image generation in text-heavy tasks

- **First 3 experiments:**
  1. Validate local vs global preference transfer: Train two reward models on local pairs vs global pairs, evaluate on held-out test set. Expect ~10-15% accuracy gap favoring local training.
  2. Reproduce scaling law with your own data split: Train on k=1,2,3,4 turns, evaluate on turn k+1. Verify diminishing returns pattern.
  3. Benchmark your model on INTERMT-BENCH: Run inference on Scoring Evaluation (Pearson), Pair Comparison (accuracy), and Crucial Step Recognition. Compare against baselines; expect <60% agreement on global dimensions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively model long-horizon, interleaved multimodal preferences by leveraging the distinct local (turn-level) and global (conversation-level) human annotations?
- Basis in paper: Section 4 explicitly lists "Modeling long-horizon values" as a future research direction, asking how to model these preferences using local and global annotations.
- Why unresolved: The paper demonstrates that modeling local preferences is more effective than global ones, yet capturing the dynamic evolution of user intent over long horizons remains a challenge.
- What evidence would resolve it: The development of algorithms that successfully aggregate local and global signals to predict conversation-level preferences more accurately than current baselines.

### Open Question 2
- Question: How can algorithms be designed to incorporate real human feedback from datasets like INTERMT to enhance MLLM performance in multi-turn interleaved understanding and generation?
- Basis in paper: Section 4 poses the question: "How can we design algorithms that effectively incorporate real human feedback from INTERMT to assess and enhance the performance of MLLMs?"
- Why unresolved: Current advanced MLLMs (e.g., o4-mini) still face significant challenges in aligning with long-horizon human values and show low agreement with humans on global evaluation tasks.
- What evidence would resolve it: Successful application of new alignment methods trained on INTERMT that result in measurable performance gains on the INTERMT-BENCH or similar multi-turn benchmarks.

### Open Question 3
- Question: Can the INTERMT workflow and preference modeling framework be extended to encompass additional modalities such as video and audio to achieve a holistic representation of communications dynamics?
- Basis in paper: Section 5.1 states: "Future work is essential to extend INTERMT to encompass these additional modalities, moving closer to a holistic representation of communications dynamics."
- Why unresolved: The current study is limited to vision-language interactions, whereas human communication involves complex audio-visual cues not yet captured in the dataset.
- What evidence would resolve it: The successful construction of a multi-turn, multi-modal dataset including video and audio, demonstrating that the observed preference scaling laws hold across these modalities.

## Limitations

- The dataset is AI-synthesized rather than human-human, potentially introducing distribution shift despite quality controls
- Multi-turn scaling law is empirically observed but lacks theoretical grounding or validation across different model architectures
- Tool-augmented workflows may introduce artifacts that bias preference signals
- Human annotation agreement shows variance across dimensions (60-90% range) even with language feedback

## Confidence

- **High confidence:** Local preference modeling effectiveness (supported by direct experimental comparison), InterMT-Bench evaluation framework validity
- **Medium confidence:** Multi-turn scaling law observations (empirical findings without theoretical explanation), tool-augmented workflow realism
- **Low confidence:** Generalization of findings to longer horizon evaluations beyond test set, claim that local modeling is universally superior for all preference alignment tasks

## Next Checks

1. Validate dataset authenticity: Compare tool-generated conversations against human-human dialogues on identical prompts using both automatic metrics (coherence scores, diversity metrics) and human evaluations to quantify distribution shift
2. Test scaling law robustness: Reproduce the multi-turn scaling law experiment with different model families (not just Qwen2.5-VL) and across multiple data splits to verify it's not an artifact of specific hyperparameters or data selection
3. Evaluate cross-task transfer: Test whether models trained on INTERMT preferences transfer effectively to non-QA multimodal tasks (visual reasoning, image captioning) to validate the framework's generalizability beyond the specific dataset domain