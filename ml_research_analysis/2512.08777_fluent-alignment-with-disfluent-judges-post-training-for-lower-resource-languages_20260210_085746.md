---
ver: rpa2
title: 'Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages'
arxiv_id: '2512.08777'
source_url: https://arxiv.org/abs/2512.08777
tags:
- language
- training
- fluency
- norwegian
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel method for aligning language models
  in lower-resource languages using on-policy reinforcement learning, which avoids
  exposure to disfluent text during training. The approach leverages a pre-trained
  base model fine-tuned with short English SFT, then aligned using online on-policy
  RL where the model learns from its own generated responses without relying on any
  instruction-tuning dataset in the target language.
---

# Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages

## Quick Facts
- **arXiv ID**: 2512.08777
- **Source URL**: https://arxiv.org/abs/2512.08777
- **Reference count**: 40
- **Primary result**: Novel RL-based alignment method for lower-resource languages that produces fluent models without exposure to disfluent text

## Executive Summary
This work addresses the challenge of aligning language models for lower-resource languages where high-quality instruction datasets are unavailable. The proposed approach uses on-policy reinforcement learning where the model learns from its own generated responses, avoiding the need for translated instruction data that often produces disfluent outputs. By leveraging a pre-trained base model fine-tuned with short English SFT, the method enables alignment in target languages without requiring instruction-tuning datasets in those languages. Human evaluation with native Norwegian speakers demonstrates that this approach produces significantly more fluent models compared to both translation-based fine-tuning and multilingual baselines.

## Method Summary
The method employs a two-stage process: first, a pre-trained base model undergoes short English supervised fine-tuning (SFT) on general conversational data. Second, on-policy reinforcement learning is used for alignment where the model generates responses and learns from its own outputs through a reward model. The key innovation is that the alignment occurs entirely in the target language without any exposure to translated instruction data, which typically introduces disfluencies. The approach assumes the judge model understands English (used in SFT) while judging responses in the target language, creating an unusual but effective dependency that avoids the fluency degradation associated with translation-based approaches.

## Key Results
- The proposed method achieved 79.7% win-rate against translation-based SFT (60.0% win-rate) in human evaluation
- The method significantly outperformed a multilingual baseline with only 10.3% win-rate
- Even minimal exposure to translated text during training measurably degraded fluency, demonstrating the importance of avoiding translated data entirely

## Why This Works (Mechanism)
The approach works by breaking the dependency between instruction-following capability and language fluency. Instead of training on translated instructions that often produce unnatural phrasing, the model learns alignment through reinforcement learning in the target language space. The judge model, even if potentially disfluent itself, can still recognize and reward fluent outputs because it understands the target language's grammatical and semantic patterns. This separation allows the policy model to develop native-level fluency while still learning to follow instructions effectively.

## Foundational Learning
- **On-policy reinforcement learning**: Training where the agent learns from its own generated responses rather than from a fixed dataset; needed because no high-quality instruction dataset exists in the target language; quick check: model generates responses and receives rewards based on its own outputs
- **Cross-lingual judge dependency**: Judge model must understand English (for SFT) while evaluating target language responses; needed to enable alignment without target-language instruction data; quick check: judge can evaluate Norwegian responses despite being trained primarily on English
- **Translation-induced disfluency**: Direct translation often produces unnatural phrasing and grammatical errors in the target language; needed to justify avoiding translated instruction data; quick check: compare fluency of translated vs natively generated responses
- **Native speaker evaluation**: Human assessment by native speakers as the gold standard for measuring language fluency; needed because automatic metrics cannot capture nuanced fluency issues; quick check: native speakers rate response quality on fluency dimensions
- **Base model transfer**: Leveraging pre-trained models with general language understanding; needed to bootstrap alignment in low-resource settings; quick check: model shows basic comprehension before alignment
- **Language-specific linguistic features**: Understanding how features like compound words, gendered nouns, and verb placement affect fluency; needed to explain why certain languages benefit more from native generation; quick check: analyze error patterns related to specific linguistic phenomena

## Architecture Onboarding

Component map: Pre-trained Base Model -> English SFT -> On-policy RL Alignment -> Target Language Judge

Critical path: The alignment process depends on the judge's ability to understand English (from SFT) while evaluating Norwegian responses, creating a cross-lingual dependency that enables training without target-language instruction data.

Design tradeoffs: Avoids translated data to maintain fluency but requires judge to understand both languages; uses on-policy RL instead of imitation learning to learn from model's own outputs; trades computational cost of RL for quality gains in low-resource settings.

Failure signatures: Judge unable to evaluate target language responses correctly; model generates disfluent responses despite training; alignment fails to improve instruction-following capability; exposure to translated text introduces unnatural phrasing patterns.

First experiments:
1. Test judge model's ability to evaluate Norwegian responses after English SFT training
2. Run initial on-policy RL alignment with small batch sizes to verify learning signal
3. Compare fluency of model outputs before and after minimal exposure to translated data

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization to other lower-resource languages remains untested, particularly for languages with different syntactic structures or those requiring code-switching
- The threshold for "minimal" exposure to translated text that degrades fluency is not quantified
- The study relies on MTurk workers with Norwegian proficiency rather than native speakers for initial data collection, potentially introducing subtle biases

## Confidence

High confidence in comparative results showing the proposed method outperforming both translation-based SFT (79.7% vs 60.0% win-rate) and the multilingual baseline (79.7% vs 10.3% win-rate).

Medium confidence in the claim that disfluent judges can train fluent policies, as this relies on the specific judge architecture and language combination tested.

Medium confidence in the claim about avoiding translated text being essential, as the study shows correlation but doesn't establish causation or test intermediate exposure levels systematically.

## Next Checks

1. Test the approach across 3-5 additional lower-resource languages with varying linguistic distances from English to establish generalization patterns and identify any language-specific factors that influence success rates.

2. Conduct ablation studies with graduated exposure levels to translated text (0%, 0.1%, 1%, 10%, 50%) to precisely quantify the relationship between exposure volume and fluency degradation, including both automatic metrics and human evaluation.

3. Implement a cross-lingual judge transfer experiment where judges trained in one language are used to align models in typologically similar but distinct languages, measuring performance degradation and identifying the minimum judge language requirements.