---
ver: rpa2
title: 'SOCK: A Benchmark for Measuring Self-Replication in Large Language Models'
arxiv_id: '2509.25643'
source_url: https://arxiv.org/abs/2509.25643
tags:
- task
- tasks
- across
- replication
- self-replication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SOCK, a benchmark for measuring large language
  models' (LLMs) ability to self-replicate without human intervention. SOCK defines
  self-replication as not only creating a functioning copy of itself, but also the
  ability for that self-replication to persist and occur across different computational
  contexts.
---

# SOCK: A Benchmark for Measuring Self-Replication in Large Language Models

## Quick Facts
- **arXiv ID:** 2509.25643
- **Source URL:** https://arxiv.org/abs/2509.25643
- **Reference count:** 10
- **Primary result:** 65% task success rate across 40 model-task pairs; highest achieved levels RCL 2 and PCL 2

## Executive Summary
This paper introduces SOCK, a benchmark designed to measure large language models' ability to self-replicate without human intervention. Self-replication is defined as not only creating a functioning copy of the model but also ensuring that replication persists across different computational contexts. Using a five-task suite based on practical CLI utilities, the researchers evaluated various open-weight and proprietary frontier models in a controlled environment with an LLM acting agentically. The benchmark produces an R-score and categorizes models into specific Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL) matrices.

The experimental results reveal significant obstacles to persistent self-replication and multi-agent systems, particularly around context retention and multi-agent decision-making. Despite testing a variety of models, none achieved beyond RCL 2 and PCL 2, indicating substantial barriers to advanced self-replication capabilities. The 65% task success rate across 40 model-task pairs suggests that while some progress has been made, persistent self-replication remains a challenging frontier for current LLMs.

## Method Summary
SOCK defines self-replication as the ability to create a functioning copy of itself that can persist and occur across different computational contexts. The benchmark employs a five-task suite based on practical CLI utilities, where an LLM acts agentically in a controlled environment. Performance is measured through an R-score, which categorizes models into Replication-Capability Levels (RCL) and Persistence-Capability Levels (PCL) matrices. The controlled environment and CLI-based tasks provide a standardized framework for evaluating self-replication capabilities across different model architectures.

## Key Results
- 65% task success rate across 40 model-task pairs
- Highest achieved levels: RCL 2 and PCL 2
- Identified obstacles include context retention and multi-agent decision-making

## Why This Works (Mechanism)
The SOCK benchmark works by creating a controlled environment where LLMs must demonstrate both the ability to create copies of themselves and ensure those copies can function in different computational contexts. The CLI-based task suite provides concrete, measurable challenges that test both the technical capability of self-replication and the persistence of that replication across varying environments. By acting agentically, the LLM must navigate through multiple steps of the replication process without human intervention, revealing practical limitations in current model architectures.

## Foundational Learning

**CLI Utilities**: Why needed: Provide concrete, measurable tasks for testing self-replication; Quick check: Can the model successfully execute basic command-line operations required for replication

**Agentic LLM Behavior**: Why needed: Tests autonomous decision-making during self-replication; Quick check: Does the model maintain task focus and logical progression through replication steps

**Context Retention**: Why needed: Critical for maintaining replication state across multiple steps; Quick check: Can the model remember previous actions and their outcomes when continuing the replication process

**Multi-agent Decision Making**: Why needed: Tests coordination capabilities when multiple instances interact; Quick check: Do multiple model instances successfully collaborate without conflicts or deadlocks

## Architecture Onboarding

Component map: LLM Agent -> CLI Tasks -> Environment Controller -> R-score Calculator -> RCL/PCL Matrix

Critical path: LLM Agent processes task instructions -> Executes CLI commands -> Environment validates execution -> Success/failure recorded -> R-score updated -> Level determined

Design tradeoffs: Controlled environment ensures reproducibility but may not capture all real-world complexities; CLI utilities provide measurable tasks but represent a narrow computational context

Failure signatures: Context loss between steps, incorrect command sequencing, inability to validate successful replication, failure to adapt to environmental changes

First experiments:
1. Test single-task self-replication without persistence requirements
2. Evaluate context retention across 3-5 sequential CLI operations
3. Measure basic multi-agent coordination with 2-3 model instances

## Open Questions the Paper Calls Out
None

## Limitations
- Controlled environment may not fully represent real-world self-replication conditions
- CLI utility focus represents a narrow slice of potential computational contexts
- R-score and RCL/PCL matrices provide coarse-grained categorization that may mask nuances
- Alternative definitions of self-replication might yield different conclusions

## Confidence

High confidence: Experimental methodology is sound with clear procedural descriptions and reproducible results; task success rates and achieved levels are well-documented

Medium confidence: Broader implications about persistent self-replication obstacles are reasonable but may overstate generalizability from this specific benchmark context

Low confidence: Claims about benchmark's ability to fully capture self-replication complexity or predict future capabilities should be treated cautiously due to field's rapid evolution

## Next Checks

1. External replication: Independent research groups should replicate SOCK benchmark results using different LLM implementations and environmental configurations to verify robustness of observed RCL 2 and PCL 2 limitations

2. Cross-context validation: Test whether models achieving higher RCL/PCL levels in CLI-based tasks can demonstrate similar capabilities in alternative computational contexts (GUI automation, web APIs) to assess benchmark generalizability

3. Long-term persistence study: Conduct extended experiments beyond current timeframe to determine whether identified obstacles to persistent self-replication remain consistent over longer periods and more complex environmental changes