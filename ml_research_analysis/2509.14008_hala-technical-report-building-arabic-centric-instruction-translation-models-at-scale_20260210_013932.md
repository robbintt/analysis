---
ver: rpa2
title: 'Hala Technical Report: Building Arabic-Centric Instruction & Translation Models
  at Scale'
arxiv_id: '2509.14008'
source_url: https://arxiv.org/abs/2509.14008
tags:
- arabic
- online
- https
- language
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces Hala, a family of Arabic-centric instruction\
  \ and translation models developed through a translate-and-tune pipeline. The authors\
  \ compress a strong AR\u2194EN teacher model to FP8 for efficient translation and\
  \ fine-tune a lightweight model to serve as an AR\u2194EN translator."
---

# Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale

## Quick Facts
- **arXiv ID:** 2509.14008
- **Source URL:** https://arxiv.org/abs/2509.14008
- **Reference count:** 40
- **Key outcome:** Introduces Hala, a family of Arabic-centric instruction and translation models developed through a translate-and-tune pipeline, achieving state-of-the-art results on Arabic benchmarks within nano (≤2B) and small (7-9B) categories.

## Executive Summary
This work presents Hala, a family of Arabic-centric instruction and translation models developed through a translate-and-tune pipeline. The authors compress a strong AR↔EN teacher model to FP8 for efficient translation and fine-tune a lightweight model to serve as an AR↔EN translator. This translator is used to convert high-quality English instruction datasets into Arabic, producing a million-scale bilingual instruction corpus. Hala models (350M, 700M, 1.2B, and 9B parameters) are trained on this data and merged with base models using slerp merging to balance Arabic specialization and general capability. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within the nano (≤2B) and small (7-9B) categories, outperforming their base models. The translation quality of the LFM2-1.2B translator is also evaluated, showing significant improvements over its base model. Models, data, and training recipes are released to accelerate Arabic NLP research.

## Method Summary
The Hala pipeline begins by compressing a strong AR↔EN teacher model (CohereLabs/command-a-translate-08-2025) to FP8 using LLM Compressor. A lightweight student model (LiquidAI/LFM2-1.2B) is then fine-tuned as a translator on a bootstrapping corpus combining 405K Open-Orca samples (translated by the teacher) and ~440K filtered OPUS-100 pairs. This specialized translator converts seven English instruction datasets (Hermes 3, SCP-116K, ReAlign-Alpaca, LaMini, Tulu 3, and Synthetic Instruct) into Arabic, creating ~4.5M samples. Base models (LFM2 variants and Fanar-1-9B-Instruct) are fine-tuned on this translated corpus and merged with their original weights using slerp merging (t=0.5) to preserve general capabilities while enhancing Arabic instruction-following performance.

## Key Results
- Hala models achieve state-of-the-art performance on Arabic-centric benchmarks within nano (≤2B) and small (7-9B) parameter categories
- The LFM2-1.2B translator shows significant BLEU/chrF++ improvements over its base model on MMLU translation tasks
- Slerp merging at t=0.5 successfully balances Arabic specialization with general capability retention
- All models, data, and training recipes are publicly released for Arabic NLP research

## Why This Works (Mechanism)
The translate-and-tune pipeline leverages existing high-quality English instruction data by translating it into Arabic using a specialized translation model. This approach circumvents the scarcity of native Arabic instruction data while maintaining quality through careful filtering and merging strategies. The slerp merging technique helps preserve general capabilities while incorporating Arabic-specific instruction-following skills.

## Foundational Learning
- **FP8 quantization** - Why needed: Reduces memory footprint for efficient deployment of large translation models. Quick check: Model runs without OOM errors on target hardware.
- **Instruction tuning** - Why needed: Adapts base models to follow human instructions effectively. Quick check: Model responds appropriately to diverse instruction formats.
- **Slerp merging** - Why needed: Combines fine-tuned and base model weights while preserving both capabilities. Quick check: Merged model performs better than either component on target benchmarks.
- **Translation quality filtering** - Why needed: Ensures high-quality parallel data for training the translator. Quick check: Filtered dataset maintains high BLEU scores.
- **Instruction data formatting** - Why needed: Consistent prompt-response structure is critical for effective instruction tuning. Quick check: Training data follows expected template without corruption.

## Architecture Onboarding

**Component Map:** CohereLabs/command-a-translate-08-2025 (FP8) -> LFM2-1.2B translator -> 7 English instruction datasets -> Translated Arabic corpus -> Base models (LFM2/Fanar) -> Hala models

**Critical Path:** Teacher translator → Lightweight translator training → Instruction corpus generation → Model fine-tuning → Slerp merging → Evaluation

**Design Tradeoffs:** The pipeline trades potential cultural nuances lost in translation for scalable access to high-quality instruction data. The 1.2B translator size balances translation quality with computational efficiency.

**Failure Signatures:** Translation collapse manifests as poor BLEU scores during translator training; catastrophic forgetting appears as significant drops on English benchmarks after Arabic fine-tuning.

**First Experiments:**
1. Train LFM2-1.2B translator on bootstrapping data and validate BLEU scores on held-out set
2. Translate a small subset of English instruction data and perform manual quality assessment
3. Fine-tune a base model on translated data and compare performance against base model on Arabic benchmarks

## Open Questions the Paper Calls Out

**Open Question 1:** Does slerp merging at t=0.5 optimally balance Arabic specialization against general multilingual capability? The authors found it "preserves general capability while boosting Arabic instruction-following performance" but present no ablation study comparing alternative merging ratios or strategies.

**Open Question 2:** To what extent does translating English instruction data into Arabic preserve culturally-specific content, safety norms, and pragmatics? While the authors acknowledge cultural alignment is important, their pipeline translates English datasets wholesale without cultural adaptation, and evaluation uses BLEU/ROUGE metrics that don't assess cultural appropriateness.

**Open Question 3:** How robust is the pipeline to code-switching, dialectal Arabic, and non-standard orthography? All evaluation benchmarks target Modern Standard Arabic, with no testing on dialectal inputs or mixed Arabic-English code-switched text.

**Open Question 4:** Does the 1.2B translator's instruction-style specialization generalize to domains beyond the training mix? Translation quality is reported only on MMLU questions, with no domain-specific translation evaluation.

**Open Question 5:** What is the minimum translation quality threshold below which instruction-tuning on translated data degrades performance? The authors filter OPUS-100 to retain ~44% of pairs but don't characterize the relationship between translation fidelity and downstream instruction-following gains.

## Limitations
- Key hyperparameters (learning rates, batch sizes, epochs) are unspecified, making precise reproduction difficult
- Translation quality evaluation relies solely on automatic metrics (BLEU/chrF++) without human assessment of instruction-following capabilities
- Slerp merging uses a heuristic weight (t=0.5) without empirical justification or ablation studies
- Evaluation benchmarks don't assess performance degradation on non-Arabic tasks or capability retention

## Confidence
- **High Confidence:** Core methodology, model architecture specifications, and Arabic benchmark results are clearly presented and reproducible
- **Medium Confidence:** Translation quality improvements are demonstrated through automatic metrics on limited datasets
- **Low Confidence:** State-of-the-art claims are difficult to verify without access to competing models and comprehensive evaluation across language pairs

## Next Checks
1. Re-run translator fine-tuning with varying learning rates (1e-5, 3e-5, 5e-5) and batch sizes to establish hyperparameter sensitivity
2. Evaluate released Hala models on established English benchmarks (MMLU, BBH) to quantify capability retention
3. Conduct human evaluation studies on Arabic instruction-following tasks to validate automatic metric alignment with actual performance