---
ver: rpa2
title: 'ReaderLM-v2: Small Language Model for HTML to Markdown and JSON'
arxiv_id: '2503.01151'
source_url: https://arxiv.org/abs/2503.01151
tags:
- data
- html
- content
- extraction
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReaderLM-v2 is a 1.5 billion parameter language model designed
  for efficient web content extraction, capable of processing documents up to 512K
  tokens. The model addresses the challenge of converting messy HTML into clean Markdown
  or JSON formats with high accuracy while maintaining significantly lower computational
  requirements than larger models.
---

# ReaderLM-v2: Small Language Model for HTML to Markdown and JSON

## Quick Facts
- **arXiv ID:** 2503.01151
- **Source URL:** https://arxiv.org/abs/2503.01151
- **Reference count:** 11
- **Primary result:** 1.5B parameter model outperforms GPT-4o by 15-20% on HTML-to-Markdown/JSON extraction while supporting 512K token context

## Executive Summary
ReaderLM-v2 is a 1.5 billion parameter language model designed for efficient web content extraction, capable of processing documents up to 512K tokens. The model addresses the challenge of converting messy HTML into clean Markdown or JSON formats with high accuracy while maintaining significantly lower computational requirements than larger models. Through a novel three-stage data synthesis pipeline and comprehensive training strategy, ReaderLM-v2 achieves state-of-the-art performance on carefully curated benchmarks, demonstrating that small models can rival much larger models when properly trained for specific extraction tasks.

## Method Summary
ReaderLM-v2 employs a three-stage data synthesis pipeline called Draft-Refine-Critique to generate high-quality training data through iterative drafting, refining, and critiquing of web content extraction. The training strategy incorporates continuous pre-training for length extension, supervised fine-tuning, direct preference optimization, and self-play iterative tuning. The model is built on Qwen2.5-1.5B-Instruct base, progressively extends context from 32K to 256K tokens, and uses ring-zag attention with RoPE base 5M for efficient long-context processing. The approach combines specialized checkpoints for Markdown and JSON extraction, merged through linear weight interpolation, followed by preference learning and self-play to create a feedback loop for continuous improvement.

## Key Results
- Achieves Rouge-L score of 0.86 for main content extraction, representing 24.6% improvement over GPT-4o
- Maintains 0.72 Rouge-L for instructed extraction while using 1/30th the parameters of GPT-4o
- Demonstrates 512K token context processing capability with strong performance on long documents
- Outperforms larger models by 15-20% on carefully curated benchmarks while using significantly less compute

## Why This Works (Mechanism)
The model's success stems from its specialized training pipeline that generates high-quality synthetic data through iterative refinement, combined with progressive context extension that enables long-document processing. The ring-zag attention mechanism and RoPE base 5M configuration allow efficient handling of extremely long sequences without the computational overhead of full attention mechanisms. The contrastive loss in supervised fine-tuning prevents repetitive output generation, while the self-play iterative tuning creates a feedback loop where the model continuously improves by generating better training data for itself.

## Foundational Learning
- **Ring-Zag Attention**: Why needed - Efficient long-context processing without quadratic complexity; Quick check - Verify attention patterns show O(n) scaling
- **RoPE Base 5M**: Why needed - Enables stable position encoding for 512K token sequences; Quick check - Confirm position embeddings remain distinct at extreme distances
- **Contrastive Loss**: Why needed - Prevents repetitive token generation in SFT; Quick check - Monitor output diversity metrics during training
- **Direct Preference Optimization**: Why needed - Aligns model outputs with human preferences without reinforcement learning; Quick check - Compare preference ranking consistency across iterations
- **Progressive Context Extension**: Why needed - Allows stable training from short to long sequences; Quick check - Track loss stability across context length transitions
- **Self-Play Iterative Tuning**: Why needed - Creates continuous improvement feedback loop; Quick check - Monitor performance gains between self-play iterations

## Architecture Onboarding

**Component Map:** HTML Input → Draft-Refine-Critique Pipeline → SFT Datasets → Progressive Context Training → 4 Specialized Checkpoints → Linear Merge → DPO → Self-Play Iteration → Final Model

**Critical Path:** HTML → Synthetic Data Generation (Draft-Refine-Critique) → Continued Pre-training (Context Extension) → SFT (4 Checkpoints + Merge) → DPO → Self-Play → Final Model

**Design Tradeoffs:** Small model size (1.5B) versus processing capability (512K tokens) requires careful attention mechanism design and progressive training. The use of synthetic data generation trades computational cost for data quality, while self-play iteration trades inference time for model improvement. Linear weight merging of specialized checkpoints balances task-specific optimization with general capability.

**Failure Signatures:** Repetitive token generation during SFT indicates contrastive loss not properly applied; degraded quality on long documents (>256K tokens) suggests RoPE base or ring-zag attention misconfiguration; poor JSON extraction performance may indicate insufficient contrastive training data.

**3 First Experiments:** 1) Validate ring-zag attention implementation by testing attention patterns on 512K token sequences; 2) Run contrastive loss ablation to confirm its impact on repetitive output prevention; 3) Test linear weight merging across 4 checkpoints to verify no catastrophic forgetting occurs.

## Open Questions the Paper Calls Out
- Can the iterative self-play tuning process continue beyond one round to yield further gains, or does performance plateau/degrade?
- What causes the performance gap between instructed Markdown extraction (0.72 Rouge-L) and main content extraction (0.86), and can this gap be closed without increasing model size?
- How robust is ReaderLM-v2 on languages and domains underrepresented in WebMarkdown-1M (e.g., non-English/Chinese content or specialized technical HTML)?
- Can the Draft-Refine-Critique pipeline achieve comparable quality without relying on a larger teacher model for initial drafting?

## Limitations
- Benchmark curation using GPT-4o as oracle may introduce bias favoring GPT-4o's behavior patterns
- 512K token capability demonstrated on only 15 documents, limiting generalizability claims
- Performance on non-English content (20% Chinese, 62.7% English) not separately evaluated
- Self-play iterative tuning lacks ablation studies showing improvements are due to iteration versus additional data

## Confidence
- **High Confidence:** Model architecture and training pipeline specifications are clearly defined and technically sound
- **Medium Confidence:** 15-20% improvement over GPT-4o is supported by benchmarks but methodology introduces potential bias
- **Low Confidence:** 512K token processing claims based on extremely limited sample size (15 documents)

## Next Checks
1. Re-evaluate ReaderLM-v2 on independently curated benchmarks (not using GPT-4o for curation) to verify 15-20% improvement claim robustness
2. Conduct separate evaluation on Chinese-only and other non-English subsets to quantify multilingual performance
3. Test 512K token capability on larger, more diverse set of long documents (minimum 50 documents) to validate scalability claims