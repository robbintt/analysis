---
ver: rpa2
title: 'Beyond Pattern Recognition: Probing Mental Representations of LMs'
arxiv_id: '2502.16717'
source_url: https://arxiv.org/abs/2502.16717
tags:
- lamps
- charlene
- quantity
- mental
- final
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates whether language models (LMs) truly perform\
  \ dynamic, incremental reasoning or merely recognize patterns. Using the MATHWORLD\
  \ dataset, it compares traditional Chain-of-Thought (CoT) prompting against step-by-step\
  \ mental modeling approaches\u2014both text-only (TextMM) and vision-based (VisMM)."
---

# Beyond Pattern Recognition: Probing Mental Representations of LMs

## Quick Facts
- arXiv ID: 2502.16717
- Source URL: https://arxiv.org/abs/2502.16717
- Reference count: 36
- Key outcome: Mental modeling approaches significantly underperform CoT prompting, suggesting LMs recognize patterns rather than perform dynamic reasoning

## Executive Summary
This study investigates whether language models truly perform incremental reasoning or merely recognize patterns by comparing Chain-of-Thought prompting against step-by-step mental modeling approaches. Using the MATHWORLD dataset with zero-sum transfer problems, researchers found that while CoT methods achieve near-perfect accuracy, incremental mental modeling approaches show substantial performance drops. Vision-based mental modeling outperforms text-only methods, and interestingly, smaller multimodal models sometimes outperform larger ones, hinting at overfitting issues. The findings challenge the depth of LM reasoning capabilities, revealing they excel at pattern recognition but struggle with dynamic mental modeling.

## Method Summary
The study evaluates three approaches on the MATHWORLD dataset: Chain-of-Thought (CoT) baseline, Text Mental Modeling (TextMM), and Vision Mental Modeling (VisMM). The dataset contains 2,400 linear transfer problems across depths 1-6, decomposed into sequential increments. TextMM requires models to update a JSON state object incrementally with each new problem statement, while VisMM uses visual graphs representing state changes. Models tested include various Llama and DeepSeek variants using vLLM inference with temperature=0. Performance is measured by final answer accuracy across depth levels, with intermediate step quality assessed through JSON object updates.

## Key Results
- Mental modeling approaches show significant accuracy drops compared to CoT, especially at higher depths
- VisMM consistently outperforms TextMM, with Llama3.2-11B VisMM achieving 0.79 accuracy at depth 6 versus 0.02 for TextMM
- Distilled models (DeepSeek-R1-Distill) maintain stable accuracy across depths while non-distilled models degrade
- Larger multimodal models (90B) sometimes underperform smaller ones (11B), with the 90B variant generating hallucinations instead of processing visual inputs

## Why This Works (Mechanism)

### Mechanism 1: Pattern Recognition vs. Dynamic State Maintenance
Standard CoT performance relies on recognizing complete problem patterns from pre-training data rather than dynamically constructing solution states. When provided with full context at once, models activate learned solution pathways. However, incremental processing requires maintaining persistent working memory (JSON state), which the significant performance drop suggests models lack robust mechanisms for without full context window access.

### Mechanism 2: Visual Grounding as a Cognitive Scaffold
Visual representations of problem states reduce ambiguity inherent in text-only state tracking. Text descriptions of transfers require parsing complex linguistic syntax to determine change direction, while visual graphs make the delta explicit. The vision encoder provides more distinct state change representations than text embeddings, reducing interference or "forgetting" in reasoning steps.

### Mechanism 3: Distillation for Algorithmic Consistency
Models distilled from stronger reasoners exhibit superior consistency in mental modeling tasks compared to standard instruct models. Standard training optimizes for next-token prediction over diverse corpora, potentially prioritizing pattern completion. Distillation from reasoning teacher models transfers specific algorithmic traces for maintaining consistency over long chains of logic, suggesting mental modeling is a learnable algorithmic behavior.

## Foundational Learning

- **Mental Models (Johnson-Laird)**: The paper defines evaluation based on cognitive science concepts of internal task representations that evolve. Understanding this distinction helps differentiate between retrieving answers and simulating scenarios. *Quick check*: Can you explain the difference between pattern matching a solution and updating a mental model?

- **State-Space Tracking**: The core failure mode is inability to maintain accurate state vectors as operations accumulate. This classic computer science problem (managing state) applied to LLMs is central to understanding the performance gap. *Quick check*: In a multi-step word problem, what constitutes the "state" that must be preserved between steps?

- **Modality Gap**: The paper relies on text-vision modality interaction, with the "Modality Switching" experiment highlighting imperfect alignment in current models. *Quick check*: Why might a model solve a problem when shown a graph but fail when reading a text description of the same graph?

## Architecture Onboarding

- **Component map**: Textual Sentence + Previous State (JSON text/Image) -> Multimodal LLM -> Next State (JSON) -> Regex parser for final answer
- **Critical path**: The "Update Step" (FormatStep) is crucial. The model must ingest Current State and New Information to produce Next State. Hallucinations here compound, causing depth performance degradation.
- **Design tradeoffs**: TextMM is computationally cheaper but prone to state drift; VisMM requires generating graphical representations (high overhead) but provides better grounding. Explicit state (JSON) is less flexible than implicit state (CoT) but more interpretable.
- **Failure signatures**: Hallucinated URLs (fake image links), compounding error (accuracy drops with depth), and modality misalignment (TextMM fails while VisMM succeeds on same steps).
- **First 3 experiments**: 1) Baseline Reproduction: Run TextMM on Llama-3.2-3B to verify performance drop phenomenon. 2) Modality Switch Analysis: Identify TextMM failure steps, then run VisMM on just those steps. 3) State Sanitization Test: Introduce correction step to verify previous JSON state before updating.

## Open Questions the Paper Calls Out

1. Can improving alignment between TextMM and VisMM representations enhance TextMM's performance? The Oracle experiment showed whenever TextMM was correct, VisMM was also correct, suggesting misaligned text representations that vision compensates for.

2. Why does distillation from stronger reasoning models enhance mental modeling capability when standard training does not? The paper observes distilled models maintain stable accuracy but doesn't investigate the mechanismâ€”whether it involves better instruction-following, improved state-tracking, or different internal representations.

3. Why do larger multimodal models (90B) underperform smaller ones (11B) in VisMM, and what causes hallucination behavior? The overfitting explanation is speculative; the paper doesn't determine whether this is training data, scale-related architectural problems, or specific to how larger models process visual inputs incrementally.

4. Do LMs employ alternative forms of mental representation not captured by JSON object structures? The JSON format may force models into representations that don't match their natural internal encoding, potentially underestimating their true mental modeling capacity.

## Limitations
- Model capacity vs. reasoning depth: The observation that smaller models sometimes outperform larger ones lacks clear mechanistic explanation beyond speculation about overfitting.
- Visual encoding reliability: Insufficient detail on how visual graphs are generated from problem states raises questions about the validity of the vision encoder's role.
- Distillation effect isolation: The comparison conflates distillation effects with potential differences in base model architecture or training data without ablation studies.

## Confidence
- High Confidence: The core finding that mental modeling approaches underperform CoT prompting, suggesting pattern recognition rather than dynamic reasoning.
- Medium Confidence: The observation that smaller multimodal models sometimes outperform larger ones, and the attribution to overfitting mechanisms.
- Low Confidence: The reliability of visual encoding in VisMM and the specific contribution of distillation to reasoning performance.

## Next Checks
1. Implement systematic comparison of intermediate JSON states between TextMM and VisMM approaches, extracting and comparing predicted state updates to ground truth values to identify specific failure modes.

2. Evaluate mental modeling approaches on a distinct dataset of incremental reasoning problems from other sources to test whether limitations are specific to MATHWORLD's structure or represent broader LM constraints.

3. Create controlled experiments where visual graphs in VisMM are systematically degraded (removing color coding, simplifying bar charts, introducing visual noise) to isolate specific visual features that enhance reasoning.