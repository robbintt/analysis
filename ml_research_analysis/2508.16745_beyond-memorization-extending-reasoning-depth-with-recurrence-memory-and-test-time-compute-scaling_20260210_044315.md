---
ver: rpa2
title: 'Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and
  Test-Time Compute Scaling'
arxiv_id: '2508.16745'
source_url: https://arxiv.org/abs/2508.16745
tags:
- reasoning
- arxiv
- state
- rule
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the reasoning capabilities of large language
  models by training them on one-dimensional cellular automata (1dCA) tasks. The authors
  created a benchmark where models must infer underlying Boolean rules from state
  sequences and then apply these rules to predict future states.
---

# Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory and Test-Time Compute Scaling

## Quick Facts
- arXiv ID: 2508.16745
- Source URL: https://arxiv.org/abs/2508.16745
- Reference count: 40
- Models struggle with multi-step reasoning beyond two steps even with increased depth or recurrence

## Executive Summary
This paper investigates the reasoning capabilities of large language models using one-dimensional cellular automata (1dCA) as a controlled benchmark. The authors find that while models can achieve high accuracy on single-step predictions, their performance degrades sharply for multi-step reasoning tasks. The study systematically compares various architectures including transformers, LSTMs, state-space models, and the Associative Recurrent Memory Transformer (ARMT), concluding that increasing model depth is more effective than width for improving reasoning capabilities.

To address the challenge of deep reasoning, the paper explores multiple techniques including Adaptive Computation Time (ACT), reinforcement learning with GRPO, and Chain-of-Thought training. The key finding is that explicit intermediate supervision through Chain-of-Thought approaches remains the most effective method for enabling deep reasoning, while ACT and GRPO provide valuable alternatives when such supervision is unavailable. The paper demonstrates that most neural architectures struggle with multi-step reasoning beyond two steps, even with increased depth or recurrence.

## Method Summary
The authors created a benchmark using one-dimensional cellular automata where models must infer underlying Boolean rules from state sequences and apply these rules to predict future states. By using disjoint train/test rule sets, they ensure models cannot simply memorize but must generalize. The study compares multiple architectures including transformers, LSTMs, state-space models (Mamba), and ARMT. Performance is evaluated across single-step and multi-step prediction tasks, with particular focus on how different architectures and training techniques affect reasoning depth.

## Key Results
- All models achieve high accuracy for single-step predictions but performance drops sharply for multi-step reasoning tasks
- Increasing model depth is more effective than increasing width for improving reasoning capabilities
- Chain-of-Thought training with step-by-step supervision achieves near-perfect accuracy for up to four-step predictions
- ACT and GRPO provide modest improvements but are less effective than explicit intermediate supervision

## Why This Works (Mechanism)
The paper demonstrates that deep reasoning in neural networks requires explicit intermediate supervision or variable computation steps. When models must chain multiple reasoning steps together, they struggle without guidance. ACT allows models to allocate more computation to difficult steps, while GRPO enables learning reasoning traces without explicit supervision. However, Chain-of-Thought training with step-by-step supervision remains most effective because it provides clear intermediate targets that guide the reasoning process.

## Foundational Learning
- **Cellular Automata**: Discrete models where simple rules generate complex behavior through iteration - needed for creating controlled reasoning benchmarks with clear ground truth
- **Multi-step reasoning**: Chaining multiple inference steps together - quick check: models must predict future states beyond immediate neighbors
- **Attention mechanisms**: Weighting input importance dynamically - needed for transformer architectures to process sequences
- **Recurrent connections**: Feedback loops that maintain state across steps - quick check: essential for processing temporal dependencies
- **Reinforcement learning**: Training through rewards rather than explicit targets - needed for GRPO approach when supervision is unavailable
- **Adaptive computation**: Variable processing steps based on difficulty - quick check: ACT adjusts token-level computation time

## Architecture Onboarding

**Component Map**: Input -> Embedding -> Transformer/LSTM/Mamba/ARMT -> Output

**Critical Path**: State sequence input → Rule inference → Multi-step prediction → Accuracy evaluation

**Design Tradeoffs**: Depth vs width optimization, recurrence vs attention, supervised vs unsupervised reasoning

**Failure Signatures**: Sharp accuracy drop after 2-3 reasoning steps, inability to generalize beyond training rule sets, overfitting to single-step patterns

**First Experiments**: 
1. Single-step prediction accuracy across all architectures
2. Multi-step prediction accuracy (2-4 steps) comparison
3. ACT and GRPO performance on 4-step tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Findings may not generalize beyond cellular automata domain to other reasoning tasks
- Study focuses on specific architectures and may not capture all reasoning approaches
- Current ACT and GRPO results may represent optimization opportunities rather than true limitations

## Confidence
- Model architecture performance on single-step tasks: High
- Multi-step reasoning capabilities beyond 2-3 steps: Medium
- Effectiveness of Chain-of-Thought methods with supervision: High
- ACT and GRPO performance improvements: Medium

## Next Checks
1. Test model performance on multi-step reasoning tasks outside cellular automata domain (arithmetic, symbolic reasoning)
2. Evaluate whether increasing depth remains more effective than width for longer reasoning chains (beyond 4 steps)
3. Compare ACT and GRPO performance with more extensive hyperparameter tuning and different reward structures