---
ver: rpa2
title: 'CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning'
arxiv_id: '2507.20411'
source_url: https://arxiv.org/abs/2507.20411
tags:
- concept
- multilingual
- language
- captions
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CONCAP introduces a multilingual image captioning model that augments
  retrieval with both captions and image-specific concepts to improve performance
  across languages. It addresses the challenge of limited multilingual data and biases
  in translated captions by combining retrieved text with targeted concept retrieval.
---

# CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning

## Quick Facts
- arXiv ID: 2507.20411
- Source URL: https://arxiv.org/abs/2507.20411
- Authors: George Ibrahim; Rita Ramos; Yova Kementchedjhieva
- Reference count: 26
- Key outcome: CONCAP outperforms state-of-the-art multilingual captioning models like Pangea and mBLIP, achieving a CIDEr score of 34.2 on 36 languages with far fewer parameters and training samples.

## Executive Summary
CONCAP introduces a multilingual image captioning model that augments retrieval with both captions and image-specific concepts to improve performance across languages. It addresses the challenge of limited multilingual data and biases in translated captions by combining retrieved text with targeted concept retrieval. Experiments on XM3600 show CONCAP outperforms state-of-the-art models like Pangea and mBLIP, achieving a CIDEr score of 34.2 on 36 languages while using far fewer parameters and training samples. Ablation studies confirm that both caption and concept retrieval contribute additively, and language-specific retrieval during training is critical. Concept enrichment attempts show mixed results, with oracle-based cultural concepts improving early-stage performance but not sustaining gains.

## Method Summary
CONCAP uses a frozen BLIP-2 vision encoder and mT0-XL language decoder with LoRA adapters, retrieving both captions and concepts to condition caption generation. Training uses an English-pivot retrieval strategy on translated datasets, while evaluation uses language-specific retrieval. The model retrieves top-4 captions and top-10 concepts per image, formatting them into a structured prompt with visual features passed through a Q-Former to the decoder.

## Key Results
- CONCAP achieves CIDEr 34.2 on XM3600 across 36 languages, outperforming Pangea and mBLIP.
- Both caption and concept retrieval contribute additively to performance gains.
- English-pivot retrieval during training outperforms direct language-specific retrieval on translated datasets.
- Concept enrichment with external cultural terms degrades performance, though oracle-based cultural concepts show initial benefits that fade with continued training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining retrieved captions with retrieved concepts improves multilingual image captioning performance more than either signal alone.
- Mechanism: Retrieved captions provide fluent syntactic patterns in the target language, while retrieved concepts ground generation in specific visual elements present in the image. The two signals complement each other: captions guide linguistic structure, concepts ensure content coverage and counteract noise from imperfect caption matches.
- Core assumption: The retrieval model can identify visually relevant concepts and captions with sufficient accuracy, and the language decoder can integrate both signals without excessive noise.
- Evidence anchors:
  - [abstract]: "CONCAP enables strong performance on low- and mid-resource languages, with highly reduced data requirements... highlighting the effectiveness of concept-aware retrieval augmentation."
  - [section 4.3]: "CONCAP considerably outperforms both ConRAG and CapRAG, indicating that the gains from these two forms of retrieval augmentation are additive."
  - [corpus]: Related work on concept-based retrieval (EVCap, VLCAP) confirms concept grounding can improve caption quality, though multilingual settings remain underexplored.
- Break condition: If retrieved captions are highly noisy or semantically mismatched, they can degrade performance below a no-retrieval baseline—observed in CapRAGM experiments.

### Mechanism 2
- Claim: Retrieving captions via an English pivot during training is more effective than direct language-specific retrieval for translated datasets.
- Mechanism: In datasets like COCO-35L where captions are machine-translated from English, shared caption IDs allow English retrieval to map reliably to target-language equivalents. Direct multilingual retrieval (e.g., mSigLIP) on these translated captions performs poorly, likely due to lower retrieval quality and alignment artifacts.
- Core assumption: The training dataset has parallel English captions with aligned IDs; without this, pivot retrieval is not applicable.
- Evidence anchors:
  - [section 4.4]: "CapRAGM considerably underperforms CapRAG across all individual languages... even than NoRAG, i.e., in this scenario, caption retrieval actively hurts performance."
  - [section 3.2]: "For training on COCO-35L, we adopt an English-pivot retrieval strategy following prior work."
  - [corpus]: Multilingual RAG literature notes language drift when evidence language mismatches query language (Language Drift in Multilingual RAG).
- Break condition: For culturally diverse, natively multilingual data without English pivots, language-specific retrieval is necessary but requires a high-quality multilingual retriever.

### Mechanism 3
- Claim: Simply enriching concept lists with external cultural terms does not improve performance and can degrade it.
- Mechanism: Adding broader lexicons (e.g., PangeaIns cultural terms, Wikipedia) introduces noisy or less relevant concepts. The retrieval similarity space may surface terms that are linguistically plausible but visually irrelevant, distracting the decoder.
- Core assumption: The base concept list from training data is sufficiently representative of encountered concepts, and the retriever's similarity metric does not reliably distinguish culturally relevant vs. irrelevant terms.
- Evidence anchors:
  - [section 5]: "ConRAGRich indicates a negative impact from concept enrichment: performance drops by 1.7 CIDEr points relative to ConRAG."
  - [section 5]: "Oracle experiments with cultural concepts show that this form of retrieval augmentation improves captioning initially, but the benefits fade as training progresses."
  - [corpus]: No direct corpus evidence on concept enrichment failures in multilingual captioning; this remains an open research direction.
- Break condition: Curated, high-quality cultural concept lists (e.g., oracle experiments) can help early in training, but sustained gains require models to integrate new vocabulary more robustly.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: CONCAP relies on RAG to condition caption generation on external retrieved information (captions and concepts), reducing the need for large-scale multilingual training.
  - Quick check question: Can you explain how RAG differs from standard fine-tuning in terms of how external knowledge is incorporated at inference time?

- Concept: Vision-Language Alignment (CLIP-style models)
  - Why needed here: Caption and concept retrieval use vision-language encoders (CLIP, mSigLIP) to embed images and text in a shared space for similarity search.
  - Quick check question: How does contrastive pre-training enable an image encoder and text encoder to retrieve semantically similar cross-modal content?

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: CONCAP freezes most model weights and only trains LoRA layers in the language decoder, keeping trainable parameters at ~111M.
  - Quick check question: What are the practical benefits of LoRA for adapting large language decoders compared to full fine-tuning?

## Architecture Onboarding

- Component map:
  - Vision Encoder (frozen) -> CLIP-ViT or similar -> Image embeddings
  - Text Encoder (frozen) -> CLIP/mSigLIP text encoder -> Caption and concept retrieval indexing
  - Q-Former + Projection Layer (trainable) -> Bridges vision and language modalities
  - Language Decoder (frozen backbone + LoRA) -> mT0-XL with injected LoRA layers -> Generates captions conditioned on retrieved text and visual features
  - Retrieval Datastores -> Separate FAISS indices for captions and concepts per language

- Critical path:
  1. Image is encoded; similarity search retrieves top-n captions and top-m concepts from language-specific datastores.
  2. Retrieved captions and concepts are formatted into a structured prompt (English prompt template, target-language generation).
  3. Prompt + visual features are fed through Q-Former to decoder, which generates the caption in the target language.

- Design tradeoffs:
  - English-pivot vs. language-specific retrieval during training (pivot is more robust for translated datasets; language-specific is necessary for native multilingual data).
  - Number of retrieved items (n=4 captions, m=10 concepts chosen empirically; more concepts add inference cost with diminishing returns).
  - Concept list curation (base list from training data is noisy but sufficient; enrichment introduces more noise than signal).

- Failure signatures:
  - Caption retrieval hurting performance: Indicates retriever quality is insufficient or retrieved captions are too noisy (as in CapRAGM).
  - Degraded performance after concept enrichment: External terms may be irrelevant; check retrieval precision.
  - Model ignoring retrieved concepts late in training: Observed in oracle experiments—vocabulary drift may limit integration.

- First 3 experiments:
  1. Replicate NoRAG vs. CapRAG vs. ConRAG vs. CONCAP ablations on a held-out subset to confirm additive contributions of caption and concept retrieval.
  2. Compare English-pivot retrieval vs. direct language-specific retrieval (CapRAGM style) on a natively multilingual dataset without English translations.
  3. Test concept enrichment with progressively larger external lexicons (CX → CXP → CXPW) and measure per-language CIDEr impact to identify which languages benefit or degrade.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific filtering or curation methods can effectively integrate culturally relevant concepts into the retrieval datastore without introducing the noise that degraded performance in this study?
- Basis in paper: [explicit] The authors state in the Conclusion that "Future work should investigate more effective ways for concept enrichment" after finding that adding cultural terms from PangeaIns and Wikipedia introduced noise and lowered CIDEr scores.
- Why unresolved: The paper demonstrates that simply appending cultural terms hurts performance ("dilute the retrieval quality"), but leaves the solution for integrating this knowledge open.
- What evidence would resolve it: A method that filters or weights concept candidates such that an enriched datastore yields higher CIDEr scores on culturally diverse datasets (like XM3600) compared to the baseline COCO-35L datastore.

### Open Question 2
- Question: Can training objectives be modified to prevent the fading of benefits from retrieved cultural concepts as training progresses?
- Basis in paper: [explicit] The paper notes that while oracle-based cultural concepts improved performance initially, "continued finetuning seems to shift the decoder’s output distribution," causing the model to revert to its learned vocabulary and ignore the retrieved concepts.
- Why unresolved: The mechanism causing the model to ignore retrieved prompts (lexical items not in the training set) after fine-tuning is identified as a limitation but not solved.
- What evidence would resolve it: A training schedule or regularization technique that results in sustained or improved utilization of oracle-retrieved concepts at the final training checkpoint compared to the early epochs.

### Open Question 3
- Question: Does improving the semantic alignment of multilingual retrievers enable effective language-specific caption retrieval during training?
- Basis in paper: [inferred] The failure of the CapRAGM model (language-specific retrieval at training) is attributed to the retriever quality being insufficient compared to the English-pivot strategy, implying that better retrievers might unlock this approach.
- Why unresolved: The paper establishes that language-specific retrieval currently "actively hurts performance," but it remains unclear if this is a fundamental limitation of the approach or a byproduct of current retriever capabilities (mSigLIP).
- What evidence would resolve it: An experiment substituting the retriever with a higher-performance multilingual model that allows CapRAGM to match or exceed the English-pivot baseline.

## Limitations

- CONCAP's training setup assumes parallel English captions with shared IDs, which may not hold for truly native multilingual datasets.
- Concept enrichment with external cultural terms degrades performance, though the underlying issue of integrating culturally relevant knowledge remains unsolved.
- Evaluation focuses on a single test set (XM3600), limiting robustness claims across domains.

## Confidence

- **High confidence:** Retrieval augmentation (caption + concept) improves performance over baselines (CIDEr gains of 1.4-1.8 points).
- **Medium confidence:** English-pivot retrieval is superior to direct language-specific retrieval for translated datasets; concept enrichment generally degrades performance.
- **Low confidence:** Sustained benefits of oracle cultural concepts; generalizability to non-translated, natively multilingual data.

## Next Checks

1. **Generalization Test:** Evaluate CONCAP on a natively multilingual image captioning dataset (e.g., Multi30K with non-English source captions) to verify if English-pivot retrieval assumptions break down and whether direct language-specific retrieval becomes necessary.

2. **Concept Enrichment Variants:** Systematically test concept enrichment with progressively curated lists (CX → CXP → CXPW) on low-resource languages (e.g., Swahili, Hindi) to identify if specific linguistic or cultural domains benefit from external knowledge.

3. **Retrieval Quality Analysis:** Measure retrieval precision@k for captions and concepts on XM3600 to quantify the noise introduced by language-specific retrieval and determine if retriever quality correlates with downstream performance degradation.