---
ver: rpa2
title: 'TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment'
arxiv_id: '2506.06343'
source_url: https://arxiv.org/abs/2506.06343
tags:
- speech
- encoder
- arxiv
- text
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training speech-capable language
  models without requiring paired speech-text data or extensive computational resources.
  The proposed TESU-LLM framework leverages a unified text-speech encoder to map semantically
  equivalent text and speech inputs to a shared latent space, then aligns these representations
  with a pre-trained LLM through a lightweight projection network.
---

# TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment

## Quick Facts
- arXiv ID: 2506.06343
- Source URL: https://arxiv.org/abs/2506.06343
- Authors: Taesoo Kim; Jong Hwan Ko
- Reference count: 30
- Key outcome: TESU-LLM achieves competitive speech-language modeling performance without paired speech-text data or extensive computational resources

## Executive Summary
TESU-LLM addresses the challenge of training speech-capable language models without requiring paired speech-text data or extensive computational resources. The framework leverages a unified text-speech encoder to map semantically equivalent text and speech inputs to a shared latent space, then aligns these representations with a pre-trained LLM through a lightweight projection network. By training only the projection network using text-only supervision, TESU-LLM achieves strong performance on speech-related benchmarks while avoiding the need for time-aligned segmentation, TTS synthesis, or speech data.

## Method Summary
TESU-LLM introduces a novel approach to speech-language modeling by unifying text and speech representations through a shared encoder architecture. The framework maps both modalities to a common latent space, enabling alignment with pre-trained language models through a lightweight projection network. The key innovation lies in training this projection network using only text data, eliminating the need for paired speech-text datasets or computationally expensive multimodal training. This approach leverages pre-trained models for both encoding and language understanding components, making it resource-efficient while maintaining competitive performance across speech-related tasks.

## Key Results
- Achieves 4.17 on AlpacaEval, 3.91 on CommonEval, and 58.23 on SD-QA on VoiceBench benchmark
- Demonstrates zero-shot ASR performance of 3.61 WER on test-clean
- Achieves 24.30 BLEU on CoVoST2 for speech-to-text translation
- Matches or exceeds models trained with large-scale multimodal datasets

## Why This Works (Mechanism)
The framework works by leveraging semantic equivalence between text and speech representations through a unified encoder architecture. By mapping both modalities to a shared latent space, TESU-LLM can exploit the rich semantic understanding of pre-trained language models while maintaining modality-agnostic representations. The projection network serves as an adapter that bridges the unified encoder outputs to the LLM's expected input format, enabling transfer learning without requiring paired data or extensive fine-tuning of the LLM itself.

## Foundational Learning
- **Unified Encoder Architecture**: Combines text and speech processing into a single framework - needed for creating shared semantic representations across modalities, quick check: verify encoder produces comparable embeddings for semantically equivalent text/speech pairs
- **Modality Alignment**: Maps different input types to common latent space - needed to enable cross-modal transfer learning, quick check: measure cosine similarity between aligned text-speech representations
- **Projection Network Training**: Lightweight adapter for LLM integration - needed to avoid expensive LLM fine-tuning, quick check: validate projection network performance on held-out text data before speech evaluation
- **Zero-shot Learning**: Ability to perform tasks without task-specific training - needed for practical deployment scenarios, quick check: test model on out-of-domain speech samples
- **Pre-trained Model Leveraging**: Building on existing encoder and LLM weights - needed for computational efficiency and leveraging existing knowledge, quick check: compare performance with random initialization

## Architecture Onboarding

**Component Map**: Unified Encoder -> Projection Network -> Pre-trained LLM

**Critical Path**: Speech/Text Input → Unified Encoder → Shared Latent Space → Projection Network → LLM → Output

**Design Tradeoffs**: The framework trades computational efficiency for potential accuracy limitations in capturing fine-grained acoustic patterns. By avoiding paired speech-text training data, it significantly reduces resource requirements but may miss nuanced acoustic-phonetic information that would be captured through direct multimodal training.

**Failure Signatures**: Poor performance on diverse acoustic conditions (accents, background noise), degraded accuracy on long-form speech content, limited generalization to out-of-domain speech samples, and potential misalignment between unified encoder outputs and LLM expectations.

**3 First Experiments**:
1. Test unified encoder performance on benchmark text-only tasks to verify baseline capability
2. Evaluate projection network alignment quality using held-out text data before introducing speech
3. Measure cross-modal consistency by comparing model outputs for semantically equivalent text and speech inputs

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily focuses on controlled benchmark settings with limited testing on diverse real-world speech variations including accents, background noise, and speaker variability
- Reliance on text-only supervision for training the projection network may constrain the model's ability to capture fine-grained acoustic-phonetic patterns
- Limited testing on long-form speech content and conversational speech beyond sentence-level or short utterance benchmarks

## Confidence

**Framework Efficacy (Medium Confidence)**: Claims competitive performance on speech benchmarks without speech data, but evaluation scope is narrow and comparison with large-scale multimodal models could be more comprehensive.

**Modality Alignment (High Confidence)**: Unified encoder approach for mapping text and speech to shared latent space is technically sound and well-grounded in existing literature.

**Zero-shot Performance (Medium Confidence)**: Notable results on ASR and speech-to-text translation, but these tasks are typically more challenging in realistic conditions than benchmark settings.

## Next Checks

1. Evaluate TESU-LLM's robustness across diverse acoustic conditions, including varying accents, background noise levels, and recording qualities, to assess real-world applicability beyond controlled benchmarks.

2. Conduct ablation studies to quantify the contribution of the unified encoder alignment versus the projection network training, particularly examining the impact of different pre-trained model combinations.

3. Test the model's performance on long-form speech content and conversational speech to verify scalability beyond the sentence-level or short utterance focus of current benchmarks.