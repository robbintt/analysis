---
ver: rpa2
title: 'Quality Assurance for LLM-RAG Systems: Empirical Insights from Tourism Application
  Testing'
arxiv_id: '2502.05782'
source_url: https://arxiv.org/abs/2502.05782
tags:
- testing
- evaluation
- quality
- response
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study presents a comprehensive testing framework for evaluating
  quality characteristics of LLM-RAG systems in tourism applications. Using 17 distinct
  metrics across syntactic, semantic, and behavioral dimensions, the framework systematically
  assesses three LLM variants (GPT 3.5 Turbo, GPT 4o, GPT 4o Mini) across multiple
  parameter configurations.
---

# Quality Assurance for LLM-RAG Systems: Empirical Insights from Tourism Application Testing

## Quick Facts
- arXiv ID: 2502.05782
- Source URL: https://arxiv.org/abs/2502.05782
- Reference count: 11
- Primary result: A comprehensive testing framework with 17 metrics revealed that extreme temperature/top-p values cause significant quality degradation while RAG integration preserves general metrics but is crucial for factual accuracy.

## Executive Summary
This study presents a systematic testing framework for evaluating quality characteristics of LLM-RAG systems applied to tourism applications. The framework combines 17 distinct metrics across syntactic, semantic, and behavioral dimensions to assess three LLM variants (GPT 3.5 Turbo, GPT 4o, GPT 4o Mini) across multiple parameter configurations. Results demonstrate that newer models show modest improvements in response length and complexity, while extreme parameter values (temperature=2.0, top-p=1.0) cause significant quality degradation. The research reveals that while RAG integration does not substantially affect general quality metrics, it is crucial for maintaining factual accuracy in domain-specific responses.

## Method Summary
The framework uses Evidently open-source tools to implement 17 quality metrics across 24 test configurations (3 models × 4 parameter configs × 2 RAG modes). Data from Värmland Tourism API is converted to JSONL format and indexed using FAISS for similarity search. Test cases are embedded in parallel, retrieved contextually, and passed to LLMs for generation. Quality is assessed through text metrics (character/word counts, OOV ratios), semantic similarity (embedding-based and BERT-based cosine similarity), and LLM-judge evaluations (sentiment, toxicity, neutrality, request fulfillment, privacy compliance, tone, bias, content safety).

## Key Results
- Extreme parameter values (temperature=2.0, top-p=1.0) cause up to 64% quality degradation with sentiment scores dropping from 0.99 to 0.35
- Newer LLM models demonstrate modest improvements primarily in response length and complexity metrics
- RAG integration preserves general quality metrics but is essential for domain-specific factual accuracy, which requires separate evaluation
- Temperature/top-p combinations significantly affect response coherence and safety characteristics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extreme parameter values (temperature=2.0, top-p=1.0) cause measurable quality degradation in LLM outputs, while conservative settings produce more reliable responses.
- Mechanism: High temperature increases stochastic variation in token selection; high top-p expands the candidate token pool. Combined, they introduce sufficient randomness that semantic coherence breaks down, manifesting as toxicity increases, sentiment instability, and reduced neutrality.
- Core assumption: The relationship between parameter values and quality degradation is monotonic beyond an unspecified threshold.
- Evidence anchors:
  - [abstract] "extreme parameter values (temperature=2.0, top-p=1.0) cause significant quality degradation"
  - [section VI.A] "strong decrease in model performance with extreme values... dropping from a mean sentiment estimation of 0.99 to 0.35"
  - [corpus] Neighbor paper "Rethinking Testing for LLM Applications" discusses non-determinism and context dependence challenges.

### Mechanism 2
- Claim: A multi-layered testing framework (syntactic → semantic → behavioral) captures distinct quality dimensions that single-metric approaches miss.
- Mechanism: Syntactic metrics detect structural anomalies; semantic similarity measures meaning alignment; LLM-judge evaluations assess safety, bias, and intent fulfillment. Together they provide orthogonal signals.
- Core assumption: LLM judges themselves are sufficiently reliable evaluators for the behavioral metrics.
- Evidence anchors:
  - [abstract] "17 distinct metrics across syntactic, semantic, and behavioral dimensions"
  - [section IV.B] "five general text metrics... four semantic similarity tests... eight LLM-based evaluation tests"
  - [corpus] Neighbor "ChatChecker" uses non-cooperative user simulation for dialogue testing.

### Mechanism 3
- Claim: RAG integration preserves general language quality metrics while being essential for domain-specific factual accuracy.
- Mechanism: RAG augments context without altering the base model's generation characteristics. General metrics remain stable because they measure linguistic properties, not factual correctness. Domain accuracy requires separate evaluation not captured by these metrics.
- Core assumption: The test cases designed for tourism queries do not inherently reward RAG-specific behavior in general metrics.
- Evidence anchors:
  - [abstract] "RAG integration does not substantially affect general quality metrics, it is crucial for maintaining factual accuracy"
  - [section VI.B] "traditional evaluation metrics might need adaptation to fully capture the benefits of RAG in domain-specific applications"
  - [corpus] Neighbor "NANOGPT" applies LLM-RAG to nanotechnology research.

## Foundational Learning

- Concept: **Temperature and Top-p Sampling Parameters**
  - Why needed here: Understanding these controls is prerequisite to interpreting parameter configuration results and avoiding deployment failures.
  - Quick check question: If you set temperature=0, what behavior should you expect compared to temperature=1?

- Concept: **Embedding-based vs. BERT-based Semantic Similarity**
  - Why needed here: The study uses both methods and reports different score distributions; understanding why prevents misinterpretation of results.
  - Quick check question: Why might BERT similarity scores be systematically lower than embedding-based scores for the same response pairs?

- Concept: **LLM-as-Judge Evaluation Pattern**
  - Why needed here: 8 of 17 metrics rely on LLM judges; understanding their limitations is critical for trusting behavioral evaluations.
  - Quick check question: What failure mode might occur if the judge LLM shares the same biases as the system under test?

## Architecture Onboarding

- Component map: Data Acquisition → Värmland Tourism API → JSONL format → Embedding Generation → Parallel processing → FAISS vector store → Query Processing → Embedding → Similarity search → Context retrieval → Augmented Generation → LLM produces travel plan using retrieved context → Testing Framework → Evidently platform → 17 metrics → Database storage → HTML reports

- Critical path: Query embedding generation → FAISS similarity search → Context retrieval quality → LLM generation. Retrieval failures propagate directly to response accuracy regardless of LLM capability.

- Design tradeoffs:
  - FAISS chosen for retrieval speed; tradeoff is approximate vs. exact search
  - 10 test cases balance coverage vs. computational cost; may miss edge cases
  - 4 parameter configurations sampled; threshold effects not precisely identified
  - Fact-verification metrics excluded due to RAG dependency; limits accuracy assessment

- Failure signatures:
  - Sentiment score drops below 0.5 → likely temperature/top-p misconfiguration
  - High OOV ratio → potential tokenization or language mismatch
  - BERT similarity near zero while embedding similarity high → check embedding model compatibility
  - Toxicity "unknown" classification → judge model uncertain, investigate prompt

- First 3 experiments:
  1. Replicate baseline configuration (temp=0, top-p=0) with RAG enabled/disabled on 5 new tourism queries to validate RAG's effect on factual accuracy vs. general metrics.
  2. Binary search between temp=1 and temp=2 to identify degradation threshold more precisely than the study's 4-point sampling.
  3. Add a fact-verification metric using ground-truth tourism data to quantify the RAG accuracy benefit that general metrics miss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: At what specific parameter value thresholds do temperature and top-p settings cause the steep decline in response quality observed in LLM-RAG systems?
- Basis in paper: [explicit] The authors state that due to the low number of sampled parameter values, "it is difficult to tell at which threshold specifically this drop in performance occurs" between baseline and extreme configurations.
- Why unresolved: The experimental design used only four distinct configurations (0, 1, 2), lacking the granularity to identify the exact inflection point where randomness degrades output coherence.
- What evidence would resolve it: A fine-grained parameter sweep (e.g., increments of 0.1) plotting quality metrics against parameter values to pinpoint the exact curve of degradation.

### Open Question 2
- Question: Is the proposed testing framework equally effective when applied to non-OpenAI or open-source Large Language Models?
- Basis in paper: [explicit] The paper lists "expanding the evaluation framework to include a broader range of LLM architectures" as a primary direction for future work.
- Why unresolved: The empirical study was restricted to three specific OpenAI variants (GPT-3.5 Turbo, GPT-4o, GPT-4o Mini) to ensure controlled comparison, leaving other model families untested.
- What evidence would resolve it: Replicating the 24 distinct test configurations using open-source models (e.g., Llama, Mistral) and comparing the variance in syntactic, semantic, and behavioral metric scores.

### Open Question 3
- Question: How can evaluation frameworks be adapted to quantify the benefits of RAG on domain-specific factual accuracy when general quality metrics remain unchanged?
- Basis in paper: [explicit] The conclusion calls for "developing more sophisticated metrics for domain-specific knowledge integration," noting that RAG's value was not reflected in general quality scores.
- Why unresolved: The study found that while RAG is crucial for accuracy, the implemented general metrics (sentiment, toxicity, semantic similarity) showed minimal difference between RAG and non-RAG responses.
- What evidence would resolve it: Developing and validating a new metric specifically for "factual precision" that correlates with human verification of tourism data in RAG-enhanced responses.

## Limitations
- The 10-test-case sample size may not capture rare failure modes or edge cases, particularly for safety and bias metrics where higher variance was observed
- LLM-judge evaluations rely on unstated judge model configurations, creating potential reproducibility gaps and judge bias concerns
- The study did not investigate fine-grained parameter thresholds between tested points, missing the precise degradation onset
- Fact-verification metrics were excluded due to RAG dependency, limiting the framework's ability to quantify accuracy improvements

## Confidence
- **High confidence**: Temperature/top-p degradation effects (supported by consistent sentiment/toxicity drops across all model variants)
- **Medium confidence**: General framework applicability (requires validation across domains beyond tourism)
- **Low confidence**: LLM-judge metric reliability (no independent validation of judge consistency or bias)

## Next Checks
1. **Judge Model Validation**: Conduct inter-annotator agreement tests using human evaluators on the same test cases to establish baseline reliability for the 8 behavioral metrics, comparing human vs. LLM judge consistency.

2. **Parameter Threshold Mapping**: Implement a binary search between temperature=1 and temperature=2 with 0.1 increments to identify the precise degradation onset point for each model variant, creating deployment safety boundaries.

3. **Factual Accuracy Benchmarking**: Add ground-truth tourism data verification for RAG-generated responses, measuring accuracy rate against the general quality metrics to quantify the trade-off between linguistic quality and domain-specific correctness.