---
ver: rpa2
title: AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework
arxiv_id: '2509.10762'
source_url: https://arxiv.org/abs/2509.10762
tags:
- citation
- pillar
- engine
- page
- citations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GEO-16, a 16-pillar framework to quantify
  on-page quality signals for AI answer engine citation. Analyzing 1,702 citations
  across three engines (Brave, Google AIO, Perplexity) from 70 prompts and 1,100 URLs,
  it finds that higher GEO scores strongly predict citation (odds ratio = 4.2).
---

# AI Answer Engine Citation Behavior An Empirical Analysis of the GEO16 Framework

## Quick Facts
- arXiv ID: 2509.10762
- Source URL: https://arxiv.org/abs/2509.10762
- Reference count: 8
- Higher GEO scores strongly predict citation (OR = 4.2); pages with GEO ≥ 0.70 and ≥ 12 pillar hits achieve 78% cross-engine citation rate.

## Executive Summary
This paper introduces GEO-16, a 16-pillar framework to quantify on-page quality signals for AI answer engine citation. Analyzing 1,702 citations across Brave, Google AIO, and Perplexity from 70 prompts and 1,100 URLs, it finds that higher GEO scores strongly predict citation (odds ratio = 4.2). Pages with GEO ≥ 0.70 and ≥ 12 pillar hits achieve a 78% cross-engine citation rate. Brave cites the highest-quality pages (mean GEO 0.727), while Perplexity lags (mean GEO 0.300). Metadata & Freshness, Semantic HTML, and Structured Data show the strongest correlations with citation. Results provide actionable benchmarks for improving AI discoverability in B2B SaaS contexts.

## Method Summary
The study defines GEO-16 as 16 quality pillars scored 0-3 each, aggregated into a normalized GEO score (0-1). Researchers collected 1,702 citations from 70 B2B SaaS prompts across three AI engines, auditing 1,100 unique URLs. Logistic regression with domain-clustered standard errors analyzed the relationship between GEO scores and citation likelihood. The analysis identified thresholds (G ≥ 0.70, H ≥ 12) using Youden's J index to optimize predictive performance.

## Key Results
- Higher GEO scores strongly predict citation (odds ratio = 4.2)
- Pages with GEO ≥ 0.70 and ≥ 12 pillar hits achieve 78% cross-engine citation rate
- Brave cites highest-quality pages (mean GEO 0.727), Perplexity lags (mean GEO 0.300)
- Metadata & Freshness, Semantic HTML, and Structured Data show strongest correlations with citation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine-readable metadata and freshness signals increase citation likelihood in AI answer engines.
- Mechanism: AI engines parse datePublished/dateModified fields and visible timestamps to assess recency; fresh, timestamped content aligns better with time-sensitive prompts, increasing retrieval and selection probability.
- Core assumption: AI engines weight recency signals in their retrieval and selection pipelines (not directly observed; inferred from correlation).
- Evidence anchors:
  - [abstract]: "pillars related to Metadata and Freshness, Semantic HTML, and Structured Data showed the strongest associations with citation"
  - [section 5.3]: "Metadata & Freshness exhibits the strongest association (r = 0.68)"
  - [corpus]: Weak — related papers discuss citation bias and coverage but do not isolate freshness mechanisms.
- Break condition: If engines shift to evergreen weighting or ignore date metadata in future versions, this mechanism weakens.

### Mechanism 2
- Claim: Semantic HTML hierarchy improves machine parseability, reducing extraction errors.
- Mechanism: Proper H1/H2/H3 nesting, single H1 per page, and descriptive headings allow AI extractive models to segment content accurately; this lowers friction for snippet extraction.
- Core assumption: AI engines rely on DOM structure for content segmentation (supported by paper but not experimentally manipulated).
- Evidence anchors:
  - [abstract]: "Semantic HTML, and Structured Data show the strongest correlations with citation"
  - [section 3, Table 1]: "Machine-readable cues (HTML hierarchy, JSON-LD, dates) improve understanding and ranking in retrieval"
  - [corpus]: Moderate — related work (Karpukhin 2020, Khattab 2020) shows structure aids dense retrieval; indirect support.
- Break condition: If engines shift to pure vision-based or transformer-only content parsing without DOM reliance, HTML structure becomes less predictive.

### Mechanism 3
- Claim: Valid structured data (JSON-LD) increases selection probability by improving entity recognition.
- Mechanism: Schema markup (Article/TechArticle/FAQPage) with author, breadcrumb, and dates provides explicit entity and relationship signals, reducing ambiguity for LLM summarization.
- Core assumption: Engines ingest and weight JSON-LD in citation selection (not directly validated by the paper).
- Evidence anchors:
  - [section 3]: "provide valid JSON-LD (Article/TechArticle/FAQPage) with datePublished, dateModified, author, and breadcrumb"
  - [section 5.3]: "Structured Data (r = 0.63)"
  - [corpus]: Weak — corpus discusses citation bias and coverage but not schema markup directly.
- Break condition: If engines degrade schema support or prioritize off-page authority signals over on-page structured data, this correlation weakens.

## Foundational Learning

- Concept: Generative Engine Optimization (GEO) vs. traditional SEO
  - Why needed here: GEO targets citation selection in synthesized answers, not blue-link ranking; signals differ (freshness, schema, structure > backlinks).
  - Quick check question: Can you explain why a page ranking #1 in Google Search might never be cited by an AI answer engine?

- Concept: Retrieval-Augmented Generation (RAG) pipeline stages
  - Why needed here: GEO-16 pillars influence the retrieval stage (fetchability, parseability) and selection stage (trust, structure); understanding RAG helps prioritize which pillars to fix first.
  - Quick check question: In a RAG pipeline, which stage would "Semantic HTML" most directly affect — retrieval, generation, or both?

- Concept: Domain-clustered standard errors
  - Why needed here: The paper uses domain clustering to handle within-domain correlation; ignoring this would overstate significance of GEO scores.
  - Quick check question: Why would citations from the same domain tend to have similar GEO scores, and how does clustering address this?

## Architecture Onboarding

- Component map:
  GEO-16 audit engine: 16 pillar scorers (0–3 scale) → aggregator → normalized GEO score (0–1) → threshold classifier

- Critical path:
  1. Instrument page rendering (DOM + headers)
  2. Score each pillar (sub-signal extraction → band mapping)
  3. Aggregate to G and H
  4. Compare against operating thresholds
  5. Prioritize fixes: Metadata & Freshness → Semantic HTML → Structured Data (highest-impact pillars first)

- Design tradeoffs:
  - Precision vs. recall: G ≥ 0.70 + H ≥ 12 optimizes Youden's J; lowering thresholds increases recall but adds false positives.
  - Engine-specific vs. cross-engine: Brave favors higher-GEO pages; Perplexity accepts lower-GEO pages — universal thresholds may underfit Perplexity.
  - Observational vs. experimental: No ablation tests; causal claims limited.

- Failure signatures:
  - High GEO but low citation: Likely off-page authority gap or brand-owned domain bias (see Discussion and [4]).
  - Low GEO but high citation: May indicate engine-specific quirks (Perplexity) or vertical effects.
  - Pillar score instability: Temporal drift or DOM rendering inconsistencies.

- First 3 experiments:
  1. Baseline audit: Run GEO-16 on 20 B2B SaaS pages; identify mean G and H; plot pillar distribution.
  2. Threshold validation: Test G ≥ 0.70 + H ≥ 12 on held-out URLs; compute precision, recall, F1.
  3. Targeted intervention: Improve Metadata & Freshness (add datePublished/dateModified, visible timestamps) on 5 low-scoring pages; re-audit after 1 week; observe score delta.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do specific GEO-16 pillar optimizations (e.g., schema ablations) causally increase citation rates, or are the observed correlations driven by unobserved confounders?
- Basis in paper: [explicit] The authors state they "do not experimentally vary publication venues" and explicitly list "causal interventions" as a requirement for future work.
- Why unresolved: The current study utilizes an observational, cross-sectional design which can identify associations but cannot prove that improving a specific pillar causes a citation.
- What evidence would resolve it: Randomized field experiments where specific on-page pillars are manipulated while holding off-page factors constant to measure the change in citation rates.

### Open Question 2
- Question: Do the identified GEO score thresholds (G ≥ 0.70) and pillar weights generalize to non-B2B SaaS verticals or non-English content?
- Basis in paper: [explicit] The authors note the study "focuses on English-language B2B SaaS content" and admit "results may differ in other languages or sectors."
- Why unresolved: The dataset was restricted to a specific domain and language, limiting the external validity of the proposed operating points.
- What evidence would resolve it: Replicating the GEO-16 audit methodology across diverse languages (e.g., multilingual analysis) and industry verticals (e.g., e-commerce, healthcare).

### Open Question 3
- Question: How stable are the observed citation behaviors and engine preferences (e.g., Brave vs. Perplexity) over time as AI models evolve?
- Basis in paper: [explicit] The authors list "longitudinal trends" as a necessary area of future research and note engine behavior may differ in "future versions."
- Why unresolved: Data was collected at a single point in time, providing a snapshot that cannot capture temporal dynamics or algorithmic drift.
- What evidence would resolve it: A longitudinal panel study tracking the same URL set's citation performance across multiple model updates over several months.

## Limitations

- Observational Design: Strong correlations identified but causation cannot be established; unmeasured confounders may influence results.
- Temporal Stability: AI answer engines evolve rapidly; citation behavior documented may shift with algorithm updates.
- Scoring Subjectivity: Exact mapping rules from page features to 0-3 pillar scores not fully specified, introducing potential inter-rater variability.

## Confidence

- High Confidence: Observed correlation between higher GEO scores and increased citation likelihood (OR=4.2), ranking of pillar importance (Metadata & Freshness > Semantic HTML > Structured Data), and domain-clustered statistical approach.
- Medium Confidence: Specific threshold recommendations (G≥0.70, H≥12) and their predictive power for cross-engine citation, given engine volatility and potential overfitting to current dataset.
- Low Confidence: Exact causal mechanisms by which individual pillars influence retrieval and selection in RAG pipelines, as these are inferred rather than experimentally validated.

## Next Checks

1. **Ablation Study**: Systematically remove or alter individual GEO-16 pillars (e.g., strip JSON-LD, flatten heading structure) on a controlled set of pages and measure citation rate changes to establish causality.

2. **Engine-Specific Calibration**: Validate the universal thresholds (G≥0.70, H≥12) on a new, temporally distinct dataset to assess their robustness against engine algorithm updates.

3. **Multi-Vertical Generalization**: Replicate the analysis across non-B2B SaaS domains (e.g., healthcare, news, e-commerce) to determine if the GEO-16 framework and thresholds generalize beyond the studied vertical.