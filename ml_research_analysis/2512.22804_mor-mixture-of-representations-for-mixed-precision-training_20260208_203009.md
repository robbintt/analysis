---
ver: rpa2
title: 'MoR: Mixture Of Representations For Mixed-Precision Training'
arxiv_id: '2512.22804'
source_url: https://arxiv.org/abs/2512.22804
tags:
- training
- bf16
- scaling
- tensor
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Representations (MoR), a dynamic
  quantization framework that selects between FP8 and BF16 formats based on tensor
  properties. The key contribution is the Group Amax Mantissa (GAM) scaling algorithm,
  which combines the dynamic range of E8M0 with the precision of an FP32 mantissa.
---

# MoR: Mixture Of Representations For Mixed-Precision Training

## Quick Facts
- arXiv ID: 2512.22804
- Source URL: https://arxiv.org/abs/2512.22804
- Reference count: 30
- Primary result: Achieves 98.38% FP8 tensor quantization while matching BF16 baseline accuracy on Nemotron-3 8B

## Executive Summary
This paper introduces Mixture-of-Representations (MoR), a dynamic quantization framework that selects between FP8 and BF16 formats based on tensor properties. The key contribution is the Group Amax Mantissa (GAM) scaling algorithm, which combines the dynamic range of E8M0 with the precision of an FP32 mantissa. Experiments on the Nemotron-3 8B model show that MoR achieves state-of-the-art results with 98.38% of tensors quantized to FP8, matching BF16 baseline accuracy. The framework demonstrates robustness across different partitioning strategies and datasets, enabling efficient low-precision training without fine-grained partitioning.

## Method Summary
MoR implements a dynamic mixed-precision training framework that analyzes tensor statistics at runtime to select optimal precision formats. The core algorithm uses GAM scaling, which decouples mantissa and exponent computations by maintaining a shared 23-bit mantissa from global tensor amax while computing per-block 8-bit exponents. For each tensor during training, MoR computes mean relative quantization error and compares it against a 4.5% threshold to decide between E4M3 FP8 and BF16 formats. The framework uses fake quantization simulation to measure precision loss without changing the underlying dataflow, enabling efficient evaluation of different partitioning strategies (per-tensor, per-block, per-channel) on linear layer activations, weights, and gradients.

## Key Results
- Achieves 98.38% FP8 utilization with per-channel partitioning on Nemotron-3 8B
- Matches BF16 baseline training/validation loss within 0.5% across all MoR variants
- Maintains downstream task performance (MMLU, WinoGrande, PIQA) comparable to BF16 baseline
- GAM scaling tracks BF16 dynamics more closely than fixed exponent approaches
- Dynamic format selection adapts to changing tensor statistics over training time

## Why This Works (Mechanism)

### Mechanism 1: Relative Error as Quantization Invariance
- Claim: Bounding mean relative quantization error below a threshold (~4.5%) is sufficient to preserve model quality when quantizing to E4M3, regardless of partition granularity.
- Core assumption: Relative error correlates with downstream model quality; the 4.5% threshold generalizes across training configurations.
- Evidence anchors:
  - [abstract]: "Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format."
  - [Section 4.1.1]: "We determined that a threshold of thE4M3 = 4.5% would quantize approximately 95% of the tensors to E4M3... The final training and validation losses for all MoR variants are within 0.5% of the BF16 baseline."

### Mechanism 2: Group Amax Mantissa (GAM) Scaling
- Claim: Decoupling the mantissa and exponent of scaling factors preserves the tensor's maximum value with FP32 precision while enabling per-block dynamic range adaptation.
- Core assumption: The global amax-derived mantissa is representative enough for all blocks; per-block exponents capture local dynamic range needs.
- Evidence anchors:
  - [Section 2]: "The core idea of GAM... is to decouple the mantissa and exponent of the scaling factors."
  - [Section 4.1.2, Figure 9(b)]: "The E8M0 scaling factor tracks the BF16 baseline most closely... This suggests that having a consistent mantissa across the tensor... plays a crucial role in maintaining stable training dynamics."

### Mechanism 3: Dynamic, Per-Tensor Format Selection
- Claim: Runtime analysis of tensor numerics enables adaptive precision selection that responds to changing training dynamics without fixed layer-wise or schedule-based rules.
- Core assumption: Tensor statistics vary meaningfully across layers and over time; dynamic selection captures this better than static rules.
- Evidence anchors:
  - [Section 4.1.3, Figure 14]: "The FC2 activation tensor and the FC1 gradient tensor do not start with high relative error... when training with more steps, the relative error grows and eventually exceeds the 4.5% threshold."

## Foundational Learning

- **Floating-point number representation (E4M3, E5M2, BF16)**
  - Why needed here: Understanding the tradeoff between dynamic range (exponent bits) and precision (mantissa bits) is essential to interpret why E4M3 works for most tensors but some require BF16 fallback.
  - Quick check question: Why does E5M2 have a larger dynamic range than E4M3 but lower precision for values within range?

- **Quantization scaling (per-tensor, per-block, per-channel)**
  - Why needed here: MoR evaluates multiple partitioning strategies; understanding how scaling factor granularity affects quantization error is prerequisite to interpreting the ablation results.
  - Quick check question: What is the tradeoff between per-tensor scaling (lower overhead) and per-block scaling (lower error)?

- **Relative vs. absolute error in numerical analysis**
  - Why needed here: The framework uses relative error as its acceptance metric; understanding why relative error is scale-invariant clarifies its suitability as an invariance across tensors with different magnitudes.
  - Quick check question: For a tensor with values in [0.001, 0.01], would absolute or relative error be more informative for quantization decisions?

## Architecture Onboarding

- **Component map:**
  - GAM Scaling Module -> Error Computation Module -> Format Selection Module -> Fake Quantization Wrapper

- **Critical path:**
  1. For each linear layer tensor (activation, weight, gradient), compute global amax and shared mantissa.
  2. Partition tensor (per-block/per-tensor/per-channel), compute per-block exponents.
  3. Compute relative error for each partition, aggregate to tensor-level error.
  4. Compare against threshold; quantize to E4M3 or fall back to BF16.
  5. Store metadata (mantissa + exponents) for downstream dequantization.

- **Design tradeoffs:**
  - Per-channel achieves highest FP8 efficiency (~98.38% quantized) but requires channel-aware implementation; per-tensor is simplest but has highest BF16 fallback.
  - 4.5% threshold empirically derived; higher threshold increases FP8 usage but risks quality degradation.
  - Sub-tensor offers finer control but increases GEMM complexity (may require upcasting); tensor-level is simpler and sufficient for FP8.

- **Failure signatures:**
  - Validation loss divergence from baseline: May indicate threshold too aggressive or partition strategy mismatched to tensor statistics.
  - High BF16 fallback rate (>10%): Suggests tensors have high dynamic range; consider per-channel partition or relaxed threshold.
  - Downstream task degradation despite matching training loss: May indicate overfitting to precision heterogeneity.

- **First 3 experiments:**
  1. Replicate tensor-level MoR with per-block partitioning on a small model (e.g., 1B parameters) for 100B tokens: Verify training/validation loss tracks BF16 baseline within 0.5%; log BF16 fallback percentage.
  2. Ablate threshold (3.5%, 4.5%, 5.5%): Measure tradeoff between FP8 utilization and final MMLU score; identify inflection point.
  3. Compare GAM scaling vs. E8M0-only scaling: Isolate the contribution of shared high-precision mantissa by running identical configurations with fixed E8M0 exponents (no mantissa).

## Open Questions the Paper Calls Out

- **Can invariance metrics more efficient than relative error be developed to enable aggressive quantization to formats like NVFP4?**
  - Basis: [explicit] "We plan to continue working in this direction to explore invariance metrics that are more efficient and can be used for more aggressive quantization formats (such as NVFP4)."
  - Why unresolved: Relative error is described as conservative—sufficient but not necessary—and may be too restrictive for lower-precision formats.

- **Why does the Three-Way Selection (E4M3/E5M2/BF16) sub-tensor algorithm cause overfitting to training/validation data while degrading downstream task performance?**
  - Basis: [explicit] "Investigating the precise mechanism by which the inclusion of E5M2 leads to this overfitting behavior will be continued in future works."
  - Why unresolved: The paper observes the phenomenon but does not identify whether it stems from E5M2's reduced mantissa precision or interaction with dynamic format selection.

## Limitations
- Empirical calibration approach with 4.5% threshold derived from single model run
- Focus on linear layers excludes embedding tables and normalization layers
- Reported 98.38% FP8 utilization specific to Nemotron-3 8B; generalization to other architectures unverified

## Confidence
- **High confidence**: GAM scaling mechanism's contribution to maintaining stable training dynamics
- **Medium confidence**: Sufficiency of relative error < 4.5% threshold for preserving model quality
- **Medium confidence**: Dynamic per-tensor format selection outperforming static strategies

## Next Checks
1. Run the same Nemotron-3 8B training with thresholds of 3.5%, 4.5%, and 5.5%, measuring the tradeoff between FP8 utilization percentage and final MMLU score degradation.
2. Apply MoR to a different LLM architecture (e.g., Llama-3 8B or Mistral-7B) with identical training setup to verify whether the 4.5% threshold generalizes across model families.
3. Implement both per-block and per-channel partitioning strategies on the same model and training run, measuring absolute FP8 tensor count, relative BF16 fallback percentage, and wall-clock training time.