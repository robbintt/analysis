---
ver: rpa2
title: 'MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn
  Conversations'
arxiv_id: '2512.13154'
source_url: https://arxiv.org/abs/2512.13154
tags:
- user
- clarification
- domain
- agent
- multi-agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MAC, the first multi-agent framework designed
  to resolve user ambiguities in conversational AI through interactive clarification.
  The key idea is to strategically delegate clarification tasks: a supervisor agent
  handles high-level, domain-agnostic ambiguities while domain-specific expert agents
  resolve task-specific underspecification.'
---

# MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations

## Quick Facts
- arXiv ID: 2512.13154
- Source URL: https://arxiv.org/abs/2512.13154
- Reference count: 37
- Multi-agent framework that increases task success by 7.8% (from 54.5% to 62.3%) on MultiWOZ 2.4

## Executive Summary
MAC introduces the first multi-agent framework specifically designed to resolve user ambiguities through interactive clarification in conversational AI. The system employs a supervisor agent for high-level, domain-agnostic ambiguities and domain-specific expert agents for task-specific underspecification. The framework's novelty lies in its strategic delegation of clarification tasks based on a comprehensive taxonomy of ambiguity types. Experiments demonstrate that enabling clarification at both supervisor and expert levels significantly improves task success rates while reducing dialogue turns, outperforming previous state-of-the-art TOD systems by 11.5%.

## Method Summary
The MAC framework implements a dual-level clarification system where a supervisor agent handles high-level, domain-agnostic ambiguities while domain-specific expert agents resolve task-specific underspecification. The framework uses a novel taxonomy that categorizes ambiguity types to guide when and how agents should clarify. This modular, role-aware approach is designed to be model-agnostic, allowing it to deliver consistent gains across different open-source LLM implementations. The system strategically delegates clarification tasks based on the nature of the ambiguity, with the supervisor agent managing cross-domain clarifications and expert agents handling domain-specific details.

## Key Results
- Task success increases by 7.8% (from 54.5% to 62.3%) when clarification is enabled at both supervisor and expert levels
- Dialogue turns reduced from 6.53 to 4.86 with MAC's clarification strategy
- Outperforms previous state-of-the-art TOD systems by 11.5% on MultiWOZ 2.4
- Demonstrates consistent gains across different open-source LLMs, confirming model-agnostic performance

## Why This Works (Mechanism)
MAC's effectiveness stems from its strategic delegation of clarification tasks based on a comprehensive ambiguity taxonomy. By separating high-level, domain-agnostic clarifications (handled by the supervisor agent) from task-specific underspecifications (handled by expert agents), the framework addresses ambiguities at the appropriate level of abstraction. This prevents the common failure modes of single-agent systems that either overgeneralize or get stuck in domain-specific details. The taxonomy ensures that clarification requests are triggered at optimal moments, reducing unnecessary back-and-forth while ensuring critical information is obtained. The model-agnostic design allows the framework to leverage the strengths of different LLMs without being constrained by their individual limitations.

## Foundational Learning
- **Ambiguity taxonomy**: Categorizes user clarifications into systematic types to guide agent responses. Why needed: Without systematic classification, agents cannot determine optimal clarification strategies. Quick check: Can the taxonomy cover at least 90% of clarification scenarios in benchmark datasets?
- **Multi-agent delegation**: Separates high-level and domain-specific clarifications across different agent types. Why needed: Single agents struggle to balance abstract reasoning with domain expertise. Quick check: Does task success improve when delegation is enabled versus disabled?
- **Model-agnostic framework design**: Allows MAC to work across different LLM implementations without modification. Why needed: Proprietary and open-source LLMs have different capabilities that should be leveraged appropriately. Quick check: Do performance gains persist across at least three different LLM families?

## Architecture Onboarding

Component map: User -> Supervisor Agent <-> Expert Agents -> Task Completion

Critical path: User query → Supervisor ambiguity detection → Expert domain resolution → Task execution → Supervisor confirmation

Design tradeoffs: The framework trades increased architectural complexity for improved clarification effectiveness. While single-agent systems are simpler to implement, they struggle with the dual demands of high-level reasoning and domain-specific knowledge. MAC's modular design introduces coordination overhead but enables more precise clarification strategies. The model-agnostic approach sacrifices potential optimizations for specific LLMs in favor of broader applicability.

Failure signatures: Clarification loops occur when agents repeatedly request the same information due to misunderstanding. Domain drift happens when expert agents provide information outside their scope. Timing issues arise when clarification requests are made too early or too late in the dialogue. These failures typically manifest as increased dialogue turns without corresponding task success improvements.

First experiments: (1) Test clarification effectiveness on single-turn ambiguities before multi-turn scenarios. (2) Compare supervisor-only versus expert-only clarification strategies to validate the delegation approach. (3) Measure performance degradation when the ambiguity taxonomy is intentionally disabled to confirm its contribution.

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those mentioned in the limitations section regarding dataset diversity and real-world applicability.

## Limitations
- Relies solely on MultiWOZ 2.4 benchmark, which may not capture the full diversity of real-world multi-turn conversations
- Performance measured only against open-source LLMs, leaving questions about generalizability to proprietary systems
- Taxonomy may not capture all possible user clarification scenarios in practical applications
- Does not include user studies to validate real-world effectiveness of the clarification strategies

## Confidence
- High confidence in experimental results showing improved task success rates and reduced dialogue turns within the MultiWOZ 2.4 framework
- Medium confidence in scalability and model-agnostic nature of the approach, as gains are demonstrated across different open-source LLMs but lack extensive testing with proprietary models
- Medium confidence in taxonomy effectiveness, given practical applicability beyond tested scenarios remains unverified

## Next Checks
1. Test MAC's performance on additional benchmark datasets beyond MultiWOZ 2.4, including those with different dialogue structures and domains
2. Evaluate the framework's robustness when integrated with proprietary LLMs to verify true model-agnostic performance
3. Conduct a user study with real-world interactions to validate the practical effectiveness of the ambiguity taxonomy and clarification strategies in diverse conversational contexts