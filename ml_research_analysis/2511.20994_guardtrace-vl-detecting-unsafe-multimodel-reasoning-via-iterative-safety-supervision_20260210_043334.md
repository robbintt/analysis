---
ver: rpa2
title: 'GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety
  Supervision'
arxiv_id: '2511.20994'
source_url: https://arxiv.org/abs/2511.20994
tags:
- safety
- harmful
- multimodal
- reasoning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GuardTrace-VL is the first vision-aware safety auditor designed
  to detect unsafe content in multimodal reasoning traces, not just final answers.
  It introduces GuardTrace, a novel QTA (Question-Thinking-Answer) safety benchmark
  with ~11.8K annotated multimodal reasoning examples.
---

# GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision

## Quick Facts
- **arXiv ID:** 2511.20994
- **Source URL:** https://arxiv.org/abs/2511.20994
- **Reference count:** 40
- **Primary result:** Achieves 93.1% F1 score on multimodal safety detection, a 13.5% improvement over prior methods.

## Executive Summary
GuardTrace-VL is the first vision-aware safety auditor designed to detect unsafe content in multimodal reasoning traces, not just final answers. It introduces GuardTrace, a novel QTA (Question-Thinking-Answer) safety benchmark with ~11.8K annotated multimodal reasoning examples. The model employs a three-stage training pipeline (SFT → DPO → Oracle-Guided DPO) to progressively refine safety judgments. Evaluated on in-domain and out-of-domain safety tests, GuardTrace-VL achieves an F1 score of 93.1%, a 13.5% improvement over prior multimodal guard methods. Ablation studies confirm the necessity of joint vision-language reasoning for effective safety detection.

## Method Summary
GuardTrace-VL is a vision-language safety detector that evaluates Question-Thinking-Answer (QTA) triples for unsafe content. It uses a three-stage progressive training pipeline: SFT on high-confidence safety data, DPO on ambiguous preference pairs, and Oracle-Guided DPO on expert-annotated hard negatives. The GuardTrace dataset is constructed via multimodal expansion of text safety queries, QTA generation using multiple MLRMs, and human-AI collaborative annotation with a three-tier safety scheme. The model is trained on Qwen2.5-VL-3B-Instruct and outputs structured "Analysis" and "Judgment" for each QTA input.

## Key Results
- Achieves 93.1% F1 score on GuardTrace-Test, outperforming baselines by 13.5%.
- Ablation shows progressive improvement: Base 43.61% → SFT 89.89% → SFT+DPO 92.16% → Full 93.33% F1 on S-Eval-VL.
- Full protocol annotation (90.00% accuracy) significantly outperforms simpler methods (e.g., 62.00% with LlamaGuard prompt).

## Why This Works (Mechanism)

### Mechanism 1
Monitoring the full Question-Thinking-Answer (QTA) pipeline enables detection of unsafe content hidden in intermediate reasoning traces but masked by benign final answers. GuardTrace-VL performs joint image-text analysis across all three components (question, reasoning trace, answer) rather than evaluating only input-output pairs. This allows identification of harmful intent or unsafe procedural content in the reasoning stage, even when the final answer appears safety-aligned. Core assumption: unsafe content can emerge and persist in the reasoning trajectory of multimodal models independently of the final output's safety. Evidence: [abstract] reasoning traces can contain unsafe content even when the final answer is non-harmful; [Figure 1 & Page 2] QA Guard and Text-only Guard fail to detect harmful intent in a multimodal QTA example, while GuardTrace-VL succeeds by analyzing all components jointly.

### Mechanism 2
A three-stage progressive training scheme (SFT → DPO → Oracle-Guided DPO) refines safety judgment boundaries and improves generalization to nuanced and adversarial cases. The training pipeline progressively focuses on more challenging data: (1) SFT on high-confidence examples builds foundational safety concepts; (2) DPO on ambiguous 2:1 voting examples sharpens preference discrimination; (3) Oracle-Guided DPO on hard negatives and expert-annotated ambiguous cases (1:1:1 votes) refines boundaries near the safety threshold. Core assumption: staged learning that incrementally introduces harder examples improves the model's ability to generalize and handle edge cases better than single-stage training. Evidence: [Page 4-5] describes the three-stage training scheme and its data sources; [Table II] ablation shows progressive F1 improvement across benchmarks as stages are added (e.g., S-Eval-VL F1: Base 43.61% → SFT 89.89% → SFT+DPO 92.16% → Full 93.33%).

### Mechanism 3
The GuardTrace dataset construction pipeline—through multimodal expansion, diverse QTA generation, and human-AI collaborative annotation—produces high-quality, heterogeneous supervision signals that enhance detector robustness. The dataset is built by (1) expanding text-only safety queries into diverse multimodal inputs (conventional and jailbreak images), (2) generating full QTA traces using multiple MLRMs, and (3) annotating with a three-tier safety scheme (0/0.5/1) via an ensemble of MLLMs and human expert review, stratified by consensus levels. Core assumption: diversity in input modalities (including adversarial jailbreak patterns) and nuanced annotation (including an intermediate "potentially harmful" label) are necessary to train a detector that generalizes across risk levels and attack types. Evidence: [Page 3-4, Figure 2a] details the multimodal expansion process covering conventional and jailbreak image sources; [Figure 3, Table III] shows dataset distribution across sources and safety labels, and ablation demonstrating the importance of the structured annotation protocol (full protocol: 90.00% accuracy vs. w/ LlamaGuard prompt: 62.00%).

## Foundational Learning

- **Multimodal Joint Processing**
  - Why needed here: GuardTrace-VL must fuse visual and textual information to understand context (e.g., identifying a restricted device in an image coupled with unsafe procedural text in reasoning).
  - Quick check question: Can you explain why replacing images with captions degrades safety detection performance, as shown in Figure 4?

- **Chain-of-Thought (CoT) Safety Analysis**
  - Why needed here: The model evaluates safety across explicit reasoning traces, not just final outputs. Understanding how unsafe content can be distributed through intermediate steps is critical.
  - Quick check question: In Figure 1, why do both QA Guard and Text-only Guard fail to detect the risk, and how does analyzing the "Thinking" component change the assessment?

- **Preference Optimization (DPO)**
  - Why needed here: Two of the three training stages use DPO to align the model with safety preferences, especially for ambiguous cases. Grasping how preference pairs refine decision boundaries is essential.
  - Quick check question: How does the DPO stage (using D2:1 data) improve over SFT alone, and what role does the β temperature hyperparameter play in equation (2)?

## Architecture Onboarding

- **Component map**: Base Model (Qwen2.5-VL-3B-Instruct) -> SFT (on D3:0) -> DPO (on D2:1) -> Oracle-Guided DPO (on D1:1:1 + hard negatives) -> Inference (greedy decoding, output Analysis-Judgment)
- **Critical path**: Data construction (quality & diversity) -> SFT (core safety concepts) -> DPO (ambiguity resolution) -> OGDPO (boundary refinement) -> Evaluation (in-domain & OOD)
- **Design tradeoffs**: Model Size (3B) vs. Performance: Smaller size aids deployment but may limit capacity; paper shows it outperforms larger guards, but generalization to other model families is untested. Annotation Cost vs. Nuance: Three-tier labels and expert review improve granularity but increase cost; the paper uses human-AI collaboration to balance this. Training Complexity: Three stages increase engineering overhead; ablation (Table II) suggests all stages contribute, but marginal gains must be weighed against added complexity.
- **Failure signatures**: Over-conservatism: Flagging benign content as harmful due to overfitting to annotation biases. Nuance blindness: Failing to distinguish "potentially harmful" (0.5) from clearly harmful (1) cases, especially in OOD scenarios. Jailbreak vulnerability: Performance drop on MMJ-Eval would indicate insufficient robustness to adversarial visual prompts.
- **First 3 experiments**: 1) Baseline validation: Evaluate the pretrained GuardTrace-VL model on GuardTrace-Test subsets (S-Eval-VL, HADES-Eval, MM-Eval, MMJ-Eval) to confirm reported F1 scores and compare against baselines (e.g., LLaMA-4-Guard-12B, GuardReasoner-VL-7B). 2) Training stage ablation: Retrain the model with only SFT, only SFT+DPO, and the full pipeline to reproduce the ablation results in Table II and understand each stage's contribution. 3) OOD generalization test: Assess the model on a new, held-out multimodal QTA dataset (e.g., constructed from a different safety taxonomy or using different jailbreak methods) to evaluate generalization beyond the paper's test sets.

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The necessity of full multimodal QTA analysis for safety detection remains empirically under-validated; while GuardTrace-VL shows improved performance, direct comparison to text-only reasoning trace analysis is limited in the paper.
- The long-term generalization of the GuardTrace dataset is uncertain; while initial results on in-domain and OOD tests are promising, broader validation across diverse safety taxonomies and real-world deployment scenarios is needed.
- The three-stage training pipeline shows marginal gains over simpler approaches (SFT+DPO), but the added complexity and computational cost may not justify the performance improvement in all deployment contexts.

## Confidence
- **High:** The core mechanism of monitoring QTA traces for safety detection and the progressive training pipeline (SFT → DPO → Oracle-Guided DPO) are well-supported by empirical results and ablation studies.
- **Medium:** The effectiveness of the GuardTrace dataset construction pipeline (multimodal expansion, diverse QTA generation, human-AI collaborative annotation) is supported by initial results but requires further validation on diverse, real-world data.
- **Low:** The claim that joint vision-language reasoning is strictly necessary for effective safety detection is not fully validated; alternative approaches (e.g., separate vision and text analysis) may achieve similar results.

## Next Checks
1. **Alternative Training Schemes:** Compare the three-stage pipeline against simpler alternatives (e.g., SFT+DPO, or single-stage DPO) on the same GuardTrace-Test datasets to quantify the marginal benefit of the full pipeline.
2. **Real-World Deployment Test:** Deploy GuardTrace-VL in a controlled real-world setting (e.g., content moderation for a multimodal platform) and evaluate its performance on actual user queries, not just benchmark datasets.
3. **Generalization to New Safety Domains:** Construct a new multimodal QTA dataset from a different safety domain (e.g., cybersecurity, misinformation) and evaluate GuardTrace-VL's ability to generalize its safety detection capabilities beyond the original benchmarks.