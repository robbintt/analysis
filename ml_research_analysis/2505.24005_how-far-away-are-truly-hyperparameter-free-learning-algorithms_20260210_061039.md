---
ver: rpa2
title: How far away are truly hyperparameter-free learning algorithms?
arxiv_id: '2505.24005'
source_url: https://arxiv.org/abs/2505.24005
tags:
- training
- methods
- learning
- algorithms
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Learning-rate-free methods from recent literature fail to match
  strong baseline optimizers when using default hyperparameter settings across diverse
  deep learning workloads. A systematic search for evidence-based default settings
  for regularization and training horizons improves their performance, but they still
  slightly lag behind similarly calibrated NadamW and AdamW methods.
---

# How far away are truly hyperparameter-free learning algorithms?

## Quick Facts
- arXiv ID: 2505.24005
- Source URL: https://arxiv.org/abs/2505.24005
- Reference count: 40
- Key outcome: Learning-rate-free methods with calibrated defaults achieve competitive results on 4-5 out of 8 workloads, but still slightly lag behind calibrated AdamW/NadamW methods.

## Executive Summary
Learning-rate-free methods from recent literature fail to match strong baseline optimizers when using default hyperparameter settings across diverse deep learning workloads. A systematic search for evidence-based default settings for regularization and training horizons improves their performance, but they still slightly lag behind similarly calibrated NadamW and AdamW methods. The best learning-rate-free methods achieve competitive results on 4-5 out of 8 workloads, showing promise but not yet surpassing standard optimizers in overall performance. Removing the base learning rate alone does not simplify hyperparameter tuning as much as hoped, and removing learning rate schedules may be equally important for future progress.

## Method Summary
The authors evaluated learning-rate-free optimizers (Prodigy, Mechanic, DoG, MoMo, COCOB) against AdamW/NadamW baselines across 8 diverse workloads in the AlgoPerf benchmark. They conducted two phases: a "naive" phase using literature defaults (no weight decay, no specific schedule tuning) and a calibration phase using quasi-random search over non-learning-rate hyperparameters including weight decay, momentum parameters, warmup fractions, label smoothing, and dropout. The search aimed to find configurations that work across multiple workloads simultaneously. Performance was measured using time-to-target metrics and the AlgoPerf benchmark score (area under performance profile curves).

## Key Results
- Default hyperparameter settings from papers performed poorly, with most learning-rate-free methods failing to hit targets on any workload
- After calibrating regularization parameters and training horizons, learning-rate-free methods achieved competitive performance, training 4-5 workloads successfully at multiple training horizons
- The best learning-rate-free methods (Prodigy and Mechanic) hit 4-5 targets compared to 5-6 for calibrated AdamW/NadamW baselines
- Learning-rate-free methods require careful calibration of non-learning-rate hyperparameters to be effective

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Learning-rate-free methods estimate the initial distance to a minimizer to automatically scale the learning rate.
- **Mechanism**: These algorithms use gradient statistics during training to estimate the distance from initialization to a minimizer, removing the need for manual base learning rate specification.
- **Core assumption**: The convex, Lipschitz-continuous theoretical framework provides useful guidance even for non-convex neural network loss landscapes.
- **Evidence anchors**: [abstract]: "learning-rate-free methods" evaluated; [Section 2.2]: optimal constant learning rate formula ηk = D/(G√K); [corpus]: learning rate control paradigms survey.
- **Break condition**: When initialization is far from any reasonable solution or when gradient statistics don't correlate with true distance due to highly non-convex landscapes.

### Mechanism 2
- **Claim**: Cross-workload calibration of non-learning-rate hyperparameters is essential; paper-supplied defaults generalize poorly.
- **Mechanism**: The authors performed quasi-random search over regularization parameters, momentum parameters, and training horizons across 8 diverse workloads simultaneously, selecting top configurations using AlgoPerf benchmark scores.
- **Core assumption**: A single hyperparameter configuration can achieve acceptable performance across diverse workloads without workload-specific tuning.
- **Evidence anchors**: [abstract]: "default hyperparameter settings from papers performed poorly, but after calibrating... learning-rate-free methods achieved competitive performance"; [Section 5.2]: "AlgoPerf-calibrated versions of Mechanic and Prodigy now can train 4–5 workloads successfully."
- **Break condition**: When workloads have fundamentally different optimal hyperparameter regimes.

### Mechanism 3
- **Claim**: "Learning-rate-free" does not mean schedule-free; training horizon and relative schedules remain critical hyperparameters.
- **Mechanism**: Even methods that remove the base learning rate still employ relative learning rate schedules (warmup phase + decay phase) that require horizon specification.
- **Core assumption**: Users can specify or accept a training horizon, which effectively encodes when training should end.
- **Evidence anchors**: [abstract]: "require careful calibration of non-learning-rate hyperparameters"; [Section 5.2]: "All the best performing methods... still needed some kind of relative learning rate schedule."
- **Break condition**: When optimal training duration varies unpredictably.

## Foundational Learning

- **Concept**: Adam optimizer and adaptive gradient methods
  - **Why needed here**: Baselines (AdamW/NadamW) and several learning-rate-free methods are Adam-derived; understanding preconditioning via second-moment estimates is essential.
  - **Quick check question**: Can you explain why Adam uses exponential moving averages of squared gradients?

- **Concept**: Learning rate schedules (warmup, cosine decay)
  - **Why needed here**: Even "learning-rate-free" methods use relative schedules; the warmup fraction and decay endpoint remain tunable.
  - **Quick check question**: What happens if you apply cosine decay with a horizon that's too short?

- **Concept**: Performance profiles and benchmark scoring
  - **Why needed here**: AlgoPerf uses performance profiles to aggregate results across heterogeneous workloads; interpreting these plots is necessary for comparing methods.
  - **Quick check question**: On a performance profile plot, what does it mean when one algorithm's curve is strictly above another's?

## Architecture Onboarding

- **Component map**: Learning-rate-free optimizer core -> Base optimizer wrapper (Adam) -> Regularization stack -> Relative schedule module -> Training horizon specification -> Initial distance estimate
- **Critical path**: 
  1. Select learning-rate-free method (Prodigy or Mechanic most promising)
  2. Configure regularization: weight decay, dropout, label smoothing (use calibrated defaults)
  3. Set training horizon as fraction of workload max steps (33–66% range)
  4. Apply warmup fraction (0.02–0.10 range)
  5. Run training; monitor for target metric achievement
- **Design tradeoffs**: 
  - Shorter horizon → faster target achievement if successful, but may fail to reach targets
  - Stronger regularization → better generalization on some workloads, worse on others
  - Paper defaults → easy adoption, poor cross-workload performance
  - Calibrated defaults → better generalization, requires benchmark access
- **Failure signatures**:
  - Early training instability: Likely warmup too short or initial distance estimate too large
  - Slow convergence without reaching target: Horizon may be too short; try 66% instead of 33%
  - Good training loss, poor validation: Regularization mismatch; tune weight decay
  - Complete divergence: Check safeguard_warmup setting for Prodigy; verify gradient norms
- **First 3 experiments**:
  1. Reproduce the naive vs. calibrated gap: Run Prodigy with paper defaults vs. AlgoPerf-calibrated settings on a single workload; observe metric difference.
  2. Cross-workload generalization test: Take the top calibrated configuration for Prodigy; apply it unchanged to all 8 workloads; record how many hit targets.
  3. Horizon ablation: For Mechanic with calibrated settings, compare success rates at 33%, 50%, and 66% horizons; note the speed vs. success tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can learning-rate-free methods effectively facilitate hyperparameter transfer across different model scales?
- Basis in paper: The authors state in Section 6 that "calibrated hyperparameters as we've defined them do not exist without techniques like dimensionally-aware parameterizations or scaling heuristics," noting this as a limitation of their fixed-scale workload study.
- Why unresolved: The AlgoPerf benchmark used consists of fixed-size workloads, so the study could not assess how these algorithms behave under scaling laws used in large language model training.
- What evidence would resolve it: Experiments evaluating if a single hyperparameter configuration, tuned on small models, retains performance when applied to significantly larger architectures within the same domain.

### Open Question 2
- Question: Does removing the learning rate schedule (using methods like Schedule-Free Adam) provide greater robustness than merely removing the base learning rate?
- Basis in paper: The authors conclude in Section 6 that "successfully removing the learning rate schedule may be just as important as removing the need for a base learning rate," noting that Schedule-Free Adam outperformed their best calibrated methods by hitting 7/8 targets.
- Why unresolved: The authors focused on "stepsize tuners" that still require a schedule; they could not fully integrate Schedule-Free Adam into their calibration framework due to implementation constraints.
- What evidence would resolve it: A controlled comparison running Schedule-Free variants of Prodigy or Mechanic against their scheduled counterparts using the "AlgoPerf-calibrated" search methodology.

### Open Question 3
- Question: Why do literature-supplied defaults for learning-rate-free methods fail to generalize compared to evidence-based defaults?
- Basis in paper: The paper notes in Section 4.1 that "We found no single, simple explanation for the poor performance of these naive learning-rate-free methods" and that defaults "performed poorly on the benchmark."
- Why unresolved: The authors focused on empirically scoring performance and finding better defaults, explicitly stating that "Providing insights into the training dynamics... is beyond the scope of this work."
- What evidence would resolve it: An ablation study analyzing the sensitivity of initial distance estimates and momentum parameters across the diverse AlgoPerf workloads to explain the variance in default performance.

## Limitations

- Evaluation relies on calibrated defaults that may not generalize to workloads outside the AlgoPerf benchmark
- Performance gains from calibration depend on access to TPUv2/v3 hardware, making exact replication difficult on other platforms
- Some algorithm-specific implementation details (like Prodigy's safeguard_warmup) are mentioned but not fully specified, potentially affecting reproducibility

## Confidence

- **High confidence**: The fundamental finding that learning-rate-free methods require careful calibration of non-LR hyperparameters is well-supported by systematic experimentation across 8 diverse workloads
- **Medium confidence**: Claims about relative schedule importance are supported but could benefit from more direct ablation studies
- **Medium confidence**: Cross-workload generalization of calibrated settings is demonstrated but limited to 8 specific workloads in a controlled benchmark

## Next Checks

1. **Implementation verification**: Reproduce the naive vs. calibrated performance gap on at least two workloads using publicly available code or a faithful reimplementation of Prodigy and Mechanic

2. **Schedule sensitivity analysis**: Systematically test whether removing relative learning rate schedules (using Schedule-Free Adam or similar) provides larger performance gains than removing base learning rates alone

3. **Generalization test**: Apply the top calibrated configuration to at least one workload not included in the original AlgoPerf benchmark to assess true "hyperparameter-free" capability