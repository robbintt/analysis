---
ver: rpa2
title: 'Beyond Training-time Poisoning: Component-level and Post-training Backdoors
  in Deep Reinforcement Learning'
arxiv_id: '2507.04883'
source_url: https://arxiv.org/abs/2507.04883
tags:
- backdoor
- attacks
- trigger
- episodic
- infrectrorl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrojanentRL and InfrectroRL, two novel backdoor
  attacks for Deep Reinforcement Learning (DRL) that operate under significantly reduced
  adversarial privileges compared to existing approaches. TrojanentRL embeds backdoors
  in the DRL rollout buffer component, making them persistent even through full model
  retraining, while InfrectroRL injects backdoors into pretrained models without requiring
  training data access.
---

# Beyond Training-time Poisoning: Component-level and Post-training Backdoors in Deep Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2507.04883
- **Source URL**: https://arxiv.org/abs/2507.04883
- **Reference count**: 40
- **Primary result**: Novel backdoor attacks TrojanentRL and InfrectroRL achieve >70% AER and >85% ASR against DRL models with reduced adversarial privileges

## Executive Summary
This paper introduces two novel backdoor attacks for Deep Reinforcement Learning that operate under significantly reduced adversarial privileges compared to existing approaches. TrojanentRL embeds backdoors in the DRL rollout buffer component, making them persistent even through full model retraining, while InfrectroRL injects backdoors into pretrained models without requiring training data access. Across six Atari environments, both attacks achieve attack effectiveness rates exceeding 70% and attack success rates above 85%, rivaling state-of-the-art training-time attacks. The findings reveal critical vulnerabilities across the DRL supply chain and challenge the prevailing focus on training-time attacks, highlighting the urgent need for robust defenses against component-level and post-training backdoors.

## Method Summary
The paper proposes two distinct backdoor attack methodologies for DRL. TrojanentRL exploits component-level flaws by compromising the rollout buffer in the training pipeline. It replaces the benign buffer with a malicious version containing a trigger detector (white pixels in observation corner) that perturbs rewards to steer policy gradients toward adversary-specified actions. The backdoor persists through full model retraining since the buffer operates throughout training. InfrectroRL is a post-training attack that injects backdoors into pretrained models without training data access. It designates specific neurons as "backdoor switches," sets weights to non-trigger features to zero, optimizes trigger patterns to maximize switch neuron activation, and propagates the amplified signal through subsequent layers to suppress non-target actions.

## Key Results
- TrojanentRL achieves AER of 78.13% and ASR of 86.00% on Pong environment
- InfrectroRL achieves AER of 78.66% and ASR of 94.66% on Pong environment
- InfrectroRL empirically evades two leading DRL backdoor defenses (BIRD, SHINE)

## Why This Works (Mechanism)

### Mechanism 1: Component-level backdoor via rollout buffer corruption
Compromising the rollout buffer creates a persistent backdoor that survives full model retraining and architecture changes. A malicious rollout buffer contains a trigger detector that perturbs rewards to steer policy gradients toward adversary-specified actions. Since the buffer operates throughout training, every weight update inherits the backdoor regardless of architecture changes. This works because practitioners treat components like rollout buffers as black-box utilities and rarely modify them due to performance sensitivity. Break condition: Users audit/replace the rollout buffer with verified versions, or supply chain integrity verification is enforced.

### Mechanism 2: Post-training backdoor via direct weight perturbation
Post-training backdoors can be implanted via direct weight perturbation without any access to training data, achieving high attack success rates with theoretical evasiveness guarantees. The attack designates specific neurons as "backdoor switches" starting from layer 1, sets weights to non-trigger features to zero, optimizes trigger pattern to maximize switch neuron activation, and propagates the amplified signal through subsequent layers to suppress non-target actions. This works because adversaries can intercept pretrained models before deployment, and small weight perturbations on a sparse "backdoor path" don't trigger detection. Break condition: Weight-level anomaly detection, model provenance tracking with cryptographic verification, or neuron activation analysis at runtime.

### Mechanism 3: Evasion of existing DRL defenses
Existing DRL backdoor defenses focused on input-space trigger detection fail against weight-level post-training attacks. BIRD and SHINE analyze observation patterns to identify triggers, but InfrectroRL modifies weights directly without leaving traces in the observation space. The backdoor pathway activates only under specific trigger patterns absent from defense analysis. This works because current defenses assume training-time poisoning where triggers manifest in observation distributions; they don't inspect weight perturbations or neuron activation pathways. Break condition: Defenses incorporate weight-distribution analysis, neuron activation profiling, or runtime behavioral anomaly detection.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDP) and policy gradients
  - **Why needed here**: The paper formalizes DRL as MDP (S, A, P, R, γ) and attacks manipulate policy optimization through reward perturbations (TrojanentRL) or direct policy network modification (InfrectroRL).
  - **Quick check question**: Can you explain how modifying rewards during training affects policy gradients?

- **Concept**: Neural network backdoor fundamentals (trigger patterns, masks, target actions)
  - **Why needed here**: Both attacks use trigger formulation: s̃ = (1-m)∘s + m∘Δ where m is binary mask and Δ is trigger pattern. Understanding this is essential for grasping attack mechanics.
  - **Quick check question**: How does a binary mask m control which input features contain the trigger?

- **Concept**: Supply chain security and provenance
  - **Why needed here**: The threat model spans sourcing (HuggingFace, TorchHub), component integration, packaging, and deployment. Attacks exploit trust in third-party components.
  - **Quick check question**: What supply chain stages does the DRL pipeline comprise, and which are most vulnerable?

## Architecture Onboarding

- **Component map**:
  Source (HF/TorchHub) → Components (RL libraries, wrappers) → Entity (train.py) → Build (training loop) → Package (.pth artifacts) → Deployment

- **Critical path**:
  1. Rollout buffer receives observations → trigger detector checks for pattern
  2. If triggered → reward manipulation → biased gradients → backdoored policy
  3. For InfrectroRL: layer-1 switch neuron → amplified pathway → output layer suppresses non-target actions

- **Design tradeoffs**:
  - Stealth vs. effectiveness: Higher γ (amplification factor) increases ASR but may reduce episodic return, potentially triggering detection
  - Trigger size vs. detectability: Larger triggers easier to activate but more visible; paper tests 1-12 pixel triggers
  - Component vs. weight attack: TrojanentRL requires earlier supply chain access but survives retraining; InfrectroRL requires only model access but is一次性

- **Failure signatures**:
  - Unusual episodic return drops under specific observation patterns
  - Consistent action selection when trigger present (ASR approaching 100%)
  - Weight anomalies concentrated on sparse neuron pathway (InfrectroRL)
  - Rollout buffer code differs from reference implementation (TrojanentRL)

- **First 3 experiments**:
  1. Replicate InfrectroRL on Pong with single-layer analysis: measure ASR/CDA while varying trigger size (1, 3, 6, 12 pixels) to validate ablation findings
  2. Test TrojanentRL persistence: train model with compromised buffer, then retrain from scratch with clean buffer—verify backdoor is eliminated only when component is replaced
  3. Defense baseline: Apply BIRD/SHINE defenses to InfrectroRL-backdoored models across Pong, Breakout, Space Invaders; compare episodic returns against TrojDRL to confirm evasion claims

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can neuron activation analysis effectively detect post-training backdoors like InfrectroRL where observation-based methods fail?
- **Basis in paper**: The authors contend that "observation-based detection methods are inherently constrained" and explicitly "advocate neuron activation analysis... as a more promising direction."
- **Why unresolved**: Current state-of-the-art defenses (BIRD, SHINE) failed to detect InfrectroRL. While the authors propose the direction, they do not implement or validate a neuron-based defense in this work.
- **What evidence would resolve it**: Empirical validation showing that neuron activation patterns differ significantly between clean and InfrectroRL-compromised models during benign operation.

### Open Question 2
- **Question**: Can the theoretical guarantees for InfrectroRL's evasiveness (Theorem 2) be extended to Convolutional Neural Networks (CNNs)?
- **Basis in paper**: The authors note that extending the proof's constraints (specifically the Lipschitz continuity bounds) to architectures involving convolutions is "not particularly straightforward, and topic for future research."
- **Why unresolved**: The current theoretical upper-bound on the performance difference (detectability) is derived specifically for 1-hidden layer MLPs.
- **What evidence would resolve it**: A derivation of the upper-bound |J(π_θ) - J(π_θp)| that accounts for the parameter sharing and spatial structure of convolutional layers.

### Open Question 3
- **Question**: Do these component-level and post-training attacks remain effective against off-policy algorithms like DQN or SAC?
- **Basis in paper**: The experimental evaluation is restricted to the PPO algorithm (an on-policy method) across Atari environments.
- **Why unresolved**: The mechanisms for TrojanentRL (rollout buffer manipulation) and InfrectroRL (policy network perturbation) were tested on PPO. It is unclear if these methods translate to off-policy agents that utilize replay buffers or value-based networks.
- **What evidence would resolve it**: Empirical results demonstrating Attack Success Rates (ASR) for TrojanentRL and InfrectroRL when applied to DQN or Soft Actor-Critic agents.

## Limitations
- Evaluation scope restricted to six Atari games using PPO algorithms, limiting generalizability to other DRL domains
- No evidence addressing transferability of backdoors between different DRL algorithms or architectures
- Component-level attack assumes complete trust in individual pipeline components, which may be unrealistic in production environments

## Confidence
- **High**: Attack effectiveness (AER/ASR) results are well-supported by ablation studies and multiple environments
- **Medium**: Theoretical evasiveness claims for InfrectroRL, as limited defense evaluation is presented
- **Medium**: Supply chain vulnerability claims, as real-world component verification practices aren't fully characterized

## Next Checks
1. Test backdoor persistence across DRL algorithm changes (PPO → SAC/DDPG) to verify TrojanentRL's retraining claims
2. Evaluate InfrectroRL against additional DRL defenses (e.g., weight anomaly detection, activation clustering) to strengthen evasiveness claims
3. Assess transferability by implanting TrojanentRL backdoors in one environment and testing on different Atari games with varying architectures