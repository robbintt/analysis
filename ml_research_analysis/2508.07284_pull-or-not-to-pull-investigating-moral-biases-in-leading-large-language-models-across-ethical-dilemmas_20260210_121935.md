---
ver: rpa2
title: '"Pull or Not to Pull?'''': Investigating Moral Biases in Leading Large Language
  Models Across Ethical Dilemmas'
arxiv_id: '2508.07284'
source_url: https://arxiv.org/abs/2508.07284
tags:
- moral
- ethical
- level
- alignment
- trolley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates 14 leading large language models (LLMs) across
  27 trolley problem scenarios framed by ten moral philosophies. Using a factorial
  prompting protocol, we elicited 3,780 binary decisions and natural language justifications,
  measuring decision assertiveness, explanation consistency, alignment with human
  consensus, and bias sensitivity.
---

# "Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas

## Quick Facts
- arXiv ID: 2508.07284
- Source URL: https://arxiv.org/abs/2508.07284
- Reference count: 12
- Primary result: Evaluating 14 LLMs across 27 trolley problems reveals that ethical framing prompts can function as both behavioral modifiers and diagnostic tools, with "sweet zones" emerging under altruistic, fairness, and virtue ethics frames.

## Executive Summary
This study systematically evaluates how 14 leading large language models respond to 27 trolley problem scenarios when prompted with ten different moral philosophies. Using a factorial prompting protocol, researchers elicited 3,780 binary decisions and natural language justifications, measuring decision assertiveness, explanation consistency, alignment with human consensus, and bias sensitivity. The findings reveal that reasoning-enhanced models show greater decisiveness and structured justifications but don't always align better with human preferences. Notably, "sweet zones" emerge under altruistic, fairness, and virtue ethics framings where models achieve high intervention rates, low explanation conflict, and minimal divergence from human judgments, while models diverge under kinship, legality, or self-interest frames.

## Method Summary
The study employs a factorial prompting protocol where 14 leading LLMs (including reasoning-enabled and general-purpose variants) respond to 27 trolley dilemmas framed by ten distinct ethical philosophies. Each model receives two-stage prompts: a system prompt containing the ethical frame and a user prompt with the dilemma. The protocol generates 3,780 responses across binary decisions and natural language justifications. Researchers measure intervention rates, explanation-answer consistency, public alignment via KL divergence from human votes, and contextual bias sensitivity through matched scenario pairs. The analysis identifies "sweet zones" where models balance decisiveness, coherence, and human alignment.

## Key Results
- Reasoning-enhanced models demonstrate greater decisiveness and structured justifications but don't inherently guarantee better alignment with human moral preferences
- "Sweet zones" emerge under altruistic, fairness, and virtue ethics frames where models achieve high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments
- Models diverge under kinship, legality, or self-interest frames, producing ethically controversial outcomes such as accepting bribery under familial frames

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ethical framing prompts can function as behavioral modifiers and diagnostic tools to reveal latent moral alignment in LLMs.
- Mechanism: The paper uses a factorial prompting protocol, embedding each trolley dilemma in ten distinct ethical frames. By comparing a model's decisions and justifications across these frames against a default baseline, researchers can isolate the influence of specific ethical priors on the model's reasoning pathway.
- Core assumption: The model's internal representation of moral concepts is sufficiently coherent that it can be systematically steered and analyzed through structured textual prompts.
- Evidence anchors:
  - [abstract] "We advocate for moral reasoning to become a primary axis in LLM alignment, calling for standardized benchmarks that evaluate not just what LLMs decide, but how and why."
  - [Section 1] "These patterns suggest that moral prompting is not only a behavioral modifier but also a diagnostic tool for uncovering latent alignment philosophies across providers."
  - [corpus] Weak evidence from corpus. Related work discusses evaluation frameworks but does not directly validate this specific factorial prompting mechanism.
- Break condition: This mechanism would fail if LLM moral decisions were entirely incoherent or random.

### Mechanism 2
- Claim: "Sweet zones" in ethical prompting exist where models achieve a balance of high intervention rates, low explanation conflict, and closer alignment with human moral consensus.
- Mechanism: Certain moral philosophies when used as prompts (Altruism, Fairness & Equality, Virtue Ethics) appear to resonate better with the base training of leading LLMs, guiding the model to more decisive action while producing justifications that are logically consistent with the chosen action and also align with aggregated human judgments.
- Core assumption: Aggregated human judgments on trolley problems represent a meaningful benchmark for "moral correctness" or "alignment" for AI systems.
- Evidence anchors:
  - [abstract] "Notably, 'sweet zones' emerge in altruistic, fairness, and virtue ethics framings, where models achieve a balance of high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments."
  - [Section 4.3] "Fairness & Equality achieves 67% Yes, 6% conflict, and KL = 0.68; Altruism yields 76% Yes, 6% conflict, and KL = 0.72; Virtue Ethics produces 80% Yes, 5% conflict, and KL = 0.73."
  - [corpus] Weak direct evidence. No corpus paper confirms these specific "sweet zones," but related work aligns with the idea of measuring misalignment with human reasoning.
- Break condition: This finding would not generalize if human moral consensus varies significantly across cultures or contexts not represented in the dataset.

### Mechanism 3
- Claim: Reasoning-enhanced models show greater decisiveness and more structured justifications but do not inherently guarantee better alignment with human moral preferences.
- Mechanism: Models designed with explicit reasoning capabilities are prompted to "think" through a problem step-by-step, amplifying their internal confidence and leading to more verbose, structured textual explanations for their choices. However, the underlying moral heuristics being reinforced can be flawed or overly simplistic.
- Core assumption: The observed behavior in reasoning models is due to the "reasoning" capability itself and not other fine-tuning or reinforcement learning differences.
- Evidence anchors:
  - [abstract] "Reasoning-enhanced models demonstrate greater decisiveness and structured justifications, yet do not always align better with human consensus."
  - [Section 4.1] "Utilitarianism yields the highest intervention rate (82%) and lowest conflict rate (5%), but incurs the largest KL divergence (0.82) due to off-target justifications such as bribery acceptance."
  - [corpus] Weak evidence. Related work agrees that LLM reasoning can be shallow, but does not provide the direct comparative evidence between reasoning/non-reasoning variants.
- Break condition: The mechanism's link between reasoning enhancement and moral divergence is a correlation observed in this study.

## Foundational Learning

- Concept: Trolley Problems as Moral Dilemma Probes.
  - Why needed here: This is the core experimental paradigm used to evaluate the LLMs. Understanding the canonical "pull vs. not pull" structure and its variations is essential to interpret the 27 scenarios and the resulting binary decisions.
  - Quick check question: What is the fundamental ethical tension in the classic trolley problem?

- Concept: Ethical Frames / Moral Philosophies.
  - Why needed here: The paper's key intervention is prompting models with different ethical philosophies. You must grasp the basic tenets of these frames to understand how they are expected to steer model behavior and create divergent outcomes.
  - Quick check question: How would a utilitarian framing likely influence a model's decision in a "sacrifice one to save five" scenario compared to a deontological one?

- Concept: Alignment vs. Coherence.
  - Why needed here: The paper evaluates models on two distinct dimensions: internal coherence (consistency between decision and justification) and external alignment (match with human consensus). A model can be coherent but misaligned.
  - Quick check question: Can a model provide a logically consistent justification for a decision that is morally controversial? Give an example based on the paper.

## Architecture Onboarding

- Component map: Ethical Frame + Dilemma -> LLM -> Binary Decision & Justification -> Metric Calculation -> Analysis across frames/models
- Critical path: The critical path for results is: `Ethical Frame + Dilemma -> LLM -> Binary Decision & Justification -> Metric Calculation -> Analysis across frames/models`. The "sweet zones" are identified by finding frames that optimize for high intervention, low conflict, and low KL divergence simultaneously.
- Design tradeoffs: The study prioritizes depth of analysis (3780 prompts, justifications) over linguistic breadth (English-only, avoiding translation artifacts). It also trades off real-world nuance for the controlled environment of trolley problems.
- Failure signatures:
  - Explanation-Action Conflict: Model argues "No" but outputs "Yes."
  - Bias Drift: Significant flip in decisions across matched scenarios.
  - Over-commitment: High intervention rate under a specific frame that leads to accepting ethically controversial outcomes.
- First 3 experiments:
  1. **Re-run the "Sweet Zone" Verification**: Prompt your target model with the three identified sweet-zone frames on a small, diverse set of trolley dilemmas. Calculate intervention rate and manually check explanation coherence.
  2. **Stress-Test for Bias**: Using the paper's paired scenarios, test your model under "Fairness" and "Familial Loyalty" frames. Measure the difference in the "Yes" rate to quantify kinship bias.
  3. **Reasoning Variant Comparison**: If your team has access to both a reasoning and a non-reasoning version of a model, run the same set of dilemmas under a "Default" frame. Compare the decisiveness and justification length.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the identified "sweet zones" (Altruism, Fairness, Virtue Ethics) remain robust when models are evaluated on emotionally charged or culturally situated dilemmas, rather than the primarily "absurd" or classic trolley scenarios used in this study?
- Basis in paper: [explicit] The authors explicitly state in Section 5 (Discussion) under Future Work that future benchmarking should "Include emotionally charged and culturally situated dilemmas."
- Why unresolved: The current study relies on the "Absurd Trolley Problems" dataset, which may lack the emotional gravity or cultural nuance of real-world ethical conflicts.
- What evidence would resolve it: A replication of the factorial prompting protocol using a dataset of culturally diverse, non-absurd moral dilemmas to see if the "sweet zone" cluster shifts or disperses.

### Open Question 2
- Question: To what extent does Chain-of-Thought (reasoning) capability causally reinforce specific moral heuristics (like utilitarianism) that lead to divergence from human consensus?
- Basis in paper: [inferred] The paper notes that reasoning-enhanced models show greater decisiveness but often amplify moral divergence from human norms. The mechanism for why reasoning reinforces these specific divergent outcomes is not fully explained.
- Why unresolved: While the paper establishes a correlation between reasoning prompts and "off-target" outcomes, it does not determine if the reasoning process itself is locking onto simplistic utilitarian logic present in training data.
- What evidence would resolve it: Mechanistic interpretability analysis to trace how specific ethical tokens in the prompt influence the final decision path in reasoning models versus non-reasoning models.

### Open Question 3
- Question: Are the high "spill-over" risks observed in Familial and Legal frames (e.g., kinship bias, bribery acceptance) mitigable through structured external ethical scaffolding or filters?
- Basis in paper: [explicit] The Discussion section explicitly calls for exploring "structured ethical scaffolding for output regulation" as a key direction for future work to address the risks of controversial outcomes.
- Why unresolved: The study successfully identifies where models fail, but it does not test interventions to reduce these risks.
- What evidence would resolve it: An experimental condition adding a secondary "safety monitor" or rule-based filter to the output pipeline to measure if it successfully reduces the "spill-over index" for high-risk frames without degrading the model's ability to engage with the dilemma.

## Limitations
- The study relies on the Absurd Trolley Problems dataset, which may not fully represent real-world moral dilemmas or cross-cultural ethical intuitions
- The English-only scope and synthetic nature of the scenarios limit generalizability
- The study assumes aggregated human judgments from the dataset represent a meaningful moral benchmark, which may not hold across different cultural contexts

## Confidence
- High confidence: The identification of "sweet zones" under specific ethical frames (Altruism, Fairness, Virtue Ethics) where models show balanced performance across metrics
- Medium confidence: The correlation between reasoning enhancement and increased decisiveness coupled with potential misalignment with human preferences
- Medium confidence: The demonstration that ethical framing functions as both a behavioral modifier and diagnostic tool, though the underlying mechanisms warrant deeper exploration

## Next Checks
1. **Cross-Cultural Validation**: Replicate the study using trolley dilemmas validated across multiple cultural contexts to test whether identified "sweet zones" persist when human consensus varies significantly by region or demographic.

2. **Mechanistic Probe**: Conduct ablation studies where models are prompted with partial ethical frames (e.g., only utilitarian outcome calculations without duty-based constraints) to isolate which components of each philosophy drive behavioral changes.

3. **Real-World Transfer**: Test whether models exhibiting optimal performance in trolley scenarios under "sweet zone" framings maintain this balance when confronted with contextually-rich, real-world moral dilemmas involving competing stakeholders and incomplete information.