---
ver: rpa2
title: 'CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning'
arxiv_id: '2509.22647'
source_url: https://arxiv.org/abs/2509.22647
tags:
- image
- caption
- caprl
- arxiv
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CapRL, a novel framework that applies Reinforcement
  Learning with Verifiable Rewards (RLVR) to the open-ended image captioning task.
  Traditional captioning models rely on supervised fine-tuning, which limits diversity
  and creativity.
---

# CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning

## Quick Facts
- arXiv ID: 2509.22647
- Source URL: https://arxiv.org/abs/2509.22647
- Reference count: 40
- Primary result: CapRL significantly improves caption quality via Reinforcement Learning with Verifiable Rewards, achieving performance comparable to a 72B model and outperforming the baseline by an average margin of 8.4%.

## Executive Summary
CapRL introduces a novel Reinforcement Learning with Verifiable Rewards (RLVR) framework for open-ended image captioning. Traditional supervised fine-tuning limits caption diversity and creativity. CapRL overcomes this by redefining caption quality through utility: a good caption enables a vision-free LLM to accurately answer questions about the image. It employs a decoupled two-stage pipeline where an LVLM generates a caption, and a separate LLM answers multiple-choice questions based solely on that caption. The accuracy of the LLM's answers serves as an objective reward for RLVR training.

Extensive experiments show that CapRL significantly improves caption quality, achieving performance comparable to a 72B model and outperforming the baseline by an average margin of 8.4%. Pretraining on the CapRL-5M dataset further enhances LVLM performance across 12 benchmarks, demonstrating strong scalability and effectiveness.

## Method Summary
CapRL redefines caption quality through utility: a good caption enables a vision-free LLM to accurately answer questions about the image. The framework employs a decoupled two-stage pipeline where an LVLM generates a caption, and a separate LLM answers multiple-choice questions based solely on that caption. The accuracy of the LLM's answers serves as an objective reward for RLVR training. The training uses GRPO with a KL penalty, and the policy model (Qwen2.5-VL-3B) is initialized from a pretrained checkpoint. The method generates 5 MCQs per image using a large vision-LLM, filters them to ensure image context dependency, and uses the accuracy of a vision-free LLM (Qwen2.5-3B-Instruct) as the reward signal for policy updates.

## Key Results
- CapRL significantly improves caption quality, achieving performance comparable to a 72B model.
- Outperforms the baseline by an average margin of 8.4% across 12 benchmarks.
- Pretraining on CapRL-5M dataset further enhances LVLM performance across benchmarks.
- Demonstrates strong scalability and effectiveness through extensive experiments.

## Why This Works (Mechanism)
CapRL works by redefining caption quality through utility: a good caption enables a vision-free LLM to accurately answer questions about the image. This creates an objective, verifiable reward signal for RLVR training. The decoupled two-stage pipeline separates caption generation from question-answering, allowing for specialized optimization of each component. By using MCQs and filtering for image-context dependency, the reward signal remains robust and meaningful. The KL penalty prevents reward hacking by encouraging captions to stay close to the reference model while improving utility.

## Foundational Learning
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Using an objective, measurable reward signal derived from a downstream task (VQA accuracy) to train an LVLM. Needed because traditional captioning metrics are subjective and hard to optimize directly.
- **Decoupled Two-Stage Pipeline**: Separating caption generation (LVLM) from question-answering (vision-free LLM) to enable specialized optimization and more stable training. Needed to create a clean reward signal without vision interference.
- **Multiple-Choice Question Filtering**: Generating MCQs and filtering to ensure questions require image context (MVf(q,I)=a AND MVf(q)â‰ a). Needed to prevent reward hacking through option guessing or commonsense reasoning.
- **GRPO with KL Penalty**: Using Group Relative Policy Optimization with a KL penalty to the reference model. Needed to balance reward maximization with caption quality preservation.
- **CapRL-5M Dataset Curation**: Constructing a large-scale dataset via sampling from existing datasets and web images, with QA pairs generated by a strong vision-LLM and filtered by a smaller one. Needed to provide sufficient training data for RLVR.

## Architecture Onboarding
- **Component Map**: Web/Image Sources -> CapRL-5M Dataset -> LVLM (Qwen2.5-VL-3B) -> Caption Generation -> Vision-Free LLM (Qwen2.5-3B-Instruct) -> MCQ Answering -> Reward Signal -> GRPO Training
- **Critical Path**: Image -> Caption Generation -> MCQ Answering -> Reward Calculation -> Policy Update
- **Design Tradeoffs**: Decoupling LVLM and LLM enables specialized optimization but may introduce misalignment; using MCQs provides verifiable rewards but may miss nuanced caption quality aspects.
- **Failure Signatures**: Reward hacking (verbose captions that exploit option biases), QA leakage (questions answerable without image context), misalignment between caption generation and question-answering.
- **3 First Experiments**: 1) Implement and validate MVf filtering on a small QA set. 2) Test CapRL reward computation with N=4 sampled questions. 3) Run GRPO with minimal hyperparameters on a subset to verify reward signal stability.

## Open Questions the Paper Calls Out
- Can CapRL be adapted for dense video captioning with temporal dependencies?
- Does the reward LLM's capacity limit the complexity of resulting captions?
- Does optimizing for VQA accuracy neglect subjective or aesthetic caption dimensions?

## Limitations
- Incomplete hyperparameter specification for GRPO training (learning rate, batch size, group size, KL penalty weight, training steps).
- Underspecified QA generation and filtering pipeline details (exact prompts, sampling strategies, filtering thresholds).
- Potential misalignment between LVLM and LLM in the decoupled pipeline.
- Focus on Qwen-family models limits cross-architecture generalization claims.

## Confidence
- **High confidence**: Core RLVR methodology and decoupled two-stage pipeline are clearly described and reproducible.
- **Medium confidence**: Reward design is sound but depends on unstated implementation details of the MVf filtering step.
- **Low confidence**: Claims about dataset curation and exact QA filtering thresholds are difficult to reproduce without additional details.

## Next Checks
1. Implement and validate MVf filtering: Test the QA filtering pipeline with Qwen2.5-VL-3B on a small set to confirm that filtered questions cannot be answered without image context.
2. Hyperparameter sensitivity analysis: Run GRPO with varying learning rates, KL penalties, and group sizes on a subset to determine stability of the reward signal and final performance.
3. Cross-model reward consistency: Evaluate whether the CapRL reward generalizes across different vision-free LLMs (e.g., GPT-4V, Claude) to ensure the utility metric is not model-specific.