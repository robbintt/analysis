---
ver: rpa2
title: 'Few-Shot Optimization for Sensor Data Using Large Language Models: A Case
  Study on Fatigue Detection'
arxiv_id: '2505.18754'
source_url: https://arxiv.org/abs/2505.18754
tags:
- hed-lm
- data
- fatigue
- distance
- user-id
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HED-LM, a hybrid few-shot optimization framework
  that improves example selection for sensor-based classification tasks. It combines
  Euclidean distance filtering with LLM-based contextual relevance scoring to address
  the limitations of purely numeric or random selection methods.
---

# Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection

## Quick Facts
- **arXiv ID**: 2505.18754
- **Source URL**: https://arxiv.org/abs/2505.18754
- **Reference count**: 40
- **Primary result**: HED-LM achieves 69.13% macro F1-score on fatigue detection, outperforming random (59.30%) and distance-only (67.61%) selection by 16.6% and 2.3% respectively.

## Executive Summary
This paper introduces HED-LM, a hybrid few-shot optimization framework that improves example selection for sensor-based classification tasks. It combines Euclidean distance filtering with LLM-based contextual relevance scoring to address the limitations of purely numeric or random selection methods. Evaluated on fatigue detection using accelerometer data, HED-LM achieves a mean macro F1-score of 69.13%, outperforming random selection (59.30%) and distance-only filtering (67.61%) by 16.6% and 2.3% respectively. The results demonstrate that integrating numerical similarity with contextual reasoning significantly enhances few-shot prompting robustness, offering a practical solution for real-world sensor-based learning tasks.

## Method Summary
The HED-LM framework preprocesses raw accelerometer signals through a 4th-order Butterworth filter (30 Hz), min-max normalization, and segmentation into three windows. It extracts 30 statistical features per sample (mean, std, RMS, skewness, kurtosis, etc.) to create a compact feature vector. The method filters top-K candidates via Euclidean distance to the target, then re-ranks them using an LLM (GPT-4o-mini) with domain knowledge to score relevance [0-1]. Finally, it constructs a 2-shot prompt using the top-ranked examples for classification.

## Key Results
- HED-LM achieves 69.13% mean macro F1-score on fatigue detection task
- Outperforms random selection (59.30%) by 16.6% and distance-only filtering (67.61%) by 2.3%
- Domain knowledge injection significantly improves LLM scoring accuracy
- 2-shot prompting often more accurate than full-shot approaches

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Filtering Resolves Efficiency-Semantic Trade-off
The system first applies Euclidean distance to retrieve a broad candidate pool efficiently, then uses an LLM to re-rank based on "label synergy" checking if candidates logically align with target patterns. This resolves the trade-off between computational efficiency and semantic ambiguity in sensor data retrieval.

### Mechanism 2: Statistical Feature Extraction as Compression Layer
Raw 180-sample accelerometer signals are transformed into 30-feature vectors (mean, std, RMS, etc.) via windowing. This reduces token count for the context window and converts abstract waveforms into "tokens" that pre-trained language models can interpret and compare via natural language prompts.

### Mechanism 3: Domain Knowledge Constrains LLM Reasoning
The prompt explicitly includes "Domain Knowledge" (e.g., "RMS > 0.5 indicates fatigue"). This guides the LLM to score relevance based on these expert rules rather than generic text logic. Without this, the LLM tends to assign "middle" relevance scores (0.4-0.6), rendering the re-ranking ineffective.

## Foundational Learning

- **Concept: In-Context Learning (Few-Shot)** - Why needed: The architecture relies on the model's ability to learn a task from examples provided in the prompt without weight updates. Quick check: How does the model's performance change if the examples in the prompt are randomly selected vs. retrieved via HED-LM?

- **Concept: Euclidean Distance & The Curse of Dimensionality** - Why needed: This is the first-stage filter. You need to know why distance alone fails in high-dimensional, overlapping signal data (it treats all dimensions equally, missing semantic context). Quick check: Why does the paper use Euclidean distance only as a filter rather than the final decision mechanism?

- **Concept: Feature Engineering (Time/Frequency Domain)** - Why needed: The model does not see raw waves; it sees "Mean," "RMS," "Kurtosis." Understanding what these represent physically is required to debug the LLM's reasoning. Quick check: Why is the signal split into three distinct windows (segments) before feature extraction?

## Architecture Onboarding

- **Component map**: Preprocessor -> Feature Extractor -> Retriever -> Scorer (LLM) -> Re-Ranker -> Prompt Constructor
- **Critical path**: The Scorer is the bottleneck. The system relies on the LLM API to act as a semantic judge. Latency is dominated by the number of candidates sent to the Scorer (K_distance) and the inference call for the final prediction.
- **Design tradeoffs**: 2-Shot vs. Full-Shot: 2-shot is faster and often more accurate than full-shot, likely because excessive context introduces noise. Param A (5/3) vs. Param B (10/5): Param A is faster (fewer LLM calls); Param B offers higher coverage.
- **Failure signatures**: "Gray Zone" Ambiguity: High inter-subject variability (e.g., User-ID 10) where numeric patterns contradict labels. The system degrades when domain knowledge rules do not generalize to specific users. Metric Mismatch: Euclidean distance retrieves numerically similar subjects that have different labels.
- **First 3 experiments**: 1) Baseline Validation: Run Random vs. Distance vs. HED-LM on a single user to verify the 16.6% relative improvement claim. 2) Ablation on Knowledge: Disable the "Domain Knowledge" block in the prompt and measure the drop in Macro F1-score. 3) Parameter Sweep: Vary K_distance (5 vs. 10) while keeping Top-K fixed to measure sensitivity of retrieval layer vs. API cost.

## Open Questions the Paper Calls Out

1. To what extent does replacing Euclidean distance with Mahalanobis or cosine similarity in the HED-LM filtering stage improve performance on high-dimensional sensor data? The authors explicitly state they plan a systematic ablation study of these metrics in future work.

2. Does implementing adaptive change-point detection (CPD) for signal segmentation yield higher classification accuracy than the currently used fixed-window strategy? The paper notes that physiological transitions seldom align with rigid boundaries and proposes a future "two-stage design" using "lightweight CPD."

3. Is the HED-LM framework robust across diverse sensor modalities (e.g., ECG, EMG) and domains beyond physical fatigue detection? While the framework is described as generalizable, the authors concede that future work will involve applying HED-LM across diverse domains and signal modalities.

## Limitations

- The system's reliance on domain knowledge thresholds presents critical fragility, with performance degrading significantly when rules don't generalize to individual users
- The feature extraction pipeline may discard temporal dependencies critical for fatigue detection, as statistical moments alone cannot capture phase shifts or waveform morphology changes
- The 2-shot prompting strategy, while computationally efficient, lacks validation against larger prompt sizes to confirm its optimality across different dataset sizes or task complexities

## Confidence

- **High Confidence**: The quantitative improvement claims (69.13% vs. 59.30% vs. 67.61%) are supported by experimental results and the ablation study
- **Medium Confidence**: The claim that HED-LM "addresses the trade-off between computational efficiency and semantic ambiguity" is mechanistically sound but lacks formal efficiency analysis
- **Low Confidence**: The assertion that the LLM can reliably infer semantic alignment from statistical features "better than pure distance metrics" is not independently validated

## Next Checks

1. **Cross-User Generalization Test**: Apply the exact domain knowledge rules generated for one user to classify samples from a different user (leave-one-user-out validation). Measure the drop in Macro F1-score to quantify the brittleness of the fixed knowledge thresholds.

2. **Temporal Dependency Analysis**: Compare HED-LM's performance on the current feature set against a version that includes simple temporal features (e.g., mean of the first derivative, autocorrelation at lag 1). This tests whether the statistical compression is discarding predictive temporal information.

3. **Latency and Cost Profiling**: Measure the total inference time and API cost (number of tokens processed) for HED-LM with K_distance=5 vs. K_distance=10 across all subjects. This validates the claimed computational efficiency trade-off and informs practical deployment constraints.