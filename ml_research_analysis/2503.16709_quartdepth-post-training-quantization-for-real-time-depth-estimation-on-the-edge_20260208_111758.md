---
ver: rpa2
title: 'QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the
  Edge'
arxiv_id: '2503.16709'
source_url: https://arxiv.org/abs/2503.16709
tags:
- quantization
- depth
- pages
- w4a4
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QuartDepth addresses the challenge of deploying accurate monocular
  depth estimation (MDE) models on resource-limited edge devices, particularly ASICs.
  The core method involves post-training quantization (PTQ) of both weights and activations
  to 4-bit precision, using LogNP activation polishing to handle outlier distributions,
  weight reconstruction to minimize quantization errors, and a flexible programmable
  hardware accelerator with kernel fusion and customized instruction support.
---

# QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge

## Quick Facts
- **arXiv ID:** 2503.16709
- **Source URL:** https://arxiv.org/abs/2503.16709
- **Reference count:** 40
- **One-line primary result:** QuartDepth achieves competitive accuracy (0.071 AbsRel, 0.970 δ1) while enabling up to 26 FPS and 5.5× faster, more energy-efficient inference on ASICs via 4-bit post-training quantization for monocular depth estimation.

## Executive Summary
QuartDepth is a post-training quantization (PTQ) framework designed to compress monocular depth estimation (MDE) models for real-time inference on edge ASICs. The method combines LogNP activation polishing to handle outlier distributions, weight compensation, and weight reconstruction via AdaRound to maintain accuracy under aggressive 4-bit weight/activation quantization. Experiments demonstrate that QuartDepth preserves high depth estimation accuracy (AbsRel ~0.07, δ1 ~0.97) while significantly improving latency and power efficiency, making it practical for edge deployment.

## Method Summary
QuartDepth applies a four-stage PTQ pipeline: (1) LogNP activation polishing to suppress outlier activations per channel using a percentile-based alpha, (2) uniform per-channel asymmetric activation quantization, (3) weight compensation to reduce quantization error using a pseudo-inverse solution, and (4) AdaRound-based weight reconstruction with KFAC-approximated Hessian for finer accuracy preservation. The method uses only 32 calibration samples and targets 4-bit weights and activations (W4A4) for depth decoder layers, achieving real-time performance on ASICs through kernel fusion and programmable instruction support.

## Key Results
- Achieves 0.071 AbsRel and 0.970 δ1 on NYUv2 validation with Metric3D ViT-Large (W4A8).
- Reaches up to 26 FPS at 256×256 resolution with W4A4 configuration.
- Provides 5.5× faster inference and 5.5× power efficiency versus float32 models on ASICs.

## Why This Works (Mechanism)
QuartDepth tackles the challenge of extreme outlier distributions in MDE decoder activations by combining LogNP polishing to clip high-magnitude values, compensating for quantization-induced weight perturbations, and reconstructing weights to minimize reconstruction error. This three-pronged approach stabilizes quantization under aggressive bitwidth reduction without requiring full retraining, enabling efficient edge deployment.

## Foundational Learning
- **LogNP Activation Polishing**: Nonlinear transformation to suppress outliers before quantization. Why needed: Depth decoder activations have heavy-tailed distributions that standard symmetric quantization handles poorly. Quick check: Verify activation histograms before/after polishing show reduced tail mass.
- **Weight Compensation**: Pseudo-inverse-based adjustment to counteract quantization-induced weight perturbations. Why needed: Standard PTQ ignores how weight quantization errors interact with activation quantization; compensation closes this gap. Quick check: Compare weight error before/after compensation using mean squared error.
- **AdaRound with KFAC Approximation**: Learned rounding of quantized weights guided by Hessian curvature. Why needed: Naive rounding leads to large reconstruction errors at low bitwidth; AdaRound optimizes rounding decisions. Quick check: Confirm weight error reduction after AdaRound versus naive rounding.
- **Per-Channel Asymmetric Quantization**: Different scaling per channel for both weights and activations. Why needed: Channel-wise variation in activation/weight distributions requires adaptive scaling for minimal error. Quick check: Ensure min/max are computed per-channel before quantization.
- **Programmable Hardware Accelerator with Kernel Fusion**: Custom ASIC design supporting fused operations and flexible instruction sets. Why needed: Low-bitwidth inference demands specialized memory access and compute patterns. Quick check: Verify kernel fusion reduces memory traffic versus naive operator execution.
- **Calibration Set Sampling**: Small random subset (32 images) used for quantization parameter calibration. Why needed: Reduces overhead while capturing activation distribution statistics. Quick check: Test calibration robustness by varying sample count and selection strategy.

## Architecture Onboarding
- **Component Map**: Input Image → Backbone (ViT) → Encoder Features → Decoder Blocks (with LogNP, Q, Compensation, Reconstruction) → Output Depth Map
- **Critical Path**: Input → Backbone → Decoder → Output. Decoder is the quantization bottleneck due to outlier activations.
- **Design Tradeoffs**: Aggressive 4-bit quantization for speed/energy efficiency versus accuracy loss; small calibration set for efficiency versus potential OOD robustness gaps; fixed LogNP percentile versus adaptive smoothing.
- **Failure Signatures**: Accuracy collapse (AbsRel >0.4) indicates missing compensation or LogNP polishing; outlier-induced spikes indicate per-channel LogNP misapplication; metric mismatch indicates asymmetric quantization or percentile ε deviation.
- **First Experiments**:
  1. Test W4A4 accuracy with only LogNP polishing (no compensation/reconstruction) to isolate polishing effect.
  2. Vary LogNP percentile ε ∈ {90, 95, 99} to assess sensitivity and optimal choice.
  3. Apply compensation and reconstruction sequentially to measure incremental accuracy gains.

## Open Questions the Paper Calls Out
- **Generalization to other architectures**: Can LogNP polishing generalize to vision transformers for segmentation, detection, or LLMs that exhibit similar activation outlier phenomena?
- **Calibration robustness**: Is the fixed calibration set size of 32 samples sufficient to maintain robustness against extreme out-of-distribution scenarios?
- **Fixed percentile robustness**: Is the static percentile (ε=95) for LogNP polishing robust across varying dynamic ranges, such as extreme low-light or high-contrast scenes?

## Limitations
- Requires detailed decoder layer specification for LogNP application, which is not fully provided.
- Unclear dampening method for rank-deficient matrices in weight compensation may affect reproducibility.
- Hardware performance claims (latency, power) are dependent on ASIC implementation and require physical validation.

## Confidence
- **Accuracy claims**: High (well-defined calibration, public metrics, reproducible pipeline)
- **Hardware performance claims**: Medium (FPGA/ASIC dependent, needs physical validation)
- **Ablation methodology**: High (clear isolation of LogNP, compensation, reconstruction contributions)

## Next Checks
1. Confirm exact decoder layers receiving LogNP polishing by testing all decoder blocks with and without polishing on NYUv2 W4A4.
2. Verify rank-deficient matrix handling by comparing results with and without dampening when x̂x̂ᵀ is ill-conditioned.
3. Test calibration sample diversity by comparing random vs. stratified sampling on NYUv2 W4A8 accuracy.