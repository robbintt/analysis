---
ver: rpa2
title: Evaluation of Hate Speech Detection Using Large Language Models and Geographical
  Contextualization
arxiv_id: '2502.19612'
source_url: https://arxiv.org/abs/2502.19612
tags:
- hate
- speech
- comment
- llms
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates the effectiveness of large language models
  (LLMs) in detecting hate speech across multilingual and geographically diverse contexts.
  A dataset of 1,000 comments from five regions was used, translated into English
  for analysis.
---

# Evaluation of Hate Speech Detection Using Large Language Models and Geographical Contextualization

## Quick Facts
- arXiv ID: 2502.19612
- Source URL: https://arxiv.org/abs/2502.19612
- Reference count: 24
- Large language models tested for multilingual hate speech detection with geographical contextualization, showing model-specific trade-offs between accuracy, contextual understanding, and adversarial robustness.

## Executive Summary
This study evaluates three large language models—Llama2 (13b), Codellama (7b), and DeepSeekCoder (6.7b)—for hate speech detection across five geographical regions using multilingual datasets. The models were tested for binary classification accuracy, geographical sensitivity, and robustness against adversarial manipulation. Codellama achieved the highest recall (70.6%) and F1-score (52.18%) for hate speech detection, while DeepSeekCoder demonstrated the best geographical identification (63 out of 265 locations). However, Llama2 showed significant vulnerability to adversarial samples, misclassifying 62.5% of manipulated texts. The findings highlight critical trade-offs between model performance, contextual understanding, and robustness, emphasizing the need for contextually aware systems in hate speech detection.

## Method Summary
The study collected 1,000 comments from five regions (Arab, Bangladesh, India, China, Russia/Ukraine) and translated them to English using Google Translate API. Three LLMs were evaluated using zero-shot prompting with structured prompts containing 3 in-context examples and output tags ([ANSWER][/ANSWER], [LOCATION][/LOCATION]). The models performed binary hate speech classification, geographical origin detection, and robustness testing against adversarial samples generated by GPT-4. No fine-tuning was performed; the approach relied entirely on prompt engineering and pre-trained model capabilities.

## Key Results
- Codellama achieved the highest recall (70.6%) and F1-score (52.18%) for hate speech detection
- DeepSeekCoder correctly identified 63 out of 265 geographical locations, outperforming other models
- Llama2 showed significant vulnerability to adversarial samples, misclassifying 62.5% of manipulated texts

## Why This Works (Mechanism)

### Mechanism 1: Structured Prompt Engineering with In-Context Examples
Providing structured prompts with labeled examples improves LLM hate speech classification by establishing decision boundaries and output formatting. The paper uses a task description followed by 3 in-context examples with explicit tag structures to constrain model outputs and provide pattern-matching anchors for the classification task. Pre-trained LLMs can generalize from few-shot examples to novel hate speech patterns without fine-tuning.

### Mechanism 2: Sentiment-Preserving Translation Pipeline
Translating multilingual content to English before classification enables resource-constrained LLMs to process diverse linguistic inputs, though at the cost of potential sentiment distortion. The pipeline uses Google Translate API followed by LLM-based translation with explicit sentiment-preservation instructions, allowing models trained primarily on English data to classify multilingual hate speech.

### Mechanism 3: Adversarial Sample Generation for Robustness Testing
Paraphrasing and noise injection via GPT-4 exposes LLM vulnerabilities by creating semantically equivalent but lexically modified inputs that flip model predictions. GPT-4 generates adversarial variants designed to "flip the labels for the LLM predictions," testing whether models rely on superficial lexical patterns versus semantic understanding.

## Foundational Learning

- **Precision-Recall Trade-off in Imbalanced Classification**
  - Why needed here: Codellama's 70.6% recall but only 35.3% accuracy suggests model bias toward the hate speech class—understanding this trade-off is essential for interpreting metrics correctly.
  - Quick check question: If a model predicts "hate speech" for every input, what would its recall be? What would its precision be?

- **Prompt Engineering with Structured Output Tags**
  - Why needed here: The paper uses `[ANSWER][/ANSWER]` and `[LOCATION][/LOCATION]` tags to enforce parseable outputs—this pattern is critical for automating evaluation pipelines.
  - Quick check question: Why might structured tags improve both model performance and downstream processing compared to free-form responses?

- **Adversarial Robustness vs. Standard Accuracy**
  - Why needed here: A model can achieve reasonable accuracy on clean data (DeepSeekCoder: 26.5% accuracy, 53% recall) but vary dramatically in robustness (only 1/25 misclassified on adversarial samples vs. Llama2's 15/24).
  - Quick check question: Why might a model with lower standard accuracy perform better under adversarial conditions?

## Architecture Onboarding

- Component map: Multilingual Input → Translation Layer (Google API + LLM) → English Text → Prompt Constructor (task desc + 3 examples) → LLM Classifier (Codellama/DeepSeek/Llama2) → Parsed Output [ANSWER] + [LOCATION] → Evaluation Metrics. Parallel path: Original samples → GPT-4 Adversarial Generator → Robustness Testing

- Critical path: Translation quality → Prompt construction → Classification accuracy. The paper explicitly states "our work is heavily dependent on the quality of the translation."

- Design tradeoffs:
  - Higher recall (Codellama) vs. better geographic sensitivity (DeepSeekCoder)—no single model optimizes both
  - Larger model (Llama2 13b) vs. adversarial robustness—larger model showed highest vulnerability
  - Google Translate vs. LLM translation—pilot study showed no significant improvement with LLM translation, but sample size was small (200 Bengali comments)

- Failure signatures:
  - High recall + low accuracy = model biased toward positive class (Codellama: 70.6% recall, 35.3% accuracy)
  - High adversarial misclassification rate = reliance on lexical patterns rather than semantic understanding (Llama2: 62.5%)
  - Low geographic accuracy despite high hate speech accuracy = geographic context not encoded in classification decision boundaries

- First 3 experiments:
  1. Baseline validation: Replicate the prompt structure on a held-out subset (100 samples) to verify reported metrics within acceptable variance
  2. Translation quality audit: Manually annotate 50 translated samples for sentiment preservation errors to quantify translation-induced information loss
  3. Adversarial stress test: Extend adversarial evaluation beyond 50 samples to 200+ with multiple paraphrase strategies (synonym replacement, word reordering, slang substitution) to characterize robustness boundaries

## Open Questions the Paper Calls Out

### Open Question 1
Can targeted fine-tuning of LLMs on expanded multilingual datasets resolve the observed trade-off between classification recall and geographic sensitivity? The authors state they "would like to work on targeted fine-tuned LLMs" and must "curate more labeled data" because the current 10k samples are insufficient.

### Open Question 2
Does native multilingual processing significantly outperform the translation-based pipeline used in this study? The authors note their work is "heavily dependent on the quality of the translation" and list it as a threat to validity; they also express interest in seeing "how the new LLMs will perform on our multilingual dataset."

### Open Question 3
What specific architectural or training characteristics cause the disparate adversarial robustness observed between Llama2 and DeepSeekCoder? The results showed Llama2 is highly susceptible (62.5% misclassification) while DeepSeekCoder remained robust (4% misclassification), but the paper did not isolate the cause of this divergence.

## Limitations
- Small sample size for adversarial testing (only 50 samples) limits generalizability of robustness findings
- Reliance on Google Translate for multilingual preprocessing introduces potential sentiment and hate speech preservation errors
- In-context examples used for prompt engineering are not specified, making exact replication difficult

## Confidence
- **High Confidence**: Relative performance ranking between models (Codellama > DeepSeekCoder > Llama2) for standard classification tasks
- **Medium Confidence**: Geographic detection accuracy claims, as evaluation methodology is less detailed than for hate speech classification
- **Low Confidence**: Claims about model robustness to real-world adversarial attacks, as study uses only GPT-4-generated paraphrases

## Next Checks
1. **Translation Quality Audit**: Manually evaluate 100 translated samples for sentiment and hate speech preservation accuracy, comparing Google Translate outputs against human translations
2. **Adversarial Attack Diversity Test**: Expand adversarial evaluation beyond 50 samples to include 5 different attack strategies across 200+ samples to test robustness boundaries
3. **Cross-Lingual Zero-Shot Validation**: Test the same prompt engineering approach directly on untranslated content for one language (e.g., Arabic) to determine whether translation is necessary