---
ver: rpa2
title: 'ACT as Human: Multimodal Large Language Model Data Annotation with Critical
  Thinking'
arxiv_id: '2511.09833'
source_url: https://arxiv.org/abs/2511.09833
tags:
- data
- annotation
- human
- budget
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes ACT, a data annotation pipeline that leverages
  multimodal large language models (MLLMs) for annotation while using another MLLM
  as a criticizer to identify potentially erroneous labels. Human reviewers focus
  only on the most suspicious cases flagged by the criticizer, significantly improving
  annotation efficiency.
---

# ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking

## Quick Facts
- arXiv ID: 2511.09833
- Source URL: https://arxiv.org/abs/2511.09833
- Reference count: 40
- Primary result: Uses MLLM criticizers to filter samples for human review, achieving up to 90% cost savings while maintaining <2% performance gap vs fully human-annotated data

## Executive Summary
ACT introduces a novel data annotation pipeline that leverages multimodal large language models (MLLMs) for initial annotation while using another MLLM as a criticizer to identify potentially erroneous labels. Human reviewers focus only on the most suspicious cases flagged by the criticizer, significantly improving annotation efficiency. The method is applicable across NLP, computer vision, and multimodal domains without requiring training or model access. Theoretical analysis supports using modified loss functions (e.g., active M-estimation) to ensure models trained on ACT data perform comparably to those trained on fully human-annotated data.

## Method Summary
The ACT pipeline uses two MLLMs: an annotator that generates initial labels for all data, and a criticizer that estimates error probabilities for each label. A sampling rule (thresholding or exponential weighting) selects high-risk instances for human correction, creating a dataset with mostly machine labels and high-quality human corrections where the model was likely wrong. The resulting data is used to train downstream models with a modified ACT loss that corrects for the bias of learning from noisy machine labels. The approach works for classification tasks across multiple modalities and can be implemented with black-box MLLM APIs.

## Key Results
- Achieves up to 90% reduction in human annotation costs while maintaining <2% performance gap on most benchmarks
- Cross-criticism (using second-best model for criticism) consistently outperforms self-criticism
- ACT loss with thresholding sampling rule shows stable performance even at low budgets, while normalization sampling rule causes catastrophic drops at small budgets
- The method is applicable to NLP, computer vision, and multimodal domains without requiring training or model access

## Why This Works (Mechanism)

### Mechanism 1: Critical Filtering via MLLM Criticism
Directing human review budget toward samples flagged as "suspicious" by a second MLLM (the criticizer) significantly increases annotation efficiency compared to random sampling. An MLLM annotator generates initial labels, then a separate MLLM criticizer estimates the probability of error for each label. A sampling rule (e.g., thresholding) selects high-risk instances for human correction, creating a dataset with mostly machine labels and high-quality human corrections where the model was likely wrong.

### Mechanism 2: Variance Reduction via Modified Loss (Active M-Estimation)
Modifying the training loss function using Active M-estimation allows downstream models to learn effectively from the mixed-quality ACT dataset (human-corrected + machine-annotated) with minimal performance gap compared to fully human data. The ACT loss corrects the bias of learning from noisy machine labels by weighting the loss difference between human and machine labels by the inverse of the sampling probability.

### Mechanism 3: Role-Specific Model Selection
Optimizing the pipeline requires selecting the best available MLLM for annotation and a distinct (often second-best) MLLM for criticism, rather than using a single model for both. Empirical results show that using the top-performing annotator with the second-best model as criticizer consistently achieves the best performance.

## Foundational Learning

- **LLM-as-a-Judge / Criticism**: Understanding how to prompt an MLLM to evaluate the probability of error in a given (data, label) pair is critical for the ACT pipeline. Quick check: Can you distinguish between prompting an LLM for "generation" vs. "verification"?
- **Active Learning & Importance Sampling**: The pipeline uses a budget-constrained sampling strategy to select samples for human review. Understanding how sampling probability relates to the loss function correction is critical for the downstream training phase. Quick check: If we sample a data point with probability p=0.1 for human review, how should we weight its loss during training to correct for the bias that it was rarely seen?
- **Noisy Label Learning**: The resulting ACT dataset contains a mix of clean (human) and potentially noisy (machine) labels. Understanding how loss functions handle label noise is necessary to appreciate why the "ACT Loss" is required. Quick check: What happens to standard Cross-Entropy loss when the training data contains systematically incorrect labels?

## Architecture Onboarding

- **Component map**: Unlabeled Dataset -> Annotator (MLLM A) -> Labels -> Criticizer (MLLM B) -> Error Probabilities -> Sampler -> Human Review -> Corrected Labels -> Combiner -> ACT Dataset -> Trainer -> Downstream Model
- **Critical path**: The accuracy of the Criticizer is the bottleneck. If the criticizer fails to flag erroneous labels, the downstream model trains on noise. If it flags too many correct labels, human budget is wasted.
- **Design tradeoffs**: Self-criticism is easier to implement but often less effective than cross-criticism. Normalization sampling works for large budgets but fails at low budgets, while thresholding is robust at low budgets but strictly limits the review set size.
- **Failure signatures**: High False Positives (criticizer flags correct labels), Loss Divergence (unstable training with normalization at small budgets), Stagnant Quality (AQG curve flattens indicating criticizer cannot distinguish hard samples).
- **First 3 experiments**: 1) Run Annotator on held-out set to determine base accuracy and set human budget, 2) Compare Self-Criticism vs Cross-Criticism on subset using ABS score, 3) Train downstream model with ACT Thresholding Loss vs Standard Cross-Entropy to verify <2% gap claim.

## Open Questions the Paper Calls Out

### Open Question 1
Can the ACT pipeline be effectively adapted for open-ended generative tasks (e.g., summarization) where "correctness" is subjective and continuous rather than binary? The current methodology relies on discrete error probabilities and 0-1 loss functions, which are incompatible with the nuanced evaluation required for generative text or complex reasoning.

### Open Question 2
Would incorporating In-Context Learning (ICL) or multi-model aggregation (e.g., majority voting) significantly enhance the robustness of the ACT annotator or criticizer? The paper only evaluates single model pairs and does not test if aggregating predictions reduces the variance or bias of the annotation and criticism process.

### Open Question 3
Will white-box criticism strategies (using logit probabilities) consistently outperform black-box strategies once open-source MLLMs achieve capability parity with proprietary models? Current white-box models are fundamentally weaker than black-box GPT-4o, making it impossible to isolate the benefit of access to internal logits from raw model intelligence.

## Limitations
- The effectiveness of ACT heavily depends on the criticizer's calibration; no empirical validation is provided for how well the criticizer's estimated error probabilities correlate with actual annotation errors
- Theoretical guarantees assume strong convexity and bounded gradients, but these assumptions may not hold for complex downstream models or real-world datasets
- The paper reports <2% performance gap vs. human-only data on standard benchmarks, but these are relatively clean datasets; ACT's robustness to noisy or adversarial data is untested

## Confidence
- **High confidence**: The core mechanism of using MLLM criticism to filter samples for human review is well-supported by empirical results (e.g., ABS scores, AQG curves)
- **Medium confidence**: The theoretical analysis for the ACT loss is sound but relies on idealized assumptions; the <2% performance gap is empirically demonstrated but may not generalize to all domains
- **Low confidence**: The robustness of ACT to mis-calibrated criticizers or adversarial data is not addressed; the role-specific model selection lacks a theoretical explanation for its effectiveness

## Next Checks
1. **Criticizer Calibration Test**: Measure the correlation between the criticizer's estimated error probabilities and actual annotation errors on a held-out set. Compute calibration metrics to assess reliability.
2. **ACT Loss Generalization**: Validate the ACT loss on a noisy or adversarial dataset to test robustness. Compare against standard Cross-Entropy and other noisy-label methods.
3. **Role Selection Theory**: Conduct ablation studies to test if the second-best model as criticizer consistently outperforms self-criticism across diverse MLLM pairs and datasets. Explore alternative selection criteria.