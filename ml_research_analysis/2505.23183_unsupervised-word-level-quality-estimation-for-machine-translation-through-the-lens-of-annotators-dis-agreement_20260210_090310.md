---
ver: rpa2
title: Unsupervised Word-level Quality Estimation for Machine Translation Through
  the Lens of Annotators (Dis)agreement
arxiv_id: '2505.23183'
source_url: https://arxiv.org/abs/2505.23183
tags:
- metrics
- association
- translation
- computational
- xcomet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates unsupervised word-level quality estimation
  (WQE) metrics across 14 metrics, 12 translation directions, and multiple human annotation
  sets. The authors investigate whether signals derived from model internals (like
  predictive uncertainty and attention) can effectively identify translation errors
  without requiring costly human-labeled data or LLM prompting.
---

# Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Disagreement)

## Quick Facts
- arXiv ID: 2505.23183
- Source URL: https://arxiv.org/abs/2505.23183
- Reference count: 40
- Primary result: Unsupervised WQE metrics based on model uncertainty signals (particularly Surprisal MCD VAR) perform competitively with supervised approaches while requiring no labeled data or LLM prompting

## Executive Summary
This study evaluates unsupervised word-level quality estimation (WQE) metrics across 14 metrics, 12 translation directions, and multiple human annotation sets. The authors investigate whether signals derived from model internals (like predictive uncertainty and attention) can effectively identify translation errors without requiring costly human-labeled data or LLM prompting. They find that unsupervised metrics, particularly those based on output distribution uncertainty (e.g., Surprisal MCD VAR), perform competitively with supervised approaches. Importantly, they demonstrate that incorporating multiple human annotation sets improves evaluation robustness and that confidence-weighted XCOMET variants significantly outperform default binary versions.

## Method Summary
The study evaluates 14 unsupervised WQE metrics on three datasets (DivEMT, WMT24, QE4PE) spanning 12 translation directions and three model architectures (mBART-50, NLLB-3.3B, Aya23-35B). Metrics include Surprisal, Entropy, MCD variants (AVG/VAR), LogitLens projections, Attention Entropy, and BLOOD. The authors extract these metrics during force-decoding of annotated MT outputs and align them with character-level error spans. They compare performance against XCOMET-XL/XXL baselines (both binary and confidence-weighted variants) using Average Precision (AP) and optimal F1 (F1*) as primary metrics, while also examining correlations with multi-annotator edit counts.

## Key Results
- Unsupervised metrics based on output distribution uncertainty (Surprisal MCD VAR) achieve competitive performance with supervised approaches (AP=.43-.56 vs .45-.61)
- Confidence-weighted XCOMET variants significantly outperform default binary versions by improving recall while maintaining precision
- Incorporating multiple human annotation sets improves evaluation robustness and reveals that annotator agreement varies substantially across datasets
- The Surprisal MCD VAR metric shows the steepest correlation increase with human edit counts as more annotators are added, suggesting epistemic uncertainty anticipates aleatoric annotator variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Surprisal signals word-level translation errors by capturing model uncertainty at generation time
- Mechanism: For each generated token t*, computes -log p(t*|t_{<i}). Higher surprisal indicates lower probability assignment and potential translation quality issues
- Core assumption: Prediction uncertainty reflects genuine translation difficulty rather than vocabulary sparsity
- Evidence anchors: Consistent with segment-level QE results (Fomicheva et al., 2020b); shows best F1 among unsupervised methods in Table 2

### Mechanism 2
- Claim: Monte Carlo Dropout variance captures epistemic uncertainty that anticipates human annotator disagreement
- Mechanism: Performs 10 forward passes with different dropout masks; variance in surprisal estimates model knowledge gaps that may predict where human annotators disagree
- Core assumption: Epistemic uncertainty correlates with aleatoric uncertainty in translation quality assessment
- Evidence anchors: Surprisal MCD VAR shows steepest correlation increase with more annotators in Figure 3; achieves best F1 among unsupervised methods

### Mechanism 3
- Claim: Confidence-weighted XCOMET outperforms binary versions by preserving calibration flexibility
- Mechanism: Sums error type probabilities (s(t*)=p(MINOR)+p(MAJOR)+p(CRITICAL)) enabling threshold tuning for precision-recall tradeoffs
- Core assumption: Underlying error probabilities are calibrated enough that their sum meaningfully reflects error likelihood
- Evidence anchors: Table 2 shows confidence-weighted variants consistently outperform binary versions; Figure 2 demonstrates improved recall while maintaining precision

## Foundational Learning

- Concept: Aleatoric vs. Epistemic Uncertainty
  - Why needed here: The paper leverages both types—aleatoric from human disagreement, epistemic from MCD variance
  - Quick check question: If dropout variance decreases after fine-tuning on more data, which type of uncertainty is being reduced?

- Concept: Precision-Recall Tradeoff in Error Detection
  - Why needed here: Binary XCOMET has high precision but very low recall, missing critical errors
  - Quick check question: In a post-editing workflow, would you optimize for precision or recall? What if the downstream user is a professional translator vs. casual user?

- Concept: Human Label Variation / Annotator Disagreement
  - Why needed here: Central methodological contribution uses multiple annotation sets to establish robust evaluation
  - Quick check question: If two annotators disagree on whether a token is an error, is this annotation noise or legitimate linguistic variation? How would you distinguish?

## Architecture Onboarding

- Component map: Input MT output + Source text -> Force-decoding through MT model -> Extract token-level metrics (Surprisal, Entropy, MCD, etc.) -> Align with error spans -> Compute AP/F1* metrics

- Critical path: 1) Force-decode MT model on annotated translations (ensures token alignment) 2) Extract metrics per-token (surprisal, entropy, attention weights, etc.) 3) For MCD: run 10 forward passes with dropout enabled, compute mean/variance 4) Align token-level scores with character-level error spans 5) Compute AP and F1* across thresholds; correlate with multi-annotator edit counts

- Design tradeoffs:
  - Unsupervised metrics: Lower ceiling but no labeled data required, computationally tractable (except MCD)
  - Binary XCOMET: High precision, very low recall—may miss critical errors
  - Confidence-weighted XCOMET: Best overall performance but requires calibration set
  - Single vs. multiple annotators: Multiple annotations improve robustness but increase annotation cost

- Failure signatures:
  - Low recall with binary XCOMET: Default behavior; switch to confidence-weighted variant
  - High variance in metric rankings across annotators: Single-annotator evaluation is insufficient
  - MCD metrics unavailable: Dropout disabled at inference (common in production LLMs)
  - Token alignment errors: Force-decoding required; verify tokenizer consistency between MT model and annotations

- First 3 experiments:
  1. Baseline replication: Extract Surprisal and Output Entropy for mBART-50 on DivEMT EN→IT; compute AP/F1*. Compare to Table 2 values (Surprisal: AP=.43, F1*=.53).
  2. MCD variance implementation: Add MCD dropout at inference, run 10 passes, compute variance. Test whether correlation with human edit counts increases with more annotators (replicate Figure 3 pattern).
  3. XCOMET_CONF comparison: Run binary XCOMET-XL and confidence-weighted variant on QE4PE. Plot precision-recall curves; verify that CONF variant maintains precision while improving recall.

## Open Questions the Paper Calls Out

- Can advanced interpretability techniques, such as tuned vocabulary projections or the identification of "confidence neurons," significantly improve unsupervised WQE performance over current uncertainty metrics?
- How does the performance of the high-performing Surprisal MCD VAR metric scale when applied to very large language models like Aya23-35B?
- Does explicitly accounting for error severity (e.g., Minor vs. Major) in human annotations alter the robustness and ranking of WQE metrics compared to binary error labeling?

## Limitations

- All evaluation focuses on European languages and high-resource settings; findings may not generalize to low-resource languages or distant language pairs
- Confidence-weighted XCOMET variants show superior performance, but the calibration procedure for threshold selection is not detailed
- The relationship between XCOMET binary variant underperformance and architecture/training data may be specific rather than universal

## Confidence

- **High confidence**: Core finding that unsupervised metrics based on output distribution uncertainty perform competitively with supervised approaches (AP=.43-.56 vs .45-.61)
- **Medium confidence**: MCD variance captures epistemic uncertainty that anticipates human annotator disagreement (Figure 3 shows steeper correlation increases)
- **Low confidence**: BLOOD metric performance and comparison to other unsupervised methods (minimal analysis provided)

## Next Checks

1. Systematically vary the number of annotators used to establish ground truth (1 vs. 2 vs. 3+) and measure how metric rankings change to validate the claim that accounting for human label variation is crucial for reliable WQE evaluation.

2. Implement and document the exact calibration procedure for confidence-weighted XCOMET thresholds, testing whether performance degrades significantly when using default thresholds versus tuned ones.

3. Evaluate the same unsupervised metrics on a dataset with substantially different characteristics—such as low-resource languages, non-European language pairs, or specialized domains—to test whether competitive performance extends beyond high-resource European language settings.