---
ver: rpa2
title: The Consistency Hypothesis in Uncertainty Quantification for Large Language
  Models
arxiv_id: '2506.21849'
source_url: https://arxiv.org/abs/2506.21849
tags:
- dataset
- generations
- consistency
- similarity
- hypothesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the consistency hypothesis underlying many
  black-box uncertainty quantification (UQ) methods for large language models (LLMs).
  The hypothesis posits that correct LLM generations are more similar to each other
  than incorrect ones.
---

# The Consistency Hypothesis in Uncertainty Quantification for Large Language Models

## Quick Facts
- arXiv ID: 2506.21849
- Source URL: https://arxiv.org/abs/2506.21849
- Reference count: 16
- This paper formalizes the consistency hypothesis underlying many black-box uncertainty quantification (UQ) methods for large language models (LLMs).

## Executive Summary
This paper introduces and validates the "consistency hypothesis" for black-box uncertainty quantification of LLMs, proposing that correct LLM generations are more similar to each other than incorrect ones. The authors formalize three mathematical versions of this hypothesis and develop metrics to evaluate their validity across diverse datasets. Through extensive experiments on 8 benchmark datasets spanning question answering, summarization, and text-to-SQL tasks, they demonstrate that the hypothesis generally holds, particularly the "Sim-Any" version which compares each generation to all others without requiring correctness labels. Based on this empirical validation, the authors propose novel black-box UQ methods using geometric and harmonic mean aggregation of similarities, which outperform baseline approaches in confidence estimation tasks, demonstrating the practical value of exploiting consistency for UQ.

## Method Summary
The paper formalizes three mathematical statements of the consistency hypothesis and proposes metrics to evaluate their validity. The methodology involves sampling multiple generations per instance using hybrid temperature sampling (5 samples at 6 different temperatures), computing pairwise similarities between generations using metrics like Jaccard, Rouge-L, and SBERT cosine similarity, and testing whether correct generations form a tighter similarity cluster than incorrect ones via one-sided t-tests. For UQ, they aggregate pairwise similarities using geometric or harmonic means to produce confidence scores, evaluating performance via AUROC and AUARC metrics against baseline approaches.

## Key Results
- The Sim-Any hypothesis (comparing each generation to all others) is empirically validated across 8 datasets with ρ(ng) > 0.5 in most cases
- Geometric and harmonic mean aggregation methods outperform arithmetic mean and baseline UQ approaches in confidence estimation
- Temperature hybrid sampling (using 6 different temperatures from 0.25-1.5) improves the validity of consistency hypotheses by increasing generation diversity

## Why This Works (Mechanism)

### Mechanism 1: The Sim-Any Hypothesis (Unlabeled Consistency)
- Claim: Correct LLM generations are statistically more similar to the entire set of generations than incorrect generations are to that same set.
- Mechanism: This leverages the convergence of valid answers toward a "truth" cluster, whereas incorrect answers are linguistically and semantically dispersed. By comparing a sample against all others without needing ground truth labels, one can infer correctness through relative density in the semantic embedding space.
- Core assumption: The semantic space of valid answers forms a denser cluster than the space of hallucinations or errors for a given query.
- Evidence anchors: [abstract] "Among the statements, we highlight the `Sim-Any' hypothesis as the most actionable... it can be leveraged by proposing data-free black-box UQ methods." [section 2.2.3] "We refer to this statement as the sim-any hypothesis, as the statement makes a claim about similarity with respect to any other generation... eliminating the need for labels during aggregation."

### Mechanism 2: Conservative Aggregation (Geometric/Harmonic Means)
- Claim: Using geometric or harmonic means to aggregate pairwise similarities yields better confidence estimates than arithmetic means or probability averaging.
- Mechanism: Arithmetic means can be skewed by a few high-similarity outliers. Geometric and harmonic means are more sensitive to low scores; a single low-similarity pair significantly drops the aggregate confidence, providing a more conservative and robust "trust" signal.
- Core assumption: A "trustworthy" generation must maintain a minimum level of similarity across most samples, rather than just a high average similarity.
- Evidence anchors: [abstract] "...propose novel black-box UQ methods using geometric and harmonic mean aggregation of similarities, which outperform baseline approaches." [section 5] "Both [geometric and harmonic] methods yield more conservative confidence estimates, typically lower than those produced by the arithmetic mean."

### Mechanism 3: Hybrid Sampling for Variance Injection
- Claim: Generating samples across multiple temperatures improves the validity of the consistency hypothesis compared to single-temperature sampling.
- Mechanism: Standard sampling at low temperature may produce clones, falsely inflating consistency. High temperature alone may produce gibberish. Hybrid sampling generates a diverse distribution of plausible answers, ensuring that the "cluster" of correct answers is distinct from the noise.
- Core assumption: Increasing temperature variability reveals the true underlying uncertainty landscape without pushing the model entirely off-distribution.
- Evidence anchors: [section 4.5] "Sampling at different temperatures (temperature and hybrid sampling) increases the variability of generations, thereby enhancing the validity of the consistency hypotheses."

## Foundational Learning

- Concept: **Semantic Similarity Metrics (Jaccard/Rouge/BERT)**
  - Why needed here: The entire framework relies on calculating $s(y_j, y_k)$ to determine if answers are "consistent." You must understand the difference between lexical overlap (Jaccard/Rouge) and semantic embedding (SBERT) to choose the right metric for your task.
  - Quick check question: If I have two SQL queries that are syntactically different but functionally identical, which similarity metric would fail to detect consistency?

- Concept: **Statistical Hypothesis Testing (T-Test)**
  - Why needed here: The paper moves beyond intuition to "verification" using t-tests to compare the means of similarity sets ($\mu_C > \mu_I$). Understanding p-values and null hypotheses is required to interpret the "verification" plots.
  - Quick check question: In the context of this paper, does a high p-value (e.g., > 0.05) imply that the consistency hypothesis holds for a specific dataset group?

- Concept: **Black-box vs. White-box UQ**
  - Why needed here: This paper focuses strictly on "Black-box" (API-only) methods. Understanding this constraint explains why they aggregate output similarities rather than analyzing hidden states or logits.
  - Quick check question: Why is the "Sim-Correct" hypothesis considered less actionable than "Sim-Any" for a black-box deployment scenario?

## Architecture Onboarding

- Component map: Generator (LLM API) -> Encoder (text to vectors/n-grams) -> Similarity Matrix (pairwise scores) -> Aggregator (geometric/harmonic mean) -> Confidence Score
- Critical path: The choice of **Aggregation Function**. While the similarity matrix is $O(m^2)$, the failure mode usually resides in averaging out critical low-similarity signals. Switching from Arithmetic to Geometric mean is the single most effective intervention mentioned in Section 5.
- Design tradeoffs:
  - **Sim-Correct vs. Sim-Any**: Sim-Correct provides higher mean differences but requires labeled data. Sim-Any is slightly noisier but fully unsupervised.
  - **Sampling Cost**: Higher $m$ (samples) improves statistical power but linearly increases latency and cost.
- Failure signatures:
  - **False Positive Confidence**: High consistency among incorrect generations (violating the hypothesis). More likely in "fine-tuned" models where errors might be systematic.
  - **Metric Mismatch**: Using Jaccard for code/SQL where semantic equivalence requires execution verification, not string matching.
- First 3 experiments:
  1. **Baseline Verification**: Run Algorithm 1 on a sample of your domain data to confirm $\mu_C > \mu_I$ (Mean Difference > 0) before deploying UQ.
  2. **Aggregation Ablation**: Compare Arithmetic vs. Geometric vs. Harmonic mean aggregation on a held-out validation set using AUROC (Area Under ROC Curve).
  3. **Temperature Sweep**: Plot "Fraction of Verified Groups" vs. Temperature settings to find the optimal temperature range where consistency is statistically significant but diversity is maintained.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What aggregation functions beyond geometric and harmonic means could better leverage the consistency hypothesis for uncertainty quantification?
- Basis in paper: [explicit] "An investigation of other aggregation methods... are potential avenues for future work."
- Why unresolved: Paper only tested arithmetic, geometric, and harmonic means; no systematic exploration of more sophisticated aggregation approaches.
- What evidence would resolve it: Comparative study of alternative aggregation functions (e.g., weighted means, learned aggregations, attention-based methods) across the same benchmark datasets.

### Open Question 2
- Question: Does the consistency hypothesis generalize to tasks with additional complexities beyond QA, summarization, and text-to-SQL?
- Basis in paper: [explicit] "...a broader empirical study with tasks involving additional complexities are potential avenues for future work."
- Why unresolved: Current study limited to 3 task types; unknown whether hypothesis holds for reasoning, dialogue, code generation, or multi-modal tasks.
- What evidence would resolve it: Validation experiments on complex reasoning datasets (e.g., GSM8K), multi-turn dialogue, or long-form generation tasks.

### Open Question 3
- Question: Why does fine-tuning reduce the validity of consistency hypotheses compared to few-shot prompting?
- Basis in paper: [inferred] Paper observes weaker hypothesis validation for fine-tuned models but states "This may occur because incorrect instances may have less variability after fine-tuning" without investigating mechanism.
- Why unresolved: Observational finding without systematic analysis of how fine-tuning affects generation variability distributions.
- What evidence would resolve it: Controlled experiments comparing generation diversity statistics and similarity distributions before/after fine-tuning across multiple model architectures.

## Limitations
- The consistency hypothesis may break down for tasks requiring highly creative or divergent responses where correct answers are inherently diverse
- The methodology relies on pairwise similarity metrics that may not capture functional equivalence (e.g., different SQL queries producing identical results)
- Black-box nature means the approach cannot distinguish between confidence from genuine knowledge versus memorized patterns

## Confidence

**High Confidence**: The empirical validation that the Sim-Any hypothesis holds across multiple datasets (ρ(ng) > 0.5 in most cases). The experimental methodology is rigorous, using multiple similarity metrics and statistical testing across diverse tasks.

**Medium Confidence**: The claim that Sim-Any is the "most actionable" hypothesis for black-box deployment. While practically convenient (no labels required), the paper doesn't extensively explore scenarios where labeled data is available but costly.

**Low Confidence**: The generalizability of these findings to extremely large models (beyond 13B parameters) or to domains with highly specialized vocabulary where semantic similarity metrics may be less reliable.

## Next Checks

1. **Cross-Domain Robustness Test**: Apply the verification framework (Algorithm 1) to a dataset from a domain not represented in the original study (e.g., biomedical question answering or legal document summarization). Measure whether ρ(ng) remains above the significance threshold and whether the geometric/harmonic aggregation still outperforms baselines.

2. **Failure Mode Analysis**: Systematically construct adversarial examples where the consistency hypothesis should fail - for instance, queries with multiple equally valid but semantically distant correct answers. Measure the false positive rate and analyze whether the geometric mean's conservatism mitigates this failure mode compared to arithmetic mean.

3. **Resource Efficiency Benchmark**: Quantify the trade-off between sampling cost (m × temperature settings) and UQ performance. Conduct an ablation study varying m from 3 to 15 samples and measuring AUROC/AUARC improvements while tracking generation latency and cost.