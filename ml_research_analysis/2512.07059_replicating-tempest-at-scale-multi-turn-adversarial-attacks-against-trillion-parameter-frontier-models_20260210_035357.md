---
ver: rpa2
title: 'Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter
  Frontier Models'
arxiv_id: '2512.07059'
source_url: https://arxiv.org/abs/2512.07059
tags:
- safety
- attack
- thinking
- attacks
- multi-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the vulnerability of ten frontier language
  models to adaptive multi-turn adversarial attacks using the TEMPEST framework across
  1,000 harmful behaviors, generating over 97,000 API queries. Results showed six
  models achieved 96% to 100% attack success rate (ASR), while four showed resistance
  with ASR ranging from 42% to 78%.
---

# Replicating TEMPEST at Scale: Multi-Turn Adversarial Attacks Against Trillion-Parameter Frontier Models

## Quick Facts
- **arXiv ID:** 2512.07059
- **Source URL:** https://arxiv.org/abs/2512.07059
- **Reference count:** 40
- **Key outcome:** Six frontier models achieved 96%-100% ASR under TEMPEST multi-turn attacks, while four showed resistance at 42%-78% ASR; thinking mode reduced ASR by 55 percentage points.

## Executive Summary
This study evaluated ten frontier language models against the TEMPEST framework, a tree-based multi-turn adversarial attack system. Using 1,000 harmful behaviors and over 97,000 API queries, the research demonstrated that sophisticated adaptive attacks can jailbreak most current models regardless of scale, while identifying deliberative inference (thinking mode) as a promising defense direction that reduced attack success by 55 percentage points on identical architecture.

## Method Summary
The study employed a tree-based exploration framework using Breadth-First Search with adaptive pruning across six parallel conversation branches per behavior. The attacker model (DeepSeek V3.1, 671B parameters) generated prompts using a "Siege" chain-of-attack format, while target models were evaluated using an independent evaluator (DeepSeek V3.1 primary, Llama-Guard-3 secondary). The system dynamically selected from seven attack strategies based on eleven detected resistance types, terminating early upon successful jailbreak (harm score ≥ 10).

## Key Results
- Six models achieved 96%-100% ASR under TEMPEST attacks
- Four models showed resistance with ASR ranging from 42%-78%
- Extended reasoning mode reduced ASR by 55 percentage points on identical architecture
- Model scale showed no correlation with robustness across 7B-671B parameter models

## Why This Works (Mechanism)

### Mechanism 1: Tree-Based Exploration Finds Paths of Least Resistance
Multi-branch conversation search discovers attack trajectories single-turn prompting cannot find. TEMPEST maintains up to 6 parallel branches per behavior, evaluating multiple attack strategies simultaneously. Low-scoring branches are pruned while promising ones receive additional turns, identifying which of 7 attack strategies each model is most vulnerable to.

### Mechanism 2: Helpfulness-Safety Tension Exploitation via Social Framing
Safety-aligned models remain vulnerable to requests framed as legitimate research, testing, or creative work. RLHF training optimizes for both helpfulness and harmlessness. When requests are framed as academic research or security audits, models weight helpfulness more heavily, allowing harmful content to pass through.

### Mechanism 3: Deliberative Inference Enables Adversarial Detection
Extended reasoning mode improves robustness by allowing models to identify adversarial framings during inference. Thinking mode allocates additional compute to reasoning before response generation, enabling models to recognize manipulation patterns that standard inference processes too quickly to detect.

## Foundational Learning

- **Multi-turn vs. Single-turn Attack Surface**: Why needed - The paper's central finding is that multi-turn attacks achieve 83.9% ASR where single-turn attacks show 10-50% ASR. Quick check - Why does maintaining parallel conversation branches increase attack success compared to optimizing a single prompt?

- **RLHF Alignment Tension (Helpfulness vs. Harmlessness)**: Why needed - TEMPEST exploits the competing objectives problem—models trained to be helpful struggle to refuse requests framed as legitimate work. Quick check - When a model receives a request framed as "academic research," which objective receives higher weight and why?

- **Thinking Mode as Inference-Time Compute Scaling**: Why needed - The 55 percentage point ASR reduction from thinking mode is the paper's most actionable finding, but the mechanism differs from training-time safety. Quick check - Why might extended reasoning help against general multi-turn attacks but not against reasoning-targeted attacks like H-CoT?

## Architecture Onboarding

- **Component map:**
  Attacker Model (DeepSeek V3.1, 671B) -> Generates attack prompts using "Siege" chain-of-attack format
  Target Model (10 evaluated) -> Responses scored for harm
  Evaluator Model (DeepSeek V3.1 primary / Llama-Guard-3 secondary) -> Scores 1-10; feedback loops if score < 10
  Strategy Selector -> Detects 11 resistance types → selects from 7 strategies
  Branch Manager -> Maintains 6 parallel branches, prunes low-scoring paths

- **Critical path:**
  1. Initialize 6 attack branches per behavior
  2. Generate first-turn prompts using random strategy assignment
  3. Evaluate responses; if score ≥ 10, record jailbreak and terminate branch
  4. If score < 10, detect resistance type and adapt strategy for next turn
  5. Prune branches below threshold; expand high-scoring branches
  6. Repeat until max turns (5) or successful jailbreak

- **Design tradeoffs:**
  - Branching factor (6) vs. computational cost: More branches increase ASR but require ~27× more queries for resistant models
  - Attacker model capability (671B) vs. realism: Large attacker represents worst-case adversary; smaller attackers would achieve lower ASR
  - Early termination vs. exploration: Stopping on success saves compute but prevents discovery of alternative attack paths

- **Failure signatures:**
  - High variance in turns-to-jailbreak across models (1.0 to 22.7) indicates inconsistent safety coverage
  - Category-specific patterns suggest uneven alignment
  - Models that generate harmful content with disclaimers are still classified as jailbroken—the safety training encourages signaling awareness rather than refusal

- **First 3 experiments:**
  1. **Reproduce baseline on single model**: Run TEMPEST on Gemma3:12b (expected: ~100% ASR in 1-2 turns) to validate pipeline integration with Ollama.
  2. **Isolate strategy effectiveness**: Disable adaptive selection; run each of the 7 strategies independently to measure per-strategy ASR on your target model.
  3. **Test thinking mode protection**: If you have access to a thinking-enabled model, run paired comparison (standard vs. thinking) on identical behaviors to measure ASR delta.

## Open Questions the Paper Calls Out

- **Generalization of thinking mode benefit:** Does safety benefit generalize to architectures beyond Kimi K2? The study only had access to one model pair for controlled comparison.
- **Emerging defense mechanisms:** Do ProAct and TSSF effectively mitigate tree-based exploration attacks? These were evaluated against linear attacks but not TEMPEST's adaptive multi-branch strategy.
- **Benchmark generalization:** Does vulnerability spectrum generalize across different safety benchmarks? The study relied exclusively on JailbreakBench dataset (100 behaviors).
- **Reasoning-targeted attacks:** Can attack frameworks be adapted to specifically target thinking modes, reversing their safety benefits? Current evidence suggests thinking mode helps against general attacks but may be vulnerable to reasoning-targeted exploits like H-CoT.

## Limitations

- The evaluation relied on commercial API access with unknown safety training specifics, making it difficult to determine whether vulnerabilities stem from fundamental architectural weaknesses or model-specific alignment choices.
- The tree-based search methodology requires substantial computational resources (up to 27× more queries for resistant models), limiting practical deployment feasibility.
- The evaluator model showed systematic conservative bias compared to Llama-Guard-3, with moderate inter-annotator agreement (kappa=0.537), suggesting potential measurement uncertainty.

## Confidence

- **High Confidence:** The core finding that multi-turn attacks achieve significantly higher ASR than single-turn attacks (83.9% vs 10-50%) is well-supported by systematic evaluation across 10 models and 1,000 behaviors.
- **Medium Confidence:** The claim that model scale shows no correlation with robustness requires cautious interpretation, as the evaluation covered only 10 models with limited scale variation.
- **Low Confidence:** The generalizability of TEMPEST's attack strategies to non-commercial or differently-aligned models remains uncertain, particularly regarding whether smaller attacker models would achieve comparable success rates.

## Next Checks

1. **Evaluator Consistency Validation:** Run a stratified sample of 100 behaviors through both DeepSeek V3.1 and Llama-Guard-3 evaluators to quantify measurement variance and assess whether high ASR results are evaluator-dependent or consistent across harm assessment protocols.

2. **Attacker Model Capability Scaling:** Evaluate TEMPEST using progressively smaller attacker models (7B, 34B, 70B) against a fixed target model to determine whether the 671B parameter attacker represents a worst-case scenario or whether similar ASRs are achievable with more accessible computational resources.

3. **Defense Transferability Assessment:** Test whether models exhibiting resistance to TEMPEST (e.g., MiniMax M2, Kimi K2 Thinking) maintain their robustness against alternative multi-turn attack frameworks like H-CoT or Bidirectional Intention Inference to validate whether observed resistance represents general alignment strength versus TEMPEST-specific evasion.