---
ver: rpa2
title: Towards Autonomous Sustainability Assessment via Multimodal AI Agents
arxiv_id: '2507.17012'
source_url: https://arxiv.org/abs/2507.17012
tags:
- data
- https
- carbon
- product
- products
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal AI agent system that automates
  Life Cycle Assessment (LCA) by autonomously retrieving inventory data from public
  sources and estimating environmental impacts. The system uses LLM agents simulating
  LCA experts and stakeholders to iteratively collect data from sources like repair
  communities and government certifications.
---

# Towards Autonomous Sustainability Assessment via Multimodal AI Agents

## Quick Facts
- arXiv ID: 2507.17012
- Source URL: https://arxiv.org/abs/2507.17012
- Reference count: 40
- Key result: AI agents autonomously generate LCIs and estimate carbon footprints within 19% of expert LCAs, running in under one minute

## Executive Summary
This paper introduces a multimodal AI agent system that automates Life Cycle Assessment (LCA) by autonomously retrieving inventory data from public sources and estimating environmental impacts. The system uses LLM agents simulating LCA experts and stakeholders to iteratively collect data from sources like repair communities and government certifications. It achieves carbon footprint estimates within 19% of expert LCAs and generates LCIs in under one minute. The authors also develop a kNN-based direct estimation method with 12.28% MAPE on electronic products and a data-driven emission factor estimation method that improves accuracy by 120.26% compared to expert heuristics.

## Method Summary
The system employs a multi-agent self-play architecture where an LCA Agent critiques completeness and generates queries while a Stakeholders Agent retrieves multimodal data using specialized tools. The workflow extracts component inventories from FCC filings, iFixit teardowns, and manufacturer reports using FFT filters for PCB image selection, YOLO for component detection, and OCR for part numbers. For direct carbon footprint estimation, a weighted Gaussian kNN estimator uses domain-specific features (memory, display size, battery capacity) to identify similar products and compute weighted means. Unknown emission factors are interpolated using physical/chemical property features rather than textual matching.

## Key Results
- Multi-agent system generates LCIs and estimates carbon footprints within 19% of expert LCAs in under one minute
- kNN direct estimation achieves 12.28% MAPE on electronic products with 3 ms runtime on laptop
- Data-driven emission factor estimation improves accuracy by 120.26% compared to expert heuristics

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Self-Play Emulates Expert-Stakeholder Iteration
A two-agent architecture (LCA Agent + Stakeholders Agent) can autonomously generate Life Cycle Inventories by simulating the iterative query-refinement loop between human LCA experts and domain stakeholders. The LCA Agent constructs a Data Abstraction defining valid component classes, critiques LCI completeness, and generates targeted queries. The Stakeholders Agent retrieves multimodal data from public sources (FCC filings, iFixit teardowns, datasheets) using specialized tools. This loop continues until the LCA Agent determines the inventory is sufficiently complete for impact assessment.

### Mechanism 2: Domain-Specific Feature Clustering Bypasses Fine-Grained LCI
Products within the same category (desktops, laptops, displays) can have their carbon footprint estimated directly from high-level features without component-level inventory decomposition. The system encodes domain-specific attributes (technology node, memory capacity, battery capacity, display size) into feature vectors. A kNN weighted Gaussian estimator identifies the 5 nearest neighbors in a reference database of products with known carbon footprints, then computes a weighted mean with uncertainty bounds derived from the distribution variance.

### Mechanism 3: Feature-Based Emission Factor Interpolation Outperforms Textual Matching
Representing unknown materials as weighted sums of similar known materials—using physical/chemical properties as features—produces more accurate emission factor estimates than expert selection of closest textual database match. For unknown emission factors, the system derives class-specific features (melting point, phase at STP, elemental category for materials; power source composition for electricity grids). kNN clustering identifies representative known entries, and a weighted Gaussian estimator interpolates.

## Foundational Learning

- **Concept: Life Cycle Assessment (LCA) System Boundaries**
  - Why needed here: The paper explicitly notes there is no single "ground truth" for LCA—results depend on choices about which life cycle stages (cradle-to-gate vs. cradle-to-grave), component granularity, and supply chain tiers to include. The Data Abstraction component enforces boundary consistency.
  - Quick check question: If two LCAs for the same product report different carbon footprints, what three methodological choices might explain the discrepancy?

- **Concept: k-Nearest Neighbors with Weighted Gaussian Estimation**
  - Why needed here: The direct estimation method uses kNN not just for nearest-neighbor lookup but as the basis for a probabilistic estimate with uncertainty bounds. Understanding how neighbor weighting by "attribute completeness" affects both the mean and standard deviation is essential for interpreting outputs.
  - Quick check question: Given 5 nearest neighbors with carbon footprints [100, 110, 105, 200, 108] kg CO2e and completeness weights [0.9, 0.8, 0.7, 0.3, 0.85], how would the outlier (200) affect the weighted estimate vs. an unweighted mean?

- **Concept: Multimodal Agent Tool Use**
  - Why needed here: The system's performance depends on specialized tools (FFT for image frequency analysis, YOLO for component detection, OCR for part numbers) that the agent must select dynamically based on intermediate reasoning. Unlike end-to-end deep learning, this requires understanding when each tool is appropriate.
  - Quick check question: Why might FFT-based image selection for PCB photos outperform a VLM directly asked to "find the most complete board view"?

## Architecture Onboarding

- **Component map:** Product name/photo → Data Abstraction → Image retrieval (FCC/iFixit) → PCB extraction (FFT + YOLO) → Component inventory with dimensions (pixel-to-mm calibration via reference components) → Emission factor lookup/interpolation → Carbon sum

- **Critical path:** The image-to-dimension step is the highest-uncertainty stage (5.48% MAPE on known PCBs; VLMs fail consistently)

- **Design tradeoffs:**
  - Speed vs. accuracy: Direct kNN estimation runs in 3 ms with ~12% MAPE; full agentic pipeline takes ~1 minute with ~18% MAPE but provides component-level traceability
  - Compute vs. explainability: kNN is inherently interpretable (neighbors can be inspected); deep learning regressors showed comparable accuracy but no dynamic learning capability
  - Proprietary vs. public data: System uses only public sources (zero proprietary data) but may fail on products without teardown disclosures

- **Failure signatures:**
  - 100% image retrieval success on Apple products except newly released models with confidential FCC reports—agent successfully falls back to iFixit but latency increases
  - VLMs consistently underperform on spatial reasoning tasks (dimension estimation); the paper explicitly flags this as a known limitation
  - Cross-company estimation shows systematic shifts reflecting different LCA methodologies—requires distributional calibration

- **First 3 experiments:**
  1. Baseline validation on held-out Apple products: Run the full agentic pipeline on 3 iPhone models not used in development. Compare generated LCI component counts and dimensions against iFixit teardown data. Measure carbon footprint MAPE against Apple environmental reports. Target: <20% MAPE.
  2. kNN feature ablation: Train the weighted Gaussian estimator on Asus laptops with progressively fewer features (remove battery capacity, then display size, then memory). Plot MAPE vs. feature completeness to validate the ~50% availability threshold identified for emission factor estimation.
  3. Emission factor interpolation stress test: Mask 20% of entries in a materials database (plastics subset from Ecoinvent). Compare three approaches: (a) expert textual matching, (b) text-embedding-only kNN, (c) text + physical properties kNN. Measure MAPE and MAE against ground truth. Replicate the paper's 120% improvement claim.

## Open Questions the Paper Calls Out
The authors note that verification will become increasingly important and highlight another opportunity to extend techniques like multi-agent self-play for reviewing these results. The current system focuses on retrieval and estimation but lacks an automated mechanism to audit the validity or methodological consistency of its own generated inventories.

## Limitations
- System fails on products without available teardown imagery or component specifications, particularly newly released electronics with confidential FCC reports
- Cross-company generalization shows systematic shifts (MAPE increases from ~11% to ~16.5% after calibration) due to inconsistent system boundaries and reporting methodologies
- Requires ~50% feature availability for emission factor interpolation; defaults to less accurate textual matching when physical/chemical properties are missing

## Confidence
- **High confidence**: The overall LCA automation concept and the specific mechanisms for image-based component detection (FFT + YOLO) are well-supported by empirical results and external validation
- **Medium confidence**: The kNN direct estimation method's accuracy claims are supported by internal testing on the Asus dataset, but cross-company validation is limited
- **Medium confidence**: The emission factor interpolation improvement is demonstrated internally, but the specific weighted-sum approach lacks external validation
- **Low confidence**: The multi-agent self-play architecture's effectiveness cannot be independently verified due to underspecification of the LLM components and tool integration

## Next Checks
1. **Component detection accuracy validation**: Run the FFT + YOLO pipeline on 10 held-out Apple products with known iFixit teardowns. Measure component count accuracy and dimension estimation MAPE against ground truth. Target: <10% MAPE on dimensions, >90% component recall.

2. **Cross-company kNN generalization test**: Apply the weighted Gaussian estimator trained on Asus products to a held-out set of HP or Dell products. Measure MAPE and compare against within-company performance to quantify methodology sensitivity. Target: MAPE increase < 8% after calibration.

3. **Emission factor interpolation ablation**: Mask 30% of materials in the Ecoinvent database and compare three approaches—(a) expert textual matching, (b) text-embedding-only kNN, (c) text + physical properties kNN—against ground truth. Replicate the 120% improvement claim and identify the feature set threshold where accuracy plateaus.