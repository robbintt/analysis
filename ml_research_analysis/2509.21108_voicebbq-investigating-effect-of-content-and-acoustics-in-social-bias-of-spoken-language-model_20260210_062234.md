---
ver: rpa2
title: 'VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of
  Spoken Language Model'
arxiv_id: '2509.21108'
source_url: https://arxiv.org/abs/2509.21108
tags:
- bias
- gender
- speech
- acoustic
- slms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VoiceBBQ, a spoken extension of the BBQ bias
  benchmark, designed to evaluate social bias in Spoken Language Models (SLMs) from
  both content and acoustic aspects. By synthesizing speech with controlled gender
  and accent variations, the study systematically isolates how these two sources contribute
  to bias.
---

# VoiceBBQ: Investigating Effect of Content and Acoustics in Social Bias of Spoken Language Model

## Quick Facts
- arXiv ID: 2509.21108
- Source URL: https://arxiv.org/abs/2509.21108
- Authors: Junhyuk Choi; Ro-hoon Oh; Jihwan Seol; Bugeun Kim
- Reference count: 22
- Key outcome: Introduces VoiceBBQ benchmark to evaluate social bias in Spoken Language Models from content and acoustic aspects, revealing differential bias inheritance and acoustic sensitivity patterns across architectures

## Executive Summary
This paper introduces VoiceBBQ, a spoken extension of the BBQ bias benchmark, designed to evaluate social bias in Spoken Language Models (SLMs) from both content and acoustic aspects. By synthesizing speech with controlled gender and accent variations, the study systematically isolates how these two sources contribute to bias. Experiments on LLaMA-Omni and Qwen2-Audio reveal that Qwen2-Audio inherits content biases strongly from its backbone LLM and minimizes acoustic bias, while LLaMA-Omni exhibits less content bias inheritance but shows significant bias variation across speaker gender and accent due to its frozen Whisper encoder. The findings demonstrate that SLMs can inherit content-induced bias and be affected by acoustic features, with architectural choices influencing both types of bias differently.

## Method Summary
VoiceBBQ synthesizes 16 speech variants (2 genders × 2 accents × 4 voices) for each of the 58,492 BBQ items, creating 935,872 audio files using Kokoro-TTS. Two SLMs (LLaMA-Omni and Qwen2-Audio) and their backbone LLMs are evaluated using the BBQ protocol, with free-form responses mapped to multiple-choice options via regex matching or prompt-based LLM classification. Content bias is analyzed through Pearson correlation between SLM and backbone bias scores, while acoustic bias is quantified by measuring bias score differences across speaker conditions and applying McNemar's test for statistical significance.

## Key Results
- Qwen2-Audio strongly inherits content bias from its backbone LLM (r = 0.844–0.848) while minimizing acoustic bias through end-to-end training
- LLaMA-Omni shows weaker content bias inheritance (r drops from 0.620 to 0.301) but exhibits significant acoustic bias variation across gender and accent conditions
- VoiceBBQ successfully isolates acoustic bias effects, with McNemar's test identifying statistically significant decision changes attributable to speaker characteristics
- The frozen Whisper encoder in LLaMA-Omni preserves acoustic information that influences bias outcomes, while Qwen2-Audio's pooling structure dampens these effects

## Why This Works (Mechanism)

### Mechanism 1
Architectural choices in SLMs determine whether acoustic features propagate to influence bias outcomes. LLaMA-Omni uses a frozen Whisper-large-v3 encoder with a lightweight speech adapter, which preserves speaker-specific acoustic information (gender, accent) and allows it to influence LLM predictions. Qwen2-Audio uses an end-to-end trained encoder with pooling operations that appear to dilute speaker-specific information before it reaches the language model.

### Mechanism 2
Content-based bias in SLMs partially inherits from backbone LLMs, but training data characteristics can reshape this inheritance. Qwen2-Audio maintains strong correlation with its backbone Qwen1 (r = 0.844–0.848), suggesting bias patterns transfer through joint training. LLaMA-Omni shows weaker correlation (r drops from 0.620 to 0.301) with LLaMA 3.1 because it was trained on the separately constructed InstructS2S-200K dataset, which appears to alter inherent biases during speech-interaction fine-tuning.

### Mechanism 3
Systematic acoustic variation with controlled content enables isolation and quantification of acoustic bias independent of content bias. VoiceBBQ synthesizes 16 voices per context (4 speakers × 2 genders × 2 accents), holding text constant. By comparing model responses across gender and accent conditions using McNemar's test, the benchmark detects statistically significant decision changes attributable to acoustic features alone.

## Foundational Learning

- **BBQ bias scoring protocol (ambiguous vs. disambiguated contexts)**: Why needed - VoiceBBQ inherits this evaluation logic; understanding when models should answer vs. say "UNKNOWN" is essential for interpreting bias scores. Quick check - In an ambiguous context where no sufficient information exists, what should an unbiased model respond?
- **McNemar's test for paired categorical data**: Why needed - The paper uses this to statistically test whether response distributions differ significantly across acoustic conditions. Quick check - What does McNemar's test evaluate that a simple accuracy difference does not capture?
- **Frozen vs. end-to-end encoder training in multimodal models**: Why needed - The architectural distinction between LLaMA-Omni and Qwen2-Audio directly explains their divergent acoustic sensitivity. Quick check - Does a frozen encoder's output representation change during SLM fine-tuning?

## Architecture Onboarding

- **Component map**: Raw audio -> Whisper encoder -> acoustic embeddings -> speech adapter -> LLM tokens -> LLM backbone -> response generation
- **Critical path**: Input speech → Whisper encoder → acoustic embeddings (may retain speaker info depending on architecture) → adapter/projector → LLM-compatible tokens → LLM processes tokens + text question → generates response → response mapped to answer choices → bias scores computed per BBQ protocol
- **Design tradeoffs**: Frozen encoder + lightweight adapter better for isolating backbone behavior but preserves acoustic bias pathways; end-to-end encoder training can learn to suppress irrelevant acoustic features but may also lose useful prosodic information; dataset choice for speech fine-tuning reshapes content bias but may introduce new biases
- **Failure signatures**: High standard deviation in bias scores across speakers indicates acoustic sensitivity; significant McNemar's chi-square values flag categories where speaker characteristics affect decisions; weak correlation between SLM and backbone LLM bias patterns suggests training data has altered bias structure
- **First 3 experiments**: 1) Run baseline bias evaluation on backbone LLM (text-only BBQ) to establish correlation reference for SLM comparison; 2) Test VoiceBBQ on target SLM across all 16 speaker conditions; compute per-category bias scores and standard deviations; 3) Apply McNemar's test on disambiguated items to identify which social categories show statistically significant acoustic bias effects

## Open Questions the Paper Calls Out

- **Generalizability of architectural contrasts**: Do the observed architectural contrasts in bias propagation generalize to a wider range of Spoken Language Models beyond LLaMA-Omni and Qwen2-Audio?
- **Bias mitigation methods**: What specific architectural designs or training interventions can actively mitigate acoustic-based social bias in SLMs without degrading content fidelity?
- **Sociocultural mechanisms**: What are the underlying sociocultural or data-driven mechanisms that cause specific acoustic features to trigger stronger biases in categories like gender identity or SES?
- **Causal role of integration method**: Does the method of speech integration (frozen encoder with lightweight adapter vs. jointly trained encoder) causally determine the degree to which content-based bias is inherited from the backbone LLM?

## Limitations

- Architectural explanations for differential bias patterns rely on design inference rather than direct measurement of encoder output representations
- Response mapping pipeline introduces potential error sources through its two-stage process, though error rates are not reported
- Benchmark construction assumes Kokoro-TTS synthesis quality is consistent across all speaker conditions, which could confound acoustic bias measurements

## Confidence

- **High confidence**: Content bias inheritance patterns from backbone LLMs (Qwen2-Audio shows strong correlation; LLaMA-Omni shows weak correlation) - directly measured through Pearson correlation
- **Medium confidence**: Acoustic bias sensitivity differences between architectures - observed consistently but causal mechanism inferred from design rather than measured
- **Medium confidence**: VoiceBBQ's effectiveness as a bias diagnostic tool - validated through systematic testing but limited to two SLM architectures

## Next Checks

1. Conduct ablation studies by modifying Qwen2-Audio's pooling layers or LLaMA-Omni's adapter architecture to test whether architectural changes predictably shift acoustic sensitivity patterns
2. Implement controlled synthesis quality tests comparing bias scores when identical content is synthesized under varying acoustic conditions to isolate synthesis artifacts from genuine acoustic bias
3. Perform direct measurement of encoder output representations across speaker conditions to quantify information retention/dilution rather than inferring from downstream bias effects