---
ver: rpa2
title: 'GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision'
arxiv_id: '2504.04954'
source_url: https://arxiv.org/abs/2504.04954
tags:
- learning
- graph
- classes
- class
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses Graph Class Incremental Learning (GCL) under
  weak supervision, where novel classes are introduced incrementally with few or zero
  labeled instances. The proposed framework, GOTHAM, combines episodic learning, prototype
  representation with extended support sets via random walks, and teacher-student
  knowledge distillation to handle evolving graph data while mitigating forgetting.
---

# GOTHAM: Graph Class Incremental Learning Framework under Weak Supervision

## Quick Facts
- **arXiv ID:** 2504.04954
- **Source URL:** https://arxiv.org/abs/2504.04954
- **Reference count:** 40
- **Primary result:** GOTHAM achieves consistent performance gains (6.4% to 54%) over state-of-the-art methods in Graph Class Incremental Learning under weak supervision.

## Executive Summary
GOTHAM addresses the challenge of Graph Class Incremental Learning (GCL) where novel classes are introduced incrementally with few or zero labeled instances. The framework combines episodic learning, prototype representation with extended support sets via random walks, and teacher-student knowledge distillation to handle evolving graph data while mitigating catastrophic forgetting. It operates in both Few-Shot Class Incremental Learning (GFSCIL) and GCL settings, leveraging semantic attributes in text-attributed graphs. Experiments on Cora-ML, Amazon, and OBGN-Arxiv datasets demonstrate significant improvements over existing methods.

## Method Summary
GOTHAM uses a meta-learning approach where episodes are sampled as tasks containing support and query sets. Prototypes are formed by averaging embeddings of an extended support set, which includes labeled nodes and their k-hop random walk neighbors. The framework employs three main losses: intra-class clustering (pulling samples to prototypes), inter-class segregation (pushing prototypes apart), and semantic manipulation (aligning semantic and attribute spaces). During incremental learning, knowledge distillation from a frozen teacher model preserves previously learned representations while adapting to new classes. For zero-shot classes, prototypes are derived from semantic descriptions using a semantic encoder.

## Key Results
- Average accuracy improvements of 6.4% to 54% across different incremental streams compared to state-of-the-art methods
- Consistent performance gains on Cora-ML, Amazon, and OBGN-Arxiv datasets
- Model-agnostic framework that works across different GNN architectures
- Effective preservation of prototype representations despite evolving graph structures

## Why This Works (Mechanism)

### Mechanism 1: Extended Support Sets via Random Walks
The framework leverages the label smoothness assumption—nodes that are close in graph structure tend to share similar labels. By performing k-hop random walks from labeled support nodes, unlabeled neighbors are gathered into an extended support set. The prototype is then computed as the average embedding of all nodes in this extended set, creating a more robust representation under weak supervision.

### Mechanism 2: Prototype-Based Metric Space with Clustering and Segregation
Jointly optimizing intra-class clustering and inter-class segregation losses creates a metric space where prototypes are well-separated class anchors. The clustering loss pulls samples toward their class prototype within a margin, while the segregation loss pushes all class prototypes apart using log-distance. This dual objective ensures compact class regions and distinguishable classes even as new ones are added.

### Mechanism 3: Teacher-Student Knowledge Distillation
Distilling embeddings and semantic alignments from a frozen teacher model to a student model mitigates catastrophic forgetting of previously learned classes. During fine-tuning on new classes, the teacher provides soft targets through embedding and alignment distillation losses, ensuring old class representations remain valid.

### Mechanism 4: Semantic-Attribute Integration for Zero-Shot Classes
Encoding class semantic descriptions enables prototype representation for unseen classes with zero training instances. For unseen classes, the prototype is defined as the output of processing the semantic attribute vector through the GNN with self-loop adjacency. For seen classes, semantic embeddings are merged with graph-based prototypes.

## Foundational Learning

- **Graph Neural Networks (GNNs) and Message Passing**
  - Why needed here: GNNs generate node embeddings that aggregate neighborhood information essential for prototype computation
  - Quick check: Can you explain how a 2-layer GCN or GAT aggregates information from a node's 2-hop neighborhood?

- **Meta-Learning and Episodic Training**
  - Why needed here: The framework uses episodic learning where tasks are sampled rather than batch training
  - Quick check: What is the difference between meta-training on episodes and standard supervised training on batches?

- **Prototype-Based Classification**
  - Why needed here: Classification is performed by finding the nearest prototype in embedding space
  - Quick check: Given query embedding q and prototypes {P₁,...,P_C}, how would you classify q using Euclidean distance?

- **Knowledge Distillation (Teacher-Student)**
  - Why needed here: Anti-forgetting is achieved by distilling knowledge from the previous-session teacher to the current student
  - Quick check: What is the intuition behind using L2 distance between teacher and student embeddings as a regularization term?

- **Zero-Shot Learning and Semantic Attributes**
  - Why needed here: GOTHAM handles unseen classes using only semantic descriptions
  - Quick check: How does a zero-shot classifier use class embeddings to classify instances at test time?

## Architecture Onboarding

- **Component map:** Episode Sampler → Support Set Extension (Random Walks) → Prototype Computation → GNN Encoder → Loss Computation → Gradient Update
- **Critical path:** Base Training (t=0) → Freeze Teacher → Incremental Fine-tuning (t>0) → Inference
- **Design tradeoffs:** Random-walk length (2-4 hops optimal), loss weights tuning, GNN backbone choice (GCN, GAT, or GraphSAGE), prototype reliance on CSD quality
- **Failure signatures:** Prototype collapse, rapid accuracy drop across sessions, zero-shot classes misclassified, extended support set adds noise
- **First 3 experiments:** 1) Reproduce GFSCIL baseline on Amazon (1-way 5-shot) to validate core mechanisms, 2) Ablation on random-walk length to confirm optimal range, 3) Zero-shot stress test on OGBN-Arxiv to validate semantic-integration robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical lower bound on prototype distortion be further tightened or overcome in evolving graphs where edge deletions or feature drift occur?
- **Basis in paper:** Section 4 proves prototype distortion is unavoidable under growing graph assumptions but doesn't analyze edge deletions or feature shifts
- **Why unresolved:** The theoretical analysis explicitly assumes new nodes connect to existing ones but doesn't model edge removal or changing feature distributions
- **What evidence would resolve it:** A theoretical extension incorporating edge deletion probabilities, or empirical results on dynamic graphs with temporal feature drift

### Open Question 2
- **Question:** How does the reliance on label smoothness assumption affect performance on graphs with high heterophily?
- **Basis in paper:** Section 3.2 states the approach leverages smoothness assumption to extend support sets via random walks
- **Why unresolved:** The paper validates on citation and co-purchase networks (typically homophilic) but doesn't test on heterophilic graphs
- **What evidence would resolve it:** Ablation studies on heterophilic datasets comparing extended support set against raw k-shot support set

### Open Question 3
- **Question:** Is the linear projection (MLP) sufficient to align semantic attribute space with structural feature space for unseen classes when the semantic gap is large?
- **Basis in paper:** Section 3.3 and 3.4 define zero-shot prototype as output of MLP on semantic attributes
- **Why unresolved:** The framework relies on quality of Class Semantic Descriptors (CSDs); if semantic descriptions are sparse or vastly different from structural features, simple MLP alignment might fail
- **What evidence would resolve it:** Experiments analyzing correlation between semantic complexity/dimensionality of CSDs and zero-shot classification accuracy

## Limitations
- Random-walk extension mechanics lack explicit specification of transition probabilities and hop-length distributions
- Semantic attribute generation depends on unspecified word2vec corpus, affecting zero-shot alignment performance
- Prototype averaging assumes unimodal class distributions; multimodal classes may suffer from representation collapse

## Confidence

- **High confidence** in clustering and segregation loss mechanics and their role in maintaining prototype separation
- **Medium confidence** in random-walk extension effectiveness; evidence is theoretical but lacks ablation on walk quality metrics
- **Medium confidence** in semantic integration for zero-shot classes; performance depends heavily on CSD quality which is underspecified

## Next Checks

1. Implement and test multiple random-walk strategies (uniform vs. degree-weighted) on Cora-ML to measure impact on prototype quality and accuracy stability
2. Generate CSDs using different word2vec corpora (Google News vs. scientific domain) and measure zero-shot accuracy variance on OGBN-Arxiv
3. Design synthetic graph streams with known multimodal class distributions to test prototype averaging limits and identify failure thresholds