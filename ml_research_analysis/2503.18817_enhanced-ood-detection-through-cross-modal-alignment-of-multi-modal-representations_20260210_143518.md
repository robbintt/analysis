---
ver: rpa2
title: Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations
arxiv_id: '2503.18817'
source_url: https://arxiv.org/abs/2503.18817
tags:
- oodd
- text
- performance
- methods
- auroc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the challenge of improving out-of-distribution\
  \ (OoD) detection (OoDD) performance in multi-modal learning. The authors identify\
  \ that na\xEFve fine-tuning methods fail to leverage pretrained textual knowledge\
  \ due to a modality gap within in-distribution embeddings in the hyperspherical\
  \ representation space."
---

# Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations

## Quick Facts
- arXiv ID: 2503.18817
- Source URL: https://arxiv.org/abs/2503.18817
- Reference count: 40
- Primary result: CMA achieves state-of-the-art OoD detection on ImageNet-1k benchmarks (MOS and OpenOOD v1.5)

## Executive Summary
This paper addresses the challenge of improving out-of-distribution (OoD) detection performance in multi-modal learning by identifying that naïve fine-tuning methods fail to leverage pretrained textual knowledge due to a modality gap within in-distribution embeddings in the hyperspherical representation space. The authors propose Cross-Modal Alignment (CMA), a novel multi-modal fine-tuning method that regularizes distances between image and text embeddings of in-distribution data, bringing them closer together while maintaining separation from negative concepts. CMA is theoretically shown to correspond to maximum likelihood estimation of an energy-based model on a hypersphere and achieves state-of-the-art OoD detection performance on ImageNet-1k benchmarks while maintaining leading in-distribution accuracy.

## Method Summary
The method fine-tunes CLIP's image and text encoders using a combination of the standard symmetric cross-entropy loss and a Cross-Modal Alignment (CMA) regularization term. The CMA loss computes the negative log-sum-exp of the logits across all image-text pairs in a batch, effectively encouraging the embeddings of in-distribution (ID) data to align across modalities. The total loss is the average of the standard CLIP loss and the CMA regularization, controlled by a hyperparameter λ. At inference, OoD detection is performed using a NegLabel score that compares the similarity of an image to ID text embeddings versus negative concept embeddings mined from WordNet.

## Key Results
- On MOS benchmark: CMANegLabel achieves FPR95 of 19.93% and AUROC of 95.13%, surpassing prior approaches
- Maintains leading in-distribution accuracy while improving OoD detection
- CMA achieves the smallest Uni-CM and Uni-CMM values, demonstrating effective reduction of the modality gap
- Enhanced separability between ID and OoD data in the hyperspherical embedding space

## Why This Works (Mechanism)

### Mechanism 1: Modality Gap Reduction via Global Alignment
Reducing the separation between image and text embeddings of in-distribution data enables more effective utilization of pretrained textual knowledge for OoD detection. The standard CLIP contrastive loss creates bipartite clustering where images and texts occupy separate regions on the hypersphere. CMA introduces a regularization term that globally increases cosine similarity between all ID image-text pairs (not just matching ones), effectively collapsing this modality gap. This is achieved by modifying the loss to include the log-sum-exp of all pairwise similarities.

### Mechanism 2: Energy-Based Model Equivalence on Hypersphere
The CMA objective corresponds to maximum likelihood estimation of a joint energy-based model on the hypersphere. By defining similarity as negative energy, the joint distribution becomes a Gibbs-Boltzmann distribution. The standard CLIP loss captures only the discriminative term, while CMA adds the generative term through regularization, which is tractable on the hypersphere using von Mises-Fisher distributions without needing the partition function.

### Mechanism 3: Enhanced Negative Concept Discriminability
Aligning ID image-text pairs increases the distinguishability between ID data and pretrained negative concept labels, improving OoD scoring. The NegLabel OoD score computes the ratio of similarity to ID texts versus similarity to negative texts. When ID images are aligned with ID texts, the numerator increases while the denominator remains lower due to preserved separation, amplifying the score difference between ID and OoD samples.

## Foundational Learning

- **Hyperspherical Embedding Space**: All similarity computations occur on a unit sphere where dot product equals cosine similarity; understanding this geometry is essential for grasping the modality gap and alignment mechanics.
  - Quick check: Given two L2-normalized vectors with dot product 0.8, what is their Euclidean distance on the hypersphere?

- **Contrastive Learning Decomposition (Alignment vs Uniformity)**: The paper reframes CLIP's loss as balancing alignment (numerator: positive pair similarity) and uniformity (denominator: distribution spread), revealing why the modality gap emerges.
  - Quick check: In a batch of B image-text pairs, which term in the CLIP loss pushes non-matching pairs apart?

- **Energy-Based Models on Constrained Spaces**: The theoretical justification connects CMA to probabilistic modeling; understanding EBM basics helps interpret why the generative term improves OoD detection.
  - Quick check: Why does the hyperspherical constraint eliminate the need to compute the partition function Z(θ)?

## Architecture Onboarding

- **Component map:**
  ```
  Input Image → Image Encoder f(·) → L2 Normalize → Image Embedding i
  Input Text → Text Encoder g(·) → L2 Normalize → Text Embedding t
  Similarity Matrix: logits = scale * (i @ t.T)
  Loss Computation:
  ├── CLIP Loss: Cross-entropy on logits (symmetric)
  └── CMA Loss: -logsumexp(logits, dim=1) per modality
  Total Loss: (img_loss + λ * CMA_img + text_loss + λ * CMA_text) / 2
  OoD Score (inference):
  S(I) = Σ exp(i·t_ID) / (Σ exp(i·t_ID) + Σ exp(i·t_neg))
  ```

- **Critical path:** Input image → fine-tuned encoder → embedding alignment quality → OoD score separation between ID and negative concepts. The λ hyperparameter is the primary control knob.

- **Design tradeoffs:**
  - Higher λ → stronger alignment → better OoD detection but risk of over-fitting to ID
  - Lower λ → closer to standard FLYP → better generalization but larger modality gap
  - Batch size affects logsumexp approximation quality for marginal density estimation

- **Failure signatures:**
  - ID accuracy drops significantly → λ too high, semantic collapse
  - OoD detection worse than zero-shot → encoders diverged from pretrained knowledge (learning rate too high)
  - Near-OoD performance degrades → negative concepts may overlap with ID semantically

- **First 3 experiments:**
  1. Reproduce modality gap visualization (Figure 2) with PCA on ImageNet-1k validation embeddings before/after CMA; verify Uni-CMM metric decreases
  2. Ablation on λ ∈ {0.0001, 0.001, 0.01, 0.1} with validation set early stopping; plot ID accuracy vs. FPR95 to find Pareto frontier
  3. Compare CMA + MCM scoring vs. CMA + NegLabel scoring on a single OoD dataset (e.g., iNaturalist) to isolate the contribution of negative concepts

## Open Questions the Paper Calls Out

- How can auxiliary negative labels be explicitly integrated into the multi-modal fine-tuning training process to further enhance OoD detection performance?
- How can the CMA objective be modified to better distinguish semantically similar "Near-OoD" samples where visual features may be more discriminative than current semantic alignments?
- Does the theoretical equivalence of CMA to a hyperspherical energy-based model imply that sampling-based training techniques could optimize the embedding space more effectively than the current regularization?

## Limitations

- Performance on semantically similar OoD datasets (SSB-hard) remains notably weaker, suggesting limitations in handling near-distribution OoD
- The method's reliance on mining 10,000 negative concepts from WordNet introduces potential brittleness if the negative labels are not sufficiently distinct from ID concepts
- The theoretical connection between CMA and maximum likelihood estimation on the hypersphere, while mathematically elegant, lacks extensive empirical validation

## Confidence

- **High Confidence:** CMA effectively reduces the modality gap and improves OoD detection metrics on standard benchmarks (MOS, OpenOOD v1.5)
- **Medium Confidence:** The hyperspherical energy-based model interpretation provides valid theoretical grounding, though practical implications require further exploration
- **Medium Confidence:** The method maintains ID accuracy while improving OoD detection, though the optimal λ value appears sensitive to dataset characteristics

## Next Checks

1. Conduct ablation studies on λ across different ImageNet-1k variants (e.g., ImageNet-21k, ImageNet-V2) to assess robustness to domain shifts
2. Implement a stress test with adversarially constructed negative labels that overlap semantically with ID concepts to evaluate failure modes
3. Compare CMA's computational overhead during inference against zero-shot baselines to quantify practical deployment costs