---
ver: rpa2
title: Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems
arxiv_id: '2510.02377'
source_url: https://arxiv.org/abs/2510.02377
tags:
- arxiv
- calibrated
- multi-llm
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Uncertainty-Aware Answer Selection for Improved Reasoning in Multi-LLM Systems

## Quick Facts
- arXiv ID: 2510.02377
- Source URL: https://arxiv.org/abs/2510.02377
- Reference count: 24
- Primary result: ~4% accuracy improvement on GSM8K over random tie-breaking

## Executive Summary
This paper introduces an uncertainty-aware method for selecting the best response from multiple LLMs without requiring external verifiers. The core innovation is using a calibrated log-likelihood score that averages per-token probabilities across diverse models to create a comparable metric for answer quality. The method is computationally efficient (O(N²) forward passes via teacher-forcing) and shows consistent improvements across reasoning tasks like GSM8K, MMLU, and ARC-Challenge, particularly in tie-break scenarios where majority voting fails.

## Method Summary
The method computes a calibrated log-likelihood score for each candidate response by averaging per-token log probabilities across all N models using teacher-forcing (single forward pass per model-response pair). This creates a normalized, comparable metric across heterogeneous models. The selection process first checks for majority agreement; if none exists, it ranks responses by calibrated score and selects the highest. The approach is designed to leverage model consensus while remaining computationally efficient compared to autoregressive generation-based scoring methods.

## Key Results
- Achieves ~4% accuracy improvement on GSM8K over random tie-breaking
- Shows consistent gains across reasoning tasks: ~3% MMLU, ~5% ARC-Challenge
- Calibrated scoring particularly effective in tie-break scenarios where majority voting fails
- Computational efficiency demonstrated through O(N²) teacher-forcing vs O(N²L) generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Averaging log-likelihood scores across multiple LLMs creates a calibrated, comparable metric for selecting the best response from heterogeneous models.
- **Mechanism:** Each model πᵢ evaluates every candidate response Yʲ via teacher-forcing, computing per-token log-likelihood. These are averaged across all N models: Score(Yʲ) = (1/N) Σᵢ log P_πᵢ(Yʲ|x, X). This normalization removes model-specific scale biases, making scores comparable across candidates from different architectures.
- **Core assumption:** Models trained on correct data will assign higher probability (lower negative log-likelihood) to correct answers than incorrect ones, and this signal persists when averaged across diverse models.
- **Evidence anchors:** [abstract] "calibrated log-likelihood score, implicitly leveraging the inherent knowledge and confidence of these models" [Section 2.1] "This metric measures consensus among models by averaging response likelihood across all models, providing a calibrated measure of answer quality." [Section 2.2] "our method normalizes log-likelihood scores across multiple models... making scores directly comparable"
- **Break condition:** If models systematically assign high confidence to incorrect answers (e.g., shared training biases), calibration fails. The paper notes limitations in high-cost settings where majority voting already works well.

### Mechanism 2
- **Claim:** Model confidence (low uncertainty) serves as a proxy for response correctness, enabling discrimination between correct and incorrect outputs without external verification.
- **Mechanism:** The method relies on the observation that models demonstrate higher confidence on familiar data. When a model "knows" an answer, it maintains confidence even when shown competing incorrect responses. The calibrated score captures this by identifying responses that all models collectively find probable.
- **Core assumption:** Training data is correct, and familiarity with a domain/problem type translates to both higher confidence and higher accuracy.
- **Evidence anchors:** [Section 1] "our method is built on the hypothesis that a model (or expert) trained on a specific example will exhibit high confidence (i.e., low uncertainty)" [Section 2.2] "According to (Yadkori et al., 2024), models demonstrate higher confidence and reduced hallucinations when familiar with a given data point" [Section 2.2] "confident models maintain their confidence even when prompted with incorrect answers, unlike less confident models whose uncertainty notably increases"
- **Break condition:** If models are confidently wrong (calibration failure) or if adversarial/counterintuitive problems cause high uncertainty even for correct reasoning, the proxy breaks.

### Mechanism 3
- **Claim:** Teacher-forcing computation of log-likelihood enables O(N²) forward passes instead of O(N²L) autoregressive generations, achieving efficiency for long responses.
- **Mechanism:** Rather than generating responses to score them, the method takes already-generated completions and computes their likelihood under each model via single forward passes (processing all L tokens in parallel). This avoids sequential autoregressive decoding costs.
- **Core assumption:** Log-likelihood computed via teacher-forcing is a valid proxy for model confidence in its own generated outputs.
- **Evidence anchors:** [abstract] "computationally efficient method... using a calibrated log-likelihood score" [Section 2.3] "by using teacher-forcing, we compute calibrated scores with a single forward pass, avoiding costly autoregressive decoding" [Section 2.3] "Cost_cal = O(N²)" vs "Cost_gen = O(N²L)" for generation-based approaches
- **Break condition:** If teacher-forced likelihood diverges significantly from autoregressive generation likelihood (exposure bias), scores may not reflect true model confidence.

## Foundational Learning

- **Concept: Log-Likelihood and Perplexity**
  - Why needed here: The method's core metric is average per-token log-likelihood; understanding how this relates to model confidence is essential.
  - Quick check question: Given a model's token probabilities [0.9, 0.8, 0.1] for a 3-token sequence, compute the average log-likelihood. Is this sequence "confident"?

- **Concept: Teacher-Forcing vs. Autoregressive Decoding**
  - Why needed here: The efficiency claim depends on computing likelihood via teacher-forcing (parallel) vs. sequential generation.
  - Quick check question: Why can teacher-forcing compute all token likelihoods in parallel while autoregressive generation cannot?

- **Concept: Calibration in Uncertainty Estimation**
  - Why needed here: The paper's central claim is that uncalibrated scores are incomparable across models; calibration via cross-model averaging is the solution.
  - Quick check question: If Model A's typical log-likelihood is -50 and Model B's is -150, why can't you directly compare a -100 score from each?

## Architecture Onboarding

- **Component map:** Response Generator Pool -> Likelihood Evaluator -> Calibration Aggregator -> Selection Logic
- **Critical path:** 1) All N models generate responses to prompt x 2) Each response Y^j is evaluated by all N models via teacher-forcing (N² total forward passes) 3) Scores are aggregated; best response selected (with majority-vote shortcut if applicable)
- **Design tradeoffs:** Diversity vs. Comparability (more diverse models increase potential but complicate calibration), Computation vs. Coverage (O(N²) teacher-forcing vs. O(N²L) generation; gains scale with response length L), Tie-break vs. All-case (applying metric only to tie-break cases saves computation with minimal accuracy loss)
- **Failure signatures:** 1) All-high-scores: If all responses score similarly, calibration isn't discriminating—check tokenization consistency 2) Model-dominance bias: If one model's scores consistently dominate, normalization may be insufficient 3) Length bias: If shorter answers consistently win, per-token normalization may be failing
- **First 3 experiments:** 1) Baseline Reproduction: Implement calibrated log-likelihood on GSM8K with the 3 specified models; verify ~4% improvement over random tie-breaking 2) Ablation on Calibration: Compare single-model log-likelihood selection, uncalibrated multi-model scores, and calibrated scores—isolate the calibration contribution 3) Length Sensitivity Test: Score synthetic responses of varying lengths but equivalent content; verify per-token normalization prevents length bias

## Open Questions the Paper Calls Out

- **Open Question 1:** How can adaptive sampling strategies be integrated with the calibrated log-likelihood metric to optimize computational efficiency? [explicit] The conclusion states, "Future work can explore adaptive sampling and extend our method to broader reasoning tasks." The current method evaluates responses from a fixed set of models and rounds; it does not dynamically adjust the number of samples or model calls based on intermediate confidence estimates.

- **Open Question 2:** Does the calibrated log-likelihood selection method generalize effectively to open-ended generation tasks where ground truth is ambiguous? [explicit] The authors restrict evaluation to GSM8K, MMLU, and ARC, which are reasoning tasks with clear ground truths. The conclusion proposes extending to "broader reasoning tasks." The metric relies on consensus and confidence grounded in correctness; it is unclear if the method remains robust for subjective tasks (e.g., creative writing) where "correctness" is not binary.

- **Open Question 3:** How does extreme heterogeneity in model size and architecture affect the fairness and accuracy of the calibrated metric? [inferred] Appendix C states, "We use the following model since they are all of same size and have quite comparable performance." Normalizing log-likelihoods across models with vastly different parameter counts (e.g., 7B vs 70B) or vocabulary sizes might introduce bias, as larger models may exhibit different probability distributions.

## Limitations
- The core calibration mechanism relies on averaging log-likelihoods across diverse models, but there's no direct empirical validation that this actually produces comparable scores across architectures.
- The teacher-forcing efficiency claim (O(N²) vs O(N²L)) is theoretically sound but not empirically validated with timing benchmarks.
- The method's performance gain (~4% GSM8K) is modest, and it's unclear how much comes from the calibrated scoring vs. simply having multiple diverse models vote.

## Confidence
- **High:** The problem framing (multi-LLM selection without external verifiers) is well-motivated, and the teacher-forcing efficiency analysis is mathematically correct.
- **Medium:** The overall methodology and experimental setup appear sound, but calibration effectiveness and confidence-correctness correlation lack direct validation.
- **Low:** The claim that calibrated log-likelihood is a reliable proxy for model confidence across heterogeneous architectures—this is the paper's central hypothesis but weakest empirically.

## Next Checks
1. **Calibration Validation:** Run an ablation comparing single-model log-likelihood selection vs. calibrated multi-model scores to isolate the calibration contribution to performance gains.
2. **Length Sensitivity Test:** Generate synthetic responses of varying lengths but equivalent content, then verify that per-token normalization prevents length bias in the scoring.
3. **Confidence-Correctness Correlation:** For correct vs. incorrect responses on the same prompt, measure whether calibrated scores consistently rank correct responses higher across all models.