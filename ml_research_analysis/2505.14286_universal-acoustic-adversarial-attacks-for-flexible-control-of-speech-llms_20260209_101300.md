---
ver: rpa2
title: Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs
arxiv_id: '2505.14286'
source_url: https://arxiv.org/abs/2505.14286
tags:
- attack
- speech
- adversarial
- audio
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates universal acoustic adversarial attacks
  on speech large language models (speech LLMs). The core method involves prepending
  a fixed adversarial audio segment to input speech, which can be optimized to either
  mute the model's output entirely or override the original task prompt, causing the
  model to perform unintended actions.
---

# Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs

## Quick Facts
- arXiv ID: 2505.14286
- Source URL: https://arxiv.org/abs/2505.14286
- Reference count: 13
- Primary result: Fixed adversarial audio segments prepended to speech can mute speech LLMs or override task prompts with >95% effectiveness

## Executive Summary
This paper introduces universal acoustic adversarial attacks that prepend a fixed audio segment to speech inputs, enabling attackers to either mute speech LLM outputs entirely or redirect them to unintended tasks. The attacks work by optimizing the adversarial segment via gradient descent on a small dataset, then applying it universally to any input. Critically, the authors develop selective attacks that conditionally activate based on input attributes like speaker gender or spoken language, allowing precise control over when the model's behavior is altered. Experiments demonstrate high success rates across multiple models (Qwen2-Audio, Granite-Speech) and tasks, with selective attacks successfully suppressing outputs for targeted groups while leaving others unaffected.

## Method Summary
The core attack prepends a fixed, learnable adversarial audio segment to input speech and optimizes it via gradient descent to maximize target output probabilities (e.g., end-of-transcription token for muting, or ASR transcription for task control). The segment is trained on a small dataset with model weights frozen, then applied universally at inference. Selective attacks condition the adversarial effect on input attributes by training on mixed datasets where targets depend on attribute labels. The approach works across different tasks and datasets through shared encoder representations, achieving >95% effectiveness in muting or redirecting task execution.

## Key Results
- Prepend-based attacks achieve >95% mute success rates on speech LLMs
- Fixed adversarial segments generalize across different tasks and datasets
- Selective attacks can suppress outputs for specific groups (e.g., female speakers or specific languages) while leaving others unaffected
- 3.2s adversarial segments achieve near 100% effectiveness on large models, while shorter segments fail
- Cross-task transfer works: segments trained for translation successfully mute ASR outputs

## Why This Works (Mechanism)

### Mechanism 1: Acoustic Token Injection via Prepend-based Perturbation
A fixed adversarial audio segment prepended to input speech functions as an "acoustic command" that overrides textual prompts. The segment is optimized via gradient descent to maximize the probability of a target output sequence (e.g., end-of-transcription token for muting, or ASR transcription for task control). The prepend position ensures the adversarial pattern influences the encoder's hidden states before the original speech is processed.

### Mechanism 2: Conditional Activation via Implicit Attribute Encoding
Selective attacks suppress outputs only for inputs with specific attributes (gender, language) without explicit attribute labels at inference. The adversarial segment is trained on a mixed dataset where targets are set per Equation 6: eot for target-attribute samples, normal transcription otherwise. The model's internal representations of gender/language become coupled with the adversarial perturbation's effect.

### Mechanism 3: Cross-Task and Cross-Dataset Transfer via Shared Latent Space
Adversarial segments trained on one task/dataset transfer to unseen tasks (translation, gender detection) and datasets. The speech encoder produces shared representations across tasks. The adversarial segment's influence on these early representations propagates regardless of downstream task prompt.

## Foundational Learning

- **Concept: Speech Encoder–LLM Interface**
  - Why needed here: Understanding how speech embeddings are transformed and fed to the LLM (pooling layer, Q-former) is critical for reasoning about where adversarial influence propagates.
  - Quick check question: Can you explain how Qwen2-Audio downsamples Whisper encoder outputs before passing to the LLM?

- **Concept: Autoregressive Generation with Special Tokens**
  - Why needed here: The muting attack works by forcing eot as the first token. You need to understand how token probabilities govern termination.
  - Quick check question: What happens to generation if P(y₁ = eot) is maximized?

- **Concept: Gradient-Based Adversarial Optimization**
  - Why needed here: The attack segments are learned via backpropagation through frozen model weights. Understanding loss landscapes and optimization stability is essential.
  - Quick check question: Why might a learning rate of 1e-2 be used for unconstrained attacks vs. 1e-4 for amplitude-constrained attacks?

## Architecture Onboarding

- **Component map:** Input audio (x) → Speech encoder (Whisper for Qwen2-Audio, Conformer for Granite-Speech) → Transformation module (pooling/Q-former) → LLM backbone (Qwen-7B, Granite-8B) → Text output (ŷ); Adversarial segment (a) prepended to x before encoder: a ⊕ x

- **Critical path:** 1) Adversarial segment optimization (white-box, gradients through frozen encoder+LLM) 2) Segment prepending at inference (no model access needed) 3) Encoder processes concatenated audio 4) LLM generates conditioned on perturbed hidden states

- **Design tradeoffs:** Segment length vs. effectiveness: 0.64s fails on large models; 3.2s achieves ~100% (Table 2); Imperceptibility (ϵ constraint) vs. attack success: constrained attacks show reduced effectiveness (Tables 10-13); Attack-ref (clean targets) vs. Attack-hyp (model-generated targets): Attack-ref overfits to training task; Attack-hyp preserves non-ASR capabilities (Table 6)

- **Failure signatures:** Beam search with length penalty > 1.0 resists muting (Granite-Speech: 98.5% greedy → 46.0% beam); Low attribute detection accuracy correlates with reduced selective attack effectiveness; Linguistically similar language pairs (en-fr) show lower selective attack success than dissimilar pairs (en-zh)

- **First 3 experiments:** 1) Reproduce mute attack on Qwen2-Audio: Train 3.2s segment on LibriSpeech dev_other with Pasr, evaluate on test_other. Verify >95% ∅ metric. 2) Test transfer to new prompt: Apply same segment with Pst-fr (translation) and Pgdr (gender detection). Confirm attack persists. 3) Implement selective attack: Train on mixed male/female LibriSpeech with conditional targets (Equation 6). Measure ∅ per gender group and correlate with model's gender detection probability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can universal acoustic adversarial attacks transfer effectively in black-box scenarios where the adversary lacks access to model weights?
- Basis in paper: [explicit] The Limitations section states: "The attack relies on access to the model's weights during training (i.e., white-box settings). The performance of similar attacks in black-box scenarios remains to be explored and may differ significantly."
- Why unresolved: All experiments assume white-box access for gradient-based optimization of the adversarial segment. Real-world attackers typically lack model parameters.
- What evidence would resolve it: Experiments applying attacks trained on surrogate models to target speech LLMs without access to their weights, measuring attack success rate degradation.

### Open Question 2
- Question: Do these universal acoustic adversarial attacks generalize to other speech LLM architectures beyond Qwen2-Audio and Granite-Speech?
- Basis in paper: [explicit] The Limitations section notes: "our attacks are evaluated primarily on a popular speech LLM, Qwen2-Audio... further experiments across diverse models is needed to fully assess the effectiveness and limitations of these attacks."
- Why unresolved: Only two models were evaluated; both use similar encoder-LLM architectures. Vulnerabilities may be architecture-specific or reflect a broader class of weaknesses.
- What evidence would resolve it: Systematic evaluation across diverse speech LLMs with different encoder designs (e.g., SALMONN, Audio-Flamingo) and varying LLM backbones.

### Open Question 3
- Question: Why does the gender-based selective attack show asymmetric effectiveness, with "Mute-female" achieving 92.2% success but "Mute-male" only 71.2%?
- Basis in paper: [inferred] Table 7 shows this asymmetry, and Figure 2 suggests the model's internal gender perception correlates with attack success, but the underlying mechanism remains unexplained.
- Why unresolved: The paper demonstrates correlation between model gender perception and attack effectiveness but does not identify whether this stems from acoustic feature differences, training data biases, or model architecture.
- What evidence would resolve it: Analysis of internal representations during selective attack, experiments with balanced training data, or tests on models with controlled gender representation during pretraining.

## Limitations
- Transferability concerns: Cross-dataset transfer works within English corpora but cross-lingual and cross-model transfer remains unverified
- Selective attack fragility: Effectiveness drops significantly when target and non-target groups have similar attribute distributions
- Defensibility not evaluated: No assessment of defensive mechanisms like input preprocessing or adversarial detection

## Confidence

**High confidence** in: The prepend-based attack mechanism is technically sound and reproducible. The gradient optimization approach for learning adversarial segments is well-specified, and empirical results showing high mute success rates (>95%) on Qwen2-Audio are convincing.

**Medium confidence** in: Cross-task and cross-dataset transfer capabilities. While transfer rates are reported, the paper does not explore failure modes systematically or test transfer to fundamentally different speech domains.

**Low confidence** in: The claimed robustness of selective attacks. Effectiveness appears highly dependent on attribute separability, and the paper does not provide ablation studies showing how performance degrades with attribute overlap.

## Next Checks

1. **Cross-model transfer validation:** Test whether adversarial segments optimized for Qwen2-Audio transfer to Granite-Speech and other speech LLM architectures. Measure effectiveness decay and identify architectural features that confer robustness.

2. **Adversarial detection evaluation:** Implement simple input-level detection methods (e.g., outlier detection on spectral features of prepended segments) to quantify how detectable these attacks are in practice.

3. **Attribute boundary ablation:** Systematically vary attribute similarity between target and non-target groups (e.g., test selective attacks on linguistically related languages like Spanish-Portuguese) to establish the limits of conditional attack effectiveness.