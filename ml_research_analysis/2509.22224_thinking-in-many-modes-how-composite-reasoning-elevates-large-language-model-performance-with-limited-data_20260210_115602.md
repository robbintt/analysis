---
ver: rpa2
title: 'Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model
  Performance with Limited Data'
arxiv_id: '2509.22224'
source_url: https://arxiv.org/abs/2509.22224
tags:
- reasoning
- grpo
- medmcqa
- performance
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Composite Reasoning (CR), a method that
  enhances large language model performance by encouraging the exploration and integration
  of multiple reasoning strategies (deductive, inductive, abductive, causal, and decompositional)
  rather than relying on a single dominant approach. The authors evaluate CR against
  standard Chain-of-Thought (CoT) and Standard Reasoning (SR) baselines using parameter-efficient
  fine-tuning (LoRA) and reinforcement learning (GRPO) on three challenging datasets:
  ARC-Complex, MedMCQA, and MedXpertQA, with a maximum of 1,500 training samples per
  dataset.'
---

# Thinking in Many Modes: How Composite Reasoning Elevates Large Language Model Performance with Limited Data

## Quick Facts
- arXiv ID: 2509.22224
- Source URL: https://arxiv.org/abs/2509.22224
- Authors: Zishan Ahmad; Saisubramaniam Gopalakrishnan
- Reference count: 5
- Primary result: CR achieves 56.30% accuracy on MedMCQA with 1,500 samples, competitive with models trained on 40k-410k samples

## Executive Summary
This paper introduces Composite Reasoning (CR), a method that enhances large language model performance by encouraging the exploration and integration of multiple reasoning strategies (deductive, inductive, abductive, causal, and decompositional) rather than relying on a single dominant approach. The authors evaluate CR against standard Chain-of-Thought (CoT) and Standard Reasoning (SR) baselines using parameter-efficient fine-tuning (LoRA) and reinforcement learning (GRPO) on three challenging datasets: ARC-Complex, MedMCQA, and MedXpertQA, with a maximum of 1,500 training samples per dataset. CR achieves superior sample efficiency, outperforming CoT and SR baselines, particularly on the highly demanding MedXpertQA dataset where it shows an 8.1% accuracy gain.

## Method Summary
The method employs a two-stage training pipeline: first, LoRA-based supervised fine-tuning (SFT) on 1,500 CR-generated trajectories using rank=32, alpha=64 LoRA adapters trained for 12 epochs; second, GRPO with outcome-based binary rewards (correct/incorrect) applied to a new LoRA adapter for 1,500 steps. The approach uses Qwen-2.5-7B-Instruct as the base model (except SR uses DeepSeek-R1-7B) and is evaluated on three question-answering datasets with maximum 1,500 training samples each.

## Key Results
- CR achieves 56.30% accuracy on MedMCQA with 1,500 samples, competitive with HuatuoGPT-o1 (40k samples) and UltraMedical (410k samples)
- On MedXpertQA, CR shows 8.1% accuracy gain over baselines with only 549 average tokens per response versus SR's 1,139 tokens
- GRPO amplifies domain-appropriate reasoning styles: abductive/deductive for medical tasks, causal/inductive for scientific reasoning
- SFT-only CR achieves 55.20% on MedMCQA, with GRPO adding ~1% improvement through style optimization

## Why This Works (Mechanism)

### Mechanism 1: Multi-Strategy Reasoning Trajectory Synthesis
CR elicits more robust problem-solving by prompting models to dynamically synthesize multiple reasoning styles rather than following a single dominant paradigm. The CR prompt explicitly instructs the model to explore deductive, inductive, abductive, causal, and decompositional reasoning within a single reasoning chain, creating diverse cognitive pathways that can cross-validate or compensate for each other during problem-solving.

### Mechanism 2: Outcome-Based GRPO Shapes Domain-Adaptive Reasoning
GRPO with a simple binary outcome reward (correct/incorrect answer) implicitly guides models toward domain-appropriate reasoning style distributions without explicit style-level supervision. GRPO generates multiple reasoning trajectories per input, compares their outcomes, and reinforces the winning trajectory, amplifying reasoning styles that correlate with correct answers in each domain over time.

### Mechanism 3: Sample Efficiency Through Reasoning Diversity
CR's diverse reasoning exposure creates richer training signal per sample, enabling competitive performance with far fewer training examples. Each CR training example demonstrates multiple reasoning modes, effectively providing the model with more "learning opportunities" per sample compared to single-strategy CoT, creating a form of implicit data augmentation at the reasoning level.

## Foundational Learning

- **Low-Rank Adaptation (LoRA)**: Why needed: All experiments use LoRA (rank=32, alpha=64) for parameter-efficient fine-tuning. Quick check: Can you explain why LoRA modifies attention projections (q/k/v/o_proj) and FFN layers (gate/up/down_proj) but keeps the base model frozen?

- **Group Relative Policy Optimization (GRPO)**: Why needed: GRPO is the RL algorithm that shapes reasoning style post-SFT. Quick check: How does GRPO differ from standard PPO in its handling of value functions and group-based reward comparison?

- **Reasoning Paradigms (Abductive, Deductive, Inductive, Causal, Decompositional)**: Why needed: CR's core innovation is integrating these five reasoning types. Quick check: In medical diagnosis, why might abductive reasoning (inferring best explanation) be more prominent than inductive reasoning (generalizing from examples)?

## Architecture Onboarding

- **Component map**: Base Model (Qwen-2.5-7B-Instruct) -> SFT Stage (LoRA adapters on CR trajectories) -> GRPO Stage (new LoRA adapter with binary reward) -> Evaluation (accuracy on test sets)

- **Critical path**: 1) Generate CR trajectories using base model with CR prompt (1,500 samples) 2) Apply SFT with LoRA on these trajectories (7 hours on A100) 3) Run GRPO with group size M, binary reward based on answer correctness (24-48 hours) 4) Evaluate on target dataset, analyze reasoning style distribution shifts

- **Design tradeoffs**: Token efficiency vs. accuracy (SR: 1,139 tokens, CR: 549 tokens after GRPO); SFT-only vs. SFT+GRPO (GRPO adds ~1% improvement); General vs. domain-specific training (CR matches models trained on 40k-410k samples with only 1,500)

- **Failure signatures**: GRPO stalls on very hard tasks if baseline accuracy is too low; Reasoning style collapse if GRPO over-optimizes for a single style; LoRA rank too low for complex reasoning if scaling to larger models

- **First 3 experiments**: 1) Reproduce SFT-only baseline: Train LoRA on CR trajectories for MedMCQA (1,500 samples, 12 epochs) 2) Ablate reasoning styles: Create reduced prompts excluding one style to measure contribution 3) Test transfer across domains: Train CR on MedMCQA, evaluate on ARC-C to assess generalization

## Open Questions the Paper Calls Out

### Open Question 1
Does Composite Reasoning's advantage over CoT persist at larger model scales (e.g., 70B+ parameters or frontier models)? All experiments were conducted on 7B parameter models; no larger models were tested.

### Open Question 2
How does CR performance scale when training data exceeds 1,500 samples? The paper emphasizes sample efficiency in low-data regimes but does not test whether CR's advantage diminishes as data increases.

### Open Question 3
Can the reasoning style classifications (deductive, inductive, abductive, etc.) be validated objectively rather than heuristically? The paper shows style distribution shifts but does not describe how styles were identified or measured.

## Limitations

- **Prompt Template Uncertainty**: The exact CR prompt template is not specified, making faithful reproduction challenging despite the mechanism being described
- **Style Classification Methodology**: The paper demonstrates shifts in reasoning style distributions but does not disclose how these styles are classified or measured
- **Generalization Boundary**: The claim that diverse reasoning creates implicit data augmentation lacks direct experimental support through controlled ablations

## Confidence

- **High Confidence**: CR achieves superior accuracy on tested datasets with 1,500 samples compared to CoT and SR baselines, particularly on MedXpertQA (8.1% gain)
- **Medium Confidence**: GRPO shifts style distributions toward domain-appropriate patterns, but the underlying classification method is not disclosed
- **Low Confidence**: The sample efficiency mechanism through reasoning diversity is plausible but not experimentally isolated

## Next Checks

1. **Reconstruct and test the CR prompt**: Create candidate prompt templates that explicitly instruct multi-strategy reasoning, generate trajectories, and evaluate whether they produce meaningfully different reasoning patterns compared to CoT

2. **Isolate reasoning style contributions**: Conduct ablations by removing one reasoning style at a time from the CR prompt and retraining to determine which styles contribute most to domain-specific performance

3. **Test GRPO style stability**: Monitor reasoning style distributions throughout GRPO training to verify that style shifts are progressive and stable rather than volatile