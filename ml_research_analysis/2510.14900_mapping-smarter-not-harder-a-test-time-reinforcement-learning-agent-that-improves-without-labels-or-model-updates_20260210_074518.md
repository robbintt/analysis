---
ver: rpa2
title: 'Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That
  Improves Without Labels or Model Updates'
arxiv_id: '2510.14900'
source_url: https://arxiv.org/abs/2510.14900
tags:
- schema
- mapping
- confidence
- evidence
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a test-time reinforcement learning agent
  for schema mapping that self-improves without labeled data or model updates. The
  agent identifies ambiguous field mappings, formulates targeted web-search queries,
  and uses confidence-based rewards to iteratively refine its mappings.
---

# Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates

## Quick Facts
- arXiv ID: 2510.14900
- Source URL: https://arxiv.org/abs/2510.14900
- Authors: Wen-Kwang Tsao; Yao-Ching Yu; Chien-Ming Huang
- Reference count: 12
- Improves accuracy from 56.4% to 93.94% on schema mapping without ground truth labels or model updates

## Executive Summary
This paper introduces a test-time reinforcement learning agent that self-improves schema mapping accuracy without requiring labeled data or model updates. The agent identifies ambiguous field mappings, generates targeted web search queries, and uses confidence-based rewards to iteratively refine its mappings. Tested on converting Microsoft Defender for Endpoint logs to a common schema, the approach improved accuracy from 56.4% to 93.94% over 100 iterations using GPT-4o, while reducing low-confidence mappings requiring expert review by 85%.

## Method Summary
The method employs a test-time reinforcement learning loop where the agent maintains a mapping hypothesis and collected evidence. For each iteration, the LLM generates mappings with current context, detects conflicts across three prompt variants, and computes confidence scores based on prediction consistency. When conflicts are detected, the agent formulates targeted search queries, retrieves web evidence, and updates its evidence set only if confidence improves. The confidence score serves as a proxy reward (r_t = C_{t+1} - C_t), enabling continual improvement without ground truth labels or weight updates.

## Key Results
- Accuracy improvement from 56.4% to 93.94% over 100 iterations using GPT-4o
- 85% reduction in low-confidence mappings requiring expert review
- Evidence collection acceptance rate of 81% (19/100 iterations rejected evidence)
- Outperformed direct search baseline (78.79%) and heuristic retrieval baseline (87.88%)

## Why This Works (Mechanism)
The approach works by using confidence scores as proxy rewards for test-time learning, where the agent iteratively refines its mapping decisions based on evidence quality. By detecting conflicts through multiple prompt variants and only accepting evidence that improves confidence, the system creates a self-reinforcing loop that progressively improves accuracy without requiring ground truth labels.

## Foundational Learning
- **Schema Mapping Fundamentals**: Understanding how fields in different data schemas correspond to each other - needed to grasp the core problem being solved
- **Confidence-based RL**: Using confidence scores as proxy rewards instead of traditional RL rewards - needed to understand the self-improvement mechanism
- **Test-time Learning**: Updating model behavior without retraining - needed to appreciate the innovation of improving without model updates
- **RAG with Evidence Filtering**: Retrieving and selectively incorporating evidence based on quality metrics - needed to understand the evidence collection process
- **Conflict Detection via Prompt Variants**: Identifying uncertain mappings by comparing multiple prompt formulations - needed to grasp the confidence estimation approach
- **Quick Check**: Verify understanding by explaining how confidence scores serve as rewards in this test-time RL setup

## Architecture Onboarding

**Component Map**: Schemas -> LLM Prompt Generator -> Confidence Estimator -> Conflict Detector -> Search Query Generator -> Web Evidence Retriever -> Evidence Filter -> Updated Evidence Context -> (loop)

**Critical Path**: The core loop flows from schema input through LLM generation, confidence estimation, conflict detection, evidence retrieval, and filtering back to updated context for the next iteration.

**Design Tradeoffs**: The approach trades off immediate accuracy for long-term improvement by accepting some uncertainty early in the process, while avoiding the need for labeled data or model updates.

**Failure Signatures**: 
- Overconfidence without accuracy gains indicates poor confidence calibration
- Stagnant accuracy suggests evidence quality issues or insufficient conflict detection
- Rapid confidence saturation without accuracy improvement indicates the confidence metric may not correlate well with actual correctness

**First Experiments**:
1. Run a single iteration to verify the LLM can generate mappings and detect conflicts
2. Test the evidence retrieval mechanism with a known query to verify web search integration
3. Validate the confidence calculation across multiple prompt variants for a sample mapping

## Open Questions the Paper Calls Out
- Do ensemble-based confidence scores or explicit LLM self-assessments improve calibration compared to the consistency-based approach? The paper notes future research could employ multiple models for ensemble confidence or prompt for self-assessed scores to address the observed "persistent overconfidence."
- Does the test-time RL framework generalize to schema mapping tasks in domains outside of cybersecurity? The paper identifies validation in other domains such as healthcare or finance as necessary to strengthen generalizability claims.
- Can the confidence-based reward mechanism effectively guide agents in handling complex mapping cardinalities (1-to-N, N-to-M)? The paper identifies the extension to complex mapping cardinalities as remaining future work.

## Limitations
- Evaluation based on a small set of 66 manually verified mappings, which may not represent full complexity
- Performance with smaller models or different LLM architectures remains untested
- Framework validated only on Microsoft Defender for Endpoint logs, limiting generalizability claims

## Confidence
- **High confidence**: The test-time RL framework using confidence scores as proxy rewards is technically sound
- **Medium confidence**: The accuracy improvement from 56.4% to 93.94% is impressive but depends heavily on the specific evaluation set
- **Low confidence**: Claims about scalability to larger schemas or applicability to other domains lack empirical validation

## Next Checks
1. Reproduce accuracy improvement curve: Run the 100-iteration test-time RL loop on the same Microsoft Defender for Endpoint to Common Schema conversion task, measuring accuracy and confidence at each iteration to verify the reported progression from 56.4% to 93.94%.

2. Validate evidence quality filtering: Test the evidence collection mechanism by running iterations with and without the confidence-based evidence acceptance criterion, measuring whether evidence rejection rate correlates with mapping quality improvements.

3. Cross-domain generalization test: Apply the same framework to a different schema mapping task (e.g., between two enterprise log formats not used in the paper) to assess whether the test-time learning approach generalizes beyond the Microsoft Defender for Endpoint case.