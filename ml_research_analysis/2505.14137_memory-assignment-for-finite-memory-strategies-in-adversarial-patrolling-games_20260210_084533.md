---
ver: rpa2
title: Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games
arxiv_id: '2505.14137'
source_url: https://arxiv.org/abs/2505.14137
tags:
- strategy
- value
- memory
- defender
- patrolling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the open problem of automatically assigning
  memory sizes to different locations in finite-memory strategies for adversarial
  patrolling games. The authors propose a general method that iteratively adjusts
  the memory allocation based on the gradients of the maximal attack values with respect
  to strategy parameters.
---

# Memory Assignment for Finite-Memory Strategies in Adversarial Patrolling Games

## Quick Facts
- arXiv ID: 2505.14137
- Source URL: https://arxiv.org/abs/2505.14137
- Reference count: 40
- Primary result: Automatic memory allocation for finite-memory strategies in adversarial patrolling games

## Executive Summary
This paper addresses the critical challenge of automatically assigning memory sizes to different locations in finite-memory strategies for adversarial patrolling games. The authors propose a gradient-based iterative method that adjusts memory allocation by analyzing the sensitivity of attack values to strategy parameters. Their approach identifies conflicting attack profiles and allocates additional memory to states exhibiting diverse behaviors, enabling more expressive strategies without manual tuning.

The method is evaluated across diverse patrolling models (hard-constrained, blinded, linear targets) and benchmarks (Offices, Airports, Stars, Terrains), consistently outperforming uniform memory assignments and matching expert-crafted degree-based heuristics. The approach achieves robust, scalable performance and integrates seamlessly with existing differentiable strategy optimization tools.

## Method Summary
The authors present a general method for automatic memory assignment that iteratively adjusts memory allocation based on gradients of maximal attack values with respect to strategy parameters. The approach works by identifying attack profiles that conflict and allocating additional memory to states where the strategy exhibits diverse behaviors. This gradient-driven memory allocation enables more expressive strategies without requiring manual tuning of memory sizes across different locations.

The method is designed to work with differentiable strategy optimization frameworks and can be applied across various patrolling game models including hard-constrained, blinded, and linear target scenarios. The iterative nature allows the system to refine memory allocation based on observed performance, with the key insight being that states with more conflicting attack profiles benefit from larger memory allocations to handle complex decision-making requirements.

## Key Results
- Outperforms uniform memory assignments across all tested patrolling models and benchmarks
- Matches performance of expert-crafted degree-based heuristics without requiring manual design
- Demonstrates scalability and robustness across diverse scenarios including Offices, Airports, Stars, and Terrains

## Why This Works (Mechanism)
The method works by leveraging gradient information to identify where additional memory capacity is most beneficial. When attack values are sensitive to strategy parameters at specific states, this indicates conflicting attack profiles that require more complex decision-making. By allocating memory based on these sensitivity gradients, the system can automatically focus computational resources where they provide the greatest benefit for handling adversarial uncertainty.

## Foundational Learning
- **Gradient-based memory allocation**: Uses derivatives of attack values to guide memory assignment decisions
  - Why needed: Provides data-driven approach to memory allocation without manual tuning
  - Quick check: Verify gradient computation correctness on simple test cases

- **Finite-memory strategies**: Strategies that maintain limited state information beyond current location
  - Why needed: Balances expressiveness with computational tractability
  - Quick check: Confirm memory bounds are respected during strategy execution

- **Adversarial patrolling games**: Security domains where defender patrols against intelligent attackers
  - Why needed: Defines the problem context and performance metrics
  - Quick check: Validate game model accurately represents security scenarios

- **Differentiable optimization**: Framework allowing gradient-based learning of strategy parameters
  - Why needed: Enables automatic memory adjustment through gradient descent
  - Quick check: Ensure all components support differentiable computation

## Architecture Onboarding

**Component map:** Game model -> Strategy parameters -> Attack value computation -> Gradient calculation -> Memory allocation adjustment -> Updated strategy

**Critical path:** The gradient computation and memory allocation adjustment loop forms the core of the method, where sensitivity analysis drives iterative improvements to memory assignment.

**Design tradeoffs:** The approach trades computational overhead from gradient computation against the benefit of automated memory tuning. This avoids manual heuristic design but requires differentiable components throughout the pipeline.

**Failure signatures:** Poor performance may indicate incorrect gradient computation, inappropriate learning rates for memory adjustment, or insufficient differentiation support in underlying game components. Memory over-allocation can occur if gradients are noisy or misinterpreted.

**3 first experiments:**
1. Verify gradient computation on simple patrolling scenarios with known optimal memory allocations
2. Test memory adjustment convergence on synthetic attack profiles with controlled conflict levels
3. Compare performance against uniform memory allocation on benchmark patrolling games

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability beyond tested scenarios remains uncertain
- Comparison with state-of-the-art memory allocation strategies from related domains is limited
- Computational complexity scaling and convergence properties require further analysis

## Confidence
- Method generalizability: Medium confidence
- Comparison with current state-of-the-art: Low confidence
- Theoretical foundation and practical implementation: Medium confidence

## Next Checks
1. Test the method on larger-scale patrolling scenarios with more complex topology and increased state spaces to verify scalability claims
2. Compare against alternative state-of-the-art memory allocation strategies from related domains like POMDPs or multi-agent reinforcement learning
3. Conduct ablation studies to quantify the relative importance of the gradient-based memory adjustment versus the underlying differentiable strategy optimization framework