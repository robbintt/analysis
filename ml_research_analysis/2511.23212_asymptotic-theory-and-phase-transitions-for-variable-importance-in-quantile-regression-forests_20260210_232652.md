---
ver: rpa2
title: Asymptotic Theory and Phase Transitions for Variable Importance in Quantile
  Regression Forests
arxiv_id: '2511.23212'
source_url: https://arxiv.org/abs/2511.23212
tags:
- bias
- estimator
- term
- variance
- conditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper develops asymptotic theory for variable importance\
  \ estimation in Quantile Regression Forests (QRF), addressing the challenge of non-smooth\
  \ pinball loss and slow convergence rates. The authors establish the asymptotic\
  \ normality of QRF estimators using Knight's identity to handle the non-differentiable\
  \ loss, then uncover a \"phase transition\" phenomenon governed by the subsampling\
  \ rate \u03B2 (where s \u224D n^\u03B2)."
---

# Asymptotic Theory and Phase Transitions for Variable Importance in Quantile Regression Forests

## Quick Facts
- arXiv ID: 2511.23212
- Source URL: https://arxiv.org/abs/2511.23212
- Authors: Tomoshige Nakamura; Hiroshi Shiraishi
- Reference count: 40
- Primary result: Standard inference fails for QRF variable importance when subsampling rate β ≥ 1/2, converging to deterministic bias instead of zero-mean normal distribution

## Executive Summary
This paper develops asymptotic theory for variable importance estimation in Quantile Regression Forests (QRF), addressing the challenge of non-smooth pinball loss and slow convergence rates. The authors establish the asymptotic normality of QRF estimators using Knight's identity to handle the non-differentiable loss, then uncover a "phase transition" phenomenon governed by the subsampling rate β (where s ≍ n^β). The key finding shows that in the bias-dominated regime (β ≥ 1/2), standard inference breaks down as the estimator converges to a deterministic bias constant rather than a zero-mean normal distribution. The authors derive the explicit analytic form of this asymptotic bias and discuss the theoretical feasibility of restoring valid inference through analytic bias correction.

## Method Summary
The method estimates variable importance in QRF by comparing pinball loss between full and restricted models using cross-fitting. Two QRF models are trained on separate training sets: one using all covariates (full model) and one excluding a feature subset S (restricted model). The variable importance estimator is computed as the difference in empirical pinball loss on an evaluation set. The analysis uses honest forests with subsample size s ≍ n^β, where each tree splits the subsample into structure and estimation sets. The paper establishes asymptotic normality for β < 1/2 and identifies a phase transition at β = 1/2, beyond which standard inference fails due to bias dominance.

## Key Results
- Subsampling rate β induces a phase transition: standard inference holds when β < 1/2 but breaks down when β ≥ 1/2
- In bias-dominated regime (β ≥ 1/2), scaled estimator n^(1-β)(V̂_τ(S) - V_τ(S)) converges to deterministic bias constant C_τ(S)
- Analytic bias correction is theoretically feasible but requires estimating nuisance components (conditional density, variance scaling)
- Standard inference fails for large subsamples typically used in practice for maximum predictive accuracy

## Why This Works (Mechanism)

### Mechanism 1: Subsampling Rate Governs Inference Validity via Bias-Variance Trade-off
The subsampling rate β (where s ≍ n^β) induces a phase transition: standard inference (√n-consistency) holds when β < 1/2 but breaks down when β ≥ 1/2. In the variance-dominated regime (β < 1/2), the nuisance estimation error from QRF decays faster than n^(-1/4), so the remainder term R_n is negligible compared to n^(-1/2). In the bias-dominated regime (β ≥ 1/2), the squared bias of the QRF estimator (O(s/n) = O(n^(β-1))) decays too slowly and dominates, causing the VI estimator to converge to a deterministic bias constant rather than a zero-mean normal distribution.

### Mechanism 2: Knight's Identity Enables Analysis of Non-Smooth Pinball Loss
Knight's identity allows decomposition of the non-differentiable pinball loss difference into a linear term (involving the score function) and an integral remainder, facilitating asymptotic analysis. The identity ρ_τ(u-v) - ρ_τ(u) = -vψ_τ(u) + ∫₀ᵛ (1{u ≤ s} - 1{u ≤ 0}) ds separates the loss change. The linear term handles first-order effects (which vanish due to orthogonality), while the integral term, upon taking expectations and using a second-order Taylor expansion of the CDF, yields a quadratic form in the estimation error, revealing the bias.

### Mechanism 3: Neyman Orthogonality Provides Robustness to Nuisance Estimation Error
The variable importance functional V_τ(S) satisfies Neyman orthogonality with respect to the nuisance parameters (conditional quantile functions), meaning first-order perturbations in the nuisances do not affect V_τ(S). This orthogonality arises from the first-order optimality conditions of the population risk functional R_τ(f) at the true quantile functions. Consequently, the impact of nuisance estimation error q̂ - q on the VI estimator is of second order (quadratic in the error), which is crucial for achieving n^(-1/2)-consistency when the nuisance error is sufficiently small.

## Foundational Learning

- **Pinball Loss and Quantile Regression**
  - Why needed here: This is the fundamental loss function being minimized. Understanding its non-differentiability and the associated score function is essential for grasping why Knight's identity is necessary.
  - Quick check question: What is the pinball loss for τ=0.9 if the prediction error is -5 (overprediction)?

- **Neyman Orthogonality**
  - Why needed here: This property is the theoretical bedrock that makes high-dimensional inference possible by reducing the sensitivity of the target parameter to nuisance estimation errors.
  - Quick check question: If a functional is Neyman orthogonal, what is the order of magnitude of the error induced in it by a nuisance estimator with error ε?

- **Subsampling Rate β in Random Forests**
  - Why needed here: This parameter is the central "tuning knob" in this paper's theory, directly controlling the trade-off between predictive accuracy and inferential validity.
  - Quick check question: For n=10,000, what subsample size s corresponds to β=0.4? In which inference regime does this fall?

## Architecture Onboarding

- **Component map**: Total dataset D → Training Set I_tr (fits QRFs) + Evaluation Set I_ev (computes VI) → Nuisance Estimators (Full Model q̂_τ(x) + Restricted Model q̂_{τ,-S}(x)) → VI Estimator (difference in empirical pinball loss) → Inference Module (apply regime-appropriate asymptotic formula)

- **Critical path**: Inference pipeline requires: (1) proper cross-fitting split, (2) correctly training two QRF models with target β, (3) applying regime-appropriate asymptotic formula.

- **Design tradeoffs**:
  - **Prediction vs. Inference**: β ≈ 1 maximizes accuracy but breaks standard inference, requiring complex bias correction. β < 1/2 sacrifices predictive power for simple, valid inference.
  - **Complexity vs. Robustness**: Bias correction is theoretically sound but requires estimating nuisance components (density, variance scaling), adding complexity and error sources.

- **Failure signatures**:
  - **Invalid Confidence Intervals**: Coverage drops to zero when using standard inference in β > 1/2 regime.
  - **Inconsistent Bias Correction**: Poor density/variance scaling estimators yield Ĉ_τ(S) that doesn't restore asymptotic normality.
  - **Overfitting**: Re-using training data for evaluation without cross-fitting invalidates orthogonality and all inference.

- **First 3 experiments**:
  1. **Sanity Check**: Simulate data with known V_τ(S). For β=0.4 (<1/2), verify 95% CI coverage approaches nominal level using standard CLT.
  2. **Phase Transition Validation**: Systematically vary β from 0.3 to 0.9. Plot empirical coverage of standard CIs—confirm sharp drop-off around β=0.5.
  3. **Bias Correction Pilot**: Implement plug-in estimator for bias constant C_τ(S). In β=0.8 regime, apply correction and check if it restores approximately correct coverage compared to uncorrected estimator.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does the proposed analytic bias correction restore valid inference in finite samples despite potential errors in density estimation?
- **Basis in paper**: Section 5.3 states that "investigating its finite-sample performance and sensitivity to density estimation errors remains an important practical task."
- **Why unresolved**: The paper proves theoretical feasibility but does not verify if the nuisance components (f̂_{Y|X}) can be estimated with sufficient precision in practice to recover asymptotic normality.
- **What evidence would resolve it**: Simulation studies demonstrating that the bias-corrected estimator Ṽ_τ(S) achieves nominal coverage probabilities for subsampling rates β > 1/2.

### Open Question 2
- **Question**: Can a data-adaptive method be developed to optimally select the subsampling rate β to balance the prediction-inference trade-off?
- **Basis in paper**: Section 5.3 identifies "developing data-adaptive methods to automatically select the optimal subsampling rate β" as a "challenging but promising direction."
- **Why unresolved**: The paper establishes that high β maximizes accuracy but breaks inference, whereas low β preserves validity but hurts accuracy.
- **What evidence would resolve it**: An algorithm that dynamically tunes β based on data characteristics to minimize mean squared error while maintaining valid confidence intervals.

### Open Question 3
- **Question**: Does the phase transition phenomenon and bias constant derivation extend to conditional variable importance or Shapley values?
- **Basis in paper**: Section 5.3 notes that "Extending this framework to conditional variable importance or Shapley values in the quantile setting would be a valuable generalization."
- **Why unresolved**: The current theory is restricted to global marginal importance; the aggregation of biases over feature subsets in Shapley values may alter the phase transition behavior.
- **What evidence would resolve it**: Theoretical derivation of the limiting distribution for QRF-based Shapley value estimators, confirming whether the transition at β = 1/2 persists.

## Limitations
- The bias correction approach requires estimating nuisance components (conditional density, variance scaling) that can be challenging in high dimensions
- Practical deviations from theoretical assumptions (e.g., greedy splitting algorithms) may violate the framework's requirements
- The paper provides limited guidance on choosing the critical subsampling rate β in real applications

## Confidence
- **High Confidence**: The phase transition phenomenon itself (β = 1/2 as the critical threshold) and the general mechanism by which large subsamples induce bias-dominated convergence are well-supported by the asymptotic analysis.
- **Medium Confidence**: The explicit bias formula C_τ(S) and its practical estimability through plug-in methods, though finite-sample performance depends on density estimation quality.
- **Low Confidence**: The practical guidance for choosing β in real applications, as the paper shows the theoretical trade-off but doesn't provide concrete recommendations.

## Next Checks
1. **Finite-Sample Phase Transition Verification**: Simulate data across multiple β values (0.3, 0.4, 0.5, 0.6, 0.7, 0.8) and systematically evaluate coverage rates of confidence intervals. Plot the sharp drop in coverage around β = 0.5 to confirm the theoretical phase transition occurs in practice.

2. **Bias Correction Performance**: Implement the plug-in estimator for C_τ(S) with multiple bandwidth selection strategies (cross-validation, rule-of-thumb). Compare coverage rates of corrected vs. uncorrected intervals in the β > 1/2 regime to quantify the practical benefit and sensitivity to tuning choices.

3. **Robustness to Assumption Violations**: Test inference validity when honesty is violated (e.g., using same data for splitting and estimation) or when the conditional density is irregular. This would validate the importance of the stated assumptions and help identify failure modes in real applications.