---
ver: rpa2
title: 'Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060
  Case Study'
arxiv_id: '2509.12229'
source_url: https://arxiv.org/abs/2509.12229
tags:
- energy
- fine-tuning
- tokens
- throughput
- efficiency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic profiling study of LoRA/QLoRA
  fine-tuning on consumer GPUs, using a single RTX 4060 with 8 GB VRAM. Three representative
  configurations were tested across batch size, sequence length, optimizer choice
  (AdamW vs.
---

# Profiling LoRA/QLoRA Fine-Tuning Efficiency on Consumer GPUs: An RTX 4060 Case Study

## Quick Facts
- arXiv ID: 2509.12229
- Source URL: https://arxiv.org/abs/2509.12229
- Authors: MSR Avinash
- Reference count: 7
- Key result: Single 8 GB RTX 4060 can fine-tune 1.5B LLM with LoRA/QLoRA at 500-628 tokens/s using PagedAdamW

## Executive Summary
This paper presents the first systematic profiling study of LoRA/QLoRA fine-tuning on consumer GPUs, using a single RTX 4060 with 8 GB VRAM. Three representative configurations were tested across batch size, sequence length, optimizer choice (AdamW vs. PagedAdamW), and precision (fp16 vs. bf16) using the Qwen2.5-1.5B-Instruct model and a subset of the Alpaca dataset. Results show that PagedAdamW improved throughput by up to 25% (628 tokens/s vs. 500 tokens/s baseline), while bf16 precision degraded efficiency compared to fp16. Long sequence lengths up to 2048 tokens were feasible within the memory constraint. Energy per token was estimated at 0.15 J under 95 W GPU power for the most efficient configuration, rising to 0.32 J for bf16. The study provides reproducible benchmarks and practical guidelines for fine-tuning LLMs on consumer-grade hardware.

## Method Summary
The study used Qwen2.5-1.5B-Instruct model with 4-bit quantization (QLoRA) on an 8 GB RTX 4060. Three runs were profiled: baseline AdamW (batch=1, seq=512, fp16), PagedAdamW stress test (batch=2, seq=2048, fp16), and PagedAdamW bf16 test (batch=2, seq=1024, bf16). Training used 5k Alpaca subset with gradient checkpointing enabled. Throughput measured via Trainer API timing, VRAM via nvidia-smi, and energy estimated from assumed board power (95-115 W) divided by tokens processed.

## Key Results
- PagedAdamW achieved 25% higher throughput (628 tokens/s vs. 500 baseline) by reducing optimizer memory footprint
- bf16 precision degraded efficiency compared to fp16, yielding only 360 tokens/s in test configuration
- 2048 sequence length fine-tuning was feasible within 8 GB VRAM using PagedAdamW
- Energy per token ranged from 0.15 J (best fp16 config) to 0.32 J (bf16 config)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PagedAdamW enables ~25% higher throughput than standard AdamW under VRAM constraints.
- Mechanism: PagedAdamW (8-bit) reduces optimizer state memory footprint via quantization and CPU offloading, allowing larger batch sizes or longer sequences without OOM errors. This increases arithmetic intensity and reduces memory bandwidth stalls.
- Core assumption: The throughput gain stems primarily from reduced memory pressure, not from algorithmic convergence differences (which were not evaluated).
- Evidence anchors:
  - [abstract] "PagedAdamW achieves 25% higher throughput (628 tokens/s vs. 500 baseline)"
  - [section 6.2] "Run 2 achieved the highest throughput at 628 tokens/s, representing a ~25% improvement over the baseline (Run 1, 500 tokens/s)"
  - [corpus] Chinese-Vicuna paper (arXiv:2504.12737) confirms LoRA enables deployment on consumer GPUs, but does not profile PagedAdamW specifically.
- Break condition: If optimizer state is small relative to model activations (e.g., very small models), paging overhead may outweigh benefits.

### Mechanism 2
- Claim: bf16 precision degrades throughput on the RTX 4060 compared to fp16.
- Mechanism: Consumer GPU architectures may have weaker bf16 tensor core throughput or require software emulation, increasing latency per operation. This negates datacenter trends where bf16 is preferred for stability.
- Core assumption: The degradation is hardware-specific to Ada Lovelace consumer variants; architectural differences vs. datacenter GPUs (H100/A100) cause the reversal.
- Evidence anchors:
  - [abstract] "bf16 degrades efficiency compared to fp16"
  - [section 6.2] "Run 3, despite using the same batch size as Run 2, performed significantly worse at only 360 tokens/s due to bf16 overheads"
  - [corpus] No corpus papers directly address bf16 vs. fp16 on consumer GPUs; this remains an underexplored claim.
- Break condition: If future consumer GPU architectures improve native bf16 throughput, the penalty may disappear.

### Mechanism 3
- Claim: Higher throughput correlates with lower energy per token.
- Mechanism: Energy per token is estimated as E_token = (P × t) / N = P / R. Since R is in the denominator, any configuration that increases throughput directly reduces energy per token for fixed power draw.
- Core assumption: Power draw P remains approximately constant across configurations (80–115 W assumed); this was not measured directly due to NVML driver limitations.
- Evidence anchors:
  - [section 6.4] "Energy per token was lowest in Run 2, estimated at 0.151 J/token... Run 3 was far less efficient at 0.26–0.32 J/token"
  - [section 2.4] Formula E_token = E / N
  - [corpus] Patterson et al. (arXiv:2104.10350) show throughput-energy correlation in datacenter training; this paper extends the observation to consumer GPUs.
- Break condition: If power draw varies significantly with configuration (e.g., higher compute intensity draws more power), the linear relationship may weaken.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: Explains why a 1.5B model fits in 8 GB VRAM—only low-rank adapters are trained, not full weights.
  - Quick check question: If LoRA rank increases from 8 to 64, what happens to VRAM and throughput?

- Concept: Paged Optimizers (e.g., PagedAdamW)
  - Why needed here: Understanding CPU-GPU memory offloading explains the 25% throughput gain.
  - Quick check question: What triggers a page-in/page-out event during training, and what is the latency penalty?

- Concept: Mixed Precision (fp16 vs. bf16)
  - Why needed here: Clarifies why bf16, preferred on H100/A100, underperforms on consumer GPUs.
  - Quick check question: What hardware feature determines native bf16 throughput on NVIDIA GPUs?

## Architecture Onboarding

- Component map:
  - GPU: NVIDIA RTX 4060 (8 GB GDDR6, 115 W TDP)
  - Model: Qwen2.5-1.5B-Instruct (1.5B params, 4-bit quantized base via QLoRA)
  - Optimizer: PagedAdamW (8-bit, bitsandbytes) or AdamW (PyTorch)
  - Precision: fp16 or bf16 via Hugging Face Trainer
  - Monitoring: nvidia-smi for VRAM; no direct power telemetry

- Critical path:
  1. Load model in 4-bit (QLoRA) → base weights frozen
  2. Inject LoRA adapters (16-bit)
  3. Initialize PagedAdamW → optimizer states paged to CPU if needed
  4. Forward/backward pass → VRAM peaks during gradient computation
  5. Optimizer step → states paged back, weights updated

- Design tradeoffs:
  - Batch size vs. sequence length: B=2, S=2048 fits at 8.06 GB; B=1, S=512 leaves margin at 6.23 GB
  - fp16 vs. bf16: fp16 yields ~1.7x higher throughput on RTX 4060; bf16 may improve numerical stability but at throughput cost
  - PagedAdamW vs. AdamW: Paging trades marginal latency for ability to run longer sequences

- Failure signatures:
  - CUDA OOM at >8.1 GB → reduce batch size or sequence length, or enable gradient checkpointing
  - Throughput drops sharply with bf16 → switch to fp16
  - Instability in loss curve → check gradient clipping, learning rate; consider bf16 for stability despite throughput cost

- First 3 experiments:
  1. Replicate Run 1 (AdamW, B=1, S=512, fp16) to establish baseline throughput and VRAM.
  2. Switch to PagedAdamW (B=2, S=1024, fp16) and measure delta in tok/s and VRAM peak.
  3. Toggle bf16 with same config as #2 to quantify throughput degradation; log loss curve for stability comparison.

## Open Questions the Paper Calls Out
None

## Limitations
- Single GPU model (RTX 4060) limits generalizability to other hardware architectures
- No direct power measurement; energy estimates rely on assumed board power (95-115 W)
- Training hyperparameters (learning rate, weight decay, LoRA rank) unspecified
- bf16 vs. fp16 findings may be specific to consumer GPU architecture

## Confidence
- **High confidence**: PagedAdamW improves throughput by ~25% under VRAM constraints
- **Medium confidence**: bf16 degrades efficiency vs. fp16 on RTX 4060
- **Medium confidence**: Energy per token correlates inversely with throughput

## Next Checks
1. Replicate PagedAdamW gains across GPU generations: Run the same LoRA/QLoRA configuration on RTX 3060 (Ampere) and RTX 4070 (Ada Lovelace, 12 GB) to test whether the 25% throughput improvement is consistent or specific to the 8 GB constraint.

2. Direct power telemetry validation: Use an external power meter to measure actual GPU board power during the three runs. Compare measured power draw to the assumed 95-115 W range and recalculate energy per token with empirical data.

3. Hyperparameter sensitivity test: Vary LoRA rank (8→32) and learning rate (1e-5→5e-5) while keeping PagedAdamW and fp16 fixed. Determine whether throughput gains persist across a range of adapter complexities and training rates.