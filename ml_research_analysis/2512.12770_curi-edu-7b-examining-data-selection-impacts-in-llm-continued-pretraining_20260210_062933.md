---
ver: rpa2
title: "Curi\xF3-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining"
arxiv_id: '2512.12770'
source_url: https://arxiv.org/abs/2512.12770
tags:
- curi
- data
- pretraining
- language
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates continued pretraining as a strategy for\
  \ adapting large language models to underrepresented languages, focusing on Portuguese.\
  \ By leveraging the ClassiCC-PT corpus, the authors train two 7-billion-parameter\
  \ models derived from LLaMA-2: Curi\xF3 7B on 100 billion uncurated tokens and Curi\xF3\
  -Edu 7B on a 10 billion token subset filtered for educational and STEM content."
---

# Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining

## Quick Facts
- arXiv ID: 2512.12770
- Source URL: https://arxiv.org/abs/2512.12770
- Reference count: 28
- Primary result: Curió-Edu 7B outperforms full-corpus model on Portuguese tasks despite using only 10% of the data

## Executive Summary
This work investigates continued pretraining as a strategy for adapting large language models to underrepresented languages, focusing on Portuguese. By leveraging the ClassiCC-PT corpus, the authors train two 7-billion-parameter models derived from LLaMA-2: Curió 7B on 100 billion uncurated tokens and Curió-Edu 7B on a 10 billion token subset filtered for educational and STEM content. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B outperforms the full-corpus model in evaluations on PoETa V2, a comprehensive benchmark for Portuguese language tasks. The performance advantage is consistent across multiple domains, including reasoning, ethics, and general knowledge, not just the STEM-aligned tasks expected from the filtering. This suggests that high-quality, targeted data can be more effective than larger, uncurated datasets, particularly for models with limited prior exposure to the target language. The benefits are more pronounced at larger model scales, with smaller models showing more mixed results, likely due to representational constraints. The findings highlight the importance of data curation and quality in efficient and effective continued pretraining, demonstrating that carefully selected corpora can yield substantial gains in low- and medium-resource language adaptation.

## Method Summary
The authors continued pretraining LLaMA-2 7B using the ClassiCC-PT corpus, which contains 100 billion Portuguese tokens. They trained two models: Curió 7B on the full corpus and Curió-Edu 7B on a 10 billion token subset filtered for educational and STEM content. Both models were evaluated on PoETa V2, a comprehensive benchmark for Portuguese language tasks. The study compared performance across multiple domains, including reasoning, ethics, and general knowledge, to assess the impact of data selection on model adaptation to underrepresented languages.

## Key Results
- Curió-Edu 7B outperforms Curió 7B on PoETa V2 despite using only 10% of the data and 20% of the computation
- Performance gains are consistent across multiple domains, not just STEM-aligned tasks
- Benefits are more pronounced at larger model scales, with smaller models showing mixed results

## Why This Works (Mechanism)
The superior performance of Curió-Edu 7B can be attributed to the quality and relevance of the filtered data. By focusing on educational and STEM content, the model receives more structured and domain-specific training signals that enhance its ability to handle complex reasoning tasks. This targeted approach likely compensates for the model's limited prior exposure to Portuguese, allowing it to develop more robust representations for the language. The consistent performance gains across multiple domains suggest that the curated data provides a strong foundation for general language understanding, not just specialized knowledge.

## Foundational Learning
- **Continued Pretraining**: Fine-tuning a pre-trained model on domain-specific data to improve performance on target tasks
  - Why needed: Enables adaptation to underrepresented languages and specialized domains
  - Quick check: Compare performance of continued pretraining vs. training from scratch on target domain
- **Data Curation**: Selecting high-quality, relevant data for training to improve model performance
  - Why needed: Reduces noise and enhances learning efficiency
  - Quick check: Measure performance gains from different levels of data filtering
- **Low-Resource Language Adaptation**: Strategies for adapting models to languages with limited available data
  - Why needed: Addresses the challenge of underrepresented languages in NLP
  - Quick check: Evaluate performance on multiple low-resource languages

## Architecture Onboarding

**Component Map**: LLaMA-2 7B -> Continued Pretraining -> ClassiCC-PT Corpus -> Curió 7B/Curió-Edu 7B

**Critical Path**: The critical path is the continued pretraining process, where the quality and relevance of the training data directly impact the model's performance on downstream tasks.

**Design Tradeoffs**: The main tradeoff is between data quantity and quality. While using more data provides broader coverage, filtering for high-quality, relevant content can yield better performance with less data, as demonstrated by Curió-Edu 7B.

**Failure Signatures**: Potential failure modes include overfitting to the filtered data, resulting in poor generalization to other domains, or insufficient coverage of the language, leading to poor performance on tasks outside the filtered scope.

**3 First Experiments**:
1. Evaluate Curió-Edu 7B on additional Portuguese benchmarks to assess generalization
2. Test the impact of different filtering strategies on model performance
3. Compare continued pretraining with other adaptation methods, such as adapter-based approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on a single language pair (Portuguese/English) and corpus limits generalizability
- Does not explore alternative filtering strategies or long-term stability of performance gains
- Comparison limited to two model variants, leaving questions about optimal data quantity-quality trade-off

## Confidence

**High**: The claim that targeted, high-quality data can outperform larger, uncurated datasets is supported by consistent performance improvements across multiple domains in the PoETa V2 benchmark.

**Medium**: The broader claims about data curation's role in low- and medium-resource language adaptation are supported by the results, but the narrow scope of languages and datasets tested limits generalizability.

**Low**: The assertion that benefits are more pronounced at larger model scales is based on limited evidence from a small set of model sizes and may not capture the full spectrum of possible behaviors.

## Next Checks
1. Replicate the experiment using a different low-resource language and corpus to assess generalizability.
2. Test additional filtering strategies (e.g., domain-specific, quality-based) to determine the most effective data selection approach.
3. Conduct ablation studies across a wider range of model scales to more precisely characterize the relationship between model size and the benefits of data curation.