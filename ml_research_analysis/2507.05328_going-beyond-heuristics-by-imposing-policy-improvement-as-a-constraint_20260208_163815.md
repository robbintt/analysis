---
ver: rpa2
title: Going Beyond Heuristics by Imposing Policy Improvement as a Constraint
arxiv_id: '2507.05328'
source_url: https://arxiv.org/abs/2507.05328
tags:
- torch
- reward
- hand
- successes
- dist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new approach for leveraging heuristic rewards
  in reinforcement learning. The core idea is to train a policy to maximize both task
  and heuristic rewards, but subject to a constraint that the task performance must
  be at least as good as a policy trained with heuristics alone.
---

# Going Beyond Heuristics by Imposing Policy Improvement as a Constraint

## Quick Facts
- **arXiv ID:** 2507.05328
- **Source URL:** https://arxiv.org/abs/2507.05328
- **Reference count:** 40
- **Primary result:** HEPO leverages heuristic rewards while guaranteeing task performance is at least as good as a policy trained with heuristics alone.

## Executive Summary
This paper proposes a new approach for leveraging heuristic rewards in reinforcement learning. The core idea is to train a policy to maximize both task and heuristic rewards, but subject to a constraint that the task performance must be at least as good as a policy trained with heuristics alone. This differs from existing approaches that either ignore task rewards or rely on policy invariance, which doesn't guarantee policy improvement. The method, called Heuristic Enhanced Policy Optimization (HEPO), uses a Lagrangian formulation with two policies trained concurrently. Experiments show HEPO outperforms strong baselines on standard benchmarks with well-engineered rewards, and surprisingly, can also leverage poorly-designed heuristics from non-expert humans to achieve expert-level performance. This demonstrates HEPO's effectiveness at mitigating reward hacking and reducing human effort in reward design.

## Method Summary
HEPO addresses the challenge of incorporating heuristic rewards into reinforcement learning while ensuring task performance improvement. The method trains two policies concurrently: an enhanced policy $\pi$ and a heuristic policy $\pi_H$. The enhanced policy optimizes a combined objective that includes both task rewards and heuristic rewards, subject to a constraint that its task performance must exceed that of the heuristic policy. This constraint is enforced through a Lagrangian multiplier $\alpha$ that dynamically adjusts the balance between task and heuristic rewards during training. Data collection alternates between the two policies, and importance sampling is used to share data between them, improving sample efficiency.

## Key Results
- HEPO outperforms strong baselines on standard benchmarks with well-engineered rewards
- Can leverage poorly-designed heuristics from non-expert humans to achieve expert-level performance
- Effectively mitigates reward hacking while reducing human effort in reward design
- Demonstrates robust performance even with highly correlated but imperfect heuristic rewards

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Task-Reward Weighting via Lagrangian Constraint
Imposing a policy improvement constraint ($J(\pi) \geq J(\pi_H)$) prevents reward hacking while allowing the policy to exploit heuristic signals when safe. The method introduces a Lagrangian multiplier $\alpha$. The gradient of $\alpha$ is proportional to the performance difference $J(\pi) - J(\pi_H)$. If the enhanced policy $\pi$ outperforms the heuristic baseline $\pi_H$, $\alpha$ decreases, reducing the pressure on the task reward and allowing the agent to prioritize the heuristic reward $H$. If $\pi$ underperforms, $\alpha$ increases, forcing the agent to refocus on the task reward $J$.

### Mechanism 2: Dual-Policy Data Sharing (Importance Sampling)
Training the enhanced policy $\pi$ and the heuristic policy $\pi_H$ concurrently with shared data improves sample efficiency and stabilizes the off-policy gradients. Instead of pre-training $\pi_H$ or collecting data separately, HEPO collects half the batch with $\pi$ and half with $\pi_H$. It uses importance sampling ratios (off-policy correction) to compute advantages for the *other* policy. This allows the gradient of $\alpha$ (which requires comparisons between $\pi$ and $\pi_H$) to be estimated from the same batch of data.

### Mechanism 3: Median-Based Gradient Smoothing
Smoothing the gradient updates for the Lagrangian multiplier $\alpha$ prevents explosive updates caused by high variance in advantage estimates. The implementation calculates the median of the previous $K$ gradient records (where $K=8$ in experiments) before updating $\alpha$ using the Adam optimizer.

## Foundational Learning

- **Concept: Policy Invariance vs. Policy Improvement**
  - **Why needed here:** The paper frames HEPO as a rejection of "Policy Invariance" (guaranteeing the optimal policy is unchanged) in favor of "Policy Improvement" (guaranteeing the learned policy is better than the heuristic baseline). You must understand that invariance methods (like PBRS) theoretically preserve the optimal policy but often fail practically because they don't strictly enforce *improvement* over a heuristic guide.
  - **Quick check question:** Why does potential-based reward shaping (PBRS) fail to improve performance if the heuristics are poorly scaled?

- **Concept: Lagrangian Duality in Constrained RL**
  - **Why needed here:** The core of HEPO is converting a hard constraint ($J(\pi) \geq J(\pi_H)$) into a soft penalty term controlled by $\alpha$.
  - **Quick check question:** In the HEPO loss function $\mathcal{L}(\pi, \alpha)$, what does the value of $\alpha$ approaching infinity imply about the constraint?

- **Concept: Generalized Advantage Estimation (GAE)**
  - **Why needed here:** The paper uses GAE not just for policy updates, but specifically to estimate the value difference $J(\pi) - J(\pi_H)$ for the $\alpha$ gradient.
  - **Quick check question:** How does the variance of GAE estimates affect the stability of the Lagrangian multiplier $\alpha$?

## Architecture Onboarding

- **Component map:** $\pi$ (Enhanced Policy) -> $\pi_H$ (Heuristic Policy) -> Shared Data Buffer -> Value Heads (4 total) -> $\alpha$ (Lagrangian Multiplier)

- **Critical path:**
  1.  **Rollout:** Collect $B/2$ steps with $\pi$, $B/2$ with $\pi_H$.
  2.  **Advantage Computation:** Compute GAE for Task ($A_r$) and Heuristic ($A_h$) on *all* data for *both* policies (using importance sampling corrections).
  3.  **Alpha Update:** Calculate expected advantages, apply median smoothing, and update $\alpha$ via SGD (minimize $\alpha \cdot (J(\pi) - J(\pi_H))$).
  4.  **Policy Update:** Update $\pi$ and $\pi_H$ using PPO clipped objectives with modified rewards ($\pi$ uses $(1+\alpha)r + h$; $\pi_H$ uses $h$).

- **Design tradeoffs:**
  - **Memory:** Requires storing two sets of policy/value parameters and optimizer states.
  - **Off-policy Bias:** Using importance sampling to share data introduces bias if policies diverge significantly; the paper mitigates this via PPO clipping but does not eliminate it.
  - **Initialization:** The paper initializes $\alpha = 0$, implying the agent starts by optimizing pure heuristics until the constraint is violated.

- **Failure signatures:**
  - **Alpha Explosion:** $\alpha \to \infty$, indicating the enhanced policy consistently fails to beat the heuristic policy (check implementation of advantage estimates).
  - **Stagnation:** $\alpha = 0$ but $J(\pi) < J(\pi_H)$, indicating the gradient estimate for $\alpha$ is broken or smoothed too aggressively.
  - **Performance Collapse:** $\pi_H$ learns a deceptive local optimum rapidly, causing $\pi$ to converge to the same sub-optimal behavior.

- **First 3 experiments:**
  1.  **Sanity Check (Joint vs. Separate):** Implement the trajectory collection strategy. Train HEPO on a simple task (e.g., Ant) with "Joint" collection (shared batch) vs. "Alternating" collection (separate batches). Verify that Joint matches or exceeds the paper's curves (Fig 3b).
  2.  **Alpha Dynamics Visualization:** Plot $\alpha$, $J(\pi)$, and $J(\pi_H)$ over training steps on a task with known reward hacking issues (e.g., FrankaCabinet). Check if $\alpha$ increases when the gap narrows.
  3.  **Robustness to Bad Heuristics:** Manually construct a "bad" heuristic (e.g., penalizing progress). Train HEPO and verify that while $J(\pi_H)$ is poor, $\pi$ manages to maintain $J(\pi) \geq J(\pi_H)$ without collapsing completely.

## Open Questions the Paper Calls Out

- **Can HEPO be reformulated to require only a single policy network to reduce memory overhead for large-scale applications like Large Language Models (LLMs)?**
  - The authors state in Section 6 that "the requirement to train two policies in HEPO may hinder its adoption for large language models (LLMs) due to high memory demands."
  - The current implementation necessitates storing and optimizing two distinct policies concurrently ($\pi$ and $\pi_H$), doubling the memory footprint compared to standard RL.
  - A theoretical derivation of a single-policy objective that maintains the constraint $J(\pi) \ge J(\pi_H)$ without explicitly maintaining $\pi_H$ in memory, validated by memory benchmarks.

- **Can theoretical guarantees of convergence to the optimal policy be established for HEPO by integrating recent advances in reward engineering?**
  - Section 6 identifies the lack of convergence guarantees as a limitation and suggests "incorporating the insight in recent theoretical advances on reward engineering... to make a convergence guarantee" as future work.
  - The paper currently provides empirical validation of policy improvement but lacks formal proof that the constrained optimization process converges to the global optimum of the task objective.
  - A formal proof or theoretical bound demonstrating that HEPO converges to the optimal policy under specific conditions regarding the quality of the heuristic reward.

- **What is the minimum performance threshold or alignment quality a heuristic reward must possess for HEPO to successfully bootstrap learning?**
  - While the paper claims HEPO handles "ill-designed" heuristics, Figure 7c shows a high correlation (0.9) between the performance of the heuristic-only policy and HEPO. Furthermore, Figure 2 shows HEPO fails to learn on heuristics H9 and H12 (which had incorrect signs), suggesting a limit to HEPO's robustness.
  - It is unclear if HEPO can extract signal from arbitrarily bad heuristics or if there is a specific "lower bound" of quality (e.g., $J(\pi_H) > J_{random}$) required for the constraint to facilitate rather than hinder learning.
  - An ablation study sweeping the noise levels or misalignment of heuristic rewards to identify the specific failure point where HEPOâ€™s performance drops below that of a policy trained on task rewards alone.

## Limitations

- The reliance on dual-policy training introduces significant memory overhead, potentially limiting scalability to large models
- The method requires computing task performance of the heuristic policy, which may be computationally expensive or infeasible in some settings
- The effectiveness depends on importance sampling corrections remaining valid, which may break down if policies diverge significantly

## Confidence

- **High Confidence:** The core mechanism of using a Lagrangian constraint to enforce policy improvement over heuristic baselines is well-established and the experimental results showing HEPO's effectiveness on standard benchmarks are convincing.
- **Medium Confidence:** The claim that HEPO can leverage poorly-designed heuristics from non-experts to achieve expert-level performance is supported by results but requires further validation across more diverse environments and heuristic types.
- **Low Confidence:** The stability of the dual-policy training approach under significant policy divergence is not thoroughly examined, and the paper's discussion of computational overhead is limited.

## Next Checks

1. **Policy Divergence Test:** Implement a variant where the heuristic policy $\pi_H$ is intentionally trained to diverge rapidly from $\pi$ (e.g., by adding noise to its policy updates). Measure how this affects importance sampling variance and the stability of $\alpha$ updates.

2. **Computational Overhead Benchmark:** Measure and report the wall-clock time and memory usage of HEPO compared to standard PPO and PBRS across multiple environments, accounting for the dual-policy setup and additional value heads.

3. **Heuristic Robustness Analysis:** Systematically vary the quality and type of heuristic rewards (e.g., random, sparse, misleading) across a suite of environments. Quantify the threshold at which HEPO fails to maintain the performance constraint and compare this to PBRS.