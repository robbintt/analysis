---
ver: rpa2
title: 'CLIP-DQA: Blindly Evaluating Dehazed Images from Global and Local Perspectives
  Using CLIP'
arxiv_id: '2502.01707'
source_url: https://arxiv.org/abs/2502.01707
tags:
- image
- quality
- dehazed
- images
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLIP-DQA, a novel blind dehazed image quality
  assessment method that leverages Contrastive Language-Image Pre-Training (CLIP)
  for evaluating dehazed images without reference information. The key innovation
  lies in adapting CLIP to BDQA by incorporating both global and local perspectives
  of dehazed images, and employing multi-modal prompt tuning to fine-tune CLIP for
  accurate quality prediction.
---

# CLIP-DQA: Blindly Evaluating Dehazed Images from Global and Local Perspectives Using CLIP

## Quick Facts
- arXiv ID: 2502.01707
- Source URL: https://arxiv.org/abs/2502.01707
- Reference count: 40
- Primary result: Achieves state-of-the-art blind dehazed image quality assessment on DHQ and exBeDDE datasets using CLIP with global-local feature fusion and multi-modal prompt tuning

## Executive Summary
CLIP-DQA introduces a novel blind dehazed image quality assessment method that leverages Contrastive Language-Image Pre-Training (CLIP) to evaluate dehazed images without reference information. The method innovatively adapts CLIP by incorporating both global and local perspectives of dehazed images and employs multi-modal prompt tuning to fine-tune CLIP for accurate quality prediction. Experimental results on two authentic DQA datasets demonstrate superior performance over existing blind quality assessment methods, with significant improvements in SRCC, PLCC, and KRCC metrics.

## Method Summary
CLIP-DQA uses a frozen CLIP ViT-B/32 backbone with learnable prompts inserted at each transformer layer in both vision and language streams. The method processes both resized global images and cropped local patches through separate token streams, aggregating quality scores via averaging. Learnable prompts in both branches adapt CLIP to the dehazing domain without modifying pre-trained weights, and quality prediction is formulated as soft classification between learned "quality" and "distortion" text embeddings using cosine similarity distribution.

## Key Results
- Achieves SRCC of 0.9179 on DHQ dataset, outperforming existing blind methods
- Shows significant improvements across SRCC, PLCC, and KRCC metrics on both DHQ and exBeDDE datasets
- Ablation studies demonstrate each component contribution: handcrafted prompts (0.3630 SRCC) → textual prompts (0.8392 SRCC) → visual prompts (0.8925 SRCC) → full method (0.9179 SRCC)
- Attention visualizations show model attends to hazy regions in patches and spreads attention globally in resized images

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Global-Local Feature Fusion
Combining resized global images with cropped local patches enables comprehensive quality assessment. The system processes two complementary inputs simultaneously—resized images preserve global structural information while cropped patches retain local detail fidelity. Both streams flow through shared transformer layers with separate token sets, allowing the model to attend to both haze distribution patterns (global) and texture/detail recovery (local). This mechanism assumes dehazing quality depends on both global haze removal completeness and local detail preservation, which human perception evaluates hierarchically.

### Mechanism 2: Multi-Modal Prompt Tuning for Domain Adaptation
Learnable prompts in both vision and language branches bridge the domain gap between natural images and dehazed images while avoiding catastrophic forgetting. Rather than fine-tuning CLIP weights directly, the method inserts learnable prompt tokens at each transformer layer in both branches. This approach uses input-space perturbations rather than weight-space modifications, assuming the semantic gap can be bridged through prompts since CLIP's pre-trained representations contain transferable quality-relevant features.

### Mechanism 3: Quality Score Regression via Similarity Distribution
Framing quality assessment as soft classification between learned "quality" and "distortion" text embeddings produces differentiable quality scores. The quality prediction computes a bounded [0,1] score from cosine similarities between visual embeddings and learned positive/negative text embeddings. MSE loss then regresses these scores against MOS labels, allowing end-to-end gradient flow through both branches. This mechanism assumes quality can be decomposed into a binary semantic opposition with continuous interpolation.

## Foundational Learning

- **Vision Transformer (ViT) Token Mechanics**: Understanding how patch embedding, class tokens, and multi-token streams interact is essential for debugging prompt insertion and attention analysis. Quick check: Can you explain why class token is preserved across layers while prompt outputs are discarded after each transformer layer?

- **Prompt Learning vs. Fine-Tuning Trade-offs**: The method explicitly keeps CLIP frozen and only trains prompts; understanding this design choice (parameter efficiency, avoiding catastrophic forgetting, limited DQA dataset size) motivates the architecture. Quick check: What would happen to performance if you fine-tuned all CLIP weights on the 1,750-image DHQ dataset instead of using prompts?

- **Blind/No-Reference Quality Assessment Paradigm**: BDQA operates without reference images, unlike FR/RDQA methods; this constraint shapes the entire feature design. Quick check: Why does Table I show FR methods (PSNR, SSIM) producing no results on DHQ? What does this imply about real-world dehazing evaluation?

## Architecture Onboarding

- **Component map**: Input Image I -> Resize -> I_s (global) -> Vision Encoder (ViT-B/32, frozen) + Visual Prompts -> [c_i, E_l^i, E_g^i] tokens -> Text Prompts -> Language Encoder (frozen) + Learnable {P_i} -> [t_p, t_n] embeddings -> Similarity Computation -> Quality Score Q̂

- **Critical path**: Data preparation (resize + crop N patches) -> Forward pass through frozen ViT with visual prompts -> Text encoding with learnable prompts -> Score computation via cosine similarity -> Loss and backprop through prompts only

- **Design tradeoffs**: Frozen CLIP preserves generalization from 400M image-text pairs but limits adaptation capacity; patch count N affects coverage vs. compute; prompt length 8 balances capacity vs. overfitting risk; shared prompts for global/local tokens vs. branch-specific alternatives

- **Failure signatures**: Low SRCC on new dehazing algorithms indicates prompts haven't generalized to artifact space; attention collapse shows prompt learning failure; score saturation indicates similarity computation collapse; text prompt degradation loses interpretability

- **First 3 experiments**: 1) Reproduce ablation trajectory M1→M2→M3→Full on DHQ to verify SRCC progression matches 0.36→0.84→0.89→0.92, 2) Replicate Figure 2 attention visualizations to confirm global attention spreads broadly while patch attention focuses on hazy regions, 3) Train on DHQ, test on exBeDDE without retraining to reveal true generalization vs. dataset-specific overfitting

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several critical uncertainties remain regarding performance scaling with larger backbones, content-aware pooling mechanisms, textual prompt initialization effects, and performance on synthetic versus authentic dehazing datasets.

## Limitations
- Only evaluated on two dehazing-specific datasets (DHQ and exBeDDE), raising generalizability concerns to other degradation types
- Patch sampling strategy remains underspecified (patch size, sampling method, value of N not defined)
- Quantitative superiority claims rely entirely on within-dataset evaluation without cross-dataset validation
- Limited comparison to recent vision transformer-based methods reduces certainty about real-world superiority

## Confidence
- **High Confidence**: Multi-modal prompt tuning mechanism is well-supported by ablation studies and aligns with established prompt learning principles
- **Medium Confidence**: State-of-the-art performance claims are credible based on reported metrics but limited by dataset diversity and lack of comparison to recent methods
- **Low Confidence**: Quality score formulation using similarity distribution assumes binary semantic opposition that may not capture full complexity of dehazing quality

## Next Checks
1. **Cross-dataset generalization test**: Train CLIP-DQA on DHQ, then evaluate on exBeDDE (and vice versa) without fine-tuning to verify whether learned prompts generalize beyond training distribution
2. **Ablation of patch sampling strategy**: Systematically vary number of patches N (e.g., 4, 8, 16) and patch sizes to determine optimal configuration and assess sensitivity to critical hyperparameter
3. **Robustness to dehazing method diversity**: Evaluate performance when test sets include dehazing algorithms not represented in training (e.g., train on DCP-based, test on diffusion-based) to validate cross-algorithm generalization claims