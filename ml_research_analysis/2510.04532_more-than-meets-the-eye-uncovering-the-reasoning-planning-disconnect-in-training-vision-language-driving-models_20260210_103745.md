---
ver: rpa2
title: More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training
  Vision-Language Driving Models
arxiv_id: '2510.04532'
source_url: https://arxiv.org/abs/2510.04532
tags:
- reasoning
- planning
- driving
- priors
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether planning in vision-language model
  (VLM) driving agents is causally driven by generated Chain-of-Thought (CoT) reasoning.
  To enable rigorous causal analysis, the authors create DriveMind, a nuPlan-based
  dataset with plan-aligned CoT reasoning and modular information structure separating
  priors from to-be-reasoned signals.
---

# More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models

## Quick Facts
- arXiv ID: 2510.04532
- Source URL: https://arxiv.org/abs/2510.04532
- Reference count: 29
- Primary result: Vision-language driving models show reasoning-planning disconnect, relying on textual priors rather than generated Chain-of-Thought reasoning for planning decisions

## Executive Summary
This study investigates whether planning in vision-language model (VLM) driving agents is causally driven by generated Chain-of-Thought (CoT) reasoning. The authors create DriveMind, a nuPlan-based dataset with plan-aligned CoT reasoning and modular information structure separating priors from to-be-reasoned signals. Through controlled experiments removing either priors or CoT from inputs, they demonstrate that planning performance heavily depends on textual priors while being minimally affected by CoT removal. Attention analysis reveals that planning focuses primarily on priors rather than CoT reasoning. This supports the Reasoning-Planning Decoupling Hypothesis that planning relies on textual priors as shortcuts rather than being causally driven by reasoning. The work provides both a new dataset for causal analysis and a diagnostic tool to evaluate the causal fidelity of future VLM driving models.

## Method Summary
The authors create DriveMind, a nuPlan-based dataset with plan-aligned CoT reasoning and modular information structure that separates priors from to-be-reasoned signals. They train representative VLM agents using supervised fine-tuning and reinforcement learning (GRPO) on this dataset. The key experimental approach involves controlled perturbations where either ego/navigation priors or CoT reasoning are removed from inputs to test their causal impact on planning performance. They evaluate agents using nuPlan metrics and perform attention analysis to understand where the model focuses during planning. Additionally, they introduce a training-free causal probe that measures planning robustness against minor input perturbations, where disproportionate performance degradation indicates heavy reliance on shortcuts.

## Key Results
- Removing ego/navigation priors causes large drops in planning performance, while removing CoT produces only minor changes
- Attention analysis shows planning focuses primarily on priors rather than CoT reasoning
- The causal probe reveals that models relying heavily on textual priors show disproportionate performance degradation when inputs are perturbed
- Results support the Reasoning-Planning Decoupling Hypothesis that planning relies on textual priors as shortcuts

## Why This Works (Mechanism)
The mechanism behind the reasoning-planning disconnect appears to be shortcut learning, where VLM driving agents learn to rely on easily accessible textual priors rather than performing the computationally expensive reasoning steps suggested by their Chain-of-Thought outputs. During training, the model discovers that planning performance can be achieved more efficiently by attending to priors (such as ego vehicle state and navigation targets) rather than processing the full reasoning chain. This creates a decoupling where the CoT serves more as a post-hoc explanation or training artifact rather than a genuine causal driver of planning decisions. The attention patterns confirm this mechanism, showing the model's focus on priors during planning while largely ignoring the CoT content.

## Foundational Learning
1. **Chain-of-Thought (CoT) Reasoning**
   - Why needed: Enables step-by-step reasoning for complex decision-making tasks
   - Quick check: Can the model generate coherent intermediate reasoning steps before producing final outputs?

2. **Vision-Language Models (VLMs)**
   - Why needed: Integrate visual perception with language understanding for embodied AI tasks
   - Quick check: Does the model process both image and text inputs effectively for driving scenarios?

3. **Causal Inference in Machine Learning**
   - Why needed: Distinguishes correlation from causation in model behavior analysis
   - Quick check: Can controlled perturbations reveal true causal relationships between model components?

4. **Attention Mechanisms**
   - Why needed: Reveal which input components the model focuses on during processing
   - Quick check: Do attention weights align with expected important features for the task?

5. **Reinforcement Learning (GRPO)**
   - Why needed: Optimizes agent behavior through trial-and-error interaction with environment
   - Quick check: Does the model improve planning performance through policy gradient updates?

6. **Dataset Curation for Causal Analysis**
   - Why needed: Enables controlled experiments to test causal hypotheses
   - Quick check: Can the dataset structure support systematic ablation of different input components?

## Architecture Onboarding

Component Map:
Vision Encoder -> Text Encoder -> CoT Generator -> Planning Module -> Action Output

Critical Path:
Visual Input + Textual Priors → CoT Generation → Planning Decision → Action Output

Design Tradeoffs:
- Efficiency vs. Explainability: CoT generation adds computational overhead but provides interpretability
- Shortcut Learning vs. Genuine Reasoning: Models may learn to bypass reasoning for faster but potentially less robust decisions
- Training Complexity: SFT provides stable learning while RL offers better reward optimization but higher variance

Failure Signatures:
- High performance when priors are present but dramatic drops when removed
- Attention focus on priors rather than reasoning chains
- Causal probe showing disproportionate sensitivity to input perturbations

First Experiments:
1. Ablation test: Remove CoT vs. remove priors and measure performance delta
2. Attention visualization: Map attention weights during planning to identify key focus areas
3. Causal probe validation: Apply controlled perturbations and measure robustness degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Causal analysis relies on controlled dataset perturbations rather than true randomized experiments
- Dataset construction through selective masking may introduce artifacts not present in real-world scenarios
- Attention analysis provides correlational rather than causal evidence about model decision-making
- Results may not generalize to more complex driving scenarios beyond the controlled nuPlan environment

## Confidence
- High confidence: Experimental methodology using controlled perturbations is sound and results showing performance degradation when removing priors are robust
- Medium confidence: Conclusion that planning relies on textual priors as shortcuts is well-supported but needs validation on more diverse datasets
- Medium confidence: Attention analysis supports findings but has inherent limitations of correlational analyses

## Next Checks
1. Test the causal probe on additional VLM driving models with different architectures and objectives to verify generalizability of shortcut learning phenomenon
2. Conduct ablation studies using original nuPlan data without controlled masking to assess whether disconnect persists in more naturalistic settings
3. Implement and evaluate interventional experiments where reasoning generation is temporally or causally separated from planning decisions to more directly test causal relationship between components