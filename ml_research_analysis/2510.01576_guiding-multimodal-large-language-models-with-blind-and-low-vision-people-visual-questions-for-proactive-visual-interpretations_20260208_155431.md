---
ver: rpa2
title: Guiding Multimodal Large Language Models with Blind and Low Vision People Visual
  Questions for Proactive Visual Interpretations
arxiv_id: '2510.01576'
source_url: https://arxiv.org/abs/2510.01576
tags:
- visual
- users
- questions
- user
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a system that uses historical questions from
  Blind and Low Vision (BLV) users to guide Multimodal Large Language Models (MLLMs)
  in generating more contextually relevant visual descriptions. The system retrieves
  semantically similar past visual contexts from the VizWiz-LF dataset and uses the
  associated user questions to inform the MLLM's descriptions.
---

# Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations

## Quick Facts
- arXiv ID: 2510.01576
- Source URL: https://arxiv.org/abs/2510.01576
- Reference count: 20
- Primary result: Context-aware descriptions achieved 76.1% anticipation rate and were preferred 54.4% of the time over context-free descriptions

## Executive Summary
This paper introduces a system that guides Multimodal Large Language Models (MLLMs) to generate more relevant visual descriptions for Blind and Low Vision (BLV) users by leveraging historical user questions. The system retrieves semantically similar past visual contexts from the VizWiz-LF dataset and uses the associated user questions to inform the MLLM's descriptions. In human evaluations comparing 92 context-aware and context-free descriptions, the context-aware approach demonstrated improved accuracy (+13.1%) and successfully anticipated user needs in 15.2% of cases where the baseline failed. The system shows promise for making visual assistance more proactive and personalized for BLV users.

## Method Summary
The system operates by first embedding images from a filtered VizWiz-LF dataset using Cohere Embed v4 multimodal embeddings, then indexing these in ChromaDB with HNSW for efficient retrieval. For each test image, the system retrieves the top-4 most visually similar images and extracts their associated user questions. These questions are injected into a modified Be My AI system prompt alongside the target image and sent to Gemini 2.5 Pro for description generation. The context-aware approach contrasts with a context-free baseline that only provides the system prompt, target image, and task instruction. Human evaluators then assess whether descriptions answer the user's actual question, anticipate unasked questions, and which approach is preferred overall.

## Key Results
- Context-aware descriptions achieved 76.1% anticipation rate, answering obfuscated user questions without explicit prompts
- Context-aware descriptions were preferred by human evaluators in 54.4% of comparisons
- The system demonstrated 13.1% higher accuracy in answering user questions compared to context-free descriptions
- Context-aware descriptions successfully anticipated and answered the obfuscated user's question in 15.2% of cases where context-free descriptions failed

## Why This Works (Mechanism)

### Mechanism 1: Semantic Retrieval as Context Signal
Retrieving visually similar images and their associated user questions provides implicit guidance about what information BLV users typically seek in comparable scenarios. Multimodal embeddings encode images into a shared semantic space, and cosine similarity over HNSW indexing retrieves the top-4 nearest neighbors. The questions attached to these similar images are extracted and injected into the MLLM prompt, assuming users encountering similar visual contexts will have similar informational needs (e.g., food products → nutritional info, expiration dates).

### Mechanism 2: Question Injection as Attention Steering
Explicitly listing retrieved questions in the prompt biases the MLLM's attention toward specific details without hard-coding rules. The context-aware prompt appends retrieved questions with instructions to use them as a guide for what kind of information is important to users. The MLLM conditions its generation on both the visual input and the textual question signals, prioritizing relevant details like cooking instructions or text transcription.

### Mechanism 3: Role Definition via System Prompt Constraints
A task-specific system prompt (derived from Be My AI) constrains output style and establishes error-handling behaviors appropriate for BLV users. The system prompt defines role, formatting (no markdown, verbatim transcription), and uncertainty protocols (suggest re-capture or volunteer assistance). This reduces output variability and aligns descriptions with accessibility norms, ensuring the model outputs are suitable for screen reader consumption.

## Foundational Learning

- **Vector embeddings and semantic similarity**
  - Why needed here: The core retrieval mechanism depends on embedding images into a shared space and computing similarity; misunderstanding this leads to poor retrieval tuning.
  - Quick check question: Given two product images with different brands but similar packaging, would you expect high or low cosine similarity in a multimodal embedding space?

- **Retrieval-Augmented Generation (RAG) fundamentals**
  - Why needed here: This system is a RAG variant where retrieved context is text (questions) derived from visually similar images, not direct document retrieval.
  - Quick check question: In standard RAG, retrieved text is appended to the prompt. What's the analogous operation in this system?

- **MLLM uncertainty and hallucination modes**
  - Why needed here: The paper shows both conditions can hallucinate (Figure 2: temperature readings); recognizing failure modes is critical for safe deployment.
  - Quick check question: If a retrieved question asks about sodium content but the image shows a product with no visible nutritional label, what should the model output?

## Architecture Onboarding

- **Component map:** Image → Cohere Embed v4 → ChromaDB (HNSW) → Top-4 retrieval → Question extraction → Prompt construction → Gemini 2.5 Pro → Description output

- **Critical path:**
  1. Image → Cohere embedding → ChromaDB query (top-4 neighbors)
  2. Extract questions from 4 retrieved pairs
  3. Construct prompt: system prompt + context-aware instruction + 4 questions + target image
  4. MLLM generates description
  5. Human evaluation: does description answer the held-out reference question?

- **Design tradeoffs:**
  - **Retrieval count (k=4):** More questions provide broader guidance but may introduce noise; paper notes limitation that all retrieved questions are weighted equally.
  - **Context set size (491 entries):** Limited diversity; paper acknowledges need for larger datasets to generalize.
  - **Single-pass generation:** No iterative refinement; errors cannot be self-corrected without user follow-up.

- **Failure signatures:**
  - **Irrelevant retrieval:** Visually similar but semantically unrelated contexts (e.g., same brand, different product line) → wrong question types injected.
  - **Hallucination under uncertainty:** Both conditions hallucinate specifics (Figure 2 temperature example); no calibrated confidence output.
  - **Over-specificity:** Context-aware descriptions sometimes miss broader context that context-free descriptions captured (20.7% preference for context-free in Table 2).

- **First 3 experiments:**
  1. **Retrieval ablation:** Run with k=1, k=2, k=8 to measure sensitivity to context quantity; expect diminishing returns and potential noise increase at higher k.
  2. **Question weighting:** Implement similarity-score weighting in the prompt (e.g., "Question 1 [highly similar]: ...") to signal relevance; compare against uniform treatment.
  3. **Cross-dataset validation:** Evaluate on a held-out dataset not derived from VizWiz (e.g., proprietary BLV interaction logs) to test generalization beyond the training distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does weighting retrieved questions by visual-context similarity improve the precision of contextual cues compared to treating all retrieved questions equally?
- Basis in paper: The authors identify a limitation where "we treat every retrieved question equally" and state they "will explore retrieval strategies that weight images by their visual-context similarity score."
- Why unresolved: The current system signals no distinction in relevance among the retrieved questions, potentially introducing noise that could misguide the MLLM if low-similarity contexts are included.
- What evidence would resolve it: A comparative evaluation measuring description accuracy and user preference when using weighted retrieval scores versus the current unweighted baseline.

### Open Question 2
- Question: Does building personalized context databases from individual user interactions improve accuracy and relevance compared to general historical datasets?
- Basis in paper: The authors state: "Finally, we also anticipate that building personalized context databases from each user's own interactions will further improve accuracy and relevance for personal use cases."
- Why unresolved: The current system relies on a generalized dataset (VizWiz-LF) to infer user needs, which may not capture the specific idiosyncrasies or priorities of individual users.
- What evidence would resolve it: A user study comparing the relevance ratings of descriptions generated from a shared context database versus those generated from a user-specific interaction history.

### Open Question 3
- Question: How well does the retrieval-guided method generalize to unseen visual contexts when utilizing larger datasets distinct from standard foundation model training data?
- Basis in paper: The authors plan to "incorporate larger and more varied datasets—including proprietary or under-studied datasets... to probe how well our method generalizes to unseen visual contexts."
- Why unresolved: The current study used VizWiz-LF, which the authors note may have been encountered by foundation models during training, potentially inflating performance metrics compared to novel data.
- What evidence would resolve it: Evaluation results on proprietary or out-of-distribution datasets that are confirmed to be absent from the foundation model's pre-training corpus.

## Limitations

- Small dataset size and limited diversity (491 context + 92 test images) constrain generalizability to broader BLV contexts and diverse visual scenarios.
- Semantic similarity does not guarantee intent alignment; retrieved questions may be visually relevant but semantically mismatched, leading to noisy guidance.
- Human evaluation subjectivity introduces inter-rater variability in preference judgments (54.4% favoring context-aware) and accuracy assessments.

## Confidence

- **High Confidence:**
  - Context-aware descriptions significantly improve accuracy (+13.1%) over context-free baselines.
  - Context-aware descriptions successfully anticipate user questions in 15.2% of cases where context-free failed.
- **Medium Confidence:**
  - Context-aware descriptions are preferred by human evaluators in 54.4% of comparisons.
  - Anticipation rate (76.1%) indicates strong but not universal success in answering obfuscated user questions.
- **Low Confidence:**
  - Generalization to larger or different BLV datasets (beyond VizWiz-LF) without further validation.
  - Model's ability to ignore conflicting retrieved questions as instructed, without hallucination.

## Next Checks

1. **Cross-Dataset Generalization Test:** Evaluate the system on a held-out BLV interaction dataset not derived from VizWiz (e.g., proprietary logs) to test performance outside the training distribution.

2. **Retrieval Quality Ablation:** Systematically vary k (retrieval count) and implement similarity-score weighting in prompts to quantify the impact of context quantity and relevance on description quality.

3. **Hallucination Detection Audit:** Compare model outputs against ground truth text/images to measure hallucination frequency and severity, particularly for specific details (e.g., temperature, brand names) not visibly present.