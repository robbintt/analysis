---
ver: rpa2
title: Semiparametric Off-Policy Inference for Optimal Policy Values under Possible
  Non-Uniqueness
arxiv_id: '2505.13809'
source_url: https://arxiv.org/abs/2505.13809
tags:
- policy
- optimal
- page
- pp0q
- nsave
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies off-policy inference for optimal policy values
  in Markov decision processes (MDPs), addressing the challenge of non-regularity
  that arises when optimal policies are non-unique or nearly deterministic. While
  existing methods focus on fixed target policies, this work develops new estimators
  and inference procedures for data-dependent optimal policies.
---

# Semiparametric Off-Policy Inference for Optimal Policy Values under Possible Non-Uniqueness

## Quick Facts
- **arXiv ID:** 2505.13809
- **Source URL:** https://arxiv.org/abs/2505.13809
- **Reference count:** 40
- **Primary result:** Develops NSAVE method achieving semiparametric efficiency for optimal policy value inference under non-uniqueness

## Executive Summary
This paper addresses the fundamental challenge of valid inference for optimal policy values in Markov decision processes when optimal policies are non-unique or nearly deterministic. Standard plug-in estimators fail because the optimal policy selection is non-differentiable, violating pathwise differentiability assumptions required for efficient influence functions. The author develops the Nonparametric SequentiAl Value Evaluation (NSAVE) method that achieves semiparametric efficiency and double robustness when optimal policies are unique, while remaining stable under non-regular regimes where existing methods break down.

The paper provides complementary inference approaches: NSAVE for stable inference across both regular and non-regular regimes, and a smoothing-based method using softmax approximation to restore differentiability. A post-selection inference framework is also developed for valid confidence sets when multiple optimal policies exist. Simulation studies demonstrate NSAVE's superior finite-sample calibration and robustness compared to existing methods, particularly in structurally misspecified settings. An application to the OhioT1DM mobile health dataset provides patient-specific confidence intervals for optimal policy values and their improvement over observed treatment policies.

## Method Summary
The paper develops three complementary approaches for off-policy inference of optimal policy values. NSAVE is a sequential variance-weighted estimator that iteratively updates policy estimates and nuisance functions using historical data, constructing martingale structures to achieve stability. The smoothing approach regularizes policy optimization via softmax approximation, restoring differentiability of the value functional. Post-selection inference constructs confidence sets for the collection of optimal policies rather than single values. The methods are evaluated against the SAVE estimator baseline in simulation studies and applied to a mobile health dataset for diabetes treatment optimization.

## Key Results
- NSAVE achieves semiparametric efficiency and retains double robustness when optimal policy is unique
- NSAVE remains stable in degenerate regimes where existing methods break down
- Simulation studies show NSAVE provides sharper finite-sample calibration and greater robustness than SAVE
- Post-selection inference framework provides valid coverage for non-unique optimal policies
- OhioT1DM application demonstrates practical utility with patient-specific confidence intervals

## Why This Works (Mechanism)

### Mechanism 1: Sequential Variance-Weighted Estimation (NSAVE)
NSAVE stabilizes inference by iteratively updating policy estimates and nuisance functions using historical data, treating current step evaluations as martingale difference sequences. The final estimator is an inverse-variance weighted average of sequential updates. This works when nuisance estimators converge at required rates and variance terms are non-zero. Fails if nuisance convergence is too slow or Lindeberg condition is violated.

### Mechanism 2: Softmax Smoothing of the Value Functional
Smoothing the policy selection via softmax approximation restores differentiability, enabling valid first-order inference even for deterministic optimal policies. The smoothing parameter β_N controls bias-variance trade-off: as β_N → ∞, the smoothed policy approaches optimal deterministic one, but finite β_N allows efficient influence function use. Works when β_N satisfies specific growth rate relative to sample size. Fails if β_N grows too fast (bias dominates) or too slow (variance dominates).

### Mechanism 3: Post-Selection Inference for Non-Unique Policies
When optimal policies are non-unique, standard single-value confidence intervals are invalid. The method constructs a "plausible optimal set" containing all policies that could be optimal within high-probability region, then calibrates simultaneous critical value over this set rather than single point. Accounts for "winner's curse" where selection noise artificially inflates estimated policy values. Works when policy class is finite to allow computationally feasible set construction. Fails if overlap between behavior and optimal policies is too small to identify set of winners.

## Foundational Learning

- **Efficient Influence Function (EIF) & Non-regularity**: Core to understanding why standard plug-in estimators fail when optimal policy is non-unique - the functional becomes non-differentiable. Quick check: Why does standard plug-in estimator fail under policy non-uniqueness? (Answer: Functional becomes non-differentiable/pathwise differentiability fails)

- **Double Robustness**: NSAVE claims to retain this property, allowing valid inference if either Q-function OR density ratio is consistently estimated, but not necessarily both. Quick check: If Q-function model is misspecified but density ratio model is perfect, does NSAVE still provide consistent value estimate? (Answer: Yes)

- **Markov Decision Processes (MDP) & Stationarity**: Entire setup relies on stationarity of state distribution to define marginal importance sampling ratios and Bellman flow constraints. Quick check: How does stationarity simplify expression of value function η(π) used in MIS estimator?

## Architecture Onboarding

- **Component map**: Data Input (O_i trajectories) -> Nuisance Estimators (Q, b, ω models) -> Policy Optimizer (generates π) -> Core Estimator (NSAVE sequential loop) -> Inference Engine (calculates variance and constructs CIs)

- **Critical path**: Sequential variance estimation is bottleneck. Variance σ²_{τ(j-1)} is estimated using sliding window of past EIF statistics. If variance estimation is unstable, final weighted average will be biased or have wrong standard errors.

- **Design tradeoffs**: NSAVE vs. Smoothing - NSAVE is theoretically superior for degenerate regimes but computationally heavier; Smoothing is computationally lighter but requires faster nuisance convergence rates. Window Size - larger window increases stability but reduces effective sample size.

- **Failure signatures**: Instability (exploding variance estimates or NaNs in NSAVE loop), Coverage Failure (CIs consistently miss true value due to non-unique optimal policy without PSI correction), SAVE Failure (baseline SAVE failing indicates ill-conditioned feature covariance).

- **First 3 experiments**: 1) Scenario C Reproduction (verify NSAVE significantly outperforms SAVE in log-MSE validating double robustness), 2) Non-Uniqueness Stress Test (measure coverage probability of standard NSAVE CI vs. PSI CI), 3) Nuisance Misspecification Ablation (bias Q-function vs. density ratio to test double robustness).

## Open Questions the Paper Calls Out

### Open Question 1
Can semiparametric efficiency and valid inference for optimal policy values be achieved under non-linear or non-parametric Q-function models? The paper critiques SAVE for assuming low-dimensional linear approximation but doesn't quantify trade-offs when using specific non-linear Q-function approximators.

### Open Question 2
How do proposed methods extend to infinite or continuous action spaces? Current analysis assumes finite action space and no extension to continuous actions is discussed.

### Open Question 3
What are optimal choice rules for smoothing parameter β_N and sequential initial sample size ℓ_N in finite samples? Theoretical guidance provides asymptotic rates but not finite-sample optimal constants or data-dependent selectors.

### Open Question 4
Can methodology be adapted for multi-agent or hierarchical decision-making settings where policies may be interdependent? Current focus is single-agent MDP with no discussion of multi-agent extensions.

## Limitations
- Theoretical results rely heavily on stationarity assumptions and nuisance estimator convergence rates that may be difficult to verify in practice
- NSAVE's sequential variance estimation could be unstable in finite samples when optimal policy is nearly deterministic
- Smoothing approach requires careful tuning of temperature parameter β_N with insufficient practical selection guidance
- Post-selection inference assumes finite policy class which may not hold in high-dimensional action spaces

## Confidence
- NSAVE methodology and double robustness claims: **High**
- Theoretical characterization of non-uniqueness and EIF failure: **High**
- Smoothing approach with softmax regularization: **Medium** (depends heavily on parameter tuning)
- Post-selection inference for non-unique policies: **Medium** (finite policy class assumption)
- Simulation results and empirical validation: **Medium** (simulation scenarios may not fully capture real-world complexity)

## Next Checks
1. Implement stress test for NSAVE under near-deterministic optimal policies to empirically validate stability claims and identify breakdown conditions when variance estimates become unstable.

2. Conduct systematic ablation study varying smoothing parameter β_N across orders of magnitude to identify optimal scaling relationship with sample size and quantify bias-variance trade-off empirically.

3. Extend post-selection inference framework to handle continuous or large discrete action spaces through approximation methods, testing whether finite policy class assumption can be relaxed without compromising coverage guarantees.