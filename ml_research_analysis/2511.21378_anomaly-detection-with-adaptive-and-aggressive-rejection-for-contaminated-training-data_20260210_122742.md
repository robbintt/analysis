---
ver: rpa2
title: Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training
  Data
arxiv_id: '2511.21378'
source_url: https://arxiv.org/abs/2511.21378
tags:
- rejection
- anomaly
- data
- detection
- ratio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of anomaly detection when training
  data is contaminated with anomalies, which violates the typical assumption of clean,
  normal-only training data. The authors propose Adaptive and Aggressive Rejection
  (AAR), a method that dynamically estimates and excludes anomalies using a modified
  z-score for hard rejection and a Gaussian Mixture Model (GMM) for soft rejection.
---

# Anomaly Detection with Adaptive and Aggressive Rejection for Contaminated Training Data

## Quick Facts
- arXiv ID: 2511.21378
- Source URL: https://arxiv.org/abs/2511.21378
- Reference count: 40
- Key outcome: Outperforms SOTA by 0.041 AUROC on contaminated datasets using adaptive rejection strategy

## Executive Summary
This paper addresses the critical challenge of anomaly detection when training data contains anomalies, violating the standard assumption of clean normal-only data. The authors propose Adaptive and Aggressive Rejection (AAR), which dynamically excludes contaminated samples during training using a hybrid hard-and-soft rejection strategy. The method adapts thresholds throughout training to balance robustness against contamination with stability of normal data preservation. Extensive experiments demonstrate significant performance improvements over state-of-the-art methods across both image and tabular datasets with varying contamination levels.

## Method Summary
AAR employs a two-phase rejection strategy: during an initial warm-up period (E=15 epochs), samples exceeding a modified z-score threshold (τ_N = 3.5×MAD/0.6745 + ŝ) are completely excluded from training. After warm-up, a Gaussian Mixture Model fits the anomaly scores to identify normal vs. anomaly distributions, with the final threshold being the maximum of GMM-derived intersection point (τ_I) and z-score threshold (τ_σ = z·σ_n + μ_n). The method uses weighted loss where excluded samples have weight=0, soft-rejected samples have weight=t_s=0.1, and normal samples have weight=1. This adaptive approach maintains robustness while preventing over-aggressive rejection of normal data.

## Key Results
- Achieves 0.041 AUROC improvement over state-of-the-art on contaminated datasets
- Demonstrates consistent performance across 30 tabular datasets and 2 image datasets
- Maintains effectiveness across contamination ratios from 0% to 30%
- Scales reliably to real-world applications in security and healthcare domains

## Why This Works (Mechanism)
AAR's effectiveness stems from its adaptive thresholding that responds to contamination levels in real-time. The warm-up phase establishes baseline reconstruction error distributions, while the GMM-based soft rejection dynamically adjusts to identify the boundary between normal and anomalous data. By using weighted loss instead of complete exclusion, AAR preserves gradient information from samples near the decision boundary, preventing catastrophic forgetting of legitimate data patterns.

## Foundational Learning
- **Modified Z-Score**: Robust outlier detection using median absolute deviation (MAD) instead of standard deviation; needed because contaminated data makes traditional z-scores unreliable
  - Quick check: Verify MAD-based threshold captures ~95% of normal samples on clean data
- **Gaussian Mixture Models**: Probabilistic clustering to separate normal vs. anomaly score distributions; needed to adapt thresholds based on data distribution
  - Quick check: Confirm GMM identifies two distinct components with significantly different means
- **Weighted Loss Functions**: Training samples assigned different importance weights; needed to balance between excluding anomalies and preserving normal data
  - Quick check: Monitor training loss contributions from each weight class

## Architecture Onboarding

Component Map: Data -> AE/MemAE/DSVDD -> Reconstruction Error -> Rejection Module -> Weighted Loss

Critical Path: The rejection module is the critical innovation, transforming fixed-threshold approaches into adaptive ones that respond to contamination levels in real-time.

Design Tradeoffs: Hard rejection provides robustness but risks excluding too many normals; soft rejection preserves gradient information but requires accurate component identification.

Failure Signatures: Poor GMM convergence on small batches, over-aggressive rejection eliminating normal samples, component identification errors swapping normal/anomaly assignments.

First Experiments:
1. Baseline AE on MNIST with 10% contamination; verify modified z-score captures expected proportion of outliers
2. GMM fitting on reconstruction scores; validate component ordering (lower mean = normal)
3. Weighted loss implementation; confirm gradient contributions from soft-rejected samples

## Open Questions the Paper Calls Out
None

## Limitations
- GMM fitting frequency and stability on small batches is not fully specified
- Component identification method (normal vs. anomaly) relies on mean ordering assumption
- Tabular contamination process lacks precise noise scale specifications per dataset

## Confidence
- Methodology: Medium - Core approach is clear but key implementation details are underspecified
- Quantitative Results: Low - Achieving identical performance requires clarification on underspecified parameters
- Reproducibility: Medium - Code implementation feasible but component ordering and fitting frequency create uncertainty

## Next Checks
1. Implement component ordering check: verify GMM normal component has lower mean score, and log cases where this fails
2. Test GMM fitting frequency: compare per-batch vs. accumulated (epoch-level) score statistics on small batches
3. Verify contamination generation: measure exact noise scale and distribution on tabular datasets, compare to paper's "zero-mean Gaussian noise" specification