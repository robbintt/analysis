---
ver: rpa2
title: 'Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN
  Inference'
arxiv_id: '2510.09018'
source_url: https://arxiv.org/abs/2510.09018
tags:
- latency
- scheduler
- energy
- accuracy
- utilization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of efficiently scheduling neural
  network inference across heterogeneous multi-GPU systems. The proposed Slim Scheduler
  integrates a Proximal Policy Optimization (PPO) reinforcement learning policy with
  local greedy schedulers to coordinate distributed inference for slimmable CNN models.
---

# Slim Scheduler: A Runtime-Aware RL and Scheduler System for Efficient CNN Inference

## Quick Facts
- arXiv ID: 2510.09018
- Source URL: https://arxiv.org/abs/2510.09018
- Reference count: 9
- One-line primary result: Achieves up to 96.45% latency reduction and 97.31% energy reduction in distributed CNN inference by combining PPO-based global routing with local greedy scheduling for slimmable models.

## Executive Summary
Slim Scheduler addresses the challenge of efficiently scheduling neural network inference across heterogeneous multi-GPU systems. The system integrates a Proximal Policy Optimization (PPO) reinforcement learning policy with local greedy schedulers to coordinate distributed inference for slimmable CNN models. The PPO router learns global routing decisions—device selection, width ratio, and batch configuration—while local greedy schedulers handle batching and scaling within VRAM and utilization constraints. Experiments on a 3-GPU cluster using SlimResNet on CIFAR-100 show significant efficiency gains at the cost of accuracy and increased variance when heavily optimized for performance.

## Method Summary
The Slim Scheduler combines a PPO-based router for global decision-making with local greedy schedulers for per-device resource management. The PPO router observes global state (queue lengths, power, utilization) and selects actions for server assignment, model width, and batch grouping. Local greedy schedulers batch requests by key, perform best-fit instance matching, scale instances within VRAM/utilization limits, and offload idle instances. The system uses a factored categorical policy with ε-greedy exploration on the server head, and trains with clipped PPO loss plus value and entropy terms. The reward function balances accuracy, latency, energy, and utilization variance, with tunable weights for different optimization objectives.

## Key Results
- Achieves 96.45% mean latency reduction and 97.31% energy reduction when optimized for efficiency (accuracy drops to 72.22%)
- Balanced configuration recovers accuracy to 75.26% with moderate efficiency gains (49.31% latency, 52.58% energy reduction)
- High efficiency optimization leads to 11.67s latency variance and 2125J energy variance, demonstrating trade-off between stability and performance

## Why This Works (Mechanism)
The system's effectiveness stems from combining global coordination with local optimization. The PPO router learns to distribute work across heterogeneous devices based on their current load and capacity, avoiding GPU saturation that causes non-linear latency increases. Local greedy schedulers maximize resource utilization through batching and instance scaling while respecting hardware constraints. The slimmable model architecture allows dynamic width adjustment to balance accuracy and computational cost. The reward function encourages policies that minimize both latency and energy consumption while maintaining acceptable accuracy levels.

## Foundational Learning
- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: PPO is the core RL algorithm used to train the router, requiring understanding of its clipped objective, advantage estimation, and exploration strategy.
  - Quick check question: What is the role of the clipping parameter $\epsilon$ in the PPO loss function, and how does it prevent overly large policy updates?

- Concept: **Slimmable Neural Networks**
  - Why needed here: The system schedules inference for models that can dynamically adjust their width (channel count). Understanding the trade-off between width, accuracy, and computational cost is fundamental.
  - Quick check question: If a model width is set to 0.25x, how would you expect its accuracy and inference latency to change compared to the full 1.0x width?

- Concept: **GPU Utilization and Saturation**
  - Why needed here: The system's efficiency gains are predicated on avoiding GPU saturation. You must understand why latency and energy spike non-linearly past ~95% utilization.
  - Quick check question: Why does increasing batch size beyond a certain point lead to a disproportionate increase in latency, even if throughput might remain constant?

## Architecture Onboarding
- Component map: PPO Router -> Local Greedy Schedulers (per-server) -> SlimResNet Backbone -> Telemetry System
- Critical path:
  1. Inference request arrives with required width
  2. PPO router observes global state and selects target server/configuration
  3. Request routed to chosen server's greedy scheduler
  4. Local scheduler batches request with others of same key
  5. Finds free compatible instance or scales up new one within VRAM/utilization limits
  6. Batch executed and telemetry updated

- Design tradeoffs:
  - PPO vs. Greedy: Hybrid approach balances optimality with training complexity; pure RL would have larger action space, pure greedy lacks global coordination
  - Reward Weighting: Heavy penalties yield maximal efficiency but low accuracy; balanced reward recovers accuracy but introduces high variance
  - Exploration: ε-greedy on server head encourages exploration but must decay to allow policy convergence

- Failure signatures:
  - Reward Hacking: Policy learns to minimize imbalance penalty by constantly shifting work, creating overhead without throughput improvement
  - VRAM Exhaustion: Greedy scheduler's scale-up logic fails when PPO router routes more work than available VRAM can handle
  - Accuracy Collapse: Strong latency/energy rewards may cause policy to always select 0.25x width, failing accuracy requirements

- First 3 experiments:
  1. Validate Greedy Baseline: Run system with random router and only local greedy schedulers; profile baseline latency, energy, and accuracy
  2. Single-Factor Ablation: Train PPO router using only latency reward term; verify if policy learns fastest configuration selection
  3. Full Multi-Objective Test: Train PPO router with complete reward function; compare policy's ability to balance trade-offs against baseline

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can adaptive reward scaling or uncertainty-aware policies reduce runtime variance while preserving efficiency gains?
- Basis in paper: "Future work should explore adaptive reward scaling or uncertainty-aware policies to maintain efficiency while improving predictability"
- Why unresolved: Fixed reward weights create binary trade-off—aggressive penalties yield stable but low-accuracy policies, while balanced weights introduce high variance
- What evidence would resolve it: Demonstration of PPO variant with dynamic reward adjustment achieving <5% variance increase while maintaining >80% efficiency reduction

### Open Question 2
- Question: How does Slim Scheduler scale to larger GPU clusters (beyond 3 devices) with greater heterogeneity?
- Basis in paper: Experiments limited to 3-GPU cluster, leaving scalability unexplored
- Why unresolved: PPO router's exploration strategy and imbalance penalty may not generalize when action space and state dimensionality grow
- What evidence would resolve it: Benchmarks on 10+ GPU clusters showing whether latency/energy reductions persist and whether training converges within comparable episodes

### Open Question 3
- Question: Does PPO+greedy framework generalize to non-CNN architectures (e.g., Transformers) with different parallelism patterns?
- Basis in paper: Only SlimResNet on CIFAR-100 tested; approach assumes segmented slimmable CNNs with per-segment width control
- Why unresolved: Transformers have different computational profiles and may require different state/action encodings
- What evidence would resolve it: Successful application to slimmable Transformer variants with comparable efficiency-accuracy trade-offs

## Limitations
- Heavy optimization for efficiency leads to significant accuracy degradation (72.22% vs 75.26% in balanced configuration)
- High variance in latency and energy consumption when using balanced reward weighting (11.67s and 2125J standard deviation)
- Limited scalability testing—only evaluated on 3-GPU cluster with specific hardware configuration

## Confidence
- **High Confidence:** Hybrid PPO + greedy scheduling approach is sound and general framework is reproducible; trade-off between efficiency and accuracy clearly demonstrated
- **Medium Confidence:** Specific efficiency gains (96.45% latency, 97.31% energy reduction) are likely achievable with correct hyperparameters but require precise tuning
- **Low Confidence:** Exact PPO training setup (reward weights, network architecture, learning rate) is underspecified, making reproduction difficult without significant trial and error

## Next Checks
1. Validate Greedy Baseline: Implement and profile local greedy scheduler in isolation to confirm VRAM/utilization constraints and establish performance baseline
2. Single-Factor Ablation: Train PPO router with only latency reward to verify learning of fastest configuration selection and measure accuracy drop
3. Full Multi-Objective Test: Train PPO router with complete reward function and compare performance against baseline and single-factor ablation, analyzing latency/energy variance for stability assessment