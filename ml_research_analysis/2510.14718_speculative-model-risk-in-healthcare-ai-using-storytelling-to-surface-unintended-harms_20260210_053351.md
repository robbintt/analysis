---
ver: rpa2
title: 'Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended
  Harms'
arxiv_id: '2510.14718'
source_url: https://arxiv.org/abs/2510.14718
tags:
- story
- health
- stories
- harms
- ethical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a human-centered framework that uses AI-generated
  user stories to help people think creatively about potential benefits and harms
  of healthcare AI before deployment. The method combines automated story generation
  with multi-agent red-team discussions to simulate realistic scenarios where AI systems
  might succeed or fail.
---

# Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms

## Quick Facts
- arXiv ID: 2510.14718
- Source URL: https://arxiv.org/abs/2510.14718
- Reference count: 40
- Primary result: Human-centered framework using AI-generated stories helps identify broader range of AI healthcare harms before deployment

## Executive Summary
This paper introduces a novel framework that leverages AI-generated user stories and multi-agent red-team discussions to help stakeholders anticipate potential benefits and harms of healthcare AI systems before deployment. The approach combines automated storytelling with simulated scenario analysis to surface unintended consequences that might not be apparent through traditional risk assessment methods. By engaging participants in narrative-based exercises, the framework enables creative thinking about AI system risks across multiple domains including privacy, equity, clinical workflow, and patient autonomy. A user study with 18 participants demonstrated that storytelling participants identified significantly more diverse harms and benefits compared to control groups.

## Method Summary
The framework operates through a three-stage process: automated story generation creates realistic healthcare scenarios featuring AI systems and diverse characters, multi-agent red-team discussions simulate adversarial thinking about system vulnerabilities, and human participants engage with these narratives to identify potential risks and benefits. The system uses large language models to generate contextually appropriate stories based on specified AI applications, then facilitates group discussions where participants role-play as different stakeholders. The framework is designed to be accessible to non-technical users while maintaining sufficient depth to surface meaningful insights about AI system impacts.

## Key Results
- Storytelling participants identified harms across all 13 categories (entropy 3.383) versus control focus on privacy/well-being (entropy 2.433)
- Narrative engagement increased benefit identification diversity (entropy 3.554 vs. 2.579)
- Participants found storytelling format more engaging and accessible than traditional risk assessment
- The framework successfully surfaced risks that participants hadn't initially considered

## Why This Works (Mechanism)
The framework works by leveraging narrative cognition - humans naturally understand and process information through stories, making abstract AI risks more concrete and relatable. By presenting AI scenarios through characters and situations that participants can empathize with, the framework bypasses technical jargon barriers and engages intuitive risk assessment capabilities. The multi-agent red-team component introduces adversarial thinking patterns that challenge optimistic assumptions about AI system performance.

## Foundational Learning
- Narrative cognition in risk assessment: Stories activate different neural pathways than abstract analysis, enabling better retention and consideration of complex scenarios. Why needed: Technical risk frameworks often fail to engage non-expert stakeholders effectively. Quick check: Compare narrative vs. abstract scenario comprehension in mixed expertise groups.
- Adversarial simulation for AI safety: Role-playing different stakeholders reveals blind spots in system design by forcing consideration of diverse perspectives and motivations. Why needed: Single-stakeholder analysis misses critical failure modes. Quick check: Track unique risks identified by each role-play persona.
- Entropy-based diversity measurement: Quantifies the breadth of risk identification across multiple categories, providing objective comparison between methods. Why needed: Traditional risk assessment focuses on severity, not diversity of considerations. Quick check: Validate entropy measures against expert risk catalogs.

## Architecture Onboarding
Component map: Story Generator -> Red-Team Simulator -> Human Discussion Facilitator -> Risk Catalog

Critical path: Story generation creates scenarios → Red-team simulation adds adversarial perspectives → Human participants engage with narratives → Risks are categorized and documented

Design tradeoffs: Automated story generation sacrifices some narrative nuance for scalability, while human facilitation ensures quality control. The framework prioritizes accessibility over technical depth to engage diverse stakeholders.

Failure signatures: Over-reliance on generic scenarios that don't reflect domain-specific contexts; facilitator bias in guiding discussions; participants defaulting to obvious rather than creative risks.

First experiments:
1. Test story generation with different healthcare specialties to assess domain adaptability
2. Compare risk identification rates between single vs. multi-agent red-team configurations
3. Measure long-term retention of identified risks compared to traditional workshop formats

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (N=18) and technical participant bias limit generalizability
- Focus on only two healthcare AI applications may not capture full risk spectrum
- Entropy measurements rely on subjective categorization that could vary between annotators
- Automated story generation may not capture full complexity of real-world healthcare scenarios

## Confidence
High confidence: Storytelling participants identified broader range of harms across all 13 categories
Medium confidence: Narrative format made exercise more engaging and accessible
Low confidence: