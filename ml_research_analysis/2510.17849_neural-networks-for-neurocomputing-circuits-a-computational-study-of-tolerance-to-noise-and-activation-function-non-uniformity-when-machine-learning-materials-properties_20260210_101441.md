---
ver: rpa2
title: 'Neural networks for neurocomputing circuits: a computational study of tolerance
  to noise and activation function non-uniformity when machine learning materials
  properties'
arxiv_id: '2510.17849'
source_url: https://arxiv.org/abs/2510.17849
tags:
- test
- noise
- page
- rmse
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the tolerance of neural networks to noise
  and activation function inhomogeneity when implemented in dedicated analog neurocomputing
  circuits for high-throughput machine learning applications in materials informatics.
  The authors focus on how circuit noise and variations in neuron activation function
  (NAF) shapes due to semiconductor device characteristics affect model performance.
---

# Neural networks for neurocomputing circuits: a computational study of tolerance to noise and activation function non-uniformity when machine learning materials properties

## Quick Facts
- **arXiv ID:** 2510.17849
- **Source URL:** https://arxiv.org/abs/2510.17849
- **Reference count:** 0
- **Key outcome:** Neural networks show low noise tolerance in analog neurocomputing circuits, but retraining with realized activation function shapes can recover most lost accuracy.

## Executive Summary
This computational study investigates how neural networks perform in dedicated analog neurocomputing circuits when subjected to circuit noise and activation function non-uniformity. The research focuses on high-throughput machine learning applications for materials informatics, using datasets including peri-condensed hydrocarbons, double perovskites, and QM9 molecules. The authors systematically evaluate how synthetic noise and activation function perturbations affect model performance, revealing that neural networks are generally intolerant to noise but can be effectively compensated through retraining with practically realized neuron activation function shapes.

## Method Summary
The study employs a systematic computational approach to evaluate neural network tolerance in analog neurocomputing contexts. Researchers trained neural networks with various architectures on representative materials datasets and then introduced controlled perturbations including random noise and smooth variations to neuron activation function (NAF) shapes. These perturbations simulate real-world circuit conditions arising from semiconductor device characteristics. The performance degradation was measured across different network configurations, with particular attention to single-hidden layer networks and larger-than-optimal sized networks, which demonstrated better noise tolerance.

## Key Results
- Neural networks exhibit generally low tolerance to noise, with accuracy degrading rapidly as noise levels increase
- Single-hidden layer networks and larger-than-optimal sized networks show better noise tolerance than deeper or optimally-sized alternatives
- Retraining neural networks using practically realized NAF shapes effectively compensates for activation function inhomogeneity, recovering most lost accuracy
- This retraining approach is particularly valuable for addressing device-to-device variability in organic electronic devices used for neuromorphic computing

## Why This Works (Mechanism)
The effectiveness of retraining with realized NAF shapes works because neural networks can adapt their weight parameters to accommodate systematic variations in activation function shapes across devices. When activation functions deviate from their idealized forms due to manufacturing variations or material properties in organic electronic devices, the network can learn to compensate by adjusting its weights during retraining. This adaptation effectively maps the distorted activation landscape, allowing the network to maintain performance despite underlying hardware inhomogeneity.

## Foundational Learning

### Neural network noise sensitivity
- **Why needed:** Understanding how noise affects network performance is critical for deploying ML in analog hardware
- **Quick check:** Monitor accuracy degradation as noise levels increase; identify threshold where performance becomes unacceptable

### Activation function variability modeling
- **Why needed:** Real circuits exhibit device-to-device variations that idealized models don't capture
- **Quick check:** Compare performance across networks with different activation function perturbation patterns

### Retraining as compensation strategy
- **Why needed:** Demonstrates practical approach to hardware non-idealities without requiring perfect manufacturing
- **Quick check:** Measure accuracy recovery after retraining with realized versus idealized activation functions

## Architecture Onboarding

### Component map
Datasets (hydrocarbons, perovskites, QM9) -> Neural network training -> Noise injection -> Performance evaluation -> Retraining with NAF variations -> Accuracy recovery assessment

### Critical path
Model training → Noise injection → Performance measurement → Retraining with realized NAF shapes → Accuracy validation

### Design tradeoffs
- Simpler architectures (single-hidden layer) offer better noise tolerance but may sacrifice representational capacity
- Larger-than-optimal networks provide noise robustness but increase computational cost and power consumption
- Retraining compensates for variability but requires additional computational resources and time

### Failure signatures
- Rapid accuracy degradation with increasing noise levels
- Systematic performance variations across different activation function perturbation patterns
- Limited recovery when attempting to compensate for noise through architecture changes alone

### First 3 experiments
1. Measure baseline accuracy on clean data across different network architectures
2. Introduce incremental noise levels and record performance degradation
3. Apply retraining with realized NAF shapes and measure accuracy recovery

## Open Questions the Paper Calls Out
None

## Limitations
- Noise tolerance assessments rely on synthetic noise models rather than measurements from actual analog hardware
- Simulation of activation function variations through smooth perturbations may not fully represent complex variations in organic electronic devices
- Study focuses on relatively small neural network architectures, limiting generalizability to larger models

## Confidence

**High confidence:**
- Retraining with realized NAF shapes effectively recovers accuracy

**Medium confidence:**
- General patterns of noise sensitivity across different network architectures
- Comparative performance of single-hidden layer versus deeper networks

## Next Checks
1. Validate noise tolerance results using measurements from fabricated analog neurocomputing circuits rather than synthetic models
2. Test the retraining compensation strategy with experimentally measured NAF variations from actual organic electronic devices
3. Evaluate whether noise tolerance patterns observed in small networks extend to larger architectures with more parameters