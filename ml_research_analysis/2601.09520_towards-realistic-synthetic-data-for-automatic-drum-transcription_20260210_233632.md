---
ver: rpa2
title: Towards Realistic Synthetic Data for Automatic Drum Transcription
arxiv_id: '2601.09520'
source_url: https://arxiv.org/abs/2601.09520
tags:
- drum
- data
- transcription
- synthetic
- instrument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatic drum transcription
  (ADT), which traditionally requires large-scale paired audio-MIDI datasets that
  are scarce. To overcome this limitation, the authors propose a novel approach that
  leverages synthetic data generation using a curated corpus of high-quality one-shot
  drum samples.
---

# Towards Realistic Synthetic Data for Automatic Drum Transcription

## Quick Facts
- arXiv ID: 2601.09520
- Source URL: https://arxiv.org/abs/2601.09520
- Reference count: 0
- Proposed method achieves F1-scores of 0.79 on MDB and 0.68 on ENST test sets

## Executive Summary
This paper addresses the scarcity of large-scale paired audio-MIDI datasets for automatic drum transcription (ADT) by proposing a synthetic data generation pipeline. The authors curate a high-quality one-shot drum sample library and use a semi-supervised approach to automatically label and standardize diverse drum samples from unlabeled sources. These samples are then used to synthesize realistic audio from MIDI files, which trains a sequence-to-sequence transformer model. The resulting model achieves state-of-the-art performance on ENST and MDB test sets, outperforming both fully supervised methods and previous synthetic-data approaches.

## Method Summary
The authors propose a synthetic data generation pipeline to overcome the limited availability of paired audio-MIDI datasets for ADT. They first curate a high-quality one-shot drum sample library and then use a semi-supervised approach to automatically label and standardize diverse drum samples from unlabeled sources using CLAP-based similarity scoring. These standardized samples are used to synthesize realistic audio from MIDI files, which are then used to train a sequence-to-sequence transformer model for ADT. The method significantly outperforms previous approaches on established test sets.

## Key Results
- Achieved F1-scores of 0.79 on MDB and 0.68 on ENST test sets
- Outperformed both fully supervised methods and previous synthetic-data approaches
- Code is publicly available for replication and further research

## Why This Works (Mechanism)
The approach works by addressing the fundamental limitation of scarce paired audio-MIDI data through synthetic data generation. By curating high-quality drum samples and using semi-supervised labeling with CLAP-based similarity scoring, the method creates realistic synthetic audio that captures diverse drum timbres and playing styles. The transformer model trained on this synthetic data learns robust representations that generalize well to real-world transcription tasks, achieving state-of-the-art performance on established benchmarks.

## Foundational Learning

**Sequence-to-sequence transformers**: Neural architectures that map input sequences to output sequences, essential for converting audio features to drum event predictions. Quick check: Verify model can handle variable-length sequences typical in drum performances.

**CLAP (Contrastive Language-Audio Pretraining)**: A model that learns joint representations of audio and text, used here for automatic labeling of drum samples. Quick check: Ensure CLAP similarity scores correlate with human perception of drum timbres.

**Semi-supervised learning**: Training approach that combines a small amount of labeled data with large amounts of unlabeled data, crucial for building the drum sample library. Quick check: Validate that unlabeled sample labeling precision meets acceptable thresholds.

## Architecture Onboarding

**Component map**: MIDI files -> Sample library -> Synthetic audio generation -> Transformer model -> Drum transcription output

**Critical path**: The core pipeline flows from curated MIDI files through the sample library to synthetic audio generation, which trains the transformer model for final transcription output.

**Design tradeoffs**: The method trades computational cost of synthetic data generation for improved model performance and reduced need for real paired data. The reliance on CLAP-based labeling introduces potential noise but enables scalability.

**Failure signatures**: Poor performance may result from insufficient sample diversity in the curated library, inaccurate automatic labeling of drum samples, or synthetic audio that doesn't capture realistic playing nuances.

**3 first experiments**:
1. Ablation study varying synthetic sample library size to quantify impact on model performance
2. Cross-dataset evaluation on additional drum transcription benchmarks beyond ENST and MDB
3. Manual validation of CLAP-based labeling precision on a subset of drum samples

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Performance gains rely on a single training/test split configuration without reported variance across multiple seeds or folds
- Synthetic data quality heavily depends on the representativeness of the curated drum sample library
- CLAP-based automatic labeling precision and recall are not quantified, raising concerns about label noise
- Evaluation limited to ENST and MDB datasets without testing on more varied or contemporary benchmarks

## Confidence

**State-of-the-art performance claims**: Medium - strong results reported but limited statistical validation
**Synthetic data realism and utility**: Medium - quality depends on unquantified sample labeling precision  
**Generalizability to diverse datasets**: Low - evaluation limited to two established test sets

## Next Checks
1. Run ablation studies with varying numbers of synthetic samples to quantify the relationship between synthetic data size and performance gains.
2. Conduct cross-dataset evaluation on additional drum transcription benchmarks (e.g., IDMT-SMT, URMP) to test generalizability.
3. Measure and report the precision/recall of the automatic CLAP-based labeling pipeline on a manually annotated validation set.