---
ver: rpa2
title: 'ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields
  with Survey-Mined Questions and Rubrics'
arxiv_id: '2509.00496'
source_url: https://arxiv.org/abs/2509.00496
tags:
- response
- rubric
- query
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RESEARCHQA leverages survey articles to create a large-scale scholarly
  question-answering benchmark, addressing the challenge of evaluating research synthesis
  systems across diverse fields. By automatically extracting queries and rubrics from
  over 54,000 survey articles, it generates 21,400 queries with 160,000 rubric items
  across 75 research fields.
---

# ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics

## Quick Facts
- arXiv ID: 2509.00496
- Source URL: https://arxiv.org/abs/2509.00496
- Reference count: 40
- Primary result: Creates benchmark of 21,400 queries with 160,000 rubric items across 75 fields; no system exceeds 70% rubric coverage, with top deep research system at 75.3%

## Executive Summary
ResearchQA addresses the challenge of evaluating scholarly question-answering systems by mining queries and rubrics from over 54,000 survey articles across 75 research fields. The benchmark provides 21,400 queries paired with 160,000 rubric items that capture essential coverage criteria for research synthesis. Expert validation shows 90% of queries reflect genuine PhD-level information needs and 87% of rubric items require meaningful coverage. Evaluation of 18 systems reveals significant performance gaps, with no parametric or retrieval-augmented system exceeding 70% rubric coverage, highlighting substantial room for improvement in scholarly synthesis capabilities.

## Method Summary
The benchmark leverages survey articles to automatically generate scholarly queries and evaluation rubrics. A hierarchical summarization pipeline extracts multi-sentence sections from surveys, which are then used to generate standalone queries through LLM prompting. These queries undergo filtering to remove context-dependent or highly variable responses. Rubrics are constructed by combining survey-grounded criteria with parametric rubrics, followed by deduplication and hallucination removal using Google Scholar verification. The ensemble judge combines direct preference prediction with rubric coverage scoring to evaluate system responses, using Bradley-Terry models for pairwise ranking. The full pipeline costs approximately $483 using gpt-4.1-mini versus $2,416 with gpt-4.1, with comparable quality.

## Key Results
- Expert annotators validated that 90% of queries reflect PhD information needs and 87% of rubric items require meaningful coverage
- Top-performing deep research system achieved 75.3% rubric coverage, while no system exceeded 70% coverage overall
- Citation-based rubric items were the most under-addressed category, with top systems covering fewer than 11% completely
- Parametric models, RAG systems, and deep research systems showed distinct performance patterns across different rubric types
- Survey-grounded rubrics outperformed generic rubrics in predicting expert preferences across 7 of 8 tested fields

## Why This Works (Mechanism)

### Mechanism 1: Survey-Distilled Query Generation
Mining queries from survey sections yields PhD-level research questions with low answer variability because survey sections contain synthesized knowledge with explicit citations. Hierarchical summarization extracts question-answer pairs, filtering removes context-dependent and highly variable queries, leaving standalone queries with constrained answer spaces. This works when survey articles represent consensus knowledge and questions derived from them reflect genuine expert information needs. The method breaks if surveys become outdated or biased in specific fields, or if answer variability scoring fails to properly constrain the answer space.

### Mechanism 2: Hybrid Rubric Construction for Grounded Evaluation
Combining survey-grounded rubrics with parametric rubrics and deduplication yields evaluation criteria that better predict expert preferences because survey citations represent essential coverage while parametric rubrics provide general evaluation dimensions. The union plus deduplication and reranking removes hallucinations and vacuous items, retaining top-8 items that capture both domain-specific and general evaluation criteria. This succeeds when important criteria are present in source surveys and LLM can identify hallucinated citations via Google Scholar matching. The approach fails if important criteria are absent from source surveys or if deduplication is too aggressive, creating coverage gaps.

### Mechanism 3: Ensemble Judge for Preference Prediction
Combining direct preference prediction with rubric coverage scores improves agreement with expert judgments because direct judges lack domain knowledge while coverage scoring grounds judgment in specific criteria. The ensemble sums coverage points plus 4× direct preference, reducing human-LLM agreement gap from 12.7% to 9.6% (24% relative reduction). This works when higher rubric coverage correlates with expert preference and positional bias can be mitigated via reverse ordering. The method fails if systems game coverage by verbose responses without quality, or if direct judge systematically prefers certain system styles.

## Foundational Learning

- **Concept: Hierarchical Summarization for Query Extraction**
  - Why needed here: Queries must integrate multiple sources from survey sections, not single sentences
  - Quick check question: Can you explain why extracting from multi-sentence spans produces more PhD-relevant queries than sentence-level extraction?

- **Concept: Rubric Coverage as Recall-Based Metric**
  - Why needed here: Coverage measures whether criteria are addressed, not overall quality; longer answers inflate scores
  - Quick check question: Why does the paper warn about length bias in coverage, and how does the 250-word constraint address this?

- **Concept: Bradley-Terry/Elo for Pairwise Comparisons**
  - Why needed here: Systems are ranked via tournament battles, not absolute scores alone
  - Quick check question: How does the ensemble judge's tie handling affect Elo calculations?

## Architecture Onboarding

- **Component map:** Survey retrieval (Crossref/Semantic Scholar/S2ORC) → venue filtering → survey classification → section extraction → hierarchical summarization → query generation → filtering (standalone, variability, length, field alignment) → rubric pipeline (survey rubrics + parametric rubrics → deduplication → reranking → hallucination removal → top-8 selection) → evaluation (system answer generation → rubric coverage scoring → direct judge → ensemble aggregation → Elo ranking)

- **Critical path:** Survey quality → query filtering thresholds → rubric deduplication → coverage scoring accuracy → ensemble judge configuration. Errors propagate; weak surveys yield weak queries.

- **Design tradeoffs:** gpt-4.1-mini vs. gpt-4.1: Mini costs ~$483 vs. ~$2416 for full pipeline; quality comparable (Table 9 shows similar outputs). 250-word constraint vs. unconstrained: Constrained reduces length bias but may limit deep research systems; unconstrained shows +9.1% coverage for deep research (Table 4). Hybrid vs. generic rubrics: Hybrid improves accuracy in 7/8 fields but requires source surveys; generic is field-agnostic but less predictive.

- **Failure signatures:** Citation rubric items under-addressed in 89% of cases (Table 5) → systems fail to ground claims. Parametric rubrics have 15% vacuous/error rate vs. hybrid's lower rate → hallucinated citations. Naive retrieval scores 7.6% lower than production retrieval on average → retrieval quality dominates.

- **First 3 experiments:** 1) Ablate rubric types (survey-only, parametric-only, hybrid) on a held-out field to measure preference prediction accuracy delta. 2) Vary coverage scoring thresholds (0-4 scale binarization) to calibrate against human judgments on 100 queries. 3) Test leakage impact: compare coverage on queries where source survey is retrievable vs. blocked in production systems.

## Open Questions the Paper Calls Out
- How can evaluation rubrics be automatically synthesized to capture tacit or implicit criteria necessary for scholarly answers, beyond the explicit content found in source survey articles? Current rubrics rely solely on text explicitly present in surveys, potentially missing crucial evaluation dimensions known to experts but not written down in the source text.

- What specific architectural or retrieval modifications are required to improve the citation accuracy of deep research systems, which currently address fewer than 11% of citation-based rubric items? While deep research systems outperform others, they still fail significantly at specific citation tasks, suggesting current RAG pipelines are insufficient for precise bibliographic verification.

- Does the performance advantage of deep research systems over parametric models generalize effectively to domains with lower digital literature coverage, such as the Humanities? It is unclear if the "deep research" advantage stems from better reasoning or simply better retrieval in data-rich fields, leaving the efficacy in data-sparse or non-standardized domains unconfirmed.

## Limitations
- The survey-mining pipeline may inherit biases from existing literature, potentially underrepresenting emerging fields or marginalized perspectives in the academic discourse.
- The 250-word constraint, while reducing length bias, may artificially disadvantage deep research systems that naturally produce longer, more comprehensive syntheses.
- The ensemble judge's reliance on LLM-based scoring introduces potential calibration drift if model versions or capabilities change over time.

## Confidence

- **High Confidence**: The survey-mining methodology for generating PhD-level queries (90% expert validation), the hybrid rubric construction approach (87% meaningful items), and the benchmark's scale (21.4K queries, 160K rubric items) are well-supported by empirical evidence.
- **Medium Confidence**: The claim that no system exceeds 70% rubric coverage requires validation across different evaluation cohorts and time periods, as model capabilities evolve rapidly.
- **Low Confidence**: The specific thresholds used in query filtering (variability scoring, standalone criteria) may not generalize well to fields with different publication patterns or discourse structures.

## Next Checks

1. **Temporal Stability Test**: Re-evaluate the same systems on ResearchQA queries with knowledge cutoff dates spanning 5+ years to assess whether performance degrades predictably as surveys age.

2. **Cross-Field Generalization**: Apply the hybrid rubric methodology to a non-STEM field (e.g., humanities) and measure whether the same preference prediction accuracy is maintained.

3. **Retrieval Quality Impact**: Conduct ablation studies varying the quality of the retrieval backend (naive vs. production) while holding the generation system constant to isolate retrieval's contribution to coverage scores.