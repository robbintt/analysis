---
ver: rpa2
title: 'DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge Computing
  Systems'
arxiv_id: '2503.01704'
source_url: https://arxiv.org/abs/2503.01704
tags:
- layer
- edge
- quantization
- each
- server
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DILEMMA, a joint optimization framework for
  LLM layer placement and quantization in edge computing systems. The framework minimizes
  total inference delay while maintaining acceptable LLM performance through layer-wise
  quantization and knowledge distillation.
---

# DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems

## Quick Facts
- **arXiv ID**: 2503.01704
- **Source URL**: https://arxiv.org/abs/2503.01704
- **Reference count**: 4
- **Primary result**: Proposes DILEMMA framework achieving 87.5% model size reduction while preserving LLM performance through joint quantization and distributed inference optimization

## Executive Summary
This paper introduces DILEMMA, a joint optimization framework for Large Language Model (LLM) layer placement and quantization in edge computing systems. The framework addresses the challenge of deploying LLMs on resource-constrained edge devices by simultaneously optimizing where model layers are placed and how they are quantized. Through Integer Linear Programming, DILEMMA minimizes total inference delay while maintaining acceptable model performance via layer-wise quantization and knowledge distillation. The approach enables significant model compression without substantial accuracy loss.

## Method Summary
DILEMMA employs an Integer Linear Programming formulation to jointly optimize LLM layer placement and quantization decisions across edge computing nodes. The framework uses knowledge distillation to preserve model performance during quantization, allowing for aggressive compression ratios. The optimization considers both computational and communication costs across the distributed edge system. The approach enables fine-grained control over the trade-off between model size, inference latency, and performance accuracy through systematic layer-wise quantization decisions.

## Key Results
- Achieves up to 87.5% model size reduction (quantization ratio of 12.75%)
- Preserves model performance while minimizing total inference delay
- Demonstrates effectiveness on OPT-350m with SQuAD dataset
- Provides systematic approach to joint quantization and placement optimization

## Why This Works (Mechanism)
DILEMMA works by breaking down the complex problem of distributed LLM inference into two interdependent optimization problems: layer placement and quantization. By formulating these as joint decisions rather than sequential ones, the framework can identify optimal trade-offs between computational load distribution, communication overhead, and model compression. The knowledge distillation component ensures that aggressive quantization doesn't lead to catastrophic performance degradation, while the ILP formulation guarantees finding optimal solutions within the defined constraints.

## Foundational Learning
- **Integer Linear Programming**: Why needed - provides optimal solutions for combinatorial optimization problems. Quick check - verify ILP solver convergence and solution quality.
- **Knowledge Distillation**: Why needed - preserves model performance during aggressive quantization. Quick check - measure performance gap between original and distilled models.
- **Layer-wise Quantization**: Why needed - enables fine-grained control over compression vs. accuracy trade-off. Quick check - validate quantization effectiveness across different bit-widths.
- **Edge Computing Systems**: Why needed - provides context for distributed inference optimization. Quick check - measure communication overhead between edge nodes.
- **Model Compression Techniques**: Why needed - reduces model size for edge deployment. Quick check - verify compressed model can fit on target edge devices.
- **Distributed Inference**: Why needed - enables processing of large models across multiple devices. Quick check - measure end-to-end latency across the distributed system.

## Architecture Onboarding
- **Component Map**: ILP Optimizer -> Knowledge Distillation Module -> Quantization Engine -> Layer Placement Module -> Edge Nodes
- **Critical Path**: User Query -> Distributed Inference Pipeline -> Quantized Layer Processing -> Result Aggregation
- **Design Tradeoffs**: Aggressive quantization reduces model size but may increase inference time; optimal layer placement balances computational load but increases communication overhead.
- **Failure Signatures**: ILP solver timeouts indicate infeasible optimization; performance degradation suggests inadequate knowledge distillation; communication bottlenecks reveal suboptimal layer placement.
- **First Experiments**: 1) Validate ILP formulation with synthetic data, 2) Test knowledge distillation on quantized layers, 3) Measure end-to-end latency on small-scale edge testbed.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to single LLM model (OPT-350m) and dataset (SQuAD)
- ILP approach may face scalability challenges with larger LLMs
- Assumes homogeneous edge computing nodes, not reflecting real-world heterogeneity
- Quantization strategy impact on different model architectures remains unexplored

## Confidence
- **High Confidence**: Core ILP formulation is mathematically sound; 87.5% model size reduction is verifiable
- **Medium Confidence**: Performance preservation through knowledge distillation is plausible but needs broader validation
- **Low Confidence**: Scalability claims for larger LLMs and effectiveness in heterogeneous environments are speculative

## Next Checks
1. Scale validation: Test framework on larger LLM models (1B+ parameters) to verify scalability and performance preservation
2. Cross-architecture testing: Evaluate effectiveness across different model families and task types beyond question-answering
3. Real-world deployment: Implement in heterogeneous edge computing testbed with varying node capabilities