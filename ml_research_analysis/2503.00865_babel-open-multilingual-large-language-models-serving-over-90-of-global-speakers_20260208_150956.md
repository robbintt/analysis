---
ver: rpa2
title: 'Babel: Open Multilingual Large Language Models Serving Over 90% of Global
  Speakers'
arxiv_id: '2503.00865'
source_url: https://arxiv.org/abs/2503.00865
tags:
- multilingual
- languages
- performance
- language
- open
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Babel is an open multilingual LLM that supports the top 25 languages
  by number of speakers, covering over 90% of the global population, including many
  under-resourced languages neglected by existing models. Unlike traditional pretraining
  approaches, Babel uses a layer extension technique to increase its parameter count,
  with two variants: Babel-9B for efficient inference and Babel-83B for state-of-the-art
  performance.'
---

# Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers

## Quick Facts
- **arXiv ID**: 2503.00865
- **Source URL**: https://arxiv.org/abs/2503.00865
- **Reference count**: 7
- **Primary result**: Babel-83B-Chat matches commercial models like GPT-4o on certain multilingual tasks, using only publicly available fine-tuning data

## Executive Summary
Babel is an open multilingual LLM supporting the top 25 languages by number of speakers, covering over 90% of the global population. Unlike traditional pretraining approaches, Babel uses a layer extension technique to increase parameter count, with two variants: Babel-9B for efficient inference and Babel-83B for state-of-the-art performance. Babel achieves superior multilingual performance on benchmarks such as MMMLU, M3Exam, XCOPA, XNLI, and Flores-200, outperforming comparable open-source models.

## Method Summary
Babel starts from Qwen2.5 checkpoints and applies layer extension by inserting new layers in the second half of the model architecture, initialized with duplicate parameters plus small Gaussian noise. The model undergoes two-stage pretraining: Stage 1 focuses on recovery with diverse multilingual data, while Stage 2 emphasizes low-resource languages and textbooks. Babel-Chat variants are fine-tuned on 1M multilingual conversations. The approach addresses the multilingual gap by covering under-resourced languages like Hindi, Bengali, Urdu, Swahili, Javanese, Hausa, and Burmese.

## Key Results
- Babel-9B-Chat leads among 10B-sized LLMs on multilingual benchmarks
- Babel-83B-Chat matches commercial models like GPT-4o on certain tasks
- Multilingual SFT outperforms English-only SFT (58.6 vs. 56.5 average on key benchmarks)
- Layer extension insertion outperforms appending (73.1 vs 9.4 on MMMLU)

## Why This Works (Mechanism)

### Mechanism 1: Layer Extension Technique
Inserting duplicate layers within the model elevates performance ceiling compared to traditional continued pretraining. New layers are inserted into the second half of the model (positions 14–24 for Babel-9B, 40–62 for Babel-83B), initialized by copying original parameters plus small Gaussian noise (μ = 0.0001). Middle and back layers are less sensitive to structural edits, preserving learned representations better than appending layers.

### Mechanism 2: Two-Stage Pretraining Strategy
Stage 1 samples diverse corpus across all languages with emphasis on English/Chinese to accelerate recovery after structural disruption. Stage 2 increases low-resource language proportions and textbook content to inject new multilingual knowledge. The model can recover general capabilities quickly with high-quality, diverse data, then strengthens under-represented languages without catastrophic forgetting.

### Mechanism 3: LLM-Based Quality Classification
GPT-4o scores potential training data across quality dimensions, with expert review and refinement. A Qwen-2.5-0.5B-Instruct classifier is trained on this curated set to filter data at scale. Strong LLMs can approximate human quality judgments sufficiently to bootstrap a smaller, efficient classifier; expert oversight corrects systematic biases.

## Foundational Learning

- **Transformer layer sensitivity**: Layer extension assumes certain layers are less sensitive to structural edits; understanding residual connections and layer roles is critical for choosing insertion points. Quick check: If you insert a zero-initialized layer in a residual network, why might the model still produce reasonable outputs initially?

- **Multilingual training data imbalance**: Babel explicitly addresses the gap between high-resource and low-resource languages through staged training and data re-weighting. Quick check: Why might naively sampling proportionally to corpus size fail to improve low-resource language performance?

- **Supervised Fine-Tuning data composition**: Babel-Chat models use 1M multilingual conversations; the paper shows multilingual SFT outperforms English-only SFT (58.6 vs. 56.5 average). Quick check: If your SFT data is 90% English, what failure modes might you expect at inference time for Hindi prompts?

## Architecture Onboarding

- **Component map**: Qwen2.5-7B → Babel-9B (28→34 layers); Qwen2.5-72B → Babel-83B (80→92 layers) → Layer extension at positions {14,16,18,20,22,24} or {40,42,...,62} → Two-stage pretraining → SFT on 1M multilingual conversations

- **Critical path**: 1) Start from Qwen2.5 checkpoint; 2) Apply layer extension (insert, do not append); 3) Stage 1 pretraining (recovery, diverse corpus); 4) Stage 2 pretraining (low-resource up-weighting, textbooks); 5) SFT with multilingual instruction data

- **Design tradeoffs**: Appending layers is simpler but causes severe performance drop (9.4 vs. 73.1 on MMMLU); Higher noise initialization (μ = 0.01) enables adaptation potential but destabilizes early training; Multilingual SFT improves average scores but may introduce alignment inconsistencies across languages

- **Failure signatures**: Sudden accuracy drop post-extension → likely appended instead of inserted, or noise too high; High-resource performance collapse in Stage 2 → low-resource data over-representation without balancing; Incoherent outputs in specific languages → SFT data missing or poorly translated for those languages

- **First 3 experiments**:
  1. Replicate layer extension ablation on Qwen2.5-3B: insert vs. append, no-noise vs. small noise, measure MMMLU recovery after 1B tokens
  2. Validate Stage 1 recovery: after extension, train on English/Chinese-only vs. full multilingual corpus for 5B tokens, compare checkpoint stability
  3. SFT composition test: train three Babel-9B-Chat variants (English-only, multilingual with 40% English, multilingual with 20% English) and evaluate on XCOPA/XNLI per-language breakdown

## Open Questions the Paper Calls Out
None

## Limitations
- Data composition and mixing ratios for two-stage pretraining are not specified, making it difficult to determine whether multilingual gains come from superior techniques or simply more low-resource language data exposure
- Quality classifier details including prompt templates, scoring rubrics, and expert review protocols are not disclosed, limiting reproducibility
- Training hyperparameters like learning rates, weight decay, and total training steps for both pretraining stages are absent

## Confidence
- **High confidence**: Layer extension technique (insertion vs. appending) and its impact on performance - directly demonstrated with ablation results showing 73.1 vs 9.4 MMMLU
- **Medium confidence**: Two-stage pretraining strategy - mechanism is sound but exact implementation details are missing, making replication challenging
- **Medium confidence**: Multilingual SFT effectiveness - shown to outperform English-only SFT, but impact of instruction alignment across 25 languages is not fully characterized

## Next Checks
1. Replicate layer extension ablation on Qwen2.5-3B, measuring MMMLU recovery after 1B tokens to verify architectural claims hold across scales
2. After layer extension, train on English/Chinese-only corpus vs. full multilingual corpus for 5B tokens, comparing checkpoint stability and recovery speed to isolate effect of data diversity
3. Train three Babel-9B-Chat variants (English-only, multilingual with 40% English, multilingual with 20% English) and evaluate on XCOPA/XNLI per-language breakdown to quantify trade-off between language coverage and instruction alignment quality