---
ver: rpa2
title: Learning Decentralized LLM Collaboration with Multi-Agent Actor Critic
arxiv_id: '2601.21972'
source_url: https://arxiv.org/abs/2601.21972
tags:
- arxiv
- agents
- multi-agent
- learning
- collaboration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops Multi-Agent Actor-Critic (MAAC) methods for
  optimizing decentralized LLM collaboration. The authors propose two approaches:
  CoLLM-CC with a centralized critic and CoLLM-DC with decentralized critics.'
---

# Learning Decentralized LLM Collaboration with Multi-Agent Actor-Critic

## Quick Facts
- **arXiv ID:** 2601.21972
- **Source URL:** https://arxiv.org/abs/2601.21972
- **Reference count:** 40
- **Primary result:** Centralized critic approach (CoLLM-CC) outperforms decentralized alternatives in LLM collaboration tasks, especially for long-horizon and sparse-reward scenarios

## Executive Summary
This paper develops Multi-Agent Actor-Critic (MAAC) methods for optimizing decentralized LLM collaboration through two approaches: CoLLM-CC with centralized critic and CoLLM-DC with decentralized critics. The authors demonstrate that while Monte Carlo methods and CoLLM-DC achieve comparable performance to CoLLM-CC in short-horizon and dense-reward settings, they significantly underperform in long-horizon or sparse-reward tasks. The centralized critic approach shows superior sample efficiency and lower variance in gradient estimates, particularly in challenging reward structures.

## Method Summary
The paper proposes two Multi-Agent Actor-Critic approaches for decentralized LLM collaboration. CoLLM-CC uses a centralized critic that observes all agents' states and actions, while CoLLM-DC employs decentralized critics where each agent has its own critic. Both methods extend standard actor-critic frameworks to multi-agent settings by addressing the non-stationarity problem through centralized training with decentralized execution. The authors implement these methods across writing, coding, and game-playing domains, comparing against Monte Carlo methods as a baseline.

## Key Results
- CoLLM-CC consistently outperforms Monte Carlo methods and CoLLM-DC, particularly in sparse-reward and long-horizon tasks
- Monte Carlo methods require substantially more samples to train effectively compared to actor-critic approaches
- CoLLM-DC struggles to converge in long-horizon or sparse-reward settings despite performing comparably in short-horizon tasks

## Why This Works (Mechanism)
The centralized critic in CoLLM-CC provides more accurate value estimates by observing the full system state, reducing variance in gradient estimates compared to decentralized approaches. This becomes particularly advantageous in sparse-reward scenarios where individual agents cannot easily attribute credit for success. The actor-critic framework enables more efficient learning by bootstrapping from value estimates rather than waiting for complete episode returns, which is critical for sample efficiency in long-horizon tasks.

## Foundational Learning
- **Multi-Agent Reinforcement Learning**: Understanding how multiple agents learn in shared environments; needed to handle non-stationarity and credit assignment in collaborative LLM systems
- **Actor-Critic Methods**: Framework combining policy optimization (actor) with value estimation (critic); needed for stable learning and sample efficiency compared to pure policy gradient methods
- **Centralized Training with Decentralized Execution**: Training architecture where critics access global information but policies remain local; needed to balance coordination benefits with practical deployment constraints
- **Reward Sparsity and Horizon Effects**: How reward structure impacts learning difficulty; needed to understand why certain methods fail in long-horizon or sparse-reward scenarios
- **Credit Assignment in Multi-Agent Systems**: Determining which agent's actions contributed to outcomes; needed to evaluate the core challenge these methods address

## Architecture Onboarding

**Component Map:**
LLM Agent 1 -> Actor Network 1 -> Action 1
LLM Agent 2 -> Actor Network 2 -> Action 2
...
Centralized Critic -> All States/Actions -> Value Estimates

**Critical Path:**
Experience collection → Centralized value estimation → Policy gradient computation → Parameter updates → Decentralized execution

**Design Tradeoffs:**
Centralized critic provides better value estimates and lower variance gradients but creates potential bottleneck; decentralized critics scale better but suffer from higher variance and poorer credit assignment

**Failure Signatures:**
Monte Carlo methods: slow convergence, high sample complexity; CoLLM-DC: unstable training, poor credit assignment; Centralized bottleneck: computational overhead, single point of failure

**First Experiments:**
1. Compare sample efficiency curves across methods on simple cooperative tasks
2. Test sensitivity to reward sparsity by gradually increasing sparsity levels
3. Evaluate scalability by increasing agent count and measuring performance degradation

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily limited to synthetic writing, coding, and game-playing tasks
- Lack of detailed ablation studies on reward structure impact
- Scalability concerns for centralized critic in larger agent populations not thoroughly explored
- Real-world deployment considerations and human-in-the-loop scenarios not addressed

## Confidence
- **CoLLM-CC outperforms alternatives**: High confidence for tested domains
- **Monte Carlo requires more samples**: Medium confidence, needs systematic analysis
- **CoLLM-DC convergence struggles**: Medium confidence, requires hyperparameter sensitivity analysis

## Next Checks
1. Conduct extensive ablation studies varying reward sparsity, horizon length, and agent count
2. Implement stress tests with larger agent populations (10+ agents) to evaluate scalability
3. Apply methods to real-world collaborative tasks with human-in-the-loop evaluation