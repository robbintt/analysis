---
ver: rpa2
title: Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech
  Generation and Robust Emotion Recognition
arxiv_id: '2509.08470'
source_url: https://arxiv.org/abs/2509.08470
tags:
- speech
- sparse
- merit
- expert
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving speech emotion
  recognition (SER) under noisy conditions, where background noise degrades performance.
  While speech enhancement (SE) can help, it often introduces artifacts that obscure
  emotional cues and increases computational overhead.
---

# Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition

## Quick Facts
- arXiv ID: 2509.08470
- Source URL: https://arxiv.org/abs/2509.08470
- Reference count: 40
- This paper addresses the challenge of improving speech emotion recognition (SER) under noisy conditions, where background noise degrades performance. While speech enhancement (SE) can help, it often introduces artifacts that obscure emotional cues and increases computational overhead. Multi-task learning (MTL) offers a promising alternative, but conventional shared-backbone models often suffer from gradient interference and representational conflicts between SE and SER tasks.

## Executive Summary
This paper proposes Sparse MERIT, a flexible multi-task learning framework that applies frame-wise expert routing over self-supervised speech representations to improve both speech emotion recognition (SER) and speech enhancement (SE) in noisy environments. The framework uses task-specific gating networks that dynamically select from a shared pool of experts for each frame, enabling parameter-efficient and task-adaptive representation learning. Experiments on the MSP-Podcast corpus demonstrate consistent performance improvements over baseline models on both SER and SE tasks, particularly under challenging noise conditions.

## Method Summary
The proposed method employs a two-phase training approach with WavLM Large as the backbone. First, the self-supervised learning backbone is frozen while task-specific heads (SER and SE) are trained separately. Then, the WavLM Transformer layers are unfrozen for joint fine-tuning. The core innovation is a Sparse Mixture-of-Experts (MoE) layer that processes concatenated representations from all 25 WavLM layers through frame-wise routing. Each frame is processed by exactly one expert selected by task-specific gating networks, with 3 experts shown to be optimal for SER performance. The SER head uses attentive statistics pooling and classification, while the SE head reconstructs log-spectrograms using a decoder architecture (referenced as BSSE-SE).

## Key Results
- Under -5 dB SNR, Sparse MERIT improves SER F1-macro by 12.0% over SE pre-processing baseline and 3.4% over naive MTL baseline
- For SE, Sparse MERIT improves segmental SNR (SSNR) by 28.2% over SE pre-processing baseline and 20.0% over naive MTL baseline
- Statistical significance achieved on unseen noise conditions, demonstrating robust generalizability
- Expert usage analysis shows high switching rates (approx. 30%) at -5dB SNR, indicating dynamic frame-level adaptation

## Why This Works (Mechanism)

### Mechanism 1
Frame-wise sparse routing resolves the feature heterogeneity conflict between Speech Enhancement (SE) and Emotion Recognition (SER) better than utterance-level routing or dense sharing. By using a Top-1 gating mechanism for each frame, the model assigns distinct processing pathways to local acoustic events, allowing isolation of noise-heavy frames for enhancement while preserving high-level semantic frames for emotion classification. This prevents the "averaging out" of task-specific features. The assumption is that optimal processing strategy varies significantly at the frame level rather than remaining uniform across an entire utterance. Evidence includes high switching rates at -5dB SNR and the paper's explicit comparison to sample-level routing limitations.

### Mechanism 2
Decoupled task-specific gating networks minimize negative transfer (gradient interference) in multi-task learning. The architecture employs two independent gating networks over a shared expert pool, allowing the SER head to prioritize experts capturing emotional prosody while the SE head prioritizes experts for noise suppression. This reduces conflict where shared weights are pulled in opposing directions by the loss function. The core assumption is that feature subspaces required for SE and SER are not perfectly overlapping and benefit from specialized projections. Evidence includes only ~40% agreement between SE and SER gates and the 3.4% F1-macro improvement over naive MTL.

### Mechanism 3
Concatenating multi-layer self-supervised learning (SSL) features provides the necessary bandwidth for the MoE to disentangle low-level noise from high-level emotion. The model concatenates hidden states from all 24 layers of WavLM into a high-dimensional vector before routing, granting experts access to both raw waveform details (early layers) and abstract contextual embeddings (deep layers). This allows gates to select appropriate level of abstraction for the task. The assumption is that information required to separate noise from emotion is distributed across the depth of the SSL model. Evidence includes expert usage analysis showing specialists based on noise/emotion and the paper's explicit definition of concatenation across all transformer layers.

## Foundational Learning

- **Concept: Gradient Interference (Negative Transfer) in MTL**
  - **Why needed here:** The paper explicitly positions itself against "naive MTL" where shared parameters receive conflicting gradient updates (e.g., SE pushing features toward denoising, SER pushing toward emotional saliency)
  - **Quick check question:** If the gradients for Task A push a weight positive and Task B push it negative, what happens to the convergence speed? (Answer: It slows or converges to a suboptimal compromise)

- **Concept: Sparse vs. Dense Routing (MoE)**
  - **Why needed here:** The paper compares "Sparse MERIT" (Top-1) vs. "Dense MERIT" (soft weights). Understanding sparsity is key to grasping why this model is computationally efficient and why it forces expert specialization
  - **Quick check question:** Why might "hard" routing (selecting exactly one expert) lead to better specialization than "soft" routing (weighted average of all experts)?

- **Concept: SSL Layer Hierarchy**
  - **Why needed here:** The input to the MoE is a concatenation of WavLM layers. One must understand that early layers retain acoustic detail (needed for SE) while deeper layers hold linguistic/contextual info (needed for SER)
  - **Quick check question:** Which layers of a speech SSL model would you expect to be most sensitive to background noise, and why might that be useful for the SE task?

## Architecture Onboarding

- **Component map:** Input (Noisy Waveform) -> WavLM Large (Frozen/Fine-tuned) -> Hidden States H0-H24 -> Concatenation Layer -> Sparse MoE Layer (3 Experts) -> Task-specific Gate Networks (G_SER, G_SE) -> SER Head (Attentive Statistics Pooling -> Classifier) and SE Head (Decoder -> Spectrogram Reconstruction)

- **Critical path:** The critical path for success is the Gate Network training. If the gates do not learn to differentiate between "noise" and "emotion" frames, the experts will receive mixed signals and fail to specialize. The two-phase training (freeze backbone -> joint fine-tuning) is essential to stabilize this.

- **Design tradeoffs:**
  - Top-1 vs. Dense: The paper chooses Top-1 (Sparse) for efficiency and specialization, noting Dense routing performed worse on SER
  - Expert Count: 3 experts were chosen as optimal for SER; more experts (up to 9) improved SE but hurt SER
  - Loss Balancing: The paper explicitly rejects "Expert Balancing Loss" because it hurt generalization on unseen noise

- **Failure signatures:**
  - Gate Collapse: All inputs route to a single expert (checking "Expert Usage" histograms prevents this)
  - Oscillation: The model fails to converge because SER and SE losses fight over shared WavLM backbone
  - Over-smoothing: Enhanced speech sounds "robotic" or emotion is stripped; checking SSNR vs. F1-macro trade-off is necessary

- **First 3 experiments:**
  1. Gate Disagreement Analysis: Calculate frame-level agreement rate between G_SER and G_SE on clean vs. noisy data. If agreement >80%, routing isn't specialized enough
  2. Noise Generalization Check: Train on CRSS noise (seen) and test on Freesound (unseen). Verify gap between Sparse MERIT and SE-P baseline widens, confirming robustness
  3. Layer Contribution Ablation: Feed only deep layers vs. only early layers to MoE to verify "Concatenation" mechanism utilizes full spectrum of features

## Open Questions the Paper Calls Out

- **Open Question 1:** Can a dynamic gating mechanism that determines the optimal number of active experts per frame improve performance over the fixed Top-1 selection strategy? The current implementation utilizes a static Top-1 routing strategy (K=1) for all frames, which enforces a constant sparsity level regardless of acoustic complexity or emotional content of a specific frame.

- **Open Question 2:** Does incorporating explicit shared experts alongside task-specific experts improve the balance between specialization and information sharing in the Sparse MERIT framework? The current architecture employs task-specific gating networks to select from a common pool of experts, but it does not distinguish between experts designated for shared low-level features versus those for task-specific high-level abstractions.

- **Open Question 3:** To what extent does the Sparse MERIT framework maintain its robustness and performance gains in reverberant environments and multilingual speech contexts? The experiments in this study were limited to additive noise conditions (babble, ambient) on English speech, explicitly excluding room impulse responses from the DNS dataset.

- **Open Question 4:** Is the Sparse MERIT architecture scalable and effective for speech-related multi-task learning scenarios beyond the SE-SER pairing, such as joint speech recognition or speaker identification? The current validation is restricted to the specific synergy between speech enhancement and emotion recognition.

## Limitations

- **Architectural Detail Gaps:** The paper references a "BSSE-SE model" for the speech enhancement decoder without providing explicit architectural specifications, creating significant reproducibility barriers.

- **Noise Generalization Evidence:** While the paper claims improved performance on unseen noise conditions, evaluation only considers two noise datasets, leaving robustness to other noise types untested.

- **Expert Capacity Analysis:** The paper selects 3 experts as optimal for SER but acknowledges that more experts improve SE performance, lacking systematic exploration of expert-to-task mapping efficiency.

## Confidence

- **High Confidence:** The core mechanism of frame-wise sparse routing improving task specialization is well-supported by experimental evidence showing high switching rates and expert usage analysis.
- **Medium Confidence:** The claim that concatenating multi-layer SSL features provides necessary bandwidth is plausible but relies on implicit assumptions about feature distribution across layers.
- **Low Confidence:** The generalizability of results to other speech datasets and noise conditions is uncertain given the limited evaluation scope.

## Next Checks

1. **Noise Generalization Test:** Train Sparse MERIT on CRSS noise and evaluate on a third, completely unseen noise dataset (e.g., DEMAND database) to verify robustness claims beyond the two tested datasets.

2. **Expert Capacity Scaling:** Systematically evaluate model performance with 1-9 experts to identify the optimal trade-off between SER and SE performance, particularly focusing on whether the 3-expert configuration remains optimal under more challenging noise conditions.

3. **Layer Contribution Ablation:** Conduct controlled experiments feeding only early layers, only deep layers, or various layer combinations to the MoE to quantify the contribution of each layer group to task performance and validate the concatenation approach.