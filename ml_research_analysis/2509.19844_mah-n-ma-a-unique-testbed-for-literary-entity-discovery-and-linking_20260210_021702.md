---
ver: rpa2
title: "Mah\u0101n\u0101ma: A Unique Testbed for Literary Entity Discovery and Linking"
arxiv_id: '2509.19844'
source_url: https://arxiv.org/abs/2509.19844
tags:
- entity
- dataset
- entities
- mentions
- linking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Mah\u0101n\u0101ma is the first large-scale dataset for Entity\
  \ Discovery and Linking (EDL) in Sanskrit, featuring 109K mentions across 5.5K entities\
  \ from the Mah\u0101bh\u0101rata. It captures extreme lexical variation, with one\
  \ entity appearing in over 1,300 distinct forms, and high ambiguity, where 47% of\
  \ entities share names requiring context-based resolution."
---

# Mahānāma: A Unique Testbed for Literary Entity Discovery and Linking

## Quick Facts
- arXiv ID: 2509.19844
- Source URL: https://arxiv.org/abs/2509.19844
- Reference count: 39
- Primary result: First large-scale Sanskrit Entity Discovery and Linking dataset with 109K mentions across 5.5K entities from the Mahābhārata

## Executive Summary
Mahānāma introduces the first large-scale dataset for Entity Discovery and Linking (EDL) in Sanskrit, featuring 109K mentions across 5.5K entities from the Mahābhārata. The dataset captures extreme lexical variation, with one entity appearing in over 1,300 distinct forms, and high ambiguity, where 47% of entities share names requiring context-based resolution. Current coreference and entity linking models struggle significantly, achieving only 51.57% F1 on global context evaluation and 64.19% F1 for end-to-end entity linking due to poor mention detection. These results highlight the challenges of resolving entities in complex literary narratives and underscore the need for more robust, context-aware resolution systems.

## Method Summary
The Mahānāma dataset consists of 109K named entity mentions across 5.5K entities from the Mahābhārata, provided as unsegmented Sanskrit verses with character-level mention spans. The evaluation framework uses both local (per-subchapter) and global (entire test set as one discourse) contexts, measuring coreference resolution with CoNLL metrics (MUC, B³, CEAFϕ4) and entity linking with InKB micro-F1. Three models were evaluated: LingMess and Dual-Cache for coreference (using Longformer-Large), and mReFiNeD for entity linking (using MuRIL encoder). The Dual-Cache model was specifically modified for subtoken-level boundary prediction to handle Multi-Word Tokens caused by Sanskrit sandhi.

## Key Results
- Best coreference model (Dual-Cache) achieves 51.57% F1 on global context vs. 74.76% locally
- End-to-end entity linking reaches 64.19% F1 due to weak mention detection (60.22% F1)
- Extreme surface-form variation: 2,187 unique forms for total mentions vs. ~400-500 in other datasets
- 47% of entities share names requiring context-based disambiguation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Morphological richness increases surface-form variation, which correlates with lower entity resolution performance.
- Mechanism: Sanskrit's inflectional morphology and sandhi (phonetic merging) create many surface forms per name. This raises the number of unique string forms a model must associate with a single entity, making cluster formation harder for both coreference and entity linking models.
- Core assumption: Models that rely on surface-form similarity for clustering or candidate generation are degraded by high surface-form variation.
- Evidence anchors: [abstract] notes Sanskrit is "morphologically rich and under-resourced"; Table 5 shows 2,187 surface forms; page 2 notes 39% of mentions occur within multi-word tokens due to sandhi.
- Break condition: If surface forms are normalized/lemmatized before modeling, the mechanism's impact should lessen.

### Mechanism 2
- Claim: Global context evaluation exposes weaknesses in models optimized for local (subchapter-level) contexts.
- Mechanism: Models trained on short documents learn local patterns but fail to generalize when evaluated across the full test set as one long discourse. The Dual-Cache model maintains local/global caches but still shows a sharp CEAFϕ4 drop globally (31.68% vs. 71.30% locally), indicating difficulty maintaining entity identity over long ranges.
- Core assumption: Long-range dependencies and bursty mention distribution require memory mechanisms beyond standard Transformer windows.
- Evidence anchors: [abstract] states models "struggle when evaluated on the global context"; Table 6 shows Dual-Cache Avg F1 drops from 74.76 (local) to 51.57 (global); Figure 2 illustrates bursty, overlapping mention spans.
- Break condition: If a model is explicitly trained on long documents with global entity tracking, the performance gap between local and global evaluation should narrow.

### Mechanism 3
- Claim: External knowledge (KB descriptions, entity types) provides modest gains for disambiguation but does not compensate for weak mention detection in end-to-end settings.
- Mechanism: mReFiNeD uses cross-lingual descriptions and types for disambiguation. With gold spans, it achieves 93.27% F1; end-to-end, EL F1 drops to 64.19% due to poor mention detection (60.22% F1). Ablations show descriptions contribute ~1.2 points; types contribute minimally.
- Core assumption: Mention detection and disambiguation are tightly coupled; errors in detection propagate and cannot be fully recovered by disambiguation modules.
- Evidence anchors: [abstract] reports "best F1 at 51.57% for coreference and 64.19% for entity linking"; Table 7 shows mention detection F1 of 60.22% for mReFiNeD vs. 83.86% for Dual-Cache; ablation w/o descriptions drops EL F1 from 64.19 to 62.98.
- Break condition: If mention detection is highly accurate, the marginal benefit of descriptions/types may become more pronounced.

## Foundational Learning

- Concept: **Morphological richness** (inflection, sandhi, free word order)
  - Why needed here: Sanskrit's morphology directly causes the extreme surface-form variation in the dataset.
  - Quick check question: Given the token "arjunāśvisutau", can you identify the two entity mentions it contains?

- Concept: **Entity Discovery and Linking (EDL)** vs. **Coreference Resolution (CR)**
  - Why needed here: The paper evaluates both; EDL includes mention detection and KB linking, while CR clusters mentions without a KB.
  - Quick check question: If a system correctly groups "Arjuna" and "Savyasācī" but does not link them to a KB, is this EDL or CR?

- Concept: **Long-range dependencies and burstiness in literary texts**
  - Why needed here: The Mahābhārata is a single long narrative; entities appear in bursts with long gaps, challenging models with limited context windows.
  - Quick check question: Why might a model perform well on a subchapter but poorly when evaluated on the full test set?

## Architecture Onboarding

- Component map:
  - Unsegmented Sanskrit verses -> Character-level mention spans -> Cluster IDs -> English KB descriptions
  - Coreference models: LingMess (mention-ranking) -> Dual-Cache (entity-ranking with local/global caches)
  - Entity Linking model: mReFiNeD (bi-encoder with MuRIL, entity types, cross-lingual descriptions)
  - Evaluation: CoNLL metrics for CR; InKB micro-F1 for EL; local vs. global evaluation

- Critical path:
  1. Adapt models to unsegmented text (use character-level spans or subtoken boundaries)
  2. Train on subchapters (local documents)
  3. Evaluate locally and globally to expose long-range dependency failures
  4. Analyze errors (conflation, division, missing/extra mentions)

- Design tradeoffs:
  - Subtoken vs. token-level boundary prediction: Subtoken improves MWT handling but requires model modification
  - Using KB descriptions/types: Small gains for disambiguation but adds dependency on external resources
  - Treating corpus as one discourse vs. splitting into documents: Splitting enables standard training but loses global context

- Failure signatures:
  - **Divided Entity**: Same entity split into multiple clusters (e.g., Draupadī variants clustered by surface form)
  - **Conflated Entity**: Different entities merged (e.g., "kaurava" referring to both Bhūri and Duryodhana)
  - **Missing/Extra Mentions**: High rates in mReFiNeD due to weak detection; in Dual-Cache global due to cache misalignment

- First 3 experiments:
  1. Run LingMess and Dual-Cache baselines with token and subtoken boundaries; report local vs. global CoNLL scores
  2. Train mReFiNeD end-to-end and with gold mentions; compare EL F1 and mention detection F1
  3. Conduct ablation: remove KB descriptions and entity types from mReFiNeD; measure impact on disambiguation and full EL

## Open Questions the Paper Calls Out
- Can models trained on the Mahābhārata's verse structure effectively generalize to modern or prose Sanskrit texts?
- How can mention detection accuracy be improved to close the performance gap with entity disambiguation?
- Can context-aware models resolve the "bursty" distribution of entities and maintain consistency over global narrative contexts?

## Limitations
- Annotation conventions introduce ambiguity regarding Multi-Word Token handling and entity variant linking
- Extreme lexical variation may be partly annotation-driven due to sandhi splitting without full specification
- Evaluation metrics combine local and global contexts without addressing potential distribution shifts between splits

## Confidence

- **High Confidence**: The core empirical finding that global context evaluation significantly degrades performance compared to local evaluation (31.68% vs. 71.30% CEAFϕ4 drop for Dual-Cache). The claim that morphological richness drives surface-form variation is mechanistically sound and directly tied to Sanskrit's sandhi rules.

- **Medium Confidence**: The claim that external knowledge (KB descriptions, types) provides only modest gains (64.19% to 62.98% EL F1 without descriptions) is supported by ablation results, but KB quality and completeness are not assessed.

- **Low Confidence**: The claim that mention detection is the primary bottleneck in end-to-end EL (60.22% F1 vs. 83.86% for Dual-Cache) conflates detection errors with disambiguation failures, as the models differ in architecture and training objectives beyond mention detection.

## Next Checks
1. Re-examine annotation guidelines to determine whether mentions within Multi-Word Tokens are split and annotated separately, and assess inter-annotator agreement on these cases.

2. Perform k-fold cross-validation on the subchapter splits to verify that the global/local performance gap is consistent across different train/dev/test partitions.

3. Manually sample 50 entities and their mentions to quantify how many variant forms are included as aliases in the KB, testing the assumption that KB incompleteness limits disambiguation gains.