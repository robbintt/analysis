---
ver: rpa2
title: 'CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective
  Multimodal and Multilingual Learning'
arxiv_id: '2512.00496'
source_url: https://arxiv.org/abs/2512.00496
tags:
- audio
- training
- multimodal
- cacara
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces CACARA, a multimodal and multilingual model\
  \ that leverages emergent alignment learning to efficiently integrate new modalities\
  \ and languages. Unlike traditional approaches that require costly retraining across\
  \ all modalities and languages, CACARA achieves multilingual capabilities by fine-tuning\
  \ the new modality only on English-aligned data, preserving the text encoder\u2019\
  s inherent cross-lingual features."
---

# CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning

## Quick Facts
- arXiv ID: 2512.00496
- Source URL: https://arxiv.org/abs/2512.00496
- Reference count: 8
- Improves R@1 by up to 14.24 percentage points over state-of-the-art models

## Executive Summary
CACARA introduces an efficient approach for integrating new modalities into multimodal models by leveraging emergent alignment learning. The model freezes pretrained image and text encoders from OpenCLIP and trains only a new audio encoder using English-aligned data, achieving zero-shot multilingual capabilities across over 100 languages. This text-centric approach enables seamless modality integration without full retraining, significantly reducing computational costs while maintaining strong performance in audio-text and audio-image retrieval tasks.

## Method Summary
CACARA builds upon the Locked-image Tuning (LiT) protocol, freezing pretrained OpenCLIP image and text encoders while training only a new BEATs audio encoder and linear projection layers. The model uses InfoNCE contrastive loss to align audio embeddings with English text embeddings, leveraging XLM-RoBERTa's inherent multilingual capabilities for zero-shot cross-lingual transfer. Training employs SpecAugment and Random Truncation augmentation on audio clips up to 10 seconds, with the audio encoder optimized using Adam (lr=5e-5) while all other components remain frozen.

## Key Results
- Achieves 14.24 percentage point improvement in R@1 for audio-text retrieval over state-of-the-art models
- Extends multilingual capabilities to over 100 languages without explicit multilingual training
- Reduces energy consumption by 73% and training time by 79% compared to fully tri-modal baselines

## Why This Works (Mechanism)

### Mechanism 1
Training only the new modality encoder against a frozen, pre-aligned text encoder yields implicit alignment with all modalities in the shared space. The text encoder (XLM-RoBERTa) and image encoder (ViT) are already co-aligned via OpenCLIP pretraining, forming a joint embedding space. When audio is contrastively trained against text only, the gradient flow shapes audio representations to land within this pre-existing space—automatically positioning audio near semantically related images without any audio-image training pairs.

### Mechanism 2
Multilingual capabilities transfer from a frozen multilingual text encoder to a monolingual-trained modality encoder. XLM-RoBERTa's pretraining on 100+ languages creates language-agnostic semantic embeddings. By freezing it and training audio only on English audio-text pairs, the audio encoder learns to produce embeddings that match English text semantics. At inference, non-English text queries produce similar embeddings (due to XLM-RoBERTa's cross-lingual alignment), enabling retrieval without multilingual audio training.

### Mechanism 3
Freezing pretrained encoders while only training projection layers and the new modality encoder preserves alignment quality while reducing computational cost. The modified LiT protocol freezes image and text encoders with their learned representations intact. Only the audio encoder (BEATs) and linear projection layers are optimized via InfoNCE loss. This prevents catastrophic forgetting and reduces trainable parameters by ~19%.

## Foundational Learning

- **Concept: Contrastive Learning (InfoNCE Loss)**
  - Why needed here: The entire alignment mechanism relies on contrastive objectives to pull matched audio-text pairs together and push non-matched pairs apart in embedding space.
  - Quick check question: Can you explain why increasing the batch size improves contrastive learning quality?

- **Concept: Multilingual Text Encoders (XLM-RoBERTa)**
  - Why needed here: Understanding how XLM-RoBERTa creates language-agnostic embeddings is critical for diagnosing cross-lingual transfer failures.
  - Quick check question: Why might similar sentences in English and Swahili produce embeddings that are farther apart than English and Portuguese?

- **Concept: Vision Transformers (ViT) and Audio Spectrogram Transformers**
  - Why needed here: The model uses ViT for images and BEATs (audio tokenizer + transformer) for audio; understanding their tokenization schemes helps debug embedding dimension mismatches.
  - Quick check question: How does patch embedding in ViT compare to spectrogram patch embedding in BEATs?

## Architecture Onboarding

- **Component map:** OpenCLIP ViT (frozen) -> OpenCLIP XLM-RoBERTa (frozen) -> BEATs audio encoder (trainable) -> Linear projection layers (trainable) -> Shared embedding space

- **Critical path:**
  1. Load OpenCLIP pretrained image and text encoders → freeze immediately
  2. Load BEATs audio encoder with pretrained weights → set as trainable
  3. Initialize linear projection layers for all three modalities
  4. Training loop: forward pass through all encoders → project to shared dim → compute InfoNCE between audio and text only
  5. Backpropagation updates only audio encoder and projections

- **Design tradeoffs:**
  - Text vs. Image as anchor: Text enables multilingual transfer; images do not. Paper claims this as novel vs. ImageBind.
  - BEATs vs. other audio encoders: Table 2 shows BEATs outperforms AudioMAE, HTS-AT, MAE-AST on retrieval; ablation is required if switching.
  - Data filtering threshold: f0.2 (20% CLIP similarity filter) improved ClothoV2 R@1 from 14.39% to 17.26% (Table 6), but degraded AudioCaps in some configurations.

- **Failure signatures:**
  - Low R@1 on specific languages (e.g., Swahili <2%): Indicates text encoder's weak representation for that language; cannot be fixed without multilingual audio data or better text encoder.
  - Audio-image retrieval fails completely: Check if text-image alignment from OpenCLIP was corrupted (should not happen if frozen correctly).
  - Training loss plateaus early: Verify learning rate schedule (CosineWarmupLR with max 5e-5); check batch size is at least 64.

- **First 3 experiments:**
  1. **Sanity check:** Train audio encoder on AudioCaps only (English) and evaluate audio-text retrieval on AudioCaps test set. Target: R@1 > 30% (Table 6).
  2. **Multilingual probe:** Without any code changes, run inference on translated ClothoV2 test queries in Portuguese, Spanish, German. Target: R@1 within 50% of English performance.
  3. **Emergent alignment test:** Evaluate audio-to-image retrieval on VGG-Sound test subset (Figure 5). This should work zero-shot; if R@1 < 5%, check that image encoder remains frozen and projection dimensions match.

## Open Questions the Paper Calls Out

### Open Question 1
Can the CACARA framework's emergent alignment strategy be effectively extended to integrate modalities beyond audio, such as video, thermal, wearable sensor, and depth data? The authors explicitly state in the Limitations section that the study was limited to audio and that "Extending the framework to incorporate additional modalities... would provide a more comprehensive assessment of the model’s scalability."

### Open Question 2
How does CACARA's performance and efficiency scale with increased model parameterization compared to fully tri-modal baselines? The authors note their experimental design was "constrained to base-level encoder models" and call for a "systematic exploration of the relationship between model parameterization and performance gains."

### Open Question 3
How does the model perform on languages with greater typological diversity and extremely low digital resources compared to the 12 currently evaluated languages? The authors acknowledge that while they tested 12 languages, "the generalizability of these findings could be further strengthened by expanding the linguistic scope" to include more low-resource languages.

## Limitations

- Performance degrades significantly for low-resource languages due to reliance on frozen text encoder's multilingual capabilities
- Emergent alignment assumes frozen text-image alignment from OpenCLIP is sufficiently robust for new modalities
- Current validation limited to audio modality; extension to other modalities remains unproven

## Confidence

- **Cross-modal alignment mechanism:** High confidence (strong empirical evidence, 14.24 pp R@1 improvement, clear architectural details)
- **Multilingual transfer capability:** Medium confidence (theoretically sound but weaker empirical support for low-resource languages)
- **Computational efficiency claims:** High confidence (specific quantitative evidence with clear baselines)

## Next Checks

1. **Emergent alignment robustness test:** Evaluate CACARA's zero-shot audio-image retrieval performance across diverse image domains to test whether frozen text-image alignment generalizes sufficiently for emergent audio-image alignment in unseen visual contexts.

2. **Low-resource language transfer analysis:** Systematically evaluate CACARA's performance across a gradient of languages from high-resource to low-resource to quantify exactly where and why multilingual transfer fails, and whether fine-tuning even a small subset of low-resource audio data improves results.

3. **Component freezing ablation:** Conduct a controlled experiment where the image encoder is unfrozen during audio training to determine whether this improves or degrades audio-text and audio-image retrieval performance, testing the assumption that frozen encoders are optimal for emergent alignment.