---
ver: rpa2
title: 'Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised
  Learning'
arxiv_id: '2502.07401'
source_url: https://arxiv.org/abs/2502.07401
tags:
- learning
- education
- course
- educational
- chatbot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Enhancing Higher Education with Generative AI: A Multimodal Approach for Personalised Learning

## Quick Facts
- arXiv ID: 2502.07401
- Source URL: https://arxiv.org/abs/2502.07401
- Authors: Johnny Chan; Yuming Li
- Reference count: 11
- Key outcome: Proof-of-concept multimodal educational chatbot demonstrated

## Executive Summary
This paper presents a proof-of-concept multimodal educational chatbot designed to enhance higher education through personalized learning. The system integrates three modules: a text-based Q&A system fine-tuned on course content, an image-based Q&A module for diagram-to-code conversion, and a file-based analyser for processing PDF course evaluations. Built using ChatGPT API fine-tuning, Google Bard API for image analysis, and various NLP models, the chatbot is delivered through a Gradio web interface. The work demonstrates technical feasibility but lacks empirical evaluation of learning outcomes.

## Method Summary
The research builds a multimodal educational chatbot with three integrated modules. The text module uses ChatGPT API fine-tuning on anonymized course Q&A data formatted as JSONL. The image module leverages Google Bard API to convert educational diagrams (e.g., ERDs) into executable code. The file module processes PDF course evaluations using text extraction, sentiment analysis, emotion classification based on Plutchik's wheel, keyword extraction, and summarization. The system is packaged as a Gradio web interface with three tabs for different input modalities. No formal evaluation metrics are provided; the work focuses on technical implementation and interface demonstration.

## Key Results
- Proof-of-concept multimodal chatbot successfully built with three integrated modules
- Diagram-to-code conversion demonstrated with Entity-Relationship Diagram example
- File analyser produces sentiment distribution, emotion classification, keyword extraction, and summaries from PDF evaluations

## Why This Works (Mechanism)
The system leverages multimodal generative AI capabilities to address diverse educational needs through specialized processing pipelines. By fine-tuning language models on course-specific content, the system provides contextually relevant responses. The integration of image analysis enables conversion of visual learning materials into executable code, bridging the gap between conceptual understanding and practical implementation. The file analysis component provides educators with automated insights from student feedback, enabling data-driven pedagogical improvements. The Gradio interface provides accessible, real-time interaction across all modalities.

## Foundational Learning
- **Fine-tuning ChatGPT on course Q&A data** - Needed to create contextually relevant responses for specific courses; quick check: verify JSONL format compliance and test inference quality on held-out questions
- **Diagram-to-code conversion using multimodal LLMs** - Needed to transform visual learning materials into practical implementations; quick check: validate SQL code output against original ERD structure
- **PDF text extraction and NLP analysis pipeline** - Needed to automate sentiment and emotion analysis from student feedback; quick check: compare extracted text against original PDF content for accuracy
- **Gradio web interface development** - Needed to provide accessible, real-time multimodal interaction; quick check: test all three input modalities (text, image, file) independently
- **Plutchik's emotion classification model** - Needed to provide nuanced emotional insights from student feedback; quick check: verify emotion distribution sums to 100% across samples
- **Keyword extraction and summarization techniques** - Needed to identify key themes and provide concise feedback summaries; quick check: manually verify top keywords against document content

## Architecture Onboarding
**Component Map:** Gradio UI -> ChatGPT API (text) -> Google Bard API (image) -> NLP pipeline (file) -> Visualizations
**Critical Path:** User input → API call → Processing → Response generation → UI display
**Design Tradeoffs:** Fine-tuning vs. RAG for text module (fine-tuning requires data but provides better context); cloud API dependency vs. local deployment (cloud offers better performance but requires internet)
**Failure Signatures:** Text module returns irrelevant answers (fine-tuning data insufficient); image module produces incorrect code (complex diagram handling inadequate); file module misclassifies sentiments (NLP model not suited for educational domain)
**First Experiments:** 1) Test text module with simple course questions and measure response relevance; 2) Convert basic ERD diagram to SQL and verify syntactic correctness; 3) Process sample PDF evaluations and validate sentiment/emotion distributions against manual annotation

## Open Questions the Paper Calls Out
**Open Question 1:** To what extent can the multimodal system accurately convert diverse diagram styles and formats into executable code across different disciplines? The proof-of-concept demonstrated diagram-to-code capability using only a single Entity-Relationship Diagram example, leaving robustness against stylistic variations untested. Quantitative evaluation across diagrams from various software tools would resolve this.

**Open Question 2:** Does the integration of multimodal inputs yield measurable improvements in student learning outcomes and engagement compared to unimodal baselines? The technical implementation is detailed but lacks empirical data on pedagogical effectiveness. A controlled study comparing student performance and engagement metrics would provide evidence.

**Open Question 3:** How accurately does the file-based analyser perform sentiment and emotion classification compared to human-coded ground truth? The methodology describes using NLP and Plutchik's wheel but presents only raw model outputs without validation. Confusion matrix or F1-score from comparison with human-labeled data would establish reliability.

## Limitations
- No code repository or specific API endpoints provided for reproduction
- Fine-tuning dataset details withheld, including size, domain, and preprocessing steps
- Absence of quantitative evaluation metrics or validation results
- No empirical evidence on learning outcomes or engagement improvements
- Limited demonstration of system robustness across diverse inputs

## Confidence
- High confidence in architectural description and intended functionality
- Medium confidence in technical feasibility based on specified APIs and components
- Low confidence in practical performance without evaluation data

## Next Checks
1. Test diagram-to-code conversion with progressively complex ER diagrams and validate syntactic correctness of generated SQL
2. Implement the file analysis pipeline with synthetic PDF course evaluations and evaluate sentiment/emotion classification accuracy against human annotations
3. Compare ChatGPT fine-tuning performance versus retrieval-augmented generation using the same course dataset