---
ver: rpa2
title: Automatic Generation of Inference Making Questions for Reading Comprehension
  Assessments
arxiv_id: '2506.08260'
source_url: https://arxiv.org/abs/2506.08260
tags:
- questions
- inference
- item
- question
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study explored using GPT-4o to automatically generate inference-making\
  \ reading comprehension questions for grades 3-12. A taxonomy of three bridging\
  \ inference types\u2014pronominal bridging, text-connecting, and gap-filling\u2014\
  was developed and validated against an operational item bank, where bridging inferences\
  \ comprised 51% of items."
---

# Automatic Generation of Inference Making Questions for Reading Comprehension Assessments

## Quick Facts
- arXiv ID: 2506.08260
- Source URL: https://arxiv.org/abs/2506.08260
- Reference count: 20
- Study found 93.8% of GPT-4o generated questions were of good operational quality

## Executive Summary
This study explored using GPT-4o to automatically generate inference-making reading comprehension questions for grades 3-12. The researchers developed a taxonomy of three bridging inference types—pronominal bridging, text-connecting, and gap-filling—and validated it against an operational item bank where bridging inferences comprised 51% of items. Using few-shot prompting with training passages, GPT-4o generated 357 questions that were evaluated for quality and inference type accuracy. While the majority of generated questions met quality standards, the system struggled to accurately target specific inference types, particularly text-connecting inferences.

## Method Summary
The study employed few-shot prompting with GPT-4o, using 4 or 6 training passages to generate inference-making questions. Researchers developed a three-category taxonomy of bridging inferences (pronominal bridging, text-connecting, and gap-filling) and validated it against an operational item bank. Generated questions were evaluated by human experts for operational quality and accuracy in targeting the intended inference type. The evaluation process involved analyzing the distribution of generated items across inference categories and comparing them to the operational item bank.

## Key Results
- 93.8% of generated questions were rated as good operational quality by human evaluators
- Only 42.6% of questions matched their targeted inference type, with text-connecting inferences showing the lowest accuracy at 24%
- The overall distribution of generated items closely mirrored the operational item bank despite inference type mismatches
- Gap-filling questions were easiest to generate (60% match accuracy) while text-connecting questions were most difficult

## Why This Works (Mechanism)
The study demonstrates that large language models can generate high-quality reading comprehension questions when given appropriate training examples and evaluation criteria. The success in producing operational-quality questions suggests that GPT-4o has sufficient understanding of reading comprehension concepts to create valid assessment items. The challenge in targeting specific inference types indicates that while the model can produce questions that fit assessment needs, precise control over cognitive demand remains difficult.

## Foundational Learning
- Bridging inference taxonomy: Understanding different types of inference skills needed for reading comprehension (why needed: to create targeted assessments; quick check: can classify items into correct categories)
- Few-shot prompting: Using limited examples to guide model behavior (why needed: to teach GPT-4o the desired output format and quality standards; quick check: prompts produce consistent results across runs)
- Operational quality standards: Criteria for valid assessment questions (why needed: to ensure generated questions are suitable for real testing contexts; quick check: questions meet established psychometric standards)

## Architecture Onboarding
Component map: Training passages -> GPT-4o model -> Generated questions -> Human evaluation -> Quality assessment
Critical path: Training data preparation → Prompt engineering → Question generation → Human review → Quality validation
Design tradeoffs: Balancing automation speed with human review quality control vs. fully manual development
Failure signatures: Inference type mismatches, quality inconsistencies, prompt sensitivity to training examples
First experiments: 1) Test different prompt structures with same training data, 2) Compare generation quality across grade levels, 3) Evaluate inter-rater reliability among human evaluators

## Open Questions the Paper Calls Out
None

## Limitations
- Human expert evaluation introduces subjectivity and potential variability in quality judgments
- Validation limited to one assessment provider's item bank, reducing generalizability
- Significant mismatches between intended and actual inference types, especially for text-connecting inferences
- Evaluation sample size of 357 questions may not capture full output range or edge cases

## Confidence
- High confidence in operational quality: 93.8% of questions rated good quality
- Medium confidence in scalability: Combining automatic generation with human review shows promise
- Low confidence in inference targeting: GPT-4o struggles to accurately produce specific inference types

## Next Checks
1. Test generation system across multiple assessment providers' item banks to evaluate generalizability
2. Conduct inter-rater reliability studies to quantify subjectivity and establish standardized evaluation criteria
3. Implement automated classification methods to verify inference types and compare against human classification accuracy