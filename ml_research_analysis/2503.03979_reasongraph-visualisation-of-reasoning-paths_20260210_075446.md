---
ver: rpa2
title: 'ReasonGraph: Visualisation of Reasoning Paths'
arxiv_id: '2503.03979'
source_url: https://arxiv.org/abs/2503.03979
tags:
- reasoning
- visualization
- platform
- reasongraph
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReasonGraph is a web-based platform for visualizing and analyzing
  LLM reasoning processes, supporting six sequential and tree-based reasoning methods
  across over 50 models. It addresses the challenge of analyzing complex LLM reasoning
  processes by providing real-time graphical visualization through a modular framework.
---

# ReasonGraph: Visualisation of Reasoning Paths

## Quick Facts
- arXiv ID: 2503.03979
- Source URL: https://arxiv.org/abs/2503.03979
- Authors: Zongqian Li; Ehsan Shareghi; Nigel Collier
- Reference count: 4
- Primary result: Web-based platform visualizing LLM reasoning processes across 6 methods with 100% parsing accuracy and 90% usability success rate

## Executive Summary
ReasonGraph is a web-based platform that provides real-time visualization of large language model (LLM) reasoning processes across six different reasoning methods. The platform addresses the challenge of analyzing complex reasoning chains by parsing LLM outputs into graphical representations using a rule-based XML parsing approach. It supports over 50 different models and integrates with major LLM providers including OpenAI, Anthropic, Google, and Together.AI.

The system enables comparative analysis of reasoning methods, supports educational applications in teaching logical reasoning, and assists in prompt engineering optimization and LLM application development. With nearly 100% parsing accuracy on properly formatted outputs and negligible visualization generation time compared to reasoning time, ReasonGraph provides an intuitive interface for understanding and debugging LLM reasoning processes.

## Method Summary
ReasonGraph employs a Flask backend with modular components including Configuration Manager, API Factory, and Reasoning Methods modules. The platform processes LLM outputs through rule-based XML parsing to extract reasoning paths, which are then visualized using Mermaid.js in the frontend. It supports six sequential and tree-based reasoning methods: Chain-of-Thoughts, Self-refine, Least-to-Most, Self-consistency, Tree-of-Thoughts, and Beam Search. The system connects to multiple LLM providers through a RESTful API layer, allowing users to compare reasoning methods across different models in real-time.

## Key Results
- Nearly 100% parsing accuracy for properly formatted LLM outputs
- Negligible visualization generation time compared to reasoning computation time
- Approximately 90% of users successfully use the platform without assistance

## Why This Works (Mechanism)
The platform works by leveraging rule-based XML parsing to extract structured reasoning paths from LLM outputs. When a user submits a query, the system sends it to the selected LLM with instructions to format the reasoning process in XML. The backend then parses this XML to identify nodes and edges in the reasoning graph, which are rendered using Mermaid.js. This approach allows for real-time visualization while maintaining flexibility to support multiple reasoning methods and models.

## Foundational Learning

**Rule-based XML parsing** - why needed: Essential for converting unstructured LLM reasoning text into structured graph representations. quick check: Verify XML output from LLM follows expected format with proper tags.

**Mermaid.js graph rendering** - why needed: Provides real-time visualization of reasoning paths in an intuitive flowchart format. quick check: Ensure graphs render correctly in browser console with no JavaScript errors.

**RESTful API architecture** - why needed: Enables modular communication between frontend, backend, and external LLM services. quick check: Test API endpoints respond correctly with valid HTTP status codes.

**Flask backend framework** - why needed: Provides lightweight, flexible web server for handling multiple simultaneous visualization requests. quick check: Verify Flask application starts without errors and serves UI correctly.

## Architecture Onboarding

**Component map:** User Interface -> Flask Backend -> LLM API Factory -> External LLM Providers

**Critical path:** User query submission → API Factory selects provider → LLM generates XML-formatted reasoning → Configuration Manager processes XML → Mermaid.js renders visualization

**Design tradeoffs:** The rule-based XML parsing approach trades flexibility for reliability - it requires strict output formatting but achieves high parsing accuracy. The modular architecture allows easy addition of new reasoning methods but adds complexity to the backend.

**Failure signatures:** Parsing errors when XML format deviates from expectations; API authentication failures when provider keys are invalid; Visualization failures when browser JavaScript is disabled or Mermaid.js encounters malformed graph definitions.

**First experiments:**
1. Submit a simple query using Chain-of-Thoughts method and verify basic graph renders in right panel
2. Compare reasoning visualization between two different models on identical query
3. Test Meta Reasoning button functionality to verify model-selected method differs from manual selection

## Open Questions the Paper Calls Out

**Open Question 1:** What is the quantitative impact of editable visualization nodes on reasoning process modification and final output quality?
The feature is planned but not implemented; no empirical data exists on how direct graph manipulation affects downstream reasoning outcomes.

**Open Question 2:** What are the failure modes of rule-based XML parsing when models produce malformed outputs, and what recovery strategies are effective?
Only success cases are reported; behavior under non-compliant model outputs remains uncharacterized.

**Open Question 3:** How effectively does the Meta Reasoning feature select appropriate reasoning methods compared to human expert selection?
No comparative data exists on model-selected versus manually-selected reasoning method performance across task types.

**Open Question 4:** How does ReasonGraph compare to existing visualization tools for error detection in LLM reasoning?
The error detection improvement claim lacks comparative metrics against alternative visualization approaches.

## Limitations

- Unspecified Python version requirements and exact dependency versions needed for reproduction
- Parsing accuracy claim is qualified to "well-formatted outputs" without defining proper formatting standards
- Usability study based on only 10 participants, providing limited statistical significance

## Confidence

- **High Confidence**: Core architecture (Flask backend, Mermaid.js frontend, RESTful API) and multiple reasoning method visualization capabilities are well-supported by description and GitHub repository
- **Medium Confidence**: Performance metrics (parsing accuracy, visualization speed, usability) lack methodological rigor in documentation
- **Low Confidence**: Educational application claims and broader utility assertions are supported by anecdotal evidence rather than systematic evaluation

## Next Checks

1. Test parsing accuracy with intentionally malformed LLM outputs to determine robustness boundaries beyond "well-formatted" cases
2. Conduct usability testing with a larger, more diverse participant pool (n>30) to validate the 90% success rate claim
3. Benchmark visualization generation time across different reasoning method complexities and graph sizes to confirm "negligible" performance claims under various load conditions