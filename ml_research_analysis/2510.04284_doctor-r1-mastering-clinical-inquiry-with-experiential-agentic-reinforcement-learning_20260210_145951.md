---
ver: rpa2
title: 'Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement
  Learning'
arxiv_id: '2510.04284'
source_url: https://arxiv.org/abs/2510.04284
tags:
- agent
- patient
- medical
- reward
- experience
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Doctor-R1 addresses the gap between static medical decision benchmarks
  and dynamic clinical inquiry by training an AI doctor agent to master both strategic
  multi-turn inquiry and medical decision-making. It introduces Experiential Agentic
  Reinforcement Learning, which combines a multi-agent interactive environment, a
  two-tiered reward architecture for conversational quality and diagnostic accuracy,
  and an experience repository that guides policy learning from high-quality prior
  trajectories.
---

# Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2510.04284
- **Source URL**: https://arxiv.org/abs/2510.04284
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art results on HealthBench and MAQuE benchmarks with 9.4% improvement over best open-source models

## Executive Summary
Doctor-R1 addresses the gap between static medical decision benchmarks and dynamic clinical inquiry by training an AI doctor agent to master both strategic multi-turn inquiry and medical decision-making. It introduces Experiential Agentic Reinforcement Learning, which combines a multi-agent interactive environment, a two-tiered reward architecture for conversational quality and diagnostic accuracy, and an experience repository that guides policy learning from high-quality prior trajectories. Evaluated on HealthBench and MAQuE, Doctor-R1 significantly outperforms both open-source specialized LLMs and powerful proprietary models, achieving state-of-the-art results with higher parameter efficiency.

## Method Summary
Doctor-R1 uses a multi-agent reinforcement learning framework where a doctor agent interacts with patient agents in a simulated clinical environment. The system employs Experiential Agentic Reinforcement Learning with a two-tiered reward architecture separating process rewards (for communication quality) from outcome rewards (for diagnostic accuracy). A three-stage experience retrieval system provides high-quality prior trajectories to guide policy learning, while a hierarchical veto system ensures safety. The policy is trained using Group Relative Policy Optimization (GRPO) on a Qwen3-8B backbone, with performance evaluated on HealthBench and MAQuE benchmarks.

## Key Results
- Achieves 47.16 Communication score on HealthBench, surpassing best open-source model by 9.4%
- Matches top proprietary models in diagnostic accuracy while demonstrating superior empathy
- Human expert evaluations confirm clinical competence and patient-centric performance
- Demonstrates 30% higher parameter efficiency compared to baseline approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating process rewards from outcome rewards enables learning both clinical communication and diagnostic accuracy.
- Mechanism: A two-tiered reward architecture provides dense turn-by-turn feedback (R_turn) on 8 dimensions via weighted sum, while a terminal outcome reward (R_final) evaluates final diagnosis correctness. A hierarchical veto system overrides all scores if safety, reasoning, or accuracy thresholds are violated.
- Core assumption: Soft skills (empathy, clarity) and hard skills (diagnostic accuracy) can be jointly optimized without interference when rewarded separately.
- Evidence anchors: Abstract mentions "two-tiered reward architecture," section 3.2 describes veto penalties, but limited direct corpus evidence for this specific structure.

### Mechanism 2
- Claim: Retrieving high-reward prior trajectories improves policy learning beyond standard semantic similarity retrieval.
- Mechanism: A three-stage retrieval pipeline—(1) candidate selection via combined similarity + reward score, (2) cross-encoder reranking, (3) novelty and dynamic reward filtering—selects "good experiences" to prepend to current query.
- Core assumption: High-reward experiences from different clinical contexts transfer useful strategic patterns to new cases.
- Evidence anchors: Abstract mentions "experience repository," Table 9 shows ablation results demonstrating retrieval contribution, but no direct corpus evidence for this specific mechanism.

### Mechanism 3
- Claim: Multi-agent simulation creates a sufficiently diverse training environment for learning adaptive inquiry policies.
- Mechanism: A POMDP environment with Doctor Agent, Patient Agent, and Consultation Evaluator provides closed-loop feedback for policy training.
- Core assumption: Simulated patient interactions transfer to real clinical scenarios despite domain shift.
- Evidence anchors: Abstract mentions "multi-agent interactive environment," section 3.1 describes the closed-loop system, but UCAgents reference lacks direct validation of patient simulation fidelity.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: Optimizes policy by contrasting high-quality actions against rejection groups, eliminating critic network variance issues common in PPO for long-horizon dialogue.
  - Quick check question: Can you explain why GRPO's listwise comparison is more stable than PPO's scalar value estimation for multi-turn dialogue?

- **Partially Observable Markov Decision Process (POMDP)**
  - Why needed here: Models clinical inquiry where the agent cannot directly observe the true patient state, only dialogue history.
  - Quick check question: How does POMDP differ from standard MDP in the context of diagnostic uncertainty?

- **Hierarchical Veto Systems in Safe RL**
  - Why needed here: Prevents catastrophic medical errors by overriding rewards when safety constraints are violated, regardless of other dimension scores.
  - Quick check question: What happens to the gradient signal when a veto is triggered? How does this affect policy updates?

## Architecture Onboarding

- **Component map**: Patient agent generates initial query → Doctor agent observes state + retrieves top-k experiences → Doctor generates action with CoT → Evaluator computes R_turn → If R_turn ≥ τ_reward, store tuple → GRPO updates policy → Repeat until diagnosis or max turns

- **Critical path**: Patient agent generates initial query → Doctor agent observes state + retrieves top-k experiences → Doctor generates action with CoT → Evaluator computes R_turn → If R_turn ≥ τ_reward, store tuple → GRPO updates policy → Repeat until diagnosis or max turns

- **Design tradeoffs**:
  - Retrieval overhead: +4.74s latency per inference for 7.76% Communication gain
  - Veto strictness: ε=0 threshold catches all negative safety scores but may over-penalize borderline cases
  - Experience storage: Only high-reward tuples stored reduces noise but limits negative-example learning

- **Failure signatures**:
  - Reward hacking: Repetitive empathetic statements without clinical content—detected by manual spot-checks
  - Patient persona break: Generated responses like "As an LLM"—filtered by rule-based checks
  - Retrieval misalignment: Retrieved action suggests wrong diagnostic path—mitigated by novelty filtering

- **First 3 experiments**:
  1. Ablate experience retrieval: Compare "No Experience" vs. "Similarity Only" vs. "Full Method" on HealthBench Communication axis
  2. Validate veto system: Train without hierarchical veto, measure Safety Violation Rate on Emergency Referrals subset
  3. Scale patient agent diversity: Train with 0, 10k, 30k, 50k, 100k simulated patients to establish scaling relationship

## Open Questions the Paper Calls Out
None

## Limitations

- **Simulation-to-Real Gap**: Strong performance on synthetic benchmarks but limited evidence for real-world clinical transferability, with 13.82% patient persona break rate suggesting simulation fidelity issues
- **Safety Threshold Rigidity**: ε=0 veto threshold may be overly conservative, potentially blocking legitimate diagnostic approaches without exploring threshold optimization
- **Generalization Across Medical Domains**: Performance metrics show strong general results but limited evidence for specialization across diverse medical domains or analysis of performance variance across specialties

## Confidence

- **High Confidence**: Claims about framework architecture and component integration (Multi-agent environment, two-tiered reward system, experience repository mechanics) are well-supported by detailed implementation descriptions and ablation studies
- **Medium Confidence**: Claims about Doctor-R1's superiority over baselines (9.4% improvement over open-source models) are supported by benchmark results but limited to synthetic datasets
- **Low Confidence**: Claims about real-world clinical competence and patient safety are primarily based on simulated evaluations and human expert assessments of benchmark performance, rather than actual clinical deployment

## Next Checks

1. **Real-World Clinical Deployment Pilot**: Deploy Doctor-R1 in a controlled clinical setting with actual patient interactions to validate whether simulation-trained performance transfers to real medical consultations, measuring safety incidents, diagnostic accuracy, and patient satisfaction against human physicians.

2. **Cross-Domain Specialty Testing**: Evaluate Doctor-R1 across diverse medical specialties (emergency medicine, rare diseases, pediatrics, geriatrics) using domain-specific benchmarks and expert evaluations to reveal whether strong general performance extends to specialized medical knowledge and complex diagnostic scenarios.

3. **Longitudinal Safety Analysis**: Conduct extended safety monitoring across multiple interaction sessions to detect whether the hierarchical veto system's conservative approach might accumulate errors or whether patients' health states deteriorate due to delayed or incorrect interventions, comparing against alternative threshold strategies.