---
ver: rpa2
title: Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning
arxiv_id: '2510.05753'
source_url: https://arxiv.org/abs/2510.05753
tags:
- learning
- lira
- efficacy
- attacks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the performance of diverse membership inference
  attacks (MIAs) against models fine-tuned with transfer learning, addressing the
  gap in prior assessments that relied on a limited subset of attacks. The authors
  systematically evaluate score-based MIAs, including shadow-model-based (ML-Leaks,
  LiRA, Trajectory-MIA, RMIA) and shadow-model-free (LOSS, Attack-P, QMIA, IHA) methods,
  using consistent experimental setups across CIFAR-10, CIFAR-100, and PatchCamelyon
  datasets.
---

# Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning

## Quick Facts
- arXiv ID: 2510.05753
- Source URL: https://arxiv.org/abs/2510.05753
- Reference count: 40
- Primary result: LiRA demonstrates the most robust and superior performance across most experimental scenarios in deep transfer learning

## Executive Summary
This study provides the first systematic empirical comparison of diverse membership inference attacks (MIAs) against models fine-tuned with transfer learning. The authors evaluate both shadow-model-based (ML-Leaks, LiRA, Trajectory-MIA, RMIA) and shadow-model-free (LOSS, Attack-P, QMIA, IHA) methods across multiple datasets and training scenarios. Their comprehensive analysis reveals that attack efficacy generally follows a power-law relationship with training data size, though with notable exceptions like the white-box Inverse Hessian Attack (IHA) showing superior performance in high-shot regimes on medical imaging data. The study also finds that different fine-tuning parameterization schemes show minimal differences in attack efficacy for the strongest attacks.

## Method Summary
The authors conduct a systematic evaluation of score-based MIAs against fine-tuned models using consistent experimental setups. They employ three datasets (CIFAR-10, CIFAR-100, and PatchCamelyon) and three fine-tuning parameterization schemes (Head-only, FiLM, ALL). The study compares both shadow-model-based attacks (ML-Leaks, LiRA, Trajectory-MIA, RMIA) and shadow-model-free attacks (LOSS, Attack-P, QMIA, IHA) across varying training data sizes (shots). Performance is measured using TPR@FPR metrics, with power-law relationships fitted to understand how attack efficacy scales with training data. The experiments systematically vary the number of fine-tuning examples per class and evaluate attack transferability across datasets.

## Key Results
- Attack efficacy generally decreases with increasing training data following a power-law relationship, though IHA shows superior performance in high-shot regimes on PatchCamelyon
- LiRA demonstrates the most robust and superior performance across most experimental scenarios and datasets
- Different parameterization schemes (Head-only, FiLM, ALL) show minimal differences in MIA efficacy for the strongest attacks

## Why This Works (Mechanism)
The study's empirical findings reveal that membership inference attack performance in transfer learning follows predictable patterns based on training data availability and attack methodology. Shadow-model-based attacks like LiRA maintain consistent performance across diverse scenarios by effectively learning the decision boundary differences between member and non-member data. The white-box IHA attack leverages access to model internals to achieve superior performance in specific high-data regimes, particularly for medical imaging tasks where feature representations may be more distinctive.

## Foundational Learning

### Membership Inference Attacks
**Why needed:** Understanding the threat model where attackers infer whether specific data points were used to train a model is fundamental to evaluating privacy risks in machine learning systems.
**Quick check:** Verify that the attack methodology correctly distinguishes between training (member) and non-training (non-member) data distributions.

### Transfer Learning
**Why needed:** The study focuses on fine-tuned models, making it essential to understand how knowledge transfer from pre-trained models affects vulnerability to privacy attacks.
**Quick check:** Confirm that the pre-trained backbone remains frozen during fine-tuning and that only specified layers are updated.

### Power-Law Relationships
**Why needed:** The authors establish that attack efficacy follows power-law scaling with training data, which has implications for predicting privacy risks at different model scales.
**Quick check:** Validate that the fitted power-law curves accurately capture the relationship between training shots and attack performance across all tested scenarios.

## Architecture Onboarding

### Component Map
CIFAR-10/CIFAR-100/PatchCamelyon datasets -> Pre-trained backbone (e.g., ResNet) -> Fine-tuning parameterization (Head-only/FiLM/ALL) -> Trained model -> Membership Inference Attacks (shadow-model-based and shadow-model-free)

### Critical Path
Dataset selection → Pre-trained model loading → Fine-tuning with specified parameterization → Attack evaluation → Performance measurement and comparison

### Design Tradeoffs
The study balances comprehensive attack comparison against computational constraints by limiting fine-tuning schemes and focusing on score-based attacks rather than exploring all possible attack vectors or model architectures.

### Failure Signatures
Attack performance degradation with increasing training data, inconsistent results across different datasets or parameterizations, and computational bottlenecks in running extensive attack evaluations.

### First 3 Experiments
1. Baseline comparison of LiRA performance across CIFAR-10, CIFAR-100, and PatchCamelyon with Head-only fine-tuning
2. Power-law fitting analysis to quantify the relationship between training shots and attack efficacy
3. Cross-dataset transferability evaluation comparing attack performance when training and target datasets differ

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does class imbalance in the fine-tuning dataset alter the efficacy of shadow-model-based versus shadow-model-free MIAs in transfer learning?
- **Basis in paper:** [explicit] The authors state in the Limitations section that they focused on balanced datasets and that "Future work could extend this analysis to imbalanced datasets commonly found in real-world deployments."
- **Why unresolved:** The paper's established power-law relationship between shots and MIA efficacy relies on balanced S (examples per class), but real-world data rarely has uniform class distribution.
- **What evidence would resolve it:** Experiments comparing LiRA and IHA performance on long-tailed or imbalanced versions of CIFAR or medical imaging datasets.

### Open Question 2
- **Question:** Why does data augmentation fail to boost MIA performance in Head-only transfer learning, contrary to its effect in from-scratch training?
- **Basis in paper:** [inferred] Section 5.4 notes that augmentation produces "negligible performance improvements," a "significant departure from from-scratch training findings," suggesting a fundamental difference in vulnerability patterns.
- **Why unresolved:** The authors identify the divergence but do not investigate the theoretical or mechanistic reasons why the frozen backbone prevents augmentation from aiding the attack.
- **What evidence would resolve it:** An ablation study analyzing feature variance in frozen versus unfrozen backbones under augmentation to explain the lack of MIA signal improvement.

### Open Question 3
- **Question:** How does the relative efficacy of diverse MIAs differ between fine-tuned and from-scratch trained models across full hyperparameter spaces?
- **Basis in paper:** [explicit] The Limitations section notes the authors "do not offer a detailed comparison... beyond Figure 1" because running from-scratch experiments is "computationally expensive."
- **Why unresolved:** While Figure 1 suggests MIA behavior differs significantly between paradigms, the lack of comprehensive data leaves the specific conditions where transfer learning is more or less vulnerable undefined.
- **What evidence would resolve it:** A large-scale study measuring TPR@FPR curves for LiRA and RMIA on identical datasets trained both from scratch and via fine-tuning.

## Limitations
- Limited transferability experiments only consider CIFAR-10 → CIFAR-100 scenarios, leaving open questions about attack performance across different domain gaps
- Power-law relationship between training data size and attack efficacy may not hold for all model architectures or learning tasks beyond those tested
- Computational constraints prevented comprehensive comparison between fine-tuned and from-scratch trained models across full hyperparameter spaces

## Confidence
- **High confidence:** LiRA's robust performance across datasets - consistently replicated across multiple experimental conditions
- **Medium confidence:** IHA findings on PatchCamelyon - specific to medical imaging data with potential domain-specific factors
- **Medium confidence:** Minimal effect of parameterization schemes on strongest attacks - based on three specific fine-tuning strategies only

## Next Checks
1. Test attack transferability across larger domain gaps (e.g., natural images to medical imaging) to verify generalization of current findings
2. Evaluate the power-law relationship with different backbone architectures (e.g., ResNet, ViT) and learning tasks to confirm broader applicability
3. Investigate the impact of varying fine-tuning dataset sizes and distributions on attack performance to better understand limits of observed trends