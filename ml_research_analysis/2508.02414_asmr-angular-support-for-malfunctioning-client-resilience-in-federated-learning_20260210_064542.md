---
ver: rpa2
title: 'ASMR: Angular Support for Malfunctioning Client Resilience in Federated Learning'
arxiv_id: '2508.02414'
source_url: https://arxiv.org/abs/2508.02414
tags:
- clients
- malfunctioning
- updates
- learning
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of malfunctioning updates in
  federated learning, which can degrade global model performance. The authors introduce
  Angular Support for Malfunctioning Client Resilience (ASMR), a novel method that
  dynamically excludes malfunctioning clients based on their angular distance without
  requiring hyperparameters or knowledge about the number of malfunctioning clients.
---

# ASMR: Angular Support for Malfunctioning Client Resilience in Federated Learning

## Quick Facts
- **arXiv ID**: 2508.02414
- **Source URL**: https://arxiv.org/abs/2508.02414
- **Reference count**: 10
- **Primary result**: Introduces ASMR, a hyperparameter-free method that dynamically excludes malfunctioning clients in federated learning using angular distance, achieving robust performance across malicious, unreliable, and combined scenarios

## Executive Summary
This paper addresses the critical challenge of malfunctioning updates in federated learning, where faulty or malicious client contributions can significantly degrade global model performance. The authors propose Angular Support for Malfunctioning Client Resilience (ASMR), a novel approach that dynamically identifies and excludes malfunctioning clients based on angular distance metrics without requiring hyperparameters or prior knowledge of client behavior. ASMR leverages normalization, cosine distance calculations, and outlier factor analysis to establish a dynamic decision boundary for client selection. The method was evaluated on histopathological data using Resnet50 architecture across three distinct scenarios: malicious clients (ANA and SFA attacks), unreliable clients (data artifacts), and combined cases. Results demonstrate ASMR's robust performance with perfect true positive rates and near-zero false positive rates for malicious clients, while maintaining competitive accuracy (0.931-0.934) compared to state-of-the-art approaches.

## Method Summary
ASMR introduces a novel approach to identifying malfunctioning clients in federated learning by leveraging angular distance metrics. The method normalizes client updates, computes pairwise cosine distances, and calculates outlier factors to establish a dynamic decision boundary for client exclusion. Unlike existing methods that require hyperparameter tuning or knowledge about the number of malfunctioning clients, ASMR operates without these dependencies, making it more adaptable to dynamic federated learning environments. The approach was evaluated on a histopathological dataset using Resnet50 architecture, comparing its performance against established baselines including MKrum, DnC, and CFL across multiple attack and failure scenarios. The evaluation demonstrates ASMR's effectiveness in maintaining model performance while accurately identifying malfunctioning clients across varying conditions.

## Key Results
- Achieved true positive rates of 1.0 and false positive rates near 0.0 for malicious client detection
- Maintained competitive accuracy of 0.931-0.934 across all tested scenarios
- Demonstrated superior adaptability to dynamic changes in the number of malfunctioning clients compared to baseline methods

## Why This Works (Mechanism)
ASMR works by normalizing client updates and computing angular distances between them, which provides a scale-invariant measure of similarity that is less susceptible to magnitude-based manipulation. The cosine distance captures the directional alignment of updates, making it effective at identifying outliers regardless of their scale. By calculating outlier factors based on these distances, ASMR can establish a dynamic decision boundary that adapts to the current distribution of client updates without requiring fixed thresholds or prior knowledge about attack patterns.

## Foundational Learning
1. **Federated Learning**: Decentralized machine learning where multiple clients collaboratively train a global model
   - *Why needed*: Context for understanding the distributed training environment
   - *Quick check*: Verify understanding of local vs. global model updates

2. **Angular Distance**: Measure of similarity between vectors based on the angle between them
   - *Why needed*: Core mathematical concept underlying ASMR's client evaluation
   - *Quick check*: Confirm cosine similarity ranges from -1 to 1

3. **Outlier Detection**: Identifying data points that deviate significantly from the majority
   - *Why needed*: Essential for distinguishing malfunctioning from healthy clients
   - *Quick check*: Understand the difference between statistical and distance-based outlier detection

4. **Robust Aggregation**: Methods for combining client updates while minimizing the impact of faulty contributions
   - *Why needed*: Framework for understanding ASMR's contribution to federated learning
   - *Quick check*: Compare coordinate-wise median vs. Krum-based approaches

5. **Client Malfunction Types**: Categories of faulty behavior including malicious attacks and unreliable data
   - *Why needed*: Framework for evaluating different failure modes
   - *Quick check*: Distinguish between targeted and untargeted attacks

6. **Cosine Similarity**: Measure of orientation similarity between vectors, independent of magnitude
   - *Why needed*: Mathematical foundation for angular distance calculation
   - *Quick check*: Verify that cosine similarity is unaffected by vector scaling

## Architecture Onboarding

**Component Map**: Client Updates -> Normalization -> Cosine Distance Matrix -> Outlier Factor Calculation -> Dynamic Decision Boundary -> Global Model Aggregation

**Critical Path**: The core algorithm flows from client update collection through normalization, pairwise cosine distance computation, outlier factor determination, and finally to the dynamic exclusion decision that affects global model aggregation.

**Design Tradeoffs**: ASMR trades computational complexity in distance calculations for improved robustness and hyperparameter-free operation. While computing pairwise distances between all clients has O(nÂ²) complexity, this enables more nuanced detection of malfunctioning clients compared to coordinate-wise approaches.

**Failure Signatures**: The method is designed to detect clients with updates that have significantly different angular orientations from the majority, regardless of whether this results from malicious attacks (ANA, SFA) or unreliable data artifacts. The normalization step ensures scale-independent detection.

**First Experiments**:
1. Implement ASMR on a simple federated learning testbed with synthetic data to verify basic functionality
2. Compare ASMR's detection accuracy against coordinate-wise median and Krum baselines on controlled attack scenarios
3. Test ASMR's performance with varying numbers of clients (10, 50, 100) to evaluate scalability

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to a single histopathological dataset using Resnet50 architecture, limiting generalizability
- No analysis of computational overhead or scalability for larger models and client populations
- Privacy implications of angular distance calculations between client updates not discussed

## Confidence
- **High confidence**: The mathematical formulation of ASMR's angular distance approach appears sound and well-defined
- **Medium confidence**: Performance claims are robust within the tested scenarios but may not generalize to more complex or diverse federated learning environments
- **Medium confidence**: The claim of "no hyperparameters" holds true for the core algorithm, though practical implementation may require threshold tuning

## Next Checks
1. Test ASMR across multiple diverse datasets and model architectures (e.g., NLP, vision transformers) to evaluate generalizability
2. Conduct ablation studies to quantify the impact of each component (normalization, cosine distance, outlier factors) on overall performance
3. Evaluate computational overhead and communication costs compared to baseline methods under varying client scales (100-1000+ clients)