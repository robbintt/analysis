---
ver: rpa2
title: 'RetrieveAll: A Multilingual Named Entity Recognition Framework with Large
  Language Models'
arxiv_id: '2505.19128'
source_url: https://arxiv.org/abs/2505.19128
tags:
- retrieveall
- languages
- entity
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of named entity recognition (NER)
  for low- and medium-resource languages, where existing multilingual methods suffer
  from severe language interference and performance degradation due to data imbalance.
  To tackle this, the authors propose RetrieveAll, a universal multilingual NER framework
  based on dynamic LoRA modules that decouple task-specific language features and
  mitigate interference.
---

# RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models

## Quick Facts
- arXiv ID: 2505.19128
- Source URL: https://arxiv.org/abs/2505.19128
- Reference count: 12
- Key outcome: RetrieveAll achieves 12.1% average F1 improvement on PAN-X multilingual NER dataset by decoupling language-specific features via dynamic LoRA modules and integrating hierarchical knowledge through cross-granularity learning

## Executive Summary
This paper addresses the challenge of multilingual Named Entity Recognition (NER) where existing joint training approaches suffer from language interference and data imbalance across low- and medium-resource languages. The authors propose RetrieveAll, a universal framework that uses dynamic LoRA (Low-Rank Adaptation) modules to decouple task-specific language features and mitigate interference. By training separate LoRA modules for each language and incorporating a cross-granularity knowledge augmented learning method, RetrieveAll shifts from prompt-guided inference to prompt-driven learning, achieving significant performance improvements over existing baselines.

## Method Summary
RetrieveAll combines dynamic LoRA modules with cross-granularity knowledge augmented learning (CKAL) to address multilingual NER challenges. The framework trains separate LoRA modules for each language, then uses hierarchical prompts during training to integrate entity-level and context-level information through similarity-based retrieval from the training set itself. At inference, language is detected via retrieved context examples, enabling dynamic LoRA selection. The method leverages base LLMs (LLaMA3-8B, Qwen2.5) with LLM2Vec encoders for semantic similarity matching, and introduces thresholds for entity (τe=0.65) and context (τc=0.7) retrieval to ensure quality augmentation.

## Key Results
- Achieves 12.1% average F1 improvement on PAN-X multilingual NER dataset
- Outperforms existing baselines across different base models and language resource levels
- Monolingual LoRA training significantly outperforms joint training across most languages
- Korean NER improved 17.3% when combining both granularities vs. entity-level only on MultiCoNER

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific LoRA modules reduce cross-language interference in multilingual NER
- Mechanism: Each language receives an independently trained LoRA module, creating isolated parameter subspaces. During inference, the framework dynamically retrieves and activates only the LoRA corresponding to the input's detected language, preventing high-resource language features from dominating shared parameters.
- Core assumption: Task-specific language features can be effectively captured in low-rank matrices without requiring full-model retraining; language interference primarily occurs in shared weight space during joint training.
- Evidence anchors:
  - [abstract] "decouples task-specific features across languages and demonstrates efficient dynamic adaptability"
  - [Table 2] Monolingual LoRA training outperforms joint training across most languages
  - [corpus] Related work on projection-based transfer suggests language-specific adaptation is effective

### Mechanism 2
- Claim: Hierarchical prompts with entity-level and context-level examples during training shift learning from "prompt-guided inference" to "prompt-driven learning"
- Mechanism: During LoRA fine-tuning, each training sample is augmented with top-k similar entity examples and context examples retrieved via cosine similarity from the training set itself. This injects multi-granularity knowledge into LoRA weights rather than relying on prompts at inference.
- Core assumption: The training data contains sufficient quality examples that similar contexts/entities will transfer useful features; LLM encoder produces meaningful semantic representations.
- Evidence anchors:
  - [abstract] "leverages hierarchical prompts to integrate entity-level and context-level information without external resources"
  - [Table 3] Korean NER improved 17.3% when combining both granularities vs. entity-level only
  - [Figure 3] Training-stage CKAL yields 26.5-44.0% F1 gains

### Mechanism 3
- Claim: Language detection via retrieved context examples enables accurate LoRA module selection without explicit language labels
- Mechanism: Given input, retrieve top-k context examples; infer language via mode of those examples' language labels; select corresponding LoRA. This creates an input-aware routing mechanism.
- Core assumption: Semantically similar examples will predominantly share the same language as the query; the mode function is robust to occasional cross-language retrieval errors.
- Evidence anchors:
  - [Section 4.3] Formula: g(θₚ(M), xᵢ) = {θᵢₚ(M), i = mode(l₁, l₂, ..., lₖ)}
  - [Table 4] Input-aware LoRA retrieval achieves 96.9-99.7% accuracy across languages

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: The entire framework depends on understanding that LoRA adds trainable matrices B and A to frozen weights, enabling efficient per-language adaptation without full fine-tuning.
  - Quick check question: Given rank r=8 and hidden dimension d=4096, how many trainable parameters does one LoRA module add per linear layer?

- **Multilingual Language Interference**
  - Why needed here: The core problem RetrieveAll solves; you must understand that joint multilingual training causes feature conflicts and competitive suppression where high-resource languages dominate.
  - Quick check question: Why might training separate models per language be preferable to joint training, and what is the scalability tradeoff?

- **In-Context Learning vs. Prompt-Driven Learning**
  - Why needed here: RetrieveAll's innovation is using prompts during training to inject knowledge into weights, not during inference to guide outputs. Understanding this paradigm shift is critical.
  - Quick check question: If prompts are used only during fine-tuning but not inference, where does the "knowledge" reside after training completes?

## Architecture Onboarding

- **Component map:**
  - Base LLM (LLaMA3-8B / Qwen2.5-7B/14B) -> LoRA Pool θₚ(M) -> LLM2Vec Encoder -> CKAL Module -> LoRA Router -> Batch Inference Engine

- **Critical path:**
  1. Offline: Train k language-specific LoRAs using CKAL-augmented prompts on each language's NER data
  2. Offline: Encode all training entities/contexts with LLM2Vec, store representations
  3. At inference: Encode input → retrieve top-k context examples → determine language via mode → activate corresponding LoRA → generate NER output

- **Design tradeoffs:**
  - τₑ=0.65, τc=0.7 thresholds: Higher = more precise examples but fewer matches; lower = more augmentation but noisier signals
  - k=5 examples: More examples increase prompt length/compute; fewer may miss useful demonstrations
  - LoRA rank r: Not specified; higher r = more expressiveness but less parameter efficiency
  - Monolingual vs. joint LoRA training: Monolingual reduces interference but requires k separate training runs

- **Failure signatures:**
  - Short inputs: Low context similarity scores, unreliable language detection, degraded CKAL benefits
  - Long-tail entity types: Insufficient similar examples in training data for effective augmentation
  - Mixed-language batches: Incorrect LoRA routing if no single language dominates retrieved examples
  - Extremely low-resource languages: LoRA modules underfit due to insufficient training data

- **First 3 experiments:**
  1. Reproduce Table 2 (zero-shot joint vs. monolingual): Train LoRAs jointly across all 8 PAN-X languages vs. separately per language
  2. Ablate CKAL components following Table 3: Train three variants (entity-only, context-only, both) on one medium-resource language
  3. Validate LoRA retrieval accuracy following Table 4: Run inference on held-out test set with language labels masked

## Open Questions the Paper Calls Out

- **Question:** How can the computational latency introduced by the dynamic LoRA retrieval and batch inference mechanisms be minimized for real-time, high-concurrency applications?
  - Basis in paper: [explicit]
  - Why unresolved: The authors explicitly state in the Limitations section that while the dynamic LoRA modules enhance flexibility, they introduce additional computational latency.
  - What evidence would resolve it: Detailed latency benchmarks comparing RetrieveAll against static monolingual models, along with proposed architectural optimizations.

- **Question:** Can the integration of external knowledge bases overcome the framework's limitations in extremely low-resource languages where internal retrievable examples are insufficient?
  - Basis in paper: [explicit]
  - Why unresolved: The paper notes that relying solely on internal examples may fail to capture language characteristics when target language resources are extremely scarce.
  - What evidence would resolve it: Experiments augmenting the framework with external resources on languages with minimal training data to see if performance floors are raised.

- **Question:** How robust is the Cross-Granularity Knowledge Augmented Learning (CKAL) method when applied to datasets dominated by short texts where context-level semantic alignment is inherently difficult?
  - Basis in paper: [inferred]
  - Why unresolved: The authors attribute lower relative performance on MultiCoNER to the difficulty of capturing entity-context relationships in short texts.
  - What evidence would resolve it: Ablation studies isolating performance on short-text samples to analyze the marginal utility of context-level examples versus entity-level examples in those specific scenarios.

## Limitations
- Computational latency introduced by dynamic LoRA retrieval and batch inference mechanisms
- Limited performance on extremely low-resource languages where internal retrievable examples are insufficient
- Reduced effectiveness on short-text datasets where context-level semantic alignment is difficult

## Confidence
- High confidence: Language-specific LoRA modules reduce interference (confirmed by Table 2)
- High confidence: Hierarchical CKAL improves performance (confirmed by Table 3)
- Medium confidence: Input-aware LoRA retrieval achieves 96.9-99.7% accuracy (Table 4 result)
- Medium confidence: Average 12.1% F1 improvement across PAN-X (based on reported metrics)

## Next Checks
1. Reproduce Table 2 comparison: Train language-specific LoRAs jointly vs. separately for all 8 PAN-X languages
2. Ablate CKAL components: Train three variants (entity-only, context-only, both) on Chinese PAN-X data
3. Validate LoRA retrieval accuracy: Run inference on held-out test set with masked language labels; compute retrieval accuracy