---
ver: rpa2
title: 'Large language models for folktale type automation based on motifs: Cinderella
  case study'
arxiv_id: '2510.18561'
source_url: https://arxiv.org/abs/2510.18561
tags:
- motifs
- tales
- cinderella
- motif
- type
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study develops a novel LLM-based methodology for automatically
  detecting folktale motifs and types. Using GPT-4.5-Preview, the authors successfully
  identified motifs in 110 Cinderella variants with 98% accuracy compared to manual
  annotation.
---

# Large language models for folktale type automation based on motifs: Cinderella case study

## Quick Facts
- arXiv ID: 2510.18561
- Source URL: https://arxiv.org/abs/2510.18561
- Reference count: 0
- Primary result: LLM-based methodology successfully detects folktale motifs with 98% accuracy and clusters variants by structural similarity

## Executive Summary
This study develops a novel LLM-based methodology for automatically detecting folktale motifs and types, using Cinderella variants as a case study. The approach employs zero-shot prompting with GPT-4.5-Preview to identify motifs from Thompson's index, achieving 98% accuracy compared to manual annotation. The methodology enables large-scale computational analysis of folktales by clustering tales based on motif similarity, revealing distinct patterns across different Cinderella variants. The research demonstrates that LLMs can effectively capture both standard and variant motifs, advancing computational folkloristics by providing tools for automated motif detection, cross-lingual analysis, and identification of narrative patterns across extensive folktale collections.

## Method Summary
The methodology uses zero-shot prompting with GPT-4.5-Preview to detect narrative motifs in folktale variants. The LLM receives structured prompts requesting yes/no responses for each motif from Thompson's index, with binary responses converted to matrices and reduced via UMAP before clustering with K-means. The approach was validated on 13 manually annotated tales showing 98% accuracy, then applied to 110 Cinderella variants. Three motif sets were tested: narrow ATU 510A motifs, extended motifs including variant patterns, and generalized supermotifs. Cross-lingual analysis compared English and Slovene variants using identical prompts.

## Key Results
- LLM achieved 98% motif detection accuracy compared to manual annotation on 13 evaluation tales
- Clustering revealed distinct patterns: 2 clusters for narrow motifs (s=0.9), 4 clusters for broad motifs (s=0.6)
- Methodology successfully mapped previously unclassified Slovenian variants into established international framework
- Semantic clustering with LaBSE embeddings produced overly fragmented results (26 clusters) compared to motif-based approach

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot LLM Motif Detection via Structured Prompting
The LLM leverages pre-trained knowledge of narrative patterns to identify motif presence without task-specific training. It receives structured prompts with tale text and motif list, outputting binary yes/no responses. The model's pre-training corpus includes sufficient narrative structures to recognize motif patterns, though it may fail with narrowly defined or highly variant motifs.

### Mechanism 2: Motif-Based Clustering with Dimensionality Reduction
Binary motif presence matrices are reduced via UMAP to address high-dimensional clustering sensitivity. K-means clustering groups tales by motif similarity, with Silhouette coefficient validating cluster quality. This approach produces interpretable clusters that capture structural relationships between tale variants.

### Mechanism 3: Hierarchical Motif Generalization Informed by LLM Feedback
Initial narrow motif detection produces yes/no responses with optional comments on variations. These comments reveal pattern variations that enable iterative refinement from narrow motifs to broader supermotifs. Researchers generalize motifs by mapping to Thompson categories, creating new sets for improved coverage of tale diversity.

## Foundational Learning

- **Zero-shot prompting**: Why needed here? The methodology relies entirely on zero-shot LLM queries without examples or fine-tuning. Quick check: Can you explain why providing no examples (zero-shot) versus few examples (few-shot) matters for this task?

- **Thompson Motif Index and ATU Classification**: Why needed here? All motif definitions derive from Thompson's hierarchical index; understanding the taxonomy is required to interpret results and design alternative motif sets. Quick check: What is the difference between a specific motif (e.g., F823.2 "glass shoes") and a supermotif (e.g., F820 "extraordinary clothing and ornaments")?

- **Clustering validation with Silhouette Coefficient**: Why needed here? The paper uses Silhouette scores to compare algorithms and determine optimal cluster counts; understanding this metric is necessary to evaluate clustering quality. Quick check: What does a Silhouette coefficient close to 1 indicate versus close to 0?

## Architecture Onboarding

- **Component map**: Tale text → Prompt construction → LLM API call → Binary response extraction → Matrix construction → UMAP reduction → K-means clustering → Cluster interpretation

- **Critical path**: Tale text → Prompt construction → LLM API call → Binary response extraction → Matrix construction → UMAP reduction → K-means clustering → Cluster interpretation. The LLM accuracy (98%) anchors all downstream validity.

- **Design tradeoffs**:
  - Narrow vs. broad motifs: Narrow (15) produces 2 clusters with high separation (s=0.9); broad (14) produces 4 clusters with moderate separation (s=0.6)
  - LaBSE semantic clustering vs. motif-based clustering: Semantic approach produced 26 clusters deemed "too fragmented"
  - Cross-lingual prompting: Same English prompts used for Slovene tales without empirical validation

- **Failure signatures**:
  - Low Silhouette scores before UMAP: Indicates motif matrix is too sparse or high-dimensional
  - Motif frequency <30% across corpus: Suggests motif definition is too narrow
  - High cluster fragmentation with embeddings: Semantic similarity does not align with structural motif patterns

- **First 3 experiments**:
  1. Replicate 13-tale validation: Manually annotate motifs in 13 tales, run GPT-4.5-Preview with identical prompts, compute accuracy against human labels
  2. Test prompt sensitivity: Run same tales with motif definitions rephrased to measure whether phrasing affects detection accuracy
  3. Compare clustering algorithms on same motif matrix: Run K-means, DBSCAN, and Agglomerative clustering on UMAP-reduced data, compare Silhouette scores

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can LLMs be utilized to generate a new, data-driven folktale typology that resolves the Eurocentrism and specificity limitations of the ATU/Thompson indexes?
**Basis in paper:** [explicit] The authors conclude that their findings "could potentially support the development of an automatized data-driven folktale typology" and that "future work focused on developing more systematic and computationally grounded frameworks" is needed.
**Why unresolved:** This study only tested LLMs' ability to detect *existing*, pre-defined motifs, rather than tasking LLMs with independently generating a new taxonomy from the ground up.
**What evidence would resolve it:** A study where LLMs successfully inductively cluster folktales and define their own motif categories without being prompted by the Thompson index.

### Open Question 2
**Question:** Why does semantic text embedding clustering (LaBSE) fail to align with motif-based clustering, and can these methods be reconciled?
**Basis in paper:** [inferred] The authors note that clustering based on LaBSE embeddings produced 26 fragmented clusters that "differed substantially" from the motif-based clusters.
**Why unresolved:** The paper highlights the discrepancy but does not determine if the failure is due to the embedding model lacking narrative understanding or if semantic similarity simply does not correlate with structural motif similarity.
**What evidence would resolve it:** A comparative error analysis of LaBSE embeddings on the Cinderella dataset to identify if semantic distance correlates with narrative divergence.

### Open Question 3
**Question:** Is the LLM-based detection methodology transferable to folktale types that lack the highly canonical and studied motif structure of Cinderella (ATU 510A)?
**Basis in paper:** [explicit] The authors state their aim is to "open doors for its wider application in computational folkloristics," but note the analysis is limited to the Cinderella case study.
**Why unresolved:** Cinderella variants possess a very distinct, well-documented set of motifs; it is uncertain if LLMs would maintain 98% accuracy on tales with more fluid or ambiguous structural definitions.
**What evidence would resolve it:** Replication of the methodology on a less structurally rigid ATU type to evaluate if the strict prompting approach remains effective.

## Limitations

- Limited validation sample size (13 tales) may not represent full corpus diversity
- LLM performance on rare or highly variant motifs remains unclear
- Cross-lingual prompting effectiveness assumed but not empirically validated
- Generalizability to other folktale types beyond Cinderella variants uncertain

## Confidence

- **High Confidence**: The core LLM-based motif detection methodology is well-specified and reproducible, with clear validation metrics and established computational workflows
- **Medium Confidence**: The 98% accuracy claim is based on a small validation set (13 tales) and has not been independently verified
- **Low Confidence**: The generalizability of this approach to other folktale types and its performance on motifs that deviate significantly from Thompson's established categories

## Next Checks

1. **Independent Replication**: Manually annotate motifs in at least 30 additional tales from the corpus (not in the original 13) and compare LLM results to establish whether the 98% accuracy holds across a broader sample

2. **Cross-Lingual Prompt Testing**: Run the same English prompts on 10 Slovene tales and compare results to Slovene-specific prompts to quantify any performance differences and validate the cross-lingual assumption

3. **Rare Motif Analysis**: Systematically identify motifs with <30% prevalence in the corpus and conduct targeted testing to determine whether the LLM's accuracy drops significantly for these less common patterns