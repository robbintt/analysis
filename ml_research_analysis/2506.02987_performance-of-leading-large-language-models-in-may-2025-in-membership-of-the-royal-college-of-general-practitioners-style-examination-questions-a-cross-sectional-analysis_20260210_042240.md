---
ver: rpa2
title: 'Performance of leading large language models in May 2025 in Membership of
  the Royal College of General Practitioners-style examination questions: a cross-sectional
  analysis'
arxiv_id: '2506.02987'
source_url: https://arxiv.org/abs/2506.02987
tags:
- correct
- questions
- clinical
- llms
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study evaluated four leading large language models\u2014o3,\
  \ Claude Opus 4, Grok-3, and Gemini 2.5 Pro\u2014on 100 MRCGP-style multiple-choice\
  \ questions from the RCGP GP SelfTest. Each model was prompted to answer as a UK\
  \ GP and scored against official correct answers."
---

# Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis

## Quick Facts
- arXiv ID: 2506.02987
- Source URL: https://arxiv.org/abs/2506.02987
- Reference count: 0
- Primary result: o3 achieved 99.0% accuracy on MRCGP-style questions, far exceeding average peer score of 73.0%

## Executive Summary
This study evaluated four leading large language models (o3, Claude Opus 4, Grok-3, and Gemini 2.5 Pro) on 100 MRCGP-style multiple-choice questions from the RCGP GP SelfTest. Each model was prompted to answer as a UK GP and scored against official correct answers. The models achieved 99.0%, 95.0%, 95.0%, and 95.0% accuracy respectively, substantially outperforming the average peer score of 73.0%. o3 performed best with only one incorrect answer, while the others had five each.

The findings suggest that reasoning models, especially those trained on primary care data, can substantially support clinical decision-making in primary care settings. However, the study also highlights important limitations, as the models presented incorrect answers with high confidence, raising concerns about potential overreliance in clinical practice. The simulation-based approach using multiple-choice questions, while methodologically sound, does not fully capture the complexity of real-world clinical scenarios.

## Method Summary
The study employed a cross-sectional analysis design using 100 MRCGP-style multiple-choice questions from the RCGP GP SelfTest database. Four leading large language models (o3, Claude Opus 4, Grok-3, and Gemini 2.5 Pro) were evaluated in May 2025. Each model was prompted to answer as a UK GP, and responses were scored against official correct answers. The average peer score for comparison was 73.0%. The analysis focused on accuracy rates and identified specific incorrect answers to assess model performance and confidence calibration.

## Key Results
- o3 achieved 99.0% accuracy (1 incorrect answer)
- Claude Opus 4, Grok-3, and Gemini 2.5 Pro each achieved 95.0% accuracy (5 incorrect answers each)
- All models substantially outperformed average peer score of 73.0%
- Models presented incorrect answers with high confidence, raising clinical safety concerns

## Why This Works (Mechanism)
The superior performance of reasoning models on MRCGP-style questions likely stems from their advanced training on primary care data and clinical reasoning capabilities. Models like o3 appear to leverage pattern recognition from extensive medical literature and clinical guidelines, combined with sophisticated reasoning algorithms that can navigate complex clinical decision pathways. The ability to synthesize information from multiple sources and apply diagnostic reasoning appears particularly strong in models trained with primary care-specific datasets.

## Foundational Learning
- MRCGP examination format: Understanding the structure and content of Membership of the Royal College of General Practitioners exams is essential for contextualizing results. Quick check: Review sample MRCGP questions to understand clinical reasoning requirements.
- Multiple-choice question design: Knowledge of how clinical scenarios are constructed in MCQ format helps interpret model performance. Quick check: Analyze question stems for diagnostic reasoning patterns.
- Clinical confidence calibration: Understanding how models express certainty in responses is critical for safety assessment. Quick check: Compare confidence levels between correct and incorrect answers.

## Architecture Onboarding
Component map: Prompt -> Reasoning Engine -> Knowledge Base -> Output Generator -> Confidence Scoring
Critical path: Question interpretation → Clinical reasoning → Answer selection → Confidence calibration
Design tradeoffs: Accuracy vs. computational cost; confidence vs. uncertainty quantification; general knowledge vs. primary care specificity
Failure signatures: High-confidence incorrect answers; over-reliance on pattern matching; inability to handle ambiguous scenarios
First experiments: 1) Test model performance with varied prompt engineering approaches; 2) Evaluate accuracy across different clinical specialties; 3) Assess confidence calibration using calibrated probability scoring

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Simulation-based approach using multiple-choice questions rather than real clinical scenarios limits generalizability
- 100-question sample size may not capture full spectrum of primary care clinical challenges
- Unclear extent to which "answer as UK GP" prompt influenced model performance

## Confidence
- High confidence: Comparative performance ranking between models and superior performance relative to average peer scores
- Medium confidence: Practical applicability of these results to real-world clinical decision support
- Medium confidence: Interpretation that reasoning models trained on primary care data provide particular advantages

## Next Checks
1. Test model performance on open-ended clinical scenarios with incomplete patient information requiring diagnostic reasoning
2. Evaluate model accuracy across different clinical specialties and complexity levels within primary care
3. Assess whether providing contextual patient information affects model accuracy and confidence calibration