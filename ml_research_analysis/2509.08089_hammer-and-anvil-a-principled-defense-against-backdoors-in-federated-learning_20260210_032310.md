---
ver: rpa2
title: 'Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning'
arxiv_id: '2509.08089'
source_url: https://arxiv.org/abs/2509.08089
tags:
- accuracy
- krum
- defense
- attacks
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of backdoor attacks in federated
  learning, where malicious clients can embed hidden behaviors into the global model
  during training. The authors introduce a new adaptive adversary that surpasses existing
  attacks by optimizing updates based on knowledge of all client updates and the aggregation
  algorithm, requiring as few as one or two malicious clients out of 20 to break state-of-the-art
  defenses.
---

# Hammer and Anvil: A Principled Defense Against Backdoors in Federated Learning

## Quick Facts
- arXiv ID: 2509.08089
- Source URL: https://arxiv.org/abs/2509.08089
- Reference count: 40
- Primary result: Introduces Hammer and Anvil (HA), a combined defense that successfully defends against adaptive backdoor attacks in federated learning with minimal accuracy degradation

## Executive Summary
This paper addresses the critical vulnerability of federated learning to backdoor attacks, where malicious clients can embed hidden behaviors into the global model during training. The authors introduce an adaptive adversary that surpasses existing attacks by optimizing updates based on knowledge of all client updates and the aggregation algorithm. To counter this threat, they propose Hammer and Anvil (HA), a principled defense approach that combines robust-aggregation-based defenses effective against large-magnitude updates with fine-tuning-based defenses effective against small-magnitude updates. The best variant, Krum+, successfully defends against all adaptive and state-of-the-art attacks across multiple scenarios, with minimal accuracy degradation.

## Method Summary
The defense combines two orthogonal mechanisms: Krum aggregation (Anvil) for detecting large-magnitude malicious updates and Clipped-Super-Fine-Tuning (CSFT) (Hammer) for removing small-magnitude backdoors through post-processing. The approach requires a small clean validation dataset (1-4% of training data) at the server. CSFT uses a cyclical learning rate schedule with gradient clipping to prevent backdoor gradients from reasserting during fine-tuning. The defense creates a "pick your poison" scenario where attackers must either use large updates (caught by Krum) or small updates (erased by CSFT).

## Key Results
- Krum+ successfully defends against all adaptive and state-of-the-art attacks across 8 different models
- Defense maintains accuracy degradation below 5% while achieving ASR < 50%
- Only 1-2 malicious clients out of 20 can break existing defenses, while HA remains effective
- The combined approach outperforms standalone defenses on all tested scenarios

## Why This Works (Mechanism)

### Mechanism 1: Robust Aggregation against Large Magnitude Updates (The Anvil)
Krum selects the update vector with the smallest sum of distances to its nearest neighbors, theoretically isolating large-magnitude backdoor attempts that drift away from the dense cluster of benign updates. The defense assumes malicious clients are a minority and their updates are semantically distant from the benign distribution.

### Mechanism 2: Post-Aggregation Erasure via Clipped-Super-Fine-Tuning (The Hammer)
CSFT utilizes a cyclical learning rate on a small clean dataset with gradient clipping to prevent backdoor gradients from reasserting during retraining. This effectively "forgets" the trigger while preserving the main task, assuming the server has access to clean validation data.

### Mechanism 3: Principle of Orthogonal Combination (The Squeeze)
By combining defenses targeting different attack magnitudes, the approach closes the "operating window" for attackers. The defense creates a scenario where attackers must either use large updates (caught by Krum) or small updates (erased by CSFT), with these defenses being additive due to their orthogonal targeting.

## Foundational Learning

- **Federated Learning Aggregation (FedAvg)**: Essential for understanding how local updates are combined and why poisoned updates from minority clients can disproportionately affect outcomes. Quick check: If 1 out of 20 clients is malicious, does standard averaging prevent the backdoor? (Answer: No)
- **L2-Norm and Distance Metrics**: Critical for visualizing how attacks are detected or constrained through Krum scoring and update clipping. Quick check: How does scaling a malicious update affect its L2-norm and likelihood of being an outlier?
- **Backdoor Attacks (Trigger vs. Semantic)**: Necessary to understand the attack goal (misclassification) and method (trigger patch vs. semantic feature) and why fine-tuning is effective at severing the trigger-target link. Quick check: What is the trade-off for the attacker between Attack Success Rate (ASR) and maintaining Clean Accuracy?

## Architecture Onboarding

- **Component map**: Local Training (Client) -> Server Aggregator (Krum) -> Post-Processor (CSFT)
- **Critical path**: The defense efficacy relies heavily on the Krum selection threshold and the gradient clipping threshold for CSFT
- **Design tradeoffs**: Fine-tuning Data Size (1% vs 4%) affects defense success and accuracy degradation; clipping threshold requires finding a "sweet spot"
- **Failure signatures**: High ASR with low accuracy indicates aggressive attack destroying utility; high ASR with high accuracy indicates adaptive attack bridging the gap; low ASR with low accuracy indicates overly aggressive defense
- **First 3 experiments**: 1) Run Adaptive Attack on Krum-only to verify ASR â‰ˆ 100%; 2) Run CSFT alone on backdoored model to verify failure on large-magnitude attacks; 3) Run Krum+ with 4% fine-tuning data to verify ASR < 50% and accuracy drop < 5%

## Open Questions the Paper Calls Out

1. What is the theoretical size of the interval where backdoor attacks remain viable? The paper lacks a formal theoretical bound quantifying the "operating window" between large-magnitude and small-magnitude defenses.

2. Do the defense principles generalize to non-image data modalities? The empirical evaluation is restricted to image classification, leaving NLP or audio applications unexplored.

3. Can the framework be improved by substituting Krum or CSFT with more robust components? The study validates the specific combination but doesn't explore other potential "Hammer" or "Anvil" components.

## Limitations

- Exact neural network architecture for CIFAR-10 experiments is not specified
- Specific epoch boundaries for the cyclical learning rate schedule in CSFT are not explicitly detailed
- Assumes availability of clean validation data at the server, which may not be feasible in real-world deployments

## Confidence

- **High Confidence**: The core defense principle combining robust aggregation and fine-tuning is sound and well-supported by theoretical analysis
- **Medium Confidence**: Experimental results demonstrating defense effectiveness across multiple scenarios and attack configurations
- **Medium Confidence**: The adaptive attack methodology and its success in breaking existing defenses

## Next Checks

1. **Component Isolation Test**: Validate CSFT effectiveness independently by applying it to a model already backdoored through standard attacks, measuring both ASR reduction and accuracy preservation

2. **Architecture Sensitivity Test**: Test Krum+ with different model architectures (e.g., ResNet-18 vs. simpler CNN) to assess defense robustness to architectural variations

3. **Data Efficiency Test**: Systematically evaluate the relationship between fine-tuning dataset size and defense effectiveness to identify optimal trade-offs between defense strength and accuracy preservation