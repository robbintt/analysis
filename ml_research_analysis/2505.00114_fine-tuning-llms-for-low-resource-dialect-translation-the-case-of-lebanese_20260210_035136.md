---
ver: rpa2
title: 'Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese'
arxiv_id: '2505.00114'
source_url: https://arxiv.org/abs/2505.00114
tags:
- translation
- lebanese
- language
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper examines the effectiveness of fine-tuning Large Language\
  \ Models (LLMs) for translating the low-resource Lebanese dialect. The study compares\
  \ three fine-tuning approaches\u2014Basic (Instruct-MT), contrastive (Instruct-Cont),\
  \ and grammar-hint (Instruct-Grammar)\u2014using the Aya23-8B model."
---

# Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese

## Quick Facts
- **arXiv ID**: 2505.00114
- **Source URL**: https://arxiv.org/abs/2505.00114
- **Reference count**: 19
- **Key outcome**: Models fine-tuned on culturally-aware, native Lebanese data (LanguageWave) outperform those trained on larger, non-native datasets, with contrastive fine-tuning and prompting achieving the best results on authentic dialect content.

## Executive Summary
This paper investigates fine-tuning Large Language Models for translating the low-resource Lebanese dialect, challenging the assumption that more training data always leads to better performance. Through systematic experiments with three fine-tuning approaches—basic translation, contrastive, and grammar-hint—the study demonstrates that culturally authentic, native datasets can significantly outperform larger non-native datasets. The best results come from contrastive fine-tuning paired with contrastive prompting, highlighting the value of exposing models to translation errors. The authors also introduce LebEval, a new benchmark derived from authentic Lebanese content, which reveals limitations in existing translated datasets.

## Method Summary
The study fine-tunes the Aya23-8B model using QLoRA on three instruction types: basic translation, contrastive (paired good/rejected translations), and grammar-hint. Two main datasets are used: a large non-native corpus (MADAR + OpenSubtitles, 140K sentences) and a smaller culturally-aware corpus (LanguageWave, ~3K sentences). During evaluation, three prompting strategies are tested: zero-shot, few-shot (3 examples), and contrastive 3-shot. The models are evaluated on both the standard FLoRes benchmark and the newly created LebEval benchmark using the reference-free xCOMET metric.

## Key Results
- Models fine-tuned on the culturally-aware LanguageWave dataset consistently outperform those trained on the larger non-native dataset across all instruction types
- Contrastive fine-tuning paired with contrastive prompting achieves the highest xCOMET score (74.4) on the authentic LebEval benchmark
- Performance on the translated FLoRes benchmark is inflated compared to the authentic LebEval benchmark, demonstrating the need for culturally relevant evaluation data
- Standard contrastive preference optimization underperforms supervised fine-tuning, likely due to limitations in automatically generated preference data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Culturally-aware, native datasets can outperform larger, non-native datasets for dialect translation.
- **Mechanism:** Authentic cultural data contains native idioms and contextual nuances, providing a higher-density signal for the specific linguistic patterns of a low-resource dialect.
- **Core assumption:** The quality and cultural relevance of training data is more predictive of translation performance for dialects than sheer volume of non-native, translated data.
- **Evidence anchors:** Models fine-tuned on LanguageWave consistently outperform those trained on non-native data; related work on Korean dialect translation also emphasizes steering away from source-copying.

### Mechanism 2
- **Claim:** Contrastive fine-tuning improves translation quality by teaching the model to distinguish between good and bad translations.
- **Mechanism:** By presenting the model with paired examples of preferred and rejected translations during fine-tuning, the model learns to discriminate against common errors.
- **Core assumption:** Exposure to negative examples helps the model build a more robust internal representation of translation quality for the specific dialect.
- **Evidence anchors:** The best results were achieved through contrastive fine-tuning paired with contrastive prompting; contrastive instructions use chosen/rejected pairs.

### Mechanism 3
- **Claim:** Authentic evaluation benchmarks are necessary for accurately assessing dialectal translation capabilities.
- **Mechanism:** Standard benchmarks often use translated text, which may not reflect the true linguistic complexity of a dialect (idioms, cultural references).
- **Core assumption:** Performance on translated benchmarks overestimates a model's true ability to handle cultural and linguistic nuances of a dialect.
- **Evidence anchors:** LebEval, derived from authentic Lebanese content, shows lower performance than FLoRes, indicating the latter's inflated scores; related work identifies evaluation distortions in n-gram metrics.

## Foundational Learning
- **Concept: Supervised Fine-Tuning (SFT) for Translation**
  - Why needed here: This is the core method for adapting the base LLM to Lebanese-to-English translation using instruction-following data.
  - Quick check question: How would you format a parallel sentence pair into a training instruction for SFT?

- **Concept: Preference Alignment vs. Fine-Tuning**
  - Why needed here: The paper investigates Contrastive Preference Optimization as an alternative to standard SFT.
  - Quick check question: What is the fundamental difference in how a model learns from a single correct answer (SFT) versus a pair of ranked answers (preference alignment)?

- **Concept: LLM Evaluation Metrics (BLEU vs. xCOMET)**
  - Why needed here: The paper uses xCOMET because traditional metrics like BLEU fail to capture nuanced aspects of translation quality for dialects.
  - Quick check question: Why would a semantic-based metric like xCOMET be preferred over a lexical-based metric like BLEU for evaluating dialect translation?

## Architecture Onboarding
- **Component map:** Base Model (Aya23-8B) -> Training Data (LW or NN) -> Instruction Types (Translation/Contrastive/Grammar) -> Fine-Tuning (QLoRA) -> Prompting (Zero-shot/3-shot/C3-shot) -> Evaluation (xCOMET on FLoRes/LebEval)

- **Critical path:** The most effective path is: (1) Start with Aya23-8B model. (2) Fine-tune using Contrastive Instructions derived from culturally-aware LW dataset. (3) During inference, use Contrastive Prompting (C3-shot).

- **Design tradeoffs:**
  - **Data Quality vs. Quantity:** Smaller, high-quality, culturally authentic dataset (LW) versus larger, lower-quality, non-native dataset (NN).
  - **Curriculum Learning:** Tested but found not to yield significant gains due to catastrophic forgetting; single-stage contrastive fine-tuning is more effective.
  - **CPO vs. SFT:** CPO underperformed compared to standard supervised fine-tuning on contrastive instructions.

- **Failure signatures:**
  - **CPO Underperformance:** Preference alignment methods yielded results below baseline, likely due to limitations in automatically generated preference data.
  - **Curriculum Learning Failure:** Multi-stage training did not improve performance, indicating benefits were lost due to catastrophic forgetting.

- **First 3 experiments:**
  1. **Ablation on Few-Shot Prompting:** Determine optimal number of examples (K) and selection method for few-shot prompting.
  2. **Contrastive vs. Basic Tuning:** Compare models fine-tuned on contrastive instructions against those on basic translation instructions using LW dataset.
  3. **Evaluation on Authentic Benchmark:** Compare model performance on LebEval versus FLoRes to test if the latter is inflated.

## Open Questions the Paper Calls Out
- **Question 1:** Do the findings regarding the superiority of culturally authentic data and contrastive fine-tuning generalize to Arabic-centric LLMs (e.g., Jais, AceGPT) or larger parameter models?
- **Question 2:** Can Mixture of Experts (MoE) architectures provide a more efficient adaptation mechanism for dialectal translation than the dense QLoRA approach?
- **Question 3:** Can Contrastive Preference Optimization (CPO) surpass Supervised Fine-Tuning (SFT) if the rejected translations are curated specifically for cultural misalignment?

## Limitations
- The study is limited to a single model (Aya23-8B) and relatively small evaluation sets (70 sentences for LebEval, 500 for FLoRes).
- Synthetically generated grammar hints may introduce biases from the prompting process that aren't fully characterized.
- The contrastive approach depends heavily on the quality of rejected translations generated by the base model.

## Confidence
- **High Confidence:** The core finding that culturally-aware native datasets outperform larger non-native datasets for dialect translation.
- **Medium Confidence:** The superiority of contrastive fine-tuning over basic fine-tuning, as it depends on the quality of automatically generated contrastive pairs.
- **Medium Confidence:** The claim that CPO underperforms SFT, as the preference data was automatically generated and may contain noise.

## Next Checks
1. **Cross-model validation:** Test whether the contrastive instruction approach generalizes to other base models (e.g., LLaMA, Mistral).
2. **Robustness to rejected examples:** Systematically vary the quality of rejected translations in contrastive fine-tuning to quantify sensitivity to this component.
3. **Long-tail dialect coverage:** Evaluate models on dialectal variants not well-represented in the LanguageWave dataset (e.g., rural vs. urban Lebanese).