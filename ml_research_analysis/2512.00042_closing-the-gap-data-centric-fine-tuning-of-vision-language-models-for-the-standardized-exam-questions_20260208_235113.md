---
ver: rpa2
title: 'Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the
  Standardized Exam Questions'
arxiv_id: '2512.00042'
source_url: https://arxiv.org/abs/2512.00042
tags:
- reasoning
- arxiv
- multimodal
- wang
- available
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work demonstrates that data-centric supervised fine-tuning\
  \ can substantially improve multimodal reasoning, achieving 78.6% accuracy on a\
  \ standardized exam benchmark\u2014only 1.0% below the proprietary Gemini 2.0 Flash.\
  \ The approach uses a 161.4 million token multimodal corpus combining curriculum-grounded\
  \ textbooks, diagrams, and contextual materials, fine-tuned using the QMSA syntax\
  \ (<question <meta <solution <answer)."
---

# Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions

## Quick Facts
- arXiv ID: 2512.00042
- Source URL: https://arxiv.org/abs/2512.00042
- Authors: Egemen Sert; Şeyda Ertekin
- Reference count: 40
- Primary result: Data-centric fine-tuning achieves 78.6% accuracy on standardized exam benchmark

## Executive Summary
This work demonstrates that data-centric supervised fine-tuning can substantially improve multimodal reasoning, achieving 78.6% accuracy on a standardized exam benchmark—only 1.0% below the proprietary Gemini 2.0 Flash. The approach uses a 161.4 million token multimodal corpus combining curriculum-grounded textbooks, diagrams, and contextual materials, fine-tuned using the QMSA syntax (<question> <meta> <solution> <answer>). Experiments reveal that data composition and representational syntax (particularly including curriculum metadata) are as critical as model scale for reasoning generalization. The released YKSUniform benchmark enables standardized evaluation of multimodal reasoning across 1,854 exam questions spanning 309 topics. These results establish that carefully curated multimodal data can elevate open-weight vision-language models to near-state-of-the-art performance without reinforcement learning.

## Method Summary
The method involves supervised fine-tuning of Qwen-2.5VL-32B using a 161.4M-token multimodal corpus with three datasets: CoreReason (solution traces), MetaReason (curriculum metadata), and ContextVQA (visual grounding). The training uses QMSA syntax format (<question> <meta> <solution> <answer>) with 20% masked token completion and aggressive quality filtering via rejection keywords. The model is trained for 6 epochs with checkpointing, evaluating on the YKSUniform benchmark. The optimal dataset mix (CR+MR+CV) and syntax (excluding verbose reasoning traces) are identified through ablation studies.

## Key Results
- Achieves 78.6% accuracy on YKSUniform benchmark, only 1.0% below Gemini 2.0 Flash
- QMSA syntax outperforms QMTSA by 2.2 percentage points (59.28% vs 57.07%)
- CR+MR+CV dataset mix outperforms individual components, with 55.99% accuracy
- Quality-filtered "CoreReason-Reviewed" set improves accuracy by +0.75 points despite smaller size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structured curriculum metadata (`<meta>` tags) grounds reasoning more effectively than verbose chain-of-thought traces.
- **Mechanism:** Conditioning on explicit curriculum context (subject, topic, difficulty) helps select appropriate reasoning schemas, while verbose "thinking" tokens can introduce noise and speculation.
- **Evidence anchors:** QMSA syntax achieves 59.28% vs QMTSA at 57.07% in Table 2; abstract notes representational syntax is critical.

### Mechanism 2
- **Claim:** Combining heterogeneous data sources creates complementary supervision that improves generalization.
- **Mechanism:** CoreReason provides deep solution traces, MetaReason provides scale and taxonomic structure, ContextVQA provides broad visual grounding.
- **Evidence anchors:** Table 1 shows full mix (CR+MR+CV) yields 55.99% accuracy, outperforming individual components.

### Mechanism 3
- **Claim:** Aggressive quality filtering of distilled data is a primary driver of performance gains.
- **Mechanism:** Using 44 rejection keywords to filter teacher model failures and manually reviewing low-performing topics increases signal-to-noise ratio.
- **Evidence anchors:** Section 2.1 describes filtering pipeline; training on reviewed set improved accuracy by +0.75 points.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT) vs. Reinforcement Learning (RL)**
  - **Why needed here:** The paper positions SFT as an alternative to RL for achieving high reasoning performance.
  - **Quick check question:** Why does the author imply that SFT is more "data-centric" than RL?

- **Concept: Multimodal Token Syntax**
  - **Why needed here:** The QMSA syntax defines how the model structures its output, directly influencing reasoning capability.
  - **Quick check question:** What is the functional difference between the `<meta>` token and the `<think>` token in this paper's findings?

- **Concept: Data Distillation & Filtering**
  - **Why needed here:** The pipeline relies on using a stronger model to generate solutions, which are then filtered.
  - **Quick check question:** What specific risk does "rejection keyword filtering" mitigate when distilling solutions from a teacher model?

## Architecture Onboarding

- **Component map:** OCR + Visual Encoder (Qwen-2.5VL) -> Three-track data pipeline (CoreReason, MetaReason, ContextVQA) -> SFT loop with QMSA syntax -> Structured text generation
- **Critical path:** 1) Distill solutions from teacher model 2) Apply rejection filtering 3) Inject `<meta>` tags 4) Train using QMSA format
- **Design tradeoffs:** Verbosity vs. precision (including `<think>` increases tokens but reduces accuracy); data scale vs. quality (smaller reviewed set outperforms larger raw set)
- **Failure signatures:** High token count with low accuracy indicates hallucinating verbose reasoning traces; drop in visual reasoning suggests insufficient ContextVQA; repetitive outputs indicate permissive filtering
- **First 3 experiments:** 1) Syntax ablation: QMSA vs QMTSA on small data slice 2) Dataset mixing ratio: CR+MR vs CR+MR+CV 3) Filtering thresholds: vary rejection keyword strictness

## Open Questions the Paper Calls Out

- **Question:** Does the EduMix-QMSA strategy maintain efficacy for abstract reasoning tasks or low-resource languages outside Turkish high-school curriculum?
- **Basis:** [explicit] Conclusion mentions extending to "more abstract reasoning tasks, cross-lingual learning, and domain adaptation in data-scarce regions"
- **Why unresolved:** Study restricted to specific Turkish educational context, transferability untested
- **What evidence would resolve it:** Evaluation on multilingual MMMU and abstract logic datasets

- **Question:** Is the performance drop with verbose reasoning traces caused by teacher model noise or is concise structuring inherently superior?
- **Basis:** [explicit] Authors note teacher traces "often inject verbose or unstable reasoning steps" but don't isolate specific failure mechanism
- **Why unresolved:** Unclear if problem is verbosity, teacher model quality, or reasoning steps themselves
- **What evidence would resolve it:** Ablation study comparing ground-truth human traces vs distilled traces with controlled token length

- **Question:** What specific alignment characteristics in CR+MR+CV mixture allowed it to outperform single-source training?
- **Basis:** [inferred] Contrasts positive results with Bansal et al.'s findings, hypothesizes "synergy depends on shared reasoning style"
- **Why unresolved:** Demonstrates mixture works but doesn't isolate preventing "heterogeneity penalty"
- **What evidence would resolve it:** Quantitative analysis of gradient alignment or embedding distribution overlap between datasets

## Limitations
- CoreReason, MetaReason, and ContextVQA datasets not publicly released, requiring recreation
- YKSUniform benchmark contains Turkish-language questions requiring translation or language proficiency
- 32B training hyperparameters only partially specified (7B values provided)

## Confidence
- **High confidence:** Data composition and quality filtering significantly impact multimodal reasoning performance (well-supported by ablation studies)
- **Medium confidence:** QMSA syntax outperforms QMTSA (supported by Table 2, mechanism inferred not directly measured)
- **Medium confidence:** CR+MR+CV dataset mix superiority demonstrated, exact contribution ratios not quantified

## Next Checks
1. **Syntax ablation reproduction:** Train two identical models using QMSA vs. QMTSA formats on small controlled dataset to verify ~2.2 percentage point difference
2. **Dataset contribution isolation:** Systematically test each dataset component individually and in combinations to quantify marginal contributions
3. **Filtering sensitivity analysis:** Vary rejection keyword stringency and measure impact on both data quality and final reasoning performance