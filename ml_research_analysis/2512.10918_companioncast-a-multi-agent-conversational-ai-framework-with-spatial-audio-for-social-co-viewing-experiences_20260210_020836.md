---
ver: rpa2
title: 'CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio
  for Social Co-Viewing Experiences'
arxiv_id: '2512.10918'
source_url: https://arxiv.org/abs/2512.10918
tags:
- agent
- agents
- viewing
- multi-agent
- social
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CompanionCast, a multi-agent conversational
  AI framework with spatial audio for enhancing social co-viewing experiences. The
  system orchestrates role-specialized AI agents that respond to video content using
  multimodal inputs and provides real-time companionship during media consumption.
---

# CompanionCast: A Multi-Agent Conversational AI Framework with Spatial Audio for Social Co-Viewing Experiences

## Quick Facts
- arXiv ID: 2512.10918
- Source URL: https://arxiv.org/abs/2512.10918
- Reference count: 10
- Key outcome: Multi-agent conversational AI framework with spatial audio enhances social co-viewing experiences

## Executive Summary
CompanionCast is a multi-agent conversational AI framework designed to enhance social co-viewing experiences through role-specialized AI agents that respond to video content using multimodal inputs. The system employs spatial audio to create an immersive environment where AI companions provide real-time interaction during media consumption. A key innovation is the LLM-as-a-Judge module that iteratively scores and refines conversations across five dimensions (relevance, authenticity, engagement, diversity, personality consistency). The framework was validated through a pilot study with soccer fans, demonstrating improved perceived social presence compared to solo viewing.

## Method Summary
The CompanionCast framework orchestrates multiple AI agents with specialized roles that interact with users during video content consumption. Agents process multimodal inputs from the video stream and generate contextually relevant responses. The system implements spatial audio to position agent voices in 3D space, creating a more natural conversational environment. Conversation quality is maintained through an iterative LLM-as-a-Judge pipeline that evaluates responses across five dimensions and refines them before delivery. The framework was tested in a pilot study with 10 soccer fans, comparing multi-agent interaction against solo viewing conditions.

## Key Results
- Pilot study showed improved perceived social presence compared to solo viewing
- Multi-agent interaction enhanced user engagement during co-viewing
- Technical limitations included response latency averaging 2.7 seconds and speech recognition error rates of approximately 18%

## Why This Works (Mechanism)
The framework leverages role-specialized agents to create diverse conversational dynamics that mirror real social interactions. By processing multimodal video inputs, agents can generate contextually relevant commentary that enhances the viewing experience. The spatial audio implementation creates a sense of physical presence and directionality in conversations, making interactions feel more natural. The iterative quality control through LLM evaluation ensures that agent responses maintain high standards across multiple conversational dimensions simultaneously.

## Foundational Learning
- Multimodal input processing: Agents must analyze both audio and visual content to generate relevant responses
  - Why needed: To understand video context and provide meaningful commentary
  - Quick check: Test with videos of varying complexity and content types
- Spatial audio positioning: Voice placement in 3D space enhances conversational realism
  - Why needed: Creates natural conversational dynamics and reduces cognitive load
  - Quick check: User preference testing between spatial and non-spatial audio
- Iterative conversation refinement: Multiple evaluation passes improve response quality
  - Why needed: Ensures consistency and relevance across conversational dimensions
  - Quick check: Compare single-pass vs multi-pass conversation quality scores

## Architecture Onboarding

**Component Map:**
User Input -> Speech Recognition -> LLM Analysis -> Agent Selection -> Response Generation -> LLM-as-a-Judge -> Spatial Audio Output

**Critical Path:**
User input → Speech recognition → Agent processing → LLM evaluation → Audio rendering

**Design Tradeoffs:**
- Latency vs quality: Multi-pass evaluation improves quality but increases response time
- Specialization vs generalization: Role-specific agents provide better interaction but require more complex coordination
- Processing power vs accuracy: More sophisticated models improve understanding but increase computational requirements

**Failure Signatures:**
- High latency (>3s) indicates model processing bottlenecks
- Recognition errors suggest poor audio quality or speech model limitations
- Inconsistent responses point to LLM evaluation pipeline issues

**First Experiments:**
1. Measure baseline response latency with single agent vs multi-agent configuration
2. Test speech recognition accuracy across different accents and background noise levels
3. Evaluate user preference for spatial audio positioning across different agent configurations

## Open Questions the Paper Calls Out
None

## Limitations
- Limited sample size (n=10) restricts generalizability of findings
- Single viewing session may not capture long-term user engagement patterns
- Technical limitations including 2.7 second average latency and 18% speech recognition error rate affected user experience

## Confidence

**High confidence in:**
- Technical architecture and system implementation
- Innovation of LLM-as-a-Judge quality control mechanism

**Medium confidence in:**
- Effectiveness of LLM-as-a-Judge module for quality control
- Social presence benefits demonstrated in pilot study

**Low confidence in:**
- Generalizability across different content types and user demographics
- Long-term sustainability of multi-agent interaction patterns

## Next Checks
1. Conduct a larger-scale study (n≥50) with diverse participant demographics and multiple content genres to establish generalizability
2. Implement and test a hybrid quality control system combining LLM evaluation with human validation to verify the reliability of automated scoring
3. Measure and optimize system latency through technical improvements, targeting sub-1.5 second response times to reduce user frustration