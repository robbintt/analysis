---
ver: rpa2
title: Gradient Multi-Normalization for Stateless and Scalable LLM Training
arxiv_id: '2502.06742'
source_url: https://arxiv.org/abs/2502.06742
tags:
- adam
- norm
- arxiv
- training
- norms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Gradient Multi-Normalization for Stateless
  and Scalable LLM Training. The authors address the challenge of training large language
  models with memory-efficient optimizers by proposing a novel framework that normalizes
  stochastic gradients according to multiple norms.
---

# Gradient Multi-Normalization for Stateless and Scalable LLM Training

## Quick Facts
- arXiv ID: 2502.06742
- Source URL: https://arxiv.org/abs/2502.06742
- Reference count: 40
- This paper introduces a novel gradient normalization framework for memory-efficient LLM training

## Executive Summary
This paper addresses the challenge of training large language models with memory-efficient optimizers by proposing Gradient Multi-Normalization (MultiNorm). The authors develop a framework that normalizes stochastic gradients according to multiple norms simultaneously, using an alternating scheme that converges to a fixed-point solution. They demonstrate that their approach can produce arbitrary precision solutions while relaxing computational constraints from existing methods like SWAN. Experiments on LLaMA models show that their SinkGD optimizer achieves 3× speedup over Adam with significantly reduced memory requirements, making it particularly suitable for large-scale LLM training.

## Method Summary
The paper introduces Gradient Multi-Normalization (MultiNorm), which normalizes stochastic gradients according to multiple norms simultaneously. The core algorithm, SR-Sinkhorn, alternates between row-wise and column-wise ℓ2-norm normalization to achieve this multi-normalization. This approach produces a fixed-point solution with arbitrary precision. The authors show that SWAN is a particular instance of their general framework but is computationally expensive. To address this, they design SinkGD, a more efficient and scalable stateless optimizer that relaxes SWAN's constraints while maintaining memory efficiency. SinkGD applies to linear layers only, using Adam for other parameters.

## Key Results
- SinkGD achieves 3× speedup over Adam on LLaMA models while maintaining competitive perplexity
- Memory requirements reduced from 3× to ~1× model size through stateless design
- Multi-normalization via alternating row/column normalization converges to fixed-point solutions
- Column-wise ℓ2 normalization provides computational efficiency compared to SWAN's spectral normalization

## Why This Works (Mechanism)

### Mechanism 1: Alternating Multi-Normalization via Fixed-Point Iteration
- Claim: Alternating between row-wise and column-wise ℓ2-norm normalization converges to a fixed-point solution that satisfies both normalization constraints simultaneously.
- Mechanism: The SR-Sinkhorn procedure alternates between `P_g1(W) = √n·Q(W)^(-1)·W` (row normalization) and `P_g2(W) = √m·W·R(W)^(-1)` (column normalization). Under Assumption 3.3, Theorem 3.6 proves that `d(x_n, F) → 0` as `n → ∞`, where F is the set of fixed points.
- Core assumption: Both norm projections must satisfy constant ℓ2-norm projections with `c = √(nm)`.
- Evidence anchors: [abstract], [section 3.2, Theorem 3.6], [section 4]

### Mechanism 2: Relaxing Spectral Normalization to Column-wise ℓ2
- Claim: Replacing SWAN's spectral norm with column-wise ℓ2-norm reduces computational complexity from `O(m²(m+n))` to `O(mn)`.
- Mechanism: SWAN requires computing `(∇_t∇_t^T)^(-1/2)` via Newton-Schulz iteration. SinkGD replaces this with column normalization `R(W)^(-1)`.
- Core assumption: Gradient matrix structure in transformers allows effective preconditioning through simple row/column balancing.
- Evidence anchors: [abstract], [section 4]

### Mechanism 3: Memory Reduction via Stateless Design
- Claim: Eliminating optimizer states reduces memory from 3× to ~1× model size while maintaining competitive perplexity.
- Mechanism: Adam stores first and second moment estimates, tripling memory. SinkGD only requires the instantaneous gradient.
- Core assumption: Per-step gradient normalization provides sufficient adaptive scaling without historical information.
- Evidence anchors: [abstract], [section 5.1, Table 1]

## Foundational Learning

- Concept: Sinkhorn Algorithm and Matrix Scaling
  - Why needed here: SinkGD's SR-Sinkhorn is a variant of the classical Sinkhorn algorithm. Understanding that Sinkhorn alternates row/column normalization to achieve doubly stochastic matrices helps explain why SinkGD works.
  - Quick check question: Given a matrix `A ∈ R^(m×n)`, what does the Sinkhorn algorithm compute, and what is its convergence guarantee?

- Concept: Dual Norms and Steepest Descent
  - Why needed here: The paper builds on Bernstein & Newhouse (2024)'s framework interpreting optimizers as steepest descent under specific norms. Understanding dual norms `||x||_* = sup_{||z||≤1} ⟨x, z⟩` is essential for understanding why normalization corresponds to projection.
  - Quick check question: What is the dual norm of the ℓ2-norm? Of the spectral norm?

- Concept: Gradient Whitening vs. Normalization
  - Why needed here: SWAN uses whitening `(WW^T)^(-1/2)W` which decorrelates gradient components. SinkGD uses simpler row/column normalization. Understanding the difference clarifies what statistical properties each preserves.
  - Quick check question: What statistical property does whitening enforce that simple normalization does not?

## Architecture Onboarding

- Component map:
  - SR-Sinkhorn module (Algorithm 3): Core preprocessing. Takes gradient matrix `∇ ∈ R^(m×n)`, applies L alternating row/column normalizations. Returns preprocessed gradient.
  - SinkGD optimizer (Algorithm 4): Wrapper managing learning rate scheduling. Calls SR-Sinkhorn on linear layer gradients, applies Adam to other layers.
  - Projection operators: `Q(W)` computes row norms (diagonal matrix), `R(W)` computes column norms. Applied as `Q(W)^(-1)W` and `WR(W)^(-1)`.
  - Scaling factors: `√n` (row), `√m` (column) ensure consistent Frobenius norm `√(nm)` matching Adam's sign gradient magnitude.

- Critical path:
  1. Backward pass produces gradient `∇_t` for each linear layer
  2. Initialize `X = ∇_t`
  3. For L iterations: `X ← √n·Q(X)^(-1)·X` then `X ← √m·X·R(X)^(-1)`
  4. Update: `W_(t+1) = W_t - η_t·α·X` where α=0.05 is layer-specific scaling
  5. Non-linear layers updated via standard Adam

- Design tradeoffs:
  1. **L (number of SR-Sinkhorn iterations)**: Paper uses L=5. Ablation (Table 4) shows L=1 vs L=5 difference of 0.08 perplexity on 130M model.
  2. **Hybrid optimizer choice**: SinkGD applied only to linear layers in transformer blocks.
  3. **Learning rate scaling (α=0.05)**: Global LR=0.02 with α=0.05 gives effective LR=0.001 for linear layers.

- Failure signatures:
  1. **Divergence at high learning rates**: If effective LR too high for normalized gradients, loss will spike early.
  2. **No improvement over SGD**: If L too low and normalization insufficient, behavior collapses to simple SGD.
  3. **Memory not reduced**: If implementation stores intermediate matrices during SR-Sinkhorn, memory savings lost.

- First 3 experiments:
  1. **Reproduce Table 1 baseline on 60M LLaMA**: Train for 10K steps on C4 with SinkGD vs Adam. Target: SinkGD perplexity ≤31.0, memory ~0.23G.
  2. **Ablation on SR-Sinkhorn iterations L**: Train 130M model with L∈{1, 3, 5, 10} for 10K steps. Plot perplexity vs L.
  3. **Learning rate sweep for α**: Fix global LR=0.02, sweep α∈{0.01, 0.025, 0.05, 0.1} on 130M model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the fixed-point solution from MultiNorm provably solve the original multi-constraint optimization problem (Eq. 5), or merely approximate it?
- Basis in paper: [explicit] "While we cannot guarantee that it solves (5), we can assert that our algorithm converges to a fixed-point solution with arbitrary precision."
- Why unresolved: Theorem 3.6 proves convergence to a fixed point but does not establish conditions under which fixed points correspond to optimal solutions of the NP-hard multi-normalization problem.
- What evidence would resolve it: Theoretical characterization of when fixed points equal optimal solutions, or counterexamples showing divergence between them.

### Open Question 2
- Question: How can the MNGD framework be extended to non-matrix parameters such as embedding layers, biases, and layer normalization weights?
- Basis in paper: [explicit] "Since SinkGD utilizes matrix-level operations on gradients, it can only be applied to 2D parameters. Therefore, in our experiments, we only apply SinkGD on all linear projection weights in transformer blocks."
- Why unresolved: The row-wise and column-wise normalization scheme inherently requires 2D structure, leaving other parameter types dependent on Adam.
- What evidence would resolve it: A principled extension of multi-normalization to arbitrary tensor dimensions with preserved convergence guarantees.

### Open Question 3
- Question: Do the efficiency advantages of SinkGD persist at scales beyond 1B parameters (e.g., 7B, 70B)?
- Basis in paper: [inferred] Experiments are limited to models up to 1.3B parameters. The paper claims O(mn) complexity but validates this only on smaller models.
- Why unresolved: Scaling behavior of the iterative normalization procedure under distributed training regimes remains untested.
- What evidence would resolve it: Benchmarks training larger models comparing wall-clock time, memory, and perplexity against Adam and other memory-efficient baselines.

## Limitations
- Theoretical guarantees limited to fixed-point convergence, not global optimality of the original NP-hard problem
- Empirical validation restricted to models up to 1.3B parameters, leaving trillion-parameter scaling unverified
- Hybrid approach (SinkGD for linear layers, Adam for others) means true statelessness is not achieved
- Column-wise ℓ2 relaxation lacks theoretical guarantees of preserving SWAN's convergence properties

## Confidence
- **High confidence**: The alternating Sinkhorn procedure converges to a fixed-point (Theorem 3.6 is well-established). Memory reduction estimates are straightforward calculations.
- **Medium confidence**: The 3× speedup claim relies on throughput measurements from a single setup. The equivalence between SR-Sinkhorn convergence and classical Sinkhorn's linear rate is stated but not fully verified.
- **Low confidence**: The generalization to extremely large models (>10B parameters) is untested. The impact of gradient matrix structure assumptions on non-transformer architectures is unknown.

## Next Checks
1. **Convergence stability under distribution shift**: Train SinkGD on a dataset with sudden vocabulary shifts mid-training. Monitor perplexity stability and compare to Adam's adaptation speed.
2. **Memory-accuracy Pareto frontier**: Systematically vary L (1, 3, 5, 10) and α (0.01, 0.025, 0.05, 0.1) on a 350M model. Plot memory usage vs. perplexity after 50K steps.
3. **Extreme scaling behavior**: Implement SinkGD on a 10B parameter model (or simulate via gradient checkpointing) and train for 10K steps. Measure actual memory usage and training throughput.