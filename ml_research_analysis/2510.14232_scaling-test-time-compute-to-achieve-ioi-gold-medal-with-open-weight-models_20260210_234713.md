---
ver: rpa2
title: Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models
arxiv_id: '2510.14232'
source_url: https://arxiv.org/abs/2510.14232
tags:
- solutions
- each
- zhang
- cluster
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work tackles the challenge of achieving gold-medal-level\
  \ performance on the International Olympiad in Informatics (IOI) using open-weight\
  \ models. The proposed approach, GENCLUSTER, employs large-scale parallel solution\
  \ generation, behavioral clustering of candidate programs, ranking via a tournament\
  \ mechanism using an LLM-as-a-judge, and a round-robin submission strategy to maximize\
  \ scores under the IOI\u2019s 50-submission limit per problem."
---

# Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models

## Quick Facts
- arXiv ID: 2510.14232
- Source URL: https://arxiv.org/abs/2510.14232
- Reference count: 14
- Key outcome: GENCLUSTER achieves gold-medal IOI score (446.75) using open-weight model gpt-oss-120b

## Executive Summary
This work tackles the challenge of achieving gold-medal-level performance on the International Olympiad in Informatics (IOI) using open-weight models. The proposed approach, GENCLUSTER, employs large-scale parallel solution generation, behavioral clustering of candidate programs, ranking via a tournament mechanism using an LLM-as-a-judge, and a round-robin submission strategy to maximize scores under the IOI's 50-submission limit per problem. Experiments show that GENCLUSTER enables the open-weight model gpt-oss-120b to achieve a gold-medal score of 446.75 on IOI 2025, marking the first time an open-weight model reaches this level. Performance scales with generation budget, and ablation studies confirm the effectiveness of behavioral clustering and tournament ranking.

## Method Summary
GENCLUSTER is a framework that generates K candidate solutions per subtask, creates 100 test cases per subtask using separate LLM calls, clusters solutions by behavioral equivalence (output hashing across test cases), ranks clusters through tournament-style pairwise comparisons using an LLM judge, and submits solutions via round-robin selection from top-ranked clusters. The method leverages test-time compute scaling, with larger K increasing the probability of generating correct solutions and improving selection quality through larger candidate pools.

## Key Results
- GENCLUSTER achieves 446.75 on IOI 2025, crossing the gold medal threshold of 438.3
- Performance scales from 332.27 (K=50) to 446.75 (K=5000) with generation budget
- Tournament ranking with 10 games per cluster outperforms heuristic methods (299.87-314.22 range for baselines)
- Behavioral clustering with 100 test cases achieves F1-score of 0.95, significantly improving selection accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavioral clustering groups solutions by functional equivalence, improving selection efficiency under submission constraints.
- Mechanism: Solutions producing identical outputs across generated test cases are clustered together. This reduces the effective search space from K candidates to fewer distinct behavioral groups, allowing the submission strategy to sample across diverse solution strategies rather than redundant variants.
- Core assumption: Solutions that produce identical outputs on representative test cases have similar correctness likelihood on hidden test cases.
- Evidence anchors:
  - [abstract]: "combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets"
  - [section 3.2]: "Solutions that produce exactly the same outputs are grouped into the same cluster"
  - [section 4.4, Figure 4]: Cluster purity (F1-score) improves from ~0.89 to ~0.95 as test cases increase from 5 to 100, validating that more test cases better separate correct from incorrect solutions
  - [corpus]: Weak direct corpus evidence on behavioral clustering for code; AlphaCode (Li et al., 2022) uses clustering but with different methodology

### Mechanism 2
- Claim: Tournament-based LLM ranking outperforms heuristic-based selection methods for identifying high-quality solution clusters.
- Mechanism: Each cluster representative (longest reasoning trace) competes in G=10 pairwise comparisons against randomly selected clusters. The LLM judge evaluates correctness and selects winners. Clusters are ranked by win count.
- Core assumption: The LLM-as-judge can reliably distinguish solution quality in competitive programming contexts, and reasoning length correlates with correctness within a cluster.
- Evidence anchors:
  - [section 4.3, Table 1]: GENCLUSTER with tournament ranking achieves 446.75 vs. Cluster-Size (299.87), Cluster-Majority (314.22), and longest-trace-only (277.36)
  - [section 4.5, Figure 5]: Score improves from ~350 (1 game) to ~446 (10 games), saturating after ~10 rounds
  - [section 4.6, Figure 6]: In 35/39 subtasks, the best solution appears in top-50 ranked clusters
  - [corpus]: OpenAI (2025) reports LLM-as-judge for solution selection; corpus evidence supports viability but not superiority over alternatives

### Mechanism 3
- Claim: Test-time compute scales performance by increasing the probability of generating correct solutions and improving selection through larger candidate pools.
- Mechanism: Generating more solutions (K) increases the likelihood that at least one correct solution exists in the pool. Larger pools enable finer-grained clustering and more robust tournament ranking.
- Core assumption: The model's pass@K rate increases with K, and selection mechanisms can identify correct solutions from larger pools without proportional degradation.
- Evidence anchors:
  - [section 4.2, Figure 3]: Submitted score scales from 332.27 (K=50) to 446.75 (K=5000); unconstrained Score@K scales from 335.45 to 499.51
  - [section 4.1, Figure 2]: gpt-oss-120b shows consistent scaling; gpt-oss models exhibit stronger gains than DeepSeek-R1 and Qwen3
  - [section 4.7, Figure 7]: Longer reasoning traces (up to 120K tokens) correlate with higher scores for gpt-oss models
  - [corpus]: AlphaCode 2 (Leblond et al., 2023) demonstrates scaling to 1M solutions; consistent with test-time scaling literature

## Foundational Learning

- Concept: **Test-Time Compute**
  - Why needed here: The entire framework relies on allocating computational resources during inference rather than training. Understanding the trade-off between generation budget and selection quality is essential.
  - Quick check question: If you double K from 1000 to 2000, would you expect the submitted score to double? Why or why not?

- Concept: **Behavioral Clustering vs. Semantic Clustering**
  - Why needed here: The paper clusters by execution outputs, not code similarity. This distinction is critical—two syntactically different solutions may be behaviorally equivalent and vice versa.
  - Quick check question: Why might semantic clustering (code similarity) fail for competitive programming where multiple correct algorithms exist?

- Concept: **LLM-as-a-Judge Limitations**
  - Why needed here: Tournament ranking depends on LLM judgment quality. Understanding position bias, calibration issues, and failure modes helps interpret ranking reliability.
  - Quick check question: If the LLM judge has 70% accuracy on pairwise comparisons, how many games per cluster are needed to achieve stable rankings?

## Architecture Onboarding

- Component map:
  1. **Parallel Generator**: Produces K candidate C++ solutions per subtask using the model (Figure 8 prompt)
  2. **Test Case Generator/Validator**: Creates 100 test cases per subtask using separate LLM calls (Figures 9-10 prompts); validators confirm constraints
  3. **Behavioral Clusterer**: Executes all solutions on test cases, groups by output hash, removes clusters with runtime errors
  4. **Tournament Ranker**: Runs G=10 pairwise comparisons per cluster using LLM judge (Figure 11 prompt)
  5. **Submission Manager**: Round-robin selection from ranked clusters, prioritizing longest reasoning traces within clusters

- Critical path: Generation → Test case creation → Clustering → Tournament → Submission. Bottleneck is parallel generation (K=5000 per subtask × 39 subtasks).

- Design tradeoffs:
  - More test cases (→100) improve cluster purity but increase cluster count, making ranking harder (Figure 4)
  - More tournament games (G>10) show diminishing returns (Figure 5)
  - Longer reasoning traces correlate with correctness but increase latency and may hit token limits

- Failure signatures:
  - Cluster purity F1 <0.90: Test cases insufficient; increase generator/validator count
  - Best solution not in top-50 clusters (Figure 6): Tournament ranking failing; check for position bias or judge calibration
  - Large gap between submitted score and Score@K: Selection mechanism degrading; revisit ranking strategy

- First 3 experiments:
  1. Replicate Table 1 ablations on a subset of subtasks to validate your implementation matches the paper's GENCLUSTER vs. Random/Longest/Cluster-Size comparisons.
  2. Sweep test case count (5, 25, 50, 100) and measure cluster purity vs. final score trade-off to find optimal for your compute budget.
  3. Test tournament games G ∈ {1, 5, 10, 20} to verify saturation point matches Figure 5 for your model/judge combination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the growing performance gap between unconstrained maximum scores (Score@K) and submitted scores be closed through more effective ranking and selection mechanisms?
- Basis in paper: [explicit] The authors state that as generation counts increase, "the gap gets larger... highlights the need for more effective ranking and selection strategies to fully realize the available potential" (Section 4.2).
- Why unresolved: Current selection strategies fail to fully utilize the potential of large candidate pools, leaving the achieved submission score significantly lower than the theoretical maximum available in the generated set.
- What evidence would resolve it: A selection method that maintains a shrinking or constant gap between the theoretical best (Score@K) and the submitted score as K scales beyond 5000.

### Open Question 2
- Question: Can the "LLM-as-a-judge" paradigm be refined to overcome the performance saturation observed after a limited number of tournament rounds?
- Basis in paper: [explicit] The paper notes that score improvement from tournament rounds "largely saturates after 10 rounds," suggesting this may reflect "inherent limitations of the LLM-as-a-judge paradigm" (Section 4.5).
- Why unresolved: Simply increasing tournament compute (games per cluster) does not yield linear improvements, implying a ceiling in the model's ability to reliably distinguish solution quality via pairwise comparison.
- What evidence would resolve it: A modified judging mechanism that continues to improve ranking accuracy (and subsequent final scores) beyond the observed saturation point of 10 rounds per cluster.

### Open Question 3
- Question: How can the trade-off between cluster purity and cluster fragmentation caused by increasing the number of test cases be optimized?
- Basis in paper: [explicit] Section 4.4 concludes that while more test cases improve cluster purity, they also increase the number of distinct clusters, "making it more difficult to identify the most promising ones under submission constraints."
- Why unresolved: The paper identifies this tension but does not propose a method to find the equilibrium where high purity is achieved without generating a prohibitive number of clusters to rank.
- What evidence would resolve it: An analysis identifying a specific optimal number of test cases that maximizes the probability of the correct solution appearing in the top-ranked cluster.

### Open Question 4
- Question: What specific architectural or training characteristics determine a model's ability to scale performance with test-time compute on competitive programming tasks?
- Basis in paper: [inferred] The results show `gpt-oss-120b` scales effectively with increased generation budgets, whereas `Qwen3` and `DeepSeek-R1` saturate quickly (Section 4.1, Figure 2), suggesting inherent differences in scalability.
- Why unresolved: The paper demonstrates that scaling behavior varies drastically across open-weight models but does not investigate the underlying causes for this disparity in test-time compute efficiency.
- What evidence would resolve it: A comparative analysis of model training datasets or reasoning mechanisms that correlates specific model properties with the slope of performance improvement over increasing K values.

## Limitations
- Unavailability of gpt-oss-120b model, which achieved the gold medal score
- Behavioral clustering effectiveness depends on test case coverage and generator quality
- Tournament ranking quality bounded by LLM-as-judge reliability and potential position bias
- 50-submission constraint is IOI-specific and may not generalize to other programming competitions

## Confidence
- **High Confidence**: Test-time compute scaling effects (Figure 3 shows consistent improvements with K); behavioral clustering methodology (F1-score metrics demonstrate measurable improvements); tournament ranking implementation (ablation studies show clear ranking superiority)
- **Medium Confidence**: Gold-medal achievement claim (depends on gpt-oss-120b availability); generalizability across problem types (tested only on IOI 2025); LLM-as-judge reliability (corroborated by corpus but not independently verified)
- **Low Confidence**: Performance ceiling (scaling appears to saturate; unclear if gold threshold is absolute or model-dependent); cross-model comparability (different max token limits complicate direct comparisons)

## Next Checks
1. **Ablation Replication**: Replicate the Table 1 comparison on a subset of subtasks using DeepSeek-R1 or Qwen3 as both generator and judge, verifying that GENCLUSTER with tournament ranking outperforms baseline selection methods by similar margins.

2. **Test Case Coverage Analysis**: Systematically vary test case count (5, 25, 50, 100) while measuring cluster purity F1-score and final submitted scores to quantify the trade-off and verify that 100 test cases provides optimal balance for your hardware constraints.

3. **Tournament Game Sensitivity**: Sweep tournament games G ∈ {1, 5, 10, 20} using your available reasoning model/judge combination to verify saturation behavior matches Figure 5 and determine optimal game count for your specific setup.