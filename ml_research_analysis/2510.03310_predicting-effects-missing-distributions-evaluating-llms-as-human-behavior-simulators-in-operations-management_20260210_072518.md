---
ver: rpa2
title: 'Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior
  Simulators in Operations Management'
arxiv_id: '2510.03310'
source_url: https://arxiv.org/abs/2510.03310
tags:
- human
- min-p0
- llms
- data
- top-p0
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can replicate most hypothesis-test
  outcomes in behavioral operations management, but their response distributions diverge
  significantly from human data. The study tested 11 LLMs across nine OM lab experiments,
  measuring distributional alignment using Wasserstein distance.
---

# Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management

## Quick Facts
- arXiv ID: 2510.03310
- Source URL: https://arxiv.org/abs/2510.03310
- Reference count: 40
- Large language models replicate most hypothesis-test outcomes in behavioral operations management but show significant distributional divergence from human data

## Executive Summary
Large language models can effectively replicate hypothesis-level effects and decision biases found in behavioral operations management experiments, but their response distributions differ substantially from human participants. The study evaluated 11 LLMs across nine OM lab experiments using Wasserstein distance to measure distributional alignment. While LLMs captured key behavioral effects, lightweight interventions like Chain-of-Thought prompting and hyperparameter tuning (temperature, top-p, min-p) were necessary to improve distributional alignment, though with computational trade-offs.

## Method Summary
The study tested 11 large language models across nine behavioral operations management lab experiments, measuring their ability to replicate both hypothesis-level effects and human response distributions. Researchers used Wasserstein distance as the primary metric for distributional alignment and examined the impact of interventions including Chain-of-Thought prompting and hyperparameter tuning (temperature, top-p, min-p sampling parameters). The evaluation focused on whether LLMs could serve as substitutes for human participants in OM research contexts.

## Key Results
- LLMs captured most hypothesis-level effects but showed large distributional gaps from human data, especially in multi-agent settings
- Chain-of-Thought prompting and hyperparameter tuning substantially improved distributional alignment
- Higher temperature settings reduced hallucination but increased computational costs
- Smaller models with tuning could match larger models' performance
- LLMs suitable for hypothesis-level prototyping but not direct substitutes for human data

## Why This Works (Mechanism)
LLMs demonstrate emergent behavioral simulation capabilities through their pretraining on diverse human-generated text, allowing them to capture statistical patterns of human decision-making. The models learn to reproduce decision biases and strategic behaviors through exposure to similar contexts in their training data. However, the gap between sampled distributions and actual human responses reveals fundamental differences in how LLMs generate responses versus human cognitive processes, particularly in multi-agent scenarios where social dynamics become more complex.

## Foundational Learning

**Wasserstein Distance**: Metric for measuring distributional differences between LLM outputs and human responses. Needed to quantify how well model distributions match human behavior beyond simple accuracy metrics. Quick check: Compute distance between two empirical distributions to verify sensitivity to shape differences.

**Chain-of-Thought Prompting**: Technique that guides models through intermediate reasoning steps before final responses. Improves distributional alignment by encouraging more deliberate decision processes. Quick check: Compare distributions with and without CoT to verify improvement in alignment metrics.

**Hyperparameter Tuning**: Adjustment of sampling parameters (temperature, top-p, min-p) to control output diversity and quality. Critical for balancing between exploration and exploitation in behavioral simulation. Quick check: Sweep temperature values and observe changes in distributional metrics.

## Architecture Onboarding

Component Map: Data Collection -> LLM Evaluation -> Distributional Analysis -> Intervention Testing -> Results Synthesis

Critical Path: Human experiment data collection -> LLM response generation -> Wasserstein distance computation -> Intervention application -> Hypothesis effect validation

Design Tradeoffs: Computational efficiency vs. distributional fidelity; model size vs. intervention effectiveness; exploration vs. exploitation in sampling

Failure Signatures: Large Wasserstein distances despite hypothesis effect replication; inconsistent distributional alignment across different experiment types; intervention effectiveness varying by model size

First Experiments:
1. Compute baseline Wasserstein distances between LLM and human distributions for a simple decision-making task
2. Apply Chain-of-Thought prompting to the same task and measure distributional improvement
3. Sweep temperature parameter values to identify optimal setting for distributional alignment

## Open Questions the Paper Calls Out

None provided in the source material.

## Limitations

- LLMs cannot fully substitute for human data when distributional fidelity is critical
- Effectiveness of interventions may not generalize beyond tested OM experiments
- Computational costs increase with higher temperature settings and more complex interventions

## Confidence

High confidence: LLMs replicate hypothesis-level effects across most experiments; CoT prompting and hyperparameter tuning improve distributional alignment

Medium confidence: Rank-order correlations between LLM and human responses are consistently positive; smaller models can match larger ones with tuning

Medium confidence: Higher temperature settings reduce hallucination while improving alignment

Low confidence: Generalizability beyond the nine specific OM experiments tested; long-term stability of tuning interventions

## Next Checks

1. Test distributional alignment across additional OM paradigms not included in the original nine experiments, particularly those involving time-pressure or high-stakes decision contexts

2. Conduct longitudinal validation to assess whether hyperparameter tuning interventions maintain effectiveness across multiple experimental sessions and different prompt formulations

3. Compare LLM-simulated distributions against human data from diverse demographic and cultural backgrounds to evaluate generalizability of findings