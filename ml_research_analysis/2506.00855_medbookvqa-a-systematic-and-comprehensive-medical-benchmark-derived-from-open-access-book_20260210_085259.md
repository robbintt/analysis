---
ver: rpa2
title: 'MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from
  Open-Access Book'
arxiv_id: '2506.00855'
source_url: https://arxiv.org/abs/2506.00855
tags:
- medical
- figure
- modality
- wang
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces MedBookVQA, a comprehensive medical multimodal
  benchmark derived from open-access medical textbooks. It proposes a pipeline to
  extract medical figures and align them with contextual narratives, generating 5,000
  clinically relevant VQA questions across five categories: modality recognition,
  disease classification, anatomical identification, symptom diagnosis, and surgical
  procedures.'
---

# MedBookVQA: A Systematic and Comprehensive Medical Benchmark Derived from Open-Access Book

## Quick Facts
- arXiv ID: 2506.00855
- Source URL: https://arxiv.org/abs/2506.00855
- Authors: Sau Lai Yip; Sunan He; Yuxiang Nie; Shu Pui Chan; Yilin Ye; Sum Ying Lam; Hao Chen
- Reference count: 40
- Primary result: Introduces MedBookVQA, a 5,000-question medical VQA benchmark derived from open-access textbooks, revealing significant performance gaps in current MLLMs, particularly for tasks requiring medical knowledge and cross-modal reasoning.

## Executive Summary
MedBookVQA addresses the need for comprehensive medical multimodal benchmarks by systematically extracting figures and narratives from open-access medical textbooks. The pipeline generates 5,000 clinically relevant VQA questions across five cognitive task categories, employing a hierarchical annotation system for fine-grained performance analysis. Evaluation reveals that proprietary models significantly outperform open-sourced, medical-specific, and reasoning-enhanced models, particularly in tasks requiring deep medical knowledge and cross-modal reasoning, exposing critical limitations in current GMAI systems.

## Method Summary
The method extracts medical figures from DOAB textbooks using MinerU for layout parsing, then generates figure-information pairs by extracting captions, categories, reference names, and contextual texts with InternVL2-8B. InternVL2.5-78B creates up to 6 VQA questions per pair across five predefined types, with Qwen-VL-Max generating distractors. A three-stage filtering process removes unsuitable questions (Qwen-VL-Max), text-answerable questions (DeepSeek-R1), and problematic entries through manual verification. Qwen-VL-72B provides hierarchical labels across 42 modalities, 125 anatomical structures, and 31 departments. The final benchmark contains 5,000 MCQ questions evaluated using zero-shot inference with temperature=0.

## Key Results
- Proprietary models (Gemini2.5-Pro, GPT-4.1, Claude3.7-Sonnet) significantly outperform open-sourced models, medical-specific models, and reasoning-enhanced models
- Performance varies substantially across the five VQA task types, with reasoning and knowledge-intensive tasks showing the largest performance gaps
- Hierarchical analysis reveals specific modality and anatomical structure weaknesses, particularly in histopathology and rare disease diagnosis
- Current MLLMs struggle with cross-modal reasoning in medical contexts despite strong general performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Medical textbooks, when parsed systematically, yield figure-narrative pairs that generate higher-quality clinical VQA questions than alternative sources like journal articles.
- **Mechanism:** Textbooks provide condensed, clinically validated knowledge rather than exploratory findings. By extracting figures and aligning them with explicit textual references (FIGname + FIGtexts), the pipeline ensures each image is contextually grounded in canonical medical descriptions. This grounding then informs a generative model (InternVL2.5-78B) to produce targeted VQA questions across five distinct cognitive tasks (modality, anatomy, symptom, disease, surgery).
- **Core assumption:** The knowledge in open-access medical textbooks is both comprehensive enough for benchmark diversity and reliable enough to serve as ground truth for VQA answer generation without introducing systematic bias.
- **Evidence anchors:**
  - [abstract] "...the potential of medical textbooks for benchmark development remains underexploited. Here, we present MedBookVQA... derived from open-access medical textbooks."
  - [section 3.1] "Medical figure-information pairs are then obtained... each consisting of the figure (Figure), figure captions (FIGcaption), figure categories (FIGcategory), figure reference names (FIGname), and sentences... (FIGtexts)."
  - [section 3.2] "InternVL2.5-78B [8] is prompted to generate up to 6 VQAs for each figure-information pair from five predefined VQA types..."
  - [corpus] Weak direct evidence on textbook quality in corpus; MM-Skin (arXiv:2505.06152) uses textbook data for dermatology VLM, suggesting growing interest in this data source.
- **Break condition:** If textbooks introduce systematic domain gaps (e.g., outdated practices, missing rare pathologies), the benchmark will test textbook knowledge rather than clinical competence.

### Mechanism 2
- **Claim:** A hierarchical annotation system (Modality → Category, Anatomy → 4 levels, Department → Category) enables fine-grained diagnostic analysis of model failures across medical subdomains.
- **Mechanism:** Each VQA entry is labeled by a high-capacity model (Qwen-VL-72B) at the lowest level of each taxonomy. These labels are then aggregated for analysis. This structure allows researchers to pinpoint *where* a model fails—not just "poor at radiology," but specifically "poor at identifying CT findings in the Head (Musculoskeletal system) for Neurosurgery cases."
- **Core assumption:** The labeling model (Qwen-VL-72B) assigns hierarchical labels with high enough accuracy that aggregate performance statistics are meaningful.
- **Evidence anchors:**
  - [abstract] "A multi-tier annotation system categorizes queries through hierarchical taxonomies encompassing medical imaging modalities (42 categories), body anatomies (125 structures), and clinical specialties (31 departments), enabling nuanced analysis..."
  - [section 3.3] "...A four-level tree is constructed for anatomy and a two-level one for department and modality... Qwen-VL-72B [38] labels each VQA at the lowest level."
  - [section 5.1.3] "We select the highest-performing models... and common types... using the hierarchical label system to perform a more fine-grained analysis..."
  - [corpus] GMAI-MMBench (referenced in section 2 and 3.3) uses a similar lexico-semantic hierarchy, suggesting this is an established evaluation pattern.
- **Break condition:** If the labeling model makes systematic errors (e.g., mislabeling a specific modality), downstream analysis will reflect the labeler's errors, not the evaluated model's true capabilities.

### Mechanism 3
- **Claim:** A multimodal filtering step, using a text-only LLM to detect answerable questions, removes cases where the question can be solved from text alone, ensuring the benchmark tests visual reasoning.
- **Mechanism:** After VQA generation and distractor creation, each question-choice pair is passed to DeepSeek-R1 (a text-only reasoning LLM) *without the image*. If DeepSeek-R1 can determine the correct answer from text alone, the VQA pair is discarded. This creates a "minimal sufficient image" property: the image is necessary to answer the question.
- **Core assumption:** DeepSeek-R1's ability to answer from text correlates inversely with the question's multimodal necessity; a question answerable by text *is not* multimodal.
- **Evidence anchors:**
  - [section 3.2] "The second step selects VQAs that require multimodality (i.e. the image) to answer. The question and choices are input to the LLM DeepSeek-R1 [10]... Those deemed answerable are excluded as the images are redundant."
  - [abstract] "...Evaluation... reveals significant performance gaps... particularly in tasks requiring medical knowledge and cross-modal reasoning."
  - [corpus] Similar multimodal necessity checks are employed in benchmarks like MMMU and MathVerse to ensure visual grounding.
- **Break condition:** If DeepSeek-R1 fails to detect subtle textual shortcuts or if it hallucinates answers, the filtering will be imperfect, either retaining trivially answerable questions or discarding valid multimodal ones.

## Foundational Learning

- **Concept:** Visual Question Answering (VQA) as a multimodal reasoning task.
  - **Why needed here:** The entire benchmark is built around this paradigm. You must understand that VQA requires simultaneous processing of an image and a text query to produce a text answer.
  - **Quick check question:** Given an X-ray image and the text "Is there a fracture in the left radius?", what are the three modalities involved in the input, and what is the output modality?

- **Concept:** Multimodal Large Language Models (MLLMs) / Vision-Language Models (VLMs).
  - **Why needed here:** The benchmark is designed to evaluate precisely these kinds of models. You need to know that these are LLMs extended with a visual encoder and alignment layers.
  - **Quick check question:** What are the two core components of an MLLM that are typically pre-trained separately before being aligned?

- **Concept:** Benchmark Construction via Scalable Pipelines.
  - **Why needed here:** The paper's primary contribution is a novel, automated pipeline for generating a benchmark from a raw data source (textbooks). Understanding this shift from manual curation to scalable, model-assisted generation is crucial.
  - **Quick check question:** What is the primary advantage of using an automated pipeline over manual curation for a benchmark with 5,000 entries across dozens of medical specialties?

## Architecture Onboarding

- **Component map:** Data Source & Ingestion: DOAB → textbooks → MinerU (layout parsing) → figure-caption pairs. Figure-Info Extraction: Figure, FIGcaption, FIGcategory (InternVL2-8B), FIGname (regex), FIGtexts (InternVL2-8B). VQA Generation Module: InternVL2.5-78B takes (Figure + Context) → generates up to 6 VQA pairs (M, A, S, D, SO). MCQ Formatting Module: Qwen-VL-Max takes (Question + Correct Answer) → generates 3 distractors. Filtering Suite: Suitability Filter: Qwen-VL-Max assesses if VQA relates to human disease/surgery. Multimodality Filter: DeepSeek-R1 (text-only) checks if Q is answerable without image. Manual Verification: Humans check for incorrect answers, text-in-image leaks, and language shortcuts. Hierarchical Labeling Module: Qwen-VL-72B assigns labels from predefined taxonomies (Anatomy, Modality, Department). Benchmark Dataset: 5,000 final VQA entries, each with Figure, MCQ, and hierarchical labels. Evaluation Harness: Script to run inference on target MLLMs (zero-shot, temp=0) and compute accuracy per task and per label.

- **Critical path:** The quality of the final benchmark hinges on the **VQA Generation Module (3)** and the **Multimodality Filter (5b)**. If the generated questions are trivial or hallucinated, or if the filter fails to remove text-answerable cases, the entire evaluation loses validity. The manual verification step is a crucial backstop.

- **Design tradeoffs:**
  - *Scalability vs. Precision:* Using generative models is highly scalable but introduces potential for hallucination, which the three-stage filtering process attempts to mitigate.
  - *General vs. Specialized:* The benchmark aims for broad coverage. This makes it a powerful tool for general assessment but may lack the depth of a single-specialty benchmark.
  - *Automation vs. Expertise:* Manual verification is used, but full expert curation is infeasible at this scale. The system trades the absolute rigor of expert-only creation for the breadth enabled by AI-assisted construction.

- **Failure signatures:**
  - **Task imbalance:** High performance on "Modality Recognition" but low scores on "Disease Diagnosis" suggests models are good at low-level visual classification but struggle with higher-level clinical reasoning.
  - **Model category inversion:** If a medical-specific MLLM (e.g., HuatuoGPT-Vision) consistently underperforms a general proprietary MLLM (e.g., GPT-4o), it may indicate overfitting to its specific training distribution.
  - **Anatomical/Modal blind spots:** Consistently low scores in specific nodes of the hierarchy (e.g., "Histopathology," "Skin") point to specific gaps in a model's training data or visual encoder capabilities.

- **First 3 experiments:**
  1. **Baseline Evaluation:** Run all MLLM categories on the full MedBookVQA benchmark to establish the performance hierarchy and identify the overall SOTA.
  2. **Ablation by Task Type:** Isolate model performance for each of the 5 VQA types (M, A, S, D, SO). This will pinpoint which cognitive clinical tasks are the primary bottlenecks for current MLLMs.
  3. **Failure Analysis on a Slice:** Select a challenging slice (e.g., "Histopathology" modality) and manually review errors from top models to categorize failure modes (visual misinterpretation, lack of knowledge, confusion between similar pathologies).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multimodal reasoning strategies be specifically adapted for medical domains to ensure consistent performance gains over standard models?
- **Basis in paper:** [explicit] The authors state that current reasoning enhancements are "insufficient for complex medical domains, necessitating further development in reasoning strategies and contextual understanding specific to medical tasks."
- **Why unresolved:** Reasoning-enhanced models (e.g., VL-Reasoner) generally underperformed compared to proprietary general models, showing inconsistent benefits.
- **What evidence would resolve it:** A reasoning model architecture that demonstrates statistically significant improvements on the Symptom Diagnosis and Disease Recognition tasks compared to non-reasoning baselines.

### Open Question 2
- **Question:** To what extent do linguistic shortcuts in LLM-generated distractors inflate performance metrics on automated medical benchmarks?
- **Basis in paper:** [explicit] The paper acknowledges that the generative models used to create multiple-choice questions "may introduce linguistic shortcuts, potentially affecting the accurate assessment of the models' capabilities."
- **Why unresolved:** The automated pipeline relied on LLMs for distractor generation without an analysis of how language cues might allow models to guess correctly without medical knowledge.
- **What evidence would resolve it:** A comparative study evaluating model accuracy on the MedBookVQA benchmark using LLM-generated distractors versus human-curated, adversarially filtered distractors.

### Open Question 3
- **Question:** What training methodologies are required to bridge the performance gap between proprietary general MLLMs and open-sourced medical MLLMs?
- **Basis in paper:** [inferred] While open-sourced medical models showed specialization, the authors note "room for improvement in generalizability and adaptability" as they lagged behind proprietary models in overall performance.
- **Why unresolved:** Specialized medical training did not enable models like HealthGPT to surpass general models like Gemini, particularly in complex tasks requiring clinical knowledge.
- **What evidence would resolve it:** An open-sourced medical model trained on textbook-derived data that matches or exceeds the performance of top proprietary models on the Disease Diagnosis subtask.

## Limitations

- **Labeling Model Reliability:** The benchmark relies heavily on Qwen-VL-72B for hierarchical annotation, which could introduce systematic biases that distort performance analysis.
- **Textbook Knowledge Gap:** Medical textbooks may lag behind current clinical practice, potentially testing textbook knowledge rather than contemporary clinical competence.
- **Manual Verification Opacity:** The final manual verification stage removed 609 VQAs, but criteria and annotator qualifications remain unspecified, creating uncertainty about potential biases in the final evaluation set.

## Confidence

**High Confidence:** The benchmark construction pipeline is well-specified and reproducible. The hierarchical annotation system design and the multimodality filtering mechanism are clearly articulated and theoretically sound.

**Medium Confidence:** The claim that proprietary models significantly outperform open-sourced, medical-specific, and reasoning-enhanced models is supported by the evaluation results, but this could reflect both model capability differences and potential labeling biases in the hierarchical system.

**Low Confidence:** The assertion that the benchmark exposes "critical limitations in current GMAI systems" is plausible given the performance gaps observed, but without error analysis across specific clinical scenarios, it's unclear whether these limitations represent fundamental architectural constraints or domain-specific training data deficiencies.

## Next Checks

1. **Labeling Model Audit:** Conduct an error analysis of Qwen-VL-72B's hierarchical labels on a stratified sample of 100 VQAs across all modalities and anatomical levels. Compare against expert annotations to quantify labeling accuracy and identify systematic biases that could distort performance analysis.

2. **Temporal Knowledge Validation:** Select 50 VQAs involving diagnostic or therapeutic concepts and verify whether the correct answers align with current clinical guidelines (2023-2025) rather than potentially outdated textbook information. This would quantify the potential textbook knowledge gap.

3. **Cross-Domain Generalization Test:** Evaluate top-performing models on MedBookVQA and a contemporary medical benchmark (e.g., MedMCQA or M3L-Bench) using identical hierarchical analysis. Compare performance drops across modalities and task types to identify whether limitations stem from textbook-specific knowledge or fundamental multimodal reasoning gaps.