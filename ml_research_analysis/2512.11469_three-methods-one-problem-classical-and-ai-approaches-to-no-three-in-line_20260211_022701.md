---
ver: rpa2
title: 'Three methods, one problem: Classical and AI approaches to no-three-in-line'
arxiv_id: '2512.11469'
source_url: https://arxiv.org/abs/2512.11469
tags:
- problem
- points
- learning
- grid
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents the first systematic comparison of classical\
  \ and AI approaches to the No-Three-In-Line problem, a classic combinatorial geometry\
  \ problem asking for the maximum number of points that can be placed on an n\xD7\
  n grid with no three collinear. Three methods were evaluated: Integer Linear Programming\
  \ (ILP) using Gurobi, PatternBoost transformer learning, and reinforcement learning\
  \ (PPO)."
---

# Three methods, one problem: Classical and AI approaches to no-three-in-line

## Quick Facts
- arXiv ID: 2512.11469
- Source URL: https://arxiv.org/abs/2512.11469
- Reference count: 25
- Primary result: First systematic comparison of ILP, PatternBoost, and PPO for No-Three-In-Line problem

## Executive Summary
This paper presents the first systematic comparison of classical and AI approaches to the No-Three-In-Line problem, a classic combinatorial geometry problem asking for the maximum number of points that can be placed on an n×n grid with no three collinear. Three methods were evaluated: Integer Linear Programming (ILP) using Gurobi, PatternBoost transformer learning, and reinforcement learning (PPO). ILP achieved provably optimal solutions up to 19×19 grids, while PatternBoost matched optimal performance up to 14×14 grids with 96% test loss reduction. PPO achieved perfect solutions on 10×10 grids but failed at 11×11 grids due to constraint violations. The results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

## Method Summary
Three distinct approaches were evaluated on the No-Three-In-Line problem. ILP formulation uses n² binary variables and O(n³) collinearity constraints solved via Gurobi's branch-and-bound. PatternBoost employs a transformer-based iterative refinement pipeline starting from greedy-generated configurations, using symmetry augmentation and TopPool selection. PPO uses an actor-critic architecture with action masking to enforce valid moves, trained with shaped rewards for constraint satisfaction. All methods target maximizing point placement while preventing three collinear points, with ILP providing exact optimality guarantees and AI methods offering competitive heuristics.

## Key Results
- ILP achieved provably optimal solutions up to 19×19 grids
- PatternBoost matched optimal performance up to 14×14 grids with 96% test loss reduction
- PPO achieved perfect solutions on 10×10 grids but failed at 11×11 grids due to constraint violations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ILP achieves provably optimal solutions through systematic constraint enumeration and branch-and-bound pruning.
- Mechanism: The solver enumerates O(n³) collinear triplet constraints (lines with ≥3 points), then uses branch-and-bound to exhaustively search the binary decision space while pruning infeasible branches. Each grid cell is a binary variable; constraints enforce ≤2 points per line.
- Core assumption: The constraint space remains tractable for the solver's cutting planes and heuristics up to some grid threshold.
- Evidence anchors:
  - [abstract] "ILP achieved provably optimal solutions up to 19×19 grids"
  - [Section 2.2] "The resulting ILP model contains n² binary variables and O(n³) linear constraints"
  - [corpus] Weak corpus support for ILP-specific mechanism; neighboring papers focus on quantum annealing and logic-based decomposition for optimization.
- Break condition: Exponential constraint growth (5,524→6,778 constraints from n=19→20 per Section 4) eventually exceeds tractable search space.

### Mechanism 2
- Claim: PatternBoost generalizes beyond training data by learning reusable geometric patterns through iterative refinement with quality-filtered examples.
- Mechanism: Greedy saturation generates initial configurations → transformer learns next-token placement patterns → TopPool (min-heap of 20K best solutions) maintains high-quality examples → new samples are generated and saturated → cycle repeats for 20 generations. Symmetry augmentation (8× expansion) and canonical deduplication prevent overfitting.
- Core assumption: Greedy-generated configurations contain learnable patterns that transfer to optimal configurations the model never sees.
- Evidence anchors:
  - [abstract] "PatternBoost matched optimal performance up to 14×14 grids with 96% test loss reduction"
  - [Section 3.3] "Test loss... dropping from 3.0 to 0.5 by step 2,000, achieving a 96% reduction"
  - [corpus] GFlowNets paper (FMR=0.589) similarly uses generative models for combinatorial optimization via reward-proportional sampling.
- Break condition: When greedy saturation quality degrades at larger grids (n≥15), learned patterns no longer generalize to optima.

### Mechanism 3
- Claim: PPO learns valid placement policies through shaped rewards but fails when sequential constraint dependencies exceed the policy's planning horizon.
- Mechanism: Each placement is a discrete action; action masking prevents revisiting occupied cells. Reward shaping (+1 valid, -10 violation, +100 perfect) creates gradients toward constraint satisfaction. The 512×2 MLP policy network learns through GAE-advantage estimates over 10M timesteps.
- Core assumption: Sparse rewards with dense penalties can encode geometric constraints without explicit constraint propagation.
- Evidence anchors:
  - [abstract] "PPO achieved perfect solutions on 10×10 grids but failed at 11×11 grids, where constraint violations prevent valid configurations"
  - [Section 3.4] "the policy network struggles to maintain global constraint awareness when the number of placed points exceeds approximately 20"
  - [corpus] Satisfiability Modulo Theory paper (FMR=0.556) addresses combining learned approaches with explicit constraint reasoning—relevant to PPO's limitation.
- Break condition: When collinearity constraint intersections become dense (≈20+ points), local action-value estimates cannot capture global constraint state.

## Foundational Learning

- Concept: **Integer Linear Programming basics (binary variables, linear constraints, branch-and-bound)**
  - Why needed here: ILP is the performance baseline; understanding its formulation reveals why constraint enumeration scales as O(n³).
  - Quick check question: Given a 5×5 grid, approximately how many collinear-triplet constraints exist?

- Concept: **Decoder-only transformer language modeling (next-token prediction, cross-entropy loss)**
  - Why needed here: PatternBoost frames grid placement as sequence generation; the transformer predicts which cell to place next.
  - Quick check question: Why does next-token prediction suit the sequential nature of point placement?

- Concept: **Proximal Policy Optimization (clipped surrogate objective, actor-critic, action masking)**
  - Why needed here: PPO is the RL baseline; action masking is critical for preventing invalid placements during training.
  - Quick check question: What happens if action masking is removed but the reward penalty for occupied cells remains?

## Architecture Onboarding

- Component map:
  - ILP pipeline: Grid → Constraint generator (Algorithm 1) → Gurobi solver → Optimal configuration
  - PatternBoost pipeline: Greedy saturation (Algorithm 2) → Tokenizer → GPT-2 transformer (4L/4H/dim64) → TopPool (20K min-heap) → Iterative refinement (Algorithm 3, 20 generations)
  - PPO pipeline: Grid state → MaskablePPO (512×2 MLP) → Action distribution → Masked sampling → Environment step → GAE advantage → Policy update

- Critical path:
  1. Reproduce ILP baseline on n≤10 to verify constraint generation correctness.
  2. Train PatternBoost on n=10 with reduced generations (5 instead of 20) to validate loss convergence matches Figure 4.
  3. Train PPO on n=10 and confirm 0 violations; then test n=11 to reproduce the failure mode.

- Design tradeoffs:
  - ILP: Exactness vs. scalability—optimal but exponential scaling.
  - PatternBoost: Data quality vs. speed—greedy saturation is fast but produces suboptimal training examples for larger grids.
  - PPO: Exploration vs. constraint satisfaction—high entropy helps discovery but increases violation risk.

- Failure signatures:
  - ILP: Timeout or memory exhaustion beyond n≈20.
  - PatternBoost: Valid configurations (0 violations) but suboptimal point counts (n=15: 29 vs. 30 optimal).
  - PPO: Constraint violations appearing mid-episode as point count exceeds ~20 (Figure 6 red-highlighted violation).

- First 3 experiments:
  1. **ILP scaling test**: Run ILP on n=5,10,15,19,20. Record solve time and constraint count. Confirm exponential time growth and identify practical timeout threshold.
  2. **PatternBoost ablation**: Train with and without symmetry augmentation on n=12. Compare final TopPool quality to isolate augmentation contribution.
  3. **PPO reward shaping sensitivity**: Reduce violation penalty from -10 to -5 and increase from -10 to -20 on n=11. Measure violation rate and solution quality to characterize reward sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid training paradigms combining PatternBoost pretraining with PPO reinforcement fine-tuning overcome the scalability limitations observed in standalone methods?
- Basis in paper: [explicit] The authors state future research should prioritize "hybrid training paradigms that combine supervised pretraining... with reinforcement fine-tuning" to leverage synergies between approaches.
- Why unresolved: The study evaluated PatternBoost and PPO in isolation; PatternBoost stopped scaling at n=15 while PPO failed at n=11.
- What evidence would resolve it: A hybrid model successfully generating valid solutions for n≥15 grids where individual methods previously failed.

### Open Question 2
- Question: Does using PatternBoost candidates to warm-start ILP solvers extend the limit of provably optimal solutions beyond the current n=19 barrier?
- Basis in paper: [explicit] The discussion suggests "ILP warm-starting with PatternBoost candidates could accelerate convergence on larger grids" as a promising direction for classical-AI integration.
- Why unresolved: The ILP experiments utilized standard branch-and-bound without heuristic initializations, facing exponential scaling at n=20.
- What evidence would resolve it: Demonstrating that AI-generated warm starts reduce ILP computation time or allow exact solvers to close the optimality gap for n≥20.

### Open Question 3
- Question: Can physics-inspired Graph Neural Networks (GNNs) outperform discrete methods by reformulating the problem as energy minimization?
- Basis in paper: [explicit] The conclusion identifies "physics-inspired approaches" reformulating the problem over relaxed Hamiltonians as a "particularly compelling direction" for gradient-based optimization.
- Why unresolved: All tested methods (ILP, PPO, PatternBoost) treated the problem as discrete constraint satisfaction; energy-based continuous relaxations were not explored.
- What evidence would resolve it: A GNN minimizing a proposed energy function to generate valid configurations with lower computational cost than ILP.

## Limitations

- ILP's exponential constraint growth makes scalability beyond n=19 computationally prohibitive
- PatternBoost's performance ceiling stems from greedy saturation's inability to generate high-quality training examples for larger grids
- PPO's failure at n≥11 appears fundamental, with local action-value estimates unable to capture global constraint state in dense configurations

## Confidence

**High Confidence**: ILP's optimal solutions up to 19×19 grids are provably correct through exhaustive search. The constraint generation mechanism and solver performance are well-established.

**Medium Confidence**: PatternBoost's 96% test loss reduction and competitive performance on smaller grids are verifiable, but the generalization limits beyond 14×14 grids remain uncertain due to potential training data quality constraints.

**Low Confidence**: PPO's failure mode at n≥11 is clearly demonstrated through constraint violations, but the root cause (reward shaping inadequacy vs. architectural limitations) remains ambiguous.

## Next Checks

1. **ILP Scalability Boundary**: Systematically test ILP on n=5,10,15,19,20 grids while recording solve time and constraint count to empirically determine the practical scaling threshold and verify exponential time growth.

2. **PatternBoost Training Data Quality**: Perform controlled ablation studies by training PatternBoost with and without symmetry augmentation on n=12 grids, comparing final TopPool quality to isolate the contribution of data augmentation to performance.

3. **PPO Reward Sensitivity Analysis**: Conduct sensitivity experiments by varying the violation penalty from -10 to -5 and -20 on n=11 grids, measuring both violation rates and solution quality to characterize the relationship between reward shaping and constraint satisfaction.