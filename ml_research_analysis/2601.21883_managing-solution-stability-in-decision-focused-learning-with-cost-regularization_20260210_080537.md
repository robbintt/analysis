---
ver: rpa2
title: Managing Solution Stability in Decision-Focused Learning with Cost Regularization
arxiv_id: '2601.21883'
source_url: https://arxiv.org/abs/2601.21883
tags:
- cost
- learning
- optimization
- stability
- seed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses a critical issue in decision-focused learning
  (DFL) where fluctuations in perturbation intensity during training can lead to ineffective
  learning. The core problem stems from the stability properties of combinatorial
  optimization mappings, where perturbations of different scales can cause the learning
  process to either fail to differentiate effectively or collapse into imitation-based
  learning.
---

# Managing Solution Stability in Decision-Focused Learning with Cost Regularization

## Quick Facts
- arXiv ID: 2601.21883
- Source URL: https://arxiv.org/abs/2601.21883
- Authors: Victor Spitzer; Francois Sanson
- Reference count: 8
- Key outcome: Cost regularization prevents scale-dependent gradient collapse in DFL, improving regret metrics across multiple benchmarks

## Executive Summary
This paper addresses a fundamental stability issue in decision-focused learning (DFL) where perturbation-based methods can suffer from gradient collapse due to mismatched scales between learned cost vectors and perturbations. The authors propose cost regularization techniques that constrain the scale of estimated cost vectors, ensuring perturbations remain effective for computing meaningful gradients. Through theoretical analysis and extensive experiments on established benchmarks and a specially designed toy problem, they demonstrate that regularization consistently improves performance across SPO, DBB, and DPO methods.

## Method Summary
The authors introduce two regularization functions applied to cost vectors before perturbation: L2 normalization (r_n) and L2-ball projection (r_p). These constrain the scale of cost vectors to maintain effective perturbation-based gradient computation. The approach is tested on three DFL methods (SPO, DBB, DPO) across shortest path and set matching benchmarks, plus a toy 2D vertex selection problem. Training uses a grid search over hyperparameters with 30 epochs and 10 random seeds, evaluating statistical significance via paired Wilcoxon signed-rank tests.

## Key Results
- Regularization consistently improves regret metrics across all three DFL methods
- Statistical significance achieved with p-values ranging from 0.001 to 0.46 across problems
- Toy problem results show regularized DPO achieving regret ~50 vs ~270 unregularized
- Regularization prevents pathological overfitting where models fail to learn challenging cost regions
- L2-ball projection with κ=100 generally outperforms other regularization variants

## Why This Works (Mechanism)

### Mechanism 1: Scale-Dependent Gradient Collapse Prevention
- **Claim:** Constraining cost vector norm prevents perturbation-based DFL methods from producing either zero gradients or gradient collapse into imitation learning.
- **Mechanism:** The stability radius of a cost vector θ is proportional to its scale. When perturbation δ has arbitrarily greater scale than θ, the perturbed decision f(θ+δ) ≈ f(δ), causing gradient estimates to lose all information about θ. Regularization bounds the cost vector norm, keeping perturbations at the effective scale for meaningful gradient computation.
- **Core assumption:** Perturbation intensity remains constant during training while learned cost vector scales can drift.
- **Evidence anchors:** Section 4.3 demonstrates that scale imbalance causes gradient information loss; corpus focuses on scalability rather than stability management.
- **Break condition:** If cost vectors naturally remain within a bounded range during training, regularization provides minimal benefit.

### Mechanism 2: Imitation-to-Experience Paradigm Preservation
- **Claim:** Regularization prevents experience-based DFL techniques from degenerating into imitation learning.
- **Mechanism:** Under scale imbalance, gradient estimates converge toward Fenchel-Young form f(θ̄) - f(θ), replicating ground-truth decisions rather than optimizing regret. Regularization constrains cost vector scale, maintaining directional information from the loss landscape.
- **Core assumption:** The distinction between imitation and experience learning matters for downstream task performance.
- **Evidence anchors:** Section 4.2 shows SPO loss gradient aligns with Fenchel-Young loss, causing degeneration; corpus papers address DFL scalability but not this degeneration mechanism.
- **Break condition:** When κ is set too large, regularization becomes ineffective; when too small, it may over-constrain useful gradient variance.

### Mechanism 3: Overfitting Mitigation in High-Instability Regions
- **Claim:** Regularization prevents pathological overfitting where models fail to learn certain cost regions and immediately memorize them instead.
- **Mechanism:** In problems with high decision cost asymmetry, unregularized models exhibit atypical overfitting: evaluation loss doesn't increase while the model fails to generalize on specific challenging samples. Regularization ensures gradient signals remain informative across all cost regions.
- **Core assumption:** Problem structure creates regions of intrinsically low stability radius.
- **Evidence anchors:** Section 6.3 shows unregularized models unable to learn from certain cases; Figure 2 demonstrates regularized DPO's proportion of samples at vertex C converging to zero.
- **Break condition:** If problem has uniform stability radius across cost space, this mechanism provides less benefit.

## Foundational Learning

- **Concept: Upper Semi-Continuity of Set-Valued Mappings**
  - **Why needed here:** Understanding that optimization mappings F(θ) are piecewise constant with stability regions is essential to grasp why perturbation scale matters.
  - **Quick check question:** Can you explain why f(θ) = f(θ+δ) when δ lies within the stability radius of θ?

- **Concept: Perturbation-Based Differentiation for Discrete Optimizers**
  - **Why needed here:** The paper builds on IMLE, DBB, and DPO approaches that use additive noise to create differentiable approximations of discrete optimization mappings.
  - **Quick check question:** How does adding perturbation δ to cost vector θ enable gradient computation through a piecewise-constant mapping?

- **Concept: Regret Loss vs. Fenchel-Young Loss**
  - **Why needed here:** The paper shows that without regularization, regret-based learning can collapse into Fenchel-Young imitation; distinguishing these paradigms is critical.
  - **Quick check question:** Why does the Fenchel-Young gradient f(θ̄) - f̃(θ) represent imitation rather than performance optimization?

## Architecture Onboarding

- **Component map:** Input x -> Neural Network h_v(x) -> Cost Vector θ -> Regularization r(θ) -> Perturbed Optimizer f̃(r(θ)+δ) -> Loss L(y,θ̄) -> Gradient Update
- **Critical path:**
  1. Initialize neural network with random weights
  2. Forward pass produces θ = h_v(x)
  3. **Apply regularization**: θ_reg = r(θ) using either r_n or r_p
  4. Sample perturbations δ (method-specific: Gaussian for DPO, Gumbel for others)
  5. Compute perturbed decisions f̃(θ_reg) via Monte Carlo sampling
  6. Backpropagate through loss using perturbed gradients
  7. Monitor stability: track cost vector norms and perturbation scale ratio during training
- **Design tradeoffs:**
  - **r_n (L2 normalization):** Strictly bounds all vectors to unit sphere; may over-regularize well-scaled predictions
  - **r_p (L2 ball projection):** Flexible for vectors below κ threshold; requires tuning κ ∈ {1, 100, 10000}
  - **κ selection:** Start with κ=100 (moderate), increase if validation loss plateaus early, decrease if gradients appear unstable
- **Failure signatures:**
  - **Gradient collapse:** ‖∇θL‖ ≈ 0 consistently -> cost vectors too large relative to perturbations
  - **Imitation drift:** Validation regret stagnates while training loss decreases -> check if gradient resembles f(θ̄)-f(θ)
  - **Scale divergence:** ‖θ‖ increasing monotonically during training -> regularization needed
- **First 3 experiments:**
  1. **Ablation on regularization type:** Compare r_n vs r_p with κ ∈ {1, 100, 10000} on shortest path benchmark; expect r_p with κ=100 to perform best based on Table 2 results
  2. **Scale monitoring:** Log ‖θ‖ and ‖∇θL‖ every epoch for both regularized and unregularized models; unregularized should show diverging ‖θ‖ correlating with regret degradation
  3. **Toy problem reproduction:** Implement the 2D vertex selection problem (Section 6.3) with category 2 settings (α=100); regularized DPO should achieve regret ~50 vs ~270 unregularized, confirming stability mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do multiplicative perturbations compare to additive perturbations in decision-focused learning, and do they circumvent the solution stability issues identified for additive approaches?
- **Basis in paper:** [explicit] "Alternative perturbations such as multiplicative ones (Dalle et al., 2022) could be investigated since they may influence the differentiation process in other ways."
- **Why unresolved:** The paper's theoretical analysis and regularization strategies specifically address additive perturbations; multiplicative perturbations interact differently with scale and may either mitigate or introduce different stability challenges.
- **What evidence would resolve it:** Theoretical analysis of multiplicative perturbation effects on solution stability, combined with numerical experiments comparing both perturbation types across the same benchmark problems.

### Open Question 2
- **Question:** Can problem-specific structural properties predict or bound solution stability a priori, enabling targeted regularization strategies?
- **Basis in paper:** [explicit] "Exploring how the problem's structure influences solution stability could further help address solution stability issues."
- **Why unresolved:** The counter-example in Appendix A demonstrates that lower-bounding cost vector norms does not guarantee lower-bounded stability radii, showing that stability depends on problem geometry in ways not yet characterized.
- **What evidence would resolve it:** Theoretical results linking combinatorial problem structure (e.g., constraint matrix properties, objective function characteristics) to stability radius distributions, validated across diverse optimization problem classes.

### Open Question 3
- **Question:** How can regularization techniques be designed to enforce lower bounds on stability radii rather than only upper bounds?
- **Basis in paper:** [explicit] "The development of regularization techniques to enforce lower bounds on the stability radius remains an open question for future research."
- **Why unresolved:** The paper establishes that L2 norm bounds only upper-bound stability radii; the counter-example proves no general relationship exists for lower bounds, suggesting problem-specific approaches are necessary.
- **What evidence would resolve it:** Derivation of problem-dependent regularization functions that guarantee minimum stability radii, with empirical validation showing improved gradient quality during training.

## Limitations
- Neural network architecture details (depth, widths, activations) are underspecified beyond "fully connected"
- Exact SP/SM dataset generation details and optimizer/weight initialization specifics are referenced to external work
- Regularization hyperparameters (especially κ) are not systematically optimized across problems
- Theoretical analysis assumes fixed perturbation scales, but practical implementations may vary

## Confidence

- **High confidence:** The core mechanism (scale-dependent gradient collapse) is mathematically sound and theoretically well-established. The toy problem results clearly demonstrate the proposed regularization's effectiveness.
- **Medium confidence:** Benchmark results show consistent directional improvements but with varying statistical significance. The methodology is sound, but real-world applicability depends on problem-specific stability characteristics.
- **Low confidence:** The optimal κ hyperparameter selection and the generalizability of the regularization approach to non-MILP problems remain uncertain without further experimentation.

## Next Checks

1. **Ablation on κ hyperparameter:** Systematically vary κ ∈ {1, 10, 100, 1000} across all benchmark problems to identify optimal regularization strength and determine if the current κ=100 choice is universally optimal.

2. **Architecture sensitivity analysis:** Test the regularization approach across different neural network depths (2-4 layers) and widths to establish robustness to model capacity variations and identify architecture-dependent regularization needs.

3. **Cross-problem stability mapping:** Quantify the relationship between problem stability characteristics (e.g., decision boundary complexity, cost vector variability) and regularization effectiveness to develop guidelines for when regularization is most beneficial.