---
ver: rpa2
title: Unsupervised Hallucination Detection by Inspecting Reasoning Processes
arxiv_id: '2509.10004'
source_url: https://arxiv.org/abs/2509.10004
tags:
- arxiv
- statement
- uncertainty
- hallucination
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unsupervised hallucination detection in LLMs,
  focusing on identifying false or hallucinated content without labeled data. The
  proposed method, IRIS, leverages internal representations from LLM verification
  processes to train a classifier.
---

# Unsupervised Hallucination Detection by Inspecting Reasoning Processes

## Quick Facts
- **arXiv ID**: 2509.10004
- **Source URL**: https://arxiv.org/abs/2509.10004
- **Reference count**: 12
- **Primary result**: Proposes IRIS, an unsupervised method for LLM hallucination detection using internal representations from verification processes, achieving accuracy improvements of 3.2%, 7.0%, and 10.2% on True-False, HaluEval2, and HELM datasets, respectively.

## Executive Summary
This paper introduces IRIS, a method for unsupervised hallucination detection in large language models (LLMs). The core idea is to leverage the internal representations generated during an LLM's verification process to train a lightweight classifier, eliminating the need for labeled training data. By prompting the LLM to carefully verify the truthfulness of a statement and extracting its contextualized embeddings, IRIS creates informative features for training. The uncertainty expressed in the LLM's response serves as a soft pseudolabel, guiding the classifier. The method is computationally efficient, works well with minimal data, and significantly outperforms existing unsupervised approaches.

## Method Summary
IRIS works by prompting an LLM (specifically Llama-3.1-8B-Instruct) to verify the truthfulness of a given statement using Chain-of-Thought reasoning. The contextualized embedding of the last token from a specific layer is extracted as the feature representation. The LLM's verbalized confidence score, normalized to [0,1], serves as a soft pseudolabel for the statement's truthfulness. A lightweight three-layer MLP (256-128-64 units) is trained on these embeddings using symmetric cross-entropy loss combined with soft bootstrapping to handle the noise in pseudolabels. The model is trained for 10 epochs with Adam optimizer, and no labeled data is required during training.

## Key Results
- IRIS achieves accuracy improvements of 3.2%, 7.0%, and 10.2% over the best unsupervised baseline (SAPLMA) on the True-False, HaluEval2, and HELM datasets, respectively.
- The method performs well even with very few training data points, with accuracy plateauing after approximately 128 samples.
- Middle layers of the LLM sometimes yield better performance than the last layer for feature extraction, suggesting that verification-induced reasoning is encoded across multiple layers.

## Why This Works (Mechanism)

### Mechanism 1: Verification-Induced Representational Enrichment
Prompting the LLM to verify a statement via Chain-of-Thought forces it to retrieve and encode relevant internal knowledge into hidden states, enriching the embeddings with factuality-related signals. This enrichment makes the embeddings more informative for hallucination detection than those from the original statement alone. The core assumption is that LLMs possess latent knowledge about factual correctness that can be surfaced through reasoning tasks. Evidence comes from the supervised ceiling experiments, which show that embeddings from the verification process outperform those from the statements themselves. If the LLM lacks relevant internal knowledge, this mechanism fails.

### Mechanism 2: Verbalized Confidence as a Soft Pseudolabel for Truth
The uncertainty or confidence expressed by the LLM when verifying a statement serves as a viable soft label for training a hallucination detector. Using CoT prompting, the model outputs a confidence score treated as a continuous pseudolabel indicating the likelihood the statement is correct. The core assumption is that verbalized confidence is calibrated to actual knowledge and correlates with truthfulness. Evidence includes the paper's finding that verbalized confidence is better calibrated than token probabilities, though this calibration is not externally validated. If the LLM is systematically overconfident or underconfident, pseudolabels will be biased, degrading classifier performance.

### Mechanism 3: Noise-Robust Learning via Symmetric Cross-Entropy and Bootstrapping
Training a lightweight probe on noisy pseudolabels benefits from loss functions and target updates that reduce overfitting to incorrect labels. Soft bootstrapping mixes pseudolabels with current classifier predictions, while symmetric cross-entropy combines standard and reverse cross-entropy to penalize overconfidence in potentially wrong targets. The core assumption is that pseudolabels contain both signal and noise, and the classifier must learn to ignore the noise. Evidence includes the paper's claim that this combination prevents overfitting and allows for better generalization, though this specific loss combination lacks external validation in hallucination detection. If pseudolabel noise is too high or asymmetric, the classifier may converge to incorrect decision boundaries.

## Foundational Learning

- **Concept: Contextualized Embeddings** (e.g., last-token hidden states from decoder layers)
  - Why needed here: IRIS relies on extracting these embeddings during the verification response as primary features.
  - Quick check question: Can you explain why the last token's embedding at a specific layer is used to represent the entire sequence's meaning in decoder-only models?

- **Concept: Soft Labels and Bootstrapping for Noisy Supervision**
  - Why needed here: The verbalized confidence scores are continuous and noisy, requiring robust training techniques.
  - Quick check question: How does soft bootstrapping differ from hard label assignment when training with uncertain targets?

- **Concept: Chain-of-Thought Prompting**
  - Why needed here: CoT is the core intervention used to elicit reasoning and, consequently, informative embeddings.
  - Quick check question: What is the expected effect of CoT on the information content of intermediate hidden states compared to direct prompting?

## Architecture Onboarding

- **Component map**: Prompting Module -> Embedding Extractor -> Pseudolabel Generator -> Classifier Probe
- **Critical path**: Prompting → Verification Response → Embedding + Pseudolabel Extraction → Probe Training → Inference on New Statements. The quality of both the embedding and pseudolabel is critical; either being uninformative breaks the pipeline.
- **Design tradeoffs**:
  - **Verbalized vs. entropy-based uncertainty**: Paper shows verbalized is better calibrated (Fig. 3), but it depends on prompt quality.
  - **Layer selection for embeddings**: Middle layers sometimes outperform final layers (Fig. 5), but this varies by dataset; default to last layer for simplicity, tune if performance lags.
  - **Training data size**: Performance plateaus after ~128 samples; minimal data is viable, but quality of statements matters.
- **Failure signatures**:
  - Probe accuracy near random (50%): Check if pseudolabels are degenerate (e.g., all 0 or 1), indicating poor prompt or calibration.
  - Large train-test gap: Overfitting to pseudolabel noise; increase β for bootstrapping or reduce model capacity.
  - Poor generalization to new domains: Embeddings may not encode domain-relevant knowledge; consider domain-specific proxy models or layer tuning.
- **First 3 experiments**:
  1. **Baseline Probe Check**: Train the probe on a small held-out set with ground-truth labels (Ceiling setup) to verify the embedding extraction and classifier pipeline work correctly.
  2. **Pseudolabel Quality Audit**: Visualize the distribution of verbalized confidence scores for known-true and known-false statements to confirm separation and calibration.
  3. **Layer Sensitivity Sweep**: Run a quick grid search over middle vs. final layers on a validation slice to identify the most informative layer for your target domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can sophisticated probe architectures successfully aggregate contextualized embeddings across multiple layer depths to outperform single-layer baselines?
- Basis in paper: [explicit] The Conclusion states that "Further examination with more sophisticated architecture is required" because a "preliminary consideration of adding a small learnable module to fuse the embeddings did not improve performance."
- Why unresolved: The authors observe varying accuracy across layer depths but their initial attempt to combine these signals failed, leaving the optimal method for integrating multi-layer information undetermined.
- What evidence would resolve it: Development of a fusion architecture (e.g., attention-based or weighted averaging) that demonstrates statistically significant accuracy improvements over the best-performing single-layer probes.

### Open Question 2
- Question: How can the IRIS framework be adapted to generate reliable pseudolabels for specialized reasoning tasks (e.g., mathematics) where standard verbalized confidence is poorly calibrated?
- Basis in paper: [explicit] Appendix E notes that on mathematical datasets like GSM8K, "it is more challenging to elicit verbalized confidence," often resulting in a confidence of 0, which renders pseudolabels "misguiding."
- Why unresolved: The current reliance on verbalized confidence fails in domains where models struggle to articulate certainty numerically, limiting the method's applicability to general reasoning tasks.
- What evidence would resolve it: Identification of alternative uncertainty metrics or prompting strategies that maintain high detection accuracy on reasoning datasets without relying on the model's direct verbalized probability.

### Open Question 3
- Question: What is the mechanistic relationship between the model's internal states during verification and the factual correctness of a statement?
- Basis in paper: [explicit] The Limitations section states that "Understanding the exact relationship between these internal cues and the truthfulness of a statement remains an open problem."
- Why unresolved: While the probe successfully utilizes these states, the "black-box nature of these signals" means researchers do not know *why* specific activations correlate with truth, hindering transparency.
- What evidence would resolve it: Causal analysis or probing studies that identify specific attention heads or neurons responsible for fact retrieval and error detection during the verification process.

## Limitations
- The entire pipeline relies on the LLM's verbalized confidence being both calibrated and informative, which is validated only on the paper's datasets and may not generalize to domains with different LLM calibration.
- The method is specifically designed for decoder-only models like Llama, and the feature extraction step would require modification for encoder-decoder or hybrid architectures, with no guidance provided.
- Critical hyperparameters for bootstrapping (β) and symmetric cross-entropy balance (φ) are referenced but not explicitly stated in the main text, hindering exact replication.

## Confidence

**High Confidence**:
- The general mechanism of using verification-induced embeddings for hallucination detection is sound and supported by the supervised ceiling experiments.
- The improvement over SAPLMA (3.2%, 7.0%, 10.2% on three datasets) is a direct, measurable claim from the results.

**Medium Confidence**:
- The claim that verbalized confidence is "better calibrated" than token probabilities is based on Figure 3, which is not fully described in the provided text.
- The necessity and optimal values of the symmetric cross-entropy and bootstrapping combination are supported by internal ablation but lack external validation.

**Low Confidence**:
- The method's performance on datasets or domains not included in the paper (e.g., non-English, highly technical, or adversarial contexts).
- The long-term stability of the pseudolabeling process as models are updated or fine-tuned.

## Next Checks

1. **Cross-Model Generalization Test**: Run the complete IRIS pipeline using a different LLM (e.g., GPT-3.5-turbo or an open-source decoder) as the proxy verifier on the True-False dataset. Compare the accuracy drop, if any, to the reported Llama-3.1-8B-Instruct results.

2. **Pseudolabel Distribution Audit**: For a held-out set of 100 statements with known ground truth, generate the verbalized confidence scores. Plot and report the distribution of scores for true vs. false statements, calculating the area under the ROC curve (AUC) to quantify separation quality.

3. **Layer Tuning Validation**: Implement a grid search over the last 3 layers of the LLM for the HELM dataset. For each layer, train the probe and report the accuracy. Confirm that middle layers can outperform the last layer, and identify the optimal layer for this dataset.