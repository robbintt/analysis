---
ver: rpa2
title: Teaching Language Models to Faithfully Express their Uncertainty
arxiv_id: '2510.12587'
source_url: https://arxiv.org/abs/2510.12587
tags:
- uncertainty
- answer
- confidence
- faithfulness
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Faithful Uncertainty Tuning (FUT) is a fine-tuning method that\
  \ teaches language models to express uncertainty in ways that accurately reflect\
  \ their internal beliefs, without altering the underlying answer distribution. The\
  \ approach constructs training data by sampling model outputs, estimating their\
  \ confidence via consistency across samples, and augmenting responses with hedges\
  \ (e.g., \u201Clikely\u201D, \u201Cpossibly\u201D) aligned to that confidence."
---

# Teaching Language Models to Faithfully Express their Uncertainty

## Quick Facts
- arXiv ID: 2510.12587
- Source URL: https://arxiv.org/abs/2510.12587
- Reference count: 26
- Primary result: FUT improves faithful uncertainty expression from ~0.52 to 0.71-0.79 cMFG while preserving QA accuracy

## Executive Summary
Faithful Uncertainty Tuning (FUT) is a fine-tuning method that teaches language models to express uncertainty in ways that accurately reflect their internal beliefs, without altering the underlying answer distribution. The approach constructs training data by sampling model outputs, estimating their confidence via consistency across samples, and augmenting responses with hedges (e.g., "likely", "possibly") aligned to that confidence. Two strategies are explored: interweaving hedges into the response text or appending them as postfixes. Evaluated on multiple instruction-tuned models across open-domain QA datasets, FUT substantially improves faithfulness while preserving QA accuracy and introducing minimal semantic distribution shift.

## Method Summary
FUT constructs training data by sampling model responses with unbiased sampling, estimating confidence through contradiction rates across samples using NLI, mapping confidence to hedges based on psycholinguistic probability mappings, and rewriting responses to integrate hedges (interweave) or append them (postfix). The fine-tuning uses standard SFT with quantized 8-bit AdamW for 3 epochs. Evaluation uses cMFG metric to measure faithfulness and TVD to assess semantic distribution shift.

## Key Results
- FUT improves conditional mean faithful generation scores from ~0.52 to 0.71-0.79 across models and datasets
- Both interweave and postfix strategies achieve substantial faithfulness improvements while preserving QA accuracy
- Total variation distance analysis shows minimal semantic distribution shift between base and FUT models
- FUT maintains faithfulness across different decoding strategies (greedy, unbiased sampling, nucleus sampling)
- Numerical uncertainty expressions achieve slightly higher faithfulness (0.81 cMFG) than linguistic hedges

## Why This Works (Mechanism)

### Mechanism 1: Pushforward Distribution Belief Preservation
- Claim: FUT preserves the model's underlying belief distribution while adding faithful uncertainty hedges.
- Mechanism: The method constructs a pushforward distribution p#f where f is a hedging function that maps responses to faithfully hedged versions. Since f only adds hedging without changing semantic content, the distribution over asserted content remains unchanged.
- Core assumption: The hedging function does not alter the semantic assertions in responses, only their decisiveness expression.
- Evidence anchors: [section 3]: "Because f does not modify the semantics of the assertion beyond hedging (e.g., it never changes who is blamed in the examples of Fig. 1), it can be shown (see Sec. F) that p#f preserves the original distribution over asserted content." [section 4.2]: Total variation distance analysis shows minimal semantic distribution shift between base and FUT models.

### Mechanism 2: Consistency-Based Confidence Estimation
- Claim: Sample consistency across multiple model generations provides a reliable estimate of the model's internal confidence in an assertion.
- Mechanism: For each assertion a, confidence is estimated as C(a) = 1 - E[a⊥Y|x], where contradiction rate is computed via Monte Carlo sampling and NLI-based contradiction detection.
- Core assumption: The contradiction rate across independent samples reflects the model's true internal uncertainty about the assertion.
- Evidence anchors: [section 2]: "Following Yona et al. (2024a), we identify the model's confidence in an assertion a with the rate at which responses sampled from the model are expected not to contradict a." [section 5.4]: FUT maintains faithfulness across decoding strategies (greedy: 0.78, unbiased sampling: 0.76, nucleus: 0.80), confirming the approach captures distributional uncertainty rather than just greedy output artifacts.

### Mechanism 3: Verbal-Numerical Hedge Alignment
- Claim: Grounding hedges in psycholinguistically-validated probability mappings enables faithful uncertainty communication.
- Mechanism: Confidence values are binned into 9 intervals [0.0-1.0], each mapped to specific hedges (e.g., "likely" for 0.55-0.70) based on human interpretation studies.
- Core assumption: Human interpretations of verbal probability expressions generalize to LLM-hedged responses.
- Evidence anchors: [section 3, Table 1]: Explicit mapping based on Vogel et al. (2022) meta-analysis of verbal-numerical correspondences. [section 5.3]: Alternative hedge mappings achieve comparable cMFG (0.80 vs. 0.78), showing robustness to reasonable hedge choices.

## Foundational Learning

- Concept: **Pushforward distributions**
  - Why needed here: FUT's theoretical justification relies on understanding how deterministic transformations of random variables create new distributions while preserving certain properties.
  - Quick check question: If Y ~ p(y|x) and R = f(Y), what conditions on f ensure that the semantic content distribution n(Y) equals n(R)?

- Concept: **Semantic clustering and contradiction detection**
  - Why needed here: The confidence estimation pipeline requires grouping responses by semantic equivalence and detecting logical contradictions, typically via NLI models.
  - Quick check question: Given two responses "Likely Paris" and "Possibly Paris," should an NLI system classify these as contradictory, equivalent, or neutral?

- Concept: **Calibration vs. faithfulness distinction**
  - Why needed here: FUT explicitly targets faithfulness (alignment with model beliefs) not calibration (alignment with ground truth), which is a critical conceptual separation.
  - Quick check question: If a model is 90% confident in an incorrect answer, would a faithful hedge be "certain" or "uncertain"?

## Architecture Onboarding

- Component map:
  - Data generation pipeline: Unbiased sampling -> assertion extraction -> Monte Carlo confidence estimation -> hedge mapping -> response rewriting
  - Confidence estimator: NLI model (DeBERTa Large used) for contradiction detection across samples
  - Hedge mapper: Lookup table from confidence bins to hedge phrases
  - Rewriter: LLM (Mistral 12B used) for interweaving; template-based for postfix
  - Fine-tuning: Standard SFT on generated faithful response pairs

- Critical path:
  1. Generate training data with unbiased sampling (not greedy) to capture true model distribution
  2. Use leave-one-out estimation when computing contradiction rates to avoid bias
  3. Apply verbal-numerical mapping consistently during both training and evaluation

- Design tradeoffs:
  - **Interweave vs. Postfix**: Interweave achieves higher faithfulness (0.78-0.79 vs. 0.71-0.73) but requires additional LLM calls; postfix preserves original response text exactly.
  - **Linguistic vs. Numerical**: FUT-numerical achieves slightly higher cMFG (0.81) but linguistic hedges are more natural for human interaction.
  - **Sample count**: Paper uses 10 samples per prompt for confidence estimation; fewer samples increase variance.

- Failure signatures:
  - **Under-hedging at high confidence**: Error analysis (Section 5.2) shows FUT-interweave over-represents "It is certain" in failure cases, indicating difficulty at extreme confidence levels.
  - **Mid-confidence miscalibration**: FUT-numerical shows more errors in middle confidence ranges (0.4-0.7).
  - **Semantic drift**: If interweaving LLM alters assertions, belief preservation breaks (Fig. 2 shows this risk is low but non-zero).

- First 3 experiments:
  1. **Ablate sample count**: Reduce from 10 to 5 samples per prompt and measure faithfulness degradation to quantify estimation variance impact.
  2. **Domain shift test**: Apply FUT trained on PopQA to long-form generation (e.g., summarization) and measure both faithfulness and semantic shift using the clustering methodology from Section 4.2.
  3. **Hedge personalization**: Test whether user-specific verbal-numerical mappings (per Ulmer et al., 2025) improve perceived faithfulness in human evaluation, as suggested in Section 5.1.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Faithful Uncertainty Tuning (FUT) generalize to tasks beyond open-domain question answering, such as long-form generation, dialogue, or multi-claim summarisation?
- Basis in paper: [explicit] The "Limitations" section states: "Finally, our experiments are limited to open-domain QA. It remains to be seen how FUT generalises to other tasks such as long-form generation, dialogue, or multi-claim summarisation."
- Why unresolved: The current methodology relies on short-form QA responses where single assertions dominate (≥98%), simplifying the assertion extraction and hedging process required for the training data construction.
- What evidence would resolve it: Successful application of FUT to long-form text datasets or multi-turn dialogue benchmarks, showing improved faithfulness scores (cMFG) without causing semantic drift or incoherence in extended text.

### Open Question 2
- Question: Can FUT be adapted to personalize uncertainty hedges to match individual users' verbal-numerical probability correspondences?
- Basis in paper: [explicit] Section 5.1 notes: "The strong performance of the numerical model opens the door to uses in applications that demand... personalisation of uncertainty hedges to match individual users’ verbal-numerical probability correspondences... We leave the exploration of these directions to future work."
- Why unresolved: While the paper demonstrates that numerical predictions are learnable, the mechanism to dynamically map these numerical confidences to user-specific linguistic preferences has not been developed.
- What evidence would resolve it: A user study or system demonstration where the model adjusts its hedge phrasing (e.g., "likely" vs. "good chance") based on a specific user's interpretation, maintaining faithfulness to the numerical confidence.

### Open Question 3
- Question: How can the issue of models being "confidently wrong" be mitigated when using FUT?
- Basis in paper: [explicit] The "Limitations" section highlights that "FUT surfaces uncertainty already present in the base model, but if the underlying distribution is misinformed or overconfident, then faithfully verbalised responses may still mislead users."
- Why unresolved: FUT is designed to preserve the model's original belief distribution rather than calibrate it towards empirical correctness, meaning it faithfully exposes incorrect beliefs without rectifying them.
- What evidence would resolve it: A proposed integration of FUT with correctness-enhancing techniques (such as Retrieval-Augmented Generation) that reduces factual error rates while still maintaining the faithfulness of the expressed uncertainty.

### Open Question 4
- Question: Can cheaper, locally-run language models replace GPT-5 in the faithfulness evaluation pipeline without significantly affecting reliability?
- Basis in paper: [inferred] The "Limitations" section mentions that the evaluation setup "may hinder large-scale reproducibility" due to the cost of GPT-5. The authors note: "In principle, cheaper locally run models could replace GPT-5 in our setup, but for comparability... we opted to follow a similar evaluation protocol."
- Why unresolved: While the authors validated GPT-5 against human annotations (Spearman correlation 0.92), they did not test if smaller, open-source models could perform the decisiveness extraction and NLI tasks with comparable accuracy.
- What evidence would resolve it: A comparative benchmark showing that a locally hosted model can extract decisiveness and perform NLI for contradiction detection with correlation scores statistically indistinguishable from the GPT-5 baseline.

## Limitations

- FUT surfaces uncertainty already present in the base model, so if the underlying distribution is misinformed or overconfident, faithfully verbalized responses may still mislead users.
- The evaluation setup relies on GPT-5 for faithfulness assessment, which may hinder large-scale reproducibility and requires access to expensive APIs.
- The approach is currently limited to open-domain QA and it remains unclear how FUT generalizes to other tasks such as long-form generation, dialogue, or multi-claim summarization.

## Confidence

**High confidence**: The core claim that FUT improves faithful uncertainty expression (cMFG increases from ~0.52 to 0.71-0.79) is well-supported by systematic evaluation across multiple models and datasets, with consistent improvements using both interweave and postfix strategies.

**Medium confidence**: Claims about minimal semantic distribution shift (TVD analysis) and robustness across decoding strategies are supported by the evidence but rely on specific methodological choices (clustering approach, NLI model) that could affect results.

**Low confidence**: The claim that FUT "preserves the model's underlying belief distribution" is theoretically sound but the practical implementation (especially interweave strategy) introduces small semantic modifications that could accumulate in longer-form generation.

## Next Checks

1. **Human evaluation study**: Conduct user studies to assess whether human interpreters perceive FUT-hedged responses as faithful representations of model uncertainty, particularly for edge cases where model confidence is extreme or assertions are complex.

2. **Calibration assessment**: Evaluate whether FUT-hedged responses also show improved alignment with ground-truth correctness (calibration), not just internal consistency, by measuring calibration error metrics on factual QA tasks.

3. **Long-form generation test**: Apply FUT to open-ended generation tasks (story completion, summarization) and measure faithfulness degradation, semantic drift accumulation, and hedge appropriateness over extended text sequences.