---
ver: rpa2
title: 'Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning'
arxiv_id: '2505.21926'
source_url: https://arxiv.org/abs/2505.21926
tags:
- graph
- knowledge
- tasks
- merry
- textual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MERRY, a foundation model for general knowledge
  graph reasoning that integrates both structural and textual information. MERRY addresses
  the challenge of leveraging rich textual information in KGs alongside structural
  data, which existing models largely ignore.
---

# Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning

## Quick Facts
- **arXiv ID**: 2505.21926
- **Source URL**: https://arxiv.org/abs/2505.21926
- **Reference count**: 33
- **Primary result**: MERRY demonstrates strong reasoning capabilities in zero-shot knowledge graph completion (KGC) and excellent generalization to out-of-KG tasks like KG question answering (KGQA), outperforming existing baselines in most scenarios across 28 datasets.

## Executive Summary
This paper introduces MERRY, a foundation model for general knowledge graph reasoning that integrates both structural and textual information. The model addresses the challenge of leveraging rich textual information in KGs alongside structural data, which existing models largely ignore. MERRY employs a multi-perspective Conditional Message Passing (CMP) architecture to bridge semantic gaps between modalities, a dynamic residual fusion module for selective textual information retention, and a flexible edge scoring mechanism for diverse tasks. Evaluated on 28 datasets, MERRY demonstrates strong reasoning capabilities in zero-shot KGC and excellent generalization to out-of-KG tasks like KGQA.

## Method Summary
MERRY integrates structural and textual information in knowledge graphs through a novel architecture. The key innovation is the multi-perspective Conditional Message Passing (CMP) mechanism that bridges semantic gaps between structural and textual modalities. The model uses a dynamic residual fusion module to selectively retain relevant textual information and employs a flexible edge scoring mechanism adaptable to various reasoning tasks. The architecture is designed to handle both knowledge graph completion and question answering tasks within a unified framework, addressing the limitation of existing models that focus primarily on structural information while ignoring rich textual data.

## Key Results
- MERRY demonstrates strong reasoning capabilities in zero-shot knowledge graph completion (KGC) across 28 datasets
- The model shows excellent generalization to out-of-KG tasks like KG question answering (KGQA)
- MERRY consistently outperforms existing baselines in most scenarios with metrics such as MRR and Hits@10 for KGC and Accuracy for KGQA

## Why This Works (Mechanism)
The paper introduces MERRY as a foundation model that bridges the gap between structural and textual information in knowledge graphs. The multi-perspective Conditional Message Passing (CMP) architecture allows the model to capture complex relationships by considering multiple viewpoints during information propagation. The dynamic residual fusion mechanism enables selective retention of textual information based on task relevance, preventing information overload while maintaining context. The flexible edge scoring mechanism adapts to different reasoning tasks without requiring task-specific modifications. This integration of modalities addresses a critical limitation in existing KG reasoning models that primarily focus on structural patterns while ignoring rich textual context that can enhance reasoning capabilities.

## Foundational Learning
- **Knowledge Graph Embeddings**: Understanding how entities and relations are represented as vectors in continuous space; needed to grasp how MERRY operates on KG data; quick check: verify understanding of TransE, DistMult, and ComplEx as baseline approaches.
- **Multi-modal Learning**: The integration of structural (graph) and textual (language) information; needed to understand MERRY's core innovation; quick check: confirm understanding of cross-modal attention mechanisms.
- **Graph Neural Networks**: Message passing and aggregation on graph structures; needed to comprehend CMP architecture; quick check: verify understanding of how information flows through graph nodes.
- **Foundation Models**: Large-scale pre-trained models adapted for downstream tasks; needed to understand MERRY's positioning; quick check: compare with GPT-style models and their adaptation mechanisms.
- **Zero-shot Learning**: Making predictions on unseen data without task-specific training; needed to understand MERRY's generalization claims; quick check: verify understanding of how zero-shot differs from few-shot learning.

## Architecture Onboarding

**Component Map**: Entity nodes -> Structural encoder -> CMP layer -> Dynamic fusion -> Edge scorer -> Task-specific output

**Critical Path**: Input KG → Multi-perspective Conditional Message Passing → Dynamic Residual Fusion → Edge Scoring → Output prediction

**Design Tradeoffs**: The model trades computational efficiency for expressiveness by incorporating large language models and complex fusion mechanisms. This increases parameter count and inference time but enables better cross-modal reasoning. The flexibility to handle multiple tasks comes at the cost of specialized optimization for any single task.

**Failure Signatures**: Performance degradation may occur when textual information is sparse or noisy, when KG structures are extremely dense or sparse, or when there's significant semantic mismatch between structural and textual modalities. The model may also struggle with long-tail entities where textual descriptions are limited.

**First Experiments**:
1. Ablation study removing the dynamic residual fusion module to quantify its contribution to overall performance.
2. Zero-shot evaluation on a held-out KG with completely disjoint entity and relation types from training data.
3. Cross-domain generalization test transferring from one KG domain (e.g., biomedical) to another (e.g., social networks).

## Open Questions the Paper Calls Out
None provided.

## Limitations
- The exact distribution and difficulty levels of the 28 evaluation datasets are not fully characterized, making it difficult to assess whether performance gains generalize across diverse KG reasoning challenges.
- The model's reliance on large pre-trained language models introduces computational overhead that may limit practical deployment in resource-constrained settings.
- While the paper demonstrates zero-shot KGC capabilities, the long-tail distribution of entity and relation types in real-world KGs may still pose challenges for generalization.

## Confidence
- **High confidence** in the model's architectural innovations (CMP and fusion mechanisms)
- **Medium confidence** in reported performance gains across all 28 datasets
- **Medium confidence** in generalization claims to out-of-KG tasks
- **Low confidence** in scalability assessments for very large KGs

## Next Checks
1. Conduct systematic ablation studies to quantify the individual contributions of the multi-perspective CMP architecture, dynamic residual fusion, and edge scoring mechanism to overall performance.

2. Evaluate MERRY's performance on KGs with significantly different characteristics (e.g., biomedical KGs, KGs with heavy temporal dependencies, or KGs with highly imbalanced relation distributions) to test robustness.

3. Perform computational complexity analysis comparing MERRY's inference time and memory requirements against traditional KG embedding models on KGs of increasing scale to assess practical deployment feasibility.