---
ver: rpa2
title: 'DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion
  Models'
arxiv_id: '2502.19924'
source_url: https://arxiv.org/abs/2502.19924
tags:
- prosody
- speech
- context
- conversational
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating diverse and expressive
  conversational speech by proposing DiffCSS, a novel framework that leverages diffusion
  models for prosody prediction and an LM-based TTS backbone for high-quality speech
  synthesis. The key innovation is a diffusion-based context-aware prosody predictor
  that samples diverse prosody embeddings conditioned on multimodal conversational
  context (text and speech).
---

# DiffCSS: Diverse and Expressive Conversational Speech Synthesis with Diffusion Models

## Quick Facts
- **arXiv ID:** 2502.19924
- **Source URL:** https://arxiv.org/abs/2502.19924
- **Reference count:** 36
- **Primary result:** Diffusion-based prosody predictor achieves NDB 4 and JSD 0.036 on DailyTalk, outperforming deterministic baselines in expressiveness (MOS 3.602), coherence (MOS 3.574), and prosody diversity.

## Executive Summary
DiffCSS addresses the challenge of generating diverse and expressive conversational speech by leveraging diffusion models for prosody prediction combined with a language model-based TTS backbone. The key innovation is a diffusion-based context-aware prosody predictor that samples diverse prosody embeddings conditioned on multimodal conversational context (text and speech). This is combined with a prosody-enhanced ParlerTTS backbone that synthesizes speech based on these embeddings. Experiments on the DailyTalk dataset show that DiffCSS significantly outperforms deterministic baselines in terms of expressiveness, contextual coherence, and prosody diversity, with the generated prosody distribution aligning more closely with ground truth.

## Method Summary
DiffCSS employs a two-stage training approach: first pre-training a prosody-enhanced ParlerTTS backbone on LibriTTS-R (585h, 2456 speakers), then fine-tuning it on DailyTalk (2541 conversations, ~20h). The core innovation is a diffusion-based prosody predictor that learns to sample diverse prosody embeddings from multimodal context. The predictor uses a denoising network (Transformer encoder) that iteratively denoises Gaussian noise into prosody embeddings conditioned on textual and acoustic context. The TTS backbone uses FACodec-extracted prosody features compressed via cross-attention with learnable queries, then conditions synthesis on the sampled prosody embeddings through cross-attention mechanisms.

## Key Results
- Expressiveness MOS: 3.602 vs. 3.347 (deterministic baseline)
- Coherence MOS: 3.574 vs. 3.362 (deterministic baseline)
- Prosody Diversity: NDB 4 vs. 13; JSD 0.036 vs. 0.156 (deterministic baseline)

## Why This Works (Mechanism)

### Mechanism 1
Diffusion models enable diverse prosody sampling that better approximates the ground-truth distribution of valid prosodic variations for a given context. A denoising network learns to reverse a fixed noise schedule, iteratively denoising Gaussian noise into prosody embeddings conditioned on multimodal context. Sampling from N(0,I) followed by T denoising steps yields diverse outputs. The one-to-many mapping from context to valid prosody is modeled as a learnable distribution rather than a point estimate.

### Mechanism 2
Prosody embeddings extracted via cross-attention over codec features provide disentangled, controllable prosody representations. FACodec extracts frame-level prosody features, which are compressed via cross-attention with learnable query tokens to produce fixed-length prosody embeddings. These serve as keys/values in the TTS backbone's cross-attention, conditioning synthesis on sampled prosody.

### Mechanism 3
Multimodal context (textual + acoustic) improves prosody appropriateness, with acoustic context contributing more. Textual context is encoded via T5 sentence embeddings; acoustic context via pre-extracted prosody embeddings from prior turns. Both are concatenated and injected via cross-attention in the denoiser, with ablation showing acoustic context ablation hurts more than textual.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPM)** - Essential for understanding the entire prosody predictor operation. Quick check: Can you explain why sampling zT ~ N(0,I) and iteratively applying the denoising equation produces diverse outputs rather than a single reconstruction?

- **Neural Codec Language Models for TTS** - Critical for understanding the TTS backbone that predicts discrete acoustic tokens from a codec rather than mel-spectrograms. Quick check: How does the "delayed pattern" from MusicGen enable faster autoregressive generation compared to standard token-by-token prediction?

- **Cross-Attention for Conditioning** - Two critical cross-attention points: prosody extractor compresses variable-length features to fixed embeddings, and TTS backbone conditions on prosody/speaker. Quick check: In the prosody extractor, why use learnable queries rather than simply averaging frame-level features?

## Architecture Onboarding

- **Component map:** FACodec → frame features → cross-attention with learnable queries → fixed-length prosody embeddings → QKV-Attn → prosody embeddings → TTS backbone (12 decoder blocks) → predicts DAC tokens → DAC decode → waveform; Diffusion predictor: 6 encoder blocks → denoiser θ(zt, sN+1, t, c) → predicts noise ε → trained via MSE

- **Critical path:** Extract context c and current text sN+1 → Sample zT ~ N(0,I) → T-step denoising to produce prosody embedding ẑ0 → Feed ẑ0 + speaker embedding to TTS backbone → Generate acoustic tokens → DAC decode to waveform

- **Design tradeoffs:** Fixed-length prosody (m tokens) reduces compute vs. frame-level features but may lose fine-grained timing; Two-stage training enables large-scale pre-training but requires freezing TTS during diffusion training; Diffusion steps T not specified (Assumption: standard 100-1000 values apply)

- **Failure signatures:** Mode collapse in prosody (NDB approaches 0, JSD approaches 0) indicates diffusion not learning diversity; Incoherent prosody (C-MOS drops) suggests context encoding or cross-attention injection failing; Artifacts in speech (MCD rises) indicates TTS backbone poorly conditioned on sampled prosody embeddings

- **First 3 experiments:** (1) Reproduce ablation: Train diffusion predictor with textual-only context (set acoustic context to zeros), confirm NDB increases and JSD degrades per Table II; (2) Prosody disentanglement test: Train speaker classifier on extracted prosody embeddings, target <60% accuracy indicates reasonable disentanglement; (3) Sampling diversity sweep: Run inference with varying diffusion steps (T=50, 100, 250, 500), plot NDB/JSD vs. T to find quality-diversity-efficiency tradeoff point

## Open Questions the Paper Calls Out

- **Open Question 1:** How does DiffCSS perform on longer conversational contexts extending beyond 4 preceding utterances? The experimental setup fixed chunk size at 5 utterances without exploring scalability to longer contexts.

- **Open Question 2:** Can the diffusion-based prosody predictor meet latency requirements for real-time conversational applications? The inference procedure requires T iterative denoising steps per utterance, but no runtime or latency analysis is reported.

- **Open Question 3:** How does DiffCSS generalize to multi-party conversations with more than two speakers? DailyTalk consists exclusively of two-speaker dialogues, and the cross-attention context encoder was not evaluated on conversations with three or more participants.

- **Open Question 4:** Does increased prosody diversity affect speaker identity consistency across synthesized utterances? The paper evaluates expressiveness, coherence, and prosody diversity but does not report speaker similarity or consistency scores.

## Limitations
- Diffusion sampling implementation lacks key details (number of steps T, noise schedule type, sampling strategy) making exact reproducibility challenging
- Prosody disentanglement from speaker identity and linguistic content is claimed but not empirically validated
- Dataset representativeness is limited to 20 hours of two-speaker dialogues with fixed 5-utterance context windows, raising questions about real-world applicability

## Confidence
- **High Confidence:** Architecture design (diffusion-based prosody predictor + prosody-enhanced TTS backbone) is technically sound and well-described; Two-stage training procedure follows established best practices
- **Medium Confidence:** Quantitative improvements over baselines appear robust, though exact reproducibility is uncertain without implementation details; Claim that acoustic context contributes more than textual context is supported by ablation results
- **Low Confidence:** Assertion that diffusion models enable better approximation of "ground-truth distribution" lacks direct validation; No analysis shows generated samples match true prosodic distribution beyond indirect metrics

## Next Checks
1. **Diffusion Sampling Analysis** - Systematically vary the number of diffusion steps T and noise schedule parameters to identify optimal configuration achieving NDB/JSD values reported in Table II; plot quality-diversity tradeoff curves

2. **Prosody Disentanglement Test** - Train speaker classifiers on extracted prosody embeddings and evaluate classification accuracy; target <60% accuracy to confirm reasonable disentanglement from speaker identity

3. **Context Window Robustness** - Evaluate the model with varying context window lengths (N=2, 3, 4, 5) to determine if reported performance depends critically on fixed 5-utterance setup