---
ver: rpa2
title: 'GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with
  Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning'
arxiv_id: '2601.06795'
source_url: https://arxiv.org/abs/2601.06795
tags:
- reward
- advantage
- sampling
- training
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'GDEPO tackles inefficiencies in reinforcement learning for automated
  theorem proving, where Group Relative Policy Optimization struggles with composite
  rewards, data wastage, and uniform training iterations. The authors propose three
  core mechanisms: dynamic additional sampling to retry invalid batches until valid
  proofs are found, equal-right advantage to decouple correctness-based advantage
  signs from auxiliary reward magnitudes ensuring correct gradient directions, and
  dynamic additional iterations to apply more updates to challenging samples that
  eventually succeed.'
---

# GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning

## Quick Facts
- arXiv ID: 2601.06795
- Source URL: https://arxiv.org/abs/2601.06795
- Authors: Zhengqing Yan, Xinyang Liu, Yi Zhang, Fan Guo, ChengXun Jia, Junchen Wan, Yao Liu, Qi Liu, Jihao Huang, Kang Song
- Reference count: 39
- Key outcome: 84% improvement on PutnamBench with dynamic sampling, equal-right advantage, and dynamic iterations

## Executive Summary
GDEPO addresses critical inefficiencies in reinforcement learning for automated theorem proving, where Group Relative Policy Optimization (GRPO) struggles with composite rewards, data wastage, and uniform training iterations. The authors propose three core mechanisms: dynamic additional sampling retries invalid batches until valid proofs are found, equal-right advantage decouples correctness-based advantage signs from auxiliary reward magnitudes ensuring correct gradient directions, and dynamic additional iterations apply more updates to challenging samples that eventually succeed. Experiments on MinF2F-test, MathOlympiadBench, and PutnamBench demonstrate significant performance gains, especially an 84% improvement on the most difficult PutnamBench, with ablation studies confirming the necessity of each component. The method improves data utilization and optimization efficiency in formal theorem proving.

## Method Summary
GDEPO builds upon GRPO with three enhancements for sample-constrained reinforcement learning in automated theorem proving. First, dynamic additional sampling performs up to n attempts of resampling invalid batches until at least one correct proof is found, exploring low-probability correct solutions. Second, equal-right advantage uses binary verifier correctness for the sign of the advantage function while auxiliary rewards (conciseness, repetition penalties) modulate only the magnitude, preventing gradient inversion where correct trajectories receive negative advantages. Third, dynamic additional iterations apply m consecutive gradient updates to samples that required multiple sampling rounds (k>1) and eventually succeeded, accelerating learning on challenging-but-solvable problems. The method is implemented on top of Goedel-Prover-V2-8B using Open-R1 framework with specified hyperparameters.

## Key Results
- 84% improvement on PutnamBench (644 problems), the most difficult benchmark
- Pass rate improvements across all three benchmarks: MiniF2F-test, MathOlympiadBench, PutnamBench
- Ablation studies confirm necessity of each component (dynamic sampling, equal-right advantage, dynamic iterations)
- Enhanced data utilization without expanding training corpus
- Improved optimization efficiency for challenging formal theorem proving problems

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Additional Sampling
- Resampling invalid batches until at least one correct proof is found increases data utilization without expanding training corpus
- Explores low-probability correct solutions that would otherwise be discarded
- Core assumption: Challenging samples contain learnable structure with non-zero probability of correct generation
- Evidence: Formal specification in Algorithm 1, dynamic sampling terminates when ∃l_i^(k) = 1 or k = n

### Mechanism 2: Equal-Right Advantage
- Decoupling advantage sign (correctness) from magnitude (auxiliary rewards) prevents gradient inversion
- Advantage sign determined solely by binary verifier feedback (+1 correct, -1 incorrect)
- Auxiliary rewards modulate only magnitude via min-max normalization when pass rate ≤ τ
- Core assumption: Verifier correctness is primary objective; auxiliary rewards should refine but not override correctness signals
- Evidence: Mathematical proof that correct trajectories can receive negative advantages with composite rewards

### Mechanism 3: Dynamic Additional Iterations
- Applying extra gradient updates to samples requiring multiple sampling rounds accelerates learning
- If valid output set contains at least one correct trajectory AND was not first round (k > 1), all G trajectories undergo m consecutive updates
- Core assumption: Problems requiring sustained search have higher pedagogical value
- Evidence: Conditional application specified in Algorithm 3 when ∃l_i^(*) = 1 and k > 1

## Foundational Learning

- **Advantage Function in Policy Gradient Methods**
  - Why needed: GDEPO's core contribution redesigns advantage computation; understanding advantage = reward - baseline and sign determines probability change direction
  - Quick check: Given trajectory with normalized advantage +0.5, does gradient ascent increase or decrease generation probability?

- **Group Relative Normalization (GRPO)**
  - Why needed: GDEPO builds on GRPO's within-group normalization; baseline is group mean reward, standard deviation normalizes scale
  - Quick check: If all trajectories in group have identical rewards, what happens to advantage values?

- **Composite Reward Functions**
  - Why needed: Core problem emerges when correctness rewards (binary, sparse) combine with auxiliary rewards (continuous, dense)
  - Quick check: If correctness weight α = 1.0, auxiliary weight w = 0.5, correct trajectory has auxiliary score 0.1, group mean 0.8, could normalized advantage be negative?

## Architecture Onboarding

- **Component map:**
Query → [Sampler: k rounds until success or max] → Valid Group G*_q → [Verifier: Lean4 server] → Core rewards L*_q ∈ {+1, -1} → Auxiliary rewards R*_q → [Equal-Right Advantage] → [Dynamic Iterations] → [PPO-style clipped objective]

- **Critical path:** Advantage computation is architectural bottleneck; incorrect advantage signs propagate through all gradient updates. Verify Eq. 5 implementation matches decoupling logic before training.

- **Design tradeoffs:**
  - Maximum sampling attempts n: Higher values increase compute cost but improve data utilization
  - Pass rate threshold τ: Controls when equal-right advantage activates vs. standard GRPO
  - Additional iterations m: Higher m accelerates hard-sample learning but risks overfitting

- **Failure signatures:**
  - Correct proofs receiving negative gradient updates → check advantage sign vs. verifier label consistency
  - No improvement despite successful sampling → verify dynamic iterations condition triggers (k > 1 check)
  - Training instability with composite rewards → τ threshold misconfigured; equal-right advantage not activating

- **First 3 experiments:**
  1. Sanity check: Single binary reward, compare standard GRPO advantage vs. equal-right advantage on held-out batch. Verify signs match verifier labels 100%.
  2. Ablation by component: Train three variants—(1) dynamic sampling only, (2) equal-right advantage only, (3) both—and measure pass rate on difficulty-stratified validation set.
  3. Composite reward stress test: Construct synthetic group where one correct trajectory has worst auxiliary scores. Confirm GRPO assigns negative advantage while equal-right assigns positive advantage.

## Open Questions the Paper Calls Out

### Open Question 1
- How can the pass rate threshold τ in the equal-right advantage mechanism be optimally determined for different task difficulty distributions?
- The paper sets τ as a "predefined threshold" but provides no guidance on adaptive selection across varying difficulty regimes
- A systematic study varying τ across datasets with known difficulty distributions, measuring optimization stability and final proof success rates would resolve this

### Open Question 2
- Does the dynamic additional iterations mechanism risk overfitting to challenging samples when m is set too high?
- The paper applies extra gradient steps to hard samples but doesn't analyze whether excessive iterations cause diminishing returns or degradation on held-out problems
- Ablation experiments varying m across a range (1–10) with evaluation on both training and held-out benchmarks would detect overfitting

### Open Question 3
- How does GDEPO's computational overhead scale with maximum sampling attempts n, and what is the trade-off between sampling cost and performance gain?
- The conclusion states future efforts should emphasize "maximizing utility of existing data," implying computational efficiency remains an open concern
- Detailed profiling of training time and sample generation counts across different n values, correlated with final benchmark accuracy, would resolve this

## Limitations
- No specified hyperparameter values for critical components (n, τ, m) makes exact reproduction challenging
- Limited analysis of computational overhead introduced by dynamic sampling and additional iterations
- Equal-right advantage assumes verifier correctness is always primary objective, which may not hold for all formal verification contexts

## Confidence
- **High confidence** in mathematical formulation of equal-right advantage and its correctness-based gradient direction guarantee
- **Medium confidence** in practical effectiveness of dynamic additional sampling and iterations due to strong empirical results but limited ablation analysis
- **Medium confidence** in generalizability beyond formal theorem proving due to tight coupling to specific ATP challenges

## Next Checks
1. **Advantage sign verification:** Implement controlled experiment with synthetic reward distributions where composite rewards cause gradient inversion. Verify equal-right advantage produces opposite sign compared to standard GRPO.
2. **Hyperparameter sensitivity analysis:** Systematically vary n, τ, and m to identify optimal settings and measure trade-off between performance gains and computational cost across different problem difficulty levels.
3. **Computational overhead quantification:** Measure wall-clock time per training step with and without dynamic sampling/iterations on representative problem classes, and compare efficiency-cost trade-off against baseline GRPO.