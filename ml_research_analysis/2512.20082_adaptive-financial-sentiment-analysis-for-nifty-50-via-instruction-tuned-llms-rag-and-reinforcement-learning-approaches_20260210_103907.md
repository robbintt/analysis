---
ver: rpa2
title: Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs
  , RAG and Reinforcement Learning Approaches
arxiv_id: '2512.20082'
source_url: https://arxiv.org/abs/2512.20082
tags:
- sentiment
- financial
- market
- source
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces an adaptive framework for financial sentiment
  analysis that integrates instruction-tuned large language models (LLMs) with real-world
  market feedback. It fine-tunes LLaMA 3.2 3B on the SentiFin dataset for Indian financial
  headlines and employs a retrieval-augmented generation (RAG) pipeline to incorporate
  multi-source news context.
---

# Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches

## Quick Facts
- arXiv ID: 2512.20082
- Source URL: https://arxiv.org/abs/2512.20082
- Reference count: 27
- Primary result: RAG + market feedback achieves 0.6153 accuracy, 0.5746 weighted F1 on NIFTY 50 sentiment classification

## Executive Summary
This paper introduces an adaptive framework for financial sentiment analysis that integrates instruction-tuned large language models with real-world market feedback. The system fine-tunes LLaMA 3.2 3B on the SentiFin dataset for Indian financial headlines, employs a retrieval-augmented generation pipeline to incorporate multi-source news context, and dynamically adjusts source credibility based on sentiment-return alignment. A reinforcement learning agent (PPO) is trained to optimize context source weights over time, demonstrating improved accuracy and weighted F1-score over baseline models and static retrieval methods.

## Method Summary
The framework fine-tunes LLaMA 3.2 3B using QLoRA with rank=16, alpha=16, learning rate=2e-5, batch size=4, for 3 epochs on the SentiFin dataset. A RAG module retrieves top-k relevant articles from an 8,000-headline corpus spanning 8 Indian financial sources using all-MiniLM-L6-v2 embeddings and cosine similarity within a 3-day window. Market feedback compares predicted sentiment against next-day stock returns (with ±0.5% neutral zone) to update source weights via α=1e-4. A PPO agent learns to generalize source weighting across temporal data through policy optimization with +1/-1 rewards. The system achieves 0.6153 accuracy and 0.5746 weighted F1 on NIFTY 50 headlines.

## Key Results
- RAG + market feedback achieves 0.6153 accuracy and 0.5746 weighted F1 on test set
- RAG + Without Market Feedback improves accuracy to 0.6094 vs. 0.5520 for Fine-Tuned LLaMA alone (+5.74 percentage points)
- RAG + PPO achieves 0.6109 accuracy, comparable to direct feedback but with refined source selection
- Neutral class dominates with 67% of test instances, highlighting class imbalance challenges

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Retrieval-augmented context improves financial sentiment classification accuracy by providing domain-specific background information that headlines alone lack.
- **Mechanism:** The RAG module retrieves top-k relevant news articles from a multi-source corpus using sentence embeddings (all-MiniLM-L6-v2) and cosine similarity within a 3-day temporal window. Retrieved context is concatenated with the query headline before LLM inference.
- **Core assumption:** Headlines from related articles contain sentiment-consistent signals that disambiguate brief or context-poor queries.
- **Evidence anchors:**
  - [abstract] "a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings"
  - [section 6, Table 4] RAG + Without Market Feedback achieves 0.6094 accuracy vs. 0.5520 for Fine-Tuned LLaMA alone (+5.74 percentage points)
  - [corpus] Related work (Zhang et al. 2023) confirms retrieval-augmented LLMs improve financial sentiment reliability
- **Break condition:** If retrieved articles introduce conflicting sentiment signals or if the 3-day window captures stale market-irrelevant news, performance may degrade.

### Mechanism 2
- **Claim:** Market feedback-driven source weighting aligns sentiment predictions with actual stock returns by iteratively reinforcing reliable sources and penalizing unreliable ones.
- **Mechanism:** After each prediction, the system compares predicted sentiment against ground-truth labels derived from next-day stock returns. Source weights are updated via: w_new = clamp(w_old ± α), where α = 1×10^-4. A neutral return zone of ±0.5% prevents penalizing insignificant price movements.
- **Core assumption:** Sources that historically contributed to sentiment-return alignment will continue to do so; past predictive utility indicates future reliability.
- **Evidence anchors:**
  - [abstract] "a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns"
  - [section 4.3.1] "If the predicted sentiment aligns with the actual stock movement... the weights of the contributing sources are increased"
  - [corpus] Zhao et al. (2024) demonstrates similar feedback-aware RAG with market return alignment
- **Break condition:** If market regime shifts (e.g., news-driven vs. fundamentals-driven periods) alter which sources are predictive, static feedback accumulation may lag or misallocate weight.

### Mechanism 3
- **Claim:** PPO-based reinforcement learning generalizes source weighting across temporal data by learning cumulative reward-maximizing policies rather than relying on per-instance updates.
- **Mechanism:** The PPO agent treats source weighting as a policy optimization problem. State = normalized source weights + query features; Action = updated weight distribution; Reward = +1 if sentiment matches ground truth, -1 otherwise. The clipped surrogate objective ensures stable updates.
- **Core assumption:** There exists a learnable, time-generalizable mapping from context features to optimal source weights that outperforms myopic feedback updates.
- **Evidence anchors:**
  - [abstract] "a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated... to generalize this adaptive mechanism across temporal data"
  - [section 6, Table 4] RAG + PPO achieves 0.6109 accuracy, comparable to direct feedback (0.6153) but with refined source selection
  - [corpus] Weak direct evidence in neighboring papers; related work (Zhao 2024) uses RL for similar adaptive weighting but in different market context
- **Break condition:** If reward signals are sparse or noisy (many neutral returns), PPO may converge to trivial policies (e.g., uniform weights) that provide no adaptive benefit.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - **Why needed here:** Financial headlines are often too brief for reliable sentiment inference. RAG supplies external context from a curated corpus.
  - **Quick check question:** Can you explain how cosine similarity over sentence embeddings selects context, and why a 3-day window is used?

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** Direct market feedback updates are noisy and short-term. PPO provides stable, cumulative policy learning for source weighting.
  - **Quick check question:** What is the role of the clipped surrogate objective in preventing destabilizing policy updates?

- **Concept: Market-Adjusted Sentiment Labeling**
  - **Why needed here:** Subjective human labels may not reflect market reality. The paper uses next-day returns relative to rolling statistics to derive ground truth.
  - **Quick check question:** How would you compute a "positive" label using 30-day rolling mean and standard deviation of returns?

## Architecture Onboarding

- **Component map:** Input headline → RAG retrieval (3-day window, cosine similarity) → weighted context selection → prompt construction → LLaMA inference → market feedback comparison → source weight update (feedback/PPO)

- **Critical path:** 1. Input headline + metadata → 2. Retrieve candidate articles (3-day window, filtered sources) → 3. Compute weighted relevance scores → 4. Select top-k, construct prompt → 5. LLaMA inference → 6. Compare prediction to next-day return → 7. Update source weights (feedback or PPO)

- **Design tradeoffs:**
  - **Accuracy vs. latency:** Larger k improves context but increases inference time
  - **Stability vs. adaptivity:** Small α (1×10^-4) prevents volatile weight swings but slows adaptation to regime changes
  - **PPO vs. direct feedback:** PPO generalizes better across time but requires more training episodes; direct feedback is simpler but noisier

- **Failure signatures:**
  - Sudden accuracy drop after market regime shift (weights stale)
  - High neutral-class bias (price context alone pushed accuracy to 0.66 but F1 did not improve)
  - PPO weight collapse to single source (insufficient reward signal diversity)

- **First 3 experiments:**
  1. **Ablation on k:** Test k = 1, 3, 5, 10 to find optimal retrieval depth for accuracy/latency tradeoff
  2. **Window sensitivity:** Compare 1-day, 3-day, 7-day retrieval windows to assess temporal context relevance
  3. **Regime robustness:** Evaluate weight stability across high-volatility vs. low-volatility market periods using rolling volatility as a regime indicator

## Open Questions the Paper Calls Out
- The paper concludes that "in many cases, news sentiment alone may not capture stock behavior, especially when price movements are driven by stock fundamentals," suggesting the need to incorporate fundamental indicators and peer stock information for improved market alignment.

## Limitations
- The framework relies solely on news headlines and short-term price context, ignoring company fundamentals that often drive longer-term market movements
- PPO provides only marginal accuracy improvements over simpler market feedback-based weighting despite added computational complexity
- The 2024-2025 dataset spans only ~2 years, limiting assessment of performance across different market regimes and temporal stability

## Confidence
- High confidence: RAG improves headline-only sentiment accuracy through context enrichment
- Medium confidence: Market feedback alignment improves source credibility assessment
- Low confidence: PPO provides generalizable improvement over direct feedback due to limited ablation evidence

## Next Checks
1. **Regime robustness test:** Evaluate source weight stability and accuracy across high-volatility vs. low-volatility market periods using rolling volatility as regime indicator
2. **PPO vs. feedback ablation:** Implement controlled experiments comparing PPO against direct feedback across multiple random seeds to quantify generalization benefits
3. **Temporal context sensitivity:** Systematically vary the retrieval window (1-day, 3-day, 7-day) to identify optimal temporal context for sentiment prediction accuracy