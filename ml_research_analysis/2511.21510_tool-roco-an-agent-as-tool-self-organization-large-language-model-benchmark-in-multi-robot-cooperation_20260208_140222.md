---
ver: rpa2
title: 'Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark
  in Multi-robot Cooperation'
arxiv_id: '2511.21510'
source_url: https://arxiv.org/abs/2511.21510
tags:
- agents
- tool
- cooperation
- agent
- tool-roco
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Tool-RoCo introduces a novel benchmark for evaluating large language
  models (LLMs) in multi-agent cooperation by treating agents as tools. The benchmark
  defines four progressive cooperation paradigms ranging from centralized coordination
  to full self-organization, and introduces two novel metrics: Cooperative Tool Ratio
  (CT) measuring agent-to-agent assistance and Self-Organization (SO) measuring autonomous
  collaboration initiation.'
---

# Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation

## Quick Facts
- arXiv ID: 2511.21510
- Source URL: https://arxiv.org/abs/2511.21510
- Authors: Ke Zhang; Xiaoning Zhao; Ce Zheng; Jiahong Ning; Dandan Zhu; Wenqi Zhang; Chen Sun; Toshiharu Sugawara
- Reference count: 6
- Primary result: Introduces benchmark treating agents as tools, revealing LLMs maintain high activation rates but low cooperative tool usage in multi-agent cooperation

## Executive Summary
Tool-RoCo introduces a novel benchmark for evaluating large language models (LLMs) in multi-agent cooperation by treating agents as tools. The benchmark defines four progressive cooperation paradigms ranging from centralized coordination to full self-organization, and introduces two novel metrics: Cooperative Tool Ratio (CT) measuring agent-to-agent assistance and Self-Organization (SO) measuring autonomous collaboration initiation. Experiments across three multi-robot tasks with four LLM models reveal that while larger models show better tool usage and coordination, they still struggle with self-organization—maintaining high activation rates (SO averaging 96.42%) but low cooperation tool usage (CT at 7.09%). The benchmark highlights the need for improved LLM autonomy in multi-agent systems and provides a systematic framework for evaluating cooperative behaviors beyond traditional tool usage metrics.

## Method Summary
Tool-RoCo evaluates LLMs on three multi-robot tasks (CABINET, PACK, SORT) using an agent-as-tool abstraction across four progressive paradigms. Each agent maintains a candidate tool set including common tools (direct environment actions) and cooperative tools (agent activation/deactivation). The framework measures traditional tool-use metrics plus novel Cooperative Tool Ratio (CT) and Self-Organization (SO) metrics. Experiments run 5 episodes per task with 4 LLM models across centralized and decentralized configurations, costing approximately $110 total.

## Key Results
- LLMs achieve high tool calling percentages (70-80%) in centralized cooperation but struggle with self-organization
- SO averages 96.42% across all models, indicating persistent agent activation regardless of task needs
- CT remains low at 7.09%, showing rare cooperative tool usage despite high activation rates
- GPT-5 outperforms other models across all paradigms, particularly in tool usage and coordination
- Larger models demonstrate better overall performance but still exhibit activation bias in self-organization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Treating agents as tools provides a structured interface for evaluating multi-agent cooperation through existing tool-use paradigms.
- Mechanism: Each LLM agent maintains a candidate tool set comprising "common tools" (direct environment actions like PICK, PLACE) and "cooperative tools" (agent activation/deactivation calls). Agents select from this unified set based on local observations, receive structured feedback, and adjust in subsequent rounds. This abstraction reduces coordination to a tool-selection problem with measurable outputs.
- Core assumption: Cooperative behavior can be meaningfully evaluated through the frequency and context of tool invocations rather than only task completion outcomes.

### Mechanism 2
- Claim: Progressive paradigm hierarchy isolates specific LLM capabilities by systematically varying centralization and agent-as-tool availability.
- Mechanism: The four paradigms incrementally increase autonomy demands: (1) Centralized Cooperation tests basic tool selection with full observability; (2) Centralized Self-organization adds activation decisions; (3) Decentralized Cooperation introduces partial observability and independent LLMs; (4) Self-organization combines decentralization with dynamic team formation starting from a single active agent. Performance degradation across paradigms reveals specific capability gaps.
- Core assumption: Capability gaps are compositional—failures in higher paradigms can be attributed to specific missing capacities (reflection, local reasoning, collaboration initiation) isolated in earlier paradigms.

### Mechanism 3
- Claim: High self-organization ratio (SO) combined with low cooperative tool ratio (CT) reveals a systematic bias toward activation over deactivation in current LLMs.
- Mechanism: SO measures activation calls / (total cooperative tool calls); CT measures cooperative tool calls / (all tool calls). The observed SO≈96% with CT≈7% indicates LLMs frequently activate agents when using cooperative tools but rarely invoke cooperative tools at all—and when they do, they almost never deactivate. This suggests LLMs default to maintaining active collaborators regardless of task needs.
- Core assumption: Optimal coordination requires balanced activation and deactivation; persistent activation indicates suboptimal resource management.

## Foundational Learning

- Concept: Dec-POMDP (Decentralized Partially Observable Markov Decision Process)
  - Why needed here: Tool-RoCo formulates each task as a Dec-POMDP with local observations, individual rewards, and probabilistic transitions. Understanding this formalism is necessary to interpret why decentralized paradigms are harder and what partial observability means for agent coordination.
  - Quick check question: Can you explain why a Dec-POMDP is harder to solve than a standard MDP, and what information each agent lacks in the decentralized cooperation paradigm?

- Concept: Tool-Use Evaluation in LLMs
  - Why needed here: The benchmark extends single-agent tool-use metrics (tool calling accuracy, parameter validation, execution validity) to multi-agent settings. Prior familiarity with how tool-use is typically evaluated clarifies what the CT and SO metrics add.
  - Quick check question: In standard tool-use benchmarks, what does "parameter validation" measure, and why does Tool-RoCo argue this is insufficient for evaluating cooperation?

- Concept: Multi-Agent Coordination Patterns
  - Why needed here: The four paradigms map to known coordination patterns (centralized vs. decentralized, fixed vs. dynamic teams). Recognizing these patterns helps engineers select the appropriate paradigm for their target application.
  - Quick check question: What is the difference between "centralized self-organization" and "self-organization cooperation" in terms of who makes activation decisions and what information they have?

## Architecture Onboarding

- Component map:
  Environment (RoCo tasks) -> LLM Agent(s) -> Tool Executor -> Evaluation Module

- Critical path:
  1. Initialize task and paradigm (select centralized vs. decentralized, agent-as-tool vs. not)
  2. For each timestep: agents receive local observation → generate structured tool selection → executor validates and executes → environment returns feedback → update agent history
  3. After episode: compute all metrics, log tool traces for CT/SO analysis

- Design tradeoffs:
  - Centralized paradigms: Higher token costs (global state encoding) but better coordination; use for establishing performance upper bounds
  - Decentralized paradigms: Lower per-agent token costs but require local reasoning under partial observability; use for evaluating autonomy
  - Agent-as-tool: Enables CT/SO metrics but increases action space complexity; smaller models struggle more with the combined decision load

- Failure signatures:
  - Low tool calling % with high param validation: LLM cannot generate valid tool names but formats correctly → prompt engineering issue
  - High CT with low Win: Agents collaborate frequently but ineffectively → coordination strategy flawed, not absence of collaboration
  - SO ≈ 100% with CT near 0%: Agents never use cooperative tools → model does not understand agent-as-tool concept or task does not require collaboration

- First 3 experiments:
  1. Replicate Centralized Cooperation on CABINET task with GPT-4o-mini and GPT-4.1 to verify token consumption scaling matches Figure 3 and tool calling % aligns with Table 3. This establishes baseline implementation correctness.
  2. Run Self-organization Cooperation paradigm on PACK task with a single model, logging each agent's active agent pool at every timestep. Analyze whether any agent ever issues a Disconnect call; if not, confirm the SO/CT asymmetry.
  3. Ablate the reflection mechanism by disabling feedback incorporation (remove environment feedback from next state). Compare modification rate and Win rate against full system to quantify reflection's contribution to task success.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be trained or fine-tuned to balance agent activation and deactivation in self-organizing multi-agent systems?
- Basis in paper: [explicit] The paper concludes that "current LLMs tend to maintain cooperation (Average SO is 96.42%) rather than disconnect redundant and unnecessary agents," identifying this as a "fundamental limitation" requiring future work.
- Why unresolved: The benchmark only measures this gap but proposes no mechanism to teach LLMs when to deactivate collaborators versus when to seek assistance.
- What evidence would resolve it: Demonstrating that a specific training intervention significantly increases contextually appropriate deactivation while maintaining task success rates.

### Open Question 2
- Question: What fine-grained metrics beyond Cooperative Tool Ratio (CT) and Self-Organization (SO) can better capture temporal efficiency and redundancy in multi-agent collaboration?
- Basis in paper: [explicit] The "Further work" section states that "these two measures are insufficient to capture all the characteristics of multi-agent collaboration" and calls for metrics including "temporal efficiency of cooperation and redundancy in agent activation."
- Why unresolved: CT and SO are ratio-based aggregate metrics that do not capture when cooperation occurs, whether timing is optimal, or whether activations are redundant.
- What evidence would resolve it: Proposing and validating new metrics that correlate with objective efficiency gains when agents collaboratively self-organize optimally.

### Open Question 3
- Question: Can reinforcement learning fine-tuning or curriculum learning using Tool-RoCo as a training environment improve LLM cooperative behaviors?
- Basis in paper: [explicit] The paper states: "Our future research will integrate Tool-RoCo into a training environment, enabling reinforcement learning fine-tuning or curriculum learning to enhance cooperative and organizational abilities of LLM agents."
- Why unresolved: Tool-RoCo currently functions only as an evaluation benchmark; no training experiments have been conducted.
- What evidence would resolve it: Showing that agents fine-tuned via RL on Tool-RoCo tasks achieve higher CT and more balanced SO scores compared to baseline models.

### Open Question 4
- Question: Does extending the cooperation tool set beyond the current two "naive" tools enable more sophisticated organizational structures?
- Basis in paper: [explicit] The paper acknowledges that "current cooperation tool usage just contains two naive tools and cannot contain all organizational structures of multi-agents," calling for extension in future work.
- Why unresolved: The limited tool set constrains the complexity of coordination patterns agents can express, potentially underestimating LLM capabilities.
- What evidence would resolve it: Implementing richer cooperative tools (e.g., role assignment, task delegation, conditional handoffs) and observing whether LLMs utilize them effectively.

## Limitations

- The benchmark's focus on discrete tool calls may miss subtle coordination dynamics like implicit communication, timing synchronization, and physical joint manipulation that are critical for real-world robot cooperation
- CT and SO metrics assume deactivation is equally valuable as activation, but task-specific optimal policies may legitimately require persistent agent activation
- The progressive paradigm hierarchy assumes monotonic capability building, but some models may exhibit non-monotonic performance jumps that violate this assumption

## Confidence

- High confidence: The basic agent-as-tool abstraction works as described, and the four progressive paradigms are correctly implemented
- Medium confidence: CT and SO metrics meaningfully capture cooperative behavior patterns, though their interpretation requires task-specific validation
- Medium confidence: The observed SO/CT asymmetry reflects a systematic LLM limitation rather than implementation artifacts
- Low confidence: The benchmark fully captures real-world multi-robot coordination complexity

## Next Checks

1. Run the same experiments with different LLM families (Claude, Gemini) to verify whether the SO/CT asymmetry is universal or model-specific
2. Design a task where persistent agent activation is clearly suboptimal and verify whether LLMs still maintain high SO, confirming the activation bias interpretation
3. Implement a continuous-time version of the benchmark to capture implicit coordination signals and compare with the discrete tool-call results to quantify what's missed by the abstraction