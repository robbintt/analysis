---
ver: rpa2
title: 'APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition'
arxiv_id: '2601.19452'
source_url: https://arxiv.org/abs/2601.19452
tags:
- learning
- priors
- prior
- demonstrations
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adaptive Policy Composition (APC), a hierarchical
  reinforcement learning method that leverages demonstration data while maintaining
  flexibility under misalignment. APC combines multiple data-driven normalizing flow
  priors with a prior-free actor under a learning-free arbitrator selector, enabling
  robust exploration and adaptation.
---

# APC-RL: Exceeding Data-Driven Behavior Priors with Adaptive Policy Composition

## Quick Facts
- arXiv ID: 2601.19452
- Source URL: https://arxiv.org/abs/2601.19452
- Reference count: 37
- Introduces APC, a hierarchical RL method that adaptively composes multiple data-driven priors with a prior-free actor to achieve robust performance under misaligned demonstrations

## Executive Summary
This paper presents Adaptive Policy Composition (APC), a novel hierarchical reinforcement learning method that combines multiple data-driven normalizing flow priors with a prior-free actor. APC addresses the challenge of leveraging demonstration data when demonstrations may be suboptimal or misaligned with the task reward. The method employs a learning-free arbitrator selector that adaptively chooses which prior to follow, enabling robust exploration and adaptation. By sharing reward signals across actors and maintaining flexibility through the prior-free component, APC demonstrates strong performance on continuous-control benchmarks, outperforming both pure imitation learning and methods like PARROT.

## Method Summary
APC is a hierarchical RL method that constructs policies as weighted compositions of multiple data-driven normalizing flow priors and a prior-free actor. Each prior is trained from demonstration data using normalizing flows to model behavior distributions. The method employs a learning-free arbitrator selector that evaluates which prior to follow based on current state information. A key innovation is the shared reward signal mechanism, where the actor with the highest priority (selected by the arbitrator) propagates its reward to lower-priority actors, enabling coordinated learning across all components. This architecture allows APC to maintain flexibility when priors are misaligned while still leveraging their guidance for efficient exploration.

## Key Results
- APC outperforms baselines like PARROT and imitation learning on continuous-control benchmarks
- Demonstrates robustness to misaligned demonstrations by adaptively selecting between priors
- Efficiently uses suboptimal priors to bootstrap exploration without performance degradation
- Achieves strong performance across multiple Mujoco and simulated robotics tasks

## Why This Works (Mechanism)
APC works by maintaining multiple data-driven priors that can capture different aspects of behavior from demonstration data, combined with a flexible prior-free actor. The learning-free arbitrator selects between these components based on state information, allowing the system to adapt when priors are misaligned with the true task. The shared reward mechanism ensures that all components learn from successful experiences, even when they weren't the primary decision-maker. This composition allows the agent to benefit from demonstration guidance while maintaining the ability to deviate when necessary, leading to robust performance across varying levels of prior alignment.

## Foundational Learning
- **Normalizing Flows**: Required for modeling complex behavior distributions from demonstration data; quick check: verify the flow architecture can represent the demonstration distributions
- **Hierarchical RL**: Needed to structure the composition of multiple policy components; quick check: ensure the hierarchical structure doesn't introduce excessive complexity
- **Reward Sharing**: Critical for coordinated learning across policy components; quick check: verify shared rewards improve learning efficiency
- **Adaptive Selection**: Essential for robustness to misaligned priors; quick check: test arbitrator performance across different alignment scenarios
- **Continuous Control**: Domain-specific knowledge for benchmarking; quick check: validate performance on standard continuous control tasks
- **Imitation Learning**: Baseline comparison framework; quick check: ensure fair comparison with pure imitation methods

## Architecture Onboarding
**Component Map**: State -> Arbitrator Selector -> [Prior 1, Prior 2, ..., Prior N, Prior-Free Actor] -> Action

**Critical Path**: State input → Arbitrator evaluation → Selected actor execution → Action output

**Design Tradeoffs**: The learning-free arbitrator provides stability but may miss optimal selection strategies; multiple priors increase flexibility but add computational overhead; shared rewards improve coordination but may slow specialization

**Failure Signatures**: Poor arbitrator selection leading to persistent use of misaligned priors; insufficient diversity among priors causing redundancy; failure to learn effective prior-free behavior when all priors are poor

**First Experiments**:
1. Test arbitrator selection accuracy on a simple environment with known prior alignment
2. Evaluate performance with a single prior versus multiple priors to quantify composition benefits
3. Measure learning efficiency with and without shared reward mechanism

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on continuous-control benchmarks with relatively simple action spaces
- Computational overhead from maintaining multiple flow-based priors not fully characterized
- Scalability analysis limited to moderate number of priors (up to 5)
- Learning-free arbitrator may become suboptimal in more complex scenarios

## Confidence
- Claims about robustness to misaligned demonstrations: High
- Claims about computational efficiency: Medium
- Claims about scalability to many priors: Low

## Next Checks
1. Test APC with significantly more than 5 priors to assess scalability and potential performance degradation
2. Evaluate the method on tasks with discrete or high-dimensional action spaces to verify generality
3. Conduct ablation studies to quantify the contribution of the learning-free arbitrator versus learned alternatives in different alignment scenarios