---
ver: rpa2
title: A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep
  Representation Learning
arxiv_id: '2510.12957'
source_url: https://arxiv.org/abs/2510.12957
tags:
- bias
- explainable
- generative
- interpretability
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel multimodal explainable AI (XAI) framework
  for trustworthy CNNs and bias detection in deep representation learning. The framework
  addresses the opacity and bias issues in deep neural networks, particularly in high-stakes
  applications, by unifying attention-augmented feature fusion, Grad-CAM++-based local
  explanations, and a Reveal-to-Revise feedback loop for bias detection and mitigation.
---

# A Multimodal XAI Framework for Trustworthy CNNs and Bias Detection in Deep Representation Learning

## Quick Facts
- arXiv ID: 2510.12957
- Source URL: https://arxiv.org/abs/2510.12957
- Authors: Noor Islam S. Mohammad
- Reference count: 40
- Primary result: Achieves 93.2% classification accuracy, 91.6% F1-score, and 78.1% explanation fidelity on multimodal MNIST extensions

## Executive Summary
This paper introduces a novel multimodal explainable AI framework that addresses opacity and bias in deep neural networks through attention-augmented feature fusion, Grad-CAM++-based local explanations, and a Reveal-to-Revise feedback loop for bias detection and mitigation. The framework unifies these components to achieve both high performance and transparency, evaluated on multimodal extensions of MNIST where it outperforms unimodal and non-explainable baselines. The approach bridges the gap between performance, transparency, and fairness, providing a practical pathway for trustworthy AI in sensitive domains.

## Method Summary
The framework employs a dual-encoder architecture with ResNet-50 for visual features and BERT-base for text embeddings, fused via scaled dot-product attention that dynamically weights spatial regions based on cross-modal relevance. Grad-CAM++ generates class-discriminative heatmaps for local explanations, while a bias-aware regularization component with Reveal-to-Revise feedback iteratively detects and mitigates distributional bias through saliency-based feature reweighting. The system is trained using AdamW optimizer with cosine annealing scheduler, incorporating both classification loss and bias penalty terms to achieve robust, explainable, and fair predictions.

## Key Results
- Achieves 93.2% classification accuracy and 91.6% F1-score on multimodal MNIST extensions
- Grad-CAM++ explanations demonstrate 78.1% explanation fidelity (IoU-XAI) against ground-truth regions
- Ablation studies show significant performance gains over unimodal and non-explainable baselines while reducing bias variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal attention fusion improves classification accuracy by 4.1% over unimodal baselines.
- Mechanism: Visual features from ResNet-50 and text embeddings from BERT-base are aligned via scaled dot-product attention, computing attention weights α = softmax(f_attn(F)) that weight feature maps by contextual relevance across modalities. This reduces modality-specific bias by forcing the model to learn complementary representations.
- Core assumption: Visual and textual modalities provide non-redundant information that can be linearly combined in attention space.
- Evidence anchors:
  - [abstract] "unifies attention-augmented feature fusion"
  - [section 4.5] "attention mechanisms... dynamically weighting spatial or feature-specific regions"
  - [corpus] Limited corpus support; neighbor papers focus on single-modality XAI (FMR avg 0.42, no direct multimodal comparison)
- Break condition: If visual and text features are highly correlated or one modality is noise, attention may overfit to dominant modality.

### Mechanism 2
- Claim: Grad-CAM++ provides faithful local explanations with 78.1% IoU-XAI alignment to ground-truth regions.
- Mechanism: Computes class-discriminative importance weights α^c_k via global average pooling of gradients ∂y^c/∂A^k_{i,j}, then combines feature maps weighted by these coefficients and applies ReLU to retain only positive contributions. The resulting heatmap highlights spatial regions most influential for prediction.
- Core assumption: Gradient flow through convolutional layers faithfully represents model reasoning.
- Evidence anchors:
  - [abstract] "78.1% explanation fidelity (IoU-XAI)"
  - [section 4.7] Equation 18-19 formalizes weight computation and ReLU filtering
  - [corpus] Neighbor paper "Explaning with trees" notes gradient methods can produce noisy maps, suggesting this mechanism may be brittle on complex architectures
- Break condition: For highly non-convex decision boundaries or saturated activations, gradients may vanish or provide misleading attributions.

### Mechanism 3
- Claim: Bias-aware regularization with Reveal-to-Revise feedback reduces distributional bias while maintaining generation fidelity.
- Mechanism: A bias regularizer R_bias = ||E[B(x̃)] - E[B(x)]||^2 penalizes discrepancies between generated and real distributions over sensitive attributes. The Reveal-to-Revise loop iteratively refines model parameters θ using saliency maps to identify bias-inducing features, then reweights training to suppress them.
- Core assumption: Bias manifests as distributional drift detectable via gradient-based saliency; the bias function B(·) correctly identifies protected attributes.
- Evidence anchors:
  - [abstract] "Reveal-to-Revise feedback loop for bias detection and mitigation"
  - [section 7] Unified framework equations and Algorithm 3 describe the training loop with bias regularization
  - [corpus] Weak corpus evidence; no neighbor papers directly validate bias regularization in generative settings
- Break condition: If bias is encoded in non-salient features or the bias function B(·) is misspecified, regularization may remove informative features without addressing true bias.

## Foundational Learning

- Concept: **Wasserstein GANs with Gradient Penalty (WGAN-GP)**
  - Why needed here: The framework builds on WGAN-GP for stable adversarial training; understanding Lipschitz constraints and gradient penalty is essential to debug mode collapse.
  - Quick check question: Can you explain why gradient penalty (Equation 9) is needed to enforce the 1-Lipschitz constraint?

- Concept: **Attention Mechanisms and Dot-Product Attention**
  - Why needed here: Cross-modal fusion relies on scaled dot-product attention; misunderstanding attention weights will lead to incorrect feature fusion debugging.
  - Quick check question: How does softmax normalization in α = softmax(f_attn(F)) ensure attention weights sum to 1?

- Concept: **Gradient-Based Attribution (Grad-CAM Family)**
  - Why needed here: Local explanations use Grad-CAM++; interpreting heatmaps requires understanding how gradients flow through convolutional layers.
  - Quick check question: Why does Grad-CAM apply ReLU to the weighted sum of feature maps (Equation 19)?

## Architecture Onboarding

- Component map:
  - Input → Encoders (ResNet-50 + BERT-base) → Attention Fusion → Classification → Grad-CAM++ Attribution → Bias Penalty Computation → Parameter Update via AdamW

- Critical path: Input → Encoders → Attention Fusion → Classification → Grad-CAM++ Attribution → Bias Penalty Computation → Parameter Update via AdamW

- Design tradeoffs:
  - Accuracy vs. interpretability: Ablation shows removing XAI module increases accuracy to 90.1% but IoU-XAI drops to 0% (Table 3)
  - Robustness vs. clean accuracy: Adversarial training improves FGSM robustness (73.2% → 76.8%) but increases test error from 11.0% to 15.8% (Table 4)
  - Computational cost: Grad-CAM++ adds backward pass overhead; total complexity dominated by attention and encoder stages

- Failure signatures:
  - Mode collapse in GAN training: Indicates insufficient critic iterations or gradient penalty weight λ_GP too low
  - Noisy Grad-CAM++ heatmaps: May indicate gradient saturation or need for hybrid perturbation-based methods (Equation 20)
  - High variance in ablation without bias feedback: Suggests bias regularization stabilizes training (Table 2)

- First 3 experiments:
  1. **Unimodal baseline comparison**: Train visual-only (ResNet-50) and text-only (BERT-base) models to replicate Table 3 baseline accuracies (87.3%, 88.5%) and confirm multimodal gain.
  2. **Grad-CAM++ fidelity test**: Generate attribution maps for validation samples and compute IoU-XAI against annotated ground-truth regions to verify 78.1% fidelity claim.
  3. **Ablation of bias feedback**: Train with and without Reveal-to-Revise loop on a biased subset (e.g., skewed class distribution) to measure variance reduction and accuracy delta per Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed bias-aware framework extend its robustness to iterative Projected Gradient Descent (PGD) attacks?
- Basis in paper: [explicit] The authors state in Section 11.3 that "robustness against iterative PGD attacks remains an open research challenge," despite the model's success against FGSM and BIM attacks.
- Why unresolved: While adversarial training significantly improved resilience to basic iterative methods (BIM), the specific vulnerability of the proposed architecture to the more complex PGD attack vector was not resolved in the presented results.
- What evidence would resolve it: Empirical evaluation showing maintained accuracy (>70%) under PGD attack conditions using the unified bias-aware regularization techniques.

### Open Question 2
- Question: How does the framework performance scale to high-dimensional, high-stakes datasets beyond simple benchmarks?
- Basis in paper: [inferred] The Abstract explicitly critiques standard benchmarks (MNIST) for failing to "expose latent biases," yet the experimental evaluation (Section 10 and 11) relies heavily on MNIST and Fashion-MNIST.
- Why unresolved: While the framework achieves 93.2% accuracy on these benchmarks, it is unclear if the Latent Attribution Mechanism and attention fusion scale to the complexities of "healthcare, law, and finance" data as claimed in the Introduction.
- What evidence would resolve it: Validation of classification accuracy and explanation fidelity (IoU-XAI) on complex, real-world datasets such as medical imaging or legal text corpora.

### Open Question 3
- Question: To what extent does the "Reveal-to-Revise" feedback loop mitigate learned societal bias compared to standard adversarial training?
- Basis in paper: [inferred] The paper links the "Reveal-to-Revise" mechanism to bias correction in Algorithm 2, but Section 11 results focus primarily on adversarial robustness (FGSM/BIM) rather than quantitative bias mitigation metrics.
- Why unresolved: It remains ambiguous whether the observed stability and robustness are byproducts of general adversarial regularization or specifically result from the XAI-driven bias correction loop.
- What evidence would resolve it: Ablation studies measuring fairness metrics (e.g., demographic parity) on datasets with known protected attributes, isolating the bias loop from the adversarial training components.

## Limitations
- Exact formulation of "multimodal extensions" of MNIST is unspecified, creating significant reproduction barriers
- Grad-CAM++ explanation mechanism may be brittle on complex architectures with saturated activations
- Bias detection/correction mechanism lacks direct validation in the literature with only weak corpus support

## Confidence
- Multimodal fusion mechanism: **High** - well-established attention framework with clear implementation
- Grad-CAM++ explanations: **Medium** - proven technique but potential brittleness on complex architectures
- Bias detection/correction: **Low** - novel mechanism with limited external validation

## Next Checks
1. Implement and verify the multimodal data construction by creating paired image-text samples from MNIST with simple captions (e.g., "digit X")
2. Conduct ablation studies comparing Grad-CAM++ heatmaps against manually annotated ground-truth attribution regions to validate the 78.1% IoU-XAI claim
3. Test the Reveal-to-Revise loop on a controlled biased dataset (e.g., artificially skewed class distributions) to measure variance reduction and bias mitigation effectiveness