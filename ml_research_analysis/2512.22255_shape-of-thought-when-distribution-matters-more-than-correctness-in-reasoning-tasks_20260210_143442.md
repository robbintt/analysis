---
ver: rpa2
title: 'Shape of Thought: When Distribution Matters More than Correctness in Reasoning
  Tasks'
arxiv_id: '2512.22255'
source_url: https://arxiv.org/abs/2512.22255
tags:
- cots
- correct
- answer
- final
- incorrect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that synthetic chain-of-thought (CoT) traces
  from more capable models can improve reasoning performance in smaller language models
  even when all those traces lead to incorrect final answers. This finding challenges
  the common assumption that correctness is the primary factor in data quality for
  reasoning tasks.
---

# Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks

## Quick Facts
- arXiv ID: 2512.22255
- Source URL: https://arxiv.org/abs/2512.22255
- Reference count: 40
- Primary result: Synthetic CoTs with incorrect answers can outperform human-written CoTs when training reasoning models

## Executive Summary
This paper challenges the assumption that correctness is paramount in training data for reasoning tasks. Through controlled experiments on math and code reasoning, the authors demonstrate that synthetic chain-of-thought (CoT) traces from more capable models can improve smaller model performance even when all traces lead to incorrect final answers. The key insight is that training data closer to the model's own distribution can be more effective than fully correct human-written traces, which may be further from the model's natural output. This finding suggests that final-answer accuracy is not a reliable indicator of reasoning quality, and highlights the importance of dataset distribution in learning effective reasoning capabilities.

## Method Summary
The study uses supervised fine-tuning (SFT) on chain-of-thought reasoning traces across four datasets (MATH, GSM8K, Countdown, MBPP). Three data categories are created: human-written correct CoTs (H), synthetic correct-answer CoTs (G), and synthetic wrong-answer CoTs (W). The teacher model (Gemma-2-27B-IT) generates 64 samples per problem at temperature 0.8, classified via math_verify into G and W sets. Student models (Gemma-2-2B, Llama-3.1-8B, Qwen2.5-1.5B) are fine-tuned with batch sizes 64-256, learning rates 1e-6 to 2e-5, AdamW optimizer, weight decay 0.05, warmup 0.1, for 10 epochs. Evaluation uses greedy decoding and measures final answer correctness.

## Key Results
- Synthetic CoTs with incorrect final answers (W) can outperform human-written CoTs (H) on Countdown and MBPP datasets
- Training on incorrect synthetic data shows lower initial training loss and perplexity compared to human data
- Performance degrades when >80% of training data contains incorrect reasoning traces
- Paraphrasing human solutions to match model distribution recovers some performance loss

## Why This Works (Mechanism)
The effectiveness of incorrect synthetic CoTs stems from distribution alignment rather than correctness. When models are trained on data that matches their natural output distribution, they learn more efficiently even if that data contains errors. The synthetic data, despite being wrong, follows patterns and reasoning structures that are more consistent with what the model would generate itself, making it easier to learn from than human-written solutions that may use different reasoning approaches or notation.

## Foundational Learning
- Chain-of-thought reasoning: Sequential step-by-step problem solving in natural language
  - Why needed: Forms the core methodology for teaching models to reason through problems
  - Quick check: Can the model generate coherent multi-step solutions to simple arithmetic

- Supervised fine-tuning: Training on labeled examples to improve specific capabilities
  - Why needed: The primary method for adapting pre-trained models to reasoning tasks
  - Quick check: Does training loss decrease over epochs on held-out validation data

- Distribution matching: Aligning training data characteristics with model's natural output
  - Why needed: Explains why synthetic data outperforms human data despite being incorrect
  - Quick check: Compare perplexity of different training datasets on the student model

## Architecture Onboarding

Component map: Problem -> Teacher Model (generate CoTs) -> math_verify (classify G/W) -> Student Model (SFT) -> Evaluation

Critical path: The teacher model generation and classification step is critical, as it determines the quality and characteristics of training data that directly impacts student performance.

Design tradeoffs: The choice between human-written vs synthetic data involves balancing correctness against distribution alignment. Synthetic data may be more consistent and model-aligned but potentially contains systematic errors, while human data is more likely correct but may be harder for the model to learn from due to distribution mismatch.

Failure signatures: High initial training loss on human data compared to synthetic data indicates distribution mismatch. Poor performance despite correct final answers suggests the model struggles with the reasoning format rather than the content.

First experiments:
1. Generate synthetic CoTs for a small subset of problems and compare training loss curves against human data
2. Test paraphrasing human solutions to match model distribution and measure performance impact
3. Gradually increase the proportion of incorrect synthetic data to find the degradation threshold

## Open Questions the Paper Calls Out
- How does finetuning on unverified, incorrect CoT data impact subsequent policy optimization in Reinforcement Learning?
- Can reliable methods be developed to verify the step-level correctness of natural language reasoning?
- What principled algorithms can automatically align external data distributions with a model's native output distribution?

## Limitations
- The study relies on synthetic data generation quality, which may not generalize across different domains
- Evaluation focuses exclusively on final answer accuracy, not intermediate reasoning quality
- The math_verify classification system is not publicly available, making independent verification difficult

## Confidence
High: The empirical observation that synthetic CoTs can outperform human-written CoTs on certain datasets is well-supported by controlled experiments with clear baselines and multiple student model sizes.

Medium: The explanation that distribution closeness drives the performance advantage is plausible but not definitively proven. While training loss and perplexity evidence supports this hypothesis, alternative explanations cannot be ruled out.

Low: The generalizability of these findings across different reasoning domains and model architectures is uncertain. The paper tests only three datasets with specific characteristics.

## Next Checks
1. Conduct KL divergence and Wasserstein distance measurements between synthetic and human-written CoT distributions to quantify distribution similarity objectively.

2. Systematically categorize error types in synthetic wrong-answer CoTs and test whether certain error types are more tolerable than others during training.

3. Repeat experiments using different teacher models (GPT-4, Claude, or open-weights alternatives) to determine whether the distribution effect holds across different sources.