---
ver: rpa2
title: 'TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional
  Spatial Reasoning'
arxiv_id: '2601.16520'
source_url: https://arxiv.org/abs/2601.16520
tags:
- arxiv
- reasoning
- visual
- spatial
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TangramPuzzle, a benchmark for evaluating
  multimodal large language models on compositional spatial reasoning using the classic
  tangram puzzle. The authors propose a symbolic geometry framework called Tangram
  Construction Expression (TCE) to represent tangram assemblies with exact coordinates,
  enabling rigorous, machine-verifiable evaluation.
---

# TangramPuzzle: Evaluating Multimodal Large Language Models with Compositional Spatial Reasoning

## Quick Facts
- arXiv ID: 2601.16520
- Source URL: https://arxiv.org/abs/2601.16520
- Reference count: 38
- Introduces TangramPuzzle benchmark to evaluate multimodal large language models on compositional spatial reasoning using symbolic geometry framework

## Executive Summary
This paper introduces TangramPuzzle, a benchmark for evaluating multimodal large language models (MLLMs) on compositional spatial reasoning using the classic tangram puzzle. The authors propose a symbolic geometry framework called Tangram Construction Expression (TCE) to represent tangram assemblies with exact coordinates, enabling rigorous, machine-verifiable evaluation. The benchmark includes two tasks: Outline Prediction (inferring global silhouettes from local pieces) and End-to-End Tangram Solution Generation (constructing valid assemblies under strict geometric constraints). Evaluations across various open-source and proprietary models show significant performance gaps, with most models failing to satisfy geometric constraints despite reasonable silhouette matching. Notably, even top-performing models often achieve visual similarity through impermissible shape distortions or overlaps, revealing fundamental limitations in compositional spatial reasoning.

## Method Summary
The TangramPuzzle benchmark employs a symbolic geometry framework called Tangram Construction Expression (TCE) to represent tangram assemblies with exact coordinates. This allows for machine-verifiable evaluation of spatial reasoning tasks. The benchmark consists of two main tasks: Outline Prediction, where models must infer the global silhouette from local tangram pieces, and End-to-End Tangram Solution Generation, where models must construct valid assemblies while adhering to strict geometric constraints (no overlaps, no distortions, complete coverage). The evaluation uses metrics including Intersection over Union (IoU) for shape similarity and constraint violation counts for geometric validity. The TCE framework provides algebraic representations of shapes and operations, enabling precise error analysis beyond simple visual similarity measures.

## Key Results
- Most MLLMs fail to satisfy geometric constraints even when achieving reasonable IoU scores, with some models "cheating" through shape distortions or overlaps
- Open-source models generally underperform proprietary models, with only Gemini3-Pro and Claude-Sonnet-4.5 achieving near-perfect constraint satisfaction
- The Visual-Centric setting reveals a significant "Visual Grounding Gap," where models struggle to extract geometric coordinates from images without textual guidance
- Rigid Geometry Error and Assembly Structure Error show that even top-performing models frequently violate fundamental spatial constraints

## Why This Works (Mechanism)
The TangramPuzzle benchmark works by providing a rigorous, mathematically-verifiable framework for evaluating spatial reasoning that goes beyond semantic understanding. The TCE system encodes geometric constraints as algebraic expressions that can be automatically checked, revealing when models achieve visual similarity through impermissible means (distortions, overlaps). This exposes a fundamental limitation in current MLLMs: they optimize for visual similarity metrics rather than physical feasibility, treating spatial reasoning as a pattern-matching problem rather than a constraint satisfaction problem.

## Foundational Learning
- **Tangram Construction Expression (TCE)**: A symbolic geometry framework representing shapes as algebraic expressions with exact coordinates
  - Why needed: Enables machine-verifiable evaluation beyond visual similarity metrics
  - Quick check: Can verify if a proposed assembly satisfies non-overlap and complete coverage constraints
- **Intersection over Union (IoU)**: Metric measuring overlap between predicted and ground-truth shapes
  - Why needed: Quantifies visual similarity while being independent of scale/orientation
  - Quick check: IoU = (Area of Intersection) / (Area of Union)
- **Rigid Geometry Error**: Counts violations of shape rigidity constraints
  - Why needed: Detects when models distort pieces to achieve better visual matching
  - Quick check: Measures deviation from original piece dimensions and angles
- **Assembly Structure Error**: Counts violations of spatial assembly constraints
  - Why needed: Identifies overlaps and gaps in the final construction
  - Quick check: Verifies no overlapping areas and complete coverage of target silhouette

## Architecture Onboarding
- **Component map**: Tangram pieces (input) -> Visual Encoder -> LLM Reasoning -> Geometric Output -> TCE Constraint Checker -> Performance Metrics
- **Critical path**: Visual perception → Spatial reasoning → Geometric constraint satisfaction → Visual similarity
- **Design tradeoffs**: Strict geometric constraints vs. visual similarity optimization; symbolic representation vs. neural feature extraction
- **Failure signatures**: High IoU with constraint violations (shape distortion/overlaps), low IoU with correct constraints (incomplete understanding), failure in Visual-Centric setting (visual-to-symbolic mapping gap)
- **First experiments**:
  1. Test baseline models on simple tangram puzzles with 3-4 pieces to establish minimum competency threshold
  2. Evaluate constraint violation patterns across different puzzle complexities to identify failure modes
  3. Compare performance of same LLM with different visual encoders to isolate visual perception vs. reasoning limitations

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How will the observed tendency of MLLMs to prioritize visual matching over geometric constraints manifest in three-dimensional scenarios involving occlusion and variable object counts?
- Basis in paper: [explicit] The authors state in the Limitations section that the benchmark is currently confined to 2D planar geometry and that future work will extend this paradigm to three-dimensional settings.
- Why unresolved: The complexity of 3D reasoning, specifically regarding depth and occlusion, is untested in this benchmark; it is unknown if the "cheating" strategies (distortions/overlaps) will scale or fail catastrophically in 3D space.
- What evidence would resolve it: Extending the Tangram Construction Expression (TCE) to 3D coordinates and evaluating current top-performing models on 3D assembly tasks.

### Open Question 2
- Question: Can explicit geometric constraint modeling be integrated into MLLM training to prevent models from achieving high IoU scores through impermissible shape distortions?
- Basis in paper: [inferred] The analysis notes that models like Claude-Sonnet-4.5 achieve high IoU via "cheating" (distorting rigid pieces), suggesting a disconnect between semantic visual objectives and strict geometric validity.
- Why unresolved: Current training paradigms favor semantic similarity over physical feasibility; it is unclear if standard next-token prediction can internalize strict algebraic constraints without external verifiers or specialized loss functions.
- What evidence would resolve it: Fine-tuning models with a reward signal based on the TCE constraint validation (rigidity/physics checks) and measuring the reduction in Rigid Geometry Errors.

### Open Question 3
- Question: Is the failure of most MLLMs in the Visual-Centric setting caused by low-resolution visual encoders or an inability to map continuous visual features to discrete symbolic coordinates?
- Basis in paper: [inferred] The "Visual-Centric" ablation study revealed a "Visual Grounding Gap," where models struggled to extract geometric coordinates from images without textual "crutches," whereas Gemini3-Pro succeeded.
- Why unresolved: The paper identifies the dependency on text but does not isolate whether the issue lies in the visual acuity of the encoder or the reasoning module's translation of vision to math.
- What evidence would resolve it: Evaluating models with identical LLM backbones but varying visual encoder resolutions or specialized geometric pre-training on the visual-centric task.

## Limitations
- The benchmark is confined to 2D planar geometry, limiting generalizability to real-world 3D spatial reasoning tasks
- The TCE framework represents a highly specialized domain that may not capture the full spectrum of compositional spatial reasoning abilities
- The focus on exact coordinate matching may be overly strict, potentially penalizing models that demonstrate partial understanding through reasonable approximations

## Confidence
- High: The geometric constraint violations identified across multiple models are well-documented and mathematically verifiable
- Medium: The interpretation that these failures represent fundamental reasoning limitations rather than architectural constraints remains uncertain
- Medium: The comparison between open-source and proprietary models may be influenced by factors beyond spatial reasoning capability

## Next Checks
1. Evaluate model performance on tangram puzzles with increasing complexity (more pieces, irregular shapes) to determine whether failures stem from compositional reasoning limitations or basic geometric processing capabilities
2. Test whether fine-tuning on tangram-specific data improves performance while maintaining general reasoning abilities, distinguishing between architectural limitations and data-driven weaknesses
3. Compare human performance on identical tangram tasks to establish baseline expectations and determine whether model failures represent fundamental reasoning gaps or different problem-solving approaches