---
ver: rpa2
title: 'Unlocking the Potential of Past Research: Using Generative AI to Reconstruct
  Healthcare Simulation Models'
arxiv_id: '2503.21646'
source_url: https://arxiv.org/abs/2503.21646
tags:
- code
- simulation
- stage
- prompt
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates the feasibility of using generative AI
  to recreate healthcare discrete-event simulation (DES) models from published descriptions.
  Using a structured methodology with iterative prompt engineering and rigorous testing,
  we successfully generated, tested, and internally reproduced two complex healthcare
  DES models in Python.
---

# Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models

## Quick Facts
- arXiv ID: 2503.21646
- Source URL: https://arxiv.org/abs/2503.21646
- Authors: Thomas Monks; Alison Harper; Amy Heather
- Reference count: 40
- Primary result: Successfully generated functional healthcare DES models from published descriptions using structured prompt engineering and RAG

## Executive Summary
This study demonstrates that generative AI can successfully recreate complex healthcare discrete-event simulation models from published descriptions. Using a structured methodology with iterative prompt engineering and rigorous testing, researchers generated, tested, and internally reproduced two complex healthcare DES models in Python. The approach established a foundation for preserving and reusing healthcare simulation models through AI, though challenges remain in prompt engineering reliability and code generation consistency.

## Method Summary
The researchers used Perplexity.AI's free tier with RAG functionality to generate Python/SimPy code from published healthcare DES model descriptions. They employed a structured 12-stage iterative process with four-section prompts (main command, general model logic, simulation inputs, simulation methodology) and validated outputs using pytest, nbdime code differencing, and visual inspection. Two modellers independently recreated models to assess internal consistency, and generated Streamlit UIs for non-programmer access.

## Key Results
- Successfully generated functional DES models ranging from 262-531 lines of code
- Passed extensive verification tests including extreme-value testing and unit tests
- Internal replication showed consistency between independently generated code versions
- Generated browser-based UIs for non-programmer use
- One model successfully replicated original study metrics; the other failed due to missing distribution information

## Why This Works (Mechanism)

### Mechanism 1
Iterative prompt engineering with structured prompts yields functional DES models that naive copy-paste cannot produce. Prompts are decomposed into four explicit sections that constrain LLM output toward executable code, using a 12-aim ordering that creates scaffolding where each iteration builds on the prior.

### Mechanism 2
Retrieval-Augmented Generation (RAG) reduces hallucinations by grounding code generation in retrieved documentation and examples. Perplexity.AI queries web sources before generation, incorporating current SimPy documentation into the context window.

### Mechanism 3
Human-in-the-loop verification with code differencing and automated testing is necessary to catch LLM hallucinations. After each iteration, code is visually inspected, differenced against prior versions, and subjected to extreme-value tests and unit tests.

## Foundational Learning

- **Concept:** Discrete-event simulation (DES) fundamentals (entities, resources, queues, event scheduling, warm-up periods, replications)
  - *Why needed:* The generated models implement multi-class patient arrivals, balking, routing, and performance measures
  - *Quick check:* Can you explain why a warm-up period is needed before collecting statistics in a steady-state simulation?

- **Concept:** Python generators and SimPy process interaction (yield, timeout, request/release resources)
  - *Why needed:* All generated models use generator functions for patient processes
  - *Quick check:* What does `yield env.timeout(duration)` do in a SimPy generator?

- **Concept:** LLM behavior: temperature, hallucination, context windows, token limits
  - *Why needed:* The paper documents context-window exhaustion and output truncation
  - *Quick check:* If an LLM generates plausible-looking code that imports a non-existent library, what failure mode is this?

## Architecture Onboarding

- **Component map:**
  ```
  Academic paper (natural language) 
      ↓ [human reads, writes STRESS-style design]
  Engineered prompt (4-section structure)
      ↓ [Perplexity.AI with RAG]
  Generated Python/SimPy code (~200-600 lines)
      ↓ [nbdime diff + pytest automated tests + visual inspection]
  Verified model module
      ↓ [Streamlit wrapper generation]
  Browser-based UI for non-programmers
  ```

- **Critical path:**
  1. Start with initial prompt generating arrivals only (verify syntax and imports)
  2. Iterate through aims 2-7 (queuing, parameters, routing, results collection)
  3. Add warm-up and multiple replications (aims 7-8) — this is where bugs often surface
  4. Implement common random number streams (aim 10) — requires careful seed management
  5. Generate Streamlit UI last (aim 11)

- **Design tradeoffs:**
  - Prompt reuse vs. specificity: Generic prompts work for UI generation; complex logic requires one-shot examples
  - Stage 1 vs. Stage 2 code: Stage 2 often produces more readable code (named parameters) but requires more lines
  - Free-tier Perplexity vs. paid alternatives: Free tier exhibited variable performance and context limits

- **Failure signatures:**
  - "Lazy generation": output truncated with `# remaining code here` comments
  - Context overflow: code forgotten mid-generation, indentation errors at output end
  - Logic deletion: critical routing lines removed between iterations (caught by nbdime diff)
  - Parameter type mismatches across model sections

- **First 3 experiments:**
  1. Run the initial arrival-only prompt and verify output compiles; check import statements against your Python environment
  2. Add a single activity with deterministic duration; confirm entity flow through the model using trace output
  3. Introduce a second modeller to re-run your prompt sequence; compare their generated code structure and outputs to quantify reproducibility variance

## Open Questions the Paper Calls Out

- **Open Question 1:** Can generative AI successfully recreate healthcare DES models in R using RSimmer with similar accuracy to the SimPy results observed in this study?
- **Open Question 2:** Do larger or reasoning-capable models (e.g., GPT-4.5, Claude 3.7, DeepSeek-R1) reduce the prompt engineering burden and hallucination rate compared to Perplexity.AI's free tier?
- **Open Question 3:** What minimum level of parameter reporting is required in published DES studies for generative AI to achieve full numerical replication of original results?
- **Open Question 4:** How does "model collapse" from training on AI-generated simulation code affect long-term LLM performance for DES model generation?

## Limitations

- Non-deterministic AI outputs create fundamental reproducibility challenges
- Missing or ambiguous parameter specifications in source papers prevent exact replication
- Free-tier AI tools with default settings introduce additional variability
- Substantial human expertise required for code verification limits accessibility

## Confidence

- **High confidence:** Feasibility of using generative AI to create functional DES models from textual descriptions
- **Medium confidence:** Internal consistency between independently generated code versions
- **Low confidence:** Exact numerical replication of original study results when source papers lack complete parameter specifications

## Next Checks

1. Conduct multi-run experiments with identical prompts to quantify output variability and establish reproducibility bounds
2. Test the methodology with source papers containing complete parameter specifications to isolate the impact of missing information
3. Evaluate whether domain experts without programming expertise can successfully use the generated Streamlit interfaces for meaningful analysis