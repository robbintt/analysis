---
ver: rpa2
title: 'Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for
  LLM Reasoning'
arxiv_id: '2506.08745'
source_url: https://arxiv.org/abs/2506.08745
tags:
- reasoning
- reward
- arxiv
- rcovo
- states
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CoVo, a self-rewarding reinforcement learning
  framework for enhancing LLM reasoning without external supervision. The core idea
  is to leverage the consistency and volatility patterns in intermediate reasoning
  states to construct intrinsic rewards, supplemented by a curiosity bonus for diverse
  exploration.
---

# Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning

## Quick Facts
- arXiv ID: 2506.08745
- Source URL: https://arxiv.org/abs/2506.08745
- Reference count: 40
- This paper proposes CoVo, a self-rewarding RL framework that achieves performance comparable to or surpassing supervised RL methods on diverse reasoning benchmarks without external supervision.

## Executive Summary
This paper introduces CoVo, a self-rewarding reinforcement learning framework that enhances large language model reasoning without external supervision. The method leverages the geometric patterns of intermediate reasoning states - specifically their consistency toward their own final answers and volatility toward others - to construct intrinsic rewards. By combining this with a curiosity bonus for diverse exploration, CoVo achieves state-of-the-art performance on multiple reasoning benchmarks while promoting reasoning diversity and training stability.

## Method Summary
CoVo employs Reinforce++ to optimize LLMs using self-generated rewards. The framework samples multiple reasoning trajectories per prompt, computes a distance matrix using the model's likelihood between intermediate states and final answers, and calculates consistency and volatility features for each trajectory. These are aggregated via robust vector-space summation to form the intrinsic reward, complemented by a curiosity bonus based on KL divergence and transition surprise. The method is trained on mathematical reasoning data and evaluated across diverse reasoning benchmarks.

## Key Results
- CoVo achieves performance comparable to or surpassing supervised RL methods on MATH-500, GSM8K, and Olympiad Bench
- The method promotes reasoning diversity while maintaining training stability
- Performance gains are demonstrated across multiple base models including Llama3.2-3B-Instruct and Qwen2.5-3B-Instruct

## Why This Works (Mechanism)

### Mechanism 1: Intermediate State Consistency & Volatility
The framework computes a distance matrix between intermediate reasoning states and sampled final answers, identifying that correct reasoning trajectories exhibit high consistency (states closest to their own final answer) and low volatility (minimal deviation toward incorrect answers). This geometric pattern forms the basis of the intrinsic reward, grounded in the assumption that correct reasoning converges stably while incorrect reasoning wanders erratically in latent space.

### Mechanism 2: Vector-Space Aggregation for Robustness
Instead of linear averaging of consistency and volatility scores, CoVo maps them to a vector v = Con · [cos(Vol), sin(Vol)] and aggregates via vector summation. This geometric formulation dampens the impact of outlier trajectories with extreme volatility that would otherwise dominate simple averaging, providing greater robustness to noisy samples.

### Mechanism 3: Curiosity-Driven Exploration
An auxiliary curiosity reward prevents mode collapse by incentivizing transitions to states with lower token probability (high surprise) relative to a uniform distribution baseline. This exploration bonus, balanced by a KL penalty term, ensures the model doesn't collapse into repetitive solutions while avoiding excessive noise generation.

## Foundational Learning

- **Concept: Negative Log-Likelihood (NLL) as Distance**
  - Why needed: The core self-rewarding mechanism relies on measuring distances between intermediate states and final answers using the model's own likelihood.
  - Quick check: How does Eq. (1) define the distance d(s_i, y), and what does a low distance value indicate about the state's relation to the answer?

- **Concept: Reinforce++ (Policy Gradient)**
  - Why needed: This optimization method updates LLM weights based on the self-generated rewards.
  - Quick check: In Eq. (3), how does the clipping coefficient ε prevent destabilizing updates during training?

- **Concept: Variational Inference (ELBO)**
  - Why needed: The framework grounds self-rewarding in a theoretical lower bound, framing reasoning as latent variable inference.
  - Quick check: According to Proposition 2, what two terms balance the "Reward Assignment" and the "KL Regularization"?

## Architecture Onboarding

- **Component map:** Rollout Engine → Distance Calculator → Reward Module → Optimizer
- **Critical path:** Sampling → Grouping by Answer → Distance Matrix Calculation → Vector Aggregation (Reward) → Policy Update
- **Design tradeoffs:** Vector aggregation (r^V_int) is robust to outliers but adds computational complexity; Linear aggregation (r^L_int) is faster but prone to noise. Higher sample count (N=16) provides better statistics but increases inference cost.
- **Failure signatures:** Reward hacking (accuracy drops despite increasing reward), mode collapse (diversity metrics crash), slow convergence (KL penalty too strong).
- **First 3 experiments:** 1) Verify hypothesis by plotting normalized distance curves for correct vs. incorrect trajectories; 2) Compare vector vs. linear aggregation on validation set; 3) Train with/without curiosity bonus and plot diversity metrics.

## Open Questions the Paper Calls Out
1. How can the consistency and volatility mechanisms be effectively adapted for Visual Language Models (VLMs) where reasoning involves multimodal perceptual understanding?
2. How does CoVo perform in semi-supervised scenarios where a limited amount of external labeled data is available?
3. What is the sensitivity of the intrinsic reward to the number of sampled trajectories (N), and can computational overhead be reduced without sacrificing reward reliability?

## Limitations
- The core self-rewarding signal depends on NLL-based distances that may be unreliable if the base model has systematic likelihood estimation biases
- Critical implementation details like trajectory parsing methodology are underspecified, potentially affecting reproducibility
- Performance gains may not transfer well from math to commonsense reasoning domains due to domain-specific pattern overfitting

## Confidence
- **High confidence** in theoretical framework (Variational Inference formulation and Proposition 2)
- **Medium confidence** in core hypothesis about intermediate state geometry distinguishing correct from incorrect reasoning
- **Medium confidence** in vector-space aggregation providing meaningful robustness gains
- **Low confidence** in exact implementation details required for faithful reproduction

## Next Checks
1. Test whether consistency/volatility distance patterns generalize across different base models (Llama-3.2-3B-Instruct, Qwen2.5-3B-Instruct, DeepSeek-3B) using identical reasoning trajectories
2. Compare the proposed NLL-based distance metric against simpler alternatives (final answer likelihood only, token-level cross-entropy) to isolate the contribution of intermediate state geometry
3. Train CoVo on math reasoning data and evaluate on commonsense reasoning tasks while measuring consistency/volatility statistics to assess domain transferability