---
ver: rpa2
title: Improving Recursive Transformers with Mixture of LoRAs
arxiv_id: '2512.12880'
source_url: https://arxiv.org/abs/2512.12880
tags:
- arxiv
- recursive
- experts
- shared
- parameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose Mixture of LoRAs (MoL), a conditional-computation
  mechanism that integrates low-rank adaptation (LoRA) experts into a shared feed-forward
  network of a recursive transformer. This design enables token-conditional weight-space
  modulation within the shared architecture, addressing the expressivity collapse
  caused by aggressive parameter sharing in models like ALBERT.
---

# Improving Recursive Transformers with Mixture of LoRAs

## Quick Facts
- arXiv ID: 2512.12880
- Source URL: https://arxiv.org/abs/2512.12880
- Reference count: 13
- Primary result: ModernALBERT (50M-120M params) achieves SoTA on GLUE/SQuAD-v2/BEIR among compact recursive transformers

## Executive Summary
This paper addresses the expressivity collapse in recursive transformers caused by aggressive parameter sharing by introducing Mixture of LoRAs (MoL). MoL integrates LoRA experts into the shared feed-forward network through token-conditional routing, enabling dynamic weight-space modulation without untying backbone parameters. The authors validate their approach by building ModernALBERT, a compact recursive transformer incorporating rotary embeddings, GeGLU, and FlashAttention, achieving state-of-the-art performance among compact models. Additionally, they introduce an expert-merging procedure that compresses MoL into a single adapter at inference, preserving accuracy while reducing latency and memory use.

## Method Summary
The authors propose Mixture of LoRAs (MoL), a conditional-computation mechanism that integrates low-rank adaptation (LoRA) experts into a shared feed-forward network of a recursive transformer. A router network assigns probabilities to E LoRA experts per token, with each expert providing low-rank updates to both down-projection and up-projection weights in the FFN. The output is a weighted combination of expert-modified FFN outputs using top-k sparsity. To validate the approach, they develop ModernALBERT, a compact recursive transformer incorporating rotary embeddings, GeGLU, and FlashAttention, and pretrain it on 30B tokens with knowledge distillation. The authors also introduce an expert-merging procedure that compresses MoL into a single adapter at inference, preserving accuracy while reducing latency and memory use.

## Key Results
- ModernALBERT (50M-120M parameters) achieves state-of-the-art performance among compact models on GLUE, SQuAD-v2, and BEIR
- MoL (8 experts, top-2) achieves GLUE average 77.24 vs MoA (post-FFN adapters) at 76.87
- EMA expert merging achieves RTE 86.28 vs Vanilla 84.83 vs No Merging 85.6; SST-2 95.1 vs 94.8 vs 95.2

## Why This Works (Mechanism)

### Mechanism 1: Token-Conditional Weight-Space Modulation
Integrating LoRA experts directly into shared FFN weights enables input-dependent transformations without untying backbone parameters. A router network assigns probabilities to E LoRA experts per token, with each expert providing low-rank updates to both down-projection and up-projection weights in the FFN. The output is a weighted combination of expert-modified FFN outputs: MoL(h) = Σ pᵢ(h) · FFN(h; θ, ψᵢ), using top-k sparsity.

### Mechanism 2: Expressivity Restoration via Expert Specialization
Conditional computation counteracts the representational collapse caused by aggressive cross-layer parameter sharing in recursive transformers. MoL introduces E expert pathways that the router learns to activate based on input characteristics (syntactic, semantic, domain patterns), effectively creating E × G distinct functional behaviors from K × G shared backbone positions.

### Mechanism 3: Training-Inference Decompression via Expert Merging
Dynamic expert knowledge can be compressed into a single static adapter at inference while preserving accuracy. Pre-trained experts {Δ₁, ..., Δₑ} are aggregated into Δₘₑᵣₘₑd = Σⱼ wⱼΔⱼ using either uniform initialization or dynamic EMA merging where routing statistics from fine-tuning inform w updates.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: MoL uses LoRA as its expert representation. Understanding W' = W + (α/r)AB is essential for grasping how experts modulate FFN weights.
  - Quick check question: Given hidden dim 1024 and rank 8, how many parameters does one LoRA expert add to a d×4d FFN weight matrix?

- **Concept: Mixture-of-Experts Routing**
  - Why needed here: The router determines which experts activate per token. Understanding sparse top-k routing and load balancing is critical.
  - Quick check question: With 8 experts and top-2 routing, what is the maximum number of FFN evaluations per token?

- **Concept: Parameter Sharing in Transformers**
  - Why needed here: MoL is designed specifically to address expressivity collapse in recursive architectures like ALBERT.
  - Quick check question: In a 12-layer transformer with group size 4, how many unique FFN parameter sets exist?

## Architecture Onboarding

- **Component map:**
  Transformer Layer → [Pre-LN → Attention → Residual] → [Pre-LN → MoL Layer → Residual]
  
  MoL Layer:
  Input h → Router(h) → Top-k expert indices + probabilities
             ↓
  Shared FFN weights: W_down, W_up (frozen during routing, shared across layers)
  Expert pool: {ψ₁...ψₑ} where ψᵢ = {Aᵢ,₁, Bᵢ,₁, Aᵢ,₂, Bᵢ,₂}
             ↓
  For each selected expert i: FFN(h; θ, ψᵢ) with weight modifications
             ↓
  Weighted sum → Output

- **Critical path:**
  1. Implement LoRA-augmented FFN (Equation 10) with configurable expert count
  2. Add router network (linear layer + softmax) producing expert probabilities
  3. Implement top-k sparse selection with renormalization
  4. Add distillation loss from teacher model during pretraining
  5. Implement EMA merging for inference compression

- **Design tradeoffs:**
  - Expert count vs parameter budget: 8 experts with r=8 adds ~0.5-2M params depending on hidden dim
  - Routing frequency vs latency: MoL at every layer vs only at group boundaries
  - Merge strategy vs task sensitivity: EMA preserves more accuracy but requires fine-tuning statistics

- **Failure signatures:**
  - Router collapse (all tokens to 1-2 experts): Check expert activation histograms
  - Training instability: Router gradients dominate FFN gradients—apply router LR scaling or pretraining
  - Merged model degradation >2%: EMA weights may not capture task-specific routing; fine-tune merged adapter longer

- **First 3 experiments:**
  1. **Ablate expert count:** Train MoL with {1, 4, 8} experts on masked language modeling (30k steps, Wikipedia), evaluate GLUE. Expect diminishing returns beyond 8.
  2. **Compare placement:** MoL at all FFN positions vs only at group boundaries. Measure accuracy/latency tradeoff.
  3. **Validate merging:** Fine-tune MoL model on MNLI, apply uniform vs EMA merging, evaluate accuracy drop and latency improvement. Expect <1% accuracy loss with EMA, ~2x latency reduction.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Mixture of LoRAs (MoL) framework be effectively extended to multimodal settings to handle conditional representations across diverse input types?
- **Open Question 2:** Does the MoL mechanism retain its efficiency and performance advantages when scaled to large autoregressive language models?
- **Open Question 3:** Does the MoL architecture underperform on tasks requiring extended context retention due to the lack of locality-aware attention?
- **Open Question 4:** Is the reported performance of ModernALBERT primarily attributable to the MoL architecture or the distillation-based initialization from a fully-parameterised teacher?

## Limitations

- Limited empirical validation scope: The approach is only validated on text-based encoder models (ModernALBERT) and compact scales (50M-120M parameters)
- Distillation dependency: Heavy reliance on knowledge distillation from ModernBERT raises questions about MoL's standalone architectural benefits
- Expert merging validation: The merging procedure is validated only on three downstream tasks with limited comparison to alternative compression methods

## Confidence

- **High Confidence**: ModernALBERT's baseline performance (achieving SoTA among compact recursive transformers on GLUE/SQuAD/BEIR) is well-supported by quantitative results across multiple benchmarks.
- **Medium Confidence**: MoL's mechanism of restoring expressivity through token-conditional LoRA experts is theoretically sound and shows modest improvements over external adapters, but the router's learning dynamics and expert specialization patterns remain incompletely characterized.
- **Low Confidence**: The expert merging procedure's claimed ~99% accuracy retention is based on limited task diversity, and the underlying assumption that pre-trained expert combinations generalize to arbitrary downstream distributions lacks robust validation.

## Next Checks

1. **Router Specialization Analysis**: Train MoL with 8 experts and log per-token expert activation patterns across GLUE tasks. Analyze whether routing correlates with input characteristics (syntactic complexity, domain, length) and whether collapsed routing (concentration on 1-2 experts) occurs on any task.

2. **Distillation Ablation**: Train ModernALBERT variants with and without knowledge distillation from ModernBERT, measuring GLUE performance delta. Additionally, train MoL on randomly initialized weights without ModernBERT initialization to isolate architectural benefits from distillation effects.

3. **Scaling and Transfer**: Evaluate MoL on a larger recursive transformer (e.g., 200M-300M parameters) and on non-GLUE tasks including long-document classification and code generation. Measure whether expert specialization patterns and merging effectiveness transfer across domains and model scales.