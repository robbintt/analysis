---
ver: rpa2
title: Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using
  Large Language Models
arxiv_id: '2501.08974'
source_url: https://arxiv.org/abs/2501.08974
tags:
- sentiment
- absa
- aspects
- aspect
- customer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for cross-domain aspect-based sentiment
  analysis (ABSA) using Large Language Models (LLMs) combined with traditional models
  like BERT. The key idea is to use LLMs with external knowledge sources to bridge
  domain gaps, allowing BERT to be trained once and applied across domains without
  costly fine-tuning.
---

# Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models

## Quick Facts
- arXiv ID: 2501.08974
- Source URL: https://arxiv.org/abs/2501.08974
- Reference count: 12
- Primary result: Achieves 92% accuracy on cross-domain ABSA by combining LLMs with external knowledge sources and a 12-layer BERT classifier.

## Executive Summary
This paper proposes a framework for cross-domain aspect-based sentiment analysis (ABSA) using Large Language Models (LLMs) combined with traditional models like BERT. The key idea is to use LLMs with external knowledge sources to bridge domain gaps, allowing BERT to be trained once and applied across domains without costly fine-tuning. Experiments on the SemEval-2015 Task 12 dataset (Laptop and Restaurant domains) show that the proposed method achieves 92% accuracy, significantly outperforming baseline methods and demonstrating strong cross-domain adaptability. The approach reduces the need for domain-specific training and expert intervention, making it more business-friendly and scalable.

## Method Summary
The framework uses an LLM (e.g., LLaMA) with external knowledge sources to extract and canonicalize domain-specific aspects from raw text, then feeds these to a 12-layer BERT classifier trained on aspect-sentiment pairs. The LLM bridges terminology gaps between domains, enabling BERT to generalize without retraining. The method is evaluated on SemEval-2015 Task 12, showing strong cross-domain performance.

## Key Results
- Achieves 92% accuracy on cross-domain ABSA using LLM + external knowledge + 12-layer BERT.
- Outperforms baselines significantly, with Laptop domain accuracy of 91.1% and Restaurant domain accuracy of 90.6%.
- Eliminates need for domain-specific fine-tuning, reducing training costs and expert intervention.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs with external knowledge sources bridge domain terminology gaps, enabling BERT to generalize across domains without retraining.
- **Mechanism:** The LLM preprocesses input by interpreting domain-specific aspects (e.g., "battery life" for laptops, "ambiance" for restaurants) using external knowledge, normalizing these into a shared semantic representation that BERT can process uniformly.
- **Core assumption:** The LLM correctly maps domain-specific terminology to a canonical aspect space that BERT learned during initial training.
- **Evidence anchors:**
  - [abstract] "use LLMs with external knowledge sources to bridge domain gaps, allowing BERT to be trained once and applied across domains"
  - [section 4] "LLMs can intermediate and help in bridging the gap in the terminology and understanding of what is meant to be domain knowledge"
  - [corpus] Neighbor papers (e.g., "Domain-Adaptive Pre-Training for Arabic ABSA") explore similar domain adaptation strategies, suggesting this is an active research direction, though comparative validation is limited.
- **Break condition:** If the target domain introduces aspects with no semantic overlap to source domain aspects (e.g., medical vs. culinary domains), the LLM's bridging capacity may degrade.

### Mechanism 2
- **Claim:** Separating aspect extraction (via LLM) from sentiment classification (via BERT) reduces joint error propagation and improves modularity.
- **Mechanism:** The pipeline first uses LLM to identify and extract aspects, then feeds these to a BERT classifier trained on aspect-sentiment pairs. This decoupling allows each component to specialize without interference.
- **Core assumption:** Aspect extraction errors do not compound significantly during sentiment classification.
- **Evidence anchors:**
  - [section 4] "we have first dealt with aspect detection, and in the last step, we have done sentiment classification"
  - [section 5] "the performance of the cross-domain aspects improved significantly without the requirement to fine-tune the BERT models"
  - [corpus] No direct corpus evidence for this specific decoupling mechanism; related work typically uses end-to-end approaches.
- **Break condition:** If aspect extraction produces false positives or misses implicit aspects, sentiment classification will operate on incorrect or incomplete inputs.

### Mechanism 3
- **Claim:** Training on mixed-domain data (Laptop + Restaurant) provides a middle-ground generalization without requiring per-domain fine-tuning.
- **Mechanism:** Joint exposure to multiple domains during training teaches BERT to recognize cross-domain aspect patterns and sentiment expressions, creating a more robust internal representation.
- **Core assumption:** The training domains are sufficiently representative of target domain distributions.
- **Evidence anchors:**
  - [section 5, Table 1] "LLMs for Aspect and 12 Layer BERT: For Laptop (BERT Trained with Laptop and Restaurant) 91.1%, For Restaurant (BERT Trained with Laptop and Restaurant) 90.6%"
  - [section 5] "Testing the model across different domains...could have been fruitful to evaluate its generalizability and will surely be a future work"
  - [corpus] Neighbor paper "M-ABSA: A Multilingual Dataset" suggests cross-domain/multilingual generalization is non-trivial and dataset-dependent.
- **Break condition:** Performance may degrade on domains with fundamentally different review structures (e.g., short social media posts vs. long-form product reviews).

## Foundational Learning

- **Concept: Aspect-Based Sentiment Analysis (ABSA)**
  - **Why needed here:** This is the core task—extracting sentiment toward specific product aspects rather than overall sentiment.
  - **Quick check question:** Given "The screen is great but the battery drains fast," can you identify two aspects and their respective sentiments?

- **Concept: BERT Architecture and [CLS] Token**
  - **Why needed here:** BERT's [CLS] token is used for classification; understanding bidirectional attention explains why it captures context for aspect-sentiment relationships.
  - **Quick check question:** Why would a bidirectional model outperform a unidirectional one for identifying that "it" refers to "battery" in "The battery is poor, but it charges fast"?

- **Concept: Cross-Domain Transfer Without Fine-Tuning**
  - **Why needed here:** The paper's main contribution is avoiding costly fine-tuning; understanding what transfers (syntactic patterns, general sentiment lexicon) vs. what doesn't (domain-specific vocabulary) is critical.
  - **Quick check question:** If a model trained on restaurant reviews sees "The trackpad is unresponsive," what knowledge might transfer and what might fail?

## Architecture Onboarding

- **Component map:** Raw text → LLM + external knowledge (aspect extraction) → BERT (sentiment classification per aspect) → Aggregated aspect-sentiment pairs → Accuracy evaluation

- **Critical path:** Raw text → LLM + external knowledge (aspect extraction) → BERT (sentiment classification per aspect) → Aggregated aspect-sentiment pairs → Accuracy evaluation

- **Design tradeoffs:**
  - 12-layer BERT (not 24-layer) chosen to reduce training time; may sacrifice some accuracy on complex sentences.
  - LLM used for preprocessing rather than end-to-end classification—reduces inference cost but introduces pipeline complexity.
  - Single-domain vs. mixed-domain training: mixed-domain sacrifices ~1% accuracy on individual domains for broader generalization.
  - Assumption: Accuracy metric is sufficient; F1 reported as similar due to balanced dataset, but this may not hold for imbalanced real-world data.

- **Failure signatures:**
  - **Low cross-domain accuracy with high same-domain accuracy:** Indicates LLM knowledge bridging is insufficient for target domain terminology.
  - **High false positives in aspect extraction:** LLM may over-generate aspects not present in ground truth annotation schema.
  - **Inconsistent results across runs:** Check for masking/augmentation randomness (Section 4 mentions bias mitigation via masking); ensure deterministic evaluation.
  - **Performance cliff on new domains:** External knowledge source may lack coverage; validate knowledge graph completeness before deployment.

- **First 3 experiments:**
  1. **Baseline reproduction:** Train BERT (12-layer) on Laptop domain only; evaluate on both Laptop and Restaurant test sets to establish single-domain and cross-domain baselines. Compare to Table 1 values (82.3% / ~81.5%).
  2. **Ablation on LLM component:** Run the full pipeline with LLM aspect extraction vs. a simple rule-based aspect extractor (e.g., noun phrase extraction); measure impact on cross-domain accuracy to isolate LLM contribution.
  3. **Domain pair expansion:** If additional domain data is available (or subset SemEval differently), train on Domain A, test on Domain B, then reverse. Document asymmetry (e.g., Restaurant→Laptop vs. Laptop→Restaurant performance difference as seen in Table 1: 88.9% vs. 90.4%).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively does the proposed framework generalize to domains significantly different from the tested subjects, such as fashion or food?
- **Basis in paper:** [explicit] The authors state in Section 5 that "Testing the model across different domains (e.g., electronics, fashion, food) could have been fruitful to evaluate its generalizability and will surely be a future work."
- **Why unresolved:** The current study is restricted to the SemEval-2015 Task 12 dataset, which contains only Laptop and Restaurant domains.
- **What evidence would resolve it:** Reporting accuracy and F1-scores when the model is trained on the current domains and tested on entirely new domains like fashion reviews.

### Open Question 2
- **Question:** Can the proposed LLM-enhanced architecture maintain its high accuracy levels while operating in real-time environments with large data volumes?
- **Basis in paper:** [explicit] Section 5 notes that "Evaluation of the model’s performance in real-time processing, especially when handling large volumes of text data can be critical."
- **Why unresolved:** The paper focuses on offline accuracy metrics and does not provide benchmarks regarding the computational cost, latency, or throughput of combining LLMs with BERT.
- **What evidence would resolve it:** Benchmarking experiments that measure inference time and resource consumption during real-time streaming analysis.

### Open Question 3
- **Question:** What is the marginal contribution of the Large Language Model's external knowledge versus the BERT model's pre-trained capabilities in bridging domain gaps?
- **Basis in paper:** [inferred] The paper attributes the cross-domain success to LLMs bridging "terminology and understanding," but does not isolate this variable through an ablation study.
- **Why unresolved:** It is unclear if the high accuracy (approx. 90%) is driven by the LLM's domain intervention or the inherent robustness of the 12-layer BERT model.
- **What evidence would resolve it:** Ablation studies comparing the framework's performance with and without the LLM-generated external knowledge.

## Limitations
- The integration mechanism between LLM and external knowledge sources is underspecified, making exact replication challenging.
- The method requires curated external knowledge sources, which may not be readily available for all domains.
- Performance on truly novel domains remains untested, limiting claims about true cross-domain generalization.

## Confidence
- **High Confidence:** The reported 92% accuracy figure and its superiority over baselines are well-supported by the experimental setup and clear methodology for BERT training and evaluation.
- **Medium Confidence:** The claim that LLMs with external knowledge bridge domain gaps is plausible given the results, but the mechanism remains underspecified, limiting our ability to verify whether this is the LLM's knowledge or the external source driving improvements.
- **Low Confidence:** The assertion that this approach eliminates the need for domain-specific training entirely is overstated—the method still requires external knowledge sources that must be domain-curated, and performance on truly novel domains remains untested.

## Next Checks
1. **Knowledge Source Validation:** Test the pipeline with three different external knowledge configurations: (a) comprehensive domain ontology, (b) minimal keyword list, and (c) no external knowledge. Measure cross-domain accuracy degradation to isolate the knowledge source's contribution versus the LLM's inherent capability.
2. **Domain Generalization Stress Test:** Apply the trained model to a genuinely different domain (e.g., hotel reviews or movie reviews) to determine whether the "domain-agnostic" claim holds beyond the Restaurant-Laptop pair. Document performance drop as a measure of true generalization capability.
3. **Ablation on BERT Layers:** Compare the 12-layer BERT performance against 24-layer and 6-layer variants under identical training conditions to quantify whether the layer reduction was optimal or merely convenient for computational constraints.