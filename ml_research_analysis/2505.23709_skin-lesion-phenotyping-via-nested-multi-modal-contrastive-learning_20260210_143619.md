---
ver: rpa2
title: Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning
arxiv_id: '2505.23709'
source_url: https://arxiv.org/abs/2505.23709
tags:
- metadata
- lesion
- slimp
- dataset
- skin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SLIMP, a nested multi-modal contrastive learning
  approach for skin lesion phenotyping that leverages both lesion-level and patient-level
  metadata alongside image data. SLIMP employs a two-tier contrastive loss architecture
  to align image features with metadata embeddings at both levels, capturing richer
  representations than image-only pretraining methods.
---

# Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning

## Quick Facts
- arXiv ID: 2505.23709
- Source URL: https://arxiv.org/abs/2505.23709
- Reference count: 40
- Primary result: Nested multi-modal contrastive learning achieves 92.8% balanced accuracy on skin lesion classification

## Executive Summary
This paper introduces SLIMP, a nested multi-modal contrastive learning approach for skin lesion phenotyping that leverages both lesion-level and patient-level metadata alongside image data. SLIMP employs a two-tier contrastive loss architecture to align image features with metadata embeddings at both levels, capturing richer representations than image-only pretraining methods. Experimental results on five skin lesion datasets show SLIMP achieves superior classification performance, outperforming strong baselines including MAE, CLIP, and domain-specific models, with up to 92.8% balanced accuracy. The method also enables effective metadata extrapolation for image-only datasets via retrieval-based pseudo-modality generation, and demonstrates robust low-shot learning capabilities, maintaining high accuracy with minimal labeled data.

## Method Summary
SLIMP uses a nested contrastive learning architecture with ViT-Small image encoder and TRACE tabular encoder for structured metadata. The method employs two InfoNCE loss functions: an inner loss aligning lesion images with lesion metadata, and an outer loss aligning patient-level aggregated features with patient metadata. Pre-training occurs in two stages: first on reference dataset SLICE-3D (150 epochs), then continual pre-training on target datasets (100 epochs) with only embedding layers fine-tuned. Linear probing evaluates downstream classification performance. For image-only datasets, retrieval-based pseudo-metadata generation (I→L→P) provides missing metadata by finding similar lesions in reference datasets.

## Key Results
- Achieves up to 92.8% balanced accuracy across five skin lesion datasets
- Outperforms strong baselines including MAE, CLIP, and domain-specific models
- Nested architecture improves performance by 3.5% to 6.8% compared to flat contrastive learning
- Retrieval-based pseudo-metadata generation (I→L→P) outperforms direct image-to-image retrieval by 4.0% to 4.3% balanced accuracy

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Nested two-level contrastive learning captures hierarchical patient–lesion relationships that single-level approaches miss.
**Mechanism:** An inner InfoNCE loss aligns each lesion's image embedding with its lesion-level metadata embedding. Aggregated patient representations (average-pooled concatenated features) are then contrasted with patient-level metadata via an outer InfoNCE loss. The total loss is λ·L_lesions + (1−λ)·L_patient.
**Core assumption:** Clinical data follows a meaningful hierarchy: patients have multiple lesions, and patient-level metadata (e.g., age, family history) is shared across lesions while lesion-level metadata (e.g., location, size) varies per lesion.
**Evidence anchors:** [abstract]: "SLIMP employs a two-tier contrastive loss architecture to align image features with metadata embeddings at both levels." [section]: Table 3 shows nested architecture outperforms flat (single-level) by +3.5% to +6.8% accuracy when training on same datasets from scratch.

### Mechanism 2
**Claim:** Structured tabular metadata encoders specialized for clinical data outperform generic text-based vision-language approaches.
**Mechanism:** SLIMP uses TRACE (Transformer for Risk Assessment in Clinical Evaluation) to encode structured tabular metadata, then concatenates image and metadata features. This preserves categorical/numerical structure rather than converting to free text.
**Core assumption:** Clinical metadata has inherent structure (discrete categories, numerical ranges) that generic text tokenization discards.
**Evidence anchors:** [section]: Section E.2 shows TRACE outperforms FT-Transformer across all datasets (e.g., +6.4% balanced accuracy on HAM10000) despite having 4× fewer parameters.

### Mechanism 3
**Claim:** A well-structured embedding space enables retrieval-based pseudo-metadata generation for image-only datasets.
**Mechanism:** For datasets lacking metadata, SLIMP retrieves the most similar lesion-level metadata embedding from a reference dataset via cosine similarity, then uses this to retrieve patient-level metadata. This two-stage I→L→P retrieval outperforms direct image-to-image retrieval.
**Core assumption:** The pre-trained embedding space has clear separation between matching and non-matching image-metadata pairs.
**Evidence anchors:** [section]: Table 4 shows matching pairs have median cosine similarity 0.836 vs. −0.006 for non-matching; distributions are nearly non-overlapping.

## Foundational Learning

- **Concept: InfoNCE Contrastive Loss**
  - Why needed here: Core objective function for both nested levels; understanding positive/negative pair construction is essential.
  - Quick check question: Given a batch of B patients each with N lesions, how many positive and negative pairs exist at the lesion level?

- **Concept: Vision Transformer (ViT) Patch Embeddings**
  - Why needed here: ViT-Small serves as image encoder; continual pre-training only updates patch embedding layer.
  - Quick check question: Why would freezing attention layers while fine-tuning patch embeddings mitigate catastrophic forgetting?

- **Concept: Hierarchical Data in Clinical Settings**
  - Why needed here: Patient→lesion hierarchy is the structural assumption underlying nested contrastive learning.
  - Quick check question: If a dataset only provides lesion-level metadata without patient identifiers, which SLIMP variant should be used?

## Architecture Onboarding

- **Component map:** Image encoder (ViT-Small) → Lesion-level tabular encoder (TRACE) → Patient-level tabular encoder (TRACE) → Inner InfoNCE loss (lesion level) → Outer InfoNCE loss (patient level) → Total loss (λ·L_lesions + (1−λ)·L_patient)

- **Critical path:**
  1. Pre-train on reference dataset (SLICE-3D): all encoder parameters optimized
  2. Continual pre-train on target dataset: only embedding layers fine-tuned, attention frozen
  3. Linear probing: train classifier only, encoders frozen

- **Design tradeoffs:**
  - Encoder size: ViT-Small balances performance vs. compute; ViT-Base underperforms possibly due to reduced N (50 vs. 100 lesions per patient)
  - Batch size: B=64 patients optimal; smaller batches underperform, larger show diminishing returns (Table 15)
  - Metadata encoder: TRACE outperforms FT-Transformer despite fewer parameters; FT-Transformer requires extensive hyperparameter tuning

- **Failure signatures:**
  - Catastrophic forgetting: Full fine-tuning on target datasets drops performance (Table 14 shows −3.9% to −9.8% balanced accuracy vs. embedding-only)
  - Attention map degradation: MAE/CLIP baselines show diffuse attention; SLIMP shows localized lesion attention (Figures 8–9)
  - Retrieval failure: Low cosine similarity distribution shift indicates domain gap (Figure 3)

- **First 3 experiments:**
  1. **Ablate nested vs. flat:** Train both variants on PAD-UFES-20 from scratch; expect ~3–7% accuracy gap per Table 3.
  2. **Encoder comparison:** Swap TRACE → FT-Transformer; expect performance drop and increased GFLOPS per Tables 10–11.
  3. **Retrieval validation:** Apply Ret-SLIMP to HAM10000 (image-only for patient metadata); compare I→L→P vs. I→I retrieval per Table 17.

## Open Questions the Paper Calls Out
- **Question:** Can the nested contrastive learning architecture be effectively adapted to domains that lack the strict patient-lesion hierarchy, or to datasets where such structural relationships are ambiguous?
  - Basis in paper: [explicit] The authors state in the "Conclusions and limitations" that the "nested pre-training strategy requires a data structure that incorporates both patient- and lesion-level metadata, which may limit its adaptability to other domains where such structured scenarios do not straight-forwardly exist."
  - Why unresolved: The current formulation hardcodes a two-tier hierarchy (inner lesion-level, outer patient-level). It is unclear if the model can dynamically infer or relax these constraints for flat data or different hierarchical graph structures.

- **Question:** How does the integration of aggressive image augmentation strategies interact with the nested multi-modal contrastive loss during pre-training?
  - Basis in paper: [explicit] The authors note that "significant shift in the image domain... can possibly downgrade downstream performance" and suggest this "can be addressed by incorporating image augmentations in the learning process," but they do not implement or test this in the current work.
  - Why unresolved: Standard contrastive learning (e.g., SimCLR) relies heavily on augmentation. It is unknown if strong augmentations would conflict with the metadata alignment objectives or if they are necessary to bridge the domain gap mentioned by the authors.

## Limitations
- The nested contrastive learning architecture requires patient-lesion hierarchical structure, limiting adaptability to domains without such relationships
- TRACE encoder architecture details are unspecified, requiring assumptions for reproduction
- Retrieval-based pseudo-metadata generation may introduce bias when target dataset demographics significantly diverge from reference dataset

## Confidence
- **High confidence:** Nested contrastive learning architecture and two-tier InfoNCE loss design; performance improvements over baselines (92.8% balanced accuracy); catastrophic forgetting mitigation through embedding-only fine-tuning
- **Medium confidence:** TRACE encoder superiority claims (due to unspecified architecture details); retrieval-based pseudo-metadata generation (limited by domain shift); specific λ=0.9 weighting choice
- **Low confidence:** Claims about interpretability improvements (attention map visualizations lack quantitative metrics); generalizability to non-hierarchical datasets (no systematic evaluation provided)

## Next Checks
1. **Ablation study on TRACE architecture:** Replace TRACE with fully-specified FT-Transformer variant using identical hyperparameters to isolate architecture effects vs. hyperparameter tuning differences
2. **Domain adaptation robustness:** Test retrieval-based pseudo-metadata generation when reference (SLICE-3D) and target datasets have different imaging modalities (dermoscopic vs. clinical)
3. **Missing data sensitivity:** Systematically vary missing value rates (0%, 10%, 30%) and imputation strategies to quantify impact on balanced accuracy and retrieval performance