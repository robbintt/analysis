---
ver: rpa2
title: Neural networks leverage nominally quantum and post-quantum representations
arxiv_id: '2507.07432'
source_url: https://arxiv.org/abs/2507.07432
tags:
- quantum
- classical
- belief
- states
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neural networks trained via next-token prediction learn to represent
  belief states over minimal generative models of their training data, spontaneously
  discovering classical, quantum, and post-quantum representations depending on the
  process structure. Through linear probing of activation vectors, researchers found
  that trained networks map their activations to belief states of classical HMMs,
  quantum systems (Bloch sphere), and post-quantum generalized probabilistic theories
  with high fidelity (RMSE values as low as 0.002 for quantum models versus 0.066
  for classical approximations).
---

# Neural networks leverage nominally quantum and post-quantum representations

## Quick Facts
- arXiv ID: 2507.07432
- Source URL: https://arxiv.org/abs/2507.07432
- Reference count: 0
- Key outcome: Neural networks learn quantum/post-quantum belief state representations via next-token prediction, with probing RMSE as low as 0.002 for quantum models

## Executive Summary
Neural networks trained via next-token prediction spontaneously discover representations that encode belief states of minimal generative models, including classical, quantum, and post-quantum structures. Through linear probing of activation vectors, researchers found that trained networks map their activations to belief states with high fidelity—networks preserve complete geometric relationships among belief states, achieving R² values between 0.99 and 1.00 for minimal generators versus 0.36-0.56 for classical approximations. This geometric representation emerges during training and is universal across architectures including transformers, LSTMs, GRUs, and vanilla RNNs. The work demonstrates that neural networks transcend classical computational models by leveraging their continuous activation spaces to implicitly perform Bayesian inference over post-classical world models.

## Method Summary
The study trains neural networks on next-token prediction tasks using four stochastic processes with known minimal generators: Mess3 (classical), Bloch Walk (quantum), FRDN, and Moon (post-quantum). Models include 4-layer transformers and RNN variants, trained with Adam optimizer for 20,000 epochs. Linear probing is performed by extracting activation vectors for all unique contexts, then fitting affine maps via weighted least-squares regression to ground truth belief states. The analysis compares probing RMSE and geometric fidelity (pairwise cosine similarities) between minimal generators and Markov-order-3 classical approximations, testing universality across architectures and emergence during training.

## Key Results
- Trained networks map activations to belief states of minimal generators with RMSE values as low as 0.002 for quantum models versus 0.066 for classical approximations
- Geometric relationships among belief states are preserved with R² values between 0.99 and 1.00 for minimal generators versus 0.36-0.56 for classical approximations
- The quantum/post-quantum representations emerge spontaneously during training and are universal across transformer, LSTM, GRU, and vanilla RNN architectures
- Networks leverage continuous activation spaces to represent non-orthogonal belief states that would require infinite classical memory states

## Why This Works (Mechanism)

### Mechanism 1: Efficient Compression via Continuous State Spaces
- **Claim:** Neural networks spontaneously discover "quantum" or "post-quantum" representations because these offer the most dimensionally efficient encoding of the training data's statistical structure
- **Core assumption:** The gradient descent process implicitly favors the minimal sufficient representation required to minimize prediction error
- **Evidence:** Neural nets easily find these representations whereas there is no finite classical circuit that would do the job; networks consistently discover the most compact representation available

### Mechanism 2: Iterative Bayesian Filtering in Activation Space
- **Claim:** NNs perform iterative Bayesian updates over a latent world model as context grows, not merely memorizing n-gram statistics
- **Core assumption:** The linear relationship observed via probing implies the network utilizes this geometry for computation
- **Evidence:** NNs map inputs to predictive states and update activations as if performing iterative Bayesian updates during inference

### Mechanism 3: Universality of Linear Representations
- **Claim:** Emergence of specific belief geometries is independent of neural architecture and driven purely by data statistics
- **Core assumption:** The geometry is an invariant property of the stochastic process itself, which the NN discovers
- **Evidence:** Universality across fundamentally different neural architectures (Transformer, LSTM, RNN) with consistent geometric fidelity

## Foundational Learning

**Concept: Predictive State / Causal State**
- **Why needed:** The paper argues NNs map inputs to these states; understanding this is key to decoding the "belief geometry"
- **Quick check:** Can two different input sequences result in the same activation vector if they predict the same future distribution? (Answer: Yes, per the paper's theory)

**Concept: Generalized Probabilistic Theories (GPTs)**
- **Why needed:** The paper claims NNs use "post-quantum" representations; GPTs generalize classical probability and quantum theory to broader convex state spaces
- **Quick check:** Why is a "Bloch Sphere" representation considered more efficient than a classical HMM for the "Bloch Walk" process?

**Concept: Linear Probing**
- **Why needed:** This is the primary validation method; it assumes if information can be extracted via simple affine transformation, it is explicitly encoded
- **Quick check:** Does a high R² in linear probing guarantee the network uses this feature causally, or just that the feature correlates with the output?

## Architecture Onboarding

**Component map:** Input tokens -> Generic Sequence Model (Transformer/LSTM/RNN) -> Activation Vector -> Probe (Affine Map) -> Ground Truth Belief Geometry

**Critical path:**
1. Select stochastic process with known minimal generator
2. Train model on next-token prediction until convergence
3. Extract activations for all unique contexts
4. Perform weighted linear regression to find map L
5. Verify geometry preservation via Cosine Similarity

**Design tradeoffs:**
- Float precision vs. Theory: NNs use floating-point numbers as continuous approximation but cannot truly replace quantum computers for large-scale problems
- Layer Selection: Later layers (L3, L4) show better geometry; early layers focus on local statistics

**Failure signatures:**
- High RMSE (>0.05) for Minimal Generator: Network failed to learn process structure or is underparameterized
- Better fit to Classical Approximation than Quantum: Network settled for sub-optimal, non-minimal model or training was insufficient

**First 3 experiments:**
1. Replicate "Bloch Walk": Train small Transformer on Bloch Walk process; verify activations form 2D circular disk when probed
2. Layer-wise Probing: Probe each layer individually to confirm geometric fidelity increases with depth (L1 → L4)
3. Random Baseline: Run probing analysis on untrained network to confirm geometry emerges from training

## Open Questions the Paper Calls Out
- **Do biological neural networks leverage quantum or post-quantum representations for temporal memory compression?** The authors note this tantalizing possibility but didn't investigate it
- **How does architecture-dependent implementation of these representations affect out-of-distribution generalization?** The paper focused on geometric fidelity but notes this likely affects generalization
- **Can these findings scale to frontier AI models trained on complex natural language?** The authors anticipate these techniques are used in frontier models but validated only on low-dimensional synthetic processes

## Limitations
- The study is restricted to small-scale stochastic processes with known minimal generators, limiting claims about general "post-quantum" capabilities
- Probing methodology assumes linear separability implies causal utilization, potentially conflating correlation with functional necessity
- Theoretical framework extends to quantum computing but practical implementation is blocked by exponential scaling of quantum systems

## Confidence
- **High Confidence:** Geometric representation emerges in trained vs untrained networks; universality across architectures is well-supported
- **Medium Confidence:** Claim that NNs "perform Bayesian updates" is plausible given linear mapping evidence but not directly tested
- **Low Confidence:** Broader implication that NNs can learn "non-classical" world models in general settings is overstated given controlled experimental conditions

## Next Checks
1. **Ablation Study on Data Diversity:** Systematically vary training data quantity and sequence diversity to determine minimum requirements for discovering minimal generators versus classical approximations
2. **Probing with Non-Linear Readouts:** Replace linear probing with kernel methods or small neural probes to test whether linear assumption underestimates representational capacity
3. **Transfer to Real-World Processes:** Apply methodology to real-world time series (financial data, natural language with known long-range dependencies) to test whether phenomenon extends beyond synthetic processes