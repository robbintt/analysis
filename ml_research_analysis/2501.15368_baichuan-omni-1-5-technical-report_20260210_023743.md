---
ver: rpa2
title: Baichuan-Omni-1.5 Technical Report
arxiv_id: '2501.15368'
source_url: https://arxiv.org/abs/2501.15368
tags:
- arxiv
- audio
- data
- zhang
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Baichuan-Omni-1.5 is an omni-modal model that achieves seamless,
  high-quality interaction across text, image, audio, and video modalities without
  compromising performance in any modality. The model integrates a comprehensive data
  cleaning and synthesis pipeline yielding about 500B high-quality multimodal data,
  an 8-layer Residual Vector Quantization (RVQ) audio tokenizer designed to capture
  both semantic and acoustic information at 12.5 Hz frame rate, and a multi-stage
  training strategy that progressively integrates multimodal alignment and multitask
  fine-tuning.
---

# Baichuan-Omni-1.5 Technical Report

## Quick Facts
- arXiv ID: 2501.15368
- Source URL: https://arxiv.org/abs/2501.15368
- Reference count: 40
- Leads contemporary models including GPT4o-mini and MiniCPM-o 2.6 on comprehensive omni-modal capabilities

## Executive Summary
Baichuan-Omni-1.5 is an omni-modal model achieving seamless, high-quality interaction across text, image, audio, and video modalities. The model employs a comprehensive data pipeline yielding ~500B high-quality multimodal data, an 8-layer RVQ audio tokenizer operating at 12.5 Hz frame rate, and a multi-stage training strategy. It achieves state-of-the-art performance across ten image-understanding benchmarks (73.3 average score) and 83.8% on OpenMM-Medical, while maintaining strong pure language capabilities (72.2 on MMLU).

## Method Summary
Baichuan-Omni-1.5 uses a 4-stage training curriculum: (1) Image-Text Pretrain with frozen LLM and visual encoder; (2) Image-Audio-Text Pretrain with staged audio integration; (3) Omni-Modal Pretrain with all parameters and extended sequence length; and (4) Supervised Fine-Tuning on 17M instruction pairs. The architecture combines a NaViT visual encoder, Whisper-based RVQ audio tokenizer (12.5 Hz), and LLM decoder with separate audio generation head. Training utilizes ~500B multimodal tokens including interleaved audio-text data, with progressive parameter unfreezing to prevent modality conflict.

## Key Results
- Achieves 73.3 average score across ten image-understanding benchmarks, surpassing GPT-4o-mini by 6 points
- Scores 83.8% on OpenMM-Medical benchmark, outperforming Qwen2-VL-72B's 80.7%
- Maintains strong text performance with 72.2 on MMLU compared to Llama3-Instruct's 67.1%

## Why This Works (Mechanism)

### Mechanism 1
Progressive stage-wise training prevents modality conflict while enabling cross-modal synergy. The 4-stage curriculum anchors visual-language alignment, introduces audio with frozen parameters, gradually unfreezes, and trains jointly on cross-modal data. This prevents catastrophic forgetting by maintaining ~40% pure text data throughout.

### Mechanism 2
Dual-objective audio tokenization enables both understanding and generation. The 8-layer RVQ tokenizer (12.5 Hz) feeds tokens to both the LLM for semantic prediction and an audio decoder for mel-spectrogram reconstruction, capturing semantic-acoustic disentanglement.

### Mechanism 3
Interleaved multimodal data enables robust cross-modal transfer. Training includes audio-text interleave, image-audio-text composites, and video-audio-text combinations where audio replaces text segments, teaching the model to predict one modality conditioned on another.

## Foundational Learning

- **Residual Vector Quantization (RVQ)**
  - Why needed here: The audio tokenizer uses 8-layer RVQ to compress continuous audio into discrete tokens at 12.5 Hz. Without understanding RVQ, you cannot debug audio token quality or adjust codebook sizes.
  - Quick check question: Can you explain why RVQ uses multiple quantization layers rather than a single large codebook?

- **Flow Matching for Audio Generation**
  - Why needed here: The audio decoder uses flow matching (not diffusion) to generate mel spectrograms. Understanding this helps when tuning inference speed vs. quality tradeoffs.
  - Quick check question: How does flow matching differ from denoising diffusion in terms of ODE trajectory parameterization?

- **NaViT Dynamic Resolution Processing**
  - Why needed here: The visual encoder handles arbitrary resolutions via NaViT. This affects memory management and batch construction strategies.
  - Quick check question: How does NaViT pack multiple images with different resolutions into a single batch?

## Architecture Onboarding

- **Component map:**
  Input → [Visual: NaViT + 2-layer MLP] ↘
          [Audio: Whisper Encoder + RVQ + Audio Embed Layer] → LLM Decoder → [Text Head / Audio Head]
                                                                       ↓
                                                          [Audio Decoder: Flow-Matching + HiFi-GAN]

- **Critical path:** Audio embedding layer → LLM hidden states → Audio head (3-layer depth transformer + 8 classification heads). This path determines end-to-end speech latency.

- **Design tradeoffs:**
  - 12.5 Hz frame rate (vs. 50+ Hz in other systems) reduces token sequence length but may lose prosodic detail
  - Separate audio head rather than generating audio tokens directly from LLM logits trades some coherence for faster inference
  - Freezing visual encoder during audio integration (Stage 2) preserves vision performance but limits vision-audio joint optimization

- **Failure signatures:**
  - Text benchmarks drop >5% after Stage 2 → data imbalance (increase pure text ratio)
  - Audio generation metallic/artifacts → flow matching decoder undertrained or vocoder mismatch
  - Cross-modal reasoning fails while unimodal works → interleave data insufficient or aligned poorly

- **First 3 experiments:**
  1. **Ablate Stage 2.1 freezing:** Train audio embed layer + audio head with partially unfrozen LLM (last 4 layers). Compare text benchmark retention vs. baseline freezing.
  2. **Tokenize rate sensitivity:** Test 8Hz, 12.5Hz, 25Hz audio tokenization rates. Measure ASR WER, TTS naturalness (MOS), and sequence length impact on memory.
  3. **Interleave ratio sweep:** Train with 10%, 25%, 50% interleaved audio-text data. Evaluate OmniBench Image & Audio scores vs. pure text performance on MMLU.

## Open Questions the Paper Calls Out

- **Can the model effectively process and generate natural environmental sounds (e.g., flowing water, bird songs) in addition to human speech?**
  - Basis: Conclusion states need to "improve audio understanding and generation to not only recognize human voices but also natural environmental sounds."
  - Unresolved because current audio training data focuses heavily on speech tasks.
  - Evidence needed: Evaluation results on non-speech audio benchmarks and qualitative generation samples.

- **How can the model's architecture be optimized to support longer video frame understanding without linearly increasing computational cost?**
  - Basis: Conclusion identifies need to "support longer video frame understanding" as key limitation. Methodology limits input to maximum 32 frames at 1 fps.
  - Unresolved because video processing as image tokens increases context length and memory usage.
  - Evidence needed: Architectural modifications or compression techniques for >32 frames while maintaining or improving benchmark scores.

- **To what extent does the integration of multimodal alignment tasks degrade the complex text reasoning capabilities of the base LLM?**
  - Basis: Conclusion lists "enhance text understanding capabilities" as priority. Introduction notes omni-modal approaches often "suffer from modality conflicts, which degrade omni-modal performance compared to unimodal performance."
  - Unresolved because degradation effects need direct measurement against text-only initialization.
  - Evidence needed: Direct ablation study comparing final model's text-only performance against its text-only initialization checkpoint on high-difficulty reasoning benchmarks.

## Limitations

- The claim of "500B high-quality multimodal data" lacks detailed specification of data sources, cleaning procedures, and quality filtering criteria
- The 4-stage training methodology relies on unpublished hyperparameter schedules and data mixing ratios that could significantly impact performance
- The evaluation methodology for cross-modal tasks may be subject to leakage or bias

## Confidence

**High Confidence:** The architectural framework combining NaViT visual encoder, RVQ audio tokenizer, and progressive stage-wise training is technically sound and consistent with established practices in multimodal learning.

**Medium Confidence:** The cross-modal performance claims depend on benchmark design choices and evaluation protocols that are not fully specified.

**Low Confidence:** The absolute scale claims (500B tokens, specific dataset compositions) cannot be independently verified from the technical report.

## Next Checks

1. **Ablation Study of Training Stages:** Systematically remove Stage 2.1 freezing and Stage 2.2 gradual unfreezing to quantify their individual contributions to performance.

2. **Independent Benchmark Reproduction:** Replicate core experiments using publicly available datasets (LAION-5B, Whisper ASR data, public TTS corpora) with controlled data mixing ratios.

3. **Audio Tokenization Rate Sensitivity Analysis:** Train parallel models with 8Hz, 12.5Hz, and 25Hz audio tokenization rates while keeping all other hyperparameters constant.