---
ver: rpa2
title: What is a Number, That a Large Language Model May Know It?
arxiv_id: '2502.01540'
source_url: https://arxiv.org/abs/2502.01540
tags:
- number
- similarity
- language
- distance
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) represent
  numbers, revealing a fundamental tension between treating numbers as strings versus
  numerical values. Using similarity-based prompting from cognitive science, the authors
  show that LLM similarity judgments between integers combine both Levenshtein edit
  distance (string-based) and Log-Linear distance (numerically-based), suggesting
  an entangled representation.
---

# What is a Number, That a Large Language Model May Know It?

## Quick Facts
- arXiv ID: 2502.01540
- Source URL: https://arxiv.org/abs/2502.01540
- Reference count: 32
- Key finding: LLMs represent numbers using entangled string and numerical distance metrics, leading to systematic errors in numerical reasoning

## Executive Summary
This paper investigates how large language models represent numbers by examining their similarity judgments between integers. Using similarity-based prompting from cognitive science, the authors reveal that LLMs combine both string-based (Levenshtein edit distance) and numerically-based (Log-Linear distance) similarity metrics in an entangled representation. This fundamental tension between treating numbers as strings versus numerical values persists across multiple models including GPT-4o, Claude-3.5-Sonnet, DeepSeek-V3, Mixtral-8x22B, and Llama variants. The study demonstrates that this representational entanglement can lead to incorrect decisions in naturalistic scenarios, particularly for longer numbers where models show 36-47% error rates in choosing the numerically closer option when string bias prevails.

## Method Summary
The authors employed similarity-based prompting techniques from cognitive science to probe how LLMs represent numbers. They generated triplets of integers where two pairs had either high string similarity and low numerical similarity (string-similarity pair) or low string similarity and high numerical similarity (numerical-similarity pair). Models were asked to identify which pair was more similar, revealing their underlying representation. The study tested multiple prompting strategies including comparisons between integers, their string representations, and integer conversions. Internal probing was conducted by extracting latent embeddings and measuring similarity between representations of numbers. Naturalistic scenarios were constructed to test how this representational tension affects real-world decision-making.

## Key Results
- LLM similarity judgments combine both Levenshtein edit distance and Log-Linear distance metrics in an entangled representation
- Context modifications (using int() vs str()) can reduce but not eliminate string bias
- Models showed 36-47% error rates in naturalistic scenarios when choosing numerically closer options
- Representational entanglement persists across multiple models and can be probed internally in latent embeddings

## Why This Works (Mechanism)
The entangled representation emerges because LLMs learn numerical concepts from text input alone, without grounding in physical experience or explicit numerical training. When trained on text data, numbers appear both as linguistic tokens (subject to string-based processing) and as representations of quantities (subject to numerical processing). The model's attention mechanisms and embedding spaces must accommodate both uses simultaneously, creating a mixed representation where string and numerical similarity cues compete. This is particularly pronounced for longer numbers where string distance effects become more significant relative to numerical differences.

## Foundational Learning
- **Numerical Cognition**: Understanding how humans and machines represent numbers is crucial because it reveals fundamental differences in learning mechanisms and potential failure modes
- **Similarity-Based Prompting**: This technique allows researchers to probe internal representations without requiring explicit model access, providing insight into how models organize conceptual knowledge
- **Distance Metrics**: Knowledge of different similarity measures (Levenshtein for strings, Log-Linear for numbers) helps explain why models might prioritize certain relationships over others
- **Latent Embedding Analysis**: Understanding how to extract and analyze internal representations is essential for diagnosing representational issues in black-box models
- **Prompt Engineering**: Context modifications like int() vs str() demonstrate how subtle changes in prompting can significantly affect model behavior
- **Numerical Reasoning**: The study highlights how seemingly abstract representational issues can manifest in concrete decision-making failures

## Architecture Onboarding

**Component Map**: Tokenization -> Embedding Layer -> Attention Mechanism -> MLP Layers -> Output Layer

**Critical Path**: The attention mechanism is critical as it determines how the model weighs string versus numerical similarity cues during processing

**Design Tradeoffs**: Models must balance efficiency (processing numbers as tokens) against accuracy (processing numbers as quantities), creating the representational tension

**Failure Signatures**: String bias dominates when numbers are longer, less familiar, or when context doesn't explicitly signal numerical interpretation

**First 3 Experiments**:
1. Test whether entanglement persists when numbers are presented in non-standard formats (scientific notation, words)
2. Evaluate whether fine-tuning on explicitly numerical data reduces string bias
3. Compare similarity judgments across different tokenization strategies to isolate the source of string bias

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Similarity judgments rely on prompting strategies that may not fully capture underlying representation mechanisms
- The study doesn't establish exact neural mechanisms responsible for entanglement, leaving questions about tokenization versus attention contributions
- Naturalistic scenario tests used specific prompting that may not generalize to all real-world numerical reasoning applications

## Confidence
- **High confidence**: Empirical observation that LLM similarity judgments combine string and numerical distance metrics
- **Medium confidence**: Claim that entanglement represents fundamental representational tension rather than prompting artifact
- **Low confidence**: Assertion that phenomenon directly parallels human numerical cognition given different learning mechanisms

## Next Checks
1. Test whether similar entanglement patterns appear when models process numerical data in non-text modalities (images, tables, code) to assess whether this is truly a text-derived phenomenon
2. Conduct ablation studies removing or modifying specific architectural components to identify which mechanisms contribute most to string bias
3. Evaluate model performance on numerical reasoning tasks with varying levels of numerical magnitude and string complexity to better characterize when and why string bias dominates numerical reasoning