---
ver: rpa2
title: 'PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question
  Answering'
arxiv_id: '2512.05336'
source_url: https://arxiv.org/abs/2512.05336
tags:
- answer
- question
- pathfinder
- reasoning
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PATHFINDER addresses multi-hop question answering by improving
  training data quality for iterative reasoning with retrieval. It uses Monte Carlo
  Tree Search (MCTS) to generate diverse Chain-of-Thought (CoT) traces, then filters
  them using sub-answer recall and LLM-as-a-judge verification.
---

# PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering

## Quick Facts
- arXiv ID: 2512.05336
- Source URL: https://arxiv.org/abs/2512.05336
- Reference count: 0
- Outperforms comparable baselines on public datasets, achieving 75.00 EM and 78.38 F1 on in-distribution 2WikiMultiHopQA, and 39.70 EM and 51.88 F1 on out-of-distribution HotpotQA using Qwen-2.5-7B-Instruct

## Executive Summary
PATHFINDER addresses multi-hop question answering by improving training data quality for iterative reasoning with retrieval. It uses Monte Carlo Tree Search (MCTS) to generate diverse Chain-of-Thought traces, then filters them using sub-answer recall and LLM-as-a-judge verification. The method also reformulates sub-queries when retrieval fails. PATHFINDER outperforms comparable baselines on public datasets, achieving 75.00 EM and 78.38 F1 on in-distribution 2WikiMultiHopQA, and 39.70 EM and 51.88 F1 on out-of-distribution HotpotQA using Qwen-2.5-7B-Instruct. It significantly improves over DeepRAG, especially when retrieval-only, due to better trace filtering and sub-question reformulation that avoids hallucinations.

## Method Summary
PATHFINDER uses MCTS to generate diverse Chain-of-Thought traces for multi-hop QA. The method employs a two-stage filtering process: first filtering traces through sub-answer recall verification, then using an LLM-as-a-Judge to score remaining traces on incorrectness, redundancy, irrelevance, and faithfulness. Optimal traces are selected based on minimum error score and shortest length. The approach includes sub-question reformulation when retrieval fails, and fine-tunes target models (Qwen-2.5-7B or Gemma-2-9B) with masked context on the filtered traces.

## Key Results
- Achieves 75.00 EM and 78.38 F1 on in-distribution 2WikiMultiHopQA
- Achieves 39.70 EM and 51.88 F1 on out-of-distribution HotpotQA
- Significantly outperforms DeepRAG, especially when retrieval-only
- Ablation shows LLM validation provides the largest accuracy gain

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MCTS generates diverse reasoning paths, increasing the probability of discovering optimal traces compared to single-path generation.
- **Mechanism:** The tree search alternates between two action types—One-Step Thought (A1) and Subquestion Generation/Answering (A2)—with temperature-controlled sampling at each node to create branching paths. UCT scores balance exploration vs. exploitation during selection.
- **Core assumption:** Multiple valid reasoning paths exist for multi-hop questions; broader search space coverage yields higher-quality training traces.
- **Evidence anchors:**
  - [abstract] "uses Monte Carlo Tree Search to generate diverse Chain-of-Thought traces"
  - [section 3.1] "we set the number of sampling nodes to 2 for A1, and 3 for A2... enabling branching at the corresponding node of the tree"
  - [corpus] Weak direct validation; DAMR paper mentions LLM-guided MCTS for KGQA but doesn't isolate diversity effects
- **Break condition:** When retrieval corpus lacks relevant documents for any sub-path, MCTS cannot find correct paths regardless of search depth or branching factor.

### Mechanism 2
- **Claim:** Two-stage filtering (sub-answer recall + LLM-as-Judge) removes erroneous traces and selects higher-quality training data.
- **Mechanism:** Stage 1 discards traces where golden sub-answer recall < 1. Stage 2 scores remaining traces on four criteria (incorrectness, redundancy, irrelevance, faithfulness), discarding traces with incorrectness/faithfulness errors, then selecting minimum-error shortest traces.
- **Core assumption:** Golden sub-answers are reliable ground truth; LLM-Judge can accurately detect reasoning errors that recall metrics miss.
- **Evidence anchors:**
  - [abstract] "filters these traces through sub-answer recall verification and LLM-as-a-Judge validation"
  - [section 3.2 + Table 2] "we see the biggest jump in accuracy with the LLM validation step"
  - [corpus] SPARE paper addresses automatic process supervision but doesn't validate LLM-Judge accuracy for this use case
- **Break condition:** If LLM-Judge has high false-positive rates on faithfulness detection, valid traces may be incorrectly discarded, reducing training data quantity and diversity.

### Mechanism 3
- **Claim:** Training models to reformulate queries when retrieval fails reduces hallucinated answers.
- **Mechanism:** When retrieved context doesn't support an answer, the model generates a reformulated sub-question with more specific details (e.g., adding "film director" to disambiguate entity) rather than extracting irrelevant information.
- **Core assumption:** Models can learn reformulation patterns from training traces where MCTS discovered successful reformulation paths.
- **Evidence anchors:**
  - [abstract] "reformulates sub-queries to handle failed retrieval cases"
  - [section 4.1, Figure 3] "PATHFINDER tries to re-query the retriever by reformulating with more relevant information in the next sub-question to disambiguate"
  - [corpus] Memory-Aware Retrieval paper addresses retrieval failure handling but doesn't isolate reformulation as a mechanism
- **Break condition:** When entity ambiguity cannot be resolved through query reformulation alone (e.g., multiple valid entities with identical names and roles).

## Foundational Learning

- **Concept:** Monte Carlo Tree Search (UCT variant)
  - **Why needed here:** Core data generation mechanism; understanding selection/expansion/simulation/backpropagation is essential for debugging trace generation.
  - **Quick check question:** If UCT exploration weight w is set too high, what behavior would you expect during node selection?

- **Concept:** Interleaved Reasoning-Retrieval (ReAct-style)
  - **Why needed here:** Action space design (OST vs. SGA) directly mirrors this paradigm; must understand when to think vs. retrieve.
  - **Quick check question:** For the question "Which film's director died earlier: X or Y?", sketch the first 4 action steps.

- **Concept:** LLM-as-a-Judge Evaluation
  - **Why needed here:** Critical for understanding filtering criteria and potential failure modes in data quality control.
  - **Quick check question:** Which two of the four Judge criteria cause immediate trace discarding vs. error-score penalty?

## Architecture Onboarding

- **Component map:**
  Input Question -> MCTS Tree Generation (A1: OST, A2: SGA + Retrieval) -> Candidate Traces (all paths reaching correct final answer) -> Filter Stage 1: Sub-answer Recall = 1.0 -> Filter Stage 2: LLM-Judge (4 criteria scoring) -> Optimal Trace Selection (min error score -> shortest length) -> Fine-tuning (context masked, loss on thoughts/queries/answers only)

- **Critical path:** MCTS generation -> Sub-answer verification -> LLM-Judge filtering. Ablation (Table 2) shows Judge validation provides the largest accuracy gain.

- **Design tradeoffs:**
  - MCTS depth=12, rollouts=12: deeper trees find more paths but increase generation time linearly
  - Sampling temps (A1=0.6, A2 query=1.0, A2 answer=0.2): high query temp encourages diverse retriever-aligned sub-questions; low answer temp ensures consistent extraction
  - Training epochs: Qwen requires 5, Gemma requires 2—model-specific tuning needed

- **Failure signatures:**
  - Low trace yield after filtering (<80% questions with valid traces): retriever-query misalignment or overly strict Judge
  - Hallucinated sub-answers in output: Judge faithfulness detection insufficient
  - OOD performance collapse: training traces overfit to in-distribution reasoning patterns

- **First 3 experiments:**
  1. Run MCTS on 50 sample questions with tree visualization to confirm branching behavior and path diversity
  2. Reproduce Table 2 ablation (shortest path -> +recall -> +Judge) to validate filtering contribution
  3. Manual comparison on 10 failure cases: PATHFINDER vs. DeepRAG to confirm reformulation triggers correctly when context is insufficient

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can preference learning from retrieval success/failure pairs generated by MCTS improve retriever-query alignment?
- **Basis in paper:** [explicit] Authors state: "In future work, we plan to investigate how to more explicitly align the sub-question generation with the retriever (e.g., using preference learning from retrieval success and failure question pairs generated by MCTS)."
- **Why unresolved:** The current approach uses temperature sampling and reformulation heuristics but does not explicitly train the model to generate retriever-friendly queries.
- **What evidence would resolve it:** Comparing PATHFINDER with and without preference learning on retrieval success rates and downstream QA accuracy.

### Open Question 2
- **Question:** Would LLM-based answer evaluation provide more accurate quality assessment than EM/F1 metrics?
- **Basis in paper:** [explicit] Authors note: "exact match and F1 scores do not always reflect answer correctness, as supplementary words (e.g., 'London, UK' vs. 'London, England') may unfairly affect scores."
- **Why unresolved:** Current evaluation relies on lexical matching that penalizes semantically equivalent answers with surface variations.
- **What evidence would resolve it:** A correlation study comparing human judgments, LLM evaluations, and EM/F1 scores on PATHFINDER outputs.

### Open Question 3
- **Question:** Does using the same model for MCTS trace generation and LLM-as-Judge introduce validation bias?
- **Basis in paper:** [inferred] Qwen-2.5-72B-Instruct serves as both the data generator (MCTS) and validator (LLM Judge), potentially creating circular validation where model-specific patterns go undetected.
- **Why unresolved:** No ablation tests alternative judge models or human validation of the filtered traces.
- **What evidence would resolve it:** Comparing trace quality when using different judge models versus the current setup, measuring downstream task performance.

## Limitations
- Method's performance gains depend heavily on quality and coverage of MCTS-generated traces, with no comprehensive error analysis of when MCTS fails
- Sub-answer recall threshold of 1.0 may be overly strict, potentially discarding partially correct traces valuable for training
- LLM-as-a-Judge component introduces additional uncertainty, as paper doesn't validate Judge's accuracy on detecting faithfulness errors versus false positives

## Confidence
- **High Confidence:** Training models to reformulate queries when retrieval fails is well-supported by qualitative examples and consistent performance improvements
- **Medium Confidence:** Two-stage filtering significantly improves training data quality, but specific contribution of each stage is not fully isolated
- **Low Confidence:** Claim that MCTS generates sufficiently diverse reasoning paths lacks direct experimental validation compared to simpler single-path generation approaches

## Next Checks
1. **Trace Diversity Analysis:** Measure and compare actual diversity of reasoning paths generated by PATHFINDER's MCTS versus a single-path generation baseline on the same 50-question validation set, quantifying path overlap and coverage of different reasoning strategies.

2. **LLM-Judge Reliability Test:** Manually annotate 100 randomly selected traces with ground truth correctness/faithfulness labels and compare against the LLM-Judge's binary decisions to calculate precision, recall, and false-positive rates for each of the four evaluation criteria.

3. **Sub-answer Recall Threshold Sensitivity:** Repeat the full training pipeline with recall thresholds of 0.8, 0.9, and 1.0 to measure the trade-off between training data quantity and final model performance, identifying the optimal balance point for this filtering criterion.