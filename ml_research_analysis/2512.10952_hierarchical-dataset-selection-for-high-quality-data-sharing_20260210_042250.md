---
ver: rpa2
title: Hierarchical Dataset Selection for High-Quality Data Sharing
arxiv_id: '2512.10952'
source_url: https://arxiv.org/abs/2512.10952
tags:
- dataset
- dash
- data
- datasets
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formalizes the task of dataset selection, where entire
  datasets must be selected from a large pool to improve model performance under resource
  constraints. Existing data selection methods focus on individual samples and ignore
  the hierarchical structure of datasets.
---

# Hierarchical Dataset Selection for High-Quality Data Sharing

## Quick Facts
- arXiv ID: 2512.10952
- Source URL: https://arxiv.org/abs/2512.10952
- Authors: Xiaona Zhou, Yingyan Zeng, Ran Jin, Ismini Lourentzou
- Reference count: 40
- Primary result: Hierarchical Bayesian method DaSH outperforms sample-level data selection baselines by up to 26.2% accuracy on Digit-Five and DomainNet benchmarks

## Executive Summary
This paper addresses the problem of selecting entire datasets from a large pool to improve model performance under resource constraints. Traditional data selection methods focus on individual samples, ignoring the hierarchical structure of datasets where datasets belong to groups based on similarity or domain characteristics. The authors propose DaSH (Dataset Selection with Hierarchical Bayesian modeling), which models dataset utility at both group and dataset levels through structured exploration and posterior inference. Experiments demonstrate that DaSH achieves superior accuracy compared to state-of-the-art baselines while requiring fewer exploration steps, particularly in low-resource settings and when relevant datasets are scarce.

## Method Summary
DaSH introduces a hierarchical Bayesian framework that models dataset utility at two levels: group-level relevance and individual dataset utility. The method performs structured exploration to infer which dataset groups are most relevant to the target task, then uses posterior inference over observed model performance to evaluate individual dataset utility within those groups. This hierarchical approach captures the relationships between datasets while respecting their structural organization, enabling more efficient and effective dataset selection compared to sample-level methods that treat each data point independently.

## Key Results
- DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy on Digit-Five and DomainNet benchmarks
- Requires significantly fewer exploration steps compared to baseline methods
- Demonstrates robustness to low-resource settings and scenarios with few relevant datasets
- Shows consistent improvements across both Digit-Five and DomainNet datasets

## Why This Works (Mechanism)
The hierarchical Bayesian approach works by capturing dependencies between datasets at multiple levels. By modeling group-level relevance first, DaSH can efficiently identify which clusters of datasets are likely to be useful before exploring individual datasets within those groups. This structured exploration reduces the search space and computational cost while improving selection quality. The Bayesian inference framework allows for principled uncertainty quantification, enabling better decisions under limited exploration budgets. The hierarchical structure also enables knowledge transfer between similar datasets, improving performance when data is scarce.

## Foundational Learning

**Bayesian Hierarchical Modeling**: Framework for modeling parameters at multiple levels with shared structure
- Why needed: Captures relationships between datasets while respecting their hierarchical organization
- Quick check: Verify that group-level parameters inform individual dataset estimates

**Posterior Inference**: Computing probability distributions over model parameters given observed data
- Why needed: Enables principled uncertainty quantification for dataset utility estimation
- Quick check: Ensure posterior distributions concentrate around true utilities with sufficient data

**Structured Exploration**: Guided search strategy that leverages hierarchical information
- Why needed: Reduces exploration cost by focusing on promising dataset groups first
- Quick check: Measure reduction in exploration steps compared to random search

## Architecture Onboarding

**Component Map**: Dataset pool → Group embedding space → Group utility inference → Dataset utility inference → Final selection
**Critical Path**: Feature extraction → Hierarchical Bayesian model → Posterior inference → Dataset selection
**Design Tradeoffs**: Balances exploration-exploitation tradeoff through hierarchical uncertainty quantification vs. computational complexity of Bayesian inference
**Failure Signatures**: Poor group embeddings lead to incorrect group prioritization; insufficient exploration causes uncertainty in utility estimates
**First Experiments**: 1) Validate group utility inference on synthetic hierarchical data 2) Test dataset utility estimation with known ground truth 3) Benchmark exploration efficiency against random selection

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes linear relationships between dataset similarity and utility may not hold in all domains
- Relies on accurate dataset similarity embeddings without explicit validation of embedding quality
- Experimental evaluation limited to image classification tasks on Digit-Five and DomainNet benchmarks
- Computational complexity of posterior inference scales with number of datasets, potentially prohibitive for very large pools

## Confidence
- **High**: Hierarchical modeling approach and distinction from sample-level methods
- **High**: Experimental results showing DaSH outperforming baselines on Digit-Five and DomainNet
- **Medium**: Claims about robustness to low-resource settings and lack of relevant datasets
- **Low**: Generalization claims to domains beyond image classification

## Next Checks
1. Test DaSH on a non-vision domain (e.g., text classification with heterogeneous datasets) to verify cross-domain applicability
2. Conduct scalability analysis measuring runtime and memory usage as dataset pool size increases from 10 to 1000+ datasets
3. Perform ablation studies varying the similarity metric (e.g., using different embedding methods or distance functions) to assess sensitivity to similarity computation