---
ver: rpa2
title: Evaluating the Utility of Grounding Documents with Reference-Free LLM-based
  Metrics
arxiv_id: '2601.23129'
source_url: https://arxiv.org/abs/2601.23129
tags:
- grogu
- utility
- documents
- gold
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes GROGU, a reference-free metric for evaluating
  the utility of grounding documents in RAG systems. GROGU defines utility as the
  change in LLM generation confidence (measured via entropy) when conditioning on
  documents versus not.
---

# Evaluating the Utility of Grounding Documents with Reference-Free LLM-based Metrics

## Quick Facts
- arXiv ID: 2601.23129
- Source URL: https://arxiv.org/abs/2601.23129
- Authors: Yilun Hua; Giuseppe Castellucci; Peter Schulam; Heba Elfardy; Kevin Small
- Reference count: 16
- Primary result: Reference-free GROGU metric enables annotation-free query-rewriter training with up to 18.2 points MRR improvement

## Executive Summary
This paper introduces GROGU, a reference-free metric for evaluating the utility of grounding documents in RAG systems by measuring changes in LLM generation confidence (entropy) when conditioning on documents versus not. Unlike traditional relevance metrics, GROGU captures LLM-specific utility differences and can identify cases where less relevant documents provide more utility. The metric is applied to train query-rewriters using Direct Preference Optimization without requiring any annotated labels, achieving significant improvements across multiple retrievers and benchmarks.

## Method Summary
GROGU defines document utility as the difference in entropy between grounded and ungrounded generation, using key-token filtering to isolate answer-relevant confidence changes. For training query-rewriters without annotations, the method generates multiple rewrites per query, computes GROGU scores for each, and creates preference pairs (highest vs. lowest utility) for Direct Preference Optimization. The training pipeline includes SFT warmup on highest-GROGU rewrites followed by DPO on filtered preference pairs, requiring only ~2.5 hours on 8×A100 80GB GPUs.

## Key Results
- GROGU-trained Qwen achieves 45.9 MRR on QReCC vs. 32.4 for Base SFT (13.5 point improvement)
- Improvements up to 18.2 points in MRR and 9.4 points in answer accuracy across multiple retrievers
- KeyEntropy achieves 77.9-95.8% win rates for gold documents vs. distractors, outperforming standard entropy
- Model-specific utility differences: Phi-4 vs Qwen show 2.4-3.4 point accuracy differences when using each other's retrieval contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Key-token entropy reduction captures document utility more reliably than full-sequence metrics
- Mechanism: GROGU identifies "key tokens" whose entropy changes meaningfully between grounded and ungrounded generation. Tokens with entropy difference above threshold α contribute to the utility score; scaffolding phrases and copied question tokens are filtered out. This isolates answer-relevant confidence changes from noise.
- Core assumption: Tokens whose entropy shifts when conditioning on documents are those most influenced by the grounding content, and this shift correlates with answer correctness.
- Evidence anchors:
  - [abstract]: "defines utility as a function of the downstream LLM's generation confidence based on entropy"
  - [Section 3.2]: "A token yi in a generated sequence yg is a key token if |Hθ(yi|q,Dr,y0,...,yi-1) - Hθ(yi|q,y0,...,yi-1)| > α"
  - [Table 1]: KeyEntropy achieves 77.9-95.8% win rates for gold documents vs. distractors, statistically significantly outperforming standard entropy in 6 of 8 comparisons
  - [corpus]: Related work on RAG evaluation (RAGVUE, Redefining Retrieval Evaluation) focuses on diagnostic frameworks rather than entropy-based utility metrics—no direct corroboration or contradiction found
- Break condition: If key-token selection identifies no tokens above threshold AND the fallback top-K% mechanism also fails to correlate with correctness, the metric degrades to random selection.

### Mechanism 2
- Claim: Model-specific utility differs from LLM-agnostic relevance because different models extract different utility from identical documents
- Mechanism: The same document positioned differently or formatted differently affects each LLM's generation confidence differently. GROGU captures this by computing utility relative to each model's θ. Weaker models may fail to utilize challenging document formats that stronger models handle successfully.
- Core assumption: Generation confidence differences reflect genuine utility differences rather than calibration artifacts, and these differences predict downstream accuracy.
- Evidence anchors:
  - [Section 4.3]: "using our GROGU scores to select which context to use for response generation improves over a random choice" and "using the best context for another model does not perform as well"
  - [Figure 1 example]: Qwen-2-1.5b-it fails to use the Balamory document while Phi-4 succeeds
  - [Table 4]: Phi(DPhi)=74.7% vs Phi(DQwen)=72.5%; Qwen(DQwen)=69.3% vs Qwen(DQwen)=66.3%
  - [corpus]: No corpus papers directly address model-specific document utility
- Break condition: If model-specific preferences are purely random noise rather than systematic differences, cross-model utility predictions would match same-model predictions.

### Mechanism 3
- Claim: GROGU-guided preference pairs enable annotation-free DPO training for query rewriters
- Mechanism: For each query, multiple rewrites are generated. GROGU scores each rewrite's top-10 retrieved documents. The highest-scoring rewrite becomes the preferred example; the lowest becomes the dispreferred. Pairs with small GROGU gaps are filtered. DPO then optimizes the rewriter to generate queries that retrieve documents with higher utility.
- Core assumption: The GROGU gap between rewrites reflects meaningful quality differences that DPO can learn to reproduce, and this generalizes to unseen queries.
- Evidence anchors:
  - [Section 5.1]: "we take the rewrite corresponding to the highest GROGU for SFT warmup and we pair the rewrites with the highest and lowest GROGU for DPO"
  - [Table 5]: GroGU-trained Qwen achieves 45.9 MRR on QReCC vs. 32.4 for Base SFT (13.5 point improvement)
  - [Section 5.3]: "improvements up to 18.2 points in MRR and 9.4 points in answer accuracy"
  - [corpus]: RetPO (Yoon et al.) uses gold passage ranks for preference data—GROGU replaces annotation-dependent signals
- Break condition: If GROGU scores are highly correlated across all rewrites (low variance), preference pairs provide weak learning signal and DPO fails to improve over SFT baseline.

## Foundational Learning

- Concept: Entropy as confidence measure
  - Why needed here: GROGU's core operation is comparing token-level entropy H = -Σp·log(p) between grounded and ungrounded conditions. Understanding that low entropy = high confidence is essential.
  - Quick check question: If a token has entropy 0.1 vs. 2.5, which represents higher model confidence?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: GROGU generates preference pairs (preferred/dispreferred rewrites) that feed into DPO training. Understanding DPO's objective—learning from pairwise preferences without explicit reward modeling—explains why annotation-free signals suffice.
  - Quick check question: Why does DPO not require a separate reward model during training?

- Concept: Retrieval metrics (MRR, Recall@K)
  - Why needed here: The paper evaluates retrieval improvements using MRR and Recall@10/20/100. These metrics measure ranking quality, not generation quality directly.
  - Quick check question: What does MRR=45.9 mean in terms of where the relevant document appears in rankings?

## Architecture Onboarding

- Component map:
  Query Rewriter (LLM being trained) -> Retriever (BM25/ANCE) -> Generator/Utility Scorer (same LLM) -> Preference Constructor -> DPO Trainer

- Critical path:
  1. Sample N rewrites per query (25 in paper)
  2. For each rewrite: retrieve top-10 docs -> compute GROGU via KeyEntropy
  3. Select max-GROGU rewrite as preferred, min-GROGU as dispreferred
  4. Filter pairs with gap < threshold (removed 50% in paper)
  5. SFT warmup on highest-GROGU rewrites (3 epochs)
  6. DPO training on filtered pairs (3 epochs)

- Design tradeoffs:
  - Same model for utility scoring and rewriter vs. larger model for scoring: Paper shows both work; 32B scorer with 7B rewriter generalizes well (Table 12)
  - Top-K document selection: Paper uses top-10; more documents increases compute but may capture more utility signal
  - Key-token threshold α and fallback percentage K: Paper uses α=0.05, K=10%; tuned on small validation set

- Failure signatures:
  - Base SFT outperforms GroGU: Indicates GROGU preferences don't correlate with actual rewrite quality—check if retriever mismatch between training and evaluation
  - Low win rates on gold vs. distractor test (Table 1): α threshold may be wrong; re-tune on validation set
  - All rewrites receive similar GROGU scores: Either rewrites are insufficiently diverse or GROGU is not discriminative for this domain

- First 3 experiments:
  1. Reproduce gold vs. distractor win rates (Table 1) on your target domain to validate GROGU discriminative power before training
  2. Train with Base SFT (random rewrite selection) as baseline to isolate GROGU's contribution over distillation alone
  3. Ablate key-token selection by comparing KeyEntropy vs. standard entropy to confirm filtering mechanism is necessary for your model/retriever combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GROGU effectively train other RAG components (e.g., rerankers, retrievers) beyond query-rewriters?
- Basis in paper: [explicit] "We expect GROGU to be also applicable to other components of RAG systems, e.g., the re-ranking stage."
- Why unresolved: The paper only demonstrates GROGU on query-rewriter training; other components require different integration patterns and evaluation protocols.
- What evidence would resolve it: Experiments applying GROGU to train rerankers or retrievers, showing improvements in retrieval and generation metrics comparable to query-rewriter gains.

### Open Question 2
- Question: Can GROGU replace gold-label-based rewards in more complex training frameworks like ConvSearch-R1's reinforcement learning with reasoning?
- Basis in paper: [explicit] "We expect that our metric can replace ConvSearch-R1's training reward that is based on gold passage labels, and leave this study to future work."
- Why unresolved: ConvSearch-R1 uses online RL with reasoning chains, requiring different reward signal properties than DPO.
- What evidence would resolve it: Training ConvSearch-R1 or similar RL-based methods with GROGU rewards, matching or exceeding performance of gold-label versions.

### Open Question 3
- Question: How does GROGU perform when applied to multi-hop reasoning tasks where document utility depends on reasoning chains rather than direct answer presence?
- Basis in paper: [inferred] GROGU measures confidence change for single-hop answers; the paper notes relevance-based metrics fail when "less relevant documents have more utility" but doesn't address compositional reasoning.
- Why unresolved: Entropy-based confidence may not capture utility for documents contributing to intermediate reasoning steps without containing final answers.
- What evidence would resolve it: Experiments on multi-hop QA benchmarks (e.g., HotpotQA) comparing GROGU's correlation with retrieval success against relevance baselines.

## Limitations

- Model-specific utility generalization remains untested across different domains and retriever configurations
- Preference pair quality control lacks analysis of gap distribution and correlation with actual rewrite quality
- Entropy-calibration relationship is assumed but not validated across document types and complexity levels

## Confidence

**High Confidence**
- Key-token entropy reduction reliably discriminates gold from distractor documents (Table 1 win rates 77.9-95.8%)
- GROGU-guided preference pairs enable annotation-free DPO training with measurable improvements (Table 5: 13.5 point MRR gain)
- Model-specific utility differences are reproducible across experiments (Table 4: 2.4-3.4 point accuracy differences)

**Medium Confidence**
- Same-model utility scoring and rewriter