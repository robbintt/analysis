---
ver: rpa2
title: 'Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with
  Targeted Representation Fine-Tuning'
arxiv_id: '2507.04250'
source_url: https://arxiv.org/abs/2507.04250
tags:
- actor
- refusal
- over-refusal
- queries
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACTOR, a training framework that reduces
  over-refusal in safety-aligned LLMs by fine-tuning only a single model layer using
  activation-based signals. Instead of traditional output-based instruction tuning,
  ACTOR adjusts model parameters to align internal representations with query-specific
  targets derived from a refusal direction vector.
---

# Just Enough Shifts: Mitigating Over-Refusal in Aligned Language Models with Targeted Representation Fine-Tuning

## Quick Facts
- arXiv ID: 2507.04250
- Source URL: https://arxiv.org/abs/2507.04250
- Reference count: 40
- This paper introduces ACTOR, a training framework that reduces over-refusal in safety-aligned LLMs by fine-tuning only a single model layer using activation-based signals.

## Executive Summary
This paper introduces ACTOR, a training framework that reduces over-refusal in safety-aligned LLMs by fine-tuning only a single model layer using activation-based signals. Instead of traditional output-based instruction tuning, ACTOR adjusts model parameters to align internal representations with query-specific targets derived from a refusal direction vector. The approach is both data- and compute-efficient, requiring no full response supervision. Experiments show ACTOR improves average compliance rates by 47.47% on over-refusal benchmarks while maintaining strong safety performance (99% safety score) and minimal impact on general capabilities.

## Method Summary
ACTOR identifies a "refusal direction" in activation space by computing the difference between harmful and benign query activations at a target layer, selected via silhouette analysis. It then fine-tunes only this single layer using a projection-calibrated loss that adjusts activations toward a "just enough" target based on each query's projection onto the refusal direction. The method requires no response-level supervision, only safe/pseudo-harmful/harmful labels, making it highly data-efficient. Training uses cosine similarity loss between current and target activations, with target activations computed as the current activation minus a scaled projection onto the refusal vector for safe/pseudo-harmful queries.

## Key Results
- ACTOR improves average compliance rates by 47.47% on over-refusal benchmarks
- Maintains strong safety performance with 99% safety score on harmful queries
- Achieves better tradeoff scores than baselines while requiring no full response supervision

## Why This Works (Mechanism)

### Mechanism 1: Refusal Direction as a Geometric Concept
The paper proposes that refusal behavior can be characterized as a direction in activation space, where harmful and benign queries cluster on opposite sides of a roughly linear boundary. Extract a "refusal vector" R using difference-in-means between harmful and benign query activations at a target layer. This vector captures the directional shift from response-generating to refusal-generating representations. The core assumption is that the refusal decision boundary is approximately linear in the activation space of middle layers.

### Mechanism 2: Projection-Calibrated Individualized Shifts
Queries require different shift magnitudes based on how strongly they align with the refusal direction—queries with larger projections need larger corrections. Instead of uniform shifts (which cause model breakdown), compute target activations as a_q - α·Proj_R(a_q) for pseudo-harmful queries, where the projection magnitude determines shift intensity. The core assumption is that a linear relationship exists between projection magnitude and the minimum shift needed for compliance.

### Mechanism 3: Middle-Layer Safety Criticality
Refusal decisions are most prominently encoded in middle transformer layers, making them the optimal targets for localized fine-tuning. Compute silhouette scores for each layer's ability to separate harmful/benign clusters; select the layer with highest score for targeted parameter updates. The core assumption is that safety-relevant representations concentrate in specific layers rather than being distributed uniformly.

## Foundational Learning

- Concept: **Difference-in-Means for Direction Extraction**
  - Why needed here: Used to compute the refusal vector R by averaging activations from harmful queries and subtracting averages from benign queries.
  - Quick check question: Given two sets of embeddings A and B, what does (mean(A) - mean(B)) represent geometrically?

- Concept: **Silhouette Score for Cluster Quality**
  - Why needed here: Quantifies how well each layer separates harmful vs. benign activations to identify safety-critical layers.
  - Quick check question: What does a silhouette score near 1.0 indicate about cluster separation?

- Concept: **Cosine Similarity Loss**
  - Why needed here: The PRD loss uses 1 - cosine_similarity to push activations toward target directions without requiring exact magnitude matching.
  - Quick check question: Why might cosine similarity be preferred over L2 distance when aligning activation directions?

## Architecture Onboarding

- Component map:
  - **Refusal Vector Extraction Module** -> **Projection-Calibrated Loss** -> **Single-Layer Optimizer**

- Critical path:
  1. Identify target layer via silhouette analysis (offline, one-time)
  2. Compute initial refusal vector R from anchor dataset
  3. For each training batch: extract activations → compute projections → generate targets → update parameters
  4. Periodically re-compute R (every K steps) as model shifts

- Design tradeoffs:
  - α (projection multiplier): Too low = minimal behavior change; too high = safety degradation. Paper found optimal values of 0.0015 (Llama-2-7b), 0.003 (Gemma), 0.0004 (Llama-2-13b)
  - Layer selection: Earlier/later layers show negligible effect (Table 6); middle layers critical
  - Re-computation frequency K: More frequent updates improve adaptation but increase compute

- Failure signatures:
  - Model produces nonsensical outputs → uniform shift being applied (Appendix B)
  - Safety score drops significantly → α too high
  - Compliance rate unchanged → wrong target layer or α too low

- First 3 experiments:
  1. **Layer ablation**: Train ACTOR on layers 5, 13, and 25 for Llama-2-7b-chat; verify middle layer (13) produces highest compliance rate improvement
  2. **Alpha sweep**: Grid search α in [0.0001, 0.01] with logarithmic spacing; plot compliance rate vs. safety score tradeoff curve
  3. **Data efficiency test**: Train with n ∈ {10, 25, 50} over-refusal examples; compare against SFT baseline to validate response-free advantage

## Open Questions the Paper Calls Out

### Open Question 1
How does ACTOR's single-layer intervention perform in multi-turn dialogue scenarios? The current study evaluates single-turn interactions, but multi-turn dialogues introduce context dependency and accumulation of error risks that may destabilize the "just enough" shifts applied to static activation targets.

### Open Question 2
Can the activation-based methodology be adapted for strict black-box API settings? The method relies on directly accessing and optimizing hidden states to compute refusal vectors, which is impossible for users of proprietary models without API access to internals.

### Open Question 3
Does the theoretical assumption of a linear refusal boundary hold robustly across diverse semantic domains? While empirically successful, it is unclear if a single direction vector accurately separates safe/harmful clusters in complex domains where the manifold geometry might be non-linear.

## Limitations
- Limited model and task scope: Validated only on Llama-2-7b-chat, Llama-2-13b-chat, and Gemma-7b-chat instruction-tuned variants
- Internal validity concerns: Moderate silhouette scores (max 0.72) suggest the refusal direction may be an approximation rather than exact geometric construct
- Evaluation methodology concerns: Binary safety scoring and use of GPT-4o as judge without inter-annotator reliability reporting

## Confidence

- **High confidence**: The empirical improvement in compliance rates (47.47% average increase) and maintenance of safety scores (>99%) are well-supported by the experimental results
- **Medium confidence**: The geometric interpretation of refusal behavior as a direction in activation space is plausible but relies on moderate separation scores
- **Low confidence**: The claim that "just enough" projection-calibrated shifts are universally optimal across different model scales and domains

## Next Checks

1. **Cross-model generalization test**: Apply ACTOR to a diverse set of 3-5 additional model architectures (e.g., Mistral, Qwen, base models) using the same α values and target layer selection procedure

2. **Robustness to anchor set composition**: Systematically vary the ratio of harmful to benign queries in the refusal vector computation (e.g., 32:96, 48:80, 80:48) and measure impact on compliance rates and safety scores

3. **Alternative direction extraction methods**: Replace the difference-in-means approach with PCA-based direction extraction or supervised linear classifiers trained to separate harmful/benign activations