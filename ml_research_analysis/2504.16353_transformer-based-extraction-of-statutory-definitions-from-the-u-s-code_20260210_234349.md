---
ver: rpa2
title: Transformer-Based Extraction of Statutory Definitions from the U.S. Code
arxiv_id: '2504.16353'
source_url: https://arxiv.org/abs/2504.16353
tags:
- legal
- definition
- definitions
- extraction
- scope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automatically extracting
  statutory definitions from the U.S. Code, a complex legal corpus of over 200,000
  pages where definitions are scattered across distinct sections.
---

# Transformer-Based Extraction of Statutory Definitions from the U.S. Code

## Quick Facts
- arXiv ID: 2504.16353
- Source URL: https://arxiv.org/abs/2504.16353
- Authors: Arpana Hosabettu; Harsh Shah
- Reference count: 30
- One-line primary result: 98.2% F1-score for definition detection using Legal-BERT fine-tuned on U.S. Code

## Executive Summary
This paper addresses the challenge of automatically extracting statutory definitions from the U.S. Code, a complex legal corpus of over 200,000 pages where definitions are scattered across distinct sections. The authors propose a transformer-based system using Legal-BERT fine-tuned on legal texts to identify definitional paragraphs, then aggregate related paragraphs and extract defined terms and scope using attention mechanisms and rule-based patterns. Their multi-stage pipeline processes XML-structured legal text with hierarchical document understanding. Evaluated across multiple U.S. Code titles, the system achieves 96.8% precision, 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. The work advances legal NLP by demonstrating domain-adapted transformers' effectiveness for statutory definition extraction, improving accessibility of legal information while establishing foundations for downstream legal reasoning tasks.

## Method Summary
The method employs a multi-stage pipeline for statutory definition extraction from XML-structured U.S. Code documents. First, Legal-BERT is fine-tuned to classify definitional paragraphs from non-definitional text using hierarchical attention mechanisms that aggregate tokens into sentences and sentences into paragraphs. Second, multi-paragraph definitions are aggregated using semantic similarity measures and document structure. Third, defined terms are extracted using BERT-based sequence labeling with BIO tagging, while definitional scope is identified through a combination of rule-based pattern matching and attention mechanisms. The system uses a multi-task learning framework with three prediction heads sharing a common encoder, optimized with weighted loss functions. Training employs AdamW optimizer with linear learning rate decay, batch size of 16, and early stopping with patience of 1.

## Key Results
- 96.8% precision, 98.9% recall (98.2% F1-score) for definition detection across multiple U.S. Code titles
- Legal-BERT achieves 98.25% F1-score vs. Generic BERT at 96.80% F1—a 1.45 percentage point improvement
- Multi-paragraph definition aggregation successfully handles complex statutory structures spanning multiple subparagraphs
- Rule-based patterns effectively identify definitional scope in explicit declarations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training on legal corpora improves definition detection accuracy over generic language models.
- Mechanism: Legal-BERT captures specialized vocabulary, citation patterns, and definitional phrasing conventions (e.g., "shall mean," "as used in this section") that generic BERT models underweight. Fine-tuning on 3,500 labeled definitions from the U.S. Code further aligns token representations to statutory language distributions.
- Core assumption: The linguistic patterns in labeled U.S. Code definitions generalize to unlabeled statutory text across different titles.
- Evidence anchors:
  - [abstract] "our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy"
  - [section IV, Table I] Legal-BERT achieves 98.25% F1 vs. Generic BERT at 96.80% F1—a 1.45 percentage point improvement
  - [corpus] Neighbor paper "Scaling Legal AI" confirms transformer architectures dominate legal NLP benchmarks but notes computational constraints; domain adaptation remains underexplored for statute-specific tasks
- Break condition: If target legal corpus differs substantially in linguistic register (e.g., case law vs. statutes), performance gains may diminish. Fine-tuning data from target domain would be required.

### Mechanism 2
- Claim: Hierarchical attention over document structure enables accurate handling of multi-paragraph definitions.
- Mechanism: The system preserves XML document graphs capturing parent-child-sibling relationships. Token-level attention aggregates into sentence representations (Eq. 4-5), which aggregate into paragraph representations (Eq. 6-7). A graph attention network (Eq. 8-9) then encodes structural relationships between paragraphs, allowing the model to recognize that scattered text elements form a single definition.
- Core assumption: Structural proximity in the document hierarchy correlates with semantic coherence of definitional units.
- Evidence anchors:
  - [section III-B] Definition of "Hispanic-serving agricultural colleges and universities" spans multiple subparagraphs with continuation cues
  - [section III-G, Eq. 4-9] Explicit mathematical formulation of hierarchical attention and document structure encoding
  - [corpus] Weak corpus evidence—neighbor papers do not directly evaluate hierarchical attention for legal texts
- Break condition: If document structure is flattened or XML hierarchy is corrupted during preprocessing, the graph attention mechanism loses its structural signal. Definitions spanning non-adjacent sections may also break the proximity assumption.

### Mechanism 3
- Claim: Multi-task joint learning improves individual task performance through shared representations.
- Mechanism: Three prediction heads (definition detection, term extraction, scope identification) share the same transformer encoder and are optimized jointly via weighted loss L = 0.4×L_def + 0.3×L_term + 0.3×L_scope. Shared encoder learns representations useful across tasks—e.g., detecting "the term X means" patterns helps both classification and term boundary detection.
- Core assumption: Tasks share underlying linguistic features; optimizing them jointly does not introduce harmful gradient interference.
- Evidence anchors:
  - [section I] "We develop a multi-task learning framework that jointly identifies definitions, extracts defined terms, and determines definitional scope"
  - [section III-G, Eq. 10] Multi-task loss formulation with empirically tuned λ weights
  - [corpus] No direct corpus evidence for multi-task learning in legal definition extraction; this is a gap in prior work
- Break condition: If tasks have conflicting gradient directions (negative transfer), single-task models may outperform. Ablation studies would be needed to verify per-task contribution.

## Foundational Learning

- Concept: **Transformer attention mechanisms**
  - Why needed here: The system relies on self-attention to weight token importance and hierarchical attention to aggregate sentences into paragraphs. Understanding attention weights helps debug what the model focuses on.
  - Quick check question: Given a sentence "The term 'employee' means any individual," which tokens should receive higher attention weights for detecting the defined term?

- Concept: **Sequence labeling with BIO tagging**
  - Why needed here: Term extraction uses BERT-based sequence labeling with BIO scheme (B-TERM, I-TERM, O) to identify term boundaries. Understanding this scheme is necessary to interpret model outputs and fix boundary errors.
  - Quick check question: For input "The term 'small business concern' includes," what would correct BIO labels look like?

- Concept: **Legal document hierarchy (Title → Section → Paragraph → Clause)**
  - Why needed here: Scope detection depends on understanding where a definition sits in the hierarchy. "As used in this title" applies differently than "as used in this subsection." The document graph encodes these relationships.
  - Quick check question: If a definition at section 3103 declares "As used in this section," does it apply to section 3104?

## Architecture Onboarding

- Component map:
  - Document Structure Processor (JAXB + StAX) → Document graph with hierarchical relationships
  - Legal-BERT Encoder (fine-tuned) → Contextual token embeddings
  - Hierarchical Attention Layer → Aggregates tokens → sentences → paragraphs
  - Graph Attention Network → Encodes document structure into paragraph representations
  - Multi-task Heads → Three outputs: definition binary, term BIO tags, scope class + span

- Critical path: XML parsing → paragraph extraction → Legal-BERT encoding → hierarchical attention → multi-task prediction → rule-based post-processing for scope. If document graph is malformed, downstream scope detection fails silently.

- Design tradeoffs:
  - Sliding window with overlap handles long paragraphs but introduces boundary artifacts
  - Rule-based scope patterns provide precision but miss implicit scope declarations
  - Weighted loss (0.4/0.3/0.3) prioritizes definition detection over term extraction—adjust if term boundaries are higher priority
  - Early stopping patience of 1 is aggressive; may underfit on smaller corpora

- Failure signatures:
  - Low recall on implicit definitions: Model misses definitions without "means" or "shall be construed" patterns
  - Fragmented multi-paragraph definitions: Aggregator creates separate units for what should be one definition—check IsRelated() threshold
  - Scope overgeneralization: Definitions incorrectly inherit parent scope when explicit scope is missing
  - Term boundary drift: BIO sequence produces illegal transitions (I-TERM without B-TERM)

- First 3 experiments:
  1. Baseline ablation: Run definition detection with Generic BERT vs. Legal-BERT on held-out Title (not in training) to confirm domain adaptation gain. Expect ~1-2% F1 gap per Table I.
  2. Aggregator threshold sweep: Vary semantic similarity threshold in IsRelated() function on a sample of multi-paragraph definitions. Measure precision/recall of correct unit formation.
  3. Single-task vs. multi-task comparison: Train three separate models (one per task) and compare F1 against joint model. Look for negative transfer in term extraction, which has smallest loss weight.

## Open Questions the Paper Calls Out

- Can transformer-based methods effectively track how statutory definitions evolve across different versions of the U.S. Code over time?
  - Basis in paper: Future work section calls for "developing methods to track changes in definitions across different versions of the U.S. Code, enabling analysis of legal concept evolution over time."
  - Why unresolved: Current system processes static snapshots; temporal aspects like amendments and effective dates are not captured (noted in limitations).
  - What evidence would resolve it: A longitudinal study showing definition change detection accuracy across multiple U.S. Code editions with manually annotated change annotations.

- How can definition extraction systems reliably identify implicit legal definitions that lack standard definitional keywords?
  - Basis in paper: Limitations section states "Implicit definitions without standard definitional keywords remain challenging to identify reliably."
  - Why unresolved: Current hybrid approach depends heavily on pattern-matching for terms like "means" and "shall mean," which implicit definitions lack.
  - What evidence would resolve it: Evaluation on a curated dataset of implicit definitions showing improved F1-scores compared to the current 98.25% baseline on explicit definitions.

- To what extent do cross-reference dependencies between definitions impact extraction accuracy, and can graph-based modeling resolve them?
  - Basis in paper: Limitations section notes "Cross-references between definitions create dependencies that are not fully modeled in the current system."
  - Why unresolved: The Definition Network Builder exists but dependency resolution between interlinked definitions (e.g., "as defined in section 7601") remains incomplete.
  - What evidence would resolve it: Ablation study comparing extraction accuracy with and without explicit cross-reference resolution on a held-out test set containing nested definitions.

## Limitations
- Evaluation primarily on Titles 7 and 11 raises concerns about generalization to other legal domains
- Rule-based scope extraction patterns are heuristic and may fail on implicit scope declarations
- Multi-task learning contribution is not empirically isolated through ablation studies
- Annotation process for multi-paragraph definitions and scope spans is not fully specified

## Confidence
- **High confidence**: Transformer-based Legal-BERT outperforms generic BERT on definition detection in U.S. Code (Table I shows consistent gains across titles).
- **Medium confidence**: Hierarchical attention and document structure encoding improve handling of multi-paragraph definitions (mechanism plausible but not directly validated in isolation).
- **Medium confidence**: Multi-task learning improves overall performance (no ablation to confirm per-task benefit or rule out negative transfer).

## Next Checks
1. **Cross-title generalization test**: Hold out one or more titles entirely from training and evaluate definition detection performance. Compare results to within-title splits to quantify domain adaptation limits.

2. **Ablation of multi-task learning**: Train three single-task models (definition detection, term extraction, scope classification) and compare their combined performance to the multi-task model. Measure whether term extraction specifically benefits from shared encoder.

3. **Robustness to document structure corruption**: Randomly remove XML hierarchy relationships in validation set and measure degradation in scope classification accuracy. This tests whether the document graph encoding is truly necessary or if flat processing would suffice.