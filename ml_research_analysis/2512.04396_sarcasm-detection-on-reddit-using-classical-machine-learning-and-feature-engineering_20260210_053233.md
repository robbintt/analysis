---
ver: rpa2
title: Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering
arxiv_id: '2512.04396'
source_url: https://arxiv.org/abs/2512.04396
tags:
- features
- sarcasm
- self
- word
- sarcastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates sarcasm detection using classical machine
  learning methods without relying on conversational context. It uses a 100k-subset
  of the Self-Annotated Reddit Corpus (SARC 2.0), extracting word and character TF-IDF
  features along with simple stylistic indicators such as length, punctuation, and
  uppercase ratios.
---

# Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering

## Quick Facts
- **arXiv ID**: 2512.04396
- **Source URL**: https://arxiv.org/abs/2512.04396
- **Reference count**: 11
- **Primary result**: Context-free sarcasm detection on Reddit achieves F1 ~0.57 using TF-IDF + stylistic features with Naive Bayes/Logistic Regression

## Executive Summary
This paper establishes a baseline for sarcasm detection using only reply text from Reddit comments, deliberately excluding conversational context to create a controlled benchmark. Using a 100k-subset of the Self-Annotated Reddit Corpus (SARC 2.0), the study combines word and character TF-IDF features with simple stylistic indicators (length, punctuation, uppercase ratios) and evaluates four classical models. Logistic regression and multinomial Naive Bayes perform best with F1-scores around 0.57 for identifying sarcastic comments. While limited by the absence of context, this work provides a clear, reproducible baseline using interpretable and lightweight methods that future research can build upon.

## Method Summary
The study extracts features from Reddit reply text using a custom TextFeatures transformer that combines word-level TF-IDF (unigrams and bigrams, 20k max features), character-level TF-IDF (3-5 character n-grams, 10k max features), and five numeric stylistic features (text length, word count, exclamation/question marks per word, uppercase ratio). These sparse and dense features are horizontally stacked and fed into four scikit-learn classifiers: logistic regression, linear SVM, multinomial Naive Bayes, and random forest. The models are trained on an 80/20 stratified split of 100,000 comments from the SARC 2.0 balanced dataset, with evaluation focused on F1-score for the sarcastic class.

## Key Results
- Logistic regression and Naive Bayes achieve the highest F1-scores (~0.569) for sarcastic comment detection
- Random Forest underperforms linear models on sparse TF-IDF features (F1 = 0.563)
- Character-level TF-IDF captures stylistic signals like elongated words and creative spellings
- Numeric stylistic features provide weak but orthogonal signal to lexical TF-IDF features

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Character-level TF-IDF captures stylistic signals that word-level features miss
- **Mechanism**: Character n-grams (3-5 characters) detect elongated words ("soooo"), creative spellings, and distinctive punctuation patterns. These sub-word units are sensitive to orthographic exaggeration often present in sarcastic text
- **Core assumption**: Stylistic exaggeration correlates with sarcastic intent more reliably than lexical content alone
- **Evidence anchors**: Abstract mentions combining TF-IDF with stylistic indicators; section 4 explains character n-grams' sensitivity to elongated words and creative spellings; related work notes prosodic and stylistic cues are important

### Mechanism 2
- **Claim**: Numeric stylistic features provide weak but orthogonal signal to TF-IDF
- **Mechanism**: Ratios of uppercase letters, exclamation marks, and question marks per word quantify "exaggeration intensity." These aggregate statistics are model-agnostic and add dimensions to the feature space that capture tone
- **Core assumption**: Sarcastic comments exhibit measurable differences in capitalization and punctuation patterns compared to sincere ones
- **Evidence anchors**: Section 4 notes these measures help quantify exaggeration; section 6 describes TF-IDF capturing lexical tendencies while numeric measures highlight stylistic exaggeration

### Mechanism 3
- **Claim**: Linear models outperform Random Forest on sparse TF-IDF features for this task
- **Mechanism**: High-dimensional sparse representations favor linear decision boundaries. Random Forest struggles with extreme sparsity because each split sees limited information gain. Naive Bayes and Logistic Regression handle sparse high-dimensional spaces more naturally
- **Core assumption**: The sarcasm signal is distributed across many weak features rather than concentrated in feature interactions
- **Evidence anchors**: Table 1 shows Random Forest F1 = 0.563 vs. Logistic Regression F1 = 0.569 and Naive Bayes F1 = 0.569; section 6 explains Naive Bayes' probabilistic structure suits sparse TF-IDF data

## Foundational Learning

- **Concept**: TF-IDF Vectorization
  - **Why needed here**: The entire feature representation rests on converting raw text into weighted sparse vectors. Without understanding TF vs. IDF weighting, you cannot interpret why certain n-grams become discriminative
  - **Quick check question**: Why would the word "great" have high TF but potentially low IDF in a sarcasm dataset?

- **Concept**: Multinomial Naive Bayes vs. Gaussian Naive Bayes
  - **Why needed here**: The paper uses MultinomialNB specifically because it handles count/frequency data (TF-IDF) correctly. Using GaussianNB on sparse data would fail
  - **Quick check question**: What distribution does MultinomialNB assume for each feature, and why is this appropriate for word counts?

- **Concept**: Stratified Train-Test Split
  - **Why needed here**: The paper emphasizes stratification to preserve class balance. Without it, random splits could distort the 50/50 sarcasm balance and bias evaluation
  - **Quick check question**: If your dataset has 10% positive class, what happens if you do a non-stratified 80/20 split?

## Architecture Onboarding

- **Component map**:
  ```
  Raw reply text → TextFeatures transformer
                    ├─ TfidfVectorizer (word, 1-2 grams, 20k features)
                    ├─ TfidfVectorizer (char, 3-5 grams, 10k features)
                    └─ Numeric features (5 dimensions: length, words, !/? per word, uppercase ratio)
                   → scipy.sparse.hstack → Classifier (LogReg/SVM/NB/RF)
  ```

- **Critical path**:
  1. Data loading: `train-balanced.csv.bz2` → sample 100k → filter empty strings
  2. Split: `train_test_split(..., stratify=y)` — ensures balanced test set
  3. Fit TextFeatures: Both vectorizers fit on training data only (via Pipeline)
  4. Transform: Concatenate sparse matrices + dense numeric features
  5. Train: `.fit()` on pipeline
  6. Evaluate: `classification_report`, `confusion_matrix`, `roc_auc_score`

- **Design tradeoffs**:
  - 100k subsample: Faster iteration, but may underrepresent rare sarcasm patterns. Full dataset could improve coverage of lexical diversity
  - No context: Clean baseline but caps performance ceiling (~0.57 F1). Adding parent comments is the obvious next step
  - max_features (20k word, 10k char): Limits memory but discards rare n-grams. Higher values may capture more signal at computational cost

- **Failure signatures**:
  - Accuracy stuck at ~50% with F1 ~0.5 for both classes → model not learning; check feature extraction or class balance
  - Random Forest underperforming linear models → expected on sparse data; verify feature sparsity ratio
  - Naive Bayes F1 significantly lower than reported → check if TF-IDF uses sublinear_tf=True (the code does)
  - Empty confusion matrix cells → test set too small or stratification failed

- **First 3 experiments**:
  1. Reproduce baseline: Run provided code on SARC 2.0 balanced subset with default hyperparameters. Verify F1 ≈ 0.57 for NB/LogReg
  2. Ablate feature types: Remove character TF-IDF, then remove numeric features, then remove word TF-IDF. Measure F1 drop to quantify each component's contribution
  3. Add minimal context: Include parent comment text concatenated to reply (simple baseline for context). Expect F1 improvement if context matters as literature suggests

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: To what extent does incorporating conversational context (parent comments) improve performance over the context-free baseline established in this study?
- **Basis in paper**: [explicit] The concluding remarks explicitly identify "incorporate conversation context" as a necessary direction for future research to address the limitations of this work
- **Why unresolved**: The study intentionally restricted input to reply text only to create a controlled baseline, leaving the performance gain of context unquantified
- **What evidence would resolve it**: A comparative evaluation where the featured classical models are trained on the concatenation of parent comments and replies from the SARC 2.0 dataset

### Open Question 2
- **Question**: How do pre-trained neural embeddings (e.g., BERT, RoBERTa) compare to the handcrafted TF-IDF features in a context-free setting?
- **Basis in paper**: [explicit] The author lists "explore pre-trained neural embeddings" as a primary avenue for future research in the conclusion
- **Why unresolved**: The paper strictly utilizes TF-IDF and stylistic features to maintain interpretability, leaving the potential accuracy trade-off of using dense neural representations unexplored
- **What evidence would resolve it**: A benchmark comparing the F1-scores of the reported Logistic Regression baseline against a neural model using static or contextual embeddings on the same 100k subsample

### Open Question 3
- **Question**: Does the inclusion of simple numeric stylistic features (e.g., uppercase ratio, punctuation) provide statistically significant improvements over TF-IDF features alone?
- **Basis in paper**: [inferred] The paper combines TF-IDF and stylistic features into a single pipeline and reports results for the combination, but does not provide an ablation study isolating the specific contribution of the stylistic indicators
- **Why unresolved**: While the author notes these features quantify "exaggeration," it remains unclear if they offer additive predictive power or if character n-grams already capture these nuances
- **What evidence would resolve it**: An ablation study comparing the full model against a version using only TF-IDF features to isolate the performance delta attributed to the stylistic variables

## Limitations

- The 100k subsample may not capture full lexical and stylistic diversity of the complete SARC 2.0 dataset
- Performance ceiling is inherently limited without conversational context (achieving only ~0.57 F1)
- Reliance on stylistic markers assumes Reddit norms haven't shifted since the dataset's collection
- TF-IDF weighting assumes sarcasm manifests in distinctive word usage rather than purely semantic or contextual shifts

## Confidence

**High Confidence**: The comparative performance ranking of models (Naive Bayes ≈ Logistic Regression > SVM > Random Forest) is well-supported by the results and aligns with theoretical expectations for sparse, high-dimensional text features. The methodology for feature extraction and model training is clearly specified and reproducible.

**Medium Confidence**: The claim that character-level TF-IDF captures sarcasm-specific stylistic signals is plausible but not directly validated within the study. While the mechanism is theoretically sound, the paper doesn't ablate character features to quantify their specific contribution.

**Low Confidence**: The assertion that this approach provides a meaningful "baseline" is somewhat limited—while the methodology is clear, the 0.57 F1 score is substantially below what context-aware or LLM-based approaches achieve, making it more of a minimal baseline than a competitive one.

## Next Checks

1. **Ablation study validation**: Systematically remove character TF-IDF features, then numeric features, then word TF-IDF features to measure F1-score degradation. This quantifies each component's contribution and tests the claim about character n-grams capturing unique sarcasm signals.

2. **Context incorporation test**: Implement the simplest possible context addition—concatenate parent comment text to reply text—and re-run all four models. Compare F1-scores to the no-context baseline to validate the common assumption that context substantially improves performance.

3. **Full dataset evaluation**: Run the complete pipeline on the full SARC 2.0 balanced dataset rather than the 100k subsample. This tests whether the performance estimates hold with greater lexical diversity and whether computational constraints significantly impact results.