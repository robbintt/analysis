---
ver: rpa2
title: Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation
arxiv_id: '2506.22776'
source_url: https://arxiv.org/abs/2506.22776
tags:
- robustness
- quantization
- llms
- quantized
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first systematic investigation of how
  quantization affects the robustness of LLMs in code generation tasks. Through extensive
  experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and StarCoder)
  with parameter scales ranging from 350M to 33B, we evaluate robustness from dual
  perspectives: adversarial attacks on input prompts and noise perturbations on model
  architecture.'
---

# Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation

## Quick Facts
- **arXiv ID:** 2506.22776
- **Source URL:** https://arxiv.org/abs/2506.22776
- **Reference count:** 40
- **Key outcome:** Quantized LLMs often exhibit greater robustness than full-precision counterparts in code generation tasks, with 51.59% of adversarial experiments showing better resilience in quantized models.

## Executive Summary
This paper presents the first systematic investigation of how quantization affects the robustness of LLMs in code generation tasks. Through extensive experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and StarCoder) with parameter scales ranging from 350M to 33B, the study evaluates robustness from dual perspectives: adversarial attacks on input prompts and noise perturbations on model architecture. Surprisingly, the results reveal that quantized LLMs often exhibit greater robustness than their full-precision counterparts, with 51.59% versus 42.86% of adversarial experiments showing better resilience in quantized LLMs. The noise perturbation experiments also confirm that LLMs after quantization generally withstand higher levels of weight disturbances. These findings suggest that quantization not only reduces computational requirements but can actually enhance LLMs' reliability in code generation tasks, providing valuable insights for developing more robust and efficient LLM deployment strategies.

## Method Summary
The study benchmarks robustness of quantized LLMs in code generation via adversarial attacks on prompts and noise perturbations on model weights. Four datasets are used: HumanEval, HumanEval+, MBPP, and MBPP+. Twelve models across four families are evaluated: LLaMA (1B/3B/8B), DeepSeek (1.3B/6.7B/33B), CodeGen (350M/2B/6B), and StarCoder (3B/7B/15B). Quantization is performed using bitsandbytes with NF4 for 4-bit and linear for 8-bit configurations. Three attack types are applied: character-level (0.5 probability, max 5 chars uppercase), word-level (0.15 probability using WordNet synonyms), and sentence-level (mbart-large-50 back-translation EN→DE→EN). Gaussian and Uniform noise at levels 1e-4, 1e-3, 3e-3, 5e-3, and 1e-2 are applied to weights. Performance is measured using pass@1 for functional correctness and Relative Robustness Score (RRS = degradation_orig / degradation_quant) where RRS > 1 indicates quantized model is more robust.

## Key Results
- Quantized LLMs showed greater robustness than full-precision models in 51.59% of adversarial experiments versus 42.86% for full-precision
- Noise perturbation experiments confirmed quantized models generally withstand higher levels of weight disturbances
- Quantization can enhance LLMs' reliability in code generation tasks, not just reduce computational requirements
- Results provide valuable insights for developing more robust and efficient LLM deployment strategies

## Why This Works (Mechanism)
None provided in source material.

## Foundational Learning
- **Relative Robustness Score (RRS)**: Metric comparing degradation between original and quantized models; needed to quantify robustness differences; quick check: RRS > 1 means quantized model is more robust
- **bitsandbytes quantization**: Framework for model quantization; needed to create quantized versions of LLMs; quick check: verify quantization config matches paper (NF4 for 4-bit, linear for 8-bit)
- **Back-translation attacks**: Using mbart-large-50 for EN→DE→EN translation to perturb prompts; needed for sentence-level adversarial testing; quick check: ensure only natural language portions are perturbed, preserving code
- **Noise perturbation levels**: 1e-4 to 1e-2 applied to weights; needed to test model resilience to weight corruption; quick check: verify noise magnitude relative to weight scale
- **pass@1 metric**: Functional correctness measure for code generation; needed as primary performance evaluation; quick check: ensure generation hyperparameters are consistent
- **Adversarial attack types**: Character, word, and sentence-level attacks; needed to comprehensively test robustness; quick check: verify attack probabilities and parameters match paper specifications

## Architecture Onboarding
**Component map:** Datasets -> Model loading/quantization -> Adversarial attacks -> Noise perturbation -> Evaluation (pass@1, RRS)

**Critical path:** Model loading and quantization → Baseline evaluation → Adversarial attack application → Robustness score calculation → Noise perturbation → Comparative degradation analysis

**Design tradeoffs:** The study prioritizes comprehensive benchmarking across multiple model families and attack types over deep investigation of individual failure modes. The RRS metric provides a simple scalar comparison but may obscure nuanced differences in how quantized vs. full-precision models fail.

**Failure signatures:** Extreme RRS values (much greater or less than 1) indicate significant robustness differences; failure to maintain code integrity during back-translation suggests attack implementation issues; NaN/inf outputs from noise perturbation indicate magnitude issues.

**First experiments:**
1. Load DeepSeek-Coder-6.7B-base, quantize to 8-bit, generate baseline pass@1 on HumanEval
2. Implement sentence-level attack using mbart-large-50-many-to-many-mmt, apply to prompts, compute pass@1_attack and RRS
3. Apply Gaussian noise at 3e-3 level to weights, compute pass@1 and compare original vs. quantized degradation

## Open Questions the Paper Calls Out
None identified in source material.

## Limitations
- Analysis focuses exclusively on pass@1 metrics from HumanEval/MBPP-style benchmarks, which may not capture broader robustness implications for real-world code deployment
- The relative robustness score (RRS) methodology assumes symmetric degradation patterns, yet quantized models may exhibit qualitatively different failure modes not captured by this scalar metric
- Noise perturbation experiments apply uniform random disturbances across all weight parameters, which doesn't reflect realistic hardware or training artifacts that might cause structured weight corruption

## Confidence
- **High confidence**: The observation that quantization doesn't universally weaken LLMs (observed in 51.59% of adversarial experiments showing equal or better quantized robustness) is well-supported by the experimental design
- **Medium confidence**: The claim that quantization "can actually enhance" robustness requires caution—the absolute performance differences between quantized and full-precision models are often small, and the distribution of RRS values isn't fully characterized
- **Medium confidence**: The noise perturbation results showing higher tolerance in quantized models are methodologically sound but may not generalize to all noise distributions or model architectures

## Next Checks
1. **Reproduce with varied seeds**: Run the complete adversarial and noise perturbation experiments across 5-10 random seeds to establish statistical significance of the observed robustness patterns, particularly for borderline cases where RRS ≈ 1
2. **Characterize failure modes**: For cases where quantization improves RRS, analyze the specific prompt modifications or noise patterns that trigger this behavior—identify whether certain attack vectors consistently show quantized advantage
3. **Expand robustness metrics**: Implement additional robustness evaluations including pass@k for k>1, execution-time error detection, and semantic equivalence checking to verify that improved RRS correlates with meaningful robustness improvements in generated code