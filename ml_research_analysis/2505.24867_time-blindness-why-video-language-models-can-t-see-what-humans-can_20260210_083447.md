---
ver: rpa2
title: 'Time Blindness: Why Video-Language Models Can''t See What Humans Can?'
arxiv_id: '2505.24867'
source_url: https://arxiv.org/abs/2505.24867
tags:
- temporal
- arxiv
- video
- motion
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Researchers evaluated how well modern video-language models (VLMs)
  process information encoded solely through temporal patterns, with individual frames
  appearing as noise. They created SpookyBench, a benchmark where content (text, shapes,
  images, and video depth maps) is embedded using opposing motion patterns between
  foreground and background noise, making it visible only through temporal integration.
---

# Time Blindness: Why Video-Language Models Can't See What Humans Can?

## Quick Facts
- arXiv ID: 2505.24867
- Source URL: https://arxiv.org/abs/2505.24867
- Reference count: 40
- Researchers find that modern video-language models fail to process information encoded solely through temporal patterns, while humans achieve over 98% accuracy on the same tasks.

## Executive Summary
Researchers evaluated whether modern video-language models can process information encoded solely through temporal patterns, where individual frames appear as noise. Using SpookyBench, a benchmark where content is embedded using opposing motion patterns between foreground and background noise, they found humans achieve over 98% accuracy while 15 state-of-the-art VLMs achieve 0% accuracy regardless of prompting strategy or frame rate. Even targeted fine-tuning on SpookyBench data did not improve model performance. The results reveal that current VLMs are fundamentally "time-blind," relying on spatial features extracted from individual frames rather than temporal cues, and are unable to integrate purely temporal information.

## Method Summary
The study created SpookyBench, a benchmark of 451 videos (960×540 resolution, avg 7.11s duration) where information is encoded solely through temporal sequences of noise-like frames. Content appears as noise in individual frames but becomes visible through temporal integration when noise moves in opposing directions. The benchmark includes four categories: Text (210 videos), Object Images (156), Dynamic Scenes (57), and Shapes (28). Human subjects achieved over 98% accuracy on all categories. The researchers tested 15 state-of-the-art VLMs including GPT-4o, Gemini, and various open-source models using direct and chain-of-thought prompting at frame rates from 1-30 FPS. They also conducted fine-tuning experiments on a subset of 400 videos to test whether additional training could overcome the limitation.

## Key Results
- Humans achieved over 98% accuracy on SpookyBench while all 15 tested VLMs achieved 0% accuracy
- Model performance remained at 0% regardless of prompting strategy (direct or chain-of-thought) or frame rate (1-30 FPS)
- Even targeted fine-tuning on SpookyBench data (400 videos, 5 epochs) failed to improve model accuracy
- The task exploits a fundamental architectural limitation where VLMs rely on spatial features from individual frames rather than temporal integration

## Why This Works (Mechanism)

### Mechanism 1: Opposing Motion Temporal Encoding
Information can be encoded purely in temporal dynamics when individual frames appear as noise, but current Video-VLMs cannot decode it. SpookyBench generates videos where foreground noise moves in one direction while background noise moves oppositely, making content visible only through temporal integration. At each pixel, animated frames sample from different noise offsets over time, creating structured temporal coherence. This encoding preserves sufficient temporal coherence for detection if a system can integrate motion information.

### Mechanism 2: Spatial-First Architecture Bias
Video-VLMs process spatial features first and treat temporal integration as secondary, causing catastrophic failure when spatial cues are absent. The dominant paradigm extracts frame-level ViT features, integrates them temporally, then fuses with language. When frame-level features contain only noise with no spatial signal, subsequent temporal integration has nothing meaningful to operate on. Even models with specialized temporal modules maintain this spatial-first dependency.

### Mechanism 3: Temporal Coherence Threshold Detection
Human perception leverages temporal coherence SNR rather than frame-level SNR, while models lack this capability. SpookyBench videos have negative basic SNR (-39 to -49 dB) but positive temporal coherence SNR (7-22 dB). Humans detect content by grouping pixels with consistent motion direction across frames, performing motion-based figure-ground segregation that models lack. A binary detection threshold exists at ~2.5 dB for text in noise.

## Foundational Learning

- **Signal-to-Noise Ratio (SNR) in temporal vs. spatial domains**: SpookyBench relies on the contrast between extremely negative spatial SNR and positive temporal coherence SNR. Understanding this distinction explains why humans succeed and models fail. Quick check: If a video has -45 dB basic SNR but +15 dB temporal coherence SNR, which metric predicts human detectability?

- **Optical flow and motion-based grouping**: The opposing motion encoding creates detectable motion boundaries. Humans perform figure-ground segregation via motion direction consistency; this is the core perceptual mechanism being tested. Quick check: In Algorithm 1, why does content become invisible when animation is paused?

- **Frame-level vs. sequence-level feature extraction**: Video-VLMs typically apply image encoders per-frame before temporal fusion. This architectural choice creates the spatial dependency that SpookyBench exploits. Quick check: If a model processes each frame independently through a ViT, what information is lost before temporal integration occurs?

## Architecture Onboarding

- **Component map**: Video input → Frame sampler → Vision encoder (ViT, strong spatial bias) → Temporal adapter → LLM → Output. SpookyBench fails at the Vision encoder stage when no spatial features exist.

- **Critical path**: The failure point is the Vision encoder expecting spatial structure. All 15 tested models (2B-78B parameters, open and closed source) fail identically here. Even finetuning on SpookyBench yields 0% accuracy, confirming architectural limitation.

- **Design tradeoffs**: Current models optimize for spatial recognition tasks (ImageNet pretraining) at the cost of pure temporal processing. Adding temporal modules on top of spatial encoders does not address the root issue. Decoupling requires fundamentally different low-level processing.

- **Failure signatures**: Models output plausible-but-wrong content based on noise patterns (attempting spatial interpretation), or refuse to answer. Chain-of-thought prompting provides no improvement. Performance is insensitive to frame rate, confirming architectural rather than sampling issues.

- **First 3 experiments**:
  1. Replicate SpookyBench evaluation on your target Video-VLM using the public dataset; establish baseline 0% accuracy before architectural changes.
  2. Add an optical flow or motion encoding stream as a parallel input to the LLM (bypassing the spatial encoder for temporal information), test if any accuracy improvement occurs.
  3. Train a small model from scratch on SpookyBench-style videos with motion contrast as the primary supervision signal; compare against spatial-pretrained baseline to isolate the pretraining bias effect.

## Open Questions the Paper Calls Out

### Open Question 1
What architectural modifications could enable video-language models to extract meaning from purely temporal patterns when spatial features are unavailable? Authors state "overcoming this limitation will require novel architectures or training paradigms that decouple spatial dependencies from temporal processing." All 15 tested models across architectures and scales achieved 0% accuracy; even chain-of-thought prompting and finetuning failed completely. A model architecture specifically designed for temporal-first processing achieving non-zero accuracy on SpookyBench would resolve this.

### Open Question 2
Can optical flow or motion-based representations serve as primary input modalities to enable temporal pattern recognition in vision models? The paper analyzes temporal coherence SNR (7.18 dB) and motion contrast SNR (14.24 dB) as key signals humans use, but all tested models process RGB frames rather than motion representations directly. No existing video-VLMs use motion-based inputs as their primary representation. Evaluating models that use optical flow or motion boundary maps as primary inputs on SpookyBench would resolve this.

### Open Question 3
Would large-scale pre-training on temporal-only data (without spatial shortcuts) enable models to develop generalizable temporal pattern recognition capabilities? The finetuning experiment (400 videos, 5 epochs on Qwen2.5-VL-7B) failed, but this limited experiment doesn't address whether extensive pre-training from scratch on temporal-only data could succeed. No models have been trained exclusively on temporal-pattern data at scale. Training a model from scratch on large-scale temporal-only datasets and evaluating on SpookyBench would resolve this.

### Open Question 4
Which component in the video-VLM pipeline (frame sampling, visual encoder, or language model) is the primary bottleneck preventing temporal pattern recognition? FPS experiments show models fail at all frame rates (1-30 FPS), ruling out temporal undersampling, but the paper doesn't isolate whether the visual encoder's spatial bias or the language model's processing is the limiting factor. The 0% accuracy persists regardless of frame rate, suggesting architectural rather than sampling issues, but component-level analysis was not performed. Ablation studies replacing individual components with temporal-aware alternatives while holding others constant would resolve this.

## Limitations

- **Noise Generation Reproducibility**: The paper lacks precise implementation details for critical parameters like tileable boundary handling, velocity values, and random seed specifications, creating uncertainty about whether different implementations would yield identical difficulty levels.

- **Human Baseline Completeness**: While human performance exceeds 98% accuracy, the paper does not report variance, training duration, or whether subjects were pre-screened for motion sensitivity or color blindness, and the ceiling effect suggests the task may be easier than intended for some individuals.

- **Generalization Claims**: The assertion that "all Video-VLMs are time-blind" based on 15 models (2B-78B parameters) may not capture the full architectural landscape. Novel temporal architectures or recurrent models not yet published could potentially succeed where current VLMs fail.

## Confidence

- **High Confidence**: The core empirical finding that tested VLMs achieve 0% accuracy on SpookyBench is highly reliable given the consistent results across multiple prompting strategies, frame rates, and fine-tuning attempts.
- **Medium Confidence**: The interpretation that this demonstrates fundamental "time-blindness" in all Video-VLMs is reasonable but requires caution, as 15 diverse models failing identically represents current architectural paradigms rather than an absolute impossibility for temporal processing.
- **Low Confidence**: The suggestion that models attempt to extract spatial features from noise patterns is inferred from response patterns rather than directly observed, and the exact failure mode within model architectures remains partially speculative.

## Next Checks

1. **Implement Independent SpookyBench Generator**: Create a clean-room implementation of the noise generation algorithms with documented parameters to verify reproducibility and test whether different implementations yield consistent human vs. model performance gaps.

2. **Test Novel Temporal Architectures**: Evaluate models specifically designed for temporal processing (RNN-based, temporal convolutional networks, or models with explicit motion encoding streams) on SpookyBench to determine if architectural innovations can overcome the reported limitations.

3. **Conduct Ablation Studies on Motion Parameters**: Systematically vary motion velocity, noise density, and animation duration in SpookyBench to identify the minimum temporal coherence threshold required for human detection and determine whether this threshold can be approached by any current VLM through architectural modifications.