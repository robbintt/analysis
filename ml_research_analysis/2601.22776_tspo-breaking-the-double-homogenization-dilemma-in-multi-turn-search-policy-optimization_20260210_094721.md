---
ver: rpa2
title: 'TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy
  Optimization'
arxiv_id: '2601.22776'
source_url: https://arxiv.org/abs/2601.22776
tags:
- reward
- tspo
- arxiv
- groups
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a "Double Homogenization Dilemma" in multi-turn
  RL for search-augmented reasoning: (1) process-level homogenization, where intermediate
  successes are ignored, and (2) intra-group homogenization, where uniform rewards
  cause vanishing gradients. To address this, the authors propose TSPO with a First-Occurrence
  Latent Reward (FOLR) mechanism that assigns partial rewards to the turn where the
  ground-truth first appears, without requiring external models or annotations.'
---

# TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization

## Quick Facts
- arXiv ID: 2601.22776
- Source URL: https://arxiv.org/abs/2601.22776
- Reference count: 34
- The paper identifies a "Double Homogenization Dilemma" in multi-turn RL for search-augmented reasoning: (1) process-level homogenization, where intermediate successes are ignored, and (2) intra-group homogenization, where uniform rewards cause vanishing gradients. To address this, the authors propose TSPO with a First-Occurrence Latent Reward (FOLR) mechanism that assigns partial rewards to the turn where the ground-truth first appears, without requiring external models or annotations. Extensive experiments on seven QA datasets show that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

## Executive Summary
This paper addresses a critical challenge in multi-turn search-augmented reasoning: the "Double Homogenization Dilemma." The problem arises when reinforcement learning treats all turns uniformly, ignoring intermediate successes and causing vanishing gradients. TSPO introduces a First-Occurrence Latent Reward (FOLR) mechanism that assigns partial rewards to the turn where the ground-truth answer first appears, without requiring external models or annotations. The approach significantly improves performance on seven QA datasets, demonstrating the importance of properly recognizing intermediate progress in reasoning chains.

## Method Summary
The paper proposes TSPO (Turn-level Search Policy Optimization) to address the double homogenization dilemma in multi-turn search reasoning. The core innovation is the First-Occurrence Latent Reward (FOLR) mechanism, which assigns partial rewards to the turn where the ground-truth answer first appears. This approach maintains simplicity by avoiding external models or annotations while effectively distinguishing between successful and unsuccessful turns. The method works by tracking when the ground-truth first emerges during the reasoning process and allocating rewards accordingly, thus preventing both process-level homogenization (ignoring intermediate successes) and intra-group homogenization (uniform rewards causing vanishing gradients).

## Key Results
- TSPO achieves average performance gains of 24% on Qwen2.5-3B models across seven QA datasets
- Performance improvements of 13.6% on Qwen2.5-7B models demonstrate scalability across model sizes
- The FOLR mechanism outperforms baseline approaches without requiring external models or annotations
- Significant improvements observed across diverse QA datasets including HotpotQA, MultiHopQA, and StrategyQA

## Why This Works (Mechanism)
The FOLR mechanism works by identifying the first occurrence of ground-truth content during the reasoning process and assigning partial rewards to that specific turn. This breaks the double homogenization by recognizing intermediate successes (process-level) and creating meaningful reward differentiation between turns (intra-group). The mechanism maintains simplicity by using the ground-truth as its own reference point, avoiding the need for additional annotation or external models. By allocating rewards based on first occurrence rather than final outcome, TSPO provides more granular credit assignment that guides the model toward better intermediate reasoning steps.

## Foundational Learning
**Multi-turn Search Reasoning**: Sequential decision-making where each turn involves search and reasoning operations. *Why needed*: Forms the context for understanding the double homogenization problem. *Quick check*: Can you explain how intermediate search results contribute to final answers?

**Reinforcement Learning Credit Assignment**: The challenge of determining which actions deserve credit for eventual outcomes. *Why needed*: Central to understanding why uniform rewards fail in multi-turn scenarios. *Quick check*: What happens to gradients when all turns receive identical rewards?

**Reward Homogenization**: The problem where rewards are treated uniformly across different stages or groups. *Why needed*: The core problem TSPO addresses. *Quick check*: Can you identify examples of process-level vs. intra-group homogenization?

**First-Occurrence Detection**: Identifying when a specific element (ground-truth) first appears in a sequence. *Why needed*: The mechanism by which FOLR assigns partial rewards. *Quick check*: How would you track first occurrences in a text generation chain?

## Architecture Onboarding

**Component Map**: Query -> Search Module -> Reasoning Chain -> FOLR Reward Assignment -> Policy Gradient Update -> Optimized Policy

**Critical Path**: The most important sequence is: Search results generation → Reasoning chain construction → FOLR mechanism application → Reward calculation → Policy update. The FOLR mechanism is the critical innovation that differentiates TSPO from standard RL approaches.

**Design Tradeoffs**: TSPO trades minimal additional complexity (tracking first occurrences) for significant performance gains. The approach avoids the complexity of external reward models or additional annotations, maintaining practical deployability. The main tradeoff is the computational overhead of tracking ground-truth occurrences across turns.

**Failure Signatures**: Performance degradation occurs when: (1) ground-truth is never generated in the chain, (2) multiple turns contain ground-truth fragments making first-occurrence ambiguous, or (3) the reasoning chain is extremely long (>10 turns) where credit assignment becomes diluted.

**3 First Experiments**:
1. Run TSPO on a simple multi-turn QA dataset with known ground-truth locations to verify FOLR correctly identifies first occurrences
2. Compare reward distributions between TSPO and uniform reward baselines on the same dataset
3. Test TSPO with artificially modified reasoning chains where intermediate ground-truth appears at different positions

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation primarily focuses on Qwen2.5 models, limiting generalizability to other architectures
- Performance on extremely long reasoning chains (>10 turns) remains untested
- Computational overhead compared to simpler baselines is not explicitly quantified
- Claims about generality across different reasoning paradigms lack comprehensive validation

## Confidence
- **High confidence**: The identification of the double homogenization problem is well-motivated and theoretically sound. The mathematical formulation of FOLR is clear and internally consistent.
- **Medium confidence**: The empirical improvements over baselines are substantial, but the ablation studies could be more comprehensive to isolate the specific contribution of each component.
- **Low confidence**: Claims about the method's generality across different reasoning paradigms and model sizes beyond the tested configurations are not fully supported.

## Next Checks
1. Evaluate TSPO on diverse model architectures (e.g., Llama, Mistral) to verify cross-model robustness and identify potential architecture-specific limitations.
2. Conduct experiments with extended reasoning chains (10+ turns) to assess FOLR's effectiveness on complex, multi-step reasoning tasks.
3. Perform ablation studies comparing TSPO with alternative reward shaping methods that don't use first-occurrence mechanisms to isolate the specific benefits of the FOLR approach.