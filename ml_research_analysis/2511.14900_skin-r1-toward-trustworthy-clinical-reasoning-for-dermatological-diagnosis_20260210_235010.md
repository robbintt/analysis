---
ver: rpa2
title: 'Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis'
arxiv_id: '2511.14900'
source_url: https://arxiv.org/abs/2511.14900
tags:
- reasoning
- diagnosis
- clinical
- diagnostic
- lesion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Skin-R1, a vision-language model for dermatological
  diagnosis that addresses the limitations of existing approaches by integrating textbook-grounded
  reasoning with reinforcement learning. Skin-R1 first constructs a high-fidelity
  dataset of hierarchy-aware and differential-diagnosis-informed reasoning trajectories
  from authoritative dermatology references, then uses supervised fine-tuning to instill
  grounded diagnostic reasoning, and finally applies reinforcement learning to generalize
  this reasoning to large, sparsely-annotated datasets.
---

# Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis

## Quick Facts
- arXiv ID: 2511.14900
- Source URL: https://arxiv.org/abs/2511.14900
- Reference count: 40
- Primary result: Achieves average accuracy of 0.6385 on in-distribution data and 0.7171 on out-of-distribution data, outperforming baselines in dermatological diagnosis.

## Executive Summary
Skin-R1 introduces a novel approach for trustworthy clinical reasoning in dermatological diagnosis by combining textbook-grounded reasoning with reinforcement learning. The model constructs a high-fidelity dataset of hierarchy-aware and differential-diagnosis-informed reasoning trajectories from authoritative dermatology references, then uses supervised fine-tuning (SFT) to instill grounded diagnostic reasoning, followed by reinforcement learning (RL) to generalize this reasoning to large, sparsely-annotated datasets. This SFT+RL approach addresses the limitations of existing methods by ensuring both diagnostic accuracy and interpretable clinical reasoning.

## Method Summary
Skin-R1 employs a three-stage pipeline: (1) synthesize a high-quality dataset called SkinRationale containing hierarchy-aware and differential-diagnosis-informed reasoning trajectories from dermatology textbooks; (2) apply supervised fine-tuning on Qwen2.5-VL-7B-Instruct with LoRA adapters to learn the structured reasoning patterns; (3) use reinforcement learning with Group Relative Policy Optimization (GRPO) to refine the model's reasoning on larger, sparsely-annotated datasets. The RL stage employs a composite reward function that includes format compliance, hierarchical granularity, and malignancy assessment to align the model with clinical diagnostic taxonomies and encourage fine-grained discrimination.

## Key Results
- Outperforms baselines with average accuracy of 0.6385 on in-distribution data and 0.7171 on out-of-distribution data
- Ablation studies confirm SFT is essential for effective RL performance
- Demonstrates superior performance specifically on differential diagnosis tasks
- Shows better generalization to out-of-distribution datasets compared to existing models

## Why This Works (Mechanism)

### Mechanism 1: SFT as Necessary Warm-Start for RL
Supervised Fine-Tuning on high-quality, textbook-grounded trajectories acts as a necessary "warm-start" that stabilizes and maximizes the efficiency of subsequent Reinforcement Learning. By first instilling a "reasoning foundation" via SFT, the policy model enters the RL phase with a higher baseline probability of generating valid diagnostic chains, allowing the RL optimizer to refine rather than discover reasoning patterns from scratch.

### Mechanism 2: Hierarchical Reward Shaping for Fine-Grained Discrimination
Hierarchical reward shaping in RL forces the model to align its internal representation with clinical diagnostic taxonomies, improving fine-grained discrimination. By rewarding deeper correct nodes in the taxonomy tree more than shallow ones, the mechanism guides the model to learn the specific visual features associated with fine-grained disease subtypes rather than defaulting to coarse-grained predictions.

### Mechanism 3: DDx-Informed Trajectory Synthesis for Robust Reasoning
Differential Diagnosis (DDx)-informed trajectory synthesis reduces diagnostic confusion by explicitly training the model on comparative reasoning. Instead of mapping Image → Label, the mechanism generates trajectories where the model must compare the primary diagnosis with a "distractor" neighbor from a DDx graph, forcing the model to learn discriminative features and provide interpretable justifications for excluding alternatives.

## Foundational Learning

- **Group Relative Policy Optimization (GRPO)**: Skin-R1 relies on GRPO for its RL stage. Unlike standard PPO, GRPO estimates advantages by comparing sampled responses within a group relative to each other, removing the need for a separate value-function critic model. *Quick check: Can you explain why GRPO is more memory-efficient than PPO for training a 7B parameter VLM?* (Answer: No critic model required).

- **Hierarchical Medical Taxonomy**: The reward function $R_{gran}$ and the data synthesis logic depend entirely on a tree structure where diseases have ancestors. *Quick check: If a model predicts a "parent" node correctly but misses the "child" node, how should the granularity reward $R_{gran}$ behave according to Equation 7?* (Answer: It should provide a partial reward proportional to the depth $i$ of the correct parent).

- **Chain-of-Thought (CoT) Distillation/Synthesis**: The "SkinRationale" dataset is synthetic. Engineers must understand that the "reasoning" shown in Figure 1 is generated by an LLM guided by rules, not human-written. *Quick check: What is the risk of using a student LLM to generate reasoning trajectories for training a teacher VLM?* (Answer: Model collapse or hallucination propagation if the student LLM's logic is unchecked).

## Architecture Onboarding

- **Component map**: Input (Dermatology Image + Instruction Prompt) → Qwen2.5-VL-7B-Instruct (Backbone with LoRA Adapter) → External Knowledge (Dermatology Taxonomy Tree + Differential Diagnosis Graph) → Output (Structured text with $<$$thinking$$>$ rationale and $<$$diagnosis$$>$ label)

- **Critical path**: 
  1. **Data Curation**: Construct SkinRationale using Algorithm 1, 2, & 3 (Type 1, 2, 3 trajectories). *Failure here ruins the SFT phase.*
  2. **SFT Stage**: Fine-tune Qwen2.5-VL on SkinRationale to get the SFT Checkpoint ($\pi_{ref}$). *This is the "foundation" mentioned in abstract.*
  3. **RL Stage**: Initialize policy $\pi_\theta$ from SFT checkpoint. Run GRPO using $R_{total}$ on sparse datasets (PAD-UFES-20, DermNet, etc.).

- **Design tradeoffs**:
  - SFT vs. RL-only: Trades off the "emergent" potential of pure RL for the stability and grounding of SFT+RL, arguing medical domains cannot rely on emergence due to safety requirements
  - Synthetic vs. Human Data: Uses synthetic trajectories (Scalable, Cheap, Hierarchy-aware) vs. Human expert annotations (Expensive, Sparse, potentially inconsistent)
  - Reward Complexity: Uses a composite reward ($Format + Gran + Malignancy$) making debugging harder compared to simple accuracy rewards, but aligns the model better with clinical utility

- **Failure signatures**:
  - Reward Hacking: Model generates valid $<$$diagnosis$$>$ tags but hallucinates content because $R_{format}$ is satisfied but verification fails
  - Granularity Collapse: Model defaults to predicting high-level nodes to secure partial $R_{gran}$ rewards without risking errors on fine-grained labels
  - Catastrophic Forgetting: During RL, if KL penalty $\beta$ is too low, model might forget reasoning patterns learned during SFT

- **First 3 experiments**:
  1. **Ablation Validation**: Run the "RL without SFT" baseline on a single dataset (e.g., HAM10000). Verify that the training curve is unstable/lower than the full Skin-R1 pipeline to confirm the "foundation" hypothesis on your infrastructure
  2. **Reward Sensitivity**: Modify the weights in $R_{total}$ (Eq 5). Try setting $R_{gran}$ weight to 0. Observe if the model loses ability to distinguish fine-grained classes but retains binary (Benign/Malignant) accuracy
  3. **DDx Stress Test**: Evaluate the trained model specifically on the "DDx diagnosis" test set (Section 4.3) using pairs of known confusable diseases (e.g., Melanoma vs. Nevus) to verify comparative reasoning mechanism is active

## Open Questions the Paper Calls Out

### Open Question 1
Can the Skin-R1 framework, which relies on synthesized textbook trajectories, be effectively transferred to medical domains where visual features are less standardized or structured than in dermatology? The method relies on constructing a "differential diagnosis graph" and "hierarchical diagnosis tree" from authoritative textbooks, a resource that may differ in availability or structure in other medical specialties. What evidence would resolve it: Applying the Skin-R1 pipeline to a distinct domain (e.g., chest X-rays) and comparing the fidelity and diagnostic utility of the generated reasoning trajectories against domain-specific baselines.

### Open Question 2
Does the specific design of the granularity reward ($R_{gran}$) inadvertently encourage the model to predict fine-grained diagnoses even when visual evidence is ambiguous or insufficient? The reward function assigns the highest value (0.75) to the deepest correct node in the hierarchy, potentially penalizing clinically safe but coarse-grained predictions. What evidence would resolve it: An error analysis on low-confidence predictions to determine if the model frequently hallucinates specific subtypes rather than correctly deferring to broader categories.

### Open Question 3
To what extent does the quality of the initial synthetic data (SkinRationale) limit the ceiling of the reinforcement learning phase? The ablation study shows that RL fails without the SFT "foundation," suggesting the RL stage is highly dependent on the initial quality of the synthesized trajectories. What evidence would resolve it: A comparative study evaluating the RL convergence and final diagnostic accuracy when initializing with synthetic data versus a smaller dataset of human-verified reasoning chains.

## Limitations
- The core findings hinge on the quality of the synthetic SkinRationale dataset and the faithfulness of the differential diagnosis graph, both of which are not fully specified in the paper
- The RL training pipeline assumes that GRPO can effectively refine SFT-initialized policies without a critic model, but this may not generalize across different VLM backbones or dermatology datasets
- The reported OOD performance on OmniMedVQA is promising but based on a small subset (44 images), limiting statistical confidence
- The model's reliance on curated taxonomy and DDx graphs introduces brittleness if these structures do not fully capture clinical reality

## Confidence
- High confidence: SFT+RL pipeline's superiority over RL-only for this task
- Medium confidence: Scalability and robustness of the synthetic data approach
- Low confidence: Exact reproducibility of the SkinRationale dataset construction and taxonomy/DDx graph definitions

## Next Checks
1. **Ablation Validation**: Run the "RL without SFT" baseline on a single dataset (e.g., HAM10000). Verify that the training curve is unstable/lower than the full Skin-R1 pipeline to confirm the "foundation" hypothesis on your infrastructure.
2. **Reward Sensitivity**: Modify the weights in $R_{total}$ (Eq 5). Try setting $R_{gran}$ weight to 0. Observe if the model loses the ability to distinguish fine-grained classes but retains binary (Benign/Malignant) accuracy.
3. **DDx Stress Test**: Evaluate the trained model specifically on the "DDx diagnosis" test set (Section 4.3) using pairs of known confusable diseases (e.g., Melanoma vs. Nevus) to verify the comparative reasoning mechanism is active.