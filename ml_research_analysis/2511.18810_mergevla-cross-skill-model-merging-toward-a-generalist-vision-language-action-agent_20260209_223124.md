---
ver: rpa2
title: 'MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action
  Agent'
arxiv_id: '2511.18810'
source_url: https://arxiv.org/abs/2511.18810
tags:
- task
- merging
- tasks
- mergevla
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MergeVLA addresses the challenge of merging vision-language-action
  (VLA) models trained on different robotic tasks, which typically fails with zero
  success rates due to task-specific parameter conflicts and architectural incompatibility.
  The core method introduces sparsely activated LoRA adapters via task masks to stabilize
  VLM merging and replaces self-attention layers in the action expert with cross-attention-only
  blocks to preserve modularity.
---

# MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent

## Quick Facts
- arXiv ID: 2511.18810
- Source URL: https://arxiv.org/abs/2511.18810
- Reference count: 40
- Primary result: Merges VLA models across multiple robotic tasks with success rates of 90.2%, 72.2%, and 70.7% on LIBERO, LIBERO-Plus, and RoboTwin benchmarks

## Executive Summary
MergeVLA introduces a novel framework for merging vision-language-action (VLA) models trained on different robotic tasks, addressing the challenge of task-specific parameter conflicts that typically cause zero success rates with standard merging approaches. The method combines task-specific LoRA parameter masking, cross-attention-only action experts, and a test-time task router to achieve robust cross-skill generalization. Across multiple benchmarks including LIBERO, LIBERO-Plus, RoboTwin, and real-world robotic experiments, MergeVLA demonstrates significant improvements over individually fine-tuned experts while maintaining modularity for cross-embodiment adaptation.

## Method Summary
MergeVLA merges multiple task-specific VLA models through three key innovations: (1) sparsely activated LoRA adapters with task masks that resolve parameter conflicts by retaining parameters where task-specific updates dominate, (2) cross-attention-only action experts that prevent task-specific dependencies from propagating across layers, and (3) a test-time task router that dynamically selects task-specific components using value subspace projections. The framework uses Qwen2.5-0.5B backbone with LoRA rank 32, trained separately per task then merged using task arithmetic or TIES. At inference, the router identifies the current task from initial observations and selects the corresponding mask and expert head without prior task knowledge.

## Key Results
- Achieves 90.2% success rate on LIBERO benchmark (vs. 0% with standard merging)
- Outperforms individually fine-tuned experts by 18.7% on out-of-distribution LIBERO-Plus tasks
- Demonstrates 70.7% success rate on cross-embodiment RoboTwin experiments
- Successfully generalizes to real-world SO-101 robotic arm with 70.7% success rate

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Masking for LoRA Conflict Resolution
Sparsely activated task masks enable VLM merging by isolating parameters that align with each task while suppressing conflicting updates. For each task m, a binary mask S_m = I[|τ_m| > λ|τ_merge - τ_m|] retains parameters where the task-specific update τ_m is both significant and dominant over residual disagreement with the merged vector. This encourages conflicting parameters to revert toward pretrained weights rather than corrupt shared representations. Evidence shows selfish ratio on LIBERO rises at around 75% with most parameters retained exclusively by a single task, and masked approaches achieve 62.4-90.2% success vs. 0% for unmasked TA merging on OpenVLA. Break condition occurs when tasks share minimal parameter overlap (extreme selfishness >90%), causing most parameters to revert to pretrained weights and lose task-specific adaptation.

### Mechanism 2: Cross-Attention-Only Architecture for Action Expert Modularity
Removing self-attention from action experts prevents task-specific dependencies from propagating across layers, enabling weight averaging to work for shallow blocks. Standard action experts use self-attention that accumulates task-specific biases over depth, causing parameter distance to "explode in the final layers." Cross-attention-only blocks force reliance on shared VLM features instead of scratch-trained parameters. Sigmoid gates ensure VLM signals are always preserved rather than suppressed. Evidence shows this reflects strong task-specific specialization and divergent layers collectively forming the expert head, achieving 18.7% higher success rate on out-of-distribution LIBERO-Plus. Break condition occurs when tasks require fundamentally different action representations, making even cross-attention blocks develop irreconcilable specializations requiring more than the final block to remain unmerged.

### Mechanism 3: Test-Time Task Router via Value Subspace Projections
Task identity can be inferred from hidden state alignment with value projection principal components, enabling unsupervised task selection. For each candidate task, compute r_m = (1/2)(||P_T h_A||_2 + ||P_A h_T||_2) where P are top-k right singular vectors of value projections. Select task via softmax over scores. Value projections directly encode task-dependent information written into hidden states, whereas query/key govern attentional selection and risk collapsing to task-specific subspaces. Evidence shows value-only routing achieves 89.7% average success vs. 53.6% for key-only on LIBERO. Break condition occurs when embodiments differ significantly, requiring deeper expert heads and more sophisticated routing strategies.

## Foundational Learning

- **Model Merging via Task Vectors**
  - Why needed: MergeVLA builds on Task Arithmetic (TA) and TIES merging; understanding τ_m = Θ_m - Θ_0 and conflict resolution strategies is prerequisite.
  - Quick check: Given three fine-tuned models with task vectors τ_1, τ_2, τ_3, what happens when you compute τ_merge = (τ_1 + τ_2 + τ_3)/3 and apply it to a new input?

- **LoRA (Low-Rank Adaptation)**
  - Why needed: The paper analyzes LoRA adapter behavior during VLA fine-tuning and identifies "selfish parameters" in LoRA updates.
  - Quick check: If LoRA decomposes weight updates as ΔW = BA where B∈R^{d×r}, A∈R^{r×k}, what happens to the rank of merged updates when combining multiple LoRA adapters?

- **Attention Mechanisms in Transformers**
  - Why needed: Understanding cross-attention vs. self-attention is critical for grasping why removing self-attention preserves modularity.
  - Quick check: In cross-attention, what do Q, K, V represent and how does the mechanism differ from self-attention where Q, K, V all come from the same sequence?

## Architecture Onboarding

- **Component map:** Qwen2.5-0.5B VLM Backbone -> LoRA adapters with task masks -> L cross-attention-only action blocks -> Expert head (final block(s) per task) -> Test-time task router

- **Critical path:**
  1. Fine-tune separate models per task with LoRA on VLM, full training on action expert
  2. Compute merged task vector τ_merge via TA/TIES
  3. Generate task masks using λ=0.6 (default)
  4. Average action expert weights for blocks 1 to (l-1); keep expert heads separate
  5. At inference: run router on t=0 observation → select mask + head → execute

- **Design tradeoffs:**
  - λ mask ratio: Lower values (0.2) activate too many parameters causing interference; higher values (0.6-0.9) work better but reduce task-specific adaptation
  - Expert head depth: Single block (L) sufficient for same embodiment; 2-3 blocks needed for cross-embodiment
  - Router subspace: Value projections more stable than key/query but require SVD computation per task

- **Failure signatures:**
  - 0% success rate with standard merging → LoRA conflicts not masked
  - 0% success rate with masked VLM but merged action expert → Self-attention accumulating task dependencies
  - Router misassignment on cross-embodiment → Value subspaces insufficiently discriminative; need deeper expert heads

- **First 3 experiments:**
  1. **Mask ratio ablation**: Train 4 LIBERO tasks, merge with λ∈{0.2, 0.4, 0.6, 0.8}, plot success rate vs. active ratio to verify 0.6-0.9 sweet spot
  2. **Routing subspace comparison**: Implement K-only, V-only, K+V routing on masked merged model; confirm V-only achieves >85% on Object/Goal tasks (Table 5 pattern)
  3. **Architecture validation**: Train VLA-Adapter (with self-attention) vs. MergeVLA (cross-attention-only) on single LIBERO task, test on LIBERO-Plus perturbations to replicate 18.7% OOD improvement claim

## Open Questions the Paper Calls Out

### Open Question 1
Does MergeVLA's architectural modifications (removing self-attention, using cross-attention-only action experts) scale effectively to larger VLM backbones beyond the 0.5B parameter model tested? All experiments use Qwen2.5-0.5B as the backbone. Larger VLMs may have different internal representations, attention patterns, or LoRA fine-tuning dynamics that could affect mergeability. What evidence would resolve it: Experiments applying MergeVLA to 7B, 13B, or larger VLM backbones on the same benchmarks, comparing success rates and mergeability.

### Open Question 2
How does MergeVLA's performance and routing accuracy degrade as the number of merged skills increases beyond 4 tasks? LIBERO experiments merge only 4 tasks (Spatial, Object, Goal, Long). RoboTwin experiments merge 3 tasks. The paper shows "selfish ratio" increases with task count in Figure 3, suggesting scalability concerns. No experiments demonstrate merging 10, 20, or 100+ skills. The test-time task router selects among M candidates, which may become unreliable as M grows. What evidence would resolve it: Systematic evaluation merging 8, 16, 32, and 64 tasks, measuring success rate degradation and router accuracy.

### Open Question 3
Can the expert heads (final unmergeable blocks) eventually be unified through alternative architectural designs or merging techniques, or is task-specific specialization fundamentally necessary? Section 4.2 states: "the deeper blocks... remain unmergeable due to their strong task specialization. Consequently, each task keeps its own expert head." This requires storing M separate heads. The paper treats expert heads as inherently unmergeable but doesn't prove this is fundamental. Alternative approaches (shared low-rank bases, conditional computation) might unify them. What evidence would resolve it: Analysis of what information expert heads encode; experiments with alternative architectures that enable head merging without performance loss.

## Limitations

- Architectural details remain unspecified including exact action expert configuration (number of blocks, FFN dimensions, attention heads) and specific LoRA target modules within the VLM backbone
- Test-time task router effectiveness on cross-embodiment tasks shows reduced accuracy with more diverse embodiments, suggesting the value subspace projection method may not generalize robustly
- The method requires storing separate expert heads for each task, creating storage overhead that scales linearly with the number of merged skills

## Confidence

- **High Confidence**: The core mechanism of task-specific masking for LoRA conflict resolution and the empirical success rates on LIBERO benchmarks are well-supported by quantitative results and ablation studies
- **Medium Confidence**: The claim that cross-attention-only action experts preserve modularity is supported by success rate improvements, but the precise architectural impact requires further validation given unspecified implementation details
- **Low Confidence**: The test-time task router's effectiveness on cross-embodiment tasks (RoboTwin) is less validated, with routing success dropping in more diverse embodiments, suggesting the mechanism may not generalize robustly

## Next Checks

1. **Architecture Specification Audit**: Verify the exact number of blocks in the action expert and LoRA placement strategy by reconstructing the model from the codebase or supplementary materials
2. **Cross-Embodiment Router Robustness**: Test the task router on a broader range of embodiment variations beyond RoboTwin to assess scalability of the value subspace projection method
3. **Parameter Conflict Analysis**: Conduct a detailed ablation study on λ mask ratios across more than four values to map the boundary between sufficient conflict resolution and loss of task specificity