---
ver: rpa2
title: Efficient Multi-Task Scene Analysis with RGB-D Transformers
arxiv_id: '2306.05242'
source_url: https://arxiv.org/abs/2306.05242
tags:
- segmentation
- encoder
- semantic
- instance
- scene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EMSAFormer, an efficient multi-task scene analysis
  approach using a single RGB-D Transformer encoder. It replaces the dual CNN-based
  encoder in the previous EMSANet with a modified SwinV2 Transformer to handle both
  RGB and depth data effectively.
---

# Efficient Multi-Task Scene Analysis with RGB-D Transformers

## Quick Facts
- arXiv ID: 2306.05242
- Source URL: https://arxiv.org/abs/2306.05242
- Reference count: 40
- Single RGB-D Transformer encoder achieves state-of-the-art multi-task performance (panoptic segmentation, orientation estimation, scene classification) with real-time inference up to 39.1 FPS on Jetson AGX Orin.

## Executive Summary
EMSAFormer replaces the dual CNN encoder in EMSANet with a single SwinV2 Transformer, incorporating depth through split patch embedding (RGB: 64 channels, depth: 32 channels) and a wider 128-channel width. This design enables efficient RGB-D fusion while preserving modality-specific feature learning. The approach achieves state-of-the-art multi-task performance on NYUv2, SUNRGB-D, and ScanNet datasets, with real-time inference enabled by a custom TensorRT extension. Multi-task training notably improves instance segmentation quality compared to single-task baselines.

## Method Summary
The method employs a modified SwinV2-T backbone with split patch embedding that processes RGB and depth separately before fusion, using 128 total channels instead of the standard 96. A custom NVIDIA TensorRT extension accelerates inference on embedded hardware by decomposing the encoder into smaller blocks and supporting arbitrary input resolutions. The architecture uses an EMSANet-style CNN decoder for instance segmentation and a modified SegFormer MLP decoder for semantic segmentation, trained jointly with scene classification via global average pooling. Multi-task training with hard parameter sharing improves instance segmentation performance compared to single-task training, particularly on smaller datasets.

## Key Results
- Achieves state-of-the-art panoptic segmentation (PQ: 49.5 on NYUv2, 45.3 on SUNRGB-D) while running up to 39.1 FPS on Jetson AGX Orin
- Single RGB-D Transformer encoder outperforms dual CNN encoders in multi-task settings despite 6% PQ gap in single-task instance segmentation
- Custom TensorRT extension enables real-time inference with up to 2.3× speedup over PyTorch implementation on embedded hardware

## Why This Works (Mechanism)

### Mechanism 1
Splitting patch embedding and widening channel capacity enables single-encoder RGB-D fusion without early modality mixing. The architecture separates RGB (64 channels) and depth (32 channels) into distinct patch embeddings, totaling 128 channels instead of the original 96. Since each attention head in SwinV2 processes a subset of 32 channels, depth features are processed independently before MLPs optionally combine them via learned skip connections. This approach assumes RGB and depth have deviating statistics that early fusion degrades; independent attention heads can learn modality-specific features before fusion.

### Mechanism 2
Multi-task training with hard parameter sharing improves instance segmentation quality compared to single-task training on small datasets. Training semantic segmentation, instance segmentation, orientation estimation, and scene classification simultaneously forces the encoder to learn more generalizable features. The ~6% PQ gap between CNN and Transformer encoders in single-task instance segmentation partially closes in multi-task settings, indicating that encoder features learned in multi-task setting are more beneficial for instance segmentation.

### Mechanism 3
Custom TensorRT block decomposition enables real-time inference while preserving architectural flexibility. Instead of optimizing the encoder as a monolithic block, the extension splits SwinV2 into smaller units, supports arbitrary input resolutions via bottom-right padding, and enables ONNX import with access to intermediate features for skip connections. This assumes Transformer operations can be efficiently mapped to TensorRT with minimal overhead; window attention padding can be handled without significant accuracy loss.

## Foundational Learning

- **Vision Transformer patch embedding**: Understanding how 4×4 convolutions convert images to tokens is essential before modifying embedding for RGB-D split. Can you explain why patch embedding uses non-overlapping patches and how this differs from CNN convolution?
- **Shifted window attention (SW-MSA)**: SwinV2 uses 8×8 windows with alternating W-MSA and SW-MSA; modifying patch embedding must preserve window alignment. Why does Swin alternate between regular and shifted windows, and what happens if input dimensions aren't divisible by window size?
- **Bottom-up panoptic segmentation**: EMSAFormer uses Panoptic DeepLab-style approach with center heatmaps and offset regression rather than mask-based instance heads. How does bottom-up instance segmentation differ from top-down approaches like Mask R-CNN in terms of computational cost and post-processing?

## Architecture Onboarding

- Component map: RGB Input (640×480) → Split Patch Embedding (RGB: 64ch, D: 32ch) → SwinV2-T Stages → Context Module → Scene Head (FC) + Semantic Decoder (SegFormer MLP) + Instance Decoder (EMSANet style) → Center/Offset/Orientation heads

- Critical path:
  1. Pretrained weights: Must use provided SwinV2-T-128-Multi-Aug weights; ImageNet-only weights will underperform
  2. TensorRT deployment: Install custom extension before attempting embedded inference; ONNX export requires the extension's padding support
  3. Multi-task loss balancing: Use provided training pipeline; PQ determines checkpoint selection

- Design tradeoffs:
  - SwinV2-T vs. larger variants: Tiny enables real-time; Small/Base increase inference 1.5-1.9× for marginal gains
  - Decoder choice: SegFormer MLP decoder better for larger datasets (ScanNet); EMSANet decoder better for NYUv2
  - Single vs. dual encoder: Single encoder improves throughput but requires careful depth integration

- Failure signatures:
  - mIoU ~48% with vanilla SwinV2-T on RGB-D: Missing split embedding or pretraining
  - Instance PQ significantly lower than CNN baseline in single-task: Expected; train multi-task to recover
  - TensorRT crash on non-standard input sizes: Missing bottom-right padding implementation

- First 3 experiments:
  1. Reproduce single-task semantic segmentation on NYUv2 with SwinV2-T-128-Multi-Aug to validate encoder setup (target: ~50.5% mIoU)
  2. Run multi-task training and compare PQ between EMSANet decoder vs. modified SegFormer decoder to select decoder for target dataset size
  3. Deploy to Jetson AGX Orin with TensorRT extension; measure FPS at 30W and 50W power modes to verify real-time constraint (≥20 FPS)

## Open Questions the Paper Calls Out
None

## Limitations
- Loss balancing for multi-task training is not specified, which could lead to task conflict and degraded performance
- Pretraining augmentation strategy details are unspecified, making exact reproduction difficult
- Real-time performance claims depend heavily on custom TensorRT extension lacking independent validation

## Confidence

- **High**: Architectural modifications (split patch embedding, wider channels) are clearly specified and should be reproducible
- **Medium**: Dataset preprocessing and multi-task training improvements require careful hyperparameter tuning
- **Low**: Custom TensorRT extension performance claims lack independent verification

## Next Checks

1. **Loss weight sensitivity analysis**: Systematically vary multi-task loss weights and measure impact on PQ, mIoU, and MAAE to identify optimal balancing strategy
2. **Pretraining ablation study**: Compare SwinV2-T-128-Multi-Aug weights against ImageNet-only weights on RGB-D downstream tasks to quantify pretraining benefits
3. **TensorRT extension robustness test**: Evaluate inference throughput and accuracy across diverse input resolutions (not just 640×480) and window sizes to verify the bottom-right padding implementation handles edge cases