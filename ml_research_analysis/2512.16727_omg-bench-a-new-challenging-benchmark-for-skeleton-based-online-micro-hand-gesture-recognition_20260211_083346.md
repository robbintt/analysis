---
ver: rpa2
title: 'OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand
  Gesture Recognition'
arxiv_id: '2512.16727'
source_url: https://arxiv.org/abs/2512.16727
tags:
- gesture
- recognition
- memory
- micro
- gestures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OMG-Bench, the first large-scale public dataset
  for skeleton-based online micro hand gesture recognition, addressing limitations
  in existing gesture recognition datasets. OMG-Bench contains 40 fine-grained gesture
  classes with 13,948 instances across 1,272 sequences, featuring subtle motions,
  rapid dynamics, and continuous execution.
---

# OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition

## Quick Facts
- arXiv ID: 2512.16727
- Source URL: https://arxiv.org/abs/2512.16727
- Reference count: 40
- Introduces OMG-Bench, the first large-scale public dataset for skeleton-based online micro hand gesture recognition with 40 fine-grained classes and 13,948 instances

## Executive Summary
This paper introduces OMG-Bench, the first large-scale public dataset for skeleton-based online micro hand gesture recognition, addressing limitations in existing gesture recognition datasets. The dataset contains 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, featuring subtle motions, rapid dynamics, and continuous execution. To tackle the challenges of micro gesture recognition, the authors propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification through hierarchical memory banks storing frame-level details and window-level semantics. HMATr also employs learnable position-aware queries to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6% in detection rate, establishing a strong baseline for online micro gesture recognition.

## Method Summary
The paper addresses online micro hand gesture recognition by introducing OMG-Bench dataset and HMATr framework. OMG-Bench contains 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, featuring subtle motions, rapid dynamics, and continuous execution. The HMATr framework uses a lightweight ST-GCN backbone to extract frame features, which are then processed through hierarchical memory banks (frame-level and window-level) to preserve historical context. Learnable position-aware queries are initialized from memory and attend to both memory banks and current features to predict gesture class and temporal interval simultaneously. The model is trained using a combined bipartite matching loss and query-based CTC loss to handle detection and classification in an end-to-end manner.

## Key Results
- HMATr achieves 89.2% detection rate on OMG-Bench, outperforming state-of-the-art methods by 7.6%
- The dataset features challenging characteristics including subtle motions, rapid dynamics, and significant temporal variation (0.26s to 2.1s)
- Hierarchical memory architecture significantly improves performance by preserving context in non-overlapping window schemes

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Context Restoration
The hierarchical memory architecture compensates for information loss in non-overlapping sliding window schemes by maintaining two FIFO queues: a Frame-level Memory Bank storing low-level joint details and a Window-level Memory Bank storing high-level semantic embeddings. By attending to these banks, the model recovers temporal context that would otherwise be truncated at window boundaries.

### Mechanism 2: Implicit Localization via Learnable Queries
Instead of separate detection and classification heads, learnable Position-aware Queries are initialized from window-level memory and cross-attend to frame features. They are trained via bipartite matching to predict both class and temporal interval simultaneously, unifying detection and classification in one framework.

### Mechanism 3: Noise Suppression via Sequence-level Alignment
The auxiliary Query-CTC loss enforces global sequence consistency by encouraging the network to output "blank" tokens for noise and align predictions to the ground truth sequence without strict frame-level alignment, reducing false positives in subtle micro-gestures.

## Foundational Learning

- **Spatial-Temporal Graph Convolutional Networks (ST-GCN)**
  - Why needed: The input data is a sequence of hand skeletons (graphs). ST-GCN is the backbone feature extractor.
  - Quick check: Can you explain how adjacency matrices are used to model bone connections in a static skeleton graph?

- **Transformer Cross-Attention**
  - Why needed: The core interaction mechanism between the Memory Banks and the Learnable Queries relies on Q, K, V attention.
  - Quick check: In the context of this paper, what represents the Query and what represents the Key/Value during the "Frame Memory Interaction"?

- **Bipartite Matching (Hungarian Algorithm)**
  - Why needed: The training loss requires matching N predicted queries to M ground truth gestures optimally before calculating loss.
  - Quick check: Why is a simple Mean Squared Error insufficient for training a variable number of output predictions?

## Architecture Onboarding

- **Component map:** Input Skeletons -> ST-GCN -> Positional Encoding -> Memory Update/Retrieval -> Query Initialization -> Cross-Attention -> Prediction Heads
- **Critical path:** Input Skeletons → ST-GCN → Positional Encoding → Memory Update/Retrieval → Query Initialization → Cross-Attention → Prediction Heads
- **Design tradeoffs:** Uses non-overlapping windows (stride=16) for efficiency, trading off context continuity restored by Memory Banks; fixed 10 queries balance multi-gesture detection vs false positives
- **Failure signatures:** Gesture Merging (high confidence for single long gesture when two short same-class gestures occurred), High False Positives (random activations during static hand holds)
- **First 3 experiments:** 1) Baseline vs. HMATr on OMG-Bench to verify 10% performance gap, 2) Memory Ablation with L_f=1, L_w=1 to prove hierarchical context value, 3) Noise Robustness test with added Gaussian noise to joint coordinates

## Open Questions the Paper Calls Out

### Open Question 1
How can the performance gap between multi-view training data and single-view inference be minimized for consumer VR/AR devices? While HMATr demonstrates generalization, the substantial accuracy loss (from 89.2% to 73.6%) highlights a domain shift between high-precision multi-view skeletons and noisy monocular inputs.

### Open Question 2
Can the proposed non-overlapping sliding window strategy robustly handle micro gestures with extremely short durations or ambiguous boundaries? Although hierarchical memory mitigates context loss, the analysis does not isolate errors caused by gestures falling precisely at window boundaries.

### Open Question 3
How does the framework extend to bi-manual micro gestures or hand-object interactions common in complex VR scenarios? Real-world interaction often involves two hands or tools, which introduces self-occlusion and complex joint correlations not present in the single-hand OMG-Bench dataset.

## Limitations
- Dataset availability: OMG-Bench is not yet publicly accessible, blocking direct reproduction
- Limited generalizability: Performance claims rely on controlled experimental comparisons within OMG-Bench domain only
- Query count assumption: The choice of 10 learnable queries appears arbitrary and may not scale to high gesture density scenarios

## Confidence
- HMATr architecture performance claims (Table 3): **High** - supported by controlled ablations and state-of-the-art comparisons
- OMG-Bench dataset quality claims: **Medium** - semi-automatic annotation described but MPJPE error only reported for one subject pair
- Generalizability of HMATr to other gesture recognition tasks: **Low** - no experiments outside OMG-Bench domain

## Next Checks
1. Reproduce the core HMATr ablation study (PQ removal) on a subset of OMG-Bench to verify the 7% DR drop
2. Implement the semi-automatic annotation pipeline on a small hand gesture dataset to validate the 2.78mm MPJPE claim
3. Test HMATr on a different skeleton-based action recognition dataset (e.g., NTU-RGB+D) to assess cross-domain performance