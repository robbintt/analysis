---
ver: rpa2
title: Generating particle physics Lagrangians with transformers
arxiv_id: '2501.09729'
source_url: https://arxiv.org/abs/2501.09729
tags:
- fields
- lagrangians
- terms
- field
- lagrangian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that transformer models can successfully
  generate valid particle physics Lagrangians given a list of fields. The authors
  treat Lagrangians as complex symbolic expressions similar to sentences, and train
  a transformer (BART) model to predict the Lagrangian terms from a set of fields
  and their symmetries.
---

# Generating particle physics Lagrangians with transformers

## Quick Facts
- arXiv ID: 2501.09729
- Source URL: https://arxiv.org/abs/2501.09729
- Reference count: 40
- Key outcome: Transformer models can successfully generate valid particle physics Lagrangians given a list of fields, treating them as symbolic-linguistic expressions.

## Executive Summary
This paper demonstrates that transformer models can successfully generate valid particle physics Lagrangians from lists of input fields. By treating Lagrangians as complex symbolic expressions similar to sentences, the authors train a BART transformer to predict Lagrangian terms from field definitions and their symmetries. Using a specialized dataset of ~280K examples and a custom tokenization scheme that encodes fields, derivatives, contractions, and symmetry information, the model achieves high accuracy (>90%) on in-distribution test data. The approach shows strong generalization to non-standard scenarios while maintaining gauge invariance, demonstrating transformers' capability for symbolic manipulation in theoretical physics.

## Method Summary
The authors construct a specialized dataset of ~280K Lagrangians using AutoEFT and custom code generation, focusing on the Standard Model gauge group SU(3)×SU(2)×U(1). They use a BART architecture (357M parameters, 12 layers, 1024 hidden size) trained on a single A100 GPU for one week. The model takes tokenized field definitions as input and generates Lagrangian terms including contractions. Evaluation uses custom metrics accounting for term order-invariance. The approach treats fields and their symmetries analogously to words and grammatical structures in natural language.

## Key Results
- The model achieves >90% accuracy on in-distribution test data for generating valid Lagrangians
- Strong out-of-distribution generalization to up to 10 fields and non-standard hypercharges
- Embedding analysis reveals learned concepts of symmetry representations and conjugation operations
- Performance degrades for very long sequences (>6 fields) and complex trilinear terms

## Why This Works (Mechanism)

### Mechanism 1: Symbolic-linguistic analogy for constraint satisfaction
The model generates valid Lagrangians by learning to complete symbolic expressions under symmetry constraints, analogous to grammatical completion in language. Lagrangians are tokenized into sequences where quantum numbers act like grammatical features, and the transformer learns to complete sequences where each term must satisfy group-theoretic singlet constraints. This works because constraints of gauge symmetry can be captured via sequence statistics without explicit group-theoretic computation.

### Mechanism 2: Specialized tokenization preserves symmetry information
A domain-specific tokenization scheme enables the model to reason about representations and contractions by making symmetry properties explicit in the sequence. Each field is tokenized with explicit symmetry labels (SU(3), SU(2), U(1), spin) and ID tokens, while contractions are encoded via explicit blocks with symmetry tags. This exposes representation type, hypercharge, and index relationships directly to attention, allowing the transformer to attend to and combine symmetry tokens to enforce singlet constraints.

### Mechanism 3: Representation learning of group-theoretic concepts
The model appears to learn structured embeddings that encode symmetry representations and conjugation as consistent directions. t-SNE visualization shows clustering by SU(3)/SU(2)/U(1) representations, with conjugate fields exhibiting consistent offset vectors in embedding space. This suggests the model has internalized conjugation as a transformation axis and learned the structure and relationships between various field types and their symmetries.

## Foundational Learning

- **Concept: Particle physics Lagrangians and gauge symmetries**
  - Why needed here: Understanding what makes a term allowed or forbidden under SU(3)×SU(2)×U(1) is prerequisite to interpreting inputs, outputs, and failure modes.
  - Quick check question: Given a triplet under SU(3) with hypercharge 1/3 and an anti-triplet with hypercharge -1/3, can they form a gauge-invariant mass term? Why or why not?

- **Concept: Transformer encoder–decoder (BART) architecture**
  - Why needed here: The model uses a BART-style bidirectional encoder and autoregressive decoder; understanding the role of each component is essential for debugging generation and analyzing attention.
  - Quick check question: What is the difference in information flow between the encoder (which sees the full input) and the decoder (which generates left-to-right)? How might this affect counting and long-range dependencies?

- **Concept: Symbolic tokenization and vocabulary design**
  - Why needed here: The tokenization is non-BPE, hand-designed to preserve symmetry semantics; evaluating generalization requires understanding what is in-vocabulary vs. OOD.
  - Quick check question: How would the tokenization represent a field with SU(3) representation 6 (sextet) if the vocabulary only includes 1, 3, and ¯3? What would be a reasonable extension?

## Architecture Onboarding

- **Component map:**
  Input fields → Custom tokenizer → 12-layer bidirectional encoder → 12-layer autoregressive decoder with cross-attention → Token sequence representing Lagrangian terms

- **Critical path:**
  1. Define field quantum numbers (spin, SU(3) rep, SU(2) rep, hypercharge)
  2. Tokenize each field with symmetry tags and a unique ID
  3. Pass input sequence through encoder; decoder generates Lagrangian term-by-term
  4. Post-process to verify reasonableness (mass dimension, index consistency, U(1) conservation)

- **Design tradeoffs:**
  - Custom tokenization vs. BPE: Chosen for interpretability and OOD robustness, at cost of larger vocabulary and potential scalability issues
  - Dataset sampling: Sampled dataset oversimplifies examples to prime learning; uniform dataset may generalize more evenly but requires more data
  - Implicit gauge fields: Represented via covariant derivatives rather than explicit gauge bosons; reduces sequence length but may limit explicit gauge interaction learning

- **Failure signatures:**
  - Counting failures: For >6 input fields, the model misses terms or reuses fields incorrectly, linked to encoder's difficulty with contextual counting
  - OOD hypercharge: Performance drops for non-minimal-fraction hypercharges; text-based number encoding may lead to memorization rather than arithmetic
  - Missing trilinear terms: Especially Yukawa terms (two fermions + one scalar) in complex scenarios like the full Standard Model

- **First 3 experiments:**
  1. **In-distribution baseline:** Evaluate the sampled model on the merged test set (64K Lagrangians) using the provided metrics (S_Lagrangian, S_Contraction, P_Length). Report accuracy by number of fields and presence of trilinear terms.
  2. **Ablate tokenization:** Replace symmetry tokens with opaque IDs (e.g., map each field to a single unique token) and retrain/fine-tune. Compare performance and embedding clustering to assess the contribution of explicit symmetry information.
  3. **OOD hypercharge test:** Create a small held-out set with non-minimal-fraction U(1) charges (e.g., 2/2, 3/6, 4/8 instead of 1). Evaluate whether the model conserves U(1) and whether it assigns correct charges.

## Open Questions the Paper Calls Out

### Open Question 1
Can a continuous numerical tokenization scheme for hypercharges improve Out-Of-Distribution (OOD) generalization compared to the current text-based encoding? The current model struggles to interpret non-minimal fractions for U(1) charges (e.g., 2/4 instead of 1/2), leading to significant accuracy drops. A continuous number encoding approach would have helped in this scenario.

### Open Question 2
Can specialized transformer architectures or specific data rebalancing strategies mitigate the "contextual counting" failures observed in high-field-count OOD scenarios? The current BART architecture's non-causal encoder struggles to count fields accurately when input sequence length increases beyond the training distribution. Specialized counting architecture or data rebalancing could help mitigate missed or faulty terms.

### Open Question 3
How does the inclusion of non-redundant Lagrangian terms affect the training complexity and convergence of the transformer model? The current dataset includes redundant terms, and going towards non-redundant scenarios would add another layer of complexity for the model to overcome. This affects the difficulty of learning minimality as an open problem.

## Limitations

- **Counting and compositionality:** The model's performance degrades notably for >6 input fields and fails to consistently generate trilinear terms, suggesting limited capacity for multi-field combinatorics.
- **OOD generalization brittleness:** The model fails for non-minimal U(1) hypercharges and higher-dimensional SU(3)/SU(2) representations not in the vocabulary, indicating pattern completion rather than symbolic arithmetic.
- **Implicit symmetry enforcement:** The model relies on training data statistics to encode singlet constraints rather than explicit algebraic reasoning, limiting extrapolation to novel theories.

## Confidence

- **High confidence:** In-distribution performance (>90% accuracy), tokenization scheme preserves symmetry semantics, embedding structure reflects representation clusters
- **Medium confidence:** Symbolic-linguistic analogy is plausible but not directly tested; OOD generalization to non-standard hypercharges is partial and brittle
- **Low confidence:** Claim of learned "concepts" like conjugation is based on t-SNE visualization, which may reflect superficial correlation rather than structured understanding

## Next Checks

1. **Counting capacity experiment:** Systematically evaluate the model's ability to generate complete Lagrangians as input field count increases (2→10 fields). Measure recall of trilinear terms and identify the field count threshold where performance collapses. Compare against brute-force enumeration to quantify combinatorial completeness.

2. **Explicit symmetry verification:** For each generated Lagrangian, compute the actual SU(3)×SU(2)×U(1) invariance by contracting indices and summing hypercharges. Report the fraction of generated terms that are truly gauge-invariant vs. those that appear valid under learned patterns but fail explicit group-theoretic checks.

3. **Representation learning stress test:** Create a held-out test set with higher-dimensional SU(3)/SU(2) representations (e.g., 6, 8, 10) and non-minimal U(1) hypercharges (e.g., 2/3, 3/4, 5/6). Evaluate whether performance degrades smoothly (suggesting learned rules) or catastrophically (suggesting memorization).