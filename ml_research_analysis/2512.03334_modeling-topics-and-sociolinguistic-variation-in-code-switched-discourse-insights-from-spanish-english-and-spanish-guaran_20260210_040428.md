---
ver: rpa2
title: "Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse:\
  \ Insights from Spanish-English and Spanish-Guaran\xED"
arxiv_id: '2512.03334'
source_url: https://arxiv.org/abs/2512.03334
tags:
- language
- topic
- bilingual
- discourse
- code-switching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper presents a large language model (LLM)-assisted annotation\
  \ pipeline for topic and sociolinguistic analysis of code-switched bilingual discourse.\
  \ It applies the approach to Spanish-English and Spanish-Guaran\xED datasets, automatically\
  \ labeling topics, genres, and discourse functions across 3,691 code-switched sentences."
---

# Modeling Topics and Sociolinguistic Variation in Code-Switched Discourse: Insights from Spanish-English and Spanish-Guaraní

## Quick Facts
- arXiv ID: 2512.03334
- Source URL: https://arxiv.org/abs/2512.03334
- Reference count: 0
- A large language model-assisted annotation pipeline successfully labels topics, genres, and discourse functions in Spanish-English and Spanish-Guaraní code-switched corpora, revealing systematic sociolinguistic patterns.

## Executive Summary
This paper presents an LLM-assisted pipeline for annotating code-switched bilingual discourse with topics, genres, and discourse functions. The approach is applied to Spanish-English data from the Miami corpus and Spanish-Guaraní data from social media, achieving high annotation accuracy through structured prompting with fixed taxonomies. The method integrates speaker metadata to reveal sociolinguistic patterns, showing gender and language dominance effects in Miami and diglossic variation in Paraguayan data. The results demonstrate that LLMs can reliably recover interpretable sociolinguistic patterns traditionally requiring manual annotation, enabling large-scale quantitative analysis of code-switching phenomena.

## Method Summary
The study uses GPT-4.1 via OpenAI API with deterministic parameters (temp=0, max_tokens=200) to annotate code-switched sentences with topic, genre, and discourse function labels. The pipeline processes batches of 50-100 sentences using structured JSON prompts that constrain output to predefined taxonomies developed through iterative schema refinement on sample sentences. For Spanish-English, 2,825 sentences from the Miami corpus with demographic metadata are annotated; for Spanish-Guaraní, 866 sentences from social media are annotated with formality and genre labels. Manual evaluation on 30 random sentences per dataset validates accuracy, which reaches 100% for primary labels in Miami and 94.17% in Spanish-Guaraní.

## Key Results
- LLM annotation pipeline achieves 100% accuracy for topic and function in Miami corpus, 94.17% for Spanish-Guaraní
- Gender and language dominance systematically correlate with discourse function patterns in Miami data
- Spanish-Guaraní data reveals diglossic patterns with formal Guaraní in official contexts versus informal Spanish in personal domains
- Secondary function annotations show lower accuracy (60%), indicating limitations in capturing fine-grained discourse functions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constrained structured prompting enables reliable discourse-level annotation of code-switched text by LLMs.
- Mechanism: Three-component prompt architecture (system instructions, base prompt, instruction lists) forces the model to select from predefined linguistic categories rather than generating free-form interpretations. JSON-only output constraint eliminates parsing ambiguity; temperature=0 ensures deterministic outputs for reproducibility.
- Core assumption: The LLM has sufficient cross-lingual representations to interpret mixed-language semantics despite being trained primarily on monolingual data.
- Evidence anchors: [abstract] "achieves high annotation accuracy—100% for topic and function in the Miami corpus, 94.17% for Spanish-Guaraní"; [section 4.2] "Return only strict JSON no extra text or explanations"; [section 3.2] "deterministic parameters (temp=0, max_tokens=200)"; [corpus] Weak corpus-level evidence—paper evaluates on only 30 sentences per dataset via manual review, not large-scale inter-annotator agreement.

### Mechanism 2
- Claim: Iterative schema refinement grounded in corpus samples yields taxonomies that balance coverage with interpretability.
- Mechanism: Sample 30 sentences → bilingual annotators develop initial schema → review additional 30 sentences → merge overlapping categories → add explanatory notes. This bootstrapping grounds categories in observed discourse patterns rather than a priori theory alone.
- Core assumption: The sampled sentences are representative of the full corpus diversity; small-sample schema design generalizes.
- Evidence anchors: [section 4.1] "After initial annotation, the schemas were iteratively refined by reviewing an additional batch of 30 sentences"; [section 4.1] "Semantically overlapping categories were merged"; [corpus] No direct corpus evidence for generalization validity—paper acknowledges "topic and discourse categorization is inherently subjective."

### Mechanism 3
- Claim: Integrating speaker metadata with discourse annotations enables recovery of sociolinguistic patterns previously accessible only through manual analysis.
- Mechanism: Each sentence is annotated with both (topic, function) labels AND linked to demographic metadata (gender, age, language dominance). Aggregating across these dimensions reveals systematic variation—for example, women showing higher rates of solidarity/directive functions.
- Core assumption: Demographic metadata is accurate and consistently collected; the relationships are not confounded by unobserved variables.
- Evidence anchors: [abstract] "systematic links between gender, language dominance, and discourse function in the Miami data"; [section 5.1.2] "Spanish-dominant segments cluster around casual, affective, and narrative domains"; [section 5.2.2] "Guaraní-dominant texts are concentrated in 'Government_Announcement'... Spanish-dominant texts emphasize 'Personal_Emotional'"; [corpus] Limited—findings replicate prior sociolinguistic observations but are correlational, not causal.

## Foundational Learning

- Concept: **Code-switching as discourse strategy**
  - Why needed here: The entire annotation schema assumes switching is motivated (topic shift, stance emphasis, lexical gap) rather than random; understanding this frames why each sentence needs a function label.
  - Quick check question: Can you name three discourse functions that might motivate a Spanish-English switch in a workplace conversation?

- Concept: **Diglossia and language dominance**
  - Why needed here: Spanish-Guaraní results show formal Guaraní vs. informal Spanish; interpreting results requires understanding how community norms assign languages to registers.
  - Quick check question: In a diglossic community, which language would you expect in a government announcement vs. a casual joke?

- Concept: **Few-shot prompting with constrained outputs**
  - Why needed here: The pipeline relies on structuring prompts so GPT outputs only valid JSON labels; understanding this enables debugging when outputs malformed.
  - Quick check question: What happens to annotation consistency if you set temperature > 0?

## Architecture Onboarding

- Component map:
  - Input layer (sentences + metadata) → Prompt constructor (system + base + exemplars) → LLM inference (GPT-4.1, temp=0) → Output parser (JSON validation) → Aggregation layer (sociolinguistic analysis)

- Critical path:
  1. Schema design (human annotators define topic/function taxonomies)
  2. Prompt engineering (encode schema into structured instructions)
  3. Batch inference (50-100 sentences per batch)
  4. Manual spot-check (30 sentences × 2 datasets)
  5. Distributional analysis by gender/dominance/formality

- Design tradeoffs:
  - Fixed vs. dynamic taxonomies: Fixed schemas enable cross-corpus comparison but may miss emergent topics; dynamic LLM-generated topics would be more flexible but less comparable.
  - High-resource vs. low-resource coverage: Spanish-English has rich metadata; Spanish-Guaraní has limited metadata but reveals diglossic patterns unavailable in high-resource pairs.
  - Annotation granularity: Primary labels are reliable; secondary_function drops to 60% accuracy—finer granularity trades accuracy for nuance.

- Failure signatures:
  - Malformed JSON output → prompt constraint failure; add stricter examples or post-hoc regex repair.
  - Inconsistent labels for similar sentences → schema ambiguity; refine category definitions with more exemplars.
  - Low accuracy on secondary labels → inherent task difficulty; consider dropping secondary annotations or using multi-pass labeling.
  - Sociolinguistic patterns that contradict prior literature → potential LLM bias or metadata quality issues; flag for manual review.

- First 3 experiments:
  1. Schema transfer test: Apply Miami schema to a new Spanish-English corpus (e.g., Bangor corpus other sections) without modification; measure accuracy drop to quantify schema portability.
  2. Ablation on metadata: Run annotation with and without speaker demographics in the prompt; compare whether gender/dominance information changes label distributions (tests for prompt-conditioned bias).
  3. Inter-annotator baseline: Have two human annotators label the same 30-sentence evaluation set independently; compute Cohen's κ against LLM labels to contextualize "100% accuracy" claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dynamic topic generation by LLMs capture fluid, context-dependent thematic structure in spontaneous discourse more effectively than fixed taxonomies?
- Basis in paper: [explicit] "Current annotation schemes rely on fixed topic taxonomies that promote comparability but may constrain discovery. Dynamic topic generation by LLMs offers a way to capture fluid, context-dependent thematic structure in spontaneous discourse."
- Why unresolved: Fixed taxonomies enable comparison but may miss overlapping topics (e.g., politics intersecting with education); the trade-off between comparability and discovery remains untested.
- What evidence would resolve it: Comparative evaluation of dynamic vs. fixed topic labeling on the same bilingual corpora, measuring topic coherence and coverage of overlapping themes.

### Open Question 2
- Question: How can measures of lexical overlap and semantic similarity operationalize the link between discourse-level switching patterns and cognitive processes of bilingual word retrieval?
- Basis in paper: [explicit] "Future modeling could operationalize these mechanisms by incorporating measures of lexical overlap and semantic similarity, thereby linking discourse-level switching patterns with cognitive processes of bilingual word retrieval."
- Why unresolved: The study identifies lexical accessibility as a potential factor in switching but does not implement or test quantitative measures.
- What evidence would resolve it: Integrating cognate detection and embedding-based similarity metrics into the annotation pipeline and correlating these with switch-point frequency and discourse function.

### Open Question 3
- Question: What frameworks can integrate discourse-pragmatic annotation with syntactic, semantic, and sociolinguistic layers to construct a unified multilevel model of bilingual production?
- Basis in paper: [explicit] "The next step is to integrate the annotation of discourse and pragmatics with syntactic, semantic, and sociolinguistic analyzes to construct a multilevel model of bilingual production."
- Why unresolved: Current work focuses on topic and function annotation; structural (dependency, syntactic) and affective (sentiment, stance) layers remain unconnected.
- What evidence would resolve it: A pilot study combining the existing annotations with dependency parsing and stance detection on the same code-switched sentences, demonstrating cross-layer correlations.

### Open Question 4
- Question: How do alternative annotation frameworks or annotator backgrounds affect topic and discourse-function labeling in code-switched data?
- Basis in paper: [inferred] The paper acknowledges that "topic and discourse categorization is inherently subjective; different annotators or frameworks could yield alternative but equally valid interpretations," but reports only one schema.
- Why unresolved: Inter-annotator agreement and cross-framework validation are not reported; generalizability of the proposed categories is unknown.
- What evidence would resolve it: Multi-annotator experiments using different linguistic frameworks (e.g., conversation analysis vs. functional pragmatics) with agreement metrics and qualitative analysis of systematic differences.

## Limitations
- The 100% accuracy claim is based on only 30-sentence evaluation samples rather than comprehensive inter-annotator agreement or large-scale validation.
- Schema development relies on just 60 sentences total, raising concerns about whether taxonomies generalize to full corpora.
- Secondary function annotations show significantly lower accuracy (60%), indicating fundamental limitations in capturing fine-grained discourse functions.
- Sociolinguistic patterns are correlational rather than causal, and lack of metadata for Spanish-Guaraní limits comparable demographic analysis.

## Confidence
- **High confidence**: The LLM annotation pipeline successfully applies structured prompts to generate JSON-labeled outputs for code-switched discourse; deterministic parameters (temp=0, max_tokens=200) and JSON-only constraints are explicitly implemented and reproducible.
- **Medium confidence**: The identified sociolinguistic patterns (gender/dominance correlations in Miami, diglossic patterns in Paraguayan data) reflect real variation, but limited evaluation sample size and correlational nature prevent definitive causal claims.
- **Low confidence**: The claim that 100% accuracy extends to the full corpus based on 30-sentence samples; generalizability of schema taxonomies to new datasets or language pairs without modification.

## Next Checks
1. Schema portability test: Apply the Miami schema to a different Spanish-English corpus (e.g., Bangor corpus sections not used in training) without modification and measure accuracy degradation to quantify schema generalization limits.
2. Metadata influence ablation: Run the annotation pipeline twice—once with demographic metadata in prompts and once without—then compare resulting label distributions to test whether metadata influences LLM output or merely organizes results.
3. Human baseline comparison: Have two independent bilingual annotators label the same 30-sentence evaluation sets for both corpora, then compute Cohen's κ agreement scores against LLM outputs to contextualize the claimed accuracy rates.