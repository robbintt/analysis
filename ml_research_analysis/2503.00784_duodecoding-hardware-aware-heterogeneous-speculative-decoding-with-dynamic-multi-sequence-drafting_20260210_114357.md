---
ver: rpa2
title: 'DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic
  Multi-Sequence Drafting'
arxiv_id: '2503.00784'
source_url: https://arxiv.org/abs/2503.00784
tags:
- draft
- decoding
- speculative
- generation
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the inefficiency of speculative decoding,
  where the draft model becomes a bottleneck during inference. The authors propose
  DuoDecoding, which deploys the draft model on the CPU and the target model on the
  GPU to enable parallel execution.
---

# DuoDecoding: Hardware-aware Heterogeneous Speculative Decoding with Dynamic Multi-Sequence Drafting

## Quick Facts
- arXiv ID: 2503.00784
- Source URL: https://arxiv.org/abs/2503.00784
- Reference count: 12
- Primary result: Achieves up to 2.61× speedup in generation latency and reduces time to first token by 17% compared to conventional speculative decoding

## Executive Summary
This paper addresses the inefficiency of speculative decoding where the draft model becomes a bottleneck during inference. The authors propose DuoDecoding, which deploys the draft model on the CPU and the target model on the GPU to enable parallel execution. They introduce a hardware-aware optimal draft budget to minimize idle times and dynamic multi-sequence drafting to improve draft quality based on uncertainty. Extensive experiments on seven tasks demonstrate significant performance improvements over conventional speculative decoding approaches.

## Method Summary
DuoDecoding eliminates the draft model bottleneck in speculative decoding by deploying the draft model on the CPU while running the target model on the GPU, enabling parallel execution. The approach uses a hardware-aware optimal draft budget γ=c (where c is the cost coefficient) to minimize idle times between draft and target model execution. Dynamic multi-sequence drafting is introduced, where draft sequences branch based on uncertainty thresholds θ=p₁,₁×p₂,₁, spawning independent sequences when the draft model shows high uncertainty. The method modifies the speculative sampling verification process to handle multi-sequence drafts, sampling from normalized max(p-q,0) distributions on rejection.

## Key Results
- Achieves up to 2.61× speedup in generation latency compared to conventional speculative decoding
- Reduces time to first token by 17% through heterogeneous deployment
- Maintains or improves acceptance rates through dynamic multi-sequence drafting
- Demonstrates consistent performance improvements across seven tasks from SpecBench and HumanEval

## Why This Works (Mechanism)
DuoDecoding works by addressing the fundamental bottleneck in speculative decoding where the draft model execution blocks the target model, creating idle GPU time. By deploying the draft model on the CPU and target model on the GPU, both can execute in parallel, eliminating this idle time. The hardware-aware budget ensures that the draft model produces exactly the right number of tokens to keep the target model busy without overproduction. Dynamic multi-sequence drafting improves draft quality by generating alternative sequences when uncertainty is high, reducing the likelihood of rejection and improving overall throughput.

## Foundational Learning
- **Speculative Decoding**: Why needed - To accelerate LLM inference by using a smaller draft model to generate candidate tokens that are verified by a larger target model. Quick check - Verify that acceptance rate remains above 50% for positions 2-γ/2.
- **Hardware-aware Budget**: Why needed - To minimize idle times by matching draft model production rate to target model consumption rate. Quick check - Measure draft_time vs target_time to ensure draft is not slower than target.
- **Heterogeneous Deployment**: Why needed - To enable parallel execution of draft and target models across different devices. Quick check - Profile CPU and GPU utilization to verify reduced idle times.
- **Dynamic Multi-Sequence Drafting**: Why needed - To improve draft quality by branching on uncertainty, reducing rejections. Quick check - Track acceptance rate by position to verify improvement in later positions.
- **Cost Coefficient (c)**: Why needed - To quantify the relative computational cost between draft and target models for optimal budget setting. Quick check - Measure time(target forward)/time(draft forward) across multiple workloads.
- **Uncertainty Threshold (θ)**: Why needed - To determine when draft model confidence is low enough to warrant branching. Quick check - Verify p₁,₁ and p₂,₁ are correctly extracted from draft output logits.

## Architecture Onboarding

**Component Map**: CPU Draft Model -> Inter-process Communication -> GPU Target Model -> Verification

**Critical Path**: Draft model generates tokens → Sends probabilities to GPU → Target model verifies tokens → Acceptance/rejection decision → Continue generation

**Design Tradeoffs**: Heterogeneous deployment introduces communication overhead between CPU and GPU but eliminates GPU idle time. Dynamic multi-sequence drafting increases complexity but improves acceptance rates. Hardware-aware budget requires careful cost coefficient measurement but optimizes resource utilization.

**Failure Signatures**: 
- Low acceptance rates at later positions indicate draft model quality issues
- GPU idle waiting for CPU draft indicates draft model is too slow or budget is too large
- Communication bottlenecks between CPU and GPU processes reduce speedup

**First Experiments**:
1. Measure cost coefficient c=time(target forward)/time(draft forward) with various sequence lengths and batch sizes
2. Implement basic heterogeneous speculative decoding with fixed budget γ=c and measure speedup
3. Add dynamic multi-sequence drafting with uncertainty threshold θ and compare acceptance rates

## Open Questions the Paper Calls Out
None

## Limitations
- Inter-process communication protocol between CPU and GPU processes is not fully specified, creating implementation challenges
- Normalization function in verification algorithm lacks specification (softmax vs. simple normalization)
- Exact input sequence length and batch configuration for cost coefficient measurement are unspecified
- KV cache synchronization strategy between heterogeneous devices not addressed

## Confidence

**High confidence**: The core concept of heterogeneous deployment to reduce idle times is sound and addresses a real bottleneck in speculative decoding.

**Medium confidence**: The reported performance improvements (2.61× speedup, 17% TTFT reduction) appear achievable given the methodology, though exact replication depends on implementation details.

**Low confidence**: The dynamic multi-sequence drafting mechanism's practical effectiveness may be limited by implementation challenges around cache management and sequence tracking.

## Next Checks

1. **Cost coefficient measurement validation**: Measure draft and target forward pass times across multiple sequence lengths and batch sizes to verify the reported cost coefficient values (c=27.5) and their stability across different workloads.

2. **Acceptance rate profiling**: Implement position-by-position acceptance rate tracking during speculative decoding to verify that acceptance rates remain above 50% in positions 2-γ/2 as claimed, and that multi-sequence drafting actually improves acceptance rates in later positions.

3. **Hardware utilization analysis**: Profile CPU and GPU utilization during heterogeneous inference to verify that the proposed approach actually reduces idle times. Compare with baseline speculative decoding where both models run on the same device.